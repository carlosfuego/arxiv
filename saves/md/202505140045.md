# Arxiv Results
## Keyword: kv cache 
 ### ABase: the Multi-Tenant NoSQL Serverless Database for Diverse and   Dynamic Workloads in Large-scale Cloud Environments
**Authors**: Rong Kang, Yanbin Chen, Ye Liu, Fuxin Jiang, Qingshuo Li, Miao Ma, Jian Liu, Guangliang Zhao, Tieying Zhang, Jianjun Chen, Lei Zhang

**Updated**: 2025-05-12T15:58:39Z

**Summary**: Multi-tenant architectures enhance the elasticity and resource utilization of NoSQL databases by allowing multiple tenants to co-locate and share resources. However, in large-scale cloud environments, the diverse and dynamic nature of workloads poses significant challenges for multi-tenant NoSQL databases. Based on our practical observations, we have identified three crucial challenges: (1) the impact of caching on performance isolation, as cache hits alter request execution and resource consumption, leading to inaccurate traffic control; (2) the dynamic changes in traffic, with changes in tenant traffic trends causing throttling or resource wastage, and changes in access distribution causing hot key pressure or cache hit ratio drops; and (3) the imbalanced layout of data nodes due to tenants' diverse resource requirements, leading to low resource utilization. To address these challenges, we introduce ABase, a multi-tenant NoSQL serverless database developed at ByteDance. ABase introduces a two-layer caching mechanism with a cache-aware isolation mechanism to ensure accurate resource consumption estimates. Furthermore, ABase employs a predictive autoscaling policy to dynamically adjust resources in response to tenant traffic changes and a multi-resource rescheduling algorithm to balance resource utilization across data nodes. With these innovations, ABase has successfully served ByteDance's large-scale cloud environment, supporting a total workload that has achieved a peak QPS of over 13 billion and total storage exceeding 1 EB.

**Link**: [arxiv](http://arxiv.org/abs/2505.07692v1),  [pdf](http://arxiv.org/pdf/2505.07692v1)

**Tags**: cs.DB 



### SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in   Large Language Models
**Authors**: Hang Wu, Jianian Zhu, Yinghui Li, Haojie Wang, Biao Hou, Jidong Zhai

**Updated**: 2025-05-12T15:46:28Z

**Summary**: Large Language Models (LLMs) present a critical trade-off between inference quality and computational cost: larger models offer superior capabilities but incur significant latency, while smaller models are faster but less powerful. Existing serving strategies often employ fixed model scales or static two-stage speculative decoding, failing to dynamically adapt to the varying complexities of user requests or fluctuations in system performance. This paper introduces \systemname{}, a novel framework that reimagines LLM inference as an adaptive routing problem solved through multi-level speculative decoding. \systemname{} dynamically constructs and optimizes inference "paths" (chains of models) based on real-time feedback, addressing the limitations of static approaches. Our contributions are threefold: (1) An \textbf{adaptive model chain scheduling} mechanism that leverages performance profiling (execution times) and predictive similarity metrics (derived from token distribution divergence) to continuously select the optimal sequence of draft and verifier models, minimizing predicted latency per generated token. (2) A \textbf{multi-level collaborative verification} framework where intermediate models within the selected chain can validate speculative tokens, reducing the verification burden on the final, most powerful target model. (3) A \textbf{synchronized state management} system providing efficient, consistent KV cache handling across heterogeneous models in the chain, including precise, low-overhead rollbacks tailored for asynchronous batch processing inherent in multi-level speculation. Preliminary experiments demonstrate the validity of our method.

**Link**: [arxiv](http://arxiv.org/abs/2505.07680v1),  [pdf](http://arxiv.org/pdf/2505.07680v1)

**Tags**: cs.LG cs.DC 



### All-optical electric field sensing with nanodiamond-doped polymer thin   films
**Authors**: Roy Styles, Mengke Han, Toon Goris, James Partridge, Brett C. Johnson, Blanca del Rosal, Amanda N. Abraham, Heike Ebendorff-Heidepriem, Brant C. Gibson, Nikolai Dontschuk, Jean-Philippe Tetienne, Philipp Reineck

**Updated**: 2025-05-12T08:44:10Z

**Summary**: The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that exists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the NV's nanoscale environment. Here, we show that photoluminescence (PL) from NV centers in fluorescent nanodiamonds (FNDs) can be employed for all-optical voltage sensing based on electric field-induced NV charge state modulation. More than 95% of FNDs integrated into a capacitor device show a transient increase in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of an external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The change in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V, corresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices. The electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$. We investigate the NV charge state photodynamics on the millisecond timescale and find that the change in NV PL strongly depends on the rate of photoexcitation. We propose a model that qualitatively explains the observed changes in NV PL based on an electric field-induced redistribution of photoexcited electrons from substitutional nitrogen defects to NV centers, leading to a transient conversion of NV$^0$ to NV$^-$ centers upon application of an external voltage. Our results contribute to the development of FNDs as reliable, all-optical, nanoscale electric field sensors in solid-state systems.

**Link**: [arxiv](http://arxiv.org/abs/2505.07350v1),  [pdf](http://arxiv.org/pdf/2505.07350v1)

**Tags**: cond-mat.mes-hall 



### Cache-Efficient Posterior Sampling for Reinforcement Learning with   LLM-Derived Priors Across Discrete and Continuous Domains
**Authors**: Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma

**Updated**: 2025-05-12T06:53:24Z

**Summary**: Integrating large language models (LLMs) as priors in reinforcement learning (RL) offers significant advantages but comes with substantial computational costs. We present a principled cache-efficient framework for posterior sampling with LLM-derived priors that dramatically reduces these costs while maintaining high performance. At the core of our approach is an adaptive caching mechanism, where cache parameters are meta-optimized using surrogate gradients derived from policy performance. This design enables efficient inference across both discrete text environments (e.g., TextWorld, ALFWorld) and continuous control domains (e.g., MuJoCo), achieving a 3.8--4.7$\times$ reduction in LLM queries and 4.0--12.0$\times$ lower median latencies (85--93\,ms on a consumer GPU) while retaining 96--98\% of uncached performance. Our theoretical analysis provides KL divergence bounds on approximation quality, validated empirically. The framework extends to offline RL, where our CQL-Prior variant improves performance by 14--29\% and reduces training time by 38--40\%. Extensive evaluations across a diverse suite of eight tasks demonstrate the generalizability and practical viability of LLM-guided RL in resource-constrained settings.

**Link**: [arxiv](http://arxiv.org/abs/2505.07274v1),  [pdf](http://arxiv.org/pdf/2505.07274v1)

**Tags**: cs.LG 



### Comet: Accelerating Private Inference for Large Language Model by   Predicting Activation Sparsity
**Authors**: Guang Yan, Yuhui Zhang, Zimu Guo, Lutan Zhao, Xiaojun Chen, Chen Wang, Wenhao Wang, Dan Meng, Rui Hou

**Updated**: 2025-05-12T05:29:30Z

**Summary**: With the growing use of large language models (LLMs) hosted on cloud platforms to offer inference services, privacy concerns about the potential leakage of sensitive information are escalating. Secure multi-party computation (MPC) is a promising solution to protect the privacy in LLM inference. However, MPC requires frequent inter-server communication, causing high performance overhead.   Inspired by the prevalent activation sparsity of LLMs, where most neuron are not activated after non-linear activation functions, we propose an efficient private inference system, Comet. This system employs an accurate and fast predictor to predict the sparsity distribution of activation function output. Additionally, we introduce a new private inference protocol. It efficiently and securely avoids computations involving zero values by exploiting the spatial locality of the predicted sparse distribution. While this computation-avoidance approach impacts the spatiotemporal continuity of KV cache entries, we address this challenge with a low-communication overhead cache refilling strategy that merges miss requests and incorporates a prefetching mechanism. Finally, we evaluate Comet on four common LLMs and compare it with six state-of-the-art private inference systems. Comet achieves a 1.87x-2.63x speedup and a 1.94x-2.64x communication reduction.

**Link**: [arxiv](http://arxiv.org/abs/2505.07239v1),  [pdf](http://arxiv.org/pdf/2505.07239v1)

**Tags**: cs.CR cs.AI 



### PrefillOnly: An Inference Engine for Prefill-only Workloads in Large   Language Model Applications
**Authors**: Kuntai Du, Bowen Wang, Chen Zhang, Yiming Cheng, Qing Lan, Hejian Sang, Yihua Cheng, Jiayi Yao, Xiaoxuan Liu, Yifan Qiao, Ion Stoica, Junchen Jiang

**Updated**: 2025-05-12T03:22:29Z

**Summary**: Besides typical generative applications, like ChatGPT, GitHub Copilot, and Cursor, we observe an emerging trend that LLMs are increasingly used in traditional discriminative tasks, such as recommendation, credit verification, and data labeling. The key characteristic of these emerging use cases is that the LLM generates only a single output token, rather than an arbitrarily long sequence of tokens. We call this prefill-only workload. However, since existing LLM engines assume arbitrary output lengths, they fail to leverage the unique properties of prefill-only workloads. In this paper, we present PrefillOnly, the first LLM inference engine that improves the inference throughput and latency by fully embracing the properties of prefill-only workloads. First, since it generates only one token, PrefillOnly only needs to store the KV cache of only the last computed layer, rather than of all layers. This drastically reduces the GPU memory footprint of LLM inference and allows handling long inputs without using solutions that reduces throughput, such as cross-GPU KV cache parallelization. Second, because the output length is fixed, rather than arbitrary, PrefillOnly can precisely determine the job completion time (JCT) of each prefill-only request before it starts. This enables efficient JCT-aware scheduling policies such as shortest remaining job first. PrefillOnly can process upto 4x larger queries per second without inflating average and P99 latency.

**Link**: [arxiv](http://arxiv.org/abs/2505.07203v1),  [pdf](http://arxiv.org/pdf/2505.07203v1)

**Tags**: cs.DC 



### Ecco: Improving Memory Bandwidth and Capacity for LLMs via Entropy-aware   Cache Compression
**Authors**: Feng Cheng, Cong Guo, Chiyue Wei, Junyao Zhang, Changchun Zhou, Edward Hanson, Jiaqi Zhang, Xiaoxiao Liu, Hai "Helen" Li, Yiran Chen

**Updated**: 2025-05-11T08:44:31Z

**Summary**: Large language models (LLMs) have demonstrated transformative capabilities across diverse artificial intelligence applications, yet their deployment is hindered by substantial memory and computational demands, especially in resource-constrained environments. Quantization techniques have emerged as a critical solution, reducing data precision to enhance memory and computational efficiency. However, existing methods often suffer from high runtime overheads and potential accuracy degradation. To address these challenges, we propose Ecco, an entropy-based cache compression technique tailored for LLMs. Ecco combines group-wise and non-uniform quantization with pre-defined shared k-means patterns and Huffman coding to exploit the inherent entropy characteristics of LLM cache data. Recognizing the inefficiencies of traditional Huffman coding in terms of parallelism and latency, we introduce a novel parallel Huffman-based decoding process with a multi-stage pipeline design, reducing latency by two orders of magnitude and achieving throughput comparable to GPU L2 caches. Comprehensive evaluations demonstrate that Ecco achieves an up to 2.9$\times$ and 1.9$\times$ speedup over the state-of-the-art AWQ and SmoothQuant framework, 2.4$\times$ over the Olive accelerator, all while increasing memory capacity by nearly 4$\times$ and maintaining state-of-the-art LLM accuracy. These results underscore the effectiveness of our entropy-based cache compression in enhancing LLM performance and efficiency, paving the way for more deployable large-scale AI models.

**Link**: [arxiv](http://arxiv.org/abs/2505.06901v1),  [pdf](http://arxiv.org/pdf/2505.06901v1)

**Tags**: cs.AR 



### I Know What You Said: Unveiling Hardware Cache Side-Channels in Local   Large Language Model Inference
**Authors**: Zibo Gao, Junjie Hu, Feng Guo, Yixin Zhang, Yinglong Han, Siyuan Liu, Haiyang Li, Zhiqiang Lv

**Updated**: 2025-05-10T19:06:37Z

**Summary**: Large Language Models (LLMs) that can be deployed locally have recently gained popularity for privacy-sensitive tasks, with companies such as Meta, Google, and Intel playing significant roles in their development. However, the security of local LLMs through the lens of hardware cache side-channels remains unexplored. In this paper, we unveil novel side-channel vulnerabilities in local LLM inference: token value and token position leakage, which can expose both the victim's input and output text, thereby compromising user privacy. Specifically, we found that adversaries can infer the token values from the cache access patterns of the token embedding operation, and deduce the token positions from the timing of autoregressive decoding phases. To demonstrate the potential of these leaks, we design a novel eavesdropping attack framework targeting both open-source and proprietary LLM inference systems. The attack framework does not directly interact with the victim's LLM and can be executed without privilege.   We evaluate the attack on a range of practical local LLM deployments (e.g., Llama, Falcon, and Gemma), and the results show that our attack achieves promising accuracy. The restored output and input text have an average edit distance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the reconstructed texts achieve average cosine similarity scores of 98.7% (input) and 98.0% (output).

**Link**: [arxiv](http://arxiv.org/abs/2505.06738v1),  [pdf](http://arxiv.org/pdf/2505.06738v1)

**Tags**: cs.CR K.6.5 



### CaMDN: Enhancing Cache Efficiency for Multi-tenant DNNs on Integrated   NPUs
**Authors**: Tianhao Cai, Liang Wang, Limin Xiao, Meng Han, Zeyu Wang, Lin Sun, Xiaojian Liao

**Updated**: 2025-05-10T12:16:50Z

**Summary**: With the rapid development of DNN applications, multi-tenant execution, where multiple DNNs are co-located on a single SoC, is becoming a prevailing trend. Although many methods are proposed in prior works to improve multi-tenant performance, the impact of shared cache is not well studied. This paper proposes CaMDN, an architecture-scheduling co-design to enhance cache efficiency for multi-tenant DNNs on integrated NPUs. Specifically, a lightweight architecture is proposed to support model-exclusive, NPU-controlled regions inside shared cache to eliminate unexpected cache contention. Moreover, a cache scheduling method is proposed to improve shared cache utilization. In particular, it includes a cache-aware mapping method for adaptability to the varying available cache capacity and a dynamic allocation algorithm to adjust the usage among co-located DNNs at runtime. Compared to prior works, CaMDN reduces the memory access by 33.4% on average and achieves a model speedup of up to 2.56$\times$ (1.88$\times$ on average).

**Link**: [arxiv](http://arxiv.org/abs/2505.06625v1),  [pdf](http://arxiv.org/pdf/2505.06625v1)

**Tags**: cs.AR cs.AI 



### TierBase: A Workload-Driven Cost-Optimized Key-Value Store
**Authors**: Zhitao Shen, Shiyu Yang, Weibo Chen, Kunming Wang, Yue Li, Jiabao Jin, Wei Jia, Junwei Chen, Yuan Su, Xiaoxia Duan, Wei Chen, Lei Wang, Jie Song, Ruoyi Ruan, Xuemin Lin

**Updated**: 2025-05-10T07:57:02Z

**Summary**: In the current era of data-intensive applications, the demand for high-performance, cost-effective storage solutions is paramount. This paper introduces a Space-Performance Cost Model for key-value store, designed to guide cost-effective storage configuration decisions. The model quantifies the trade-offs between performance and storage costs, providing a framework for optimizing resource allocation in large-scale data serving environments. Guided by this cost model, we present TierBase, a distributed key-value store developed by Ant Group that optimizes total cost by strategically synchronizing data between cache and storage tiers, maximizing resource utilization and effectively handling skewed workloads. To enhance cost-efficiency, TierBase incorporates several optimization techniques, including pre-trained data compression, elastic threading mechanisms, and the utilization of persistent memory. We detail TierBase's architecture, key components, and the implementation of cost optimization strategies. Extensive evaluations using both synthetic benchmarks and real-world workloads demonstrate TierBase's superior cost-effectiveness compared to existing solutions. Furthermore, case studies from Ant Group's production environments showcase TierBase's ability to achieve up to 62% cost reduction in primary scenarios, highlighting its practical impact in large-scale online data serving.

**Link**: [arxiv](http://arxiv.org/abs/2505.06556v1),  [pdf](http://arxiv.org/pdf/2505.06556v1)

**Tags**: cs.DB cs.DC 



### An extension of C++ with memory-centric specifications for HPC to reduce   memory footprints and streamline MPI development
**Authors**: Pawel K. Radtke, Cristian G. Barrera-Hinojosa, Mladen Ivkovic, Tobias Weinzierl

**Updated**: 2025-05-09T07:26:29Z

**Summary**: The C++ programming language and its cousins lean towards a memory-inefficient storage of structs: The compiler inserts helper bits such that individual instance variables fit to byte or cache boundaries, while it is not able to exploit knowledge about the range of integers, enums or bitsets. Furthermore, the language provides neither support for data exchange via MPI nor for arbitrary floating-point precisions. We propose C++ attributes through which developers can guide the compiler what memory arrangements would be beneficial: Can multiple booleans or integers with limited range be squeezed into one bit field, do floating point numbers hold fewer significant bits than in the IEEE standard, or does the code benefit from a MPI datatype for subsets of attributes? The extension offers the opportunity to fall back to normal alignment via plain C++ assignments, no dependencies upon external libraries are introduced, and the resulting code remains standard C++ subject to some weakened guarantees on addresses and pointer arithmetics. Our work implements the language annotations within LLVM and demonstrates their potential impact, both upon the runtime and the memory footprint, through smoothed particle hydrodynamics (SPH) benchmarks. They uncover the potential gains in terms of performance and development productivity.

**Link**: [arxiv](http://arxiv.org/abs/2406.06095v3),  [pdf](http://arxiv.org/pdf/2406.06095v3)

**Tags**: cs.MS 



### Accelerating Diffusion Transformer via Increment-Calibrated Caching with   Channel-Aware Singular Value Decomposition
**Authors**: Zhiyuan Chen, Keyi Li, Yifan Jia, Le Ye, Yufei Ma

**Updated**: 2025-05-09T06:56:17Z

**Summary**: Diffusion transformer (DiT) models have achieved remarkable success in image generation, thanks for their exceptional generative capabilities and scalability. Nonetheless, the iterative nature of diffusion models (DMs) results in high computation complexity, posing challenges for deployment. Although existing cache-based acceleration methods try to utilize the inherent temporal similarity to skip redundant computations of DiT, the lack of correction may induce potential quality degradation. In this paper, we propose increment-calibrated caching, a training-free method for DiT acceleration, where the calibration parameters are generated from the pre-trained model itself with low-rank approximation. To deal with the possible correction failure arising from outlier activations, we introduce channel-aware Singular Value Decomposition (SVD), which further strengthens the calibration effect. Experimental results show that our method always achieve better performance than existing naive caching methods with a similar computation resource budget. When compared with 35-step DDIM, our method eliminates more than 45% computation and improves IS by 12 at the cost of less than 0.06 FID increase. Code is available at https://github.com/ccccczzy/icc.

**Link**: [arxiv](http://arxiv.org/abs/2505.05829v1),  [pdf](http://arxiv.org/pdf/2505.05829v1)

**Tags**: cs.CV cs.LG eess.IV 



### Sparse Attention Remapping with Clustering for Efficient LLM Decoding on   PIM
**Authors**: Zehao Fan, Garrett Gagnon, Zhenyu Liu, Liu Liu

**Updated**: 2025-05-09T04:17:05Z

**Summary**: Transformer-based models are the foundation of modern machine learning, but their execution, particularly during autoregressive decoding in large language models (LLMs), places significant pressure on memory systems due to frequent memory accesses and growing key-value (KV) caches. This creates a bottleneck in memory bandwidth, especially as context lengths increase. Processing-in-memory (PIM) architectures are a promising solution, offering high internal bandwidth and compute parallelism near memory. However, current PIM designs are primarily optimized for dense attention and struggle with the dynamic, irregular access patterns introduced by modern KV cache sparsity techniques. Consequently, they suffer from workload imbalance, reducing throughput and resource utilization. In this work, we propose STARC, a novel sparsity-optimized data mapping scheme tailored specifically for efficient LLM decoding on PIM architectures. STARC clusters KV pairs by semantic similarity and maps them to contiguous memory regions aligned with PIM bank structures. During decoding, queries retrieve relevant tokens at cluster granularity by matching against precomputed centroids, enabling selective attention and parallel processing without frequent reclustering or data movement overhead. Experiments on the HBM-PIM system show that, compared to common token-wise sparsity methods, STARC reduces attention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a KV cache budget of 1024, it achieves up to 54%--74% latency reduction and 45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC maintains model accuracy comparable to state-of-the-art sparse attention methods, demonstrating its effectiveness in enabling efficient and hardware-friendly long-context LLM inference on PIM architectures.

**Link**: [arxiv](http://arxiv.org/abs/2505.05772v1),  [pdf](http://arxiv.org/pdf/2505.05772v1)

**Tags**: cs.CL cs.LG 



### Medha: Efficiently Serving Multi-Million Context Length LLM Inference   Requests Without Approximations
**Authors**: Amey Agrawal, Haoran Qiu, Junda Chen, Íñigo Goiri, Chaojie Zhang, Rayyan Shahid, Ramachandran Ramjee, Alexey Tumanov, Esha Choukse

**Updated**: 2025-05-09T00:31:24Z

**Summary**: As large language models (LLMs) handle increasingly longer contexts, serving long inference requests of millions of tokens presents unique challenges. We show that existing work for long context inference is largely based on techniques from long context training, and does not handle the high variability in input lengths during inference. This leads to inefficient resource utilization, server fragmentation, and head-of-line (HOL) blocking.   We present Medha, an end-to-end system for efficient long-context LLM inference that addresses these challenges through fine-grained time sharing. Medha introduces three key innovations: (1) the mechanism of adaptive prefill chunking to help mitigate HOL blocking with preemption; (2) two new parallelism strategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token by pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower time-peroutput-token by distributing decoding across servers; and (3) a novel input-length aware least remaining slack scheduling to meet Service Level Objectives (SLOs).   Medha enables exact inference scaling beyond 10 million tokens, maintaining high throughput and low latency across mixed-length workloads. Compared to state-of-the-art systems, Medha reduces server fragmentation, cuts median latency by up to 30x, and improves throughput by over 5x, delivering production-scale long-context inference without compromising performance on shorter requests.

**Link**: [arxiv](http://arxiv.org/abs/2409.17264v3),  [pdf](http://arxiv.org/pdf/2409.17264v3)

**Tags**: cs.LG cs.DC 



### High Altitude Platform-Based Caching and Multicasting for Rural   Connectivity
**Authors**: Yongqiang Zhang, Mustafa A. Kishk, Mohamed-Slim Alouini

**Updated**: 2025-05-08T13:56:20Z

**Summary**: Providing efficient and reliable content delivery in rural areas remains a significant challenge due to the lack of communication infrastructure. To bridge the digital divide, this paper investigates the potential of leveraging multiple high-altitude platforms (HAPs) for energy-efficient content delivery in wide rural regions. Each caching-enabled HAP is equipped with both Free-Space Optical (FSO) transceivers for backhaul links and Radio Frequency (RF) antenna arrays for access links. To further enhance network efficiency, we consider a network coding-based multicasting scheme, where different types of content are treated as distinct multicast sessions. With the objective of minimizing long-term power cost, we propose a hierarchical framework that integrates deep reinforcement learn-ing (DRL) and convex optimization to jointly optimize dynamic caching strategies and resource allocation across the network. Simulation results demonstrate that our approach significantly reduces power cost compared to several baseline approaches, providing a practical solution for improving rural connectivity.

**Link**: [arxiv](http://arxiv.org/abs/2505.05251v1),  [pdf](http://arxiv.org/pdf/2505.05251v1)

**Tags**: eess.SY cs.SY 49 H.4.0 



### CacheFL: Efficient Federated Cache Model Fine-Tuning for Vision-Language   Models
**Authors**: Mengjun Yi, Hanwen Zhang, Hui Dou, Jian Zhao, Furao Shen

**Updated**: 2025-05-08T11:07:35Z

**Summary**: Large pre-trained Vision-Language Models (VLMs), such as Contrastive Language-Image Pre-training (CLIP), have exhibited remarkable zero-shot performance across various image classification tasks. Fine-tuning these models on domain-specific datasets further enhances their effectiveness for downstream applications. However, fine-tuning in cloud environments raises significant concerns regarding data security and privacy. Federated Learning (FL) offers a decentralized solution by enabling model training across local clients without centralizing sensitive data, but the high communication and computation costs of transmitting full pre-trained models during training limit its scalability. Additionally, non-Independent and Identically Distributed (non-IID) data across local clients can negatively impact model convergence and performance. To address these challenges, we propose CacheFL, a novel federated learning method that replaces traditional full model fine-tuning with lightweight cache model fine-tuning. The cache model is initialized using a class-balanced dataset generated by a generative pre-trained model, effectively mitigating the impact of non-IID data. This cache model is then distributed to local clients for fine-tuning, and the updated parameters from each client are aggregated on the server and redistributed. With the updated cache model, the classification performance of CLIP is improved after just a few epochs. By limiting the training and communication to the cache model, CacheFL significantly reduces resource demands while ensuring data privacy and security. Extensive experiments conducted on ImageNet and 10 additional datasets demonstrate that CacheFL outperforms traditional approaches in terms of classification accuracy, resource efficiency, and privacy preservation.

**Link**: [arxiv](http://arxiv.org/abs/2505.05130v1),  [pdf](http://arxiv.org/pdf/2505.05130v1)

**Tags**: cs.DC 



### CVA6S+: A Superscalar RISC-V Core with High-Throughput Memory   Architecture
**Authors**: Riccardo Tedeschi, Gianmarco Ottavi, Côme Allart, Nils Wistoff, Zexin Fu, Filippo Grillotti, Fabio De Ambroggi, Elio Guidetti, Jean-Baptiste Rigaud, Olivier Potin, Jean Roch Coulon, César Fuguet, Luca Benini, Davide Rossi

**Updated**: 2025-05-08T09:05:51Z

**Summary**: Open-source RISC-V cores are increasingly adopted in high-end embedded domains such as automotive, where maximizing instructions per cycle (IPC) is becoming critical. Building on the industry-supported open-source CVA6 core and its superscalar variant, CVA6S, we introduce CVA6S+, an enhanced version incorporating improved branch prediction, register renaming and enhanced operand forwarding. These optimizations enable CVA6S+ to achieve a 43.5% performance improvement over the scalar configuration and 10.9% over CVA6S, with an area overhead of just 9.30% over the scalar core (CVA6). Furthermore, we integrate CVA6S+ with the OpenHW Core-V High-Performance L1 Dcache (HPDCache) and report a 74.1% bandwidth improvement over the legacy CVA6 cache subsystem.

**Link**: [arxiv](http://arxiv.org/abs/2505.03762v2),  [pdf](http://arxiv.org/pdf/2505.03762v2)

**Tags**: cs.AR 



### CacheSquash: Making caches speculation-aware
**Authors**: Hossam ElAtali, N. Asokan

**Updated**: 2025-05-08T07:55:38Z

**Summary**: Speculation is key to achieving high CPU performance, yet it enables risks like Spectre attacks which remain a significant challenge to mitigate without incurring substantial performance overheads. These attacks typically unfold in three stages: access, transmit, and receive. Typically, they exploit a cache timing side channel during the transmit and receive phases: speculatively accessing sensitive data (access), altering cache state (transmit), and then utilizing a cache timing attack (e.g., Flush+Reload) to extract the secret (receive). Our key observation is that Spectre attacks only require the transmit instruction to execute and dispatch a request to the cache hierarchy. It need not complete before a misprediction is detected (and mis-speculated instructions squashed) because responses from memory that arrive at the cache after squashing still alter cache state. We propose a novel mitigation, CacheSquash, that cancels mis-speculated memory accesses. Immediately upon squashing, a cancellation is sent to the cache hierarchy, propagating downstream and preventing any changes to caches that have not yet received a response. This minimizes cache state changes, thereby reducing the likelihood of Spectre attacks succeeding. We implement CacheSquash on gem5 and show that it thwarts practical Spectre attacks, with near-zero performance overheads.

**Link**: [arxiv](http://arxiv.org/abs/2406.12110v2),  [pdf](http://arxiv.org/pdf/2406.12110v2)

**Tags**: cs.CR cs.AR 



### A Survey on Inference Engines for Large Language Models: Perspectives on   Optimization and Efficiency
**Authors**: Sihyeong Park, Sungryeol Jeon, Chaelyn Lee, Seokhun Jeon, Byung-Soo Kim, Jemin Lee

**Updated**: 2025-05-08T07:08:40Z

**Summary**: Large language models (LLMs) are widely applied in chatbots, code generators, and search engines. Workloads such as chain-of-thought, complex reasoning, and agent services significantly increase the inference cost by invoking the model repeatedly. Optimization methods such as parallelism, compression, and caching have been adopted to reduce costs, but the diverse service requirements make it hard to select the right method. Recently, specialized LLM inference engines have emerged as a key component for integrating the optimization methods into service-oriented infrastructures. However, a systematic study on inference engines is still lacking. This paper provides a comprehensive evaluation of 25 open-source and commercial inference engines. We examine each inference engine in terms of ease-of-use, ease-of-deployment, general-purpose support, scalability, and suitability for throughput- and latency-aware computation. Furthermore, we explore the design goals of each inference engine by investigating the optimization techniques it supports. In addition, we assess the ecosystem maturity of open source inference engines and handle the performance and cost policy of commercial solutions. We outline future research directions that include support for complex LLM-based services, support of various hardware, and enhanced security, offering practical guidance to researchers and developers in selecting and designing optimized LLM inference engines. We also provide a public repository to continually track developments in this fast-evolving field: https://github.com/sihyeong/Awesome-LLM-Inference-Engine

**Link**: [arxiv](http://arxiv.org/abs/2505.01658v2),  [pdf](http://arxiv.org/pdf/2505.01658v2)

**Tags**: cs.CL 



### Memory Under Siege: A Comprehensive Survey of Side-Channel Attacks on   Memory
**Authors**: MD Mahady Hassan, Shanto Roy, Reza Rahaeimehr

**Updated**: 2025-05-08T02:16:08Z

**Summary**: Side-channel attacks on memory (SCAM) exploit unintended data leaks from memory subsystems to infer sensitive information, posing significant threats to system security. These attacks exploit vulnerabilities in memory access patterns, cache behaviors, and other microarchitectural features to bypass traditional security measures. The purpose of this research is to examine SCAM, classify various attack techniques, and evaluate existing defense mechanisms. It guides researchers and industry professionals in improving memory security and mitigating emerging threats. We begin by identifying the major vulnerabilities in the memory system that are frequently exploited in SCAM, such as cache timing, speculative execution, \textit{Rowhammer}, and other sophisticated approaches. Next, we outline a comprehensive taxonomy that systematically classifies these attacks based on their types, target systems, attack vectors, and adversarial capabilities required to execute them. In addition, we review the current landscape of mitigation strategies, emphasizing their strengths and limitations. This work aims to provide a comprehensive overview of memory-based side-channel attacks with the goal of providing significant insights for researchers and practitioners to better understand, detect, and mitigate SCAM risks.

**Link**: [arxiv](http://arxiv.org/abs/2505.04896v1),  [pdf](http://arxiv.org/pdf/2505.04896v1)

**Tags**: cs.CR 



### Comparing CPU and GPU compute of PERMANOVA on MI300A
**Authors**: Igor Sfiligoi

**Updated**: 2025-05-07T16:44:21Z

**Summary**: Comparing the tradeoffs of CPU and GPU compute for memory-heavy algorithms is often challenging, due to the drastically different memory subsystems on host CPUs and discrete GPUs. The AMD MI300A is an exception, since it sports both CPU and GPU cores in a single package, all backed by the same type of HBM memory. In this paper we analyze the performance of Permutational Multivariate Analysis of Variance (PERMANOVA), a non-parametric method that tests whether two or more groups of objects are significantly different based on a categorical factor. This method is memory-bound and has been recently optimized for CPU cache locality. Our tests show that GPU cores on the MI300A prefer the brute force approach instead, significantly outperforming the CPU-based implementation. The significant benefit of Simultaneous Multithreading (SMT) was also a pleasant surprise.

**Link**: [arxiv](http://arxiv.org/abs/2505.04556v1),  [pdf](http://arxiv.org/pdf/2505.04556v1)

**Tags**: cs.DC cs.PF q-bio.QM 



### Securing Immersive 360 Video Streams through Attribute-Based Selective   Encryption
**Authors**: Mohammad Waquas Usmani, Susmit Shannigrahi, Michael Zink

**Updated**: 2025-05-07T14:37:13Z

**Summary**: Delivering high-quality, secure 360{\deg} video content introduces unique challenges, primarily due to the high bitrates and interactive demands of immersive media. Traditional HTTPS-based methods, although widely used, face limitations in computational efficiency and scalability when securing these high-resolution streams. To address these issues, this paper proposes a novel framework integrating Attribute-Based Encryption (ABE) with selective encryption techniques tailored specifically for tiled 360{\deg} video streaming. Our approach employs selective encryption of frames at varying levels to reduce computational overhead while ensuring robust protection against unauthorized access.   Moreover, we explore viewport-adaptive encryption, dynamically encrypting more frames within tiles occupying larger portions of the viewer's field of view. This targeted method significantly enhances security in critical viewing areas without unnecessary overhead in peripheral regions. We deploy and evaluate our proposed approach using the CloudLab testbed, comparing its performance against traditional HTTPS streaming. Experimental results demonstrate that our ABE-based model achieves reduced computational load on intermediate caches, improves cache hit rates, and maintains comparable visual quality to HTTPS, as assessed by Video Multimethod Assessment Fusion (VMAF).

**Link**: [arxiv](http://arxiv.org/abs/2505.04466v1),  [pdf](http://arxiv.org/pdf/2505.04466v1)

**Tags**: cs.MM cs.CR eess.IV 



### LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders
**Authors**: Zheng Chai, Qin Ren, Xijun Xiao, Huizhi Yang, Bo Han, Sijun Zhang, Di Chen, Hui Lu, Wenlin Zhao, Lele Yu, Xionghang Xie, Shiru Ren, Xiang Sun, Yaocheng Tan, Peng Xu, Yuchao Zheng, Di Wu

**Updated**: 2025-05-07T13:54:26Z

**Summary**: Modeling ultra-long user behavior sequences is critical for capturing both long- and short-term preferences in industrial recommender systems. Existing solutions typically rely on two-stage retrieval or indirect modeling paradigms, incuring upstream-downstream inconsistency and computational inefficiency. In this paper, we present LONGER, a Long-sequence Optimized traNsformer for GPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism for stabilizing attention over long contexts, (ii) a token merge module with lightweight InnerTransformers and hybrid attention strategy to reduce quadratic complexity, and (iii) a series of engineering optimizations, including training with mixed-precision and activation recomputation, KV cache serving, and the fully synchronous model training and serving framework for unified GPU-based dense and sparse parameter updates. LONGER consistently outperforms strong baselines in both offline metrics and online A/B testing in both advertising and e-commerce services at ByteDance, validating its consistent effectiveness and industrial-level scaling laws. Currently, LONGER has been fully deployed at more than 10 influential scenarios at ByteDance, serving billion users.

**Link**: [arxiv](http://arxiv.org/abs/2505.04421v1),  [pdf](http://arxiv.org/pdf/2505.04421v1)

**Tags**: cs.IR 



### Rehearsal-Free Continual Federated Learning with Synergistic Synaptic   Intelligence
**Authors**: Yichen Li, Yuying Wang, Haozhao Wang, Yining Qi, Tianzhe Xiao, Ruixuan Li

**Updated**: 2025-05-07T13:07:25Z

**Summary**: Continual Federated Learning (CFL) allows distributed devices to collaboratively learn novel concepts from continuously shifting training data while avoiding knowledge forgetting of previously seen tasks. To tackle this challenge, most current CFL approaches rely on extensive rehearsal of previous data. Despite effectiveness, rehearsal comes at a cost to memory, and it may also violate data privacy. Considering these, we seek to apply regularization techniques to CFL by considering their cost-efficient properties that do not require sample caching or rehearsal. Specifically, we first apply traditional regularization techniques to CFL and observe that existing regularization techniques, especially synaptic intelligence, can achieve promising results under homogeneous data distribution but fail when the data is heterogeneous. Based on this observation, we propose a simple yet effective regularization algorithm for CFL named FedSSI, which tailors the synaptic intelligence for the CFL with heterogeneous data settings. FedSSI can not only reduce computational overhead without rehearsal but also address the data heterogeneity issue. Extensive experiments show that FedSSI achieves superior performance compared to state-of-the-art methods.

**Link**: [arxiv](http://arxiv.org/abs/2412.13779v2),  [pdf](http://arxiv.org/pdf/2412.13779v2)

**Tags**: cs.LG cs.DC 



### Design and Evaluation of an NDN-Based Network for Distributed Digital   Twins
**Authors**: Chen Chen, Zihan Jia, Ze Wang, Lin Cui, Fung Po Tso

**Updated**: 2025-05-07T11:21:12Z

**Summary**: Digital twins (DT) have received significant attention due to their numerous benefits, such as real-time data analytics and cost reduction in production. DT serves as a fundamental component of many applications, encompassing smart manufacturing, intelligent vehicles, and smart cities. By using Machine Learning (ML) and Artificial Intelligence (AI) techniques, DTs can efficiently facilitate decision-making and productivity by simulating the status and changes of a physical entity. To handle the massive amount of data brought by DTs, it is challenging to achieve low response latency for data fetching over existing IP-based networks. IP-based networks use host addresses for end-to-end communication, making data distribution between DTs inefficient. Thus, we propose to use DTs in a distributed manner over Named Data Networking (NDN) networks. NDN is data-centric where data is routed based on content names, dynamically adjusting paths to optimize latency. Popular data is cached in network nodes, reducing data transmission and network congestion. Since data is fetched by content names, users and mobile devices can move freely without IP address reassignment. By using in-network caching and adaptive routing, we reckon NDN is an ideal fit for Future G Networks in the context of Digital Twins. We compared DTs in edge scenarios with cloud scenarios over NDN and IP-based networks to validate our insights. Extensive simulation results show that using DT in the edge reduces response latency by 10.2x. This position paper represents an initial investigation into the gap in distributed DTs over NDN, serving as an early-stage study.

**Link**: [arxiv](http://arxiv.org/abs/2505.04326v1),  [pdf](http://arxiv.org/pdf/2505.04326v1)

**Tags**: cs.NI 



### Computational Model for Photoionization in Pure SF6 Streamer
**Authors**: Zihao Feng

**Updated**: 2025-05-07T08:10:39Z

**Summary**: Photoionization plays a crucial role in achieving spatial numerical convergence and accurate quantitative predictions in SF6 streamer simulations, but accurate models for SF6 photoionization remains limited, motivating this paper. First, we develop a computational model for SF6 photoionization and provide the detailed modeling process. Then, we perform comparative studies against simplified approaches. The results demonstrate that the proposed model effectively captures the non-local effects of SF6 photoionization, enhancing both the spatial numerical convergence and the accuracy of the streamer structure. Finally, we perform comparative studies by artificially increasing the photoionization intensity through multiplying the photoionization source term Sph by a factor of 10 (10*Sph) relative to the baseline intensity. Regarding breakdown voltage prediction, 10*Sph leads to a significant underestimation of the breakdown voltage for positive streamers, introducing errors greater than 0.5 kV, while exerting a relatively small impact on negative streamers. Regarding streamer propagation dynamics, 10*Sph reduces the contraction at the positive streamer head and significantly lowers the local field by more than 700 Td, thereby slowing down its speed. In contrast, 10*Sph has little impact on the morphology of the negative streamers and slightly enhances the local field by less than 200 Td, thereby consistently accelerating its propagation.

**Link**: [arxiv](http://arxiv.org/abs/2505.04216v1),  [pdf](http://arxiv.org/pdf/2505.04216v1)

**Tags**: physics.plasm-ph 



### Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer   Gate
**Authors**: Zhiyuan Fang, Zicong Hong, Yuegui Huang, Yufeng Lyu, Wuhui Chen, Yue Yu, Fan Yu, Zibin Zheng

**Updated**: 2025-05-07T07:57:21Z

**Summary**: Large Language Models (LLMs) have demonstrated impressive performance across various tasks, and their application in edge scenarios has attracted significant attention. However, sparse-activated Mixture-of-Experts (MoE) models, which are well suited for edge scenarios, have received relatively little attention due to their high memory demands. Offload-based methods have been proposed to address this challenge, but they face difficulties with expert prediction. Inaccurate expert predictions can result in prolonged inference delays. To promote the application of MoE models in edge scenarios, we propose Fate, an offloading system designed for MoE models to enable efficient inference in resource-constrained environments. The key insight behind Fate is that gate inputs from adjacent layers can be effectively used for expert prefetching, achieving high prediction accuracy without additional GPU overhead. Furthermore, Fate employs a shallow-favoring expert caching strategy that increases the expert hit rate to 99\%. Additionally, Fate integrates tailored quantization strategies for cache optimization and IO efficiency. Experimental results show that, compared to Load on Demand and Expert Activation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in prefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively, while maintaining inference quality. Moreover, Fate's performance improvements are scalable across different memory budgets.

**Link**: [arxiv](http://arxiv.org/abs/2502.12224v2),  [pdf](http://arxiv.org/pdf/2502.12224v2)

**Tags**: cs.AI cs.LG 



### Maxing Out the SVM: Performance Impact of Memory and Program Cache Sizes   in the Agave Validator
**Authors**: Turan Vural, Yuki Yuminaga, Alex Petrosyan, Ben Livshits

**Updated**: 2025-05-07T05:00:10Z

**Summary**: In this paper we analyze some of the bottlenecks in the execution pipeline of Solana's Agave validator client, focusing on RAM and program cache usage under mainnet conditions. Through a series of controlled experiments, we measure the validator's throughput and resource efficiency as RAM availability ranges between 128 GB to 1,536 GB (1.5 TB). We discover that the validator performance degrades significantly below 256 GB, with transaction processing falling behind real-time block production. Additionally, we study the program cache behavior, identifying inefficiencies in program eviction and load latency. Our results provide practical guidance for hardware provisioning and suggest improvements to the Solana execution and caching strategy, reducing latency due to the program cache by 90%.

**Link**: [arxiv](http://arxiv.org/abs/2505.04129v1),  [pdf](http://arxiv.org/pdf/2505.04129v1)

**Tags**: cs.DC 



### Spineless Traversal for Layout Invalidation
**Authors**: Marisa Kirisame, Tiezhi Wang, Pavel Panchekha

**Updated**: 2025-05-07T01:29:10Z

**Summary**: Latency is a major concern for web rendering engines like those in Chrome, Safari, and Firefox. These engines reduce latency by using an incremental layout algorithm to redraw the page when the user interacts with it. In such an algorithm, elements that change frame-to-frame are marked dirty, and only those elements are processed to draw the next frame, dramatically reducing latency. However, the standard incremental layout algorithm must search the page for dirty elements, accessing auxiliary elements in the process. These auxiliary elements add cache misses and stalled cycles, and are responsible for a sizable fraction of all layout latency. We introduce a new, faster incremental layout algorithm called Spineless Traversal. Spineless Traversal uses a cache-friendlier priority queue algorithm that avoids accessing auxiliary nodes and thus reduces cache traffic and stalls. This leads to dramatic speedups on the most latency-critical interactions such as hovering, typing, and animation. Moreover, thanks to numerous low-level optimizations, Spineless Traversal is competitive across the whole spectrum of incremental layout workloads. Spineless Traversal is faster than the standard approach on 83.0% of 2216 benchmarks, with a mean speedup of 1.80x concentrated in the most latency-critical interactions.

**Link**: [arxiv](http://arxiv.org/abs/2411.10659v6),  [pdf](http://arxiv.org/pdf/2411.10659v6)

**Tags**: cs.PL 



### Reciprocating Locks
**Authors**: Dave Dice, Alex Kogan

**Updated**: 2025-05-06T19:28:56Z

**Summary**: We present "Reciprocating Locks", a novel mutual exclusion locking algorithm, targeting cache-coherent shared memory (CC), that enjoys a number of desirable properties. The doorway arrival phase and the release operation both run in constant-time. Waiting threads use local spinning and only a single waiting element is required per thread, regardless of the number of locks a thread might hold at a given time. While our lock does not provide strict FIFO admission, it bounds bypass and has strong anti-starvation properties. The lock is compact, space efficient, and has been intentionally designed to be readily usable in real-world general purpose computing environments such as the linux kernel, pthreads, or C++. We show the lock exhibits high throughput under contention and low latency in the uncontended case. The performance of Reciprocating Locks is competitive with and often better than the best state-of-the-art scalable spin locks.

**Link**: [arxiv](http://arxiv.org/abs/2501.02380v6),  [pdf](http://arxiv.org/pdf/2501.02380v6)

**Tags**: cs.DC D.4.1 



### Cobra: Efficient Line Art COlorization with BRoAder References
**Authors**: Junhao Zhuang, Lingen Li, Xuan Ju, Zhaoyang Zhang, Chun Yuan, Ying Shan

**Updated**: 2025-05-06T15:23:12Z

**Summary**: The comic production industry requires reference-based line art colorization with high accuracy, efficiency, contextual consistency, and flexible control. A comic page often involves diverse characters, objects, and backgrounds, which complicates the coloring process. Despite advancements in diffusion models for image generation, their application in line art colorization remains limited, facing challenges related to handling extensive reference images, time-consuming inference, and flexible control. We investigate the necessity of extensive contextual image guidance on the quality of line art colorization. To address these challenges, we introduce Cobra, an efficient and versatile method that supports color hints and utilizes over 200 reference images while maintaining low latency. Central to Cobra is a Causal Sparse DiT architecture, which leverages specially designed positional encodings, causal sparse attention, and Key-Value Cache to effectively manage long-context references and ensure color identity consistency. Results demonstrate that Cobra achieves accurate line art colorization through extensive contextual reference, significantly enhancing inference speed and interactivity, thereby meeting critical industrial demands. We release our codes and models on our project page: https://zhuang2002.github.io/Cobra/.

**Link**: [arxiv](http://arxiv.org/abs/2504.12240v3),  [pdf](http://arxiv.org/pdf/2504.12240v3)

**Tags**: cs.CV 



### RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM   Inference
**Authors**: Yaoqi Chen, Jinkai Zhang, Baotong Lu, Qianxi Zhang, Chengruidong Zhang, Jingjia Luo, Di Liu, Huiqiang Jiang, Qi Chen, Jing Liu, Bailu Ding, Xiao Yan, Jiawei Jiang, Chen Chen, Mingxing Zhang, Yuqing Yang, Fan Yang, Mao Yang

**Updated**: 2025-05-05T18:01:17Z

**Summary**: The growing context lengths of large language models (LLMs) pose significant challenges for efficient inference, primarily due to GPU memory and bandwidth constraints. We present RetroInfer, a novel system that reconceptualizes the key-value (KV) cache as a vector storage system which exploits the inherent attention sparsity to accelerate long-context LLM inference. At its core is the wave index, an Attention-aWare VEctor index that enables efficient and accurate retrieval of critical tokens through techniques such as tripartite attention approximation, accuracy-bounded attention estimation, and segmented clustering. Complementing this is the wave buffer, which coordinates KV cache placement and overlaps computation and data transfer across GPU and CPU to sustain high throughput. Unlike prior sparsity-based methods that struggle with token selection and hardware coordination, RetroInfer delivers robust performance without compromising model accuracy. Experiments on long-context benchmarks show up to 4.5X speedup over full attention within GPU memory limits and up to 10.5X over sparse attention baselines when KV cache is extended to CPU memory, all while preserving full-attention-level accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2505.02922v1),  [pdf](http://arxiv.org/pdf/2505.02922v1)

**Tags**: cs.LG 



### Large Language Model Partitioning for Low-Latency Inference at the Edge
**Authors**: Dimitrios Kafetzis, Ramin Khalili, Iordanis Koutsopoulos

**Updated**: 2025-05-05T10:16:16Z

**Summary**: Large Language Models (LLMs) based on autoregressive, decoder-only Transformers generate text one token at a time, where a token represents a discrete unit of text. As each newly produced token is appended to the partial output sequence, the length grows and so does the memory and compute load, due to the expanding key-value caches, which store intermediate representations of all previously generated tokens in the multi-head attention (MHA) layer. As this iterative process steadily increases memory and compute demands, layer-based partitioning in resource-constrained edge environments often results in memory overload or high inference latency. To address this and reduce inference latency, we propose a resource-aware Transformer architecture partitioning algorithm, where the partitioning decision is updated at regular intervals during token generation. The approach is myopic in that it is based on instantaneous information about device resource availability and network link bandwidths. When first executed, the algorithm places blocks on devices, and in later executions, it migrates these blocks among devices so that the sum of migration delay and inference delay remains low. Our approach partitions the decoder at the attention head level, co-locating each attention head with its key-value cache and allowing dynamic migrations whenever resources become tight. By allocating different attention heads to different devices, we exploit parallel execution of attention heads and thus achieve substantial reductions in inference delays. Our experiments show that in small-scale settings (3-5 devices), the proposed method achieves within 15 to 20 percent of an exact optimal solver's latency, while in larger-scale tests it achieves notable improvements in inference speed and memory usage compared to state-of-the-art layer-based partitioning approaches.

**Link**: [arxiv](http://arxiv.org/abs/2505.02533v1),  [pdf](http://arxiv.org/pdf/2505.02533v1)

**Tags**: cs.DC cs.AI 



### An Empirical Study on the Performance and Energy Usage of Compiled   Python Code
**Authors**: Vincenzo Stoico, Andrei Calin Dragomir, Patricia Lago

**Updated**: 2025-05-05T04:01:56Z

**Summary**: Python is a popular programming language known for its ease of learning and extensive libraries. However, concerns about performance and energy consumption have led to the development of compilers to enhance Python code efficiency. Despite the proven benefits of existing compilers on the efficiency of Python code, there is limited analysis comparing their performance and energy efficiency, particularly considering code characteristics and factors like CPU frequency and core count. Our study investigates how compilation impacts the performance and energy consumption of Python code, using seven benchmarks compiled with eight different tools: PyPy, Numba, Nuitka, Mypyc, Codon, Cython, Pyston-lite, and the experimental Python 3.13 version, compared to CPython. The benchmarks are single-threaded and executed on an NUC and a server, measuring energy usage, execution time, memory usage, and Last-Level Cache (LLC) miss rates at a fixed frequency and on a single core. The results show that compilation can significantly enhance execution time, energy and memory usage, with Codon, PyPy, and Numba achieving over 90\% speed and energy improvements. Nuitka optimizes memory usage consistently on both testbeds. The impact of compilation on LLC miss rate is not clear since it varies considerably across benchmarks for each compiler. Our study is important for researchers and practitioners focused on improving Python code performance and energy efficiency. We outline future research directions, such as exploring caching effects on energy usage. Our findings help practitioners choose the best compiler based on their efficiency benefits and accessibility.

**Link**: [arxiv](http://arxiv.org/abs/2505.02346v1),  [pdf](http://arxiv.org/pdf/2505.02346v1)

**Tags**: cs.PL cs.PF cs.SE 



### DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient   MoE Inference
**Authors**: Yujie Zhang, Shivam Aggarwal, Tulika Mitra

**Updated**: 2025-05-04T09:49:42Z

**Summary**: Mixture-of-Experts (MoE) models, though highly effective for various machine learning tasks, face significant deployment challenges on memory-constrained devices. While GPUs offer fast inference, their limited memory compared to CPUs means not all experts can be stored on the GPU simultaneously, necessitating frequent, costly data transfers from CPU memory, often negating GPU speed advantages. To address this, we present DAOP, an on-device MoE inference engine to optimize parallel GPU-CPU execution. DAOP dynamically allocates experts between CPU and GPU based on per-sequence activation patterns, and selectively pre-calculates predicted experts on CPUs to minimize transfer latency. This approach enables efficient resource utilization across various expert cache ratios while maintaining model accuracy through a novel graceful degradation mechanism. Comprehensive evaluations across various datasets show that DAOP outperforms traditional expert caching and prefetching methods by up to 8.20x and offloading techniques by 1.35x while maintaining accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2501.10375v2),  [pdf](http://arxiv.org/pdf/2501.10375v2)

**Tags**: cs.DC cs.LG 



### GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph   In-Context Learning
**Authors**: Rui Lv, Zaixi Zhang, Kai Zhang, Qi Liu, Weibo Gao, Jiawei Liu, Jiaxia Yan, Linan Yue, Fangzhou Yao

**Updated**: 2025-05-04T08:30:00Z

**Summary**: Graph In-Context Learning, with the ability to adapt pre-trained graph models to novel and diverse downstream graphs without updating any parameters, has gained much attention in the community. The key to graph in-context learning is to perform downstream graphs conditioned on chosen prompt examples. Existing methods randomly select subgraphs or edges as prompts, leading to noisy graph prompts and inferior model performance. Additionally, due to the gap between pre-training and testing graphs, when the number of classes in the testing graphs is much greater than that in the training, the in-context learning ability will also significantly deteriorate. To tackle the aforementioned challenges, we develop a multi-stage adaptive prompt optimization method GraphPrompter, which optimizes the entire process of generating, selecting, and using graph prompts for better in-context learning capabilities. Firstly, Prompt Generator introduces a reconstruction layer to highlight the most informative edges and reduce irrelevant noise for graph prompt construction. Furthermore, in the selection stage, Prompt Selector employs the $k$-nearest neighbors algorithm and pre-trained selection layers to dynamically choose appropriate samples and minimize the influence of irrelevant prompts. Finally, we leverage a Prompt Augmenter with a cache replacement strategy to enhance the generalization capability of the pre-trained model on new datasets. Extensive experiments show that GraphPrompter effectively enhances the in-context learning ability of graph models. On average across all the settings, our approach surpasses the state-of-the-art baselines by over 8%. Our code is released at https://github.com/karin0018/GraphPrompter.

**Link**: [arxiv](http://arxiv.org/abs/2505.02027v1),  [pdf](http://arxiv.org/pdf/2505.02027v1)

**Tags**: cs.LG cs.AI cs.SI 



### PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language   Model Inference
**Authors**: Yufeng Gu, Alireza Khadem, Sumanth Umesh, Ning Liang, Xavier Servot, Onur Mutlu, Ravi Iyer, Reetuparna Das

**Updated**: 2025-05-03T04:07:07Z

**Summary**: Large Language Model (LLM) inference uses an autoregressive manner to generate one token at a time, which exhibits notably lower operational intensity compared to earlier Machine Learning (ML) models such as encoder-only transformers and Convolutional Neural Networks. At the same time, LLMs possess large parameter sizes and use key-value caches to store context information. Modern LLMs support context windows with up to 1 million tokens to generate versatile text, audio, and video content. A large key-value cache unique to each prompt requires a large memory capacity, limiting the inference batch size. Both low operational intensity and limited batch size necessitate a high memory bandwidth. However, contemporary hardware systems for ML model deployment, such as GPUs and TPUs, are primarily optimized for compute throughput. This mismatch challenges the efficient deployment of advanced LLMs and makes users pay for expensive compute resources that are poorly utilized for the memory-bound LLM inference tasks.   We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which harnesses CXL memory expansion capabilities to accommodate substantial LLM sizes, and utilizes near-bank processing units to deliver high memory bandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable CXL network to support peer-to-peer and collective communication primitives across CXL devices. We implement various parallelism strategies to distribute LLMs across these devices. Compared to GPU baselines with maximum supported batch sizes and similar average power, CENT achieves 2.3$\times$ higher throughput and consumes 2.9$\times$ less energy. CENT enhances the Total Cost of Ownership (TCO), generating 5.2$\times$ more tokens per dollar than GPUs.

**Link**: [arxiv](http://arxiv.org/abs/2502.07578v3),  [pdf](http://arxiv.org/pdf/2502.07578v3)

**Tags**: cs.AR 



### VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with   Delayed Hits
**Authors**: Bowen Jiang, Chaofan Ma, Duo Wang

**Updated**: 2025-05-03T01:10:30Z

**Summary**: Caches are fundamental to latency-sensitive systems like Content Delivery Networks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit phenomenon where multiple requests for an object occur during its fetch from the remote server after a miss significantly inflates user-perceived latency. While recent algorithms acknowledge delayed hits by estimating the resulting aggregate delay, they predominantly focus on its mean value. We identify and demonstrate that such approaches are insufficient, as the real aggregate delay frequently exhibits substantial variance in the true production system, leading to suboptimal latency performance when ignored. Thus, we propose VA-CDH, a variance-aware method to optimize latency for caching with delayed hits. It employs a novel ranking function that explicitly incorporates both the empirically estimated mean and standard deviation of aggregate delay, allowing caching decisions to account for its variation. We derive the analytical distribution of aggregate delay under Poisson arrivals as a theoretical contribution, offering more statistical insight beyond the mean value. Through the simulations conducted on synthetic and real-world datasets, we show that VA-CDH reduces the total latency by 1%-6% approximately compared to state-of-the-art algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2504.20335v2),  [pdf](http://arxiv.org/pdf/2504.20335v2)

**Tags**: cs.NI 



### Collaborative Coded Caching for Partially Connected Networks
**Authors**: Kagan Akcay, Eleftherios Lampiris, MohammadJavad Salehi, Giuseppe Caire

**Updated**: 2025-05-02T13:55:21Z

**Summary**: Coded caching leverages the differences in user cache memories to achieve gains that scale with the total cache size, alleviating network congestion due to high-quality content requests. Additionally, distributing transmitters over a wide area can mitigate the adverse effects of path loss. In this work, we consider a partially connected network where the channel between distributed transmitters (helpers) and users is modeled as a distributed multiple-input-multiple-output (MIMO) Gaussian broadcast channel. We propose a novel delivery scheme consisting of two phases: partitioning and transmission. In the partitioning phase, users with identical cache profiles are partitioned into the minimum number of sets, such that users within each set can successfully decode their desired message from a joint transmission enabled by MIMO precoding. To optimally partition the users, we employ the branch and bound method. In the transmission phase, each partition is treated as a single entity, and codewords are multicast to partitions with distinct cache profiles. The proposed delivery scheme is applicable to any partially connected network, and while the partitioning is optimal, the overall delivery scheme, including transmission, is heuristic. Interestingly, simulation results show that its performance closely approximates that of the fully connected optimal solution.

**Link**: [arxiv](http://arxiv.org/abs/2501.13298v3),  [pdf](http://arxiv.org/pdf/2501.13298v3)

**Tags**: cs.IT math.IT 



### HarmoniCa: Harmonizing Training and Inference for Better Feature Caching   in Diffusion Transformer Acceleration
**Authors**: Yushi Huang, Zining Wang, Ruihao Gong, Jing Liu, Xinjie Zhang, Jinyang Guo, Xianglong Liu, Jun Zhang

**Updated**: 2025-05-02T11:29:31Z

**Summary**: Diffusion Transformers (DiTs) excel in generative tasks but face practical deployment challenges due to high inference costs. Feature caching, which stores and retrieves redundant computations, offers the potential for acceleration. Existing learning-based caching, though adaptive, overlooks the impact of the prior timestep. It also suffers from misaligned objectives--aligned predicted noise vs. high-quality images--between training and inference. These two discrepancies compromise both performance and efficiency. To this end, we harmonize training and inference with a novel learning-based caching framework dubbed HarmoniCa. It first incorporates Step-Wise Denoising Training (SDT) to ensure the continuity of the denoising process, where prior steps can be leveraged. In addition, an Image Error Proxy-Guided Objective (IEPO) is applied to balance image quality against cache utilization through an efficient proxy to approximate the image error. Extensive experiments across $8$ models, $4$ samplers, and resolutions from $256\times256$ to $2K$ demonstrate superior performance and speedup of our framework. For instance, it achieves over $40\%$ latency reduction (i.e., $2.07\times$ theoretical speedup) and improved performance on PixArt-$\alpha$. Remarkably, our image-free approach reduces training time by $25\%$ compared with the previous method. Our code is available at https://github.com/ModelTC/HarmoniCa.

**Link**: [arxiv](http://arxiv.org/abs/2410.01723v4),  [pdf](http://arxiv.org/pdf/2410.01723v4)

**Tags**: cs.CV 



### CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in   RAG Systems
**Authors**: Yeonwoo Jeong, Kyuli Park, Hyunji Cho, Sungyong Park

**Updated**: 2025-05-02T10:13:12Z

**Summary**: Modern embedding models capture both semantic and syntactic structures of queries, often mapping different queries to similar regions in vector space. This results in non-uniform cluster access patterns in disk-based vector search systems, particularly in Retrieval Augmented Generation (RAG) framework. While existing approaches optimize individual queries, they overlook the impact of cluster access patterns, failing to account for the locality effects of queries that access similar clusters. This oversight reduces cache efficiency and increases search latency due to excessive disk I/O. To address this, we introduce CaGR-RAG, a context-aware query grouping mechanism that organizes queries based on shared cluster access patterns. Additionally, it incorporates opportunistic cluster prefetching to minimize cache misses during transitions between query groups, further optimizing retrieval performance. Experimental results show that CaGR-RAG reduces 99th percentile tail latency by up to 51.55% while consistently maintaining a higher cache hit ratio than the baseline.

**Link**: [arxiv](http://arxiv.org/abs/2505.01164v1),  [pdf](http://arxiv.org/pdf/2505.01164v1)

**Tags**: cs.DC 



### High Voltage Delivery and Distribution for the NEXT-100 Time Projection   Chamber
**Authors**: NEXT Collaboration, C. Adams, H. Almazán, V. Álvarez, K. Bailey, R. Guenette, B. J. P. Jones, S. Johnston, K. Mistry, F. Monrabal, D. R. Nygren, B. Palmeiro, L. Rogers, J. Waldschmidt, B. Aparicio, A. I. Aranburu, L. Arazi, I. J. Arnquist, F. Auria-Luna, S. Ayet, C. D. R. Azevedo, F. Ballester, M. del Barrio-Torregrosa, A. Bayo, J. M. Benlloch-Rodríguez, F. I. G. M. Borges, A. Brodolin, S. Cárcel, A. Castillo, L. Cid, C. A. N. Conde, T. Contreras, F. P. Cossío, R. Coupe, E. Dey, G. Díaz, C. Echevarria, M. Elorza, J. Escada, R. Esteve, R. Felkai, L. M. P. Fernandes, P. Ferrario, A. L. Ferreira, F. W. Foss, Z. Freixa, J. García-Barrena, J. J. Gómez-Cadenas, J. W. R. Grocott, R. Guenette, J. Hauptman, C. A. O. Henriques, J. A. Hernando Morata, P. Herrero-Gómez, V. Herrero, C. Hervés Carrete, Y. Ifergan, F. Kellerer, L. Larizgoitia, A. Larumbe, P. Lebrun, F. Lopez, N. López-March, R. Madigan, R. D. P. Mano, A. P. Marques, J. Martín-Albo, G. Martínez-Lema, M. Martínez-Vara, R. L. Miller, J. Molina-Canteras, F. Monrabal, C. M. B. Monteiro, F. J. Mora, P. Novella, A. Nuñez, E. Oblak, J. Palacio, B. Palmeiro, A. Para, A. Pazos, J. Pelegrin, M. Pérez Maneiro, M. Querol, J. Renner, I. Rivilla, C. Rogero, B. Romeo, C. Romo-Luque, V. San Nacienciano, F. P. Santos, J. M. F. dos Santos, M. Seemann, I. Shomroni, P. A. O. C. Silva, A. Simón, S. R. Soleti, M. Sorel, J. Soto-Oton, J. M. R. Teixeira, S. Teruel-Pardo, J. F. Toledo, C. Tonnelé, S. Torelli, J. Torrent, A. Trettin, A. Usón, P. R. G. Valle, J. F. C. A. Veloso, J. Waiton, A. Yubero-Navarro

**Updated**: 2025-05-02T04:57:06Z

**Summary**: A critical element in the realization of large liquid and gas time projection chambers (TPCs) is the delivery and distribution of high voltages into and around the detector. Such experiments require of order tens of kilovolts to enable electron drift over meter-scale distances. This paper describes the design and operation of the cathode feedthrough and high voltage distribution through the field cage of the NEXT-100 experiment, an underground TPC that will search for neutrinoless double beta decay $0\nu\beta\beta$. The feedthrough has been demonstrated to hold pressures up to 20~bar and sustain voltages as high as -65~kV, and the TPC is operating stably at its design high voltages. The system has been realized within the constraints of a stringent radiopurity budget and is now being used to execute a suite of sensitive double beta decay analyses.

**Link**: [arxiv](http://arxiv.org/abs/2505.01002v1),  [pdf](http://arxiv.org/pdf/2505.01002v1)

**Tags**: physics.ins-det hep-ex 



### The Open-Source BlackParrot-BedRock Cache Coherence System
**Authors**: Mark Unruh Wyse

**Updated**: 2025-05-02T02:36:23Z

**Summary**: This dissertation revisits the topic of programmable cache coherence engines in the context of modern shared-memory multicore processors. First, the open-source BedRock cache coherence protocol is described. BedRock employs the canonical MOESIF coherence states and reduces implementation burden by eliminating transient coherence states from the protocol. The protocol's design complexity, concurrency, and verification effort are analyzed and compared to a canonical directory-based invalidate coherence protocol. Second, the architecture and microarchitecture of three separate cache coherence directories implementing the BedRock protocol within the BlackParrot 64-bit RISC-V multicore processor, collectively called BlackParrot-BedRock (BP-BedRock), are described. A fixed-function coherence directory engine implementation provides a baseline design for performance and area comparisons. A microcode-programmable coherence directory implementation demonstrates the feasibility of implementing a programmable coherence engine capable of maintaining sufficient protocol processing performance. A hybrid fixed-function and programmable coherence directory blends the protocol processing performance of the fixed-function design with the programmable flexibility of the microcode-programmable design. Collectively, the BedRock coherence protocol and its three BP-BedRock implementations demonstrate the feasibility and challenges of including programmable logic within the coherence system of modern shared-memory multicore processors, paving the way for future research into the application- and system-level benefits of programmable coherence engines.

**Link**: [arxiv](http://arxiv.org/abs/2505.00962v1),  [pdf](http://arxiv.org/pdf/2505.00962v1)

**Tags**: cs.AR 



### Heterogeneous Memory Benchmarking Toolkit
**Authors**: Golsana Ghaemi, Kazem Taram, Renato Mancuso

**Updated**: 2025-05-01T22:32:29Z

**Summary**: This paper presents an open-source kernel-level heterogeneous memory characterization framework (MemScope) for embedded systems that enables users to understand and precisely characterize the temporal behavior of all available memory modules under configurable contention stress scenarios. Since kernel-level provides a high degree of control over allocation, cache maintenance, $CPUs$, interrupts, and I/O device activity, seeking the most accurate way to benchmark heterogeneous memory subsystems, would be achieved by implementing it in the kernel. This gives us the privilege to directly map pieces of contiguous physical memory and instantiate allocators, allowing us to finely control cores to create and eliminate interference. Additionally, we can minimize noise and interruptions, guaranteeing more consistent and precise results compared to equivalent user-space solutions. Running our Framework on a Xilinx Zynq UltraScale+ ZCU102 CPU_FPGA platform, demonstrates its capability to precisely benchmark bandwidth and latency across various memory types, including PL-side DRAM and BRAM, in a multi-core system.

**Link**: [arxiv](http://arxiv.org/abs/2505.00901v1),  [pdf](http://arxiv.org/pdf/2505.00901v1)

**Tags**: cs.AR cs.PF 



### Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from   Large Language Models
**Authors**: Andrew Adiletta, Berk Sunar

**Updated**: 2025-05-01T19:18:56Z

**Summary**: Side-channel attacks on shared hardware resources increasingly threaten confidentiality, especially with the rise of Large Language Models (LLMs). In this work, we introduce Spill The Beans, a novel application of cache side-channels to leak tokens generated by an LLM. By co-locating an attack process on the same hardware as the victim model, we flush and reload embedding vectors from the embedding layer, where each token corresponds to a unique embedding vector. When accessed during token generation, it results in a cache hit detectable by our attack on shared lower-level caches.   A significant challenge is the massive size of LLMs, which, by nature of their compute intensive operation, quickly evicts embedding vectors from the cache. We address this by balancing the number of tokens monitored against the amount of information leaked. Monitoring more tokens increases potential vocabulary leakage but raises the chance of missing cache hits due to eviction; monitoring fewer tokens improves detection reliability but limits vocabulary coverage.   Through extensive experimentation, we demonstrate the feasibility of leaking tokens from LLMs via cache side-channels. Our findings reveal a new vulnerability in LLM deployments, highlighting that even sophisticated models are susceptible to traditional side-channel attacks. We discuss the implications for privacy and security in LLM-serving infrastructures and suggest considerations for mitigating such threats. For proof of concept we consider two concrete attack scenarios: Our experiments show that an attacker can recover as much as 80%-90% of a high entropy API key with single shot monitoring. As for English text we can reach a 40% recovery rate with a single shot. We should note that the rate highly depends on the monitored token set and these rates can be improved by targeting more specialized output domains.

**Link**: [arxiv](http://arxiv.org/abs/2505.00817v1),  [pdf](http://arxiv.org/pdf/2505.00817v1)

**Tags**: cs.CR cs.AI K.6.5 



### Optomechanical resource for fault-tolerant quantum computing
**Authors**: Margaret Pavlovich, Peter Rakich, Shruti Puri

**Updated**: 2025-05-01T18:00:40Z

**Summary**: Fusion-based quantum computing with dual-rail qubits is a leading candidate for scalable quantum computing using linear optics. This paradigm requires single photons which are entangled into small resource states before being fed into a fusion network. The most common sources for single optical photons and for small entangled states are probabilistic and heralded. The realization of a single reliable deterministic source requires many redundant probabilistic sources and a complex optical network for rerouting and retiming probabilistic outputs. In this work, we show how optomechanics enables reliable production of resources for photonic quantum computing without the redundancy of the all-optical approach. This is achieved by using acoustic modes as caches of quantum resources, ranging from single-particle states to small entangled states, with on-demand read-out. The advantages of acoustic modes as optical quantum memories, compared to other technologies, include their intrinsically long lifetimes and that they are solid state, highly tailorable, and insensitive to electromagnetic noise. We show how the resource states can be prepared directly in the acoustic modes using optical controls. This is still probabilistic and heralded, as in the all-optical approach, but the acoustic modes act as a quantum memory which is integrated into the production of the states. The quantum states may be deterministically transferred from acoustic modes to optical modes, on demand, with another optical drive.

**Link**: [arxiv](http://arxiv.org/abs/2505.00768v1),  [pdf](http://arxiv.org/pdf/2505.00768v1)

**Tags**: quant-ph 



### FreqKV: Frequency Domain Key-Value Compression for Efficient Context   Window Extension
**Authors**: Jushi Kai, Boyi Zeng, Yixuan Wang, Haoli Bai, Bo Jiang, Zhouhan Lin

**Updated**: 2025-05-01T14:53:12Z

**Summary**: Extending the context window in large language models (LLMs) is essential for applications involving long-form content generation. However, the linear increase in key-value (KV) cache memory requirements and the quadratic complexity of self-attention with respect to sequence length present significant challenges during fine-tuning and inference. Existing methods suffer from performance degradation when extending to longer contexts. In this work, we introduce a novel context extension method that optimizes both fine-tuning and inference efficiency. Our method exploits a key observation: in the frequency domain, the energy distribution of the KV cache is primarily concentrated in low-frequency components. By filtering out the high-frequency components, the KV cache can be effectively compressed with minimal information loss. Building on this insight, we propose an efficient compression technique, FreqKV, that iteratively compresses the increasing KV cache to a fixed size in the frequency domain, applicable to both fine-tuning and inference. FreqKV introduces no additional parameters or architectural modifications. With minimal fine-tuning, LLMs can learn to leverage the limited cache that is compressed in the frequency domain and extend the context window efficiently. Experiments on various long context language modeling and understanding tasks demonstrate the efficiency and efficacy of the proposed method.

**Link**: [arxiv](http://arxiv.org/abs/2505.00570v1),  [pdf](http://arxiv.org/pdf/2505.00570v1)

**Tags**: cs.CL cs.AI 



### Mixture of Sparse Attention: Content-Based Learnable Sparse Attention   via Expert-Choice Routing
**Authors**: Piotr Piękos, Róbert Csordás, Jürgen Schmidhuber

**Updated**: 2025-05-01T05:22:11Z

**Summary**: Recent advances in large language models highlighted the excessive quadratic cost of self-attention. Despite the significant research efforts, subquadratic attention methods still suffer from inferior performance in practice. We hypothesize that dynamic, learned content-based sparsity can lead to more efficient attention mechanisms. We present Mixture of Sparse Attention (MoSA), a novel approach inspired by Mixture of Experts (MoE) with expert choice routing. MoSA dynamically selects tokens for each attention head, allowing arbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of length $T$, MoSA reduces the computational complexity of each attention head from $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same computational budget, allowing higher specialization. We show that among the tested sparse attention variants, MoSA is the only one that can outperform the dense baseline, sometimes with up to 27% better perplexity for an identical compute budget. MoSA can also reduce the resource usage compared to dense self-attention. Despite using torch implementation without an optimized kernel, perplexity-matched MoSA models are simultaneously faster in wall-clock time, require less memory for training, and drastically reduce the size of the KV-cache compared to the dense transformer baselines.

**Link**: [arxiv](http://arxiv.org/abs/2505.00315v1),  [pdf](http://arxiv.org/pdf/2505.00315v1)

**Tags**: cs.LG cs.CL 



### QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM   Serving
**Authors**: Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, Song Han

**Updated**: 2025-05-01T02:14:05Z

**Summary**: Quantization can accelerate large language model (LLM) inference. Going beyond INT8 quantization, the research community is actively exploring even lower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization techniques only accelerate low-batch, edge LLM inference, failing to deliver performance gains in large-batch, cloud-based LLM serving. We uncover a critical issue: existing INT4 quantization methods suffer from significant runtime overhead (20-90%) when dequantizing either weights or partial sums on GPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands for quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented by the QServe inference library that achieves measured speedup. The key insight driving QServe is that the efficiency of LLM serving on GPUs is critically influenced by operations on low-throughput CUDA cores. Building upon this insight, in QoQ algorithm, we introduce progressive quantization that can allow low dequantization overhead in W4A8 GEMM. Additionally, we develop SmoothAttention to effectively mitigate the accuracy degradation incurred by 4-bit KV quantization. In the QServe system, we perform compute-aware weight reordering and take advantage of register-level parallelism to reduce dequantization latency. We also make fused attention memory-bound, harnessing the performance gain brought by KV4 quantization. As a result, QServe improves the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x on L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to TensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput than TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of LLM serving by 3x. Code is available at https://github.com/mit-han-lab/omniserve.

**Link**: [arxiv](http://arxiv.org/abs/2405.04532v3),  [pdf](http://arxiv.org/pdf/2405.04532v3)

**Tags**: cs.CL cs.AI cs.LG cs.PF 



### Soft-Label Caching and Sharpening for Communication-Efficient Federated   Distillation
**Authors**: Kitsuya Azuma, Takayuki Nishio, Yuichi Kitagawa, Wakako Nakano, Takahito Tanimura

**Updated**: 2025-05-01T00:13:06Z

**Summary**: Federated Learning (FL) enables collaborative model training across decentralized clients, enhancing privacy by keeping data local. Yet conventional FL, relying on frequent parameter-sharing, suffers from high communication overhead and limited model heterogeneity. Distillation-based FL approaches address these issues by sharing predictions (soft-labels) instead, but they often involve redundant transmissions across communication rounds, reducing efficiency. We propose SCARLET, a novel framework integrating synchronized soft-label caching and an enhanced Entropy Reduction Aggregation (Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing cached soft-labels, achieving up to 50% reduction in communication costs compared to existing methods while maintaining accuracy. Enhanced ERA can be tuned to adapt to non-IID data variations, ensuring robust aggregation and performance in diverse client scenarios. Experimental evaluations demonstrate that SCARLET consistently outperforms state-of-the-art distillation-based FL methods in terms of accuracy and communication efficiency. The implementation of SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.

**Link**: [arxiv](http://arxiv.org/abs/2504.19602v2),  [pdf](http://arxiv.org/pdf/2504.19602v2)

**Tags**: cs.LG 



### Accelerating Diffusion Transformer via Error-Optimized Cache
**Authors**: Junxiang Qiu, Shuo Wang, Jinda Lu, Lin Liu, Houcheng Jiang, Xingyu Zhu, Yanbin Hao

**Updated**: 2025-04-30T19:48:41Z

**Summary**: Diffusion Transformer (DiT) is a crucial method for content generation. However, it needs a lot of time to sample. Many studies have attempted to use caching to reduce the time consumption of sampling. Existing caching methods accelerate generation by reusing DiT features from the previous time step and skipping calculations in the next, but they tend to locate and cache low-error modules without focusing on reducing caching-induced errors, resulting in a sharp decline in generated content quality when increasing caching intensity. To solve this problem, we propose the Error-Optimized Cache (EOC). This method introduces three key improvements: (1) Prior knowledge extraction: Extract and process the caching differences; (2) A judgment method for cache optimization: Determine whether certain caching steps need to be optimized; (3) Cache optimization: reduce caching errors. Experiments show that this algorithm significantly reduces the error accumulation caused by caching, especially excessive caching. On the ImageNet dataset, without substantially increasing the computational load, this method improves the FID of the generated images when the rule-based model FORA has a caching level of 75%, 50%, and 25%, and the training-based model Learning-to-cache has a caching level of 22%. Specifically, the FID values change from 30.454 to 21.690 (28.8%), from 6.857 to 5.821 (15.1%), from 3.870 to 3.692 (4.6%), and from 3.539 to 3.451 (2.5%) respectively.

**Link**: [arxiv](http://arxiv.org/abs/2501.19243v2),  [pdf](http://arxiv.org/pdf/2501.19243v2)

**Tags**: cs.CV 



### SDW driven "magnetic breakdown" in a d-wave altermagnet KV$_2$Se$_2$O
**Authors**: Xu Yan, Ziyin Song, Juntao Song, Zhong Fang, Hongming Weng, Quansheng Wu

**Updated**: 2025-04-30T18:00:02Z

**Summary**: Altermagnets, combining zero net magnetization with intrinsic spin splitting, demonstrate unique quantum phenomena crucial for spintronic applications. KV$_2$Se$_2$O is proven to be a d-wave altermagnet with phase transition from a checkerboard-type (C-type) antiferromagnetic (AFM) state to a spin density wave (SDW) state as the temperature decreases. After phase transition, the apparent paradox emerges where angle-resolved photoemission spectroscopy (ARPES) reveals negligible Fermi surface modifications, while physical property measurement system (PPMS) measurements uncover substantial changes in transport properties. Our study explores the microscopic mechanisms governing phase-dependent transport properties of KV$_2$Se$_2$O base on first-principles calculations. The spin canting driven by periodic spin modulation in the SDW phase reduces the magnetic symmetry of KV$_2$Se$_2$O. The resultant band degeneracy lifting and Fermi surface reconstruction induce the ``magnetic breakdown" phenomenon, which alters carrier trajectories, modifies carrier concentration, strengthens electron-hole compensation, and ultimately accounts for the contrasting magnetic-field-dependent Hall resistivity relative to the C-type AFM state. Our work proposes an innovative method for identifying the electronic structure evolution across phase transitions from transport signatures, providing a novel paradigm for altermagnets research.

**Link**: [arxiv](http://arxiv.org/abs/2505.00074v1),  [pdf](http://arxiv.org/pdf/2505.00074v1)

**Tags**: cond-mat.mtrl-sci 



### Switching Transients in Constrained Transformer-Line/Cable   Configurations
**Authors**: Y. Xiang, L. Wu, K. Velitsikakis, A. L. J. Janssen

**Updated**: 2025-04-30T12:51:59Z

**Summary**: This paper investigates the transient phenomena that occur in two special cases in the Netherlands: (A) during the energization of a power transformer via a cable feeder and (B) the energization of a power transformer together with an overhead line (OHL). In Case A a 7 km long 150 kV cable and a 150/50 kV transformer are connected and energized at the same time. In Case B a 150/50 kV transformer and a short 50 kV OHL are connected and energized simultaneously. The reason behind this kind of situations is related to space restrictions and cost efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2504.21594v1),  [pdf](http://arxiv.org/pdf/2504.21594v1)

**Tags**: eess.SY cs.SY 



### Responsive DNN Adaptation for Video Analytics against Environment Shift   via Hierarchical Mobile-Cloud Collaborations
**Authors**: Maozhe Zhao, Shengzhong Liu, Fan Wu, Guihai Chen

**Updated**: 2025-04-30T08:08:15Z

**Summary**: Mobile video analysis systems often encounter various deploying environments, where environment shifts present greater demands for responsiveness in adaptations of deployed "expert DNN models". Existing model adaptation frameworks primarily operate in a cloud-centric way, exhibiting degraded performance during adaptation and delayed reactions to environment shifts. Instead, this paper proposes MOCHA, a novel framework optimizing the responsiveness of continuous model adaptation through hierarchical collaborations between mobile and cloud resources. Specifically, MOCHA (1) reduces adaptation response delays by performing on-device model reuse and fast fine-tuning before requesting cloud model retrieval and end-to-end retraining; (2) accelerates history expert model retrieval by organizing them into a structured taxonomy utilizing domain semantics analyzed by a cloud foundation model as indices; (3) enables efficient local model reuse by maintaining onboard expert model caches for frequent scenes, which proactively prefetch model weights from the cloud model database. Extensive evaluations with real-world videos on three DNN tasks show MOCHA improves the model accuracy during adaptation by up to 6.8% while saving the response delay and retraining time by up to 35.5x and 3.0x respectively.

**Link**: [arxiv](http://arxiv.org/abs/2505.00745v1),  [pdf](http://arxiv.org/pdf/2505.00745v1)

**Tags**: cs.CV cs.LG 



### Kimina Lean Server: Technical Report
**Authors**: Marco Dos Santos, Haiming Wang, Hugues de Saxcé, Ran Wang, Mantas Baksys, Mert Unsal, Junqi Liu, Zhengying Liu, Jia Li

**Updated**: 2025-04-29T23:43:59Z

**Summary**: We introduce the Kimina Lean Server, an open-source project that enables fast and scalable interaction with Lean 4 via a unified REST API, designed as a simple verifier for reinforcement learning pipelines. Built on top of the Lean FRO's LeanREPL, it combines server-side parallelization by managing multiple Lean REPL processes in parallel, with an LRU caching strategy that reuses Lean imports across multiple requests. These features help reduce initialization overhead and allow large-scale batch processing of Lean code. The client-side interface allows users to submit batches of proofs and receive Lean feedback, including extracted tactics and tactic states via infotree processing. These features enable a high-performance, scalable workflow for both interaction and extraction of proofs, tactics, and tactic states. We open source our implementation on GitHub.

**Link**: [arxiv](http://arxiv.org/abs/2504.21230v1),  [pdf](http://arxiv.org/pdf/2504.21230v1)

**Tags**: cs.LO 



### CachePrune: Neural-Based Attribution Defense Against Indirect Prompt   Injection Attacks
**Authors**: Rui Wang, Junda Wu, Yu Xia, Tong Yu, Ruiyi Zhang, Ryan Rossi, Lina Yao, Julian McAuley

**Updated**: 2025-04-29T23:42:21Z

**Summary**: Large Language Models (LLMs) are identified as being susceptible to indirect prompt injection attack, where the model undesirably deviates from user-provided instructions by executing tasks injected in the prompt context. This vulnerability stems from LLMs' inability to distinguish between data and instructions within a prompt. In this paper, we propose CachePrune that defends against this attack by identifying and pruning task-triggering neurons from the KV cache of the input prompt context. By pruning such neurons, we encourage the LLM to treat the text spans of input prompt context as only pure data, instead of any indicator of instruction following. These neurons are identified via feature attribution with a loss function induced from an upperbound of the Direct Preference Optimization (DPO) objective. We show that such a loss function enables effective feature attribution with only a few samples. We further improve on the quality of feature attribution, by exploiting an observed triggering effect in instruction following. Our approach does not impose any formatting on the original prompt or introduce extra test-time LLM calls. Experiments show that CachePrune significantly reduces attack success rates without compromising the response quality. Note: This paper aims to defend against indirect prompt injection attacks, with the goal of developing more secure and robust AI systems.

**Link**: [arxiv](http://arxiv.org/abs/2504.21228v1),  [pdf](http://arxiv.org/pdf/2504.21228v1)

**Tags**: cs.CR cs.AI 



### An Achievable Scheme for the K-user Linear Computation Broadcast Channel
**Authors**: Yinbin Ma, Daniela Tuninetti

**Updated**: 2025-04-29T17:54:42Z

**Summary**: This paper presents a new achievable scheme for the K-user Linear Computation Broadcast Channel (K-LCBC). A K-LCBC comprises data stored on a server and K users, each aiming to retrieve a desired linear function of the data by leveraging their prior locally available side information in the form of another linear function of the data. The proposed scheme is based on a subspace decomposition derived from representable polymatroid spaces. This decomposition enables the server to effectively design multicast messages that simultaneously benefit multiple users and allow users to eliminate interference using their available side information. This work extends existing results for the 3-LCBC by introducing a linear programming framework to optimize multicast opportunities across an arbitrary number of users. The proposed approach can be used to derive achievable scheme for the K-user coded caching problem with linear coded placement and scalar linear function retrieval, which was our original motivation to investigate the K-LCBC.

**Link**: [arxiv](http://arxiv.org/abs/2501.12322v2),  [pdf](http://arxiv.org/pdf/2501.12322v2)

**Tags**: cs.IT math.IT 



### Activated LoRA: Fine-tuned LLMs for Intrinsics
**Authors**: Kristjan Greenewald, Luis Lastras, Thomas Parnell, Vraj Shah, Lucian Popa, Giulio Zizzo, Chulaka Gunasekara, Ambrish Rawat, David Cox

**Updated**: 2025-04-29T14:25:08Z

**Summary**: Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for finetuning the weights of large foundation models, and has become the go-to method for data-driven customization of LLMs. Despite the promise of highly customized behaviors and capabilities, switching between relevant LoRAs in a multiturn setting is highly inefficient, as the key-value (KV) cache of the entire turn history must be recomputed with the LoRA weights before generation can begin. To address this problem, we propose Activated LoRA (aLoRA), which modifies the LoRA framework to only adapt weights for the tokens in the sequence \emph{after} the aLoRA is invoked. This change crucially allows aLoRA to accept the base model's KV cache of the input string, meaning that aLoRA can be instantly activated whenever needed in a chain without recomputing the cache. This enables building what we call \emph{intrinsics}, i.e. highly specialized models invoked to perform well-defined operations on portions of an input chain or conversation that otherwise uses the base model by default. We use aLoRA to train a set of intrinsics models, demonstrating competitive accuracy with standard LoRA while achieving significant inference benefits.

**Link**: [arxiv](http://arxiv.org/abs/2504.12397v2),  [pdf](http://arxiv.org/pdf/2504.12397v2)

**Tags**: cs.LG cs.AI 



### Tree embedding based mapping system for low-latency mobile applications   in multi-access networks
**Authors**: Yu Mi, Randeep Bhatia, Fang Hao, An Wang, Steve Benno, Tv Lakshman

**Updated**: 2025-04-28T20:30:59Z

**Summary**: Low-latency applications like AR/VR and online gaming need fast, stable connections. New technologies such as V2X, LEO satellites, and 6G bring unique challenges in mobility management. Traditional solutions based on centralized or distributed anchors often fall short in supporting rapid mobility due to inefficient routing, low versatility, and insufficient multi-access support. In this paper, we design a new end-to-end system for tracking multi-connected mobile devices at scale and optimizing performance for latency-sensitive, highly dynamic applications. Our system, based on the locator/ID separation principle, extends to multi-access networks without requiring specialized routers or caching. Using a novel tree embedding-based overlay, we enable fast session setup while allowing endpoints to directly handle mobility between them. Evaluation with real network data shows our solution cuts connection latency to 7.42% inflation over the shortest path, compared to LISP's 359\% due to cache misses. It also significantly reduces location update overhead and disruption time during mobility.

**Link**: [arxiv](http://arxiv.org/abs/2504.20246v1),  [pdf](http://arxiv.org/pdf/2504.20246v1)

**Tags**: cs.NI 



### Hierarchical Coded Caching with Low Subpacketization and Coding Delay   using Combinatorial t-Designs
**Authors**: Rashid Ummer N. T., B. Sundar Rajan

**Updated**: 2025-04-28T17:17:53Z

**Summary**: Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN) considered a broadcast network consisting of a single server connected to a set of users each having a cache memory. Motivated by practical scenarios, Karamchandani \textit{et al.} in [16] proposed a coded caching scheme for a two-layer hierarchical network consisting of a single server connected to multiple mirror sites and each mirror site connected to a distinct set of users, in which both mirror sites and users having cache memories. Low subpacketization level coded caching schemes are desirable for practical implementations. Placement delivery array (PDA) was proposed as a tool to design coded caching schemes with reduced subpacketization level by Yan \textit{et al.} in [4]. Schemes with reduced subpacketization levels are studied extensively in the literature for single-layer networks. Kong \textit{et al.} in [17] proposed a structure called hierarchical placement delivery arrays (HPDA), which characterizes a hierarchical coded caching system and also proposed a class of HPDAs that gives low subpacketization level schemes by using two PDAs. Low subpacketization level hierarchical schemes using combinatorial $t$-designs is proposed in [20]. Apart from that there is no other existing work that discusses the subpacketization problem in a hierarchical network. This paper proposes a class of HPDA construction that gives low subpacketization level hierarchical coded caching schemes, by first constructing a new class of PDAs. Compared with the existing schemes, in cases where the system parameters and subpacketization level are the same, the proposed hierarchical scheme has a better coding delay. Further, the new class of PDAs constructed either subsumes several known PDA constructions or achieves better transmission load for the same system parameters.

**Link**: [arxiv](http://arxiv.org/abs/2405.12747v3),  [pdf](http://arxiv.org/pdf/2405.12747v3)

**Tags**: cs.IT math.IT 



### 3D MPSoC with On-Chip Cache Support -- Design and Exploitation
**Authors**: Rodrigo Cataldo, Cesar Marcon, Debora Matos

**Updated**: 2025-04-28T16:59:13Z

**Summary**: The increasing density of transistors in Integrated Circuits (ICs) has enabled the development of highly integrated Systems-on-Chip (SoCs) and, more recently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability challenges in communication and memory performance, three-dimensional (3D) Network-on-Chip (NoC) architectures have emerged, offering improvements in communication latency and throughput. However, memory system efficiency remains a critical bottleneck in NoC-based designs. This work proposes the design and experimental exploration of 3D MPSoCs with on-chip cache support by employing distinct communication infrastructures for inter-processor and memory interactions. Specifically, packet-based NoCs are adopted for inter-processor communication, while a crossbar-based infrastructure supports a cache coherence hierarchy for memory access. A two-layer system architecture is introduced, combining a Uniform Memory Access (UMA) model within clusters and a No Remote Memory Access (NORMA) model between clusters, aiming to balance scalability and coherence requirements. Emerging memory technologies such as PCRAM and MRAM are explored to optimize performance, energy consumption, and area usage. Experimental evaluations are conducted using the Gem5 simulator, targeting a model based on the ARM Versatile Express platform. The outcomes of this study aim to enhance MPSoC scalability while meeting the stringent demands of memory-centric applications.

**Link**: [arxiv](http://arxiv.org/abs/2504.19984v1),  [pdf](http://arxiv.org/pdf/2504.19984v1)

**Tags**: cs.AR 



### TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate
**Authors**: Amir Zandieh, Majid Daliri, Majid Hadian, Vahab Mirrokni

**Updated**: 2025-04-28T15:05:35Z

**Summary**: Vector quantization, a problem rooted in Shannon's source coding theory, aims to quantize high-dimensional Euclidean vectors while minimizing distortion in their geometric structure. We propose TurboQuant to address both mean-squared error (MSE) and inner product distortion, overcoming limitations of existing methods that fail to achieve optimal distortion rates. Our data-oblivious algorithms, suitable for online applications, achieve near-optimal distortion rates (within a small constant factor) across all bit-widths and dimensions. TurboQuant achieves this by randomly rotating input vectors, inducing a concentrated Beta distribution on coordinates, and leveraging the near-independence property of distinct coordinates in high dimensions to simply apply optimal scalar quantizers per each coordinate. Recognizing that MSE-optimal quantizers introduce bias in inner product estimation, we propose a two-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL (QJL) transform on the residual, resulting in an unbiased inner product quantizer. We also provide a formal proof of the information-theoretic lower bounds on best achievable distortion rate by any vector quantizer, demonstrating that TurboQuant closely matches these bounds, differing only by a small constant ($\approx 2.7$) factor. Experimental results validate our theoretical findings, showing that for KV cache quantization, we achieve absolute quality neutrality with 3.5 bits per channel and marginal quality degradation with 2.5 bits per channel. Furthermore, in nearest neighbor search tasks, our method outperforms existing product quantization techniques in recall while reducing indexing time to virtually zero.

**Link**: [arxiv](http://arxiv.org/abs/2504.19874v1),  [pdf](http://arxiv.org/pdf/2504.19874v1)

**Tags**: cs.LG cs.AI cs.DB cs.DS 



### semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated   Computation and Unified Storage
**Authors**: Ke Hong, Lufang Chen, Zhong Wang, Xiuhong Li, Qiuli Mao, Jianping Ma, Chao Xiong, Guanyu Wu, Buhe Han, Guohao Dai, Yun Liang, Yu Wang

**Updated**: 2025-04-28T15:00:03Z

**Summary**: Existing large language model (LLM) serving systems fall into two categories: 1) a unified system where prefill phase and decode phase are co-located on the same GPU, sharing the unified computational resource and storage, and 2) a disaggregated system where the two phases are disaggregated to different GPUs. The design of the disaggregated system addresses the latency interference and sophisticated scheduling issues in the unified system but leads to storage challenges including 1) replicated weights for both phases that prevent flexible deployment, 2) KV cache transfer overhead between the two phases, 3) storage imbalance that causes substantial wasted space of the GPU capacity, and 4) suboptimal resource adjustment arising from the difficulties in migrating KV cache. Such storage inefficiency delivers poor serving performance under high request rates.   In this paper, we identify that the advantage of the disaggregated system lies in the disaggregated computation, i.e., partitioning the computational resource to enable the asynchronous computation of two phases. Thus, we propose a novel LLM serving system, semi-PD, characterized by disaggregated computation and unified storage. In semi-PD, we introduce a computation resource controller to achieve disaggregated computation at the streaming multi-processor (SM) level, and a unified memory manager to manage the asynchronous memory access from both phases. semi-PD has a low-overhead resource adjustment mechanism between the two phases, and a service-level objective (SLO) aware dynamic partitioning algorithm to optimize the SLO attainment. Compared to state-of-the-art systems, semi-PD maintains lower latency at higher request rates, reducing the average end-to-end latency per request by 1.27-2.58x on DeepSeek series models, and serves 1.55-1.72x more requests adhering to latency constraints on Llama series models.

**Link**: [arxiv](http://arxiv.org/abs/2504.19867v1),  [pdf](http://arxiv.org/pdf/2504.19867v1)

**Tags**: cs.CL cs.DC cs.LG 



### Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching   for Small Buffer or Small Rate
**Authors**: Han Fang, Nan Liu, Wei Kang

**Updated**: 2025-04-28T09:03:45Z

**Summary**: We consider the secure coded caching problem proposed by Ravindrakumar et. al where no user can obtain information about files other than the one requested. We first propose three new schemes for the three cases of cache size $M=1$, $N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files and $K$ users, and the general case for arbitrary $N$ files and $K$ users, respectively. Then we derive converse results by characterizing new properties of secure coded caching schemes. As a result, we characterize the two end-points of the optimal memory-rate tradeoff curve for arbitrary number of users and files. Furthermore, for the case of $N=2$ files and arbitrary number of users, we also characterize a segment of the optimal memory-rate tradeoff curve, where the cache size is relatively small.

**Link**: [arxiv](http://arxiv.org/abs/2504.19601v1),  [pdf](http://arxiv.org/pdf/2504.19601v1)

**Tags**: cs.IT math.IT 



### Quantifying Memory Utilization with Effective State-Size
**Authors**: Rom N. Parnichkun, Neehal Tumma, Armin W. Thomas, Alessandro Moro, Qi An, Taiji Suzuki, Atsushi Yamashita, Michael Poli, Stefano Massaroli

**Updated**: 2025-04-28T08:12:30Z

**Summary**: The need to develop a general framework for architecture analysis is becoming increasingly important, given the expanding design space of sequence models. To this end, we draw insights from classical signal processing and control theory, to develop a quantitative measure of \textit{memory utilization}: the internal mechanisms through which a model stores past information to produce future outputs. This metric, which we call \textbf{\textit{effective state-size}} (ESS), is tailored to the fundamental class of systems with \textit{input-invariant} and \textit{input-varying linear operators}, encompassing a variety of computational units such as variants of attention, convolutions, and recurrences. Unlike prior work on memory utilization, which either relies on raw operator visualizations (e.g. attention maps), or simply the total \textit{memory capacity} (i.e. cache size) of a model, our metrics provide highly interpretable and actionable measurements. In particular, we show how ESS can be leveraged to improve initialization strategies, inform novel regularizers and advance the performance-efficiency frontier through model distillation. Furthermore, we demonstrate that the effect of context delimiters (such as end-of-speech tokens) on ESS highlights cross-architectural differences in how large language models utilize their available memory to recall information. Overall, we find that ESS provides valuable insights into the dynamics that dictate memory utilization, enabling the design of more efficient and effective sequence models.

**Link**: [arxiv](http://arxiv.org/abs/2504.19561v1),  [pdf](http://arxiv.org/pdf/2504.19561v1)

**Tags**: cs.LG 



### Prisma: An Open Source Toolkit for Mechanistic Interpretability in   Vision and Video
**Authors**: Sonia Joseph, Praneet Suresh, Lorenz Hufe, Edward Stevinson, Robert Graham, Yash Vadi, Danilo Bzdok, Sebastian Lapuschkin, Lee Sharkey, Blake Aaron Richards

**Updated**: 2025-04-28T04:31:24Z

**Summary**: Robust tooling and publicly available pre-trained models have helped drive recent advances in mechanistic interpretability for language models. However, similar progress in vision mechanistic interpretability has been hindered by the lack of accessible frameworks and pre-trained weights. We present Prisma (Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an open-source framework designed to accelerate vision mechanistic interpretability research, providing a unified toolkit for accessing 75+ vision and video transformers; support for sparse autoencoder (SAE), transcoder, and crosscoder training; a suite of 80+ pre-trained SAE weights; activation caching, circuit analysis tools, and visualization tools; and educational resources. Our analysis reveals surprising findings, including that effective vision SAEs can exhibit substantially lower sparsity patterns than language SAEs, and that in some instances, SAE reconstructions can decrease model loss. Prisma enables new research directions for understanding vision model internals while lowering barriers to entry in this emerging field.

**Link**: [arxiv](http://arxiv.org/abs/2504.19475v1),  [pdf](http://arxiv.org/pdf/2504.19475v1)

**Tags**: cs.CV cs.AI cs.LG 



### From Cluster to Desktop: A Cache-Accelerated INR framework for   Interactive Visualization of Tera-Scale Data
**Authors**: Daniel Zavorotny, Qi Wu, David Bauer, Kwan-Liu Ma

**Updated**: 2025-04-28T04:02:30Z

**Summary**: Machine learning has enabled the use of implicit neural representations (INRs) to efficiently compress and reconstruct massive scientific datasets. However, despite advances in fast INR rendering algorithms, INR-based rendering remains computationally expensive, as computing data values from an INR is significantly slower than reading them from GPU memory. This bottleneck currently restricts interactive INR visualization to professional workstations. To address this challenge, we introduce an INR rendering framework accelerated by a scalable, multi-resolution GPU cache capable of efficiently representing tera-scale datasets. By minimizing redundant data queries and prioritizing novel volume regions, our method reduces the number of INR computations per frame, achieving an average 5x speedup over the state-of-the-art INR rendering method while still maintaining high visualization quality. Coupled with existing hardware-accelerated INR compressors, our framework enables scientists to generate and compress massive datasets in situ on high-performance computing platforms and then interactively explore them on consumer-grade hardware post hoc.

**Link**: [arxiv](http://arxiv.org/abs/2504.18001v2),  [pdf](http://arxiv.org/pdf/2504.18001v2)

**Tags**: cs.GR 



### Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and   Generalizable Point Cloud Analysis
**Authors**: Hongyu Sun, Qiuhong Ke, Ming Cheng, Yongcai Wang, Deying Li, Chenhui Gou, Jianfei Cai

**Updated**: 2025-04-28T02:58:27Z

**Summary**: This paper proposes a general solution to enable point cloud recognition models to handle distribution shifts at test time. Unlike prior methods, which rely heavily on training data (often inaccessible during online inference) and are limited to recognizing a fixed set of point cloud classes predefined during training, we explore a more practical and challenging scenario: adapting the model solely based on online test data to recognize both previously seen classes and novel, unseen classes at test time. To this end, we develop \textbf{Point-Cache}, a hierarchical cache model that captures essential clues of online test samples, particularly focusing on the global structure of point clouds and their local-part details. Point-Cache, which serves as a rich 3D knowledge base, is dynamically managed to prioritize the inclusion of high-quality samples. Designed as a plug-and-play module, our method can be flexibly integrated into large multimodal 3D models to support open-vocabulary point cloud recognition. Notably, our solution operates with efficiency comparable to zero-shot inference, as it is entirely training-free. Point-Cache demonstrates substantial gains across 8 challenging benchmarks and 4 representative large 3D models, highlighting its effectiveness. Code is available at https://github.com/auniquesun/Point-Cache.

**Link**: [arxiv](http://arxiv.org/abs/2503.12150v3),  [pdf](http://arxiv.org/pdf/2503.12150v3)

**Tags**: cs.CV 



### AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration
**Authors**: Zhuoping Yang, Jinming Zhuang, Xingzhen Chen, Alex K. Jones, Peipei Zhou

**Updated**: 2025-04-27T22:05:14Z

**Summary**: Graphics Processing Units (GPUs) have become essential for computationally intensive applications. However, emerging workloads such as recommender systems, graph analytics, and data analytics often involve processing data exceeding GPU on-chip memory capacity. To mitigate this issue, existing solutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them, the GPU-centric approach lets GPU threads directly initiate NVMe requests, eliminating CPU intervention overhead over traditional methods. However, the SOTA GPU-centric approach adopts a synchronous IO model, and threads must tolerate the long latency in communication before starting any tasks.   In this work, we propose AGILE, a lightweight and efficient asynchronous library allowing GPU threads to access SSDs asynchronously while eliminating deadlock risks. AGILE also integrates a flexible software cache using GPU High-Bandwidth Mamory (HBM). We demonstrate that the asynchronous GPU-centric IO achieves up to 1.88$\times$ improvement in workloads with different computation-to-communication (CTC) ratios. We also compare AGILE with the SOTA work BaM on Deep Learning Recommendation Models (DLRM) with various settings, and the results show that AGILE achieves 1.75$\times$ performance improvement due to its efficient design and the overlapping strategy enabled by an asynchronous IO model. We further evaluate AGILE's API overhead on graph applications, and the results demonstrate AGILE reduces software cache overhead by up to 3.12$\times$ and overhead in NVMe IO requests by up to 2.85$\times$. Compared with BaM, AGILE consumes fewer registers and exhibits up to 1.32$\times$ reduction in the usage of registers.

**Link**: [arxiv](http://arxiv.org/abs/2504.19365v1),  [pdf](http://arxiv.org/pdf/2504.19365v1)

**Tags**: cs.DC 



### OpenFusion++: An Open-vocabulary Real-time Scene Understanding System
**Authors**: Xiaofeng Jin, Matteo Frosi, Matteo Matteucci

**Updated**: 2025-04-27T14:46:43Z

**Summary**: Real-time open-vocabulary scene understanding is essential for efficient 3D perception in applications such as vision-language navigation, embodied intelligence, and augmented reality. However, existing methods suffer from imprecise instance segmentation, static semantic updates, and limited handling of complex queries. To address these issues, we present OpenFusion++, a TSDF-based real-time 3D semantic-geometric reconstruction system. Our approach refines 3D point clouds by fusing confidence maps from foundational models, dynamically updates global semantic labels via an adaptive cache based on instance area, and employs a dual-path encoding framework that integrates object attributes with environmental context for precise query responses. Experiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate that OpenFusion++ significantly outperforms the baseline in both semantic accuracy and query responsiveness.

**Link**: [arxiv](http://arxiv.org/abs/2504.19266v1),  [pdf](http://arxiv.org/pdf/2504.19266v1)

**Tags**: cs.CV 68T45, 68U05 I.2.10; I.4.8 



### WuNeng: Hybrid State with Attention
**Authors**: Liu Xiao, Li Zhiyuan, Lin Yueyu

**Updated**: 2025-04-27T10:48:56Z

**Summary**: The WuNeng architecture introduces a novel approach to enhancing the expressivity and power of large language models by integrating recurrent neural network (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing heightened contextual coherence over reducing KV cache size. Building upon the hybrid-head concept from Hymba, WuNeng augments standard multi-head attention with additional RWKV-7 state-driven heads, rather than replacing existing heads, to enrich the model's representational capacity. A cross-head interaction technique fosters dynamic synergy among standard, state-driven, and newly introduced middle heads, leveraging concatenation, additive modulation, and gated fusion for robust information integration. Furthermore, a multi-token state processing mechanism harnesses the continuous RWKV-7 state to capture intricate, sequence-wide dependencies, significantly boosting expressivity. Remarkably, these enhancements are achieved with minimal additional parameters, ensuring efficiency while empowering the model to excel in complex reasoning and sequence generation tasks. WuNeng sets a new standard for balancing expressivity and computational efficiency in modern neural architectures.

**Link**: [arxiv](http://arxiv.org/abs/2504.19191v1),  [pdf](http://arxiv.org/pdf/2504.19191v1)

**Tags**: cs.CL 



### I Know What You Sync: Covert and Side Channel Attacks on File Systems   via syncfs
**Authors**: Cheng Gu, Yicheng Zhang, Nael Abu-Ghazaleh

**Updated**: 2025-04-26T12:07:35Z

**Summary**: Operating Systems enforce logical isolation using abstractions such as processes, containers, and isolation technologies to protect a system from malicious or buggy code. In this paper, we show new types of side channels through the file system that break this logical isolation. The file system plays a critical role in the operating system, managing all I/O activities between the application layer and the physical storage device. We observe that the file system implementation is shared, leading to timing leakage when using common I/O system calls. Specifically, we found that modern operating systems take advantage of any flush operation (which saves cached blocks in memory to the SSD or disk) to flush all of the I/O buffers, even those used by other isolation domains. Thus, by measuring the delay of syncfs, the attacker can infer the I/O behavior of victim programs. We then demonstrate a syncfs covert channel attack on multiple file systems, including both Linux native file systems and the Windows file system, achieving a maximum bandwidth of 5 Kbps with an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on Windows. In addition, we construct three side-channel attacks targeting both Linux and Android devices. On Linux devices, we implement a website fingerprinting attack and a video fingerprinting attack by tracking the write patterns of temporary buffering files. On Android devices, we design an application fingerprinting attack that leaks application write patterns during boot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally, we demonstrate that these attacks can be exploited across containers implementing a container detection technique and a cross-container covert channel attack.

**Link**: [arxiv](http://arxiv.org/abs/2411.10883v2),  [pdf](http://arxiv.org/pdf/2411.10883v2)

**Tags**: cs.CR 



### ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM   Inference
**Authors**: Hanshi Sun, Li-Wen Chang, Wenlei Bao, Size Zheng, Ningxin Zheng, Xin Liu, Harry Dong, Yuejie Chi, Beidi Chen

**Updated**: 2025-04-25T19:40:54Z

**Summary**: With the widespread deployment of long-context large language models (LLMs), there has been a growing demand for efficient support of high-throughput inference. However, as the key-value (KV) cache expands with the sequence length, the increasing memory footprint and the need to access it for each token generation both result in low throughput when serving long-context LLMs. While various dynamic sparse attention methods have been proposed to speed up inference while maintaining generation quality, they either fail to sufficiently reduce GPU memory consumption or introduce significant decoding latency by offloading the KV cache to the CPU. We present ShadowKV, a high-throughput long-context LLM inference system that stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences. To minimize decoding latency, ShadowKV employs an accurate KV selection strategy that reconstructs minimal sparse KV pairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks, including RULER, LongBench, and Needle In A Haystack, and models like Llama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K, we demonstrate that it can support up to 6$\times$ larger batch sizes and boost throughput by up to 3.04$\times$ on an A100 GPU without sacrificing accuracy, even surpassing the performance achievable with infinite batch size under the assumption of infinite GPU memory. The code is available at https://github.com/bytedance/ShadowKV.

**Link**: [arxiv](http://arxiv.org/abs/2410.21465v3),  [pdf](http://arxiv.org/pdf/2410.21465v3)

**Tags**: cs.LG cs.CL 



### Constructing Hamiltonian Decompositions of Complete $k$-Uniform   Hypergraphs
**Authors**: Javad Maheri, Petros Elia

**Updated**: 2025-04-25T15:45:36Z

**Summary**: Motivated by the wide-ranging applications of Hamiltonian decompositions in distributed computing, coded caching, routing, resource allocation, load balancing, and fault tolerance, our work presents a comprehensive design for Hamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$. Building upon the resolution of the long-standing conjecture of the existence of Hamiltonian decompositions of complete hypergraphs, a problem that was resolved using existence-based methods, our contribution goes beyond the previous explicit designs, which were confined to the specific cases of $k=2$ and $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing for a broad applicability of Hamiltonian decompositions in various settings.

**Link**: [arxiv](http://arxiv.org/abs/2504.18434v1),  [pdf](http://arxiv.org/pdf/2504.18434v1)

**Tags**: cs.IT math.IT 



### FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack
**Authors**: Xuzheng Chen, Jie Zhang, Baolin Zhu, Xueying Zhu, Zhongqing Chen, Shu Ma, Lingjun Zhu, Chao Shi, Yin Zhang, Zeke Wang

**Updated**: 2025-04-25T15:44:38Z

**Summary**: As the gap between network and CPU speeds rapidly increases, the CPU-centric network stack proves inadequate due to excessive CPU and memory overhead. While hardware-offloaded network stacks alleviate these issues, they suffer from limited flexibility in both control and data planes. Offloading network stack to off-path SmartNIC seems promising to provide high flexibility; however, throughput remains constrained by inherent SmartNIC architectural limitations.   To this end, we design FlexiNS, a SmartNIC-centric network stack with software transport programmability and line-rate packet processing capabilities. To grapple with the limitation of SmartNIC-induced challenges, FlexiNS introduces: (a) a header-only offloading TX path; (b) an unlimited-working-set in-cache processing RX path; (c) a high-performance DMA-only notification pipe; and (d) a programmable offloading engine. We prototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box RDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\times$ higher throughput than the microkernel-based baseline in block storage disaggregation and 1.3$\times$ higher throughput than the hardware-offloaded baseline in KVCache transfer.

**Link**: [arxiv](http://arxiv.org/abs/2504.18432v1),  [pdf](http://arxiv.org/pdf/2504.18432v1)

**Tags**: cs.NI 



### Demand Private Coded Caching: Small Cache Size
**Authors**: Qinyi Lu, Nan Liu, Wei Kang, Chunguo Li

**Updated**: 2025-04-25T10:43:23Z

**Summary**: We investigate the demand private coded caching problem, which is an $(N,K)$ coded caching problem with $N$ files, $K$ users, each equipped with a cache of size $M$, and an additional privacy constraint on user demands, i.e., each user can not gain any information about the demands of other users. We focus on scenarios where the size of users' caches is small, aiming to further characterize the fundamental limits of this problem. We first present a new virtual-user-based achievable scheme for arbitrary number of users and files, and two MDS-code-based achievable schemes for the case $N \le K$. With a newly derived converse bound for the case $N \le K$, these proposed schemes lead to the optimal memory-rate tradeoff of the demand private coded caching problem for $M \in \big[0, \frac{N}{(K+1)(N-1)} \big] $ where $N \le K \le 2N-2$, and the optimal memory-rate tradeoff for $M \in \big[0, \frac{1}{K+1} \big] $ where $ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users, by deriving another new converse bound, the optimal memory-rate tradeoff is characterized for $M\in \big[0,\frac{2}{K}\big] \cup \big[\frac{2(K-1)}{K+1},2\big]$. Finally, we provide the optimal memory-rate tradeoff of the demand private coded caching problem for 2 files and 3 users.

**Link**: [arxiv](http://arxiv.org/abs/2504.18242v1),  [pdf](http://arxiv.org/pdf/2504.18242v1)

**Tags**: cs.IT math.IT 



### Efficient GNN Training Through Structure-Aware Randomized Mini-Batching
**Authors**: Vignesh Balaji, Christos Kozyrakis, Gal Chechik, Haggai Maron

**Updated**: 2025-04-25T05:16:53Z

**Summary**: Graph Neural Networks (GNNs) enable learning on realworld graphs and mini-batch training has emerged as the de facto standard for training GNNs because it can scale to very large graphs and improve convergence. Current mini-batch construction policies largely ignore efficiency considerations of GNN training. Specifically, existing mini-batching techniques employ randomization schemes to improve accuracy and convergence. However, these randomization schemes are often agnostic to the structural properties of the graph (for eg. community structure), resulting in highly irregular memory access patterns during GNN training that make suboptimal use of on-chip GPU caches. On the other hand, while deterministic mini-batching based solely on graph structure delivers fast runtime performance, the lack of randomness compromises both the final model accuracy and training convergence speed. In this paper, we present Community-structure-aware Randomized Mini-batching (COMM-RAND), a novel methodology that bridges the gap between the above extremes. COMM-RAND allows practitioners to explore the space between pure randomness and pure graph structural awareness during mini-batch construction, leading to significantly more efficient GNN training with similar accuracy. We evaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND cuts down GNN training time by up to 2.76x (1.8x on average) while achieving an accuracy that is within 1.79% points (0.42% on average) compared to popular random mini-batching approaches.

**Link**: [arxiv](http://arxiv.org/abs/2504.18082v1),  [pdf](http://arxiv.org/pdf/2504.18082v1)

**Tags**: cs.LG cs.AI 



### Optimizing ML Concurrent Computation and Communication with GPU DMA   Engines
**Authors**: Anirudha Agrawal, Shaizeen Aga, Suchita Pati, Mahzabeen Islam

**Updated**: 2025-04-25T05:08:45Z

**Summary**: Concurrent computation and communication (C3) is a pervasive paradigm in ML and other domains, making its performance optimization crucial. In this paper, we carefully characterize C3 in ML on GPUs, which are most widely deployed for ML training and inference. We observe that while C3 leads to performance uplifts, the uplifts are far lower than ideal speedups (serial computation and communication versus maximum of computation or communication; all times from isolated executions). That is, C3 on average achieves only 21% of ideal speedup. This is so, due to known challenges of compute and memory interference between concurrent GPU kernels (that is, sharing of GPU's compute units, caches and HBM).   To attain better performance for C3, first, we evaluate dual strategies of schedule prioritization and careful resource partitioning of compute units on GPUs to push performance attained with C3 (on average 42% of ideal speedup). We also provide heuristics that can guide a runtime while employing these strategies. To further enhance C3 performance, we propose to mitigate C3 interference by offloading communication tasks to the GPU's DMA engines. To this end, we build concurrent communication collectives (ConCCL) proof-of-concepts that harness DMA engines for communication. We show how ConCCL considerably closes the gap between realized and ideal speedup for C3 (on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall, our work makes a strong case for GPU DMA engine advancements to better support C3 on GPUs.

**Link**: [arxiv](http://arxiv.org/abs/2412.14335v2),  [pdf](http://arxiv.org/pdf/2412.14335v2)

**Tags**: cs.AR cs.DC 



### Fluctuated lattice-driven charge density wave far above the condensation   temperature in kagome superconductor KV$_3$Sb$_5$
**Authors**: Haoran Liu, Shaofeng Duan, Xiangqi Liu, Zhihua Liu, Shichong Wang, Lingxiao Gu, Jiongyu Huang, Wenxuan Yang, Jianzhe Liu, Dong Qian, Yanfeng Guo, Wentao Zhang

**Updated**: 2025-04-25T05:05:49Z

**Summary**: The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including an unconventional charge density wave (CDW). Elucidating the underlying mechanism behind the CDW transition is crucial for unraveling the complex interactions among these phases. However, the driving force of the CDW remains a topic of debate due to the intertwined interactions among the system's various excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by isolating the ultrafast electronic phase transition using time- and angleresolved photoemission spectroscopy. An ultrafast electronic phase transition was observed at a critical photoexcitation fluence, F$_c$, without reduction in CDW lattice-distortion-induced band folding. This folded band persisted up to 150 K under equilibrium heating, well above the CDW condensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts at F$_c$ were comparable to those caused by thermal effects at T$_c$. These findings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane CDW emerges above 150 K, with out-of-plane electronic correlations leading to the $2\times2 \times 2$ CDW near T$_c$, offering key insights into the interplay between the electronic and structural dynamics in AV$_3$Sb$_5$.

**Link**: [arxiv](http://arxiv.org/abs/2504.16620v2),  [pdf](http://arxiv.org/pdf/2504.16620v2)

**Tags**: cond-mat.str-el cond-mat.mtrl-sci cond-mat.supr-con 



### Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A   first-principles DFT+$U$+$V$ study
**Authors**: Indukuru Ramesh Reddy, Sayandeep Ghosh, Bongjae Kim, Chang-Jong Kang

**Updated**: 2025-04-25T00:41:43Z

**Summary**: Nonlocal Coulomb interactions play a crucial role in stabilizing distinct electronic phases in kagome materials. In this work, we systematically investigate the effects of on-site ($U$) and inter-site ($V$) Coulomb interactions on the electronic structure and stability of charge-density-wave (CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory (DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and stability of CDW phases, whereas $U$ suppresses these phases, highlighting a fundamental competition between local and nonlocal electronic correlations. By directly comparing our theoretical results with angle-resolved photoemission spectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that accurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings establish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable insights into the correlated electronic states in kagome metals and serving as a foundation for future explorations of correlation-driven phenomena in related materials.

**Link**: [arxiv](http://arxiv.org/abs/2504.17995v1),  [pdf](http://arxiv.org/pdf/2504.17995v1)

**Tags**: cond-mat.str-el 



### Updated parameters of the LArQL model
**Authors**: L. Paulucci, F. Cavanna, V. Vale, F. Marinho

**Updated**: 2025-04-24T18:09:25Z

**Summary**: The need for a microscopic description of scintillation light generation in liquid argon becomes increasingly desirable with the upcoming operation of large scale LArTPCs in the next decade. While a detailed mathematical account of the process is still to be achieved, a phenomenological model for simultaneously treating ionization and scintillation, LArQL, has been successfully employed to describe the range of electric fields from 0 to 0.75 kV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the free ionization charge and scintillation light. A reanalysis of the original model parameter values has been performed within a global fit procedure and is presented.

**Link**: [arxiv](http://arxiv.org/abs/2504.17866v1),  [pdf](http://arxiv.org/pdf/2504.17866v1)

**Tags**: hep-ex 



### L3: DIMM-PIM Integrated Architecture and Coordination for Scalable   Long-Context LLM Inference
**Authors**: Qingyuan Liu, Liyan Chen, Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen

**Updated**: 2025-04-24T14:14:07Z

**Summary**: Large Language Models (LLMs) increasingly require processing long text sequences, but GPU memory limitations force difficult trade-offs between memory capacity and bandwidth. While HBM-based acceleration offers high bandwidth, its capacity remains constrained. Offloading data to host-side DIMMs improves capacity but introduces costly data swapping overhead. We identify that the critical memory bottleneck lies in the decoding phase of multi-head attention (MHA) exclusively, which demands substantial capacity for storing KV caches and high bandwidth for attention computation. Our key insight reveals this operation uniquely aligns with modern DIMM-based processing-in-memory (PIM) architectures, which offers scalability of both capacity and bandwidth.   Based on this observation and insight, we propose L3, a hardware-software co-designed system integrating DIMM-PIM and GPU devices. L3 introduces three innovations: First, hardware redesigns resolve data layout mismatches and computational element mismatches in DIMM-PIM, enhancing LLM inference utilization. Second, communication optimization enables hiding the data transfer overhead with the computation. Third, an adaptive scheduler coordinates GPU-DIMM-PIM operations to maximize parallelism between devices. Evaluations using real-world traces show L3 achieves up to 6.1$\times$ speedup over state-of-the-art HBM-PIM solutions while significantly improving batch sizes.

**Link**: [arxiv](http://arxiv.org/abs/2504.17584v1),  [pdf](http://arxiv.org/pdf/2504.17584v1)

**Tags**: cs.AR cs.LG 



### Rethinking PM Crash Consistency in the CXL Era
**Authors**: João Oliveira, João Gonçalves, Miguel Matos

**Updated**: 2025-04-24T13:47:35Z

**Summary**: Persistent Memory (PM) introduces new opportunities for designing crash-consistent applications without the traditional storage overheads. However, ensuring crash consistency in PM demands intricate knowledge of CPU, cache, and memory interactions. Hardware and software mechanisms have been proposed to ease this burden, but neither proved sufficient, prompting a variety of bug detection tools.   With the sunset of Intel Optane comes the rise of Compute Express Link (CXL) for PM. In this position paper, we discuss the impact of CXL's disaggregated and heterogeneous nature in the development of crash-consistent PM applications, and outline three research directions: hardware primitives, persistency frameworks, and bug detection tools.

**Link**: [arxiv](http://arxiv.org/abs/2504.17554v1),  [pdf](http://arxiv.org/pdf/2504.17554v1)

**Tags**: cs.ET 



### Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent   Interconnects
**Authors**: Anastasiia Ruzhanskaia, Pengcheng Xu, David Cock, Timothy Roscoe

**Updated**: 2025-04-24T08:39:13Z

**Summary**: Conventional wisdom holds that an efficient interface between an OS running on a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA) to offload data transfer, descriptor rings for buffering and queuing, and interrupts for asynchrony between cores and device.   In this paper we question this wisdom in the light of two trends: modern and emerging cache-coherent interconnects like CXL3.0, and workloads, particularly microservices and serverless computing. Like some others before us, we argue that the assumptions of the DMA-based model are obsolete, and in many use-cases programmed I/O, where the CPU explicitly transfers data and control information to and from a device via loads and stores, delivers a more efficient system.   However, we push this idea much further. We show, in a real hardware implementation, the gains in latency for fine-grained communication achievable using an open cache-coherence protocol which exposes cache transitions to a smart device, and that throughput is competitive with DMA over modern interconnects. We also demonstrate three use-cases: fine-grained RPC-style invocation of functions on an accelerator, offloading of operators in a streaming dataflow engine, and a network interface targeting serverless functions, comparing our use of coherence with both traditional DMA-style interaction and a highly-optimized implementation using memory-mapped programmed I/O over PCIe.

**Link**: [arxiv](http://arxiv.org/abs/2409.08141v3),  [pdf](http://arxiv.org/pdf/2409.08141v3)

**Tags**: cs.AR cs.OS 



### SPAARC: Spatial Proximity and Association based prefetching for   Augmented Reality in edge Cache
**Authors**: Nikhil Sreekumar, Abhishek Chandra, Jon Weissman

**Updated**: 2025-04-24T04:36:20Z

**Summary**: Mobile Augmented Reality (MAR) applications face performance challenges due to their high computational demands and need for low-latency responses. Traditional approaches like on-device storage or reactive data fetching from the cloud often result in limited AR experiences or unacceptable lag. Edge caching, which caches AR objects closer to the user, provides a promising solution. However, existing edge caching approaches do not consider AR-specific features such as AR object sizes, user interactions, and physical location. This paper investigates how to further optimize edge caching by employing AR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and Association-based Prefetching policy specifically designed for MAR Caches. SPAARC intelligently prioritizes the caching of virtual objects based on their association with other similar objects and the user's proximity to them. It also considers the recency of associations and uses a lazy fetching strategy to efficiently manage edge resources and maximize Quality of Experience (QoE).   Through extensive evaluation using both synthetic and real-world workloads, we demonstrate that SPAARC significantly improves cache hit rates compared to standard caching algorithms, achieving gains ranging from 3% to 40% while reducing the need for on-demand data retrieval from the cloud. Further, we present an adaptive tuning algorithm that automatically tunes SPAARC parameters to achieve optimal performance. Our findings demonstrate the potential of SPAARC to substantially enhance the user experience in MAR applications by ensuring the timely availability of virtual objects.

**Link**: [arxiv](http://arxiv.org/abs/2502.15192v2),  [pdf](http://arxiv.org/pdf/2502.15192v2)

**Tags**: cs.ET cs.DC 



### Efficient Pretraining Length Scaling
**Authors**: Bohong Wu, Shen Yan, Sijun Zhang, Jianqiao Lu, Yutao Zeng, Ya Wang, Xun Zhou

**Updated**: 2025-04-24T04:13:49Z

**Summary**: Recent advances in large language models have demonstrated the effectiveness of length scaling during post-training, yet its potential in pre-training remains underexplored. We present the Parallel Hidden Decoding Transformer (\textit{PHD}-Transformer), a novel framework that enables efficient length scaling during pre-training while maintaining inference efficiency. \textit{PHD}-Transformer achieves this through an innovative KV cache management strategy that distinguishes between original tokens and hidden decoding tokens. By retaining only the KV cache of original tokens for long-range dependencies while immediately discarding hidden decoding tokens after use, our approach maintains the same KV cache size as the vanilla transformer while enabling effective length scaling. To further enhance performance, we introduce two optimized variants: \textit{PHD-SWA} employs sliding window attention to preserve local dependencies, while \textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate linear growth in pre-filling time. Extensive experiments demonstrate consistent improvements across multiple benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2504.14992v2),  [pdf](http://arxiv.org/pdf/2504.14992v2)

**Tags**: cs.CL 



### Cognitive Memory in Large Language Models
**Authors**: Lianlei Shan, Shixian Luo, Zezhou Zhu, Yu Yuan, Yong Wu

**Updated**: 2025-04-24T01:47:25Z

**Summary**: This paper examines memory mechanisms in Large Language Models (LLMs), emphasizing their importance for context-rich responses, reduced hallucinations, and improved efficiency. It categorizes memory into sensory, short-term, and long-term, with sensory memory corresponding to input prompts, short-term memory processing immediate context, and long-term memory implemented via external databases or structures. The text-based memory section covers acquisition (selection and summarization), management (updating, accessing, storing, and resolving conflicts), and utilization (full-text search, SQL queries, semantic search). The KV cache-based memory section discusses selection methods (regularity-based summarization, score-based approaches, special token embeddings) and compression techniques (low-rank compression, KV merging, multimodal compression), along with management strategies like offloading and shared attention mechanisms. Parameter-based memory methods (LoRA, TTT, MoE) transform memories into model parameters to enhance efficiency, while hidden-state-based memory approaches (chunk mechanisms, recurrent transformers, Mamba model) improve long-text processing by combining RNN hidden states with current methods. Overall, the paper offers a comprehensive analysis of LLM memory mechanisms, highlighting their significance and future research directions.

**Link**: [arxiv](http://arxiv.org/abs/2504.02441v2),  [pdf](http://arxiv.org/pdf/2504.02441v2)

**Tags**: cs.CL cs.AI 



### KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM   Inference in Resource-Constrained Environments
**Authors**: Junyoung Park, Dalton Jones, Matt J Morse, Raghavv Goel, Mingu Lee, Chris Lott

**Updated**: 2025-04-23T18:02:55Z

**Summary**: In this work, we demonstrate that distinctive keys during LLM inference tend to have high attention scores. We explore this phenomenon and propose KeyDiff, a training-free KV cache eviction method based on key similarity. This method facilitates the deployment of LLM-based application requiring long input prompts in resource-constrained environments with limited memory and compute budgets. Unlike other KV cache eviction methods, KeyDiff can process arbitrarily long prompts within strict resource constraints and efficiently generate responses. We demonstrate that KeyDiff computes the optimal solution to a KV cache selection problem that maximizes key diversity, providing a theoretical understanding of KeyDiff. Notably,KeyDiff does not rely on attention scores, allowing the use of optimized attention mechanisms like FlashAttention. We demonstrate the effectiveness of KeyDiff across diverse tasks and models, illustrating a performance gap of less than 0.04\% with 8K cache budget ($\sim$ 23\% KV cache reduction) from the non-evicting baseline on the LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B.

**Link**: [arxiv](http://arxiv.org/abs/2504.15364v2),  [pdf](http://arxiv.org/pdf/2504.15364v2)

**Tags**: cs.AI 



### Iris: A Next Generation Digital Pathology Rendering Engine
**Authors**: Ryan Erik Landvater, Ulysses Balis

**Updated**: 2025-04-23T15:02:16Z

**Summary**: Digital pathology is a tool of rapidly evolving importance within the discipline of pathology. Whole slide imaging promises numerous advantages; however, adoption is limited by challenges in ease of use and speed of high-quality image rendering relative to the simplicity and visual quality of glass slides. We introduce Iris, a new high-performance digital pathology rendering system. Specifically, we outline and detail the performance metrics of Iris Core, the core rendering engine technology. Iris Core comprises machine code modules written from the ground up in C++ and using Vulkan, a low-level and low-overhead cross-platform graphical processing unit application program interface, and our novel rapid tile buffering algorithms. We provide a detailed explanation of Iris Core's system architecture, including the stateless isolation of core processes, interprocess communication paradigms, and explicit synchronization paradigms that provide powerful control over the graphical processing unit. Iris Core achieves slide rendering at the sustained maximum frame rate on all tested platforms and buffers an entire new slide field of, view without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is able to buffer and compute high-fidelity reduction-enhancements for viewing low-power cytology with increased visual quality at a rate of 100-160 us per slide tile, and with a cumulative median buffering rate of 1.36 GB of decompressed image data per second. This buffering rate allows for an entirely new field of view to be fully buffered and rendered in less than a single monitor refresh on a standard display, and high detail features within 2-3 monitor refresh frames. These metrics far exceed previously published specifications, beyond an order of magnitude in some contexts. The system shows no slowing with high use loads, but rather increases performance due to cache mechanisms.

**Link**: [arxiv](http://arxiv.org/abs/2504.15437v2),  [pdf](http://arxiv.org/pdf/2504.15437v2)

**Tags**: cs.GR 



### The NIC should be part of the OS
**Authors**: Pengcheng Xu, Timothy Roscoe

**Updated**: 2025-04-23T10:48:52Z

**Summary**: The network interface adapter (NIC) is a critical component of a cloud server occupying a unique position. Not only is network performance vital to efficient operation of the machine, but unlike compute accelerators like GPUs, the network subsystem must react to unpredictable events like the arrival of a network packet and communicate with the appropriate application end point with minimal latency.   Current approaches to server stacks navigate a trade-off between flexibility, efficiency, and performance: the fastest kernel-bypass approaches dedicate cores to applications, busy-wait on receive queues, etc. while more flexible approaches appropriate to more dynamic workload mixes incur much greater software overhead on the data path.   However, we reject this trade-off, which we ascribe to an arbitrary (and sub-optimal) split in system state between the OS and the NIC. Instead, by exploiting the properties of cache-coherent interconnects and integrating the NIC closely with the OS kernel, we can achieve something surprising: performance for RPC workloads better than the fastest kernelbypass approaches without sacrificing the robustness and dynamic adaptation of kernel-based network subsystems.

**Link**: [arxiv](http://arxiv.org/abs/2501.10138v2),  [pdf](http://arxiv.org/pdf/2501.10138v2)

**Tags**: cs.OS cs.AR cs.NI 



### CAOTE: KV Caching through Attention Output Error based Token Eviction
**Authors**: Raghavv Goel, Junyoung Park, Mukul Gagrani, Dalton Jones, Matthew Morse, Harper Langston, Mingu Lee, Chris Lott

**Updated**: 2025-04-23T05:04:58Z

**Summary**: While long context support of large language models has extended their abilities, it also incurs challenges in memory and compute which becomes crucial bottlenecks in resource-restricted devices. Token eviction, a widely adopted post-training methodology designed to alleviate the bottlenecks by evicting less important tokens from the cache, typically uses attention scores as proxy metrics for token importance. However, one major limitation of attention score as a token-wise importance metrics is that it lacks the information about contribution of tokens to the attention output. In this paper, we propose a simple eviction criterion based on the contribution of cached tokens to attention outputs. Our method, CAOTE, optimizes for eviction error due to token eviction, by seamlessly integrating attention scores and value vectors. This is the first method which uses value vector information on top of attention-based eviction scores. Additionally, CAOTE can act as a meta-heuristic method with flexible usage with any token eviction method. We show that CAOTE, when combined with the state-of-the-art attention score-based methods, always improves accuracies on the downstream task, indicating the importance of leveraging information from values during token eviction process.

**Link**: [arxiv](http://arxiv.org/abs/2504.14051v2),  [pdf](http://arxiv.org/pdf/2504.14051v2)

**Tags**: cs.LG 



### ML-based Adaptive Prefetching and Data Placement for US HEP Systems
**Authors**: Venkat Sai Suman Lamba Karanam, Sarat Sasank Barla, Byrav Ramamurthy, Derek Weitzel

**Updated**: 2025-04-23T04:21:49Z

**Summary**: Although benefits from caching in US HEP are well-known, current caching strategies are not adaptive i.e they do not adapt to changing cache access patterns. Newer developments such as the High-Luminosity - Large Hadron Collider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move toward streaming readout based Data Acquisition systems (DAQs) will increase the data production exponentially and hence burden the storage, compute & network infrastructures. Moreover, existing caching frameworks are optimized to reduce latency, but not optimized for storage. This, in combination with limited cache capacities relative to total data, makes it difficult to achieve data locality.   In this work, we present Machine Learning-aided (ML) caching strategies. Specifically, we first present a Long Short-Term Memory-based (LSTM) hourly and multi-step cache usage prediction. Second, we present an hourly file-level access prediction model based on CatboostRegressor. To date, most ML-based cache prediction strategies in HEP have focused on daily cache usage and limited works tackled hourly cache usage and even fewer strategies addressed hourly file-level access prediction. File-level access prediction allows for the design of intelligent prefetching and data placement strategies with fine-grained control. We validated our cache prediction strategies using data collected from SoCal MINI caches in August 2024. We are currently extending the WRENCH simulator to reflect the US HEP ecosystem at the storage, network and compute levels. We plan to deploy our cache prediction strategies into WRENCH and later perform extensive analysis with complex data access patterns and candidate infrastructure configurations.

**Link**: [arxiv](http://arxiv.org/abs/2503.06015v2),  [pdf](http://arxiv.org/pdf/2503.06015v2)

**Tags**: cs.DC 



### The Dawn of Disaggregation and the Coherence Conundrum: A Call for   Federated Coherence
**Authors**: Jaewan Hong, Marcos K. Aguilera, Emmanuel Amaro, Vincent Liu, Aurojit Panda, Ion Stoica

**Updated**: 2025-04-22T23:52:13Z

**Summary**: Disaggregated memory is an upcoming data center technology that will allow nodes (servers) to share data efficiently. Sharing data creates a debate on the level of cache coherence the system should provide. While current proposals aim to provide coherence for all or parts of the disaggregated memory, we argue that this approach is problematic, because of scalability limitations and hardware complexity. Instead, we propose and formally define federated coherence, a model that provides coherence only within nodes, not across nodes. Federated coherence can use current intra-node coherence provided by processors without requiring expensive mechanisms for inter-node coherence. Developers can use federated coherence with a few simple programming paradigms and a synchronization library. We sketch some potential applications.

**Link**: [arxiv](http://arxiv.org/abs/2504.16324v1),  [pdf](http://arxiv.org/pdf/2504.16324v1)

**Tags**: cs.DC cs.AR 



### Key, Value, Compress: A Systematic Exploration of KV Cache Compression   Techniques
**Authors**: Neusha Javidnia, Bita Darvish Rouhani, Farinaz Koushanfar

**Updated**: 2025-04-22T17:34:34Z

**Summary**: Large language models (LLMs) have demonstrated exceptional capabilities in generating text, images, and video content. However, as context length grows, the computational cost of attention increases quadratically with the number of tokens, presenting significant efficiency challenges. This paper presents an analysis of various Key-Value (KV) cache compression strategies, offering a comprehensive taxonomy that categorizes these methods by their underlying principles and implementation techniques. Furthermore, we evaluate their impact on performance and inference latency, providing critical insights into their effectiveness. Our findings highlight the trade-offs involved in KV cache compression and its influence on handling long-context scenarios, paving the way for more efficient LLM implementations.

**Link**: [arxiv](http://arxiv.org/abs/2503.11816v3),  [pdf](http://arxiv.org/pdf/2503.11816v3)

**Tags**: cs.CL 



### GainSight: Application-Guided Profiling for Composing Heterogeneous   On-Chip Memories in AI Hardware Accelerators
**Authors**: Peijing Li, Matthew Hung, Yiming Tan, Konstantin Hoßfeld, Jake Cheng Jiajun, Shuhan Liu, Lixian Yan, Xinxin Wang, H. -S. Philip Wong, Thierry Tambe

**Updated**: 2025-04-22T17:23:28Z

**Summary**: As AI workloads drive soaring memory requirements, there is a need for higher-density on-chip memory for domain-specific accelerators that goes beyond what current SRAM technology can provide. We motivate that algorithms and application behavior should guide the composition of heterogeneous on-chip memories. However, there has been little work in factoring dynamic application profiles into such design decisions. We present GainSight, a profiling framework that analyzes fine-grained memory access patterns and computes data lifetimes in domain-specific accelerators. By combining instrumentation and simulation across retargetable hardware backends, GainSight aligns heterogeneous memory designs with workload-specific traffic and lifetime metrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA H100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic array scratchpad accesses across profiled workloads are short-lived and suitable for silicon-based gain cell RAM (Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3) Up to 90% of GPU cache fetches are never reused, highlighting inefficiencies in terms of cache pollution. These insights that GainSight provides can be used to better understand the design spaces of both emerging on-chip memories and software algorithmic optimizations for the next generation of AI accelerators.

**Link**: [arxiv](http://arxiv.org/abs/2504.14866v2),  [pdf](http://arxiv.org/pdf/2504.14866v2)

**Tags**: cs.AR cs.ET B.7.1; B.3.1; C.3; I.6; I.2.6 



### Optimizing SLO-oriented LLM Serving with PD-Multiplexing
**Authors**: Weihao Cui, Yukang Chen, Han Zhao, Ziyi Xu, Quan Chen, Xusheng Chen, Yangjie Zhou, Shixuan Sun, Minyi Guo

**Updated**: 2025-04-22T15:19:48Z

**Summary**: Modern LLM services demand high throughput and stringent SLO guarantees across two distinct inference phases-prefill and decode-and complex multi-turn workflows. However, current systems face a fundamental tradeoff: out-of-place compute partition enables per-phase SLO attainment, while in-place memory sharing maximizes throughput via KV cache reuse. Moreover, existing in-place compute partition also encounters low utilization and high overhead due to phase-coupling design. We present Drift, a new LLM serving framework that resolves this tension via PD multiplexing, enabling in-place and phase-decoupled compute partition. Drift leverages low-level GPU partitioning techniques to multiplex prefill and decode phases spatially and adaptively on shared GPUs, while preserving in-place memory sharing. To fully leverage the multiplexing capability, Drift introduces an adaptive gang scheduling mechanism, a contention-free modeling method, and a SLO-aware dispatching policy. Evaluation shows that Drift achieves an average $5.1\times$ throughput improvement (up to $17.5\times$) over state-of-the-art baselines, while consistently meeting SLO targets under complex LLM workloads.

**Link**: [arxiv](http://arxiv.org/abs/2504.14489v2),  [pdf](http://arxiv.org/pdf/2504.14489v2)

**Tags**: cs.OS 



### SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large   Language Model Inference
**Authors**: Yihao Zhao, Jiadun Chen, Peng Sun, Lei Li, Xuanzhe Liu, Xin Jin

**Updated**: 2025-04-22T09:08:46Z

**Summary**: Large language models (LLMs) with different architectures and sizes have been developed. Serving each LLM with dedicated GPUs leads to resource waste and service inefficiency due to the varying demand of LLM requests. A common practice is to share multiple LLMs. However, existing sharing systems either do not consider the autoregressive pattern of LLM services, or only focus on improving the throughput, which impairs the sharing performance, especially the serving latency. We present SeaLLM, which enables service-aware and latency-optimized LLM sharing. SeaLLM improves the overall sharing performance by (1) a latency-optimized scheduling algorithm utilizing the characteristics of LLM services, (2) a placement algorithm to determine the placement plan and an adaptive replacement algorithm to decide the replacement interval, and (3) a unified key-value cache to share GPU memory among LLM services efficiently. Our evaluation under real-world traces and LLM services demonstrates that SeaLLM improves the normalized latency by up to $13.60\times$, the tail latency by up to $18.69\times$, and the SLO attainment by up to $3.64\times$ compared to existing solutions.

**Link**: [arxiv](http://arxiv.org/abs/2504.15720v1),  [pdf](http://arxiv.org/pdf/2504.15720v1)

**Tags**: cs.DC 



### Reimagining Memory Access for LLM Inference: Compression-Aware Memory   Controller Design
**Authors**: Rui Xie, Asad Ul Haq, Linsen Ma, Yunhua Fang, Zirak Burzin Engineer, Liu Liu, Tong Zhang

**Updated**: 2025-04-21T22:13:07Z

**Summary**: The efficiency of Large Language Model~(LLM) inference is often constrained by substantial memory bandwidth and capacity demands. Existing techniques, such as pruning, quantization, and mixture of experts/depth, reduce memory capacity and/or bandwidth consumption at the cost of slight degradation in inference quality. This paper introduces a design solution that further alleviates memory bottlenecks by enhancing the on-chip memory controller in AI accelerators to achieve two main objectives: (1) significantly reducing memory capacity and bandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of model weights and key-value (KV) cache without compromising inference quality, and (2) enabling memory bandwidth and energy consumption to scale proportionally with context-dependent dynamic quantization. These goals are accomplished by equipping the on-chip memory controller with mechanisms to improve fine-grained bit-level accessibility and compressibility of weights and KV cache through LLM-aware configuration of in-memory placement and representation. Experimental results on publicly available LLMs demonstrate the effectiveness of this approach, showing memory footprint reductions of 25.2\% for model weights and 46.9\% for KV cache. In addition, our hardware prototype at 4\,GHz and 32 lanes (7\,nm) achieves 8\,TB/s throughput with a modest area overhead (under 3.8\,mm\(^2\)), which underscores the viability of LLM-aware memory control as a key to efficient large-scale inference.

**Link**: [arxiv](http://arxiv.org/abs/2503.18869v3),  [pdf](http://arxiv.org/pdf/2503.18869v3)

**Tags**: cs.AR 



### FlashInfer: Efficient and Customizable Attention Engine for LLM   Inference Serving
**Authors**: Zihao Ye, Lequn Chen, Ruihang Lai, Wuwei Lin, Yineng Zhang, Stephanie Wang, Tianqi Chen, Baris Kasikci, Vinod Grover, Arvind Krishnamurthy, Luis Ceze

**Updated**: 2025-04-21T20:10:11Z

**Summary**: Transformers, driven by attention mechanisms, form the foundation of large language models (LLMs). As these models scale up, efficient GPU attention kernels become essential for high-throughput and low-latency inference. Diverse LLM applications demand flexible and high-performance attention solutions. We present FlashInfer: a customizable and efficient attention engine for LLM serving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse format and composable formats to optimize memory access and reduce redundancy. It also offers a customizable attention template, enabling adaptation to various settings through Just-In-Time (JIT) compilation. Additionally, FlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user requests while maintaining compatibility with CUDAGraph which requires static configuration. FlashInfer have been integrated into leading LLM serving frameworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and end-to-end evaluations demonstrate FlashInfer's ability to significantly boost kernel performance across diverse inference scenarios: compared to state-of-the-art LLM serving solutions, FlashInfer achieve 29-69% inter-token-latency reduction compared to compiler backends for LLM serving benchmark, 28-30% latency reduction for long-context inference, and 13-17% speedup for LLM serving with parallel generation.

**Link**: [arxiv](http://arxiv.org/abs/2501.01005v2),  [pdf](http://arxiv.org/pdf/2501.01005v2)

**Tags**: cs.DC cs.AI cs.LG 



### Joint Knowledge and Power Management for Secure Semantic Communication   Networks
**Authors**: Xuesong Liu, Yansong Liu, Haoyu Tang, Fangzhou Zhao, Le Xia, Yao Sun

**Updated**: 2025-04-21T17:39:59Z

**Summary**: Recently, semantic communication (SemCom) has shown its great superiorities in resource savings and information exchanges. However, while its unique background knowledge guarantees accurate semantic reasoning and recovery, semantic information security-related concerns are introduced at the same time. Since the potential eavesdroppers may have the same background knowledge to accurately decrypt the private semantic information transmitted between legal SemCom users, this makes the knowledge management in SemCom networks rather challenging in joint consideration with the power control. To this end, this paper focuses on jointly addressing three core issues of power allocation, knowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in secure SemCom networks. We first develop a novel performance metric, namely semantic secrecy throughput (SST), to quantify the information security level that can be achieved at each pair of D2D SemCom users. Next, an SST maximization problem is formulated subject to secure SemCom-related delay and reliability constraints. Afterward, we propose a security-aware resource management solution using the Lagrange primal-dual method and a two-stage method. Simulation results demonstrate our proposed solution nearly doubles the SST performance and realizes less than half of the queuing delay performance compared to different benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2504.15260v1),  [pdf](http://arxiv.org/pdf/2504.15260v1)

**Tags**: eess.SP 



## Keyword: LLM Inference 
 ### DanceGRPO: Unleashing GRPO on Visual Generation
**Authors**: Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, Ping Luo

**Updated**: 2025-05-12T17:59:34Z

**Summary**: Recent breakthroughs in generative models-particularly diffusion models and rectified flows-have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. Existing reinforcement learning (RL)-based methods for visual generation face critical limitations: incompatibility with modern Ordinary Differential Equations (ODEs)-based sampling paradigms, instability in large-scale training, and lack of validation for video generation. This paper introduces DanceGRPO, the first unified framework to adapt Group Relative Policy Optimization (GRPO) to visual generation paradigms, unleashing one unified RL algorithm across two generative paradigms (diffusion models and rectified flows), three tasks (text-to-image, text-to-video, image-to-video), four foundation models (Stable Diffusion, HunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video aesthetics, text-image alignment, video motion quality, and binary reward). To our knowledge, DanceGRPO is the first RL-based unified framework capable of seamless adaptation across diverse generative paradigms, tasks, foundational models, and reward models. DanceGRPO demonstrates consistent and substantial improvements, which outperform baselines by up to 181% on benchmarks such as HPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can stabilize policy optimization for complex video generation, but also enables generative policy to better capture denoising trajectories for Best-of-N inference scaling and learn from sparse binary feedback. Our results establish DanceGRPO as a robust and versatile solution for scaling Reinforcement Learning from Human Feedback (RLHF) tasks in visual generation, offering new insights into harmonizing reinforcement learning and visual synthesis. The code will be released.

**Link**: [arxiv](http://arxiv.org/abs/2505.07818v1),  [pdf](http://arxiv.org/pdf/2505.07818v1)

**Tags**: cs.CV 



### WATCH: Adaptive Monitoring for AI Deployments via Weighted-Conformal   Martingales
**Authors**: Drew Prinster, Xing Han, Anqi Liu, Suchi Saria

**Updated**: 2025-05-12T17:56:52Z

**Summary**: Responsibly deploying artificial intelligence (AI) / machine learning (ML) systems in high-stakes settings arguably requires not only proof of system reliability, but moreover continual, post-deployment monitoring to quickly detect and address any unsafe behavior. Statistical methods for nonparametric change-point detection -- especially the tools of conformal test martingales (CTMs) and anytime-valid inference -- offer promising approaches to this monitoring task. However, existing methods are restricted to monitoring limited hypothesis classes or ``alarm criteria'' (such as data shifts that violate certain exchangeability assumptions), do not allow for online adaptation in response to shifts, and/or do not enable root-cause analysis of any degradation. In this paper, we expand the scope of these monitoring methods by proposing a weighted generalization of conformal test martingales (WCTMs), which lay a theoretical foundation for online monitoring for any unexpected changepoints in the data distribution while controlling false-alarms. For practical applications, we propose specific WCTM algorithms that adapt online to mild covariate shifts (in the marginal input distribution) while quickly detecting and diagnosing more severe shifts, such as concept shifts (in the conditional label distribution) or extreme (out-of-support) covariate shifts that cannot be easily adapted to. On real-world datasets, we demonstrate improved performance relative to state-of-the-art baselines.

**Link**: [arxiv](http://arxiv.org/abs/2505.04608v2),  [pdf](http://arxiv.org/pdf/2505.04608v2)

**Tags**: cs.LG cs.AI stat.ML 



### GP-GS: Gaussian Processes for Enhanced Gaussian Splatting
**Authors**: Zhihao Guo, Jingxuan Su, Shenglin Wang, Jinlong Fan, Jing Zhang, Wei Zhou, Hadi Amirpour, Yunlong Zhao, Liangxiu Han, Peng Wang

**Updated**: 2025-05-12T17:53:38Z

**Summary**: 3D Gaussian Splatting has emerged as an efficient photorealistic novel view synthesis method. However, its reliance on sparse Structure-from-Motion (SfM) point clouds often limits scene reconstruction quality. To address the limitation, this paper proposes a novel 3D reconstruction framework, Gaussian Processes enhanced Gaussian Splatting (GP-GS), in which a multi-output Gaussian Process model is developed to enable adaptive and uncertainty-guided densification of sparse SfM point clouds. Specifically, we propose a dynamic sampling and filtering pipeline that adaptively expands the SfM point clouds by leveraging GP-based predictions to infer new candidate points from the input 2D pixels and depth maps. The pipeline utilizes uncertainty estimates to guide the pruning of high-variance predictions, ensuring geometric consistency and enabling the generation of dense point clouds. These densified point clouds provide high-quality initial 3D Gaussians, enhancing reconstruction performance. Extensive experiments conducted on synthetic and real-world datasets across various scales validate the effectiveness and practicality of the proposed framework.

**Link**: [arxiv](http://arxiv.org/abs/2502.02283v4),  [pdf](http://arxiv.org/pdf/2502.02283v4)

**Tags**: cs.CV cs.AI 68T45 



### Understanding Stragglers in Large Model Training Using What-if Analysis
**Authors**: Jinkun Lin, Ziheng Jiang, Zuquan Song, Sida Zhao, Menghan Yu, Zhanghan Wang, Chenyuan Wang, Zuocheng Shi, Xiang Shi, Wei Jia, Zherui Liu, Shuguang Wang, Haibin Lin, Xin Liu, Aurojit Panda, Jinyang Li

**Updated**: 2025-05-12T17:52:35Z

**Summary**: Large language model (LLM) training is one of the most demanding distributed computations today, often requiring thousands of GPUs with frequent synchronization across machines. Such a workload pattern makes it susceptible to stragglers, where the training can be stalled by few slow workers. At ByteDance we find stragglers are not trivially always caused by hardware failures, but can arise from multiple complex factors. This work aims to present a comprehensive study on the straggler issues in LLM training, using a five-month trace collected from our ByteDance LLM training cluster. The core methodology is what-if analysis that simulates the scenario without any stragglers and contrasts with the actual case. We use this method to study the following questions: (1) how often do stragglers affect training jobs, and what effect do they have on job performance; (2) do stragglers exhibit temporal or spatial patterns; and (3) what are the potential root causes for stragglers?

**Link**: [arxiv](http://arxiv.org/abs/2505.05713v2),  [pdf](http://arxiv.org/pdf/2505.05713v2)

**Tags**: cs.DC cs.LG 



### Improving Trajectory Stitching with Flow Models
**Authors**: Reece O'Mahoney, Wanming Yu, Ioannis Havoutis

**Updated**: 2025-05-12T17:50:10Z

**Summary**: Generative models have shown great promise as trajectory planners, given their affinity to modeling complex distributions and guidable inference process. Previous works have successfully applied these in the context of robotic manipulation but perform poorly when the required solution does not exist as a complete trajectory within the training set. We identify that this is a result of being unable to plan via stitching, and subsequently address the architectural and dataset choices needed to remedy this. On top of this, we propose a novel addition to the training and inference procedures to both stabilize and enhance these capabilities. We demonstrate the efficacy of our approach by generating plans with out of distribution boundary conditions and performing obstacle avoidance on the Franka Panda in simulation and on real hardware. In both of these tasks our method performs significantly better than the baselines and is able to avoid obstacles up to four times as large.

**Link**: [arxiv](http://arxiv.org/abs/2505.07802v1),  [pdf](http://arxiv.org/pdf/2505.07802v1)

**Tags**: cs.RO cs.AI cs.LG 



### Overflow Prevention Enhances Long-Context Recurrent LLMs
**Authors**: Assaf Ben-Kish, Itamar Zimerman, M. Jehanzeb Mirza, James Glass, Leonid Karlinsky, Raja Giryes

**Updated**: 2025-05-12T17:45:05Z

**Summary**: A recent trend in LLMs is developing recurrent sub-quadratic models that improve long-context processing efficiency. We investigate leading large long-context models, focusing on how their fixed-size recurrent memory affects their performance. Our experiments reveal that, even when these models are trained for extended contexts, their use of long contexts remains underutilized. Specifically, we demonstrate that a chunk-based inference procedure, which identifies and processes only the most relevant portion of the input can mitigate recurrent memory failures and be effective for many long-context tasks: On LongBench, our method improves the overall performance of Falcon3-Mamba-Inst-7B by 14%, Falcon-Mamba-Inst-7B by 28%, RecurrentGemma-IT-9B by 50%, and RWKV6-Finch-7B by 51%. Surprisingly, this simple approach also leads to state-of-the-art results in the challenging LongBench v2 benchmark, showing competitive performance with equivalent size Transformers. Furthermore, our findings raise questions about whether recurrent models genuinely exploit long-range dependencies, as our single-chunk strategy delivers stronger performance - even in tasks that presumably require cross-context relations.

**Link**: [arxiv](http://arxiv.org/abs/2505.07793v1),  [pdf](http://arxiv.org/pdf/2505.07793v1)

**Tags**: cs.LG cs.AI 



### Learning from Peers in Reasoning Models
**Authors**: Tongxu Luo, Wenyu Du, Jiaxi Bi, Stephen Chung, Zhengyang Tang, Hao Yang, Min Zhang, Benyou Wang

**Updated**: 2025-05-12T17:39:56Z

**Summary**: Large Reasoning Models (LRMs) have the ability to self-correct even when they make mistakes in their reasoning paths. However, our study reveals that when the reasoning process starts with a short but poor beginning, it becomes difficult for the model to recover. We refer to this phenomenon as the "Prefix Dominance Trap". Inspired by psychological findings that peer interaction can promote self-correction without negatively impacting already accurate individuals, we propose **Learning from Peers** (LeaP) to address this phenomenon. Specifically, every tokens, each reasoning path summarizes its intermediate reasoning and shares it with others through a routing mechanism, enabling paths to incorporate peer insights during inference. However, we observe that smaller models sometimes fail to follow summarization and reflection instructions effectively. To address this, we fine-tune them into our **LeaP-T** model series. Experiments on AIME 2024, AIME 2025, AIMO 2025, and GPQA Diamond show that LeaP provides substantial improvements. For instance, QwQ-32B with LeaP achieves nearly 5 absolute points higher than the baseline on average, and surpasses DeepSeek-R1-671B on three math benchmarks with an average gain of 3.3 points. Notably, our fine-tuned LeaP-T-7B matches the performance of DeepSeek-R1-Distill-Qwen-14B on AIME 2024. In-depth analysis reveals LeaP's robust error correction by timely peer insights, showing strong error tolerance and handling varied task difficulty. LeaP marks a milestone by enabling LRMs to collaborate during reasoning. Our code, datasets, and models are available at https://learning-from-peers.github.io/ .

**Link**: [arxiv](http://arxiv.org/abs/2505.07787v1),  [pdf](http://arxiv.org/pdf/2505.07787v1)

**Tags**: cs.CL 



### Conditions for accretion favoring an unmelted Callisto and a   differentiated Ganymede
**Authors**: Yannis Bennacer, Olivier Mousis, Marc Monnereau, Vincent Hue, Antoine Schneeberger

**Updated**: 2025-05-12T17:38:09Z

**Summary**: Analysis of Callisto's moments of inertia, derived from Galileo's gravity data, suggests that its structure is not fully differentiated. This possibly undifferentiated state contrasts sharply with the globally molten state inferred in its counterpart, Ganymede, and poses unique challenges to theories of the formation and evolution of the Galilean moons. During their formation, both moons experienced multiple heating mechanisms, including tidal heating, radiogenic heating from short-lived radionuclides, accretional heating from impacts, and heat from the surrounding circumplanetary disk. Our study investigates the optimal conditions required to account for Callisto's partially differentiated state in contrast to Ganymede's complete differentiation. We investigate crucial accretion parameters, such as the timing of accretion onset, the duration of accretion, and the impactor size distribution. We find that the observed dichotomy between Ganymede and Callisto can be attributed to similar formation conditions, assuming an identical impactor size distribution and composition in the Jovian circumplanetary disk. The key differences in the formation of Ganymede and Callisto are the disk temperature at their respective formation locations and their final radii. Our results indicate that both moons accreted gradually over more than 2 Myr, concluding at least 5.5 Myr after the formation of calcium-aluminum-rich inclusions in the protosolar nebula. Our model demonstrates that Callisto can remain undifferentiated despite accreting a substantial influx of kilometer-sized impactors, potentially contributing up to 30% of the total mass inflow, while still allowing for the complete differentiation of Ganymede.

**Link**: [arxiv](http://arxiv.org/abs/2505.07785v1),  [pdf](http://arxiv.org/pdf/2505.07785v1)

**Tags**: astro-ph.EP 



### Domain Regeneration: How well do LLMs match syntactic properties of text   domains?
**Authors**: Da Ju, Hagen Blix, Adina Williams

**Updated**: 2025-05-12T17:37:17Z

**Summary**: Recent improvement in large language model performance have, in all likelihood, been accompanied by improvement in how well they can approximate the distribution of their training data. In this work, we explore the following question: which properties of text domains do LLMs faithfully approximate, and how well do they do so? Applying observational approaches familiar from corpus linguistics, we prompt a commonly used, opensource LLM to regenerate text from two domains of permissively licensed English text which are often contained in LLM training data -- Wikipedia and news text. This regeneration paradigm allows us to investigate whether LLMs can faithfully match the original human text domains in a fairly semantically-controlled setting. We investigate varying levels of syntactic abstraction, from more simple properties like sentence length, and article readability, to more complex and higher order properties such as dependency tag distribution, parse depth, and parse complexity. We find that the majority of the regenerated distributions show a shifted mean, a lower standard deviation, and a reduction of the long tail, as compared to the human originals.

**Link**: [arxiv](http://arxiv.org/abs/2505.07784v1),  [pdf](http://arxiv.org/pdf/2505.07784v1)

**Tags**: cs.CL 



### Relative Overfitting and Accept-Reject Framework
**Authors**: Yanxin Liu, Yunqi Zhang

**Updated**: 2025-05-12T17:36:14Z

**Summary**: Currently, the scaling law of Large Language Models (LLMs) faces challenges and bottlenecks. This paper posits that noise effects, stemming from changes in the signal-to-noise ratio under diminishing marginal returns, are the root cause of these issues. To control this noise, we investigated the differences between models with performance advantages and disadvantages, introducing the concept of "relative overfitting." Based on their complementary strengths, we have proposed an application framework, Accept-Reject (AR). In Natural Language Processing (NLP), we use LLMs and Small Language Models (SLMs) as the medium for discussion. This framework enables SLMs to exert a universal positive influence on LLM decision outputs, rather than the intuitively expected negative influence. We validated our approach using self-built models based on mainstream architectures and pre-trained mainstream models across multiple datasets, including basic language modeling, long-context tasks, subject examination, and question-answering (QA) benchmarks. The results demonstrate that through our structure, compared to increasing the LLM's parameters, we can achieve better performance improvements with significantly lower parameter and computational costs in many scenarios. These improvements are universal, stable, and effective. Furthermore, we explore the potential of "relative overfitting" and the AR framework in other machine learning domains, such as computer vision (CV) and AI for science. We hope the proposed approach can help scale laws overcome existing bottlenecks.

**Link**: [arxiv](http://arxiv.org/abs/2505.07783v1),  [pdf](http://arxiv.org/pdf/2505.07783v1)

**Tags**: cs.LG 



### MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine   Learning Engineering
**Authors**: Rushi Qiang, Yuchen Zhuang, Yinghao Li, Dingu Sagar V K, Rongzhi Zhang, Changhao Li, Ian Shu-Hei Wong, Sherry Yang, Percy Liang, Chao Zhang, Bo Dai

**Updated**: 2025-05-12T17:35:43Z

**Summary**: We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement learning, evaluating, and improving autonomous large language model (LLM) agents in iterative machine learning engineering (MLE) workflows. Unlike existing benchmarks that primarily rely on static datasets or single-attempt evaluations, MLE-Dojo provides an interactive environment enabling agents to iteratively experiment, debug, and refine solutions through structured feedback loops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse, open-ended MLE tasks carefully curated to reflect realistic engineering scenarios such as data processing, architecture search, hyperparameter tuning, and code debugging. Its fully executable environment supports comprehensive agent training via both supervised fine-tuning and reinforcement learning, facilitating iterative experimentation, realistic data sampling, and real-time outcome verification. Extensive evaluations of eight frontier LLMs reveal that while current models achieve meaningful iterative improvements, they still exhibit significant limitations in autonomously generating long-horizon solutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's flexible and extensible architecture seamlessly integrates diverse data sources, tools, and evaluation protocols, uniquely enabling model-based agent tuning and promoting interoperability, scalability, and reproducibility. We open-source our framework and benchmarks to foster community-driven innovation towards next-generation MLE agents.

**Link**: [arxiv](http://arxiv.org/abs/2505.07782v1),  [pdf](http://arxiv.org/pdf/2505.07782v1)

**Tags**: cs.LG 



### Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for   Mathematical Problem Solving
**Authors**: Xinji Mai, Haotian Xu, Xing W, Weinong Wang, Yingying Zhang, Wenqiang Zhang

**Updated**: 2025-05-12T17:23:34Z

**Summary**: Large Language Models (LLMs) often struggle with mathematical reasoning tasks requiring precise, verifiable computation. While Reinforcement Learning (RL) from outcome-based rewards enhances text-based reasoning, understanding how agents autonomously learn to leverage external tools like code execution remains crucial. We investigate RL from outcome-based rewards for Tool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously generate and execute Python code for mathematical problems without supervised tool-use examples. Our central contribution is we demonstrate that as RL training progresses, key metrics scale predictably. Specifically, we observe strong positive correlations where increased training steps lead to increases in the spontaneous code execution frequency, the average response length, and, critically, the final task accuracy. This suggests a quantifiable relationship between computational effort invested in training and the emergence of effective, tool-augmented reasoning strategies. We implement a robust framework featuring a decoupled code execution environment and validate our findings across standard RL algorithms and frameworks. Experiments show ZeroTIR significantly surpasses non-tool ZeroRL baselines on challenging math benchmarks. Our findings provide a foundational understanding of how autonomous tool use is acquired and scales within Agent RL, offering a reproducible benchmark for future studies. Code is released at \href{https://github.com/Anonymize-Author/AgentRL}{https://github.com/Anonymize-Author/AgentRL}.

**Link**: [arxiv](http://arxiv.org/abs/2505.07773v1),  [pdf](http://arxiv.org/pdf/2505.07773v1)

**Tags**: cs.AI 



### The Beginner's Textbook for Fully Homomorphic Encryption
**Authors**: Ronny Ko

**Updated**: 2025-05-12T17:20:32Z

**Summary**: Fully Homomorphic Encryption (FHE) is a cryptographic scheme that enables computations to be performed directly on encrypted data, as if the data were in plaintext. After all computations are performed on the encrypted data, it can be decrypted to reveal the result. The decrypted value matches the result that would have been obtained if the same computations were applied to the plaintext data.   FHE supports basic operations such as addition and multiplication on encrypted numbers. Using these fundamental operations, more complex computations can be constructed, including subtraction, division, logic gates (e.g., AND, OR, XOR, NAND, MUX), and even advanced mathematical functions such as ReLU, sigmoid, and trigonometric functions (e.g., sin, cos). These functions can be implemented either as exact formulas or as approximations, depending on the trade-off between computational efficiency and accuracy.   Fully Homomorphic Encryption (FHE) enables privacy-preserving machine learning by allowing a server to process the client's data in its encrypted form through an ML model. With FHE, the server learns neither the plaintext version of the input features nor the inference results. Only the client, using their secret key, can decrypt and access the results at the end of the service protocol.FHE can also be applied to confidential blockchain services, ensuring that sensitive data in smart contracts remains encrypted and confidential while maintaining the transparency and integrity of the execution process. Other applications of FHE include secure outsourcing of data analytics, encrypted database queries, privacy-preserving searches, efficient multi-party computation for digital signatures, and more.   This article is designed to help the reader understand how FHE works from the mathematical level.

**Link**: [arxiv](http://arxiv.org/abs/2503.05136v7),  [pdf](http://arxiv.org/pdf/2503.05136v7)

**Tags**: cs.CR cs.DM 



### Enhancing Code Generation via Bidirectional Comment-Level Mutual   Grounding
**Authors**: Yifeng Di, Tianyi Zhang

**Updated**: 2025-05-12T17:20:30Z

**Summary**: Large Language Models (LLMs) have demonstrated unprecedented capability in code generation. However, LLM-generated code is still plagued with a wide range of functional errors, especially for complex programming tasks that LLMs have not seen before. Recent studies have shown that developers often struggle with inspecting and fixing incorrect code generated by LLMs, diminishing their productivity and trust in LLM-based code generation. Inspired by the mutual grounding theory in communication, we propose an interactive approach that leverages code comments as a medium for developers and LLMs to establish a shared understanding. Our approach facilitates iterative grounding by interleaving code generation, inline comment generation, and contextualized user feedback through editable comments to align generated code with developer intent. We evaluated our approach on two popular benchmarks and demonstrated that our approach significantly improved multiple state-of-the-art LLMs, e.g., 17.1% pass@1 improvement for code-davinci-002 on HumanEval. Furthermore, we conducted a user study with 12 participants in comparison to two baselines: (1) interacting with GitHub Copilot, and (2) interacting with a multi-step code generation paradigm called Multi-Turn Program Synthesis. Participants completed the given programming tasks 16.7% faster and with 10.5% improvement in task success rate when using our approach. Both results show that interactively refining code comments enables the collaborative establishment of mutual grounding, leading to more accurate code generation and higher developer confidence.

**Link**: [arxiv](http://arxiv.org/abs/2505.07768v1),  [pdf](http://arxiv.org/pdf/2505.07768v1)

**Tags**: cs.SE cs.AI cs.CL 



### Naturalistic Metaphysics and the Parity Thesis: Why Scientific Realism   Doesn't Lead to Realism about Metaphysics
**Authors**: Raoni Arroyo, Matteo Morganti

**Updated**: 2025-05-12T16:58:08Z

**Summary**: In recent work, Nina Emery has defended the view that, in the context of naturalistic metaphysics, one should maintain the same epistemic attitude towards science and metaphysics. That is, naturalists who are scientific realists ought to be realists about metaphysics as well; and naturalists who are antirealists about science should also be antirealists about metaphysics. We call this the `parity thesis'. This paper suggests that the parity thesis is widely, albeit often implicitly, accepted among naturalistically inclined philosophers, and essentially for reasons similar to Emery's. Then, reasons are provided for resisting Emery's specific inference from scientific realism to realism about metaphysics. The resulting picture is a more nuanced view of the relationship between science and metaphysics within the naturalistic setting than the one which is currently most popular.

**Link**: [arxiv](http://arxiv.org/abs/2505.07751v1),  [pdf](http://arxiv.org/pdf/2505.07751v1)

**Tags**: physics.hist-ph quant-ph 



### Spoken Language Understanding on Unseen Tasks With In-Context Learning
**Authors**: Neeraj Agrawal, Sriram Ganapathy

**Updated**: 2025-05-12T16:38:43Z

**Summary**: Spoken language understanding (SLU) tasks involve diverse skills that probe the information extraction, classification and/or generation capabilities of models. In this setting, task-specific training data may not always be available. While traditional task-specific SLU models are unable to cater to such requirements, the speech-text large language models (LLMs) offer a promising alternative with emergent abilities. However, out of-the-box, our evaluations indicate that the zero/few-shot performance of prominent open-source speech-text LLMs on SLU tasks are not up to the mark. In this paper, we introduce a novel approach to robust task-agnostic fine-tuning using randomized class labels. With this proposed fine-tuning, we illustrate that the performance of the speech-text LLMs on an unseen task is significantly improved over standard approaches. Critically, the proposed approach avoids the requirement of task-specific data annotations for enabling new tasks in speech-text LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2505.07731v1),  [pdf](http://arxiv.org/pdf/2505.07731v1)

**Tags**: cs.CL cs.LG eess.AS 



### Reproducibility, Replicability, and Insights into Visual Document   Retrieval with Late Interaction
**Authors**: Jingfen Qiao, Jia-Huei Ju, Xinyu Ma, Evangelos Kanoulas, Andrew Yates

**Updated**: 2025-05-12T16:37:47Z

**Summary**: Visual Document Retrieval (VDR) is an emerging research area that focuses on encoding and retrieving document images directly, bypassing the dependence on Optical Character Recognition (OCR) for document search. A recent advance in VDR was introduced by ColPali, which significantly improved retrieval effectiveness through a late interaction mechanism. ColPali's approach demonstrated substantial performance gains over existing baselines that do not use late interaction on an established benchmark. In this study, we investigate the reproducibility and replicability of VDR methods with and without late interaction mechanisms by systematically evaluating their performance across multiple pre-trained vision-language models. Our findings confirm that late interaction yields considerable improvements in retrieval effectiveness; however, it also introduces computational inefficiencies during inference. Additionally, we examine the adaptability of VDR models to textual inputs and assess their robustness across text-intensive datasets within the proposed benchmark, particularly when scaling the indexing mechanism. Furthermore, our research investigates the specific contributions of late interaction by looking into query-patch matching in the context of visual document retrieval. We find that although query tokens cannot explicitly match image patches as in the text retrieval scenario, they tend to match the patch contains visually similar tokens or their surrounding patches.

**Link**: [arxiv](http://arxiv.org/abs/2505.07730v1),  [pdf](http://arxiv.org/pdf/2505.07730v1)

**Tags**: cs.IR 



### Nonparametric Instrumental Variable Inference with Many Weak Instruments
**Authors**: Lars van der Laan, Nathan Kallus, Aurélien Bibaut

**Updated**: 2025-05-12T16:36:55Z

**Summary**: We study inference on linear functionals in the nonparametric instrumental variable (NPIV) problem with a discretely-valued instrument under a many-weak-instruments asymptotic regime, where the number of instrument values grows with the sample size. A key motivating example is estimating long-term causal effects in a new experiment with only short-term outcomes, using past experiments to instrument for the effect of short- on long-term outcomes. Here, the assignment to a past experiment serves as the instrument: we have many past experiments but only a limited number of units in each. Since the structural function is nonparametric but constrained by only finitely many moment restrictions, point identification typically fails. To address this, we consider linear functionals of the minimum-norm solution to the moment restrictions, which is always well-defined. As the number of instrument levels grows, these functionals define an approximating sequence to a target functional, replacing point identification with a weaker asymptotic notion suited to discrete instruments. Extending the Jackknife Instrumental Variable Estimator (JIVE) beyond the classical parametric setting, we propose npJIVE, a nonparametric estimator for solutions to linear inverse problems with many weak instruments. We construct automatic debiased machine learning estimators for linear functionals of both the structural function and its minimum-norm projection, and establish their efficiency in the many-weak-instruments regime.

**Link**: [arxiv](http://arxiv.org/abs/2505.07729v1),  [pdf](http://arxiv.org/pdf/2505.07729v1)

**Tags**: stat.ME math.ST stat.ML stat.TH 



### The Leaderboard Illusion
**Authors**: Shivalika Singh, Yiyang Nan, Alex Wang, Daniel D'Souza, Sayash Kapoor, Ahmet Üstün, Sanmi Koyejo, Yuntian Deng, Shayne Longpre, Noah A. Smith, Beyza Ermis, Marzieh Fadaee, Sara Hooker

**Updated**: 2025-05-12T16:33:58Z

**Summary**: Measuring progress is fundamental to the advancement of any scientific field. As benchmarks play an increasingly central role, they also grow more susceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard for ranking the most capable AI systems. Yet, in this work we identify systematic issues that have resulted in a distorted playing field. We find that undisclosed private testing practices benefit a handful of providers who are able to test multiple variants before public release and retract scores if desired. We establish that the ability of these providers to choose the best score leads to biased Arena scores due to selective disclosure of performance results. At an extreme, we identify 27 private LLM variants tested by Meta in the lead-up to the Llama-4 release. We also establish that proprietary closed models are sampled at higher rates (number of battles) and have fewer models removed from the arena than open-weight and open-source alternatives. Both these policies lead to large data access asymmetries over time. Providers like Google and OpenAI have received an estimated 19.2% and 20.4% of all data on the arena, respectively. In contrast, a combined 83 open-weight models have only received an estimated 29.7% of the total data. We show that access to Chatbot Arena data yields substantial benefits; even limited additional data can result in relative performance gains of up to 112% on the arena distribution, based on our conservative estimates. Together, these dynamics result in overfitting to Arena-specific dynamics rather than general model quality. The Arena builds on the substantial efforts of both the organizers and an open community that maintains this valuable evaluation platform. We offer actionable recommendations to reform the Chatbot Arena's evaluation framework and promote fairer, more transparent benchmarking for the field

**Link**: [arxiv](http://arxiv.org/abs/2504.20879v2),  [pdf](http://arxiv.org/pdf/2504.20879v2)

**Tags**: cs.AI cs.CL cs.LG stat.ME 



### Particle Gibbs without the Gibbs bit
**Authors**: Adrien Corenflos

**Updated**: 2025-05-12T16:33:46Z

**Summary**: Exact parameter and trajectory inference in state-space models is typically achieved by one of two methods: particle marginal Metropolis-Hastings (PMMH) or particle Gibbs (PGibbs). PMMH is a pseudo-marginal algorithm which jointly proposes a new trajectory and parameter, and accepts or rejects both at once. PGibbs instead alternates between sampling from the trajectory, using an algorithm known as conditional sequential Monte Carlo (CSMC) and the parameter in a Hastings-within-Gibbs fashion. While particle independent Metropolis Hastings (PIMH), the parameter-free version of PMMH, is known to be statistically worse than CSMC, PGibbs can induce a slow mixing if the parameter and the state trajectory are very correlated. This has made PMMH the method of choice for many practitioners, despite theory and experiments favouring CSMC over PIMH for the parameter-free problem. In this article, we describe a formulation of PGibbs which bypasses the Gibbs step, essentially marginalizing over the trajectory distribution in a fashion similar to PMMH. This is achieved by considering the implementation of a CSMC algortihm for the state-space model integrated over the joint distribution of the current parameter and the parameter proposal. We illustrate the benefits of method on a simple example known to be challenging for PMMH.

**Link**: [arxiv](http://arxiv.org/abs/2505.04611v3),  [pdf](http://arxiv.org/pdf/2505.04611v3)

**Tags**: stat.CO eess.SP stat.ME 



### Gameplay Highlights Generation
**Authors**: Vignesh Edithal, Le Zhang, Ilia Blank, Imran Junejo

**Updated**: 2025-05-12T16:28:22Z

**Summary**: In this work, we enable gamers to share their gaming experience on social media by automatically generating eye-catching highlight reels from their gameplay session Our automation will save time for gamers while increasing audience engagement. We approach the highlight generation problem by first identifying intervals in the video where interesting events occur and then concatenate them. We developed an in-house gameplay event detection dataset containing interesting events annotated by humans using VIA video annotator. Traditional techniques for highlight detection such as game engine integration requires expensive collaboration with game developers. OCR techniques which detect patches of specific images or texts require expensive per game engineering and may not generalize across game UI and different language. We finetuned a multimodal general purpose video understanding model such as X-CLIP using our dataset which generalizes across multiple games in a genre without per game engineering. Prompt engineering was performed to improve the classification performance of this multimodal model. Our evaluation showed that such a finetuned model can detect interesting events in first person shooting games from unseen gameplay footage with more than 90% accuracy. Moreover, our model performed significantly better on low resource games (small dataset) when trained along with high resource games, showing signs of transfer learning. To make the model production ready, we used ONNX libraries to enable cross platform inference. These libraries also provide post training quantization tools to reduce model size and inference time for deployment. ONNX runtime libraries with DirectML backend were used to perform efficient inference on Windows OS. We show that natural language supervision in the X-CLIP model leads to data efficient and highly performant video recognition models.

**Link**: [arxiv](http://arxiv.org/abs/2505.07721v1),  [pdf](http://arxiv.org/pdf/2505.07721v1)

**Tags**: cs.CV 



### The First Prompt Counts the Most! An Evaluation of Large Language Models   on Iterative Example-Based Code Generation
**Authors**: Yingjie Fu, Bozhou Li, Linyi Li, Wentao Zhang, Tao Xie

**Updated**: 2025-05-12T16:21:52Z

**Summary**: The capabilities of Large Language Models (LLMs) in code generation have been extensively studied, particularly for implementing target functionalities from natural-language descriptions. Alternatively, input-output (I/O) examples provide an accessible, unambiguous, and flexible way to describe functionalities. However, their inherent diversity, opaqueness, and incompleteness impose greater challenges for understanding and implementing the target requirements. Therefore, generating code from I/O examples (i.e., example-based code generation) provides a new perspective, allowing us to additionally evaluate LLMs' capability to infer target functionalities from limited information and to process new-form requirements. However, related research about LLMs in example-based code generation remains largely unexplored. To fill this gap, this paper presents the first comprehensive study on example-based code generation using LLMs. We adopt an iterative evaluation framework and formalize the objective of example-based code generation as two sequential sub-objectives: generating code conforming to the given examples and generating code that successfully implements the target functionalities from (iteratively) given examples. We assess six state-of-the-art LLMs using a new benchmark of 172 diverse target functionalities. The results demonstrate that when requirements are described using iterative I/O examples rather than natural language, the LLMs' score decreases by over 60%, and the vast majority (even over 95%) of successfully implemented functionalities are achieved in the first round of the iterations. Furthermore, we also find that combining I/O examples with even imprecise and fragmental natural language descriptions greatly improves LLM performance, and the selection of initial I/O examples can also influence the score, suggesting opportunities for prompt optimization.

**Link**: [arxiv](http://arxiv.org/abs/2411.06774v2),  [pdf](http://arxiv.org/pdf/2411.06774v2)

**Tags**: cs.SE 



### Routing Attacks in Ethereum PoS: A Systematic Exploration
**Authors**: Constantine Doumanidis, Maria Apostolaki

**Updated**: 2025-05-12T16:19:03Z

**Summary**: With the promise of greater decentralization and sustainability, Ethereum transitioned from a Proof-of-Work (PoW) to a Proof-of-Stake (PoS) consensus mechanism. The new consensus protocol introduces novel vulnerabilities that warrant further investigation. The goal of this paper is to investigate the security of Ethereum's PoS system from an Internet routing perspective.   To this end, this paper makes two contributions: First, we devise a novel framework for inferring the distribution of validators on the Internet without disturbing the real network. Second, we introduce a class of network-level attacks on Ethereum's PoS system that jointly exploit Internet routing vulnerabilities with the protocol's reward and penalty mechanisms. We describe two representative attacks: StakeBleed, where the attacker triggers an inactivity leak, halting block finality and causing financial losses for all validators; and KnockBlock, where the attacker increases her expected MEV gains by preventing targeted blocks from being included in the chain. We find that both attacks are practical and effective. An attacker executing StakeBleed can inflict losses of almost 300 ETH in just 2 hours by hijacking as few as 30 IP prefixes. An attacker implementing KnockBlock could increase their MEV expected gains by 44.5% while hijacking a single prefix for less than 2 minutes.   Our paper serves as a call to action for validators to reinforce their Internet routing infrastructure and for the Ethereum P2P protocol to implement stronger mechanisms to conceal validator locations.

**Link**: [arxiv](http://arxiv.org/abs/2505.07713v1),  [pdf](http://arxiv.org/pdf/2505.07713v1)

**Tags**: cs.NI 



### Circuit Partitioning Using Large Language Models for Quantum Compilation   and Simulations
**Authors**: Pranav Sinha, Sumit Kumar Jha, Sunny Raj

**Updated**: 2025-05-12T16:18:48Z

**Summary**: We are in the midst of the noisy intermediate-scale quantum (NISQ) era, where quantum computers are limited by noisy gates, some of which are more error-prone than others and can render the final computation incomprehensible. Quantum circuit compilation algorithms attempt to minimize these noisy gates when mapping quantum algorithms onto quantum hardware but face computational challenges that restrict their application to circuits with no more than 5-6 qubits, necessitating the need to partition large circuits before the application of noisy quantum gate minimization algorithms. The existing generation of these algorithms is heuristic in nature and does not account for downstream gate minimization tasks. Large language models (LLMs) have the potential to change this and help improve quantum circuit partitions. This paper investigates the use of LLMs, such as Llama and Mistral, for partitioning quantum circuits by capitalizing on their abilities to understand and generate code, including QASM. Specifically, we teach LLMs to partition circuits using the quick partition approach of the Berkeley Quantum Synthesis Toolkit. Through experimental evaluations, we show that careful fine-tuning of open source LLMs enables us to obtain an accuracy of 53.4% for the partition task while over-the-shelf LLMs are unable to correctly partition circuits, using standard 1-shot and few-shot training approaches.

**Link**: [arxiv](http://arxiv.org/abs/2505.07711v1),  [pdf](http://arxiv.org/pdf/2505.07711v1)

**Tags**: cs.ET cs.AI quant-ph 



### A constraints-based approach to fully interpretable neural networks for   detecting learner behaviors
**Authors**: Juan D. Pinto, Luc Paquette

**Updated**: 2025-05-12T16:12:50Z

**Summary**: The increasing use of complex machine learning models in education has led to concerns about their interpretability, which in turn has spurred interest in developing explainability techniques that are both faithful to the model's inner workings and intelligible to human end-users. In this paper, we describe a novel approach to creating a neural-network-based behavior detection model that is interpretable by design. Our model is fully interpretable, meaning that the parameters we extract for our explanations have a clear interpretation, fully capture the model's learned knowledge about the learner behavior of interest, and can be used to create explanations that are both faithful and intelligible. We achieve this by implementing a series of constraints to the model that both simplify its inference process and bring it closer to a human conception of the task at hand. We train the model to detect gaming-the-system behavior, evaluate its performance on this task, and compare its learned patterns to those identified by human experts. Our results show that the model is successfully able to learn patterns indicative of gaming-the-system behavior while providing evidence for fully interpretable explanations. We discuss the implications of our approach and suggest ways to evaluate explainability using a human-grounded approach.

**Link**: [arxiv](http://arxiv.org/abs/2504.20055v2),  [pdf](http://arxiv.org/pdf/2504.20055v2)

**Tags**: cs.LG cs.AI 



### Codifying Character Logic in Role-Playing
**Authors**: Letian Peng, Jingbo Shang

**Updated**: 2025-05-13T02:16:35Z

**Summary**: This paper introduces Codified Profiles for role-playing, a novel approach that represents character logic as structured, executable functions for behavioral decision-making. Each profile defines a set of functions parse_by_scene(scene) that outputs a list of logic-grounded assertions triggered_statements, using both explicit control structures (e.g., if-then-else) and condition checks like check_condition(scene, question), where each question is a semantically meaningful prompt about the scene (e.g., "Is the character in danger?") discriminated by the role-playing LLM as true, false, or unknown. This explicit representation offers three key advantages over traditional prompt-based profiles, which append character descriptions directly into text prompts: (1) Persistence, by enforcing complete and consistent execution of character logic, rather than relying on the model's implicit reasoning; (2) Updatability, through systematic inspection and revision of behavioral logic, which is difficult to track or debug in prompt-only approaches; (3) Controllable Randomness, by supporting stochastic behavior directly within the logic, enabling fine-grained variability that prompting alone struggles to achieve. To validate these advantages, we introduce a new benchmark constructed from 83 characters and 5,141 scenes curated from Fandom, using NLI-based scoring to compare character responses against ground-truth actions. Our experiments demonstrate the significant benefits of codified profiles in improving persistence, updatability, and behavioral diversity. Notably, by offloading a significant portion of reasoning to preprocessing, codified profiles enable even 1B-parameter models to perform high-quality role-playing, providing a scalable and efficient foundation for local deployment of role-play agents.

**Link**: [arxiv](http://arxiv.org/abs/2505.07705v2),  [pdf](http://arxiv.org/pdf/2505.07705v2)

**Tags**: cs.CL 



### From Distributional to Overton Pluralism: Investigating Large Language   Model Alignment
**Authors**: Thom Lake, Eunsol Choi, Greg Durrett

**Updated**: 2025-05-12T16:11:53Z

**Summary**: The alignment process changes several properties of a large language model's (LLM's) output distribution. We analyze two aspects of post-alignment distributional shift of LLM responses. First, we re-examine previously reported reductions in response diversity post-alignment. Our analysis suggests that an apparent drop in the diversity of responses is largely explained by quality control and information aggregation. Alignment suppresses irrelevant and unhelpful content while shifting the output distribution toward longer responses that cover information spanning several responses from the base LLM, essentially presenting diverse information in a single response. Finding little evidence that alignment suppresses useful information, it is natural to ask the opposite question: do aligned models surface information that cannot be recovered from base models? Our second investigation shows this is not the case and the behavior of aligned models is recoverable from base models without fine-tuning. A combination of in-context examples and lower-resolution semantic hints about response content can elicit responses from base LLMs that are as similar to alignment-tuned LLM responses as alignment-tuned LLM responses are to each other. Taken together, these results indicate that current alignment techniques capture but do not extend the useful subset of assistant-like base LLM behavior, providing further evidence for the Superficial Alignment Hypothesis. They also show that in-context alignment can go surprisingly far as a strategy for imitating aligned LLMs without fine-tuning. Our code and data is available at https://github.com/thomlake/investigating-alignment.

**Link**: [arxiv](http://arxiv.org/abs/2406.17692v2),  [pdf](http://arxiv.org/pdf/2406.17692v2)

**Tags**: cs.CL cs.LG 



### PatchTrack: A Comprehensive Analysis of ChatGPT's Influence on Pull   Request Outcomes
**Authors**: Daniel Ogenrwot, John Businge

**Updated**: 2025-05-12T16:09:33Z

**Summary**: The rapid adoption of large language models (LLMs) like ChatGPT in software development has introduced new ways for developers to interact with AI, particularly in pull request workflows. While prior research has examined AI-generated code quality, there is limited understanding of how ChatGPT is utilized in real-world pull request decision-making and how its suggestions influence patch integration and rejection. To explore these aspects, we analyze self-admitted ChatGPT usage (SACU), where developers explicitly disclose their reliance on ChatGPT within pull request discussions. Our study examines 338 pull requests (285 merged, 53 closed) across 255 GitHub repositories, containing 645 ChatGPT-generated code snippets and 3,486 patches. We introduce PatchTrack, a classification tool that determines whether ChatGPT-generated patches were applied (PA, 115 cases), not applied (PN, 64 cases), or not suggested (NE, 106 cases). Our findings reveal that full adoption of ChatGPT-generated code is rare, developers frequently modify or selectively integrate AI-generated patches to align with project constraints, with a median integration rate of 25%. Through qualitative analysis, we identify key factors influencing patch integration and pull request rejection, including scope misalignment, maintainability concerns, redundant solutions, and procedural barriers such as incomplete documentation or administrative policies. By providing empirical insights into ChatGPT's role in pull request workflows, this study informs developers, maintainers, and educators on the evolving use of generative AI in collaborative software development. It also lays the groundwork for future research on optimizing AI-assisted development, improving transparency in AI adoption, and enhancing patch integration workflows.

**Link**: [arxiv](http://arxiv.org/abs/2505.07700v1),  [pdf](http://arxiv.org/pdf/2505.07700v1)

**Tags**: cs.SE D.2.0; K.6.3 



### Large Language Models Think Too Fast To Explore Effectively
**Authors**: Lan Pan, Hanbo Xie, Robert C. Wilson

**Updated**: 2025-05-12T16:02:08Z

**Summary**: Large Language Models (LLMs) have emerged with many intellectual capacities. While numerous benchmarks assess their intelligence, limited attention has been given to their ability to explore--an essential capacity for discovering new information and adapting to novel environments in both natural and artificial systems. The extent to which LLMs can effectively explore, particularly in open-ended tasks, remains unclear. This study investigates whether LLMs can surpass humans in exploration during an open-ended task, using Little Alchemy 2 as a paradigm, where agents combine elements to discover new ones. Results show most LLMs underperform compared to humans, except for the o1 model, with traditional LLMs relying primarily on uncertainty-driven strategies, unlike humans who balance uncertainty and empowerment. Results indicate that traditional reasoning-focused LLMs, such as GPT-4o, exhibit a significantly faster and less detailed reasoning process, limiting their exploratory performance. In contrast, the DeepSeek reasoning model demonstrates prolonged, iterative thought processes marked by repetitive analysis of combinations and past trials, reflecting a more thorough and human-like exploration strategy. Representational analysis of the models with Sparse Autoencoders (SAE) revealed that uncertainty and choices are represented at earlier transformer blocks, while empowerment values are processed later, causing LLMs to think too fast and make premature decisions, hindering effective exploration. These findings shed light on the limitations of LLM exploration and suggest directions for improving their adaptability.

**Link**: [arxiv](http://arxiv.org/abs/2501.18009v2),  [pdf](http://arxiv.org/pdf/2501.18009v2)

**Tags**: cs.AI q-bio.NC 



### VLM2-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit   Matching Visual Cues
**Authors**: Jianshu Zhang, Dongyu Yao, Renjie Pi, Paul Pu Liang, Yi R. Fung

**Updated**: 2025-05-12T15:55:55Z

**Summary**: Visually linking matching cues is a crucial ability in daily life, such as identifying the same person in multiple photos based on their cues, even without knowing who they are. Despite the extensive knowledge that vision-language models (VLMs) possess, it remains largely unexplored whether they are capable of performing this fundamental task. To address this, we introduce VLM2-Bench, a benchmark designed to assess whether VLMs can Visually Link Matching cues, with 9 subtasks and over 3,000 test cases. Comprehensive evaluation across eight open-source VLMs and GPT-4o, along with further analysis of various language-side and vision-side prompting methods, leads to a total of eight key findings. We identify critical challenges in models' ability to link visual cues, highlighting a significant performance gap where even GPT-4o lags 34.80% behind humans. Based on these insights, we advocate for (i) enhancing core visual capabilities to improve adaptability and reduce reliance on prior knowledge, (ii) establishing clearer principles for integrating language-based reasoning in vision-centric tasks to prevent unnecessary biases, and (iii) shifting vision-text training paradigms toward fostering models' ability to independently structure and infer relationships among visual cues.

**Link**: [arxiv](http://arxiv.org/abs/2502.12084v3),  [pdf](http://arxiv.org/pdf/2502.12084v3)

**Tags**: cs.CL 



### SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in   Large Language Models
**Authors**: Hang Wu, Jianian Zhu, Yinghui Li, Haojie Wang, Biao Hou, Jidong Zhai

**Updated**: 2025-05-12T15:46:28Z

**Summary**: Large Language Models (LLMs) present a critical trade-off between inference quality and computational cost: larger models offer superior capabilities but incur significant latency, while smaller models are faster but less powerful. Existing serving strategies often employ fixed model scales or static two-stage speculative decoding, failing to dynamically adapt to the varying complexities of user requests or fluctuations in system performance. This paper introduces \systemname{}, a novel framework that reimagines LLM inference as an adaptive routing problem solved through multi-level speculative decoding. \systemname{} dynamically constructs and optimizes inference "paths" (chains of models) based on real-time feedback, addressing the limitations of static approaches. Our contributions are threefold: (1) An \textbf{adaptive model chain scheduling} mechanism that leverages performance profiling (execution times) and predictive similarity metrics (derived from token distribution divergence) to continuously select the optimal sequence of draft and verifier models, minimizing predicted latency per generated token. (2) A \textbf{multi-level collaborative verification} framework where intermediate models within the selected chain can validate speculative tokens, reducing the verification burden on the final, most powerful target model. (3) A \textbf{synchronized state management} system providing efficient, consistent KV cache handling across heterogeneous models in the chain, including precise, low-overhead rollbacks tailored for asynchronous batch processing inherent in multi-level speculation. Preliminary experiments demonstrate the validity of our method.

**Link**: [arxiv](http://arxiv.org/abs/2505.07680v1),  [pdf](http://arxiv.org/pdf/2505.07680v1)

**Tags**: cs.LG cs.DC 



### Geospatial Mechanistic Interpretability of Large Language Models
**Authors**: Stef De Sabbata, Stefano Mizzaro, Kevin Roitero

**Updated**: 2025-05-12T15:44:44Z

**Summary**: Large Language Models (LLMs) have demonstrated unprecedented capabilities across various natural language processing tasks. Their ability to process and generate viable text and code has made them ubiquitous in many fields, while their deployment as knowledge bases and "reasoning" tools remains an area of ongoing research. In geography, a growing body of literature has been focusing on evaluating LLMs' geographical knowledge and their ability to perform spatial reasoning. However, very little is still known about the internal functioning of these models, especially about how they process geographical information.   In this chapter, we establish a novel framework for the study of geospatial mechanistic interpretability - using spatial analysis to reverse engineer how LLMs handle geographical information. Our aim is to advance our understanding of the internal representations that these complex models generate while processing geographical information - what one might call "how LLMs think about geographic information" if such phrasing was not an undue anthropomorphism.   We first outline the use of probing in revealing internal structures within LLMs. We then introduce the field of mechanistic interpretability, discussing the superposition hypothesis and the role of sparse autoencoders in disentangling polysemantic internal representations of LLMs into more interpretable, monosemantic features. In our experiments, we use spatial autocorrelation to show how features obtained for placenames display spatial patterns related to their geographic location and can thus be interpreted geospatially, providing insights into how these models process geographical information. We conclude by discussing how our framework can help shape the study and use of foundation models in geography.

**Link**: [arxiv](http://arxiv.org/abs/2505.03368v2),  [pdf](http://arxiv.org/pdf/2505.03368v2)

**Tags**: cs.LG 



### Nonparametric extensions of nuclear equations of state: probing the   breakdown scale of relativistic mean-field theory
**Authors**: Isaac Legred, Liam Brodie, Alexander Haber, Reed Essick, Katerina Chatziioannou

**Updated**: 2025-05-12T15:44:18Z

**Summary**: Phenomenological calculations of the properties of dense matter, such as relativistic mean-field theories, represent a pathway to predicting the microscopic and macroscopic properties of neutron stars. However, such theories do not generically have well-controlled uncertainties and may break down within neutron stars. To faithfully represent the uncertainty in this breakdown scale, we develop a hybrid representation of the dense-matter equation of state, which assumes the form of a relativistic mean-field theory at low densities, while remaining agnostic to any nuclear theory at high densities. To achieve this, we use a nonparametric equation of state model to incorporate the correlations of the underlying relativistic mean-field theory equation of state at low pressures and transition to more flexible correlations above some chosen pressure scale. We perform astrophysical inference under various choices of the transition pressure between the theory-informed and theory-agnostic models. We further study whether the chosen relativistic mean-field theory breaks down above some particular pressure and find no such evidence. Using simulated data for future astrophysical observations at about two-to-three times the precision of current constraints, we show that our method can identify the breakdown pressure associated with a potential strong phase transition.

**Link**: [arxiv](http://arxiv.org/abs/2505.07677v1),  [pdf](http://arxiv.org/pdf/2505.07677v1)

**Tags**: nucl-th astro-ph.HE gr-qc 



### Mapping Biomedical Ontology Terms to IDs: Effect of Domain Prevalence on   Prediction Accuracy
**Authors**: Thanh Son Do, Daniel B. Hier, Tayo Obafemi-Ajayi

**Updated**: 2025-05-12T15:43:37Z

**Summary**: This study evaluates the ability of large language models (LLMs) to map biomedical ontology terms to their corresponding ontology IDs across the Human Phenotype Ontology (HPO), Gene Ontology (GO), and UniProtKB terminologies. Using counts of ontology IDs in the PubMed Central (PMC) dataset as a surrogate for their prevalence in the biomedical literature, we examined the relationship between ontology ID prevalence and mapping accuracy. Results indicate that ontology ID prevalence strongly predicts accurate mapping of HPO terms to HPO IDs, GO terms to GO IDs, and protein names to UniProtKB accession numbers. Higher prevalence of ontology IDs in the biomedical literature correlated with higher mapping accuracy. Predictive models based on receiver operating characteristic (ROC) curves confirmed this relationship.   In contrast, this pattern did not apply to mapping protein names to Human Genome Organisation's (HUGO) gene symbols. GPT-4 achieved a high baseline performance (95%) in mapping protein names to HUGO gene symbols, with mapping accuracy unaffected by prevalence. We propose that the high prevalence of HUGO gene symbols in the literature has caused these symbols to become lexicalized, enabling GPT-4 to map protein names to HUGO gene symbols with high accuracy. These findings highlight the limitations of LLMs in mapping ontology terms to low-prevalence ontology IDs and underscore the importance of incorporating ontology ID prevalence into the training and evaluation of LLMs for biomedical applications.

**Link**: [arxiv](http://arxiv.org/abs/2409.13746v2),  [pdf](http://arxiv.org/pdf/2409.13746v2)

**Tags**: cs.CL cs.AI I.2 



### Simple Semi-supervised Knowledge Distillation from Vision-Language   Models via $\mathbf{\texttt{D}}$ual-$\mathbf{\texttt{H}}$ead   $\mathbf{\texttt{O}}$ptimization
**Authors**: Seongjae Kang, Dong Bok Lee, Hyungjoon Jang, Sung Ju Hwang

**Updated**: 2025-05-12T15:39:51Z

**Summary**: Vision-language models (VLMs) have achieved remarkable success across diverse tasks by leveraging rich textual information with minimal labeled data. However, deploying such large models remains challenging, particularly in resource-constrained environments. Knowledge distillation (KD) offers a well-established solution to this problem; however, recent KD approaches from VLMs often involve multi-stage training or additional tuning, increasing computational overhead and optimization complexity. In this paper, we propose $\mathbf{\texttt{D}}$ual-$\mathbf{\texttt{H}}$ead $\mathbf{\texttt{O}}$ptimization ($\mathbf{\texttt{DHO}}$) -- a simple yet effective KD framework that transfers knowledge from VLMs to compact, task-specific models in semi-supervised settings. Specifically, we introduce dual prediction heads that independently learn from labeled data and teacher predictions, and propose to linearly combine their outputs during inference. We observe that $\texttt{DHO}$ mitigates gradient conflicts between supervised and distillation signals, enabling more effective feature learning than single-head KD baselines. As a result, extensive experiments show that $\texttt{DHO}$ consistently outperforms baselines across multiple domains and fine-grained datasets. Notably, on ImageNet, it achieves state-of-the-art performance, improving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively, while using fewer parameters.

**Link**: [arxiv](http://arxiv.org/abs/2505.07675v1),  [pdf](http://arxiv.org/pdf/2505.07675v1)

**Tags**: cs.LG cs.AI cs.CV 



### OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit
**Authors**: Arun S. Maiya

**Updated**: 2025-05-13T02:43:26Z

**Summary**: We present OnPrem$.$LLM, a Python-based toolkit for applying large language models (LLMs) to sensitive, non-public data in offline or restricted environments. The system is designed for privacy-preserving use cases and provides prebuilt pipelines for document processing and storage, retrieval-augmented generation (RAG), information extraction, summarization, classification, and prompt/output processing with minimal configuration. OnPrem$.$LLM supports multiple LLM backends -- including llama$.$cpp, Ollama, vLLM, and Hugging Face Transformers -- with quantized model support, GPU acceleration, and seamless backend switching. Although designed for fully local execution, OnPrem$.$LLM also supports integration with a wide range of cloud LLM providers when permitted, enabling hybrid deployments that balance performance with data control. A no-code web interface extends accessibility to non-technical users.

**Link**: [arxiv](http://arxiv.org/abs/2505.07672v2),  [pdf](http://arxiv.org/pdf/2505.07672v2)

**Tags**: cs.CL cs.AI cs.LG 



### Benchmarking Retrieval-Augmented Generation for Chemistry
**Authors**: Xianrui Zhong, Bowen Jin, Siru Ouyang, Yanzhen Shen, Qiao Jin, Yin Fang, Zhiyong Lu, Jiawei Han

**Updated**: 2025-05-12T15:34:45Z

**Summary**: Retrieval-augmented generation (RAG) has emerged as a powerful framework for enhancing large language models (LLMs) with external knowledge, particularly in scientific domains that demand specialized and dynamic information. Despite its promise, the application of RAG in the chemistry domain remains underexplored, primarily due to the lack of high-quality, domain-specific corpora and well-curated evaluation benchmarks. In this work, we introduce ChemRAG-Bench, a comprehensive benchmark designed to systematically assess the effectiveness of RAG across a diverse set of chemistry-related tasks. The accompanying chemistry corpus integrates heterogeneous knowledge sources, including scientific literature, the PubChem database, PubMed abstracts, textbooks, and Wikipedia entries. In addition, we present ChemRAG-Toolkit, a modular and extensible RAG toolkit that supports five retrieval algorithms and eight LLMs. Using ChemRAG-Toolkit, we demonstrate that RAG yields a substantial performance gain -- achieving an average relative improvement of 17.4% over direct inference methods. We further conduct in-depth analyses on retriever architectures, corpus selection, and the number of retrieved passages, culminating in practical recommendations to guide future research and deployment of RAG systems in the chemistry domain. The code and data is available at https://chemrag.github.io.

**Link**: [arxiv](http://arxiv.org/abs/2505.07671v1),  [pdf](http://arxiv.org/pdf/2505.07671v1)

**Tags**: cs.CL cs.AI cs.IR 



### Separable models for dynamic signed networks
**Authors**: Alberto Caimo, Isabella Gollini

**Updated**: 2025-05-12T15:33:57Z

**Summary**: Signed networks capture the polarity of relationships between nodes, providing valuable insights into complex systems where both supportive and antagonistic interactions play a critical role in shaping the network's dynamics. We propose a separable temporal generative framework based on multi-layer exponential random graph models, characterised by the assumption of conditional independence between the sign and interaction effects. This structure preserves the flexibly and explanatory power inherent in the binary network specification while adhering to consistent balance theory assumptions. Using a fully probabilistic Bayesian paradigm, we infer the doubly intractable posterior distribution of model parameters via an adaptive Metropolis-Hastings approximate exchange algorithm. We illustrate the interpretability of our model by analysing signed relations among U.S. Senators during Ronald Reagan's second term (1985-1989). Specifically, we aim to understand whether these relations are consistent and balanced or reflect patterns of supportive or antagonistic alliances.

**Link**: [arxiv](http://arxiv.org/abs/2505.07669v1),  [pdf](http://arxiv.org/pdf/2505.07669v1)

**Tags**: stat.ME stat.AP 



### A Case Study Investigating the Role of Generative AI in Quality   Evaluations of Epics in Agile Software Development
**Authors**: Werner Geyer, Jessica He, Daita Sarkar, Michelle Brachman, Chris Hammond, Jennifer Heins, Zahra Ashktorab, Carlos Rosemberg, Charlie Hill

**Updated**: 2025-05-12T15:31:16Z

**Summary**: The broad availability of generative AI offers new opportunities to support various work domains, including agile software development. Agile epics are a key artifact for product managers to communicate requirements to stakeholders. However, in practice, they are often poorly defined, leading to churn, delivery delays, and cost overruns. In this industry case study, we investigate opportunities for large language models (LLMs) to evaluate agile epic quality in a global company. Results from a user study with 17 product managers indicate how LLM evaluations could be integrated into their work practices, including perceived values and usage in improving their epics. High levels of satisfaction indicate that agile epics are a new, viable application of AI evaluations. However, our findings also outline challenges, limitations, and adoption barriers that can inform both practitioners and researchers on the integration of such evaluations into future agile work practices.

**Link**: [arxiv](http://arxiv.org/abs/2505.07664v1),  [pdf](http://arxiv.org/pdf/2505.07664v1)

**Tags**: cs.SE cs.AI cs.HC 



### JobHop: A Large-Scale Dataset of Career Trajectories
**Authors**: Iman Johary, Raphael Romero, Alexandru C. Mara, Tijl De Bie

**Updated**: 2025-05-12T15:22:29Z

**Summary**: Understanding labor market dynamics is essential for policymakers, employers, and job seekers. However, comprehensive datasets that capture real-world career trajectories are scarce. In this paper, we introduce JobHop, a large-scale public dataset derived from anonymized resumes provided by VDAB, the public employment service in Flanders, Belgium. Utilizing Large Language Models (LLMs), we process unstructured resume data to extract structured career information, which is then mapped to standardized ESCO occupation codes using a multi-label classification model. This results in a rich dataset of over 2.3 million work experiences, extracted from and grouped into more than 391,000 user resumes and mapped to standardized ESCO occupation codes, offering valuable insights into real-world occupational transitions. This dataset enables diverse applications, such as analyzing labor market mobility, job stability, and the effects of career breaks on occupational transitions. It also supports career path prediction and other data-driven decision-making processes. To illustrate its potential, we explore key dataset characteristics, including job distributions, career breaks, and job transitions, demonstrating its value for advancing labor market research.

**Link**: [arxiv](http://arxiv.org/abs/2505.07653v1),  [pdf](http://arxiv.org/pdf/2505.07653v1)

**Tags**: cs.CL 



### Hybrid Local Causal Discovery
**Authors**: Zhaolong Ling, Honghui Peng, Yiwen Zhang, Debo Cheng, Xingyu Wu, Peng Zhou, Kui Yu

**Updated**: 2025-05-12T15:18:30Z

**Summary**: Local causal discovery aims to learn and distinguish the direct causes and effects of a target variable from observed data. Existing constraint-based local causal discovery methods use AND or OR rules in constructing the local causal skeleton, but using either rule alone is prone to produce cascading errors in the learned local causal skeleton, and thus impacting the inference of local causal relationships. On the other hand, directly applying score-based global causal discovery methods to local causal discovery may randomly return incorrect results due to the existence of local equivalence classes. To address the above issues, we propose a Hybrid Local Causal Discovery algorithm, called HLCD. Specifically, HLCD initially utilizes a constraint-based approach combined with the OR rule to obtain a candidate skeleton and then employs a score-based method to eliminate redundant portions in the candidate skeleton. Furthermore, during the local causal orientation phase, HLCD distinguishes between V-structures and equivalence classes by comparing the local structure scores between the two, thereby avoiding orientation interference caused by local equivalence classes. We conducted extensive experiments with seven state-of-the-art competitors on 14 benchmark Bayesian network datasets, and the experimental results demonstrate that HLCD significantly outperforms existing local causal discovery algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2412.19507v2),  [pdf](http://arxiv.org/pdf/2412.19507v2)

**Tags**: cs.AI 



### ConTextual: Improving Clinical Text Summarization in LLMs with   Context-preserving Token Filtering and Knowledge Graphs
**Authors**: Fahmida Liza Piya, Rahmatollah Beheshti

**Updated**: 2025-05-12T14:57:14Z

**Summary**: Unstructured clinical data can serve as a unique and rich source of information that can meaningfully inform clinical practice. Extracting the most pertinent context from such data is critical for exploiting its true potential toward optimal and timely decision-making in patient care. While prior research has explored various methods for clinical text summarization, most prior studies either process all input tokens uniformly or rely on heuristic-based filters, which can overlook nuanced clinical cues and fail to prioritize information critical for decision-making. In this study, we propose Contextual, a novel framework that integrates a Context-Preserving Token Filtering method with a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By preserving context-specific important tokens and enriching them with structured knowledge, ConTextual improves both linguistic coherence and clinical fidelity. Our extensive empirical evaluations on two public benchmark datasets demonstrate that ConTextual consistently outperforms other baselines. Our proposed approach highlights the complementary role of token-level filtering and structured retrieval in enhancing both linguistic and clinical integrity, as well as offering a scalable solution for improving precision in clinical text generation.

**Link**: [arxiv](http://arxiv.org/abs/2504.16394v2),  [pdf](http://arxiv.org/pdf/2504.16394v2)

**Tags**: cs.CL cs.AI 



### KAQG: A Knowledge-Graph-Enhanced RAG for Difficulty-Controlled Question   Generation
**Authors**: Ching Han Chen, Ming Fang Shiu

**Updated**: 2025-05-12T14:42:19Z

**Summary**: KAQG introduces a decisive breakthrough for Retrieval-Augmented Generation (RAG) by explicitly tackling the two chronic weaknesses of current pipelines: transparent multi-step reasoning and fine-grained cognitive difficulty control. This transforms RAG from a passive retriever into an accountable generator of calibrated exam items. Technically, the framework fuses knowledge graphs, RAG retrieval, and educational assessment theory into a single pipeline. Domain passages are parsed into a structured graph; graph-aware retrieval feeds fact chains to an LLM; and an assessment layer governed by Bloom's Taxonomy levels and Item Response Theory (IRT) transforms those chains into psychometrically sound questions. This cross-disciplinary marriage yields two scholarly contributions: it shows how semantic graph contexts guide LLM reasoning paths, and it operationalizes difficulty metrics within the generation process, producing items whose IRT parameters match expert benchmarks. Every module, from KG construction scripts to the multi-agent reasoning scheduler and the automatic IRT validator, is openly released on GitHub. This enables peer laboratories to replicate experiments, benchmark against baselines, and extend individual components without licensing barriers. Its reproducible design paves the way for rigorous ablation studies, cross-domain transfer experiments, and shared leaderboards on multi-step reasoning benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2505.07618v1),  [pdf](http://arxiv.org/pdf/2505.07618v1)

**Tags**: cs.IR 



### Cosmology with Supernova Encore in the lensing cluster MACS J0138$-$2155   -- Spectroscopy with MUSE
**Authors**: G. Granata, G. B. Caminha, S. Ertl, C. Grillo, S. Schuldt, S. H. Suyu, A. Acebron, P. Bergamini, R. Cañameras, A. M. Koekemoer, P. Rosati, S. Taubenberger

**Updated**: 2025-05-12T14:37:55Z

**Summary**: We present a spectroscopic analysis of MACS J0138$-$2155, at $z=0.336$, the first galaxy cluster hosting two strongly-lensed supernovae (SNe), Requiem and Encore, providing us with a chance to obtain a reliable $H_0$ measurement from the time delays between the multiple images. We take advantage of new data from the Multi Unit Spectroscopic Explorer (MUSE) on the Very Large Telescope, covering a central $1 \rm \, arcmin^2$ of the lensing cluster, for a total depth of 3.7 hours, including 2.9 hours recently obtained by our Target of Opportunity programme. Our new spectroscopic catalogue contains reliable redshifts for 107 objects, including 50 galaxy cluster members with secure redshift values in the range $0.324 < z < 0.349$, and 13 lensed multiple images from four background sources between $0.767\leq z \leq 3.420$, including four images of the host galaxy of the two SNe. We exploit the MUSE data to study the stellar kinematics of 14 bright cluster members and two background galaxies, obtaining reliable measurements of their line-of-sight velocity dispersion. Finally, we combine these results with measurements of the total magnitude of the cluster members in the Hubble Space Telescope F160W band to calibrate the Faber-Jackson relation between luminosity and stellar velocity dispersion ($L \propto \sigma^{1/\alpha}$) for the early-type cluster member galaxies, measuring a slope $\alpha=0.25^{+0.05}_{-0.05}$. A pure and complete sample of cluster member galaxies and a reliable characterisation of their total mass structure are key to building accurate total mass maps of the cluster, mitigating the impact of parametric degeneracies, which is necessary for inferring the value of $H_0$ from the measured time delays between the lensed images of the two SNe.

**Link**: [arxiv](http://arxiv.org/abs/2412.13250v3),  [pdf](http://arxiv.org/pdf/2412.13250v3)

**Tags**: astro-ph.GA astro-ph.CO 



### Diffused Responsibility: Analyzing the Energy Consumption of Generative   Text-to-Audio Diffusion Models
**Authors**: Riccardo Passoni, Francesca Ronchini, Luca Comanducci, Romain Serizel, Fabio Antonacci

**Updated**: 2025-05-12T14:36:47Z

**Summary**: Text-to-audio models have recently emerged as a powerful technology for generating sound from textual descriptions. However, their high computational demands raise concerns about energy consumption and environmental impact. In this paper, we conduct an analysis of the energy usage of 7 state-of-the-art text-to-audio diffusion-based generative models, evaluating to what extent variations in generation parameters affect energy consumption at inference time. We also aim to identify an optimal balance between audio quality and energy consumption by considering Pareto-optimal solutions across all selected models. Our findings provide insights into the trade-offs between performance and environmental impact, contributing to the development of more efficient generative audio models.

**Link**: [arxiv](http://arxiv.org/abs/2505.07615v1),  [pdf](http://arxiv.org/pdf/2505.07615v1)

**Tags**: eess.AS cs.AI cs.LG cs.SD 



### Unbiased Evaluation of Large Language Models from a Causal Perspective
**Authors**: Meilin Chen, Jian Tian, Liang Ma, Di Xie, Weijie Chen, Jiang Zhu

**Updated**: 2025-05-12T14:34:05Z

**Summary**: Benchmark contamination has become a significant concern in the LLM evaluation community. Previous Agents-as-an-Evaluator address this issue by involving agents in the generation of questions. Despite their success, the biases in Agents-as-an-Evaluator methods remain largely unexplored. In this paper, we present a theoretical formulation of evaluation bias, providing valuable insights into designing unbiased evaluation protocols. Furthermore, we identify two type of bias in Agents-as-an-Evaluator through carefully designed probing tasks on a minimal Agents-as-an-Evaluator setup. To address these issues, we propose the Unbiased Evaluator, an evaluation protocol that delivers a more comprehensive, unbiased, and interpretable assessment of LLMs.Extensive experiments reveal significant room for improvement in current LLMs. Additionally, we demonstrate that the Unbiased Evaluator not only offers strong evidence of benchmark contamination but also provides interpretable evaluation results.

**Link**: [arxiv](http://arxiv.org/abs/2502.06655v2),  [pdf](http://arxiv.org/pdf/2502.06655v2)

**Tags**: cs.AI 



### Concept-Level Explainability for Auditing & Steering LLM Responses
**Authors**: Kenza Amara, Rita Sevastjanova, Mennatallah El-Assady

**Updated**: 2025-05-12T14:31:51Z

**Summary**: As large language models (LLMs) become widely deployed, concerns about their safety and alignment grow. An approach to steer LLM behavior, such as mitigating biases or defending against jailbreaks, is to identify which parts of a prompt influence specific aspects of the model's output. Token-level attribution methods offer a promising solution, but still struggle in text generation, explaining the presence of each token in the output separately, rather than the underlying semantics of the entire LLM response. We introduce ConceptX, a model-agnostic, concept-level explainability method that identifies the concepts, i.e., semantically rich tokens in the prompt, and assigns them importance based on the outputs' semantic similarity. Unlike current token-level methods, ConceptX also offers to preserve context integrity through in-place token replacements and supports flexible explanation goals, e.g., gender bias. ConceptX enables both auditing, by uncovering sources of bias, and steering, by modifying prompts to shift the sentiment or reduce the harmfulness of LLM responses, without requiring retraining. Across three LLMs, ConceptX outperforms token-level methods like TokenSHAP in both faithfulness and human alignment. Steering tasks boost sentiment shift by 0.252 versus 0.131 for random edits and lower attack success rates from 0.463 to 0.242, outperforming attribution and paraphrasing baselines. While prompt engineering and self-explaining methods sometimes yield safer responses, ConceptX offers a transparent and faithful alternative for improving LLM safety and alignment, demonstrating the practical value of attribution-based explainability in guiding LLM behavior.

**Link**: [arxiv](http://arxiv.org/abs/2505.07610v1),  [pdf](http://arxiv.org/pdf/2505.07610v1)

**Tags**: cs.CL cs.AI 



### MiMo: Unlocking the Reasoning Potential of Language Model -- From   Pretraining to Posttraining
**Authors**: Xiaomi LLM-Core Team, :, Bingquan Xia, Bowen Shen, Cici, Dawei Zhu, Di Zhang, Gang Wang, Hailin Zhang, Huaqiu Liu, Jiebao Xiao, Jinhao Dong, Liang Zhao, Peidian Li, Peng Wang, Shihua Yu, Shimao Chen, Weikun Wang, Wenhan Ma, Xiangwei Deng, Yi Huang, Yifan Song, Zihan Jiang, Bowen Ye, Can Cai, Chenhong He, Dong Zhang, Duo Zhang, Guoan Wang, Hao Tian, Haochen Zhao, Heng Qu, Hongshen Xu, Jun Shi, Kainan Bao, QingKai Fang, Kang Zhou, Kangyang Zhou, Lei Li, Menghang Zhu, Nuo Chen, Qiantong Wang, Shaohui Liu, Shicheng Li, Shuhao Gu, Shuhuai Ren, Shuo Liu, Sirui Deng, Weiji Zhuang, Weiwei Lv, Wenyu Yang, Xin Zhang, Xing Yong, Xing Zhang, Xingchen Song, Xinzhe Xu, Xu Wang, Yihan Yan, Yu Tu, Yuanyuan Tian, Yudong Wang, Yue Yu, Zhenru Lin, Zhichao Song, Zihao Yue

**Updated**: 2025-05-12T14:30:11Z

**Summary**: We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model's reasoning potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional Multi-Token Prediction objective for enhanced performance and accelerated inference speed. During post-training, we curate a dataset of 130K verifiable mathematics and programming problems for reinforcement learning, integrating a test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and employing strategic data resampling to stabilize training. Extensive evaluations show that MiMo-7B-Base possesses exceptional reasoning potential, outperforming even much larger 32B models. The final RL-tuned model, MiMo-7B-RL, achieves superior performance on mathematics, code and general reasoning tasks, surpassing the performance of OpenAI o1-mini. The model checkpoints are available at https://github.com/xiaomimimo/MiMo.

**Link**: [arxiv](http://arxiv.org/abs/2505.07608v1),  [pdf](http://arxiv.org/pdf/2505.07608v1)

**Tags**: cs.CL cs.AI cs.LG 



### WISE 12 micron search for exozodi candidates within 10 parsecs
**Authors**: Dong Huang, Qiong Liu, Mark C. Wyatt, Grant M. Kennedy

**Updated**: 2025-05-12T14:25:10Z

**Summary**: The discovery of extra-terrestrial life is one of the ultimate goals for future exoplanet-seeking missions, with one major challenge being the presence of 'exozodiacal' dust near target stars or within their habitable zone. Therefore, it is critical to identify which stars possess exozodiacal dust and quantify their emission levels. In this study, we conducted a search for exozodi candidates within 10 parsecs using the Reyl'e sample. We performed proper motion calculations and cross-matched the sample with the WISE and 2MASS database, resulting in 339 preliminary target samples. We further analysed the infrared radiation characteristics of these targets, using spectral energy distribution (SED) fitting to predict photometric flux levels in the infrared and searching for 3sigma excesses in the WISE W3 band. During further selection processes, we applied various analysis methods to perform rigorous validation. We identified five exozodi candidates all of which are brown dwarfs (BDs). Given the clustering in candidate spectral types, we expect that these are not true exozodi candidates, rather the apparent excess arises from the inability of the BD photosphere models to accurately represent the SEDs of objects at the L-T transition. Indeed, for the object DENIS J025503.3-470049, excess is likely due to silicate clouds in the BD atmosphere. We suggest that a more stringent 5sigma excess is required to infer excess for this spectral type. The detection rate (0/339) in our sample shows that less than 1% M stars have exozodi above 21% excess levels. This is consistent with the rate of exozodi at similar level towards FGK stars in the Kennedy & Wyatt sample (25/24,174). We provide upper limits on the 12 micron exozodi emission for the sample, which is typically at 21% relative to the star. For most stars, in particular the low mass M stars, this is the first such upper limit in the literature.

**Link**: [arxiv](http://arxiv.org/abs/2505.07602v1),  [pdf](http://arxiv.org/pdf/2505.07602v1)

**Tags**: astro-ph.EP astro-ph.SR 



### Characterizing the Investigative Methods of Fictional Detectives with   Large Language Models
**Authors**: Edirlei Soares de Lima, Marco A. Casanova, Bruno Feijó, Antonio L. Furtado

**Updated**: 2025-05-12T14:24:58Z

**Summary**: Detective fiction, a genre defined by its complex narrative structures and character-driven storytelling, presents unique challenges for computational narratology, a research field focused on integrating literary theory into automated narrative generation. While traditional literary studies have offered deep insights into the methods and archetypes of fictional detectives, these analyses often focus on a limited number of characters and lack the scalability needed for the extraction of unique traits that can be used to guide narrative generation methods. In this paper, we present an AI-driven approach for systematically characterizing the investigative methods of fictional detectives. Our multi-phase workflow explores the capabilities of 15 Large Language Models (LLMs) to extract, synthesize, and validate distinctive investigative traits of fictional detectives. This approach was tested on a diverse set of seven iconic detectives - Hercule Poirot, Sherlock Holmes, William Murdoch, Columbo, Father Brown, Miss Marple, and Auguste Dupin - capturing the distinctive investigative styles that define each character. The identified traits were validated against existing literary analyses and further tested in a reverse identification phase, achieving an overall accuracy of 91.43%, demonstrating the method's effectiveness in capturing the distinctive investigative approaches of each detective. This work contributes to the broader field of computational narratology by providing a scalable framework for character analysis, with potential applications in AI-driven interactive storytelling and automated narrative generation.

**Link**: [arxiv](http://arxiv.org/abs/2505.07601v1),  [pdf](http://arxiv.org/pdf/2505.07601v1)

**Tags**: cs.CL cs.AI 



### NCL-UoR at SemEval-2025 Task 3: Detecting Multilingual Hallucination and   Related Observable Overgeneration Text Spans with Modified RefChecker and   Modified SeflCheckGPT
**Authors**: Jiaying Hong, Thanet Markchom, Jianfei Xu, Tong Wu, Huizhi Liang

**Updated**: 2025-05-12T14:24:25Z

**Summary**: SemEval-2025 Task 3 (Mu-SHROOM) focuses on detecting hallucinations in content generated by various large language models (LLMs) across multiple languages. This task involves not only identifying the presence of hallucinations but also pinpointing their specific occurrences. To tackle this challenge, this study introduces two methods: modified RefChecker and modified SelfCheckGPT. The modified RefChecker integrates prompt-based factual verification into References, structuring them as claim-based tests rather than single external knowledge sources. The modified SelfCheckGPT incorporates external knowledge to overcome its reliance on internal knowledge. In addition, both methods' original prompt designs are enhanced to identify hallucinated words within LLM-generated texts. Experimental results demonstrate the effectiveness of the approach, achieving a high ranking on the test dataset in detecting hallucinations across various languages, with an average IoU of 0.5310 and an average COR of 0.5669.

**Link**: [arxiv](http://arxiv.org/abs/2503.01921v2),  [pdf](http://arxiv.org/pdf/2503.01921v2)

**Tags**: cs.CL cs.AI 



### Reinforced Internal-External Knowledge Synergistic Reasoning for   Efficient Adaptive Search Agent
**Authors**: Ziyang Huang, Xiaowei Yuan, Yiming Ju, Jun Zhao, Kang Liu

**Updated**: 2025-05-12T14:21:57Z

**Summary**: Retrieval-augmented generation (RAG) is a common strategy to reduce hallucinations in Large Language Models (LLMs). While reinforcement learning (RL) can enable LLMs to act as search agents by activating retrieval capabilities, existing ones often underutilize their internal knowledge. This can lead to redundant retrievals, potential harmful knowledge conflicts, and increased inference latency. To address these limitations, an efficient and adaptive search agent capable of discerning optimal retrieval timing and synergistically integrating parametric (internal) and retrieved (external) knowledge is in urgent need. This paper introduces the Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA), which could indentify its own knowledge boundary and prioritize the utilization of internal knowledge, resorting to external search only when internal knowledge is deemed insufficient. This is achieved using a novel knowledge-boundary aware reward function and a knowledge-boundary aware training dataset. These are designed for internal-external knowledge synergy oriented RL, incentivizing the model to deliver accurate answers, minimize unnecessary retrievals, and encourage appropriate external searches when its own knowledge is lacking. Evaluations across multiple knowledge reasoning tasks demonstrate that IKEA significantly outperforms baseline methods, reduces retrieval frequency significantly, and exhibits robust generalization capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2505.07596v1),  [pdf](http://arxiv.org/pdf/2505.07596v1)

**Tags**: cs.CL cs.AI 



### LLMs Outperform Experts on Challenging Biology Benchmarks
**Authors**: Lennart Justen

**Updated**: 2025-05-12T14:17:41Z

**Summary**: This study systematically evaluates 27 frontier Large Language Models on eight biology benchmarks spanning molecular biology, genetics, cloning, virology, and biosecurity. Models from major AI developers released between November 2022 and April 2025 were assessed through ten independent runs per benchmark. The findings reveal dramatic improvements in biological capabilities. Top model performance increased more than 4-fold on the challenging text-only subset of the Virology Capabilities Test over the study period, with OpenAI's o3 now performing twice as well as expert virologists. Several models now match or exceed expert-level performance on other challenging benchmarks, including the biology subsets of GPQA and WMDP and LAB-Bench CloningScenarios. Contrary to expectations, chain-of-thought did not substantially improve performance over zero-shot evaluation, while extended reasoning features in o3-mini and Claude 3.7 Sonnet typically improved performance as predicted by inference scaling. Benchmarks such as PubMedQA and the MMLU and WMDP biology subsets exhibited performance plateaus well below 100%, suggesting benchmark saturation and errors in the underlying benchmark data. The analysis highlights the need for more sophisticated evaluation methodologies as AI systems continue to advance.

**Link**: [arxiv](http://arxiv.org/abs/2505.06108v2),  [pdf](http://arxiv.org/pdf/2505.06108v2)

**Tags**: cs.LG cs.AI q-bio.QM 



### A Multi-Dimensional Constraint Framework for Evaluating and Improving   Instruction Following in Large Language Models
**Authors**: Junjie Ye, Caishuang Huang, Zhuohan Chen, Wenjie Fu, Chenyuan Yang, Leyi Yang, Yilong Wu, Peng Wang, Meng Zhou, Xiaolong Yang, Tao Gui, Qi Zhang, Zhongchao Shi, Jianping Fan, Xuanjing Huang

**Updated**: 2025-05-12T14:16:55Z

**Summary**: Instruction following evaluates large language models (LLMs) on their ability to generate outputs that adhere to user-defined constraints. However, existing benchmarks often rely on templated constraint prompts, which lack the diversity of real-world usage and limit fine-grained performance assessment. To fill this gap, we propose a multi-dimensional constraint framework encompassing three constraint patterns, four constraint categories, and four difficulty levels. Building on this framework, we develop an automated instruction generation pipeline that performs constraint expansion, conflict detection, and instruction rewriting, yielding 1,200 code-verifiable instruction-following test samples. We evaluate 19 LLMs across seven model families and uncover substantial variation in performance across constraint forms. For instance, average performance drops from 77.67% at Level I to 32.96% at Level IV. Furthermore, we demonstrate the utility of our approach by using it to generate data for reinforcement learning, achieving substantial gains in instruction following without degrading general performance. In-depth analysis indicates that these gains stem primarily from modifications in the model's attention modules parameters, which enhance constraint recognition and adherence. Code and data are available in https://github.com/Junjie-Ye/MulDimIF.

**Link**: [arxiv](http://arxiv.org/abs/2505.07591v1),  [pdf](http://arxiv.org/pdf/2505.07591v1)

**Tags**: cs.CL cs.AI 



### Integrated Bayesian non-parametric spatial modeling for cross-sample   identification of spatially variable genes
**Authors**: Meng Zhou, Shuangge Ma, Mengyun Wu

**Updated**: 2025-05-12T14:14:27Z

**Summary**: Spatial transcriptomics has revolutionized tissue analysis by simultaneously mapping gene expression, spatial topography, and histological context across consecutive tissue sections, enabling systematic investigation of spatial heterogeneity. The detection of spatially variable (SV) genes, which are molecular signatures with position-dependent expression, provides critical insights into disease mechanisms spanning oncology, neurology, and cardiovascular research. Current methodologies, however, confront dual constraints: predominant reliance on predefined spatial pattern templates restricts detection of novel complex spatial architectures, and inconsistent sample selection strategies compromise analytical stability and biological interpretability. To overcome these challenges, we propose a novel Bayesian hierarchical framework incorporating non-parametric spatial modeling and across-sample integration. It takes advantage of the non-parametric technique and develops an adaptive spatial process accommodating complex pattern discovery while maintaining biological interpretability. A novel cross-sample bi-level shrinkage prior is further introduced for robust multi-sample SV gene detection, facilitating more effective information fusion. An efficient variational inference is developed for posterior inference ensuring computational scalability. Comprehensive simulations demonstrate the improved performance of our proposed method over existing analytical frameworks, and its application to DLPFC data reveals interpretable SV genes whose spatial patterns delineate neuroanatomically relevant clusters and gradients, advancing brain transcriptomics.

**Link**: [arxiv](http://arxiv.org/abs/2504.09654v2),  [pdf](http://arxiv.org/pdf/2504.09654v2)

**Tags**: stat.AP 



### Multi-mode Pulsations in AGB Stars: Insights from 3D RHD CO5BOLD   Simulations
**Authors**: Arief Ahmad, Bernd Freytag, Susanne Höfner

**Updated**: 2025-05-12T14:05:33Z

**Summary**: Stars on the AGB can exhibit acoustic pulsation modes of different radial orders, along with non-radial modes. These pulsations are essential to the mass-loss process and influence the evolutionary pathways of AGB stars. P-L relations serve as a valuable diagnostic for understanding stellar evolution along the AGB. 3D RHD simulations provide a powerful tool for investigating pulsation phenomena driven by convective processes and their non-linear coupling with stellar oscillations. We investigate multi-mode pulsations in AGB stars using advanced 3D 'star-in-a-box' simulations with the CO5BOLD code. Signatures of these multi-mode pulsations were weak in our previous 3D models. Our focus is on identifying and characterising the various pulsation modes, examining their persistence and transitions, and comparing the results with 1D model predictions and observational data where applicable. We produced a new model grid comprising AGB stars with current masses of $0.7$, $0.8$, and $1\,\mathrm{M}_{\odot}$. Fourier analysis was applied to dynamic, time-dependent quantities to extract dominant pulsation modes and their corresponding periods. Additionally, wavelet transforms were employed to identify mode-switching behaviour over time. The models successfully reproduce the P-L sequences found in AGB stars. Mode-switching phenomena are found in both the models and wavelet analyses of observational data, allowing us to infer similarities in the underlying pulsation dynamics. These 3D simulations highlight the natural emergence of multi-mode pulsations, including both radial and non-radial modes, driven by the self-consistent interplay of convection and oscillations. Our findings underscore the value of 3D RHD models in capturing the non-linear behaviour of AGB pulsations, providing insights into mode switching, envelope structures, and potential links to episodic mass-loss events.

**Link**: [arxiv](http://arxiv.org/abs/2502.11978v3),  [pdf](http://arxiv.org/pdf/2502.11978v3)

**Tags**: astro-ph.SR 



### YuLan-OneSim: Towards the Next Generation of Social Simulator with Large   Language Models
**Authors**: Lei Wang, Heyang Gao, Xiaohe Bo, Xu Chen, Ji-Rong Wen

**Updated**: 2025-05-12T14:05:17Z

**Summary**: Leveraging large language model (LLM) based agents to simulate human social behaviors has recently gained significant attention. In this paper, we introduce a novel social simulator called YuLan-OneSim. Compared to previous works, YuLan-OneSim distinguishes itself in five key aspects: (1) Code-free scenario construction: Users can simply describe and refine their simulation scenarios through natural language interactions with our simulator. All simulation code is automatically generated, significantly reducing the need for programming expertise. (2) Comprehensive default scenarios: We implement 50 default simulation scenarios spanning 8 domains, including economics, sociology, politics, psychology, organization, demographics, law, and communication, broadening access for a diverse range of social researchers. (3) Evolvable simulation: Our simulator is capable of receiving external feedback and automatically fine-tuning the backbone LLMs, significantly enhancing the simulation quality. (4) Large-scale simulation: By developing a fully responsive agent framework and a distributed simulation architecture, our simulator can handle up to 100,000 agents, ensuring more stable and reliable simulation results. (5) AI social researcher: Leveraging the above features, we develop an AI social researcher. Users only need to propose a research topic, and the AI researcher will automatically analyze the input, construct simulation environments, summarize results, generate technical reports, review and refine the reports--completing the social science research loop. To demonstrate the advantages of YuLan-OneSim, we conduct experiments to evaluate the quality of the automatically generated scenarios, the reliability, efficiency, and scalability of the simulation process, as well as the performance of the AI social researcher.

**Link**: [arxiv](http://arxiv.org/abs/2505.07581v1),  [pdf](http://arxiv.org/pdf/2505.07581v1)

**Tags**: cs.AI cs.CY 



### SCAM: A Real-World Typographic Robustness Evaluation for Multimodal   Foundation Models
**Authors**: Justus Westerhoff, Erblina Purelku, Jakob Hackstein, Jonas Loos, Leo Pinetzki, Lorenz Hufe

**Updated**: 2025-05-12T13:45:06Z

**Summary**: Typographic attacks exploit the interplay between text and visual content in multimodal foundation models, causing misclassifications when misleading text is embedded within images. However, existing datasets are limited in size and diversity, making it difficult to study such vulnerabilities. In this paper, we introduce SCAM, the largest and most diverse dataset of real-world typographic attack images to date, containing 1,162 images across hundreds of object categories and attack words. Through extensive benchmarking of Vision-Language Models (VLMs) on SCAM, we demonstrate that typographic attacks significantly degrade performance, and identify that training data and model architecture influence the susceptibility to these attacks. Our findings reveal that typographic attacks persist in state-of-the-art Large Vision-Language Models (LVLMs) due to the choice of their vision encoder, though larger Large Language Models (LLMs) backbones help mitigate their vulnerability. Additionally, we demonstrate that synthetic attacks closely resemble real-world (handwritten) attacks, validating their use in research. Our work provides a comprehensive resource and empirical insights to facilitate future research toward robust and trustworthy multimodal AI systems. We publicly release the datasets introduced in this paper along with the code for evaluations at www.bliss.berlin/research/scam.

**Link**: [arxiv](http://arxiv.org/abs/2504.04893v3),  [pdf](http://arxiv.org/pdf/2504.04893v3)

**Tags**: cs.CV cs.AI 



### Fine-scale opposite-polarity magnetic fields in a solar plage revealed   by integral field spectropolarimetry
**Authors**: G. Liu, I. Milić, J. S. Castellanos Duran, J. M. Borrero, M. van Noort, C. Kuckein

**Updated**: 2025-05-12T13:43:46Z

**Summary**: Plages are small concentrations of strong, nearly vertical magnetic fields in the solar photosphere that expand with height. A high spatial and spectral resolution that can resolve their fine structure is required to characterize them, and spectropolarimetric capabilities are needed to infer their magnetic fields. We constrain the 3D fine structure of the magnetic field in the photosphere of a solar plage from a unique spectropolarimetric dataset with a very high spatial and spectral resolution and a fast temporal cadence. We analyzed spectropolarimetric observations of a solar plage in the two magnetically sensitive spectral lines of neutral iron around 630 nm. The observations were obtained with MiHI, which is an integral field unit attached to the Swedish Solar Telescope. MiHI obtained diffraction-limited, high-cadence observations with high spectral fidelity. These observations were interpreted using the spectropolarimetric inversion with magnetohydrostatic constraints, which allowed us to recover the magnetic and thermodynamic structure of the plage on a geometrical scale. The inversion results reveal that the magnetic field can reach up to 2 kG and that it expands significantly from the deep to the mid-photosphere. Weaker (200 G), and very small (subarcsecond) vertical magnetic loops lie beneath this canopy, rooted in the photosphere. This novel picture of a solar plage, in which weak opposite-polarity field patches surround the main polarity, provides new insight into convection in strongly magnetized plasma.

**Link**: [arxiv](http://arxiv.org/abs/2505.07561v1),  [pdf](http://arxiv.org/pdf/2505.07561v1)

**Tags**: astro-ph.SR 



### Direct Density Ratio Optimization: A Statistically Consistent Approach   to Aligning Large Language Models
**Authors**: Rei Higuchi, Taiji Suzuki

**Updated**: 2025-05-12T13:36:25Z

**Summary**: Aligning large language models (LLMs) with human preferences is crucial for safe deployment, yet existing methods assume specific preference models like Bradley-Terry model. This assumption leads to statistical inconsistency, where more data doesn't guarantee convergence to true human preferences. To address this critical gap, we introduce a novel alignment method Direct Density Ratio Optimization (DDRO). DDRO directly estimates the density ratio between preferred and unpreferred output distributions, circumventing the need for explicit human preference modeling. We theoretically prove that DDRO is statistically consistent, ensuring convergence to the true preferred distribution as the data size grows, regardless of the underlying preference structure. Experiments demonstrate that DDRO achieves superior performance compared to existing methods on many major benchmarks. DDRO unlocks the potential for truly data-driven alignment, paving the way for more reliable and human-aligned LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2505.07558v1),  [pdf](http://arxiv.org/pdf/2505.07558v1)

**Tags**: cs.LG cs.CL stat.ML 



### Reheating ACTs on Starobinsky and Higgs inflation
**Authors**: D. S. Zharov, O. O. Sobol, S. I. Vilchinskii

**Updated**: 2025-05-12T13:32:13Z

**Summary**: In the recent sixth data release (DR6) of the Atacama Cosmology Telescope (ACT) collaboration, the value of $n_{\rm s}=0.9743 \pm 0.0034$ for the scalar spectral index is reported, which excludes the Starobinsky and Higgs inflationary models at $2\sigma$ level. In this paper, we perform a Bayesian inference of the parameters of the Starobinsky or Higgs inflationary model with non-instantaneous reheating using the Markov chain Monte Carlo method. For the analysis, we use observational data on the cosmic microwave background collected by the Planck and ACT collaborations and on baryonic acoustic oscillations from the DESI collaboration. The reheating stage is modelled by a single parameter $R_{\rm reh}$. Using the modified Boltzmann code CLASS and the cobaya software with the GetDist package, we perform a direct inference of the model parameter space and obtain their posterior distributions. Using the Kullback--Leibler divergence, we estimate the information gain from the data, yielding $2.52$ bits for the reheating parameter. Inclusion of the ACT DR6 data provides $75\%$ more information about the reheating stage compared to analysis without ACT data. We draw constraints on the reheating temperature and the average equation of state. While the former can vary within $10$ orders of magnitude, values in the $95\%$ credible interval indicate a sufficiently low reheating temperature; for the latter there is a clear preference for values greater than $0.5$, which means that the conventional equations of state for dust $\omega=0$ and relativistic matter $\omega=1/3$ are excluded with more than $2\sigma$ level of significance. However, there still is a big part of parameter space where Starobinsky and Higgs inflationary models exhibit a high degree of consistency with the latest observational data, particularly from ACT DR6. Therefore, it is premature to reject these models.

**Link**: [arxiv](http://arxiv.org/abs/2505.01129v2),  [pdf](http://arxiv.org/pdf/2505.01129v2)

**Tags**: astro-ph.CO gr-qc hep-ph 



### Self-Supervised Event Representations: Towards Accurate, Real-Time   Perception on SoC FPGAs
**Authors**: Kamil Jeziorek, Tomasz Kryjak

**Updated**: 2025-05-12T13:32:08Z

**Summary**: Event cameras offer significant advantages over traditional frame-based sensors. These include microsecond temporal resolution, robustness under varying lighting conditions and low power consumption. Nevertheless, the effective processing of their sparse, asynchronous event streams remains challenging. Existing approaches to this problem can be categorised into two distinct groups. The first group involves the direct processing of event data with neural models, such as Spiking Neural Networks or Graph Convolutional Neural Networks. However, this approach is often accompanied by a compromise in terms of qualitative performance. The second group involves the conversion of events into dense representations with handcrafted aggregation functions, which can boost accuracy at the cost of temporal fidelity. This paper introduces a novel Self-Supervised Event Representation (SSER) method leveraging Gated Recurrent Unit (GRU) networks to achieve precise per-pixel encoding of event timestamps and polarities without temporal discretisation. The recurrent layers are trained in a self-supervised manner to maximise the fidelity of event-time encoding. The inference is performed with event representations generated asynchronously, thus ensuring compatibility with high-throughput sensors. The experimental validation demonstrates that SSER outperforms aggregation-based baselines, achieving improvements of 2.4% mAP and 0.6% on the Gen1 and 1 Mpx object detection datasets. Furthermore, the paper presents the first hardware implementation of recurrent representation for event data on a System-on-Chip FPGA, achieving sub-microsecond latency and power consumption between 1-2 W, suitable for real-time, power-efficient applications. Code is available at https://github.com/vision-agh/RecRepEvent.

**Link**: [arxiv](http://arxiv.org/abs/2505.07556v1),  [pdf](http://arxiv.org/pdf/2505.07556v1)

**Tags**: cs.CV 



### Injecting Knowledge Graphs into Large Language Models
**Authors**: Erica Coppolillo

**Updated**: 2025-05-12T13:31:26Z

**Summary**: Integrating structured knowledge from Knowledge Graphs (KGs) into Large Language Models (LLMs) remains a key challenge for symbolic reasoning. Existing methods mainly rely on prompt engineering or fine-tuning, which lose structural fidelity or incur high computational costs. Building on recent encoding techniques which integrate graph embeddings within the LLM input as tokens, we extend this paradigm to the KG domain by leveraging Knowledge Graph Embedding (KGE) models, thus enabling graph-aware reasoning. Our approach is model-agnostic, resource-efficient, and compatible with any LLMs. Extensive experimentation on synthetic and real-world datasets shows that our method improves reasoning performance over established baselines, further achieving the best trade-off in terms of accuracy and efficiency against state-of-the-art LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2505.07554v1),  [pdf](http://arxiv.org/pdf/2505.07554v1)

**Tags**: cs.LG cs.IR 



### Towards Requirements Engineering for RAG Systems
**Authors**: Tor Sporsem, Rasmus Ulfsnes

**Updated**: 2025-05-12T13:30:44Z

**Summary**: This short paper explores how a maritime company develops and integrates large-language models (LLM). Specifically by looking at the requirements engineering for Retrieval Augmented Generation (RAG) systems in expert settings. Through a case study at a maritime service provider, we demonstrate how data scientists face a fundamental tension between user expectations of AI perfection and the correctness of the generated outputs. Our findings reveal that data scientists must identify context-specific "retrieval requirements" through iterative experimentation together with users because they are the ones who can determine correctness. We present an empirical process model describing how data scientists practically elicited these "retrieval requirements" and managed system limitations. This work advances software engineering knowledge by providing insights into the specialized requirements engineering processes for implementing RAG systems in complex domain-specific applications.

**Link**: [arxiv](http://arxiv.org/abs/2505.07553v1),  [pdf](http://arxiv.org/pdf/2505.07553v1)

**Tags**: cs.SE cs.AI 



### Automated Visual Attention Detection using Mobile Eye Tracking in   Behavioral Classroom Studies
**Authors**: Efe Bozkir, Christian Kosel, Tina Seidel, Enkelejda Kasneci

**Updated**: 2025-05-12T13:30:30Z

**Summary**: Teachers' visual attention and its distribution across the students in classrooms can constitute important implications for student engagement, achievement, and professional teacher training. Despite that, inferring the information about where and which student teachers focus on is not trivial. Mobile eye tracking can provide vital help to solve this issue; however, the use of mobile eye tracking alone requires a significant amount of manual annotations. To address this limitation, we present an automated processing pipeline concept that requires minimal manually annotated data to recognize which student the teachers focus on. To this end, we utilize state-of-the-art face detection models and face recognition feature embeddings to train face recognition models with transfer learning in the classroom context and combine these models with the teachers' gaze from mobile eye trackers. We evaluated our approach with data collected from four different classrooms, and our results show that while it is possible to estimate the visually focused students with reasonable performance in all of our classroom setups, U-shaped and small classrooms led to the best results with accuracies of approximately 0.7 and 0.9, respectively. While we did not evaluate our method for teacher-student interactions and focused on the validity of the technical approach, as our methodology does not require a vast amount of manually annotated data and offers a non-intrusive way of handling teachers' visual attention, it could help improve instructional strategies, enhance classroom management, and provide feedback for professional teacher development.

**Link**: [arxiv](http://arxiv.org/abs/2505.07552v1),  [pdf](http://arxiv.org/pdf/2505.07552v1)

**Tags**: cs.CV cs.AI cs.HC 



### GRADA: Graph-based Reranker against Adversarial Documents Attack
**Authors**: Jingjie Zheng, Aryo Pradipta Gema, Giwon Hong, Xuanli He, Pasquale Minervini, Youcheng Sun, Qiongkai Xu

**Updated**: 2025-05-12T13:27:35Z

**Summary**: Retrieval Augmented Generation (RAG) frameworks improve the accuracy of large language models (LLMs) by integrating external knowledge from retrieved documents, thereby overcoming the limitations of models' static intrinsic knowledge. However, these systems are susceptible to adversarial attacks that manipulate the retrieval process by introducing documents that are adversarial yet semantically similar to the query. Notably, while these adversarial documents resemble the query, they exhibit weak similarity to benign documents in the retrieval set. Thus, we propose a simple yet effective Graph-based Reranking against Adversarial Document Attacks (GRADA) framework aiming at preserving retrieval quality while significantly reducing the success of adversaries. Our study evaluates the effectiveness of our approach through experiments conducted on five LLMs: GPT-3.5-Turbo, GPT-4o, Llama3.1-8b, Llama3.1-70b, and Qwen2.5-7b. We use three datasets to assess performance, with results from the Natural Questions dataset demonstrating up to an 80% reduction in attack success rates while maintaining minimal loss in accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2505.07546v1),  [pdf](http://arxiv.org/pdf/2505.07546v1)

**Tags**: cs.IR cs.AI 



### Discrete Visual Tokens of Autoregression, by Diffusion, and for   Reasoning
**Authors**: Bohan Wang, Zhongqi Yue, Fengda Zhang, Shuo Chen, Li'an Bi, Junzhe Zhang, Xue Song, Kennard Yanting Chan, Jiachun Pan, Weijia Wu, Mingze Zhou, Wang Lin, Kaihang Pan, Saining Zhang, Liyu Jia, Wentao Hu, Wei Zhao, Hanwang Zhang

**Updated**: 2025-05-12T13:19:08Z

**Summary**: We completely discard the conventional spatial prior in image representation and introduce a novel discrete visual tokenizer: Self-consistency Tokenizer (Selftok). At its design core, we compose an autoregressive (AR) prior -- mirroring the causal structure of language -- into visual tokens by using the reverse diffusion process of image generation. The AR property makes Selftok fundamentally distinct from traditional spatial tokens in the following two key ways: - Selftok offers an elegant and minimalist approach to unify diffusion and AR for vision-language models (VLMs): By representing images with Selftok tokens, we can train a VLM using a purely discrete autoregressive architecture -- like that in LLMs -- without requiring additional modules or training objectives. - We theoretically show that the AR prior satisfies the Bellman equation, whereas the spatial prior does not. Therefore, Selftok supports reinforcement learning (RL) for visual generation with effectiveness comparable to that achieved in LLMs. Besides the AR property, Selftok is also a SoTA tokenizer that achieves a favorable trade-off between high-quality reconstruction and compression rate. We use Selftok to build a pure AR VLM for both visual comprehension and generation tasks. Impressively, without using any text-image training pairs, a simple policy gradient RL working in the visual tokens can significantly boost the visual generation benchmark, surpassing all the existing models by a large margin. Therefore, we believe that Selftok effectively addresses the long-standing challenge that visual tokens cannot support effective RL. When combined with the well-established strengths of RL in LLMs, this brings us one step closer to realizing a truly multimodal LLM. Project Page: https://selftok-team.github.io/report/.

**Link**: [arxiv](http://arxiv.org/abs/2505.07538v1),  [pdf](http://arxiv.org/pdf/2505.07538v1)

**Tags**: cs.CV 



### QuantX: A Framework for Hardware-Aware Quantization of Generative AI   Workloads
**Authors**: Khurram Mazher, Saad Bin Nasir

**Updated**: 2025-05-12T13:13:06Z

**Summary**: We present QuantX: a tailored suite of recipes for LLM and VLM quantization. It is capable of quantizing down to 3-bit resolutions with minimal loss in performance. The quantization strategies in QuantX take into account hardware-specific constraints to achieve efficient dequantization during inference ensuring flexible trade-off between runtime speed, memory requirement and model accuracy. Our results demonstrate that QuantX achieves performance within 6% of the unquantized model for LlaVa-v1.6 quantized down to 3-bits for multiple end user tasks and outperforms recently published state-of-the-art quantization techniques. This manuscript provides insights into the LLM quantization process that motivated the range of recipes and options that are incorporated in QuantX.

**Link**: [arxiv](http://arxiv.org/abs/2505.07531v1),  [pdf](http://arxiv.org/pdf/2505.07531v1)

**Tags**: cs.AI 



### TableCenterNet: A one-stage network for table structure recognition
**Authors**: Anyi Xiao, Cihui Yang

**Updated**: 2025-05-12T13:12:56Z

**Summary**: Table structure recognition aims to parse tables in unstructured data into machine-understandable formats. Recent methods address this problem through a two-stage process or optimized one-stage approaches. However, these methods either require multiple networks to be serially trained and perform more time-consuming sequential decoding, or rely on complex post-processing algorithms to parse the logical structure of tables. They struggle to balance cross-scenario adaptability, robustness, and computational efficiency. In this paper, we propose a one-stage end-to-end table structure parsing network called TableCenterNet. This network unifies the prediction of table spatial and logical structure into a parallel regression task for the first time, and implicitly learns the spatial-logical location mapping laws of cells through a synergistic architecture of shared feature extraction layers and task-specific decoding. Compared with two-stage methods, our method is easier to train and faster to infer. Experiments on benchmark datasets show that TableCenterNet can effectively parse table structures in diverse scenarios and achieve state-of-the-art performance on the TableGraph-24k dataset. Code is available at https://github.com/dreamy-xay/TableCenterNet.

**Link**: [arxiv](http://arxiv.org/abs/2504.17522v2),  [pdf](http://arxiv.org/pdf/2504.17522v2)

**Tags**: cs.CV 



### Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large   Language Models via a Multi-Paradigm Perspective
**Authors**: Yiyao Yu, Yuxiang Zhang, Dongdong Zhang, Xiao Liang, Hengyuan Zhang, Xingxing Zhang, Mahmoud Khademi, Hany Awadalla, Junjie Wang, Yujiu Yang, Furu Wei

**Updated**: 2025-05-12T13:04:44Z

**Summary**: Large Language Models (LLMs) have made notable progress in mathematical reasoning, yet often rely on single-paradigm reasoning, limiting their effectiveness across diverse tasks. We introduce Chain-of-Reasoning (CoR), a novel unified framework integrating multiple reasoning paradigms--Natural Language Reasoning (NLR), Algorithmic Reasoning (AR), and Symbolic Reasoning (SR)--to enable synergistic collaboration. CoR generates multiple potential answers via different reasoning paradigms and synthesizes them into a coherent final solution. We propose a Progressive Paradigm Training (PPT) strategy for models to progressively master these paradigms, leading to CoR-Math-7B. Experimental results demonstrate that CoR-Math-7B significantly outperforms current SOTA models, achieving up to a 41.0% absolute improvement over GPT-4o in theorem proving and a 15.0% improvement over RL-based methods on the MATH benchmark in arithmetic tasks. These results show the enhanced mathematical comprehension ability of our model, enabling zero-shot generalization across tasks.

**Link**: [arxiv](http://arxiv.org/abs/2501.11110v2),  [pdf](http://arxiv.org/pdf/2501.11110v2)

**Tags**: cs.CL 



### Byam: Fixing Breaking Dependency Updates with Large Language Models
**Authors**: Frank Reyes, May Mahmoud, Federico Bono, Sarah Nadi, Benoit Baudry, Martin Monperrus

**Updated**: 2025-05-12T13:03:26Z

**Summary**: Application Programming Interfaces (APIs) facilitate the integration of third-party dependencies within the code of client applications. However, changes to an API, such as deprecation, modification of parameter names or types, or complete replacement with a new API, can break existing client code. These changes are called breaking dependency updates; It is often tedious for API users to identify the cause of these breaks and update their code accordingly. In this paper, we explore the use of Large Language Models (LLMs) to automate client code updates in response to breaking dependency updates. We evaluate our approach on the BUMP dataset, a benchmark for breaking dependency updates in Java projects. Our approach leverages LLMs with advanced prompts, including information from the build process and from the breaking dependency analysis. We assess effectiveness at three granularity levels: at the build level, the file level, and the individual compilation error level. We experiment with five LLMs: Google Gemini-2.0 Flash, OpenAI GPT4o-mini, OpenAI o3-mini, Alibaba Qwen2.5-32b-instruct, and DeepSeek V3. Our results show that LLMs can automatically repair breaking updates. Among the considered models, OpenAI's o3-mini is the best, able to completely fix 27% of the builds when using prompts that include contextual information such as the buggy line, API differences, error messages, and step-by-step reasoning instructions. Also, it fixes 78% of the individual compilation errors. Overall, our findings demonstrate the potential for LLMs to fix compilation errors due to breaking dependency updates, supporting developers in their efforts to stay up-to-date with changes in their dependencies.

**Link**: [arxiv](http://arxiv.org/abs/2505.07522v1),  [pdf](http://arxiv.org/pdf/2505.07522v1)

**Tags**: cs.SE 



### Clickbait Detection via Large Language Models
**Authors**: Han Wang, Yi Zhu, Ye Wang, Yun Li, Yunhao Yuan, Jipeng Qiang

**Updated**: 2025-05-12T12:57:14Z

**Summary**: Clickbait, which aims to induce users with some surprising and even thrilling headlines for increasing click-through rates, permeates almost all online content publishers, such as news portals and social media. Recently, Large Language Models (LLMs) have emerged as a powerful instrument and achieved tremendous success in a series of NLP downstream tasks. However, it is not yet known whether LLMs can be served as a high-quality clickbait detection system. In this paper, we analyze the performance of LLMs in the few-shot and zero-shot scenarios on several English and Chinese benchmark datasets. Experimental results show that LLMs cannot achieve the best results compared to the state-of-the-art deep and fine-tuning PLMs methods. Different from human intuition, the experiments demonstrated that LLMs cannot make satisfied clickbait detection just by the headlines.

**Link**: [arxiv](http://arxiv.org/abs/2306.09597v4),  [pdf](http://arxiv.org/pdf/2306.09597v4)

**Tags**: cs.CL cs.AI 



### ToolACE-DEV: Self-Improving Tool Learning via Decomposition and   EVolution
**Authors**: Xu Huang, Weiwen Liu, Xingshan Zeng, Yuefeng Huang, Xinlong Hao, Yuxian Wang, Yirong Zeng, Chuhan Wu, Yasheng Wang, Ruiming Tang, Defu Lian

**Updated**: 2025-05-12T12:48:30Z

**Summary**: The tool-using capability of large language models (LLMs) enables them to access up-to-date external information and handle complex tasks. Current approaches to enhancing this capability primarily rely on distilling advanced models by data synthesis. However, this method incurs significant costs associated with advanced model usage and often results in data compatibility issues, led by the high discrepancy in the knowledge scope between the advanced model and the target model. To address these challenges, we propose ToolACE-DEV, a self-improving framework for tool learning. First, we decompose the tool-learning objective into sub-tasks that enhance basic tool-making and tool-using abilities. Then, we introduce a self-evolving paradigm that allows lightweight models to self-improve, reducing reliance on advanced LLMs. Extensive experiments validate the effectiveness of our approach across models of varying scales and architectures.

**Link**: [arxiv](http://arxiv.org/abs/2505.07512v1),  [pdf](http://arxiv.org/pdf/2505.07512v1)

**Tags**: cs.CL cs.AI 



### A Sliding Layer Merging Method for Efficient Depth-Wise Pruning in LLMs
**Authors**: Xuan Ding, Rui Sun, Yunjian Zhang, Xiu Yan, Yueqi Zhou, Kaihao Huang, Suzhong Fu, Chuanlong Xie, Yao Zhu

**Updated**: 2025-05-12T12:43:10Z

**Summary**: Compared to width-wise pruning, depth-wise pruning can significantly accelerate inference in resource-constrained scenarios. However, treating the entire Transformer layer as the minimum pruning unit may degrade model performance by indiscriminately discarding the entire information of the layer. This paper reveals the ``Patch-like'' feature relationship between layers in large language models by analyzing the correlation of the outputs of different layers in the reproducing kernel Hilbert space. Building on this observation, we propose a sliding layer merging method that dynamically selects and fuses consecutive layers from top to bottom according to a pre-defined similarity threshold, thereby simplifying the model structure while maintaining its performance. Extensive experiments on LLMs with various architectures and different parameter scales show that our method outperforms existing pruning techniques in both zero-shot inference performance and retraining recovery quality after pruning. In particular, in the experiment with 35% pruning on the Vicuna-7B model, our method achieved a 1.654% improvement in average performance on zero-shot tasks compared to the existing method. Moreover, we further reveal the potential of combining depth pruning with width pruning to enhance the pruning effect. Our codes are available at https://github.com/920927/SLM-a-sliding-layer-merging-method.

**Link**: [arxiv](http://arxiv.org/abs/2502.19159v2),  [pdf](http://arxiv.org/pdf/2502.19159v2)

**Tags**: cs.CV 



### Learning to Reason and Navigate: Parameter Efficient Action Planning   with Large Language Models
**Authors**: Bahram Mohammadi, Ehsan Abbasnejad, Yuankai Qi, Qi Wu, Anton Van Den Hengel, Javen Qinfeng Shi

**Updated**: 2025-05-12T12:38:20Z

**Summary**: The remote embodied referring expression (REVERIE) task requires an agent to navigate through complex indoor environments and localize a remote object specified by high-level instructions, such as "bring me a spoon", without pre-exploration. Hence, an efficient navigation plan is essential for the final success. This paper proposes a novel parameter-efficient action planner using large language models (PEAP-LLM) to generate a single-step instruction at each location. The proposed model consists of two modules, LLM goal planner (LGP) and LoRA action planner (LAP). Initially, LGP extracts the goal-oriented plan from REVERIE instructions, including the target object and room. Then, LAP generates a single-step instruction with the goal-oriented plan, high-level instruction, and current visual observation as input. PEAP-LLM enables the embodied agent to interact with LAP as the path planner on the fly. A simple direct application of LLMs hardly achieves good performance. Also, existing hard-prompt-based methods are error-prone in complicated scenarios and need human intervention. To address these issues and prevent the LLM from generating hallucinations and biased information, we propose a novel two-stage method for fine-tuning the LLM, consisting of supervised fine-tuning (STF) and direct preference optimization (DPO). SFT improves the quality of generated instructions, while DPO utilizes environmental feedback. Experimental results show the superiority of our proposed model on REVERIE compared to the previous state-of-the-art.

**Link**: [arxiv](http://arxiv.org/abs/2505.07500v1),  [pdf](http://arxiv.org/pdf/2505.07500v1)

**Tags**: cs.CV 



### Design-Based Inference under Random Potential Outcomes via Riesz   Representation
**Authors**: Yukai Yang

**Updated**: 2025-05-12T12:31:34Z

**Summary**: We introduce a general framework for design-based causal inference that accommodates random potential outcomes, thereby extending the classical Neyman-Rubin setup in which outcomes are treated as fixed. In our formulation, each unit's potential outcome is modelled as a function \( \tilde{y}_i(z, \omega) \), where \( \omega \) denotes latent randomness external to the treatment assignment. Building on recent work connecting design-based estimation with the Riesz representation theorem, we construct causal estimators by embedding potential outcomes in a Hilbert space and defining treatment effects as linear functionals. This approach yields unbiased and consistent estimators, even when outcomes exhibit random variation. The framework retains the key strength of design-based analysis, identification via a known randomisation scheme, while enabling inference in settings with outcome-level stochasticity. We establish large-sample properties under local dependence and propose plug-in variance estimators, including a correlation-based version that improves efficiency under sparse dependence. A simulation study illustrates the finite-sample behaviour of the estimator. Our results unify design-based reasoning with random outcome modelling, broadening the applicability of causal inference in complex experimental environments.

**Link**: [arxiv](http://arxiv.org/abs/2505.01324v4),  [pdf](http://arxiv.org/pdf/2505.01324v4)

**Tags**: stat.ME econ.EM math.ST stat.TH 62G20, 62K99, 62D05 



### A super-resolution reconstruction method for lightweight building images   based on an expanding feature modulation network
**Authors**: Yi Zhang, Ruonan Lin, Ang Ping

**Updated**: 2025-05-12T12:15:11Z

**Summary**: This study proposes a lightweight method for building image super-resolution using a Dilated Contextual Feature Modulation Network (DCFMN). The process includes obtaining high-resolution images, down-sampling them to low-resolution, enhancing the low-resolution images, constructing and training a lightweight network model, and generating super-resolution outputs. To address challenges such as regular textures and long-range dependencies in building images, the DCFMN integrates an expansion separable modulation unit and a local feature enhancement module. The former employs multiple expansion convolutions equivalent to a large kernel to efficiently aggregate multi-scale features while leveraging a simple attention mechanism for adaptivity. The latter encodes local features, mixes channel information, and ensures no additional computational burden during inference through reparameterization. This approach effectively resolves the limitations of existing lightweight super-resolution networks in modeling long-range dependencies, achieving accurate and efficient global feature modeling without increasing computational costs, and significantly improving both reconstruction quality and lightweight efficiency for building image super-resolution models.

**Link**: [arxiv](http://arxiv.org/abs/2503.13179v2),  [pdf](http://arxiv.org/pdf/2503.13179v2)

**Tags**: cs.CV 



### Web-Bench: A LLM Code Benchmark Based on Web Standards and Frameworks
**Authors**: Kai Xu, YiWei Mao, XinYi Guan, ZiLong Feng

**Updated**: 2025-05-12T12:06:23Z

**Summary**: The application of large language models (LLMs) in the field of coding is evolving rapidly: from code assistants, to autonomous coding agents, and then to generating complete projects through natural language. Early LLM code benchmarks primarily focused on code generation accuracy, but these benchmarks have gradually become saturated. Benchmark saturation weakens their guiding role for LLMs. For example, HumanEval Pass@1 has reached 99.4% and MBPP 94.2%. Among various attempts to address benchmark saturation, approaches based on software engineering have stood out, but the saturation of existing software engineering benchmarks is rapidly increasing. To address this, we propose a new benchmark, Web-Bench, which contains 50 projects, each consisting of 20 tasks with sequential dependencies. The tasks implement project features in sequence, simulating real-world human development workflows. When designing Web-Bench, we aim to cover the foundational elements of Web development: Web Standards and Web Frameworks. Given the scale and complexity of these projects, which were designed by engineers with 5 to 10 years of experience, each presents a significant challenge. On average, a single project takes 4 to 8 hours for a senior engineer to complete. On our given benchmark agent (Web-Agent), SOTA (Claude 3.7 Sonnet) achieves only 25.1% Pass@1, significantly lower (better) than SWE-Bench's Verified (65.4%) and Full (33.8%) scores. Finally, we discuss that in any development field, Standards and Frameworks represent foundational knowledge and efficiency tools, respectively, and LLMs require optimization tailored to them.

**Link**: [arxiv](http://arxiv.org/abs/2505.07473v1),  [pdf](http://arxiv.org/pdf/2505.07473v1)

**Tags**: cs.AI 



### A Survey on Collaborative Mechanisms Between Large and Small Language   Models
**Authors**: Yi Chen, JiaHao Zhao, HaoHao Han

**Updated**: 2025-05-12T11:48:42Z

**Summary**: Large Language Models (LLMs) deliver powerful AI capabilities but face deployment challenges due to high resource costs and latency, whereas Small Language Models (SLMs) offer efficiency and deployability at the cost of reduced performance. Collaboration between LLMs and SLMs emerges as a crucial paradigm to synergistically balance these trade-offs, enabling advanced AI applications, especially on resource-constrained edge devices. This survey provides a comprehensive overview of LLM-SLM collaboration, detailing various interaction mechanisms (pipeline, routing, auxiliary, distillation, fusion), key enabling technologies, and diverse application scenarios driven by on-device needs like low latency, privacy, personalization, and offline operation. While highlighting the significant potential for creating more efficient, adaptable, and accessible AI, we also discuss persistent challenges including system overhead, inter-model consistency, robust task allocation, evaluation complexity, and security/privacy concerns. Future directions point towards more intelligent adaptive frameworks, deeper model fusion, and expansion into multimodal and embodied AI, positioning LLM-SLM collaboration as a key driver for the next generation of practical and ubiquitous artificial intelligence.

**Link**: [arxiv](http://arxiv.org/abs/2505.07460v1),  [pdf](http://arxiv.org/pdf/2505.07460v1)

**Tags**: cs.AI cs.CL 



### Why Uncertainty Estimation Methods Fall Short in RAG: An Axiomatic   Analysis
**Authors**: Heydar Soudani, Evangelos Kanoulas, Faegheh Hasibi

**Updated**: 2025-05-12T11:47:42Z

**Summary**: Large Language Models (LLMs) are valued for their strong performance across various tasks, but they also produce inaccurate or misleading outputs. Uncertainty Estimation (UE) quantifies the model's confidence and helps users assess response reliability. However, existing UE methods have not been thoroughly examined in scenarios like Retrieval-Augmented Generation (RAG), where the input prompt includes non-parametric knowledge. This paper shows that current UE methods cannot reliably assess correctness in the RAG setting. We further propose an axiomatic framework to identify deficiencies in existing methods and guide the development of improved approaches. Our framework introduces five constraints that an effective UE method should meet after incorporating retrieved documents into the LLM's prompt. Experimental results reveal that no existing UE method fully satisfies all the axioms, explaining their suboptimal performance in RAG. We further introduce a simple yet effective calibration function based on our framework, which not only satisfies more axioms than baseline methods but also improves the correlation between uncertainty estimates and correctness.

**Link**: [arxiv](http://arxiv.org/abs/2505.07459v1),  [pdf](http://arxiv.org/pdf/2505.07459v1)

**Tags**: cs.IR 



### Can Generative AI agents behave like humans? Evidence from laboratory   market experiments
**Authors**: R. Maria del Rio-Chanona, Marco Pangallo, Cars Hommes

**Updated**: 2025-05-12T11:44:46Z

**Summary**: We explore the potential of Large Language Models (LLMs) to replicate human behavior in economic market experiments. Compared to previous studies, we focus on dynamic feedback between LLM agents: the decisions of each LLM impact the market price at the current step, and so affect the decisions of the other LLMs at the next step. We compare LLM behavior to market dynamics observed in laboratory settings and assess their alignment with human participants' behavior. Our findings indicate that LLMs do not adhere strictly to rational expectations, displaying instead bounded rationality, similarly to human participants. Providing a minimal context window i.e. memory of three previous time steps, combined with a high variability setting capturing response heterogeneity, allows LLMs to replicate broad trends seen in human experiments, such as the distinction between positive and negative feedback markets. However, differences remain at a granular level--LLMs exhibit less heterogeneity in behavior than humans. These results suggest that LLMs hold promise as tools for simulating realistic human behavior in economic contexts, though further research is needed to refine their accuracy and increase behavioral diversity.

**Link**: [arxiv](http://arxiv.org/abs/2505.07457v1),  [pdf](http://arxiv.org/pdf/2505.07457v1)

**Tags**: econ.GN cs.AI q-fin.EC 



### Fast radio bursts as a probe of gravity on cosmological scales
**Authors**: Dennis Neumann, Robert Reischke, Steffen Hagstotz, Hendrik Hildebrandt

**Updated**: 2025-05-12T11:39:55Z

**Summary**: We explore the potential for improving constraints on gravity by leveraging correlations in the dispersion measure derived from Fast Radio Bursts (FRBs) in combination with cosmic shear. Specifically, we focus on Horndeski gravity, inferring the kinetic braiding and Planck mass run rate from a stage-4 cosmic shear mock survey alongside a survey comprising $10^4$ FRBs. For the inference pipeline, we utilise the Boltzmann code \texttt{hi\_class} to predict the linear matter power spectrum in modified gravity scenarios, while non-linear corrections are obtained from the halo-model employed in \texttt{HMcode}, including feedback mechanisms. Our findings indicate that FRBs can disentangle degeneracies between baryonic feedback and cosmological parameters, as well as the mass of massive neutrinos. Since these parameters are also degenerate with modified gravity parameters, the inclusion of FRBs can enhance constraints on Horndeski parameters by up to $40$ percent, despite being a less significant measurement. Additionally, we apply our model to current FRB data and use the uncertainty in the $\mathrm{DM}-z$ relation to impose limits on gravity. However, due to the limited sample size of current data, constraints are predominantly influenced by theoretical priors. Despite this, our study demonstrates that FRBs will significantly augment the limited set of cosmological probes available, playing a critical role in providing alternative tests of feedback, cosmology, and gravity. All codes used in this work are made publically available.

**Link**: [arxiv](http://arxiv.org/abs/2409.11163v2),  [pdf](http://arxiv.org/pdf/2409.11163v2)

**Tags**: astro-ph.CO 



### How well do LLMs reason over tabular data, really?
**Authors**: Cornelius Wolff, Madelon Hulsebos

**Updated**: 2025-05-12T11:35:28Z

**Summary**: Large Language Models (LLMs) excel in natural language tasks, but less is known about their reasoning capabilities over tabular data. Prior analyses devise evaluation strategies that poorly reflect an LLM's realistic performance on tabular queries. Moreover, we have a limited understanding of the robustness of LLMs towards realistic variations in tabular inputs. Therefore, we ask: Can general-purpose LLMs reason over tabular data, really?, and focus on two questions 1) are tabular reasoning capabilities of general-purpose LLMs robust to real-world characteristics of tabular inputs, and 2) how can we realistically evaluate an LLM's performance on analytical tabular queries? Building on a recent tabular reasoning benchmark, we first surface shortcomings of its multiple-choice prompt evaluation strategy, as well as commonly used free-form text metrics such as SacreBleu and BERT-score. We show that an LLM-as-a-judge procedure yields more reliable performance insights and unveil a significant deficit in tabular reasoning performance of LLMs. We then extend the tabular inputs reflecting three common characteristics in practice: 1) missing values, 2) duplicate entities, and 3) structural variations. Experiments show that the tabular reasoning capabilities of general-purpose LLMs suffer from these variations, stressing the importance of improving their robustness for realistic tabular inputs.

**Link**: [arxiv](http://arxiv.org/abs/2505.07453v1),  [pdf](http://arxiv.org/pdf/2505.07453v1)

**Tags**: cs.AI 



### Transfer Learning with Foundational Models for Time Series Forecasting   using Low-Rank Adaptations
**Authors**: M. Germán-Morales, A. J. Rivera-Rivas, M. J. del Jesus Díaz, C. J. Carmona

**Updated**: 2025-05-12T11:26:58Z

**Summary**: Foundational Models are an emerging widely used technique of GenAI. These models are distinguished by their scalability and the ease with which they can be adapted through the exploitation of Transfer Learning. The availability of high computational power and large datasets have supported their development, achieving a high generalization capacity due to the enormous and heterogeneous amounts of data used in their initial training. These characteristics contribute to a solid base that can be adapted or adjusted to a wide range of tasks, increasing their applicability. This study proposes the methodology LLIAM, a straightforward adaptation of a kind of FM, Large Language Models, for the Time Series Forecasting task. An adequate time-series prompting schema and Low-Rank Adaptations are used to enhance the knowledge of the model with diverse time series datasets, known as the fine-tuning phase. A study divided in two stages has been performed for evaluating the effectiveness of the proposed methodology. Initially, a comparison was made between the performance of LLIAM and different state-of-the-art DL algorithms, including Recurrent Neural Networks and Temporal Convolutional Networks, as well as a LLM-based method, TimeLLM. Following this, a zero-shot study is presented in order to evaluate the generalization capacity of the proposed methodology with time series datasets from unknown domains not considered in the model training. The outcomes of this investigation demonstrate the efficacy of LLIAM, highlighting that this straightforward and general approach can attain competent results without the necessity for applying complex modifications. This work also encourages the use of available resources (such as these pre-trained models) and efficient fine-tuning techniques to avoid unnecessary and costly training, narrowing the gap between the goals of traditional AI and Green AI.

**Link**: [arxiv](http://arxiv.org/abs/2410.11539v3),  [pdf](http://arxiv.org/pdf/2410.11539v3)

**Tags**: cs.LG 



### Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI   Systems
**Authors**: Guangjing Wang, Ce Zhou, Yuanda Wang, Bocheng Chen, Hanqing Guo, Qiben Yan

**Updated**: 2025-05-12T11:25:11Z

**Summary**: As Artificial Intelligence (AI) systems increasingly underpin critical applications, from autonomous vehicles to biometric authentication, their vulnerability to transferable attacks presents a growing concern. These attacks, designed to generalize across instances, domains, models, tasks, modalities, or even hardware platforms, pose severe risks to security, privacy, and system integrity. This survey delivers the first comprehensive review of transferable attacks across seven major categories, including evasion, backdoor, data poisoning, model stealing, model inversion, membership inference, and side-channel attacks. We introduce a unified six-dimensional taxonomy: cross-instance, cross-domain, cross-modality, cross-model, cross-task, and cross-hardware, which systematically captures the diverse transfer pathways of adversarial strategies. Through this framework, we examine both the underlying mechanics and practical implications of transferable attacks on AI systems. Furthermore, we review cutting-edge methods for enhancing attack transferability, organized around data augmentation and optimization strategies. By consolidating fragmented research and identifying critical future directions, this work provides a foundational roadmap for understanding, evaluating, and defending against transferable threats in real-world AI systems.

**Link**: [arxiv](http://arxiv.org/abs/2311.11796v2),  [pdf](http://arxiv.org/pdf/2311.11796v2)

**Tags**: cs.CR cs.AI cs.CL cs.CV 



### The Energy Cost of Artificial Intelligence Lifecycle in Communication   Networks
**Authors**: Shih-Kai Chou, Jernej Hribar, Vid Hanžel, Mihael Mohorčič, Carolina Fortuna

**Updated**: 2025-05-12T11:18:06Z

**Summary**: Artificial Intelligence (AI) is being incorporated in several optimization, scheduling, orchestration as well as in native communication network functions. While this paradigm shift results in increased energy consumption, quantifying the end-toend energy consumption of adding intelligence to such systems is particularly challenging. Conventional metrics focus on either communication, computation infrastructure, or model development. To address this, we propose a new metric, the Energy Cost of AI Lifecycle (eCAL) of one AI model in a system. eCAL captures the energy consumption throughout the development and deployment of an AI-model providing intelligence in a wireless communication network by analyzing the complexity of data collection and manipulation in individual components and deriving overall and per-bit energy consumption. We show that the better a model is and the more it is used, the more energy efficient an inference is. For a simple case study, eCAL for making 100 inferences is 2.73 times higher than for 1000 inferences. Additionally, we have developed a modular and extendable opensource simulation tool to enable researchers, practitioners, and engineers to calculate the end-to-end energy cost with various configurations and across various systems, ensuring adaptability to diverse use cases.

**Link**: [arxiv](http://arxiv.org/abs/2408.00540v3),  [pdf](http://arxiv.org/pdf/2408.00540v3)

**Tags**: cs.ET cs.AI cs.LG 



### Isotropy Test with Quasars Using Method of Smoothed Residuals
**Authors**: Akhil Antony, Stephen Appleby, William L Matthewson, Arman Shafieloo

**Updated**: 2025-05-12T11:01:47Z

**Summary**: To assess the significance and scale dependence of anomalous large scale modes in the CatWISE quasar data, we generate smoothed number density fields on the sphere and study their extreme values -- maximum, minimum, maximum antipodal difference. By comparing these summary statistics to those obtained from random isotropic realisations of the data, we determine the statistical significance of large scale modes as a function of smoothing scale. We perform our analysis using five different versions of the data -- the original quasar map, the maps after separately subtracting the ecliptic bias and the CMB dipole, the map obtained after subtracting both, and the map after subtracting the ecliptic bias and anomalous dipole inferred in \cite{Secrest2021}. We find that the ecliptic-corrected, CMB dipole-removed map exhibits large scale modes that are in tension with random realisations of the data (p-values $p \sim 10^{-4}$), over a wide range of smoothing scales $\pi/8 \leq \delta \leq \pi/2$. The most prominent feature in the data is an underdensity in the southern galactic plane at $(b,\ell) = (-31^\circ,78^\circ)$, which reaches its highest statistical significance when smoothed on scales $\delta = \pi/6$ ($p \ll 10^{-5}$). Notably, the minima statistics align with the maximum antipodal difference statistics, whereas the maxima do not. This suggests that the observed dipole-like behavior in the data is primarily driven by the underdensity in the southern sky. The ecliptic corrected, anomalous dipole subtracted map reduces the significance of any residual anisotropic features, but an underdensity in the south sky persists with p-value $p =0.0018$.

**Link**: [arxiv](http://arxiv.org/abs/2505.07439v1),  [pdf](http://arxiv.org/pdf/2505.07439v1)

**Tags**: astro-ph.CO hep-ph 



### LEAD: Iterative Data Selection for Efficient LLM Instruction Tuning
**Authors**: Xiaotian Lin, Yanlin Qi, Yizhang Zhu, Themis Palpanas, Chengliang Chai, Nan Tang, Yuyu Luo

**Updated**: 2025-05-12T10:57:51Z

**Summary**: Instruction tuning has emerged as a critical paradigm for improving the capabilities and alignment of large language models (LLMs). However, existing iterative model-aware data selection methods incur significant computational overhead, as they rely on repeatedly performing full-dataset model inference to estimate sample utility for subsequent training iterations, creating a fundamental efficiency bottleneck. In this paper, we propose LEAD, an efficient iterative data selection framework that accurately estimates sample utility entirely within the standard training loop, eliminating the need for costly additional model inference. At its core, LEAD introduces Instance-Level Dynamic Uncertainty (IDU), a theoretically grounded utility function combining instantaneous training loss, gradient-based approximation of loss changes, and exponential smoothing of historical loss signals. To further scale efficiently to large datasets, LEAD employs a two-stage, coarse-to-fine selection strategy, adaptively prioritizing informative clusters through a multi-armed bandit mechanism, followed by precise fine-grained selection of high-utility samples using IDU. Extensive experiments across four diverse benchmarks show that LEAD significantly outperforms state-of-the-art methods, improving average model performance by 6.1%-10.8% while using only 2.5% of the training data and reducing overall training time by 5-10x.

**Link**: [arxiv](http://arxiv.org/abs/2505.07437v1),  [pdf](http://arxiv.org/pdf/2505.07437v1)

**Tags**: cs.LG cs.AI cs.DB 



### I Predict Therefore I Am: Is Next Token Prediction Enough to Learn   Human-Interpretable Concepts from Data?
**Authors**: Yuhang Liu, Dong Gong, Yichao Cai, Erdun Gao, Zhen Zhang, Biwei Huang, Mingming Gong, Anton van den Hengel, Javen Qinfeng Shi

**Updated**: 2025-05-12T10:45:23Z

**Summary**: The remarkable achievements of large language models (LLMs) have led many to conclude that they exhibit a form of intelligence. This is as opposed to explanations of their capabilities based on their ability to perform relatively simple manipulations of vast volumes of data. To illuminate the distinction between these explanations, we introduce a novel generative model that generates tokens on the basis of human-interpretable concepts represented as latent discrete variables. Under mild conditions, even when the mapping from the latent space to the observed space is non-invertible, we establish an identifiability result, i.e., the representations learned by LLMs through next-token prediction can be approximately modeled as the logarithm of the posterior probabilities of these latent discrete concepts given input context, up to an invertible linear transformation. This theoretical finding not only provides evidence that LLMs capture underlying generative factors, but also provide a unified prospective for understanding of the linear representation hypothesis. Taking this a step further, our finding motivates a reliable evaluation of sparse autoencoders by treating the performance of supervised concept extractors as an upper bound. Pushing this idea even further, it inspires a structural variant that enforces dependence among latent concepts in addition to promoting sparsity. Empirically, we validate our theoretical results through evaluations on both simulation data and the Pythia, Llama, and DeepSeek model families, and demonstrate the effectiveness of our structured sparse autoencoder.

**Link**: [arxiv](http://arxiv.org/abs/2503.08980v3),  [pdf](http://arxiv.org/pdf/2503.08980v3)

**Tags**: cs.LG cs.CL 



### Methods for robustly measuring the minimum spanning tree and other field   level statistics from galaxy surveys
**Authors**: Krishna Naidoo, Ofer Lahav

**Updated**: 2025-05-12T10:44:51Z

**Summary**: Field level statistics, such as the minimum spanning tree (MST), have been shown to be a promising tool for parameter inference in cosmology. However, applications to real galaxy surveys are challenging, due to the presence of small scale systematic effects and non-trivial survey selection functions. Since many field level statistics are 'hard-wired', the common practice is to forward model survey systematic effects to synthetic galaxy catalogues. However, this can be computationally demanding and produces results that are a product of cosmology and systematic effects, making it difficult to directly compare results from different experiments. We introduce a method for inverting survey systematic effects through a Monte Carlo subsampling technique where galaxies are assigned probabilities based on their galaxy weight and survey selection functions. Small scale systematic effects are mitigated through the addition of a point-process smoothing technique called jittering. The inversion technique removes the requirement for a computational and labour intensive forward modelling pipeline for parameter inference. We demonstrate that jittering can mask small scale theoretical uncertainties and survey systematic effects like fibre collisions and we show that Monte Carlo subsampling can remove the effects of survey selection functions. We outline how to measure field level statistics from future surveys.

**Link**: [arxiv](http://arxiv.org/abs/2410.06202v2),  [pdf](http://arxiv.org/pdf/2410.06202v2)

**Tags**: astro-ph.CO 



### LLM-assisted Mutation for Whitebox API Testing
**Authors**: Jia Li, Jiacheng Shen, Yuxin Su, Michael R. Lyu

**Updated**: 2025-05-12T10:31:30Z

**Summary**: Cloud applications heavily rely on APIs to communicate with each other and exchange data. To ensure the reliability of cloud applications, cloud providers widely adopt API testing techniques. Unfortunately, existing API testing approaches are insufficient to reach strict conditions, a problem known as fitness plateaus, due to the lack of gradient provided by coverage metrics. To address this issue, we propose MioHint, a novel white-box API testing approach that leverages the code comprehension capabilities of Large Language Model (LLM) to boost API testing. The key challenge of LLM-based API testing lies in system-level testing, which emphasizes the dependencies between requests and targets across functions and files, thereby making the entire codebase the object of analysis. However, feeding the entire codebase to an LLM is impractical due to its limited context length and short memory. MioHint addresses this challenge by synergizing static analysis with LLMs. We retrieve relevant code with data-dependency analysis at the statement level, including def-use analysis for variables used in the target and function expansion for subfunctions called by the target.   To evaluate the effectiveness of our method, we conducted experiments across 16 real-world REST API services. The findings reveal that MioHint achieves an average increase of 4.95% absolute in line coverage compared to the baseline, EvoMaster, alongside a remarkable factor of 67x improvement in mutation accuracy. Furthermore, our method successfully covers over 57% of hard-to-cover targets while in baseline the coverage is less than 10%.

**Link**: [arxiv](http://arxiv.org/abs/2504.05738v2),  [pdf](http://arxiv.org/pdf/2504.05738v2)

**Tags**: cs.SE 



### None of the Others: a General Technique to Distinguish Reasoning from   Memorization in Multiple-Choice LLM Evaluation Benchmarks
**Authors**: Eva Sánchez Salido, Julio Gonzalo, Guillermo Marco

**Updated**: 2025-05-12T10:30:51Z

**Summary**: In LLM evaluations, reasoning is often distinguished from recall/memorization by performing numerical variations to math-oriented questions. Here we introduce a general variation method for multiple-choice questions that completely dissociates the correct answer from previously seen tokens or concepts, requiring LLMs to understand and reason (rather than memorizing) in order to answer correctly. Using this method, we evaluate state-of-the-art proprietary and open-source LLMs on two datasets available in English and Spanish: the public MMLU benchmark and the private UNED-Access 2024 dataset. Results show that all models experience remarkable accuracy drops under our proposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access 2024, ranging from 10% to 93% across models. Notably, the most accurate model in our experimentation (OpenAI-o3-mini) is not the most robust (DeepSeek-R1-70B), suggesting that the best models in standard evaluations may not be the ones with better reasoning capabilities. Also, we see larger accuracy drops in public (vs private) datasets and questions posed in their original language (vs a manual translation), which are signs of contamination and also point to a relevant role of recall/memorization in current LLMs' answers.

**Link**: [arxiv](http://arxiv.org/abs/2502.12896v3),  [pdf](http://arxiv.org/pdf/2502.12896v3)

**Tags**: cs.CL 



### FullStack Bench: Evaluating LLMs as Full Stack Coders
**Authors**: Bytedance-Seed-Foundation-Code-Team, :, Yao Cheng, Jianfeng Chen, Jie Chen, Li Chen, Liyu Chen, Wentao Chen, Zhengyu Chen, Shijie Geng, Aoyan Li, Bo Li, Bowen Li, Linyi Li, Boyi Liu, Jiaheng Liu, Kaibo Liu, Qi Liu, Shukai Liu, Siyao Liu, Tianyi Liu, Tingkai Liu, Yongfei Liu, Rui Long, Jing Mai, Guanghan Ning, Z. Y. Peng, Kai Shen, Jiahao Su, Jing Su, Tao Sun, Yifan Sun, Yunzhe Tao, Guoyin Wang, Siwei Wang, Xuwu Wang, Yite Wang, Zihan Wang, Jinxiang Xia, Liang Xiang, Xia Xiao, Yongsheng Xiao, Chenguang Xi, Shulin Xin, Jingjing Xu, Shikun Xu, Hongxia Yang, Jack Yang, Yingxiang Yang, Jianbo Yuan, Jun Zhang, Yufeng Zhang, Yuyu Zhang, Shen Zheng, He Zhu, Ming Zhu

**Updated**: 2025-05-12T10:24:14Z

**Summary**: As the capabilities of code large language models (LLMs) continue to expand, their applications across diverse code intelligence domains are rapidly increasing. However, most existing datasets only evaluate limited application domains. To address this gap, we have developed a comprehensive code evaluation dataset FullStack Bench focusing on full-stack programming, which encompasses a wide range of application domains (e.g., basic programming, data analysis, software engineering, mathematics, and machine learning). Besides, to assess multilingual programming capabilities, in FullStack Bench, we design real-world instructions and corresponding unit test cases from 16 widely-used programming languages to reflect real-world usage scenarios rather than simple translations. Moreover, we also release an effective code sandbox execution tool (i.e., SandboxFusion) supporting various programming languages and packages to evaluate the performance of our FullStack Bench efficiently. Comprehensive experimental results on our FullStack Bench demonstrate the necessity and effectiveness of our FullStack Bench and SandboxFusion.

**Link**: [arxiv](http://arxiv.org/abs/2412.00535v6),  [pdf](http://arxiv.org/pdf/2412.00535v6)

**Tags**: cs.AI cs.SE 



### LA-IMR: Latency-Aware, Predictive In-Memory Routing and Proactive   Autoscaling for Tail-Latency-Sensitive Cloud Robotics
**Authors**: Eunil Seo, Chanh Nguyen, Erik Elmroth

**Updated**: 2025-05-12T10:12:24Z

**Summary**: Hybrid cloud-edge infrastructures now support latency-critical workloads ranging from autonomous vehicles and surgical robotics to immersive AR/VR. However, they continue to experience crippling long-tail latency spikes whenever bursty request streams exceed the capacity of heterogeneous edge and cloud tiers. To address these long-tail latency issues, we present Latency-Aware, Predictive In-Memory Routing and Proactive Autoscaling (LA-IMR). This control layer integrates a closed-form, utilization-driven latency model with event-driven scheduling, replica autoscaling, and edge-to-cloud offloading to mitigate 99th-percentile (P99) delays. Our analytic model decomposes end-to-end latency into processing, network, and queuing components, expressing inference latency as an affine power-law function of instance utilization. Once calibrated, it produces two complementary functions that drive: (i) millisecond-scale routing decisions for traffic offloading, and (ii) capacity planning that jointly determines replica pool sizes. LA-IMR enacts these decisions through a quality-differentiated, multi-queue scheduler and a custom-metric Kubernetes autoscaler that scales replicas proactively -- before queues build up -- rather than reactively based on lagging CPU metrics. Across representative vision workloads (YOLOv5m and EfficientDet) and bursty arrival traces, LA-IMR reduces P99 latency by up to 20.7 percent compared to traditional latency-only autoscaling, laying a principled foundation for next-generation, tail-tolerant cloud-edge inference services.

**Link**: [arxiv](http://arxiv.org/abs/2505.07417v1),  [pdf](http://arxiv.org/pdf/2505.07417v1)

**Tags**: cs.DC 



### Computational Fact-Checking of Online Discourse: Scoring scientific   accuracy in climate change related news articles
**Authors**: Tim Wittenborg, Constantin Sebastian Tremel, Markus Stocker, Sören Auer

**Updated**: 2025-05-12T10:03:15Z

**Summary**: Democratic societies need reliable information. Misinformation in popular media such as news articles or videos threatens to impair civic discourse. Citizens are, unfortunately, not equipped to verify this content flood consumed daily at increasing rates. This work aims to semi-automatically quantify scientific accuracy of online media. By semantifying media of unknown veracity, their statements can be compared against equally processed trusted sources. We implemented a workflow using LLM-based statement extraction and knowledge graph analysis. Our neurosymbolic system was able to evidently streamline state-of-the-art veracity quantification. Evaluated via expert interviews and a user survey, the tool provides a beneficial veracity indication. This indicator, however, is unable to annotate public media at the required granularity and scale. Further work towards a FAIR (Findable, Accessible, Interoperable, Reusable) ground truth and complementary metrics are required to scientifically support civic discourse.

**Link**: [arxiv](http://arxiv.org/abs/2505.07409v1),  [pdf](http://arxiv.org/pdf/2505.07409v1)

**Tags**: cs.CL 



### Causality-enhanced Decision-Making for Autonomous Mobile Robots in   Dynamic Environments
**Authors**: Luca Castri, Gloria Beraldo, Nicola Bellotto

**Updated**: 2025-05-12T09:35:07Z

**Summary**: The growing integration of robots in shared environments -- such as warehouses, shopping centres, and hospitals -- demands a deep understanding of the underlying dynamics and human behaviours, including how, when, and where individuals engage in various activities and interactions. This knowledge goes beyond simple correlation studies and requires a more comprehensive causal analysis. By leveraging causal inference to model cause-and-effect relationships, we can better anticipate critical environmental factors and enable autonomous robots to plan and execute tasks more effectively. To this end, we propose a novel causality-based decision-making framework that reasons over a learned causal model to predict battery usage and human obstructions, understanding how these factors could influence robot task execution. Such reasoning framework assists the robot in deciding when and how to complete a given task. To achieve this, we developed also PeopleFlow, a new Gazebo-based simulator designed to model context-sensitive human-robot spatial interactions in shared workspaces. PeopleFlow features realistic human and robot trajectories influenced by contextual factors such as time, environment layout, and robot state, and can simulate a large number of agents. While the simulator is general-purpose, in this paper we focus on a warehouse-like environment as a case study, where we conduct an extensive evaluation benchmarking our causal approach against a non-causal baseline. Our findings demonstrate the efficacy of the proposed solutions, highlighting how causal reasoning enables autonomous robots to operate more efficiently and safely in dynamic environments shared with humans.

**Link**: [arxiv](http://arxiv.org/abs/2504.11901v3),  [pdf](http://arxiv.org/pdf/2504.11901v3)

**Tags**: cs.RO cs.AI 



### Process-Supervised LLM Recommenders via Flow-guided Tuning
**Authors**: Chongming Gao, Mengyao Gao, Chenxiao Fan, Shuai Yuan, Wentao Shi, Xiangnan He

**Updated**: 2025-05-12T09:24:03Z

**Summary**: While large language models (LLMs) are increasingly adapted for recommendation systems via supervised fine-tuning (SFT), this approach amplifies popularity bias due to its likelihood maximization objective, compromising recommendation diversity and fairness. To address this, we present Flow-guided fine-tuning recommender (Flower), which replaces SFT with a Generative Flow Network (GFlowNet) framework that enacts process supervision through token-level reward propagation. Flower's key innovation lies in decomposing item-level rewards into constituent token rewards, enabling direct alignment between token generation probabilities and their reward signals. This mechanism achieves three critical advancements: (1) popularity bias mitigation and fairness enhancement through empirical distribution matching, (2) preservation of diversity through GFlowNet's proportional sampling, and (3) flexible integration of personalized preferences via adaptable token rewards. Experiments demonstrate Flower's superior distribution-fitting capability and its significant advantages over traditional SFT in terms of accuracy, fairness, and diversity, highlighting its potential to improve LLM-based recommendation systems. The implementation is available via https://github.com/MrPeach0301/Flower

**Link**: [arxiv](http://arxiv.org/abs/2503.07377v3),  [pdf](http://arxiv.org/pdf/2503.07377v3)

**Tags**: cs.IR 



### Amortized Safe Active Learning for Real-Time Data Acquisition:   Pretrained Neural Policies from Simulated Nonparametric Functions
**Authors**: Cen-You Li, Marc Toussaint, Barbara Rakitsch, Christoph Zimmer

**Updated**: 2025-05-12T09:21:39Z

**Summary**: Safe active learning (AL) is a sequential scheme for learning unknown systems while respecting safety constraints during data acquisition. Existing methods often rely on Gaussian processes (GPs) to model the task and safety constraints, requiring repeated GP updates and constrained acquisition optimization-incurring in significant computations which are challenging for real-time decision-making. We propose an amortized safe AL framework that replaces expensive online computations with a pretrained neural policy. Inspired by recent advances in amortized Bayesian experimental design, we turn GPs into a pretraining simulator. We train our policy prior to the AL deployment on simulated nonparametric functions, using Fourier feature-based GP sampling and a differentiable, safety-aware acquisition objective. At deployment, our policy selects safe and informative queries via a single forward pass, eliminating the need for GP inference or constrained optimization. This leads to substantial speed improvements while preserving safety and learning quality. Our framework is modular and can be adapted to unconstrained, time-sensitive AL tasks by omitting the safety requirement.

**Link**: [arxiv](http://arxiv.org/abs/2501.15458v2),  [pdf](http://arxiv.org/pdf/2501.15458v2)

**Tags**: cs.LG 



### Examining the Role of LLM-Driven Interactions on Attention and Cognitive   Engagement in Virtual Classrooms
**Authors**: Suleyman Ozdel, Can Sarpkaya, Efe Bozkir, Hong Gao, Enkelejda Kasneci

**Updated**: 2025-05-12T09:21:19Z

**Summary**: Transforming educational technologies through the integration of large language models (LLMs) and virtual reality (VR) offers the potential for immersive and interactive learning experiences. However, the effects of LLMs on user engagement and attention in educational environments remain open questions. In this study, we utilized a fully LLM-driven virtual learning environment, where peers and teachers were LLM-driven, to examine how students behaved in such settings. Specifically, we investigate how peer question-asking behaviors influenced student engagement, attention, cognitive load, and learning outcomes and found that, in conditions where LLM-driven peer learners asked questions, students exhibited more targeted visual scanpaths, with their attention directed toward the learning content, particularly in complex subjects. Our results suggest that peer questions did not introduce extraneous cognitive load directly, as the cognitive load is strongly correlated with increased attention to the learning material. Considering these findings, we provide design recommendations for optimizing VR learning spaces.

**Link**: [arxiv](http://arxiv.org/abs/2505.07377v1),  [pdf](http://arxiv.org/pdf/2505.07377v1)

**Tags**: cs.HC cs.AI 



### A Preliminary Study of Large Language Models for Multilingual   Vulnerability Detection
**Authors**: Junji Yu, Honglin Shu, Michael Fu, Dong Wang, Chakkrit Tantithamthavorn, Yasutaka Kamei, Junjie Chen

**Updated**: 2025-05-12T09:19:31Z

**Summary**: Deep learning-based approaches, particularly those leveraging pre-trained language models (PLMs), have shown promise in automated software vulnerability detection. However, existing methods are predominantly limited to specific programming languages, restricting their applicability in multilingual settings. Recent advancements in large language models (LLMs) offer language-agnostic capabilities and enhanced semantic understanding, presenting a potential solution to this limitation. While existing studies have explored LLMs for vulnerability detection, their detection performance remains unknown for multilingual vulnerabilities. To address this gap, we conducted a preliminary study to evaluate the effectiveness of PLMs and state-of-the-art LLMs across seven popular programming languages. Our findings reveal that the PLM CodeT5P achieves the best performance in multilingual vulnerability detection, particularly in identifying the most critical vulnerabilities. Based on these results, we further discuss the potential of LLMs in advancing real-world multilingual vulnerability detection. This work represents an initial step toward exploring PLMs and LLMs for cross-language vulnerability detection, offering key insights for future research and practical deployment.

**Link**: [arxiv](http://arxiv.org/abs/2505.07376v1),  [pdf](http://arxiv.org/pdf/2505.07376v1)

**Tags**: cs.SE 



## Keyword: LLM Deployment 
 ### WATCH: Adaptive Monitoring for AI Deployments via Weighted-Conformal   Martingales
**Authors**: Drew Prinster, Xing Han, Anqi Liu, Suchi Saria

**Updated**: 2025-05-12T17:56:52Z

**Summary**: Responsibly deploying artificial intelligence (AI) / machine learning (ML) systems in high-stakes settings arguably requires not only proof of system reliability, but moreover continual, post-deployment monitoring to quickly detect and address any unsafe behavior. Statistical methods for nonparametric change-point detection -- especially the tools of conformal test martingales (CTMs) and anytime-valid inference -- offer promising approaches to this monitoring task. However, existing methods are restricted to monitoring limited hypothesis classes or ``alarm criteria'' (such as data shifts that violate certain exchangeability assumptions), do not allow for online adaptation in response to shifts, and/or do not enable root-cause analysis of any degradation. In this paper, we expand the scope of these monitoring methods by proposing a weighted generalization of conformal test martingales (WCTMs), which lay a theoretical foundation for online monitoring for any unexpected changepoints in the data distribution while controlling false-alarms. For practical applications, we propose specific WCTM algorithms that adapt online to mild covariate shifts (in the marginal input distribution) while quickly detecting and diagnosing more severe shifts, such as concept shifts (in the conditional label distribution) or extreme (out-of-support) covariate shifts that cannot be easily adapted to. On real-world datasets, we demonstrate improved performance relative to state-of-the-art baselines.

**Link**: [arxiv](http://arxiv.org/abs/2505.04608v2),  [pdf](http://arxiv.org/pdf/2505.04608v2)

**Tags**: cs.LG cs.AI stat.ML 



### AcoustoBots: A swarm of robots for acoustophoretic multimodal   interactions
**Authors**: Narsimlu Kemsaram, James Hardwick, Jincheng Wang, Bonot Gautam, Ceylan Besevli, Giorgos Christopoulos, Sourabh Dogra, Lei Gao, Akin Delibasi, Diego Martinez Plasencia, Orestis Georgiou, Marianna Obrist, Ryuji Hirayama, Sriram Subramanian

**Updated**: 2025-05-12T17:56:07Z

**Summary**: Acoustophoresis has enabled novel interaction capabilities, such as levitation, volumetric displays, mid-air haptic feedback, and directional sound generation, to open new forms of multimodal interactions. However, its traditional implementation as a singular static unit limits its dynamic range and application versatility. This paper introduces AcoustoBots - a novel convergence of acoustophoresis with a movable and reconfigurable phased array of transducers for enhanced application versatility. We mount a phased array of transducers on a swarm of robots to harness the benefits of multiple mobile acoustophoretic units. This offers a more flexible and interactive platform that enables a swarm of acoustophoretic multimodal interactions. Our novel AcoustoBots design includes a hinge actuation system that controls the orientation of the mounted phased array of transducers to achieve high flexibility in a swarm of acoustophoretic multimodal interactions. In addition, we designed a BeadDispenserBot that can deliver particles to trapping locations, which automates the acoustic levitation interaction. These attributes allow AcoustoBots to independently work for a common cause and interchange between modalities, allowing for novel augmentations (e.g., a swarm of haptics, audio, and levitation) and bilateral interactions with users in an expanded interaction area. We detail our design considerations, challenges, and methodological approach to extend acoustophoretic central control in distributed settings. This work demonstrates a scalable acoustic control framework with two mobile robots, laying the groundwork for future deployment in larger robotic swarms. Finally, we characterize the performance of our AcoustoBots and explore the potential interactive scenarios they can enable.

**Link**: [arxiv](http://arxiv.org/abs/2505.07808v1),  [pdf](http://arxiv.org/pdf/2505.07808v1)

**Tags**: cs.RO 



### COCOA: a compact Compton camera for astrophysical observation of   MeV-scale gamma rays
**Authors**: LiquidO Collaboration, S. R. Soleti, J. J. Gómez-Cadenas, J. Apilluelo, L. Asquith, E. F. Bannister, N. P. Barradas, C. L. Baylis, J. L. Beney, M. Berberan e Santos, X. de la Bernardie, T. J. C. Bezerra, M. Bongrand, C. Bourgeois, D. Breton, J. Busto, K. Burns, A. Cabrera, A. Cadiou, E. Calvo, M. de Carlos Generowicz, E. Chauveau, B. J. Cattermole, M. Chen, P. Chimenti, D. F. Cowen, S. Kr. Das, S. Dusini, A. Earle, M. Felizardo, C. Frigerio Martins, J. Galán, J. A. García, R. Gazzini, A. Gibson-Foster, C. Girard-Carillo, W. C. Griffith, M. Guitière, F. Haddad, J. Hartnell, A. Holin, I. G. Irastorza, I. Jovanovic, A. Kling, L. Koch, P. Lasorak, J. F. Le Du, F. Lefevre, P. Loaiza, J. A. Lock, G. Luzón, J. Maalmi, J. P. Malhado, F. Mantovani, J. G. Marques, C. Marquet, M. Martínez, D. Navas-Nicolás, H. Nunokawa, J. P. Ochoa-Ricoux, T. Palmeira, C. Palomares, D. Petyt, P. Pillot, A. Pin, J. C. C. Porter, M. S. Pravikoff, S. Richards, N. Rodrigues, M. Roche, R. Rosero, B. Roskovec, N. Roy, M. L. Sarsa, A. Serafini, C. Shepherd-Themistocleous, W. Shorrock, M. Silva, L. Simard, D. Stocco, V. Strati, J. S. Stutzmann, F. Suekane, N. Tuccori, A. Verdugo, B. Viaud, S. M. Wakely, G. Wendel, A. S. Wilhelm, A. W. R. Wong, M. Yeh, F. Yermia

**Updated**: 2025-05-12T17:52:35Z

**Summary**: COCOA (COmpact COmpton cAmera) is a next-generation gamma-ray telescope designed for astrophysical observations in the MeV energy range. The detector comprises a scatterer volume employing the LiquidO detection technology and an array of scintillating crystals acting as absorber. Surrounding plastic scintillator panels serve as a veto system for charged particles. The detector's compact, scalable design enables flexible deployment on microsatellites or high-altitude balloons. Gamma rays at MeV energies have not been well explored historically (the so-called "MeV gap") and COCOA has the potential to improve the sensitivity in this energy band.

**Link**: [arxiv](http://arxiv.org/abs/2502.20916v3),  [pdf](http://arxiv.org/pdf/2502.20916v3)

**Tags**: astro-ph.IM physics.ins-det 



### Understanding Stragglers in Large Model Training Using What-if Analysis
**Authors**: Jinkun Lin, Ziheng Jiang, Zuquan Song, Sida Zhao, Menghan Yu, Zhanghan Wang, Chenyuan Wang, Zuocheng Shi, Xiang Shi, Wei Jia, Zherui Liu, Shuguang Wang, Haibin Lin, Xin Liu, Aurojit Panda, Jinyang Li

**Updated**: 2025-05-12T17:52:35Z

**Summary**: Large language model (LLM) training is one of the most demanding distributed computations today, often requiring thousands of GPUs with frequent synchronization across machines. Such a workload pattern makes it susceptible to stragglers, where the training can be stalled by few slow workers. At ByteDance we find stragglers are not trivially always caused by hardware failures, but can arise from multiple complex factors. This work aims to present a comprehensive study on the straggler issues in LLM training, using a five-month trace collected from our ByteDance LLM training cluster. The core methodology is what-if analysis that simulates the scenario without any stragglers and contrasts with the actual case. We use this method to study the following questions: (1) how often do stragglers affect training jobs, and what effect do they have on job performance; (2) do stragglers exhibit temporal or spatial patterns; and (3) what are the potential root causes for stragglers?

**Link**: [arxiv](http://arxiv.org/abs/2505.05713v2),  [pdf](http://arxiv.org/pdf/2505.05713v2)

**Tags**: cs.DC cs.LG 



### A Theoretical Framework for Explaining Reinforcement Learning with   Shapley Values
**Authors**: Daniel Beechey, Thomas M. S. Smith, Özgür Şimşek

**Updated**: 2025-05-12T17:48:28Z

**Summary**: Reinforcement learning agents can achieve superhuman performance, but their decisions are often difficult to interpret. This lack of transparency limits deployment, especially in safety-critical settings where human trust and accountability are essential. In this work, we develop a theoretical framework for explaining reinforcement learning through the influence of state features, which represent what the agent observes in its environment. We identify three core elements of the agent-environment interaction that benefit from explanation: behaviour (what the agent does), performance (what the agent achieves), and value estimation (what the agent expects to achieve). We treat state features as players cooperating to produce each element and apply Shapley values, a principled method from cooperative game theory, to identify the influence of each feature. This approach yields a family of mathematically grounded explanations with clear semantics and theoretical guarantees. We use illustrative examples to show how these explanations align with human intuition and reveal novel insights. Our framework unifies and extends prior work, making explicit the assumptions behind existing approaches, and offers a principled foundation for more interpretable and trustworthy reinforcement learning.

**Link**: [arxiv](http://arxiv.org/abs/2505.07797v1),  [pdf](http://arxiv.org/pdf/2505.07797v1)

**Tags**: cs.LG 



### Overflow Prevention Enhances Long-Context Recurrent LLMs
**Authors**: Assaf Ben-Kish, Itamar Zimerman, M. Jehanzeb Mirza, James Glass, Leonid Karlinsky, Raja Giryes

**Updated**: 2025-05-12T17:45:05Z

**Summary**: A recent trend in LLMs is developing recurrent sub-quadratic models that improve long-context processing efficiency. We investigate leading large long-context models, focusing on how their fixed-size recurrent memory affects their performance. Our experiments reveal that, even when these models are trained for extended contexts, their use of long contexts remains underutilized. Specifically, we demonstrate that a chunk-based inference procedure, which identifies and processes only the most relevant portion of the input can mitigate recurrent memory failures and be effective for many long-context tasks: On LongBench, our method improves the overall performance of Falcon3-Mamba-Inst-7B by 14%, Falcon-Mamba-Inst-7B by 28%, RecurrentGemma-IT-9B by 50%, and RWKV6-Finch-7B by 51%. Surprisingly, this simple approach also leads to state-of-the-art results in the challenging LongBench v2 benchmark, showing competitive performance with equivalent size Transformers. Furthermore, our findings raise questions about whether recurrent models genuinely exploit long-range dependencies, as our single-chunk strategy delivers stronger performance - even in tasks that presumably require cross-context relations.

**Link**: [arxiv](http://arxiv.org/abs/2505.07793v1),  [pdf](http://arxiv.org/pdf/2505.07793v1)

**Tags**: cs.LG cs.AI 



### Domain Regeneration: How well do LLMs match syntactic properties of text   domains?
**Authors**: Da Ju, Hagen Blix, Adina Williams

**Updated**: 2025-05-12T17:37:17Z

**Summary**: Recent improvement in large language model performance have, in all likelihood, been accompanied by improvement in how well they can approximate the distribution of their training data. In this work, we explore the following question: which properties of text domains do LLMs faithfully approximate, and how well do they do so? Applying observational approaches familiar from corpus linguistics, we prompt a commonly used, opensource LLM to regenerate text from two domains of permissively licensed English text which are often contained in LLM training data -- Wikipedia and news text. This regeneration paradigm allows us to investigate whether LLMs can faithfully match the original human text domains in a fairly semantically-controlled setting. We investigate varying levels of syntactic abstraction, from more simple properties like sentence length, and article readability, to more complex and higher order properties such as dependency tag distribution, parse depth, and parse complexity. We find that the majority of the regenerated distributions show a shifted mean, a lower standard deviation, and a reduction of the long tail, as compared to the human originals.

**Link**: [arxiv](http://arxiv.org/abs/2505.07784v1),  [pdf](http://arxiv.org/pdf/2505.07784v1)

**Tags**: cs.CL 



### Relative Overfitting and Accept-Reject Framework
**Authors**: Yanxin Liu, Yunqi Zhang

**Updated**: 2025-05-12T17:36:14Z

**Summary**: Currently, the scaling law of Large Language Models (LLMs) faces challenges and bottlenecks. This paper posits that noise effects, stemming from changes in the signal-to-noise ratio under diminishing marginal returns, are the root cause of these issues. To control this noise, we investigated the differences between models with performance advantages and disadvantages, introducing the concept of "relative overfitting." Based on their complementary strengths, we have proposed an application framework, Accept-Reject (AR). In Natural Language Processing (NLP), we use LLMs and Small Language Models (SLMs) as the medium for discussion. This framework enables SLMs to exert a universal positive influence on LLM decision outputs, rather than the intuitively expected negative influence. We validated our approach using self-built models based on mainstream architectures and pre-trained mainstream models across multiple datasets, including basic language modeling, long-context tasks, subject examination, and question-answering (QA) benchmarks. The results demonstrate that through our structure, compared to increasing the LLM's parameters, we can achieve better performance improvements with significantly lower parameter and computational costs in many scenarios. These improvements are universal, stable, and effective. Furthermore, we explore the potential of "relative overfitting" and the AR framework in other machine learning domains, such as computer vision (CV) and AI for science. We hope the proposed approach can help scale laws overcome existing bottlenecks.

**Link**: [arxiv](http://arxiv.org/abs/2505.07783v1),  [pdf](http://arxiv.org/pdf/2505.07783v1)

**Tags**: cs.LG 



### MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine   Learning Engineering
**Authors**: Rushi Qiang, Yuchen Zhuang, Yinghao Li, Dingu Sagar V K, Rongzhi Zhang, Changhao Li, Ian Shu-Hei Wong, Sherry Yang, Percy Liang, Chao Zhang, Bo Dai

**Updated**: 2025-05-12T17:35:43Z

**Summary**: We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement learning, evaluating, and improving autonomous large language model (LLM) agents in iterative machine learning engineering (MLE) workflows. Unlike existing benchmarks that primarily rely on static datasets or single-attempt evaluations, MLE-Dojo provides an interactive environment enabling agents to iteratively experiment, debug, and refine solutions through structured feedback loops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse, open-ended MLE tasks carefully curated to reflect realistic engineering scenarios such as data processing, architecture search, hyperparameter tuning, and code debugging. Its fully executable environment supports comprehensive agent training via both supervised fine-tuning and reinforcement learning, facilitating iterative experimentation, realistic data sampling, and real-time outcome verification. Extensive evaluations of eight frontier LLMs reveal that while current models achieve meaningful iterative improvements, they still exhibit significant limitations in autonomously generating long-horizon solutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's flexible and extensible architecture seamlessly integrates diverse data sources, tools, and evaluation protocols, uniquely enabling model-based agent tuning and promoting interoperability, scalability, and reproducibility. We open-source our framework and benchmarks to foster community-driven innovation towards next-generation MLE agents.

**Link**: [arxiv](http://arxiv.org/abs/2505.07782v1),  [pdf](http://arxiv.org/pdf/2505.07782v1)

**Tags**: cs.LG 



### Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for   Mathematical Problem Solving
**Authors**: Xinji Mai, Haotian Xu, Xing W, Weinong Wang, Yingying Zhang, Wenqiang Zhang

**Updated**: 2025-05-12T17:23:34Z

**Summary**: Large Language Models (LLMs) often struggle with mathematical reasoning tasks requiring precise, verifiable computation. While Reinforcement Learning (RL) from outcome-based rewards enhances text-based reasoning, understanding how agents autonomously learn to leverage external tools like code execution remains crucial. We investigate RL from outcome-based rewards for Tool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously generate and execute Python code for mathematical problems without supervised tool-use examples. Our central contribution is we demonstrate that as RL training progresses, key metrics scale predictably. Specifically, we observe strong positive correlations where increased training steps lead to increases in the spontaneous code execution frequency, the average response length, and, critically, the final task accuracy. This suggests a quantifiable relationship between computational effort invested in training and the emergence of effective, tool-augmented reasoning strategies. We implement a robust framework featuring a decoupled code execution environment and validate our findings across standard RL algorithms and frameworks. Experiments show ZeroTIR significantly surpasses non-tool ZeroRL baselines on challenging math benchmarks. Our findings provide a foundational understanding of how autonomous tool use is acquired and scales within Agent RL, offering a reproducible benchmark for future studies. Code is released at \href{https://github.com/Anonymize-Author/AgentRL}{https://github.com/Anonymize-Author/AgentRL}.

**Link**: [arxiv](http://arxiv.org/abs/2505.07773v1),  [pdf](http://arxiv.org/pdf/2505.07773v1)

**Tags**: cs.AI 



### Enhancing Code Generation via Bidirectional Comment-Level Mutual   Grounding
**Authors**: Yifeng Di, Tianyi Zhang

**Updated**: 2025-05-12T17:20:30Z

**Summary**: Large Language Models (LLMs) have demonstrated unprecedented capability in code generation. However, LLM-generated code is still plagued with a wide range of functional errors, especially for complex programming tasks that LLMs have not seen before. Recent studies have shown that developers often struggle with inspecting and fixing incorrect code generated by LLMs, diminishing their productivity and trust in LLM-based code generation. Inspired by the mutual grounding theory in communication, we propose an interactive approach that leverages code comments as a medium for developers and LLMs to establish a shared understanding. Our approach facilitates iterative grounding by interleaving code generation, inline comment generation, and contextualized user feedback through editable comments to align generated code with developer intent. We evaluated our approach on two popular benchmarks and demonstrated that our approach significantly improved multiple state-of-the-art LLMs, e.g., 17.1% pass@1 improvement for code-davinci-002 on HumanEval. Furthermore, we conducted a user study with 12 participants in comparison to two baselines: (1) interacting with GitHub Copilot, and (2) interacting with a multi-step code generation paradigm called Multi-Turn Program Synthesis. Participants completed the given programming tasks 16.7% faster and with 10.5% improvement in task success rate when using our approach. Both results show that interactively refining code comments enables the collaborative establishment of mutual grounding, leading to more accurate code generation and higher developer confidence.

**Link**: [arxiv](http://arxiv.org/abs/2505.07768v1),  [pdf](http://arxiv.org/pdf/2505.07768v1)

**Tags**: cs.SE cs.AI cs.CL 



### When Near Becomes Far: From Rayleigh to Optimal Near-Field and Far-Field   Boundaries
**Authors**: Sajad Daei, Gabor Fodor, Mikael Skoglund

**Updated**: 2025-05-12T16:51:22Z

**Summary**: The transition toward 6G is pushing wireless communication into a regime where the classical plane-wave assumption no longer holds. Millimeter-wave and sub-THz frequencies shrink wavelengths to millimeters, while meter-scale arrays featuring hundreds of antenna elements dramatically enlarge the aperture. Together, these trends collapse the classical Rayleigh far-field boundary from kilometers to mere single-digit meters. Consequently, most practical 6G indoor, vehicular, and industrial deployments will inherently operate within the radiating near-field, where reliance on the plane-wave approximation leads to severe array-gain losses, degraded localization accuracy, and excessive pilot overhead. This paper re-examines the fundamental question: Where does the far-field truly begin? Rather than adopting purely geometric definitions, we introduce an application-oriented approach based on user-defined error budgets and a rigorous Fresnel-zone analysis that fully accounts for both amplitude and phase curvature. We propose three practical mismatch metrics: worst-case element mismatch, worst-case normalized mean square error, and spectral efficiency loss. For each metric, we derive a provably optimal transition distance--the minimal range beyond which mismatch permanently remains below a given tolerance--and provide closed-form solutions. Extensive numerical evaluations across diverse frequencies and antenna-array dimensions show that our proposed thresholds can exceed the Rayleigh distance by more than an order of magnitude. By transforming the near-field from a design nuisance into a precise, quantifiable tool, our results provide a clear roadmap for enabling reliable and resource-efficient near-field communications and sensing in emerging 6G systems.

**Link**: [arxiv](http://arxiv.org/abs/2505.07743v1),  [pdf](http://arxiv.org/pdf/2505.07743v1)

**Tags**: eess.SP 



### LAMM-ViT: AI Face Detection via Layer-Aware Modulation of Region-Guided   Attention
**Authors**: Jiangling Zhang, Weijie Zhu, Jirui Huang, Yaxiong Chen

**Updated**: 2025-05-12T16:42:19Z

**Summary**: Detecting AI-synthetic faces presents a critical challenge: it is hard to capture consistent structural relationships between facial regions across diverse generation techniques. Current methods, which focus on specific artifacts rather than fundamental inconsistencies, often fail when confronted with novel generative models. To address this limitation, we introduce Layer-aware Mask Modulation Vision Transformer (LAMM-ViT), a Vision Transformer designed for robust facial forgery detection. This model integrates distinct Region-Guided Multi-Head Attention (RG-MHA) and Layer-aware Mask Modulation (LAMM) components within each layer. RG-MHA utilizes facial landmarks to create regional attention masks, guiding the model to scrutinize architectural inconsistencies across different facial areas. Crucially, the separate LAMM module dynamically generates layer-specific parameters, including mask weights and gating values, based on network context. These parameters then modulate the behavior of RG-MHA, enabling adaptive adjustment of regional focus across network depths. This architecture facilitates the capture of subtle, hierarchical forgery cues ubiquitous among diverse generation techniques, such as GANs and Diffusion Models. In cross-model generalization tests, LAMM-ViT demonstrates superior performance, achieving 94.09% mean ACC (a +5.45% improvement over SoTA) and 98.62% mean AP (a +3.09% improvement). These results demonstrate LAMM-ViT's exceptional ability to generalize and its potential for reliable deployment against evolving synthetic media threats.

**Link**: [arxiv](http://arxiv.org/abs/2505.07734v1),  [pdf](http://arxiv.org/pdf/2505.07734v1)

**Tags**: cs.CV 



### Spoken Language Understanding on Unseen Tasks With In-Context Learning
**Authors**: Neeraj Agrawal, Sriram Ganapathy

**Updated**: 2025-05-12T16:38:43Z

**Summary**: Spoken language understanding (SLU) tasks involve diverse skills that probe the information extraction, classification and/or generation capabilities of models. In this setting, task-specific training data may not always be available. While traditional task-specific SLU models are unable to cater to such requirements, the speech-text large language models (LLMs) offer a promising alternative with emergent abilities. However, out of-the-box, our evaluations indicate that the zero/few-shot performance of prominent open-source speech-text LLMs on SLU tasks are not up to the mark. In this paper, we introduce a novel approach to robust task-agnostic fine-tuning using randomized class labels. With this proposed fine-tuning, we illustrate that the performance of the speech-text LLMs on an unseen task is significantly improved over standard approaches. Critically, the proposed approach avoids the requirement of task-specific data annotations for enabling new tasks in speech-text LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2505.07731v1),  [pdf](http://arxiv.org/pdf/2505.07731v1)

**Tags**: cs.CL cs.LG eess.AS 



### The Leaderboard Illusion
**Authors**: Shivalika Singh, Yiyang Nan, Alex Wang, Daniel D'Souza, Sayash Kapoor, Ahmet Üstün, Sanmi Koyejo, Yuntian Deng, Shayne Longpre, Noah A. Smith, Beyza Ermis, Marzieh Fadaee, Sara Hooker

**Updated**: 2025-05-12T16:33:58Z

**Summary**: Measuring progress is fundamental to the advancement of any scientific field. As benchmarks play an increasingly central role, they also grow more susceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard for ranking the most capable AI systems. Yet, in this work we identify systematic issues that have resulted in a distorted playing field. We find that undisclosed private testing practices benefit a handful of providers who are able to test multiple variants before public release and retract scores if desired. We establish that the ability of these providers to choose the best score leads to biased Arena scores due to selective disclosure of performance results. At an extreme, we identify 27 private LLM variants tested by Meta in the lead-up to the Llama-4 release. We also establish that proprietary closed models are sampled at higher rates (number of battles) and have fewer models removed from the arena than open-weight and open-source alternatives. Both these policies lead to large data access asymmetries over time. Providers like Google and OpenAI have received an estimated 19.2% and 20.4% of all data on the arena, respectively. In contrast, a combined 83 open-weight models have only received an estimated 29.7% of the total data. We show that access to Chatbot Arena data yields substantial benefits; even limited additional data can result in relative performance gains of up to 112% on the arena distribution, based on our conservative estimates. Together, these dynamics result in overfitting to Arena-specific dynamics rather than general model quality. The Arena builds on the substantial efforts of both the organizers and an open community that maintains this valuable evaluation platform. We offer actionable recommendations to reform the Chatbot Arena's evaluation framework and promote fairer, more transparent benchmarking for the field

**Link**: [arxiv](http://arxiv.org/abs/2504.20879v2),  [pdf](http://arxiv.org/pdf/2504.20879v2)

**Tags**: cs.AI cs.CL cs.LG stat.ME 



### Gameplay Highlights Generation
**Authors**: Vignesh Edithal, Le Zhang, Ilia Blank, Imran Junejo

**Updated**: 2025-05-12T16:28:22Z

**Summary**: In this work, we enable gamers to share their gaming experience on social media by automatically generating eye-catching highlight reels from their gameplay session Our automation will save time for gamers while increasing audience engagement. We approach the highlight generation problem by first identifying intervals in the video where interesting events occur and then concatenate them. We developed an in-house gameplay event detection dataset containing interesting events annotated by humans using VIA video annotator. Traditional techniques for highlight detection such as game engine integration requires expensive collaboration with game developers. OCR techniques which detect patches of specific images or texts require expensive per game engineering and may not generalize across game UI and different language. We finetuned a multimodal general purpose video understanding model such as X-CLIP using our dataset which generalizes across multiple games in a genre without per game engineering. Prompt engineering was performed to improve the classification performance of this multimodal model. Our evaluation showed that such a finetuned model can detect interesting events in first person shooting games from unseen gameplay footage with more than 90% accuracy. Moreover, our model performed significantly better on low resource games (small dataset) when trained along with high resource games, showing signs of transfer learning. To make the model production ready, we used ONNX libraries to enable cross platform inference. These libraries also provide post training quantization tools to reduce model size and inference time for deployment. ONNX runtime libraries with DirectML backend were used to perform efficient inference on Windows OS. We show that natural language supervision in the X-CLIP model leads to data efficient and highly performant video recognition models.

**Link**: [arxiv](http://arxiv.org/abs/2505.07721v1),  [pdf](http://arxiv.org/pdf/2505.07721v1)

**Tags**: cs.CV 



### The First Prompt Counts the Most! An Evaluation of Large Language Models   on Iterative Example-Based Code Generation
**Authors**: Yingjie Fu, Bozhou Li, Linyi Li, Wentao Zhang, Tao Xie

**Updated**: 2025-05-12T16:21:52Z

**Summary**: The capabilities of Large Language Models (LLMs) in code generation have been extensively studied, particularly for implementing target functionalities from natural-language descriptions. Alternatively, input-output (I/O) examples provide an accessible, unambiguous, and flexible way to describe functionalities. However, their inherent diversity, opaqueness, and incompleteness impose greater challenges for understanding and implementing the target requirements. Therefore, generating code from I/O examples (i.e., example-based code generation) provides a new perspective, allowing us to additionally evaluate LLMs' capability to infer target functionalities from limited information and to process new-form requirements. However, related research about LLMs in example-based code generation remains largely unexplored. To fill this gap, this paper presents the first comprehensive study on example-based code generation using LLMs. We adopt an iterative evaluation framework and formalize the objective of example-based code generation as two sequential sub-objectives: generating code conforming to the given examples and generating code that successfully implements the target functionalities from (iteratively) given examples. We assess six state-of-the-art LLMs using a new benchmark of 172 diverse target functionalities. The results demonstrate that when requirements are described using iterative I/O examples rather than natural language, the LLMs' score decreases by over 60%, and the vast majority (even over 95%) of successfully implemented functionalities are achieved in the first round of the iterations. Furthermore, we also find that combining I/O examples with even imprecise and fragmental natural language descriptions greatly improves LLM performance, and the selection of initial I/O examples can also influence the score, suggesting opportunities for prompt optimization.

**Link**: [arxiv](http://arxiv.org/abs/2411.06774v2),  [pdf](http://arxiv.org/pdf/2411.06774v2)

**Tags**: cs.SE 



### Circuit Partitioning Using Large Language Models for Quantum Compilation   and Simulations
**Authors**: Pranav Sinha, Sumit Kumar Jha, Sunny Raj

**Updated**: 2025-05-12T16:18:48Z

**Summary**: We are in the midst of the noisy intermediate-scale quantum (NISQ) era, where quantum computers are limited by noisy gates, some of which are more error-prone than others and can render the final computation incomprehensible. Quantum circuit compilation algorithms attempt to minimize these noisy gates when mapping quantum algorithms onto quantum hardware but face computational challenges that restrict their application to circuits with no more than 5-6 qubits, necessitating the need to partition large circuits before the application of noisy quantum gate minimization algorithms. The existing generation of these algorithms is heuristic in nature and does not account for downstream gate minimization tasks. Large language models (LLMs) have the potential to change this and help improve quantum circuit partitions. This paper investigates the use of LLMs, such as Llama and Mistral, for partitioning quantum circuits by capitalizing on their abilities to understand and generate code, including QASM. Specifically, we teach LLMs to partition circuits using the quick partition approach of the Berkeley Quantum Synthesis Toolkit. Through experimental evaluations, we show that careful fine-tuning of open source LLMs enables us to obtain an accuracy of 53.4% for the partition task while over-the-shelf LLMs are unable to correctly partition circuits, using standard 1-shot and few-shot training approaches.

**Link**: [arxiv](http://arxiv.org/abs/2505.07711v1),  [pdf](http://arxiv.org/pdf/2505.07711v1)

**Tags**: cs.ET cs.AI quant-ph 



### Codifying Character Logic in Role-Playing
**Authors**: Letian Peng, Jingbo Shang

**Updated**: 2025-05-13T02:16:35Z

**Summary**: This paper introduces Codified Profiles for role-playing, a novel approach that represents character logic as structured, executable functions for behavioral decision-making. Each profile defines a set of functions parse_by_scene(scene) that outputs a list of logic-grounded assertions triggered_statements, using both explicit control structures (e.g., if-then-else) and condition checks like check_condition(scene, question), where each question is a semantically meaningful prompt about the scene (e.g., "Is the character in danger?") discriminated by the role-playing LLM as true, false, or unknown. This explicit representation offers three key advantages over traditional prompt-based profiles, which append character descriptions directly into text prompts: (1) Persistence, by enforcing complete and consistent execution of character logic, rather than relying on the model's implicit reasoning; (2) Updatability, through systematic inspection and revision of behavioral logic, which is difficult to track or debug in prompt-only approaches; (3) Controllable Randomness, by supporting stochastic behavior directly within the logic, enabling fine-grained variability that prompting alone struggles to achieve. To validate these advantages, we introduce a new benchmark constructed from 83 characters and 5,141 scenes curated from Fandom, using NLI-based scoring to compare character responses against ground-truth actions. Our experiments demonstrate the significant benefits of codified profiles in improving persistence, updatability, and behavioral diversity. Notably, by offloading a significant portion of reasoning to preprocessing, codified profiles enable even 1B-parameter models to perform high-quality role-playing, providing a scalable and efficient foundation for local deployment of role-play agents.

**Link**: [arxiv](http://arxiv.org/abs/2505.07705v2),  [pdf](http://arxiv.org/pdf/2505.07705v2)

**Tags**: cs.CL 



### From Distributional to Overton Pluralism: Investigating Large Language   Model Alignment
**Authors**: Thom Lake, Eunsol Choi, Greg Durrett

**Updated**: 2025-05-12T16:11:53Z

**Summary**: The alignment process changes several properties of a large language model's (LLM's) output distribution. We analyze two aspects of post-alignment distributional shift of LLM responses. First, we re-examine previously reported reductions in response diversity post-alignment. Our analysis suggests that an apparent drop in the diversity of responses is largely explained by quality control and information aggregation. Alignment suppresses irrelevant and unhelpful content while shifting the output distribution toward longer responses that cover information spanning several responses from the base LLM, essentially presenting diverse information in a single response. Finding little evidence that alignment suppresses useful information, it is natural to ask the opposite question: do aligned models surface information that cannot be recovered from base models? Our second investigation shows this is not the case and the behavior of aligned models is recoverable from base models without fine-tuning. A combination of in-context examples and lower-resolution semantic hints about response content can elicit responses from base LLMs that are as similar to alignment-tuned LLM responses as alignment-tuned LLM responses are to each other. Taken together, these results indicate that current alignment techniques capture but do not extend the useful subset of assistant-like base LLM behavior, providing further evidence for the Superficial Alignment Hypothesis. They also show that in-context alignment can go surprisingly far as a strategy for imitating aligned LLMs without fine-tuning. Our code and data is available at https://github.com/thomlake/investigating-alignment.

**Link**: [arxiv](http://arxiv.org/abs/2406.17692v2),  [pdf](http://arxiv.org/pdf/2406.17692v2)

**Tags**: cs.CL cs.LG 



### PatchTrack: A Comprehensive Analysis of ChatGPT's Influence on Pull   Request Outcomes
**Authors**: Daniel Ogenrwot, John Businge

**Updated**: 2025-05-12T16:09:33Z

**Summary**: The rapid adoption of large language models (LLMs) like ChatGPT in software development has introduced new ways for developers to interact with AI, particularly in pull request workflows. While prior research has examined AI-generated code quality, there is limited understanding of how ChatGPT is utilized in real-world pull request decision-making and how its suggestions influence patch integration and rejection. To explore these aspects, we analyze self-admitted ChatGPT usage (SACU), where developers explicitly disclose their reliance on ChatGPT within pull request discussions. Our study examines 338 pull requests (285 merged, 53 closed) across 255 GitHub repositories, containing 645 ChatGPT-generated code snippets and 3,486 patches. We introduce PatchTrack, a classification tool that determines whether ChatGPT-generated patches were applied (PA, 115 cases), not applied (PN, 64 cases), or not suggested (NE, 106 cases). Our findings reveal that full adoption of ChatGPT-generated code is rare, developers frequently modify or selectively integrate AI-generated patches to align with project constraints, with a median integration rate of 25%. Through qualitative analysis, we identify key factors influencing patch integration and pull request rejection, including scope misalignment, maintainability concerns, redundant solutions, and procedural barriers such as incomplete documentation or administrative policies. By providing empirical insights into ChatGPT's role in pull request workflows, this study informs developers, maintainers, and educators on the evolving use of generative AI in collaborative software development. It also lays the groundwork for future research on optimizing AI-assisted development, improving transparency in AI adoption, and enhancing patch integration workflows.

**Link**: [arxiv](http://arxiv.org/abs/2505.07700v1),  [pdf](http://arxiv.org/pdf/2505.07700v1)

**Tags**: cs.SE D.2.0; K.6.3 



### Design Principles for Realizable Discrete Surface Embeddings in Physical   Systems
**Authors**: Kyungeun Kim, Christian D. Santangelo

**Updated**: 2025-05-12T16:05:17Z

**Summary**: The isometric embedding of surfaces in three-dimensional space is fundamental to various physical systems, from elastic sheets to programmable materials. While continuous surfaces typically admit unique solutions under suitable boundary conditions, their discrete counterparts-represented as networks of vertices connected by edges-can exhibit multiple distinct embeddings for identical edge lengths. We present a systematic approach to constructing discrete meshes that yield a controlled number of embeddings. By analyzing the relationship between mesh connectivity and embedding multiplicity through rigidity theory, we develop criteria for designing meshes that minimize solution multiplicity. We demonstrate computational methods based on local matrix operations and trilateration techniques, enabling practical implementation for meshes with approximately a thousand vertices. Our analysis provides both theoretical bounds on the number of possible embeddings based on B\'ezout's theorem and practical guidelines for mesh construction in physical applications. Through numerical simulations, we show that this approach achieves comparable accuracy to traditional minimization methods while offering computational advantages through sequential computation. Importantly, we demonstrate that in cases where a unique smooth solution exists, local fluctuations in reconstructed shapes derived from the computational grid can serve as indicators of insufficient geometric constraints. This work bridges the gap between discrete and continuous embedding problems, providing insights for applications in 4D printing, mechanical meta-materials, and deployable structures.

**Link**: [arxiv](http://arxiv.org/abs/2505.07696v1),  [pdf](http://arxiv.org/pdf/2505.07696v1)

**Tags**: cond-mat.dis-nn physics.comp-ph 



### ApproXAI: Energy-Efficient Hardware Acceleration of Explainable AI using   Approximate Computing
**Authors**: Ayesha Siddique, Khurram Khalil, Khaza Anuarul Hoque

**Updated**: 2025-05-12T16:04:45Z

**Summary**: Explainable artificial intelligence (XAI) enhances AI system transparency by framing interpretability as an optimization problem. However, this approach often necessitates numerous iterations of computationally intensive operations, limiting its applicability in real-time scenarios. While recent research has focused on XAI hardware acceleration on FPGAs and TPU, these methods do not fully address energy efficiency in real-time settings. To address this limitation, we propose XAIedge, a novel framework that leverages approximate computing techniques into XAI algorithms, including integrated gradients, model distillation, and Shapley analysis. XAIedge translates these algorithms into approximate matrix computations and exploits the synergy between convolution, Fourier transform, and approximate computing paradigms. This approach enables efficient hardware acceleration on TPU-based edge devices, facilitating faster real-time outcome interpretations. Our comprehensive evaluation demonstrates that XAIedge achieves a $2\times$ improvement in energy efficiency compared to existing accurate XAI hardware acceleration techniques while maintaining comparable accuracy. These results highlight the potential of XAIedge to significantly advance the deployment of explainable AI in energy-constrained real-time applications.

**Link**: [arxiv](http://arxiv.org/abs/2504.17929v2),  [pdf](http://arxiv.org/pdf/2504.17929v2)

**Tags**: cs.AI cs.AR 



### FD-RIO: Fast Dense Radar Inertial Odometry
**Authors**: Nader J. Abu-Alrub, Nathir A. Rawashdeh

**Updated**: 2025-05-12T16:03:14Z

**Summary**: Radar-based odometry is a popular solution for ego-motion estimation in conditions where other exteroceptive sensors may degrade, whether due to poor lighting or challenging weather conditions; however, scanning radars have the downside of relatively lower sampling rate and spatial resolution. In this work, we present FD-RIO, a method to alleviate this problem by fusing noisy, drift-prone, but high-frequency IMU data with dense radar scans. To the best of our knowledge, this is the first attempt to fuse dense scanning radar odometry with IMU using a Kalman filter. We evaluate our methods using two publicly available datasets and report accuracies using standard KITTI evaluation metrics, in addition to ablation tests and runtime analysis. Our phase correlation -based approach is compact, intuitive, and is designed to be a practical solution deployable on a realistic hardware setup of a mobile platform. Despite its simplicity, FD-RIO is on par with other state-of-the-art methods and outperforms in some test sequences.

**Link**: [arxiv](http://arxiv.org/abs/2505.07694v1),  [pdf](http://arxiv.org/pdf/2505.07694v1)

**Tags**: cs.RO 



### Large Language Models Think Too Fast To Explore Effectively
**Authors**: Lan Pan, Hanbo Xie, Robert C. Wilson

**Updated**: 2025-05-12T16:02:08Z

**Summary**: Large Language Models (LLMs) have emerged with many intellectual capacities. While numerous benchmarks assess their intelligence, limited attention has been given to their ability to explore--an essential capacity for discovering new information and adapting to novel environments in both natural and artificial systems. The extent to which LLMs can effectively explore, particularly in open-ended tasks, remains unclear. This study investigates whether LLMs can surpass humans in exploration during an open-ended task, using Little Alchemy 2 as a paradigm, where agents combine elements to discover new ones. Results show most LLMs underperform compared to humans, except for the o1 model, with traditional LLMs relying primarily on uncertainty-driven strategies, unlike humans who balance uncertainty and empowerment. Results indicate that traditional reasoning-focused LLMs, such as GPT-4o, exhibit a significantly faster and less detailed reasoning process, limiting their exploratory performance. In contrast, the DeepSeek reasoning model demonstrates prolonged, iterative thought processes marked by repetitive analysis of combinations and past trials, reflecting a more thorough and human-like exploration strategy. Representational analysis of the models with Sparse Autoencoders (SAE) revealed that uncertainty and choices are represented at earlier transformer blocks, while empowerment values are processed later, causing LLMs to think too fast and make premature decisions, hindering effective exploration. These findings shed light on the limitations of LLM exploration and suggest directions for improving their adaptability.

**Link**: [arxiv](http://arxiv.org/abs/2501.18009v2),  [pdf](http://arxiv.org/pdf/2501.18009v2)

**Tags**: cs.AI q-bio.NC 



### SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in   Large Language Models
**Authors**: Hang Wu, Jianian Zhu, Yinghui Li, Haojie Wang, Biao Hou, Jidong Zhai

**Updated**: 2025-05-12T15:46:28Z

**Summary**: Large Language Models (LLMs) present a critical trade-off between inference quality and computational cost: larger models offer superior capabilities but incur significant latency, while smaller models are faster but less powerful. Existing serving strategies often employ fixed model scales or static two-stage speculative decoding, failing to dynamically adapt to the varying complexities of user requests or fluctuations in system performance. This paper introduces \systemname{}, a novel framework that reimagines LLM inference as an adaptive routing problem solved through multi-level speculative decoding. \systemname{} dynamically constructs and optimizes inference "paths" (chains of models) based on real-time feedback, addressing the limitations of static approaches. Our contributions are threefold: (1) An \textbf{adaptive model chain scheduling} mechanism that leverages performance profiling (execution times) and predictive similarity metrics (derived from token distribution divergence) to continuously select the optimal sequence of draft and verifier models, minimizing predicted latency per generated token. (2) A \textbf{multi-level collaborative verification} framework where intermediate models within the selected chain can validate speculative tokens, reducing the verification burden on the final, most powerful target model. (3) A \textbf{synchronized state management} system providing efficient, consistent KV cache handling across heterogeneous models in the chain, including precise, low-overhead rollbacks tailored for asynchronous batch processing inherent in multi-level speculation. Preliminary experiments demonstrate the validity of our method.

**Link**: [arxiv](http://arxiv.org/abs/2505.07680v1),  [pdf](http://arxiv.org/pdf/2505.07680v1)

**Tags**: cs.LG cs.DC 



### Geospatial Mechanistic Interpretability of Large Language Models
**Authors**: Stef De Sabbata, Stefano Mizzaro, Kevin Roitero

**Updated**: 2025-05-12T15:44:44Z

**Summary**: Large Language Models (LLMs) have demonstrated unprecedented capabilities across various natural language processing tasks. Their ability to process and generate viable text and code has made them ubiquitous in many fields, while their deployment as knowledge bases and "reasoning" tools remains an area of ongoing research. In geography, a growing body of literature has been focusing on evaluating LLMs' geographical knowledge and their ability to perform spatial reasoning. However, very little is still known about the internal functioning of these models, especially about how they process geographical information.   In this chapter, we establish a novel framework for the study of geospatial mechanistic interpretability - using spatial analysis to reverse engineer how LLMs handle geographical information. Our aim is to advance our understanding of the internal representations that these complex models generate while processing geographical information - what one might call "how LLMs think about geographic information" if such phrasing was not an undue anthropomorphism.   We first outline the use of probing in revealing internal structures within LLMs. We then introduce the field of mechanistic interpretability, discussing the superposition hypothesis and the role of sparse autoencoders in disentangling polysemantic internal representations of LLMs into more interpretable, monosemantic features. In our experiments, we use spatial autocorrelation to show how features obtained for placenames display spatial patterns related to their geographic location and can thus be interpreted geospatially, providing insights into how these models process geographical information. We conclude by discussing how our framework can help shape the study and use of foundation models in geography.

**Link**: [arxiv](http://arxiv.org/abs/2505.03368v2),  [pdf](http://arxiv.org/pdf/2505.03368v2)

**Tags**: cs.LG 



### Mapping Biomedical Ontology Terms to IDs: Effect of Domain Prevalence on   Prediction Accuracy
**Authors**: Thanh Son Do, Daniel B. Hier, Tayo Obafemi-Ajayi

**Updated**: 2025-05-12T15:43:37Z

**Summary**: This study evaluates the ability of large language models (LLMs) to map biomedical ontology terms to their corresponding ontology IDs across the Human Phenotype Ontology (HPO), Gene Ontology (GO), and UniProtKB terminologies. Using counts of ontology IDs in the PubMed Central (PMC) dataset as a surrogate for their prevalence in the biomedical literature, we examined the relationship between ontology ID prevalence and mapping accuracy. Results indicate that ontology ID prevalence strongly predicts accurate mapping of HPO terms to HPO IDs, GO terms to GO IDs, and protein names to UniProtKB accession numbers. Higher prevalence of ontology IDs in the biomedical literature correlated with higher mapping accuracy. Predictive models based on receiver operating characteristic (ROC) curves confirmed this relationship.   In contrast, this pattern did not apply to mapping protein names to Human Genome Organisation's (HUGO) gene symbols. GPT-4 achieved a high baseline performance (95%) in mapping protein names to HUGO gene symbols, with mapping accuracy unaffected by prevalence. We propose that the high prevalence of HUGO gene symbols in the literature has caused these symbols to become lexicalized, enabling GPT-4 to map protein names to HUGO gene symbols with high accuracy. These findings highlight the limitations of LLMs in mapping ontology terms to low-prevalence ontology IDs and underscore the importance of incorporating ontology ID prevalence into the training and evaluation of LLMs for biomedical applications.

**Link**: [arxiv](http://arxiv.org/abs/2409.13746v2),  [pdf](http://arxiv.org/pdf/2409.13746v2)

**Tags**: cs.CL cs.AI I.2 



### OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit
**Authors**: Arun S. Maiya

**Updated**: 2025-05-13T02:43:26Z

**Summary**: We present OnPrem$.$LLM, a Python-based toolkit for applying large language models (LLMs) to sensitive, non-public data in offline or restricted environments. The system is designed for privacy-preserving use cases and provides prebuilt pipelines for document processing and storage, retrieval-augmented generation (RAG), information extraction, summarization, classification, and prompt/output processing with minimal configuration. OnPrem$.$LLM supports multiple LLM backends -- including llama$.$cpp, Ollama, vLLM, and Hugging Face Transformers -- with quantized model support, GPU acceleration, and seamless backend switching. Although designed for fully local execution, OnPrem$.$LLM also supports integration with a wide range of cloud LLM providers when permitted, enabling hybrid deployments that balance performance with data control. A no-code web interface extends accessibility to non-technical users.

**Link**: [arxiv](http://arxiv.org/abs/2505.07672v2),  [pdf](http://arxiv.org/pdf/2505.07672v2)

**Tags**: cs.CL cs.AI cs.LG 



### Benchmarking Retrieval-Augmented Generation for Chemistry
**Authors**: Xianrui Zhong, Bowen Jin, Siru Ouyang, Yanzhen Shen, Qiao Jin, Yin Fang, Zhiyong Lu, Jiawei Han

**Updated**: 2025-05-12T15:34:45Z

**Summary**: Retrieval-augmented generation (RAG) has emerged as a powerful framework for enhancing large language models (LLMs) with external knowledge, particularly in scientific domains that demand specialized and dynamic information. Despite its promise, the application of RAG in the chemistry domain remains underexplored, primarily due to the lack of high-quality, domain-specific corpora and well-curated evaluation benchmarks. In this work, we introduce ChemRAG-Bench, a comprehensive benchmark designed to systematically assess the effectiveness of RAG across a diverse set of chemistry-related tasks. The accompanying chemistry corpus integrates heterogeneous knowledge sources, including scientific literature, the PubChem database, PubMed abstracts, textbooks, and Wikipedia entries. In addition, we present ChemRAG-Toolkit, a modular and extensible RAG toolkit that supports five retrieval algorithms and eight LLMs. Using ChemRAG-Toolkit, we demonstrate that RAG yields a substantial performance gain -- achieving an average relative improvement of 17.4% over direct inference methods. We further conduct in-depth analyses on retriever architectures, corpus selection, and the number of retrieved passages, culminating in practical recommendations to guide future research and deployment of RAG systems in the chemistry domain. The code and data is available at https://chemrag.github.io.

**Link**: [arxiv](http://arxiv.org/abs/2505.07671v1),  [pdf](http://arxiv.org/pdf/2505.07671v1)

**Tags**: cs.CL cs.AI cs.IR 



### A Case Study Investigating the Role of Generative AI in Quality   Evaluations of Epics in Agile Software Development
**Authors**: Werner Geyer, Jessica He, Daita Sarkar, Michelle Brachman, Chris Hammond, Jennifer Heins, Zahra Ashktorab, Carlos Rosemberg, Charlie Hill

**Updated**: 2025-05-12T15:31:16Z

**Summary**: The broad availability of generative AI offers new opportunities to support various work domains, including agile software development. Agile epics are a key artifact for product managers to communicate requirements to stakeholders. However, in practice, they are often poorly defined, leading to churn, delivery delays, and cost overruns. In this industry case study, we investigate opportunities for large language models (LLMs) to evaluate agile epic quality in a global company. Results from a user study with 17 product managers indicate how LLM evaluations could be integrated into their work practices, including perceived values and usage in improving their epics. High levels of satisfaction indicate that agile epics are a new, viable application of AI evaluations. However, our findings also outline challenges, limitations, and adoption barriers that can inform both practitioners and researchers on the integration of such evaluations into future agile work practices.

**Link**: [arxiv](http://arxiv.org/abs/2505.07664v1),  [pdf](http://arxiv.org/pdf/2505.07664v1)

**Tags**: cs.SE cs.AI cs.HC 



### Hierarchical Sparse Attention Framework for Computationally Efficient   Classification of Biological Cells
**Authors**: Elad Yoshai, Dana Yagoda-Aharoni, Eden Dotan, Natan T. Shaked

**Updated**: 2025-05-12T15:29:08Z

**Summary**: We present SparseAttnNet, a new hierarchical attention-driven framework for efficient image classification that adaptively selects and processes only the most informative pixels from images. Traditional convolutional neural networks typically process the entire images regardless of information density, leading to computational inefficiency and potential focus on irrelevant features. Our approach leverages a dynamic selection mechanism that uses coarse attention distilled by fine multi-head attention from the downstream layers of the model, allowing the model to identify and extract the most salient k pixels, where k is adaptively learned during training based on loss convergence trends. Once the top-k pixels are selected, the model processes only these pixels, embedding them as words in a language model to capture their semantics, followed by multi-head attention to incorporate global context. For biological cell images, we demonstrate that SparseAttnNet can process approximately 15% of the pixels instead of the full image. Applied to cell classification tasks using white blood cells images from the following modalities: optical path difference (OPD) images from digital holography for stain-free cells, images from motion-sensitive (event) camera from stain-free cells, and brightfield microscopy images of stained cells, For all three imaging modalities, SparseAttnNet achieves competitive accuracy while drastically reducing computational requirements in terms of both parameters and floating-point operations per second, compared to traditional CNNs and Vision Transformers. Since the model focuses on biologically relevant regions, it also offers improved explainability. The adaptive and lightweight nature of SparseAttnNet makes it ideal for deployment in resource-constrained and high-throughput settings, including imaging flow cytometry.

**Link**: [arxiv](http://arxiv.org/abs/2505.07661v1),  [pdf](http://arxiv.org/pdf/2505.07661v1)

**Tags**: eess.IV cs.CV 



### JobHop: A Large-Scale Dataset of Career Trajectories
**Authors**: Iman Johary, Raphael Romero, Alexandru C. Mara, Tijl De Bie

**Updated**: 2025-05-12T15:22:29Z

**Summary**: Understanding labor market dynamics is essential for policymakers, employers, and job seekers. However, comprehensive datasets that capture real-world career trajectories are scarce. In this paper, we introduce JobHop, a large-scale public dataset derived from anonymized resumes provided by VDAB, the public employment service in Flanders, Belgium. Utilizing Large Language Models (LLMs), we process unstructured resume data to extract structured career information, which is then mapped to standardized ESCO occupation codes using a multi-label classification model. This results in a rich dataset of over 2.3 million work experiences, extracted from and grouped into more than 391,000 user resumes and mapped to standardized ESCO occupation codes, offering valuable insights into real-world occupational transitions. This dataset enables diverse applications, such as analyzing labor market mobility, job stability, and the effects of career breaks on occupational transitions. It also supports career path prediction and other data-driven decision-making processes. To illustrate its potential, we explore key dataset characteristics, including job distributions, career breaks, and job transitions, demonstrating its value for advancing labor market research.

**Link**: [arxiv](http://arxiv.org/abs/2505.07653v1),  [pdf](http://arxiv.org/pdf/2505.07653v1)

**Tags**: cs.CL 



### Neural Brain: A Neuroscience-inspired Framework for Embodied Agents
**Authors**: Jian Liu, Xiongtao Shi, Thai Duy Nguyen, Haitian Zhang, Tianxiang Zhang, Wei Sun, Yanjie Li, Athanasios V. Vasilakos, Giovanni Iacca, Arshad Ali Khan, Arvind Kumar, Jae Won Cho, Ajmal Mian, Lihua Xie, Erik Cambria, Lin Wang

**Updated**: 2025-05-12T15:05:34Z

**Summary**: The rapid evolution of artificial intelligence (AI) has shifted from static, data-driven models to dynamic systems capable of perceiving and interacting with real-world environments. Despite advancements in pattern recognition and symbolic reasoning, current AI systems, such as large language models, remain disembodied, unable to physically engage with the world. This limitation has driven the rise of embodied AI, where autonomous agents, such as humanoid robots, must navigate and manipulate unstructured environments with human-like adaptability. At the core of this challenge lies the concept of Neural Brain, a central intelligence system designed to drive embodied agents with human-like adaptability. A Neural Brain must seamlessly integrate multimodal sensing and perception with cognitive capabilities. Achieving this also requires an adaptive memory system and energy-efficient hardware-software co-design, enabling real-time action in dynamic environments. This paper introduces a unified framework for the Neural Brain of embodied agents, addressing two fundamental challenges: (1) defining the core components of Neural Brain and (2) bridging the gap between static AI models and the dynamic adaptability required for real-world deployment. To this end, we propose a biologically inspired architecture that integrates multimodal active sensing, perception-cognition-action function, neuroplasticity-based memory storage and updating, and neuromorphic hardware/software optimization. Furthermore, we also review the latest research on embodied agents across these four aspects and analyze the gap between current AI systems and human intelligence. By synthesizing insights from neuroscience, we outline a roadmap towards the development of generalizable, autonomous agents capable of human-level intelligence in real-world scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2505.07634v1),  [pdf](http://arxiv.org/pdf/2505.07634v1)

**Tags**: cs.RO cs.AI cs.CV 



### ConTextual: Improving Clinical Text Summarization in LLMs with   Context-preserving Token Filtering and Knowledge Graphs
**Authors**: Fahmida Liza Piya, Rahmatollah Beheshti

**Updated**: 2025-05-12T14:57:14Z

**Summary**: Unstructured clinical data can serve as a unique and rich source of information that can meaningfully inform clinical practice. Extracting the most pertinent context from such data is critical for exploiting its true potential toward optimal and timely decision-making in patient care. While prior research has explored various methods for clinical text summarization, most prior studies either process all input tokens uniformly or rely on heuristic-based filters, which can overlook nuanced clinical cues and fail to prioritize information critical for decision-making. In this study, we propose Contextual, a novel framework that integrates a Context-Preserving Token Filtering method with a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By preserving context-specific important tokens and enriching them with structured knowledge, ConTextual improves both linguistic coherence and clinical fidelity. Our extensive empirical evaluations on two public benchmark datasets demonstrate that ConTextual consistently outperforms other baselines. Our proposed approach highlights the complementary role of token-level filtering and structured retrieval in enhancing both linguistic and clinical integrity, as well as offering a scalable solution for improving precision in clinical text generation.

**Link**: [arxiv](http://arxiv.org/abs/2504.16394v2),  [pdf](http://arxiv.org/pdf/2504.16394v2)

**Tags**: cs.CL cs.AI 



### KAQG: A Knowledge-Graph-Enhanced RAG for Difficulty-Controlled Question   Generation
**Authors**: Ching Han Chen, Ming Fang Shiu

**Updated**: 2025-05-12T14:42:19Z

**Summary**: KAQG introduces a decisive breakthrough for Retrieval-Augmented Generation (RAG) by explicitly tackling the two chronic weaknesses of current pipelines: transparent multi-step reasoning and fine-grained cognitive difficulty control. This transforms RAG from a passive retriever into an accountable generator of calibrated exam items. Technically, the framework fuses knowledge graphs, RAG retrieval, and educational assessment theory into a single pipeline. Domain passages are parsed into a structured graph; graph-aware retrieval feeds fact chains to an LLM; and an assessment layer governed by Bloom's Taxonomy levels and Item Response Theory (IRT) transforms those chains into psychometrically sound questions. This cross-disciplinary marriage yields two scholarly contributions: it shows how semantic graph contexts guide LLM reasoning paths, and it operationalizes difficulty metrics within the generation process, producing items whose IRT parameters match expert benchmarks. Every module, from KG construction scripts to the multi-agent reasoning scheduler and the automatic IRT validator, is openly released on GitHub. This enables peer laboratories to replicate experiments, benchmark against baselines, and extend individual components without licensing barriers. Its reproducible design paves the way for rigorous ablation studies, cross-domain transfer experiments, and shared leaderboards on multi-step reasoning benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2505.07618v1),  [pdf](http://arxiv.org/pdf/2505.07618v1)

**Tags**: cs.IR 



### Unbiased Evaluation of Large Language Models from a Causal Perspective
**Authors**: Meilin Chen, Jian Tian, Liang Ma, Di Xie, Weijie Chen, Jiang Zhu

**Updated**: 2025-05-12T14:34:05Z

**Summary**: Benchmark contamination has become a significant concern in the LLM evaluation community. Previous Agents-as-an-Evaluator address this issue by involving agents in the generation of questions. Despite their success, the biases in Agents-as-an-Evaluator methods remain largely unexplored. In this paper, we present a theoretical formulation of evaluation bias, providing valuable insights into designing unbiased evaluation protocols. Furthermore, we identify two type of bias in Agents-as-an-Evaluator through carefully designed probing tasks on a minimal Agents-as-an-Evaluator setup. To address these issues, we propose the Unbiased Evaluator, an evaluation protocol that delivers a more comprehensive, unbiased, and interpretable assessment of LLMs.Extensive experiments reveal significant room for improvement in current LLMs. Additionally, we demonstrate that the Unbiased Evaluator not only offers strong evidence of benchmark contamination but also provides interpretable evaluation results.

**Link**: [arxiv](http://arxiv.org/abs/2502.06655v2),  [pdf](http://arxiv.org/pdf/2502.06655v2)

**Tags**: cs.AI 



### Concept-Level Explainability for Auditing & Steering LLM Responses
**Authors**: Kenza Amara, Rita Sevastjanova, Mennatallah El-Assady

**Updated**: 2025-05-12T14:31:51Z

**Summary**: As large language models (LLMs) become widely deployed, concerns about their safety and alignment grow. An approach to steer LLM behavior, such as mitigating biases or defending against jailbreaks, is to identify which parts of a prompt influence specific aspects of the model's output. Token-level attribution methods offer a promising solution, but still struggle in text generation, explaining the presence of each token in the output separately, rather than the underlying semantics of the entire LLM response. We introduce ConceptX, a model-agnostic, concept-level explainability method that identifies the concepts, i.e., semantically rich tokens in the prompt, and assigns them importance based on the outputs' semantic similarity. Unlike current token-level methods, ConceptX also offers to preserve context integrity through in-place token replacements and supports flexible explanation goals, e.g., gender bias. ConceptX enables both auditing, by uncovering sources of bias, and steering, by modifying prompts to shift the sentiment or reduce the harmfulness of LLM responses, without requiring retraining. Across three LLMs, ConceptX outperforms token-level methods like TokenSHAP in both faithfulness and human alignment. Steering tasks boost sentiment shift by 0.252 versus 0.131 for random edits and lower attack success rates from 0.463 to 0.242, outperforming attribution and paraphrasing baselines. While prompt engineering and self-explaining methods sometimes yield safer responses, ConceptX offers a transparent and faithful alternative for improving LLM safety and alignment, demonstrating the practical value of attribution-based explainability in guiding LLM behavior.

**Link**: [arxiv](http://arxiv.org/abs/2505.07610v1),  [pdf](http://arxiv.org/pdf/2505.07610v1)

**Tags**: cs.CL cs.AI 



### Multi-Objective Reinforcement Learning for Energy-Efficient Industrial   Control
**Authors**: Georg Schäfer, Raphael Seliger, Jakob Rehrl, Stefan Huber, Simon Hirlaender

**Updated**: 2025-05-12T14:28:42Z

**Summary**: Industrial automation increasingly demands energy-efficient control strategies to balance performance with environmental and cost constraints. In this work, we present a multi-objective reinforcement learning (MORL) framework for energy-efficient control of the Quanser Aero 2 testbed in its one-degree-of-freedom configuration. We design a composite reward function that simultaneously penalizes tracking error and electrical power consumption. Preliminary experiments explore the influence of varying the Energy penalty weight, alpha, on the trade-off between pitch tracking and energy savings. Our results reveal a marked performance shift for alpha values between 0.0 and 0.25, with non-Pareto optimal solutions emerging at lower alpha values, on both the simulation and the real system. We hypothesize that these effects may be attributed to artifacts introduced by the adaptive behavior of the Adam optimizer, which could bias the learning process and favor bang-bang control strategies. Future work will focus on automating alpha selection through Gaussian Process-based Pareto front modeling and transitioning the approach from simulation to real-world deployment.

**Link**: [arxiv](http://arxiv.org/abs/2505.07607v1),  [pdf](http://arxiv.org/pdf/2505.07607v1)

**Tags**: eess.SY cs.LG cs.SY 



### Characterizing the Investigative Methods of Fictional Detectives with   Large Language Models
**Authors**: Edirlei Soares de Lima, Marco A. Casanova, Bruno Feijó, Antonio L. Furtado

**Updated**: 2025-05-12T14:24:58Z

**Summary**: Detective fiction, a genre defined by its complex narrative structures and character-driven storytelling, presents unique challenges for computational narratology, a research field focused on integrating literary theory into automated narrative generation. While traditional literary studies have offered deep insights into the methods and archetypes of fictional detectives, these analyses often focus on a limited number of characters and lack the scalability needed for the extraction of unique traits that can be used to guide narrative generation methods. In this paper, we present an AI-driven approach for systematically characterizing the investigative methods of fictional detectives. Our multi-phase workflow explores the capabilities of 15 Large Language Models (LLMs) to extract, synthesize, and validate distinctive investigative traits of fictional detectives. This approach was tested on a diverse set of seven iconic detectives - Hercule Poirot, Sherlock Holmes, William Murdoch, Columbo, Father Brown, Miss Marple, and Auguste Dupin - capturing the distinctive investigative styles that define each character. The identified traits were validated against existing literary analyses and further tested in a reverse identification phase, achieving an overall accuracy of 91.43%, demonstrating the method's effectiveness in capturing the distinctive investigative approaches of each detective. This work contributes to the broader field of computational narratology by providing a scalable framework for character analysis, with potential applications in AI-driven interactive storytelling and automated narrative generation.

**Link**: [arxiv](http://arxiv.org/abs/2505.07601v1),  [pdf](http://arxiv.org/pdf/2505.07601v1)

**Tags**: cs.CL cs.AI 



### NCL-UoR at SemEval-2025 Task 3: Detecting Multilingual Hallucination and   Related Observable Overgeneration Text Spans with Modified RefChecker and   Modified SeflCheckGPT
**Authors**: Jiaying Hong, Thanet Markchom, Jianfei Xu, Tong Wu, Huizhi Liang

**Updated**: 2025-05-12T14:24:25Z

**Summary**: SemEval-2025 Task 3 (Mu-SHROOM) focuses on detecting hallucinations in content generated by various large language models (LLMs) across multiple languages. This task involves not only identifying the presence of hallucinations but also pinpointing their specific occurrences. To tackle this challenge, this study introduces two methods: modified RefChecker and modified SelfCheckGPT. The modified RefChecker integrates prompt-based factual verification into References, structuring them as claim-based tests rather than single external knowledge sources. The modified SelfCheckGPT incorporates external knowledge to overcome its reliance on internal knowledge. In addition, both methods' original prompt designs are enhanced to identify hallucinated words within LLM-generated texts. Experimental results demonstrate the effectiveness of the approach, achieving a high ranking on the test dataset in detecting hallucinations across various languages, with an average IoU of 0.5310 and an average COR of 0.5669.

**Link**: [arxiv](http://arxiv.org/abs/2503.01921v2),  [pdf](http://arxiv.org/pdf/2503.01921v2)

**Tags**: cs.CL cs.AI 



### Reinforced Internal-External Knowledge Synergistic Reasoning for   Efficient Adaptive Search Agent
**Authors**: Ziyang Huang, Xiaowei Yuan, Yiming Ju, Jun Zhao, Kang Liu

**Updated**: 2025-05-12T14:21:57Z

**Summary**: Retrieval-augmented generation (RAG) is a common strategy to reduce hallucinations in Large Language Models (LLMs). While reinforcement learning (RL) can enable LLMs to act as search agents by activating retrieval capabilities, existing ones often underutilize their internal knowledge. This can lead to redundant retrievals, potential harmful knowledge conflicts, and increased inference latency. To address these limitations, an efficient and adaptive search agent capable of discerning optimal retrieval timing and synergistically integrating parametric (internal) and retrieved (external) knowledge is in urgent need. This paper introduces the Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA), which could indentify its own knowledge boundary and prioritize the utilization of internal knowledge, resorting to external search only when internal knowledge is deemed insufficient. This is achieved using a novel knowledge-boundary aware reward function and a knowledge-boundary aware training dataset. These are designed for internal-external knowledge synergy oriented RL, incentivizing the model to deliver accurate answers, minimize unnecessary retrievals, and encourage appropriate external searches when its own knowledge is lacking. Evaluations across multiple knowledge reasoning tasks demonstrate that IKEA significantly outperforms baseline methods, reduces retrieval frequency significantly, and exhibits robust generalization capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2505.07596v1),  [pdf](http://arxiv.org/pdf/2505.07596v1)

**Tags**: cs.CL cs.AI 



### LLMs Outperform Experts on Challenging Biology Benchmarks
**Authors**: Lennart Justen

**Updated**: 2025-05-12T14:17:41Z

**Summary**: This study systematically evaluates 27 frontier Large Language Models on eight biology benchmarks spanning molecular biology, genetics, cloning, virology, and biosecurity. Models from major AI developers released between November 2022 and April 2025 were assessed through ten independent runs per benchmark. The findings reveal dramatic improvements in biological capabilities. Top model performance increased more than 4-fold on the challenging text-only subset of the Virology Capabilities Test over the study period, with OpenAI's o3 now performing twice as well as expert virologists. Several models now match or exceed expert-level performance on other challenging benchmarks, including the biology subsets of GPQA and WMDP and LAB-Bench CloningScenarios. Contrary to expectations, chain-of-thought did not substantially improve performance over zero-shot evaluation, while extended reasoning features in o3-mini and Claude 3.7 Sonnet typically improved performance as predicted by inference scaling. Benchmarks such as PubMedQA and the MMLU and WMDP biology subsets exhibited performance plateaus well below 100%, suggesting benchmark saturation and errors in the underlying benchmark data. The analysis highlights the need for more sophisticated evaluation methodologies as AI systems continue to advance.

**Link**: [arxiv](http://arxiv.org/abs/2505.06108v2),  [pdf](http://arxiv.org/pdf/2505.06108v2)

**Tags**: cs.LG cs.AI q-bio.QM 



### A Multi-Dimensional Constraint Framework for Evaluating and Improving   Instruction Following in Large Language Models
**Authors**: Junjie Ye, Caishuang Huang, Zhuohan Chen, Wenjie Fu, Chenyuan Yang, Leyi Yang, Yilong Wu, Peng Wang, Meng Zhou, Xiaolong Yang, Tao Gui, Qi Zhang, Zhongchao Shi, Jianping Fan, Xuanjing Huang

**Updated**: 2025-05-12T14:16:55Z

**Summary**: Instruction following evaluates large language models (LLMs) on their ability to generate outputs that adhere to user-defined constraints. However, existing benchmarks often rely on templated constraint prompts, which lack the diversity of real-world usage and limit fine-grained performance assessment. To fill this gap, we propose a multi-dimensional constraint framework encompassing three constraint patterns, four constraint categories, and four difficulty levels. Building on this framework, we develop an automated instruction generation pipeline that performs constraint expansion, conflict detection, and instruction rewriting, yielding 1,200 code-verifiable instruction-following test samples. We evaluate 19 LLMs across seven model families and uncover substantial variation in performance across constraint forms. For instance, average performance drops from 77.67% at Level I to 32.96% at Level IV. Furthermore, we demonstrate the utility of our approach by using it to generate data for reinforcement learning, achieving substantial gains in instruction following without degrading general performance. In-depth analysis indicates that these gains stem primarily from modifications in the model's attention modules parameters, which enhance constraint recognition and adherence. Code and data are available in https://github.com/Junjie-Ye/MulDimIF.

**Link**: [arxiv](http://arxiv.org/abs/2505.07591v1),  [pdf](http://arxiv.org/pdf/2505.07591v1)

**Tags**: cs.CL cs.AI 



### SecReEvalBench: A Multi-turned Security Resilience Evaluation Benchmark   for Large Language Models
**Authors**: Huining Cui, Wei Liu

**Updated**: 2025-05-12T14:09:24Z

**Summary**: The increasing deployment of large language models in security-sensitive domains necessitates rigorous evaluation of their resilience against adversarial prompt-based attacks. While previous benchmarks have focused on security evaluations with limited and predefined attack domains, such as cybersecurity attacks, they often lack a comprehensive assessment of intent-driven adversarial prompts and the consideration of real-life scenario-based multi-turn attacks. To address this gap, we present SecReEvalBench, the Security Resilience Evaluation Benchmark, which defines four novel metrics: Prompt Attack Resilience Score, Prompt Attack Refusal Logic Score, Chain-Based Attack Resilience Score and Chain-Based Attack Rejection Time Score. Moreover, SecReEvalBench employs six questioning sequences for model assessment: one-off attack, successive attack, successive reverse attack, alternative attack, sequential ascending attack with escalating threat levels and sequential descending attack with diminishing threat levels. In addition, we introduce a dataset customized for the benchmark, which incorporates both neutral and malicious prompts, categorised across seven security domains and sixteen attack techniques. In applying this benchmark, we systematically evaluate five state-of-the-art open-weighted large language models, Llama 3.1, Gemma 2, Mistral v0.3, DeepSeek-R1 and Qwen 3. Our findings offer critical insights into the strengths and weaknesses of modern large language models in defending against evolving adversarial threats. The SecReEvalBench dataset is publicly available at https://kaggle.com/datasets/5a7ee22cf9dab6c93b55a73f630f6c9b42e936351b0ae98fbae6ddaca7fe248d, which provides a groundwork for advancing research in large language model security.

**Link**: [arxiv](http://arxiv.org/abs/2505.07584v1),  [pdf](http://arxiv.org/pdf/2505.07584v1)

**Tags**: cs.CR 



### Privacy-Preserving Real-Time Vietnamese-English Translation on iOS using   Edge AI
**Authors**: Cong Le

**Updated**: 2025-05-12T14:05:39Z

**Summary**: This research addresses the growing need for privacy-preserving and accessible language translation by developing a fully offline Neural Machine Translation (NMT) system for Vietnamese-English translation on iOS devices. Given increasing concerns about data privacy and unreliable network connectivity, on-device translation offers critical advantages. This project confronts challenges in deploying complex NMT models on resource-limited mobile devices, prioritizing efficiency, accuracy, and a seamless user experience. Leveraging advances such as MobileBERT and, specifically, the lightweight \textbf{TinyLlama 1.1B Chat v1.0} in GGUF format, \textbf{a} quantized Transformer-based model is implemented and optimized. The application is realized as a real-time iOS prototype, tightly integrating modern iOS frameworks and privacy-by-design principles. Comprehensive documentation covers model selection, technical architecture, challenges, and final implementation, including functional Swift code for deployment.

**Link**: [arxiv](http://arxiv.org/abs/2505.07583v1),  [pdf](http://arxiv.org/pdf/2505.07583v1)

**Tags**: cs.SE 



### YuLan-OneSim: Towards the Next Generation of Social Simulator with Large   Language Models
**Authors**: Lei Wang, Heyang Gao, Xiaohe Bo, Xu Chen, Ji-Rong Wen

**Updated**: 2025-05-12T14:05:17Z

**Summary**: Leveraging large language model (LLM) based agents to simulate human social behaviors has recently gained significant attention. In this paper, we introduce a novel social simulator called YuLan-OneSim. Compared to previous works, YuLan-OneSim distinguishes itself in five key aspects: (1) Code-free scenario construction: Users can simply describe and refine their simulation scenarios through natural language interactions with our simulator. All simulation code is automatically generated, significantly reducing the need for programming expertise. (2) Comprehensive default scenarios: We implement 50 default simulation scenarios spanning 8 domains, including economics, sociology, politics, psychology, organization, demographics, law, and communication, broadening access for a diverse range of social researchers. (3) Evolvable simulation: Our simulator is capable of receiving external feedback and automatically fine-tuning the backbone LLMs, significantly enhancing the simulation quality. (4) Large-scale simulation: By developing a fully responsive agent framework and a distributed simulation architecture, our simulator can handle up to 100,000 agents, ensuring more stable and reliable simulation results. (5) AI social researcher: Leveraging the above features, we develop an AI social researcher. Users only need to propose a research topic, and the AI researcher will automatically analyze the input, construct simulation environments, summarize results, generate technical reports, review and refine the reports--completing the social science research loop. To demonstrate the advantages of YuLan-OneSim, we conduct experiments to evaluate the quality of the automatically generated scenarios, the reliability, efficiency, and scalability of the simulation process, as well as the performance of the AI social researcher.

**Link**: [arxiv](http://arxiv.org/abs/2505.07581v1),  [pdf](http://arxiv.org/pdf/2505.07581v1)

**Tags**: cs.AI cs.CY 



### SCAM: A Real-World Typographic Robustness Evaluation for Multimodal   Foundation Models
**Authors**: Justus Westerhoff, Erblina Purelku, Jakob Hackstein, Jonas Loos, Leo Pinetzki, Lorenz Hufe

**Updated**: 2025-05-12T13:45:06Z

**Summary**: Typographic attacks exploit the interplay between text and visual content in multimodal foundation models, causing misclassifications when misleading text is embedded within images. However, existing datasets are limited in size and diversity, making it difficult to study such vulnerabilities. In this paper, we introduce SCAM, the largest and most diverse dataset of real-world typographic attack images to date, containing 1,162 images across hundreds of object categories and attack words. Through extensive benchmarking of Vision-Language Models (VLMs) on SCAM, we demonstrate that typographic attacks significantly degrade performance, and identify that training data and model architecture influence the susceptibility to these attacks. Our findings reveal that typographic attacks persist in state-of-the-art Large Vision-Language Models (LVLMs) due to the choice of their vision encoder, though larger Large Language Models (LLMs) backbones help mitigate their vulnerability. Additionally, we demonstrate that synthetic attacks closely resemble real-world (handwritten) attacks, validating their use in research. Our work provides a comprehensive resource and empirical insights to facilitate future research toward robust and trustworthy multimodal AI systems. We publicly release the datasets introduced in this paper along with the code for evaluations at www.bliss.berlin/research/scam.

**Link**: [arxiv](http://arxiv.org/abs/2504.04893v3),  [pdf](http://arxiv.org/pdf/2504.04893v3)

**Tags**: cs.CV cs.AI 



### Pinching-Antenna Systems (PASS) Aided Over-the-air Computation
**Authors**: Zhonghao Lyu, Haoyun Li, Yulan Gao, Ming Xiao, H. Vincent Poor

**Updated**: 2025-05-12T13:37:51Z

**Summary**: Over-the-air computation (AirComp) enables fast data aggregation for edge intelligence applications. However the performance of AirComp can be severely degraded by channel misalignments. Pinching antenna systems (PASS) have recently emerged as a promising solution for physically reshaping favorable wireless channels to reduce misalignments and thus AirComp errors, via low-cost, fully passive, and highly reconfigurable antenna deployment. Motivated by these benefits, we propose a novel PASS-aided AirComp system that introduces new design degrees of freedom through flexible pinching antenna (PA) placement. To improve performance, we consider a mean squared error (MSE) minimization problem by jointly optimizing the PA position, transmit power, and decoding vector. To solve this highly non-convex problem, we propose an alternating optimization based framework with Gauss-Seidel based PA position updates. Simulation results show that our proposed joint PA position and communication design significantly outperforms various benchmark schemes in AirComp accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2505.07559v1),  [pdf](http://arxiv.org/pdf/2505.07559v1)

**Tags**: cs.IT eess.SP math.IT 



### Direct Density Ratio Optimization: A Statistically Consistent Approach   to Aligning Large Language Models
**Authors**: Rei Higuchi, Taiji Suzuki

**Updated**: 2025-05-12T13:36:25Z

**Summary**: Aligning large language models (LLMs) with human preferences is crucial for safe deployment, yet existing methods assume specific preference models like Bradley-Terry model. This assumption leads to statistical inconsistency, where more data doesn't guarantee convergence to true human preferences. To address this critical gap, we introduce a novel alignment method Direct Density Ratio Optimization (DDRO). DDRO directly estimates the density ratio between preferred and unpreferred output distributions, circumventing the need for explicit human preference modeling. We theoretically prove that DDRO is statistically consistent, ensuring convergence to the true preferred distribution as the data size grows, regardless of the underlying preference structure. Experiments demonstrate that DDRO achieves superior performance compared to existing methods on many major benchmarks. DDRO unlocks the potential for truly data-driven alignment, paving the way for more reliable and human-aligned LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2505.07558v1),  [pdf](http://arxiv.org/pdf/2505.07558v1)

**Tags**: cs.LG cs.CL stat.ML 



### Injecting Knowledge Graphs into Large Language Models
**Authors**: Erica Coppolillo

**Updated**: 2025-05-12T13:31:26Z

**Summary**: Integrating structured knowledge from Knowledge Graphs (KGs) into Large Language Models (LLMs) remains a key challenge for symbolic reasoning. Existing methods mainly rely on prompt engineering or fine-tuning, which lose structural fidelity or incur high computational costs. Building on recent encoding techniques which integrate graph embeddings within the LLM input as tokens, we extend this paradigm to the KG domain by leveraging Knowledge Graph Embedding (KGE) models, thus enabling graph-aware reasoning. Our approach is model-agnostic, resource-efficient, and compatible with any LLMs. Extensive experimentation on synthetic and real-world datasets shows that our method improves reasoning performance over established baselines, further achieving the best trade-off in terms of accuracy and efficiency against state-of-the-art LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2505.07554v1),  [pdf](http://arxiv.org/pdf/2505.07554v1)

**Tags**: cs.LG cs.IR 



### Towards Requirements Engineering for RAG Systems
**Authors**: Tor Sporsem, Rasmus Ulfsnes

**Updated**: 2025-05-12T13:30:44Z

**Summary**: This short paper explores how a maritime company develops and integrates large-language models (LLM). Specifically by looking at the requirements engineering for Retrieval Augmented Generation (RAG) systems in expert settings. Through a case study at a maritime service provider, we demonstrate how data scientists face a fundamental tension between user expectations of AI perfection and the correctness of the generated outputs. Our findings reveal that data scientists must identify context-specific "retrieval requirements" through iterative experimentation together with users because they are the ones who can determine correctness. We present an empirical process model describing how data scientists practically elicited these "retrieval requirements" and managed system limitations. This work advances software engineering knowledge by providing insights into the specialized requirements engineering processes for implementing RAG systems in complex domain-specific applications.

**Link**: [arxiv](http://arxiv.org/abs/2505.07553v1),  [pdf](http://arxiv.org/pdf/2505.07553v1)

**Tags**: cs.SE cs.AI 



### GRADA: Graph-based Reranker against Adversarial Documents Attack
**Authors**: Jingjie Zheng, Aryo Pradipta Gema, Giwon Hong, Xuanli He, Pasquale Minervini, Youcheng Sun, Qiongkai Xu

**Updated**: 2025-05-12T13:27:35Z

**Summary**: Retrieval Augmented Generation (RAG) frameworks improve the accuracy of large language models (LLMs) by integrating external knowledge from retrieved documents, thereby overcoming the limitations of models' static intrinsic knowledge. However, these systems are susceptible to adversarial attacks that manipulate the retrieval process by introducing documents that are adversarial yet semantically similar to the query. Notably, while these adversarial documents resemble the query, they exhibit weak similarity to benign documents in the retrieval set. Thus, we propose a simple yet effective Graph-based Reranking against Adversarial Document Attacks (GRADA) framework aiming at preserving retrieval quality while significantly reducing the success of adversaries. Our study evaluates the effectiveness of our approach through experiments conducted on five LLMs: GPT-3.5-Turbo, GPT-4o, Llama3.1-8b, Llama3.1-70b, and Qwen2.5-7b. We use three datasets to assess performance, with results from the Natural Questions dataset demonstrating up to an 80% reduction in attack success rates while maintaining minimal loss in accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2505.07546v1),  [pdf](http://arxiv.org/pdf/2505.07546v1)

**Tags**: cs.IR cs.AI 



### Discrete Visual Tokens of Autoregression, by Diffusion, and for   Reasoning
**Authors**: Bohan Wang, Zhongqi Yue, Fengda Zhang, Shuo Chen, Li'an Bi, Junzhe Zhang, Xue Song, Kennard Yanting Chan, Jiachun Pan, Weijia Wu, Mingze Zhou, Wang Lin, Kaihang Pan, Saining Zhang, Liyu Jia, Wentao Hu, Wei Zhao, Hanwang Zhang

**Updated**: 2025-05-12T13:19:08Z

**Summary**: We completely discard the conventional spatial prior in image representation and introduce a novel discrete visual tokenizer: Self-consistency Tokenizer (Selftok). At its design core, we compose an autoregressive (AR) prior -- mirroring the causal structure of language -- into visual tokens by using the reverse diffusion process of image generation. The AR property makes Selftok fundamentally distinct from traditional spatial tokens in the following two key ways: - Selftok offers an elegant and minimalist approach to unify diffusion and AR for vision-language models (VLMs): By representing images with Selftok tokens, we can train a VLM using a purely discrete autoregressive architecture -- like that in LLMs -- without requiring additional modules or training objectives. - We theoretically show that the AR prior satisfies the Bellman equation, whereas the spatial prior does not. Therefore, Selftok supports reinforcement learning (RL) for visual generation with effectiveness comparable to that achieved in LLMs. Besides the AR property, Selftok is also a SoTA tokenizer that achieves a favorable trade-off between high-quality reconstruction and compression rate. We use Selftok to build a pure AR VLM for both visual comprehension and generation tasks. Impressively, without using any text-image training pairs, a simple policy gradient RL working in the visual tokens can significantly boost the visual generation benchmark, surpassing all the existing models by a large margin. Therefore, we believe that Selftok effectively addresses the long-standing challenge that visual tokens cannot support effective RL. When combined with the well-established strengths of RL in LLMs, this brings us one step closer to realizing a truly multimodal LLM. Project Page: https://selftok-team.github.io/report/.

**Link**: [arxiv](http://arxiv.org/abs/2505.07538v1),  [pdf](http://arxiv.org/pdf/2505.07538v1)

**Tags**: cs.CV 



### RAI: Flexible Agent Framework for Embodied AI
**Authors**: Kajetan Rachwał, Maciej Majek, Bartłomiej Boczek, Kacper Dąbrowski, Paweł Liberadzki, Adam Dąbrowski, Maria Ganzha

**Updated**: 2025-05-12T13:13:47Z

**Summary**: With an increase in the capabilities of generative language models, a growing interest in embodied AI has followed. This contribution introduces RAI - a framework for creating embodied Multi Agent Systems for robotics. The proposed framework implements tools for Agents' integration with robotic stacks, Large Language Models, and simulations. It provides out-of-the-box integration with state-of-the-art systems like ROS 2. It also comes with dedicated mechanisms for the embodiment of Agents. These mechanisms have been tested on a physical robot, Husarion ROSBot XL, which was coupled with its digital twin, for rapid prototyping. Furthermore, these mechanisms have been deployed in two simulations: (1) robot arm manipulator and (2) tractor controller. All of these deployments have been evaluated in terms of their control capabilities, effectiveness of embodiment, and perception ability. The proposed framework has been used successfully to build systems with multiple agents. It has demonstrated effectiveness in all the aforementioned tasks. It also enabled identifying and addressing the shortcomings of the generative models used for embodied AI.

**Link**: [arxiv](http://arxiv.org/abs/2505.07532v1),  [pdf](http://arxiv.org/pdf/2505.07532v1)

**Tags**: cs.MA 



### QuantX: A Framework for Hardware-Aware Quantization of Generative AI   Workloads
**Authors**: Khurram Mazher, Saad Bin Nasir

**Updated**: 2025-05-12T13:13:06Z

**Summary**: We present QuantX: a tailored suite of recipes for LLM and VLM quantization. It is capable of quantizing down to 3-bit resolutions with minimal loss in performance. The quantization strategies in QuantX take into account hardware-specific constraints to achieve efficient dequantization during inference ensuring flexible trade-off between runtime speed, memory requirement and model accuracy. Our results demonstrate that QuantX achieves performance within 6% of the unquantized model for LlaVa-v1.6 quantized down to 3-bits for multiple end user tasks and outperforms recently published state-of-the-art quantization techniques. This manuscript provides insights into the LLM quantization process that motivated the range of recipes and options that are incorporated in QuantX.

**Link**: [arxiv](http://arxiv.org/abs/2505.07531v1),  [pdf](http://arxiv.org/pdf/2505.07531v1)

**Tags**: cs.AI 



### Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large   Language Models via a Multi-Paradigm Perspective
**Authors**: Yiyao Yu, Yuxiang Zhang, Dongdong Zhang, Xiao Liang, Hengyuan Zhang, Xingxing Zhang, Mahmoud Khademi, Hany Awadalla, Junjie Wang, Yujiu Yang, Furu Wei

**Updated**: 2025-05-12T13:04:44Z

**Summary**: Large Language Models (LLMs) have made notable progress in mathematical reasoning, yet often rely on single-paradigm reasoning, limiting their effectiveness across diverse tasks. We introduce Chain-of-Reasoning (CoR), a novel unified framework integrating multiple reasoning paradigms--Natural Language Reasoning (NLR), Algorithmic Reasoning (AR), and Symbolic Reasoning (SR)--to enable synergistic collaboration. CoR generates multiple potential answers via different reasoning paradigms and synthesizes them into a coherent final solution. We propose a Progressive Paradigm Training (PPT) strategy for models to progressively master these paradigms, leading to CoR-Math-7B. Experimental results demonstrate that CoR-Math-7B significantly outperforms current SOTA models, achieving up to a 41.0% absolute improvement over GPT-4o in theorem proving and a 15.0% improvement over RL-based methods on the MATH benchmark in arithmetic tasks. These results show the enhanced mathematical comprehension ability of our model, enabling zero-shot generalization across tasks.

**Link**: [arxiv](http://arxiv.org/abs/2501.11110v2),  [pdf](http://arxiv.org/pdf/2501.11110v2)

**Tags**: cs.CL 



### Byam: Fixing Breaking Dependency Updates with Large Language Models
**Authors**: Frank Reyes, May Mahmoud, Federico Bono, Sarah Nadi, Benoit Baudry, Martin Monperrus

**Updated**: 2025-05-12T13:03:26Z

**Summary**: Application Programming Interfaces (APIs) facilitate the integration of third-party dependencies within the code of client applications. However, changes to an API, such as deprecation, modification of parameter names or types, or complete replacement with a new API, can break existing client code. These changes are called breaking dependency updates; It is often tedious for API users to identify the cause of these breaks and update their code accordingly. In this paper, we explore the use of Large Language Models (LLMs) to automate client code updates in response to breaking dependency updates. We evaluate our approach on the BUMP dataset, a benchmark for breaking dependency updates in Java projects. Our approach leverages LLMs with advanced prompts, including information from the build process and from the breaking dependency analysis. We assess effectiveness at three granularity levels: at the build level, the file level, and the individual compilation error level. We experiment with five LLMs: Google Gemini-2.0 Flash, OpenAI GPT4o-mini, OpenAI o3-mini, Alibaba Qwen2.5-32b-instruct, and DeepSeek V3. Our results show that LLMs can automatically repair breaking updates. Among the considered models, OpenAI's o3-mini is the best, able to completely fix 27% of the builds when using prompts that include contextual information such as the buggy line, API differences, error messages, and step-by-step reasoning instructions. Also, it fixes 78% of the individual compilation errors. Overall, our findings demonstrate the potential for LLMs to fix compilation errors due to breaking dependency updates, supporting developers in their efforts to stay up-to-date with changes in their dependencies.

**Link**: [arxiv](http://arxiv.org/abs/2505.07522v1),  [pdf](http://arxiv.org/pdf/2505.07522v1)

**Tags**: cs.SE 



### Clickbait Detection via Large Language Models
**Authors**: Han Wang, Yi Zhu, Ye Wang, Yun Li, Yunhao Yuan, Jipeng Qiang

**Updated**: 2025-05-12T12:57:14Z

**Summary**: Clickbait, which aims to induce users with some surprising and even thrilling headlines for increasing click-through rates, permeates almost all online content publishers, such as news portals and social media. Recently, Large Language Models (LLMs) have emerged as a powerful instrument and achieved tremendous success in a series of NLP downstream tasks. However, it is not yet known whether LLMs can be served as a high-quality clickbait detection system. In this paper, we analyze the performance of LLMs in the few-shot and zero-shot scenarios on several English and Chinese benchmark datasets. Experimental results show that LLMs cannot achieve the best results compared to the state-of-the-art deep and fine-tuning PLMs methods. Different from human intuition, the experiments demonstrated that LLMs cannot make satisfied clickbait detection just by the headlines.

**Link**: [arxiv](http://arxiv.org/abs/2306.09597v4),  [pdf](http://arxiv.org/pdf/2306.09597v4)

**Tags**: cs.CL cs.AI 



### Leveraging Gradients for Unsupervised Accuracy Estimation under   Distribution Shift
**Authors**: Renchunzi Xie, Ambroise Odonnat, Vasilii Feofanov, Ievgen Redko, Jianfeng Zhang, Bo An

**Updated**: 2025-05-12T12:53:48Z

**Summary**: Estimating the test performance of a model, possibly under distribution shift, without having access to the ground-truth labels is a challenging, yet very important problem for the safe deployment of machine learning algorithms in the wild. Existing works mostly rely on information from either the outputs or the extracted features of neural networks to estimate a score that correlates with the ground-truth test accuracy. In this paper, we investigate -- both empirically and theoretically -- how the information provided by the gradients can be predictive of the ground-truth test accuracy even under distribution shifts. More specifically, we use the norm of classification-layer gradients, backpropagated from the cross-entropy loss after only one gradient step over test data. Our intuition is that these gradients should be of higher magnitude when the model generalizes poorly. We provide the theoretical insights behind our approach and the key ingredients that ensure its empirical success. Extensive experiments conducted with various architectures on diverse distribution shifts demonstrate that our method significantly outperforms current state-of-the-art approaches. The code is available at https://github.com/Renchunzi-Xie/GdScore

**Link**: [arxiv](http://arxiv.org/abs/2401.08909v3),  [pdf](http://arxiv.org/pdf/2401.08909v3)

**Tags**: cs.LG 



### ToolACE-DEV: Self-Improving Tool Learning via Decomposition and   EVolution
**Authors**: Xu Huang, Weiwen Liu, Xingshan Zeng, Yuefeng Huang, Xinlong Hao, Yuxian Wang, Yirong Zeng, Chuhan Wu, Yasheng Wang, Ruiming Tang, Defu Lian

**Updated**: 2025-05-12T12:48:30Z

**Summary**: The tool-using capability of large language models (LLMs) enables them to access up-to-date external information and handle complex tasks. Current approaches to enhancing this capability primarily rely on distilling advanced models by data synthesis. However, this method incurs significant costs associated with advanced model usage and often results in data compatibility issues, led by the high discrepancy in the knowledge scope between the advanced model and the target model. To address these challenges, we propose ToolACE-DEV, a self-improving framework for tool learning. First, we decompose the tool-learning objective into sub-tasks that enhance basic tool-making and tool-using abilities. Then, we introduce a self-evolving paradigm that allows lightweight models to self-improve, reducing reliance on advanced LLMs. Extensive experiments validate the effectiveness of our approach across models of varying scales and architectures.

**Link**: [arxiv](http://arxiv.org/abs/2505.07512v1),  [pdf](http://arxiv.org/pdf/2505.07512v1)

**Tags**: cs.CL cs.AI 



### A Sliding Layer Merging Method for Efficient Depth-Wise Pruning in LLMs
**Authors**: Xuan Ding, Rui Sun, Yunjian Zhang, Xiu Yan, Yueqi Zhou, Kaihao Huang, Suzhong Fu, Chuanlong Xie, Yao Zhu

**Updated**: 2025-05-12T12:43:10Z

**Summary**: Compared to width-wise pruning, depth-wise pruning can significantly accelerate inference in resource-constrained scenarios. However, treating the entire Transformer layer as the minimum pruning unit may degrade model performance by indiscriminately discarding the entire information of the layer. This paper reveals the ``Patch-like'' feature relationship between layers in large language models by analyzing the correlation of the outputs of different layers in the reproducing kernel Hilbert space. Building on this observation, we propose a sliding layer merging method that dynamically selects and fuses consecutive layers from top to bottom according to a pre-defined similarity threshold, thereby simplifying the model structure while maintaining its performance. Extensive experiments on LLMs with various architectures and different parameter scales show that our method outperforms existing pruning techniques in both zero-shot inference performance and retraining recovery quality after pruning. In particular, in the experiment with 35% pruning on the Vicuna-7B model, our method achieved a 1.654% improvement in average performance on zero-shot tasks compared to the existing method. Moreover, we further reveal the potential of combining depth pruning with width pruning to enhance the pruning effect. Our codes are available at https://github.com/920927/SLM-a-sliding-layer-merging-method.

**Link**: [arxiv](http://arxiv.org/abs/2502.19159v2),  [pdf](http://arxiv.org/pdf/2502.19159v2)

**Tags**: cs.CV 



### Learning to Reason and Navigate: Parameter Efficient Action Planning   with Large Language Models
**Authors**: Bahram Mohammadi, Ehsan Abbasnejad, Yuankai Qi, Qi Wu, Anton Van Den Hengel, Javen Qinfeng Shi

**Updated**: 2025-05-12T12:38:20Z

**Summary**: The remote embodied referring expression (REVERIE) task requires an agent to navigate through complex indoor environments and localize a remote object specified by high-level instructions, such as "bring me a spoon", without pre-exploration. Hence, an efficient navigation plan is essential for the final success. This paper proposes a novel parameter-efficient action planner using large language models (PEAP-LLM) to generate a single-step instruction at each location. The proposed model consists of two modules, LLM goal planner (LGP) and LoRA action planner (LAP). Initially, LGP extracts the goal-oriented plan from REVERIE instructions, including the target object and room. Then, LAP generates a single-step instruction with the goal-oriented plan, high-level instruction, and current visual observation as input. PEAP-LLM enables the embodied agent to interact with LAP as the path planner on the fly. A simple direct application of LLMs hardly achieves good performance. Also, existing hard-prompt-based methods are error-prone in complicated scenarios and need human intervention. To address these issues and prevent the LLM from generating hallucinations and biased information, we propose a novel two-stage method for fine-tuning the LLM, consisting of supervised fine-tuning (STF) and direct preference optimization (DPO). SFT improves the quality of generated instructions, while DPO utilizes environmental feedback. Experimental results show the superiority of our proposed model on REVERIE compared to the previous state-of-the-art.

**Link**: [arxiv](http://arxiv.org/abs/2505.07500v1),  [pdf](http://arxiv.org/pdf/2505.07500v1)

**Tags**: cs.CV 



### Systolic Arrays and Structured Pruning Co-design for Efficient   Transformers in Edge Systems
**Authors**: Pedro Palacios, Rafael Medina, Jean-Luc Rouas, Giovanni Ansaloni, David Atienza

**Updated**: 2025-05-12T12:15:01Z

**Summary**: Efficient deployment of resource-intensive transformers on edge devices necessitates cross-stack optimization. We thus study the interrelation between structured pruning and systolic acceleration, matching the size of pruned blocks with the systolic array dimensions. In this setting, computations of pruned weight blocks can be skipped, reducing run-time and energy consumption, but potentially impacting quality of service (QoS). To evaluate the trade-offs between systolic array size and sparsity opportunities, we present a novel co-design framework that integrates algorithmic optimization, system simulation, and hardware design. Targeting speech recognition and machine translation using transformers as case study, we analyze how configuration choices across the stack affect performance metrics. Results demonstrate that structured pruning on systems featuring systolic array acceleration can effectively increase performance, while maintaining high QoS levels. Up to 44% system-wide speedups due to structured pruning and quantization were measured, with only 1.4% word error rate degradation on the standard LibriSpeech dataset.

**Link**: [arxiv](http://arxiv.org/abs/2411.10285v2),  [pdf](http://arxiv.org/pdf/2411.10285v2)

**Tags**: cs.AR cs.AI 68T50 C.3; B.5.1; I.2.7 



### Web-Bench: A LLM Code Benchmark Based on Web Standards and Frameworks
**Authors**: Kai Xu, YiWei Mao, XinYi Guan, ZiLong Feng

**Updated**: 2025-05-12T12:06:23Z

**Summary**: The application of large language models (LLMs) in the field of coding is evolving rapidly: from code assistants, to autonomous coding agents, and then to generating complete projects through natural language. Early LLM code benchmarks primarily focused on code generation accuracy, but these benchmarks have gradually become saturated. Benchmark saturation weakens their guiding role for LLMs. For example, HumanEval Pass@1 has reached 99.4% and MBPP 94.2%. Among various attempts to address benchmark saturation, approaches based on software engineering have stood out, but the saturation of existing software engineering benchmarks is rapidly increasing. To address this, we propose a new benchmark, Web-Bench, which contains 50 projects, each consisting of 20 tasks with sequential dependencies. The tasks implement project features in sequence, simulating real-world human development workflows. When designing Web-Bench, we aim to cover the foundational elements of Web development: Web Standards and Web Frameworks. Given the scale and complexity of these projects, which were designed by engineers with 5 to 10 years of experience, each presents a significant challenge. On average, a single project takes 4 to 8 hours for a senior engineer to complete. On our given benchmark agent (Web-Agent), SOTA (Claude 3.7 Sonnet) achieves only 25.1% Pass@1, significantly lower (better) than SWE-Bench's Verified (65.4%) and Full (33.8%) scores. Finally, we discuss that in any development field, Standards and Frameworks represent foundational knowledge and efficiency tools, respectively, and LLMs require optimization tailored to them.

**Link**: [arxiv](http://arxiv.org/abs/2505.07473v1),  [pdf](http://arxiv.org/pdf/2505.07473v1)

**Tags**: cs.AI 



### A Survey on Collaborative Mechanisms Between Large and Small Language   Models
**Authors**: Yi Chen, JiaHao Zhao, HaoHao Han

**Updated**: 2025-05-12T11:48:42Z

**Summary**: Large Language Models (LLMs) deliver powerful AI capabilities but face deployment challenges due to high resource costs and latency, whereas Small Language Models (SLMs) offer efficiency and deployability at the cost of reduced performance. Collaboration between LLMs and SLMs emerges as a crucial paradigm to synergistically balance these trade-offs, enabling advanced AI applications, especially on resource-constrained edge devices. This survey provides a comprehensive overview of LLM-SLM collaboration, detailing various interaction mechanisms (pipeline, routing, auxiliary, distillation, fusion), key enabling technologies, and diverse application scenarios driven by on-device needs like low latency, privacy, personalization, and offline operation. While highlighting the significant potential for creating more efficient, adaptable, and accessible AI, we also discuss persistent challenges including system overhead, inter-model consistency, robust task allocation, evaluation complexity, and security/privacy concerns. Future directions point towards more intelligent adaptive frameworks, deeper model fusion, and expansion into multimodal and embodied AI, positioning LLM-SLM collaboration as a key driver for the next generation of practical and ubiquitous artificial intelligence.

**Link**: [arxiv](http://arxiv.org/abs/2505.07460v1),  [pdf](http://arxiv.org/pdf/2505.07460v1)

**Tags**: cs.AI cs.CL 



### Why Uncertainty Estimation Methods Fall Short in RAG: An Axiomatic   Analysis
**Authors**: Heydar Soudani, Evangelos Kanoulas, Faegheh Hasibi

**Updated**: 2025-05-12T11:47:42Z

**Summary**: Large Language Models (LLMs) are valued for their strong performance across various tasks, but they also produce inaccurate or misleading outputs. Uncertainty Estimation (UE) quantifies the model's confidence and helps users assess response reliability. However, existing UE methods have not been thoroughly examined in scenarios like Retrieval-Augmented Generation (RAG), where the input prompt includes non-parametric knowledge. This paper shows that current UE methods cannot reliably assess correctness in the RAG setting. We further propose an axiomatic framework to identify deficiencies in existing methods and guide the development of improved approaches. Our framework introduces five constraints that an effective UE method should meet after incorporating retrieved documents into the LLM's prompt. Experimental results reveal that no existing UE method fully satisfies all the axioms, explaining their suboptimal performance in RAG. We further introduce a simple yet effective calibration function based on our framework, which not only satisfies more axioms than baseline methods but also improves the correlation between uncertainty estimates and correctness.

**Link**: [arxiv](http://arxiv.org/abs/2505.07459v1),  [pdf](http://arxiv.org/pdf/2505.07459v1)

**Tags**: cs.IR 



### Can Generative AI agents behave like humans? Evidence from laboratory   market experiments
**Authors**: R. Maria del Rio-Chanona, Marco Pangallo, Cars Hommes

**Updated**: 2025-05-12T11:44:46Z

**Summary**: We explore the potential of Large Language Models (LLMs) to replicate human behavior in economic market experiments. Compared to previous studies, we focus on dynamic feedback between LLM agents: the decisions of each LLM impact the market price at the current step, and so affect the decisions of the other LLMs at the next step. We compare LLM behavior to market dynamics observed in laboratory settings and assess their alignment with human participants' behavior. Our findings indicate that LLMs do not adhere strictly to rational expectations, displaying instead bounded rationality, similarly to human participants. Providing a minimal context window i.e. memory of three previous time steps, combined with a high variability setting capturing response heterogeneity, allows LLMs to replicate broad trends seen in human experiments, such as the distinction between positive and negative feedback markets. However, differences remain at a granular level--LLMs exhibit less heterogeneity in behavior than humans. These results suggest that LLMs hold promise as tools for simulating realistic human behavior in economic contexts, though further research is needed to refine their accuracy and increase behavioral diversity.

**Link**: [arxiv](http://arxiv.org/abs/2505.07457v1),  [pdf](http://arxiv.org/pdf/2505.07457v1)

**Tags**: econ.GN cs.AI q-fin.EC 



### How well do LLMs reason over tabular data, really?
**Authors**: Cornelius Wolff, Madelon Hulsebos

**Updated**: 2025-05-12T11:35:28Z

**Summary**: Large Language Models (LLMs) excel in natural language tasks, but less is known about their reasoning capabilities over tabular data. Prior analyses devise evaluation strategies that poorly reflect an LLM's realistic performance on tabular queries. Moreover, we have a limited understanding of the robustness of LLMs towards realistic variations in tabular inputs. Therefore, we ask: Can general-purpose LLMs reason over tabular data, really?, and focus on two questions 1) are tabular reasoning capabilities of general-purpose LLMs robust to real-world characteristics of tabular inputs, and 2) how can we realistically evaluate an LLM's performance on analytical tabular queries? Building on a recent tabular reasoning benchmark, we first surface shortcomings of its multiple-choice prompt evaluation strategy, as well as commonly used free-form text metrics such as SacreBleu and BERT-score. We show that an LLM-as-a-judge procedure yields more reliable performance insights and unveil a significant deficit in tabular reasoning performance of LLMs. We then extend the tabular inputs reflecting three common characteristics in practice: 1) missing values, 2) duplicate entities, and 3) structural variations. Experiments show that the tabular reasoning capabilities of general-purpose LLMs suffer from these variations, stressing the importance of improving their robustness for realistic tabular inputs.

**Link**: [arxiv](http://arxiv.org/abs/2505.07453v1),  [pdf](http://arxiv.org/pdf/2505.07453v1)

**Tags**: cs.AI 



### SwarmSearch: Decentralized Search Engine with Self-Funding Economy
**Authors**: Marcel Gregoriadis, Rowdy Chotkan, Petru Neague, Johan Pouwelse

**Updated**: 2025-05-12T11:33:55Z

**Summary**: Centralized search engines control what we see, read, believe, and vote. Consequently, they raise concerns over information control, censorship, and bias. Decentralized search engines offer a remedy to this problem, but their adoption has been hindered by their inferior quality and lack of a self-sustaining economic framework. We present SwarmSearch, a fully decentralized, AI-powered search engine with a self-funding architecture. Our system is designed for deployment within the decentralized file-sharing software Tribler. SwarmSearch integrates volunteer-based with profit-driven mechanisms to foster an implicit marketplace for resources. Employing the state-of-the-art of AI-based retrieval and relevance ranking, we also aim to close the quality gap between decentralized search and centralized alternatives. Our system demonstrates high retrieval accuracy while showing robustness in the presence of 50% adversarial nodes.

**Link**: [arxiv](http://arxiv.org/abs/2505.07452v1),  [pdf](http://arxiv.org/pdf/2505.07452v1)

**Tags**: cs.DC 



### Transfer Learning with Foundational Models for Time Series Forecasting   using Low-Rank Adaptations
**Authors**: M. Germán-Morales, A. J. Rivera-Rivas, M. J. del Jesus Díaz, C. J. Carmona

**Updated**: 2025-05-12T11:26:58Z

**Summary**: Foundational Models are an emerging widely used technique of GenAI. These models are distinguished by their scalability and the ease with which they can be adapted through the exploitation of Transfer Learning. The availability of high computational power and large datasets have supported their development, achieving a high generalization capacity due to the enormous and heterogeneous amounts of data used in their initial training. These characteristics contribute to a solid base that can be adapted or adjusted to a wide range of tasks, increasing their applicability. This study proposes the methodology LLIAM, a straightforward adaptation of a kind of FM, Large Language Models, for the Time Series Forecasting task. An adequate time-series prompting schema and Low-Rank Adaptations are used to enhance the knowledge of the model with diverse time series datasets, known as the fine-tuning phase. A study divided in two stages has been performed for evaluating the effectiveness of the proposed methodology. Initially, a comparison was made between the performance of LLIAM and different state-of-the-art DL algorithms, including Recurrent Neural Networks and Temporal Convolutional Networks, as well as a LLM-based method, TimeLLM. Following this, a zero-shot study is presented in order to evaluate the generalization capacity of the proposed methodology with time series datasets from unknown domains not considered in the model training. The outcomes of this investigation demonstrate the efficacy of LLIAM, highlighting that this straightforward and general approach can attain competent results without the necessity for applying complex modifications. This work also encourages the use of available resources (such as these pre-trained models) and efficient fine-tuning techniques to avoid unnecessary and costly training, narrowing the gap between the goals of traditional AI and Green AI.

**Link**: [arxiv](http://arxiv.org/abs/2410.11539v3),  [pdf](http://arxiv.org/pdf/2410.11539v3)

**Tags**: cs.LG 



### The Energy Cost of Artificial Intelligence Lifecycle in Communication   Networks
**Authors**: Shih-Kai Chou, Jernej Hribar, Vid Hanžel, Mihael Mohorčič, Carolina Fortuna

**Updated**: 2025-05-12T11:18:06Z

**Summary**: Artificial Intelligence (AI) is being incorporated in several optimization, scheduling, orchestration as well as in native communication network functions. While this paradigm shift results in increased energy consumption, quantifying the end-toend energy consumption of adding intelligence to such systems is particularly challenging. Conventional metrics focus on either communication, computation infrastructure, or model development. To address this, we propose a new metric, the Energy Cost of AI Lifecycle (eCAL) of one AI model in a system. eCAL captures the energy consumption throughout the development and deployment of an AI-model providing intelligence in a wireless communication network by analyzing the complexity of data collection and manipulation in individual components and deriving overall and per-bit energy consumption. We show that the better a model is and the more it is used, the more energy efficient an inference is. For a simple case study, eCAL for making 100 inferences is 2.73 times higher than for 1000 inferences. Additionally, we have developed a modular and extendable opensource simulation tool to enable researchers, practitioners, and engineers to calculate the end-to-end energy cost with various configurations and across various systems, ensuring adaptability to diverse use cases.

**Link**: [arxiv](http://arxiv.org/abs/2408.00540v3),  [pdf](http://arxiv.org/pdf/2408.00540v3)

**Tags**: cs.ET cs.AI cs.LG 



### Lessons learned from establishing a rooftop photovoltaic system   crowdsourced by students and employees at Aarhus University
**Authors**: Marta Victoria, Zhe Zhang, Gorm B. Andresen, Parisa Rahdan, Ebbe K. Gøtske

**Updated**: 2025-05-12T11:11:08Z

**Summary**: Energy communities are promoted in the European legislation as a strategy to enable citizen participation in the energy transition. Solar photovoltaic (PV) systems, due to their distributed nature, present an opportunity to create such communities. At Aarhus University (Denmark), we have established an energy community consisting of a 98-kW rooftop solar PV installation, crowdsourced by students and employees of the university. The participants can buy one or several shares of the installation (which is divided into 900 shares), the electricity is consumed by the university, and the shareowners receive some economic compensation every year. The road to establishing this energy community has been rough, and we have gathered many lessons. In this manuscript, we present the 10 largest challenges which might arise when setting up a university energy community and our particular approach to facing them. Sharing these learnings might pave the way for those willing to establish their own energy community. We also include policy recommendations at the European, national, and municipal levels to facilitate the deployment of energy communities

**Link**: [arxiv](http://arxiv.org/abs/2412.02258v2),  [pdf](http://arxiv.org/pdf/2412.02258v2)

**Tags**: physics.soc-ph 



### Lightweight Multispectral Crop-Weed Segmentation for Precision   Agriculture
**Authors**: Zeynep Galymzhankyzy, Eric Martinson

**Updated**: 2025-05-12T11:08:42Z

**Summary**: Efficient crop-weed segmentation is critical for site-specific weed control in precision agriculture. Conventional CNN-based methods struggle to generalize and rely on RGB imagery, limiting performance under complex field conditions. To address these challenges, we propose a lightweight transformer-CNN hybrid. It processes RGB, Near-Infrared (NIR), and Red-Edge (RE) bands using specialized encoders and dynamic modality integration. Evaluated on the WeedsGalore dataset, the model achieves a segmentation accuracy (mean IoU) of 78.88%, outperforming RGB-only models by 15.8 percentage points. With only 8.7 million parameters, the model offers high accuracy, computational efficiency, and potential for real-time deployment on Unmanned Aerial Vehicles (UAVs) and edge devices, advancing precision weed management.

**Link**: [arxiv](http://arxiv.org/abs/2505.07444v1),  [pdf](http://arxiv.org/pdf/2505.07444v1)

**Tags**: cs.CV I.4.6 



### LEAD: Iterative Data Selection for Efficient LLM Instruction Tuning
**Authors**: Xiaotian Lin, Yanlin Qi, Yizhang Zhu, Themis Palpanas, Chengliang Chai, Nan Tang, Yuyu Luo

**Updated**: 2025-05-12T10:57:51Z

**Summary**: Instruction tuning has emerged as a critical paradigm for improving the capabilities and alignment of large language models (LLMs). However, existing iterative model-aware data selection methods incur significant computational overhead, as they rely on repeatedly performing full-dataset model inference to estimate sample utility for subsequent training iterations, creating a fundamental efficiency bottleneck. In this paper, we propose LEAD, an efficient iterative data selection framework that accurately estimates sample utility entirely within the standard training loop, eliminating the need for costly additional model inference. At its core, LEAD introduces Instance-Level Dynamic Uncertainty (IDU), a theoretically grounded utility function combining instantaneous training loss, gradient-based approximation of loss changes, and exponential smoothing of historical loss signals. To further scale efficiently to large datasets, LEAD employs a two-stage, coarse-to-fine selection strategy, adaptively prioritizing informative clusters through a multi-armed bandit mechanism, followed by precise fine-grained selection of high-utility samples using IDU. Extensive experiments across four diverse benchmarks show that LEAD significantly outperforms state-of-the-art methods, improving average model performance by 6.1%-10.8% while using only 2.5% of the training data and reducing overall training time by 5-10x.

**Link**: [arxiv](http://arxiv.org/abs/2505.07437v1),  [pdf](http://arxiv.org/pdf/2505.07437v1)

**Tags**: cs.LG cs.AI cs.DB 



### I Predict Therefore I Am: Is Next Token Prediction Enough to Learn   Human-Interpretable Concepts from Data?
**Authors**: Yuhang Liu, Dong Gong, Yichao Cai, Erdun Gao, Zhen Zhang, Biwei Huang, Mingming Gong, Anton van den Hengel, Javen Qinfeng Shi

**Updated**: 2025-05-12T10:45:23Z

**Summary**: The remarkable achievements of large language models (LLMs) have led many to conclude that they exhibit a form of intelligence. This is as opposed to explanations of their capabilities based on their ability to perform relatively simple manipulations of vast volumes of data. To illuminate the distinction between these explanations, we introduce a novel generative model that generates tokens on the basis of human-interpretable concepts represented as latent discrete variables. Under mild conditions, even when the mapping from the latent space to the observed space is non-invertible, we establish an identifiability result, i.e., the representations learned by LLMs through next-token prediction can be approximately modeled as the logarithm of the posterior probabilities of these latent discrete concepts given input context, up to an invertible linear transformation. This theoretical finding not only provides evidence that LLMs capture underlying generative factors, but also provide a unified prospective for understanding of the linear representation hypothesis. Taking this a step further, our finding motivates a reliable evaluation of sparse autoencoders by treating the performance of supervised concept extractors as an upper bound. Pushing this idea even further, it inspires a structural variant that enforces dependence among latent concepts in addition to promoting sparsity. Empirically, we validate our theoretical results through evaluations on both simulation data and the Pythia, Llama, and DeepSeek model families, and demonstrate the effectiveness of our structured sparse autoencoder.

**Link**: [arxiv](http://arxiv.org/abs/2503.08980v3),  [pdf](http://arxiv.org/pdf/2503.08980v3)

**Tags**: cs.LG cs.CL 



### LLM-assisted Mutation for Whitebox API Testing
**Authors**: Jia Li, Jiacheng Shen, Yuxin Su, Michael R. Lyu

**Updated**: 2025-05-12T10:31:30Z

**Summary**: Cloud applications heavily rely on APIs to communicate with each other and exchange data. To ensure the reliability of cloud applications, cloud providers widely adopt API testing techniques. Unfortunately, existing API testing approaches are insufficient to reach strict conditions, a problem known as fitness plateaus, due to the lack of gradient provided by coverage metrics. To address this issue, we propose MioHint, a novel white-box API testing approach that leverages the code comprehension capabilities of Large Language Model (LLM) to boost API testing. The key challenge of LLM-based API testing lies in system-level testing, which emphasizes the dependencies between requests and targets across functions and files, thereby making the entire codebase the object of analysis. However, feeding the entire codebase to an LLM is impractical due to its limited context length and short memory. MioHint addresses this challenge by synergizing static analysis with LLMs. We retrieve relevant code with data-dependency analysis at the statement level, including def-use analysis for variables used in the target and function expansion for subfunctions called by the target.   To evaluate the effectiveness of our method, we conducted experiments across 16 real-world REST API services. The findings reveal that MioHint achieves an average increase of 4.95% absolute in line coverage compared to the baseline, EvoMaster, alongside a remarkable factor of 67x improvement in mutation accuracy. Furthermore, our method successfully covers over 57% of hard-to-cover targets while in baseline the coverage is less than 10%.

**Link**: [arxiv](http://arxiv.org/abs/2504.05738v2),  [pdf](http://arxiv.org/pdf/2504.05738v2)

**Tags**: cs.SE 



### None of the Others: a General Technique to Distinguish Reasoning from   Memorization in Multiple-Choice LLM Evaluation Benchmarks
**Authors**: Eva Sánchez Salido, Julio Gonzalo, Guillermo Marco

**Updated**: 2025-05-12T10:30:51Z

**Summary**: In LLM evaluations, reasoning is often distinguished from recall/memorization by performing numerical variations to math-oriented questions. Here we introduce a general variation method for multiple-choice questions that completely dissociates the correct answer from previously seen tokens or concepts, requiring LLMs to understand and reason (rather than memorizing) in order to answer correctly. Using this method, we evaluate state-of-the-art proprietary and open-source LLMs on two datasets available in English and Spanish: the public MMLU benchmark and the private UNED-Access 2024 dataset. Results show that all models experience remarkable accuracy drops under our proposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access 2024, ranging from 10% to 93% across models. Notably, the most accurate model in our experimentation (OpenAI-o3-mini) is not the most robust (DeepSeek-R1-70B), suggesting that the best models in standard evaluations may not be the ones with better reasoning capabilities. Also, we see larger accuracy drops in public (vs private) datasets and questions posed in their original language (vs a manual translation), which are signs of contamination and also point to a relevant role of recall/memorization in current LLMs' answers.

**Link**: [arxiv](http://arxiv.org/abs/2502.12896v3),  [pdf](http://arxiv.org/pdf/2502.12896v3)

**Tags**: cs.CL 



### FullStack Bench: Evaluating LLMs as Full Stack Coders
**Authors**: Bytedance-Seed-Foundation-Code-Team, :, Yao Cheng, Jianfeng Chen, Jie Chen, Li Chen, Liyu Chen, Wentao Chen, Zhengyu Chen, Shijie Geng, Aoyan Li, Bo Li, Bowen Li, Linyi Li, Boyi Liu, Jiaheng Liu, Kaibo Liu, Qi Liu, Shukai Liu, Siyao Liu, Tianyi Liu, Tingkai Liu, Yongfei Liu, Rui Long, Jing Mai, Guanghan Ning, Z. Y. Peng, Kai Shen, Jiahao Su, Jing Su, Tao Sun, Yifan Sun, Yunzhe Tao, Guoyin Wang, Siwei Wang, Xuwu Wang, Yite Wang, Zihan Wang, Jinxiang Xia, Liang Xiang, Xia Xiao, Yongsheng Xiao, Chenguang Xi, Shulin Xin, Jingjing Xu, Shikun Xu, Hongxia Yang, Jack Yang, Yingxiang Yang, Jianbo Yuan, Jun Zhang, Yufeng Zhang, Yuyu Zhang, Shen Zheng, He Zhu, Ming Zhu

**Updated**: 2025-05-12T10:24:14Z

**Summary**: As the capabilities of code large language models (LLMs) continue to expand, their applications across diverse code intelligence domains are rapidly increasing. However, most existing datasets only evaluate limited application domains. To address this gap, we have developed a comprehensive code evaluation dataset FullStack Bench focusing on full-stack programming, which encompasses a wide range of application domains (e.g., basic programming, data analysis, software engineering, mathematics, and machine learning). Besides, to assess multilingual programming capabilities, in FullStack Bench, we design real-world instructions and corresponding unit test cases from 16 widely-used programming languages to reflect real-world usage scenarios rather than simple translations. Moreover, we also release an effective code sandbox execution tool (i.e., SandboxFusion) supporting various programming languages and packages to evaluate the performance of our FullStack Bench efficiently. Comprehensive experimental results on our FullStack Bench demonstrate the necessity and effectiveness of our FullStack Bench and SandboxFusion.

**Link**: [arxiv](http://arxiv.org/abs/2412.00535v6),  [pdf](http://arxiv.org/pdf/2412.00535v6)

**Tags**: cs.AI cs.SE 



### Computational Fact-Checking of Online Discourse: Scoring scientific   accuracy in climate change related news articles
**Authors**: Tim Wittenborg, Constantin Sebastian Tremel, Markus Stocker, Sören Auer

**Updated**: 2025-05-12T10:03:15Z

**Summary**: Democratic societies need reliable information. Misinformation in popular media such as news articles or videos threatens to impair civic discourse. Citizens are, unfortunately, not equipped to verify this content flood consumed daily at increasing rates. This work aims to semi-automatically quantify scientific accuracy of online media. By semantifying media of unknown veracity, their statements can be compared against equally processed trusted sources. We implemented a workflow using LLM-based statement extraction and knowledge graph analysis. Our neurosymbolic system was able to evidently streamline state-of-the-art veracity quantification. Evaluated via expert interviews and a user survey, the tool provides a beneficial veracity indication. This indicator, however, is unable to annotate public media at the required granularity and scale. Further work towards a FAIR (Findable, Accessible, Interoperable, Reusable) ground truth and complementary metrics are required to scientifically support civic discourse.

**Link**: [arxiv](http://arxiv.org/abs/2505.07409v1),  [pdf](http://arxiv.org/pdf/2505.07409v1)

**Tags**: cs.CL 



### Process-Supervised LLM Recommenders via Flow-guided Tuning
**Authors**: Chongming Gao, Mengyao Gao, Chenxiao Fan, Shuai Yuan, Wentao Shi, Xiangnan He

**Updated**: 2025-05-12T09:24:03Z

**Summary**: While large language models (LLMs) are increasingly adapted for recommendation systems via supervised fine-tuning (SFT), this approach amplifies popularity bias due to its likelihood maximization objective, compromising recommendation diversity and fairness. To address this, we present Flow-guided fine-tuning recommender (Flower), which replaces SFT with a Generative Flow Network (GFlowNet) framework that enacts process supervision through token-level reward propagation. Flower's key innovation lies in decomposing item-level rewards into constituent token rewards, enabling direct alignment between token generation probabilities and their reward signals. This mechanism achieves three critical advancements: (1) popularity bias mitigation and fairness enhancement through empirical distribution matching, (2) preservation of diversity through GFlowNet's proportional sampling, and (3) flexible integration of personalized preferences via adaptable token rewards. Experiments demonstrate Flower's superior distribution-fitting capability and its significant advantages over traditional SFT in terms of accuracy, fairness, and diversity, highlighting its potential to improve LLM-based recommendation systems. The implementation is available via https://github.com/MrPeach0301/Flower

**Link**: [arxiv](http://arxiv.org/abs/2503.07377v3),  [pdf](http://arxiv.org/pdf/2503.07377v3)

**Tags**: cs.IR 



### Amortized Safe Active Learning for Real-Time Data Acquisition:   Pretrained Neural Policies from Simulated Nonparametric Functions
**Authors**: Cen-You Li, Marc Toussaint, Barbara Rakitsch, Christoph Zimmer

**Updated**: 2025-05-12T09:21:39Z

**Summary**: Safe active learning (AL) is a sequential scheme for learning unknown systems while respecting safety constraints during data acquisition. Existing methods often rely on Gaussian processes (GPs) to model the task and safety constraints, requiring repeated GP updates and constrained acquisition optimization-incurring in significant computations which are challenging for real-time decision-making. We propose an amortized safe AL framework that replaces expensive online computations with a pretrained neural policy. Inspired by recent advances in amortized Bayesian experimental design, we turn GPs into a pretraining simulator. We train our policy prior to the AL deployment on simulated nonparametric functions, using Fourier feature-based GP sampling and a differentiable, safety-aware acquisition objective. At deployment, our policy selects safe and informative queries via a single forward pass, eliminating the need for GP inference or constrained optimization. This leads to substantial speed improvements while preserving safety and learning quality. Our framework is modular and can be adapted to unconstrained, time-sensitive AL tasks by omitting the safety requirement.

**Link**: [arxiv](http://arxiv.org/abs/2501.15458v2),  [pdf](http://arxiv.org/pdf/2501.15458v2)

**Tags**: cs.LG 



### Examining the Role of LLM-Driven Interactions on Attention and Cognitive   Engagement in Virtual Classrooms
**Authors**: Suleyman Ozdel, Can Sarpkaya, Efe Bozkir, Hong Gao, Enkelejda Kasneci

**Updated**: 2025-05-12T09:21:19Z

**Summary**: Transforming educational technologies through the integration of large language models (LLMs) and virtual reality (VR) offers the potential for immersive and interactive learning experiences. However, the effects of LLMs on user engagement and attention in educational environments remain open questions. In this study, we utilized a fully LLM-driven virtual learning environment, where peers and teachers were LLM-driven, to examine how students behaved in such settings. Specifically, we investigate how peer question-asking behaviors influenced student engagement, attention, cognitive load, and learning outcomes and found that, in conditions where LLM-driven peer learners asked questions, students exhibited more targeted visual scanpaths, with their attention directed toward the learning content, particularly in complex subjects. Our results suggest that peer questions did not introduce extraneous cognitive load directly, as the cognitive load is strongly correlated with increased attention to the learning material. Considering these findings, we provide design recommendations for optimizing VR learning spaces.

**Link**: [arxiv](http://arxiv.org/abs/2505.07377v1),  [pdf](http://arxiv.org/pdf/2505.07377v1)

**Tags**: cs.HC cs.AI 



### A Preliminary Study of Large Language Models for Multilingual   Vulnerability Detection
**Authors**: Junji Yu, Honglin Shu, Michael Fu, Dong Wang, Chakkrit Tantithamthavorn, Yasutaka Kamei, Junjie Chen

**Updated**: 2025-05-12T09:19:31Z

**Summary**: Deep learning-based approaches, particularly those leveraging pre-trained language models (PLMs), have shown promise in automated software vulnerability detection. However, existing methods are predominantly limited to specific programming languages, restricting their applicability in multilingual settings. Recent advancements in large language models (LLMs) offer language-agnostic capabilities and enhanced semantic understanding, presenting a potential solution to this limitation. While existing studies have explored LLMs for vulnerability detection, their detection performance remains unknown for multilingual vulnerabilities. To address this gap, we conducted a preliminary study to evaluate the effectiveness of PLMs and state-of-the-art LLMs across seven popular programming languages. Our findings reveal that the PLM CodeT5P achieves the best performance in multilingual vulnerability detection, particularly in identifying the most critical vulnerabilities. Based on these results, we further discuss the potential of LLMs in advancing real-world multilingual vulnerability detection. This work represents an initial step toward exploring PLMs and LLMs for cross-language vulnerability detection, offering key insights for future research and practical deployment.

**Link**: [arxiv](http://arxiv.org/abs/2505.07376v1),  [pdf](http://arxiv.org/pdf/2505.07376v1)

**Tags**: cs.SE 



### Synthetic Code Surgery: Repairing Bugs and Vulnerabilities with LLMs and   Synthetic Data
**Authors**: David de-Fitero-Dominguez, Antonio Garcia-Cabot, Eva Garcia-Lopez

**Updated**: 2025-05-12T09:14:20Z

**Summary**: This paper presents a novel methodology for enhancing Automated Program Repair (APR) through synthetic data generation utilizing Large Language Models (LLMs). Current APR systems are constrained by the limited availability of high-quality training data encompassing diverse bug types across multiple programming languages. The proposed approach addresses this limitation through a two-phase process: a synthetic sample generation followed by a rigorous quality assessment. Multiple state-of-the-art LLMs were employed to generate approximately 30,000 paired examples of buggy and fixed code across 12 programming languages and 13 bug categories. Subsequently, these samples underwent cross-model evaluation against five criteria: correctness, code quality, security, performance, and completeness. Experimental evaluation on the VulRepair test set dataset showed statistically significant improvements in Perfect Prediction rates, with the quality-filtered synthetic dataset outperforming both baseline and real-world commit data configurations in certain scenarios. The methodology was validated through rigorous statistical testing, including ANOVA and post-hoc Tukey's Honest Significant Difference analysis. Furthermore, the best-performing configurations surpassed existing systems despite using a less computationally intensive decoding strategy. This research establishes a self-bootstrapping paradigm in which LLMs generate and evaluate their own training data, potentially transforming approaches to data scarcity across software engineering tasks and advancing the development of robust, adaptable tools for automated code maintenance.

**Link**: [arxiv](http://arxiv.org/abs/2505.07372v1),  [pdf](http://arxiv.org/pdf/2505.07372v1)

**Tags**: cs.SE cs.AI 



### Steering Large Language Models using Conceptors: Improving   Addition-Based Activation Engineering
**Authors**: Joris Postmus, Steven Abreu

**Updated**: 2025-05-12T08:59:12Z

**Summary**: Large language models have transformed AI, yet reliably controlling their outputs remains a challenge. This paper explores activation engineering, where outputs of pre-trained LLMs are controlled by manipulating their activations at inference time. Unlike traditional methods using a single steering vector, we introduce conceptors - mathematical constructs that represent sets of activation vectors as ellipsoidal regions. Conceptors act as soft projection matrices and offer more precise control over complex activation patterns. Our experiments demonstrate that conceptors outperform traditional methods across multiple steering tasks. We further use Boolean operations on conceptors for combined steering goals that empirically outperform additively combining steering vectors on a set of tasks. These results highlight conceptors as a promising tool for more effective steering of LLMs. Our code is available on github.com/jorispos/conceptorsteering.

**Link**: [arxiv](http://arxiv.org/abs/2410.16314v4),  [pdf](http://arxiv.org/pdf/2410.16314v4)

**Tags**: cs.NE cs.LG 



### BinMetric: A Comprehensive Binary Analysis Benchmark for Large Language   Models
**Authors**: Xiuwei Shang, Guoqiang Chen, Shaoyin Cheng, Benlong Wu, Li Hu, Gangyang Li, Weiming Zhang, Nenghai Yu

**Updated**: 2025-05-12T08:54:07Z

**Summary**: Binary analysis remains pivotal in software security, offering insights into compiled programs without source code access. As large language models (LLMs) continue to excel in diverse language understanding and generation tasks, their potential in decoding complex binary data structures becomes evident. However, the lack of standardized benchmarks in this domain limits the assessment and comparison of LLM's capabilities in binary analysis and hinders the progress of research and practical applications. To bridge this gap, we introduce BinMetric, a comprehensive benchmark designed specifically to evaluate the performance of large language models on binary analysis tasks. BinMetric comprises 1,000 questions derived from 20 real-world open-source projects across 6 practical binary analysis tasks, including decompilation, code summarization, assembly instruction generation, etc., which reflect actual reverse engineering scenarios. Our empirical study on this benchmark investigates the binary analysis capabilities of various state-of-the-art LLMs, revealing their strengths and limitations in this field. The findings indicate that while LLMs show strong potential, challenges still exist, particularly in the areas of precise binary lifting and assembly synthesis. In summary, BinMetric makes a significant step forward in measuring the binary analysis capabilities of LLMs, establishing a new benchmark leaderboard, and our study provides valuable insights for the future development of these LLMs in software security.

**Link**: [arxiv](http://arxiv.org/abs/2505.07360v1),  [pdf](http://arxiv.org/pdf/2505.07360v1)

**Tags**: cs.SE 



### Integrating Expert Knowledge into Logical Programs via LLMs
**Authors**: Franciszek Górski, Oskar Wysocki, Marco Valentino, Andre Freitas

**Updated**: 2025-05-12T08:43:52Z

**Summary**: This paper introduces ExKLoP, a novel framework designed to evaluate how effectively Large Language Models (LLMs) integrate expert knowledge into logical reasoning systems. This capability is especially valuable in engineering, where expert knowledge-such as manufacturer-recommended operational ranges-can be directly embedded into automated monitoring systems. By mirroring expert verification steps, tasks like range checking and constraint validation help ensure system safety and reliability. Our approach systematically evaluates LLM-generated logical rules, assessing both syntactic fluency and logical correctness in these critical validation tasks. We also explore the models' capacity for self-correction via an iterative feedback loop based on code execution outcomes. ExKLoP presents an extensible dataset comprising 130 engineering premises, 950 prompts, and corresponding validation points. It enables comprehensive benchmarking while allowing control over task complexity and scalability of experiments. We leverage the synthetic data creation methodology to conduct extensive empirical evaluation on a diverse set of LLMs including Llama3, Gemma3, Codestral and QwenCoder. The results reveal that most models generate nearly perfect syntactically correct code and exhibit strong performance in translating expert knowledge into correct code. At the same time, while most LLMs produce nearly flawless syntactic output, their ability to correctly implement logical rules varies, as does their capacity for self-improvement. Overall, ExKLoP serves as a robust evaluation platform that streamlines the selection of effective models for self-correcting systems while clearly delineating the types of errors encountered.

**Link**: [arxiv](http://arxiv.org/abs/2502.12275v2),  [pdf](http://arxiv.org/pdf/2502.12275v2)

**Tags**: cs.AI cs.CL cs.MA 



### QUPID: Quantified Understanding for Enhanced Performance, Insights, and   Decisions in Korean Search Engines
**Authors**: Ohjoon Kwon, Changsu Lee, Jihye Back, Lim Sun Suk, Inho Kang, Donghyeon Jeon

**Updated**: 2025-05-12T08:35:09Z

**Summary**: Large language models (LLMs) have been widely used for relevance assessment in information retrieval. However, our study demonstrates that combining two distinct small language models (SLMs) with different architectures can outperform LLMs in this task. Our approach -- QUPID -- integrates a generative SLM with an embedding-based SLM, achieving higher relevance judgment accuracy while reducing computational costs compared to state-of-the-art LLM solutions. This computational efficiency makes QUPID highly scalable for real-world search systems processing millions of queries daily. In experiments across diverse document types, our method demonstrated consistent performance improvements (Cohen's Kappa of 0.646 versus 0.387 for leading LLMs) while offering 60x faster inference times. Furthermore, when integrated into production search pipelines, QUPID improved nDCG@5 scores by 1.9%. These findings underscore how architectural diversity in model combinations can significantly enhance both search relevance and operational efficiency in information retrieval systems.

**Link**: [arxiv](http://arxiv.org/abs/2505.07345v1),  [pdf](http://arxiv.org/pdf/2505.07345v1)

**Tags**: cs.CL cs.AI cs.IR 



### XGrammar: Flexible and Efficient Structured Generation Engine for Large   Language Models
**Authors**: Yixin Dong, Charlie F. Ruan, Yaxing Cai, Ruihang Lai, Ziyi Xu, Yilong Zhao, Tianqi Chen

**Updated**: 2025-05-12T08:20:08Z

**Summary**: The applications of LLM Agents are becoming increasingly complex and diverse, leading to a high demand for structured outputs that can be parsed into code, structured function calls, and embodied agent commands. These developments bring significant demands for structured generation in LLM inference. Context-free grammar is a flexible approach to enable structured generation via constrained decoding. However, executing context-free grammar requires going through several stack states over all tokens in vocabulary during runtime, bringing non-negligible overhead for structured generation. In this paper, we propose XGrammar, a flexible and efficient structure generation engine for large language models. XGrammar accelerates context-free grammar execution by dividing the vocabulary into context-independent tokens that can be prechecked and context-dependent tokens that need to be interpreted during runtime. We further build transformations to expand the grammar context and reduce the number of context-independent tokens. Additionally, we build an efficient persistent stack to accelerate the context-dependent token checks. Finally, we co-design the grammar engine with LLM inference engine to overlap grammar computation with GPU executions. Evaluation results show that XGrammar can achieve up to 100x speedup over existing solutions. Combined with an LLM inference engine, it can generate near-zero overhead structure generation in end-to-end low-LLM serving.

**Link**: [arxiv](http://arxiv.org/abs/2411.15100v3),  [pdf](http://arxiv.org/pdf/2411.15100v3)

**Tags**: cs.CL cs.AI cs.PL 



### The Devil Is in the Details: Tackling Unimodal Spurious Correlations for   Generalizable Multimodal Reward Models
**Authors**: Zichao Li, Xueru Wen, Jie Lou, Yuqiu Ji, Yaojie Lu, Xianpei Han, Debing Zhang, Le Sun

**Updated**: 2025-05-12T08:19:25Z

**Summary**: Multimodal Reward Models (MM-RMs) are crucial for aligning Large Language Models (LLMs) with human preferences, particularly as LLMs increasingly interact with multimodal data. However, we find that MM-RMs trained on existing datasets often struggle to generalize to out-of-distribution data due to their reliance on unimodal spurious correlations, primarily text-only shortcuts within the training distribution, which prevents them from leveraging true multimodal reward functions. To address this, we introduce a Shortcut-aware MM-RM learning algorithm that mitigates this issue by dynamically reweighting training samples, shifting the distribution toward better multimodal understanding, and reducing dependence on unimodal spurious correlations. Our experiments demonstrate significant improvements in generalization, downstream task performance, and scalability, establishing a more robust framework for multimodal reward modeling.

**Link**: [arxiv](http://arxiv.org/abs/2503.03122v3),  [pdf](http://arxiv.org/pdf/2503.03122v3)

**Tags**: cs.CL cs.AI 



### Private LoRA Fine-tuning of Open-Source LLMs with Homomorphic Encryption
**Authors**: Jordan Frery, Roman Bredehoft, Jakub Klemsa, Arthur Meyre, Andrei Stoian

**Updated**: 2025-05-12T08:14:33Z

**Summary**: Preserving data confidentiality during the fine-tuning of open-source Large Language Models (LLMs) is crucial for sensitive applications. This work introduces an interactive protocol adapting the Low-Rank Adaptation (LoRA) technique for private fine-tuning. Homomorphic Encryption (HE) protects the confidentiality of training data and gradients handled by remote worker nodes performing the bulk of computations involving the base model weights. The data owner orchestrates training, requiring minimal local computing power and memory, thus alleviating the need for expensive client-side GPUs. We demonstrate feasibility by fine-tuning a Llama-3.2-1B model, presenting convergence results using HE-compatible quantization and performance benchmarks for HE computations on GPU hardware. This approach enables applications such as confidential knowledge base question answering, private codebase fine-tuning for AI code assistants, AI agents for drafting emails based on a company's email archive, and adapting models to analyze sensitive legal or healthcare documents.

**Link**: [arxiv](http://arxiv.org/abs/2505.07329v1),  [pdf](http://arxiv.org/pdf/2505.07329v1)

**Tags**: cs.CR cs.LG 



### APOLLO: Automated LLM and Lean Collaboration for Advanced Formal   Reasoning
**Authors**: Azim Ospanov, Farzan Farnia, Roozbeh Yousefzadeh

**Updated**: 2025-05-12T08:03:49Z

**Summary**: Formal reasoning and automated theorem proving constitute a challenging subfield of machine learning, in which machines are tasked with proving mathematical theorems using formal languages like Lean. A formal verification system can check whether a formal proof is correct or not almost instantaneously, but generating a completely correct formal proof with large language models (LLMs) remains a formidable task. The usual approach in the literature is to prompt the LLM many times (up to several thousands) until one of the generated proofs passes the verification system. In this work, we present APOLLO (Automated PrOof repair via LLM and Lean cOllaboration), a modular, model-agnostic pipeline that combines the strengths of the Lean compiler with an LLM's reasoning abilities to achieve better proof-generation results at a low sampling budget. Apollo directs a fully automated process in which the LLM generates proofs for theorems, a set of agents analyze the proofs, fix the syntax errors, identify the mistakes in the proofs using Lean, isolate failing sub-lemmas, utilize automated solvers, and invoke an LLM on each remaining goal with a low top-K budget. The repaired sub-proofs are recombined and reverified, iterating up to a user-controlled maximum number of attempts. On the miniF2F benchmark, we establish a new state-of-the-art accuracy of 75.0% among 7B-parameter models while keeping the sampling budget below one thousand. Moreover, Apollo raises the state-of-the-art accuracy for Goedel-Prover-SFT to 65.6% while cutting sample complexity from 25,600 to a few hundred. General-purpose models (o3-mini, o4-mini) jump from 3-7% to over 40% accuracy. Our results demonstrate that targeted, compiler-guided repair of LLM outputs yields dramatic gains in both efficiency and correctness, suggesting a general paradigm for scalable automated theorem proving.

**Link**: [arxiv](http://arxiv.org/abs/2505.05758v2),  [pdf](http://arxiv.org/pdf/2505.05758v2)

**Tags**: cs.AI cs.LO 



### The ANTARES detector: two decades of neutrino searches in the   Mediterranean Sea
**Authors**: A. Albert, S. Alves, M. André, M. Ardid, S. Ardid, J. -J. Aubert, J. Aublin, B. Baret, S. Basa, Y. Becherini, B. Belhorma, F. Benfenati, V. Bertin, S. Biagi, J. Boumaaza, M. Bouta, M. C. Bouwhuis, H. Branzas, R. Bruijn, J. Brunner, J. Busto, B. Caiffi, D. Calvo, S. Campion, A. Capone, F. Carenini, J. Carr, V. Carretero, T. Cartraud, S. Celli, L. Cerisy, M. Chabab, R. Cherkaoui El Moursli, T. Chiarusi, M. Circella, J. A. B. Coelho, A. Coleiro, R. Coniglione, P. Coyle, A. Creusot, A. F. Dìaz, B. De Martino, I. Del Rosso, C. Distefano, I. Di Palma, C. Donzaud, D. Dornic, D. Drouhin, T. Eberl, A. Eddymaoui, T. van Eeden, D. van Eijk, S. El Hedri, N. El Khayati, A. Enzenhöfer, P. Fermani, G. Ferrara, F. Filippini, L. Fusco, S. Gagliardini, J. Garcìa-Méndez, C. Gatius Oliver, P. Gay, N. Geisselbrecht, H. Glotin, R. Gozzini, R. Gracia Ruiz, K. Graf, C. Guidi, L. Haegel, H. van Haren, A. J. Heijboer, Y. Hello, L. Hennig, J. J. Hernàndez-Rey, J. Hössl, F. Huang, G. Illuminati, B. Jisse-Jung, M. de Jong, P. de Jong, M. Kadler, O. Kalekin, U. Katz, A. Kouchner, I. Kreykenbohm, V. Kulikovskiy, R. Lahmann, M. Lamoureux, A. Lazo, D. Lefèvre, E. Leonora, G. Levi, S. Le Stum, S. Loucatos, J. Manczak, M. Marcelin, A. Margiotta, A. Marinelli, J. A. Martìnez-Mora, P. Migliozzi, A. Moussa, R. Muller, S. Navas, E. Nezri, B. 'O Fearraigh, E. Oukacha, A. M. Paun, G. E. Pavalas, S. Pena-Martìnez, M. Perrin-Terrin, P. Piattelli, C. Poirè, V. Popa, T. Pradier, N. Randazzo, D. Real, G. Riccobene, A. Romanov, A. Sànchez Losa, A. Saina, F. Salesa Greus, D. F. E. Samtleben, M. Sanguineti, P. Sapienza, F. Schüssler, J. Seneca, M. Spurio, Th. Stolarczyk, M. Taiuti, Y. Tayalati, B. Vallage, G. Vannoye, V. Van Elewyck, S. Viola, D. Vivolo, J. Wilms, S. Zavatarelli, A. Zegarelli, J. D. Zornoza, J. Zúniga

**Updated**: 2025-05-12T08:02:56Z

**Summary**: Interest for studying cosmic neutrinos using deep-sea detectors has increase after the discovery of a diffuse flux of cosmic neutrinos by the IceCube collaboration and the possibility of wider multi-messenger studies with the observations of gravitational waves. The ANTARES detector was the first neutrino telescope in seawater, operating successfully in the Mediterranean Sea for more than a decade and a half. All challenges related to the operation in the deep sea were accurately addressed by the collaboration. Deployment and connection operations became smoother over time; data taking and constant re-calibration of the detector due to the variable environmental conditions were fully automated. A wealth of results on the subject of astroparticle physics, particle physics and multi-messenger astronomy have been obtained, despite the relative modest size of the detector, paving the way to a new generation of larger undersea detectors. This review summarizes the efforts by the ANTARES collaboration that made the possibility to operate neutrino telescopes in seawater a reality and the results obtained in this endeavor.

**Link**: [arxiv](http://arxiv.org/abs/2504.09473v2),  [pdf](http://arxiv.org/pdf/2504.09473v2)

**Tags**: hep-ex 



### Towards Multi-Agent Reasoning Systems for Collaborative Expertise   Delegation: An Exploratory Design Study
**Authors**: Baixuan Xu, Chunyang Li, Weiqi Wang, Wei Fan, Tianshi Zheng, Haochen Shi, Tao Fan, Yangqiu Song, Qiang Yang

**Updated**: 2025-05-12T07:59:13Z

**Summary**: Designing effective collaboration structure for multi-agent LLM systems to enhance collective reasoning is crucial yet remains under-explored. In this paper, we systematically investigate how collaborative reasoning performance is affected by three key design dimensions: (1) Expertise-Domain Alignment, (2) Collaboration Paradigm (structured workflow vs. diversity-driven integration), and (3) System Scale. Our findings reveal that expertise alignment benefits are highly domain-contingent, proving most effective for contextual reasoning tasks. Furthermore, collaboration focused on integrating diverse knowledge consistently outperforms rigid task decomposition. Finally, we empirically explore the impact of scaling the multi-agent system with expertise specialization and study the computational trade off, highlighting the need for more efficient communication protocol design. This work provides concrete guidelines for configuring specialized multi-agent system and identifies critical architectural trade-offs and bottlenecks for scalable multi-agent reasoning. The code will be made available upon acceptance.

**Link**: [arxiv](http://arxiv.org/abs/2505.07313v1),  [pdf](http://arxiv.org/pdf/2505.07313v1)

**Tags**: cs.CL cs.AI 



### From Prompting to Alignment: A Generative Framework for Query   Recommendation
**Authors**: Erxue Min, Hsiu-Yuan Huang, Min Yang, Xihong Yang, Xin Jia, Yunfang Wu, Hengyi Cai, Junfeng Wang, Shuaiqiang Wang, Dawei Yin

**Updated**: 2025-05-12T07:58:20Z

**Summary**: In modern search systems, search engines often suggest relevant queries to users through various panels or components, helping refine their information needs. Traditionally, these recommendations heavily rely on historical search logs to build models, which suffer from cold-start or long-tail issues. Furthermore, tasks such as query suggestion, completion or clarification are studied separately by specific design, which lacks generalizability and hinders adaptation to novel applications. Despite recent attempts to explore the use of LLMs for query recommendation, these methods mainly rely on the inherent knowledge of LLMs or external sources like few-shot examples, retrieved documents, or knowledge bases, neglecting the importance of the calibration and alignment with user feedback, thus limiting their practical utility. To address these challenges, we first propose a general Generative Query Recommendation (GQR) framework that aligns LLM-based query generation with user preference. Specifically, we unify diverse query recommendation tasks by a universal prompt framework, leveraging the instruct-following capability of LLMs for effective generation. Secondly, we align LLMs with user feedback via presenting a CTR-alignment framework, which involves training a query-wise CTR predictor as a process reward model and employing list-wise preference alignment to maximize the click probability of the generated query list. Furthermore, recognizing the inconsistency between LLM knowledge and proactive search intents arising from the separation of user-initiated queries from models, we align LLMs with user initiative via retrieving co-occurrence queries as side information when historical logs are available.

**Link**: [arxiv](http://arxiv.org/abs/2504.10208v2),  [pdf](http://arxiv.org/pdf/2504.10208v2)

**Tags**: cs.IR cs.LG 



### Uncertainty Profiles for LLMs: Uncertainty Source Decomposition and   Adaptive Model-Metric Selection
**Authors**: Pei-Fu Guo, Yun-Da Tsai, Shou-De Lin

**Updated**: 2025-05-12T07:55:22Z

**Summary**: Large language models (LLMs) often generate fluent but factually incorrect outputs, known as hallucinations, which undermine their reliability in real-world applications. While uncertainty estimation has emerged as a promising strategy for detecting such errors, current metrics offer limited interpretability and lack clarity about the types of uncertainty they capture. In this paper, we present a systematic framework for decomposing LLM uncertainty into four distinct sources, inspired by previous research. We develop a source-specific estimation pipeline to quantify these uncertainty types and evaluate how existing metrics relate to each source across tasks and models. Our results show that metrics, task, and model exhibit systematic variation in uncertainty characteristic. Building on this, we propose a method for task specific metric/model selection guided by the alignment or divergence between their uncertainty characteristics and that of a given task. Our experiments across datasets and models demonstrate that our uncertainty-aware selection strategy consistently outperforms baseline strategies, helping us select appropriate models or uncertainty metrics, and contributing to more reliable and efficient deployment in uncertainty estimation.

**Link**: [arxiv](http://arxiv.org/abs/2505.07309v1),  [pdf](http://arxiv.org/pdf/2505.07309v1)

**Tags**: cs.LG 



### AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong   Pretraining Data Selection
**Authors**: Kai Hua, Steven Wu, Ge Zhang, Ke Shen

**Updated**: 2025-05-12T07:25:51Z

**Summary**: Recently, there has been growing interest in collecting reasoning-intensive pretraining data to improve LLMs' complex reasoning ability. Prior approaches typically rely on supervised classifiers to identify such data, which requires labeling by humans or LLMs, often introducing domain-specific biases. Due to the attention heads being crucial to in-context reasoning, we propose AttentionInfluence, a simple yet effective, training-free method without supervision signal. Our approach enables a small pretrained language model to act as a strong data selector through a simple attention head masking operation. Specifically, we identify retrieval heads and compute the loss difference when masking these heads. We apply AttentionInfluence to a 1.3B-parameter dense model to conduct data selection on the SmolLM corpus of 241B tokens, and mix the SmolLM corpus with the selected subset comprising 73B tokens to pretrain a 7B-parameter dense model using 1T training tokens and WSD learning rate scheduling. Our experimental results demonstrate substantial improvements, ranging from 1.4pp to 3.5pp, across several knowledge-intensive and reasoning-heavy benchmarks (i.e., MMLU, MMLU-Pro, AGIEval-en, GSM8K, and HumanEval). This demonstrates an effective weak-to-strong scaling property, with small models improving the final performance of larger models-offering a promising and scalable path for reasoning-centric data selection.

**Link**: [arxiv](http://arxiv.org/abs/2505.07293v1),  [pdf](http://arxiv.org/pdf/2505.07293v1)

**Tags**: cs.CL 



### Semantic Retention and Extreme Compression in LLMs: Can We Have Both?
**Authors**: Stanislas Laborde, Martin Cousseau, Antoun Yaacoub, Lionel Prevost

**Updated**: 2025-05-12T07:23:19Z

**Summary**: The exponential growth in Large Language Model (LLM) deployment has intensified the need for efficient model compression techniques to reduce computational and memory costs. While pruning and quantization have shown promise, their combined potential remains largely unexplored. In this paper, we examine joint compression and how strategically combining pruning and quantization could yield superior performance-to-compression ratios compared to single-method approaches. Recognizing the challenges in accurately assessing LLM performance, we address key limitations of previous evaluation frameworks and introduce the Semantic Retention Compression Rate (SrCr), a novel metric that quantifies the trade-off between model compression and semantic preservation, facilitating the optimization of pruning-quantization configurations. Experiments demonstrate that our recommended combination achieves, on average, a 20% performance increase compared to an equivalent quantization-only model at the same theoretical compression rate.

**Link**: [arxiv](http://arxiv.org/abs/2505.07289v1),  [pdf](http://arxiv.org/pdf/2505.07289v1)

**Tags**: cs.CL cs.AI cs.LG 68P30 (Primary) 68T07, 68T50 (Secondary) I.2.6; I.5.1; I.2.7 



### Coordinated Spatial Reuse Scheduling With Machine Learning in IEEE   802.11 MAPC Networks
**Authors**: Maksymilian Wojnar, Wojciech Ciężobka, Artur Tomaszewski, Piotr Chołda, Krzysztof Rusek, Katarzyna Kosek-Szott, Jetmir Haxhibeqiri, Jeroen Hoebeke, Boris Bellalta, Anatolij Zubow, Falko Dressler, Szymon Szott

**Updated**: 2025-05-13T10:03:37Z

**Summary**: The densification of Wi-Fi deployments means that fully distributed random channel access is no longer sufficient for high and predictable performance. Therefore, the upcoming IEEE 802.11bn amendment introduces multi-access point coordination (MAPC) methods. This paper addresses a variant of MAPC called coordinated spatial reuse (C-SR), where devices transmit simultaneously on the same channel, with the power adjusted to minimize interference. The C-SR scheduling problem is selecting which devices transmit concurrently and with what settings. We provide a theoretical upper bound model, optimized for either throughput or fairness, which finds the best possible transmission schedule using mixed-integer linear programming. Then, a practical, probing-based approach is proposed which uses multi-armed bandits (MABs), a type of reinforcement learning, to solve the C-SR scheduling problem. We validate both classical (flat) MAB and hierarchical MAB (H-MAB) schemes with simulations and in a testbed. Using H-MABs for C-SR improves aggregate throughput over legacy IEEE 802.11 (on average by 80\% in random scenarios), without reducing the number of transmission opportunities per station. Finally, our framework is lightweight and ready for implementation in Wi-Fi devices.

**Link**: [arxiv](http://arxiv.org/abs/2505.07278v2),  [pdf](http://arxiv.org/pdf/2505.07278v2)

**Tags**: cs.NI 



