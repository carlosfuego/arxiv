# Arxiv Results
## Keyword: kv cache 
 ### SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long   Sequences
**Authors**: Jungyoub Cha, Hyunjong Kim, Sungzoon Cho

**Updated**: 2025-08-22T08:45:04Z

**Summary**: Speculative decoding is a widely adopted technique for accelerating inference in large language models (LLMs), but its performance degrades on long inputs due to increased attention cost and reduced draft accuracy. We introduce SpecExtend, a drop-in enhancement that improves the performance of speculative decoding on long sequences without any additional training. First, SpecExtend integrates efficient attention mechanisms such as FlashAttention and Hybrid Tree Attention into both the draft and target models. To improve draft accuracy and speed on long inputs without retraining, we propose Cross-model Retrieval, a novel KV cache eviction strategy that uses the target model's attention scores to dynamically select relevant context for the draft model. Extensive evaluations on three long-context understanding datasets show that SpecExtend accelerates standard tree-based speculative decoding by up to 2.22x for inputs up to 16K tokens, providing an effective solution for speculative decoding of long sequences. Our code is available at https://github.com/jycha98/SpecExtend .

**Link**: [arxiv](http://arxiv.org/abs/2505.20776v2),  [pdf](http://arxiv.org/pdf/2505.20776v2)

**Tags**: cs.CL cs.AI cs.LG I.2.7; C.4 



### OmniCache: A Trajectory-Oriented Global Perspective on Training-Free   Cache Reuse for Diffusion Transformer Models
**Authors**: Huanpeng Chu, Wei Wu, Guanyu Fen, Yutao Zhang

**Updated**: 2025-08-22T08:36:58Z

**Summary**: Diffusion models have emerged as a powerful paradigm for generative tasks such as image synthesis and video generation, with Transformer architectures further enhancing performance. However, the high computational cost of diffusion Transformers-stemming from a large number of sampling steps and complex per-step computations-presents significant challenges for real-time deployment. In this paper, we introduce OmniCache, a training-free acceleration method that exploits the global redundancy inherent in the denoising process. Unlike existing methods that determine caching strategies based on inter-step similarities and tend to prioritize reusing later sampling steps, our approach originates from the sampling perspective of DIT models. We systematically analyze the model's sampling trajectories and strategically distribute cache reuse across the entire sampling process. This global perspective enables more effective utilization of cached computations throughout the diffusion trajectory, rather than concentrating reuse within limited segments of the sampling procedure.In addition, during cache reuse, we dynamically estimate the corresponding noise and filter it out to reduce its impact on the sampling direction.Extensive experiments demonstrate that our approach accelerates the sampling process while maintaining competitive generative quality, offering a promising and practical solution for efficient deployment of diffusion-based generative models.

**Link**: [arxiv](http://arxiv.org/abs/2508.16212v1),  [pdf](http://arxiv.org/pdf/2508.16212v1)

**Tags**: cs.CV cs.AI cs.LG 



### Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion   Transformers
**Authors**: Shikang Zheng, Liang Feng, Xinyu Wang, Qinming Zhou, Peiliang Cai, Chang Zou, Jiacheng Liu, Yuqi Lin, Junjie Chen, Yue Ma, Linfeng Zhang

**Updated**: 2025-08-22T08:34:03Z

**Summary**: Diffusion Transformers (DiTs) have demonstrated exceptional performance in high-fidelity image and video generation. To reduce their substantial computational costs, feature caching techniques have been proposed to accelerate inference by reusing hidden representations from previous timesteps. However, current methods often struggle to maintain generation quality at high acceleration ratios, where prediction errors increase sharply due to the inherent instability of long-step forecasting. In this work, we adopt an ordinary differential equation (ODE) perspective on the hidden-feature sequence, modeling layer representations along the trajectory as a feature-ODE. We attribute the degradation of existing caching strategies to their inability to robustly integrate historical features under large skipping intervals. To address this, we propose FoCa (Forecast-then-Calibrate), which treats feature caching as a feature-ODE solving problem. Extensive experiments on image synthesis, video generation, and super-resolution tasks demonstrate the effectiveness of FoCa, especially under aggressive acceleration. Without additional training, FoCa achieves near-lossless speedups of 5.50 times on FLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high quality with a 4.53 times speedup on DiT.

**Link**: [arxiv](http://arxiv.org/abs/2508.16211v1),  [pdf](http://arxiv.org/pdf/2508.16211v1)

**Tags**: cs.CV 



### Joint Cache Placement and Routing in Satellite-Terrestrial Edge   Computing Network: A GNN-Enabled DRL Approach
**Authors**: Yuhao Zheng, Ting You, Kejia Peng, Chang Liu

**Updated**: 2025-08-22T07:57:28Z

**Summary**: In this letter, we investigate the problem of joint content caching and routing in satellite-terrestrial edge computing networks (STECNs) to improve caching service for geographically distributed users. To handle the challenges arising from dynamic low Earth orbit (LEO) satellite topologies and heterogeneous content demands, we propose a learning-based framework that integrates graph neural networks (GNNs) with deep reinforcement learning (DRL). The satellite network is represented as a dynamic graph, where GNNs are embedded within the DRL agent to capture spatial and topological dependencies and support routing-aware decision-making. The caching strategy is optimized by formulating the problem as a Markov decision process (MDP) and applying soft actor-critic (SAC) algorithm. Simulation results demonstrate that our approach significantly improves the delivery success rate and reduces communication traffic cost.

**Link**: [arxiv](http://arxiv.org/abs/2508.16184v1),  [pdf](http://arxiv.org/pdf/2508.16184v1)

**Tags**: cs.NI 



### CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing
**Authors**: Yixuan Wang, Haoyu Qiao, Lujun Li, Qingfu Zhu, Wanxiang Che

**Updated**: 2025-08-22T06:55:45Z

**Summary**: Large Language Models (LLMs) confront significant memory challenges due to the escalating KV cache with increasing sequence length. As a crucial technique, existing cross-layer KV cache sharing methods either necessitate modified model architectures with subsequent pre-training or incur significant performance degradation at high compression rates. To mitigate these challenges, we propose CommonKV, a training-free method for cross-layer KV cache compression through adjacent parameters sharing. Inspired by the high similarity observed in cross-layer hidden states, we utilize Singular Value Decomposition (SVD) to achieve weight sharing across adjacent parameters, resulting in a more easily mergeable latent KV cache. Furthermore, we also introduce an adaptive budget allocation strategy. It dynamically assigns compression budgets based on cosine similarity, ensuring that dissimilar caches are not over-compressed. Experiments across multiple backbone models and benchmarks including LongBench and Ruler demonstrate that the proposed method consistently outperforms existing low-rank and cross-layer approaches at various compression ratios. Moreover, we find that the benefits of CommonKV are orthogonal to other quantization and eviction methods. By integrating these approaches, we can ultimately achieve a 98\% compression ratio without significant performance loss.

**Link**: [arxiv](http://arxiv.org/abs/2508.16134v1),  [pdf](http://arxiv.org/pdf/2508.16134v1)

**Tags**: cs.LG cs.AI 



### Lightweight and Fast Real-time Image Enhancement via Decomposition of   the Spatial-aware Lookup Tables
**Authors**: Wontae Kim, Keuntek Lee, Nam Ik Cho

**Updated**: 2025-08-22T06:28:24Z

**Summary**: The image enhancement methods based on 3D lookup tables (3D LUTs) efficiently reduce both model size and runtime by interpolating pre-calculated values at the vertices. However, the 3D LUT methods have a limitation due to their lack of spatial information, as they convert color values on a point-by-point basis. Although spatial-aware 3D LUT methods address this limitation, they introduce additional modules that require a substantial number of parameters, leading to increased runtime as image resolution increases. To address this issue, we propose a method for generating image-adaptive LUTs by focusing on the redundant parts of the tables. Our efficient framework decomposes a 3D LUT into a linear sum of low-dimensional LUTs and employs singular value decomposition (SVD). Furthermore, we enhance the modules for spatial feature fusion to be more cache-efficient. Extensive experimental results demonstrate that our model effectively decreases both the number of parameters and runtime while maintaining spatial awareness and performance.

**Link**: [arxiv](http://arxiv.org/abs/2508.16121v1),  [pdf](http://arxiv.org/pdf/2508.16121v1)

**Tags**: eess.IV cs.CV 



### Dynamic Optimization of Storage Systems Using Reinforcement Learning   Techniques
**Authors**: Chiyu Cheng, Chang Zhou, Yang Zhao

**Updated**: 2025-08-22T03:36:44Z

**Summary**: The exponential growth of data-intensive applications has placed unprecedented demands on modern storage systems, necessitating dynamic and efficient optimization strategies. Traditional heuristics employed for storage performance optimization often fail to adapt to the variability and complexity of contemporary workloads, leading to significant performance bottlenecks and resource inefficiencies. To address these challenges, this paper introduces RL-Storage, a novel reinforcement learning (RL)-based framework designed to dynamically optimize storage system configurations. RL-Storage leverages deep Q-learning algorithms to continuously learn from real-time I/O patterns and predict optimal storage parameters, such as cache size, queue depths, and readahead settings[1].This work underscores the transformative potential of reinforcement learning techniques in addressing the dynamic nature of modern storage systems. By autonomously adapting to workload variations in real time, RL-Storage provides a robust and scalable solution for optimizing storage performance, paving the way for next-generation intelligent storage infrastructures.

**Link**: [arxiv](http://arxiv.org/abs/2501.00068v2),  [pdf](http://arxiv.org/pdf/2501.00068v2)

**Tags**: cs.OS cs.DC cs.LG 



### Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based   Side-Channel Attacks on Fully Associative Randomized Caches
**Authors**: Chris Cao, Gururaj Saileshwar

**Updated**: 2025-08-21T22:45:06Z

**Summary**: Recent work presented at USENIX Security 2025 (SEC'25) claims that occupancy-based attacks can recover AES keys from the MIRAGE randomized cache. In this paper, we examine these claims and find that they arise from a modeling flaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of MIRAGE uses a constant seed to initialize the random number generator used for global evictions in MIRAGE, causing every AES encryption they trace to evict the same deterministic sequence of cache lines. This artificially creates a highly repeatable timing pattern that is not representative of a realistic implementation of MIRAGE, where eviction sequences vary randomly between encryptions. When we instead randomize the eviction seed for each run, reflecting realistic operation, the correlation between AES T-table accesses and attacker runtimes disappears, and the attack fails. These findings show that the reported leakage is an artifact of incorrect modeling, and not an actual vulnerability in MIRAGE.

**Link**: [arxiv](http://arxiv.org/abs/2508.10431v2),  [pdf](http://arxiv.org/pdf/2508.10431v2)

**Tags**: cs.CR 



### Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of   Vision-Language Models
**Authors**: Xinyu Chen, Haotian Zhai, Can Zhang, Xiupeng Shi, Ruirui Li

**Updated**: 2025-08-21T20:13:40Z

**Summary**: In zero-shot setting, test-time adaptation adjusts pre-trained models using unlabeled data from the test phase to enhance performance on unknown test distributions. Existing cache-enhanced TTA methods rely on a low-entropy criterion to select samples for prototype construction, assuming intra-class compactness. However, low-entropy samples may be unreliable under distribution shifts, and the resulting prototypes may not ensure compact intra-class distributions. This study identifies a positive correlation between cache-enhanced performance and intra-class compactness. Based on this observation, we propose a Multi-Cache enhanced Prototype-based Test-Time Adaptation (MCP) featuring three caches: an entropy cache for initializing prototype representations with low-entropy samples, an align cache for integrating visual and textual information to achieve compact intra-class distributions, and a negative cache for prediction calibration using high-entropy samples. We further developed MCP++, a framework incorporating cross-modal prototype alignment and residual learning, introducing prototype residual fine-tuning. Comparative and ablation experiments across 15 downstream tasks demonstrate that the proposed method and framework achieve state-of-the-art generalization performance. Project Page available at: https://zhaihaotian.github.io/MCP-ICCV25/

**Link**: [arxiv](http://arxiv.org/abs/2508.01225v2),  [pdf](http://arxiv.org/pdf/2508.01225v2)

**Tags**: cs.CV cs.AI 



### HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO   Serving and Fast Scaling
**Authors**: Zahra Yousefijamarani, Xinglu Wang, Qian Wang, Morgan Lindsay Heisler, Taha Shabani, Niloofar Gholipour, Parham Yassini, Hong Chang, Kan Chen, Qiantao Zhang, Xiaolong Bai, Jiannan Wang, Ying Xiong, Yong Zhang, Zhenan Fan

**Updated**: 2025-08-21T18:40:20Z

**Summary**: Modern large language model (LLM) serving systems face challenges from highly variable requests with diverse lengths, priorities, and stage-specific service-level objectives (SLOs). Meeting these requires real-time scheduling, rapid and cost-effective scaling, and support for both collocated and disaggregated Prefill/Decode (P/D) architectures.   We present \textbf{HyperFlexis}, a unified LLM serving system that integrates algorithmic and system-level innovations to jointly optimize scheduling and scaling under multiple SLOs. It features a multi-SLO-aware scheduler that leverages budget estimation and request prioritization to ensure proactive SLO compliance for both new and ongoing requests. The system supports prefill- and decode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV cache transfers. It also enables cost-effective scaling decisions, prefill-decode instance linking during scaling, and rapid P/D role transitions. To accelerate scaling and reduce cold-start latency, a device-to-device (D2D) weight transfer mechanism is proposed that lowers weight loading overhead by up to \textbf{19.39$\times$}. These optimizations allow the system to achieve up to \textbf{4.44$\times$} higher SLO attainment, \textbf{65.82\%} lower request latency, and cost parity with state-of-the-art baselines. The code will be released soon.

**Link**: [arxiv](http://arxiv.org/abs/2508.15919v1),  [pdf](http://arxiv.org/pdf/2508.15919v1)

**Tags**: cs.DC cs.AI 



### StreamMem: Query-Agnostic KV Cache Memory for Streaming Video   Understanding
**Authors**: Yanlai Yang, Zhuokai Zhao, Satya Narayan Shukla, Aashu Singh, Shlok Kumar Mishra, Lizhu Zhang, Mengye Ren

**Updated**: 2025-08-21T16:56:29Z

**Summary**: Multimodal large language models (MLLMs) have made significant progress in visual-language reasoning, but their ability to efficiently handle long videos remains limited. Despite recent advances in long-context MLLMs, storing and attending to the key-value (KV) cache for long visual contexts incurs substantial memory and computational overhead. Existing visual compression methods require either encoding the entire visual context before compression or having access to the questions in advance, which is impractical for long video understanding and multi-turn conversational settings. In this work, we propose StreamMem, a query-agnostic KV cache memory mechanism for streaming video understanding. Specifically, StreamMem encodes new video frames in a streaming manner, compressing the KV cache using attention scores between visual tokens and generic query tokens, while maintaining a fixed-size KV memory to enable efficient question answering (QA) in memory-constrained, long-video scenarios. Evaluation on three long video understanding and two streaming video question answering benchmarks shows that StreamMem achieves state-of-the-art performance in query-agnostic KV cache compression and is competitive with query-aware compression approaches.

**Link**: [arxiv](http://arxiv.org/abs/2508.15717v1),  [pdf](http://arxiv.org/pdf/2508.15717v1)

**Tags**: cs.CV cs.AI 



### GoVector: An I/O-Efficient Caching Strategy for High-Dimensional Vector   Nearest Neighbor Search
**Authors**: Yijie Zhou, Shengyuan Lin, Shufeng Gong, Song Yu, Shuhao Fan, Yanfeng Zhang, Ge Yu

**Updated**: 2025-08-21T16:21:46Z

**Summary**: Graph-based high-dimensional vector indices have become a mainstream solution for large-scale approximate nearest neighbor search (ANNS). However, their substantial memory footprint often requires storage on secondary devices, where frequent on-demand loading of graph and vector data leads to I/O becoming the dominant bottleneck, accounting for over 90\% of query latency. Existing static caching strategies mitigate this issue only in the initial navigation phase by preloading entry points and multi-hop neighbors, but they fail in the second phase where query-dependent nodes must be dynamically accessed to achieve high recall. We propose GoVector, an I/O-efficient caching strategy tailored for disk-based graph indices. GoVector combines (1) a static cache that stores entry points and frequently accessed neighbors, and (2) a dynamic cache that adaptively captures nodes with high spatial locality during the second search phase. To further align storage layout with similarity-driven search patterns, GoVector reorders nodes on disk so that similar vectors are colocated on the same or adjacent pages, thereby improving locality and reducing I/O overhead. Extensive experiments on multiple public datasets show that GoVector achieves substantial performance improvements. At 90% recall, it reduces I/O operations by 46% on average, increases query throughput by 1.73x, and lowers query latency by 42% compared to state-of-the-art disk-based graph indexing systems.

**Link**: [arxiv](http://arxiv.org/abs/2508.15694v1),  [pdf](http://arxiv.org/pdf/2508.15694v1)

**Tags**: cs.DB 



### TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated   Prefill \& Decode Inference
**Authors**: Xiaojuan Tang, Fanxu Meng, Pingzhi Tang, Yuxuan Wang, Di Yin, Xing Sun, Muhan Zhang

**Updated**: 2025-08-21T15:25:40Z

**Summary**: Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses key-value states into a low-rank latent vector, caching only this vector to reduce memory. In tensor parallelism (TP), however, attention heads are computed across multiple devices, and each device must load the full cache, eroding the advantage of MLA over Grouped Query Attention (GQA). We propose Tensor-Parallel Latent Attention (TPLA): a scheme that partitions both the latent representation and each head's input dimension across devices, performs attention independently per shard, and then combines results with an all-reduce. TPLA preserves the benefits of a compressed KV cache while unlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in TPLA still leverages the full latent representation, maintaining stronger representational capacity. TPLA is drop-in compatible with models pre-trained using MLA: it supports MLA-style prefilling and enables efficient tensor-parallel decoding without retraining. Applying simple orthogonal transforms -- e.g., the Hadamard transform or PCA -- before TP slicing further mitigates cross-shard interference, yielding minimal accuracy degradation. By reducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x and 1.93x speedups, respectively, at a 32K-token context length while maintaining performance on commonsense and LongBench benchmarks. TPLA can be implemented with FlashAttention-3, enabling practical end-to-end acceleration.

**Link**: [arxiv](http://arxiv.org/abs/2508.15881v1),  [pdf](http://arxiv.org/pdf/2508.15881v1)

**Tags**: cs.LG cs.AI 



### CausalMesh: A Formally Verified Causal Cache for Stateful Serverless   Computing
**Authors**: Haoran Zhang, Zihao Zhang, Shuai Mu, Sebastian Angel, Vincent Liu

**Updated**: 2025-08-21T15:25:30Z

**Summary**: Stateful serverless workflows consist of multiple serverless functions that access state on a remote database. Developers sometimes add a cache layer between the serverless runtime and the database to improve I/O latency. However, in a serverless environment, functions in the same workflow may be scheduled to different nodes with different caches, which can cause non-intuitive anomalies. This paper presents CausalMesh, a novel approach to causally consistent caching in environments where a computation may migrate from one machine to another, such as in serverless computing. CausalMesh is the first cache system that supports coordination-free and abort-free read/write operations and read transactions when clients roam among multiple servers. CausalMesh also supports read-write transactional causal consistency in the presence of client roaming, but at the cost of abort-freedom.   We have formally verified CausalMesh's protocol in Dafny, and our experimental evaluation shows that CausalMesh has lower latency and higher throughput than existing proposals

**Link**: [arxiv](http://arxiv.org/abs/2508.15647v1),  [pdf](http://arxiv.org/pdf/2508.15647v1)

**Tags**: cs.DC 



### GATEBLEED: Exploiting On-Core Accelerator Power Gating for High   Performance & Stealthy Attacks on AI
**Authors**: Joshua Kalyanapu, Farshad Dizani, Darsh Asher, Azam Ghanbari, Rosario Cammarota, Aydin Aysu, Samira Mirbagher Ajorpaz

**Updated**: 2025-08-21T14:58:12Z

**Summary**: As power consumption from AI training and inference continues to increase, AI accelerators are being integrated directly into the CPU. Intel's Advanced Matrix Extensions (AMX) is one such example, debuting on the 4th generation Intel Xeon Scalable CPU. We discover a timing side and covert channel, GATEBLEED, caused by the aggressive power gating utilized to keep the CPU within operating limits. We show that the GATEBLEED side channel is a threat to AI privacy as many ML models such as transformers and CNNs make critical computationally-heavy decisions based on private values like confidence thresholds and routing logits. Timing delays from selective powering down of AMX components mean that each matrix multiplication is a potential leakage point when executed on the AMX accelerator. Our research identifies over a dozen potential gadgets across popular ML libraries (HuggingFace, PyTorch, TensorFlow, etc.), revealing that they can leak sensitive and private information. GATEBLEED poses a risk for local and remote timing inference, even under previous protective measures. GATEBLEED can be used as a high performance, stealthy remote covert channel and a generic magnifier for timing transmission channels, capable of bypassing traditional cache defenses to leak arbitrary memory addresses and evading state of the art microarchitectural attack detectors under realistic network conditions and system configurations in which previous attacks fail. We implement an end-to-end microarchitectural inference attack on a transformer model optimized with Intel AMX, achieving a membership inference accuracy of 81% and a precision of 0.89. In a CNN-based or transformer-based mixture-of-experts model optimized with Intel AMX, we leak expert choice with 100% accuracy. To our knowledge, this is the first side-channel attack on AI privacy that exploits hardware optimizations.

**Link**: [arxiv](http://arxiv.org/abs/2507.17033v2),  [pdf](http://arxiv.org/pdf/2507.17033v2)

**Tags**: cs.CR 



### Efficient Mixed-Precision Large Language Model Inference with TurboMind
**Authors**: Li Zhang, Youhe Jiang, Guoliang He, Xin Chen, Han Lv, Qian Yao, Fangcheng Fu, Kai Chen

**Updated**: 2025-08-21T14:24:52Z

**Summary**: Mixed-precision inference techniques reduce the memory and computational demands of Large Language Models (LLMs) by applying hybrid precision formats to model weights, activations, and KV caches. This work introduces mixed-precision LLM inference techniques that encompass (i) systematic memory and compute optimization across hierarchical storage and tensor core architectures, and (ii) comprehensive end-to-end mixed-precision optimization across diverse precision formats and hardware configurations. Our approach features two novel mixed-precision pipelines designed for optimal hardware utilization: a General Matrix Multiply (GEMM) pipeline that optimizes matrix operations through offline weight packing and online acceleration, and an attention pipeline that enables efficient attention computation with arbitrary Query, Key, and Value precision combinations. The key implementation of the pipelines includes (i) hardware-aware weight packing for automatic format optimization, (ii) adaptive head alignment for efficient attention computation, (iii) instruction-level parallelism for memory hierarchy exploitation, and (iv) KV memory loading pipeline for enhanced inference efficiency. We conduct comprehensive evaluations across 16 popular LLMs and 4 representative GPU architectures. Results demonstrate that our approach achieves up to 61% lower serving latency (30% on average) and up to 156% higher throughput (58% on average) in mixed-precision workloads compared to existing mixed-precision frameworks, establishing consistent performance improvements across all tested configurations and hardware types. This work is integrated into TurboMind, a high-performance inference engine of the LMDeploy project, which is open-sourced and publicly available at https://github.com/InternLM/lmdeploy.

**Link**: [arxiv](http://arxiv.org/abs/2508.15601v1),  [pdf](http://arxiv.org/pdf/2508.15601v1)

**Tags**: cs.DC cs.PF 



### Time-Optimal Directed q-Analysis
**Authors**: Felix Windisch, Florian Unger

**Updated**: 2025-08-21T13:57:09Z

**Summary**: Directed q-analysis is a recent extension of q-analysis, an established method for extracting structure from networks, to directed graphs. Until recently, a lack of efficient algorithms heavily restricted the application of this technique: Previous approaches scale with the square of the input size, which is also the maximal size of the output, rendering such approaches worst-case optimal. In practice, output sizes of relevant networks are usually far from the worst case, a fact that could be exploited by an (efficient) output-sensitive algorithm. We develop such an algorithm and formally describe it in detail. The key insight, obtained by carefully studying various approaches to directed q-analysis and how they relate to each other, is that inverting the order of computation leads to significant complexity gains. Targeted precomputation and caching tactics further reduce the introduced overhead, enough to achieve (under mild assumptions) a time complexity that is linear in output size. The resulting algorithm for performing directed q-analysis is shown to be time-optimal.

**Link**: [arxiv](http://arxiv.org/abs/2508.15583v1),  [pdf](http://arxiv.org/pdf/2508.15583v1)

**Tags**: cs.DS 



### QVecOpt: An Efficient Storage and Computing Opti-mization Framework for   Large-scale Quantum State Simulation
**Authors**: Mingyang Yu, Haorui Yang, Donglin Wang, Desheng Kong, Ji Du, Yulong Fu, Jing Xu

**Updated**: 2025-08-21T13:24:13Z

**Summary**: In response to the challenges in large-scale quantum state simulation on classical computing platforms, including memory limits, frequent disk I/O, and high computational complexity, this study builds upon a previously proposed hierarchical storage-based quantum simulation system and introduces an optimization framework, the Quantum Vector Optimization Framework (QVecOpt). QVecOpt integrates four strategies: amplitude pairing, cache optimization, block storage optimization, and parallel optimization. These collectively enhance state vector storage and computational scheduling. The amplitude pairing mechanism locates relevant amplitude pairs via bitwise XOR, reducing traversal complexity of single-qubit gates from $O(2^n)$ to $O(1)$. Cache optimization pre-allocates buffers and loads only required data, cutting disk I/O. Block storage optimization partitions the state vector for on-demand loading and local updates, reducing redundant access. Parallel optimization distributes the state vector across nodes for collaborative computation, achieving near-linear speedup. Complexity analysis shows that, compared with hierarchical storage simulation, the method reduces state vector traversals for single-qubit gates from $2^n$ to 1, removing the main bottleneck. It also lowers computational and I/O complexity from $O(2^n)$ to $O(2^n/C)$ and $O(2^n/B)$. In simulations of 16-29 qubits, efficiency improves nearly tenfold, breaking the memory bottleneck of existing tools and enabling high-bit quantum circuit simulations beyond traditional methods. This work provides an efficient, scalable solution for classical simulation of large-scale quantum computation with significant academic and practical value.

**Link**: [arxiv](http://arxiv.org/abs/2508.15545v1),  [pdf](http://arxiv.org/pdf/2508.15545v1)

**Tags**: cs.ET 



### 3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt
**Authors**: Lukas Höllein, Aljaž Božič, Michael Zollhöfer, Matthias Nießner

**Updated**: 2025-08-21T12:52:11Z

**Summary**: We present 3DGS-LM, a new method that accelerates the reconstruction of 3D Gaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored Levenberg-Marquardt (LM). Existing methods reduce the optimization time by decreasing the number of Gaussians or by improving the implementation of the differentiable rasterizer. However, they still rely on the ADAM optimizer to fit Gaussian parameters of a scene in thousands of iterations, which can take up to an hour. To this end, we change the optimizer to LM that runs in conjunction with the 3DGS differentiable rasterizer. For efficient GPU parallization, we propose a caching data structure for intermediate gradients that allows us to efficiently calculate Jacobian-vector products in custom CUDA kernels. In every LM iteration, we calculate update directions from multiple image subsets using these kernels and combine them in a weighted mean. Overall, our method is 20% faster than the original 3DGS while obtaining the same reconstruction quality. Our optimization is also agnostic to other methods that acclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.

**Link**: [arxiv](http://arxiv.org/abs/2409.12892v2),  [pdf](http://arxiv.org/pdf/2409.12892v2)

**Tags**: cs.CV 



### TrimCaching: Parameter-sharing Edge Caching for AI Model Downloading
**Authors**: Guanqiao Qu, Zheng Lin, Qian Chen, Jian Li, Fangming Liu, Xianhao Chen, Kaibin Huang

**Updated**: 2025-08-21T11:43:48Z

**Summary**: Next-generation mobile networks are expected to facilitate fast AI model downloading to end users. By caching models on edge servers, mobile networks can deliver models to end users with low latency, resulting in a paradigm of edge model caching. In this paper, we develop a novel model placement framework, called parameter-sharing model caching (TrimCaching). TrimCaching exploits the key observation that a wide range of AI models, such as convolutional neural networks or large language models, can share a significant proportion of parameter blocks containing reusable knowledge, thereby improving storage efficiency. To this end, we formulate a parameter-sharing model placement problem to maximize the cache hit ratio in multi-edge wireless networks by balancing the fundamental tradeoff between storage efficiency and service latency. We show that the formulated problem is a submodular maximization problem with submodular constraints, for which no polynomial-time approximation algorithm exists. To tackle this challenge, we study an important special case, where a small fixed number of parameter blocks are shared across models, which often holds in practice. In such a case, a polynomial-time algorithm with a $\left(1-\epsilon\right)/2$-approximation guarantee is developed. Subsequently, we address the original problem for the general case by developing a greedy algorithm. Simulation results demonstrate that the proposed TrimCaching framework significantly improves the cache hit ratio compared with state-of-the-art content caching without exploiting shared parameters in AI models.

**Link**: [arxiv](http://arxiv.org/abs/2404.14204v3),  [pdf](http://arxiv.org/pdf/2404.14204v3)

**Tags**: cs.NI 



### Gorgeous: Revisiting the Data Layout for Disk-Resident High-Dimensional   Vector Search
**Authors**: Peiqi Yin, Xiao Yan, Qihui Zhou, Hui Li, Xiaolu Li, Lin Zhang, Meiling Wang, Xin Yao, James Cheng

**Updated**: 2025-08-21T06:26:18Z

**Summary**: Similarity-based vector search underpins many important applications, but a key challenge is processing massive vector datasets (e.g., in TBs). To reduce costs, some systems utilize SSDs as the primary data storage. They employ a proximity graph, which connects similar vectors to form a graph and is the state-of-the-art index for vector search. However, these systems are hindered by sub-optimal data layouts that fail to effectively utilize valuable memory space to reduce disk access and suffer from poor locality for accessing disk-resident data. Through extensive profiling and analysis, we found that the structure of the proximity graph index is accessed more frequently than the vectors themselves, yet existing systems do not distinguish between the two. To address this problem, we design the Gorgeous system with the principle of prioritizing graph structure over vectors. Specifically, Gorgeous features a memory cache that keeps the adjacency lists of graph nodes to improve cache hits and a disk block format that explicitly stores neighbors' adjacency lists along with a vector to enhance data locality. Experimental results show that Gorgeous consistently outperforms two state-of-the-art disk-based systems for vector search, boosting average query throughput by over 60% and reducing query latency by over 35%.

**Link**: [arxiv](http://arxiv.org/abs/2508.15290v1),  [pdf](http://arxiv.org/pdf/2508.15290v1)

**Tags**: cs.DB 



### SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache   Channel Pruning
**Authors**: Huanxuan Liao, Yixing Xu, Shizhu He, Guanchen Li, Xuanwu Yin, Dong Li, Emad Barsoum, Jun Zhao, Kang Liu

**Updated**: 2025-08-21T03:48:28Z

**Summary**: Long-context inference in large language models (LLMs) is increasingly constrained by the KV cache bottleneck: memory usage grows linearly with sequence length, while attention computation scales quadratically. Existing approaches address this issue by compressing the KV cache along the temporal axis through strategies such as token eviction or merging to reduce memory and computational overhead. However, these methods often neglect fine-grained importance variations across feature dimensions (i.e., the channel axis), thereby limiting their ability to effectively balance efficiency and model accuracy. In reality, we observe that channel saliency varies dramatically across both queries and positions: certain feature channels carry near-zero information for a given query, while others spike in relevance. To address this oversight, we propose SPARK, a training-free plug-and-play method that applies unstructured sparsity by pruning KV at the channel level, while dynamically restoring the pruned entries during attention score computation. Notably, our approach is orthogonal to existing KV compression and quantization techniques, making it compatible for integration with them to achieve further acceleration. By reducing channel-level redundancy, SPARK enables processing of longer sequences within the same memory budget. For sequences of equal length, SPARK not only preserves or improves model accuracy but also reduces KV cache storage by over 30% compared to eviction-based methods. Furthermore, even with an aggressive pruning ratio of 80%, SPARK maintains performance with less degradation than 5% compared to the baseline eviction method, demonstrating its robustness and effectiveness. Our code will be available at https://github.com/Xnhyacinth/SparK.

**Link**: [arxiv](http://arxiv.org/abs/2508.15212v1),  [pdf](http://arxiv.org/pdf/2508.15212v1)

**Tags**: cs.CL cs.AI cs.LG 



### MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in   Mixture-of-Experts LLMs
**Authors**: Ruyi Ding, Tianhong Xu, Xinyi Shen, Aidong Adam Ding, Yunsi Fei

**Updated**: 2025-08-20T20:02:35Z

**Summary**: The transformer architecture has become a cornerstone of modern AI, fueling remarkable progress across applications in natural language processing, computer vision, and multimodal learning. As these models continue to scale explosively for performance, implementation efficiency remains a critical challenge. Mixture of Experts (MoE) architectures, selectively activating specialized subnetworks (experts), offer a unique balance between model accuracy and computational cost. However, the adaptive routing in MoE architectures, where input tokens are dynamically directed to specialized experts based on their semantic meaning inadvertently opens up a new attack surface for privacy breaches. These input-dependent activation patterns leave distinctive temporal and spatial traces in hardware execution, which adversaries could exploit to deduce sensitive user data. In this work, we propose MoEcho, discovering a side channel analysis based attack surface that compromises user privacy on MoE based systems. Specifically, in MoEcho, we introduce four novel architectural side channels on different computing platforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and Performance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting these vulnerabilities, we propose four attacks that effectively breach user privacy in large language models (LLMs) and vision language models (VLMs) based on MoE architectures: Prompt Inference Attack, Response Reconstruction Attack, Visual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first runtime architecture level security analysis of the popular MoE structure common in modern transformers, highlighting a serious security and privacy threat and calling for effective and timely safeguards when harnessing MoE based models for developing efficient large scale AI services.

**Link**: [arxiv](http://arxiv.org/abs/2508.15036v1),  [pdf](http://arxiv.org/pdf/2508.15036v1)

**Tags**: cs.CR cs.AI 



### Rethinking the Potential of Layer Freezing for Efficient DNN Training
**Authors**: Chence Yang, Ci Zhang, Lei Lu, Qitao Tan, Sheng Li, Ao Li, Xulong Tang, Shaoyi Huang, Jinzhen Wang, Guoming Li, Jundong Li, Xiaoming Zhai, Jin Lu, Geng Yuan

**Updated**: 2025-08-20T19:54:41Z

**Summary**: With the growing size of deep neural networks and datasets, the computational costs of training have significantly increased. The layer-freezing technique has recently attracted great attention as a promising method to effectively reduce the cost of network training. However, in traditional layer-freezing methods, frozen layers are still required for forward propagation to generate feature maps for unfrozen layers, limiting the reduction of computation costs. To overcome this, prior works proposed a hypothetical solution, which caches feature maps from frozen layers as a new dataset, allowing later layers to train directly on stored feature maps. While this approach appears to be straightforward, it presents several major challenges that are severely overlooked by prior literature, such as how to effectively apply augmentations to feature maps and the substantial storage overhead introduced. If these overlooked challenges are not addressed, the performance of the caching method will be severely impacted and even make it infeasible. This paper is the first to comprehensively explore these challenges and provides a systematic solution. To improve training accuracy, we propose \textit{similarity-aware channel augmentation}, which caches channels with high augmentation sensitivity with a minimum additional storage cost. To mitigate storage overhead, we incorporate lossy data compression into layer freezing and design a \textit{progressive compression} strategy, which increases compression rates as more layers are frozen, effectively reducing storage costs. Finally, our solution achieves significant reductions in training cost while maintaining model accuracy, with a minor time overhead. Additionally, we conduct a comprehensive evaluation of freezing and compression strategies, providing insights into optimizing their application for efficient DNN training.

**Link**: [arxiv](http://arxiv.org/abs/2508.15033v1),  [pdf](http://arxiv.org/pdf/2508.15033v1)

**Tags**: cs.LG 



### Diverse Negative Sampling for Implicit Collaborative Filtering
**Authors**: Yueqing Xuan, Kacper Sokol, Mark Sanderson, Jeffrey Chan

**Updated**: 2025-08-20T06:48:54Z

**Summary**: Implicit collaborative filtering recommenders are usually trained to learn user positive preferences. Negative sampling, which selects informative negative items to form negative training data, plays a crucial role in this process. Since items are often clustered in the latent space, existing negative sampling strategies normally oversample negative items from the dense regions. This leads to homogeneous negative data and limited model expressiveness. In this paper, we propose Diverse Negative Sampling (DivNS), a novel approach that explicitly accounts for diversity in negative training data during the negative sampling process. DivNS first finds hard negative items with large preference scores and constructs user-specific caches that store unused but highly informative negative samples. Then, its diversity-augmented sampler selects a diverse subset of negative items from the cache while ensuring dissimilarity from the user's hard negatives. Finally, a synthetic negatives generator combines the selected diverse negatives with hard negatives to form more effective training data. The resulting synthetic negatives are both informative and diverse, enabling recommenders to learn a broader item space and improve their generalisability. Extensive experiments on four public datasets demonstrate the effectiveness of DivNS in improving recommendation quality while maintaining computational efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2508.14468v1),  [pdf](http://arxiv.org/pdf/2508.14468v1)

**Tags**: cs.IR 



### You Only Evaluate Once: A Tree-based Rerank Method at Meituan
**Authors**: Shuli Wang, Yinqiu Huang, Changhao Li, Yuan Zhou, Yonggang Liu, Yongqiang Zhang, Yinhua Zhu, Haitao Wang, Xingxing Wang

**Updated**: 2025-08-20T04:36:25Z

**Summary**: Reranking plays a crucial role in modern recommender systems by capturing the mutual influences within the list. Due to the inherent challenges of combinatorial search spaces, most methods adopt a two-stage search paradigm: a simple General Search Unit (GSU) efficiently reduces the candidate space, and an Exact Search Unit (ESU) effectively selects the optimal sequence. These methods essentially involve making trade-offs between effectiveness and efficiency, while suffering from a severe \textbf{inconsistency problem}, that is, the GSU often misses high-value lists from ESU. To address this problem, we propose YOLOR, a one-stage reranking method that removes the GSU while retaining only the ESU. Specifically, YOLOR includes: (1) a Tree-based Context Extraction Module (TCEM) that hierarchically aggregates multi-scale contextual features to achieve "list-level effectiveness", and (2) a Context Cache Module (CCM) that enables efficient feature reuse across candidate permutations to achieve "permutation-level efficiency". Extensive experiments across public and industry datasets validate YOLOR's performance, and we have successfully deployed YOLOR on the Meituan food delivery platform.

**Link**: [arxiv](http://arxiv.org/abs/2508.14420v1),  [pdf](http://arxiv.org/pdf/2508.14420v1)

**Tags**: cs.IR 



### DPad: Efficient Diffusion Language Models with Suffix Dropout
**Authors**: Xinhua Chen, Sitao Huang, Cong Guo, Chiyue Wei, Yintao He, Jianyi Zhang, Hai "Helen" Li, Yiran Chen

**Updated**: 2025-08-23T20:28:45Z

**Summary**: Diffusion-based Large Language Models (dLLMs) parallelize text generation by framing decoding as a denoising process, but suffer from high computational overhead since they predict all future suffix tokens at each step while retaining only a small fraction. We propose Diffusion Scratchpad (DPad), a training-free method that restricts attention to a small set of nearby suffix tokens, preserving fidelity while eliminating redundancy. DPad integrates two strategies: (i) a sliding window, which maintains a fixed-length suffix window, and (ii) distance-decay dropout, which deterministically removes distant suffix tokens before attention computation. This simple design is compatible with existing optimizations such as prefix caching and can be implemented with only a few lines of code. Comprehensive evaluations across multiple benchmarks on LLaDA-1.5 and Dream models demonstrate that DPad delivers up to $\mathbf{61.4\times}$ speedup over vanilla dLLMs while maintaining comparable accuracy, highlighting its potential for efficient and scalable long-sequence inference. Our code is available at https://github.com/Crys-Chen/DPad.

**Link**: [arxiv](http://arxiv.org/abs/2508.14148v2),  [pdf](http://arxiv.org/pdf/2508.14148v2)

**Tags**: cs.CL cs.LG 



### Scavenger+: Revisiting Space-Time Tradeoffs in Key-Value Separated   LSM-trees
**Authors**: Jianshun Zhang, Fang Wang, Jiaxin Ou, Yi Wang, Ming Zhao, Sheng Qiu, Junxun Huang, Baoquan Li, Peng Fang, Dan Feng

**Updated**: 2025-08-19T15:26:36Z

**Summary**: Key-Value Stores (KVS) based on log-structured merge-trees (LSM-trees) are widely used in storage systems but face significant challenges, such as high write amplification caused by compaction. KV-separated LSM-trees address write amplification but introduce significant space amplification, a critical concern in cost-sensitive scenarios. Garbage collection (GC) can reduce space amplification, but existing strategies are often inefficient and fail to account for workload characteristics. Moreover, current key-value (KV) separated LSM-trees overlook the space amplification caused by the index LSM-tree. In this paper, we systematically analyze the sources of space amplification in KV-separated LSM-trees and propose Scavenger+, which achieves a better performance-space trade-off. Scavenger+ introduces (1) an I/O-efficient garbage collection scheme to reduce I/O overhead, (2) a space-aware compaction strategy based on compensated size to mitigate index-induced space amplification, and (3) a dynamic GC scheduler that adapts to system load to make better use of CPU and storage resources. Extensive experiments demonstrate that Scavenger+ significantly improves write performance and reduces space amplification compared to state-of-the-art KV-separated LSM-trees, including BlobDB, Titan, and TerarkDB.

**Link**: [arxiv](http://arxiv.org/abs/2508.13935v1),  [pdf](http://arxiv.org/pdf/2508.13935v1)

**Tags**: cs.DB 



### Scavenger: Better Space-Time Trade-Offs for Key-Value Separated   LSM-trees
**Authors**: Jianshun Zhang, Fang Wang, Sheng Qiu, Yi Wang, Jiaxin Ou, Junxun Huang, Baoquan Li, Peng Fang, Dan Feng

**Updated**: 2025-08-19T15:08:39Z

**Summary**: Key-Value Stores (KVS) implemented with log-structured merge-tree (LSM-tree) have gained widespread acceptance in storage systems. Nonetheless, a significant challenge arises in the form of high write amplification due to the compaction process. While KV-separated LSM-trees successfully tackle this issue, they also bring about substantial space amplification problems, a concern that cannot be overlooked in cost-sensitive scenarios. Garbage collection (GC) holds significant promise for space amplification reduction, yet existing GC strategies often fall short in optimization performance, lacking thorough consideration of workload characteristics. Additionally, current KV-separated LSM-trees also ignore the adverse effect of the space amplification in the index LSM-tree. In this paper, we systematically analyze the sources of space amplification of KV-separated LSM-trees and introduce Scavenger, which achieves a better trade-off between performance and space amplification. Scavenger initially proposes an I/O-efficient garbage collection scheme to reduce I/O overhead and incorporates a space-aware compaction strategy based on compensated size to minimize the space amplification of index LSM-trees. Extensive experiments show that Scavenger significantly improves write performance and achieves lower space amplification than other KV-separated LSM-trees (including BlobDB, Titan, and TerarkDB).

**Link**: [arxiv](http://arxiv.org/abs/2508.13909v1),  [pdf](http://arxiv.org/pdf/2508.13909v1)

**Tags**: cs.DB 



### Tight Inter-Core Cache Contention Analysis for WCET Estimation on   Multicore Systems
**Authors**: Shuai Zhao, Jieyu Jiang, Shenlin Cai, Yaowei Liang, Chen Jie, Yinjie Fang, Wei Zhang, Guoquan Zhang, Yaoyao Gu, Xiang Xiao, Wei Qin, Xiangzhen Ouyang, Wanli Chang

**Updated**: 2025-08-19T14:30:41Z

**Summary**: WCET (Worst-Case Execution Time) estimation on multicore architecture is particularly challenging mainly due to the complex accesses over cache shared by multiple cores. Existing analysis identifies possible contentions between parallel tasks by leveraging the partial order of the tasks or their program regions. Unfortunately, they overestimate the number of cache misses caused by a remote block access without considering the actual cache state and the number of accesses. This paper reports a new analysis for inter-core cache contention. Based on the order of program regions in a task, we first identify memory references that could be affected if a remote access occurs in a region. Afterwards, a fine-grained contention analysis is constructed that computes the number of cache misses based on the access quantity of local and remote blocks. We demonstrate that the overall inter-core cache interference of a task can be obtained via dynamic programming. Experiments show that compared to existing methods, the proposed analysis reduces inter-core cache interference and WCET estimations by 52.31% and 8.94% on average, without significantly increasing computation overhead.

**Link**: [arxiv](http://arxiv.org/abs/2508.13863v1),  [pdf](http://arxiv.org/pdf/2508.13863v1)

**Tags**: cs.SE 



### Zobrist Hash-based Duplicate Detection in Symbolic Regression
**Authors**: Bogdan Burlacu

**Updated**: 2025-08-19T14:18:16Z

**Summary**: Symbolic regression encompasses a family of search algorithms that aim to discover the best fitting function for a set of data without requiring an a priori specification of the model structure. The most successful and commonly used technique for symbolic regression is Genetic Programming (GP), an evolutionary search method that evolves a population of mathematical expressions through the mechanism of natural selection. In this work we analyze the efficiency of the evolutionary search in GP and show that many points in the search space are re-visited and re-evaluated multiple times by the algorithm, leading to wasted computational effort. We address this issue by introducing a caching mechanism based on the Zobrist hash, a type of hashing frequently used in abstract board games for the efficient construction and subsequent update of transposition tables. We implement our caching approach using the open-source framework Operon and demonstrate its performance on a selection of real-world regression problems, where we observe up to 34\% speedups without any detrimental effects on search quality. The hashing approach represents a straightforward way to improve runtime performance while also offering some interesting possibilities for adjusting search strategy based on cached information.

**Link**: [arxiv](http://arxiv.org/abs/2508.13859v1),  [pdf](http://arxiv.org/pdf/2508.13859v1)

**Tags**: cs.NE 



### INDS: Incremental Named Data Streaming for Real-Time Point Cloud Video
**Authors**: Ruonan Chai, Yixiang Zhu, Xinjiao Li, Jiawei Li, Zili Meng, Dirk Kutscher

**Updated**: 2025-08-19T11:54:30Z

**Summary**: Real-time streaming of point cloud video, characterized by massive data volumes and high sensitivity to packet loss, remains a key challenge for immersive applications under dynamic network conditions. While connection-oriented protocols such as TCP and more modern alternatives like QUIC alleviate some transport-layer inefficiencies, including head-of-line blocking, they still retain a coarse-grained, segment-based delivery model and a centralized control loop that limit fine-grained adaptation and effective caching. We introduce INDS (Incremental Named Data Streaming), an adaptive streaming framework based on Information-Centric Networking (ICN) that rethinks delivery for hierarchical, layered media. INDS leverages the Octree structure of point cloud video and expressive content naming to support progressive, partial retrieval of enhancement layers based on consumer bandwidth and decoding capability. By combining time-windows with Group-of-Frames (GoF), INDS's naming scheme supports fine-grained in-network caching and facilitates efficient multi-user data reuse. INDS can be deployed as an overlay, remaining compatible with QUIC-based transport infrastructure as well as future Media-over-QUIC (MoQ) architectures, without requiring changes to underlying IP networks. Our prototype implementation shows up to 80% lower delay, 15-50% higher throughput, and 20-30% increased cache hit rates compared to state-of-the-art DASH-style systems. Together, these results establish INDS as a scalable, cache-friendly solution for real-time point cloud streaming under variable and lossy conditions, while its compatibility with MoQ overlays further positions it as a practical, forward-compatible architecture for emerging immersive media systems.

**Link**: [arxiv](http://arxiv.org/abs/2508.13756v1),  [pdf](http://arxiv.org/pdf/2508.13756v1)

**Tags**: cs.MM C.2.1; C.2.4; H.5.1 



### CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint   Caching and Resource-Aware Graph Partitioning
**Authors**: Xianfeng Song, Yi Zou, Zheng Shi

**Updated**: 2025-08-19T10:21:33Z

**Summary**: Graph Neural Networks (GNNs) have shown remarkable capabilities in processing graph-structured data prevalent in various real-world applications. However, the scalability of full-batch GNN training becomes severely limited by high communication overhead and load imbalance in distributed environments. In this paper, we present CaPGNN, a novel framework for efficient parallel full-batch GNN training on single-server with multi-GPU, designed specifically to reduce redundant inter-GPU communication and balance computational workloads. We propose a joint adaptive caching algorithm that leverages both CPU and GPU memory to significantly reduce the repetitive transmission of vertex features across partitions. Additionally, we introduce a resource-aware graph partitioning algorithm that adjusts subgraph sizes dynamically according to the heterogeneous computational and communication capacities of GPUs. Extensive experiments on large-scale benchmark datasets demonstrate that CaPGNN effectively reduces communication costs by up to 96% and accelerates GNN training by up to 12.7 times compared to state-of-the-art approaches. Our results highlight the potential of adaptive caching and resource-aware partitioning to facilitate scalable, efficient, and practical deployment of full-batch GNN training in distributed computing environments.

**Link**: [arxiv](http://arxiv.org/abs/2508.13716v1),  [pdf](http://arxiv.org/pdf/2508.13716v1)

**Tags**: cs.DC 



### SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units   with Precision Recovery
**Authors**: Weicheng Xue, Baisong Xu, Kai Yang, Yongxiang Liu, Dengdeng Fan, Pengxiang Xu, Yonghong Tian

**Updated**: 2025-08-19T09:13:13Z

**Summary**: Low-precision matrix engines, such as FP16 cube, offer high throughput but lack support for full-precision computation. In this work, we propose SGEMM-cube, a high-performance algorithm for emulating FP32 general matrix-matrix multiplication (GEMM) using only FP16 computation units on a representative AI accelerator. The method decomposes each FP32 operand into two FP16 values and compensates for numerical errors through a tunable scaling strategy. A detailed analysis of numerical errors, including underflow conditions and precision loss, guides the selection of scaling parameters to preserve up to 22 bits of mantissa accuracy. We further investigate the effect of computation order on accuracy and demonstrate that a term-wise accumulation scheme improves numerical stability over conventional FP32 GEMM in low-exponent regimes. Finally, a cache-aware blocking strategy and double-buffered pipeline are introduced to overlap memory transfers with computation, enabling SGEMM-cube to achieve up to 77\% of the theoretical FP32-equivalent peak performance on Ascend 910A NPU lacking native FP32 support. Extensive numerical experiments confirm that our method not only recovers the accuracy of native FP32 GEMM but also exhibits superior numerical stability under certain conditions, due to its structured and error-aware computation order.

**Link**: [arxiv](http://arxiv.org/abs/2507.23387v3),  [pdf](http://arxiv.org/pdf/2507.23387v3)

**Tags**: cs.DC 



### LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale   Architectures
**Authors**: Anders Johansson, Evan Weinberg, Christian R. Trott, Megan J. McCarthy, Stan G. Moore

**Updated**: 2025-08-19T05:27:53Z

**Summary**: Since its inception in 1995, LAMMPS has grown to be a world-class molecular dynamics code, with thousands of users, over one million lines of code, and multi-scale simulation capabilities. We discuss how LAMMPS has adapted to the modern heterogeneous computing landscape by integrating the Kokkos performance portability library into the existing C++ code. We investigate performance portability of simple pairwise, many-body reactive, and machine-learned force-field interatomic potentials. We present results on GPUs across different vendors and generations, and analyze performance trends, probing FLOPS throughput, memory bandwidths, cache capabilities, and thread-atomic operation performance. Finally, we demonstrate strong scaling on all current US exascale machines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the three potentials.

**Link**: [arxiv](http://arxiv.org/abs/2508.13523v1),  [pdf](http://arxiv.org/pdf/2508.13523v1)

**Tags**: cs.DC cs.PF physics.comp-ph C.1.4; C.2.4; C.4; D.1.3; D.3.4; E.1; I.6; I.6.8; J.2 



### Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated   Diffusion Transformers
**Authors**: Wongi Jeong, Kyungryeol Lee, Hoigi Seo, Se Young Chun

**Updated**: 2025-08-19T03:13:39Z

**Summary**: Diffusion transformers have emerged as an alternative to U-net-based diffusion models for high-fidelity image and video generation, offering superior scalability. However, their heavy computation remains a major obstacle to real-world deployment. Existing acceleration methods primarily exploit the temporal dimension such as reusing cached features across diffusion timesteps. Here, we propose Region-Adaptive Latent Upsampling (RALU), a training-free framework that accelerates inference along spatial dimension. RALU performs mixed-resolution sampling across three stages: 1) low-resolution denoising latent diffusion to efficiently capture global semantic structure, 2) region-adaptive upsampling on specific regions prone to artifacts at full-resolution, and 3) all latent upsampling at full-resolution for detail refinement. To stabilize generations across resolution transitions, we leverage noise-timestep rescheduling to adapt the noise level across varying resolutions. Our method significantly reduces computation while preserving image quality by achieving up to 7.0$\times$ speed-up on FLUX and 3.0$\times$ on Stable Diffusion 3 with minimal degradation. Furthermore, RALU is complementary to existing temporal accelerations such as caching methods, thus can be seamlessly integrated to further reduce inference latency without compromising generation quality.

**Link**: [arxiv](http://arxiv.org/abs/2507.08422v2),  [pdf](http://arxiv.org/pdf/2507.08422v2)

**Tags**: cs.CV eess.IV 



### Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A   first-principles DFT+$U$+$V$ study
**Authors**: Indukuru Ramesh Reddy, Sayandeep Ghosh, Bongjae Kim, Chang-Jong Kang

**Updated**: 2025-08-19T01:38:23Z

**Summary**: Nonlocal Coulomb interactions play a crucial role in stabilizing distinct electronic phases in kagome materials. In this work, we systematically investigate the effects of on-site ($U$) and inter-site ($V$) Coulomb interactions on the electronic structure and stability of charge-density-wave (CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory (DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and stability of CDW phases, whereas $U$ suppresses these phases, highlighting a fundamental competition between local and nonlocal Coulomb interactions. By directly comparing our theoretical results with angle-resolved photoemission spectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that accurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings establish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable insights into the correlated electronic states in kagome metals and serving as a foundation for future explorations of correlation-driven phenomena in related materials.

**Link**: [arxiv](http://arxiv.org/abs/2504.17995v2),  [pdf](http://arxiv.org/pdf/2504.17995v2)

**Tags**: cond-mat.str-el 



### Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data   Analysis
**Authors**: Ayoub Ben Chaliah, Hela Dellagi

**Updated**: 2025-08-18T21:58:18Z

**Summary**: We present Datarus-R1-14B, a 14 B-parameter open-weights language model fine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and graduate-level problem solver. Datarus is trained not on isolated question-answer pairs but on full analytical trajectories including reasoning steps, code execution, error traces, self-corrections, and final conclusions, all captured in a ReAct-style notebook format spanning finance, medicine, numerical analysis, and other quantitative domains. Our training pipeline combines (i) a trajectory-centric synthetic data generator that yielded 144 000 tagged notebook episodes, (ii) a dual-reward framework blending a lightweight tag-based structural signal with a Hierarchical Reward Model (HRM) that scores both single-step soundness and end-to-end coherence, and (iii) a memory-optimized implementation of Group Relative Policy Optimization (GRPO) featuring KV-cache reuse, sequential generation, and reference-model sharding. A cosine curriculum smoothly shifts emphasis from structural fidelity to semantic depth, reducing the format collapse and verbosity that often plague RL-aligned LLMs. A central design choice in Datarus is it dual reasoning interface. In agentic mode the model produces ReAct-tagged steps that invoke Python tools to execute real code; in reflection mode it outputs compact Chain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On demanding postgraduate-level problems, Datarus exhibits an "AHA-moment" pattern: it sketches hypotheses, revises them once or twice, and converges avoiding the circular, token-inflating loops common to contemporary systems. Across standard public benchmarks Datarus surpasses similar size models and even reaches the level of larger reasoning models such as QwQ-32B achieving up to 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting 18-49% fewer tokens per solution.

**Link**: [arxiv](http://arxiv.org/abs/2508.13382v1),  [pdf](http://arxiv.org/pdf/2508.13382v1)

**Tags**: cs.CL cs.AI 



### AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical   Manufacturing Scale-Up
**Authors**: Sakhinana Sagar Srinivas, Shivam Gupta, Venkataramana Runkana

**Updated**: 2025-08-18T16:52:22Z

**Summary**: Recent advances in generative AI have accelerated the discovery of novel chemicals and materials. However, scaling these discoveries to industrial production remains a major bottleneck due to the synthesis gap -- the need to develop entirely new manufacturing processes. This challenge requires detailed engineering blueprints: PFDs for equipment layouts and material/energy flows, and PIDs for process plant operations. Current AI systems cannot yet reliably generate these critical engineering schematics, creating a fundamental obstacle to manufacturing scale-up of novel discoveries. We present a closed-loop, physics-aware framework for automated generation of industrially viable PFDs and PIDs. The framework integrates three key components: (1) domain-specialized small language models (SLMs) trained for auto-generation of PFDs and PIDs, (2) a hierarchical knowledge graph containing process flow and instrumentation descriptions for 1,020+ chemicals for Graph Retrieval-Augmented Generation (GRAG), and (3) an open-source chemical process simulator for modeling, simulation, optimization, and analysis of novel chemical processes. The SLMs are trained through a multi-stage pipeline on synthetic datasets, with process simulator-in-the-loop validation ensuring feasibility. To enhance computational efficiency, the framework implements structural pruning (width and depth) guided by importance heuristics to reduce language model size while preserving accuracy, followed by advanced inference optimizations including FlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization, and Test-Time Inference Scaling. Experimental results demonstrate that our framework generates simulator-validated process descriptions with high fidelity.

**Link**: [arxiv](http://arxiv.org/abs/2505.24584v3),  [pdf](http://arxiv.org/pdf/2505.24584v3)

**Tags**: cs.LG cs.AI cs.IR 



### Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning   Models
**Authors**: Ruikang Liu, Yuxuan Sun, Manyi Zhang, Haoli Bai, Xianzhi Yu, Tiezheng Yu, Chun Yuan, Lu Hou

**Updated**: 2025-08-18T16:06:09Z

**Summary**: Recent advancements in reasoning language models have demonstrated remarkable performance in complex tasks, but their extended chain-of-thought reasoning process increases inference overhead. While quantization has been widely adopted to reduce the inference cost of large language models, its impact on reasoning models remains understudied. In this paper, we conduct the first systematic study on quantized reasoning models, evaluating the open-sourced DeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B parameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache, and activation quantization using state-of-the-art algorithms at varying bit-widths, with extensive evaluation across mathematical (AIME, MATH-500), scientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our findings reveal that while lossless quantization can be achieved with W8A8 or W4A16 quantization, lower bit-widths introduce significant accuracy risks. We further identify model size, model origin, and task difficulty as critical determinants of performance. Contrary to expectations, quantized models do not exhibit increased output lengths. In addition, strategically scaling the model sizes or reasoning steps can effectively enhance the performance. All quantized models and codes are open-sourced in https://github.com/ruikangliu/Quantized-Reasoning-Models.

**Link**: [arxiv](http://arxiv.org/abs/2504.04823v2),  [pdf](http://arxiv.org/pdf/2504.04823v2)

**Tags**: cs.CL cs.AI 



### Some optimization possibilities in data plane programming
**Authors**: Altangerel Gereltsetseg, Tejfel Máté

**Updated**: 2025-08-18T09:41:28Z

**Summary**: Software-defined networking (SDN) technology aims to create a highly flexible network by decoupling control plane and the data plane and programming them independently. There has been a lot of research on improving and optimizing the control plane, and data plane programming is a relatively new concept, so study on it is one of the hot topics for researchers. At the 2019 Dagstuhl Seminar, well-known scientists on computer networking discussed challenges and problems in the field of data plane programming that need to be addressed over the next 10 years. Based on this seminar issues and papers review, we suggested some possible solutions which are for optimizing data plane to improve packet processing performance and link utilization. The suggestions include (i) enriching data plane language with asynchronous external function, (ii) compression based on payload size, (iii) in-network caching for fast packet processing, and (iv) offloading external functions to an additional thread, virtual machine (VM) or server, etc. In addition, we implemented some of these in the P4 data plane language to illustrate the practicality.

**Link**: [arxiv](http://arxiv.org/abs/2508.12767v1),  [pdf](http://arxiv.org/pdf/2508.12767v1)

**Tags**: cs.NI 



### Dissecting CPU-GPU Unified Physical Memory on AMD MI300A APUs
**Authors**: Jacob Wahlgren, Gabin Schieffer, Ruimin Shi, Edgar A. León, Roger Pearce, Maya Gokhale, Ivy Peng

**Updated**: 2025-08-18T09:06:49Z

**Summary**: Discrete GPUs are a cornerstone of HPC and data center systems, requiring management of separate CPU and GPU memory spaces. Unified Virtual Memory (UVM) has been proposed to ease the burden of memory management; however, at a high cost in performance. The recent introduction of AMD's MI300A Accelerated Processing Units (APUs)--as deployed in the El Capitan supercomputer--enables HPC systems featuring integrated CPU and GPU with Unified Physical Memory (UPM) for the first time. This work presents the first comprehensive characterization of the UPM architecture on MI300A. We first analyze the UPM system properties, including memory latency, bandwidth, and coherence overhead. We then assess the efficiency of the system software in memory allocation, page fault handling, TLB management, and Infinity Cache utilization. We propose a set of porting strategies for transforming applications for the UPM architecture and evaluate six applications on the MI300A APU. Our results show that applications on UPM using the unified memory model can match or outperform those in the explicitly managed model--while reducing memory costs by up to 44%.

**Link**: [arxiv](http://arxiv.org/abs/2508.12743v1),  [pdf](http://arxiv.org/pdf/2508.12743v1)

**Tags**: cs.DC cs.PF 



### MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration
**Authors**: Yuanxin Wei, Lansong Diao, Bujiao Chen, Shenggan Cheng, Zhengping Qian, Wenyuan Yu, Nong Xiao, Wei Lin, Jiangsu Du

**Updated**: 2025-08-18T07:49:33Z

**Summary**: Leveraging the Transformer architecture and the diffusion process, video DiT models have emerged as a dominant approach for high-quality video generation. However, their multi-step iterative denoising process incurs high computational cost and inference latency. Caching, a widely adopted optimization method in DiT models, leverages the redundancy in the diffusion process to skip computations in different granularities (e.g., step, cfg, block). Nevertheless, existing caching methods are limited to single-granularity strategies, struggling to balance generation quality and inference speed in a flexible manner. In this work, we propose MixCache, a training-free caching-based framework for efficient video DiT inference. It first distinguishes the interference and boundary between different caching strategies, and then introduces a context-aware cache triggering strategy to determine when caching should be enabled, along with an adaptive hybrid cache decision strategy for dynamically selecting the optimal caching granularity. Extensive experiments on diverse models demonstrate that, MixCache can significantly accelerate video generation (e.g., 1.94$\times$ speedup on Wan 14B, 1.97$\times$ speedup on HunyuanVideo) while delivering both superior generation quality and inference efficiency compared to baseline methods.

**Link**: [arxiv](http://arxiv.org/abs/2508.12691v1),  [pdf](http://arxiv.org/pdf/2508.12691v1)

**Tags**: cs.GR cs.CV cs.LG 



### Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for   NGINX
**Authors**: Aayush Gupta, Arpit Bhayani

**Updated**: 2025-08-17T20:01:12Z

**Summary**: Web proxies such as NGINX commonly rely on least-recently-used (LRU) eviction, which is size agnostic and can thrash under periodic bursts and mixed object sizes. We introduce Cold-RL, a learned eviction policy for NGINX that replaces LRU's forced-expire path with a dueling Deep Q-Network served by an ONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL samples the K least-recently-used objects, extracts six lightweight features (age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT), and requests a bitmask of victims; a hard timeout of 500 microseconds triggers immediate fallback to native LRU. Policies are trained offline by replaying NGINX access logs through a cache simulator with a simple reward: a retained object earns one point if it is hit again before TTL expiry. We compare against LRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial workloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538, a 146 percent improvement over the best classical baseline; at 100 MB, from 0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods (about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th percentile eviction latency within budget. To our knowledge, this is the first reinforcement learning eviction policy integrated into NGINX with strict SLOs.

**Link**: [arxiv](http://arxiv.org/abs/2508.12485v1),  [pdf](http://arxiv.org/pdf/2508.12485v1)

**Tags**: cs.LG cs.AI cs.DB cs.NI C.2.4; C.4; D.4.2; I.2.6 



### Accelerating LLM Inference via Dynamic KV Cache Placement in   Heterogeneous Memory System
**Authors**: Yunhua Fang, Rui Xie, Asad Ul Haq, Linsen Ma, Kaoutar El Maghraoui, Naigang Wang, Meng Wang, Liu Liu, Tong Zhang

**Updated**: 2025-08-17T19:07:08Z

**Summary**: Large Language Model (LLM) inference is increasingly constrained by memory bandwidth, with frequent access to the key-value (KV) cache dominating data movement. While attention sparsity reduces some memory traffic, the relevance of past tokens varies over time, requiring the full KV cache to remain accessible and sustaining pressure on both bandwidth and capacity. With advances in interconnects such as NVLink and LPDDR5X, modern AI hardware now integrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making heterogeneous memory systems a practical solution. This work investigates dynamic KV cache placement across such systems to maximize aggregated bandwidth utilization under capacity constraints. Rather than proposing a specific scheduling policy, we formulate the placement problem mathematically and derive a theoretical upper bound, revealing substantial headroom for runtime optimization. To our knowledge, this is the first formal treatment of dynamic KV cache scheduling in heterogeneous memory systems for LLM inference.

**Link**: [arxiv](http://arxiv.org/abs/2508.13231v1),  [pdf](http://arxiv.org/pdf/2508.13231v1)

**Tags**: cs.AR cs.AI cs.PF 



### ZigzagAttention: Efficient Long-Context Inference with Exclusive   Retrieval and Streaming Heads
**Authors**: Zhuorui Liu, Chen Zhang, Dawei Song

**Updated**: 2025-08-17T15:48:50Z

**Summary**: With the rapid development of large language models (LLMs), handling long context has become one of the vital abilities in LLMs. Such long-context ability is accompanied by difficulties in deployment, especially due to the increased consumption of KV cache. There is certain work aiming to optimize the memory footprint of KV cache, inspired by the observation that attention heads can be categorized into retrieval heads that are of great significance and streaming heads that are of less significance. Typically, identifying the streaming heads and and waiving the KV cache in the streaming heads would largely reduce the overhead without hurting the performance that much. However, since employing both retrieval and streaming heads in one layer decomposes one large round of attention computation into two small ones, it may unexpectedly bring extra latency on accessing and indexing tensors. Based on this intuition, we impose an important improvement to the identification process of retrieval and streaming heads, in which we design a criterion that enforces exclusively retrieval or streaming heads gathered in one unique layer. In this way, we further eliminate the extra latency and only incur negligible performance degradation. Our method named \textsc{ZigzagAttention} is competitive among considered baselines owing to reduced latency and comparable performance.

**Link**: [arxiv](http://arxiv.org/abs/2508.12407v1),  [pdf](http://arxiv.org/pdf/2508.12407v1)

**Tags**: cs.CL 



### Enhancement of the energy storage and electrocaloric effect performances   in 0.4 BCZT 0.6 BSTSn medium entropy ceramic prepared by sol gel method
**Authors**: S. Khardazi, Z. Gargar, A. Lyubchyk, O. Zakir, D. Mezzane, M. Amjoud, A. Alimoussa, Z. Kutnjak

**Updated**: 2025-08-17T13:05:52Z

**Summary**: Based on the traditional polycrystalline ferroelectric Ba0.85Ca0.15Zr0.10Ti0.90O3, the 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6 Ba0.9Sr0.1Ti0.9Sn0.1O3 medium entropy material with good energy storage and electrocaloric effect performances is designed and synthesized by the solgel method. The structural, dielectric, energy storage and electrocaloric effect properties of the prepared sample were studied. The findings demonstrate that the 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6 Ba0.9Sr0.1Ti0.9Sn0.1O3 ceramic simultaneously has a significant recoverable energy storage density of 255.4 mJ/cm3, an efficiency of 67.9%, a large ECE temperature change of 1.36 K, and a high ECE responsivity of 0.453 K.mm/kV under a low electric field of 30 kV/cm. Moreover, excellent temperature stability of Wrec (less than 10%) was achieved in the investigated sample 0.4BCZT 0.6BSTSn.

**Link**: [arxiv](http://arxiv.org/abs/2508.12357v1),  [pdf](http://arxiv.org/pdf/2508.12357v1)

**Tags**: cond-mat.mtrl-sci 



### CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based   Token Eviction
**Authors**: Raghavv Goel, Junyoung Park, Mukul Gagrani, Dalton Jones, Matthew Morse, Harper Langston, Mingu Lee, Chris Lott

**Updated**: 2025-08-16T23:41:48Z

**Summary**: While long context support of large language models has extended their abilities, it also incurs challenges in memory and compute which becomes crucial bottlenecks in resource-restricted devices. Token eviction, a widely adopted post-training methodology designed to alleviate the bottlenecks by evicting less important tokens from the cache, typically uses attention scores as proxy metrics for token importance. However, one major limitation of attention score as a token-wise importance metrics is that it lacks the information about contribution of tokens to the attention output. In this paper, we propose a simple eviction criterion based on the contribution of cached tokens to attention outputs. Our method, CAOTE, optimizes for eviction error due to token eviction, by seamlessly integrating attention scores and value vectors. This is the first method which uses value tokens on top of attention-based eviction scores in closed-form. Additionally, CAOTE can act as a meta-heuristic method with flexible usage with any token eviction method. We show that CAOTE, when combined with the state-of-the-art attention score-based methods, always improves accuracies on the downstream task, indicating the importance of leveraging information from values during token eviction process.

**Link**: [arxiv](http://arxiv.org/abs/2504.14051v5),  [pdf](http://arxiv.org/pdf/2504.14051v5)

**Tags**: cs.LG 



### RT-Cache: Training-Free Retrieval for Real-Time Manipulation
**Authors**: Owen Kwon, Abraham George, Alison Bartsch, Amir Barati Farimani

**Updated**: 2025-08-16T18:49:41Z

**Summary**: Real robots are expected to repeat the same behavior in new environments with very little new data, yet modern controllers either incur heavy per-step inference or require deployment-time fine-tuning. We propose RT-Cache, a training-free retrieval-as-control pipeline that caches diverse image action trajectories in a unified vector memory and, at test time, embeds the current frame to retrieve and replay multi-step snippets, replacing per-step model calls. A hierarchical search keeps lookups sub-second at million scale, shifting cost from compute to storage and enabling real-time control on modest GPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher success and lower completion time than strong retrieval baselines (approximately x2 higher success and ~30% faster in our settings), and a single-episode anchoring study shows immediate adaptation to a more complex, contact-rich task without fine-tuning. RT-Cache turns experience into an append-only memory, offering a simple, scalable path to few-shot deployment today and a foundation for multimodal keys and optional integration with high-level policies. Project page: https://rt-cache.github.io/.

**Link**: [arxiv](http://arxiv.org/abs/2505.09040v2),  [pdf](http://arxiv.org/pdf/2505.09040v2)

**Tags**: cs.RO cs.AI cs.CV cs.LG 



### Memory-Augmented Transformers: A Systematic Review from Neuroscience   Principles to Enhanced Model Architectures
**Authors**: Parsa Omidi, Xingshuai Huang, Axel Laborieux, Bahareh Nikpour, Tianyu Shi, Armaghan Eshaghi

**Updated**: 2025-08-16T03:17:35Z

**Summary**: Memory is fundamental to intelligence, enabling learning, reasoning, and adaptability across biological and artificial systems. While Transformer architectures excel at sequence modeling, they face critical limitations in long-range context retention, continual learning, and knowledge integration. This review presents a unified framework bridging neuroscience principles, including dynamic multi-timescale memory, selective attention, and consolidation, with engineering advances in Memory-Augmented Transformers. We organize recent progress through three taxonomic dimensions: functional objectives (context extension, reasoning, knowledge integration, adaptation), memory representations (parameter-encoded, state-based, explicit, hybrid), and integration mechanisms (attention fusion, gated control, associative retrieval). Our analysis of core memory operations (reading, writing, forgetting, and capacity management) reveals a shift from static caches toward adaptive, test-time learning systems. We identify persistent challenges in scalability and interference, alongside emerging solutions including hierarchical buffering and surprise-gated updates. This synthesis provides a roadmap toward cognitively-inspired, lifelong-learning Transformer architectures.

**Link**: [arxiv](http://arxiv.org/abs/2508.10824v2),  [pdf](http://arxiv.org/pdf/2508.10824v2)

**Tags**: cs.LG cs.CL 



### KV-Auditor: Auditing Local Differential Privacy for Correlated Key-Value   Estimation
**Authors**: Jingnan Xu, Leixia Wang, Xiaofeng Meng

**Updated**: 2025-08-15T14:17:24Z

**Summary**: To protect privacy for data-collection-based services, local differential privacy (LDP) is widely adopted due to its rigorous theoretical bound on privacy loss. However, mistakes in complex theoretical analysis or subtle implementation errors may undermine its practical guarantee. To address this, auditing is crucial to confirm that LDP protocols truly protect user data. However, existing auditing methods, though, mainly target machine learning and federated learning tasks based on centralized differentially privacy (DP), with limited attention to LDP. Moreover, the few studies on LDP auditing focus solely on simple frequency estimation task for discrete data, leaving correlated key-value data - which requires both discrete frequency estimation for keys and continuous mean estimation for values - unexplored.   To bridge this gap, we propose KV-Auditor, a framework for auditing LDP-based key-value estimation mechanisms by estimating their empirical privacy lower bounds. Rather than traditional LDP auditing methods that relies on binary output predictions, KV-Auditor estimates this lower bound by analyzing unbounded output distributions, supporting continuous data. Specifically, we classify state-of-the-art LDP key-value mechanisms into interactive and non-interactive types. For non-interactive mechanisms, we propose horizontal KV-Auditor for small domains with sufficient samples and vertical KV-Auditor for large domains with limited samples. For interactive mechanisms, we design a segmentation strategy to capture incremental privacy leakage across iterations. Finally, we perform extensive experiments to validate the effectiveness of our approach, offering insights for optimizing LDP-based key-value estimators.

**Link**: [arxiv](http://arxiv.org/abs/2508.11495v1),  [pdf](http://arxiv.org/pdf/2508.11495v1)

**Tags**: cs.CR 



### Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless   Edge-Device Networks
**Authors**: Rui Bao, Nan Xue, Yaping Sun, Zhiyong Chen

**Updated**: 2025-08-15T07:55:05Z

**Summary**: The integration of wireless communications and Large Language Models (LLMs) is poised to unlock ubiquitous intelligent services, yet deploying them in wireless edge-device collaborative environments presents a critical trade-off between inference quality and end-to-end latency. A fundamental mismatch exists between task complexity and resource allocation: offloading simple queries invites prohibitive latency, while on-device models lack the capacity for demanding computations. To address this challenge, we propose a dynamic, quality-latency aware routing framework that orchestrates inference between a lightweight model on the mobile device and a powerful model on the edge server. Our framework employs two distinct cost models: for single-turn queries, it fuses a BERT-predicted semantic score with communication and computation overheads; for multi-turn dialogues, it further quantifies context-aware costs arising from model switching and KV-cache management. While maintaining full inference quality, extensive experiments demonstrate that our framework cuts average response latency by 5-15% and reduces large model invocations by 10-20% against competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2508.11291v1),  [pdf](http://arxiv.org/pdf/2508.11291v1)

**Tags**: cs.IT cs.AI cs.LG math.IT 



### UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?
**Authors**: Mukund Choudhary, KV Aditya Srivatsa, Gaurja Aeron, Antara Raaghavi Bhattacharya, Dang Khoa Dang Dinh, Ikhlasul Akmal Hanif, Daria Kotova, Ekaterina Kochmar, Monojit Choudhury

**Updated**: 2025-08-15T06:53:28Z

**Summary**: Large language models (LLMs) have demonstrated potential in reasoning tasks, but their performance on linguistics puzzles remains consistently poor. These puzzles, often derived from Linguistics Olympiad (LO) contests, provide a minimal contamination environment to assess LLMs' linguistic reasoning abilities across low-resource languages. This work analyses LLMs' performance on 629 problems across 41 low-resource languages by labelling each with linguistically informed features to unveil weaknesses. Our analyses show that LLMs struggle with puzzles involving higher morphological complexity and perform better on puzzles involving linguistic features that are also found in English. We also show that splitting words into morphemes as a pre-processing step improves solvability, indicating a need for more informed and language-specific tokenisers. These findings thus offer insights into some challenges in linguistic reasoning and modelling of low-resource languages.

**Link**: [arxiv](http://arxiv.org/abs/2508.11260v1),  [pdf](http://arxiv.org/pdf/2508.11260v1)

**Tags**: cs.CL 



### ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal   Parallelism
**Authors**: Zedong Liu, Shenggan Cheng, Guangming Tan, Yang You, Dingwen Tao

**Updated**: 2025-08-15T04:27:30Z

**Summary**: Multimodal large language models (MLLMs) extend LLMs to handle images, videos, and audio by incorporating feature extractors and projection modules. However, these additional components -- combined with complex inference pipelines and heterogeneous workloads -- introduce significant inference overhead. Therefore, efficiently serving MLLMs remains a major challenge. Current tightly coupled serving architectures struggle to distinguish between mixed request types or adapt parallelism strategies to different inference stages, leading to increased time-to-first-token (TTFT) latency and poor resource utilization. To address this, we introduce Elastic Multimodal Parallelism (EMP), a new serving paradigm that elastically adapts to resource heterogeneity across request types and inference stages. Building upon EMP, we develop ElasticMM, an MLLM serving system that (1) separates requests into independent modality groups with dynamic resource allocation via a modality-aware load balancer; (2) decouples inference stages and enables parallelism adjustment and adaptive scaling via elastic partition scheduling; and (3) improves inference efficiency through unified multimodal prefix caching and non-blocking encoding. Experiments on diverse real-world datasets show that ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by up to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level objectives (SLOs).

**Link**: [arxiv](http://arxiv.org/abs/2507.10069v2),  [pdf](http://arxiv.org/pdf/2507.10069v2)

**Tags**: cs.DC cs.LG 



### A Survey on Diffusion Language Models
**Authors**: Tianyi Li, Mingda Chen, Bowei Guo, Zhiqiang Shen

**Updated**: 2025-08-14T17:47:22Z

**Summary**: Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs.

**Link**: [arxiv](http://arxiv.org/abs/2508.10875v1),  [pdf](http://arxiv.org/pdf/2508.10875v1)

**Tags**: cs.CL cs.AI cs.LG 



### FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference
**Authors**: Guangda Liu, Chengwei Li, Zhenyu Ning, Minyi Guo, Jieru Zhao

**Updated**: 2025-08-14T16:12:44Z

**Summary**: Large language models (LLMs) have been widely deployed with rapidly expanding context windows to support increasingly demanding applications. However, long contexts pose significant deployment challenges, primarily due to the KV cache whose size grows proportionally with context length. While KV cache compression methods are proposed to address this issue, KV dropping methods incur considerable accuracy loss, and KV retrieval methods suffer from significant efficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization framework to enhance KV retrieval efficiency while preserving accuracy. On the algorithm side, FreeKV introduces speculative retrieval to shift the KV selection and recall processes out of the critical path, combined with fine-grained correction to ensure accuracy. On the system side, FreeKV employs hybrid KV layouts across CPU and GPU memory to eliminate fragmented data transfers, and leverages double-buffered streamed recall to further improve efficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy across various scenarios and models, delivering up to 13$\times$ speedup compared to SOTA KV retrieval methods.

**Link**: [arxiv](http://arxiv.org/abs/2505.13109v2),  [pdf](http://arxiv.org/pdf/2505.13109v2)

**Tags**: cs.LG cs.AI cs.CL 



### BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit   KV Cache
**Authors**: Dayou Du, Shijie Cao, Jianyi Cheng, Luo Mai, Ting Cao, Mao Yang

**Updated**: 2025-08-14T15:37:43Z

**Summary**: The rise of long-context Large Language Models (LLMs) amplifies memory and bandwidth demands during autoregressive decoding, as the Key-Value (KV) cache grows with each generated token. Low-bit KV-cache quantization (e.g., 4-bit or 2-bit) can reduce memory footprint while preserving accuracy, but existing systems suffer from slow decoding due to their exclusive reliance on CUDA cores, neglecting Tensor Cores (the primary source of compute on modern GPUs). We present BitDecoding, a new long-context LLM inference system with a low-bit KV cache. BitDecoding enables efficient low-bit KV-cache decoding by cooperatively leveraging CUDA cores and Tensor Cores. It introduces methods for automatically inducing optimized layouts to exploit Tensor Cores, along with warp-level parallelization strategies for dequantization. For unified system support, BitDecoding includes a query transformation module supporting diverse attention variants, a quantization kernel that supports both tensor-wise and channel-wise scaling used in various quantization algorithms with high performance, and a dequantization kernel with a software-defined pipeline to coordinate CUDA and Tensor Cores execution for mixed-precision operations. Evaluated on RTX 4090, A100, and H100, BitDecoding accelerates decoding by up to 7.5x, 4.8x, and 8.9x, respectively, over FP16 FlashDecoding-v2, and surpasses the state-of-the-art low-bit system QServe by up to 4.3x. On LLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding latency by 3x, showing substantial improvements for long-context generation. The code is available at https://github.com/DD-DuDa/BitDecoding.

**Link**: [arxiv](http://arxiv.org/abs/2503.18773v2),  [pdf](http://arxiv.org/pdf/2503.18773v2)

**Tags**: cs.AR cs.AI cs.CL cs.PF 



### EVCtrl: Efficient Control Adapter for Visual Generation
**Authors**: Zixiang Yang, Yue Ma, Yinhan Zhang, Shanhui Mo, Dongrui Liu, Linfeng Zhang

**Updated**: 2025-08-14T14:11:48Z

**Summary**: Visual generation includes both image and video generation, training probabilistic models to create coherent, diverse, and semantically faithful content from scratch. While early research focused on unconditional sampling, practitioners now demand controllable generation that allows precise specification of layout, pose, motion, or style. While ControlNet grants precise spatial-temporal control, its auxiliary branch markedly increases latency and introduces redundant computation in both uncontrolled regions and denoising steps, especially for video. To address this problem, we introduce EVCtrl, a lightweight, plug-and-play control adapter that slashes overhead without retraining the model. Specifically, we propose a spatio-temporal dual caching strategy for sparse control information. For spatial redundancy, we first profile how each layer of DiT-ControlNet responds to fine-grained control, then partition the network into global and local functional zones. A locality-aware cache focuses computation on the local zones that truly need the control signal, skipping the bulk of redundant computation in global regions. For temporal redundancy, we selectively omit unnecessary denoising steps to improve efficiency. Extensive experiments on CogVideo-Controlnet, Wan2.1-Controlnet, and Flux demonstrate that our method is effective in image and video control generation without the need for training. For example, it achieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and Wan2.1-Controlnet, respectively, with almost no degradation in generation quality.Codes are available in the supplementary materials.

**Link**: [arxiv](http://arxiv.org/abs/2508.10963v1),  [pdf](http://arxiv.org/pdf/2508.10963v1)

**Tags**: cs.CV 



### SurfaceLogicKV: Surface and Logic Attention Behaviors are All You Need   for Robust KV Cache Compression
**Authors**: Mengjie Li, William J. Song

**Updated**: 2025-08-14T14:08:58Z

**Summary**: The increasing input sequence length in Large Language Models (LLMs) puts significant pressure on key-value (KV) cache storage, making efficient inference challenging. Explicitly distinguishing attention behavior into our self-defined surface memorization and logic construction reveals essential roles in long-context reasoning. We observe that an individual attention head can display various behaviors, with nearly 98.5% effectively ignoring completely irrelevant information. The remaining 1.5% behaves as logic construction, and 0.5% behaves as surface memorization. Based on layer- and head-wise integration, we propose a novel two-stage SurfaceLogicKV method to utilize these attention behaviors for KV Cache compression. As a result, it achieves improved compressing robustness while maintaining competitive performance across various tasks and long sequences compared to baselines or even FullKV in some specific situations

**Link**: [arxiv](http://arxiv.org/abs/2508.15806v1),  [pdf](http://arxiv.org/pdf/2508.15806v1)

**Tags**: cs.CL cs.AI 



### Routing and Wavelength Assignment with Minimal Attack Radius for QKD   Networks
**Authors**: Mengyao Li, Qiaolun Zhang, Zongshuai Yang, Stefano Bregni, Alberto Gatto, Raouf Boutaba, Massimo Tornatore

**Updated**: 2025-08-14T13:10:43Z

**Summary**: Quantum Key Distribution (QKD) can distribute keys with guaranteed security but remains susceptible to key exchange interruption due to physical-layer threats, such as high-power jamming attacks. To address this challenge, we first introduce a novel metric, namely Maximum Number of Affected Requests (maxNAR), to quantify the worst-case impact of a single physical-layer attack, and then we investigate a new problem of Routing and Wavelength Assignment with Minimal Attack Radius (RWA-MAR). We formulate the problem using an Integer Linear Programming (ILP) model and propose a scalable heuristic to efficiently minimize maxNAR. Our approach incorporates key caching through Quantum Key Pools (QKPs) to enhance resilience and optimize resource utilization. Moreover, we model the impact of different QKD network architectures, employing Optical Bypass (OB) for optical switching of quantum channels and Trusted Relay (TR) for secure key forwarding. Moreover, a tunable parameter is designed in the heuristic to guide the preference for OB or TR, offering enhanced adaptability and dynamic control in diverse network scenarios. Simulation results confirm that our method significantly outperforms the baseline in terms of security and scalability.

**Link**: [arxiv](http://arxiv.org/abs/2508.10613v1),  [pdf](http://arxiv.org/pdf/2508.10613v1)

**Tags**: quant-ph cs.NI 



### Yan: Foundational Interactive Video Generation
**Authors**: Deheng Ye, Fangyun Zhou, Jiacheng Lv, Jianqi Ma, Jun Zhang, Junyan Lv, Junyou Li, Minwen Deng, Mingyu Yang, Qiang Fu, Wei Yang, Wenkai Lv, Yangbin Yu, Yewen Wang, Yonghang Guan, Zhihao Hu, Zhongbin Fang, Zhongqian Sun

**Updated**: 2025-08-14T10:26:51Z

**Summary**: We present Yan, a foundational framework for interactive video generation, covering the entire pipeline from simulation and generation to editing. Specifically, Yan comprises three core modules. AAA-level Simulation: We design a highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based shift-window denoising inference process, achieving real-time 1080P/60FPS interactive simulation. Multi-Modal Generation: We introduce a hierarchical autoregressive caption method that injects game-specific knowledge into open-domain multi-modal video diffusion models (VDMs), then transforming the VDM into a frame-wise, action-controllable, real-time infinite interactive video generator. Notably, when the textual and visual prompts are sourced from different domains, the model demonstrates strong generalization, allowing it to blend and compose the style and mechanics across domains flexibly according to user prompts. Multi-Granularity Editing: We propose a hybrid model that explicitly disentangles interactive mechanics simulation from visual rendering, enabling multi-granularity video content editing during interaction through text. Collectively, Yan offers an integration of these modules, pushing interactive video generation beyond isolated capabilities toward a comprehensive AI-driven interactive creation paradigm, paving the way for the next generation of creative tools, media, and entertainment. The project page is: https://greatx3.github.io/Yan/.

**Link**: [arxiv](http://arxiv.org/abs/2508.08601v3),  [pdf](http://arxiv.org/pdf/2508.08601v3)

**Tags**: cs.CV cs.AI 



### ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic   Parallelism in LLMs
**Authors**: Keyu Chen, Zhifeng Shen, Daohai Yu, Haoqian Wu, Wei Wen, Jianfeng He, Ruizhi Qiao, Xing Sun

**Updated**: 2025-08-14T09:04:56Z

**Summary**: The increasing scale and complexity of large language models (LLMs) pose significant inference latency challenges, primarily due to their autoregressive decoding paradigm characterized by the sequential nature of next-token prediction. By re-examining the outputs of autoregressive models, we observed that some segments exhibit parallelizable structures, which we term intrinsic parallelism. Decoding each parallelizable branch simultaneously (i.e. parallel decoding) can significantly improve the overall inference speed of LLMs. In this paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which addresses two core challenges: automated construction of parallelizable data and efficient parallel decoding mechanism. More specifically, we introduce a non-invasive pipeline that automatically extracts and validates parallelizable structures from the responses of autoregressive models. To empower efficient adaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which enables seamless transitions between serial and parallel decoding modes while maintaining a reusable KV cache, maximizing computational efficiency. Extensive evaluations across General Tasks, Retrieval-Augmented Generation, Mathematical Reasoning, demonstrate that ASPD achieves unprecedented performance in both effectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up to 3.19x speedup (1.85x on average) while maintaining response quality within 1% difference compared to autoregressive models, realizing significant acceleration without compromising generation quality. Our framework sets a groundbreaking benchmark for efficient LLM parallel inference, paving the way for its deployment in latency-sensitive applications such as AI-powered customer service bots and answer retrieval engines.

**Link**: [arxiv](http://arxiv.org/abs/2508.08895v2),  [pdf](http://arxiv.org/pdf/2508.08895v2)

**Tags**: cs.CL cs.AI 



### NanoControl: A Lightweight Framework for Precise and Efficient Control   in Diffusion Transformer
**Authors**: Shanyuan Liu, Jian Zhu, Junda Lu, Yue Gong, Liuzhuozheng Li, Bo Cheng, Yuhang Ma, Liebucha Wu, Xiaoyu Wu, Dawei Leng, Yuhui Yin

**Updated**: 2025-08-14T07:54:44Z

**Summary**: Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in text-to-image synthesis. However, in the domain of controllable text-to-image generation using DiTs, most existing methods still rely on the ControlNet paradigm originally designed for UNet-based diffusion models. This paradigm introduces significant parameter overhead and increased computational costs. To address these challenges, we propose the Nano Control Diffusion Transformer (NanoControl), which employs Flux as the backbone network. Our model achieves state-of-the-art controllable text-to-image generation performance while incurring only a 0.024\% increase in parameter count and a 0.029\% increase in GFLOPs, thus enabling highly efficient controllable generation. Specifically, rather than duplicating the DiT backbone for control, we design a LoRA-style (low-rank adaptation) control module that directly learns control signals from raw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation mechanism that integrates condition-specific key-value information into the backbone in a simple yet highly effective manner, facilitating deep fusion of conditional features. Extensive benchmark experiments demonstrate that NanoControl significantly reduces computational overhead compared to conventional control approaches, while maintaining superior generation quality and achieving improved controllability.

**Link**: [arxiv](http://arxiv.org/abs/2508.10424v1),  [pdf](http://arxiv.org/pdf/2508.10424v1)

**Tags**: cs.CV 



### XQuant: Breaking the Memory Wall for LLM Inference with KV Cache   Rematerialization
**Authors**: Aditya Tomar, Coleman Hooper, Minjae Lee, Haocheng Xi, Rishabh Tiwari, Wonjun Kang, Luca Manolache, Michael W. Mahoney, Kurt Keutzer, Amir Gholami

**Updated**: 2025-08-14T06:52:38Z

**Summary**: Although LLM inference has emerged as a critical workload for many downstream applications, efficiently inferring LLMs is challenging due to the substantial memory footprint and bandwidth requirements. In parallel, compute capabilities have steadily outpaced both memory capacity and bandwidth over the last few decades, a trend that remains evident in modern GPU hardware and exacerbates the challenge of LLM inference. As such, new algorithms are emerging that trade increased computation for reduced memory operations. To that end, we present XQuant, which takes advantage of this trend, enabling an order-of-magnitude reduction in memory consumption through low-bit quantization with substantial accuracy benefits relative to state-of-the-art KV cache quantization methods. We accomplish this by quantizing and caching the layer input activations X, instead of using standard KV caching, and then rematerializing the Keys and Values on-the-fly during inference. This results in an immediate 2$\times$ memory savings compared to KV caching. By applying XQuant, we achieve up to $\sim 7.7\times$ memory savings with $<0.1$ perplexity degradation compared to the FP16 baseline. Furthermore, our approach leverages the fact that X values are similar across layers. Building on this observation, we introduce XQuant-CL, which exploits the cross-layer similarity in the X embeddings for extreme compression. Across different models, XQuant-CL attains up to 10$\times$ memory savings relative to the FP16 baseline with only 0.01 perplexity degradation, and 12.5$\times$ memory savings with only $0.1$ perplexity degradation. XQuant exploits the rapidly increasing compute capabilities of hardware platforms to eliminate the memory bottleneck, while surpassing state-of-the-art KV cache quantization methods and achieving near-FP16 accuracy across a wide range of models.

**Link**: [arxiv](http://arxiv.org/abs/2508.10395v1),  [pdf](http://arxiv.org/pdf/2508.10395v1)

**Tags**: cs.LG 



### RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache   Compression
**Authors**: Payman Behnam, Yaosheng Fu, Ritchie Zhao, Po-An Tsai, Zhiding Yu, Alexey Tumanov

**Updated**: 2025-08-13T17:55:58Z

**Summary**: Transformer-based Large Language Models rely critically on the KV cache to efficiently handle extended contexts during the decode phase. Yet, the size of the KV cache grows proportionally with the input length, burdening both memory bandwidth and capacity as decoding progresses. To address this challenge, we present RocketKV, a training-free KV cache compression strategy containing two consecutive stages. In the first stage, it performs coarse-grain permanent KV cache eviction on the input sequence tokens. In the second stage, it adopts a hybrid sparse attention method to conduct fine-grain top-k sparse attention, approximating the attention scores by leveraging both head and sequence dimensionality reductions. We show that RocketKV provides a compression ratio of up to 400$\times$, end-to-end speedup of up to 3.7$\times$ as well as peak memory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU compared to the full KV cache baseline, while achieving negligible accuracy loss on a variety of long-context tasks. We also propose a variant of RocketKV for multi-turn scenarios, which consistently outperforms other existing methods and achieves accuracy nearly on par with an oracle top-k attention scheme. The source code is available here: https://github.com/NVlabs/RocketKV.

**Link**: [arxiv](http://arxiv.org/abs/2502.14051v3),  [pdf](http://arxiv.org/pdf/2502.14051v3)

**Tags**: cs.CL cs.LG 



### Physical Autoregressive Model for Robotic Manipulation without Action   Pretraining
**Authors**: Zijian Song, Sihan Qin, Tianshui Chen, Liang Lin, Guangrun Wang

**Updated**: 2025-08-13T13:54:51Z

**Summary**: The scarcity of manipulation data has motivated the use of pretrained large models from other modalities in robotics. In this work, we build upon autoregressive video generation models to propose a Physical Autoregressive Model (PAR), where physical tokens combine frames and actions to represent the joint evolution of the robot and its environment. PAR leverages the world knowledge embedded in video pretraining to understand physical dynamics without requiring action pretraining, enabling accurate video prediction and consistent action trajectories. It also adopts a DiT-based de-tokenizer to model frames and actions as continuous tokens, mitigating quantization errors and facilitating mutual enhancement. Furthermore, we incorporate a causal mask with inverse kinematics, parallel training, and the KV-cache mechanism to further improve performance and efficiency. Experiments on the ManiSkill benchmark show that PAR achieves a 100\% success rate on the PushCube task, matches the performance of action-pretrained baselines on other tasks, and accurately predicts future videos with tightly aligned action trajectories. These findings underscore a promising direction for robotic manipulation by transferring world knowledge from autoregressive video pretraining.

**Link**: [arxiv](http://arxiv.org/abs/2508.09822v1),  [pdf](http://arxiv.org/pdf/2508.09822v1)

**Tags**: cs.CV 



### Re-thinking Memory-Bound Limitations in CGRAs
**Authors**: Xiangfeng Liu, Zhe Jiang, Anzhen Zhu, Xiaomeng Han, Mingsong Lyu, Qingxu Deng, Nan Guan

**Updated**: 2025-08-13T07:40:25Z

**Summary**: Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators commonly employed to boost performance in workloads with iterative structures. Existing research typically focuses on compiler or architecture optimizations aimed at improving CGRA performance, energy efficiency, flexibility, and area utilization, under the idealistic assumption that kernels can access all data from Scratchpad Memory (SPM). However, certain complex workloads-particularly in fields like graph analytics, irregular database operations, and specialized forms of high-performance computing (e.g., unstructured mesh simulations)-exhibit irregular memory access patterns that hinder CGRA utilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To address this challenge, we conduct a thorough analysis of the underlying causes of performance degradation, then propose a redesigned memory subsystem and refine the memory model. With both microarchitectural and theoretical optimization, our solution can effectively manage irregular memory accesses through CGRA-specific runahead execution mechanism and cache reconfiguration techniques. Our results demonstrate that we can achieve performance comparable to the original SPM-only system while requiring only 1.27% of the storage size. The runahead execution mechanism achieves an average 3.04x speedup (up to 6.91x), with cache reconfiguration technique providing an additional 6.02% improvement, significantly enhancing CGRA performance for irregular memory access patterns.

**Link**: [arxiv](http://arxiv.org/abs/2508.09570v1),  [pdf](http://arxiv.org/pdf/2508.09570v1)

**Tags**: cs.AR B.3.0; B.6.0 



### Performant Automatic BLAS Offloading on Unified Memory Architecture with   OpenMP First-Touch Style Data Movement
**Authors**: Junjie Li

**Updated**: 2025-08-13T06:13:36Z

**Summary**: BLAS is a fundamental building block of advanced linear algebra libraries and many modern scientific computing applications. GPUs are known for their strong arithmetic computing capabilities and are highly suited for BLAS operations. However, porting code to GPUs often requires significant effort, especially for large, complex codes or legacy codes, even for BLAS-heavy applications. While various tools exist to automatically offload BLAS to GPUs, they are often impractical due to the high costs associated with mandatory data transfers. The advent of unified memory architectures in recent GPU designs, such as the NVIDIA Grace-Hopper, allows cache-coherent memory access across all types of memory for both CPU and GPU, potentially eliminating the bottlenecks faced in conventional architectures. This breakthrough paves the way for innovative application developments and porting strategies. Building on our preliminary work demonstrating the potential of automatic *gemm offload, this paper extends the framework to all level-3 BLAS operations and introduces SCILIB-Accel, a novel tool for automatic BLAS offload. SCILIB-Accel leverages the memory coherency in Grace-Hopper and introduces a Device First-Use data movement policy inspired by the OpenMP First-Touch approach in multi-socket CPU programming, minimizing CPU-GPU data transfers for typical scientific computing codes. Additionally, utilizing dynamic binary instrumentation, the tool intercepts BLAS symbols directly from a CPU binary, requiring no code modifications or recompilation. SCILIB-Accel has been evaluated using multiple quantum physics codes on up to a few hundred GPU nodes, yielding promising speedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was achieved on Grace-Hopper compared to Grace-Grace.

**Link**: [arxiv](http://arxiv.org/abs/2501.00279v4),  [pdf](http://arxiv.org/pdf/2501.00279v4)

**Tags**: cs.DC cs.MS cs.PF cs.SE 



### Advancing Reliable Test-Time Adaptation of Vision-Language Models under   Visual Variations
**Authors**: Yiwen Liang, Hui Chen, Yizhe Xiong, Zihan Zhou, Mengyao Lyu, Zijia Lin, Shuaicheng Niu, Sicheng Zhao, Jungong Han, Guiguang Ding

**Updated**: 2025-08-13T04:24:56Z

**Summary**: Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but struggle with distribution shifts in downstream tasks when labeled data is unavailable, which has motivated the development of Test-Time Adaptation (TTA) to improve VLMs' performance during inference without annotations. Among various TTA approaches, cache-based methods show promise by preserving historical knowledge from low-entropy samples in a dynamic cache and fostering efficient adaptation. However, these methods face two critical reliability challenges: (1) entropy often becomes unreliable under distribution shifts, causing error accumulation in the cache and degradation in adaptation performance; (2) the final predictions may be unreliable due to inflexible decision boundaries that fail to accommodate large downstream shifts. To address these challenges, we propose a Reliable Test-time Adaptation (ReTA) method that integrates two complementary strategies to enhance reliability from two perspectives. First, to mitigate the unreliability of entropy as a sample selection criterion for cache construction, we introduce Consistency-aware Entropy Reweighting (CER), which incorporates consistency constraints to weight entropy during cache updating. While conventional approaches rely solely on low entropy for cache prioritization and risk introducing noise, our method leverages predictive consistency to maintain a high-quality cache and facilitate more robust adaptation. Second, we present Diversity-driven Distribution Calibration (DDC), which models class-wise text embeddings as multivariate Gaussian distributions, enabling adaptive decision boundaries for more accurate predictions across visually diverse content. Extensive experiments demonstrate that ReTA consistently outperforms state-of-the-art methods, particularly under real-world distribution shifts. Code: https://github.com/Evelyn1ywliang/ReTA.

**Link**: [arxiv](http://arxiv.org/abs/2507.09500v2),  [pdf](http://arxiv.org/pdf/2507.09500v2)

**Tags**: cs.CV 



### The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM   Serving Systems
**Authors**: Linke Song, Zixuan Pang, Wenhao Wang, Zihao Wang, XiaoFeng Wang, Hongbo Chen, Wei Song, Yier Jin, Dan Meng, Rui Hou

**Updated**: 2025-08-13T04:03:10Z

**Summary**: The wide deployment of Large Language Models (LLMs) has given rise to strong demands for optimizing their inference performance. Today's techniques serving this purpose primarily focus on reducing latency and improving throughput through algorithmic and hardware enhancements, while largely overlooking their privacy side effects, particularly in a multi-user environment. In our research, for the first time, we discovered a set of new timing side channels in LLM systems, arising from shared caches and GPU memory allocations, which can be exploited to infer both confidential system prompts and those issued by other users. These vulnerabilities echo security challenges observed in traditional computing systems, highlighting an urgent need to address potential information leakage in LLM serving infrastructures. In this paper, we report novel attack strategies designed to exploit such timing side channels inherent in LLM deployments, specifically targeting the Key-Value (KV) cache and semantic cache widely used to enhance LLM inference performance. Our approach leverages timing measurements and classification models to detect cache hits, allowing an adversary to infer private prompts with high accuracy. We also propose a token-by-token search algorithm to efficiently recover shared prompt prefixes in the caches, showing the feasibility of stealing system prompts and those produced by peer users. Our experimental studies on black-box testing of popular online LLM services demonstrate that such privacy risks are completely realistic, with significant consequences. Our findings underscore the need for robust mitigation to protect LLM systems against such emerging threats.

**Link**: [arxiv](http://arxiv.org/abs/2409.20002v4),  [pdf](http://arxiv.org/pdf/2409.20002v4)

**Tags**: cs.CR 



### Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache   in LLM Inference
**Authors**: Zhifan Luo, Shuo Shao, Su Zhang, Lijing Zhou, Yuke Hu, Chenxu Zhao, Zhihao Liu, Zhan Qin

**Updated**: 2025-08-13T02:48:25Z

**Summary**: The Key-Value (KV) cache, which stores intermediate attention computations (Key and Value pairs) to avoid redundant calculations, is a fundamental mechanism for accelerating Large Language Model (LLM) inference. However, this efficiency optimization introduces significant yet underexplored privacy risks. This paper provides the first comprehensive analysis of these vulnerabilities, demonstrating that an attacker can reconstruct sensitive user inputs directly from the KV-cache. We design and implement three distinct attack vectors: a direct Inversion Attack, a more broadly applicable and potent Collision Attack, and a semantic-based Injection Attack. These methods demonstrate the practicality and severity of KV-cache privacy leakage issues. To mitigate this, we propose KV-Cloak, a novel, lightweight, and efficient defense mechanism. KV-Cloak uses a reversible matrix-based obfuscation scheme, combined with operator fusion, to secure the KV-cache. Our extensive experiments show that KV-Cloak effectively thwarts all proposed attacks, reducing reconstruction quality to random noise. Crucially, it achieves this robust security with virtually no degradation in model accuracy and minimal performance overhead, offering a practical solution for trustworthy LLM deployment.

**Link**: [arxiv](http://arxiv.org/abs/2508.09442v1),  [pdf](http://arxiv.org/pdf/2508.09442v1)

**Tags**: cs.CR cs.AI cs.CL 



### Design and Simulation of 6T SRAM Array
**Authors**: Justin London

**Updated**: 2025-08-13T01:39:09Z

**Summary**: Conventional 6T SRAM is used in microprocessors in the cache memory design. The basic 6T SRAM cell and a 6 bit memory array layout are designed in LEdit. The design and analysis of key SRAM components, sense amplifiers, decoders, write drivers and precharge circuits are also provided. The pulse voltage waveforms generated for read and write operations as well as Q and Qbar nodes are simulated in LTSpice. Parasitic capacitances are extracted and their impact on the waveforms analyzed. Static noise margin, propagation delays, and power dissipation are calculated. Comparison of SRAM read and write operational performance using CMOS transistors is made with edge-triggered D flip flops. If certain size area and ratio constraints are satisfied, the 6T cell with CMOS transistors will possess stability, speed, and power efficiency. Both theoretical and simulated results are given.

**Link**: [arxiv](http://arxiv.org/abs/2508.09419v1),  [pdf](http://arxiv.org/pdf/2508.09419v1)

**Tags**: eess.SY cs.SY 



### Scalable Graph Indexing using GPUs for Approximate Nearest Neighbor   Search
**Authors**: Zhonggen Li, Xiangyu Ke, Yifan Zhu, Bocheng Yu, Baihua Zheng, Yunjun Gao

**Updated**: 2025-08-13T01:39:03Z

**Summary**: Approximate nearest neighbor search (ANNS) in high-dimensional vector spaces has a wide range of real-world applications. Numerous methods have been proposed to handle ANNS efficiently, while graph-based indexes have gained prominence due to their high accuracy and efficiency. However, the indexing overhead of graph-based indexes remains substantial. With exponential growth in data volume and increasing demands for dynamic index adjustments, this overhead continues to escalate, posing a critical challenge. In this paper, we introduce Tagore, a fast library accelerated by GPUs for graph indexing, which has powerful capabilities of constructing refinement-based graph indexes such as NSG and Vamana. We first introduce GNN-Descent, a GPU-specific algorithm for efficient k-Nearest Neighbor (k-NN) graph initialization. GNN-Descent speeds up the similarity comparison by a two-phase descent procedure and enables highly parallelized neighbor updates. Next, aiming to support various k-NN graph pruning strategies, we formulate a universal computing procedure termed CFS and devise two generalized GPU kernels for parallel processing complex dependencies in neighbor relationships. For large-scale datasets exceeding GPU memory capacity, we propose an asynchronous GPU-CPU-disk indexing framework with a cluster-aware caching mechanism to minimize the I/O pressure on the disk. Extensive experiments on 7 real-world datasets exhibit that Tagore achieves 1.32x-112.79x speedup while maintaining the index quality.

**Link**: [arxiv](http://arxiv.org/abs/2508.08744v2),  [pdf](http://arxiv.org/pdf/2508.08744v2)

**Tags**: cs.DB cs.DC 



### Harnessing Input-Adaptive Inference for Efficient VLN
**Authors**: Dongwoo Kang, Akhil Perincherry, Zachary Coalson, Aiden Gabriel, Stefan Lee, Sanghyun Hong

**Updated**: 2025-08-12T18:05:33Z

**Summary**: An emerging paradigm in vision-and-language navigation (VLN) is the use of history-aware multi-modal transformer models. Given a language instruction, these models process observation and navigation history to predict the most appropriate action for an agent. While they have significantly improved performance, the scale of these models can be a bottleneck in practical settings with limited computational resources. In this work, we propose a novel input-adaptive navigation method to enhance VLN model efficiency. We first show that existing input-adaptive mechanisms fail to reduce computations without substantial performance degradation. To address this, we introduce three adaptive algorithms, each deployed at a different level: (1) To improve spatial efficiency, we selectively process panoramic views at each observation of an agent. (2) To improve intra-model efficiency, we propose importance-based adaptive thresholding for the early-exit methods. (3) To improve temporal efficiency, we implement a caching mechanism that prevents reprocessing of views previously seen by the agent. In evaluations on seven VLN benchmarks, we demonstrate over a 2$\times$ reduction in computation across three off-the-shelf agents in both standard and continuous environments. Our code is publicly available at https://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.

**Link**: [arxiv](http://arxiv.org/abs/2508.09262v1),  [pdf](http://arxiv.org/pdf/2508.09262v1)

**Tags**: cs.CV cs.LG 



### READER: Retrieval-Assisted Drafter for Efficient LLM Inference
**Authors**: Maxim Divilkovskiy, Vitaly Malygin, Sergey Zlobin, Sultan Isali, Vasily Kalugin, Stanislav Ilyushin, Nuriza Aitassova, Yi Fei, Zeng Weidi

**Updated**: 2025-08-12T16:47:48Z

**Summary**: Large Language Models (LLMs) generate tokens autoregressively, with each token depending on the preceding context. This sequential nature makes the inference process inherently difficult to accelerate, posing a significant challenge for efficient deployment. In recent years, various methods have been proposed to address this issue, with the most effective approaches often involving the training of additional draft models. In this paper, we introduce READER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel lossless speculative decoding method that enhances model-based approaches by leveraging self-repetitions in the text. Our algorithm expands the speculative decoding tree using tokens obtained through statistical search. This work focuses on large batch sizes (>= 8), an underexplored yet important area for industrial applications. We also analyze the key-value (KV) cache size during speculative decoding and propose an optimization to improve performance for large batches. As a result, READER outperforms existing speculative decoding methods. Notably, READER requires no additional training and can reuse pre-trained speculator models, increasing the speedup by over 40\%. Our method demonstrates particularly strong performance on search-based tasks, such as retrieval-augmented generation, where we achieve more than 10x speedup.

**Link**: [arxiv](http://arxiv.org/abs/2508.09072v1),  [pdf](http://arxiv.org/pdf/2508.09072v1)

**Tags**: cs.CL 



### Retrospective Sparse Attention for Efficient Long-Context Generation
**Authors**: Seonghwan Choi, Beomseok Kang, Dongwon Jo, Jae-Joon Kim

**Updated**: 2025-08-12T15:11:47Z

**Summary**: Large Language Models (LLMs) are increasingly deployed in long-context tasks such as reasoning, code generation, and multi-turn dialogue. However, inference over extended contexts is bottlenecked by the Key-Value (KV) cache, whose memory footprint grows linearly with sequence length and dominates latency at each decoding step. While recent KV cache compression methods identify and load important tokens, they focus predominantly on input contexts and fail to address the cumulative attention errors that arise during long decoding. In this paper, we introduce RetroAttention, a novel KV cache update technique that retrospectively revises past attention outputs using newly arrived KV entries from subsequent decoding steps. By maintaining a lightweight output cache, RetroAttention enables past queries to efficiently access more relevant context, while incurring minimal latency overhead. This breaks the fixed-attention-output paradigm and allows continual correction of prior approximations. Extensive experiments on long-generation benchmarks show that RetroAttention consistently outperforms state-of-the-art (SOTA) KV compression methods, increasing effective KV exposure by up to 1.6$\times$ and accuracy by up to 21.9\%.

**Link**: [arxiv](http://arxiv.org/abs/2508.09001v1),  [pdf](http://arxiv.org/pdf/2508.09001v1)

**Tags**: cs.CL cs.AI cs.LG 



### TaoCache: Structure-Maintained Video Generation Acceleration
**Authors**: Zhentao Fan, Zongzuo Wang, Weiwei Zhang

**Updated**: 2025-08-12T14:40:36Z

**Summary**: Existing cache-based acceleration methods for video diffusion models primarily skip early or mid denoising steps, which often leads to structural discrepancies relative to full-timestep generation and can hinder instruction following and character consistency. We present TaoCache, a training-free, plug-and-play caching strategy that, instead of residual-based caching, adopts a fixed-point perspective to predict the model's noise output and is specifically effective in late denoising stages. By calibrating cosine similarities and norm ratios of consecutive noise deltas, TaoCache preserves high-resolution structure while enabling aggressive skipping. The approach is orthogonal to complementary accelerations such as Pyramid Attention Broadcast (PAB) and TeaCache, and it integrates seamlessly into DiT-based frameworks. Across Latte-1, OpenSora-Plan v110, and Wan2.1, TaoCache attains substantially higher visual quality (LPIPS, SSIM, PSNR) than prior caching methods under the same speedups.

**Link**: [arxiv](http://arxiv.org/abs/2508.08978v1),  [pdf](http://arxiv.org/pdf/2508.08978v1)

**Tags**: cs.CV 



### Keep Your Friends Close: Leveraging Affinity Groups to Accelerate AI   Inference Workflows
**Authors**: Thiago Garrett, Weijia Song, Roman Vitenberg, Ken Birman

**Updated**: 2025-08-12T10:43:55Z

**Summary**: AI inference workflows are typically structured as a pipeline or graph of AI programs triggered by events. As events occur, the AIs perform inference or classification tasks under time pressure to respond or take some action. Standard techniques that reduce latency in other streaming settings (such as caching and optimization-driven scheduling) are of limited value because AI data access patterns (models, databases) change depending on the triggering event: a significant departure from traditional streaming. In this work, we propose a novel affinity grouping mechanism that makes it easier for developers to express application-specific data access correlations, enabling coordinated management of data objects in server clusters hosting streaming inference tasks. Our proposals are thus complementary to other approaches such as caching and scheduling. Experiments confirm the limitations of standard techniques, while showing that the proposed mechanism is able to maintain significantly lower latency as workload and scale-out increase, and yet requires only minor code changes.

**Link**: [arxiv](http://arxiv.org/abs/2312.11488v2),  [pdf](http://arxiv.org/pdf/2312.11488v2)

**Tags**: cs.DC cs.AI 



### From Slow Bidirectional to Fast Autoregressive Video Diffusion Models
**Authors**: Tianwei Yin, Qiang Zhang, Richard Zhang, William T. Freeman, Fredo Durand, Eli Shechtman, Xun Huang

**Updated**: 2025-08-12T05:51:37Z

**Summary**: Current video diffusion models achieve impressive generation quality but struggle in interactive applications due to bidirectional attention dependencies. The generation of a single frame requires the model to process the entire sequence, including the future. We address this limitation by adapting a pretrained bidirectional diffusion transformer to an autoregressive transformer that generates frames on-the-fly. To further reduce latency, we extend distribution matching distillation (DMD) to videos, distilling 50-step diffusion model into a 4-step generator. To enable stable and high-quality distillation, we introduce a student initialization scheme based on teacher's ODE trajectories, as well as an asymmetric distillation strategy that supervises a causal student model with a bidirectional teacher. This approach effectively mitigates error accumulation in autoregressive generation, allowing long-duration video synthesis despite training on short clips. Our model achieves a total score of 84.27 on the VBench-Long benchmark, surpassing all previous video generation models. It enables fast streaming generation of high-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our approach also enables streaming video-to-video translation, image-to-video, and dynamic prompting in a zero-shot manner.

**Link**: [arxiv](http://arxiv.org/abs/2412.07772v3),  [pdf](http://arxiv.org/pdf/2412.07772v3)

**Tags**: cs.CV 



### Rigorous quantum calculations for atom-molecule chemical reactions in   electric fields: from single to multiple partial wave regimes
**Authors**: Timur V. Tscherbul, Roman V. Krems

**Updated**: 2025-08-12T03:33:15Z

**Summary**: We present an efficient method for rigorous quantum calculations of cross sections for atom-molecule reactive scattering in the presence of a dc electric field. The wavefunction of the reaction complex is expanded in an overcomplete set of arrangement-dependent Fock-Delves hyperspherical basis functions and the interactions of the reactants and products with electric fields are accounted for in the total angular momentum representation. A significant computational challenge affecting our previously developed approach [Phys. Rev. Lett. $\mathbf{115}$, 023201 (2015)] is addressed by an efficient asymptotic frame transformation between the hyperspherical and Jacobi coordinates in the presence of an external field. Using accurate {\it ab initio} potential energy surfaces, we calculate total and state-resolved cross sections for the chemical reactions LiF$(v=1,j=0)$ + H $\to$ Li + HF($v'=0,j'$) and F + HD$(v=0,j=0)$ $\to$ HF + D, DF + H as functions of collision energy and electric field strength. The field dependence of the cross sections for the LiF + H chemical reaction exhibits resonance structure mediated by tunneling-driven interactions between reactants and products. No significant field effects are found for the F + HD $\to$ HF + D, DF + H chemical reaction at 1 Kelvin, even for state-resolved transitions and with field magnitudes reaching 200 kV/cm. Our calculations illustrate the essential role of basis set convergence for the proper interpretation of external field effects on chemical reaction dynamics. While reduced-basis calculations for the F + HD reaction indicate significant effects of electric fields on product state distributions, these effects vanish when the number of total angular momentum basis states is increased.

**Link**: [arxiv](http://arxiv.org/abs/2508.08600v1),  [pdf](http://arxiv.org/pdf/2508.08600v1)

**Tags**: physics.chem-ph physics.atom-ph 



### Semantic Caching for Low-Cost LLM Serving: From Offline Learning to   Online Adaptation
**Authors**: Xutong Liu, Baran Atalar, Xiangxiang Dai, Jinhang Zuo, Siwei Wang, John C. S. Lui, Wei Chen, Carlee Joe-Wong

**Updated**: 2025-08-12T02:51:12Z

**Summary**: Large Language Models (LLMs) are revolutionizing how users interact with information systems, yet their high inference cost poses serious scalability and sustainability challenges. Caching inference responses, allowing them to be retrieved without another forward pass through the LLM, has emerged as one possible solution. Traditional exact-match caching, however, overlooks the semantic similarity between queries, leading to unnecessary recomputation. Semantic caching addresses this by retrieving responses based on semantic similarity, but introduces a fundamentally different cache eviction problem: one must account for mismatch costs between incoming queries and cached responses. Moreover, key system parameters, such as query arrival probabilities and serving costs, are often unknown and must be learned over time. Existing semantic caching methods are largely ad-hoc, lacking theoretical foundations and unable to adapt to real-world uncertainty. In this paper, we present a principled, learning-based framework for semantic cache eviction under unknown query and cost distributions. We formulate both offline optimization and online learning variants of the problem, and develop provably efficient algorithms with state-of-the-art guarantees. We also evaluate our framework on a synthetic dataset, showing that our proposed algorithms perform matching or superior performance compared with baselines.

**Link**: [arxiv](http://arxiv.org/abs/2508.07675v2),  [pdf](http://arxiv.org/pdf/2508.07675v2)

**Tags**: cs.LG 



### Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided   Region Control
**Authors**: Zeqian Long, Mingzhe Zheng, Kunyu Feng, Xinhua Zhang, Hongyu Liu, Harry Yang, Linfeng Zhang, Qifeng Chen, Yue Ma

**Updated**: 2025-08-12T02:27:05Z

**Summary**: While recent flow-based image editing models demonstrate general-purpose capabilities across diverse tasks, they often struggle to specialize in challenging scenarios -- particularly those involving large-scale shape transformations. When performing such structural edits, these methods either fail to achieve the intended shape change or inadvertently alter non-target regions, resulting in degraded background quality. We propose Follow-Your-Shape, a training-free and mask-free framework that supports precise and controllable editing of object shapes while strictly preserving non-target content. Motivated by the divergence between inversion and editing trajectories, we compute a Trajectory Divergence Map (TDM) by comparing token-wise velocity differences between the inversion and denoising paths. The TDM enables precise localization of editable regions and guides a Scheduled KV Injection mechanism that ensures stable and faithful editing. To facilitate a rigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120 new images and enriched prompt pairs specifically curated for shape-aware editing. Experiments demonstrate that our method achieves superior editability and visual fidelity, particularly in tasks requiring large-scale shape replacement.

**Link**: [arxiv](http://arxiv.org/abs/2508.08134v2),  [pdf](http://arxiv.org/pdf/2508.08134v2)

**Tags**: cs.CV 



### Profiling Large Language Model Inference on Apple Silicon: A   Quantization Perspective
**Authors**: Afsara Benazir, Felix Xiaozhu Lin

**Updated**: 2025-08-12T00:06:34Z

**Summary**: A systematic understanding of Apple Silicon is lacking in the current landscape of hardware efficiency; research focus is largely centered on accelerating GPUs for large-scale training or inference on CUDA devices. This paper investigates Apple Silicon's unique memory architecture that offers a unified memory integrating CPU and GPU memory and its implications for on-device LLM inference.   We decipher myths about whether Apple Silicon is efficient for on-device inference compared to competitors such as NVIDIA GPUs by directly conducting latency and throughput comparison benchmarks. We explain the performance gap between them through profiling low level hardware metrics - ALU utilization, memory bandwidth, buffer usage, cache residency etc. at runtime. We draw several insights regarding performance bottlenecks such as dequantization overhead, compute throughput and memory bandwidth. We debunk existing false claims regarding large language model inference such as compressing models to lower bit precision is a defacto promise for faster inference across all hardware platforms. We find that the large unified memory enables Apple Silicon to be both cost effective and efficient against NVIDIA GPUs for ultra large language models.   Our large scale evaluation on 5 hardware testbeds incorporating three Apple M-series devices: M2 Ultra, M2 Max and M4 Pro and two NVIDIA GPUs: NVIDIA RTX A6000, a multi GPU setup with 2xNVIDIA RTX A6000, 5 model scales ranging from 8B to 405B parameters and 14 quantization schemes gives an understanding of how Apple Silicon fits within the paradigm of on-device LLM inference. Our analysis reveals multiple resource interdependencies and unexpected findings, while also quantifying established insights. To the best of our knowledge, this study makes the first attempt to present a thorough characterization and analysis of Apple Silicon for on-device inference.

**Link**: [arxiv](http://arxiv.org/abs/2508.08531v1),  [pdf](http://arxiv.org/pdf/2508.08531v1)

**Tags**: cs.PF 



### Architecting Long-Context LLM Acceleration with Packing-Prefetch   Scheduler and Ultra-Large Capacity On-Chip Memories
**Authors**: Ming-Yen Lee, Faaiq Waqar, Hanchen Yang, Muhammed Ahosan Ul Karim, Harsono Simka, Shimeng Yu

**Updated**: 2025-08-11T20:30:31Z

**Summary**: Long-context Large Language Model (LLM) inference faces increasing compute bottlenecks as attention calculations scale with context length, primarily due to the growing KV-cache transfer overhead that saturates High Bandwidth Memory (HBM). While prefetching techniques mitigate cache misses by fetching KV data in advance, their spatial and temporal benefits present new opportunities to exploit. This work proposes a packing-prefetch scheduling architecture with monolithic 3D (M3D) back-end-of-line (BEOL) compatible embedded memories with ultra-large on-chip capacity to accelerate long-context LLM inference. Our optimizations demonstrate 8.06x decode speedup and 1.83x overall latency reduction on Llama3.1-8B using TPUv6e-like hardware with additional 512MB BEOL memories over the serial execution. Evaluations of multi-request workloads on TPU-like architectures show 1.7x-2.4x throughput improvement and 1.5x-2.4x HBM bandwidth reduction compared to packing-only methods on Llama3.1-8B and Llama3.1-70B models. With the co-design of packing, prefetching, and BEOL memories, our approach alleviates HBM constraints and enables efficient long-context LLM inference.

**Link**: [arxiv](http://arxiv.org/abs/2508.08457v1),  [pdf](http://arxiv.org/pdf/2508.08457v1)

**Tags**: cs.AR cs.ET C.1.3; B.3.1 



### Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM   Inference
**Authors**: Kexin Chu, Zecheng Lin, Dawei Xiang, Zixu Shen, Jianchang Su, Cheng Chu, Yiwei Yang, Wenhui Zhang, Wenfei Wu, Wei Zhang

**Updated**: 2025-08-11T19:55:44Z

**Summary**: Global KV-cache sharing has emerged as a key optimization for accelerating large language model (LLM) inference. However, it exposes a new class of timing side-channel attacks, enabling adversaries to infer sensitive user inputs via shared cache entries. Existing defenses, such as per-user isolation, eliminate leakage but degrade performance by up to 38.9% in time-to-first-token (TTFT), making them impractical for high-throughput deployment. To address this gap, we introduce SafeKV (Secure and Flexible KV Cache Sharing), a privacy-aware KV-cache management framework that selectively shares non-sensitive entries while confining sensitive content to private caches. SafeKV comprises three components: (i) a hybrid, multi-tier detection pipeline that integrates rule-based pattern matching, a general-purpose privacy detector, and context-aware validation; (ii) a unified radix-tree index that manages public and private entries across heterogeneous memory tiers (HBM, DRAM, SSD); and (iii) entropy-based access monitoring to detect and mitigate residual information leakage. Our evaluation shows that SafeKV mitigates 94% - 97% of timing-based side-channel attacks. Compared to per-user isolation method, SafeKV improves TTFT by up to 40.58% and throughput by up to 2.66X across diverse LLMs and workloads. SafeKV reduces cache-induced TTFT overhead from 50.41% to 11.74% on Qwen3-235B. By combining fine-grained privacy control with high cache reuse efficiency, SafeKV reclaims the performance advantages of global sharing while providing robust runtime privacy guarantees for LLM inference.

**Link**: [arxiv](http://arxiv.org/abs/2508.08438v1),  [pdf](http://arxiv.org/pdf/2508.08438v1)

**Tags**: cs.CR cs.LG cs.OS 



### Numerical computation of linearized KV and the Deligne-Drinfeld and   Broadhurst-Kreimer conjectures
**Authors**: Florian Naef, Thomas Willwacher

**Updated**: 2025-08-11T15:28:28Z

**Summary**: We compute numerically the dimensions of the graded quotients of the linearized Kashiwara-Vergne Lie algebra lkv in low weight, confirming a conjecture of Raphael-Schneps in those weights. The Lie algebra lkv appears in a chain of inclusions of Lie algebras, including also the linearized double shuffle Lie algebra and the (depth associated graded of the) Grothendieck-Teichm\"uller Lie algebra. Hence our computations also allow us to check the validity of the Deligne-Drinfeld conjecture on the structure of the Grothendieck-Teichm\"uller group up to weight 29, and (a version of) the the Broadhurst-Kreimer conjecture on the number of multiple zeta values for a range of weight-depth pairs significantly exceeding the previous bounds. Our computations also verify a conjecture by Alekseev-Torossian on the Kashiwara-Vergne Lie algebra up to weight 29.

**Link**: [arxiv](http://arxiv.org/abs/2508.08081v1),  [pdf](http://arxiv.org/pdf/2508.08081v1)

**Tags**: math.QA 



### From Reusing to Forecasting: Accelerating Diffusion Models with   TaylorSeers
**Authors**: Jiacheng Liu, Chang Zou, Yuanhuiyi Lyu, Junjie Chen, Linfeng Zhang

**Updated**: 2025-08-11T14:15:27Z

**Summary**: Diffusion Transformers (DiT) have revolutionized high-fidelity image and video synthesis, yet their computational demands remain prohibitive for real-time applications. To solve this problem, feature caching has been proposed to accelerate diffusion models by caching the features in the previous timesteps and then reusing them in the following timesteps. However, at timesteps with significant intervals, the feature similarity in diffusion models decreases substantially, leading to a pronounced increase in errors introduced by feature caching, significantly harming the generation quality. To solve this problem, we propose TaylorSeer, which firstly shows that features of diffusion models at future timesteps can be predicted based on their values at previous timesteps. Based on the fact that features change slowly and continuously across timesteps, TaylorSeer employs a differential method to approximate the higher-order derivatives of features and predict features in future timesteps with Taylor series expansion. Extensive experiments demonstrate its significant effectiveness in both image and video synthesis, especially in high acceleration ratios. For instance, it achieves an almost lossless acceleration of 4.99$\times$ on FLUX and 5.00$\times$ on HunyuanVideo without additional training. On DiT, it achieves $3.41$ lower FID compared with previous SOTA at $4.53$$\times$ acceleration. %Our code is provided in the supplementary materials and will be made publicly available on GitHub. Our codes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer

**Link**: [arxiv](http://arxiv.org/abs/2503.06923v2),  [pdf](http://arxiv.org/pdf/2503.06923v2)

**Tags**: cs.CV cs.AI 



### Maximizing GPU Efficiency via Optimal Adapter Caching: An Analytical   Approach for Multi-Tenant LLM Serving
**Authors**: Ferran Agullo, Joan Oliveras, Chen Wang, Alberto Gutierrez-Torre, Olivier Tardieu, Alaa Youssef, Jordi Torres, Josep Ll. Berral

**Updated**: 2025-08-11T10:47:35Z

**Summary**: Serving LLM adapters has gained significant attention as an effective approach to adapt general-purpose language models to diverse, task-specific use cases. However, serving a wide range of adapters introduces several and substantial overheads, leading to performance degradation and challenges in optimal placement. To address these challenges, we present an analytical, AI-driven pipeline that accurately determines the optimal allocation of adapters in single-node setups. This allocation maximizes performance, effectively using GPU resources, while preventing request starvation. Crucially, the proposed allocation is given based on current workload patterns. These insights in single-node setups can be leveraged in multi-replica deployments for overall placement, load balancing and server configuration, ultimately enhancing overall performance and improving resource efficiency. Our approach builds on an in-depth analysis of LLM adapter serving, accounting for overheads and performance variability, and includes the development of the first Digital Twin capable of replicating online LLM-adapter serving systems with matching key performance metrics. The experimental results demonstrate that the Digital Twin achieves a SMAPE difference of no more than 5.5% in throughput compared to real results, and the proposed pipeline accurately predicts the optimal placement with minimal latency.

**Link**: [arxiv](http://arxiv.org/abs/2508.08343v1),  [pdf](http://arxiv.org/pdf/2508.08343v1)

**Tags**: cs.PF cs.AI cs.CL 



### DiTVR: Zero-Shot Diffusion Transformer for Video Restoration
**Authors**: Sicheng Gao, Nancy Mehta, Zongwei Wu, Radu Timofte

**Updated**: 2025-08-11T09:54:45Z

**Summary**: Video restoration aims to reconstruct high quality video sequences from low quality inputs, addressing tasks such as super resolution, denoising, and deblurring. Traditional regression based methods often produce unrealistic details and require extensive paired datasets, while recent generative diffusion models face challenges in ensuring temporal consistency. We introduce DiTVR, a zero shot video restoration framework that couples a diffusion transformer with trajectory aware attention and a wavelet guided, flow consistent sampler. Unlike prior 3D convolutional or frame wise diffusion approaches, our attention mechanism aligns tokens along optical flow trajectories, with particular emphasis on vital layers that exhibit the highest sensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically selects relevant tokens based on motion correspondences across frames. The flow guided sampler injects data consistency only into low-frequency bands, preserving high frequency priors while accelerating convergence. DiTVR establishes a new zero shot state of the art on video restoration benchmarks, demonstrating superior temporal consistency and detail preservation while remaining robust to flow noise and occlusions.

**Link**: [arxiv](http://arxiv.org/abs/2508.07811v1),  [pdf](http://arxiv.org/pdf/2508.07811v1)

**Tags**: cs.CV 



### SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by   Exploiting Temporal Continuity
**Authors**: Kunyun Wang, Shuo Yang, Jieru Zhao, Wenchao Ding, Quan Chen, Jingwen Leng, Minyi Guo

**Updated**: 2025-08-11T08:10:21Z

**Summary**: Deep learning models have become pivotal in the field of video processing and is increasingly critical in practical applications such as autonomous driving and object detection. Although Vision Transformers (ViTs) have demonstrated their power, Convolutional Neural Networks (CNNs) remain a highly efficient and high-performance choice for feature extraction and encoding. However, the intensive computational demands of convolution operations hinder its broader adoption as a video encoder. Given the inherent temporal continuity in video frames, changes between consecutive frames are minimal, allowing for the skipping of redundant computations. This technique, which we term as Diff Computation, presents two primary challenges. First, Diff Computation requires to cache intermediate feature maps to ensure the correctness of non-linear computations, leading to significant memory consumption. Second, the imbalance of sparsity among layers, introduced by Diff Computation, incurs accuracy degradation. To address these issues, we propose a memory-efficient scheduling method to eliminate memory overhead and an online adjustment mechanism to minimize accuracy degradation. We integrate these techniques into our framework, SparseTem, to seamlessly support various CNN-based video encoders. SparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with minimal accuracy drop and no additional memory overhead. Extensive experimental results demonstrate that SparseTem sets a new state-of-the-art by effectively utilizing temporal continuity to accelerate CNN-based video encoders.

**Link**: [arxiv](http://arxiv.org/abs/2410.20790v2),  [pdf](http://arxiv.org/pdf/2410.20790v2)

**Tags**: cs.CV 



### Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language   Models
**Authors**: Khanh-Binh Nguyen, Phuoc-Nguyen Bui, Hyunseung Choo, Duc Thanh Nguyen

**Updated**: 2025-08-11T03:03:34Z

**Summary**: Vision-language models (VLMs) exhibit remarkable zero-shot generalization but suffer performance degradation under distribution shifts in downstream tasks, particularly in the absence of labeled data. Test-Time Adaptation (TTA) addresses this challenge by enabling online optimization of VLMs during inference, eliminating the need for annotated data. Cache-based TTA methods exploit historical knowledge by maintaining a dynamic memory cache of low-entropy or high-confidence samples, promoting efficient adaptation to out-of-distribution data. Nevertheless, these methods face two critical challenges: (1) unreliable confidence metrics under significant distribution shifts, resulting in error accumulation within the cache and degraded adaptation performance; and (2) rigid decision boundaries that fail to accommodate substantial distributional variations, leading to suboptimal predictions. To overcome these limitations, we introduce the Adaptive Cache Enhancement (ACE) framework, which constructs a robust cache by selectively storing high-confidence or low-entropy image embeddings per class, guided by dynamic, class-specific thresholds initialized from zero-shot statistics and iteratively refined using an exponential moving average and exploration-augmented updates. This approach enables adaptive, class-wise decision boundaries, ensuring robust and accurate predictions across diverse visual distributions. Extensive experiments on 15 diverse benchmark datasets demonstrate that ACE achieves state-of-the-art performance, delivering superior robustness and generalization compared to existing TTA methods in challenging out-of-distribution scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2508.07570v1),  [pdf](http://arxiv.org/pdf/2508.07570v1)

**Tags**: cs.CV 



### CoMoE: Collaborative Optimization of Expert Aggregation and Offloading   for MoE-based LLMs at Edge
**Authors**: Muqing Li, Ning Li, Xin Yuan, Wenchao Xu, Quan Chen, Song Guo, Haijun Zhang

**Updated**: 2025-08-10T14:05:36Z

**Summary**: The proliferation of large language models (LLMs) has driven the adoption of Mixture-of-Experts (MoE) architectures as a promising solution to scale model capacity while controlling computational costs. However, deploying MoE models in resource-constrained mobile edge computing environments presents significant challenges due to their large memory footprint and dynamic expert activation patterns. To address these challenges, we propose a novel dynamic resource-aware collaborative optimization framework that jointly optimizes expert aggregation granularity and offloading strategies based on real-time device resource states, network conditions, and input characteristics in mobile edge environments, denoted as CoMoE. In CoMoE, we first systematically analyze existing expert aggregation techniques, including expert parameter merging,knowledge distillation,and parameter sharing decomposition, identifying their limitations in dynamic mobile environments.We then investigate expert offloading strategies encompassing expert prediction and prefetching, expert caching and scheduling, and multi-tier storage architectures, revealing the interdependencies between routing decisions and offloading performance.The CoMoE incorporates adaptive scheduling mechanisms that respond to user mobility and varying network conditions, enabling efficient MoE deployment across heterogeneous edge devices. Extensive experiments on real mobile edge testbeds demonstrate that CoMoE achieves approximately 70% reduction in memory usage compared to baseline methods, 10.5% lower inference latency than existing expert offloading techniques, while maintaining model performance stability. For large-scale MoE models (e.g,7.4B-parameter Switch-Base-128), the CoMoE reduces memory requirements from 15.6GB to 4.7GB, enabling deployment on resource-constrained mobile edge devices that previously could only support much smaller models.

**Link**: [arxiv](http://arxiv.org/abs/2508.09208v1),  [pdf](http://arxiv.org/pdf/2508.09208v1)

**Tags**: cs.NI cs.AI 



### CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal   Diffusion
**Authors**: Jiahua Ma, Yiran Qin, Yixiong Li, Xuanqi Liao, Yulan Guo, Ruimao Zhang

**Updated**: 2025-08-09T11:31:44Z

**Summary**: Diffusion Policy (DP) enables robots to learn complex behaviors by imitating expert demonstrations through action diffusion. However, in practical applications, hardware limitations often degrade data quality, while real-time constraints restrict model inference to instantaneous state and scene observations. These limitations seriously reduce the efficacy of learning from expert demonstrations, resulting in failures in object localization, grasp planning, and long-horizon task execution. To address these challenges, we propose Causal Diffusion Policy (CDP), a novel transformer-based diffusion model that enhances action prediction by conditioning on historical action sequences, thereby enabling more coherent and context-aware visuomotor policy learning. To further mitigate the computational cost associated with autoregressive inference, a caching mechanism is also introduced to store attention key-value pairs from previous timesteps, substantially reducing redundant computations during execution. Extensive experiments in both simulated and real-world environments, spanning diverse 2D and 3D manipulation tasks, demonstrate that CDP uniquely leverages historical action sequences to achieve significantly higher accuracy than existing methods. Moreover, even when faced with degraded input observation quality, CDP maintains remarkable precision by reasoning through temporal continuity, which highlights its practical robustness for robotic control under realistic, imperfect conditions.

**Link**: [arxiv](http://arxiv.org/abs/2506.14769v2),  [pdf](http://arxiv.org/pdf/2506.14769v2)

**Tags**: cs.CV cs.RO 



### CannyEdit: Selective Canny Control and Dual-Prompt Guidance for   Training-Free Image Editing
**Authors**: Weiyan Xie, Han Gao, Didan Deng, Kaican Li, April Hua Liu, Yongxiang Huang, Nevin L. Zhang

**Updated**: 2025-08-09T11:06:58Z

**Summary**: Recent advances in text-to-image (T2I) models have enabled training-free regional image editing by leveraging the generative priors of foundation models. However, existing methods struggle to balance text adherence in edited regions, context fidelity in unedited areas, and seamless integration of edits. We introduce CannyEdit, a novel training-free framework that addresses these challenges through two key innovations: (1) Selective Canny Control, which masks the structural guidance of Canny ControlNet in user-specified editable regions while strictly preserving details of the source images in unedited areas via inversion-phase ControlNet information retention. This enables precise, text-driven edits without compromising contextual integrity. (2) Dual-Prompt Guidance, which combines local prompts for object-specific edits with a global target prompt to maintain coherent scene interactions. On real-world image editing tasks (addition, replacement, removal), CannyEdit outperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent improvement in the balance of text adherence and context fidelity. In terms of editing seamlessness, user studies reveal only 49.2 percent of general users and 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited when paired with real images without edits, versus 76.08 to 89.09 percent for competitor methods.

**Link**: [arxiv](http://arxiv.org/abs/2508.06937v1),  [pdf](http://arxiv.org/pdf/2508.06937v1)

**Tags**: cs.CV cs.AI 



### Managing Data for Scalable and Interactive Event Sequence Visualization
**Authors**: Sayef Azad Sakin, Katherine E. Isaacs

**Updated**: 2025-08-09T02:33:21Z

**Summary**: Parallel event sequences, such as those collected in program execution traces and automated manufacturing pipelines, are typically visualized as interactive parallel timelines. As the dataset size grows, these charts frequently experience lag during common interactions such as zooming, panning, and filtering. Summarization approaches can improve interaction performance, but at the cost of accuracy in representation. To address this challenge, we introduce ESeMan (Event Sequence Manager), an event sequence management system designed to support interactive rendering of timeline visualizations with tunable accuracy. ESeMan employs hierarchical data structures and intelligent caching to provide visualizations with only the data necessary to generate accurate summarizations with significantly reduced data fetch time. We evaluate ESeMan's query times against summed area tables, M4 aggregation, and statistical sub-sampling on a variety of program execution traces. Our results demonstrate ESeMan provides better performance, achieving sub-100ms fetch times while maintaining visualization accuracy at the pixel level. We further present our benchmarking harness, enabling future performance evaluations for event sequence visualization.

**Link**: [arxiv](http://arxiv.org/abs/2508.03974v2),  [pdf](http://arxiv.org/pdf/2508.03974v2)

**Tags**: cs.HC 



### PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile   Device via Additive Side-Tuning
**Authors**: Xingke Yang, Liang Li, Zhiyi Wan, Sicong Li, Xiaoqi Qi, Jiang Liu, Tomoaki Ohtsuki, Xin Fu, Miao Pan

**Updated**: 2025-08-09T00:12:01Z

**Summary**: There is a huge gap between numerous intriguing applications fostered by on-device large language model (LLM) fine-tuning (FT) from fresh mobile data and the limited resources of a mobile device. While existing server-assisted methods (e.g., split learning or side-tuning) may enable LLM FT on the local mobile device, they suffer from heavy communication burdens of activation transmissions, and may disclose data and labels to the server. To address those issues, we develop PAE MobiLLM, a a privacy-aware and efficient LLM FT method which can be deployed on the mobile device via server-assisted additive side-tuning. To further accelerate FT convergence and improve computing efficiency, PAE MobiLLM integrates activation caching on the server side, which allows the server to reuse historical activations and saves the mobile device from repeatedly computing forward passes for the recurring data samples. Besides, to reduce communication cost, PAE MobiLLM develops an activation shortcut that transmits only the token involved in the loss calculation instead of full activation matrices to guide the side network tuning. Last but not least, PAE MobiLLM introduces the additive adapter side-network design which makes the server train the adapter modules based on device-defined prediction differences rather than raw ground-truth labels. In this way, the server can only assist device-defined side-network computing, and learn nothing about data and labels. Extensive experimental results demonstrate PAE MobiLLM's superiority.

**Link**: [arxiv](http://arxiv.org/abs/2507.01216v2),  [pdf](http://arxiv.org/pdf/2507.01216v2)

**Tags**: cs.LG cs.CR 



### X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and   Extreme KV Compression
**Authors**: Guihong Li, Mehdi Rezagholizadeh, Mingyu Yang, Vikram Appia, Emad Barsoum

**Updated**: 2025-08-08T18:16:33Z

**Summary**: Multi-head latent attention (MLA) is designed to optimize KV cache memory through low-rank key-value joint compression. Rather than caching keys and values separately, MLA stores their compressed latent representations, reducing memory overhead while maintaining the performance. While MLA improves memory efficiency without compromising language model accuracy, its major limitation lies in its integration during the pre-training phase, requiring models to be trained from scratch. This raises a key question: can we use MLA's benefits fully or partially in models that have already been pre-trained with different attention mechanisms? In this paper, we propose X-EcoMLA to deploy post training distillation to enable the upcycling of Transformer-based attention into an efficient hybrid MLA variant through lightweight post-training adaptation, bypassing the need for extensive pre-training. We demonstrate that leveraging the dark knowledge of a well-trained model can enhance training accuracy and enable extreme KV cache compression in MLA without compromising model performance. The experimental results show that our proposed method can effectively compress the KV cache while preserving the performance on the benchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression achieves the same average score by using only 3.6B training tokens and 70 GPU hours on AMD MI300, whereas a 10.6x compression have less than 0.1% average score drop with 7B training tokens and 140 GPU hours. The code for this work is available at https://github.com/AMD-AIG-AIMA/AMD-Hybrid-Models.

**Link**: [arxiv](http://arxiv.org/abs/2503.11132v3),  [pdf](http://arxiv.org/pdf/2503.11132v3)

**Tags**: cs.CL 



### SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token   Pruning
**Authors**: Lingkun Long, Rubing Yang, Yushi Huang, Desheng Hui, Ao Zhou, Jianlei Yang

**Updated**: 2025-08-08T16:42:38Z

**Summary**: Long-context inference for Large Language Models (LLMs) is heavily limited by high computational demands. While several existing methods optimize attention computation, they still process the full set of hidden states at each layer, limiting overall efficiency. In this work, we propose SlimInfer, an innovative framework that aims to accelerate inference by directly pruning less critical prompt tokens during the forward pass. Our key insight is an information diffusion phenomenon: As information from critical tokens propagates through layers, it becomes distributed across the entire sequence. This diffusion process suggests that LLMs can maintain their semantic integrity when excessive tokens, even including these critical ones, are pruned in hidden states. Motivated by this, SlimInfer introduces a dynamic fine-grained pruning mechanism that accurately removes redundant tokens of hidden state at intermediate layers. This layer-wise pruning naturally enables an asynchronous KV cache manager that prefetches required token blocks without complex predictors, reducing both memory usage and I/O costs. Extensive experiments show that SlimInfer can achieve up to $\mathbf{2.53\times}$ time-to-first-token (TTFT) speedup and $\mathbf{1.88\times}$ end-to-end latency reduction for LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on LongBench. Our code will be released upon acceptance.

**Link**: [arxiv](http://arxiv.org/abs/2508.06447v1),  [pdf](http://arxiv.org/pdf/2508.06447v1)

**Tags**: cs.CL 



### LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs
**Authors**: Fatemeh Hadadi, Qinghua Xu, Domenico Bianculli, Lionel Briand

**Updated**: 2025-08-08T14:25:24Z

**Summary**: Most log-based anomaly detectors assume logs are stable, though logs are often unstable due to software or environmental changes. Anomaly detection on unstable logs (ULAD) is therefore a more realistic, yet under-investigated challenge. Current approaches predominantly employ machine learning (ML) models, which often require extensive labeled data for training. To mitigate data insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that combines ML models -- decision tree, k-nearest neighbors, and a feedforward neural network -- with a Large Language Model (Mistral) through ensemble learning. FlexLog also incorporates a cache and retrieval-augmented generation (RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we configured four datasets for \task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and SYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points (pp) in F1 score while using much less labeled data (62.87 pp reduction). When trained on the same amount of data as the baselines, FlexLog achieves up to a 13 pp increase in F1 score on ADFA-U across varying training dataset sizes. Additionally, FlexLog maintains inference time under one second per log sequence, making it suitable for most applications, except latency-sensitive systems. Further analysis reveals the positive impact of FlexLog's key components: cache, RAG and ensemble learning.

**Link**: [arxiv](http://arxiv.org/abs/2406.07467v3),  [pdf](http://arxiv.org/pdf/2406.07467v3)

**Tags**: cs.SE 



### KV Cache Compression for Inference Efficiency in LLMs: A Review
**Authors**: Yanyu Liu, Jingying Fu, Sixiang Liu, Yitian Zou, You Fu, Jiehan Zhou, Shouhua Zhang

**Updated**: 2025-08-08T13:19:30Z

**Summary**: Withtherapid advancement of large language models (LLMs), the context length for inference has been continuously increasing, leading to an exponential growth in the demand for Key-Value (KV) caching. This has resulted in a significant memory bottleneck, limiting the inference efficiency and scalability of the models. Therefore, optimizing the KV cache during inference is crucial for enhancing performance and efficiency. This review systematically examines current KV cache optimization techniques, including compression strategies such as selective token strategies, quantization, and attention compression. We evaluate the effectiveness, trade-offs, and application scenarios of these methods, providing a comprehensive analysis of their impact on memory usage and inference speed. We focus on identifying the limitations and challenges of existing methods, such as compatibility issues with different models and tasks. Additionally, this review highlights future research directions, including hybrid optimization techniques, adaptive dynamic strategies, and software-hardware co-design. These approaches aim to improve inference efficiency and promote the practical application of large language models.

**Link**: [arxiv](http://arxiv.org/abs/2508.06297v1),  [pdf](http://arxiv.org/pdf/2508.06297v1)

**Tags**: cs.DC 



## Keyword: LLM Inference 
 ### MV-RAG: Retrieval Augmented Multiview Diffusion
**Authors**: Yosef Dayani, Omer Benishu, Sagie Benaim

**Updated**: 2025-08-22T17:59:40Z

**Summary**: Text-to-3D generation approaches have advanced significantly by leveraging pretrained 2D diffusion priors, producing high-quality and 3D-consistent outputs. However, they often fail to produce out-of-domain (OOD) or rare concepts, yielding inconsistent or inaccurate results. To this end, we propose MV-RAG, a novel text-to-3D pipeline that first retrieves relevant 2D images from a large in-the-wild 2D database and then conditions a multiview diffusion model on these images to synthesize consistent and accurate multiview outputs. Training such a retrieval-conditioned model is achieved via a novel hybrid strategy bridging structured multiview data and diverse 2D image collections. This involves training on multiview data using augmented conditioning views that simulate retrieval variance for view-specific reconstruction, alongside training on sets of retrieved real-world 2D images using a distinctive held-out view prediction objective: the model predicts the held-out view from the other views to infer 3D consistency from 2D data. To facilitate a rigorous OOD evaluation, we introduce a new collection of challenging OOD prompts. Experiments against state-of-the-art text-to-3D, image-to-3D, and personalization baselines show that our approach significantly improves 3D consistency, photorealism, and text adherence for OOD/rare concepts, while maintaining competitive performance on standard benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2508.16577v1),  [pdf](http://arxiv.org/pdf/2508.16577v1)

**Tags**: cs.CV cs.AI 



### Are LLM-Powered Social Media Bots Realistic?
**Authors**: Lynnette Hui Xian Ng, Kathleen M. Carley

**Updated**: 2025-08-22T17:56:26Z

**Summary**: As Large Language Models (LLMs) become more sophisticated, there is a possibility to harness LLMs to power social media bots. This work investigates the realism of generating LLM-Powered social media bot networks. Through a combination of manual effort, network science and LLMs, we create synthetic bot agent personas, their tweets and their interactions, thereby simulating social media networks. We compare the generated networks against empirical bot/human data, observing that both network and linguistic properties of LLM-Powered Bots differ from Wild Bots/Humans. This has implications towards the detection and effectiveness of LLM-Powered Bots.

**Link**: [arxiv](http://arxiv.org/abs/2508.00998v2),  [pdf](http://arxiv.org/pdf/2508.00998v2)

**Tags**: cs.SI cs.AI 



### LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due   Diligence
**Authors**: Alisa Vinogradova, Vlad Vinogradov, Dmitrii Radkevich, Ilya Yasny, Dmitry Kobyzev, Ivan Izmailov, Katsiaryna Yanchanka, Andrey Doronichev

**Updated**: 2025-08-22T17:50:00Z

**Summary**: In this paper, we describe and benchmark a competitor-discovery component used within an agentic AI system for fast drug asset due diligence. A competitor-discovery AI agent, given an indication, retrieves all drugs comprising the competitive landscape of that indication and extracts canonical attributes for these drugs. The competitor definition is investor-specific, and data is paywalled/licensed, fragmented across registries, ontology-mismatched by indication, alias-heavy for drug names, multimodal, and rapidly changing. Although considered the best tool for this problem, the current LLM-based AI systems aren't capable of reliably retrieving all competing drug names, and there is no accepted public benchmark for this task. To address the lack of evaluation, we use LLM-based agents to transform five years of multi-modal, unstructured diligence memos from a private biotech VC fund into a structured evaluation corpus mapping indications to competitor drugs with normalized attributes. We also introduce a competitor validating LLM-as-a-judge agent that filters out false positives from the list of predicted competitors to maximize precision and suppress hallucinations. On this benchmark, our competitor-discovery agent achieves 83% recall, exceeding OpenAI Deep Research (65%) and Perplexity Labs (60%). The system is deployed in production with enterprise users; in a case study with a biotech VC investment fund, analyst turnaround time dropped from 2.5 days to $\sim$3 hours ($\sim$20x) for the competitive analysis.

**Link**: [arxiv](http://arxiv.org/abs/2508.16571v1),  [pdf](http://arxiv.org/pdf/2508.16571v1)

**Tags**: cs.AI cs.IR cs.MA 



### Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse   Autoencoders
**Authors**: David Chanin, Adrià Garriga-Alonso

**Updated**: 2025-08-22T17:26:33Z

**Summary**: Sparse Autoencoders (SAEs) extract features from LLM internal activations, meant to correspond to single concepts. A core SAE training hyperparameter is L0: how many features should fire per token on average. Existing work compares SAE algorithms using sparsity--reconstruction tradeoff plots, implying L0 is a free parameter with no single correct value. In this work we study the effect of L0 on BatchTopK SAEs, and show that if L0 is not set precisely, the SAE fails to learn the underlying features of the LLM. If L0 is too low, the SAE will mix correlated features to improve reconstruction. If L0 is too high, the SAE finds degenerate solutions that also mix features. Further, we demonstrate a method to determine the correct L0 value for an SAE on a given training distribution, which finds the true L0 in toy models and coincides with peak sparse probing performance in LLMs. We find that most commonly used SAEs have an L0 that is too low. Our work shows that, to train SAEs with correct features, practitioners must set L0 correctly.

**Link**: [arxiv](http://arxiv.org/abs/2508.16560v1),  [pdf](http://arxiv.org/pdf/2508.16560v1)

**Tags**: cs.LG cs.AI cs.CL 



### Spherical latent space models for social network analysis
**Authors**: Juan Sosa, Carlos Nosa

**Updated**: 2025-08-22T17:23:25Z

**Summary**: This article introduces a spherical latent space model for social network analysis, embedding actors on a hypersphere rather than in Euclidean space as in standard latent space models. The spherical geometry facilitates the representation of transitive relationships and community structure, naturally captures cyclical patterns, and ensures bounded distances, thereby mitigating degeneracy issues common in traditional approaches. Bayesian inference is performed via Markov chain Monte Carlo methods to estimate both latent positions and other model parameters. The approach is demonstrated using two benchmark social network datasets, yielding improved model fit and interpretability relative to conventional latent space models.

**Link**: [arxiv](http://arxiv.org/abs/2508.16556v1),  [pdf](http://arxiv.org/pdf/2508.16556v1)

**Tags**: stat.ME stat.CO 



### TinyML Towards Industry 4.0: Resource-Efficient Process Monitoring of a   Milling Machine
**Authors**: Tim Langer, Matthias Widra, Volkhard Beyer

**Updated**: 2025-08-22T17:21:56Z

**Summary**: In the context of industry 4.0, long-serving industrial machines can be retrofitted with process monitoring capabilities for future use in a smart factory. One possible approach is the deployment of wireless monitoring systems, which can benefit substantially from the TinyML paradigm. This work presents a complete TinyML flow from dataset generation, to machine learning model development, up to implementation and evaluation of a full preprocessing and classification pipeline on a microcontroller. After a short review on TinyML in industrial process monitoring, the creation of the novel MillingVibes dataset is described. The feasibility of a TinyML system for structure-integrated process quality monitoring could be shown by the development of an 8-bit-quantized convolutional neural network (CNN) model with 12.59kiB parameter storage. A test accuracy of 100.0% could be reached at 15.4ms inference time and 1.462mJ per quantized CNN inference on an ARM Cortex M4F microcontroller, serving as a reference for future TinyML process monitoring solutions.

**Link**: [arxiv](http://arxiv.org/abs/2508.16553v1),  [pdf](http://arxiv.org/pdf/2508.16553v1)

**Tags**: cs.LG cs.CV cs.ET cs.SY eess.SP eess.SY I.2.1; I.5.4; C.5.3; C.3 



### Data Gluttony: Epistemic Risks, Dependent Testing and Data Reuse in   Large Datasets
**Authors**: Reid Dale, Jordan Rodu, Maria E. Currie, Mike Baiocchi

**Updated**: 2025-08-22T17:21:21Z

**Summary**: Large-scale registries have collected vast amounts of data which has enabled investigators to efficiently conduct studies of observational data. Common practice is for investigators to use all data meeting the inclusion criteria of their study to perform their analysis. We term this common practice data gluttony. It has apparent formal justification insofar as this approach maximizes per-study power. But this comes at a cost: data reuse affects the shape of the tail distribution of inferential errors. Using the theory of risk orderings we demonstrate how positively dependent testing procedures result in strictly riskier distributions of inferential error.   We identify two remedies to this state of affairs: research portfolio optimization and what we term data temperance. Research portfolio optimization requires that we formulate the enterprise of inference in a utility theoretic framework: associated to each hypothesis to be evaluated is some utility dependent on its truth as well as the impact of the statistical decision rendered on the basis of the data. Under certain models of data governance, this approach can be used to optimally allocate data usage across multiple inferential tasks. On the other hand, data temperance is a more flexible strategy for managing the distribution of inferential errors. Data temperance is the principle that an investigator use only as much data as is necessary to perform the task at hand. This is possible due to the diminishing marginal returns in power and precision in sample size. We analyze the effectiveness of data temperance at reducing the dependence across testing and develop a theory of the capacity of a static database to sustain large numbers of inferential tasks with low probability of inducing pairwise dependent testing procedures.

**Link**: [arxiv](http://arxiv.org/abs/2508.16552v1),  [pdf](http://arxiv.org/pdf/2508.16552v1)

**Tags**: math.ST stat.ME stat.TH 



### RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs.   Reinforcement Learning Fine-Tuning for LLMs
**Authors**: Hangzhan Jin, Sicheng Lv, Sifan Wu, Mohammad Hamdaqa

**Updated**: 2025-08-22T17:10:37Z

**Summary**: Training large language models (LLMs) from scratch is increasingly impractical, making post-training methods such as supervised fine-tuning (SFT) and reinforcement-learning fine-tuning (RL-FT, e.g., PPO) central to modern practice. Using an out-of-distribution (OOD) variant of the 24-point card game and new spectrum-based diagnostics, we revisit how these two stages reshape model representation and OOD performance. Our key findings are- (1) RL-FT can restore much of the OOD performance loss from SFT (e.g., Llama-11B 8.97% to 15.38%, Qwen-7B 17.09% to 19.66%). But when SFT induces severe overfitting and a clear distribution shift, RL-FT cannot fully recover OOD performance. (2) Direction shifts of singular vectors matter more than singular value magnitudes. These shifts concentrate on directions linked to the largest and smallest singular values, leaving the bulk spectrum intact. (3) Low-rank and shallow recovery is effective: restoring singular vector directions for the top 20% of values or first 25% of layers recovers 70-80% of OOD performance. (4) Stronger SFT checkpoints enable better recovery by RL, while overfitted ones resist restoration. These results reconcile prior reports of RL superior OOD performance: RL primarily counteracts SFT-induced directional drift rather than finding new solutions. Our spectrum-aware analysis highlights inexpensive recovery knobs low-rank UV merging and shallow-layer resets that practitioners can use before costly RL fine-tuning.

**Link**: [arxiv](http://arxiv.org/abs/2508.16546v1),  [pdf](http://arxiv.org/pdf/2508.16546v1)

**Tags**: cs.LG cs.AI 



### FACET: Teacher-Centred LLM-Based Multi-Agent Systems-Towards   Personalized Educational Worksheets
**Authors**: Jana Gonnermann-Müller, Jennifer Haase, Konstantin Fackeldey, Sebastian Pokutta

**Updated**: 2025-08-22T17:10:33Z

**Summary**: The increasing heterogeneity of student populations poses significant challenges for teachers, particularly in mathematics education, where cognitive, motivational, and emotional differences strongly influence learning outcomes. While AI-driven personalization tools have emerged, most remain performance-focused, offering limited support for teachers and neglecting broader pedagogical needs. This paper presents the FACET framework, a teacher-facing, large language model (LLM)-based multi-agent system designed to generate individualized classroom materials that integrate both cognitive and motivational dimensions of learner profiles. The framework comprises three specialized agents: (1) learner agents that simulate diverse profiles incorporating topic proficiency and intrinsic motivation, (2) a teacher agent that adapts instructional content according to didactical principles, and (3) an evaluator agent that provides automated quality assurance. We tested the system using authentic grade 8 mathematics curriculum content and evaluated its feasibility through a) automated agent-based assessment of output quality and b) exploratory feedback from K-12 in-service teachers. Results from ten internal evaluations highlighted high stability and alignment between generated materials and learner profiles, and teacher feedback particularly highlighted structure and suitability of tasks. The findings demonstrate the potential of multi-agent LLM architectures to provide scalable, context-aware personalization in heterogeneous classroom settings, and outline directions for extending the framework to richer learner profiles and real-world classroom trials.

**Link**: [arxiv](http://arxiv.org/abs/2508.11401v2),  [pdf](http://arxiv.org/pdf/2508.11401v2)

**Tags**: cs.HC cs.MA 



### Can Large Language Models Simulate Human Responses? A Case Study of   Stated Preference Experiments in the Context of Heating-related Choices
**Authors**: Han Wang, Jacek Pawlak, Aruna Sivakumar

**Updated**: 2025-08-22T17:01:56Z

**Summary**: Stated preference (SP) surveys are a key method to research how individuals make trade-offs in hypothetical, also futuristic, scenarios. In energy context this includes key decarbonisation enablement contexts, such as low-carbon technologies, distributed renewable energy generation, and demand-side response [1,2]. However, they tend to be costly, time-consuming, and can be affected by respondent fatigue and ethical constraints. Large language models (LLMs) have demonstrated remarkable capabilities in generating human-like textual responses, prompting growing interest in their application to survey research. This study investigates the use of LLMs to simulate consumer choices in energy-related SP surveys and explores their integration into data analysis workflows. A series of test scenarios were designed to systematically assess the simulation performance of several LLMs (LLaMA 3.1, Mistral, GPT-3.5 and DeepSeek-R1) at both individual and aggregated levels, considering contexts factors such as prompt design, in-context learning (ICL), chain-of-thought (CoT) reasoning, LLM types, integration with traditional choice models, and potential biases. Cloud-based LLMs do not consistently outperform smaller local models. In this study, the reasoning model DeepSeek-R1 achieves the highest average accuracy (77%) and outperforms non-reasoning LLMs in accuracy, factor identification, and choice distribution alignment. Across models, systematic biases are observed against the gas boiler and no-retrofit options, with a preference for more energy-efficient alternatives. The findings suggest that previous SP choices are the most effective input factor, while longer prompts with additional factors and varied formats can cause LLMs to lose focus, reducing accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2503.10652v3),  [pdf](http://arxiv.org/pdf/2503.10652v3)

**Tags**: cs.CL cs.AI cs.CY 



### A Simple "Try Again" Can Elicit Multi-Turn LLM Reasoning
**Authors**: Licheng Liu, Zihan Wang, Linjie Li, Chenwei Xu, Yiping Lu, Han Liu, Avirup Sil, Manling Li

**Updated**: 2025-08-22T16:49:10Z

**Summary**: Multi-turn problem solving is critical yet challenging for Large Reasoning Models (LRMs) to reflect on their reasoning and revise from feedback. Existing Reinforcement Learning (RL) methods train large reasoning models on a single-turn paradigm with verifiable rewards. However, we observe that models trained with existing RL paradigms often lose their ability to solve problems across multiple turns and struggle to revise answers based on contextual feedback, leading to repetitive responses. We ask: can LRMs learn to reflect their answers in a multi-turn context? In this work, we find that training models with multi-turn RL using only unary feedback (e.g., "Let's try again") after wrong answers can improve both single-turn performance and multi-turn reasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement learning, which uses minimal yet common unary user feedback during iterative problem solving. It can be easily applied to existing single-turn RL training setups. Experimental results show that RL training with UFO keeps single-turn performance and improves multi-turn reasoning accuracy by up to 14%, enabling language models to better react to feedback in multi-turn problem solving. To further minimize the number of turns needed for a correct answer while encouraging diverse reasoning when mistakes occur, we design reward structures that guide models to produce careful and deliberate answers in each turn. Code: https://github.com/lichengliu03/unary-feedback

**Link**: [arxiv](http://arxiv.org/abs/2507.14295v2),  [pdf](http://arxiv.org/pdf/2507.14295v2)

**Tags**: cs.LG cs.AI 



### ARSP: Automated Repair of Verilog Designs via Semantic Partitioning
**Authors**: Bingkun Yao, Ning Wang, Xiangfeng Liu, Yuxin Du, Yuchen Hu, Hong Gao, Zhe Jiang, Nan Guan

**Updated**: 2025-08-22T16:40:17Z

**Summary**: Debugging functional Verilog bugs consumes a significant portion of front-end design time. While Large Language Models (LLMs) have demonstrated great potential in mitigating this effort, existing LLM-based automated debugging methods underperform on industrial-scale modules. A major reason for this is bug signal dilution in long contexts, where a few bug-relevant tokens are overwhelmed by hundreds of unrelated lines, diffusing the model's attention. To address this issue, we introduce ARSP, a two-stage system that mitigates dilution via semantics-guided fragmentation. A Partition LLM splits a module into semantically tight fragments; a Repair LLM patches each fragment; edits are merged without altering unrelated logic. A synthetic data framework generates fragment-level training pairs spanning bug types, design styles, and scales to supervise both models. Experiments show that ARSP achieves 77.92% pass@1 and 83.88% pass@5, outperforming mainstream commercial LLMs including Claude-3.7 and SOTA automated Verilog debugging tools Strider and MEIC. Also, semantic partitioning improves pass@1 by 11.6% and pass@5 by 10.2% over whole-module debugging, validating the effectiveness of fragment-level scope reduction in LLM-based Verilog debugging.

**Link**: [arxiv](http://arxiv.org/abs/2508.16517v1),  [pdf](http://arxiv.org/pdf/2508.16517v1)

**Tags**: cs.SE cs.PL 



### FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the   Data Synthesis Pipeline
**Authors**: Parker Seegmiller, Kartik Mehta, Soumya Saha, Chenyang Tao, Shereen Oraby, Arpit Gupta, Tagyoung Chung, Mohit Bansal, Nanyun Peng

**Updated**: 2025-08-22T16:37:40Z

**Summary**: Recent works improving LLM math reasoning with synthetic data have used unique setups, making comparison of data synthesis strategies impractical. This leaves many unanswered questions about the roles of different factors in the synthetic data pipeline, such as the impact of filtering low-quality problems. To address this gap, we introduce FLAMES, a Framework for LLM Assessment of Math rEasoning Data Synthesis, and perform a systematic study of 10 existing data synthesis strategies and multiple other factors impacting the performance of synthetic math reasoning data. Our FLAMES experiments provide several valuable insights about the optimal balance of difficulty and diversity of synthetic data. First, data agents designed to increase problem complexity lead to best improvements on most math metrics. Second, with a fixed data generation budget, keeping higher problem coverage is more important than keeping only problems with reliable solutions. Third, GSM8K- and MATH-based synthetic data can lead to improvements on competition-level benchmarks, showcasing easy-to-hard generalization. Leveraging insights from our FLAMES experiments, we design two novel data synthesis strategies for improving out-of-domain generalization and robustness. Further, we develop the FLAMES dataset, an effective blend of our novel and existing data synthesis strategies, outperforming public datasets on OlympiadBench (+15.7), CollegeMath (+4.5), GSMPlus (+6.5), and MATH (+3.1). Fine-tuning Qwen2.5-Math-7B on the FLAMES dataset achieves 81.4% on MATH, surpassing larger Llama3 405B, GPT-4o and Claude 3.5 Sonnet.

**Link**: [arxiv](http://arxiv.org/abs/2508.16514v1),  [pdf](http://arxiv.org/pdf/2508.16514v1)

**Tags**: cs.LG cs.AI cs.CL 



### Extreme Lithium Depletion in Solar Twins: Challenging Non-Standard   Mixing Models
**Authors**: Isabelle Winnick, Jhon Yana Galarza, Henrique Reggiani, Thiago Ferreira, Isabelle Baraffe, Diego Lorenzo-Oliveira, Micaela Oyague, Rita Valle, Renzo Trujillo Diaz, Nathan Leigh, Matias Flores Trivigno, Ricardo Lopez-Valdivia, Gabriela Carvalho Silva, Eder Martioli, Helio Perottoni

**Updated**: 2025-08-22T16:36:52Z

**Summary**: Lithium (Li) is a powerful tracer of stellar mixing, gradually depleted in solar twins by non-standard transport below the convective zone. Here, we identify six new solar twins with exceptionally low Li levels that are not explained by current non-standard mixing models and, together with our previously reported anomalous solar twin HIP 8522, suggest a distinct population marked by a violent evolutionary past. Employing high-resolution spectra ($R=60,000 - 165,000$), we infer precise stellar parameters and chemical compositions, including Li abundances. We consider possible scenarios generating enhanced mixing, including planetary engulfment, blue straggler stars (BSSs), and early episodic accretion. Our planet engulfment simulations indicate that only one star may have engulfed an exoplanet, rapidly depleting Li via thermohaline convection. In the BSS scenario, radial velocity data rule out binary mass transfer, revealing no stellar companions but instead two new exoplanets. If these stars are field BSSs, a binary merger is likely though uncertain given that current BSS models focus mostly on stars in open clusters. Using pre-main-sequence episodic accretion models, we find that solar-mass stars can experience enhanced Li depletion without significant beryllium (Be) depletion. This is consistent with the Be abundances measured in two of our stars and represents the most plausible scenario, pending Be measurements for the remaining stars. These unique stars, together with HIP 8522, represent exceptional cases for testing stellar evolution models and probing internal mixing processes in Sun-like stars.

**Link**: [arxiv](http://arxiv.org/abs/2508.16513v1),  [pdf](http://arxiv.org/pdf/2508.16513v1)

**Tags**: astro-ph.SR astro-ph.EP 



### How Small is Enough? Empirical Evidence of Quantized Small Language   Models for Automated Program Repair
**Authors**: Kazuki Kusama, Honglin Shu, Masanari Kondo, Yasutaka Kamei

**Updated**: 2025-08-22T16:25:08Z

**Summary**: Background: Large language models (LLMs) have greatly improved the accuracy of automated program repair (APR) methods. However, LLMs are constrained by high computational resource requirements. Aims: We focus on small language models (SLMs), which perform well even with limited computational resources compared to LLMs. We aim to evaluate whether SLMs can achieve competitive performance in APR tasks. Method: We conducted experiments on the QuixBugs benchmark to compare the bug-fixing accuracy of SLMs and LLMs. We also analyzed the impact of int8 quantization on APR performance. Results: The latest SLMs can fix bugs as accurately as--or even more accurately than--LLMs. Also, int8 quantization had minimal effect on APR accuracy while significantly reducing memory requirements. Conclusions: SLMs present a viable alternative to LLMs for APR, offering competitive accuracy with lower computational costs, and quantization can further enhance their efficiency without compromising effectiveness.

**Link**: [arxiv](http://arxiv.org/abs/2508.16499v1),  [pdf](http://arxiv.org/pdf/2508.16499v1)

**Tags**: cs.SE 



### Not Only Consistency: Enhance Test-Time Adaptation with Spatio-temporal   Inconsistency for Remote Physiological Measurement
**Authors**: Xiao Yang, Jiyao Wang, Yuxuan Fan, Can Liu, Houcheng Su, Weichen Guo, Zitong Yu, Dengbo He, Kaishun Wu

**Updated**: 2025-08-22T16:20:32Z

**Summary**: Remote physiological measurement (RPM) has emerged as a promising non-invasive method for monitoring physiological signals using the non-contact device. Although various domain adaptation and generalization methods were proposed to promote the adaptability of deep-based RPM models in unseen deployment environments, considerations in aspects such as privacy concerns and real-time adaptation restrict their application in real-world deployment. Thus, we aim to propose a novel fully Test-Time Adaptation (TTA) strategy tailored for RPM tasks in this work. Specifically, based on prior knowledge in physiology and our observations, we noticed not only there is spatio-temporal consistency in the frequency domain of BVP signals, but also that inconsistency in the time domain was significant. Given this, by leveraging both consistency and inconsistency priors, we introduce an innovative expert knowledge-based self-supervised \textbf{C}onsistency-\textbf{i}n\textbf{C}onsistency-\textbf{i}ntegration (\textbf{CiCi}) framework to enhances model adaptation during inference. Besides, our approach further incorporates a gradient dynamic control mechanism to mitigate potential conflicts between priors, ensuring stable adaptation across instances. Through extensive experiments on five diverse datasets under the TTA protocol, our method consistently outperforms existing techniques, presenting state-of-the-art performance in real-time self-supervised adaptation without accessing source data. The code will be released later.

**Link**: [arxiv](http://arxiv.org/abs/2507.07908v2),  [pdf](http://arxiv.org/pdf/2507.07908v2)

**Tags**: cs.CV 



### Post Hoc Regression Refinement via Pairwise Rankings
**Authors**: Kevin Tirta Wijaya, Michael Sun, Minghao Guo, Hans-Peter Seidel, Wojciech Matusik, Vahid Babaei

**Updated**: 2025-08-22T16:17:31Z

**Summary**: Accurate prediction of continuous properties is essential to many scientific and engineering tasks. Although deep-learning regressors excel with abundant labels, their accuracy deteriorates in data-scarce regimes. We introduce RankRefine, a model-agnostic, plug-and-play post hoc method that refines regression with expert knowledge coming from pairwise rankings. Given a query item and a small reference set with known properties, RankRefine combines the base regressor's output with a rank-based estimate via inverse variance weighting, requiring no retraining. In molecular property prediction task, RankRefine achieves up to 10% relative reduction in mean absolute error using only 20 pairwise comparisons obtained through a general-purpose large language model (LLM) with no finetuning. As rankings provided by human experts or general-purpose LLMs are sufficient for improving regression across diverse domains, RankRefine offers practicality and broad applicability, especially in low-data settings.

**Link**: [arxiv](http://arxiv.org/abs/2508.16495v1),  [pdf](http://arxiv.org/pdf/2508.16495v1)

**Tags**: cs.LG cs.AI 



### Tracking flat bands via phonon-mediated interband scattering
**Authors**: Fabian Garmroudi, Xinlin Yan, Silke Paschen, Sean M. Thomas, Eric D. Bauer, Andrej Pustogow, Priscila F. S. Rosa

**Updated**: 2025-08-22T16:15:53Z

**Summary**: Flat-band (FB) materials have emerged as promising platforms for exploring exotic quantum phases. While numerous candidates have recently been identified through spectroscopic techniques such as angle-resolved photoemission spectroscopy, central challenges remain on how to tune FBs towards the Fermi level $E_F$ and to understand their impact on low-energy excitations probed in electronic transport experiments. Here, we show that, by attributing the temperature dependence of the electrical resistivity at elevated temperatures to electron-phonon interband scattering, one can infer the position of FBs near $E_F$ across diverse material classes. As charge carriers scatter off phonons, interband transitions into FB states lead to distinctive sub- or superlinear resistivity at elevated temperatures, governed by the proximity of the FB to $E_F$. Our phenomenological model captures these universal transport behaviors observed across several recently studied FB compounds and offers a simple, broadly applicable method for detecting flat bands.

**Link**: [arxiv](http://arxiv.org/abs/2508.16491v1),  [pdf](http://arxiv.org/pdf/2508.16491v1)

**Tags**: cond-mat.str-el cond-mat.mtrl-sci 



### AutoVerus: Automated Proof Generation for Rust Code
**Authors**: Chenyuan Yang, Xuheng Li, Md Rakib Hossain Misu, Jianan Yao, Weidong Cui, Yeyun Gong, Chris Hawblitzel, Shuvendu Lahiri, Jacob R. Lorch, Shuai Lu, Fan Yang, Ziqiao Zhou, Shan Lu

**Updated**: 2025-08-22T16:12:45Z

**Summary**: Generative AI has shown its values for many software engineering tasks. Still in its infancy, large language model (LLM)-based proof generation lags behind LLM-based code generation. In this paper, we present AutoVerus. AutoVerus uses LLMs to automatically generate correctness proof for Rust code. AutoVerus is designed to match the unique features of Verus, a verification tool that can prove the correctness of Rust code using proofs and specifications also written in Rust. AutoVerus consists of a network of LLM agents that are crafted and orchestrated to mimic human experts' three phases of proof construction: preliminary proof generation, proof refinement guided by generic tips, and proof debugging guided by verification errors. To thoroughly evaluate AutoVerus and help foster future research in this direction, we have built a benchmark suite of 150 non-trivial proof tasks, based on existing code-generation benchmarks and verification benchmarks. Our evaluation shows that AutoVerus can automatically generate correct proof for more than 90% of them, with more than half of them tackled in less than 30 seconds or 3 LLM calls.

**Link**: [arxiv](http://arxiv.org/abs/2409.13082v3),  [pdf](http://arxiv.org/pdf/2409.13082v3)

**Tags**: cs.SE cs.AI cs.FL 



### MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds
**Authors**: Bingquan Dai, Li Ray Luo, Qihong Tang, Jie Wang, Xinyu Lian, Hao Xu, Minghan Qin, Xudong Xu, Bo Dai, Haoqian Wang, Zhaoyang Lyu, Jiangmiao Pang

**Updated**: 2025-08-22T16:12:04Z

**Summary**: Reconstructing 3D objects into editable programs is pivotal for applications like reverse engineering and shape editing. However, existing methods often rely on limited domain-specific languages (DSLs) and small-scale datasets, restricting their ability to model complex geometries and structures. To address these challenges, we introduce MeshCoder, a novel framework that reconstructs complex 3D objects from point clouds into editable Blender Python scripts. We develop a comprehensive set of expressive Blender Python APIs capable of synthesizing intricate geometries. Leveraging these APIs, we construct a large-scale paired object-code dataset, where the code for each object is decomposed into distinct semantic parts. Subsequently, we train a multimodal large language model (LLM) that translates 3D point cloud into executable Blender Python scripts. Our approach not only achieves superior performance in shape-to-code reconstruction tasks but also facilitates intuitive geometric and topological editing through convenient code modifications. Furthermore, our code-based representation enhances the reasoning capabilities of LLMs in 3D shape understanding tasks. Together, these contributions establish MeshCoder as a powerful and flexible solution for programmatic 3D shape reconstruction and understanding. The project homepage is available at \href{https://daibingquan.github.io/MeshCoder}{this link}.

**Link**: [arxiv](http://arxiv.org/abs/2508.14879v2),  [pdf](http://arxiv.org/pdf/2508.14879v2)

**Tags**: cs.GR cs.CV 



### Enhancing Code-switched Text-to-Speech Synthesis Capability in Large   Language Models with only Monolingual Corpora
**Authors**: Jing Xu, Daxin Tan, Jiaqi Wang, Xiao Chen

**Updated**: 2025-08-22T16:11:58Z

**Summary**: While Large Language Models (LLMs) have shown potential in speech generation and recognition, their applications are mainly confined to monolingual scenarios, with limited explorations in code-switched (CS) contexts. In this paper, we propose a Code-Switched Large Language Model (CS-LLM) to enhance the code-switched text-to-speech synthesis (CS TTS) capability in LLMs with only monolingual corpora. Specifically, we begin by enhancing the multilingual speech processing ability of LLMs through multilingual speech recognition and synthesis tasks. Then, we develop an effective code-switched (CS) data construction strategy that splits and concatenates words from different monolingual speech corpora to equip LLMs with improved CS TTS ability. Experiments show that our approach outperforms baselines in CS TTS in terms of naturalness, speaker consistency and similarity even with limited data. Additionally, the constructed CS data further improves multilingual speech synthesis and recognition.

**Link**: [arxiv](http://arxiv.org/abs/2409.10969v2),  [pdf](http://arxiv.org/pdf/2409.10969v2)

**Tags**: eess.AS cs.CL cs.SD 



### Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy,   Expertise, and Reasoning
**Authors**: Chongyuan Dai, Jinpeng Hu, Hongchang Shi, Zhuo Li, Xun Yang, Meng Wang

**Updated**: 2025-08-22T16:11:37Z

**Summary**: Amidst a shortage of qualified mental health professionals, the integration of large language models (LLMs) into psychological applications offers a promising way to alleviate the growing burden of mental health disorders. Recent reasoning-augmented LLMs have achieved remarkable performance in mathematics and programming, while research in the psychological domain has predominantly emphasized emotional support and empathetic dialogue, with limited attention to reasoning mechanisms that are beneficial to generating reliable responses. Therefore, in this paper, we propose Psyche-R1, the first Chinese psychological LLM that jointly integrates empathy, psychological expertise, and reasoning, built upon a novel data curation pipeline. Specifically, we design a comprehensive data synthesis pipeline that produces over 75k high-quality psychological questions paired with detailed rationales, generated through chain-of-thought (CoT) reasoning and iterative prompt-rationale optimization, along with 73k empathetic dialogues. Subsequently, we employ a hybrid training strategy wherein challenging samples are identified through a multi-LLM cross-selection strategy for group relative policy optimization (GRPO) to improve reasoning ability, while the remaining data is used for supervised fine-tuning (SFT) to enhance empathetic response generation and psychological domain knowledge. Extensive experiment results demonstrate the effectiveness of the Psyche-R1 across several psychological benchmarks, where our 7B Psyche-R1 achieves comparable results to 671B DeepSeek-R1.

**Link**: [arxiv](http://arxiv.org/abs/2508.10848v2),  [pdf](http://arxiv.org/pdf/2508.10848v2)

**Tags**: cs.CL 



### Is Small Language Model the Silver Bullet to Low-Resource Languages   Machine Translation?
**Authors**: Yewei Song, Lujun Li, Cedric Lothritz, Saad Ezzini, Lama Sleem, Niccolo Gentile, Radu State, Tegawendé F. Bissyandé, Jacques Klein

**Updated**: 2025-08-22T15:58:31Z

**Summary**: Low-resource languages (LRLs) lack sufficient linguistic resources and are underrepresented in benchmark datasets, resulting in persistently lower translation quality than high-resource languages, especially in privacy-sensitive and resource-limited contexts. Firstly, this study systematically evaluates state-of-the-art smaller Large Language Models in 200 languages using the FLORES-200 benchmark, highlighting persistent deficiencies and disparities in the translation of LRLs. To mitigate these limitations, we investigate knowledge distillation from large pre-trained teacher models to Small Language Models (SLMs) through supervised fine-tuning. The results show substantial improvements; for example, the translation performance of English to Luxembourgish (EN to LB), measured by the LLM-as-a-Judge score, increases from 0.36 to 0.89 in the validation set for Llama-3.2-3B. We further investigate various fine-tuning configurations and tasks to clarify the trade-offs between data scale and training efficiency, verify that the model retains its general capabilities without significant catastrophic forgetting after training, and explore the distillation benefits to other LRLs on SLMs (Khasi, Assamese, and Ukrainian). In general, this work exposes the limitations and fairness issues of current SLMs in LRL translation and systematically explores the potential of using the distillation of knowledge from large to small models, offering practical, empirically grounded recommendations to improve LRL translation systems

**Link**: [arxiv](http://arxiv.org/abs/2503.24102v3),  [pdf](http://arxiv.org/pdf/2503.24102v3)

**Tags**: cs.CL 



### HAMSA: Hijacking Aligned Compact Models via Stealthy Automation
**Authors**: Alexey Krylov, Iskander Vagizov, Dmitrii Korzh, Maryam Douiba, Azidine Guezzaz, Vladimir Kokh, Sergey D. Erokhin, Elena V. Tutubalina, Oleg Y. Rogov

**Updated**: 2025-08-22T15:57:57Z

**Summary**: Large Language Models (LLMs), especially their compact efficiency-oriented variants, remain susceptible to jailbreak attacks that can elicit harmful outputs despite extensive alignment efforts. Existing adversarial prompt generation techniques often rely on manual engineering or rudimentary obfuscation, producing low-quality or incoherent text that is easily flagged by perplexity-based filters. We present an automated red-teaming framework that evolves semantically meaningful and stealthy jailbreak prompts for aligned compact LLMs. The approach employs a multi-stage evolutionary search, where candidate prompts are iteratively refined using a population-based strategy augmented with temperature-controlled variability to balance exploration and coherence preservation. This enables the systematic discovery of prompts capable of bypassing alignment safeguards while maintaining natural language fluency. We evaluate our method on benchmarks in English (In-The-Wild Jailbreak Prompts on LLMs), and a newly curated Arabic one derived from In-The-Wild Jailbreak Prompts on LLMs and annotated by native Arabic linguists, enabling multilingual assessment.

**Link**: [arxiv](http://arxiv.org/abs/2508.16484v1),  [pdf](http://arxiv.org/pdf/2508.16484v1)

**Tags**: cs.CL 



### A unified worst case for classical simplex and policy iteration pivot   rules
**Authors**: Yann Disser, Nils Mosis

**Updated**: 2025-08-22T15:53:30Z

**Summary**: We construct a family of Markov decision processes for which the policy iteration algorithm needs an exponential number of improving switches with Dantzig's rule, with Bland's rule, and with the Largest Increase pivot rule. This immediately translates to a family of linear programs for which the simplex algorithm needs an exponential number of pivot steps with the same three pivot rules. Our results yield a unified construction that simultaneously reproduces well-known lower bounds for these classical pivot rules, and we are able to infer that any (deterministic or randomized) combination of them cannot avoid an exponential worst-case behavior. Regarding the policy iteration algorithm, pivot rules typically switch multiple edges simultaneously and our lower bound for Dantzig's rule and the Largest Increase rule, which perform only single switches, seem novel. Regarding the simplex algorithm, the individual lower bounds were previously obtained separately via deformed hypercube constructions. In contrast to previous bounds for the simplex algorithm via Markov decision processes, our rigorous analysis is reasonably concise.

**Link**: [arxiv](http://arxiv.org/abs/2309.14034v2),  [pdf](http://arxiv.org/pdf/2309.14034v2)

**Tags**: cs.DM cs.CC cs.DS math.CO 90C05 (Primary) 90C40 (Secondary) F.2.2; G.1.6; G.3 



### Benchmarking the Robustness of Agentic Systems to Adversarially-Induced   Harms
**Authors**: Jonathan Nöther, Adish Singla, Goran Radanovic

**Updated**: 2025-08-22T15:53:22Z

**Summary**: Ensuring the safe use of agentic systems requires a thorough understanding of the range of malicious behaviors these systems may exhibit when under attack. In this paper, we evaluate the robustness of LLM-based agentic systems against attacks that aim to elicit harmful actions from agents. To this end, we propose a novel taxonomy of harms for agentic systems and a novel benchmark, BAD-ACTS, for studying the security of agentic systems with respect to a wide range of harmful actions. BAD-ACTS consists of 4 implementations of agentic systems in distinct application environments, as well as a dataset of 188 high-quality examples of harmful actions. This enables a comprehensive study of the robustness of agentic systems across a wide range of categories of harmful behaviors, available tools, and inter-agent communication structures. Using this benchmark, we analyze the robustness of agentic systems against an attacker that controls one of the agents in the system and aims to manipulate other agents to execute a harmful target action. Our results show that the attack has a high success rate, demonstrating that even a single adversarial agent within the system can have a significant impact on the security. This attack remains effective even when agents use a simple prompting-based defense strategy. However, we additionally propose a more effective defense based on message monitoring. We believe that this benchmark provides a diverse testbed for the security research of agentic systems. The benchmark can be found at github.com/JNoether/BAD-ACTS

**Link**: [arxiv](http://arxiv.org/abs/2508.16481v1),  [pdf](http://arxiv.org/pdf/2508.16481v1)

**Tags**: cs.LG 



### Disentangled Multi-modal Learning of Histology and Transcriptomics for   Cancer Characterization
**Authors**: Yupei Zhang, Xiaofei Wang, Anran Liu, Lequan Yu, Chao Li

**Updated**: 2025-08-22T15:51:33Z

**Summary**: Histopathology remains the gold standard for cancer diagnosis and prognosis. With the advent of transcriptome profiling, multi-modal learning combining transcriptomics with histology offers more comprehensive information. However, existing multi-modal approaches are challenged by intrinsic multi-modal heterogeneity, insufficient multi-scale integration, and reliance on paired data, restricting clinical applicability. To address these challenges, we propose a disentangled multi-modal framework with four contributions: 1) To mitigate multi-modal heterogeneity, we decompose WSIs and transcriptomes into tumor and microenvironment subspaces using a disentangled multi-modal fusion module, and introduce a confidence-guided gradient coordination strategy to balance subspace optimization. 2) To enhance multi-scale integration, we propose an inter-magnification gene-expression consistency strategy that aligns transcriptomic signals across WSI magnifications. 3) To reduce dependency on paired data, we propose a subspace knowledge distillation strategy enabling transcriptome-agnostic inference through a WSI-only student model. 4) To improve inference efficiency, we propose an informative token aggregation module that suppresses WSI redundancy while preserving subspace semantics. Extensive experiments on cancer diagnosis, prognosis, and survival prediction demonstrate our superiority over state-of-the-art methods across multiple settings. Code is available at https://github.com/helenypzhang/Disentangled-Multimodal-Learning.

**Link**: [arxiv](http://arxiv.org/abs/2508.16479v1),  [pdf](http://arxiv.org/pdf/2508.16479v1)

**Tags**: eess.IV cs.AI cs.CV 



### LLM-as-classifier: Semi-Supervised, Iterative Framework for Hierarchical   Text Classification using Large Language Models
**Authors**: Doohee You, Andy Parisi, Zach Vander Velden, Lara Dantas Inojosa

**Updated**: 2025-08-22T15:47:17Z

**Summary**: The advent of Large Language Models (LLMs) has provided unprecedented capabilities for analyzing unstructured text data. However, deploying these models as reliable, robust, and scalable classifiers in production environments presents significant methodological challenges. Standard fine-tuning approaches can be resource-intensive and often struggle with the dynamic nature of real-world data distributions, which is common in the industry. In this paper, we propose a comprehensive, semi-supervised framework that leverages the zero- and few-shot capabilities of LLMs for building hierarchical text classifiers as a framework for a solution to these industry-wide challenges. Our methodology emphasizes an iterative, human-in-the-loop process that begins with domain knowledge elicitation and progresses through prompt refinement, hierarchical expansion, and multi-faceted validation. We introduce techniques for assessing and mitigating sequence-based biases and outline a protocol for continuous monitoring and adaptation. This framework is designed to bridge the gap between the raw power of LLMs and the practical need for accurate, interpretable, and maintainable classification systems in industry applications.

**Link**: [arxiv](http://arxiv.org/abs/2508.16478v1),  [pdf](http://arxiv.org/pdf/2508.16478v1)

**Tags**: cs.CL cs.IR 



### Exploring natural variation in tendon constitutive parameters via   Bayesian data selection and mixed effects models
**Authors**: James Casey, Jessica Forsyth, Timothy Waite, Simon Cotter, Tom Shearer

**Updated**: 2025-08-22T15:39:27Z

**Summary**: Combining microstructural mechanical models with experimental data enhances our understanding of the mechanics of soft tissue, such as tendons. In previous work, a Bayesian framework was used to infer constitutive parameters from uniaxial stress-strain experiments on horse tendons, specifically the superficial digital flexor tendon (SDFT) and common digital extensor tendon (CDET), on a per-experiment basis. Here, we extend this analysis to investigate the natural variation of these parameters across a population of horses. Using a Bayesian mixed effects model, we infer population distributions of these parameters. Given that the chosen hyperelastic model does not account for tendon damage, careful data selection is necessary. Avoiding ad hoc methods, we introduce a hierarchical Bayesian data selection method. This two-stage approach selects data per experiment, and integrates data weightings into the Bayesian mixed effects model. Our results indicate that the CDET is stiffer than the SDFT, likely due to a higher collagen volume fraction. The modes of the parameter distributions yield estimates of the product of the collagen volume fraction and Young's modulus as 811.5 MPa for the SDFT and 1430.2 MPa for the CDET. This suggests that positional tendons have stiffer collagen fibrils and/or higher collagen volume density than energy-storing tendons.

**Link**: [arxiv](http://arxiv.org/abs/2412.12983v2),  [pdf](http://arxiv.org/pdf/2412.12983v2)

**Tags**: stat.AP 



### Your Reward Function for RL is Your Best PRM for Search: Unifying RL and   Search-Based TTS
**Authors**: Can Jin, Yang Zhou, Qixin Zhang, Hongwu Peng, Di Zhang, Marco Pavone, Ligong Han, Zhang-Wei Hong, Tong Che, Dimitris N. Metaxas

**Updated**: 2025-08-22T15:37:12Z

**Summary**: Test-time scaling (TTS) for large language models (LLMs) has thus far fallen into two largely separate paradigms: (1) reinforcement learning (RL) methods that optimize sparse outcome-based rewards, yet suffer from instability and low sample efficiency; and (2) search-based techniques guided by independently trained, static process reward models (PRMs), which require expensive human- or LLM-generated labels and often degrade under distribution shifts. In this paper, we introduce AIRL-S, the first natural unification of RL-based and search-based TTS. Central to AIRL-S is the insight that the reward function learned during RL training inherently represents the ideal PRM for guiding downstream search. Specifically, we leverage adversarial inverse reinforcement learning (AIRL) combined with group relative policy optimization (GRPO) to learn a dense, dynamic PRM directly from correct reasoning traces, entirely eliminating the need for labeled intermediate process data. At inference, the resulting PRM simultaneously serves as the critic for RL rollouts and as a heuristic to effectively guide search procedures, facilitating robust reasoning chain extension, mitigating reward hacking, and enhancing cross-task generalization. Experimental results across eight benchmarks, including mathematics, scientific reasoning, and code generation, demonstrate that our unified approach improves performance by 9 % on average over the base model, matching GPT-4o. Furthermore, when integrated into multiple search algorithms, our PRM consistently outperforms all baseline PRMs trained with labeled data. These results underscore that, indeed, your reward function for RL is your best PRM for search, providing a robust and cost-effective solution to complex reasoning tasks in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2508.14313v2),  [pdf](http://arxiv.org/pdf/2508.14313v2)

**Tags**: cs.LG cs.AI 



### MCP-Guard: A Defense Framework for Model Context Protocol Integrity in   Large Language Model Applications
**Authors**: Wenpeng Xing, Zhonghao Qi, Yupeng Qin, Yilin Li, Caini Chang, Jiahui Yu, Changting Lin, Zhenzhen Xie, Meng Han

**Updated**: 2025-08-22T15:35:04Z

**Summary**: The integration of Large Language Models (LLMs) with external tools via protocols such as the Model Context Protocol (MCP) introduces critical security vulnerabilities, including prompt injection, data exfiltration, and other threats. To counter these challenges, we propose MCP-Guard, a robust, layered defense architecture designed for LLM--tool interactions. MCP-Guard employs a three-stage detection pipeline that balances efficiency with accuracy: it progresses from lightweight static scanning for overt threats and a deep neural detector for semantic attacks, to our fine-tuned E5-based model achieves (96.01) accuracy in identifying adversarial prompts. Finally, a lightweight LLM arbitrator synthesizes these signals to deliver the final decision while minimizing false positives. To facilitate rigorous training and evaluation, we also introduce MCP-AttackBench, a comprehensive benchmark of over 70,000 samples. Sourced from public datasets and augmented by GPT-4, MCP-AttackBench simulates diverse, real-world attack vectors in the MCP format, providing a foundation for future research into securing LLM-tool ecosystems.

**Link**: [arxiv](http://arxiv.org/abs/2508.10991v2),  [pdf](http://arxiv.org/pdf/2508.10991v2)

**Tags**: cs.CR cs.AI 



### What makes an entity salient in discourse?
**Authors**: Amir Zeldes, Jessica Lin

**Updated**: 2025-08-22T15:30:40Z

**Summary**: Entities in discourse vary broadly in salience: main participants, objects and locations are noticeable and memorable, while tangential ones are less important and quickly forgotten, raising questions about how humans signal and infer relative salience. Using a graded operationalization of salience based on summary-worthiness in multiple summaries of a discourse, this paper explores data from 24 spoken and written genres of English to extract a multifactorial complex of overt and implicit linguistic cues, such as recurring subjecthood or definiteness, discourse relations and hierarchy across utterances, as well as pragmatic functional inferences based on genre and communicative intent. Tackling the question 'how is the degree of salience expressed for each and every entity mentioned?' our results show that while previous approaches to salience all correlate with our salience scores to some extent, no single generalization is without exceptions, and the phenomenon cuts across all levels of linguistic representation.

**Link**: [arxiv](http://arxiv.org/abs/2508.16464v1),  [pdf](http://arxiv.org/pdf/2508.16464v1)

**Tags**: cs.CL 



### Modular Embedding Recomposition for Incremental Learning
**Authors**: Aniello Panariello, Emanuele Frascaroli, Pietro Buzzega, Lorenzo Bonicelli, Angelo Porrello, Simone Calderara

**Updated**: 2025-08-22T15:25:40Z

**Summary**: The advent of pre-trained Vision-Language Models (VLMs) has significantly transformed Continual Learning (CL), mainly due to their zero-shot classification abilities. Such proficiency makes VLMs well-suited for real-world applications, enabling robust performance on novel unseen classes without requiring adaptation. However, fine-tuning remains essential when downstream tasks deviate significantly from the pre-training domain. Prior CL approaches primarily focus on preserving the zero-shot capabilities of VLMs during incremental fine-tuning on a downstream task. We take a step further by devising an approach that transforms preservation into enhancement of the zero-shot capabilities of VLMs. Our approach, named MoDular Embedding Recomposition (MoDER), introduces a modular framework that trains multiple textual experts, each specialized in a single seen class, and stores them in a foundational hub. At inference time, for each unseen class, we query the hub and compose the retrieved experts to synthesize a refined prototype that improves classification. We show the effectiveness of our method across two popular zero-shot incremental protocols, Class-IL and MTIL, comprising a total of 14 datasets. The codebase is available at https://github.com/aimagelab/mammoth.

**Link**: [arxiv](http://arxiv.org/abs/2508.16463v1),  [pdf](http://arxiv.org/pdf/2508.16463v1)

**Tags**: cs.AI cs.CV 



### GPL-SLAM: A Laser SLAM Framework with Gaussian Process Based Extended   Landmarks
**Authors**: Ali Emre Balcı, Erhan Ege Keyvan, Emre Özkan

**Updated**: 2025-08-22T15:20:43Z

**Summary**: We present a novel Simultaneous Localization and Mapping (SLAM) method that employs Gaussian Process (GP) based landmark (object) representations. Instead of conventional grid maps or point cloud registration, we model the environment on a per object basis using GP based contour representations. These contours are updated online through a recursive scheme, enabling efficient memory usage. The SLAM problem is formulated within a fully Bayesian framework, allowing joint inference over the robot pose and object based map. This representation provides semantic information such as the number of objects and their areas, while also supporting probabilistic measurement to object associations. Furthermore, the GP based contours yield confidence bounds on object shapes, offering valuable information for downstream tasks like safe navigation and exploration. We validate our method on synthetic and real world experiments, and show that it delivers accurate localization and mapping performance across diverse structured environments.

**Link**: [arxiv](http://arxiv.org/abs/2508.16459v1),  [pdf](http://arxiv.org/pdf/2508.16459v1)

**Tags**: cs.RO 



### A Probabilistic Inference Scaling Theory for LLM Self-Correction
**Authors**: Zhe Yang, Yichang Zhang, Yudong Wang, Ziyao Xu, Junyang Lin, Zhifang Sui

**Updated**: 2025-08-22T15:15:38Z

**Summary**: Large Language Models (LLMs) have demonstrated the capability to refine their generated answers through self-correction, enabling continuous performance improvement over multiple rounds. However, the mechanisms underlying how and why accuracy evolves during this iterative process remain unexplored. To fill this gap, we propose a probabilistic theory to model the dynamics of accuracy change and explain the performance improvements observed in multi-round self-correction. Through mathematical derivation, we establish that the accuracy after the $t^{th}$ round of self-correction is given by: $Acc_t = Upp - \alpha^t(Upp - Acc_0),$ where $Acc_0$ denotes the initial accuracy, $Upp$ represents the upper bound of accuracy convergence, and $\alpha$ determines the rate of convergence. Based on our theory, these parameters can be calculated and the predicted accuracy curve then can be obtained through only a single round of self-correction. Extensive experiments across diverse models and datasets demonstrate that our theoretical predictions align closely with empirical accuracy curves, validating the effectiveness of the theory. Our work provides a theoretical foundation for understanding LLM self-correction, thus paving the way for further explorations.

**Link**: [arxiv](http://arxiv.org/abs/2508.16456v1),  [pdf](http://arxiv.org/pdf/2508.16456v1)

**Tags**: cs.CL 



### GreenLLM: SLO-Aware Dynamic Frequency Scaling for Energy-Efficient LLM   Serving
**Authors**: Qunyou Liu, Darong Huang, Marina Zapater, David Atienza

**Updated**: 2025-08-22T15:08:34Z

**Summary**: Large Language Models (LLMs) are becoming the backbone of modern cloud services, yet their inference costs are dominated by GPU energy. Unlike traditional GPU workloads, LLM inference has two stages with different characteristics: the prefill phase, which is latency sensitive and scales quadratically with prompt length, and the decode phase, which progresses token by token with unpredictable length. Current GPU power governors (for example, NVIDIA's default) overlook this asymmetry and treat both stages uniformly. The result is mismatched voltage and frequency settings, head-of-line blocking, and excessive energy use.   We introduce GreenLLM, an SLO-aware serving framework that minimizes GPU energy by explicitly separating prefill and decode control. At ingress, requests are routed into length-based queues so short prompts avoid head-of-line blocking and TTFT improves. For prefill, GreenLLM collects short traces on a GPU node, fits compact latency-power models over SM frequency, and solves a queueing-aware optimization to select energy-minimal clocks per class. During decode, a lightweight dual-loop controller tracks throughput (tokens per second) and adjusts frequency with hysteretic, fine-grained steps to hold tail TBT within target bounds. Across Alibaba and Azure trace replays, GreenLLM reduces total energy by up to 34 percent versus the default DVFS baseline, with no loss of throughput and with less than 3.5 percent additional SLO violations.

**Link**: [arxiv](http://arxiv.org/abs/2508.16449v1),  [pdf](http://arxiv.org/pdf/2508.16449v1)

**Tags**: cs.PF 



### Boardwalk: Towards a Framework for Creating Board Games with LLMs
**Authors**: Álvaro Guglielmin Becker, Gabriel Bauer de Oliveira, Lana Bertoldo Rossato, Anderson Rocha Tavares

**Updated**: 2025-08-22T15:02:07Z

**Summary**: Implementing board games in code can be a time-consuming task. However, Large Language Models (LLMs) have been proven effective at generating code for domain-specific tasks with simple contextual information. We aim to investigate whether LLMs can implement digital versions of board games from rules described in natural language. This would be a step towards an LLM-assisted framework for quick board game code generation. We expect to determine the main challenges for LLMs to implement the board games, and how different approaches and models compare to one another. We task three state-of-the-art LLMs (Claude, DeepSeek and ChatGPT) with coding a selection of 12 popular and obscure games in free-form and within Boardwalk, our proposed General Game Playing API. We anonymize the games and components to avoid evoking pre-trained LLM knowledge. The implementations are tested for playability and rule compliance. We evaluate success rate and common errors across LLMs and game popularity. Our approach proves viable, with the best performing model, Claude 3.7 Sonnet, yielding 55.6\% of games without any errors. While compliance with the API increases error frequency, the severity of errors is more significantly dependent on the LLM. We outline future steps for creating a framework to integrate this process, making the elaboration of board games more accessible.

**Link**: [arxiv](http://arxiv.org/abs/2508.16447v1),  [pdf](http://arxiv.org/pdf/2508.16447v1)

**Tags**: cs.LG 



### Structured Prompts, Better Outcomes? Exploring the Effects of a   Structured Interface with ChatGPT in a Graduate Robotics Course
**Authors**: Jerome Brender, Laila El-Hamamsy, Kim Uittenhove, Francesco Mondada, Engin Bumbacher

**Updated**: 2025-08-22T15:01:14Z

**Summary**: Prior research shows that how students engage with Large Language Models (LLMs) influences their problem-solving and understanding, reinforcing the need to support productive LLM-uses that promote learning. This study evaluates the impact of a structured GPT platform designed to promote 'good' prompting behavior with data from 58 students in a graduate-level robotics course. The students were assigned to either an intervention group using the structured platform or a control group using ChatGPT freely for two practice lab sessions, before a third session where all students could freely use ChatGPT. We analyzed student perception (pre-post surveys), prompting behavior (logs), performance (task scores), and learning (pre-post tests). Although we found no differences in performance or learning between groups, we identified prompting behaviors - such as having clear prompts focused on understanding code - that were linked with higher learning gains and were more prominent when students used the structured platform. However, such behaviors did not transfer once students were no longer constrained to use the structured platform. Qualitative survey data showed mixed perceptions: some students perceived the value of the structured platform, but most did not perceive its relevance and resisted changing their habits. These findings contribute to ongoing efforts to identify effective strategies for integrating LLMs into learning and question the effectiveness of bottom-up approaches that temporarily alter user interfaces to influence students' interaction. Future research could instead explore top-down strategies that address students' motivations and explicitly demonstrate how certain interaction patterns support learning.

**Link**: [arxiv](http://arxiv.org/abs/2507.07767v2),  [pdf](http://arxiv.org/pdf/2507.07767v2)

**Tags**: cs.CY 



### Scalable Bayesian inference on high-dimensional multivariate linear   regression
**Authors**: Xuan Cao, Kyoungjae Lee

**Updated**: 2025-08-22T15:01:03Z

**Summary**: We consider jointly estimating the coefficient matrix and the error precision matrix in high-dimensional multivariate linear regression models. Bayesian methods in this context often face computational challenges, leading to previous approaches that either utilize a generalized likelihood without ensuring the positive definiteness of the precision matrix or rely on maximization algorithms targeting only the posterior mode, thus failing to address uncertainty. In this work, we propose two Bayesian methods: an exact method and an approximate two-step method. We first propose an exact method based on spike and slab priors for the coefficient matrix and DAG-Wishart prior for the error precision matrix, whose computational complexity is comparable to the state-of-the-art generalized likelihood-based Bayesian method. To further enhance scalability, a two-step approach is developed by ignoring the dependency structure among response variables. This method estimates the coefficient matrix first, followed by the calculation of the posterior of the error precision matrix based on the estimated errors. We validate the two-step method by demonstrating (i) selection consistency and posterior convergence rates for the coefficient matrix and (ii) selection consistency for the directed acyclic graph (DAG) of errors. We demonstrate the practical performance of proposed methods through synthetic and real data analysis.

**Link**: [arxiv](http://arxiv.org/abs/2508.16446v1),  [pdf](http://arxiv.org/pdf/2508.16446v1)

**Tags**: stat.ME 



### Using LLMs and Essence to Support Software Practice Adoption
**Authors**: Sonia Nicoletti, Paolo Ciancarini

**Updated**: 2025-08-22T14:59:35Z

**Summary**: Recent advancements in natural language processing (NLP) have enabled the development of automated tools that support various domains, including software engineering. However, while NLP and artificial intelligence (AI) research has extensively focused on tasks such as code generation, less attention has been given to automating support for the adoption of best practices, the evolution of ways of working, and the monitoring of process health. This study addresses this gap by exploring the integration of Essence, a standard and thinking framework for managing software engineering practices, with large language models (LLMs). To this end, a specialised chatbot was developed to assist students and professionals in understanding and applying Essence. The chatbot employs a retrieval-augmented generation (RAG) system to retrieve relevant contextual information from a curated knowledge base. Four different LLMs were used to create multiple chatbot configurations, each evaluated both as a base model and augmented with the RAG system. The system performance was evaluated through both the relevance of retrieved context and the quality of generated responses. Comparative analysis against the general-purpose LLMs demonstrated that the proposed system consistently outperforms its baseline counterpart in domain-specific tasks. By facilitating access to structured software engineering knowledge, this work contributes to bridging the gap between theoretical frameworks and practical application, potentially improving process management and the adoption of software development practices. While further validation through user studies is required, these findings highlight the potential of LLM-based automation to enhance learning and decision-making in software engineering.

**Link**: [arxiv](http://arxiv.org/abs/2508.16445v1),  [pdf](http://arxiv.org/pdf/2508.16445v1)

**Tags**: cs.SE 



### Classification or Prompting: A Case Study on Legal Requirements   Traceability
**Authors**: Romina Etezadi, Sallam Abualhaija, Chetan Arora, Lionel Briand

**Updated**: 2025-08-22T14:52:02Z

**Summary**: New regulations are introduced to ensure software development aligns with ethical concerns and protects public safety. Showing compliance requires tracing requirements to legal provisions. Requirements traceability is a key task where engineers must analyze technical requirements against target artifacts, often within limited time. Manually analyzing complex systems with hundreds of requirements is infeasible. The legal dimension adds challenges that increase effort. In this paper, we investigate two automated solutions based on language models, including large ones (LLMs). The first solution, Kashif, is a classifier that leverages sentence transformers and semantic similarity. The second solution, RICE_LRT, prompts a recent generative LLM based on RICE, a prompt engineering framework. On a benchmark dataset, we empirically evaluate Kashif and compare it against five different baseline classifiers from the literature. Kashif can identify trace links with a recall of 67%, precision of 50%, and F2 score of 63%, outperforming the best baseline by a substantial margin of 41 percentage points (pp) in F2. However, on unseen, more complex requirements documents traced to the European General Data Protection Regulation (GDPR), Kashif performs poorly, yielding an average recall of 15%, an average precision of 10%, and an average F2 score of 13.5%. On the same documents, however, our RICE solution yields an average recall of 84%, an average precision of 30%, and an average F2 score of 61%. RICE achieved a remarkable improvement of 47.5 pp over Kashif in terms of F2 score. Our results suggest that requirements traceability in the legal context cannot be simply addressed by building classifiers, as such solutions do not generalize and fail to perform well on complex regulations and requirements. Resorting to generative LLMs, with careful prompt engineering, is thus a more promising alternative.

**Link**: [arxiv](http://arxiv.org/abs/2502.04916v4),  [pdf](http://arxiv.org/pdf/2502.04916v4)

**Tags**: cs.SE 



### PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark
**Authors**: Adil Bahaj, Mounir Ghogho

**Updated**: 2025-08-22T14:50:55Z

**Summary**: Large language models (LLMs) and vision-augmented LLMs (VLMs) have significantly advanced medical informatics, diagnostics, and decision support. However, these models exhibit systematic biases, particularly age bias, compromising their reliability and equity. This is evident in their poorer performance on pediatric-focused text and visual question-answering tasks. This bias reflects a broader imbalance in medical research, where pediatric studies receive less funding and representation despite the significant disease burden in children. To address these issues, a new comprehensive multi-modal pediatric question-answering benchmark, PediatricsMQA, has been introduced. It consists of 3,417 text-based multiple-choice questions (MCQs) covering 131 pediatric topics across seven developmental stages (prenatal to adolescent) and 2,067 vision-based MCQs using 634 pediatric images from 67 imaging modalities and 256 anatomical regions. The dataset was developed using a hybrid manual-automatic pipeline, incorporating peer-reviewed pediatric literature, validated question banks, existing benchmarks, and existing QA resources. Evaluating state-of-the-art open models, we find dramatic performance drops in younger cohorts, highlighting the need for age-aware methods to ensure equitable AI support in pediatric care.

**Link**: [arxiv](http://arxiv.org/abs/2508.16439v1),  [pdf](http://arxiv.org/pdf/2508.16439v1)

**Tags**: cs.CY cs.AI cs.CL cs.GR cs.MM 



### OPERA: A Reinforcement Learning--Enhanced Orchestrated Planner-Executor   Architecture for Reasoning-Oriented Multi-Hop Retrieval
**Authors**: Yu Liu, Yanbing Liu, Fangfang Yuan, Cong Cao, Youbang Sun, Kun Peng, WeiZhuo Chen, Jianjun Li, Zhiyuan Ma

**Updated**: 2025-08-22T14:50:26Z

**Summary**: Recent advances in large language models (LLMs) and dense retrievers have driven significant progress in retrieval-augmented generation (RAG). However, existing approaches face significant challenges in complex reasoning-oriented multi-hop retrieval tasks: 1) Ineffective reasoning-oriented planning: Prior methods struggle to generate robust multi-step plans for complex queries, as rule-based decomposers perform poorly on out-of-template questions. 2) Suboptimal reasoning-driven retrieval: Related methods employ limited query reformulation, leading to iterative retrieval loops that often fail to locate golden documents. 3) Insufficient reasoning-guided filtering: Prevailing methods lack the fine-grained reasoning to effectively filter salient information from noisy results, hindering utilization of retrieved knowledge. Fundamentally, these limitations all stem from the weak coupling between retrieval and reasoning in current RAG architectures. We introduce the Orchestrated Planner-Executor Reasoning Architecture (OPERA), a novel reasoning-driven retrieval framework. OPERA's Goal Planning Module (GPM) decomposes questions into sub-goals, which are executed by a Reason-Execute Module (REM) with specialized components for precise reasoning and effective retrieval. To train OPERA, we propose Multi-Agents Progressive Group Relative Policy Optimization (MAPGRPO), a novel variant of GRPO. Experiments on complex multi-hop benchmarks show OPERA's superior performance, validating both the MAPGRPO method and OPERA's design. Code is available at https://github.com/Ameame1/OPERA.

**Link**: [arxiv](http://arxiv.org/abs/2508.16438v1),  [pdf](http://arxiv.org/pdf/2508.16438v1)

**Tags**: cs.IR cs.AI 



### Classical post-processing approach for quantum amplitude estimation
**Authors**: Yongdan Yang, Ruyu Yang

**Updated**: 2025-08-22T14:49:24Z

**Summary**: We propose an approach for quantum amplitude estimation (QAE) designed to enhance computational efficiency while minimizing the reliance on quantum resources. Our method leverages quantum computers to generate a sequence of signals, from which the quantum amplitude is inferred through classical post-processing techniques. Unlike traditional methods that use quantum phase estimation (QPE), which requires numerous controlled unitary operations and the quantum Fourier transform, our method avoids these complex and resource-demanding steps. By integrating quantum computing with classical post-processing techniques, our method significantly reduces the need for quantum gates and qubits, thus optimizing the utilization of quantum hardware. We present numerical simulations to validate the effectiveness of our method and provide a comprehensive analysis of its computational complexity and error. This hybrid strategy not only improves the practicality of QAE but also broadens its applicability in quantum computing.

**Link**: [arxiv](http://arxiv.org/abs/2502.05617v2),  [pdf](http://arxiv.org/pdf/2502.05617v2)

**Tags**: quant-ph 



### Coarse-to-Fine Process Reward Modeling for Mathematical Reasoning
**Authors**: Yulan Hu, Sheng Ouyang, Jinman Zhao, Yong Liu

**Updated**: 2025-08-22T14:48:22Z

**Summary**: The Process Reward Model (PRM) plays a crucial role in mathematical reasoning tasks, requiring high-quality supervised process data. However, we observe that reasoning steps generated by Large Language Models (LLMs) often fail to exhibit strictly incremental information, leading to redundancy that can hinder effective reasoning. To address this issue, we propose CFPRM, a simple yet effective coarse-to-fine strategy. Instead of focusing on the detection of redundant steps, our approach first establishes a coarse-grained window to merge adjacent reasoning steps into unified, holistic steps. The window size is then progressively reduced to extract fine-grained reasoning steps, enabling data collection at multiple granularities for training. By leveraging this hierarchical refinement process, CFPRM mitigates redundancy while preserving essential fine-grained knowledge. Extensive experiments on two reasoning datasets across three loss criteria validate the CFPRM's effectiveness and versatility.

**Link**: [arxiv](http://arxiv.org/abs/2501.13622v4),  [pdf](http://arxiv.org/pdf/2501.13622v4)

**Tags**: cs.AI 



### Cetvel: A Unified Benchmark for Evaluating Language Understanding,   Generation and Cultural Capacity of LLMs for Turkish
**Authors**: Yakup Abrek Er, Ilker Kesen, Gözde Gül Şahin, Aykut Erdem

**Updated**: 2025-08-22T14:42:50Z

**Summary**: We introduce Cetvel, a comprehensive benchmark designed to evaluate large language models (LLMs) in Turkish. Existing Turkish benchmarks often lack either task diversity or culturally relevant content, or both. Cetvel addresses these gaps by combining a broad range of both discriminative and generative tasks ensuring content that reflects the linguistic and cultural richness of Turkish language. Cetvel covers 23 tasks grouped into seven categories, including tasks such as grammatical error correction, machine translation, and question answering rooted in Turkish history and idiomatic language. We evaluate 33 open-weight LLMs (up to 70B parameters) covering different model families and instruction paradigms. Our experiments reveal that Turkish-centric instruction-tuned models generally underperform relative to multilingual or general-purpose models (e.g. Llama 3 and Mistral), despite being tailored for the language. Moreover, we show that tasks such as grammatical error correction and extractive question answering are particularly discriminative in differentiating model capabilities. Cetvel offers a comprehensive and culturally grounded evaluation suite for advancing the development and assessment of LLMs in Turkish.

**Link**: [arxiv](http://arxiv.org/abs/2508.16431v1),  [pdf](http://arxiv.org/pdf/2508.16431v1)

**Tags**: cs.CL cs.AI I.2.7 



### Geodesic slice sampling on Riemannian manifolds
**Authors**: Alain Durmus, Samuel Gruffaz, Mareike Hasenpflug, Daniel Rudolf

**Updated**: 2025-08-22T14:31:34Z

**Summary**: We propose a theoretically justified and practically applicable slice sampling based Markov chain Monte Carlo (MCMC) method for approximate sampling from probability measures on Riemannian manifolds. The latter naturally arise as posterior distributions in Bayesian inference of matrix-valued parameters, for example belonging to either the Stiefel or the Grassmann manifold. Our method, called geodesic slice sampling, is reversible with respect to the distribution of interest, and generalizes Hit-and-run slice sampling on $\mathbb{R}^{d}$ to Riemannian manifolds by using geodesics instead of straight lines. We demonstrate the robustness of our sampler's performance compared to other MCMC methods dealing with manifold valued distributions through extensive numerical experiments, on both synthetic and real data. In particular, we illustrate its remarkable ability to cope with anisotropic target densities, without using gradient information and preconditioning.

**Link**: [arxiv](http://arxiv.org/abs/2312.00417v3),  [pdf](http://arxiv.org/pdf/2312.00417v3)

**Tags**: stat.CO math.PR stat.AP 60-08 G.3 



### Terrestrial Exoplanet Internal Structure Constraints Enabled by   Comprehensive Host Star Characterization Reveal that Terrestrial Planets in   Mean-motion Resonances are Water Rich
**Authors**: Alejandra Ross, Henrique Reggiani, Kevin C. Schlaufman, Mykhaylo Plotnykov, Diana Valencia

**Updated**: 2025-08-22T14:31:23Z

**Summary**: Exoplanet mass and radius inferences fundamentally rely on host star mass and radius inferences. Despite the importance of host star mass, radius, and elemental abundance inferences for the derivation of exoplanet internal structure constraints, published constraints have often been based on inferences that are not self-consistent. For 24 dwarf stars hosting terrestrial exoplanets, we use astrometric and photometric data plus high-resolution spectroscopy to infer accurate, precise, homogeneous, and physically self-consistent photospheric and fundamental stellar parameters as well as elemental abundances. We infer updated planetary masses and radii using these data plus Doppler and transit observables, then use the complete data set to derive constraints on the core-mass fractions of these terrestrial exoplanets. We find that the population of resonant or likely formerly resonant terrestrial exoplanets represented by Kepler-36 b and Kepler-105 c has a significantly lower mean core-mass fraction than the rest of the terrestrial exoplanets in our sample. Their resonant configurations suggest that they migrated inwards from more distant formation locations, and we attribute their low densities to the incorporation and retention of significant amounts of water during their formation. We confirm that the ultra-short-period exoplanets 55 Cnc e and WASP-47 e have densities inconsistent with pure-rock compositions. We propose that they are both the stripped cores of mini-Neptunes and associate their low densities with the presence of significant amounts of hydrogen, helium, water, and/or other volatiles in their interiors. We verify that our results are independent of stellar parameter and elemental abundance inference approach and therefore robust.

**Link**: [arxiv](http://arxiv.org/abs/2508.16421v1),  [pdf](http://arxiv.org/pdf/2508.16421v1)

**Tags**: astro-ph.EP astro-ph.SR 



### LLM-GUARD: Large Language Model-Based Detection and Repair of Bugs and   Security Vulnerabilities in C++ and Python
**Authors**: Akshay Mhatre, Noujoud Nader, Patrick Diehl, Deepti Gupta

**Updated**: 2025-08-22T14:30:24Z

**Summary**: Large Language Models (LLMs) such as ChatGPT-4, Claude 3, and LLaMA 4 are increasingly embedded in software/application development, supporting tasks from code generation to debugging. Yet, their real-world effectiveness in detecting diverse software bugs, particularly complex, security-relevant vulnerabilities, remains underexplored. This study presents a systematic, empirical evaluation of these three leading LLMs using a benchmark of foundational programming errors, classic security flaws, and advanced, production-grade bugs in C++ and Python. The dataset integrates real code from SEED Labs, OpenSSL (via the Suresoft GLaDOS database), and PyBugHive, validated through local compilation and testing pipelines. A novel multi-stage, context-aware prompting protocol simulates realistic debugging scenarios, while a graded rubric measures detection accuracy, reasoning depth, and remediation quality. Our results show that all models excel at identifying syntactic and semantic issues in well-scoped code, making them promising for educational use and as first-pass reviewers in automated code auditing. Performance diminishes in scenarios involving complex security vulnerabilities and large-scale production code, with ChatGPT-4 and Claude 3 generally providing more nuanced contextual analyses than LLaMA 4. This highlights both the promise and the present constraints of LLMs in serving as reliable code analysis tools.

**Link**: [arxiv](http://arxiv.org/abs/2508.16419v1),  [pdf](http://arxiv.org/pdf/2508.16419v1)

**Tags**: cs.SE cs.LG 



### Towards Bridging the Reward-Generation Gap in Direct Alignment   Algorithms
**Authors**: Zeguan Xiao, Yun Chen, Guanhua Chen, Ke Tang

**Updated**: 2025-08-22T14:16:43Z

**Summary**: Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization (DPO) and Simple Preference Optimization (SimPO), have emerged as efficient alternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms for aligning large language models (LLMs) with human preferences. However, DAAs suffer from a fundamental limitation we identify as the "reward-generation gap" -- a misalignment between optimization objectives during training and actual generation performance during inference. In this paper, we find a contributor to the reward-generation gap is the mismatch between the inherent importance of prefix tokens during the LLM generation process and how this importance is reflected in the implicit reward functions of DAAs. To bridge the gap, we adopt a token-level MDP perspective of DAAs to analyze its limitations and introduce a simple yet effective approach called Prefix-Oriented Equal-length Training (POET), which truncates both preferred and dispreferred responses to match the shorter one's length. Training with \mname, where both responses in each sample are truncated to equal length, resulting in diverse truncated lengths across samples, the optimization of DAAs objective is implicitly constrained to converge across all timesteps of token-level MDP, thus paying more attention to prefix tokens than the standard DAAs. We conduct experiments with DPO and SimPO, two representative DAAs, demonstrating that POET improves over their standard implementations, achieving up to 15.6 points in AlpacaEval 2 and overall improvements across downstream tasks. Our results highlight the importance of addressing the misalignment between reward optimization and generation performance in DAAs.

**Link**: [arxiv](http://arxiv.org/abs/2506.09457v2),  [pdf](http://arxiv.org/pdf/2506.09457v2)

**Tags**: cs.CL cs.LG 



### Retrieval-Augmented Defense: Adaptive and Controllable Jailbreak   Prevention for Large Language Models
**Authors**: Guangyu Yang, Jinghong Chen, Jingbiao Mei, Weizhe Lin, Bill Byrne

**Updated**: 2025-08-22T14:13:16Z

**Summary**: Large Language Models (LLMs) remain vulnerable to jailbreak attacks, which attempt to elicit harmful responses from LLMs. The evolving nature and diversity of these attacks pose many challenges for defense systems, including (1) adaptation to counter emerging attack strategies without costly retraining, and (2) control of the trade-off between safety and utility. To address these challenges, we propose Retrieval-Augmented Defense (RAD), a novel framework for jailbreak detection that incorporates a database of known attack examples into Retrieval-Augmented Generation, which is used to infer the underlying, malicious user query and jailbreak strategy used to attack the system. RAD enables training-free updates for newly discovered jailbreak strategies and provides a mechanism to balance safety and utility. Experiments on StrongREJECT show that RAD substantially reduces the effectiveness of strong jailbreak attacks such as PAP and PAIR while maintaining low rejection rates for benign queries. We propose a novel evaluation scheme and show that RAD achieves a robust safety-utility trade-off across a range of operating points in a controllable manner.

**Link**: [arxiv](http://arxiv.org/abs/2508.16406v1),  [pdf](http://arxiv.org/pdf/2508.16406v1)

**Tags**: cs.CR cs.CL 



### MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal   Pairs
**Authors**: Jaap Jumelet, Leonie Weissweiler, Joakim Nivre, Arianna Bisazza

**Updated**: 2025-08-22T14:08:58Z

**Summary**: We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic minimal pairs, covering 101 languages and 2 types of subject-verb agreement, containing more than 128,000 minimal pairs. Our minimal pairs are created using a fully automated pipeline, leveraging the large-scale linguistic resources of Universal Dependencies and UniMorph. MultiBLiMP 1.0 evaluates abilities of LLMs at an unprecedented multilingual scale, and highlights the shortcomings of the current state-of-the-art in modelling low-resource languages.

**Link**: [arxiv](http://arxiv.org/abs/2504.02768v3),  [pdf](http://arxiv.org/pdf/2504.02768v3)

**Tags**: cs.CL 



### AetherCode: Evaluating LLMs' Ability to Win In Premier Programming   Competitions
**Authors**: Zihan Wang, Jiaze Chen, Zhicheng Liu, Markus Mak, Yidi Du, Geonsik Moon, Luoqi Xu, Aaron Tua, Kunshuo Peng, Jiayi Lu, Mingfei Xia, Boqian Zou, Chenyang Ran, Guang Tian, Shoutai Zhu, Yeheng Duan, Zhenghui Kang, Zhenxing Lin, Shangshu Li, Qiang Luo, Qingshen Long, Zhiyong Chen, Yihan Xiao, Yurong Wu, Daoguang Zan, Yuyi Fu, Mingxuan Wang, Ming Ding

**Updated**: 2025-08-22T14:04:55Z

**Summary**: Competitive programming has emerged as a critical benchmark for evaluating the reasoning and coding capabilities of Large Language Models (LLMs). Despite impressive progress on existing benchmarks, we argue that current evaluations overstate model proficiency, masking a substantial gap between LLMs and elite human programmers. This gap arises from two key limitations: insufficient difficulty and scope of benchmark problems, and evaluation bias from low-quality test cases. To address these shortcomings, we present AetherCode, a new benchmark that draws problems from premier programming competitions such as IOI and ICPC, offering broader coverage and higher difficulty. AetherCode further incorporates comprehensive, expert-validated test suites built through a hybrid of automated generation and human curation, ensuring rigorous and reliable assessment. By combining challenging problem design with robust evaluation, AetherCode provides a more faithful measure of LLM capabilities and sets a new standard for future research in code reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2508.16402v1),  [pdf](http://arxiv.org/pdf/2508.16402v1)

**Tags**: cs.SE cs.CL 



### Constraints on the extreme mass-ratio inspiral population from LISA data
**Authors**: Shashwat Singh, Christian E. A. Chapman-Bird, Christopher P L Berry, John Veitch

**Updated**: 2025-08-22T14:00:58Z

**Summary**: Gravitational waves from extreme mass-ratio inspirals (EMRIs), the inspirals of stellar-mass compact objects into massive black holes, are predicted to be observed by the Laser Interferometer Space Antenna (LISA). A sufficiently large number of EMRI observations will provide unique insights into the massive black hole population. We have developed a hierarchical Bayesian inference framework capable of constraining the parameters of the EMRI population, accounting for selection biases. We leverage the capacity of a feed-forward neural network as an emulator, enabling detectability calculations of $\sim10^5$ EMRIs in a fraction of a second, speeding up the likelihood evaluation by $\gtrsim6$ orders of magnitude. We validate our framework on a phenomenological EMRI population model. This framework enables studies of how well we can constrain EMRI population parameters, such as the slope of both the massive and stellar-mass black hole mass spectra and the branching fractions of different formation channels, allowing further investigation into the evolution of massive black holes.

**Link**: [arxiv](http://arxiv.org/abs/2508.16399v1),  [pdf](http://arxiv.org/pdf/2508.16399v1)

**Tags**: gr-qc astro-ph.HE 



### A Lightweight Group Multiscale Bidirectional Interactive Network for   Real-Time Steel Surface Defect Detection
**Authors**: Yong Zhang, Cunjian Chen, Qiang Gao, Yi Wang, Bin Fang

**Updated**: 2025-08-22T13:58:35Z

**Summary**: Real-time surface defect detection is critical for maintaining product quality and production efficiency in the steel manufacturing industry. Despite promising accuracy, existing deep learning methods often suffer from high computational complexity and slow inference speeds, which limit their deployment in resource-constrained industrial environments. Recent lightweight approaches adopt multibranch architectures based on depthwise separable convolution (DSConv) to capture multiscale contextual information. However, these methods often suffer from increased computational overhead and lack effective cross-scale feature interaction, limiting their ability to fully leverage multiscale representations. To address these challenges, we propose GMBINet, a lightweight framework that enhances multiscale feature extraction and interaction through novel Group Multiscale Bidirectional Interactive (GMBI) modules. The GMBI adopts a group-wise strategy for multiscale feature extraction, ensuring scale-agnostic computational complexity. It further integrates a Bidirectional Progressive Feature Interactor (BPFI) and a parameter-free Element-Wise Multiplication-Summation (EWMS) operation to enhance cross-scale interaction without introducing additional computational overhead. Experiments on SD-Saliency-900 and NRSD-MN datasets demonstrate that GMBINet delivers competitive accuracy with real-time speeds of 1048 FPS on GPU and 16.53 FPS on CPU at 512 resolution, using only 0.19 M parameters. Additional evaluations on the NEU-CLS defect classification dataset further confirm the strong generalization ability of our method, demonstrating its potential for broader industrial vision applications beyond surface defect detection. The dataset and code are publicly available at: https://github.com/zhangyongcode/GMBINet.

**Link**: [arxiv](http://arxiv.org/abs/2508.16397v1),  [pdf](http://arxiv.org/pdf/2508.16397v1)

**Tags**: cs.CV cs.AI 



### Collaborative Stance Detection via Small-Large Language Model   Consistency Verification
**Authors**: Yu Yan, Sheng Sun, Zixiang Tang, Teli Liu, Min Liu

**Updated**: 2025-08-22T13:55:07Z

**Summary**: Stance detection on social media aims to identify attitudes expressed in tweets towards specific targets. Current studies prioritize Large Language Models (LLMs) over Small Language Models (SLMs) due to the overwhelming performance improving provided by LLMs. However, heavily relying on LLMs for stance detection, regardless of the cost, is impractical for real-world social media monitoring systems that require vast data analysis. To this end, we propose \textbf{\underline{Co}}llaborative Stance Detection via Small-Large Language Model Consistency \textbf{\underline{Ver}}ification (\textbf{CoVer}) framework, which enhances LLM utilization via context-shared batch reasoning and logical verification between LLM and SLM. Specifically, instead of processing each text individually, CoVer processes texts batch-by-batch, obtaining stance predictions and corresponding explanations via LLM reasoning in a shared context. Then, to exclude the bias caused by context noises, CoVer introduces the SLM for logical consistency verification. Finally, texts that repeatedly exhibit low logical consistency are classified using consistency-weighted aggregation of prior LLM stance predictions. Our experiments show that CoVer outperforms state-of-the-art methods across multiple benchmarks in the zero-shot setting, achieving 0.54 LLM queries per tweet while significantly enhancing performance. Our CoVer offers a more practical solution for LLM deploying for social media stance detection.

**Link**: [arxiv](http://arxiv.org/abs/2502.19954v2),  [pdf](http://arxiv.org/pdf/2502.19954v2)

**Tags**: cs.CL cs.AI 



### What do we mean by stellar mass? The impact of the pre-main sequence on   the mass to light ratio of young and intermediate age stellar populations
**Authors**: Elizabeth R. Stanway, Conor M. Byrne, Ankur Upadhyaya

**Updated**: 2025-08-22T13:54:01Z

**Summary**: Stellar population synthesis models are an essential tool with which galaxy physical parameters are extracted from observations. However they are built on assumptions designed for use in the local Universe, and not always appropriate to high redshift galaxies. Here we consider the impact of including the hitherto-neglected stellar pre-main sequence delay timescale on the interpretation of composite stellar populations at ages of <1 Gyr. We find that doing so has an impact on the optical luminosity of very young stellar populations of up to ~10 per cent, although smaller changes in observed light (<5 per cent) are expected in most use cases. However the impact on the inferred stellar mass and mass-to-light ratios is significant (a factor of 2 or more), depending on how those properties are defined. We find that the short time scales for star formation in the distant Universe require a clearer definition for the stellar mass in a population, and will impact assumptions about the inferred shape of the stellar initial mass function from observations.

**Link**: [arxiv](http://arxiv.org/abs/2508.16394v1),  [pdf](http://arxiv.org/pdf/2508.16394v1)

**Tags**: astro-ph.GA astro-ph.CO astro-ph.SR 



### RoMedQA: The First Benchmark for Romanian Medical Question Answering
**Authors**: Ana-Cristina Rogoz, Radu Tudor Ionescu, Alexandra-Valentina Anghel, Ionut-Lucian Antone-Iordache, Simona Coniac, Andreea Iuliana Ionescu

**Updated**: 2025-08-22T13:48:37Z

**Summary**: Question answering (QA) is an actively studied topic, being a core natural language processing (NLP) task that needs to be addressed before achieving Artificial General Intelligence (AGI). However, the lack of QA datasets in specific domains and languages hinders the development of robust AI models able to generalize across various domains and languages. To this end, we introduce RoMedQA, the first Romanian QA benchmark for the medical domain, alongside a comprehensive evaluation of state-of-the-art large language models (LLMs). We construct a high-quality and large-scale dataset comprising 102,646 QA pairs related to cancer patients. The questions regard medical case summaries of 1,011 patients, requiring either keyword extraction or reasoning to be answered correctly. RoMedQA is the result of a time-consuming manual annotation process carried out by seven physicians specialized in oncology or radiotherapy, who spent a total of about 2,100 work hours to generate the QA pairs. We experiment with four LLMs from distinct families of models on RoMedQA. Each model is employed in two scenarios, namely one based on zero-shot prompting and one based on supervised fine-tuning. Our results show that fine-tuned models significantly outperform their zero-shot counterparts, clearly indicating that pretrained models fail to generalize on RoMedQA. Our findings demonstrate the importance of both domain-specific and language-specific fine-tuning for reliable clinical QA in Romanian. We publicly release our dataset and code at https://github.com/ana-rogoz/RoMedQA.

**Link**: [arxiv](http://arxiv.org/abs/2508.16390v1),  [pdf](http://arxiv.org/pdf/2508.16390v1)

**Tags**: cs.CL cs.AI cs.LG 



### Rotary Offset Features in Large Language Models
**Authors**: André Jonasson

**Updated**: 2025-08-22T13:41:57Z

**Summary**: Transformer-based Large Language Models (LLMs) rely on positional encodings to provide sequence position information to their attention mechanism. Rotary Positional Encodings (RoPE), which encode relative position by rotating queries and keys, have become widely used in modern LLMs. We study the features and patterns that emerge in queries and keys when using rotary embeddings and introduce the concept of rotary offset features. Our analysis reveals that these features, which frequently exhibit large activations and are often interpreted as outliers, arise consistently across layers, attention heads, and model architectures. We derive bounds predicting which rotary frequencies give rise to rotary offset features and the minimum angle between the query-key pairs for these features. We verify our predictions empirically across models of different sizes and architectures.

**Link**: [arxiv](http://arxiv.org/abs/2503.01832v2),  [pdf](http://arxiv.org/pdf/2503.01832v2)

**Tags**: cs.CL cs.LG 



### Bayesian Multi-wavelength Imaging of the LMC SN1987A with SRG/eROSITA
**Authors**: Vincent Eberle, Matteo Guardiani, Margret Westerkamp, Philipp Frank, Michael Freyberg, Mara Salvato, Torsten Enßlin

**Updated**: 2025-08-22T13:39:45Z

**Summary**: The eROSITA Early Data Release (EDR) and eROSITA All-Sky Survey (eRASS1) data have already revealed a remarkable number of undiscovered X-ray sources. Using Bayesian inference and generative modeling techniques for X-ray imaging, we aim to increase the sensitivity and scientific value of these observations by denoising, deconvolving, and decomposing the X-ray sky. Leveraging information field theory, we can exploit the spatial and spectral correlation structures of the different physical components of the sky with non-parametric priors to enhance the image reconstruction. By incorporating instrumental effects into the forward model, we develop a comprehensive Bayesian imaging algorithm for eROSITA pointing observations. Finally, we apply the developed algorithm to EDR data of the Large Magellanic Cloud (LMC) SN1987A, fusing data sets from observations made by five different telescope modules. The final result is a denoised, deconvolved, and decomposed view of the LMC, which enables the analysis of its fine-scale structures, the identification of point sources in this region, and enhanced calibration for future work.

**Link**: [arxiv](http://arxiv.org/abs/2410.14599v2),  [pdf](http://arxiv.org/pdf/2410.14599v2)

**Tags**: astro-ph.IM astro-ph.HE cs.IT math.IT physics.data-an 



### GLARE: Agentic Reasoning for Legal Judgment Prediction
**Authors**: Xinyu Yang, Chenlong Deng, Zhicheng Dou

**Updated**: 2025-08-22T13:38:12Z

**Summary**: Legal judgment prediction (LJP) has become increasingly important in the legal field. In this paper, we identify that existing large language models (LLMs) have significant problems of insufficient reasoning due to a lack of legal knowledge. Therefore, we introduce GLARE, an agentic legal reasoning framework that dynamically acquires key legal knowledge by invoking different modules, thereby improving the breadth and depth of reasoning. Experiments conducted on the real-world dataset verify the effectiveness of our method. Furthermore, the reasoning chain generated during the analysis process can increase interpretability and provide the possibility for practical applications.

**Link**: [arxiv](http://arxiv.org/abs/2508.16383v1),  [pdf](http://arxiv.org/pdf/2508.16383v1)

**Tags**: cs.AI cs.CL cs.CY 



### Agentic AI Empowered Multi-UAV Trajectory Optimization in Low-Altitude   Economy Networks
**Authors**: Feibo Jiang, Li Dong, Xitao Pan, Kezhi Wang, Cunhua Pan

**Updated**: 2025-08-22T13:34:49Z

**Summary**: This paper proposes a novel Agentic Retrieval-augmented generation with Mamba-Attention Integrated Transformer (ARMAIT) framework for multi-Unmanned Aerial Vehicle (UAV) trajectory optimization. The framework is built upon Large Language Models (LLMs), incorporating Retrieval-Augmented Generation (RAG) empowered by Agentic AI and integrated with a UAV-specific knowledge base. Through the Agentic RAG, the LLM autonomously interprets high-level task requirements and identifies the key components necessary for trajectory optimization, including model inputs and outputs, network architecture, reward functions, and task constraints. To support efficient modeling across different system scales, we introduce the Mamba-Attention Integrated Transformer (MAIT), a hybrid neural architecture that combines the long-range dependency modeling capability of attention mechanisms with the efficient temporal dynamic representation of Mamba. Furthermore, a Trajectory-Group Relative Policy Optimization (T-GRPO) method is proposed to achieve unified policy gradient optimization in both discrete and continuous trajectory spaces for MAIT training. Extensive experimental results validate the feasibility and effectiveness of the proposed ARMAIT framework.

**Link**: [arxiv](http://arxiv.org/abs/2508.16379v1),  [pdf](http://arxiv.org/pdf/2508.16379v1)

**Tags**: cs.IT eess.SP math.IT 



### The Mediomatix Corpus: Parallel Data for Romansh Idioms via Comparable   Schoolbooks
**Authors**: Zachary Hopton, Jannis Vamvas, Andrin Büchler, Anna Rutkiewicz, Rico Cathomas, Rico Sennrich

**Updated**: 2025-08-22T13:25:00Z

**Summary**: The five idioms (i.e., varieties) of the Romansh language are largely standardized and are taught in the schools of the respective communities in Switzerland. In this paper, we present the first parallel corpus of Romansh idioms. The corpus is based on 291 schoolbook volumes, which are comparable in content for the five idioms. We use automatic alignment methods to extract 207k multi-parallel segments from the books, with more than 2M tokens in total. A small-scale human evaluation confirms that the segments are highly parallel, making the dataset suitable for NLP applications such as machine translation between Romansh idioms. We release the parallel and unaligned versions of the dataset under a CC-BY-NC-SA license and demonstrate its utility for machine translation by training and evaluating an LLM on a sample of the dataset.

**Link**: [arxiv](http://arxiv.org/abs/2508.16371v1),  [pdf](http://arxiv.org/pdf/2508.16371v1)

**Tags**: cs.CL 



### Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort   for Retrieval and RAG
**Authors**: Hengran Zhang, Minghao Tang, Keping Bi, Jiafeng Guo, Shihao Liu, Daiting Shi, Dawei Yin, Xueqi Cheng

**Updated**: 2025-08-22T13:24:48Z

**Summary**: Retrieval models typically rely on costly human-labeled query-document relevance annotations for training and evaluation. To reduce this cost and leverage the potential of Large Language Models (LLMs) in relevance judgments, we aim to explore whether LLM-generated annotations can effectively replace human annotations in training retrieval models. Retrieval usually emphasizes relevance, which indicates "topic-relatedness" of a document to a query, while in RAG, the value of a document (or utility) depends on how it contributes to answer generation. Recognizing this mismatch, some researchers use LLM performance on downstream tasks with documents as labels, but this approach requires manual answers for specific tasks, leading to high costs and limited generalization. In another line of work, prompting LLMs to select useful documents as RAG references eliminates the need for human annotation and is not task-specific. If we leverage LLMs' utility judgments to annotate retrieval data, we may retain cross-task generalization without human annotation in large-scale corpora. Therefore, we investigate utility-focused annotation via LLMs for large-scale retriever training data across both in-domain and out-of-domain settings on the retrieval and RAG tasks. To reduce the impact of low-quality positives labeled by LLMs, we design a novel loss function, i.e., Disj-InfoNCE. Our experiments reveal that: (1) Retrievers trained on utility-focused annotations significantly outperform those trained on human annotations in the out-of-domain setting on both tasks, demonstrating superior generalization capabilities. (2) LLM annotation does not replace human annotation in the in-domain setting. However, incorporating just 20% human-annotated data enables retrievers trained with utility-focused annotations to match the performance of models trained entirely with human annotations.

**Link**: [arxiv](http://arxiv.org/abs/2504.05220v3),  [pdf](http://arxiv.org/pdf/2504.05220v3)

**Tags**: cs.IR cs.AI cs.CL 



### From chambers to echo chambers: Quantifying polarization with a   second-neighbor approach applied to Twitter's climate discussion
**Authors**: Blas Kolic, Fabián Aguirre-López, Sergio Hernández-Williams, Guillermo Garduño-Hernández

**Updated**: 2025-08-22T13:16:50Z

**Summary**: Social media platforms often foster environments where users primarily engage with content that aligns with their existing beliefs, thereby reinforcing their views and limiting exposure to opposing viewpoints. In this paper, we analyze X (formerly Twitter) discussions on climate change throughout 2019, using an unsupervised method centered on chambers--second-order information sources--to uncover ideological patterns at scale. Beyond direct connections, chambers capture shared sources of influence, revealing polarization dynamics efficiently and effectively. Analyzing retweet patterns, we identify echo chambers of climate believers and skeptics, revealing strong chamber overlap within ideological groups and minimal overlap between them, resulting in a robust bimodal structure that characterizes polarization. Our method enables us to infer the stance of high-impact users based on their audience's chamber alignment, allowing for the classification of over half the retweeting population with minimal cross-group interaction, in what we term augmented echo chamber classification. We benchmark our approach against manual labeling and a state-of-the-art latent ideology model, finding comparable performance but with nearly four times greater coverage. Moreover, we find that echo chamber structures remain stable over time, even as their members change significantly, suggesting that these structures are a persistent and emergent property of the system. Notably, polarization decreases and climate skepticism rises during the #FridaysForFuture strikes in September 2019. This chamber-based analysis offers valuable insights into the persistence and fluidity of ideological polarization on social media.

**Link**: [arxiv](http://arxiv.org/abs/2206.14501v3),  [pdf](http://arxiv.org/pdf/2206.14501v3)

**Tags**: cs.SI 62P25, 91C20, 91D30 J.4 



### Sentiment Reasoning for Healthcare
**Authors**: Khai-Nguyen Nguyen, Khai Le-Duc, Bach Phan Tat, Duy Le, Long Vo-Dang, Truong-Son Hy

**Updated**: 2025-08-22T13:16:29Z

**Summary**: Transparency in AI healthcare decision-making is crucial. By incorporating rationales to explain reason for each predicted label, users could understand Large Language Models (LLMs)'s reasoning to make better decision. In this work, we introduce a new task - Sentiment Reasoning - for both speech and text modalities, and our proposed multimodal multitask framework and the world's largest multimodal sentiment analysis dataset. Sentiment Reasoning is an auxiliary task in sentiment analysis where the model predicts both the sentiment label and generates the rationale behind it based on the input transcript. Our study conducted on both human transcripts and Automatic Speech Recognition (ASR) transcripts shows that Sentiment Reasoning helps improve model transparency by providing rationale for model prediction with quality semantically comparable to humans while also improving model's classification performance (+2% increase in both accuracy and macro-F1) via rationale-augmented fine-tuning. Also, no significant difference in the semantic quality of generated rationales between human and ASR transcripts. All code, data (five languages - Vietnamese, English, Chinese, German, and French) and models are published online: https://github.com/leduckhai/Sentiment-Reasoning

**Link**: [arxiv](http://arxiv.org/abs/2407.21054v5),  [pdf](http://arxiv.org/pdf/2407.21054v5)

**Tags**: cs.CL cs.AI cs.LG cs.SD eess.AS 



### MizanQA: Benchmarking Large Language Models on Moroccan Legal Question   Answering
**Authors**: Adil Bahaj, Mounir Ghogho

**Updated**: 2025-08-22T13:04:43Z

**Summary**: The rapid advancement of large language models (LLMs) has significantly propelled progress in natural language processing (NLP). However, their effectiveness in specialized, low-resource domains-such as Arabic legal contexts-remains limited. This paper introduces MizanQA (pronounced Mizan, meaning "scale" in Arabic, a universal symbol of justice), a benchmark designed to evaluate LLMs on Moroccan legal question answering (QA) tasks, characterised by rich linguistic and legal complexity. The dataset draws on Modern Standard Arabic, Islamic Maliki jurisprudence, Moroccan customary law, and French legal influences. Comprising over 1,700 multiple-choice questions, including multi-answer formats, MizanQA captures the nuances of authentic legal reasoning. Benchmarking experiments with multilingual and Arabic-focused LLMs reveal substantial performance gaps, highlighting the need for tailored evaluation metrics and culturally grounded, domain-specific LLM development.

**Link**: [arxiv](http://arxiv.org/abs/2508.16357v1),  [pdf](http://arxiv.org/pdf/2508.16357v1)

**Tags**: cs.CL cs.AI cs.IR 



### NovoMolGen: Rethinking Molecular Language Model Pretraining
**Authors**: Kamran Chitsaz, Roshan Balaji, Quentin Fournier, Nirav Pravinbhai Bhatt, Sarath Chandar

**Updated**: 2025-08-22T13:02:19Z

**Summary**: Designing de-novo molecules with desired property profiles requires efficient exploration of the vast chemical space ranging from $10^{23}$ to $10^{60}$ possible synthesizable candidates. While various deep generative models have been developed to design small molecules using diverse input representations, Molecular Large Language Models (Mol-LLMs) based on string representations have emerged as a scalable approach capable of exploring billions of molecules. However, there remains limited understanding regarding how standard language modeling practices such as textual representations, tokenization strategies, model size, and dataset scale impact molecular generation performance. In this work, we systematically investigate these critical aspects by introducing NovoMolGen, a family of transformer-based foundation models pretrained on 1.5 billion molecules for de-novo molecule generation. Through extensive empirical analyses, we identify a weak correlation between performance metrics measured during pretraining and actual downstream performance, revealing important distinctions between molecular and general NLP training dynamics. NovoMolGen establishes new state-of-the-art results, substantially outperforming prior Mol-LLMs and specialized generative models in both unconstrained and goal-directed molecular generation tasks, thus providing a robust foundation for advancing efficient and effective molecular modeling strategies.

**Link**: [arxiv](http://arxiv.org/abs/2508.13408v2),  [pdf](http://arxiv.org/pdf/2508.13408v2)

**Tags**: cs.LG 



### PublicHearingBR: A Brazilian Portuguese Dataset of Public Hearing   Transcripts for Summarization of Long Documents
**Authors**: Leandro Carísio Fernandes, Guilherme Zeferino Rodrigues Dobins, Roberto Lotufo, Jayr Alencar Pereira

**Updated**: 2025-08-22T13:02:16Z

**Summary**: This paper introduces PublicHearingBR, a Brazilian Portuguese dataset designed for summarizing long documents. The dataset consists of transcripts of public hearings held by the Brazilian Chamber of Deputies, paired with news articles and structured summaries containing the individuals participating in the hearing and their statements or opinions. The dataset supports the development and evaluation of long document summarization systems in Portuguese. Our contributions include the dataset, a hybrid summarization system to establish a baseline for future studies, and a discussion of evaluation metrics for summarization involving large language models, addressing the challenge of hallucination in the generated summaries. As a result of this discussion, the dataset also includes annotated data to evaluate natural language inference tasks in Portuguese.

**Link**: [arxiv](http://arxiv.org/abs/2410.07495v2),  [pdf](http://arxiv.org/pdf/2410.07495v2)

**Tags**: cs.CL 



### PoisonSwarm: Universal Harmful Information Synthesis via Model   Crowdsourcing
**Authors**: Yu Yan, Sheng Sun, Zhifei Zheng, Ziji Hao, Teli Liu, Min Liu

**Updated**: 2025-08-22T12:57:08Z

**Summary**: To construct responsible and secure AI applications, harmful information data is widely utilized for adversarial testing and the development of safeguards. Existing studies mainly leverage Large Language Models (LLMs) to synthesize data to obtain high-quality task datasets at scale, thereby avoiding costly human annotation. However, limited by the safety alignment mechanisms of LLMs, the synthesis of harmful data still faces challenges in generation reliability and content diversity. In this study, we propose a novel harmful information synthesis framework, PoisonSwarm, which applies the model crowdsourcing strategy to generate diverse harmful data while maintaining a high success rate. Specifically, we generate abundant benign data as the based templates in a counterfactual manner. Subsequently, we decompose each based template into multiple semantic units and perform unit-by-unit toxification and final refinement through dynamic model switching, thus ensuring the success of synthesis. Experimental results demonstrate that PoisonSwarm achieves state-of-the-art performance in synthesizing different categories of harmful data with high scalability and diversity.

**Link**: [arxiv](http://arxiv.org/abs/2505.21184v2),  [pdf](http://arxiv.org/pdf/2505.21184v2)

**Tags**: cs.LG cs.AI cs.CL 



### Contextualize-then-Aggregate: Circuits for In-Context Learning in   Gemma-2 2B
**Authors**: Aleksandra Bakalova, Yana Veitsman, Xinting Huang, Michael Hahn

**Updated**: 2025-08-22T12:52:38Z

**Summary**: In-Context Learning (ICL) is an intriguing ability of large language models (LLMs). Despite a substantial amount of work on its behavioral aspects and how it emerges in miniature setups, it remains unclear which mechanism assembles task information from the individual examples in a fewshot prompt. We use causal interventions to identify information flow in Gemma-2 2B for five naturalistic ICL tasks. We find that the model infers task information using a two-step strategy we call contextualize-then-aggregate: In the lower layers, the model builds up representations of individual fewshot examples, which are contextualized by preceding examples through connections between fewshot input and output tokens across the sequence. In the higher layers, these representations are aggregated to identify the task and prepare prediction of the next output. The importance of the contextualization step differs between tasks, and it may become more important in the presence of ambiguous examples. Overall, by providing rigorous causal analysis, our results shed light on the mechanisms through which ICL happens in language models.

**Link**: [arxiv](http://arxiv.org/abs/2504.00132v2),  [pdf](http://arxiv.org/pdf/2504.00132v2)

**Tags**: cs.CL cs.LG 



### Identification of Nonlinear Damping of Transverse Loop Oscillations by   KHI-induced Turbulence
**Authors**: Sihui Zhong, Andrew Hillier, Iñigo Arregui

**Updated**: 2025-08-22T12:41:55Z

**Summary**: Kink oscillations in coronal loops have been extensively studied for their potential contributions to coronal heating and their role in plasma diagnostics through coronal seismology. A key focus is the strong damping of large-amplitude kink oscillations, which observational evidence suggests is nonlinear. However, directly identifying the nonlinearity is a challenge. This work presents an analytic formula describing nonlinear standing kink oscillations dissipated by turbulence, characterised by a time-varying damping rate and period drift. We investigate how the damping behaviour depends on the driving amplitude and loop properties, showing that the initial damping time $\tau$ is inversely proportional to the velocity disturbance over the loop radius, $V_i/R$. Using MCMC fitting with Bayesian inference, the nonlinear function better fits an observed decaying kink oscillation than traditional linear models, including exponential damping, suggesting its nonlinear nature. By applying a Bayesian model comparison, we establish regimes in which nonlinear and linear resonant absorption mechanisms dominate based on the relationship between the damping rate $\tau/P$ and $V_i/R$. Additionally, analysis of two specific events reveals that while one favours the nonlinear model, the other is better explained by the linear model. Our results suggest that this analytical approximation of nonlinear damping due to turbulence provides a valid and reliable description of large-amplitude decaying kink oscillations in coronal loops.

**Link**: [arxiv](http://arxiv.org/abs/2508.16349v1),  [pdf](http://arxiv.org/pdf/2508.16349v1)

**Tags**: astro-ph.SR physics.plasm-ph 



### Confusion is the Final Barrier: Rethinking Jailbreak Evaluation and   Investigating the Real Misuse Threat of LLMs
**Authors**: Yu Yan, Sheng Sun, Zhe Wang, Yijun Lin, Zenghao Duan, zhifei zheng, Min Liu, Zhiyi yin, Jianping Zhang

**Updated**: 2025-08-22T12:41:26Z

**Summary**: With the development of Large Language Models (LLMs), numerous efforts have revealed their vulnerabilities to jailbreak attacks. Although these studies have driven the progress in LLMs' safety alignment, it remains unclear whether LLMs have internalized authentic knowledge to deal with real-world crimes, or are merely forced to simulate toxic language patterns. This ambiguity raises concerns that jailbreak success is often attributable to a hallucination loop between jailbroken LLM and judger LLM. By decoupling the use of jailbreak techniques, we construct knowledge-intensive Q\&A to investigate the misuse threats of LLMs in terms of dangerous knowledge possession, harmful task planning utility, and harmfulness judgment robustness. Experiments reveal a mismatch between jailbreak success rates and harmful knowledge possession in LLMs, and existing LLM-as-a-judge frameworks tend to anchor harmfulness judgments on toxic language patterns. Our study reveals a gap between existing LLM safety assessments and real-world threat potential.

**Link**: [arxiv](http://arxiv.org/abs/2508.16347v1),  [pdf](http://arxiv.org/pdf/2508.16347v1)

**Tags**: cs.CR cs.AI 



### from Benign import Toxic: Jailbreaking the Language Model via   Adversarial Metaphors
**Authors**: Yu Yan, Sheng Sun, Zenghao Duan, Teli Liu, Min Liu, Zhiyi Yin, Jiangyu Lei, Qi Li

**Updated**: 2025-08-22T12:35:34Z

**Summary**: Current studies have exposed the risk of Large Language Models (LLMs) generating harmful content by jailbreak attacks. However, they overlook that the direct generation of harmful content from scratch is more difficult than inducing LLM to calibrate benign content into harmful forms. In our study, we introduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR) to induce the LLM to calibrate malicious metaphors for jailbreaking. Specifically, to answer harmful queries, AVATAR adaptively identifies a set of benign but logically related metaphors as the initial seed. Then, driven by these metaphors, the target LLM is induced to reason and calibrate about the metaphorical content, thus jailbroken by either directly outputting harmful responses or calibrating residuals between metaphorical and professional harmful content. Experimental results demonstrate that AVATAR can effectively and transferable jailbreak LLMs and achieve a state-of-the-art attack success rate across multiple advanced LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2503.00038v4),  [pdf](http://arxiv.org/pdf/2503.00038v4)

**Tags**: cs.CL cs.AI cs.CR 



### Cyberbullying Detection via Aggression-Enhanced Prompting
**Authors**: Aisha Saeid, Anu Sabu, Girish A. Koushik, Ferrante Neri, Diptesh Kanojia

**Updated**: 2025-08-22T12:32:59Z

**Summary**: Detecting cyberbullying on social media remains a critical challenge due to its subtle and varied expressions. This study investigates whether integrating aggression detection as an auxiliary task within a unified training framework can enhance the generalisation and performance of large language models (LLMs) in cyberbullying detection. Experiments are conducted on five aggression datasets and one cyberbullying dataset using instruction-tuned LLMs. We evaluated multiple strategies: zero-shot, few-shot, independent LoRA fine-tuning, and multi-task learning (MTL). Given the inconsistent results of MTL, we propose an enriched prompt pipeline approach in which aggression predictions are embedded into cyberbullying detection prompts to provide contextual augmentation. Preliminary results show that the enriched prompt pipeline consistently outperforms standard LoRA fine-tuning, indicating that aggression-informed context significantly boosts cyberbullying detection. This study highlights the potential of auxiliary tasks, such as aggression detection, to improve the generalisation of LLMs for safety-critical applications on social networks.

**Link**: [arxiv](http://arxiv.org/abs/2508.06360v2),  [pdf](http://arxiv.org/pdf/2508.06360v2)

**Tags**: cs.CL 



### Mutation-Guided Unit Test Generation with a Large Language Model
**Authors**: Guancheng Wang, Qinghua Xu, Lionel C. Briand, Kui Liu

**Updated**: 2025-08-22T12:30:44Z

**Summary**: Unit tests play a vital role in uncovering potential faults in software. While tools like EvoSuite focus on maximizing code coverage, recent advances in large language models (LLMs) have shifted attention toward LLM-based test generation. However, code coverage metrics -- such as line and branch coverage -- remain overly emphasized in reported research, despite being weak indicators of a test suite's fault-detection capability. In contrast, mutation score offers a more reliable and stringent measure, as demonstrated in our findings where some test suites achieve 100% coverage but only 4% mutation score. Although a few studies consider mutation score, the effectiveness of LLMs in killing mutants remains underexplored. In this paper, we propose MUTGEN, a mutation-guided, LLM-based test generation approach that incorporates mutation feedback directly into the prompt. Evaluated on 204 subjects from two benchmarks, MUTGEN significantly outperforms both EvoSuite and vanilla prompt-based strategies in terms of mutation score. Furthermore, MUTGEN introduces an iterative generation mechanism that pushes the limits of LLMs in killing additional mutants. Our study also provide insights into the limitations of LLM-based generation, analyzing the reasons for live and uncovered mutants, and the impact of different mutation operators on generation effectiveness.

**Link**: [arxiv](http://arxiv.org/abs/2506.02954v4),  [pdf](http://arxiv.org/pdf/2506.02954v4)

**Tags**: cs.SE 



### From Linear to Hierarchical: Evolving Tree-structured Thoughts for   Efficient Alpha Mining
**Authors**: Junji Ren, Junjie Zhao, Shengcai Liu, Peng Yang

**Updated**: 2025-08-22T12:21:37Z

**Summary**: Alpha mining, which discovers signals that predict asset returns, has long been attractive for automatic quantitative investment. This problem is typically formulated as a tree-based symbolic regression with handcrafted market data features and arithmetic operators. Unfortunately, existing symbolic methods are concerned with computational inefficiency and dependence on prior knowledge. Recent implementation of Large Language Models (LLMs) show that they can automatically generate executable codes for various tasks efficiently, thus can be considered as a new promising way for alpha mining. Specifically, LLMs-driven methods evolve a set of heuristics, including thoughts and codes, where the thoughts are usually represented as plain-text prompts of codes. Unfortunately, trivially adopting them in alpha mining ignores the fact that alphas are with hierarchical tree structures. This paper introduces Tree-structured thought Evolution (TreEvo), which evolves hierarchical reasoning ideas solely at the thought level. Experiments on four real-market datasets demonstrate that TreEvo can obtain better alphas with much less computational time and human expert efforts. And this superiority hardly holds without the tree-structured thoughts and the compatible evolutionary operators.

**Link**: [arxiv](http://arxiv.org/abs/2508.16334v1),  [pdf](http://arxiv.org/pdf/2508.16334v1)

**Tags**: cs.CE 



### Soteria: Language-Specific Functional Parameter Steering for   Multilingual Safety Alignment
**Authors**: Somnath Banerjee, Sayan Layek, Pratyush Chatterjee, Animesh Mukherjee, Rima Hazra

**Updated**: 2025-08-22T12:17:17Z

**Summary**: Ensuring consistent safety across multiple languages remains a significant challenge for large language models (LLMs). We introduce Soteria, a lightweight yet powerful strategy that locates and minimally adjusts the "functional heads" most responsible for harmful content generation in each language. By altering only a fraction of parameters, Soteria drastically reduces policy violations without sacrificing overall model performance, even in low-resource settings. To rigorously evaluate our approach, we also present XThreatBench, a specialized multilingual dataset capturing fine-grained harmful behaviors drawn from real policy guidelines. Experiments with leading open-source LLMs (e.g., Llama, Qwen, Mistral) show that Soteria consistently improves safety metrics across high-, mid-, and low-resource languages. These findings highlight a promising path toward scalable, linguistically attuned, and ethically aligned LLMs worldwide.

**Link**: [arxiv](http://arxiv.org/abs/2502.11244v2),  [pdf](http://arxiv.org/pdf/2502.11244v2)

**Tags**: cs.CL cs.AI 



### Simulation based inference of the ionization history from the 2D 21 cm   power spectrum
**Authors**: Nadia Cooper, Carina Norregaard, Romain Meriot, Jonathan R. Pritchard

**Updated**: 2025-08-22T12:15:22Z

**Summary**: The 21 cm signal contains a wealth of information about the formation of the first stars and the reionization of the intergalactic medium during the Cosmic Dawn (CD) and Epoch of Reionization (EoR). The timing of these important milestones has only roughly been constrained through indirect measurements, such as from the cosmic microwave background (CMB) optical depth, and Lyman-$\alpha$ forest. Therefore, inferring the neutral fraction over cosmic time is a goal of upcoming 21 cm experiments, such as the Square Kilometer Array (SKA). We contrast two approaches to infer astrophysical parameters and ionization history from 21 cm 2D power spectra (2DPS). We develop an emulator of the 21 cm 2DPS, trained on 21cmFAST simulations, taking into account the expected instrumental noise from the SKA and sample variance. We then perform simulation based inference (SBI) using neural posterior estimation (NPE). We compare training on datasets of noisy 2DPS obtained from 21cmFAST simulations and an emulator, to infer astrophysical parameters of interest. Using an emulator of the ionization history, which has been trained on simulations from the same astrophysical parameters, we then obtain posterior distributions of the ionization history over the redshift range z $\sim$ 5-12. We demonstrate that both methods are capable of accurately recovering the ionization history and astrophysical parameters. However, coverage tests indicate that adding emulated samples does not improve predictions. This work suggests that due to the stochastic nature of the 2DPS, using an emulator of this summary statistic may result in poorer inference.

**Link**: [arxiv](http://arxiv.org/abs/2508.16329v1),  [pdf](http://arxiv.org/pdf/2508.16329v1)

**Tags**: astro-ph.CO 



### Heterogeneous Quantile Treatment Effect Estimation for Longitudinal Data   with High-Dimensional Confounding
**Authors**: Zhixin Qiu, Huichen Zhu, Wenjie Wang, Yanlin Tang

**Updated**: 2025-08-22T12:13:49Z

**Summary**: Causal inference plays a fundamental role in various real-world applications. However, in the motivating non-small cell lung cancer (NSCLC) study, it is challenging to estimate the treatment effect of chemotherapy on circulating tumor DNA (ctDNA). First, the heterogeneous treatment effects vary across patient subgroups defined by baseline characteristics. Second, there exists a broad set of demographic, clinical and molecular variables act as potential confounders. Third, ctDNA trajectories over time show heavy-tailed non-Gaussian behavior. Finally, repeated measurements within subjects introduce unknown correlation. Combining convolution-smoothed quantile regression and orthogonal random forest, we propose a framework to estimate heterogeneous quantile treatment effects in the presence of high-dimensional confounding, which not only captures effect heterogeneity across covariates, but also behaves robustly to nuisance parameter estimation error. We establish the theoretical properties of the proposed estimator and demonstrate its finite-sample performance through comprehensive simulations. We illustrate its practical utility in the motivated NSCLC study.

**Link**: [arxiv](http://arxiv.org/abs/2508.16326v1),  [pdf](http://arxiv.org/pdf/2508.16326v1)

**Tags**: stat.ME 



### LLMSymGuard: A Symbolic Safety Guardrail Framework Leveraging   Interpretable Jailbreak Concepts
**Authors**: Darpan Aswal, Céline Hudelot

**Updated**: 2025-08-22T12:13:38Z

**Summary**: Large Language Models have found success in a variety of applications; however, their safety remains a matter of concern due to the existence of various types of jailbreaking methods. Despite significant efforts, alignment and safety fine-tuning only provide a certain degree of robustness against jailbreak attacks that covertly mislead LLMs towards the generation of harmful content. This leaves them prone to a number of vulnerabilities, ranging from targeted misuse to accidental profiling of users. This work introduces \textbf{LLMSymGuard}, a novel framework that leverages Sparse Autoencoders (SAEs) to identify interpretable concepts within LLM internals associated with different jailbreak themes. By extracting semantically meaningful internal representations, LLMSymGuard enables building symbolic, logical safety guardrails -- offering transparent and robust defenses without sacrificing model capabilities or requiring further fine-tuning. Leveraging advances in mechanistic interpretability of LLMs, our approach demonstrates that LLMs learn human-interpretable concepts from jailbreaks, and provides a foundation for designing more interpretable and logical safeguard measures against attackers. Code will be released upon publication.

**Link**: [arxiv](http://arxiv.org/abs/2508.16325v1),  [pdf](http://arxiv.org/pdf/2508.16325v1)

**Tags**: cs.CL cs.AI cs.SC 



### VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic   Reasoning
**Authors**: Benjamin Callewaert, Simon Vandevelde, Joost Vennekens

**Updated**: 2025-08-22T12:13:32Z

**Summary**: A recent approach to neurosymbolic reasoning is to explicitly combine the strengths of large language models (LLMs) and symbolic solvers to tackle complex reasoning tasks. However, current approaches face significant limitations, including poor generalizability due to task-specific prompts, inefficiencies caused by the lack of separation between knowledge and queries, and restricted inferential capabilities. These shortcomings hinder their scalability and applicability across diverse domains. In this paper, we introduce VERUS-LM, a novel framework designed to address these challenges. VERUS-LM employs a generic prompting mechanism, clearly separates domain knowledge from queries, and supports a wide range of different logical reasoning tasks. This framework enhances adaptability, reduces computational cost, and allows for richer forms of reasoning, such as optimization and constraint satisfaction. We show that our approach succeeds in diverse reasoning on a novel dataset, markedly outperforming LLMs. Additionally, our system achieves competitive results on common reasoning benchmarks when compared to other state-of-the-art approaches, and significantly surpasses them on the difficult AR-LSAT dataset. By pushing the boundaries of hybrid reasoning, VERUS-LM represents a significant step towards more versatile neurosymbolic AI systems

**Link**: [arxiv](http://arxiv.org/abs/2501.14540v2),  [pdf](http://arxiv.org/pdf/2501.14540v2)

**Tags**: cs.AI 



### Can Hallucinations Help? Boosting LLMs for Drug Discovery
**Authors**: Shuzhou Yuan, Zhan Qu, Ashish Yashwanth Kangen, Michael Färber

**Updated**: 2025-08-22T12:12:09Z

**Summary**: Hallucinations in large language models (LLMs), plausible but factually inaccurate text, are often viewed as undesirable. However, recent work suggests that such outputs may hold creative potential. In this paper, we investigate whether hallucinations can improve LLMs on molecule property prediction, a key task in early-stage drug discovery. We prompt LLMs to generate natural language descriptions from molecular SMILES strings and incorporate these often hallucinated descriptions into downstream classification tasks. Evaluating seven instruction-tuned LLMs across five datasets, we find that hallucinations significantly improve predictive accuracy for some models. Notably, Falcon3-Mamba-7B outperforms all baselines when hallucinated text is included, while hallucinations generated by GPT-4o consistently yield the greatest gains between models. We further identify and categorize over 18,000 beneficial hallucinations, with structural misdescriptions emerging as the most impactful type, suggesting that hallucinated statements about molecular structure may increase model confidence. Ablation studies show that larger models benefit more from hallucinations, while temperature has a limited effect. Our findings challenge conventional views of hallucination as purely problematic and suggest new directions for leveraging hallucinations as a useful signal in scientific modeling tasks like drug discovery.

**Link**: [arxiv](http://arxiv.org/abs/2501.13824v2),  [pdf](http://arxiv.org/pdf/2501.13824v2)

**Tags**: cs.CL cs.AI 



### MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks
**Authors**: Mouath Abu Daoud, Chaimae Abouzahir, Leen Kharouf, Walid Al-Eisawi, Nizar Habash, Farah E. Shamout

**Updated**: 2025-08-22T11:58:27Z

**Summary**: Large Language Models (LLMs) have demonstrated significant promise for various applications in healthcare. However, their efficacy in the Arabic medical domain remains unexplored due to the lack of high-quality domain-specific datasets and benchmarks. This study introduces MedArabiQ, a novel benchmark dataset consisting of seven Arabic medical tasks, covering multiple specialties and including multiple choice questions, fill-in-the-blank, and patient-doctor question answering. We first constructed the dataset using past medical exams and publicly available datasets. We then introduced different modifications to evaluate various LLM capabilities, including bias mitigation. We conducted an extensive evaluation with five state-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude 3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of new high-quality benchmarks that span different languages to ensure fair deployment and scalability of LLMs in healthcare. By establishing this benchmark and releasing the dataset, we provide a foundation for future research aimed at evaluating and enhancing the multilingual capabilities of LLMs for the equitable use of generative AI in healthcare.

**Link**: [arxiv](http://arxiv.org/abs/2505.03427v2),  [pdf](http://arxiv.org/pdf/2505.03427v2)

**Tags**: cs.CL cs.AI cs.HC 



### SATORI: Static Test Oracle Generation for REST APIs
**Authors**: Juan C. Alonso, Alberto Martin-Lopez, Sergio Segura, Gabriele Bavota, Antonio Ruiz-Cortés

**Updated**: 2025-08-22T11:57:55Z

**Summary**: REST API test case generation tools are evolving rapidly, with growing capabilities for the automated generation of complex tests. However, despite their strengths in test data generation, these tools are constrained by the types of test oracles they support, often limited to crashes, regressions, and noncompliance with API specifications or design standards. This paper introduces SATORI (Static API Test ORacle Inference), a black-box approach for generating test oracles for REST APIs by analyzing their OpenAPI Specification. SATORI uses large language models to infer the expected behavior of an API by analyzing the properties of the response fields of its operations, such as their name and descriptions. To foster its adoption, we extended the PostmanAssertify tool to automatically convert the test oracles reported by SATORI into executable assertions. Evaluation results on 17 operations from 12 industrial APIs show that SATORI can automatically generate up to hundreds of valid test oracles per operation. SATORI achieved an F1-score of 74.3%, outperforming the state-of-the-art dynamic approach AGORA+ (69.3%)-which requires executing the API-when generating comparable oracle types. Moreover, our findings show that static and dynamic oracle inference methods are complementary: together, SATORI and AGORA+ found 90% of the oracles in our annotated ground-truth dataset. Notably, SATORI uncovered 18 bugs in popular APIs (Amadeus Hotel, Deutschebahn, FDIC, GitLab, Marvel, OMDb and Vimeo) leading to documentation updates by the API maintainers.

**Link**: [arxiv](http://arxiv.org/abs/2508.16318v1),  [pdf](http://arxiv.org/pdf/2508.16318v1)

**Tags**: cs.SE 



### OwkinZero: Accelerating Biological Discovery with AI
**Authors**: Nathan Bigaud, Vincent Cabeli, Meltem Gurel, Arthur Pignet, John Klein, Gilles Wainrib, Eric Durand

**Updated**: 2025-08-22T11:52:04Z

**Summary**: While large language models (LLMs) are rapidly advancing scientific research, they continue to struggle with core biological reasoning tasks essential for translational and biomedical discovery. To address this limitation, we created and curated eight comprehensive benchmark datasets comprising over 300,000 verifiable question-and-answer pairs, each targeting critical challenges in drug discovery including target druggability, modality suitability, and drug perturbation effects. Using this resource, we developed the OwkinZero models by post-training open-source LLMs through a Reinforcement Learning from Verifiable Rewards strategy. Our results demonstrate that specialized 8-32B OwkinZero models substantially outperform larger, state-of-the-art commercial LLMs on these biological benchmarks. Remarkably, we uncover evidence of a key aspect of generalization: specialist models trained on a single task consistently outperform their base models on previously unseen tasks. This generalization effect is further amplified in our comprehensive OwkinZero models, which were trained on a mixture of datasets and achieve even broader cross-task improvements. This study represents a significant step toward addressing the biological reasoning blind spot in current LLMs, demonstrating that targeted reinforcement learning on carefully curated data can unlock generalizable performance in specialized models, thereby accelerating AI-driven biological discovery.

**Link**: [arxiv](http://arxiv.org/abs/2508.16315v1),  [pdf](http://arxiv.org/pdf/2508.16315v1)

**Tags**: cs.LG 



### Retrieval Enhanced Feedback via In-context Neural Error-book
**Authors**: Jongyeop Hyun, Bumsoo Kim

**Updated**: 2025-08-22T11:50:04Z

**Summary**: Recent advancements in Large Language Models (LLMs) have significantly improved reasoning capabilities, with in-context learning (ICL) emerging as a key technique for adaptation without retraining. While previous works have focused on leveraging correct examples, recent research highlights the importance of learning from errors to enhance performance. However, existing methods lack a structured framework for analyzing and mitigating errors, particularly in Multimodal Large Language Models (MLLMs), where integrating visual and textual inputs adds complexity. To address this issue, we propose REFINE: Retrieval-Enhanced Feedback via In-context Neural Error-book, a teacher-student framework that systematically structures errors and provides targeted feedback. REFINE introduces three systematic queries to construct structured feedback -- Feed-Target, Feed-Check, and Feed-Path -- to enhance multimodal reasoning by prioritizing relevant visual information, diagnosing critical failure points, and formulating corrective actions. Unlike prior approaches that rely on redundant retrievals, REFINE optimizes structured feedback retrieval, improving inference efficiency, token usage, and scalability. Our results demonstrate substantial speedup, reduced computational costs, and successful generalization, highlighting REFINE's potential for enhancing multimodal reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2508.16313v1),  [pdf](http://arxiv.org/pdf/2508.16313v1)

**Tags**: cs.LG cs.AI cs.CL 



### Tree-based methods for length-biased survival data
**Authors**: Jinwoo Lee, Jiyu Sun, Hyunwoo Lee, Donghwan Lee

**Updated**: 2025-08-22T11:48:24Z

**Summary**: Left-truncated survival data commonly arise in prevalent cohort studies, where only individuals who have experienced disease onset and survived until enrollment in the study. When the onset process follows a stationary Poisson process, the resulting data are length-biased. This sampling mechanism induces a selection bias towards longer survival individuals, and nonparametric and semiparametric methods for traditional survival data are not directly applicable. While tree-based methods developed for left-truncated data can be applied, they may be inefficient for length-biased data, as they do not account for the distribution of truncation times. To address this, we propose new survival trees and forests for length-biased right-censored data within the conditional inference framework. Our approach uses a score function derived from the full likelihood to construct permutation test statistics for unbiased variable selection. For survival prediction, we consider two estimators of the unbiased survival function, differing in statistical efficiency and computational complexity. These elements enhance efficiency in tree construction and improve accuracy of survival prediction in ensemble settings. Simulation studies demonstrate efficiency gains in both tree recovery and survival prediction, often exceeding the gains from ensembling alone. We further illustrate the utility of the proposed methods using lung cancer data from the Cancer Public Library Database.

**Link**: [arxiv](http://arxiv.org/abs/2508.16312v1),  [pdf](http://arxiv.org/pdf/2508.16312v1)

**Tags**: stat.ME 



### Exploiting Information Redundancy in Attention Maps for Extreme   Quantization of Vision Transformers
**Authors**: Lucas Maisonnave, Karim Haroun, Tom Pegeot

**Updated**: 2025-08-22T11:43:39Z

**Summary**: Transformer models rely on Multi-Head Self-Attention (MHSA) mechanisms, where each attention head contributes to the final representation. However, their computational complexity and high memory demands due to MHSA hinders their deployment at the edge. In this work, we analyze and exploit information redundancy in attention maps to accelerate model inference. By quantifying the information captured by each attention head using Shannon entropy, our analysis reveals that attention heads with lower entropy, i.e., exhibiting more deterministic behavior, tend to contribute less information, motivating targeted compression strategies. Relying on these insights, we propose Entropy Attention Maps (EAM), a model that freezes the weights of low-entropy attention maps and quantizes these values to low precision to avoid redundant re-computation. Empirical validation on ImageNet-1k shows that EAM achieves similar or higher accuracy at $\leq$20\% sparsity in attention maps and competitive performance beyond this level for the DeiT and Swin Transformer models.

**Link**: [arxiv](http://arxiv.org/abs/2508.16311v1),  [pdf](http://arxiv.org/pdf/2508.16311v1)

**Tags**: cs.CV cs.AI cs.IT math.IT 



### DIDS: Domain Impact-aware Data Sampling for Large Language Model   Training
**Authors**: Weijie Shi, Jipeng Zhang, Yaguang Wu, Jingzhi Fang, Ruiyuan Zhang, Jiajie Xu, Jia Zhu, Hao Chen, Yao Zhao, Sirui Han, Xiaofang Zhou

**Updated**: 2025-08-22T11:26:53Z

**Summary**: Large language models (LLMs) are commonly trained on multi-domain datasets, where domain sampling strategies significantly impact model performance due to varying domain importance across downstream tasks. Existing approaches for optimizing domain-level sampling strategies struggle with maintaining intra-domain consistency and accurately measuring domain impact. In this paper, we present Domain Impact-aware Data Sampling (DIDS). To ensure intra-domain consistency, a gradient clustering algorithm is proposed to group training data based on their learning effects, where a proxy language model and dimensionality reduction are employed to reduce computational overhead. To accurately measure domain impact, we develop a Fisher Information Matrix (FIM) guided metric that quantifies how domain-specific parameter updates affect the model's output distributions on downstream tasks, with theoretical guarantees. Furthermore, to determine optimal sampling ratios, DIDS combines both the FIM-guided domain impact assessment and loss learning trajectories that indicate domain-specific potential, while accounting for diminishing marginal returns. Extensive experiments demonstrate that DIDS achieves 3.4% higher average performance while maintaining comparable training efficiency. The code is available at https://github.com/shiweijiezero/DIDS.

**Link**: [arxiv](http://arxiv.org/abs/2504.13227v2),  [pdf](http://arxiv.org/pdf/2504.13227v2)

**Tags**: cs.CL cs.AI cs.LG 



### Bridging the Culture Gap: A Framework for LLM-Driven Socio-Cultural   Localization of Math Word Problems in Low-Resource Languages
**Authors**: Israel Abebe Azime, Tadesse Destaw Belay, Dietrich Klakow, Philipp Slusallek, Anshuman Chhabra

**Updated**: 2025-08-22T11:14:44Z

**Summary**: Large language models (LLMs) have demonstrated significant capabilities in solving mathematical problems expressed in natural language. However, multilingual and culturally-grounded mathematical reasoning in low-resource languages lags behind English due to the scarcity of socio-cultural task datasets that reflect accurate native entities such as person names, organization names, and currencies. Existing multilingual benchmarks are predominantly produced via translation and typically retain English-centric entities, owing to the high cost associated with human annotater-based localization. Moreover, automated localization tools are limited, and hence, truly localized datasets remain scarce. To bridge this gap, we introduce a framework for LLM-driven cultural localization of math word problems that automatically constructs datasets with native names, organizations, and currencies from existing sources. We find that translated benchmarks can obscure true multilingual math ability under appropriate socio-cultural contexts. Through extensive experiments, we also show that our framework can help mitigate English-centric entity bias and improves robustness when native entities are introduced across various languages.

**Link**: [arxiv](http://arxiv.org/abs/2508.14913v2),  [pdf](http://arxiv.org/pdf/2508.14913v2)

**Tags**: cs.CL 



### AgentScope 1.0: A Developer-Centric Framework for Building Agentic   Applications
**Authors**: Dawei Gao, Zitao Li, Yuexiang Xie, Weirui Kuang, Liuyi Yao, Bingchen Qian, Zhijian Ma, Yue Cui, Haohao Luo, Shen Li, Lu Yi, Yi Yu, Shiqi He, Zhiling Luo, Wenmeng Zhou, Zhicheng Zhang, Xuguang He, Ziqian Chen, Weikai Liao, Farruh Isakulovich Kushnazarov, Yaliang Li, Bolin Ding, Jingren Zhou

**Updated**: 2025-08-22T10:35:56Z

**Summary**: Driven by rapid advancements of Large Language Models (LLMs), agents are empowered to combine intrinsic knowledge with dynamic tool use, greatly enhancing their capacity to address real-world tasks. In line with such an evolution, AgentScope introduces major improvements in a new version (1.0), towards comprehensively supporting flexible and efficient tool-based agent-environment interactions for building agentic applications. Specifically, we abstract foundational components essential for agentic applications and provide unified interfaces and extensible modules, enabling developers to easily leverage the latest progress, such as new models and MCPs. Furthermore, we ground agent behaviors in the ReAct paradigm and offer advanced agent-level infrastructure based on a systematic asynchronous design, which enriches both human-agent and agent-agent interaction patterns while improving execution efficiency. Building on this foundation, we integrate several built-in agents tailored to specific practical scenarios. AgentScope also includes robust engineering support for developer-friendly experiences. We provide a scalable evaluation module with a visual studio interface, making the development of long-trajectory agentic applications more manageable and easier to trace. In addition, AgentScope offers a runtime sandbox to ensure safe agent execution and facilitates rapid deployment in production environments. With these enhancements, AgentScope provides a practical foundation for building scalable, adaptive, and effective agentic applications.

**Link**: [arxiv](http://arxiv.org/abs/2508.16279v1),  [pdf](http://arxiv.org/pdf/2508.16279v1)

**Tags**: cs.AI 



### Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and   Context-Aware KGQA
**Authors**: Yingxu Wang, Shiqi Fan, Mengzhu Wang, Siyang Gao, Siwei Liu, Nan Yin

**Updated**: 2025-08-22T10:30:51Z

**Summary**: Knowledge Graph Question Answering (KGQA) aims to interpret natural language queries and perform structured reasoning over knowledge graphs by leveraging their relational and semantic structures to retrieve accurate answers. Recent KGQA methods primarily follow either retrieve-then-reason paradigm, relying on GNNs or heuristic rules for static paths extraction, or dynamic path generation strategies that use large language models (LLMs) with prompting to jointly perform retrieval and reasoning. However, the former suffers from limited adaptability due to static path extraction and lack of contextual refinement, while the latter incurs high computational costs and struggles with accurate path evaluation due to reliance on fixed scoring functions and extensive LLM calls. To address these issues, this paper proposes Dynamically Adaptive MCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search with adaptive path evaluation for efficient and context-aware KGQA. DAMR employs a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based planner, which selects top-$k$ relevant relations at each step to reduce search space. To improve path evaluation accuracy, we introduce a lightweight Transformer-based scorer that performs context-aware plausibility estimation by jointly encoding the question and relation sequence through cross-attention, enabling the model to capture fine-grained semantic shifts during multi-hop reasoning. Furthermore, to alleviate the scarcity of high-quality supervision, DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically generates training signals from partial paths explored during search, allowing the scorer to continuously adapt to the evolving distribution of reasoning trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR significantly outperforms state-of-the-art methods.

**Link**: [arxiv](http://arxiv.org/abs/2508.00719v2),  [pdf](http://arxiv.org/pdf/2508.00719v2)

**Tags**: cs.CL cs.AI 



### LBM: Latent Bridge Matching for Fast Image-to-Image Translation
**Authors**: Clément Chadebec, Onur Tasar, Sanjeev Sreetharan, Benjamin Aubin

**Updated**: 2025-08-22T10:28:35Z

**Summary**: In this paper, we introduce Latent Bridge Matching (LBM), a new, versatile and scalable method that relies on Bridge Matching in a latent space to achieve fast image-to-image translation. We show that the method can reach state-of-the-art results for various image-to-image tasks using only a single inference step. In addition to its efficiency, we also demonstrate the versatility of the method across different image translation tasks such as object removal, normal and depth estimation, and object relighting. We also derive a conditional framework of LBM and demonstrate its effectiveness by tackling the tasks of controllable image relighting and shadow generation. We provide an implementation at https://github.com/gojasper/LBM.

**Link**: [arxiv](http://arxiv.org/abs/2503.07535v2),  [pdf](http://arxiv.org/pdf/2503.07535v2)

**Tags**: cs.CV 



### The next question after Turing's question: Introducing the Grow-AI test
**Authors**: Alexandru Tugui

**Updated**: 2025-08-22T10:19:42Z

**Summary**: This study aims to extend the framework for assessing artificial intelligence, called GROW-AI (Growth and Realization of Autonomous Wisdom), designed to answer the question "Can machines grow up?" -- a natural successor to the Turing Test. The methodology applied is based on a system of six primary criteria (C1-C6), each assessed through a specific "game", divided into four arenas that explore both the human dimension and its transposition into AI. All decisions and actions of the entity are recorded in a standardized AI Journal, the primary source for calculating composite scores. The assessment uses the prior expert method to establish initial weights, and the global score -- Grow Up Index -- is calculated as the arithmetic mean of the six scores, with interpretation on maturity thresholds. The results show that the methodology allows for a coherent and comparable assessment of the level of "growth" of AI entities, regardless of their type (robots, software agents, LLMs). The multi-game structure highlights strengths and vulnerable areas, and the use of a unified journal guarantees traceability and replicability in the evaluation. The originality of the work lies in the conceptual transposition of the process of "growing" from the human world to that of artificial intelligence, in an integrated testing format that combines perspectives from psychology, robotics, computer science, and ethics. Through this approach, GROW-AI not only measures performance but also captures the evolutionary path of an AI entity towards maturity.

**Link**: [arxiv](http://arxiv.org/abs/2508.16277v1),  [pdf](http://arxiv.org/pdf/2508.16277v1)

**Tags**: cs.AI cs.HC 68T01, 68T05, 68T42, 91A80 I.2; K.4 



### LLMs that Understand Processes: Instruction-tuning for Semantics-Aware   Process Mining
**Authors**: Vira Pyrih, Adrian Rebmann, Han van der Aa

**Updated**: 2025-08-22T10:13:13Z

**Summary**: Process mining is increasingly using textual information associated with events to tackle tasks such as anomaly detection and process discovery. Such semantics-aware process mining focuses on what behavior should be possible in a process (i.e., expectations), thus providing an important complement to traditional, frequency-based techniques that focus on recorded behavior (i.e., reality). Large Language Models (LLMs) provide a powerful means for tackling semantics-aware tasks. However, the best performance is so far achieved through task-specific fine-tuning, which is computationally intensive and results in models that can only handle one specific task. To overcome this lack of generalization, we use this paper to investigate the potential of instruction-tuning for semantics-aware process mining. The idea of instruction-tuning here is to expose an LLM to prompt-answer pairs for different tasks, e.g., anomaly detection and next-activity prediction, making it more familiar with process mining, thus allowing it to also perform better at unseen tasks, such as process discovery. Our findings demonstrate a varied impact of instruction-tuning: while performance considerably improved on process discovery and prediction tasks, it varies across models on anomaly detection tasks, highlighting that the selection of tasks for instruction-tuning is critical to achieving desired outcomes.

**Link**: [arxiv](http://arxiv.org/abs/2508.16270v1),  [pdf](http://arxiv.org/pdf/2508.16270v1)

**Tags**: cs.CL 



### From Confidence to Collapse in LLM Factual Robustness
**Authors**: Alina Fastowski, Bardh Prenkaj, Gjergji Kasneci

**Updated**: 2025-08-22T09:59:23Z

**Summary**: Ensuring the robustness of factual knowledge in LLMs is critical for reliable applications in tasks such as question answering and reasoning. However, existing evaluation methods predominantly focus on performance-based metrics, often investigating from the perspective of prompt perturbations, which captures only the externally triggered side of knowledge robustness. To bridge this gap, we introduce a principled approach to measure factual robustness from the perspective of the generation process by analyzing token distribution entropy in combination with temperature scaling sensitivity. These two factors build the Factual Robustness Score (FRS), a novel metric which quantifies the stability of a fact against perturbations in decoding conditions, given its initial uncertainty. To validate our approach, we conduct extensive experiments on 5 LLMs across 3 closed-book QA datasets (SQuAD, TriviaQA, and HotpotQA). We show that factual robustness varies significantly -- smaller models report an FRS of $0.76$, larger ones $0.93$ -- with accuracy degrading by ~$60\%$ under increased uncertainty. These insights demonstrate how entropy and temperature scaling impact factual accuracy, and lay a foundation for developing more robust knowledge retention and retrieval in future models.

**Link**: [arxiv](http://arxiv.org/abs/2508.16267v1),  [pdf](http://arxiv.org/pdf/2508.16267v1)

**Tags**: cs.CL cs.AI 



### M3TQA: Massively Multilingual Multitask Table Question Answering
**Authors**: Daixin Shu, Jian Yang, Zhenhe Wu, Xianjie Wu, Xianfu Cheng, Xiangyuan Guan, Yanghai Wang, Pengfei Wu, Tingyang Yang, Hualei Zhu, Wei Zhang, Ge Zhang, Jiaheng Liu, Zhoujun Li

**Updated**: 2025-08-22T09:57:40Z

**Summary**: Tabular data is a fundamental component of real-world information systems, yet most research in table understanding remains confined to English, leaving multilingual comprehension significantly underexplored. Existing multilingual table benchmarks suffer from geolinguistic imbalance - overrepresenting certain languages and lacking sufficient scale for rigorous cross-lingual analysis. To address these limitations, we introduce a comprehensive framework for massively multilingual multitask table question answering, featuring m3TQA-Instruct, a large-scale benchmark spanning 97 languages across diverse language families, including underrepresented and low-resource languages. We construct m3TQA by curating 50 real-world tables in Chinese and English, then applying a robust six-step LLM-based translation pipeline powered by DeepSeek and GPT-4o, achieving high translation fidelity with a median BLEU score of 60.19 as validated through back-translation. The benchmark includes 2,916 professionally annotated question-answering pairs across four tasks designed to evaluate nuanced table reasoning capabilities. Experiments on state-of-the-art LLMs reveal critical insights into cross-lingual generalization, demonstrating that synthetically generated, unannotated QA data can significantly boost performance, particularly for low-resource languages. M3T-Bench establishes a new standard for multilingual table understanding, providing both a challenging evaluation platform and a scalable methodology for future research.

**Link**: [arxiv](http://arxiv.org/abs/2508.16265v1),  [pdf](http://arxiv.org/pdf/2508.16265v1)

**Tags**: cs.CL 



### On the Evolution of Federated Post-Training Large Language Models: A   Model Accessibility View
**Authors**: Tao Guo, Junxiao Wang, Fushuo Huo, Laizhong Cui, Song Guo, Jie Gui, Dacheng Tao

**Updated**: 2025-08-22T09:52:31Z

**Summary**: Federated Learning (FL) enables training models across decentralized data silos while preserving client data privacy. Recent research has explored efficient methods for post-training large language models (LLMs) within FL to address computational and communication challenges. While existing approaches often rely on access to LLMs' internal information, which is frequently restricted in real-world scenarios, an inference-only paradigm (black-box FedLLM) has emerged to address these limitations. This paper presents a comprehensive survey on federated tuning for LLMs. We propose a taxonomy categorizing existing studies along two axes: model access-based and parameter efficiency-based optimization. We classify FedLLM approaches into white-box, gray-box, and black-box techniques, highlighting representative methods within each category. We review emerging research treating LLMs as black-box inference APIs and discuss promising directions and open challenges for future research.

**Link**: [arxiv](http://arxiv.org/abs/2508.16261v1),  [pdf](http://arxiv.org/pdf/2508.16261v1)

**Tags**: cs.LG 



### MCPVerse: An Expansive, Real-World Benchmark for Agentic Tool Use
**Authors**: Fei Lei, Yibo Yang, Wenxiu Sun, Dahua Lin

**Updated**: 2025-08-22T09:47:53Z

**Summary**: Large Language Models (LLMs) are evolving from text generators into reasoning agents. This transition makes their ability to use external tools a critical capability. However, evaluating this skill presents a significant challenge. Existing benchmarks are often limited by their reliance on synthetic tools and severely constrained action spaces. To address these limitations, we introduce MCPVerse, an expansive, real-world benchmark for evaluating agentic tool use. MCPVerse integrates more than 550 real-world, executable tools to create an unprecedented action space exceeding 140k tokens, and employs outcome-based evaluation with real-time ground truth for time-sensitive tasks. We benchmarked the state-of-the-art LLMs across three modes (Oracle, Standard, and Max-Scale), revealing that while most models suffer performance degradation when confronted with larger tool sets, the agentic models, such as Claude-4-Sonnet, can effectively leverage expanded exploration spaces to improve accuracy. This finding not only exposes the limitations of state-of-the-art models in complex, real-world scenarios but also establishes MCPVerse as a critical benchmark for measuring and advancing agentic tool use capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2508.16260v1),  [pdf](http://arxiv.org/pdf/2508.16260v1)

**Tags**: cs.CL cs.AI 



## Keyword: LLM Deployment 
 ### Are LLM-Powered Social Media Bots Realistic?
**Authors**: Lynnette Hui Xian Ng, Kathleen M. Carley

**Updated**: 2025-08-22T17:56:26Z

**Summary**: As Large Language Models (LLMs) become more sophisticated, there is a possibility to harness LLMs to power social media bots. This work investigates the realism of generating LLM-Powered social media bot networks. Through a combination of manual effort, network science and LLMs, we create synthetic bot agent personas, their tweets and their interactions, thereby simulating social media networks. We compare the generated networks against empirical bot/human data, observing that both network and linguistic properties of LLM-Powered Bots differ from Wild Bots/Humans. This has implications towards the detection and effectiveness of LLM-Powered Bots.

**Link**: [arxiv](http://arxiv.org/abs/2508.00998v2),  [pdf](http://arxiv.org/pdf/2508.00998v2)

**Tags**: cs.SI cs.AI 



### LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due   Diligence
**Authors**: Alisa Vinogradova, Vlad Vinogradov, Dmitrii Radkevich, Ilya Yasny, Dmitry Kobyzev, Ivan Izmailov, Katsiaryna Yanchanka, Andrey Doronichev

**Updated**: 2025-08-22T17:50:00Z

**Summary**: In this paper, we describe and benchmark a competitor-discovery component used within an agentic AI system for fast drug asset due diligence. A competitor-discovery AI agent, given an indication, retrieves all drugs comprising the competitive landscape of that indication and extracts canonical attributes for these drugs. The competitor definition is investor-specific, and data is paywalled/licensed, fragmented across registries, ontology-mismatched by indication, alias-heavy for drug names, multimodal, and rapidly changing. Although considered the best tool for this problem, the current LLM-based AI systems aren't capable of reliably retrieving all competing drug names, and there is no accepted public benchmark for this task. To address the lack of evaluation, we use LLM-based agents to transform five years of multi-modal, unstructured diligence memos from a private biotech VC fund into a structured evaluation corpus mapping indications to competitor drugs with normalized attributes. We also introduce a competitor validating LLM-as-a-judge agent that filters out false positives from the list of predicted competitors to maximize precision and suppress hallucinations. On this benchmark, our competitor-discovery agent achieves 83% recall, exceeding OpenAI Deep Research (65%) and Perplexity Labs (60%). The system is deployed in production with enterprise users; in a case study with a biotech VC investment fund, analyst turnaround time dropped from 2.5 days to $\sim$3 hours ($\sim$20x) for the competitive analysis.

**Link**: [arxiv](http://arxiv.org/abs/2508.16571v1),  [pdf](http://arxiv.org/pdf/2508.16571v1)

**Tags**: cs.AI cs.IR cs.MA 



### Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse   Autoencoders
**Authors**: David Chanin, Adrià Garriga-Alonso

**Updated**: 2025-08-22T17:26:33Z

**Summary**: Sparse Autoencoders (SAEs) extract features from LLM internal activations, meant to correspond to single concepts. A core SAE training hyperparameter is L0: how many features should fire per token on average. Existing work compares SAE algorithms using sparsity--reconstruction tradeoff plots, implying L0 is a free parameter with no single correct value. In this work we study the effect of L0 on BatchTopK SAEs, and show that if L0 is not set precisely, the SAE fails to learn the underlying features of the LLM. If L0 is too low, the SAE will mix correlated features to improve reconstruction. If L0 is too high, the SAE finds degenerate solutions that also mix features. Further, we demonstrate a method to determine the correct L0 value for an SAE on a given training distribution, which finds the true L0 in toy models and coincides with peak sparse probing performance in LLMs. We find that most commonly used SAEs have an L0 that is too low. Our work shows that, to train SAEs with correct features, practitioners must set L0 correctly.

**Link**: [arxiv](http://arxiv.org/abs/2508.16560v1),  [pdf](http://arxiv.org/pdf/2508.16560v1)

**Tags**: cs.LG cs.AI cs.CL 



### TinyML Towards Industry 4.0: Resource-Efficient Process Monitoring of a   Milling Machine
**Authors**: Tim Langer, Matthias Widra, Volkhard Beyer

**Updated**: 2025-08-22T17:21:56Z

**Summary**: In the context of industry 4.0, long-serving industrial machines can be retrofitted with process monitoring capabilities for future use in a smart factory. One possible approach is the deployment of wireless monitoring systems, which can benefit substantially from the TinyML paradigm. This work presents a complete TinyML flow from dataset generation, to machine learning model development, up to implementation and evaluation of a full preprocessing and classification pipeline on a microcontroller. After a short review on TinyML in industrial process monitoring, the creation of the novel MillingVibes dataset is described. The feasibility of a TinyML system for structure-integrated process quality monitoring could be shown by the development of an 8-bit-quantized convolutional neural network (CNN) model with 12.59kiB parameter storage. A test accuracy of 100.0% could be reached at 15.4ms inference time and 1.462mJ per quantized CNN inference on an ARM Cortex M4F microcontroller, serving as a reference for future TinyML process monitoring solutions.

**Link**: [arxiv](http://arxiv.org/abs/2508.16553v1),  [pdf](http://arxiv.org/pdf/2508.16553v1)

**Tags**: cs.LG cs.CV cs.ET cs.SY eess.SP eess.SY I.2.1; I.5.4; C.5.3; C.3 



### RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs.   Reinforcement Learning Fine-Tuning for LLMs
**Authors**: Hangzhan Jin, Sicheng Lv, Sifan Wu, Mohammad Hamdaqa

**Updated**: 2025-08-22T17:10:37Z

**Summary**: Training large language models (LLMs) from scratch is increasingly impractical, making post-training methods such as supervised fine-tuning (SFT) and reinforcement-learning fine-tuning (RL-FT, e.g., PPO) central to modern practice. Using an out-of-distribution (OOD) variant of the 24-point card game and new spectrum-based diagnostics, we revisit how these two stages reshape model representation and OOD performance. Our key findings are- (1) RL-FT can restore much of the OOD performance loss from SFT (e.g., Llama-11B 8.97% to 15.38%, Qwen-7B 17.09% to 19.66%). But when SFT induces severe overfitting and a clear distribution shift, RL-FT cannot fully recover OOD performance. (2) Direction shifts of singular vectors matter more than singular value magnitudes. These shifts concentrate on directions linked to the largest and smallest singular values, leaving the bulk spectrum intact. (3) Low-rank and shallow recovery is effective: restoring singular vector directions for the top 20% of values or first 25% of layers recovers 70-80% of OOD performance. (4) Stronger SFT checkpoints enable better recovery by RL, while overfitted ones resist restoration. These results reconcile prior reports of RL superior OOD performance: RL primarily counteracts SFT-induced directional drift rather than finding new solutions. Our spectrum-aware analysis highlights inexpensive recovery knobs low-rank UV merging and shallow-layer resets that practitioners can use before costly RL fine-tuning.

**Link**: [arxiv](http://arxiv.org/abs/2508.16546v1),  [pdf](http://arxiv.org/pdf/2508.16546v1)

**Tags**: cs.LG cs.AI 



### FACET: Teacher-Centred LLM-Based Multi-Agent Systems-Towards   Personalized Educational Worksheets
**Authors**: Jana Gonnermann-Müller, Jennifer Haase, Konstantin Fackeldey, Sebastian Pokutta

**Updated**: 2025-08-22T17:10:33Z

**Summary**: The increasing heterogeneity of student populations poses significant challenges for teachers, particularly in mathematics education, where cognitive, motivational, and emotional differences strongly influence learning outcomes. While AI-driven personalization tools have emerged, most remain performance-focused, offering limited support for teachers and neglecting broader pedagogical needs. This paper presents the FACET framework, a teacher-facing, large language model (LLM)-based multi-agent system designed to generate individualized classroom materials that integrate both cognitive and motivational dimensions of learner profiles. The framework comprises three specialized agents: (1) learner agents that simulate diverse profiles incorporating topic proficiency and intrinsic motivation, (2) a teacher agent that adapts instructional content according to didactical principles, and (3) an evaluator agent that provides automated quality assurance. We tested the system using authentic grade 8 mathematics curriculum content and evaluated its feasibility through a) automated agent-based assessment of output quality and b) exploratory feedback from K-12 in-service teachers. Results from ten internal evaluations highlighted high stability and alignment between generated materials and learner profiles, and teacher feedback particularly highlighted structure and suitability of tasks. The findings demonstrate the potential of multi-agent LLM architectures to provide scalable, context-aware personalization in heterogeneous classroom settings, and outline directions for extending the framework to richer learner profiles and real-world classroom trials.

**Link**: [arxiv](http://arxiv.org/abs/2508.11401v2),  [pdf](http://arxiv.org/pdf/2508.11401v2)

**Tags**: cs.HC cs.MA 



### Can Large Language Models Simulate Human Responses? A Case Study of   Stated Preference Experiments in the Context of Heating-related Choices
**Authors**: Han Wang, Jacek Pawlak, Aruna Sivakumar

**Updated**: 2025-08-22T17:01:56Z

**Summary**: Stated preference (SP) surveys are a key method to research how individuals make trade-offs in hypothetical, also futuristic, scenarios. In energy context this includes key decarbonisation enablement contexts, such as low-carbon technologies, distributed renewable energy generation, and demand-side response [1,2]. However, they tend to be costly, time-consuming, and can be affected by respondent fatigue and ethical constraints. Large language models (LLMs) have demonstrated remarkable capabilities in generating human-like textual responses, prompting growing interest in their application to survey research. This study investigates the use of LLMs to simulate consumer choices in energy-related SP surveys and explores their integration into data analysis workflows. A series of test scenarios were designed to systematically assess the simulation performance of several LLMs (LLaMA 3.1, Mistral, GPT-3.5 and DeepSeek-R1) at both individual and aggregated levels, considering contexts factors such as prompt design, in-context learning (ICL), chain-of-thought (CoT) reasoning, LLM types, integration with traditional choice models, and potential biases. Cloud-based LLMs do not consistently outperform smaller local models. In this study, the reasoning model DeepSeek-R1 achieves the highest average accuracy (77%) and outperforms non-reasoning LLMs in accuracy, factor identification, and choice distribution alignment. Across models, systematic biases are observed against the gas boiler and no-retrofit options, with a preference for more energy-efficient alternatives. The findings suggest that previous SP choices are the most effective input factor, while longer prompts with additional factors and varied formats can cause LLMs to lose focus, reducing accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2503.10652v3),  [pdf](http://arxiv.org/pdf/2503.10652v3)

**Tags**: cs.CL cs.AI cs.CY 



### A Simple "Try Again" Can Elicit Multi-Turn LLM Reasoning
**Authors**: Licheng Liu, Zihan Wang, Linjie Li, Chenwei Xu, Yiping Lu, Han Liu, Avirup Sil, Manling Li

**Updated**: 2025-08-22T16:49:10Z

**Summary**: Multi-turn problem solving is critical yet challenging for Large Reasoning Models (LRMs) to reflect on their reasoning and revise from feedback. Existing Reinforcement Learning (RL) methods train large reasoning models on a single-turn paradigm with verifiable rewards. However, we observe that models trained with existing RL paradigms often lose their ability to solve problems across multiple turns and struggle to revise answers based on contextual feedback, leading to repetitive responses. We ask: can LRMs learn to reflect their answers in a multi-turn context? In this work, we find that training models with multi-turn RL using only unary feedback (e.g., "Let's try again") after wrong answers can improve both single-turn performance and multi-turn reasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement learning, which uses minimal yet common unary user feedback during iterative problem solving. It can be easily applied to existing single-turn RL training setups. Experimental results show that RL training with UFO keeps single-turn performance and improves multi-turn reasoning accuracy by up to 14%, enabling language models to better react to feedback in multi-turn problem solving. To further minimize the number of turns needed for a correct answer while encouraging diverse reasoning when mistakes occur, we design reward structures that guide models to produce careful and deliberate answers in each turn. Code: https://github.com/lichengliu03/unary-feedback

**Link**: [arxiv](http://arxiv.org/abs/2507.14295v2),  [pdf](http://arxiv.org/pdf/2507.14295v2)

**Tags**: cs.LG cs.AI 



### ARSP: Automated Repair of Verilog Designs via Semantic Partitioning
**Authors**: Bingkun Yao, Ning Wang, Xiangfeng Liu, Yuxin Du, Yuchen Hu, Hong Gao, Zhe Jiang, Nan Guan

**Updated**: 2025-08-22T16:40:17Z

**Summary**: Debugging functional Verilog bugs consumes a significant portion of front-end design time. While Large Language Models (LLMs) have demonstrated great potential in mitigating this effort, existing LLM-based automated debugging methods underperform on industrial-scale modules. A major reason for this is bug signal dilution in long contexts, where a few bug-relevant tokens are overwhelmed by hundreds of unrelated lines, diffusing the model's attention. To address this issue, we introduce ARSP, a two-stage system that mitigates dilution via semantics-guided fragmentation. A Partition LLM splits a module into semantically tight fragments; a Repair LLM patches each fragment; edits are merged without altering unrelated logic. A synthetic data framework generates fragment-level training pairs spanning bug types, design styles, and scales to supervise both models. Experiments show that ARSP achieves 77.92% pass@1 and 83.88% pass@5, outperforming mainstream commercial LLMs including Claude-3.7 and SOTA automated Verilog debugging tools Strider and MEIC. Also, semantic partitioning improves pass@1 by 11.6% and pass@5 by 10.2% over whole-module debugging, validating the effectiveness of fragment-level scope reduction in LLM-based Verilog debugging.

**Link**: [arxiv](http://arxiv.org/abs/2508.16517v1),  [pdf](http://arxiv.org/pdf/2508.16517v1)

**Tags**: cs.SE cs.PL 



### A Node-Aware Dynamic Quantization Approach for Graph Collaborative   Filtering
**Authors**: Lin Li, Chunyang Li, Yu Yin, Xiaohui Tao, Jianwei Zhang

**Updated**: 2025-08-22T16:39:53Z

**Summary**: In the realm of collaborative filtering recommendation systems, Graph Neural Networks (GNNs) have demonstrated remarkable performance but face significant challenges in deployment on resource-constrained edge devices due to their high embedding parameter requirements and computational costs. Using common quantization method directly on node embeddings may overlooks their graph based structure, causing error accumulation during message passing and degrading the quality of quantized embeddings.To address this, we propose Graph based Node-Aware Dynamic Quantization training for collaborative filtering (GNAQ), a novel quantization approach that leverages graph structural information to enhance the balance between efficiency and accuracy of GNNs for Top-K recommendation. GNAQ introduces a node-aware dynamic quantization strategy that adapts quantization scales to individual node embeddings by incorporating graph interaction relationships. Specifically, it initializes quantization intervals based on node-wise feature distributions and dynamically refines them through message passing in GNN layers. This approach mitigates information loss caused by fixed quantization scales and captures hierarchical semantic features in user-item interaction graphs. Additionally, GNAQ employs graph relation-aware gradient estimation to replace traditional straight-through estimators, ensuring more accurate gradient propagation during training. Extensive experiments on four real-world datasets demonstrate that GNAQ outperforms state-of-the-art quantization methods, including BiGeaR and N2UQ, by achieving average improvement in 27.8\% Recall@10 and 17.6\% NDCG@10 under 2-bit quantization. In particular, GNAQ is capable of maintaining the performance of full-precision models while reducing their model sizes by 8 to 12 times; in addition, the training time is twice as fast compared to quantization baseline methods.

**Link**: [arxiv](http://arxiv.org/abs/2508.16516v1),  [pdf](http://arxiv.org/pdf/2508.16516v1)

**Tags**: cs.IR 



### FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the   Data Synthesis Pipeline
**Authors**: Parker Seegmiller, Kartik Mehta, Soumya Saha, Chenyang Tao, Shereen Oraby, Arpit Gupta, Tagyoung Chung, Mohit Bansal, Nanyun Peng

**Updated**: 2025-08-22T16:37:40Z

**Summary**: Recent works improving LLM math reasoning with synthetic data have used unique setups, making comparison of data synthesis strategies impractical. This leaves many unanswered questions about the roles of different factors in the synthetic data pipeline, such as the impact of filtering low-quality problems. To address this gap, we introduce FLAMES, a Framework for LLM Assessment of Math rEasoning Data Synthesis, and perform a systematic study of 10 existing data synthesis strategies and multiple other factors impacting the performance of synthetic math reasoning data. Our FLAMES experiments provide several valuable insights about the optimal balance of difficulty and diversity of synthetic data. First, data agents designed to increase problem complexity lead to best improvements on most math metrics. Second, with a fixed data generation budget, keeping higher problem coverage is more important than keeping only problems with reliable solutions. Third, GSM8K- and MATH-based synthetic data can lead to improvements on competition-level benchmarks, showcasing easy-to-hard generalization. Leveraging insights from our FLAMES experiments, we design two novel data synthesis strategies for improving out-of-domain generalization and robustness. Further, we develop the FLAMES dataset, an effective blend of our novel and existing data synthesis strategies, outperforming public datasets on OlympiadBench (+15.7), CollegeMath (+4.5), GSMPlus (+6.5), and MATH (+3.1). Fine-tuning Qwen2.5-Math-7B on the FLAMES dataset achieves 81.4% on MATH, surpassing larger Llama3 405B, GPT-4o and Claude 3.5 Sonnet.

**Link**: [arxiv](http://arxiv.org/abs/2508.16514v1),  [pdf](http://arxiv.org/pdf/2508.16514v1)

**Tags**: cs.LG cs.AI cs.CL 



### How Small is Enough? Empirical Evidence of Quantized Small Language   Models for Automated Program Repair
**Authors**: Kazuki Kusama, Honglin Shu, Masanari Kondo, Yasutaka Kamei

**Updated**: 2025-08-22T16:25:08Z

**Summary**: Background: Large language models (LLMs) have greatly improved the accuracy of automated program repair (APR) methods. However, LLMs are constrained by high computational resource requirements. Aims: We focus on small language models (SLMs), which perform well even with limited computational resources compared to LLMs. We aim to evaluate whether SLMs can achieve competitive performance in APR tasks. Method: We conducted experiments on the QuixBugs benchmark to compare the bug-fixing accuracy of SLMs and LLMs. We also analyzed the impact of int8 quantization on APR performance. Results: The latest SLMs can fix bugs as accurately as--or even more accurately than--LLMs. Also, int8 quantization had minimal effect on APR accuracy while significantly reducing memory requirements. Conclusions: SLMs present a viable alternative to LLMs for APR, offering competitive accuracy with lower computational costs, and quantization can further enhance their efficiency without compromising effectiveness.

**Link**: [arxiv](http://arxiv.org/abs/2508.16499v1),  [pdf](http://arxiv.org/pdf/2508.16499v1)

**Tags**: cs.SE 



### Not Only Consistency: Enhance Test-Time Adaptation with Spatio-temporal   Inconsistency for Remote Physiological Measurement
**Authors**: Xiao Yang, Jiyao Wang, Yuxuan Fan, Can Liu, Houcheng Su, Weichen Guo, Zitong Yu, Dengbo He, Kaishun Wu

**Updated**: 2025-08-22T16:20:32Z

**Summary**: Remote physiological measurement (RPM) has emerged as a promising non-invasive method for monitoring physiological signals using the non-contact device. Although various domain adaptation and generalization methods were proposed to promote the adaptability of deep-based RPM models in unseen deployment environments, considerations in aspects such as privacy concerns and real-time adaptation restrict their application in real-world deployment. Thus, we aim to propose a novel fully Test-Time Adaptation (TTA) strategy tailored for RPM tasks in this work. Specifically, based on prior knowledge in physiology and our observations, we noticed not only there is spatio-temporal consistency in the frequency domain of BVP signals, but also that inconsistency in the time domain was significant. Given this, by leveraging both consistency and inconsistency priors, we introduce an innovative expert knowledge-based self-supervised \textbf{C}onsistency-\textbf{i}n\textbf{C}onsistency-\textbf{i}ntegration (\textbf{CiCi}) framework to enhances model adaptation during inference. Besides, our approach further incorporates a gradient dynamic control mechanism to mitigate potential conflicts between priors, ensuring stable adaptation across instances. Through extensive experiments on five diverse datasets under the TTA protocol, our method consistently outperforms existing techniques, presenting state-of-the-art performance in real-time self-supervised adaptation without accessing source data. The code will be released later.

**Link**: [arxiv](http://arxiv.org/abs/2507.07908v2),  [pdf](http://arxiv.org/pdf/2507.07908v2)

**Tags**: cs.CV 



### Post Hoc Regression Refinement via Pairwise Rankings
**Authors**: Kevin Tirta Wijaya, Michael Sun, Minghao Guo, Hans-Peter Seidel, Wojciech Matusik, Vahid Babaei

**Updated**: 2025-08-22T16:17:31Z

**Summary**: Accurate prediction of continuous properties is essential to many scientific and engineering tasks. Although deep-learning regressors excel with abundant labels, their accuracy deteriorates in data-scarce regimes. We introduce RankRefine, a model-agnostic, plug-and-play post hoc method that refines regression with expert knowledge coming from pairwise rankings. Given a query item and a small reference set with known properties, RankRefine combines the base regressor's output with a rank-based estimate via inverse variance weighting, requiring no retraining. In molecular property prediction task, RankRefine achieves up to 10% relative reduction in mean absolute error using only 20 pairwise comparisons obtained through a general-purpose large language model (LLM) with no finetuning. As rankings provided by human experts or general-purpose LLMs are sufficient for improving regression across diverse domains, RankRefine offers practicality and broad applicability, especially in low-data settings.

**Link**: [arxiv](http://arxiv.org/abs/2508.16495v1),  [pdf](http://arxiv.org/pdf/2508.16495v1)

**Tags**: cs.LG cs.AI 



### AutoVerus: Automated Proof Generation for Rust Code
**Authors**: Chenyuan Yang, Xuheng Li, Md Rakib Hossain Misu, Jianan Yao, Weidong Cui, Yeyun Gong, Chris Hawblitzel, Shuvendu Lahiri, Jacob R. Lorch, Shuai Lu, Fan Yang, Ziqiao Zhou, Shan Lu

**Updated**: 2025-08-22T16:12:45Z

**Summary**: Generative AI has shown its values for many software engineering tasks. Still in its infancy, large language model (LLM)-based proof generation lags behind LLM-based code generation. In this paper, we present AutoVerus. AutoVerus uses LLMs to automatically generate correctness proof for Rust code. AutoVerus is designed to match the unique features of Verus, a verification tool that can prove the correctness of Rust code using proofs and specifications also written in Rust. AutoVerus consists of a network of LLM agents that are crafted and orchestrated to mimic human experts' three phases of proof construction: preliminary proof generation, proof refinement guided by generic tips, and proof debugging guided by verification errors. To thoroughly evaluate AutoVerus and help foster future research in this direction, we have built a benchmark suite of 150 non-trivial proof tasks, based on existing code-generation benchmarks and verification benchmarks. Our evaluation shows that AutoVerus can automatically generate correct proof for more than 90% of them, with more than half of them tackled in less than 30 seconds or 3 LLM calls.

**Link**: [arxiv](http://arxiv.org/abs/2409.13082v3),  [pdf](http://arxiv.org/pdf/2409.13082v3)

**Tags**: cs.SE cs.AI cs.FL 



### MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds
**Authors**: Bingquan Dai, Li Ray Luo, Qihong Tang, Jie Wang, Xinyu Lian, Hao Xu, Minghan Qin, Xudong Xu, Bo Dai, Haoqian Wang, Zhaoyang Lyu, Jiangmiao Pang

**Updated**: 2025-08-22T16:12:04Z

**Summary**: Reconstructing 3D objects into editable programs is pivotal for applications like reverse engineering and shape editing. However, existing methods often rely on limited domain-specific languages (DSLs) and small-scale datasets, restricting their ability to model complex geometries and structures. To address these challenges, we introduce MeshCoder, a novel framework that reconstructs complex 3D objects from point clouds into editable Blender Python scripts. We develop a comprehensive set of expressive Blender Python APIs capable of synthesizing intricate geometries. Leveraging these APIs, we construct a large-scale paired object-code dataset, where the code for each object is decomposed into distinct semantic parts. Subsequently, we train a multimodal large language model (LLM) that translates 3D point cloud into executable Blender Python scripts. Our approach not only achieves superior performance in shape-to-code reconstruction tasks but also facilitates intuitive geometric and topological editing through convenient code modifications. Furthermore, our code-based representation enhances the reasoning capabilities of LLMs in 3D shape understanding tasks. Together, these contributions establish MeshCoder as a powerful and flexible solution for programmatic 3D shape reconstruction and understanding. The project homepage is available at \href{https://daibingquan.github.io/MeshCoder}{this link}.

**Link**: [arxiv](http://arxiv.org/abs/2508.14879v2),  [pdf](http://arxiv.org/pdf/2508.14879v2)

**Tags**: cs.GR cs.CV 



### Enhancing Code-switched Text-to-Speech Synthesis Capability in Large   Language Models with only Monolingual Corpora
**Authors**: Jing Xu, Daxin Tan, Jiaqi Wang, Xiao Chen

**Updated**: 2025-08-22T16:11:58Z

**Summary**: While Large Language Models (LLMs) have shown potential in speech generation and recognition, their applications are mainly confined to monolingual scenarios, with limited explorations in code-switched (CS) contexts. In this paper, we propose a Code-Switched Large Language Model (CS-LLM) to enhance the code-switched text-to-speech synthesis (CS TTS) capability in LLMs with only monolingual corpora. Specifically, we begin by enhancing the multilingual speech processing ability of LLMs through multilingual speech recognition and synthesis tasks. Then, we develop an effective code-switched (CS) data construction strategy that splits and concatenates words from different monolingual speech corpora to equip LLMs with improved CS TTS ability. Experiments show that our approach outperforms baselines in CS TTS in terms of naturalness, speaker consistency and similarity even with limited data. Additionally, the constructed CS data further improves multilingual speech synthesis and recognition.

**Link**: [arxiv](http://arxiv.org/abs/2409.10969v2),  [pdf](http://arxiv.org/pdf/2409.10969v2)

**Tags**: eess.AS cs.CL cs.SD 



### Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy,   Expertise, and Reasoning
**Authors**: Chongyuan Dai, Jinpeng Hu, Hongchang Shi, Zhuo Li, Xun Yang, Meng Wang

**Updated**: 2025-08-22T16:11:37Z

**Summary**: Amidst a shortage of qualified mental health professionals, the integration of large language models (LLMs) into psychological applications offers a promising way to alleviate the growing burden of mental health disorders. Recent reasoning-augmented LLMs have achieved remarkable performance in mathematics and programming, while research in the psychological domain has predominantly emphasized emotional support and empathetic dialogue, with limited attention to reasoning mechanisms that are beneficial to generating reliable responses. Therefore, in this paper, we propose Psyche-R1, the first Chinese psychological LLM that jointly integrates empathy, psychological expertise, and reasoning, built upon a novel data curation pipeline. Specifically, we design a comprehensive data synthesis pipeline that produces over 75k high-quality psychological questions paired with detailed rationales, generated through chain-of-thought (CoT) reasoning and iterative prompt-rationale optimization, along with 73k empathetic dialogues. Subsequently, we employ a hybrid training strategy wherein challenging samples are identified through a multi-LLM cross-selection strategy for group relative policy optimization (GRPO) to improve reasoning ability, while the remaining data is used for supervised fine-tuning (SFT) to enhance empathetic response generation and psychological domain knowledge. Extensive experiment results demonstrate the effectiveness of the Psyche-R1 across several psychological benchmarks, where our 7B Psyche-R1 achieves comparable results to 671B DeepSeek-R1.

**Link**: [arxiv](http://arxiv.org/abs/2508.10848v2),  [pdf](http://arxiv.org/pdf/2508.10848v2)

**Tags**: cs.CL 



### SafeSpace: An Integrated Web Application for Digital Safety and   Emotional Well-being
**Authors**: Kayenat Fatmi, Mohammad Abbas

**Updated**: 2025-08-22T16:07:29Z

**Summary**: In the digital era, individuals are increasingly exposed to online harms such as toxicity, manipulation, and grooming, which often pose emotional and safety risks. Existing systems for detecting abusive content or issuing safety alerts operate in isolation and rarely combine digital safety with emotional well-being. In this paper, we present SafeSpace, a unified web application that integrates three modules: (1) toxicity detection in chats and screenshots using NLP models and Google's Perspective API, (2) a configurable safety ping system that issues emergency alerts with the user's live location (longitude and latitude) via SMTP-based emails when check-ins are missed or SOS alerts are manually triggered, and (3) a reflective questionnaire that evaluates relationship health and emotional resilience. The system employs Firebase for alert management and a modular architecture designed for usability, privacy, and scalability. The experimental evaluation shows 93% precision in toxicity detection, 100% reliability in safety alerts under emulator tests, and 92% alignment between automated and manual questionnaire scoring. SafeSpace, implemented as a web application, demonstrates the feasibility of integrating detection, protection, and reflection within a single platform, with future deployment envisioned as a mobile application for broader accessibility.

**Link**: [arxiv](http://arxiv.org/abs/2508.16488v1),  [pdf](http://arxiv.org/pdf/2508.16488v1)

**Tags**: cs.HC cs.AI cs.CY 



### Hyper Yoshimura: How a slight tweak on a classical folding pattern   unleashes meta-stability for deployable robots
**Authors**: Ziyang Zhou, Yogesh Phalak, Vishrut Deshpande, Ethan O'Brien, Ian Walker, Suyi Li

**Updated**: 2025-08-22T16:04:28Z

**Summary**: Deployable structures inspired by origami have provided lightweight, compact, and reconfigurable solutions for various robotic and architectural applications. However, creating an integrated structural system that can effectively balance the competing requirements of high packing efficiency, simple deployment, and precise morphing into multiple load-bearing configurations remains a significant challenge. This study introduces a new class of hyper-Yoshimura origami, which exhibits a wide range of kinematically admissible and locally metastable states, including newly discovered symmetric "self-packing" and asymmetric "pop-out" states. This metastability is achieved by breaking a design rule of Yoshimura origami that has been in place for many decades. To this end, this study derives a new set of mathematically rigorous design rules and geometric formulations. Based on this, forward and inverse kinematic strategies are developed to stack hyper-Yoshimura modules into deployable booms that can approximate complex 3D shapes. Finally, this study showcases the potential of hyper-Yoshimura with a meter-scale pop-up cellphone charging station deployed at our university's bus transit station, along with a 3D-printed, scaled prototype of a space crane that can function as an object manipulator, solar tracking device, or high-load-bearing structure. These results establish hyper-Yoshimura as a promising platform for deployable and adaptable robotic systems in both terrestrial and space environments.

**Link**: [arxiv](http://arxiv.org/abs/2505.09919v2),  [pdf](http://arxiv.org/pdf/2505.09919v2)

**Tags**: cs.RO cs.SY eess.SY 



### Is Small Language Model the Silver Bullet to Low-Resource Languages   Machine Translation?
**Authors**: Yewei Song, Lujun Li, Cedric Lothritz, Saad Ezzini, Lama Sleem, Niccolo Gentile, Radu State, Tegawendé F. Bissyandé, Jacques Klein

**Updated**: 2025-08-22T15:58:31Z

**Summary**: Low-resource languages (LRLs) lack sufficient linguistic resources and are underrepresented in benchmark datasets, resulting in persistently lower translation quality than high-resource languages, especially in privacy-sensitive and resource-limited contexts. Firstly, this study systematically evaluates state-of-the-art smaller Large Language Models in 200 languages using the FLORES-200 benchmark, highlighting persistent deficiencies and disparities in the translation of LRLs. To mitigate these limitations, we investigate knowledge distillation from large pre-trained teacher models to Small Language Models (SLMs) through supervised fine-tuning. The results show substantial improvements; for example, the translation performance of English to Luxembourgish (EN to LB), measured by the LLM-as-a-Judge score, increases from 0.36 to 0.89 in the validation set for Llama-3.2-3B. We further investigate various fine-tuning configurations and tasks to clarify the trade-offs between data scale and training efficiency, verify that the model retains its general capabilities without significant catastrophic forgetting after training, and explore the distillation benefits to other LRLs on SLMs (Khasi, Assamese, and Ukrainian). In general, this work exposes the limitations and fairness issues of current SLMs in LRL translation and systematically explores the potential of using the distillation of knowledge from large to small models, offering practical, empirically grounded recommendations to improve LRL translation systems

**Link**: [arxiv](http://arxiv.org/abs/2503.24102v3),  [pdf](http://arxiv.org/pdf/2503.24102v3)

**Tags**: cs.CL 



### HAMSA: Hijacking Aligned Compact Models via Stealthy Automation
**Authors**: Alexey Krylov, Iskander Vagizov, Dmitrii Korzh, Maryam Douiba, Azidine Guezzaz, Vladimir Kokh, Sergey D. Erokhin, Elena V. Tutubalina, Oleg Y. Rogov

**Updated**: 2025-08-22T15:57:57Z

**Summary**: Large Language Models (LLMs), especially their compact efficiency-oriented variants, remain susceptible to jailbreak attacks that can elicit harmful outputs despite extensive alignment efforts. Existing adversarial prompt generation techniques often rely on manual engineering or rudimentary obfuscation, producing low-quality or incoherent text that is easily flagged by perplexity-based filters. We present an automated red-teaming framework that evolves semantically meaningful and stealthy jailbreak prompts for aligned compact LLMs. The approach employs a multi-stage evolutionary search, where candidate prompts are iteratively refined using a population-based strategy augmented with temperature-controlled variability to balance exploration and coherence preservation. This enables the systematic discovery of prompts capable of bypassing alignment safeguards while maintaining natural language fluency. We evaluate our method on benchmarks in English (In-The-Wild Jailbreak Prompts on LLMs), and a newly curated Arabic one derived from In-The-Wild Jailbreak Prompts on LLMs and annotated by native Arabic linguists, enabling multilingual assessment.

**Link**: [arxiv](http://arxiv.org/abs/2508.16484v1),  [pdf](http://arxiv.org/pdf/2508.16484v1)

**Tags**: cs.CL 



### Benchmarking the Robustness of Agentic Systems to Adversarially-Induced   Harms
**Authors**: Jonathan Nöther, Adish Singla, Goran Radanovic

**Updated**: 2025-08-22T15:53:22Z

**Summary**: Ensuring the safe use of agentic systems requires a thorough understanding of the range of malicious behaviors these systems may exhibit when under attack. In this paper, we evaluate the robustness of LLM-based agentic systems against attacks that aim to elicit harmful actions from agents. To this end, we propose a novel taxonomy of harms for agentic systems and a novel benchmark, BAD-ACTS, for studying the security of agentic systems with respect to a wide range of harmful actions. BAD-ACTS consists of 4 implementations of agentic systems in distinct application environments, as well as a dataset of 188 high-quality examples of harmful actions. This enables a comprehensive study of the robustness of agentic systems across a wide range of categories of harmful behaviors, available tools, and inter-agent communication structures. Using this benchmark, we analyze the robustness of agentic systems against an attacker that controls one of the agents in the system and aims to manipulate other agents to execute a harmful target action. Our results show that the attack has a high success rate, demonstrating that even a single adversarial agent within the system can have a significant impact on the security. This attack remains effective even when agents use a simple prompting-based defense strategy. However, we additionally propose a more effective defense based on message monitoring. We believe that this benchmark provides a diverse testbed for the security research of agentic systems. The benchmark can be found at github.com/JNoether/BAD-ACTS

**Link**: [arxiv](http://arxiv.org/abs/2508.16481v1),  [pdf](http://arxiv.org/pdf/2508.16481v1)

**Tags**: cs.LG 



### LLM-as-classifier: Semi-Supervised, Iterative Framework for Hierarchical   Text Classification using Large Language Models
**Authors**: Doohee You, Andy Parisi, Zach Vander Velden, Lara Dantas Inojosa

**Updated**: 2025-08-22T15:47:17Z

**Summary**: The advent of Large Language Models (LLMs) has provided unprecedented capabilities for analyzing unstructured text data. However, deploying these models as reliable, robust, and scalable classifiers in production environments presents significant methodological challenges. Standard fine-tuning approaches can be resource-intensive and often struggle with the dynamic nature of real-world data distributions, which is common in the industry. In this paper, we propose a comprehensive, semi-supervised framework that leverages the zero- and few-shot capabilities of LLMs for building hierarchical text classifiers as a framework for a solution to these industry-wide challenges. Our methodology emphasizes an iterative, human-in-the-loop process that begins with domain knowledge elicitation and progresses through prompt refinement, hierarchical expansion, and multi-faceted validation. We introduce techniques for assessing and mitigating sequence-based biases and outline a protocol for continuous monitoring and adaptation. This framework is designed to bridge the gap between the raw power of LLMs and the practical need for accurate, interpretable, and maintainable classification systems in industry applications.

**Link**: [arxiv](http://arxiv.org/abs/2508.16478v1),  [pdf](http://arxiv.org/pdf/2508.16478v1)

**Tags**: cs.CL cs.IR 



### Your Reward Function for RL is Your Best PRM for Search: Unifying RL and   Search-Based TTS
**Authors**: Can Jin, Yang Zhou, Qixin Zhang, Hongwu Peng, Di Zhang, Marco Pavone, Ligong Han, Zhang-Wei Hong, Tong Che, Dimitris N. Metaxas

**Updated**: 2025-08-22T15:37:12Z

**Summary**: Test-time scaling (TTS) for large language models (LLMs) has thus far fallen into two largely separate paradigms: (1) reinforcement learning (RL) methods that optimize sparse outcome-based rewards, yet suffer from instability and low sample efficiency; and (2) search-based techniques guided by independently trained, static process reward models (PRMs), which require expensive human- or LLM-generated labels and often degrade under distribution shifts. In this paper, we introduce AIRL-S, the first natural unification of RL-based and search-based TTS. Central to AIRL-S is the insight that the reward function learned during RL training inherently represents the ideal PRM for guiding downstream search. Specifically, we leverage adversarial inverse reinforcement learning (AIRL) combined with group relative policy optimization (GRPO) to learn a dense, dynamic PRM directly from correct reasoning traces, entirely eliminating the need for labeled intermediate process data. At inference, the resulting PRM simultaneously serves as the critic for RL rollouts and as a heuristic to effectively guide search procedures, facilitating robust reasoning chain extension, mitigating reward hacking, and enhancing cross-task generalization. Experimental results across eight benchmarks, including mathematics, scientific reasoning, and code generation, demonstrate that our unified approach improves performance by 9 % on average over the base model, matching GPT-4o. Furthermore, when integrated into multiple search algorithms, our PRM consistently outperforms all baseline PRMs trained with labeled data. These results underscore that, indeed, your reward function for RL is your best PRM for search, providing a robust and cost-effective solution to complex reasoning tasks in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2508.14313v2),  [pdf](http://arxiv.org/pdf/2508.14313v2)

**Tags**: cs.LG cs.AI 



### MCP-Guard: A Defense Framework for Model Context Protocol Integrity in   Large Language Model Applications
**Authors**: Wenpeng Xing, Zhonghao Qi, Yupeng Qin, Yilin Li, Caini Chang, Jiahui Yu, Changting Lin, Zhenzhen Xie, Meng Han

**Updated**: 2025-08-22T15:35:04Z

**Summary**: The integration of Large Language Models (LLMs) with external tools via protocols such as the Model Context Protocol (MCP) introduces critical security vulnerabilities, including prompt injection, data exfiltration, and other threats. To counter these challenges, we propose MCP-Guard, a robust, layered defense architecture designed for LLM--tool interactions. MCP-Guard employs a three-stage detection pipeline that balances efficiency with accuracy: it progresses from lightweight static scanning for overt threats and a deep neural detector for semantic attacks, to our fine-tuned E5-based model achieves (96.01) accuracy in identifying adversarial prompts. Finally, a lightweight LLM arbitrator synthesizes these signals to deliver the final decision while minimizing false positives. To facilitate rigorous training and evaluation, we also introduce MCP-AttackBench, a comprehensive benchmark of over 70,000 samples. Sourced from public datasets and augmented by GPT-4, MCP-AttackBench simulates diverse, real-world attack vectors in the MCP format, providing a foundation for future research into securing LLM-tool ecosystems.

**Link**: [arxiv](http://arxiv.org/abs/2508.10991v2),  [pdf](http://arxiv.org/pdf/2508.10991v2)

**Tags**: cs.CR cs.AI 



### Wide-Area Power System Oscillations from Large-Scale AI Workloads
**Authors**: Min-Seung Ko, Hao Zhu

**Updated**: 2025-08-22T15:18:50Z

**Summary**: This paper develops a new dynamic power profiling approach for modeling AI-centric datacenter loads and analyzing their impact on grid operations, particularly their potential to induce wide-area grid oscillations. We characterize the periodic stochastic power fluctuations inherent to large-scale AI workloads during both the training and fine-tuning stages, driven by the state-of-the-art GPU computing architecture designs. These sustained, large power fluctuations, unlike conventional load ramping, act as persistent forcing inputs capable of interacting with and amplifying local and inter-area oscillation modes. Using the WECC 179-bus system as a test case, we examine the amplitude and variability of oscillatory responses under different factors, ranging from system strength, penetration level, fluctuation frequency range, individual datacenter size, to geographical deployment. Simulation results show that, notably, narrower fluctuation bands, larger single-site capacities, or dispersed siting can intensify oscillations across multiple modes. Our models and numerical studies provide a quantitative basis for integrating AI-dominant electricity demands into grid oscillation studies, and further support the development of new planning and operational measures to power the continuous AI load growth.

**Link**: [arxiv](http://arxiv.org/abs/2508.16457v1),  [pdf](http://arxiv.org/pdf/2508.16457v1)

**Tags**: eess.SY cs.SY 



### A Probabilistic Inference Scaling Theory for LLM Self-Correction
**Authors**: Zhe Yang, Yichang Zhang, Yudong Wang, Ziyao Xu, Junyang Lin, Zhifang Sui

**Updated**: 2025-08-22T15:15:38Z

**Summary**: Large Language Models (LLMs) have demonstrated the capability to refine their generated answers through self-correction, enabling continuous performance improvement over multiple rounds. However, the mechanisms underlying how and why accuracy evolves during this iterative process remain unexplored. To fill this gap, we propose a probabilistic theory to model the dynamics of accuracy change and explain the performance improvements observed in multi-round self-correction. Through mathematical derivation, we establish that the accuracy after the $t^{th}$ round of self-correction is given by: $Acc_t = Upp - \alpha^t(Upp - Acc_0),$ where $Acc_0$ denotes the initial accuracy, $Upp$ represents the upper bound of accuracy convergence, and $\alpha$ determines the rate of convergence. Based on our theory, these parameters can be calculated and the predicted accuracy curve then can be obtained through only a single round of self-correction. Extensive experiments across diverse models and datasets demonstrate that our theoretical predictions align closely with empirical accuracy curves, validating the effectiveness of the theory. Our work provides a theoretical foundation for understanding LLM self-correction, thus paving the way for further explorations.

**Link**: [arxiv](http://arxiv.org/abs/2508.16456v1),  [pdf](http://arxiv.org/pdf/2508.16456v1)

**Tags**: cs.CL 



### GreenLLM: SLO-Aware Dynamic Frequency Scaling for Energy-Efficient LLM   Serving
**Authors**: Qunyou Liu, Darong Huang, Marina Zapater, David Atienza

**Updated**: 2025-08-22T15:08:34Z

**Summary**: Large Language Models (LLMs) are becoming the backbone of modern cloud services, yet their inference costs are dominated by GPU energy. Unlike traditional GPU workloads, LLM inference has two stages with different characteristics: the prefill phase, which is latency sensitive and scales quadratically with prompt length, and the decode phase, which progresses token by token with unpredictable length. Current GPU power governors (for example, NVIDIA's default) overlook this asymmetry and treat both stages uniformly. The result is mismatched voltage and frequency settings, head-of-line blocking, and excessive energy use.   We introduce GreenLLM, an SLO-aware serving framework that minimizes GPU energy by explicitly separating prefill and decode control. At ingress, requests are routed into length-based queues so short prompts avoid head-of-line blocking and TTFT improves. For prefill, GreenLLM collects short traces on a GPU node, fits compact latency-power models over SM frequency, and solves a queueing-aware optimization to select energy-minimal clocks per class. During decode, a lightweight dual-loop controller tracks throughput (tokens per second) and adjusts frequency with hysteretic, fine-grained steps to hold tail TBT within target bounds. Across Alibaba and Azure trace replays, GreenLLM reduces total energy by up to 34 percent versus the default DVFS baseline, with no loss of throughput and with less than 3.5 percent additional SLO violations.

**Link**: [arxiv](http://arxiv.org/abs/2508.16449v1),  [pdf](http://arxiv.org/pdf/2508.16449v1)

**Tags**: cs.PF 



### Boardwalk: Towards a Framework for Creating Board Games with LLMs
**Authors**: Álvaro Guglielmin Becker, Gabriel Bauer de Oliveira, Lana Bertoldo Rossato, Anderson Rocha Tavares

**Updated**: 2025-08-22T15:02:07Z

**Summary**: Implementing board games in code can be a time-consuming task. However, Large Language Models (LLMs) have been proven effective at generating code for domain-specific tasks with simple contextual information. We aim to investigate whether LLMs can implement digital versions of board games from rules described in natural language. This would be a step towards an LLM-assisted framework for quick board game code generation. We expect to determine the main challenges for LLMs to implement the board games, and how different approaches and models compare to one another. We task three state-of-the-art LLMs (Claude, DeepSeek and ChatGPT) with coding a selection of 12 popular and obscure games in free-form and within Boardwalk, our proposed General Game Playing API. We anonymize the games and components to avoid evoking pre-trained LLM knowledge. The implementations are tested for playability and rule compliance. We evaluate success rate and common errors across LLMs and game popularity. Our approach proves viable, with the best performing model, Claude 3.7 Sonnet, yielding 55.6\% of games without any errors. While compliance with the API increases error frequency, the severity of errors is more significantly dependent on the LLM. We outline future steps for creating a framework to integrate this process, making the elaboration of board games more accessible.

**Link**: [arxiv](http://arxiv.org/abs/2508.16447v1),  [pdf](http://arxiv.org/pdf/2508.16447v1)

**Tags**: cs.LG 



### Structured Prompts, Better Outcomes? Exploring the Effects of a   Structured Interface with ChatGPT in a Graduate Robotics Course
**Authors**: Jerome Brender, Laila El-Hamamsy, Kim Uittenhove, Francesco Mondada, Engin Bumbacher

**Updated**: 2025-08-22T15:01:14Z

**Summary**: Prior research shows that how students engage with Large Language Models (LLMs) influences their problem-solving and understanding, reinforcing the need to support productive LLM-uses that promote learning. This study evaluates the impact of a structured GPT platform designed to promote 'good' prompting behavior with data from 58 students in a graduate-level robotics course. The students were assigned to either an intervention group using the structured platform or a control group using ChatGPT freely for two practice lab sessions, before a third session where all students could freely use ChatGPT. We analyzed student perception (pre-post surveys), prompting behavior (logs), performance (task scores), and learning (pre-post tests). Although we found no differences in performance or learning between groups, we identified prompting behaviors - such as having clear prompts focused on understanding code - that were linked with higher learning gains and were more prominent when students used the structured platform. However, such behaviors did not transfer once students were no longer constrained to use the structured platform. Qualitative survey data showed mixed perceptions: some students perceived the value of the structured platform, but most did not perceive its relevance and resisted changing their habits. These findings contribute to ongoing efforts to identify effective strategies for integrating LLMs into learning and question the effectiveness of bottom-up approaches that temporarily alter user interfaces to influence students' interaction. Future research could instead explore top-down strategies that address students' motivations and explicitly demonstrate how certain interaction patterns support learning.

**Link**: [arxiv](http://arxiv.org/abs/2507.07767v2),  [pdf](http://arxiv.org/pdf/2507.07767v2)

**Tags**: cs.CY 



### Using LLMs and Essence to Support Software Practice Adoption
**Authors**: Sonia Nicoletti, Paolo Ciancarini

**Updated**: 2025-08-22T14:59:35Z

**Summary**: Recent advancements in natural language processing (NLP) have enabled the development of automated tools that support various domains, including software engineering. However, while NLP and artificial intelligence (AI) research has extensively focused on tasks such as code generation, less attention has been given to automating support for the adoption of best practices, the evolution of ways of working, and the monitoring of process health. This study addresses this gap by exploring the integration of Essence, a standard and thinking framework for managing software engineering practices, with large language models (LLMs). To this end, a specialised chatbot was developed to assist students and professionals in understanding and applying Essence. The chatbot employs a retrieval-augmented generation (RAG) system to retrieve relevant contextual information from a curated knowledge base. Four different LLMs were used to create multiple chatbot configurations, each evaluated both as a base model and augmented with the RAG system. The system performance was evaluated through both the relevance of retrieved context and the quality of generated responses. Comparative analysis against the general-purpose LLMs demonstrated that the proposed system consistently outperforms its baseline counterpart in domain-specific tasks. By facilitating access to structured software engineering knowledge, this work contributes to bridging the gap between theoretical frameworks and practical application, potentially improving process management and the adoption of software development practices. While further validation through user studies is required, these findings highlight the potential of LLM-based automation to enhance learning and decision-making in software engineering.

**Link**: [arxiv](http://arxiv.org/abs/2508.16445v1),  [pdf](http://arxiv.org/pdf/2508.16445v1)

**Tags**: cs.SE 



### Classification or Prompting: A Case Study on Legal Requirements   Traceability
**Authors**: Romina Etezadi, Sallam Abualhaija, Chetan Arora, Lionel Briand

**Updated**: 2025-08-22T14:52:02Z

**Summary**: New regulations are introduced to ensure software development aligns with ethical concerns and protects public safety. Showing compliance requires tracing requirements to legal provisions. Requirements traceability is a key task where engineers must analyze technical requirements against target artifacts, often within limited time. Manually analyzing complex systems with hundreds of requirements is infeasible. The legal dimension adds challenges that increase effort. In this paper, we investigate two automated solutions based on language models, including large ones (LLMs). The first solution, Kashif, is a classifier that leverages sentence transformers and semantic similarity. The second solution, RICE_LRT, prompts a recent generative LLM based on RICE, a prompt engineering framework. On a benchmark dataset, we empirically evaluate Kashif and compare it against five different baseline classifiers from the literature. Kashif can identify trace links with a recall of 67%, precision of 50%, and F2 score of 63%, outperforming the best baseline by a substantial margin of 41 percentage points (pp) in F2. However, on unseen, more complex requirements documents traced to the European General Data Protection Regulation (GDPR), Kashif performs poorly, yielding an average recall of 15%, an average precision of 10%, and an average F2 score of 13.5%. On the same documents, however, our RICE solution yields an average recall of 84%, an average precision of 30%, and an average F2 score of 61%. RICE achieved a remarkable improvement of 47.5 pp over Kashif in terms of F2 score. Our results suggest that requirements traceability in the legal context cannot be simply addressed by building classifiers, as such solutions do not generalize and fail to perform well on complex regulations and requirements. Resorting to generative LLMs, with careful prompt engineering, is thus a more promising alternative.

**Link**: [arxiv](http://arxiv.org/abs/2502.04916v4),  [pdf](http://arxiv.org/pdf/2502.04916v4)

**Tags**: cs.SE 



### PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark
**Authors**: Adil Bahaj, Mounir Ghogho

**Updated**: 2025-08-22T14:50:55Z

**Summary**: Large language models (LLMs) and vision-augmented LLMs (VLMs) have significantly advanced medical informatics, diagnostics, and decision support. However, these models exhibit systematic biases, particularly age bias, compromising their reliability and equity. This is evident in their poorer performance on pediatric-focused text and visual question-answering tasks. This bias reflects a broader imbalance in medical research, where pediatric studies receive less funding and representation despite the significant disease burden in children. To address these issues, a new comprehensive multi-modal pediatric question-answering benchmark, PediatricsMQA, has been introduced. It consists of 3,417 text-based multiple-choice questions (MCQs) covering 131 pediatric topics across seven developmental stages (prenatal to adolescent) and 2,067 vision-based MCQs using 634 pediatric images from 67 imaging modalities and 256 anatomical regions. The dataset was developed using a hybrid manual-automatic pipeline, incorporating peer-reviewed pediatric literature, validated question banks, existing benchmarks, and existing QA resources. Evaluating state-of-the-art open models, we find dramatic performance drops in younger cohorts, highlighting the need for age-aware methods to ensure equitable AI support in pediatric care.

**Link**: [arxiv](http://arxiv.org/abs/2508.16439v1),  [pdf](http://arxiv.org/pdf/2508.16439v1)

**Tags**: cs.CY cs.AI cs.CL cs.GR cs.MM 



### OPERA: A Reinforcement Learning--Enhanced Orchestrated Planner-Executor   Architecture for Reasoning-Oriented Multi-Hop Retrieval
**Authors**: Yu Liu, Yanbing Liu, Fangfang Yuan, Cong Cao, Youbang Sun, Kun Peng, WeiZhuo Chen, Jianjun Li, Zhiyuan Ma

**Updated**: 2025-08-22T14:50:26Z

**Summary**: Recent advances in large language models (LLMs) and dense retrievers have driven significant progress in retrieval-augmented generation (RAG). However, existing approaches face significant challenges in complex reasoning-oriented multi-hop retrieval tasks: 1) Ineffective reasoning-oriented planning: Prior methods struggle to generate robust multi-step plans for complex queries, as rule-based decomposers perform poorly on out-of-template questions. 2) Suboptimal reasoning-driven retrieval: Related methods employ limited query reformulation, leading to iterative retrieval loops that often fail to locate golden documents. 3) Insufficient reasoning-guided filtering: Prevailing methods lack the fine-grained reasoning to effectively filter salient information from noisy results, hindering utilization of retrieved knowledge. Fundamentally, these limitations all stem from the weak coupling between retrieval and reasoning in current RAG architectures. We introduce the Orchestrated Planner-Executor Reasoning Architecture (OPERA), a novel reasoning-driven retrieval framework. OPERA's Goal Planning Module (GPM) decomposes questions into sub-goals, which are executed by a Reason-Execute Module (REM) with specialized components for precise reasoning and effective retrieval. To train OPERA, we propose Multi-Agents Progressive Group Relative Policy Optimization (MAPGRPO), a novel variant of GRPO. Experiments on complex multi-hop benchmarks show OPERA's superior performance, validating both the MAPGRPO method and OPERA's design. Code is available at https://github.com/Ameame1/OPERA.

**Link**: [arxiv](http://arxiv.org/abs/2508.16438v1),  [pdf](http://arxiv.org/pdf/2508.16438v1)

**Tags**: cs.IR cs.AI 



### Coarse-to-Fine Process Reward Modeling for Mathematical Reasoning
**Authors**: Yulan Hu, Sheng Ouyang, Jinman Zhao, Yong Liu

**Updated**: 2025-08-22T14:48:22Z

**Summary**: The Process Reward Model (PRM) plays a crucial role in mathematical reasoning tasks, requiring high-quality supervised process data. However, we observe that reasoning steps generated by Large Language Models (LLMs) often fail to exhibit strictly incremental information, leading to redundancy that can hinder effective reasoning. To address this issue, we propose CFPRM, a simple yet effective coarse-to-fine strategy. Instead of focusing on the detection of redundant steps, our approach first establishes a coarse-grained window to merge adjacent reasoning steps into unified, holistic steps. The window size is then progressively reduced to extract fine-grained reasoning steps, enabling data collection at multiple granularities for training. By leveraging this hierarchical refinement process, CFPRM mitigates redundancy while preserving essential fine-grained knowledge. Extensive experiments on two reasoning datasets across three loss criteria validate the CFPRM's effectiveness and versatility.

**Link**: [arxiv](http://arxiv.org/abs/2501.13622v4),  [pdf](http://arxiv.org/pdf/2501.13622v4)

**Tags**: cs.AI 



### Cetvel: A Unified Benchmark for Evaluating Language Understanding,   Generation and Cultural Capacity of LLMs for Turkish
**Authors**: Yakup Abrek Er, Ilker Kesen, Gözde Gül Şahin, Aykut Erdem

**Updated**: 2025-08-22T14:42:50Z

**Summary**: We introduce Cetvel, a comprehensive benchmark designed to evaluate large language models (LLMs) in Turkish. Existing Turkish benchmarks often lack either task diversity or culturally relevant content, or both. Cetvel addresses these gaps by combining a broad range of both discriminative and generative tasks ensuring content that reflects the linguistic and cultural richness of Turkish language. Cetvel covers 23 tasks grouped into seven categories, including tasks such as grammatical error correction, machine translation, and question answering rooted in Turkish history and idiomatic language. We evaluate 33 open-weight LLMs (up to 70B parameters) covering different model families and instruction paradigms. Our experiments reveal that Turkish-centric instruction-tuned models generally underperform relative to multilingual or general-purpose models (e.g. Llama 3 and Mistral), despite being tailored for the language. Moreover, we show that tasks such as grammatical error correction and extractive question answering are particularly discriminative in differentiating model capabilities. Cetvel offers a comprehensive and culturally grounded evaluation suite for advancing the development and assessment of LLMs in Turkish.

**Link**: [arxiv](http://arxiv.org/abs/2508.16431v1),  [pdf](http://arxiv.org/pdf/2508.16431v1)

**Tags**: cs.CL cs.AI I.2.7 



### LLM-GUARD: Large Language Model-Based Detection and Repair of Bugs and   Security Vulnerabilities in C++ and Python
**Authors**: Akshay Mhatre, Noujoud Nader, Patrick Diehl, Deepti Gupta

**Updated**: 2025-08-22T14:30:24Z

**Summary**: Large Language Models (LLMs) such as ChatGPT-4, Claude 3, and LLaMA 4 are increasingly embedded in software/application development, supporting tasks from code generation to debugging. Yet, their real-world effectiveness in detecting diverse software bugs, particularly complex, security-relevant vulnerabilities, remains underexplored. This study presents a systematic, empirical evaluation of these three leading LLMs using a benchmark of foundational programming errors, classic security flaws, and advanced, production-grade bugs in C++ and Python. The dataset integrates real code from SEED Labs, OpenSSL (via the Suresoft GLaDOS database), and PyBugHive, validated through local compilation and testing pipelines. A novel multi-stage, context-aware prompting protocol simulates realistic debugging scenarios, while a graded rubric measures detection accuracy, reasoning depth, and remediation quality. Our results show that all models excel at identifying syntactic and semantic issues in well-scoped code, making them promising for educational use and as first-pass reviewers in automated code auditing. Performance diminishes in scenarios involving complex security vulnerabilities and large-scale production code, with ChatGPT-4 and Claude 3 generally providing more nuanced contextual analyses than LLaMA 4. This highlights both the promise and the present constraints of LLMs in serving as reliable code analysis tools.

**Link**: [arxiv](http://arxiv.org/abs/2508.16419v1),  [pdf](http://arxiv.org/pdf/2508.16419v1)

**Tags**: cs.SE cs.LG 



### Towards Bridging the Reward-Generation Gap in Direct Alignment   Algorithms
**Authors**: Zeguan Xiao, Yun Chen, Guanhua Chen, Ke Tang

**Updated**: 2025-08-22T14:16:43Z

**Summary**: Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization (DPO) and Simple Preference Optimization (SimPO), have emerged as efficient alternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms for aligning large language models (LLMs) with human preferences. However, DAAs suffer from a fundamental limitation we identify as the "reward-generation gap" -- a misalignment between optimization objectives during training and actual generation performance during inference. In this paper, we find a contributor to the reward-generation gap is the mismatch between the inherent importance of prefix tokens during the LLM generation process and how this importance is reflected in the implicit reward functions of DAAs. To bridge the gap, we adopt a token-level MDP perspective of DAAs to analyze its limitations and introduce a simple yet effective approach called Prefix-Oriented Equal-length Training (POET), which truncates both preferred and dispreferred responses to match the shorter one's length. Training with \mname, where both responses in each sample are truncated to equal length, resulting in diverse truncated lengths across samples, the optimization of DAAs objective is implicitly constrained to converge across all timesteps of token-level MDP, thus paying more attention to prefix tokens than the standard DAAs. We conduct experiments with DPO and SimPO, two representative DAAs, demonstrating that POET improves over their standard implementations, achieving up to 15.6 points in AlpacaEval 2 and overall improvements across downstream tasks. Our results highlight the importance of addressing the misalignment between reward optimization and generation performance in DAAs.

**Link**: [arxiv](http://arxiv.org/abs/2506.09457v2),  [pdf](http://arxiv.org/pdf/2506.09457v2)

**Tags**: cs.CL cs.LG 



### Retrieval-Augmented Defense: Adaptive and Controllable Jailbreak   Prevention for Large Language Models
**Authors**: Guangyu Yang, Jinghong Chen, Jingbiao Mei, Weizhe Lin, Bill Byrne

**Updated**: 2025-08-22T14:13:16Z

**Summary**: Large Language Models (LLMs) remain vulnerable to jailbreak attacks, which attempt to elicit harmful responses from LLMs. The evolving nature and diversity of these attacks pose many challenges for defense systems, including (1) adaptation to counter emerging attack strategies without costly retraining, and (2) control of the trade-off between safety and utility. To address these challenges, we propose Retrieval-Augmented Defense (RAD), a novel framework for jailbreak detection that incorporates a database of known attack examples into Retrieval-Augmented Generation, which is used to infer the underlying, malicious user query and jailbreak strategy used to attack the system. RAD enables training-free updates for newly discovered jailbreak strategies and provides a mechanism to balance safety and utility. Experiments on StrongREJECT show that RAD substantially reduces the effectiveness of strong jailbreak attacks such as PAP and PAIR while maintaining low rejection rates for benign queries. We propose a novel evaluation scheme and show that RAD achieves a robust safety-utility trade-off across a range of operating points in a controllable manner.

**Link**: [arxiv](http://arxiv.org/abs/2508.16406v1),  [pdf](http://arxiv.org/pdf/2508.16406v1)

**Tags**: cs.CR cs.CL 



### MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal   Pairs
**Authors**: Jaap Jumelet, Leonie Weissweiler, Joakim Nivre, Arianna Bisazza

**Updated**: 2025-08-22T14:08:58Z

**Summary**: We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic minimal pairs, covering 101 languages and 2 types of subject-verb agreement, containing more than 128,000 minimal pairs. Our minimal pairs are created using a fully automated pipeline, leveraging the large-scale linguistic resources of Universal Dependencies and UniMorph. MultiBLiMP 1.0 evaluates abilities of LLMs at an unprecedented multilingual scale, and highlights the shortcomings of the current state-of-the-art in modelling low-resource languages.

**Link**: [arxiv](http://arxiv.org/abs/2504.02768v3),  [pdf](http://arxiv.org/pdf/2504.02768v3)

**Tags**: cs.CL 



### AetherCode: Evaluating LLMs' Ability to Win In Premier Programming   Competitions
**Authors**: Zihan Wang, Jiaze Chen, Zhicheng Liu, Markus Mak, Yidi Du, Geonsik Moon, Luoqi Xu, Aaron Tua, Kunshuo Peng, Jiayi Lu, Mingfei Xia, Boqian Zou, Chenyang Ran, Guang Tian, Shoutai Zhu, Yeheng Duan, Zhenghui Kang, Zhenxing Lin, Shangshu Li, Qiang Luo, Qingshen Long, Zhiyong Chen, Yihan Xiao, Yurong Wu, Daoguang Zan, Yuyi Fu, Mingxuan Wang, Ming Ding

**Updated**: 2025-08-22T14:04:55Z

**Summary**: Competitive programming has emerged as a critical benchmark for evaluating the reasoning and coding capabilities of Large Language Models (LLMs). Despite impressive progress on existing benchmarks, we argue that current evaluations overstate model proficiency, masking a substantial gap between LLMs and elite human programmers. This gap arises from two key limitations: insufficient difficulty and scope of benchmark problems, and evaluation bias from low-quality test cases. To address these shortcomings, we present AetherCode, a new benchmark that draws problems from premier programming competitions such as IOI and ICPC, offering broader coverage and higher difficulty. AetherCode further incorporates comprehensive, expert-validated test suites built through a hybrid of automated generation and human curation, ensuring rigorous and reliable assessment. By combining challenging problem design with robust evaluation, AetherCode provides a more faithful measure of LLM capabilities and sets a new standard for future research in code reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2508.16402v1),  [pdf](http://arxiv.org/pdf/2508.16402v1)

**Tags**: cs.SE cs.CL 



### A Lightweight Group Multiscale Bidirectional Interactive Network for   Real-Time Steel Surface Defect Detection
**Authors**: Yong Zhang, Cunjian Chen, Qiang Gao, Yi Wang, Bin Fang

**Updated**: 2025-08-22T13:58:35Z

**Summary**: Real-time surface defect detection is critical for maintaining product quality and production efficiency in the steel manufacturing industry. Despite promising accuracy, existing deep learning methods often suffer from high computational complexity and slow inference speeds, which limit their deployment in resource-constrained industrial environments. Recent lightweight approaches adopt multibranch architectures based on depthwise separable convolution (DSConv) to capture multiscale contextual information. However, these methods often suffer from increased computational overhead and lack effective cross-scale feature interaction, limiting their ability to fully leverage multiscale representations. To address these challenges, we propose GMBINet, a lightweight framework that enhances multiscale feature extraction and interaction through novel Group Multiscale Bidirectional Interactive (GMBI) modules. The GMBI adopts a group-wise strategy for multiscale feature extraction, ensuring scale-agnostic computational complexity. It further integrates a Bidirectional Progressive Feature Interactor (BPFI) and a parameter-free Element-Wise Multiplication-Summation (EWMS) operation to enhance cross-scale interaction without introducing additional computational overhead. Experiments on SD-Saliency-900 and NRSD-MN datasets demonstrate that GMBINet delivers competitive accuracy with real-time speeds of 1048 FPS on GPU and 16.53 FPS on CPU at 512 resolution, using only 0.19 M parameters. Additional evaluations on the NEU-CLS defect classification dataset further confirm the strong generalization ability of our method, demonstrating its potential for broader industrial vision applications beyond surface defect detection. The dataset and code are publicly available at: https://github.com/zhangyongcode/GMBINet.

**Link**: [arxiv](http://arxiv.org/abs/2508.16397v1),  [pdf](http://arxiv.org/pdf/2508.16397v1)

**Tags**: cs.CV cs.AI 



### Collaborative Stance Detection via Small-Large Language Model   Consistency Verification
**Authors**: Yu Yan, Sheng Sun, Zixiang Tang, Teli Liu, Min Liu

**Updated**: 2025-08-22T13:55:07Z

**Summary**: Stance detection on social media aims to identify attitudes expressed in tweets towards specific targets. Current studies prioritize Large Language Models (LLMs) over Small Language Models (SLMs) due to the overwhelming performance improving provided by LLMs. However, heavily relying on LLMs for stance detection, regardless of the cost, is impractical for real-world social media monitoring systems that require vast data analysis. To this end, we propose \textbf{\underline{Co}}llaborative Stance Detection via Small-Large Language Model Consistency \textbf{\underline{Ver}}ification (\textbf{CoVer}) framework, which enhances LLM utilization via context-shared batch reasoning and logical verification between LLM and SLM. Specifically, instead of processing each text individually, CoVer processes texts batch-by-batch, obtaining stance predictions and corresponding explanations via LLM reasoning in a shared context. Then, to exclude the bias caused by context noises, CoVer introduces the SLM for logical consistency verification. Finally, texts that repeatedly exhibit low logical consistency are classified using consistency-weighted aggregation of prior LLM stance predictions. Our experiments show that CoVer outperforms state-of-the-art methods across multiple benchmarks in the zero-shot setting, achieving 0.54 LLM queries per tweet while significantly enhancing performance. Our CoVer offers a more practical solution for LLM deploying for social media stance detection.

**Link**: [arxiv](http://arxiv.org/abs/2502.19954v2),  [pdf](http://arxiv.org/pdf/2502.19954v2)

**Tags**: cs.CL cs.AI 



### RoMedQA: The First Benchmark for Romanian Medical Question Answering
**Authors**: Ana-Cristina Rogoz, Radu Tudor Ionescu, Alexandra-Valentina Anghel, Ionut-Lucian Antone-Iordache, Simona Coniac, Andreea Iuliana Ionescu

**Updated**: 2025-08-22T13:48:37Z

**Summary**: Question answering (QA) is an actively studied topic, being a core natural language processing (NLP) task that needs to be addressed before achieving Artificial General Intelligence (AGI). However, the lack of QA datasets in specific domains and languages hinders the development of robust AI models able to generalize across various domains and languages. To this end, we introduce RoMedQA, the first Romanian QA benchmark for the medical domain, alongside a comprehensive evaluation of state-of-the-art large language models (LLMs). We construct a high-quality and large-scale dataset comprising 102,646 QA pairs related to cancer patients. The questions regard medical case summaries of 1,011 patients, requiring either keyword extraction or reasoning to be answered correctly. RoMedQA is the result of a time-consuming manual annotation process carried out by seven physicians specialized in oncology or radiotherapy, who spent a total of about 2,100 work hours to generate the QA pairs. We experiment with four LLMs from distinct families of models on RoMedQA. Each model is employed in two scenarios, namely one based on zero-shot prompting and one based on supervised fine-tuning. Our results show that fine-tuned models significantly outperform their zero-shot counterparts, clearly indicating that pretrained models fail to generalize on RoMedQA. Our findings demonstrate the importance of both domain-specific and language-specific fine-tuning for reliable clinical QA in Romanian. We publicly release our dataset and code at https://github.com/ana-rogoz/RoMedQA.

**Link**: [arxiv](http://arxiv.org/abs/2508.16390v1),  [pdf](http://arxiv.org/pdf/2508.16390v1)

**Tags**: cs.CL cs.AI cs.LG 



### Rotary Offset Features in Large Language Models
**Authors**: André Jonasson

**Updated**: 2025-08-22T13:41:57Z

**Summary**: Transformer-based Large Language Models (LLMs) rely on positional encodings to provide sequence position information to their attention mechanism. Rotary Positional Encodings (RoPE), which encode relative position by rotating queries and keys, have become widely used in modern LLMs. We study the features and patterns that emerge in queries and keys when using rotary embeddings and introduce the concept of rotary offset features. Our analysis reveals that these features, which frequently exhibit large activations and are often interpreted as outliers, arise consistently across layers, attention heads, and model architectures. We derive bounds predicting which rotary frequencies give rise to rotary offset features and the minimum angle between the query-key pairs for these features. We verify our predictions empirically across models of different sizes and architectures.

**Link**: [arxiv](http://arxiv.org/abs/2503.01832v2),  [pdf](http://arxiv.org/pdf/2503.01832v2)

**Tags**: cs.CL cs.LG 



### GLARE: Agentic Reasoning for Legal Judgment Prediction
**Authors**: Xinyu Yang, Chenlong Deng, Zhicheng Dou

**Updated**: 2025-08-22T13:38:12Z

**Summary**: Legal judgment prediction (LJP) has become increasingly important in the legal field. In this paper, we identify that existing large language models (LLMs) have significant problems of insufficient reasoning due to a lack of legal knowledge. Therefore, we introduce GLARE, an agentic legal reasoning framework that dynamically acquires key legal knowledge by invoking different modules, thereby improving the breadth and depth of reasoning. Experiments conducted on the real-world dataset verify the effectiveness of our method. Furthermore, the reasoning chain generated during the analysis process can increase interpretability and provide the possibility for practical applications.

**Link**: [arxiv](http://arxiv.org/abs/2508.16383v1),  [pdf](http://arxiv.org/pdf/2508.16383v1)

**Tags**: cs.AI cs.CL cs.CY 



### Agentic AI Empowered Multi-UAV Trajectory Optimization in Low-Altitude   Economy Networks
**Authors**: Feibo Jiang, Li Dong, Xitao Pan, Kezhi Wang, Cunhua Pan

**Updated**: 2025-08-22T13:34:49Z

**Summary**: This paper proposes a novel Agentic Retrieval-augmented generation with Mamba-Attention Integrated Transformer (ARMAIT) framework for multi-Unmanned Aerial Vehicle (UAV) trajectory optimization. The framework is built upon Large Language Models (LLMs), incorporating Retrieval-Augmented Generation (RAG) empowered by Agentic AI and integrated with a UAV-specific knowledge base. Through the Agentic RAG, the LLM autonomously interprets high-level task requirements and identifies the key components necessary for trajectory optimization, including model inputs and outputs, network architecture, reward functions, and task constraints. To support efficient modeling across different system scales, we introduce the Mamba-Attention Integrated Transformer (MAIT), a hybrid neural architecture that combines the long-range dependency modeling capability of attention mechanisms with the efficient temporal dynamic representation of Mamba. Furthermore, a Trajectory-Group Relative Policy Optimization (T-GRPO) method is proposed to achieve unified policy gradient optimization in both discrete and continuous trajectory spaces for MAIT training. Extensive experimental results validate the feasibility and effectiveness of the proposed ARMAIT framework.

**Link**: [arxiv](http://arxiv.org/abs/2508.16379v1),  [pdf](http://arxiv.org/pdf/2508.16379v1)

**Tags**: cs.IT eess.SP math.IT 



### Environmental Feature Engineering and Statistical Validation for   ML-Based Path Loss Prediction
**Authors**: Jonathan Ethier, Mathieu Chateauvert, Ryan G. Dempsey, Alexis Bose

**Updated**: 2025-08-22T13:29:52Z

**Summary**: Wireless communications rely on path loss modeling, which is most effective when it includes the physical details of the propagation environment. Acquiring this data has historically been challenging, but geographic information systems data is becoming increasingly available with higher resolution and accuracy. Access to such details enables propagation models to more accurately predict coverage and account for interference in wireless deployments. Machine learning-based modeling can significantly support this effort, with feature based approaches allowing for accurate, efficient, and scalable propagation modeling. Building on previous work, we introduce an extended set of features that improves prediction accuracy while, most importantly, proving model generalization through rigorous statistical assessment and the use of test set holdouts.

**Link**: [arxiv](http://arxiv.org/abs/2501.08306v3),  [pdf](http://arxiv.org/pdf/2501.08306v3)

**Tags**: cs.LG eess.SP 



### The Mediomatix Corpus: Parallel Data for Romansh Idioms via Comparable   Schoolbooks
**Authors**: Zachary Hopton, Jannis Vamvas, Andrin Büchler, Anna Rutkiewicz, Rico Cathomas, Rico Sennrich

**Updated**: 2025-08-22T13:25:00Z

**Summary**: The five idioms (i.e., varieties) of the Romansh language are largely standardized and are taught in the schools of the respective communities in Switzerland. In this paper, we present the first parallel corpus of Romansh idioms. The corpus is based on 291 schoolbook volumes, which are comparable in content for the five idioms. We use automatic alignment methods to extract 207k multi-parallel segments from the books, with more than 2M tokens in total. A small-scale human evaluation confirms that the segments are highly parallel, making the dataset suitable for NLP applications such as machine translation between Romansh idioms. We release the parallel and unaligned versions of the dataset under a CC-BY-NC-SA license and demonstrate its utility for machine translation by training and evaluating an LLM on a sample of the dataset.

**Link**: [arxiv](http://arxiv.org/abs/2508.16371v1),  [pdf](http://arxiv.org/pdf/2508.16371v1)

**Tags**: cs.CL 



### Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort   for Retrieval and RAG
**Authors**: Hengran Zhang, Minghao Tang, Keping Bi, Jiafeng Guo, Shihao Liu, Daiting Shi, Dawei Yin, Xueqi Cheng

**Updated**: 2025-08-22T13:24:48Z

**Summary**: Retrieval models typically rely on costly human-labeled query-document relevance annotations for training and evaluation. To reduce this cost and leverage the potential of Large Language Models (LLMs) in relevance judgments, we aim to explore whether LLM-generated annotations can effectively replace human annotations in training retrieval models. Retrieval usually emphasizes relevance, which indicates "topic-relatedness" of a document to a query, while in RAG, the value of a document (or utility) depends on how it contributes to answer generation. Recognizing this mismatch, some researchers use LLM performance on downstream tasks with documents as labels, but this approach requires manual answers for specific tasks, leading to high costs and limited generalization. In another line of work, prompting LLMs to select useful documents as RAG references eliminates the need for human annotation and is not task-specific. If we leverage LLMs' utility judgments to annotate retrieval data, we may retain cross-task generalization without human annotation in large-scale corpora. Therefore, we investigate utility-focused annotation via LLMs for large-scale retriever training data across both in-domain and out-of-domain settings on the retrieval and RAG tasks. To reduce the impact of low-quality positives labeled by LLMs, we design a novel loss function, i.e., Disj-InfoNCE. Our experiments reveal that: (1) Retrievers trained on utility-focused annotations significantly outperform those trained on human annotations in the out-of-domain setting on both tasks, demonstrating superior generalization capabilities. (2) LLM annotation does not replace human annotation in the in-domain setting. However, incorporating just 20% human-annotated data enables retrievers trained with utility-focused annotations to match the performance of models trained entirely with human annotations.

**Link**: [arxiv](http://arxiv.org/abs/2504.05220v3),  [pdf](http://arxiv.org/pdf/2504.05220v3)

**Tags**: cs.IR cs.AI cs.CL 



### Sentiment Reasoning for Healthcare
**Authors**: Khai-Nguyen Nguyen, Khai Le-Duc, Bach Phan Tat, Duy Le, Long Vo-Dang, Truong-Son Hy

**Updated**: 2025-08-22T13:16:29Z

**Summary**: Transparency in AI healthcare decision-making is crucial. By incorporating rationales to explain reason for each predicted label, users could understand Large Language Models (LLMs)'s reasoning to make better decision. In this work, we introduce a new task - Sentiment Reasoning - for both speech and text modalities, and our proposed multimodal multitask framework and the world's largest multimodal sentiment analysis dataset. Sentiment Reasoning is an auxiliary task in sentiment analysis where the model predicts both the sentiment label and generates the rationale behind it based on the input transcript. Our study conducted on both human transcripts and Automatic Speech Recognition (ASR) transcripts shows that Sentiment Reasoning helps improve model transparency by providing rationale for model prediction with quality semantically comparable to humans while also improving model's classification performance (+2% increase in both accuracy and macro-F1) via rationale-augmented fine-tuning. Also, no significant difference in the semantic quality of generated rationales between human and ASR transcripts. All code, data (five languages - Vietnamese, English, Chinese, German, and French) and models are published online: https://github.com/leduckhai/Sentiment-Reasoning

**Link**: [arxiv](http://arxiv.org/abs/2407.21054v5),  [pdf](http://arxiv.org/pdf/2407.21054v5)

**Tags**: cs.CL cs.AI cs.LG cs.SD eess.AS 



### MizanQA: Benchmarking Large Language Models on Moroccan Legal Question   Answering
**Authors**: Adil Bahaj, Mounir Ghogho

**Updated**: 2025-08-22T13:04:43Z

**Summary**: The rapid advancement of large language models (LLMs) has significantly propelled progress in natural language processing (NLP). However, their effectiveness in specialized, low-resource domains-such as Arabic legal contexts-remains limited. This paper introduces MizanQA (pronounced Mizan, meaning "scale" in Arabic, a universal symbol of justice), a benchmark designed to evaluate LLMs on Moroccan legal question answering (QA) tasks, characterised by rich linguistic and legal complexity. The dataset draws on Modern Standard Arabic, Islamic Maliki jurisprudence, Moroccan customary law, and French legal influences. Comprising over 1,700 multiple-choice questions, including multi-answer formats, MizanQA captures the nuances of authentic legal reasoning. Benchmarking experiments with multilingual and Arabic-focused LLMs reveal substantial performance gaps, highlighting the need for tailored evaluation metrics and culturally grounded, domain-specific LLM development.

**Link**: [arxiv](http://arxiv.org/abs/2508.16357v1),  [pdf](http://arxiv.org/pdf/2508.16357v1)

**Tags**: cs.CL cs.AI cs.IR 



### NovoMolGen: Rethinking Molecular Language Model Pretraining
**Authors**: Kamran Chitsaz, Roshan Balaji, Quentin Fournier, Nirav Pravinbhai Bhatt, Sarath Chandar

**Updated**: 2025-08-22T13:02:19Z

**Summary**: Designing de-novo molecules with desired property profiles requires efficient exploration of the vast chemical space ranging from $10^{23}$ to $10^{60}$ possible synthesizable candidates. While various deep generative models have been developed to design small molecules using diverse input representations, Molecular Large Language Models (Mol-LLMs) based on string representations have emerged as a scalable approach capable of exploring billions of molecules. However, there remains limited understanding regarding how standard language modeling practices such as textual representations, tokenization strategies, model size, and dataset scale impact molecular generation performance. In this work, we systematically investigate these critical aspects by introducing NovoMolGen, a family of transformer-based foundation models pretrained on 1.5 billion molecules for de-novo molecule generation. Through extensive empirical analyses, we identify a weak correlation between performance metrics measured during pretraining and actual downstream performance, revealing important distinctions between molecular and general NLP training dynamics. NovoMolGen establishes new state-of-the-art results, substantially outperforming prior Mol-LLMs and specialized generative models in both unconstrained and goal-directed molecular generation tasks, thus providing a robust foundation for advancing efficient and effective molecular modeling strategies.

**Link**: [arxiv](http://arxiv.org/abs/2508.13408v2),  [pdf](http://arxiv.org/pdf/2508.13408v2)

**Tags**: cs.LG 



### PoisonSwarm: Universal Harmful Information Synthesis via Model   Crowdsourcing
**Authors**: Yu Yan, Sheng Sun, Zhifei Zheng, Ziji Hao, Teli Liu, Min Liu

**Updated**: 2025-08-22T12:57:08Z

**Summary**: To construct responsible and secure AI applications, harmful information data is widely utilized for adversarial testing and the development of safeguards. Existing studies mainly leverage Large Language Models (LLMs) to synthesize data to obtain high-quality task datasets at scale, thereby avoiding costly human annotation. However, limited by the safety alignment mechanisms of LLMs, the synthesis of harmful data still faces challenges in generation reliability and content diversity. In this study, we propose a novel harmful information synthesis framework, PoisonSwarm, which applies the model crowdsourcing strategy to generate diverse harmful data while maintaining a high success rate. Specifically, we generate abundant benign data as the based templates in a counterfactual manner. Subsequently, we decompose each based template into multiple semantic units and perform unit-by-unit toxification and final refinement through dynamic model switching, thus ensuring the success of synthesis. Experimental results demonstrate that PoisonSwarm achieves state-of-the-art performance in synthesizing different categories of harmful data with high scalability and diversity.

**Link**: [arxiv](http://arxiv.org/abs/2505.21184v2),  [pdf](http://arxiv.org/pdf/2505.21184v2)

**Tags**: cs.LG cs.AI cs.CL 



### Contextualize-then-Aggregate: Circuits for In-Context Learning in   Gemma-2 2B
**Authors**: Aleksandra Bakalova, Yana Veitsman, Xinting Huang, Michael Hahn

**Updated**: 2025-08-22T12:52:38Z

**Summary**: In-Context Learning (ICL) is an intriguing ability of large language models (LLMs). Despite a substantial amount of work on its behavioral aspects and how it emerges in miniature setups, it remains unclear which mechanism assembles task information from the individual examples in a fewshot prompt. We use causal interventions to identify information flow in Gemma-2 2B for five naturalistic ICL tasks. We find that the model infers task information using a two-step strategy we call contextualize-then-aggregate: In the lower layers, the model builds up representations of individual fewshot examples, which are contextualized by preceding examples through connections between fewshot input and output tokens across the sequence. In the higher layers, these representations are aggregated to identify the task and prepare prediction of the next output. The importance of the contextualization step differs between tasks, and it may become more important in the presence of ambiguous examples. Overall, by providing rigorous causal analysis, our results shed light on the mechanisms through which ICL happens in language models.

**Link**: [arxiv](http://arxiv.org/abs/2504.00132v2),  [pdf](http://arxiv.org/pdf/2504.00132v2)

**Tags**: cs.CL cs.LG 



### Confusion is the Final Barrier: Rethinking Jailbreak Evaluation and   Investigating the Real Misuse Threat of LLMs
**Authors**: Yu Yan, Sheng Sun, Zhe Wang, Yijun Lin, Zenghao Duan, zhifei zheng, Min Liu, Zhiyi yin, Jianping Zhang

**Updated**: 2025-08-22T12:41:26Z

**Summary**: With the development of Large Language Models (LLMs), numerous efforts have revealed their vulnerabilities to jailbreak attacks. Although these studies have driven the progress in LLMs' safety alignment, it remains unclear whether LLMs have internalized authentic knowledge to deal with real-world crimes, or are merely forced to simulate toxic language patterns. This ambiguity raises concerns that jailbreak success is often attributable to a hallucination loop between jailbroken LLM and judger LLM. By decoupling the use of jailbreak techniques, we construct knowledge-intensive Q\&A to investigate the misuse threats of LLMs in terms of dangerous knowledge possession, harmful task planning utility, and harmfulness judgment robustness. Experiments reveal a mismatch between jailbreak success rates and harmful knowledge possession in LLMs, and existing LLM-as-a-judge frameworks tend to anchor harmfulness judgments on toxic language patterns. Our study reveals a gap between existing LLM safety assessments and real-world threat potential.

**Link**: [arxiv](http://arxiv.org/abs/2508.16347v1),  [pdf](http://arxiv.org/pdf/2508.16347v1)

**Tags**: cs.CR cs.AI 



### B*: Efficient and Optimal Base Placement for Fixed-Base Manipulators
**Authors**: Zihang Zhao, Leiyao Cui, Sirui Xie, Saiyao Zhang, Zhi Han, Lecheng Ruan, Yixin Zhu

**Updated**: 2025-08-22T12:38:13Z

**Summary**: B* is a novel optimization framework that addresses a critical challenge in fixed-base manipulator robotics: optimal base placement. Current methods rely on pre-computed kinematics databases generated through sampling to search for solutions. However, they face an inherent trade-off between solution optimality and computational efficiency when determining sampling resolution. To address these limitations, B* unifies multiple objectives without database dependence. The framework employs a two-layer hierarchical approach. The outer layer systematically manages terminal constraints through progressive tightening, particularly for base mobility, enabling feasible initialization and broad solution exploration. The inner layer addresses non-convexities in each outer-layer subproblem through sequential local linearization, converting the original problem into tractable sequential linear programming (SLP). Testing across multiple robot platforms demonstrates B*'s effectiveness. The framework achieves solution optimality five orders of magnitude better than sampling-based approaches while maintaining perfect success rates and reduced computational overhead. Operating directly in configuration space, B* enables simultaneous path planning with customizable optimization criteria. B* serves as a crucial initialization tool that bridges the gap between theoretical motion planning and practical deployment, where feasible trajectory existence is fundamental.

**Link**: [arxiv](http://arxiv.org/abs/2504.12719v2),  [pdf](http://arxiv.org/pdf/2504.12719v2)

**Tags**: cs.RO 



### from Benign import Toxic: Jailbreaking the Language Model via   Adversarial Metaphors
**Authors**: Yu Yan, Sheng Sun, Zenghao Duan, Teli Liu, Min Liu, Zhiyi Yin, Jiangyu Lei, Qi Li

**Updated**: 2025-08-22T12:35:34Z

**Summary**: Current studies have exposed the risk of Large Language Models (LLMs) generating harmful content by jailbreak attacks. However, they overlook that the direct generation of harmful content from scratch is more difficult than inducing LLM to calibrate benign content into harmful forms. In our study, we introduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR) to induce the LLM to calibrate malicious metaphors for jailbreaking. Specifically, to answer harmful queries, AVATAR adaptively identifies a set of benign but logically related metaphors as the initial seed. Then, driven by these metaphors, the target LLM is induced to reason and calibrate about the metaphorical content, thus jailbroken by either directly outputting harmful responses or calibrating residuals between metaphorical and professional harmful content. Experimental results demonstrate that AVATAR can effectively and transferable jailbreak LLMs and achieve a state-of-the-art attack success rate across multiple advanced LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2503.00038v4),  [pdf](http://arxiv.org/pdf/2503.00038v4)

**Tags**: cs.CL cs.AI cs.CR 



### Cyberbullying Detection via Aggression-Enhanced Prompting
**Authors**: Aisha Saeid, Anu Sabu, Girish A. Koushik, Ferrante Neri, Diptesh Kanojia

**Updated**: 2025-08-22T12:32:59Z

**Summary**: Detecting cyberbullying on social media remains a critical challenge due to its subtle and varied expressions. This study investigates whether integrating aggression detection as an auxiliary task within a unified training framework can enhance the generalisation and performance of large language models (LLMs) in cyberbullying detection. Experiments are conducted on five aggression datasets and one cyberbullying dataset using instruction-tuned LLMs. We evaluated multiple strategies: zero-shot, few-shot, independent LoRA fine-tuning, and multi-task learning (MTL). Given the inconsistent results of MTL, we propose an enriched prompt pipeline approach in which aggression predictions are embedded into cyberbullying detection prompts to provide contextual augmentation. Preliminary results show that the enriched prompt pipeline consistently outperforms standard LoRA fine-tuning, indicating that aggression-informed context significantly boosts cyberbullying detection. This study highlights the potential of auxiliary tasks, such as aggression detection, to improve the generalisation of LLMs for safety-critical applications on social networks.

**Link**: [arxiv](http://arxiv.org/abs/2508.06360v2),  [pdf](http://arxiv.org/pdf/2508.06360v2)

**Tags**: cs.CL 



### Mutation-Guided Unit Test Generation with a Large Language Model
**Authors**: Guancheng Wang, Qinghua Xu, Lionel C. Briand, Kui Liu

**Updated**: 2025-08-22T12:30:44Z

**Summary**: Unit tests play a vital role in uncovering potential faults in software. While tools like EvoSuite focus on maximizing code coverage, recent advances in large language models (LLMs) have shifted attention toward LLM-based test generation. However, code coverage metrics -- such as line and branch coverage -- remain overly emphasized in reported research, despite being weak indicators of a test suite's fault-detection capability. In contrast, mutation score offers a more reliable and stringent measure, as demonstrated in our findings where some test suites achieve 100% coverage but only 4% mutation score. Although a few studies consider mutation score, the effectiveness of LLMs in killing mutants remains underexplored. In this paper, we propose MUTGEN, a mutation-guided, LLM-based test generation approach that incorporates mutation feedback directly into the prompt. Evaluated on 204 subjects from two benchmarks, MUTGEN significantly outperforms both EvoSuite and vanilla prompt-based strategies in terms of mutation score. Furthermore, MUTGEN introduces an iterative generation mechanism that pushes the limits of LLMs in killing additional mutants. Our study also provide insights into the limitations of LLM-based generation, analyzing the reasons for live and uncovered mutants, and the impact of different mutation operators on generation effectiveness.

**Link**: [arxiv](http://arxiv.org/abs/2506.02954v4),  [pdf](http://arxiv.org/pdf/2506.02954v4)

**Tags**: cs.SE 



### From Linear to Hierarchical: Evolving Tree-structured Thoughts for   Efficient Alpha Mining
**Authors**: Junji Ren, Junjie Zhao, Shengcai Liu, Peng Yang

**Updated**: 2025-08-22T12:21:37Z

**Summary**: Alpha mining, which discovers signals that predict asset returns, has long been attractive for automatic quantitative investment. This problem is typically formulated as a tree-based symbolic regression with handcrafted market data features and arithmetic operators. Unfortunately, existing symbolic methods are concerned with computational inefficiency and dependence on prior knowledge. Recent implementation of Large Language Models (LLMs) show that they can automatically generate executable codes for various tasks efficiently, thus can be considered as a new promising way for alpha mining. Specifically, LLMs-driven methods evolve a set of heuristics, including thoughts and codes, where the thoughts are usually represented as plain-text prompts of codes. Unfortunately, trivially adopting them in alpha mining ignores the fact that alphas are with hierarchical tree structures. This paper introduces Tree-structured thought Evolution (TreEvo), which evolves hierarchical reasoning ideas solely at the thought level. Experiments on four real-market datasets demonstrate that TreEvo can obtain better alphas with much less computational time and human expert efforts. And this superiority hardly holds without the tree-structured thoughts and the compatible evolutionary operators.

**Link**: [arxiv](http://arxiv.org/abs/2508.16334v1),  [pdf](http://arxiv.org/pdf/2508.16334v1)

**Tags**: cs.CE 



### Soteria: Language-Specific Functional Parameter Steering for   Multilingual Safety Alignment
**Authors**: Somnath Banerjee, Sayan Layek, Pratyush Chatterjee, Animesh Mukherjee, Rima Hazra

**Updated**: 2025-08-22T12:17:17Z

**Summary**: Ensuring consistent safety across multiple languages remains a significant challenge for large language models (LLMs). We introduce Soteria, a lightweight yet powerful strategy that locates and minimally adjusts the "functional heads" most responsible for harmful content generation in each language. By altering only a fraction of parameters, Soteria drastically reduces policy violations without sacrificing overall model performance, even in low-resource settings. To rigorously evaluate our approach, we also present XThreatBench, a specialized multilingual dataset capturing fine-grained harmful behaviors drawn from real policy guidelines. Experiments with leading open-source LLMs (e.g., Llama, Qwen, Mistral) show that Soteria consistently improves safety metrics across high-, mid-, and low-resource languages. These findings highlight a promising path toward scalable, linguistically attuned, and ethically aligned LLMs worldwide.

**Link**: [arxiv](http://arxiv.org/abs/2502.11244v2),  [pdf](http://arxiv.org/pdf/2502.11244v2)

**Tags**: cs.CL cs.AI 



### LLMSymGuard: A Symbolic Safety Guardrail Framework Leveraging   Interpretable Jailbreak Concepts
**Authors**: Darpan Aswal, Céline Hudelot

**Updated**: 2025-08-22T12:13:38Z

**Summary**: Large Language Models have found success in a variety of applications; however, their safety remains a matter of concern due to the existence of various types of jailbreaking methods. Despite significant efforts, alignment and safety fine-tuning only provide a certain degree of robustness against jailbreak attacks that covertly mislead LLMs towards the generation of harmful content. This leaves them prone to a number of vulnerabilities, ranging from targeted misuse to accidental profiling of users. This work introduces \textbf{LLMSymGuard}, a novel framework that leverages Sparse Autoencoders (SAEs) to identify interpretable concepts within LLM internals associated with different jailbreak themes. By extracting semantically meaningful internal representations, LLMSymGuard enables building symbolic, logical safety guardrails -- offering transparent and robust defenses without sacrificing model capabilities or requiring further fine-tuning. Leveraging advances in mechanistic interpretability of LLMs, our approach demonstrates that LLMs learn human-interpretable concepts from jailbreaks, and provides a foundation for designing more interpretable and logical safeguard measures against attackers. Code will be released upon publication.

**Link**: [arxiv](http://arxiv.org/abs/2508.16325v1),  [pdf](http://arxiv.org/pdf/2508.16325v1)

**Tags**: cs.CL cs.AI cs.SC 



### VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic   Reasoning
**Authors**: Benjamin Callewaert, Simon Vandevelde, Joost Vennekens

**Updated**: 2025-08-22T12:13:32Z

**Summary**: A recent approach to neurosymbolic reasoning is to explicitly combine the strengths of large language models (LLMs) and symbolic solvers to tackle complex reasoning tasks. However, current approaches face significant limitations, including poor generalizability due to task-specific prompts, inefficiencies caused by the lack of separation between knowledge and queries, and restricted inferential capabilities. These shortcomings hinder their scalability and applicability across diverse domains. In this paper, we introduce VERUS-LM, a novel framework designed to address these challenges. VERUS-LM employs a generic prompting mechanism, clearly separates domain knowledge from queries, and supports a wide range of different logical reasoning tasks. This framework enhances adaptability, reduces computational cost, and allows for richer forms of reasoning, such as optimization and constraint satisfaction. We show that our approach succeeds in diverse reasoning on a novel dataset, markedly outperforming LLMs. Additionally, our system achieves competitive results on common reasoning benchmarks when compared to other state-of-the-art approaches, and significantly surpasses them on the difficult AR-LSAT dataset. By pushing the boundaries of hybrid reasoning, VERUS-LM represents a significant step towards more versatile neurosymbolic AI systems

**Link**: [arxiv](http://arxiv.org/abs/2501.14540v2),  [pdf](http://arxiv.org/pdf/2501.14540v2)

**Tags**: cs.AI 



### Can Hallucinations Help? Boosting LLMs for Drug Discovery
**Authors**: Shuzhou Yuan, Zhan Qu, Ashish Yashwanth Kangen, Michael Färber

**Updated**: 2025-08-22T12:12:09Z

**Summary**: Hallucinations in large language models (LLMs), plausible but factually inaccurate text, are often viewed as undesirable. However, recent work suggests that such outputs may hold creative potential. In this paper, we investigate whether hallucinations can improve LLMs on molecule property prediction, a key task in early-stage drug discovery. We prompt LLMs to generate natural language descriptions from molecular SMILES strings and incorporate these often hallucinated descriptions into downstream classification tasks. Evaluating seven instruction-tuned LLMs across five datasets, we find that hallucinations significantly improve predictive accuracy for some models. Notably, Falcon3-Mamba-7B outperforms all baselines when hallucinated text is included, while hallucinations generated by GPT-4o consistently yield the greatest gains between models. We further identify and categorize over 18,000 beneficial hallucinations, with structural misdescriptions emerging as the most impactful type, suggesting that hallucinated statements about molecular structure may increase model confidence. Ablation studies show that larger models benefit more from hallucinations, while temperature has a limited effect. Our findings challenge conventional views of hallucination as purely problematic and suggest new directions for leveraging hallucinations as a useful signal in scientific modeling tasks like drug discovery.

**Link**: [arxiv](http://arxiv.org/abs/2501.13824v2),  [pdf](http://arxiv.org/pdf/2501.13824v2)

**Tags**: cs.CL cs.AI 



### MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks
**Authors**: Mouath Abu Daoud, Chaimae Abouzahir, Leen Kharouf, Walid Al-Eisawi, Nizar Habash, Farah E. Shamout

**Updated**: 2025-08-22T11:58:27Z

**Summary**: Large Language Models (LLMs) have demonstrated significant promise for various applications in healthcare. However, their efficacy in the Arabic medical domain remains unexplored due to the lack of high-quality domain-specific datasets and benchmarks. This study introduces MedArabiQ, a novel benchmark dataset consisting of seven Arabic medical tasks, covering multiple specialties and including multiple choice questions, fill-in-the-blank, and patient-doctor question answering. We first constructed the dataset using past medical exams and publicly available datasets. We then introduced different modifications to evaluate various LLM capabilities, including bias mitigation. We conducted an extensive evaluation with five state-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude 3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of new high-quality benchmarks that span different languages to ensure fair deployment and scalability of LLMs in healthcare. By establishing this benchmark and releasing the dataset, we provide a foundation for future research aimed at evaluating and enhancing the multilingual capabilities of LLMs for the equitable use of generative AI in healthcare.

**Link**: [arxiv](http://arxiv.org/abs/2505.03427v2),  [pdf](http://arxiv.org/pdf/2505.03427v2)

**Tags**: cs.CL cs.AI cs.HC 



### OwkinZero: Accelerating Biological Discovery with AI
**Authors**: Nathan Bigaud, Vincent Cabeli, Meltem Gurel, Arthur Pignet, John Klein, Gilles Wainrib, Eric Durand

**Updated**: 2025-08-22T11:52:04Z

**Summary**: While large language models (LLMs) are rapidly advancing scientific research, they continue to struggle with core biological reasoning tasks essential for translational and biomedical discovery. To address this limitation, we created and curated eight comprehensive benchmark datasets comprising over 300,000 verifiable question-and-answer pairs, each targeting critical challenges in drug discovery including target druggability, modality suitability, and drug perturbation effects. Using this resource, we developed the OwkinZero models by post-training open-source LLMs through a Reinforcement Learning from Verifiable Rewards strategy. Our results demonstrate that specialized 8-32B OwkinZero models substantially outperform larger, state-of-the-art commercial LLMs on these biological benchmarks. Remarkably, we uncover evidence of a key aspect of generalization: specialist models trained on a single task consistently outperform their base models on previously unseen tasks. This generalization effect is further amplified in our comprehensive OwkinZero models, which were trained on a mixture of datasets and achieve even broader cross-task improvements. This study represents a significant step toward addressing the biological reasoning blind spot in current LLMs, demonstrating that targeted reinforcement learning on carefully curated data can unlock generalizable performance in specialized models, thereby accelerating AI-driven biological discovery.

**Link**: [arxiv](http://arxiv.org/abs/2508.16315v1),  [pdf](http://arxiv.org/pdf/2508.16315v1)

**Tags**: cs.LG 



### Retrieval Enhanced Feedback via In-context Neural Error-book
**Authors**: Jongyeop Hyun, Bumsoo Kim

**Updated**: 2025-08-22T11:50:04Z

**Summary**: Recent advancements in Large Language Models (LLMs) have significantly improved reasoning capabilities, with in-context learning (ICL) emerging as a key technique for adaptation without retraining. While previous works have focused on leveraging correct examples, recent research highlights the importance of learning from errors to enhance performance. However, existing methods lack a structured framework for analyzing and mitigating errors, particularly in Multimodal Large Language Models (MLLMs), where integrating visual and textual inputs adds complexity. To address this issue, we propose REFINE: Retrieval-Enhanced Feedback via In-context Neural Error-book, a teacher-student framework that systematically structures errors and provides targeted feedback. REFINE introduces three systematic queries to construct structured feedback -- Feed-Target, Feed-Check, and Feed-Path -- to enhance multimodal reasoning by prioritizing relevant visual information, diagnosing critical failure points, and formulating corrective actions. Unlike prior approaches that rely on redundant retrievals, REFINE optimizes structured feedback retrieval, improving inference efficiency, token usage, and scalability. Our results demonstrate substantial speedup, reduced computational costs, and successful generalization, highlighting REFINE's potential for enhancing multimodal reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2508.16313v1),  [pdf](http://arxiv.org/pdf/2508.16313v1)

**Tags**: cs.LG cs.AI cs.CL 



### Exploiting Information Redundancy in Attention Maps for Extreme   Quantization of Vision Transformers
**Authors**: Lucas Maisonnave, Karim Haroun, Tom Pegeot

**Updated**: 2025-08-22T11:43:39Z

**Summary**: Transformer models rely on Multi-Head Self-Attention (MHSA) mechanisms, where each attention head contributes to the final representation. However, their computational complexity and high memory demands due to MHSA hinders their deployment at the edge. In this work, we analyze and exploit information redundancy in attention maps to accelerate model inference. By quantifying the information captured by each attention head using Shannon entropy, our analysis reveals that attention heads with lower entropy, i.e., exhibiting more deterministic behavior, tend to contribute less information, motivating targeted compression strategies. Relying on these insights, we propose Entropy Attention Maps (EAM), a model that freezes the weights of low-entropy attention maps and quantizes these values to low precision to avoid redundant re-computation. Empirical validation on ImageNet-1k shows that EAM achieves similar or higher accuracy at $\leq$20\% sparsity in attention maps and competitive performance beyond this level for the DeiT and Swin Transformer models.

**Link**: [arxiv](http://arxiv.org/abs/2508.16311v1),  [pdf](http://arxiv.org/pdf/2508.16311v1)

**Tags**: cs.CV cs.AI cs.IT math.IT 



### DIDS: Domain Impact-aware Data Sampling for Large Language Model   Training
**Authors**: Weijie Shi, Jipeng Zhang, Yaguang Wu, Jingzhi Fang, Ruiyuan Zhang, Jiajie Xu, Jia Zhu, Hao Chen, Yao Zhao, Sirui Han, Xiaofang Zhou

**Updated**: 2025-08-22T11:26:53Z

**Summary**: Large language models (LLMs) are commonly trained on multi-domain datasets, where domain sampling strategies significantly impact model performance due to varying domain importance across downstream tasks. Existing approaches for optimizing domain-level sampling strategies struggle with maintaining intra-domain consistency and accurately measuring domain impact. In this paper, we present Domain Impact-aware Data Sampling (DIDS). To ensure intra-domain consistency, a gradient clustering algorithm is proposed to group training data based on their learning effects, where a proxy language model and dimensionality reduction are employed to reduce computational overhead. To accurately measure domain impact, we develop a Fisher Information Matrix (FIM) guided metric that quantifies how domain-specific parameter updates affect the model's output distributions on downstream tasks, with theoretical guarantees. Furthermore, to determine optimal sampling ratios, DIDS combines both the FIM-guided domain impact assessment and loss learning trajectories that indicate domain-specific potential, while accounting for diminishing marginal returns. Extensive experiments demonstrate that DIDS achieves 3.4% higher average performance while maintaining comparable training efficiency. The code is available at https://github.com/shiweijiezero/DIDS.

**Link**: [arxiv](http://arxiv.org/abs/2504.13227v2),  [pdf](http://arxiv.org/pdf/2504.13227v2)

**Tags**: cs.CL cs.AI cs.LG 



### Bridging the Culture Gap: A Framework for LLM-Driven Socio-Cultural   Localization of Math Word Problems in Low-Resource Languages
**Authors**: Israel Abebe Azime, Tadesse Destaw Belay, Dietrich Klakow, Philipp Slusallek, Anshuman Chhabra

**Updated**: 2025-08-22T11:14:44Z

**Summary**: Large language models (LLMs) have demonstrated significant capabilities in solving mathematical problems expressed in natural language. However, multilingual and culturally-grounded mathematical reasoning in low-resource languages lags behind English due to the scarcity of socio-cultural task datasets that reflect accurate native entities such as person names, organization names, and currencies. Existing multilingual benchmarks are predominantly produced via translation and typically retain English-centric entities, owing to the high cost associated with human annotater-based localization. Moreover, automated localization tools are limited, and hence, truly localized datasets remain scarce. To bridge this gap, we introduce a framework for LLM-driven cultural localization of math word problems that automatically constructs datasets with native names, organizations, and currencies from existing sources. We find that translated benchmarks can obscure true multilingual math ability under appropriate socio-cultural contexts. Through extensive experiments, we also show that our framework can help mitigate English-centric entity bias and improves robustness when native entities are introduced across various languages.

**Link**: [arxiv](http://arxiv.org/abs/2508.14913v2),  [pdf](http://arxiv.org/pdf/2508.14913v2)

**Tags**: cs.CL 



### Two-Timescale Dynamic Service Deployment and Task Scheduling with   Spatiotemporal Collaboration in Mobile Edge Networks
**Authors**: Yang Li, Xing Zhang, Yunji Zhao, Wenbo Wang

**Updated**: 2025-08-22T10:55:16Z

**Summary**: Collaborative edge computing addresses the resource constraints of individual edge nodes by enabling resource sharing and task co-processing across multiple nodes. To fully leverage the advantages of collaborative edge computing, joint optimization of service deployment and task scheduling is necessary. Existing optimization methods insufficiently address the collaboration across spatial and temporal dimensions, which hinders their adaptability to the spatiotemporally varying nature of user demands and system states. This paper focuses on optimizing the expected task processing delay in edge networks. We propose a two-timescale online optimization framework to jointly determine: i) service deployment decisions at each large timescale; and ii) task scheduling decisions at each small timescale. Specifically, the convex optimization technique is used to solve the task scheduling problem, while a multi-agent deep reinforcement learning technique is employed for the service deployment problem. These two methods are combined for spatiotemporal co-optimization through a two-timescale alternating optimization approach. Compared to the baseline algorithms, the proposed scheme achieves better delay performance, while also exhibiting low running time and favorable convergence behavior.

**Link**: [arxiv](http://arxiv.org/abs/2508.16293v1),  [pdf](http://arxiv.org/pdf/2508.16293v1)

**Tags**: cs.PF 



### Optimal Batch-Size Control for Low-Latency Federated Learning with   Device Heterogeneity
**Authors**: Huiling Yang, Zhanwei Wang, Kaibin Huang

**Updated**: 2025-08-22T10:51:55Z

**Summary**: Federated learning (FL) has emerged as a popular approach for collaborative machine learning in sixth-generation (6G) networks, primarily due to its privacy-preserving capabilities. The deployment of FL algorithms is expected to empower a wide range of Internet-of-Things (IoT) applications, e.g., autonomous driving, augmented reality, and healthcare. The mission-critical and time-sensitive nature of these applications necessitates the design of low-latency FL frameworks that guarantee high learning performance. In practice, achieving low-latency FL faces two challenges: the overhead of computing and transmitting high-dimensional model updates, and the heterogeneity in communication-and-computation (C$^2$) capabilities across devices. To address these challenges, we propose a novel C$^2$-aware framework for optimal batch-size control that minimizes end-to-end (E2E) learning latency while ensuring convergence. The framework is designed to balance a fundamental C$^2$ tradeoff as revealed through convergence analysis. Specifically, increasing batch sizes improves the accuracy of gradient estimation in FL and thus reduces the number of communication rounds required for convergence, but results in higher per-round latency, and vice versa. The associated problem of latency minimization is intractable; however, we solve it by designing an accurate and tractable surrogate for convergence speed, with parameters fitted to real data. This approach yields two batch-size control strategies tailored to scenarios with slow and fast fading, while also accommodating device heterogeneity. Extensive experiments using real datasets demonstrate that the proposed strategies outperform conventional batch-size adaptation schemes that do not consider the C$^2$ tradeoff or device heterogeneity.

**Link**: [arxiv](http://arxiv.org/abs/2507.15601v2),  [pdf](http://arxiv.org/pdf/2507.15601v2)

**Tags**: cs.LG cs.SY eess.SY 



### AgentScope 1.0: A Developer-Centric Framework for Building Agentic   Applications
**Authors**: Dawei Gao, Zitao Li, Yuexiang Xie, Weirui Kuang, Liuyi Yao, Bingchen Qian, Zhijian Ma, Yue Cui, Haohao Luo, Shen Li, Lu Yi, Yi Yu, Shiqi He, Zhiling Luo, Wenmeng Zhou, Zhicheng Zhang, Xuguang He, Ziqian Chen, Weikai Liao, Farruh Isakulovich Kushnazarov, Yaliang Li, Bolin Ding, Jingren Zhou

**Updated**: 2025-08-22T10:35:56Z

**Summary**: Driven by rapid advancements of Large Language Models (LLMs), agents are empowered to combine intrinsic knowledge with dynamic tool use, greatly enhancing their capacity to address real-world tasks. In line with such an evolution, AgentScope introduces major improvements in a new version (1.0), towards comprehensively supporting flexible and efficient tool-based agent-environment interactions for building agentic applications. Specifically, we abstract foundational components essential for agentic applications and provide unified interfaces and extensible modules, enabling developers to easily leverage the latest progress, such as new models and MCPs. Furthermore, we ground agent behaviors in the ReAct paradigm and offer advanced agent-level infrastructure based on a systematic asynchronous design, which enriches both human-agent and agent-agent interaction patterns while improving execution efficiency. Building on this foundation, we integrate several built-in agents tailored to specific practical scenarios. AgentScope also includes robust engineering support for developer-friendly experiences. We provide a scalable evaluation module with a visual studio interface, making the development of long-trajectory agentic applications more manageable and easier to trace. In addition, AgentScope offers a runtime sandbox to ensure safe agent execution and facilitates rapid deployment in production environments. With these enhancements, AgentScope provides a practical foundation for building scalable, adaptive, and effective agentic applications.

**Link**: [arxiv](http://arxiv.org/abs/2508.16279v1),  [pdf](http://arxiv.org/pdf/2508.16279v1)

**Tags**: cs.AI 



### Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and   Context-Aware KGQA
**Authors**: Yingxu Wang, Shiqi Fan, Mengzhu Wang, Siyang Gao, Siwei Liu, Nan Yin

**Updated**: 2025-08-22T10:30:51Z

**Summary**: Knowledge Graph Question Answering (KGQA) aims to interpret natural language queries and perform structured reasoning over knowledge graphs by leveraging their relational and semantic structures to retrieve accurate answers. Recent KGQA methods primarily follow either retrieve-then-reason paradigm, relying on GNNs or heuristic rules for static paths extraction, or dynamic path generation strategies that use large language models (LLMs) with prompting to jointly perform retrieval and reasoning. However, the former suffers from limited adaptability due to static path extraction and lack of contextual refinement, while the latter incurs high computational costs and struggles with accurate path evaluation due to reliance on fixed scoring functions and extensive LLM calls. To address these issues, this paper proposes Dynamically Adaptive MCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search with adaptive path evaluation for efficient and context-aware KGQA. DAMR employs a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based planner, which selects top-$k$ relevant relations at each step to reduce search space. To improve path evaluation accuracy, we introduce a lightweight Transformer-based scorer that performs context-aware plausibility estimation by jointly encoding the question and relation sequence through cross-attention, enabling the model to capture fine-grained semantic shifts during multi-hop reasoning. Furthermore, to alleviate the scarcity of high-quality supervision, DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically generates training signals from partial paths explored during search, allowing the scorer to continuously adapt to the evolving distribution of reasoning trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR significantly outperforms state-of-the-art methods.

**Link**: [arxiv](http://arxiv.org/abs/2508.00719v2),  [pdf](http://arxiv.org/pdf/2508.00719v2)

**Tags**: cs.CL cs.AI 



### The next question after Turing's question: Introducing the Grow-AI test
**Authors**: Alexandru Tugui

**Updated**: 2025-08-22T10:19:42Z

**Summary**: This study aims to extend the framework for assessing artificial intelligence, called GROW-AI (Growth and Realization of Autonomous Wisdom), designed to answer the question "Can machines grow up?" -- a natural successor to the Turing Test. The methodology applied is based on a system of six primary criteria (C1-C6), each assessed through a specific "game", divided into four arenas that explore both the human dimension and its transposition into AI. All decisions and actions of the entity are recorded in a standardized AI Journal, the primary source for calculating composite scores. The assessment uses the prior expert method to establish initial weights, and the global score -- Grow Up Index -- is calculated as the arithmetic mean of the six scores, with interpretation on maturity thresholds. The results show that the methodology allows for a coherent and comparable assessment of the level of "growth" of AI entities, regardless of their type (robots, software agents, LLMs). The multi-game structure highlights strengths and vulnerable areas, and the use of a unified journal guarantees traceability and replicability in the evaluation. The originality of the work lies in the conceptual transposition of the process of "growing" from the human world to that of artificial intelligence, in an integrated testing format that combines perspectives from psychology, robotics, computer science, and ethics. Through this approach, GROW-AI not only measures performance but also captures the evolutionary path of an AI entity towards maturity.

**Link**: [arxiv](http://arxiv.org/abs/2508.16277v1),  [pdf](http://arxiv.org/pdf/2508.16277v1)

**Tags**: cs.AI cs.HC 68T01, 68T05, 68T42, 91A80 I.2; K.4 



### LLMs that Understand Processes: Instruction-tuning for Semantics-Aware   Process Mining
**Authors**: Vira Pyrih, Adrian Rebmann, Han van der Aa

**Updated**: 2025-08-22T10:13:13Z

**Summary**: Process mining is increasingly using textual information associated with events to tackle tasks such as anomaly detection and process discovery. Such semantics-aware process mining focuses on what behavior should be possible in a process (i.e., expectations), thus providing an important complement to traditional, frequency-based techniques that focus on recorded behavior (i.e., reality). Large Language Models (LLMs) provide a powerful means for tackling semantics-aware tasks. However, the best performance is so far achieved through task-specific fine-tuning, which is computationally intensive and results in models that can only handle one specific task. To overcome this lack of generalization, we use this paper to investigate the potential of instruction-tuning for semantics-aware process mining. The idea of instruction-tuning here is to expose an LLM to prompt-answer pairs for different tasks, e.g., anomaly detection and next-activity prediction, making it more familiar with process mining, thus allowing it to also perform better at unseen tasks, such as process discovery. Our findings demonstrate a varied impact of instruction-tuning: while performance considerably improved on process discovery and prediction tasks, it varies across models on anomaly detection tasks, highlighting that the selection of tasks for instruction-tuning is critical to achieving desired outcomes.

**Link**: [arxiv](http://arxiv.org/abs/2508.16270v1),  [pdf](http://arxiv.org/pdf/2508.16270v1)

**Tags**: cs.CL 



### Self-Healing Network of Interconnected Edge Devices Empowered by   Infrastructure-as-Code and LoRa Communication
**Authors**: Rob Carson, Mohamed Chahine Ghanem, Feriel Bouakkaz

**Updated**: 2025-08-22T10:09:08Z

**Summary**: This Paper proposes a self-healing, automated network of Raspberry Pi devices designed for deployment in scenarios where traditional networking is unavailable. Leveraging the low-power, long-range capabilities of the LoRa (Long Range) protocol alongside Infrastructure as Code (IaC) methodologies, the research addresses challenges such as limited bandwidth, data collisions, and node failures. Given that LoRa's packet-based system is incompatible with conventional IaC tools like Ansible and Terraform, which rely on TCP/IP networking, the research adapts IaC principles within a containerised architecture deployed across a Raspberry Pi cluster. Evaluation experiments indicate that fragmenting data packets and retransmitting any missed fragments can mitigate LoRa's inherent throughput and packet size limitations, although issues such as collisions and line-of-sight interference persist. An automated failover mechanism was integrated into the architecture, enabling unresponsive services to be redeployed to alternative nodes within one second, demonstrating the system's resilience in maintaining operational continuity despite node or service failures. The paper also identifies practical challenges, including the necessity for time-slotting transmissions to prevent data packet overlap and collisions. Future research should explore the integration of mesh networking to enhance range, develop more advanced scheduling algorithms, and adopt cutting-edge low-power wide-area network (LPWAN) techniques.

**Link**: [arxiv](http://arxiv.org/abs/2508.16268v1),  [pdf](http://arxiv.org/pdf/2508.16268v1)

**Tags**: cs.NI cs.DC 



### From Confidence to Collapse in LLM Factual Robustness
**Authors**: Alina Fastowski, Bardh Prenkaj, Gjergji Kasneci

**Updated**: 2025-08-22T09:59:23Z

**Summary**: Ensuring the robustness of factual knowledge in LLMs is critical for reliable applications in tasks such as question answering and reasoning. However, existing evaluation methods predominantly focus on performance-based metrics, often investigating from the perspective of prompt perturbations, which captures only the externally triggered side of knowledge robustness. To bridge this gap, we introduce a principled approach to measure factual robustness from the perspective of the generation process by analyzing token distribution entropy in combination with temperature scaling sensitivity. These two factors build the Factual Robustness Score (FRS), a novel metric which quantifies the stability of a fact against perturbations in decoding conditions, given its initial uncertainty. To validate our approach, we conduct extensive experiments on 5 LLMs across 3 closed-book QA datasets (SQuAD, TriviaQA, and HotpotQA). We show that factual robustness varies significantly -- smaller models report an FRS of $0.76$, larger ones $0.93$ -- with accuracy degrading by ~$60\%$ under increased uncertainty. These insights demonstrate how entropy and temperature scaling impact factual accuracy, and lay a foundation for developing more robust knowledge retention and retrieval in future models.

**Link**: [arxiv](http://arxiv.org/abs/2508.16267v1),  [pdf](http://arxiv.org/pdf/2508.16267v1)

**Tags**: cs.CL cs.AI 



### M3TQA: Massively Multilingual Multitask Table Question Answering
**Authors**: Daixin Shu, Jian Yang, Zhenhe Wu, Xianjie Wu, Xianfu Cheng, Xiangyuan Guan, Yanghai Wang, Pengfei Wu, Tingyang Yang, Hualei Zhu, Wei Zhang, Ge Zhang, Jiaheng Liu, Zhoujun Li

**Updated**: 2025-08-22T09:57:40Z

**Summary**: Tabular data is a fundamental component of real-world information systems, yet most research in table understanding remains confined to English, leaving multilingual comprehension significantly underexplored. Existing multilingual table benchmarks suffer from geolinguistic imbalance - overrepresenting certain languages and lacking sufficient scale for rigorous cross-lingual analysis. To address these limitations, we introduce a comprehensive framework for massively multilingual multitask table question answering, featuring m3TQA-Instruct, a large-scale benchmark spanning 97 languages across diverse language families, including underrepresented and low-resource languages. We construct m3TQA by curating 50 real-world tables in Chinese and English, then applying a robust six-step LLM-based translation pipeline powered by DeepSeek and GPT-4o, achieving high translation fidelity with a median BLEU score of 60.19 as validated through back-translation. The benchmark includes 2,916 professionally annotated question-answering pairs across four tasks designed to evaluate nuanced table reasoning capabilities. Experiments on state-of-the-art LLMs reveal critical insights into cross-lingual generalization, demonstrating that synthetically generated, unannotated QA data can significantly boost performance, particularly for low-resource languages. M3T-Bench establishes a new standard for multilingual table understanding, providing both a challenging evaluation platform and a scalable methodology for future research.

**Link**: [arxiv](http://arxiv.org/abs/2508.16265v1),  [pdf](http://arxiv.org/pdf/2508.16265v1)

**Tags**: cs.CL 



### On the Evolution of Federated Post-Training Large Language Models: A   Model Accessibility View
**Authors**: Tao Guo, Junxiao Wang, Fushuo Huo, Laizhong Cui, Song Guo, Jie Gui, Dacheng Tao

**Updated**: 2025-08-22T09:52:31Z

**Summary**: Federated Learning (FL) enables training models across decentralized data silos while preserving client data privacy. Recent research has explored efficient methods for post-training large language models (LLMs) within FL to address computational and communication challenges. While existing approaches often rely on access to LLMs' internal information, which is frequently restricted in real-world scenarios, an inference-only paradigm (black-box FedLLM) has emerged to address these limitations. This paper presents a comprehensive survey on federated tuning for LLMs. We propose a taxonomy categorizing existing studies along two axes: model access-based and parameter efficiency-based optimization. We classify FedLLM approaches into white-box, gray-box, and black-box techniques, highlighting representative methods within each category. We review emerging research treating LLMs as black-box inference APIs and discuss promising directions and open challenges for future research.

**Link**: [arxiv](http://arxiv.org/abs/2508.16261v1),  [pdf](http://arxiv.org/pdf/2508.16261v1)

**Tags**: cs.LG 



### MCPVerse: An Expansive, Real-World Benchmark for Agentic Tool Use
**Authors**: Fei Lei, Yibo Yang, Wenxiu Sun, Dahua Lin

**Updated**: 2025-08-22T09:47:53Z

**Summary**: Large Language Models (LLMs) are evolving from text generators into reasoning agents. This transition makes their ability to use external tools a critical capability. However, evaluating this skill presents a significant challenge. Existing benchmarks are often limited by their reliance on synthetic tools and severely constrained action spaces. To address these limitations, we introduce MCPVerse, an expansive, real-world benchmark for evaluating agentic tool use. MCPVerse integrates more than 550 real-world, executable tools to create an unprecedented action space exceeding 140k tokens, and employs outcome-based evaluation with real-time ground truth for time-sensitive tasks. We benchmarked the state-of-the-art LLMs across three modes (Oracle, Standard, and Max-Scale), revealing that while most models suffer performance degradation when confronted with larger tool sets, the agentic models, such as Claude-4-Sonnet, can effectively leverage expanded exploration spaces to improve accuracy. This finding not only exposes the limitations of state-of-the-art models in complex, real-world scenarios but also establishes MCPVerse as a critical benchmark for measuring and advancing agentic tool use capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2508.16260v1),  [pdf](http://arxiv.org/pdf/2508.16260v1)

**Tags**: cs.CL cs.AI 



### A QoE-Driven Personalized Incentive Mechanism Design for AIGC Services   in Resource-Constrained Edge Networks
**Authors**: Hongjia Wu, Minrui Xu, Zehui Xiong, Lin Gao, Haoyuan Pan, Dusit Niyato, Tse-Tin Chan

**Updated**: 2025-08-22T09:32:40Z

**Summary**: With rapid advancements in large language models (LLMs), AI-generated content (AIGC) has emerged as a key driver of technological innovation and economic transformation. Personalizing AIGC services to meet individual user demands is essential but challenging for AIGC service providers (ASPs) due to the subjective and complex demands of mobile users (MUs), as well as the computational and communication resource constraints faced by ASPs. To tackle these challenges, we first develop a novel multi-dimensional quality-of-experience (QoE) metric. This metric comprehensively evaluates AIGC services by integrating accuracy, token count, and timeliness. We focus on a mobile edge computing (MEC)-enabled AIGC network, consisting of multiple ASPs deploying differentiated AIGC models on edge servers and multiple MUs with heterogeneous QoE requirements requesting AIGC services from ASPs. To incentivize ASPs to provide personalized AIGC services under MEC resource constraints, we propose a QoE-driven incentive mechanism. We formulate the problem as an equilibrium problem with equilibrium constraints (EPEC), where MUs as leaders determine rewards, while ASPs as followers optimize resource allocation. To solve this, we develop a dual-perturbation reward optimization algorithm, reducing the implementation complexity of adaptive pricing. Experimental results demonstrate that our proposed mechanism achieves a reduction of approximately $64.9\%$ in average computational and communication overhead, while the average service cost for MUs and the resource consumption of ASPs decrease by $66.5\%$ and $76.8\%$, respectively, compared to state-of-the-art benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2508.16251v1),  [pdf](http://arxiv.org/pdf/2508.16251v1)

**Tags**: cs.GT 



### ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability
**Authors**: Wenhan Liu, Xinyu Ma, Weiwei Sun, Yutao Zhu, Yuchen Li, Dawei Yin, Zhicheng Dou

**Updated**: 2025-08-22T09:15:24Z

**Summary**: Large Language Model (LLM) based listwise ranking has shown superior performance in many passage ranking tasks. With the development of Large Reasoning Models, many studies have demonstrated that step-by-step reasoning during test-time helps improve listwise ranking performance. However, due to the scarcity of reasoning-intensive training data, existing rerankers perform poorly in many complex ranking scenarios and the ranking ability of reasoning-intensive rerankers remains largely underdeveloped. In this paper, we first propose an automated reasoning-intensive training data synthesis framework, which sources training queries and passages from diverse domains and applies DeepSeek-R1 to generate high-quality training labels. A self-consistency data filtering mechanism is designed to ensure the data quality. To empower the listwise reranker with strong reasoning ability, we further propose a two-stage post-training approach, which includes a cold-start supervised fine-tuning (SFT) stage for reasoning pattern learning and a reinforcement learning (RL) stage for further ranking ability enhancement. During the RL stage, based on the nature of listwise ranking, we design a multi-view ranking reward, which is more effective than a ranking metric-based reward. Extensive experiments demonstrate that our trained reasoning-intensive reranker \textbf{ReasonRank} outperforms existing baselines significantly and also achieves much lower latency than pointwise reranker Rank1. \textbf{Through further experiments, our ReasonRank has achieved state-of-the-art (SOTA) performance 40.6 on the BRIGHT leaderboard\footnote{https://brightbenchmark.github.io/}.} Our codes are available at https://github.com/8421BCD/ReasonRank.

**Link**: [arxiv](http://arxiv.org/abs/2508.07050v2),  [pdf](http://arxiv.org/pdf/2508.07050v2)

**Tags**: cs.IR cs.AI cs.CL cs.LG 



### Hybrid Pruning: In-Situ Compression of Self-Supervised Speech Models for   Speaker Verification and Anti-Spoofing
**Authors**: Junyi Peng, Lin Zhang, Jiangyu Han, Oldřich Plchot, Johan Rohdin, Themos Stafylakis, Shuai Wang, Jan Černocký

**Updated**: 2025-08-22T09:10:37Z

**Summary**: Although large-scale self-supervised learning (SSL) models like WavLM have achieved state-of-the-art performance in speech processing, their significant size impedes deployment on resource-constrained devices. While structured pruning is a key technique for model compression, existing methods typically separate it from task-specific fine-tuning. This multi-stage approach struggles to create optimal architectures tailored for diverse downstream tasks. In this work, we introduce a unified framework that integrates structured pruning into the downstream fine-tuning process. Our framework unifies these steps, jointly optimizing for task performance and model sparsity in a single stage. This allows the model to learn a compressed architecture specifically for the end task, eliminating the need for complex multi-stage pipelines and knowledge distillation. Our pruned models achieve up to a 70\% parameter reduction with negligible performance degradation on large-scale datasets, achieving equal error rates of 0.7\%, 0.8\%, and 1.6\% on Vox1-O, -E, and -H, respectively. Furthermore, our approach demonstrates improved generalization in low-resource scenarios, reducing overfitting and achieving a state-of-the-art 3.7\% EER on ASVspoof5.

**Link**: [arxiv](http://arxiv.org/abs/2508.16232v1),  [pdf](http://arxiv.org/pdf/2508.16232v1)

**Tags**: eess.AS 



### GPU Kernel Scientist: An LLM-Driven Framework for Iterative Kernel   Optimization
**Authors**: Martin Andrews, Sam Witteveen

**Updated**: 2025-08-22T08:48:16Z

**Summary**: Optimizing GPU kernels for high performance is a complex task, often demanding deep architectural knowledge, extensive profiling, and iterative experimentation. This challenge is amplified when targeting newer or less-documented GPU architectures where traditional development aids are scarce. This paper introduces an LLM-powered "GPU Kernel Scientist," an automated methodology for iteratively refining accelerator kernels.   Our methodology employs LLMs in a multi-stage, evolutionary process: (a) strategically selecting promising prior code versions as a basis for new iterations; (b) generating hypotheses for optimization experiments, based on existing code and assimilated knowledge from general GPU literature; and (c) autonomously implementing these experiments through code modification and subsequent submission to an external evaluation system, using only observed timing data as performance feedback. We detail how this approach navigates the challenges of the AMD MI300 target architecture and leverages LLMs to compensate for limited domain-specific human expertise.   In addition to our results, we present the architectural design, operational workflow, and qualitative insights, highlighting the potential of LLM-driven agents to democratise and accelerate GPU kernel optimization, especially in resource-constrained or rapidly updating hardware environment.

**Link**: [arxiv](http://arxiv.org/abs/2506.20807v2),  [pdf](http://arxiv.org/pdf/2506.20807v2)

**Tags**: cs.LG cs.AI cs.PF cs.SE 



### SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long   Sequences
**Authors**: Jungyoub Cha, Hyunjong Kim, Sungzoon Cho

**Updated**: 2025-08-22T08:45:04Z

**Summary**: Speculative decoding is a widely adopted technique for accelerating inference in large language models (LLMs), but its performance degrades on long inputs due to increased attention cost and reduced draft accuracy. We introduce SpecExtend, a drop-in enhancement that improves the performance of speculative decoding on long sequences without any additional training. First, SpecExtend integrates efficient attention mechanisms such as FlashAttention and Hybrid Tree Attention into both the draft and target models. To improve draft accuracy and speed on long inputs without retraining, we propose Cross-model Retrieval, a novel KV cache eviction strategy that uses the target model's attention scores to dynamically select relevant context for the draft model. Extensive evaluations on three long-context understanding datasets show that SpecExtend accelerates standard tree-based speculative decoding by up to 2.22x for inputs up to 16K tokens, providing an effective solution for speculative decoding of long sequences. Our code is available at https://github.com/jycha98/SpecExtend .

**Link**: [arxiv](http://arxiv.org/abs/2505.20776v2),  [pdf](http://arxiv.org/pdf/2505.20776v2)

**Tags**: cs.CL cs.AI cs.LG I.2.7; C.4 



### Tripartite-GraphRAG via Plugin Ontologies
**Authors**: Michael Banf, Johannes Kuhn

**Updated**: 2025-08-22T08:39:20Z

**Summary**: Large Language Models (LLMs) have shown remarkable capabilities across various domains, yet they struggle with knowledge-intensive tasks in areas that demand factual accuracy, e.g. industrial automation and healthcare. Key limitations include their tendency to hallucinate, lack of source traceability (provenance), and challenges in timely knowledge updates. Combining language models with knowledge graphs (GraphRAG) offers promising avenues for overcoming these deficits. However, a major challenge lies in creating such a knowledge graph in the first place. Here, we propose a novel approach that combines LLMs with a tripartite knowledge graph representation, which is constructed by connecting complex, domain-specific objects via a curated ontology of corresponding, domain-specific concepts to relevant sections within chunks of text through a concept-anchored pre-analysis of source documents starting from an initial lexical graph. Subsequently, we formulate LLM prompt creation as an unsupervised node classification problem allowing for the optimization of information density, coverage, and arrangement of LLM prompts at significantly reduced lengths. An initial experimental evaluation of our approach on a healthcare use case, involving multi-faceted analyses of patient anamneses given a set of medical concepts as well as a series of clinical guideline literature, indicates its potential to optimize information density, coverage, and arrangement of LLM prompts while significantly reducing their lengths, which, in turn, may lead to reduced costs as well as more consistent and reliable LLM outputs.

**Link**: [arxiv](http://arxiv.org/abs/2504.19667v2),  [pdf](http://arxiv.org/pdf/2504.19667v2)

**Tags**: cs.LG cs.AI 



### MedOmni-45°: A Safety-Performance Benchmark for Reasoning-Oriented   LLMs in Medicine
**Authors**: Kaiyuan Ji, Yijin Guo, Zicheng Zhang, Xiangyang Zhu, Yuan Tian, Ning Liu, Guangtao Zhai

**Updated**: 2025-08-22T08:38:16Z

**Summary**: With the increasing use of large language models (LLMs) in medical decision-support, it is essential to evaluate not only their final answers but also the reliability of their reasoning. Two key risks are Chain-of-Thought (CoT) faithfulness -- whether reasoning aligns with responses and medical facts -- and sycophancy, where models follow misleading cues over correctness. Existing benchmarks often collapse such vulnerabilities into single accuracy scores. To address this, we introduce MedOmni-45 Degrees, a benchmark and workflow designed to quantify safety-performance trade-offs under manipulative hint conditions. It contains 1,804 reasoning-focused medical questions across six specialties and three task types, including 500 from MedMCQA. Each question is paired with seven manipulative hint types and a no-hint baseline, producing about 27K inputs. We evaluate seven LLMs spanning open- vs. closed-source, general-purpose vs. medical, and base vs. reasoning-enhanced models, totaling over 189K inferences. Three metrics -- Accuracy, CoT-Faithfulness, and Anti-Sycophancy -- are combined into a composite score visualized with a 45 Degrees plot. Results show a consistent safety-performance trade-off, with no model surpassing the diagonal. The open-source QwQ-32B performs closest (43.81 Degrees), balancing safety and accuracy but not leading in both. MedOmni-45 Degrees thus provides a focused benchmark for exposing reasoning vulnerabilities in medical LLMs and guiding safer model development.

**Link**: [arxiv](http://arxiv.org/abs/2508.16213v1),  [pdf](http://arxiv.org/pdf/2508.16213v1)

**Tags**: cs.CV 



### OmniCache: A Trajectory-Oriented Global Perspective on Training-Free   Cache Reuse for Diffusion Transformer Models
**Authors**: Huanpeng Chu, Wei Wu, Guanyu Fen, Yutao Zhang

**Updated**: 2025-08-22T08:36:58Z

**Summary**: Diffusion models have emerged as a powerful paradigm for generative tasks such as image synthesis and video generation, with Transformer architectures further enhancing performance. However, the high computational cost of diffusion Transformers-stemming from a large number of sampling steps and complex per-step computations-presents significant challenges for real-time deployment. In this paper, we introduce OmniCache, a training-free acceleration method that exploits the global redundancy inherent in the denoising process. Unlike existing methods that determine caching strategies based on inter-step similarities and tend to prioritize reusing later sampling steps, our approach originates from the sampling perspective of DIT models. We systematically analyze the model's sampling trajectories and strategically distribute cache reuse across the entire sampling process. This global perspective enables more effective utilization of cached computations throughout the diffusion trajectory, rather than concentrating reuse within limited segments of the sampling procedure.In addition, during cache reuse, we dynamically estimate the corresponding noise and filter it out to reduce its impact on the sampling direction.Extensive experiments demonstrate that our approach accelerates the sampling process while maintaining competitive generative quality, offering a promising and practical solution for efficient deployment of diffusion-based generative models.

**Link**: [arxiv](http://arxiv.org/abs/2508.16212v1),  [pdf](http://arxiv.org/pdf/2508.16212v1)

**Tags**: cs.CV cs.AI cs.LG 



### SpecVLM: Enhancing Speculative Decoding of Video LLMs via   Verifier-Guided Token Pruning
**Authors**: Yicheng Ji, Jun Zhang, Heming Xia, Jinpeng Chen, Lidan Shou, Gang Chen, Huan Li

**Updated**: 2025-08-22T08:23:09Z

**Summary**: Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding. To mitigate the information loss of recent video token reduction methods and accelerate the decoding stage of Vid-LLMs losslessly, we introduce SpecVLM, a training-free speculative decoding (SD) framework tailored for Vid-LLMs that incorporates staged video token pruning. Building on our novel finding that the draft model's speculation exhibits low sensitivity to video token pruning, SpecVLM prunes up to 90% of video tokens, enabling efficient speculation without sacrificing accuracy. To achieve this, it performs a two-stage pruning process: Stage I selects highly informative tokens guided by attention signals from the verifier (target model), while Stage II prunes remaining redundant ones in a spatially uniform manner. Extensive experiments on four video understanding benchmarks demonstrate the effectiveness and robustness of SpecVLM, which achieves up to 2.68$\times$ decoding speedup for LLaVA-OneVision-72B and 2.11$\times$ speedup for Qwen2.5-VL-32B.

**Link**: [arxiv](http://arxiv.org/abs/2508.16201v1),  [pdf](http://arxiv.org/pdf/2508.16201v1)

**Tags**: cs.CV cs.AI cs.CL 



### CO-Bench: Benchmarking Language Model Agents in Algorithm Search for   Combinatorial Optimization
**Authors**: Weiwei Sun, Shengyu Feng, Shanda Li, Yiming Yang

**Updated**: 2025-08-22T08:00:53Z

**Summary**: Although LLM-based agents have attracted significant attention in domains such as software engineering and machine learning research, their role in advancing combinatorial optimization (CO) remains relatively underexplored. This gap underscores the need for a deeper understanding of their potential in tackling structured, constraint-intensive problems -- a pursuit currently limited by the absence of comprehensive benchmarks for systematic investigation. To address this, we introduce CO-Bench, a benchmark suite featuring 36 real-world CO problems drawn from a broad range of domains and complexity levels. CO-Bench includes structured problem formulations and curated data to support rigorous investigation of LLM agents. We evaluate multiple agentic frameworks against established human-designed algorithms, revealing the strengths and limitations of existing LLM agents and identifying promising directions for future research. CO-Bench is publicly available at https://github.com/sunnweiwei/CO-Bench.

**Link**: [arxiv](http://arxiv.org/abs/2504.04310v3),  [pdf](http://arxiv.org/pdf/2504.04310v3)

**Tags**: cs.CL cs.AI 



### ParamBench: A Graduate-Level Benchmark for Evaluating LLM Understanding   on Indic Subjects
**Authors**: Kaushal Sharma, Vivek Patel, Ayush Maheshwari, Aditya Maheshwari

**Updated**: 2025-08-22T07:59:37Z

**Summary**: Large language models (LLMs) have been widely evaluated on tasks such as comprehension, question answering, summarization, code generation, etc. However, their performance on graduate-level, culturally grounded questions in the Indian context remains largely unexplored. Existing Indian benchmarks emphasise basic fact-orientated queries that offer limited assessment of a deeper disciplinary understanding tailored to the Indian setting. In this paper, we present ParamBench, consisting of around 11.5K questions in Hindi language comprising questionnaires from 16 diverse subjects. These questions are primarily derived from nation-wide graduate level entrance examination covering topics such as history, music, instruments, yoga, literature, philosophy, law, etc., specifically for the Indian context. Additionally, we assess the ability of LLMs to handle diverse question formats-such as list-based matching, assertion-reason pairs, and sequence ordering-alongside conventional multiple-choice questions. We evaluated the performance of more than 17 open source LLMs on this benchmark, observing that Llama 3.3 70B attains the highest overall accuracy of 48%. Furthermore, subject-wise analysis indicates that even for the best performing LLMs, performance remains weak on topics such as music, classical instruments, politics and archaeology, underscoring persistent challenges in culturally grounded reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2508.16185v1),  [pdf](http://arxiv.org/pdf/2508.16185v1)

**Tags**: cs.CL 



### One Example Shown, Many Concepts Known! Counterexample-Driven Conceptual   Reasoning in Mathematical LLMs
**Authors**: Yinghui Li, Jiayi Kuang, Haojing Huang, Zhikun Xu, Xinnian Liang, Yi Yu, Wenlian Lu, Yangning Li, Xiaoyu Tan, Chao Qu, Ying Shen, Hai-Tao Zheng, Philip S. Yu

**Updated**: 2025-08-22T07:58:49Z

**Summary**: Leveraging mathematical Large Language Models (LLMs) for proof generation is a fundamental topic in LLMs research. We argue that the ability of current LLMs to prove statements largely depends on whether they have encountered the relevant proof process during training. This reliance limits their deeper understanding of mathematical theorems and related concepts. Inspired by the pedagogical method of "proof by counterexamples" commonly used in human mathematics education, our work aims to enhance LLMs' ability to conduct mathematical reasoning and proof through counterexamples. Specifically, we manually create a high-quality, university-level mathematical benchmark, CounterMATH, which requires LLMs to prove mathematical statements by providing counterexamples, thereby assessing their grasp of mathematical concepts. Additionally, we develop a data engineering framework to automatically obtain training data for further model improvement. Extensive experiments and detailed analyses demonstrate that CounterMATH is challenging, indicating that LLMs, such as OpenAI o1, have insufficient counterexample-driven proof capabilities. Moreover, our exploration into model training reveals that strengthening LLMs' counterexample-driven conceptual reasoning abilities is crucial for improving their overall mathematical capabilities. We believe that our work offers new perspectives on the community of mathematical LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2502.10454v2),  [pdf](http://arxiv.org/pdf/2502.10454v2)

**Tags**: cs.LG cs.AI cs.CL 



### LLM-Assisted Semantic Alignment and Integration in Collaborative   Model-Based Systems Engineering Using SysML v2
**Authors**: Zirui Li, Stephan Husung, Haoze Wang

**Updated**: 2025-08-22T07:56:33Z

**Summary**: Cross-organizational collaboration in Model-Based Systems Engineering (MBSE) faces many challenges in achieving semantic alignment across independently developed system models. SysML v2 introduces enhanced structural modularity and formal semantics, offering a stronger foundation for interoperable modeling. Meanwhile, GPT-based Large Language Models (LLMs) provide new capabilities for assisting model understanding and integration. This paper proposes a structured, prompt-driven approach for LLM-assisted semantic alignment of SysML v2 models. The core contribution lies in the iterative development of an alignment approach and interaction prompts, incorporating model extraction, semantic matching, and verification. The approach leverages SysML v2 constructs such as alias, import, and metadata extensions to support traceable, soft alignment integration. It is demonstrated with a GPT-based LLM through an example of a measurement system. Benefits and limitations are discussed.

**Link**: [arxiv](http://arxiv.org/abs/2508.16181v1),  [pdf](http://arxiv.org/pdf/2508.16181v1)

**Tags**: cs.SE cs.AI cs.SY eess.SY 



### Planning for future EV charging infrastructure: A city-scale assessment   of demand and capacity
**Authors**: Hong Yuan, Minda Ma, Nan Zhou, Yanqiao Deng, Junhong Liu, Shufan Zhang, Zhili Ma

**Updated**: 2025-08-22T07:51:46Z

**Summary**: As the global shift toward transportation electrification has accelerated, capacity planning for electric vehicle (EV) charging infrastructure has become a critical challenge in the development of low-carbon urban energy systems. This study proposes the first demand-driven, multi-objective planning model for optimizing city-scale capacity allocation of EV charging infrastructure. The model employs a bottom-up approach to estimate charging demand differentiated by vehicle type-battery electric vehicles (BEVs), extended-range electric vehicles (EREVs), and plug-in hybrid electric vehicles (PHEVs). Chongqing, a rapidly expanding EV hub in China with a strong industrial base, supportive policies, and diverse urban morphologies, is selected as the case study. The results show that (1) monthly EV electricity consumption in Chongqing rose from 18.9 gigawatt-hours (GWh) in June 2022 to 57.5 GWh in December 2024, with associated carbon emissions increasing from 9.9 kilotons of carbon dioxide (ktCO2) to 30 ktCO2, driven primarily by BEVs; (2) 181,622 additional charging piles were installed between 2022 and 2024, concentrated in densely populated areas, reflecting a demand-responsive strategy that prioritizes population density over geographic coverage; and (3) between 2025 and 2030, EV electricity demand is projected to reach 1940 GWh, with the number of charging piles exceeding 1.4 million, and charging demand from EREVs and PHEVs expected to overtake BEVs later in the period. While Chongqing serves as the pilot area, the proposed planning platform is adaptable for application in cities worldwide, enabling cross-regional comparisons under diverse socio-economic, geographic, and policy conditions. Overall, this work offers policymakers a versatile tool to support sustainable, cost-effective EV infrastructure deployment aligned with low-carbon electrification targets in the transportation sector.

**Link**: [arxiv](http://arxiv.org/abs/2508.16175v1),  [pdf](http://arxiv.org/pdf/2508.16175v1)

**Tags**: physics.soc-ph 



### Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent   with Preference Chain
**Authors**: Kai Hu, Parfait Atchade-Adelomou, Carlo Adornetto, Adrian Mora-Carrero, Luis Alonso-Pastor, Ariel Noyman, Yubo Liu, Kent Larson

**Updated**: 2025-08-22T07:50:57Z

**Summary**: Understanding human behavior in urban environments is a crucial field within city sciences. However, collecting accurate behavioral data, particularly in newly developed areas, poses significant challenges. Recent advances in generative agents, powered by Large Language Models (LLMs), have shown promise in simulating human behaviors without relying on extensive datasets. Nevertheless, these methods often struggle with generating consistent, context-sensitive, and realistic behavioral outputs. To address these limitations, this paper introduces the Preference Chain, a novel method that integrates Graph Retrieval-Augmented Generation (RAG) with LLMs to enhance context-aware simulation of human behavior in transportation systems. Experiments conducted on the Replica dataset demonstrate that the Preference Chain outperforms standard LLM in aligning with real-world transportation mode choices. The development of the Mobility Agent highlights potential applications of proposed method in urban mobility modeling for emerging cities, personalized travel behavior analysis, and dynamic traffic forecasting. Despite limitations such as slow inference and the risk of hallucination, the method offers a promising framework for simulating complex human behavior in data-scarce environments, where traditional data-driven models struggle due to limited data availability.

**Link**: [arxiv](http://arxiv.org/abs/2508.16172v1),  [pdf](http://arxiv.org/pdf/2508.16172v1)

**Tags**: cs.AI 



### A Scalable Hybrid Track-Before-Detect Tracking System: Application to   Coastal Maritime Radar Surveillance
**Authors**: Lukas Herrmann, Ángel F. García-Fernández, Edmund F. Brekke, Egil Eide

**Updated**: 2025-08-22T07:45:34Z

**Summary**: Despite their theoretical advantages, track-before-detect (TBD) methods remain largely absent from real-world multi-target tracking applications due to their computational complexity and limited scalability. This paper presents a scalable hybrid tracking framework that combines a TBD multi-target tracking algorithm with a detection-based multi-target tracking algorithm for coastal radar surveillance. In particular, the approach uses an integrated existence Poisson histogram-probabilistic multi-hypothesis tracking (IE-PHPMHT)-based TBD module with a conventional Poisson multi-Bernoulli Mixture (PMBM) point tracker. The system processes raw radar data through land clutter suppression, cell-wise detection, and clustering-based feature extraction. High-threshold detections are used to track strong targets via the point tracker, while low-threshold detections are employed for adaptive birth in the TBD module, enabling early initiation and sustained tracking of weak or ambiguous targets. Validated using real X-band radar data from the Trondheim Fjord, Norway, the approach demonstrates robust multi-target tracking performance in a full-scale application with a large observation area under resource constraints, highlighting its suitability for operational deployment in complex maritime environments needed for coastal surveillance and to support autonomy.

**Link**: [arxiv](http://arxiv.org/abs/2508.16169v1),  [pdf](http://arxiv.org/pdf/2508.16169v1)

**Tags**: eess.SP 



### Towards Recommending Usability Improvements with Multimodal Large   Language Models
**Authors**: Sebastian Lubos, Alexander Felfernig, Gerhard Leitner, Julian Schwazer

**Updated**: 2025-08-22T07:38:37Z

**Summary**: Usability describes a set of essential quality attributes of user interfaces (UI) that influence human-computer interaction. Common evaluation methods, such as usability testing and inspection, are effective but resource-intensive and require expert involvement. This makes them less accessible for smaller organizations. Recent advances in multimodal LLMs offer promising opportunities to automate usability evaluation processes partly by analyzing textual, visual, and structural aspects of software interfaces. To investigate this possibility, we formulate usability evaluation as a recommendation task, where multimodal LLMs rank usability issues by severity. We conducted an initial proof-of-concept study to compare LLM-generated usability improvement recommendations with usability expert assessments. Our findings indicate the potential of LLMs to enable faster and more cost-effective usability evaluation, which makes it a practical alternative in contexts with limited expert resources.

**Link**: [arxiv](http://arxiv.org/abs/2508.16165v1),  [pdf](http://arxiv.org/pdf/2508.16165v1)

**Tags**: cs.SE cs.AI cs.HC 



