# Arxiv Results
## Keyword: kv cache 
 ### A Taxonomy and Comparative Analysis of IPv4 Identifier Selection   Correctness, Security, and Performance
**Authors**: Joshua J. Daymude, Antonio M. Espinoza, Holly Bergen, Benjamin Mixon-Baca, Jeffrey Knockel, Jedidiah R. Crandall

**Updated**: 2025-11-07T18:03:32Z

**Summary**: The battle for a more secure Internet is waged on many fronts, including the most basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an IPv4 header field as old as the Internet with an equally long history as an exploited side channel for scanning network properties, inferring off-path connections, and poisoning DNS caches. This article taxonomizes the 25-year history of IPID-based exploits and the corresponding changes to IPID selection methods. By mathematically analyzing these methods' correctness and security and empirically evaluating their performance, we reveal recommendations for best practice as well as shortcomings of current operating system implementations, emphasizing the value of systematic evaluations in network security.

**Link**: [arxiv](http://arxiv.org/abs/2406.06483v4),  [pdf](http://arxiv.org/pdf/2406.06483v4)

**Tags**: cs.NI cs.CR 



### Inference-Time Hyper-Scaling with KV Cache Compression
**Authors**: Adrian Łańcucki, Konrad Staniszewski, Piotr Nawrot, Edoardo M. Ponti

**Updated**: 2025-11-07T16:42:30Z

**Summary**: Inference-time scaling trades efficiency for increased reasoning accuracy by generating longer or more parallel sequences. However, in Transformer LLMs, generation cost is bottlenecked by the size of the key-value (KV) cache, rather than the number of generated tokens. Hence, we explore inference-time hyper-scaling: by compressing the KV cache, we can generate more tokens within the same compute budget and further improve the accuracy of scaled inference. The success of this approach, however, hinges on the ability of compression methods to preserve accuracy even at high compression ratios. To make hyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a novel method for sparsifying KV caches that only requires 1K training steps to achieve 8$\times$ compression, while maintaining better accuracy than training-free sparse attention. Instead of prematurely discarding cached tokens, DMS delays token eviction, implicitly merging representations and preserving critical information. We demonstrate the effectiveness of inference-time hyper-scaling with DMS on multiple families of LLMs, showing that it boosts accuracy for comparable inference latency and memory load. For instance, we enhance Qwen-R1 32B by 12.0 points on AIME 24, 8.6 on GPQA, and 9.7 on LiveCodeBench on average for an equivalent number of memory reads.

**Link**: [arxiv](http://arxiv.org/abs/2506.05345v2),  [pdf](http://arxiv.org/pdf/2506.05345v2)

**Tags**: cs.LG cs.CL 



### LiveStar: Live Streaming Assistant for Real-World Online Video   Understanding
**Authors**: Zhenyu Yang, Kairui Zhang, Yuhang Hu, Bing Wang, Shengsheng Qian, Bin Wen, Fan Yang, Tingting Gao, Weiming Dong, Changsheng Xu

**Updated**: 2025-11-07T15:00:37Z

**Summary**: Despite significant progress in Video Large Language Models (Video-LLMs) for offline video understanding, existing online Video-LLMs typically struggle to simultaneously process continuous frame-by-frame inputs and determine optimal response timing, often compromising real-time responsiveness and narrative coherence. To address these limitations, we introduce LiveStar, a pioneering live streaming assistant that achieves always-on proactive responses through adaptive streaming decoding. Specifically, LiveStar incorporates: (1) a training strategy enabling incremental video-language alignment for variable-length video streams, preserving temporal consistency across dynamically evolving frame sequences; (2) a response-silence decoding framework that determines optimal proactive response timing via a single forward pass verification; (3) memory-aware acceleration via peak-end memory compression for online inference on 10+ minute videos, combined with streaming key-value cache to achieve 1.53x faster inference. We also construct an OmniStar dataset, a comprehensive dataset for training and benchmarking that encompasses 15 diverse real-world scenarios and 5 evaluation tasks for online video understanding. Extensive experiments across three benchmarks demonstrate LiveStar's state-of-the-art performance, achieving an average 19.5% improvement in semantic correctness with 18.1% reduced timing difference compared to existing online Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks. Our model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar.

**Link**: [arxiv](http://arxiv.org/abs/2511.05299v1),  [pdf](http://arxiv.org/pdf/2511.05299v1)

**Tags**: cs.CV cs.AI 



### kV-Class Lateral NiOx/GaN Super-Heterojunction Diode via Ammonia   Molecular Beam Epitaxy (NH3-MBE)
**Authors**: Yizheng Liu, Zachary J. Biegler, Ashley E. Wissel-Garcia, James S. Speck, Sriram Krishnamoorthy

**Updated**: 2025-11-07T08:07:19Z

**Summary**: This work reports the demonstration of lateral p-NiOx/p-GaN/n-GaN-based super-heterojunction (SHJ) diodes using p-GaN with additional sputtered p-type nickel oxide (NiOx) layers to realize charge-balanced structures. The heterojunction diode capacitance-voltage (C-V) model is applied to extract effective the acceptor concentration from the p-NiOx. Net donor and acceptor concentration in n-GaN and p-GaN are extracted by using metal-oxide-semiconductor (MOS) test structures. The fabricated p-NiOx/p-GaN/n-GaN SHJ diodes with charge-balanced region between anode and cathode exhibit a forward on-state current density of 10-30 mA/mm across an anode-to-cathode distance (LAC) from 16 {\mu}m to 80 {\mu}m. The SHJ diodes show rectifying behavior with a maximum on/off ratio of 10^9 and a low reverse leakage density. The highest breakdown voltage achieved for the SHJ diodes is ~2.8 kV with reverse leakage density of 10^-4 mA/mm at ~80% of devices catastrophic breakdown voltage. The SHJ diodes across all types of dimensions exhibit significant breakdown voltage improvements (~6X on average) with ultra-low reverse leakage current compared to corresponding reference structures without a charge-balanced extension, clearly demonstrating the superjunction effect for devices fabricated on GaN epitaxial layer with ~10^17 cm^-3 electron density.

**Link**: [arxiv](http://arxiv.org/abs/2511.05060v1),  [pdf](http://arxiv.org/pdf/2511.05060v1)

**Tags**: physics.app-ph 



### AWARE: Evaluating PriorityFresh Caching for Offline Emergency Warning   Systems
**Authors**: Charles Melvin, N. Rich Nguyen

**Updated**: 2025-11-07T06:53:48Z

**Summary**: PriorityFresh is a semantic, actionability-first caching policy designed for offline emergency warning systems. Within the AWARE system's simulation environment, PriorityFresh optimizes which alerts to retain and surface under constrained connectivity. Experiments indicate improved actionability-first performance without harming efficiency. A separate Priority Forecasting model is used only to synthesize realistic alert sequences for controlled experiments and does not influence caching or push decisions.

**Link**: [arxiv](http://arxiv.org/abs/2511.05022v1),  [pdf](http://arxiv.org/pdf/2511.05022v1)

**Tags**: cs.NI H.3.3; H.3.4; H.m 



### Towards Mitigating Hallucinations in Large Vision-Language Models by   Refining Textual Embeddings
**Authors**: Aakriti Agrawal, Gouthaman KV, Rohith Aralikatti, Gauri Jagatap, Jiaxin Yuan, Vijay Kamarshi, Andrea Fanelli, Furong Huang

**Updated**: 2025-11-07T06:39:54Z

**Summary**: In this work, we identify an inherent bias in prevailing LVLM architectures toward the language modality, largely resulting from the common practice of simply appending visual embeddings to the input text sequence. To address this, we propose a simple yet effective method that refines textual embeddings by integrating average-pooled visual features. Our approach demonstrably improves visual grounding and significantly reduces hallucinations on established benchmarks. While average pooling offers a straightforward, robust, and efficient means of incorporating visual information, we believe that more sophisticated fusion methods could further enhance visual grounding and cross-modal alignment. Given that the primary focus of this work is to highlight the modality imbalance and its impact on hallucinations -- and to show that refining textual embeddings with visual information mitigates this issue -- we leave exploration of advanced fusion strategies for future work.

**Link**: [arxiv](http://arxiv.org/abs/2511.05017v1),  [pdf](http://arxiv.org/pdf/2511.05017v1)

**Tags**: cs.CV cs.CL 



### SkyWalker: A Locality-Aware Cross-Region Load Balancer for LLM Inference
**Authors**: Tian Xia, Ziming Mao, Jamison Kerney, Ethan J. Jackson, Zhifei Li, Jiarong Xing, Scott Shenker, Ion Stoica

**Updated**: 2025-11-06T21:05:23Z

**Summary**: Serving Large Language Models (LLMs) efficiently in multi-region setups remains a challenge. Due to cost and GPU availability concerns, providers typically deploy LLMs in multiple regions using instance with long-term commitments, like reserved instances or on-premise clusters, which are often underutilized due to their region-local traffic handling and diurnal traffic variance. In this paper, we introduce SkyWalker, a multi-region load balancer for LLM inference that aggregates regional diurnal patterns through cross-region traffic handling. By doing so, SkyWalker enables providers to reserve instances based on expected global demand, rather than peak demand in each individual region. Meanwhile, SkyWalker preserves KV-Cache locality and load balancing, ensuring cost efficiency without sacrificing performance. SkyWalker achieves this with a cache-aware cross-region traffic handler and a selective pushing based load balancing mechanism. Our evaluation on real-world workloads shows that it achieves 1.12-2.06x higher throughput and 1.74-6.30x lower latency compared to existing load balancers, while reducing total serving cost by 25%.

**Link**: [arxiv](http://arxiv.org/abs/2505.24095v2),  [pdf](http://arxiv.org/pdf/2505.24095v2)

**Tags**: cs.DC 



### Simplex-FEM Networks (SiFEN): Learning A Triangulated Function   Approximator
**Authors**: Chaymae Yahyati, Ismail Lamaakal, Khalid El Makkaoui, Ibrahim Ouahbi, Yassine Maleh

**Updated**: 2025-11-06T20:49:13Z

**Summary**: We introduce Simplex-FEM Networks (SiFEN), a learned piecewise-polynomial predictor that represents f: R^d -> R^k as a globally C^r finite-element field on a learned simplicial mesh in an optionally warped input space. Each query activates exactly one simplex and at most d+1 basis functions via barycentric coordinates, yielding explicit locality, controllable smoothness, and cache-friendly sparsity. SiFEN pairs degree-m Bernstein-Bezier polynomials with a light invertible warp and trains end-to-end with shape regularization, semi-discrete OT coverage, and differentiable edge flips. Under standard shape-regularity and bi-Lipschitz warp assumptions, SiFEN achieves the classic FEM approximation rate M^(-m/d) with M mesh vertices. Empirically, on synthetic approximation tasks, tabular regression/classification, and as a drop-in head on compact CNNs, SiFEN matches or surpasses MLPs and KANs at matched parameter budgets, improves calibration (lower ECE/Brier), and reduces inference latency due to geometric locality. These properties make SiFEN a compact, interpretable, and theoretically grounded alternative to dense MLPs and edge-spline networks.

**Link**: [arxiv](http://arxiv.org/abs/2511.04804v1),  [pdf](http://arxiv.org/pdf/2511.04804v1)

**Tags**: cs.LG 



### DuetServe: Harmonizing Prefill and Decode for LLM Serving via Adaptive   GPU Multiplexing
**Authors**: Lei Gao, Chaoyi Jiang, Hossein Entezari Zarch, Daniel Wong, Murali Annavaram

**Updated**: 2025-11-06T20:18:34Z

**Summary**: Modern LLM serving systems must sustain high throughput while meeting strict latency SLOs across two distinct inference phases: compute-intensive prefill and memory-bound decode phases. Existing approaches either (1) aggregate both phases on shared GPUs, leading to interference between prefill and decode phases, which degrades time-between-tokens (TBT); or (2) disaggregate the two phases across GPUs, improving latency but wasting resources through duplicated models and KV cache transfers. We present DuetServe, a unified LLM serving framework that achieves disaggregation-level isolation within a single GPU. DuetServe operates in aggregated mode by default and dynamically activates SM-level GPU spatial multiplexing when TBT degradation is predicted. Its key idea is to decouple prefill and decode execution only when needed through fine-grained, adaptive SM partitioning that provides phase isolation only when contention threatens latency service level objectives (SLOs). DuetServe integrates (1) an attention-aware roofline model to forecast iteration latency, (2) a partitioning optimizer that selects the optimal SM split to maximize throughput under TBT constraints, and (3) an interruption-free execution engine that eliminates CPU-GPU synchronization overhead. Evaluations show that DuetServe improves total throughput by up to 1.3x while maintaining low generation latency compared to state-of-the-art frameworks.

**Link**: [arxiv](http://arxiv.org/abs/2511.04791v1),  [pdf](http://arxiv.org/pdf/2511.04791v1)

**Tags**: cs.LG 



### SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators
**Authors**: Jonathan Li, Nasim Farahini, Evgenii Iuliugin, Magnus Vesterlund, Christian Haggstrom, Guangtao Wang, Shubhangi Upasani, Ayush Sachdeva, Rui Li, Faline Fu, Chen Wu, Ayesha Siddiqua, John Long, Tuowen Zhao, Matheen Musaddiq, Hakan Zeffer, Yun Du, Mingran Wang, Qinghua Li, Bo Li, Urmish Thakker, Raghu Prabhakar

**Updated**: 2025-11-06T18:27:11Z

**Summary**: The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+ context length support have resulted in increasing demands for on-chip memory to support large KV caches. Techniques such as StreamingLLM and SnapKV demonstrate how to control KV cache size while maintaining model accuracy. Yet, these techniques are not commonly used within industrial deployments using frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static graphs and continuous batching methodology employed by these frameworks make it difficult to admit modifications to the standard multi-head attention algorithm, while on the other hand, the accuracy implications of such techniques on modern instruction-following and reasoning models are not well understood, obfuscating the need for implementing these techniques. In this paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators running at 128k context length and up to 1832 tokens per second in a real production setting. SnapStream enables $4\times$ improved on-chip memory usage and introduces minimal accuracy degradation on LongBench-v2, AIME24 and LiveCodeBench. To the best of our knowledge, this is the first implementation of sparse KV attention techniques deployed in a production inference system with static graphs and continuous batching.

**Link**: [arxiv](http://arxiv.org/abs/2511.03092v2),  [pdf](http://arxiv.org/pdf/2511.03092v2)

**Tags**: cs.AI cs.AR cs.DC 



### Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache   Asymmetry for Long-Context LLMs
**Authors**: Wanyun Cui, Mingwei Xu

**Updated**: 2025-11-06T17:09:52Z

**Summary**: Recent advances in Large Language Models (LLMs) have highlighted the critical importance of extending context length, yet the quadratic complexity of attention mechanisms poses significant challenges for efficient long-context modeling. KV cache compression has emerged as a key approach to address this challenge. Through extensive empirical analysis, we reveal a fundamental yet previously overlooked asymmetry in KV caches: while adjacent keys receive similar attention weights ({\it local homogeneity}), adjacent values demonstrate distinct {\it heterogeneous} distributions. This key-value asymmetry reveals a critical limitation in existing compression methods that treat keys and values uniformly. To address the limitation, we propose a training-free compression framework (AsymKV) that combines homogeneity-based key merging with a mathematically proven lossless value compression. Extensive experiments demonstrate that AsymKV consistently outperforms existing long-context methods across various tasks and base models. For example, on LLaMA3.1-8B, AsymKV achieves an average score of 43.95 on LongBench, surpassing SOTA methods like H$_2$O (38.89) by a large margin.Our code can be found in this link:https://github.com/the-scale-lab/Asymkv.

**Link**: [arxiv](http://arxiv.org/abs/2506.05410v2),  [pdf](http://arxiv.org/pdf/2506.05410v2)

**Tags**: cs.CL I.2.7 



### Scalable Domain-decomposed Monte Carlo Neutral Transport for Nuclear   Fusion
**Authors**: Oskar Lappi, Huw Leggate, Yannick Marandet, Jan Åström, Keijo Heljanko, Dmitriy V. Borodin

**Updated**: 2025-11-06T16:08:24Z

**Summary**: EIRENE [1] is a Monte Carlo neutral transport solver heavily used in the fusion community. EIRENE does not implement domain decomposition, making it impossible to use for simulations where the grid data does not fit on one compute node (see e.g. [2]). This paper presents a domain-decomposed Monte Carlo (DDMC) algorithm implemented in a new open source Monte Carlo code, Eiron. Two parallel algorithms currently used in EIRENE are also implemented in Eiron, and the three algorithms are compared by running strong scaling tests, with DDMC performing better than the other two algorithms in nearly all cases. On the supercomputer Mahti [3], DDMC strong scaling is superlinear for grids that do not fit into an L3 cache slice (4 MiB). The DDMC algorithm is also scaled up to 16384 cores in weak scaling tests, with a weak scaling efficiency of 45% in a high-collisional (heavier compute load) case, and 26% in a low-collisional (lighter compute load) case. We conclude that implementing this domain decomposition algorithm in EIRENE would improve performance and enable simulations that are currently impossible due to memory constraints.

**Link**: [arxiv](http://arxiv.org/abs/2511.04489v1),  [pdf](http://arxiv.org/pdf/2511.04489v1)

**Tags**: physics.comp-ph cs.DC cs.PF 68W10 (Primary), 68W15, 65C05 (Secondary) D.1.3; G.3; I.6.8; J.2 



### Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context
**Authors**: Carnot Braun, Rafael O. Jarczewski, Gabriel U. Talasso, Leandro A. Villas, Allan M. de Souza

**Updated**: 2025-11-06T15:37:11Z

**Summary**: Traditional vehicle routing systems efficiently optimize singular metrics like time or distance, and when considering multiple metrics, they need more processes to optimize . However, they lack the capability to interpret and integrate the complex, semantic, and dynamic contexts of human drivers, such as multi-step tasks, situational constraints, or urgent needs. This paper introduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a hybrid agentic assistant designed to augment classical pathfinding algorithms with contextual reasoning. Our approach employs a Large Language Model (LLM) agent that operates on a candidate set of routes generated by a multi-objective (time, CO2) Dijkstra algorithm. The agent evaluates these options against user-provided tasks, preferences, and avoidance rules by leveraging a pre-processed geospatial cache of urban Points of Interest (POIs). In a benchmark of realistic urban scenarios, PAVe successfully used complex user intent into appropriate route modifications, achieving over 88% accuracy in its initial route selections with a local model. We conclude that combining classical routing algorithms with an LLM-based semantic reasoning layer is a robust and effective approach for creating personalized, adaptive, and scalable solutions for urban mobility optimization.

**Link**: [arxiv](http://arxiv.org/abs/2511.04464v1),  [pdf](http://arxiv.org/pdf/2511.04464v1)

**Tags**: cs.AI 



### Temporal Action Selection for Action Chunking
**Authors**: Yueyang Weng, Xiaopeng Zhang, Yongjin Mu, Yingcong Zhu, Yanjie Li, Qi Liu

**Updated**: 2025-11-06T14:52:54Z

**Summary**: Action chunking is a widely adopted approach in Learning from Demonstration (LfD). By modeling multi-step action chunks rather than single-step actions, action chunking significantly enhances modeling capabilities for human expert policies. However, the reduced decision frequency restricts the utilization of recent observations, degrading reactivity - particularly evident in the inadequate adaptation to sensor noise and dynamic environmental changes. Existing efforts to address this issue have primarily resorted to trading off reactivity against decision consistency, without achieving both. To address this limitation, we propose a novel algorithm, Temporal Action Selector (TAS), which caches predicted action chunks from multiple timesteps and dynamically selects the optimal action through a lightweight selector network. TAS achieves balanced optimization across three critical dimensions: reactivity, decision consistency, and motion coherence. Experiments across multiple tasks with diverse base policies show that TAS significantly improves success rates - yielding an absolute gain of up to 73.3%. Furthermore, integrating TAS as a base policy with residual reinforcement learning (RL) substantially enhances training efficiency and elevates the performance plateau. Experiments in both simulation and physical robots confirm the method's efficacy.

**Link**: [arxiv](http://arxiv.org/abs/2511.04421v1),  [pdf](http://arxiv.org/pdf/2511.04421v1)

**Tags**: cs.RO 



### Dynamic Jointly Batch Selection for Data Efficient Machine Translation   Fine-Tuning
**Authors**: Mohammad Amin Ghanizadeh, Mohammad Javad Dousti

**Updated**: 2025-11-06T14:33:29Z

**Summary**: Data quality and its effective selection are fundamental to improving the performance of machine translation models, serving as cornerstones for achieving robust and reliable translation systems. This paper presents a data selection methodology specifically designed for fine-tuning machine translation systems, which leverages the synergy between a learner model and a pre-trained reference model to enhance overall training effectiveness. By defining a learnability score, our approach systematically evaluates the utility of data points for training, ensuring that only the most relevant and impactful examples contribute to the fine-tuning process. Furthermore, our method employs a batch selection strategy which considers interdependencies among data points, optimizing the efficiency of the training process while maintaining a focus on data relevance. Experiments on English to Persian and several other language pairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that our method can achieve up to a fivefold improvement in data efficiency compared to an iid baseline. Experimental results indicate that our approach improves computational efficiency by 24 when utilizing cached embeddings, as it requires fewer training data points. Additionally, it enhances generalization, resulting in superior translation performance compared to random selection method.

**Link**: [arxiv](http://arxiv.org/abs/2511.04406v1),  [pdf](http://arxiv.org/pdf/2511.04406v1)

**Tags**: cs.CL 



### A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale   Reconstruction with External Memory
**Authors**: Felix Windisch, Thomas Köhler, Lukas Radl, Michael Steiner, Dieter Schmalstieg, Markus Steinberger

**Updated**: 2025-11-06T13:44:12Z

**Summary**: Gaussian Splatting has emerged as a high-performance technique for novel view synthesis, enabling real-time rendering and high-quality reconstruction of small scenes. However, scaling to larger environments has so far relied on partitioning the scene into chunks -- a strategy that introduces artifacts at chunk boundaries, complicates training across varying scales, and is poorly suited to unstructured scenarios such as city-scale flyovers combined with street-level views. Moreover, rendering remains fundamentally limited by GPU memory, as all visible chunks must reside in VRAM simultaneously. We introduce A LoD of Gaussians, a framework for training and rendering ultra-large-scale Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our method stores the full scene out-of-core (e.g., in CPU memory) and trains a Level-of-Detail (LoD) representation directly, dynamically streaming only the relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with Sequential Point Trees enables efficient, view-dependent LoD selection, while a lightweight caching and view scheduling system exploits temporal coherence to support real-time streaming and rendering. Together, these innovations enable seamless multi-scale reconstruction and interactive visualization of complex scenes -- from broad aerial views to fine-grained ground-level details.

**Link**: [arxiv](http://arxiv.org/abs/2507.01110v3),  [pdf](http://arxiv.org/pdf/2507.01110v3)

**Tags**: cs.GR cs.LG 



### Memory- and Latency-Constrained Inference of Large Language Models via   Adaptive Split Computing
**Authors**: Mingyu Sung, Vikas Palakonda, Suhwan Im, Sunghwan Moon, Il-Min Kim, Sangseok Yun, Jae-Mo Kang

**Updated**: 2025-11-06T02:55:07Z

**Summary**: Large language models (LLMs) have achieved near-human performance across diverse reasoning tasks, yet their deployment on resource-constrained Internet-of-Things (IoT) devices remains impractical due to massive parameter footprints and memory-intensive autoregressive decoding. While split computing offers a promising solution by partitioning model execution between edge devices and cloud servers, existing approaches fail to address the unique challenges of autoregressive inference, particularly the iterative token generation process and expanding key-value (KV) cache requirements. This work introduces the first autoregressive-aware split computing framework designed explicitly for LLM deployment on edge devices. Our approach makes three key contributions. First, we develop one-point split compression (OPSC), a mixed-precision quantization scheme that prevents out-of-memory failures by strategically partitioning models into front-end and back-end segments with different precision levels. Second, we propose a two-stage intermediate compression pipeline that combines threshold splitting (TS) and token-wise adaptive bit quantization (TAB-Q) to preserve accuracy-critical activations while dramatically reducing communication overhead. Third, we formulate a unified optimization framework that jointly selects optimal split points, quantization settings, and sequence lengths to satisfy strict memory and latency constraints. Extensive evaluations across diverse LLMs and hardware platforms demonstrate superior performance compared to state-of-the-art quantization methods, including SmoothQuant, OmniQuant, and Atom. The framework achieves a 1.49 inference speedup and significant communication overhead reduction while maintaining or improving model accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2511.04002v1),  [pdf](http://arxiv.org/pdf/2511.04002v1)

**Tags**: cs.LG cs.AI 



### HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts
**Authors**: Neil He, Rishabh Anand, Hiren Madhu, Ali Maatouk, Smita Krishnaswamy, Leandros Tassiulas, Menglin Yang, Rex Ying

**Updated**: 2025-11-06T01:18:03Z

**Summary**: Large language models (LLMs) have shown great success in text modeling tasks across domains. However, natural language exhibits inherent semantic hierarchies and nuanced geometric structure, which current LLMs do not capture completely owing to their reliance on Euclidean operations. Recent studies have also shown that not respecting the geometry of token embeddings leads to training instabilities and degradation of generative capabilities. These findings suggest that shifting to non-Euclidean geometries can better align language models with the underlying geometry of text. We thus propose to operate fully in Hyperbolic space, known for its expansive, scale-free, and low-distortion properties. We thus introduce HELM, a family of HypErbolic Large Language Models, offering a geometric rethinking of the Transformer-based LLM that addresses the representational inflexibility, missing set of necessary operations, and poor scalability of existing hyperbolic LMs. We additionally introduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert operates in a distinct curvature space to encode more fine-grained geometric structure from text, as well as a dense model, HELM-D. For HELM-MICE, we further develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient, reduced-KV-cache training and inference. For both models, we develop essential hyperbolic equivalents of rotary positional encodings and RMS normalization. We are the first to train fully hyperbolic LLMs at billion-parameter scale, and evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM problem-solving, general knowledge, and commonsense reasoning. Our results show consistent gains from our HELM architectures -- up to 4% -- over popular Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy and enhanced reasoning afforded by hyperbolic geometry in large-scale LM pretraining.

**Link**: [arxiv](http://arxiv.org/abs/2505.24722v2),  [pdf](http://arxiv.org/pdf/2505.24722v2)

**Tags**: cs.LG cs.AI 



### From Minutes to Seconds: Redefining the Five-Minute Rule for AI-Era   Memory Hierarchies
**Authors**: Tong Zhang, Vikram Sharma Mailthody, Fei Sun, Linsen Ma, Chris J. Newburn, Teresa Zhang, Yang Liu, Jiangpeng Li, Hao Zhong, Wen-Mei Hwu

**Updated**: 2025-11-06T00:42:29Z

**Summary**: In 1987, Jim Gray and Gianfranco Putzolu introduced the five-minute rule, a simple, storage-memory-economics-based heuristic for deciding when data should live in DRAM rather than on storage. Subsequent revisits to the rule largely retained that economics-only view, leaving host costs, feasibility limits, and workload behavior out of scope. This paper revisits the rule from first principles, integrating host costs, DRAM bandwidth/capacity, and physics-grounded models of SSD performance and cost, and then embedding these elements in a constraint- and workload-aware framework that yields actionable provisioning guidance. We show that, for modern AI platforms, especially GPU-centric hosts paired with ultra-high-IOPS SSDs engineered for fine-grained random access, the DRAM-to-flash caching threshold collapses from minutes to a few seconds. This shift reframes NAND flash memory as an active data tier and exposes a broad research space across the hardware-software stack. We further introduce MQSim-Next, a calibrated SSD simulator that supports validation and sensitivity analysis and facilitates future architectural and system research. Finally, we present two concrete case studies that showcase the software system design space opened by such memory hierarchy paradigm shift. Overall, we turn a classical heuristic into an actionable, feasibility-aware analysis and provisioning framework and set the stage for further research on AI-era memory hierarchy.

**Link**: [arxiv](http://arxiv.org/abs/2511.03944v1),  [pdf](http://arxiv.org/pdf/2511.03944v1)

**Tags**: cs.AR 



### Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM   Inference
**Authors**: Donghyeon Joo, Helya Hosseini, Ramyad Hadidi, Bahar Asgari

**Updated**: 2025-11-06T00:11:18Z

**Summary**: We demonstrate that unstructured sparsity significantly improves KV cache compression for LLMs, enabling sparsity levels up to 70% without compromising accuracy or requiring fine-tuning. We conduct a systematic exploration of pruning strategies and find per-token magnitude-based pruning as highly effective for both Key and Value caches under unstructured sparsity, surpassing prior structured pruning schemes. The Key cache benefits from prominent outlier elements, while the Value cache surprisingly benefits from a simple magnitude-based pruning despite its uniform distribution. KV cache size is the major bottleneck in decode performance due to high memory overhead for large context lengths. To address this, we use a bitmap-based sparse format and a custom attention kernel capable of compressing and directly computing over compressed caches pruned to arbitrary sparsity patterns, significantly accelerating memory-bound operations in decode computations and thereby compensating for the overhead of runtime pruning and compression. Our custom attention kernel coupled with the bitmap-based format delivers substantial compression of KV cache upto 45% of dense inference and thereby enables longer context length and increased tokens/sec throughput of upto 2.23x compared to dense inference. Our pruning mechanism and sparse attention kernel is available at https://github.com/dhjoo98/mustafar.

**Link**: [arxiv](http://arxiv.org/abs/2505.22913v2),  [pdf](http://arxiv.org/pdf/2505.22913v2)

**Tags**: cs.LG 



### Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label   LLM-Based Classification
**Authors**: Mikołaj Langner, Jan Eliasz, Ewa Rudnicka, Jan Kocoń

**Updated**: 2025-11-05T19:53:51Z

**Summary**: We introduce a method for efficient multi-label text classification with large language models (LLMs), built on reformulating classification tasks as sequences of dichotomic (yes/no) decisions. Instead of generating all labels in a single structured response, each target dimension is queried independently, which, combined with a prefix caching mechanism, yields substantial efficiency gains for short-text inference without loss of accuracy. To demonstrate the approach, we focus on affective text analysis, covering 24 dimensions including emotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator model (DeepSeek-V3) provides multiple annotations per text, which are aggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B, Gemma3-1B). The fine-tuned models show significant improvements over zero-shot baselines, particularly on the dimensions seen during training. Our findings suggest that decomposing multi-label classification into dichotomic queries, combined with distillation and cache-aware inference, offers a scalable and effective framework for LLM-based classification. While we validate the method on affective states, the approach is general and applicable across domains.

**Link**: [arxiv](http://arxiv.org/abs/2511.03830v1),  [pdf](http://arxiv.org/pdf/2511.03830v1)

**Tags**: cs.CL 



### Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction
**Authors**: Yuerong Song, Xiaoran Liu, Ruixiao Li, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu

**Updated**: 2025-11-05T14:29:12Z

**Summary**: Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and parallel decoding but suffer from prohibitive quadratic computational complexity and memory overhead during inference. Current caching techniques accelerate decoding by storing full-layer states, yet impose substantial memory usage that limit long-context applications. Our analysis of attention patterns in dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining salient across decoding steps and low-relevance tokens staying unimportant, motivating selective cache eviction. We propose Sparse-dLLM, the first training-free framework integrating dynamic cache eviction with sparse attention via delayed bidirectional sparse caching. By leveraging the stability of token saliency over steps, it retains critical tokens and dynamically evicts unimportant prefix/suffix entries using an attention-guided strategy. Extensive experiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to 10$\times$ higher throughput than vanilla dLLMs, with comparable performance and similar peak memory costs, outperforming previous methods in efficiency and effectiveness. The code is available at https://github.com/OpenMOSS/Sparse-dLLM.

**Link**: [arxiv](http://arxiv.org/abs/2508.02558v2),  [pdf](http://arxiv.org/pdf/2508.02558v2)

**Tags**: cs.CL 



### RAGBoost: Efficient Retrieval-Augmented Generation with   Accuracy-Preserving Context Reuse
**Authors**: Yinsicheng Jiang, Yeqi Huang, Liang Cheng, Cheng Deng, Xuan Sun, Luo Mai

**Updated**: 2025-11-05T13:59:01Z

**Summary**: Retrieval-augmented generation (RAG) enhances large language models (LLMs) with retrieved context but often suffers from downgraded prefill performance as modern applications demand longer and more complex inputs. Existing caching techniques either preserve accuracy with low cache reuse or improve reuse at the cost of degraded reasoning quality. We present RAGBoost, an efficient RAG system that achieves high cache reuse without sacrificing accuracy through accuracy-preserving context reuse. RAGBoost detects overlapping retrieved items across concurrent sessions and multi-turn interactions, using efficient context indexing, ordering, and de-duplication to maximize reuse, while lightweight contextual hints maintain reasoning fidelity. It integrates seamlessly with existing LLM inference engines and improves their prefill performance by 1.5-3X over state-of-the-art methods, while preserving or even enhancing reasoning accuracy across diverse RAG and agentic AI workloads. Our code is released at: https://github.com/Edinburgh-AgenticAI/RAGBoost.

**Link**: [arxiv](http://arxiv.org/abs/2511.03475v1),  [pdf](http://arxiv.org/pdf/2511.03475v1)

**Tags**: cs.LG 



### MagCache: Fast Video Generation with Magnitude-Aware Cache
**Authors**: Zehong Ma, Longhui Wei, Feng Wang, Shiliang Zhang, Qi Tian

**Updated**: 2025-11-05T09:18:48Z

**Summary**: Existing acceleration techniques for video diffusion models often rely on uniform heuristics or time-embedding variants to skip timesteps and reuse cached features. These approaches typically require extensive calibration with curated prompts and risk inconsistent outputs due to prompt-specific overfitting. In this paper, we introduce a novel and robust discovery: a unified magnitude law observed across different models and prompts. Specifically, the magnitude ratio of successive residual outputs decreases monotonically, steadily in most timesteps while rapidly in the last several steps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache) that adaptively skips unimportant timesteps using an error modeling mechanism and adaptive caching strategy. Unlike existing methods requiring dozens of curated samples for calibration, MagCache only requires a single sample for calibration. Experimental results show that MagCache achieves 2.10x-2.68x speedups on Open-Sora, CogVideoX, Wan 2.1, and HunyuanVideo, while preserving superior visual fidelity. It significantly outperforms existing methods in LPIPS, SSIM, and PSNR, under similar computational budgets.

**Link**: [arxiv](http://arxiv.org/abs/2506.09045v2),  [pdf](http://arxiv.org/pdf/2506.09045v2)

**Tags**: cs.CV 



### Joint Optimization of DNN Model Caching and Request Routing in Mobile   Edge Computing
**Authors**: Shuting Qiu, Fang Dong, Siyu Tan, Ruiting Zhou, Dian Shen, Patrick P. C. Lee, Qilin Fan

**Updated**: 2025-11-05T03:40:44Z

**Summary**: Mobile edge computing (MEC) can pre-cache deep neural networks (DNNs) near end-users, providing low-latency services and improving users' quality of experience (QoE). However, caching all DNN models at edge servers with limited capacity is difficult, and the impact of model loading time on QoE remains underexplored. Hence, we introduce dynamic DNNs in edge scenarios, disassembling a complete DNN model into interrelated submodels for more fine-grained and flexible model caching and request routing solutions. This raises the pressing issue of jointly deciding request routing and submodel caching for dynamic DNNs to balance model inference precision and loading latency for QoE optimization. In this paper, we study the joint dynamic model caching and request routing problem in MEC networks, aiming to maximize user request inference precision under constraints of server resources, latency, and model loading time. To tackle this problem, we propose CoCaR, an offline algorithm based on linear programming and random rounding that leverages dynamic DNNs to optimize caching and routing schemes, achieving near-optimal performance. Furthermore, we develop an online variant of CoCaR, named CoCaR-OL, enabling effective adaptation to dynamic and unpredictable online request patterns. The simulation results demonstrate that the proposed CoCaR improves the average inference precision of user requests by 46\% compared to state-of-the-art baselines. In addition, in online scenarios, CoCaR-OL achieves an improvement of no less than 32.3\% in user QoE over competitive baselines.

**Link**: [arxiv](http://arxiv.org/abs/2511.03159v1),  [pdf](http://arxiv.org/pdf/2511.03159v1)

**Tags**: cs.NI 



### Cache Mechanism for Agent RAG Systems
**Authors**: Shuhang Lin, Zhencan Peng, Lingyao Li, Xiao Lin, Xi Zhu, Yongfeng Zhang

**Updated**: 2025-11-04T19:02:29Z

**Summary**: Recent advances in Large Language Model (LLM)-based agents have been propelled by Retrieval-Augmented Generation (RAG), which grants the models access to vast external knowledge bases. Despite RAG's success in improving agent performance, agent-level cache management, particularly constructing, maintaining, and updating a compact, relevant corpus dynamically tailored to each agent's need, remains underexplored. Therefore, we introduce ARC (Agent RAG Cache Mechanism), a novel, annotation-free caching framework that dynamically manages small, high-value corpora for each agent. By synthesizing historical query distribution patterns with the intrinsic geometry of cached items in the embedding space, ARC automatically maintains a high-relevance cache. With comprehensive experiments on three retrieval datasets, our experimental results demonstrate that ARC reduces storage requirements to 0.015% of the original corpus while offering up to 79.8% has-answer rate and reducing average retrieval latency by 80%. Our results demonstrate that ARC can drastically enhance efficiency and effectiveness in RAG-powered LLM agents.

**Link**: [arxiv](http://arxiv.org/abs/2511.02919v1),  [pdf](http://arxiv.org/pdf/2511.02919v1)

**Tags**: cs.CL 



### Non-Contact Manipulation of Induced Magnetic Dipoles
**Authors**: Seth Stewart, Joseph Pawelski, Steve Ward, Andrew J. Petruska

**Updated**: 2025-11-04T17:40:31Z

**Summary**: Extending the field of magnetic manipulation to conductive, non-magnetic objects opens the door for a wide array of applications previously limited to hard or soft magnetic materials. Of particular interest is the recycling of space debris through the use of oscillating magnetic fields, which represent a cache of raw materials in an environment particularly suited to the low forces generated from inductive magnetic manipulation. Building upon previous work that demonstrated 3D open-loop position control by leveraging the opposing dipole moment created from induced eddy currents, this work demonstrates closed-loop position control of a semi-buoyant aluminum sphere in lab tests, and the efficacy of varying methods for force inversion is explored. The closed-loop methods represent a critical first step towards wider applications for 3-DOF position control of induced magnetic dipoles.

**Link**: [arxiv](http://arxiv.org/abs/2511.02761v1),  [pdf](http://arxiv.org/pdf/2511.02761v1)

**Tags**: cs.RO 



### Using Span Queries to Optimize for Cache and Attention Locality
**Authors**: Paul Castro, Nick Mitchell, Nathan Ordonez, Thomas Parnell, Mudhakar Srivatsa, Antoni Viros i Martin

**Updated**: 2025-11-04T17:22:49Z

**Summary**: Clients are evolving beyond chat completion, and now include a variety of innovative inference-time scaling and deep reasoning techniques. At the same time, inference servers remain heavily optimized for chat completion. Prior work has shown that large improvements to KV cache hit rate are possible if inference servers evolve towards these non-chat use cases. However, they offer solutions that are also optimized for a single use case, RAG. In this paper, we introduce the span query to generalize the interface to the inference server. We demonstrate that chat, RAG, inference-time scaling, and agentic workloads can all be expressed as span queries. We show how the critical distinction that had been assumed by prior work lies in whether the order of the inputs matter -- do they commute? In chat, they do not. In RAG, they often do. This paper introduces span queries, which are expression trees of inference calls, linked together with commutativity constraints. We describe span query syntax and semantics. We show how they can be automatically optimized to improve KV cache locality. We show how a small change to vLLM (affecting only 492 lines) can enable high-performance execution of span queries. Using this stack, we demonstrate that span queries can achieve 10-20x reductions in TTFT for two distinct non-chat use cases. Finally, we show that span queries can also be optimized to improve attention locality, so as to avoid the so-called lost-in-the-middle problem. We demonstrate that an attention-optimized span query on a 2b parameter model vastly outperforms the accuracy of a stock inference server using an 8b model.

**Link**: [arxiv](http://arxiv.org/abs/2511.02749v1),  [pdf](http://arxiv.org/pdf/2511.02749v1)

**Tags**: cs.AI 



### Apriel-H1: Towards Efficient Enterprise Reasoning Models
**Authors**: Oleksiy Ostapenko, Luke Kumar, Raymond Li, Denis Kocetkov, Joel Lamy-Poirier, Shruthan Radhakrishna, Soham Parikh, Shambhavi Mishra, Sebastien Paquet, Srinivas Sunkara, Valérie Bécaert, Sathwik Tejaswi Madhusudhan, Torsten Scholak

**Updated**: 2025-11-04T15:17:43Z

**Summary**: Large Language Models (LLMs) achieve remarkable reasoning capabilities through transformer architectures with attention mechanisms. However, transformers suffer from quadratic time and memory complexity in the attention module (MHA) and require caching key-value states during inference, which severely limits throughput and scalability. High inference throughput is critical for agentic tasks, long-context reasoning, efficient deployment under high request loads, and more efficient test-time compute scaling.   State Space Models (SSMs) such as Mamba offer a promising alternative with linear inference complexity and a constant memory footprint via recurrent computation with fixed-size hidden states. In this technical report we introduce the Apriel-H1 family of hybrid LLMs that combine transformer attention and SSM sequence mixers for efficient reasoning at 15B model size. These models are obtained through incremental distillation from a pretrained reasoning transformer, Apriel-Nemotron-15B-Thinker, progressively replacing less critical attention layers with linear Mamba blocks.   We release multiple post-distillation variants of Apriel-H1-15B-Thinker with different SSM-to-MHA ratios and analyse how reasoning performance degrades as more Mamba layers replace MHA. Additionally, we release a 30/50 hybrid variant of Apriel-H1, further fine-tuned on a supervised dataset of reasoning traces, achieving over 2x higher inference throughput when deployed in the production-ready vLLM environment, with minimal degradation in reasoning performance. This shows that distilled hybrid SSM-Transformer architectures can deliver substantial efficiency gains over the pretrained transformer equivalent without substantially compromising the reasoning quality.

**Link**: [arxiv](http://arxiv.org/abs/2511.02651v1),  [pdf](http://arxiv.org/pdf/2511.02651v1)

**Tags**: cs.LG cs.AI 



### Federated Attention: A Distributed Paradigm for Collaborative LLM   Inference over Edge Networks
**Authors**: Xiumei Deng, Zehui Xiong, Binbin Chen, Dong In Kim, Merouane Debbah, H. Vincent Poor

**Updated**: 2025-11-04T15:14:58Z

**Summary**: Large language models (LLMs) are proliferating rapidly at the edge, delivering intelligent capabilities across diverse application scenarios. However, their practical deployment in collaborative scenarios confronts fundamental challenges: privacy vulnerabilities, communication overhead, and computational bottlenecks. To address these, we propose Federated Attention (FedAttn), which integrates the federated paradigm into the self-attention mechanism, creating a new distributed LLM inference framework that simultaneously achieves privacy protection, communication efficiency, and computational efficiency. FedAttn enables participants to perform local self-attention over their own token representations while periodically exchanging and aggregating Key-Value (KV) matrices across multiple Transformer blocks, collaboratively generating LLM responses without exposing private prompts. Further, we identify a structural duality between contextual representation refinement in FedAttn and parameter optimization in FL across private data, local computation, and global aggregation. This key insight provides a principled foundation for systematically porting federated optimization techniques to collaborative LLM inference. Building on this framework, we theoretically analyze how local self-attention computation within participants and heterogeneous token relevance among participants shape error propagation dynamics across Transformer blocks. Moreover, we characterize the fundamental trade-off between response quality and communication/computation efficiency, which is governed by the synchronization interval and the number of participants. Experimental results validate our theoretical analysis, and reveal significant optimization opportunities through sparse attention and adaptive KV aggregation, highlighting FedAttn's potential to deliver scalability and efficiency in real-world edge deployments.

**Link**: [arxiv](http://arxiv.org/abs/2511.02647v1),  [pdf](http://arxiv.org/pdf/2511.02647v1)

**Tags**: cs.DC cs.AI cs.LG 



### Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning
**Authors**: Chaofan Lin, Jiaming Tang, Shuo Yang, Hanshuo Wang, Tian Tang, Boyu Tian, Ion Stoica, Song Han, Mingyu Gao

**Updated**: 2025-11-04T12:04:06Z

**Summary**: Leveraging attention sparsity to accelerate long-context large language models (LLMs) has been a hot research topic. However, current algorithms such as sparse attention or key-value (KV) cache compression tend to use a fixed budget, which presents a significant challenge during deployment because it fails to account for the dynamic nature of real-world scenarios, where the optimal balance between accuracy and efficiency can vary greatly. In this paper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse attention can surprisingly achieve adaptive budgeting. Based on this, we propose Twilight, a framework to bring adaptive sparsity to any existing sparse attention algorithm without sacrificing their accuracy. Empirical results show that Twilight can adaptively prune at most 98% of redundant tokens, leading to $15.4\times$ acceleration in self-attention operations and $3.9\times$ acceleration in end-to-end per token latency in long context LLM decoding.

**Link**: [arxiv](http://arxiv.org/abs/2502.02770v5),  [pdf](http://arxiv.org/pdf/2502.02770v5)

**Tags**: cs.LG cs.CL 



### Laser diagnostics for negative ion source optimization: insights from   SPIDER at the ITER Neutral Beam Test Facility
**Authors**: R. Agnello, M. Barbisan, R. Pasqualotto, B. Pouradier-Duteil, E. Sartori, A. Tiso, B. Zaniol

**Updated**: 2025-11-04T09:03:09Z

**Summary**: The ITER Heating Neutral Beams (HNBs) require large, high-energy H/D atom beams (285/330 A/m^2 extracted current density, and 1/0.87 MeV acceleration energy, respectively for H and D). To address the associated challenges, the SPIDER negative ion RF beam source at the Neutral Beam Test Facility (NBTF) in Padova (Italy) serves as a full-scale source prototype with a 100 kV triode accelerator, for design validation and performance verification. SPIDER is equipped with two advanced laser diagnostics to monitor key plasma parameters; Cavity Ring-Down Spectroscopy (CRDS) is used to measure H$^-$\slash D$^-$ ion densities, while Laser Absorption Spectroscopy (LAS) tracks caesium neutral density in the source. These measurements are essential for optimizing negative ion production and meeting ITER source targets. We present diagnostic upgrade details, recent experimental results, and correlations with other machine parameters. Since CRDS relies on a single 4.637-meter-long optical cavity, the longest used in such sources, it has demonstrated sensitivity to alignment. Based on recent experimental experience, structural improvements are being implemented to enhance both stability and measurement reliability. LAS has mainly been employed as a tool to monitor the caesium conditioning status of SPIDER. Additionally, due to a distributed measurement over four lines of sight, LAS has proven effective in monitoring the caesium distribution within the source. This work demonstrates the essential role of laser diagnostics in developing ITER-relevant plasma sources and informs ongoing efforts to improve measurement accuracy in challenging environments.

**Link**: [arxiv](http://arxiv.org/abs/2511.02381v1),  [pdf](http://arxiv.org/pdf/2511.02381v1)

**Tags**: physics.plasm-ph 



### Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV   Cache Time-to-Live
**Authors**: Hanchen Li, Qiuyang Mang, Runyuan He, Qizheng Zhang, Huanzhi Mao, Xiaokun Chen, Alvin Cheung, Joseph Gonzalez, Ion Stoica

**Updated**: 2025-11-04T03:43:05Z

**Summary**: Agentic LLM applications interleave LLM generation requests with tool calls. These tool calls break the continuity of the workflow by creating pauses between LLM requests, bringing many challenges for the serving system, especially under multi-turn scenarios. Each pause potentially causes KV cache eviction and extra waiting time before entering the continuous batch for the following LLM request. Since these pauses happen for each call, this problem becomes increasingly severe as turn number grow for agentic programs. Previous works either fail to incorporate information from the tool call, evicting KV cache that leads to repetitive prefill or loading, or ignore the continuity of a multi-turn program, creating waiting time between turns that increases per-request latency.   We present Continuum, a serving system to optimize job completion time for multi-turn agent workloads by combining tool-aware KV cache timeout with program-level scheduling. By predicting tool call durations in agentic workflows, Continuum selectively pins the KV cache in GPU memory with a time-to-live value based on total turn number. When combined with program-level first-come-first-serve, Continuum prevents scheduling bubbles, preserves multi-turn continuity, and optimizes for throughput for complex agentic workflows. By modeling the variability of tool call and agent program continuity, Continuum outperforms state-of-the-art baselines. Our evaluation on real-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B models shows that Continuum significantly improves the average job completion times, and remains performant across different hardware setups and DRAM offloading schemes. Preview code is available at: https://github.com/Hanchenli/vllm-continuum

**Link**: [arxiv](http://arxiv.org/abs/2511.02230v1),  [pdf](http://arxiv.org/pdf/2511.02230v1)

**Tags**: cs.OS cs.AI cs.NI 



### Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA   Effects
**Authors**: Mansi Choudhary, Karthik Sangaiah, Sonali Singh, Muhammad Osama, Lisa Wu Wills, Ganesh Dasika

**Updated**: 2025-11-03T23:48:39Z

**Summary**: The rise of disaggregated AI GPUs has exposed a critical bottleneck in large-scale attention workloads: non-uniform memory access (NUMA). As multi-chiplet designs become the norm for scaling compute capabilities, memory latency and bandwidth vary sharply across compute regions, undermining the performance of traditional GPU kernel scheduling strategies that assume uniform memory access. We identify how these NUMA effects distort locality in multi-head attention (MHA) and present Swizzled Head-first Mapping, a spatially-aware scheduling strategy that aligns attention heads with GPU NUMA domains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our method achieves up to 50% higher performance over state-of-the-art attention algorithms using conventional scheduling techniques and sustains consistently high L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware scheduling is now fundamental to achieving full efficiency on next-generation disaggregated GPUs, offering a path forward for scalable AI training and inference.

**Link**: [arxiv](http://arxiv.org/abs/2511.02132v1),  [pdf](http://arxiv.org/pdf/2511.02132v1)

**Tags**: cs.AR cs.DC cs.LG cs.PF 



### KV Cache Transform Coding for Compact Storage in LLM Inference
**Authors**: Konrad Staniszewski, Adrian Łańcucki

**Updated**: 2025-11-03T18:20:35Z

**Summary**: Serving large language models (LLMs) at scale necessitates efficient key-value (KV) cache management. KV caches can be reused across conversation turns via shared-prefix prompts that are common in iterative code editing and chat. However, stale caches consume scarce GPU memory, require offloading, or force recomputation. We present KVTC, a lightweight transform coder that compresses KV caches for compact on-GPU and off-GPU storage. Drawing on classical media compression, KVTC combines PCA-based feature decorrelation, adaptive quantization, and entropy coding. It requires only a brief initial calibration and leaves model parameters unchanged. By exploiting redundancies in KV caches, KVTC achieves up to 20$\times$ compression while maintaining reasoning and long-context accuracy, and 40$\times$ or higher for specific use cases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across benchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and MATH-500. It consistently outperforms inference-time baselines such as token eviction, quantization, and SVD-based methods, while achieving higher compression ratios. These results support KVTC as a practical building block for memory-efficient LLM serving with reusable KV caches.

**Link**: [arxiv](http://arxiv.org/abs/2511.01815v1),  [pdf](http://arxiv.org/pdf/2511.01815v1)

**Tags**: cs.CL cs.AI cs.LG 



### Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with   Efficient LLM Serving
**Authors**: Chengying Huan, Ziheng Meng, Yongchao Liu, Zhengyi Yang, Yun Zhu, Yue Yun, Shipeng Li, Rong Gu, Xiabao Wu, Haitao Zhang, Chuntao Hong, Shaonan Ma, Guihai Chen, Chen Tian

**Updated**: 2025-11-03T14:42:53Z

**Summary**: Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to perform step-by-step reasoning over graph-structured knowledge, but existing pipelines suffer from low accuracy, excessive token usage, high latency, and low throughput due to single-agent monolithic prompts, repeated context re-encoding, and inefficient serving execution. We present GLM, the first multi-agent Graph-CoT system co-designed with an optimized LLM serving architecture. GLM decomposes reasoning into specialized agents for classification, reasoning, action generation, and graph retrieval, enabling branching and selective context sharing to reduce prompt length and reasoning iterations while preserving reasoning quality, thereby improving accuracy and reducing overall token consumption. To scale inference, we introduce a Graph-CoT-aware LLM inference mechanism with graph-specific KV-cache management, priority-based eviction, and pipelined execution to improve serving efficiency. Experiments demonstrate that GLM improves answer accuracy by up to 38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and achieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT baselines, enabling efficient adoption for complex real-world reasoning at scale.

**Link**: [arxiv](http://arxiv.org/abs/2511.01633v1),  [pdf](http://arxiv.org/pdf/2511.01633v1)

**Tags**: cs.LG cs.AI 



### Representation Consistency for Accurate and Coherent LLM Answer   Aggregation
**Authors**: Junqi Jiang, Tom Bewley, Salim I. Amoukou, Francesco Leofante, Antonio Rago, Saumitra Mishra, Francesca Toni

**Updated**: 2025-11-03T11:32:13Z

**Summary**: Test-time scaling improves large language models' (LLMs) performance by allocating more compute budget during inference. To achieve this, existing methods often require intricate modifications to prompting and sampling strategies. In this work, we introduce representation consistency (RC), a test-time scaling method for aggregating answers drawn from multiple candidate responses of an LLM regardless of how they were generated, including variations in prompt phrasing and sampling strategy. RC enhances answer aggregation by not only considering the number of occurrences of each answer in the candidate response set, but also the consistency of the model's internal activations while generating the set of responses leading to each answer. These activations can be either dense (raw model activations) or sparse (encoded via pretrained sparse autoencoders). Our rationale is that if the model's representations of multiple responses converging on the same answer are highly variable, this answer is more likely to be the result of incoherent reasoning and should be down-weighted during aggregation. Importantly, our method only uses cached activations and lightweight similarity computations and requires no additional model queries. Through experiments with four open-source LLMs and four reasoning datasets, we validate the effectiveness of RC for improving task performance during inference, with consistent accuracy improvements (up to 4%) over strong test-time scaling baselines. We also show that consistency in the sparse activation signals aligns well with the common notion of coherent reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2506.21590v2),  [pdf](http://arxiv.org/pdf/2506.21590v2)

**Tags**: cs.CL cs.LG 



### Memory-Efficient Training with In-Place FFT Implementation
**Authors**: Xinyu Ding, Bangtian Liu, Siyu Liao, Zhongfeng Wang

**Updated**: 2025-11-03T09:36:11Z

**Summary**: Fast Fourier Transforms (FFT) are widely used to reduce memory and computational costs in deep learning. However, existing implementations, including standard FFT and real FFT (rFFT), cannot achieve true in-place computation. In particular, rFFT maps an input of size n to a complex output of size n/2+1, causing dimensional mismatch and requiring additional memory allocation. We propose the first real-domain, fully in-place FFT framework (rdFFT) that preserves input-output memory space consistency. By leveraging butterfly operation symmetry and conjugate properties in the frequency domain, we design an implicit complex encoding scheme that eliminates intermediate cache usage entirely. Experiments on multiple natural language understanding tasks demonstrate the method effectiveness in reducing training memory cost, offering a promising direction for frequency-domain lightweight adaptation.

**Link**: [arxiv](http://arxiv.org/abs/2511.01385v1),  [pdf](http://arxiv.org/pdf/2511.01385v1)

**Tags**: cs.LG I.2.6; G.1.2; D.1.3 



### MotionStream: Real-Time Video Generation with Interactive Motion   Controls
**Authors**: Joonghyuk Shin, Zhengqi Li, Richard Zhang, Jun-Yan Zhu, Jaesik Park, Eli Schechtman, Xun Huang

**Updated**: 2025-11-03T06:37:53Z

**Summary**: Current motion-conditioned video generation methods suffer from prohibitive latency (minutes per video) and non-causal processing that prevents real-time interaction. We present MotionStream, enabling sub-second latency with up to 29 FPS streaming generation on a single GPU. Our approach begins by augmenting a text-to-video model with motion control, which generates high-quality videos that adhere to the global text prompt and local motion guidance, but does not perform inference on the fly. As such, we distill this bidirectional teacher into a causal student through Self Forcing with Distribution Matching Distillation, enabling real-time streaming inference. Several key challenges arise when generating videos of long, potentially infinite time-horizons: (1) bridging the domain gap from training on finite length and extrapolating to infinite horizons, (2) sustaining high quality by preventing error accumulation, and (3) maintaining fast inference, without incurring growth in computational cost due to increasing context windows. A key to our approach is introducing carefully designed sliding-window causal attention, combined with attention sinks. By incorporating self-rollout with attention sinks and KV cache rolling during training, we properly simulate inference-time extrapolations with a fixed context window, enabling constant-speed generation of arbitrarily long videos. Our models achieve state-of-the-art results in motion following and video quality while being two orders of magnitude faster, uniquely enabling infinite-length streaming. With MotionStream, users can paint trajectories, control cameras, or transfer motion, and see results unfold in real-time, delivering a truly interactive experience.

**Link**: [arxiv](http://arxiv.org/abs/2511.01266v1),  [pdf](http://arxiv.org/pdf/2511.01266v1)

**Tags**: cs.CV cs.LG 



### Multi-head Temporal Latent Attention
**Authors**: Keqi Deng, Philip C. Woodland

**Updated**: 2025-11-02T20:27:27Z

**Summary**: While Transformer self-attention offers strong parallelism, the Key-Value (KV) cache grows linearly with sequence length and becomes a bottleneck for inference efficiency. Multi-head latent attention was recently developed to compress the KV cache into a low-rank latent space. This paper proposes Multi-head Temporal Latent Attention (MTLA), which further reduces the KV cache size along the temporal dimension, greatly lowering the memory footprint of self-attention inference. MTLA employs a hyper-network to dynamically merge temporally adjacent KV cache vectors. To address the mismatch between the compressed KV cache and processed sequence lengths, a stride-aware causal mask is proposed to ensure efficient parallel training and consistency with inference behaviour. Experiments across tasks, including speech translation, speech recognition, speech understanding and text summarisation, demonstrate that MTLA achieves competitive performance compared to standard Multi-Head Attention (MHA), while greatly improving inference speed and GPU memory usage. For example, on a English-German speech translation task, MTLA achieves a 5.3x speedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA, while maintaining translation quality.

**Link**: [arxiv](http://arxiv.org/abs/2505.13544v3),  [pdf](http://arxiv.org/pdf/2505.13544v3)

**Tags**: cs.LG cs.AI 



### FlexiCache: Leveraging Temporal Stability of Attention Heads for   Efficient KV Cache Management
**Authors**: Nazmul Takbir, Hamidreza Alikhani, Nikil Dutt, Sangeetha Abdu Jyothi

**Updated**: 2025-11-02T09:33:12Z

**Summary**: Large Language Model (LLM) serving is increasingly constrained by the growing size of the key-value (KV) cache, which scales with both context length and generation length. Prior work shows that attention is dominated by a small subset of critical tokens, yet existing systems struggle to exploit this efficiently without degrading accuracy, especially in long generation. We make a key observation: the temporal stability of these critical tokens varies significantly across KV heads: some heads consistently focus on the same tokens, while others shift frequently. Building on this insight, we introduce FlexiCache, a hierarchical KV-cache management system that leverages the temporal stability of KV heads to reduce GPU memory usage and computation overhead, while preserving model accuracy. FlexiCache classifies KV heads as stable or unstable: it retains all KV-cache pages from unstable heads in GPU memory, whereas for stable heads, it keeps only the top-K pages on the GPU and offloads the rest to host memory. By exploiting temporal stability, FlexiCache performs periodic reranking for stable heads to fetch newly promoted top pages. Implemented atop vLLM, FlexiCache reduces GPU memory footprint for long-context requests by up to 70%, improves offline serving throughput by 1.38-1.55x, and lowers online token latency by 1.6-2.1x, all while maintaining accuracy in long-context, long-generation scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2511.00868v1),  [pdf](http://arxiv.org/pdf/2511.00868v1)

**Tags**: cs.LG 



### Optimizing Native Sparse Attention with Latent Attention and Local   Global Alternating Strategies
**Authors**: Yuxuan Hu, Jianchao Tan, Jiaqi Zhang, Wen Zan, Pingwei Sun, Yifan Lu, Yerui Sun, Yuchen Xie, Xunliang Cai, Jing Zhang

**Updated**: 2025-11-02T06:15:14Z

**Summary**: In this work, we conduct a systematic analysis of Native Sparse Attention (NSA) and propose targeted improvements that enhance long-context modeling. A key insight is that alternating between local (sliding-window) and global (compression, selective) attention across layers, rather than using fixed patterns, enables more effective propagation of long-range dependencies and substantially boosts performance on long-sequence tasks. Meanwhile, we further refine NSA's branches with Latent Attention that the sliding-window branch is enhanced with Multi-head Latent Attention (MLA) while compression and selective branches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache memory by 50\% versus NSA while improving the model's common-sense reasoning and long-text understanding capabilities. Experiments on models from 340M to 1.3B parameters (trained on 15B and 100B tokens) show our method matches or exceeds full attention and native sparse attention in both common-sense reasoning and long-context understanding tasks.

**Link**: [arxiv](http://arxiv.org/abs/2511.00819v1),  [pdf](http://arxiv.org/pdf/2511.00819v1)

**Tags**: cs.CL 



### High-Power Dual-Channel Field Chamber for High-Frequency Magnetic   Neuromodulation
**Authors**: Xiaoyang Tian, Hui Wang, Boshuo Wang, Jinshui Zhang, Dong Yan, Jeannette Ingabire, Samantha Coffler, Guillaume Duret, Quoc-Khanh Pham, Gang Bao, Jacob T. Robinson, Stefan M. Goetz, Angel V. Peterchev

**Updated**: 2025-11-02T00:04:54Z

**Summary**: Several novel methods, including magnetogenetics and magnetoelectric stimulation, use high frequency alternating magnetic fields to precisely manipulate neural activity. To quantify the behavioral effects of such interventions in a freely moving mouse, we developed a dual-channel magnetic chamber, specifically designed for rate-sensitive magnetothermal-genetic stimulation, and adaptable for other uses of alternating magnetic fields. Through an optimized coil design, the system allows independent control of two spatially orthogonal uniform magnetic fields delivered at different frequencies within a 10 cm x 10 cm x 6 cm chamber. The two channels have nominal frequencies of 50 and 550 kHz with peak magnetic field strengths of 88 and 12.5 mT, achieved with resonant coil drives having peak voltages of 1.6 and 1.8 kV and currents of 1.0 and 0.26 kA, respectively. Additionally, a liquid cooling system enables magnetic field generation for second-level duration, and an observation port and camera allow video capture of the animal's behavior within the chamber. The system generates high-amplitude magnetic fields across two widely separated frequency channels with negligible interference (< 1%). Relatively uniform magnetic field distribution (+/-10% across 94% of the chamber volume) is maintained throughout the chamber, and temperature increase of the inner side of the coil enclosure during the operation is limited to < 0.35 {\deg}C/s to ensure in vivo safety. Using cobalt-doped and undoped iron oxide nanoparticles, we demonstrate channel-specific heating rates of 3.5 {\deg}C/s and 1.5 {\deg}C/s, respectively, validating frequency-selectivity. Both channels can run continuously for four seconds stably.

**Link**: [arxiv](http://arxiv.org/abs/2511.00745v1),  [pdf](http://arxiv.org/pdf/2511.00745v1)

**Tags**: eess.SY cs.SY physics.med-ph q-bio.NC 



### Kimi Linear: An Expressive, Efficient Attention Architecture
**Authors**: Kimi Team, Yu Zhang, Zongyu Lin, Xingcheng Yao, Jiaxi Hu, Fanqing Meng, Chengyin Liu, Xin Men, Songlin Yang, Zhiyuan Li, Wentao Li, Enzhe Lu, Weizhou Liu, Yanru Chen, Weixin Xu, Longhui Yu, Yejie Wang, Yu Fan, Longguang Zhong, Enming Yuan, Dehao Zhang, Yizhi Zhang, T. Y. Liu, Haiming Wang, Shengjun Fang, Weiran He, Shaowei Liu, Yiwei Li, Jianlin Su, Jiezhong Qiu, Bo Pang, Junjie Yan, Zhejun Jiang, Weixiao Huang, Bohong Yin, Jiacheng You, Chu Wei, Zhengtao Wang, Chao Hong, Yutian Chen, Guanduo Chen, Yucheng Wang, Huabin Zheng, Feng Wang, Yibo Liu, Mengnan Dong, Zheng Zhang, Siyuan Pan, Wenhao Wu, Yuhao Wu, Longyu Guan, Jiawen Tao, Guohong Fu, Xinran Xu, Yuzhi Wang, Guokun Lai, Yuxin Wu, Xinyu Zhou, Zhilin Yang, Yulun Du

**Updated**: 2025-11-01T12:05:18Z

**Summary**: We introduce Kimi Linear, a hybrid linear attention architecture that, for the first time, outperforms full attention under fair comparisons across various scenarios -- including short-context, long-context, and reinforcement learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an expressive linear attention module that extends Gated DeltaNet with a finer-grained gating mechanism, enabling more effective use of limited finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR) transition matrices, which substantially reduces computation compared to the general DPLR formulation while remaining more consistent with the classical delta rule.   We pretrain a Kimi Linear model with 3B activated parameters and 48B total parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention (MLA). Our experiments show that with an identical training recipe, Kimi Linear outperforms full MLA with a sizeable margin across all evaluated tasks, while reducing KV cache usage by up to 75% and achieving up to 6 times decoding throughput for a 1M context. These results demonstrate that Kimi Linear can be a drop-in replacement for full attention architectures with superior performance and efficiency, including tasks with longer input and output lengths.   To support further research, we open-source the KDA kernel and vLLM implementations, and release the pre-trained and instruction-tuned model checkpoints.

**Link**: [arxiv](http://arxiv.org/abs/2510.26692v2),  [pdf](http://arxiv.org/pdf/2510.26692v2)

**Tags**: cs.CL cs.LG 



### Drinfeld associators and Kashiwara-Vergne associators in higher genera
**Authors**: Toyo Taniguchi

**Updated**: 2025-11-01T09:53:47Z

**Summary**: For $g\geq 0$, a genus $g$ Kashiwara-Vergne associator, introduced by Alekseev-Kawazumi-Kuno-Naef as a solution to the generalised KV equations in relation to the formality problem of the Goldman-Turaev Lie bialgebra on an oriented surface with a framing, is directly constructed from a genus $g$ analogue of a Drinfeld associator formulated by Gonzalez, which we call a Gonzalez-Drinfeld associator. The proof is based on Massuyeau's work in genus 0. The framing is automatically determined from the choice of a Gonzalez-Drinfeld associator, and in the case of genus 1, we show that only one particular framing is realised by our construction.

**Link**: [arxiv](http://arxiv.org/abs/2511.00473v1),  [pdf](http://arxiv.org/pdf/2511.00473v1)

**Tags**: math.QA math.AT 57K20(primary), 16T05, 16W70, 18M75, 20F36, 20F40, 57M05 (secondary) 



### A Survey on Cache Methods in Diffusion Models: Toward Efficient   Multi-Modal Generation
**Authors**: Jiacheng Liu, Xinyu Wang, Yuqi Lin, Zhikai Wang, Peiru Wang, Peiliang Cai, Qinming Zhou, Zhengan Yan, Zexuan Yan, Zhengyi Shi, Chang Zou, Yue Ma, Linfeng Zhang

**Updated**: 2025-11-01T08:49:20Z

**Summary**: Diffusion Models have become a cornerstone of modern generative AI for their exceptional generation quality and controllability. However, their inherent \textit{multi-step iterations} and \textit{complex backbone networks} lead to prohibitive computational overhead and generation latency, forming a major bottleneck for real-time applications. Although existing acceleration techniques have made progress, they still face challenges such as limited applicability, high training costs, or quality degradation.   Against this backdrop, \textbf{Diffusion Caching} offers a promising training-free, architecture-agnostic, and efficient inference paradigm. Its core mechanism identifies and reuses intrinsic computational redundancies in the diffusion process. By enabling feature-level cross-step reuse and inter-layer scheduling, it reduces computation without modifying model parameters. This paper systematically reviews the theoretical foundations and evolution of Diffusion Caching and proposes a unified framework for its classification and analysis.   Through comparative analysis of representative methods, we show that Diffusion Caching evolves from \textit{static reuse} to \textit{dynamic prediction}. This trend enhances caching flexibility across diverse tasks and enables integration with other acceleration techniques such as sampling optimization and model distillation, paving the way for a unified, efficient inference framework for future multimodal and interactive applications. We argue that this paradigm will become a key enabler of real-time and efficient generative AI, injecting new vitality into both theory and practice of \textit{Efficient Generative Intelligence}.

**Link**: [arxiv](http://arxiv.org/abs/2510.19755v3),  [pdf](http://arxiv.org/pdf/2510.19755v3)

**Tags**: cs.LG cs.AI cs.CV 



### KVCOMM: Online Cross-context KV-cache Communication for Efficient   LLM-based Multi-agent Systems
**Authors**: Hancheng Ye, Zhengqi Gao, Mingyuan Ma, Qinsi Wang, Yuzhe Fu, Ming-Yu Chung, Yueqian Lin, Zhijian Liu, Jianyi Zhang, Danyang Zhuo, Yiran Chen

**Updated**: 2025-11-01T08:26:24Z

**Summary**: Multi-agent large language model (LLM) systems are increasingly adopted for complex language processing tasks that require communication and coordination among agents. However, these systems often suffer substantial overhead from repeated reprocessing of overlapping contexts across agents. In typical pipelines, once an agent receives a message from its predecessor, the full context-including prior turns-must be reprocessed from scratch, leading to inefficient processing. While key-value (KV) caching is an effective solution for avoiding redundant computation in single-agent settings where prefixes remain unchanged, it cannot be directly reused in multi-agent scenarios due to diverging prefixes introduced by agent-specific context extensions. We identify that the core challenge lies in the offset variance of KV-caches across agents. To address this, we propose KVCOMM, a training-free framework that enables efficient prefilling in multi-agent inference by reusing KV-caches and aligning cache offsets of overlapping contexts under diverse prefix contexts. KVCOMM estimates and adjusts KV-caches for shared content by referencing a pool of cached examples-termed anchors-that store observed cache deviations under varying prefixes. The anchor pool is maintained and updated online, allowing dynamic adaptation to distinct user requests and context structures. KVCOMM achieves over 70% reuse rate across diverse multi-agent workloads, including retrieval-augmented generation, math reasoning, and collaborative coding tasks, all without quality degradation. Particularly, when each fully-connected agent receives 1K input tokens with 512 prefix tokens and 512 output tokens under a five-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard prefill pipeline, reducing TTFT from ~430 ms to ~55 ms.

**Link**: [arxiv](http://arxiv.org/abs/2510.12872v2),  [pdf](http://arxiv.org/pdf/2510.12872v2)

**Tags**: cs.MA cs.AI stat.ML 



### Jarvis: Towards Personalized AI Assistant via Personal KV-Cache   Retrieval
**Authors**: Binxiao Xu, Junyu Feng, Shaolin Lu, Yulin Luo, Shilin Yan, Hao Liang, Ming Lu, Wentao Zhang

**Updated**: 2025-11-01T07:01:00Z

**Summary**: The rapid development of Vision-language models (VLMs) enables open-ended perception and reasoning. Recent works have started to investigate how to adapt general-purpose VLMs into personalized assistants. Even commercial models such as ChatGPT now support model personalization by incorporating user-specific information. However, existing methods either learn a set of concept tokens or train a VLM to utilize user-specific information. However, both pipelines struggle to generate accurate answers as personalized assistants. We introduce Jarvis, an innovative framework for a personalized AI assistant through personal KV-Cache retrieval, which stores user-specific information in the KV-Caches of both textual and visual tokens. The textual tokens are created by summarizing user information into metadata, while the visual tokens are produced by extracting distinct image patches from the user's images. When answering a question, Jarvis first retrieves related KV-Caches from personal storage and uses them to ensure accuracy in responses. We also introduce a fine-grained benchmark built with the same distinct image patch mining pipeline, emphasizing accurate question answering based on fine-grained user-specific information. Jarvis is capable of providing more accurate responses, particularly when they depend on specific local details. Jarvis achieves state-of-the-art results in both visual question answering and text-only tasks across multiple datasets, indicating a practical path toward personalized AI assistants. The code and dataset will be released.

**Link**: [arxiv](http://arxiv.org/abs/2510.22765v2),  [pdf](http://arxiv.org/pdf/2510.22765v2)

**Tags**: cs.AI 



### Robustifying Learning-Augmented Caching Efficiently without Compromising   1-Consistency
**Authors**: Peng Chen, Hailiang Zhao, Jiaji Zhang, Xueyan Tang, Yixuan Wang, Shuiguang Deng

**Updated**: 2025-11-01T04:26:03Z

**Summary**: The online caching problem aims to minimize cache misses when serving a sequence of requests under a limited cache size. While naive learning-augmented caching algorithms achieve ideal $1$-consistency, they lack robustness guarantees. Existing robustification methods either sacrifice $1$-consistency or introduce excessive computational overhead. In this paper, we introduce Guard, a lightweight robustification framework that enhances the robustness of a broad class of learning-augmented caching algorithms to $2H_{k-1} + 2$, while preserving their $1$-consistency. Guard achieves the current best-known trade-off between consistency and robustness, with only O(1) additional per-request overhead, thereby maintaining the original time complexity of the base algorithm. Extensive experiments across multiple real-world datasets and prediction models validate the effectiveness of Guard in practice.

**Link**: [arxiv](http://arxiv.org/abs/2507.16242v7),  [pdf](http://arxiv.org/pdf/2507.16242v7)

**Tags**: cs.DS cs.LG 



### Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled   KV-Cache Management Beyond GPU Limits
**Authors**: Dowon Kim, MinJae Lee, Janghyeon Kim, HyuckSung Kwon, Hyeonggyu Jeong, Sang-Soo Park, Minyong Yoon, Si-Dong Roh, Yongsuk Kwon, Jinin So, Jungwook Choi

**Updated**: 2025-10-31T23:50:44Z

**Summary**: The expansion of context windows in large language models (LLMs) to multi-million tokens introduces severe memory and compute bottlenecks, particularly in managing the growing Key-Value (KV) cache. While Compute Express Link (CXL) enables non-eviction frameworks that offload the full KV-cache to scalable external memory, these frameworks still suffer from costly data transfers when recalling non-resident KV tokens to limited GPU memory as context lengths increase. This work proposes scalable Processing-Near-Memory (PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that coordinates memory and computation beyond GPU limits. Our design offloads token page selection to a PNM accelerator within CXL memory, eliminating costly recalls and enabling larger GPU batch sizes. We further introduce a hybrid parallelization strategy and a steady-token selection mechanism to enhance compute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM system, our solution delivers consistent performance gains for LLMs with up to 405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV) and GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x throughput improvement, up to 60x lower energy per token, and up to 7.3x better total cost efficiency than the baseline, demonstrating that CXL-enabled multi-PNM architectures can serve as a scalable backbone for future long-context LLM inference.

**Link**: [arxiv](http://arxiv.org/abs/2511.00321v1),  [pdf](http://arxiv.org/pdf/2511.00321v1)

**Tags**: cs.AR cs.AI 



### AttnCache: Accelerating Self-Attention Inference for LLM Prefill via   Attention Cache
**Authors**: Dinghong Song, Yuan Feng, Yiwei Wang, Shangye Chen, Cyril Guyot, Filip Blagojevic, Hyeran Jeon, Pengfei Su, Dong Li

**Updated**: 2025-10-31T18:19:55Z

**Summary**: Large Language Models (LLMs) are widely used in generative applications such as chatting, code generation, and reasoning. However, many realworld workloads such as classification, question answering, recommendation, and text embedding rely solely on the prefill stage of inference, where the model encodes input sequences without performing autoregressive decoding. In these prefill only scenarios, the self-attention computation becomes the primary performance bottleneck due to its quadratic complexity with respect to sequence length. In this paper, we observe that semantically different sentences often produce similar attention maps across layers and heads. Building on this insight, we propose AttnCache, a framework that accelerates the prefill stage of LLM inference by retrieving and reusing similar attention maps. Based on an attention map memorization database, AttnCache employs efficient caching and similarity search techniques to identify and reuse pre-cached attention maps during inference, thereby reducing the computational overhead of self-attention. Experimental results show that AttnCache achieves an average of 1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x attention speedup on GPU, with negligible accuracy degradation.

**Link**: [arxiv](http://arxiv.org/abs/2510.25979v2),  [pdf](http://arxiv.org/pdf/2510.25979v2)

**Tags**: cs.CL cs.LG 



### SpecAttn: Speculating Sparse Attention
**Authors**: Harsh Shah

**Updated**: 2025-10-31T17:12:34Z

**Summary**: Large Language Models (LLMs) face significant computational bottlenecks during inference due to the quadratic complexity of self-attention mechanisms, particularly as context lengths increase. We introduce SpecAttn, a novel training-free approach that seamlessly integrates with existing speculative decoding techniques to enable efficient sparse attention in pre-trained transformers. Our key insight is to exploit the attention weights already computed by the draft model during speculative decoding to identify important tokens for the target model, eliminating redundant computation while maintaining output quality. SpecAttn employs three core techniques: KL divergence-based layer alignment between draft and target models, a GPU-optimized sorting-free algorithm for top-p token selection from draft attention patterns, and dynamic key-value cache pruning guided by these predictions. By leveraging the computational work already performed in standard speculative decoding pipelines, SpecAttn achieves over 75% reduction in key-value cache accesses with a mere 15.29% increase in perplexity on the PG-19 dataset, significantly outperforming existing sparse attention methods. Our approach demonstrates that speculative execution can be enhanced to provide approximate verification without significant performance degradation.

**Link**: [arxiv](http://arxiv.org/abs/2510.27641v1),  [pdf](http://arxiv.org/pdf/2510.27641v1)

**Tags**: cs.CL cs.LG cs.SY eess.SY 



### VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation
**Authors**: Heng Ping, Arijit Bhattacharjee, Peiyu Zhang, Shixuan Li, Wei Yang, Anzhe Cheng, Xiaole Zhang, Jesse Thomason, Ali Jannesari, Nesreen Ahmed, Paul Bogdan

**Updated**: 2025-10-31T16:40:58Z

**Summary**: Automation of Register Transfer Level (RTL) design can help developers meet increasing computational demands. Large Language Models (LLMs) show promise for Hardware Description Language (HDL) generation, but face challenges due to limited parametric knowledge and domain-specific constraints. While prompt engineering and fine-tuning have limitations in knowledge coverage and training costs, multi-agent architectures offer a training-free paradigm to enhance reasoning through collaborative generation. However, current multi-agent approaches suffer from two critical deficiencies: susceptibility to noise propagation and constrained reasoning space exploration. We propose VeriMoA, a training-free mixture-of-agents (MoA) framework with two synergistic innovations. First, a quality-guided caching mechanism to maintain all intermediate HDL outputs and enables quality-based ranking and selection across the entire generation process, encouraging knowledge accumulation over layers of reasoning. Second, a multi-path generation strategy that leverages C++ and Python as intermediate representations, decomposing specification-to-HDL translation into two-stage processes that exploit LLM fluency in high-resource languages while promoting solution diversity. Comprehensive experiments on VerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves 15--30% improvements in Pass@1 across diverse LLM backbones, especially enabling smaller models to match larger models and fine-tuned alternatives without requiring costly training.

**Link**: [arxiv](http://arxiv.org/abs/2510.27617v1),  [pdf](http://arxiv.org/pdf/2510.27617v1)

**Tags**: cs.AI 



### Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in   Masked Diffusion
**Authors**: Satoshi Hayakawa, Yuhta Takida, Masaaki Imaizumi, Hiromi Wakaki, Yuki Mitsufuji

**Updated**: 2025-10-31T05:31:58Z

**Summary**: Masked diffusion models have shown promising performance in generating high-quality samples in a wide range of domains, but accelerating their sampling process remains relatively underexplored. To investigate efficient samplers for masked diffusion, this paper theoretically analyzes the MaskGIT sampler for image modeling, revealing its implicit temperature sampling mechanism. Through this analysis, we introduce the "moment sampler," an asymptotically equivalent but more tractable and interpretable alternative to MaskGIT, which employs a "choose-then-sample" approach by selecting unmasking positions before sampling tokens. In addition, we improve the efficiency of choose-then-sample algorithms through two key innovations: a partial caching technique for transformers that approximates longer sampling trajectories without proportional computational cost, and a hybrid approach formalizing the exploration-exploitation trade-off in adaptive unmasking. Experiments in image and text domains demonstrate our theory as well as the efficiency of our proposed methods, advancing both theoretical understanding and practical implementation of masked diffusion samplers.

**Link**: [arxiv](http://arxiv.org/abs/2510.04525v2),  [pdf](http://arxiv.org/pdf/2510.04525v2)

**Tags**: cs.LG math.PR stat.ML 



### H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance   Acceleration of Generative Diffusion Models
**Authors**: Mingyu Sung, Il-Min Kim, Sangseok Yun, Jae-Mo Kang

**Updated**: 2025-10-31T04:47:14Z

**Summary**: Diffusion models have emerged as state-of-the-art in image generation, but their practical deployment is hindered by the significant computational cost of their iterative denoising process. While existing caching techniques can accelerate inference, they often create a challenging trade-off between speed and fidelity, suffering from quality degradation and high computational overhead. To address these limitations, we introduce H2-Cache, a novel hierarchical caching mechanism designed for modern generative diffusion model architectures. Our method is founded on the key insight that the denoising process can be functionally separated into a structure-defining stage and a detail-refining stage. H2-cache leverages this by employing a dual-threshold system, using independent thresholds to selectively cache each stage. To ensure the efficiency of our dual-check approach, we introduce pooled feature summarization (PFS), a lightweight technique for robust and fast similarity estimation. Extensive experiments on the Flux architecture demonstrate that H2-cache achieves significant acceleration (up to 5.08x) while maintaining image quality nearly identical to the baseline, quantitatively and qualitatively outperforming existing caching methods. Our work presents a robust and practical solution that effectively resolves the speed-quality dilemma, significantly lowering the barrier for the real-world application of high-fidelity diffusion models. Source code is available at https://github.com/Bluear7878/H2-cache-A-Hierarchical-Dual-Stage-Cache.

**Link**: [arxiv](http://arxiv.org/abs/2510.27171v1),  [pdf](http://arxiv.org/pdf/2510.27171v1)

**Tags**: cs.CV cs.AI 



### Tokencake: A KV-Cache-centric Serving Framework for LLM-based   Multi-Agent Applications
**Authors**: Zhuohang Bian, Feiyang Wu, Teng Ma, Youwei Zhuo

**Updated**: 2025-10-31T04:17:05Z

**Summary**: Large Language Models (LLMs) are increasingly deployed in complex multi-agent applications that use external function calls. This workload creates severe performance challenges for the KV Cache: space contention leads to the eviction of critical agents' caches and time underutilization leaves the cache of agents stalled on long-running tool calls idling in GPU memory. We present Tokencake, a KV-Cache-centric serving framework that co-optimizes scheduling and memory management with an agent-aware design. Tokencake's Space Scheduler uses dynamic memory partitioning to shield critical agents from contention, while its Time Scheduler employs a proactive offload and predictive upload mechanism to repurpose GPU memory during function call stalls. Our evaluation on representative multi-agent benchmarks shows that Tokencake can reduce end-to-end latency by over 47.06%, improve effective GPU memory utilization by up to 16.9% compared to vLLM.

**Link**: [arxiv](http://arxiv.org/abs/2510.18586v2),  [pdf](http://arxiv.org/pdf/2510.18586v2)

**Tags**: cs.DC 



### NeuronMM: High-Performance Matrix Multiplication for LLM Inference on   AWS Trainium
**Authors**: Dinghong Song, Jierui Xu, Weichu Yang, Pengfei Su, Dong Li

**Updated**: 2025-10-31T01:52:13Z

**Summary**: AI accelerators, customized to AI workloads, provide cost-effective and high-performance solutions for training and inference. Trainium, an AI accelerator recently developed by Amazon Web Services (AWS), provides an attractive option for LLM training and inference through its heterogeneous architecture. However, leveraging Trainium architecture for high performance can be challenging because of its systolic array architecture and special requirement on data layout. In this paper, we design high-performance matrix multiplication (matmul), a critical compute kernel, for LLM inference on Trainium. We introduce a series of techniques customized to Trainium based on kernel fusion and novel caching strategies to reduce data movement across the software-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive matrix transpose. Evaluating with nine datasets and four recent LLMs, we show that our system largely outperforms the state-of-the-art matmul implemented by AWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x speedup (up to 2.22x), which translates to an average 1.66x speedup (up to 2.49x) for end-to-end LLM inference.

**Link**: [arxiv](http://arxiv.org/abs/2510.25977v2),  [pdf](http://arxiv.org/pdf/2510.25977v2)

**Tags**: cs.CL 



### Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review
**Authors**: Dong Tong

**Updated**: 2025-10-31T00:39:27Z

**Summary**: The security and efficiency of modern computing systems are fundamentally undermined by the absence of a native architectural mechanism to propagate high-level program semantics, such as object identity, bounds, and lifetime, across the hardware/software interface. This paper presents a comprehensive survey of the architectural paradigm designed to bridge this semantic gap: descriptor-based, object-aware memory systems. By elevating the descriptor to a first-class architectural abstraction, this paradigm enables hardware to dynamically acquire and enforce the rich semantics of software-defined objects. This survey systematically charts the evolution and current landscape of this approach. We establish the foundational concepts of memory objects and descriptors and introduce a novel taxonomy of descriptor addressing modes, providing a structured framework for analyzing and comparing diverse implementations. Our unified analysis reveals how this paradigm holistically addresses the intertwined challenges of memory protection, management, and processing. As a culminating case study, we re-examine the CentroID model, demonstrating how its hybrid tagged-pointer encoding and descriptor processing mechanisms embody the path toward practical and efficient object-aware designs. Finally, we outline how the explicit cross-layer communication of object semantics provides a foundational research direction for next-generation cache hierarchies, unified virtual memory, and even 128-bit architectures.

**Link**: [arxiv](http://arxiv.org/abs/2510.27070v1),  [pdf](http://arxiv.org/pdf/2510.27070v1)

**Tags**: cs.AR cs.CR 



### Accelerating Diffusion LLMs via Adaptive Parallel Decoding
**Authors**: Daniel Israel, Guy Van den Broeck, Aditya Grover

**Updated**: 2025-10-30T21:11:33Z

**Summary**: The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2506.00413v2),  [pdf](http://arxiv.org/pdf/2506.00413v2)

**Tags**: cs.CL cs.AI cs.LG cs.PF 



### Choreographer: A Full-System Framework for Fine-Grained Tasks in Cache   Hierarchies
**Authors**: Hoa Nguyen, Pongstorn Maidee, Jason Lowe-Power, Alireza Kaviani

**Updated**: 2025-10-30T18:58:02Z

**Summary**: In this paper, we introduce Choreographer, a simulation framework that enables a holistic system-level evaluation of fine-grained accelerators designed for latency-sensitive tasks. Unlike existing frameworks, Choreographer captures all hardware and software overheads in core-accelerator and cache-accelerator interactions, integrating a detailed gem5-based hardware stack featuring an AMBA coherent hub interface (CHI) mesh network and a complete Linux-based software stack. To facilitate rapid prototyping, it offers a C++ application programming interface and modular configuration options. Our detailed cache model provides accurate insights into performance variations caused by cache configurations, which are not captured by other frameworks. The framework is demonstrated through two case studies: a data-aware prefetcher for graph analytics workloads, and a quicksort accelerator. Our evaluation shows that the prefetcher achieves speedups between 1.08x and 1.88x by reducing memory access latency, while the quicksort accelerator delivers more than 2x speedup with minimal address translation overhead. These findings underscore the ability of Choreographer to model complex hardware-software interactions and optimize performance in small task offloading scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2510.26944v1),  [pdf](http://arxiv.org/pdf/2510.26944v1)

**Tags**: cs.AR 



### ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for   Efficient MoE Inference
**Authors**: Zixu Shen, Kexin Chu, Yifan Zhang, Dawei Xiang, Runxin Wu, Wei Zhang

**Updated**: 2025-10-30T17:29:27Z

**Summary**: The expansion of large language models is increasingly limited by the constrained memory capacity of modern GPUs. To mitigate this, Mixture-of-Experts (MoE) architectures activate only a small portion of parameters during inference, significantly lowering both memory demand and computational overhead. However, conventional MoE inference approaches, which select active experts independently at each layer, often introduce considerable latency because of frequent parameter transfers between host and GPU memory. In addition, current cross-layer prediction strategies, which are typically based on fixed steps, lack adaptability across different hardware platforms and workloads, thereby reducing their robustness and effectiveness.   To address these challenges, we present ExpertFlow, a runtime system for MoE inference that combines adaptive expert prefetching and cache-aware routing. ExpertFlow continuously adjusts its prediction horizon for expert activation by leveraging runtime statistics such as transfer bandwidth, parameter dimensionality, and model feedback signals. Furthermore, it incorporates a hybrid cross-layer prediction scheme that fuses pregating information with intermediate computational states to anticipate future expert needs. By adaptively refining prefetching decisions and aligning them with actual usage behavior, ExpertFlow effectively decreases cache misses and removes latency caused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces model stall time to less than 0.1% of the baseline, highlighting its capability to optimize MoE inference under stringent memory constraints.

**Link**: [arxiv](http://arxiv.org/abs/2510.26730v1),  [pdf](http://arxiv.org/pdf/2510.26730v1)

**Tags**: cs.DC cs.AI cs.PF 



### GPU-Accelerated Primal Heuristics for Mixed Integer Programming
**Authors**: Akif Çördük, Piotr Sielski, Alice Boucher, Kumar Aatish

**Updated**: 2025-10-30T13:43:31Z

**Summary**: We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer Programming. Leveraging GPU acceleration enables exploration of larger search regions and faster iterations. A GPU-accelerated PDLP serves as an approximate LP solver, while a new probing cache facilitates rapid roundings and early infeasibility detection. Several state-of-the-art heuristics, including Feasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further accelerated and enhanced. The combined approach of these GPU-driven algorithms yields significant improvements over existing methods, both in the number of feasible solutions and the quality of objectives by achieving 221 feasible solutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved dataset.

**Link**: [arxiv](http://arxiv.org/abs/2510.20499v2),  [pdf](http://arxiv.org/pdf/2510.20499v2)

**Tags**: math.OC cs.DC 



### LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human   Smuggling Networks
**Authors**: Dipak Meher, Carlotta Domeniconi, Guadalupe Correa-Cabrera

**Updated**: 2025-10-30T13:39:08Z

**Summary**: Human smuggling networks are complex and constantly evolving, making them difficult to analyze comprehensively. Legal case documents offer rich factual and procedural insights into these networks but are often long, unstructured, and filled with ambiguous or shifting references, posing significant challenges for automated knowledge graph (KG) construction. Existing methods either overlook coreference resolution or fail to scale beyond short text spans, leading to fragmented graphs and inconsistent entity linking. We propose LINK-KG, a modular framework that integrates a three-stage, LLM-guided coreference resolution pipeline with downstream KG extraction. At the core of our approach is a type-specific Prompt Cache, which consistently tracks and resolves references across document chunks, enabling clean and disambiguated narratives for structured knowledge graph construction from both short and long legal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes by 32.22% compared to baseline methods, resulting in cleaner and more coherent graph structures. These improvements establish LINK-KG as a strong foundation for analyzing complex criminal networks.

**Link**: [arxiv](http://arxiv.org/abs/2510.26486v1),  [pdf](http://arxiv.org/pdf/2510.26486v1)

**Tags**: cs.AI cs.IR cs.LG 



### Model-Document Protocol for AI Search
**Authors**: Hongjin Qian, Zheng Liu

**Updated**: 2025-10-30T08:52:17Z

**Summary**: AI search depends on linking large language models (LLMs) with vast external knowledge sources. Yet web pages, PDF files, and other raw documents are not inherently LLM-ready: they are long, noisy, and unstructured. Conventional retrieval methods treat these documents as verbatim text and return raw passages, leaving the burden of fragment assembly and contextual reasoning to the LLM. This gap underscores the need for a new retrieval paradigm that redefines how models interact with documents.   We introduce the Model-Document Protocol (MDP), a general framework that formalizes how raw text is bridged to LLMs through consumable knowledge representations. Rather than treating retrieval as passage fetching, MDP defines multiple pathways that transform unstructured documents into task-specific, LLM-ready inputs. These include agentic reasoning, which curates raw evidence into coherent context; memory grounding, which accumulates reusable notes to enrich reasoning; and structured leveraging, which encodes documents into formal representations such as graphs or key-value caches. All three pathways share the same goal: ensuring that what reaches the LLM is not raw fragments but compact, structured knowledge directly consumable for reasoning.   As an instantiation, we present MDP-Agent, which realizes the protocol through an agentic process: constructing document-level gist memories for global coverage, performing diffusion-based exploration with vertical exploitation to uncover layered dependencies, and applying map-reduce style synthesis to integrate large-scale evidence into compact yet sufficient context. Experiments on information-seeking benchmarks demonstrate that MDP-Agent outperforms baselines, validating both the soundness of the MDP framework and the effectiveness of its agentic instantiation.

**Link**: [arxiv](http://arxiv.org/abs/2510.25160v2),  [pdf](http://arxiv.org/pdf/2510.25160v2)

**Tags**: cs.CL cs.AI cs.IR 



### How Efficient Are Diffusion Language Models? A Critical Examination of   Efficiency Evaluation Practices
**Authors**: Han Peng, Peiyu Liu, Zican Dong, Daixuan Cheng, Junyi Li, Yiru Tang, Shuo Wang, Wayne Xin Zhao

**Updated**: 2025-10-30T08:46:37Z

**Summary**: Diffusion language models (DLMs) have emerged as a promising alternative to the long-dominant autoregressive (AR) paradigm, offering a parallelable decoding process that could yield greater efficiency. Yet, in practice, current open-source DLMs often underperform their AR counterparts in speed, limiting their real-world utility. This work presents a systematic study of DLM efficiency, identifying key issues in prior evaluation methods. Through empirical benchmarking and a roofline-based theoretical analysis, we demonstrate that AR models generally achieve higher throughput, while DLMs consistently lag. We also investigate acceleration strategies, finding that techniques like dual cache and parallel decoding mainly offer gains at small batch sizes, with their benefits diminishing upon scaling. Our findings underscore the necessity of robust evaluation methods and improved acceleration strategies to advance research on DLMs.

**Link**: [arxiv](http://arxiv.org/abs/2510.18480v2),  [pdf](http://arxiv.org/pdf/2510.18480v2)

**Tags**: cs.CL 



### From req/res to pub/sub: Exploring Media over QUIC Transport for DNS
**Authors**: Mathis Engelbart, Mike Kosek, Lars Eggert, Jörg Ott

**Updated**: 2025-10-30T08:12:53Z

**Summary**: The DNS is a key component of the Internet. Originally designed to facilitate the resolution of host names to IP addresses, its scope has continuously expanded over the years, today covering use cases such as load balancing or service discovery. While DNS was initially conceived as a rather static directory service in which resource records (RR) only change rarely, we have seen a number of use cases over the years where a DNS flavor that isn't purely based upon requesting and caching RRs, but rather on an active distribution of updates for all resolvers that showed interest in the respective records in the past, would be preferable. In this paper, we thus explore a publish-subscribe variant of DNS based on the Media-over-QUIC architecture, where we devise a strawman system and protocol proposal to enable pushing RR updates. We provide a prototype implementation, finding that DNS can benefit from a publish-subscribe variant: next to limiting update traffic, it can considerably reduce the time it takes for a resolver to receive the latest version of a record, thereby supporting use cases such as load balancing in content distribution networks. The publish-subscribe architecture also brings new challenges to the DNS, including a higher overhead for endpoints due to additional state management, and increased query latencies on first lookup, due to session establishment latencies.

**Link**: [arxiv](http://arxiv.org/abs/2510.26234v1),  [pdf](http://arxiv.org/pdf/2510.26234v1)

**Tags**: cs.NI 



### LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based   Video Generation
**Authors**: Huanlin Gao, Ping Chen, Fuyuan Shi, Chao Tan, Zhaoxiang Liu, Fang Zhao, Kai Wang, Shiguo Lian

**Updated**: 2025-10-30T04:57:26Z

**Summary**: We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9x speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa

**Link**: [arxiv](http://arxiv.org/abs/2511.00090v1),  [pdf](http://arxiv.org/pdf/2511.00090v1)

**Tags**: cs.CV cs.AI 



### PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse   Attention for Vision-Language Large Models
**Authors**: Zhonghua Jiang, Kunxi Li, Yiyun Zhou, Sihao Liu, Zhaode Wang, Chengfei lv, Shengyu Zhang

**Updated**: 2025-10-30T03:43:02Z

**Summary**: Vision-Language Large Models (VLLMs) face significant efficiency challenges when processing high-resolution inputs. The quadratic complexity in attention and autoregressive generation, as well as the constantly growing key value (KV) cache size, severely hinder the prefilling and decoding stages. Recent efforts have attempted to compress KV cache by identifying and pruning KV cache of less important tokens, but these methods typically rely on attention scores to estimate token importance, making them incompatible with efficient attention mechanisms such as FlashAttention and Sparse Attention, which do not explicitly compute attention matrices. Moreover, existing methods overlook how sparse attention, while accelerating the prefilling stage, alters the information structure of the KV cache, thereby compromising the effectiveness of downstream KV cache compression strategies. To address this issue, we propose PureKV, a plug-and-play framework for joint optimization of sparse attention and KV cache compression. We first introduce a KV cache compression strategy that is fully compatible with efficient attention accelerators. Our method utilizes lower layer attention scores to estimate the importance of high layers' KV cache, enabling active pruning without compromising accuracy. In addition, we have designed a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically tailored for video KV cache compression algorithms. This module combines spatial and temporal attention sparsity to improve the compression efficiency of KV cache optimization algorithms by purifying spatial noise and temporal redundancy in KV cache. At the same time, ST-SpAttn also accelerated the prefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2, Qwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and 3.16 times prefill acceleration, with negligible quality degradation.

**Link**: [arxiv](http://arxiv.org/abs/2510.25600v2),  [pdf](http://arxiv.org/pdf/2510.25600v2)

**Tags**: cs.MM 



### OneTrans: Unified Feature Interaction and Sequence Modeling with One   Transformer in Industrial Recommender
**Authors**: Zhaoqi Zhang, Haolei Pei, Jun Guo, Tianyu Wang, Yufei Feng, Hui Sun, Shaowei Liu, Aixin Sun

**Updated**: 2025-10-30T03:30:12Z

**Summary**: In recommendation systems, scaling up feature-interaction modules (e.g., Wukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has achieved notable success. However, these efforts typically proceed on separate tracks, which not only hinders bidirectional information exchange but also prevents unified optimization and scaling. In this paper, we propose OneTrans, a unified Transformer backbone that simultaneously performs user-behavior sequence modeling and feature interaction. OneTrans employs a unified tokenizer to convert both sequential and non-sequential attributes into a single token sequence. The stacked OneTrans blocks share parameters across similar sequential tokens while assigning token-specific parameters to non-sequential tokens. Through causal attention and cross-request KV caching, OneTrans enables precomputation and caching of intermediate representations, significantly reducing computational costs during both training and inference. Experimental results on industrial-scale datasets demonstrate that OneTrans scales efficiently with increasing parameters, consistently outperforms strong baselines, and yields a 5.68% lift in per-user GMV in online A/B tests.

**Link**: [arxiv](http://arxiv.org/abs/2510.26104v1),  [pdf](http://arxiv.org/pdf/2510.26104v1)

**Tags**: cs.IR 



### Oneiros: KV Cache Optimization through Parameter Remapping for   Multi-tenant LLM Serving
**Authors**: Ruihao Li, Shagnik Pal, Vineeth Narayan Pullu, Prasoon Sinha, Jeeho Ryoo, Lizy K. John, Neeraja J. Yadwadkar

**Updated**: 2025-10-29T21:56:19Z

**Summary**: KV cache accelerates LLM inference by avoiding redundant computation, at the expense of memory. To support larger KV caches, prior work extends GPU memory with CPU memory via CPU-offloading. This involves swapping KV cache between GPU and CPU memory. However, because the cache updates dynamically, such swapping incurs high CPU memory traffic. We make a key observation that model parameters remain constant during runtime, unlike the dynamically updated KV cache. Building on this, we introduce Oneiros, which avoids KV cache swapping by remapping, and thereby repurposing, the memory allocated to model parameters for KV cache. This parameter remapping is especially beneficial in multi-tenant environments, where the memory used for the parameters of the inactive models can be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth offered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we show that Oneiros significantly outperforms state-of-the-art solutions, achieving a reduction of 44.8%-82.5% in tail time-between-token latency, 20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher throughput compared to vLLM. Source code of Oneiros is available at https://github.com/UT-SysML/Oneiros/.

**Link**: [arxiv](http://arxiv.org/abs/2507.11507v2),  [pdf](http://arxiv.org/pdf/2507.11507v2)

**Tags**: cs.OS 



### Category-Aware Semantic Caching for Heterogeneous LLM Workloads
**Authors**: Chen Wang, Xunzhuo Liu, Yue Zhu, Alaa Youssef, Priya Nagpurkar, Huamin Chen

**Updated**: 2025-10-29T19:59:45Z

**Summary**: LLM serving systems process heterogeneous query workloads where different categories exhibit different characteristics. Code queries cluster densely in embedding space while conversational queries distribute sparsely. Content staleness varies from minutes (stock data) to months (code patterns). Query repetition patterns range from power-law (code) to uniform (conversation), producing long tail cache hit rate distributions: high-repetition categories achieve 40-60% hit rates while low-repetition or volatile categories achieve 5-15% hit rates. Vector databases must exclude the long tail because remote search costs (30ms) require 15--20% hit rates to break even, leaving 20-30% of production traffic uncached. Uniform cache policies compound this problem: fixed thresholds cause false positives in dense spaces and miss valid paraphrases in sparse spaces; fixed TTLs waste memory or serve stale data. This paper presents category-aware semantic caching where similarity thresholds, TTLs, and quotas vary by query category. We present a hybrid architecture separating in-memory HNSW search from external document storage, reducing miss cost from 30ms to 2ms. This reduction makes low-hit-rate categories economically viable (break-even at 3-5% versus 15-20%), enabling cache coverage across the entire workload distribution. Adaptive load-based policies extend this framework to respond to downstream model load, dynamically adjusting thresholds and TTLs to reduce traffic to overloaded models by 9-17% in theoretical projections.

**Link**: [arxiv](http://arxiv.org/abs/2510.26835v1),  [pdf](http://arxiv.org/pdf/2510.26835v1)

**Tags**: cs.DB cs.AI cs.LG 



### Over 3 kV and Ultra-Low leakage Vertical (011) \b{eta}-Ga2O3 Power   Diodes with Engineered Schottky Contact and High-permittivity Dielectric   Field Plate
**Authors**: Emerson J. Hollar, Esmat Farzana

**Updated**: 2025-10-29T17:00:16Z

**Summary**: We report over 3 kV breakdown voltage and ultra-low leakage (011) \b{eta}-Ga2O3 power devices utilizing Schottky barrier engineering and high-permittivity (\k{appa}) dielectric (ZrO2) field plate. The (011) orientation of \b{eta}-Ga2O3 enabled low background doping and thick drift layers which are promising to support kV-class vertical \b{eta}-Ga2O3 power switches. The Schottky barrier engineering was performed with a composite Pt cap/PtOx/Pt (1.5 nm) anode contact to take advantage of the enhanced reverse blocking capabilities enabled by PtOx while allowing low turn-on voltage by the interfacing thin Pt layer. We also performed a systematic study using a co-processed Pt/(011) \b{eta}-Ga2O3 Schottky barrier diodes (SBDs) on the same wafer. The bare SBDs revealed a breakdown voltage of ~1.5 kV, while the field-plate Pt/(011) \b{eta}-Ga2O3 SBDs achieved an increased breakdown voltage of 2.75 kV owing to the edge field management. Further enhancement of the breakdown voltage was achieved by tunneling leakage management using composite Pt cap/PtOx/Pt (1.5 nm) Schottky contacts that ultimately enabled breakdown voltage of 3.7 kV for the field-plate diodes. Remarkably, the Pt cap/PtOx/Pt (1.5 nm) Schottky contacts maintained similar turn-on voltage as the Pt/(011) \b{eta}-Ga2O3 SBDs. The combination of efficient tunneling leakage management by composite Pt cap/PtOx/Pt (1.5 nm) contacts with similar turn-on voltage, edge field reduction by high-\k{appa} dielectric ZrO2 field plate, as well as the advantageous material properties offered by (011) \b{eta}-Ga2O3 demonstrate a promising strategy for developing ultra-low leakage and multi-kV class vertical (011) \b{eta}-Ga2O3 power devices.

**Link**: [arxiv](http://arxiv.org/abs/2510.25695v1),  [pdf](http://arxiv.org/pdf/2510.25695v1)

**Tags**: eess.SY cond-mat.mtrl-sci cs.SY 



### Quickest Change Point Detection with Measurements over a Lossy Link
**Authors**: Krishna Chaythanya KV, Saqib Abbas Baba, Anurag Kumar, Arpan Chattopadhyay, Rajesh Sundaresan

**Updated**: 2025-10-29T15:12:35Z

**Summary**: Motivated by Industry 4.0 applications, we consider quickest change detection (QCD) of an abrupt change in a process when its measurements are transmitted by a sensor over a lossy wireless link to a decision maker (DM). The sensor node samples measurements using a Bernoulli sampling process, and places the measurement samples in the transmit queue of its transmitter. The transmitter uses a retransmit-until-success transmission strategy to deliver packets to the DM over the lossy link, in which the packet losses are modeled as a Bernoulli process, with different loss probabilities before and after the change. We pose the QCD problem in the non-Bayesian setting under Lorden's framework, and propose a CUSUM algorithm. By defining a suitable Markov process, involving the DM measurements and the queue length process, we show that the problem reduces to QCD in a Markov process. Characterizing the information measure per measurement sample at the DM, we establish the asymptotic optimality of our algorithm when the false alarm rate tends to zero. Further, when the DM receives incomplete data due to channel loss, we present asymptotically optimal QCD algorithms by suitably modifying the CUSUM algorithm. We then explore the last-come-first-served (LCFS) queuing discipline at the sensor transmit queue to lower detection delay in the non-asymptotic case. Next, we consider the case of multiple sensors, each with its own wireless transmitter queue, and show that our analysis extends to the case of multiple homogeneous sensors. When the sensors are heterogeneous, we present a sensor scheduling algorithm that minimizes detection delay by balancing the trade-off between the age of the observations and their information content. Numerical analysis demonstrate trade-offs that can be used to optimize system design parameters in the non-asymptotic regime.

**Link**: [arxiv](http://arxiv.org/abs/2510.25604v1),  [pdf](http://arxiv.org/pdf/2510.25604v1)

**Tags**: eess.SP 



### RegionE: Adaptive Region-Aware Generation for Efficient Image Editing
**Authors**: Pengtao Chen, Xianfang Zeng, Maosen Zhao, Mingzhu Shen, Peng Ye, Bangyin Xiang, Zhibo Wang, Wei Cheng, Gang Yu, Tao Chen

**Updated**: 2025-10-29T14:58:37Z

**Summary**: Recently, instruction-based image editing (IIE) has received widespread attention. In practice, IIE often modifies only specific regions of an image, while the remaining areas largely remain unchanged. Although these two types of regions differ significantly in generation difficulty and computational redundancy, existing IIE models do not account for this distinction, instead applying a uniform generation process across the entire image. This motivates us to propose RegionE, an adaptive, region-aware generation framework that accelerates IIE tasks without additional training. Specifically, the RegionE framework consists of three main components: 1) Adaptive Region Partition. We observed that the trajectory of unedited regions is straight, allowing for multi-step denoised predictions to be inferred in a single step. Therefore, in the early denoising stages, we partition the image into edited and unedited regions based on the difference between the final estimated result and the reference image. 2) Region-Aware Generation. After distinguishing the regions, we replace multi-step denoising with one-step prediction for unedited areas. For edited regions, the trajectory is curved, requiring local iterative denoising. To improve the efficiency and quality of local iterative generation, we propose the Region-Instruction KV Cache, which reduces computational cost while incorporating global information. 3) Adaptive Velocity Decay Cache. Observing that adjacent timesteps in edited regions exhibit strong velocity similarity, we further propose an adaptive velocity decay cache to accelerate the local denoising process. We applied RegionE to state-of-the-art IIE base models, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE achieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o confirmed that semantic and perceptual fidelity were well preserved.

**Link**: [arxiv](http://arxiv.org/abs/2510.25590v1),  [pdf](http://arxiv.org/pdf/2510.25590v1)

**Tags**: cs.CV cs.AI 



### FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual   Question Answering
**Authors**: Liangyu Zhong, Fabio Rosenthal, Joachim Sicking, Fabian Hüger, Thorsten Bagdonat, Hanno Gottschalk, Leo Schwinn

**Updated**: 2025-10-29T14:46:17Z

**Summary**: While Multimodal Large Language Models (MLLMs) offer strong perception and reasoning capabilities for image-text input, Visual Question Answering (VQA) focusing on small image details still remains a challenge. Although visual cropping techniques seem promising, recent approaches have several limitations: the need for task-specific fine-tuning, low efficiency due to uninformed exhaustive search, or incompatibility with efficient attention implementations. We address these shortcomings by proposing a training-free visual cropping method, dubbed FOCUS, that leverages MLLM-internal representations to guide the search for the most relevant image region. This is accomplished in four steps: first, we identify the target object(s) in the VQA prompt; second, we compute an object relevance map using the key-value (KV) cache; third, we propose and rank relevant image regions based on the map; and finally, we perform the fine-grained VQA task using the top-ranked region. As a result of this informed search strategy, FOCUS achieves strong performance across four fine-grained VQA datasets and three types of MLLMs. It outperforms three popular visual cropping methods in both accuracy and efficiency, and matches the best-performing baseline, ZoomEye, while requiring 3 - 6.5 x less compute.

**Link**: [arxiv](http://arxiv.org/abs/2506.21710v2),  [pdf](http://arxiv.org/pdf/2506.21710v2)

**Tags**: cs.CV 



### Serve Programs, Not Prompts
**Authors**: In Gim, Lin Zhong

**Updated**: 2025-10-29T11:29:03Z

**Summary**: Current large language model (LLM) serving systems, primarily designed for text completion, are neither efficient nor adaptable for increasingly complex LLM applications due to their inflexible design. We propose a new LLM serving system architecture that serves programs instead of prompts to address this problem. These programs, called LLM Inference Programs (LIPs), allow users to customize token prediction and KV cache management at runtime and to offload parts of their application logic, such as tool execution, to the server. We describe an example of this architecture through a system named Symphony, which functions as an operating system for LIPs. Symphony exposes LLM model computations via system calls and virtualizes KV cache with a dedicated file system, while ensuring GPU efficiency with a two-level process scheduling scheme. Symphony has the potential to open the door to a more efficient and extensible ecosystem for LLM applications.

**Link**: [arxiv](http://arxiv.org/abs/2510.25412v1),  [pdf](http://arxiv.org/pdf/2510.25412v1)

**Tags**: cs.CL 



### Off-Centered WoS-Type Solvers with Statistical Weighting
**Authors**: Anchang Bao, Jie Xu, Enya Shen, Jianmin Wang

**Updated**: 2025-10-29T04:09:50Z

**Summary**: Stochastic PDE solvers have emerged as a powerful alternative to traditional discretization-based methods for solving partial differential equations (PDEs), especially in geometry processing and graphics. While off-centered estimators enhance sample reuse in WoS-type Monte Carlo solvers, they introduce correlation artifacts and bias when Green's functions are approximated. In this paper, we propose a statistically weighted off-centered WoS-type estimator that leverages local similarity filtering to selectively combine samples across neighboring evaluation points. Our method balances bias and variance through a principled weighting strategy that suppresses unreliable estimators. We demonstrate our approach's effectiveness on various PDEs,including screened Poisson equations and boundary conditions, achieving consistent improvements over existing solvers such as vanilla Walk on Spheres, mean value caching, and boundary value caching. Our method also naturally extends to gradient field estimation and mixed boundary problems.

**Link**: [arxiv](http://arxiv.org/abs/2510.25152v1),  [pdf](http://arxiv.org/pdf/2510.25152v1)

**Tags**: cs.GR 



### NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized   Generalist Robotic Policies
**Authors**: Jiahong Chen, Jing Wang, Long Chen, Chuwei Cai, Jinghui Lu

**Updated**: 2025-10-29T03:00:36Z

**Summary**: Vision-language-action (VLA) models have significantly advanced robotic manipulation by integrating vision-language models (VLMs), and action decoders into a unified architecture. However, their deployment on resource-constrained edge devices, such as mobile robots or embedded systems (e.g., Jetson Orin Nano), remains challenging due to high computational demands, especially in real-world scenarios where power, latency, and computational resources are critical. To close this gap, we introduce Nano-scale Vision-Language Action (NanoVLA), a family of lightweight VLA architectures that achieve high performance with minimal resources. Our core innovations include: (1) vision-language decoupling that moves conventional early vision and language inputs fusion in VLM to late stage, achieving better performance while enabling caching and reduce inference overhead and latency; (2) long-short action chunking to ensure smooth, coherent multi-step planning without sacrificing real-time responsiveness; (3) dynamic routing that adaptively assigns lightweight or heavy backbones based on task complexity, further optimizing inference efficiency. Experimental results on several benchmarks, as well as real-world deployments, demonstrate that NanoVLA achieves up to 52x faster inference on edge devices compared to previous state-of-the-art VLA models, with 98% less parameters while maintaining or surpassing their task accuracy and generalization. Ablation studies confirm that our decoupling strategy preserves cross-task transferability, and the routing module enhances cost-performance trade-offs, enabling practical, high-precision robotic manipulation on resource-constrained hardware.

**Link**: [arxiv](http://arxiv.org/abs/2510.25122v1),  [pdf](http://arxiv.org/pdf/2510.25122v1)

**Tags**: cs.RO 



### Parallel Loop Transformer for Efficient Test-Time Computation Scaling
**Authors**: Bohong Wu, Mengzhao Chen, Xiang Luo, Shen Yan, Qifan Yu, Fan Xia, Tianqi Zhang, Hongrui Zhan, Zheng Zhong, Xun Zhou, Siyuan Qiao, Xingyan Bin

**Updated**: 2025-10-28T15:35:50Z

**Summary**: Large Language Models (LLMs) are powerful but often too slow and costly for real-world use during inference. Looped transformers save on parameters by reusing the same weights for multiple computational steps, or "loops." However, this approach has a major flaw: the loops run one after another, causing inference latency and memory requirements to increase with each added loop. This makes them impractical for fast applications. To solve this problem, we introduce the Parallel Loop Transformer (PLT). PLT is a new architecture that delivers the performance benefits of a deep, looped model but with the low latency of a standard, non-looped model. PLT works using two key techniques. First, Cross-Loop Parallelism (CLP) breaks the sequential dependency by computing different loops for different tokens at the same time, all within a single pass. Second, to prevent memory costs from growing, we use an Efficient Representation Enhancement strategy. This method shares the memory (KV cache) from the first loop with all other loops. It then uses a Gated Sliding-Window Attention (G-SWA) to combine this shared global information with local information, maintaining high accuracy. Our experiments show that PLT achieves the high accuracy of a traditional looped model but with almost no extra latency or memory cost compared to a standard transformer.

**Link**: [arxiv](http://arxiv.org/abs/2510.24824v1),  [pdf](http://arxiv.org/pdf/2510.24824v1)

**Tags**: cs.CL 



### An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine
**Authors**: Pedram Fard, Alaleh Azhir, Neguine Rezaii, Jiazi Tian, Hossein Estiri

**Updated**: 2025-10-28T12:28:02Z

**Summary**: Artificial intelligence in medicine is built to serve the average patient. By minimizing error across large datasets, most systems deliver strong aggregate accuracy yet falter at the margins: patients with rare variants, multimorbidity, or underrepresented demographics. This average patient fallacy erodes both equity and trust. We propose a different design: a multi-agent ecosystem for N-of-1 decision support. In this environment, agents clustered by organ systems, patient populations, and analytic modalities draw on a shared library of models and evidence synthesis tools. Their results converge in a coordination layer that weighs reliability, uncertainty, and data density before presenting the clinician with a decision-support packet: risk estimates bounded by confidence ranges, outlier flags, and linked evidence. Validation shifts from population averages to individual reliability, measured by error in low-density regions, calibration in the small, and risk--coverage trade-offs. Anticipated challenges include computational demands, automation bias, and regulatory fit, addressed through caching strategies, consensus checks, and adaptive trial frameworks. By moving from monolithic models to orchestrated intelligence, this approach seeks to align medical AI with the first principle of medicine: care that is transparent, equitable, and centered on the individual.

**Link**: [arxiv](http://arxiv.org/abs/2510.24359v1),  [pdf](http://arxiv.org/pdf/2510.24359v1)

**Tags**: cs.AI cs.SY eess.SY q-bio.QM stat.AP 



### SALS: Sparse Attention in Latent Space for KV cache Compression
**Authors**: Junlin Mu, Hantao Huang, Jihang Zhang, Minghui Yu, Tao Wang, Yidong Li

**Updated**: 2025-10-28T10:32:52Z

**Summary**: Large Language Models capable of handling extended contexts are in high demand, yet their inference remains challenging due to substantial Key-Value cache size and high memory bandwidth requirements. Previous research has demonstrated that KV cache exhibits low-rank characteristics within the hidden dimension, suggesting the potential for effective compression. However, due to the widely adopted Rotary Position Embedding mechanism in modern LLMs, naive low-rank compression suffers severe accuracy degradation or creates a new speed bottleneck, as the low-rank cache must first be reconstructed in order to apply RoPE. In this paper, we introduce two key insights: first, the application of RoPE to the key vectors increases their variance, which in turn results in a higher rank; second, after the key vectors are transformed into the latent space, they largely maintain their representation across most layers. Based on these insights, we propose the Sparse Attention in Latent Space framework. SALS projects the KV cache into a compact latent space via low-rank projection, and performs sparse token selection using RoPE-free query-key interactions in this space. By reconstructing only a small subset of important tokens, it avoids the overhead of full KV cache reconstruction. We comprehensively evaluate SALS on various tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and additionally verify its scalability on the RULER-128k benchmark with LLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA performance by maintaining competitive accuracy. Under different settings, SALS achieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention operator compared to FlashAttention2 on the 4K sequence. For the end-to-end throughput performance, we achieves 1.4-fold and 4.5-fold improvement compared to GPT-fast on 4k and 32K sequences, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2510.24273v1),  [pdf](http://arxiv.org/pdf/2510.24273v1)

**Tags**: cs.LG 



### Pie: A Programmable Serving System for Emerging LLM Applications
**Authors**: In Gim, Zhiyao Ma, Seung-seob Lee, Lin Zhong

**Updated**: 2025-10-28T04:17:55Z

**Summary**: Emerging large language model (LLM) applications involve diverse reasoning strategies and agentic workflows, straining the capabilities of existing serving systems built on a monolithic token generation loop. This paper introduces Pie, a programmable LLM serving system designed for flexibility and efficiency. Pie decomposes the traditional generation loop into fine-grained service handlers exposed via an API and delegates control of the generation process to user-provided programs, called inferlets. This enables applications to implement new KV cache strategies, bespoke generation logic, and seamlessly integrate computation and I/O-entirely within the application, without requiring modifications to the serving system. Pie executes inferlets using WebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows Pie matches state-of-the-art performance on standard tasks (3-12% latency overhead) while significantly improving latency and throughput (1.3x-3.4x higher) on agentic workflows by enabling application-specific optimizations.

**Link**: [arxiv](http://arxiv.org/abs/2510.24051v1),  [pdf](http://arxiv.org/pdf/2510.24051v1)

**Tags**: cs.CL 



### FastKV: KV Cache Compression for Fast Long-Context Processing with   Token-Selective Propagation
**Authors**: Dongwon Jo, Jiwon Song, Yulhwa Kim, Jae-Joon Kim

**Updated**: 2025-10-28T04:00:18Z

**Summary**: While large language models (LLMs) excel at handling long-context sequences, they require substantial prefill computation and key-value (KV) cache, which can heavily burden computational efficiency and memory usage in both prefill and decoding stages. Recent works that compress KV caches with prefill acceleration reduce this cost but inadvertently tie the prefill compute reduction to the decoding KV budget. This coupling arises from overlooking the layer-dependent variation of critical context, often leading to accuracy degradation. To address this issue, we introduce FastKV, a KV cache compression framework designed to reduce latency in both prefill and decoding by leveraging the stabilization of token importance in later layers. FastKV performs full-context computation until a Token-Selective Propagation (TSP) layer, which forwards only the most informative tokens to subsequent layers. From these propagated tokens, FastKV independently selects salient KV entries for caching, thereby decoupling KV budget from the prefill compute reduction based on the TSP decision. This independent control of the TSP rate and KV retention rate enables flexible optimization of efficiency and accuracy. Experimental results show that FastKV achieves speedups of up to 1.82$\times$ in prefill and 2.87$\times$ in decoding compared to the full-context baseline, while matching the accuracy of the baselines that only accelerate the decoding stage. Our code is available at https://github.com/dongwonjo/FastKV.

**Link**: [arxiv](http://arxiv.org/abs/2502.01068v4),  [pdf](http://arxiv.org/pdf/2502.01068v4)

**Tags**: cs.LG cs.CL 



### STree: Speculative Tree Decoding for Hybrid State-Space Models
**Authors**: Yangchao Wu, Zongyue Qin, Alex Wong, Stefano Soatto

**Updated**: 2025-10-27T21:48:48Z

**Summary**: Speculative decoding is a technique to leverage hardware concurrency in order to enable multiple steps of token generation in a single forward pass, thus improving the efficiency of large-scale autoregressive (AR) Transformer models. State-space models (SSMs) are already more efficient than AR Transformers, since their state summarizes all past data with no need to cache or re-process tokens in the sliding window context. However, their state can also comprise thousands of tokens; so, speculative decoding has recently been extended to SSMs. Existing approaches, however, do not leverage the tree-based verification methods, since current SSMs lack the means to compute a token tree efficiently. We propose the first scalable algorithm to perform tree-based speculative decoding in state-space models (SSMs) and hybrid architectures of SSMs and Transformer layers. We exploit the structure of accumulated state transition matrices to facilitate tree-based speculative decoding with minimal overhead relative to current SSM implementations. Along with the algorithm, we describe a hardware-aware implementation that improves naive application of AR Transformer tree-based speculative decoding methods to SSMs. Furthermore, we outperform vanilla speculative decoding with SSMs even with a baseline drafting model and tree structure on three different benchmarks, opening up opportunities for further speed up with SSM and hybrid model inference. Code can be found at: https://github.com/wyc1997/stree.

**Link**: [arxiv](http://arxiv.org/abs/2505.14969v2),  [pdf](http://arxiv.org/pdf/2505.14969v2)

**Tags**: cs.LG cs.AI 



### KV-weights are all you need for skipless transformers
**Authors**: Nils Graef

**Updated**: 2025-10-27T17:31:15Z

**Summary**: He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the V and P (post-attention projection) linear layers, which reduces the total number of weights. However, this scheme is only applicable to MHA (multi-head attention), but not for MQA (multi-query attention) and GQA (grouped-query attention). The latter schemes are used by many popular LLMs such as Llama 2, Mistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes mathematically equivalent versions that are suitable for MQA and GQA. For example, removing Q and P from a skipless version of Mistral-7B would remove 15% of its weights (and thus reduce its compute and memory complexity). Watch our explainer video https://youtu.be/Tx_lMpphd2g and see https://github.com/OpenMachine-ai/transformer-tricks for code and more transformer tricks.

**Link**: [arxiv](http://arxiv.org/abs/2404.12362v2),  [pdf](http://arxiv.org/pdf/2404.12362v2)

**Tags**: cs.LG 



### Leveraging Approximate Caching for Faster Retrieval-Augmented Generation
**Authors**: Shai Bergman, Anne-Marie Kermarrec, Diana Petrescu, Rafael Pires, Mathis Randl, Martijn de Vos, Ji Zhang

**Updated**: 2025-10-27T16:20:28Z

**Summary**: Retrieval-augmented generation (RAG) improves the reliability of large language model (LLM) answers by integrating external knowledge. However, RAG increases the end-to-end inference time since looking for relevant documents from large vector databases is computationally expensive. To address this, we introduce Proximity, an approximate key-value cache that optimizes the RAG workflow by leveraging similarities in user queries. Instead of treating each query independently, Proximity reuses previously retrieved documents when similar queries appear, substantially reducing the reliance on expensive vector database lookups. To efficiently scale, Proximity employs a locality-sensitive hashing (LSH) scheme that enables fast cache lookups while preserving retrieval accuracy. We evaluate Proximity using the MMLU and MedRAG question-answering benchmarks. Our experiments demonstrate that Proximity with our LSH scheme and a realistically-skewed MedRAG workload reduces database calls by 77.2% while maintaining database recall and test accuracy. We experiment with different similarity tolerances and cache capacities, and show that the time spent within the Proximity cache remains low and constant (4.8 microseconds) even as the cache grows substantially in size. Our results demonstrate that approximate caching is a practical and effective strategy for optimizing RAG-based systems.

**Link**: [arxiv](http://arxiv.org/abs/2503.05530v3),  [pdf](http://arxiv.org/pdf/2503.05530v3)

**Tags**: cs.DB cs.LG cs.PF 



### A Data-driven ML Approach for Maximizing Performance in LLM-Adapter   Serving
**Authors**: Ferran Agullo, Joan Oliveras, Chen Wang, Alberto Gutierrez-Torre, Olivier Tardieu, Alaa Youssef, Jordi Torres, Josep Ll. Berral

**Updated**: 2025-10-27T14:59:46Z

**Summary**: With the rapid adoption of Large Language Models (LLMs), LLM-adapters have become increasingly common, providing lightweight specialization of large-scale models. Serving hundreds or thousands of these adapters on a single GPU allows request aggregation, increasing throughput, but may also cause request starvation if GPU memory limits are exceeded. To address this issue, this study focuses on determining the joint configuration of concurrent and parallel adapters that maximizes GPU throughput without inducing starvation, given heterogeneous adapter and traffic properties. We propose a data-driven ML approach leveraging interpretable models to tackle this caching problem and introduce the first Digital Twin capable of reproducing an LLM-adapter serving system, enabling efficient training data generation. Experiments with the vLLM framework and LoRA adapters show that the Digital Twin reproduces throughput within 5.1% of real results, while the ML approach predicts optimal numbers of concurrent and parallel adapters with an error of at most 7.2% under heterogeneous, real-world workloads.

**Link**: [arxiv](http://arxiv.org/abs/2508.08343v2),  [pdf](http://arxiv.org/pdf/2508.08343v2)

**Tags**: cs.PF cs.AI cs.CL 



### PESTO: Real-Time Pitch Estimation with Self-supervised   Transposition-equivariant Objective
**Authors**: Alain Riou, Bernardo Torres, Ben Hayes, Stefan Lattner, Gaëtan Hadjeres, Gaël Richard, Geoffroy Peeters

**Updated**: 2025-10-27T11:55:07Z

**Summary**: In this paper, we introduce PESTO, a self-supervised learning approach for single-pitch estimation using a Siamese architecture. Our model processes individual frames of a Variable-$Q$ Transform (VQT) and predicts pitch distributions. The neural network is designed to be equivariant to translations, notably thanks to a Toeplitz fully-connected layer. In addition, we construct pitch-shifted pairs by translating and cropping the VQT frames and train our model with a novel class-based transposition-equivariant objective, eliminating the need for annotated data. Thanks to this architecture and training objective, our model achieves remarkable performances while being very lightweight ($130$k parameters). Evaluations on music and speech datasets (MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms self-supervised baselines but also competes with supervised methods, exhibiting superior cross-dataset generalization. Finally, we enhance PESTO's practical utility by developing a streamable VQT implementation using cached convolutions. Combined with our model's low latency (less than 10 ms) and minimal parameter count, this makes PESTO particularly suitable for real-time applications.

**Link**: [arxiv](http://arxiv.org/abs/2508.01488v2),  [pdf](http://arxiv.org/pdf/2508.01488v2)

**Tags**: cs.SD cs.AI cs.LG eess.AS 



### Batch Speculative Decoding Done Right
**Authors**: Ranran Haoran Zhang, Soumik Dey, Ashirbad Mishra, Hansi Wu, Binbin Li, Rui Zhang

**Updated**: 2025-10-26T23:59:23Z

**Summary**: Speculative decoding speeds up LLM inference by using a small draft model to propose multiple tokens that a target model verifies in parallel. Extending this idea to batches is essential for production serving, but it introduces the ragged tensor problem: sequences in the same batch accept different numbers of draft tokens, breaking right-alignment and corrupting position IDs, attention masks, and KV-cache state. We show that several existing batch implementations violate output equivalence-the fundamental requirement that speculative decoding must produce identical token sequences to standard autoregressive generation. These violations occur precisely due to improper handling of the ragged tensor problem. In response, we (1) characterize the synchronization requirements that guarantee correctness, (2) present a correctness-first batch speculative decoding EQSPEC that exposes realignment as consuming 40% of overhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences and dynamically forms same-length groups, to reduce the realignment overhead while preserving per-sequence speculative speedups. On the SpecBench dataset, across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our approach achieves up to 3$\times$ throughput improvement at batch size 8 compared to batch size 1, with efficient scaling through batch size 8, while maintaining 95% output equivalence. Our method requires no custom kernels and integrates cleanly with existing inference stacks. Our code is available at https://github.com/eBay/spec_dec.

**Link**: [arxiv](http://arxiv.org/abs/2510.22876v1),  [pdf](http://arxiv.org/pdf/2510.22876v1)

**Tags**: cs.CL cs.AI 



### FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline
**Authors**: Jingwei Xu, Junbin Kang, Mingkai Dong, Mingyu Liu, Lu Zhang, Shaohong Guo, Ziyan Qiu, Mingzhen You, Ziyi Tian, Anqi Yu, Tianhong Ding, Xinwei Hu, Haibo Chen

**Updated**: 2025-10-26T13:31:41Z

**Summary**: Client-side metadata caching has long been considered an effective method for accelerating metadata operations in distributed file systems (DFSs). However, we have found that client-side state (e.g., caching) is not only ineffective but also consumes valuable memory resources in the deep learning pipelines. We thus propose FalconFS, a DFS optimized for deep learning pipelines with the stateless-client architecture. Specifically, instead of performing client-side path resolution and caching, FalconFS efficiently resolves paths on the server side using hybrid metadata indexing and lazy namespace replication. FalconFS also boosts server concurrency with concurrent request merging and provides easy deployment with VFS shortcut. Evaluations against CephFS and Lustre show that FalconFS achieves up to 5.72$\times$ throughput for small file read/write and up to 12.81$\times$ throughput for deep learning model training. FalconFS has been running in Huawei autonomous driving system's production environment with 10,000 NPUs for one year and has been open-sourced.

**Link**: [arxiv](http://arxiv.org/abs/2507.10367v3),  [pdf](http://arxiv.org/pdf/2507.10367v3)

**Tags**: cs.DC cs.PF 



### SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression   Block Size
**Authors**: Jinhan Chen, Jianchun Liu, Hongli Xu, Xianjun Gao, Shilong Wang

**Updated**: 2025-10-26T07:17:10Z

**Summary**: The growing memory footprint of the Key-Value (KV) cache poses a severe scalability bottleneck for long-context Large Language Model (LLM) inference. While KV cache eviction has emerged as an effective solution by discarding less critical tokens, existing token-, block-, and sentence-level compression methods struggle to balance semantic coherence and memory efficiency. To this end, we introduce SABlock, a \underline{s}emantic-aware KV cache eviction framework with \underline{a}daptive \underline{block} sizes. Specifically, SABlock first performs semantic segmentation to align compression boundaries with linguistic structures, then applies segment-guided token scoring to refine token importance estimation. Finally, for each segment, a budget-driven search strategy adaptively determines the optimal block size that preserves semantic integrity while improving compression efficiency under a given cache budget. Extensive experiments on long-context benchmarks demonstrate that SABlock consistently outperforms state-of-the-art baselines under the same memory budgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9% retrieval accuracy with only 96 KV entries, nearly matching the performance of the full-cache baseline that retains up to 8K entries. Under a fixed cache budget of 1,024, SABlock further reduces peak memory usage by 46.28% and achieves up to 9.5x faster decoding on a 128K context length.

**Link**: [arxiv](http://arxiv.org/abs/2510.22556v1),  [pdf](http://arxiv.org/pdf/2510.22556v1)

**Tags**: cs.CL 



### AttentionPredictor: Temporal Patterns Matter for KV Cache Compression
**Authors**: Qingyue Yang, Jie Wang, Xing Li, Zhihai Wang, Chen Chen, Lei Chen, Xianzhi Yu, Wulong Liu, Jianye Hao, Mingxuan Yuan, Bin Li

**Updated**: 2025-10-26T04:25:10Z

**Summary**: With the development of large language models (LLMs), efficient inference through Key-Value (KV) cache compression has attracted considerable attention, especially for long-context generation. To compress the KV cache, recent methods identify critical KV tokens through static modeling of attention scores. However, these methods often struggle to accurately determine critical tokens as they neglect the temporal patterns in attention scores, resulting in a noticeable degradation in LLM performance. To address this challenge, we propose AttentionPredictor, which is the first learning-based method to directly predict attention patterns for KV cache compression and critical token identification. Specifically, AttentionPredictor learns a lightweight, unified convolution model to dynamically capture spatiotemporal patterns and predict the next-token attention scores. An appealing feature of AttentionPredictor is that it accurately predicts the attention score and shares the unified prediction model, which consumes negligible memory, among all transformer layers. Moreover, we propose a cross-token critical cache prefetching framework that hides the token estimation time overhead to accelerate the decoding stage. By retaining most of the attention information, AttentionPredictor achieves 13$\times$ KV cache compression and 5.6$\times$ speedup in a cache offloading scenario with comparable LLM performance, significantly outperforming the state-of-the-arts. The code is available at https://github.com/MIRALab-USTC/LLM-AttentionPredictor.

**Link**: [arxiv](http://arxiv.org/abs/2502.04077v3),  [pdf](http://arxiv.org/pdf/2502.04077v3)

**Tags**: cs.CL cs.LG 



### A machine learning framework integrating seed traits and plasma   parameters for predicting germination uplift in crops
**Authors**: Saklain Niam, Tashfiqur Rahman, Md. Amjad Patwary, Mukarram Hossain

**Updated**: 2025-10-26T01:25:24Z

**Summary**: Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet outcomes remain difficult to predict due to complex seed--plasma--environment interactions. This study introduces the first machine learning framework to forecast germination uplift in soybean, barley, sunflower, radish, and tomato under dielectric barrier discharge (DBD) plasma. Among the models tested (GB, XGB, ET, and hybrids), Extra Trees (ET) performed best (R\textsuperscript{2} = 0.919; RMSE = 3.21; MAE = 2.62), improving to R\textsuperscript{2} = 0.925 after feature reduction. Engineering analysis revealed a hormetic response: negligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for 200--500 s, and reduced germination beyond 20 kV or prolonged exposures. Discharge power was also a dominant factor, with germination rate maximizing at $\geq$100 W with low exposure time. Species and cultivar-level predictions showed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high consistency, while sunflower remained slightly higher variable (MAE = 3.80). Among cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted, while Arian (2.86) and Ny\'{\i}rs\'{e}gi fekete (3.74) were comparatively poorly captured. This framework was also embedded into MLflow, providing a decision-support tool for optimizing CP seed germination in precision agriculture.

**Link**: [arxiv](http://arxiv.org/abs/2510.23657v1),  [pdf](http://arxiv.org/pdf/2510.23657v1)

**Tags**: cs.LG 



### Backward-Friendly Optimization: Training Large Language Models with   Approximate Gradients under Memory Constraints
**Authors**: Jing Yang, Kaitong Cai, Yijia Fan, Yufeng Yang, Keze Wang

**Updated**: 2025-10-26T00:50:12Z

**Summary**: Full fine-tuning of Large Language Models (LLMs) is notoriously memory-intensive, primarily because conventional optimizers such as SGD or Adam assume access to exact gradients derived from cached activations. Existing solutions either alter the model architecture (e.g., reversible networks) or trade memory for computation (e.g., activation checkpointing), but the optimizer itself remains untouched. In this work, we introduce GradLite, a backward-friendly optimizer that relaxes the requirement of exact gradients, enabling efficient training even when intermediate activations are aggressively discarded or approximated. GradLite leverages two key techniques: (i) low-rank Jacobian approximation, which reduces the dimensionality of backpropagated error signals, and (ii) error-feedback correction, which accumulates and compensates approximation errors across iterations to preserve convergence guarantees. We provide a theoretical analysis showing that GradLite maintains unbiased gradient estimates with bounded variance, ensuring convergence rates comparable to Adam. Empirically, GradLite reduces optimizer-state and activation memory consumption by up to 50\% without architectural changes, and achieves on-par or superior downstream performance on reasoning (MMLU, GSM8K), multilingual, and dialogue benchmarks compared to checkpointing and optimizer-centric baselines (LoMo, GaLore).

**Link**: [arxiv](http://arxiv.org/abs/2510.22467v1),  [pdf](http://arxiv.org/pdf/2510.22467v1)

**Tags**: cs.LG cs.AI 



### Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive   Token-Level Computation
**Authors**: Sangmin Bae, Yujin Kim, Reza Bayat, Sungnyun Kim, Jiyoun Ha, Tal Schuster, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Aaron Courville, Se-Young Yun

**Updated**: 2025-10-25T14:12:56Z

**Summary**: Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, we also propose a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to further decrease memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines. These gains demonstrate that MoR is an effective path towards large-model quality without incurring large-model cost.

**Link**: [arxiv](http://arxiv.org/abs/2507.10524v3),  [pdf](http://arxiv.org/pdf/2507.10524v3)

**Tags**: cs.CL cs.LG 



### Efficient Low Rank Attention for Long-Context Inference in Large   Language Models
**Authors**: Tenghui Li, Guoxu Zhou, Xuyang Zhao, Yuning Qiu, Qibin Zhao

**Updated**: 2025-10-25T11:43:27Z

**Summary**: As the length of input text grows, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long-context inference on resource constrained devices. Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or suboptimal retention of key-value pairs. We introduce Low Rank Query and Key attention (LRQK), a two-stage framework that jointly decomposes the full-precision query and key matrices into compact rank-\(r\) factors during the prefill stage, and then uses these low-dimensional projections to compute proxy attention scores in \(\mathcal{O}(lr)\) time at each decode step. By selecting only the top-\(k\) tokens and a small fixed set of recent tokens, LRQK employs a mixed GPU-CPU cache with a hit-and-miss mechanism that transfers only missing full-precision KV pairs, thereby preserving exact attention outputs while reducing CPU-GPU data movement. Extensive experiments on the RULER and LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK matches or surpasses leading sparse-attention methods in long context settings, while delivering significant memory savings with minimal loss in accuracy. Our code is available at https://github.com/tenghuilee/LRQK.

**Link**: [arxiv](http://arxiv.org/abs/2510.23649v1),  [pdf](http://arxiv.org/pdf/2510.23649v1)

**Tags**: cs.LG cs.AI 



### Fundamental Limits of Coded Caching with Fixed Subpacketization
**Authors**: Minquan Cheng, Yifei Huang, Youlong Wu, Jinyan Wang

**Updated**: 2025-10-25T03:34:34Z

**Summary**: Coded caching is a promising technique to create coded multicast opportunities for cache-aided networks. By splitting each file into $F$ equal packets (i.e., the subpacketization level $F$) and letting each user cache a set of packets, the transmission load can be significantly reduced via coded multicasting. It has been shown that a higher subpacketization level could potentially lead to a lower transmission load, as more packets can be combined for efficient transmission. On the other hand, a larger $F$ indicates a higher coding complexity and is problematic from a practical perspective when $F$ is extremely large. Despite many works attempting to design coded caching schemes with low subpacketization levels, a fundamental problem remains open: What is the minimum transmission load given any fixed subpacketization level? In this paper, we consider the classical cache-aided networks with identically uncoded placement and one-shot delivery strategy, and investigate the fundamental trade-off between the transmission load and the subpacketization level. We propose a \emph{general} lower bound on the transmission load for any fixed subpacketization by reformulating the centralized coded caching schemes via the combinatorial structure of the corresponding placement delivery array. The lower bound also recovers existing optimality results for the bipartite graph scheme (including the well-known Maddah-Ali and Niesen (MN) scheme and the conjugate MN scheme) as well as the grouping bipartite graph scheme. Furthermore, by carefully exploiting the combinatorial structure and computing the union size of sorted sets, we establish a new optimality result, i.e., the partition scheme can achieve the optimal rate-subpacketization trade-off.

**Link**: [arxiv](http://arxiv.org/abs/2510.22145v1),  [pdf](http://arxiv.org/pdf/2510.22145v1)

**Tags**: cs.IT math.IT 



### EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient   Image Editing
**Authors**: Zexuan Yan, Yue Ma, Chang Zou, Wenteng Chen, Qifeng Chen, Linfeng Zhang

**Updated**: 2025-10-25T02:29:47Z

**Summary**: Inversion-based image editing is rapidly gaining momentum while suffering from significant computation overhead, hindering its application in real-time interactive scenarios. In this paper, we rethink that the redundancy in inversion-based image editing exists in both the spatial and temporal dimensions, such as the unnecessary computation in unedited regions and the redundancy in the inversion progress. To tackle these challenges, we propose a practical framework, named EEdit, to achieve efficient image editing. Specifically, we introduce three techniques to solve them one by one. For spatial redundancy, spatial locality caching is introduced to compute the edited region and its neighboring regions while skipping the unedited regions, and token indexing preprocessing is designed to further accelerate the caching. For temporal redundancy, inversion step skipping is proposed to reuse the latent for efficient editing. Our experiments demonstrate an average of 2.46 $\times$ acceleration without performance drop in a wide range of editing tasks including prompt-guided image editing, dragging and image composition. Our codes are available at https://github.com/yuriYanZeXuan/EEdit

**Link**: [arxiv](http://arxiv.org/abs/2503.10270v3),  [pdf](http://arxiv.org/pdf/2503.10270v3)

**Tags**: cs.CV 



### Massive Memorization with Hundreds of Trillions of Parameters for   Sequential Transducer Generative Recommenders
**Authors**: Zhimin Chen, Chenyu Zhao, Ka Chun Mo, Yunjiang Jiang, Jane H. Lee, Shouwei Chen, Khushhall Chandra Mahajan, Ning Jiang, Kai Ren, Jinhui Li, Wen-Yun Yang

**Updated**: 2025-10-24T22:17:49Z

**Summary**: Modern large-scale recommendation systems rely heavily on user interaction history sequences to enhance the model performance. The advent of large language models and sequential modeling techniques, particularly transformer-like architectures, has led to significant advancements recently (e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories (10k to 100k items) generally improves model performance, it also creates significant challenges on latency, queries per second (QPS) and GPU cost in industry-scale recommendation systems. Existing models do not adequately address these industrial scalability issues. In this paper, we propose a novel two-stage modeling framework, namely VIrtual Sequential Target Attention (VISTA), which decomposes traditional target attention from a candidate item to user history items into two distinct stages: (1) user history summarization into a few hundred tokens; followed by (2) candidate item attention to those tokens. These summarization token embeddings are then cached in storage system and then utilized as sequence features for downstream model training and inference. This novel design for scalability enables VISTA to scale to lifelong user histories (up to one million items) while keeping downstream training and inference costs fixed, which is essential in industry. Our approach achieves significant improvements in offline and online metrics and has been successfully deployed on an industry leading recommendation platform serving billions of users.

**Link**: [arxiv](http://arxiv.org/abs/2510.22049v1),  [pdf](http://arxiv.org/pdf/2510.22049v1)

**Tags**: cs.IR cs.LG 



### BachVid: Training-Free Video Generation with Consistent Background and   Character
**Authors**: Han Yan, Xibin Song, Yifu Wang, Hongdong Li, Pan Ji, Chao Ma

**Updated**: 2025-10-24T17:56:37Z

**Summary**: Diffusion Transformers (DiTs) have recently driven significant progress in text-to-video (T2V) generation. However, generating multiple videos with consistent characters and backgrounds remains a significant challenge. Existing methods typically rely on reference images or extensive training, and often only address character consistency, leaving background consistency to image-to-video models. We introduce BachVid, the first training-free method that achieves consistent video generation without needing any reference images. Our approach is based on a systematic analysis of DiT's attention mechanism and intermediate features, revealing its ability to extract foreground masks and identify matching points during the denoising process. Our method leverages this finding by first generating an identity video and caching the intermediate variables, and then inject these cached variables into corresponding positions in newly generated videos, ensuring both foreground and background consistency across multiple videos. Experimental results demonstrate that BachVid achieves robust consistency in generated videos without requiring additional training, offering a novel and efficient solution for consistent video generation without relying on reference images or additional training.

**Link**: [arxiv](http://arxiv.org/abs/2510.21696v1),  [pdf](http://arxiv.org/pdf/2510.21696v1)

**Tags**: cs.CV 



## Keyword: LLM Inference 
 ### Function on Scalar Regression with Complex Survey Designs
**Authors**: Lily Koffman, Sunan Gao, Xinkai Zhou, Andrew Leroux, Ciprian Crainiceanu, John Muschelli III

**Updated**: 2025-11-07T18:56:18Z

**Summary**: Large health surveys increasingly collect high-dimensional functional data from wearable devices, and function on scalar regression (FoSR) is often used to quantify the relationship between these functional outcomes and scalar covariates such as age and sex. However, existing methods for FoSR fail to account for complex survey design. We introduce inferential methods for FoSR for studies with complex survey designs. The method combines fast univariate inference (FUI) developed for functional data outcomes and survey sampling inferential methods developed for scalar outcomes. Our approach consists of three steps: (1) fit survey weighted GLMs at each point along the functional domain, (2) smooth coefficients along the functional domain, and (3) use balanced repeated replication (BRR) or the Rao-Wu-Yue-Beaumont (RWYB) bootstrap to obtain pointwise and joint confidence bands for the functional coefficients. The method is motivated by association studies between continuous physical activity data and covariates collected in the National Health and Nutrition Examination Survey (NHANES). A first-of-its-kind analytical simulation study and empirical simulation using the NHANES data demonstrates that our method performs better than existing methods that do not account for the survey structure. Finally, application of the method in NHANES shows the practical implications of accounting for survey structure. The method is implemented in the R package svyfosr.

**Link**: [arxiv](http://arxiv.org/abs/2511.05487v1),  [pdf](http://arxiv.org/pdf/2511.05487v1)

**Tags**: stat.ME stat.AP 



### Non-Gaussian Galaxy Stochasticity and the Noise-Field Formulation
**Authors**: Henrique Rubira, Fabian Schmidt

**Updated**: 2025-11-07T18:53:39Z

**Summary**: We revisit the stochastic, or noise, contributions to the galaxy density field within the effective field theory (EFT) of large-scale structure. Starting from the general, all-order expression of the EFT partition function, we elucidate how the stochastic contributions can be described by local nonlinear couplings of a single Gaussian noise field. We introduce an alternative formulation of the partition function in terms of such a noise field, and derive the corresponding field-level likelihood for biased tracers. This noise-field formulation can capture the complete set of stochastic contributions to the galaxy density at the field level in a normalized, positive-definite probability density which is suitable for numerical sampling. We illustrate this by presenting the first results of EFT-based field-level inference with non-Gaussian and density-dependent stochasticity on dark matter halos using LEFTfield.

**Link**: [arxiv](http://arxiv.org/abs/2511.05484v1),  [pdf](http://arxiv.org/pdf/2511.05484v1)

**Tags**: astro-ph.CO 



### FPGA-Based Real-Time Waveform Classification
**Authors**: Alperen Aksoy, Ilja Bekman, Chimezie Eguzo, Christian Grewing, Andre Zambanini

**Updated**: 2025-11-07T18:44:50Z

**Summary**: For self-triggered readout of SiPM sum signals, a waveform classification can aid a simple threshold trigger to reliably extract calorimetric particle hit information online at an early stage and thus reduce the volume of transmitted data. Typically, the ADC data acquisition is based on FPGAs for edge data processing. In this study, we consider look-up-table-based neural-networks and address challenges of binary multi-layer neural networks' layout, footprint, performance and training. We show that these structures can be trained using a genetic algorithm and achieve the inference latency compatible with dead-time free processing online.

**Link**: [arxiv](http://arxiv.org/abs/2511.05479v1),  [pdf](http://arxiv.org/pdf/2511.05479v1)

**Tags**: cs.NE physics.ins-det 



### A Metamorphic Testing Perspective on Knowledge Distillation for Language   Models of Code: Does the Student Deeply Mimic the Teacher?
**Authors**: Md. Abdul Awal, Mrigank Rochan, Chanchal K. Roy

**Updated**: 2025-11-07T18:38:54Z

**Summary**: Transformer-based language models of code have achieved state-of-the-art performance across a wide range of software analytics tasks, but their practical deployment remains limited due to high computational costs, slow inference speeds, and significant environmental impact. To address these challenges, recent research has increasingly explored knowledge distillation as a method for compressing a large language model of code (the teacher) into a smaller model (the student) while maintaining performance. However, the degree to which a student model deeply mimics the predictive behavior and internal representations of its teacher remains largely unexplored, as current accuracy-based evaluation provides only a surface-level view of model quality and often fails to capture more profound discrepancies in behavioral fidelity between the teacher and student models. To address this gap, we empirically show that the student model often fails to deeply mimic the teacher model, resulting in up to 285% greater performance drop under adversarial attacks, which is not captured by traditional accuracy-based evaluation. Therefore, we propose MetaCompress, a metamorphic testing framework that systematically evaluates behavioral fidelity by comparing the outputs of teacher and student models under a set of behavior-preserving metamorphic relations. We evaluate MetaCompress on two widely studied tasks, using compressed versions of popular language models of code, obtained via three different knowledge distillation techniques: Compressor, AVATAR, and MORPH. The results show that MetaCompress identifies up to 62% behavioral discrepancies in student models, underscoring the need for behavioral fidelity evaluation within the knowledge distillation pipeline and establishing MetaCompress as a practical framework for testing compressed language models of code derived through knowledge distillation.

**Link**: [arxiv](http://arxiv.org/abs/2511.05476v1),  [pdf](http://arxiv.org/pdf/2511.05476v1)

**Tags**: cs.SE cs.LG 



### Precipitation nowcasting of satellite data using physically conditioned   neural networks
**Authors**: Antônio Catão, Melvin Poveda, Leonardo Voltarelli, Paulo Orenstein

**Updated**: 2025-11-07T18:33:40Z

**Summary**: Accurate short-term precipitation forecasts predominantly rely on dense weather-radar networks, limiting operational value in places most exposed to climate extremes. We present TUPANN (Transferable and Universal Physics-Aligned Nowcasting Network), a satellite-only model trained on GOES-16 RRQPE. Unlike most deep learning models for nowcasting, TUPANN decomposes the forecast into physically meaningful components: a variational encoder-decoder infers motion and intensity fields from recent imagery under optical-flow supervision, a lead-time-conditioned MaxViT evolves the latent state, and a differentiable advection operator reconstructs future frames. We evaluate TUPANN on both GOES-16 and IMERG data, in up to four distinct climates (Rio de Janeiro, Manaus, Miami, La Paz) at 10-180min lead times using the CSI and HSS metrics over 4-64 mm/h thresholds. Comparisons against optical-flow, deep learning and hybrid baselines show that TUPANN achieves the best or second-best skill in most settings, with pronounced gains at higher thresholds. Training on multiple cities further improves performance, while cross-city experiments show modest degradation and occasional gains for rare heavy-rain regimes. The model produces smooth, interpretable motion fields aligned with numerical optical flow and runs in near real time due to the low latency of GOES-16. These results indicate that physically aligned learning can provide nowcasts that are skillful, transferable and global.

**Link**: [arxiv](http://arxiv.org/abs/2511.05471v1),  [pdf](http://arxiv.org/pdf/2511.05471v1)

**Tags**: cs.LG 



### $\texttt{unimpeded}$: A Public Nested Sampling Database for Bayesian   Cosmology
**Authors**: Dily Duan Yi Ong, Will Handley

**Updated**: 2025-11-07T18:30:44Z

**Summary**: Bayesian inference is central to modern cosmology. While parameter estimation is achievable with unnormalised posteriors traditionally obtained via MCMC methods, comprehensive model comparison and tension quantification require Bayesian evidences and normalised posteriors, which remain computationally prohibitive for many researchers. To address this, we present $\texttt{unimpeded}$, a publicly available Python library and data repository providing DiRAC-funded (DP192 and 264) pre-computed nested sampling and MCMC chains with their normalised posterior samples, computed using $\texttt{Cobaya}$ and the Boltzmann solver $\texttt{CAMB}$. $\texttt{unimpeded}$ delivers systematic analysis across a grid of eight cosmological models (including $\Lambda$CDM and seven extensions) and 39 modern cosmological datasets (comprising individual probes and their pairwise combinations). The built-in tension statistics calculator enables rapid computation of six tension quantification metrics. All chains are hosted on Zenodo with permanent access via the unimpeded API, analogous to the renowned Planck Legacy Archive but utilising nested sampling in addition to traditional MCMC methods.

**Link**: [arxiv](http://arxiv.org/abs/2511.05470v1),  [pdf](http://arxiv.org/pdf/2511.05470v1)

**Tags**: astro-ph.IM astro-ph.CO 



### Photo Dating by Facial Age Aggregation
**Authors**: Jakub Paplham, Vojtech Franc

**Updated**: 2025-11-07T18:08:04Z

**Summary**: We introduce a novel method for Photo Dating which estimates the year a photograph was taken by leveraging information from the faces of people present in the image. To facilitate this research, we publicly release CSFD-1.6M, a new dataset containing over 1.6 million annotated faces, primarily from movie stills, with identity and birth year annotations. Uniquely, our dataset provides annotations for multiple individuals within a single image, enabling the study of multi-face information aggregation. We propose a probabilistic framework that formally combines visual evidence from modern face recognition and age estimation models, and career-based temporal priors to infer the photo capture year. Our experiments demonstrate that aggregating evidence from multiple faces consistently improves the performance and the approach significantly outperforms strong, scene-based baselines, particularly for images containing several identifiable individuals.

**Link**: [arxiv](http://arxiv.org/abs/2511.05464v1),  [pdf](http://arxiv.org/pdf/2511.05464v1)

**Tags**: cs.CV 



### Bayesian Self-Calibration and Parametric Channel Estimation for 6G   Antenna Arrays
**Authors**: Patrick Hödl, Jakob Möderl, Erik Leitinger, Klaus Witrisal

**Updated**: 2025-11-07T18:04:29Z

**Summary**: Accurate channel estimation is essential for both high-rate communication and high-precision sensing in 6G wireless systems. However, a major performance limitation arises from calibration mismatches when operating phased-array antennas under real-world conditions. To address this issue, we propose to integrate antenna element self-calibration into a variational sparse Bayesian learning (VSBL) algorithm for parametric channel estimation. We model antenna gain and phase deviations as latent variables and derive explicit update equations to jointly infer these calibration parameters and the channel parameters: the model order, complex amplitudes, delays, angles, and the noise variance. The resulting algorithm operates online and adapts in real time to hardware-induced mismatches. We assess its performance in terms of the root mean square error (RMSE) and the optimal subpattern-assignment (OSPA) metric, demonstrating consistent improvements over conventional VSBL without calibration. Our results demonstrate that embedding self-calibration within Bayesian inference significantly enhances the robustness of channel estimation.

**Link**: [arxiv](http://arxiv.org/abs/2510.11628v2),  [pdf](http://arxiv.org/pdf/2510.11628v2)

**Tags**: eess.SP 



### A Taxonomy and Comparative Analysis of IPv4 Identifier Selection   Correctness, Security, and Performance
**Authors**: Joshua J. Daymude, Antonio M. Espinoza, Holly Bergen, Benjamin Mixon-Baca, Jeffrey Knockel, Jedidiah R. Crandall

**Updated**: 2025-11-07T18:03:32Z

**Summary**: The battle for a more secure Internet is waged on many fronts, including the most basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an IPv4 header field as old as the Internet with an equally long history as an exploited side channel for scanning network properties, inferring off-path connections, and poisoning DNS caches. This article taxonomizes the 25-year history of IPID-based exploits and the corresponding changes to IPID selection methods. By mathematically analyzing these methods' correctness and security and empirically evaluating their performance, we reveal recommendations for best practice as well as shortcomings of current operating system implementations, emphasizing the value of systematic evaluations in network security.

**Link**: [arxiv](http://arxiv.org/abs/2406.06483v4),  [pdf](http://arxiv.org/pdf/2406.06483v4)

**Tags**: cs.NI cs.CR 



### SWE-Compass: Towards Unified Evaluation of Agentic Coding Abilities for   Large Language Models
**Authors**: Jingxuan Xu, Ken Deng, Weihao Li, Songwei Yu, Huaixi Tang, Haoyang Huang, Zhiyi Lai, Zizheng Zhan, Yanan Wu, Chenchen Zhang, Kepeng Lei, Yifan Yao, Xinping Lei, Wenqiang Zhu, Zongxian Feng, Han Li, Junqi Xiong, Dailin Li, Zuchen Gao, Kun Wu, Wen Xiang, Ziqi Zhan, Yuanxing Zhang, Wuxuan Gong, Ziyuan Gao, Guanxiang Wang, Yirong Xue, Xiaojiang Zhang, Jinghui Wang, Huiming Wang, Wenhao Zhuang, Zhaoxiang Zhang, Yuqun Zhang, Haotian Zhang, Bin Chen, Jiaheng Liu

**Updated**: 2025-11-07T18:01:32Z

**Summary**: Evaluating large language models (LLMs) for software engineering has been limited by narrow task coverage, language bias, and insufficient alignment with real-world developer workflows. Existing benchmarks often focus on algorithmic problems or Python-centric bug fixing, leaving critical dimensions of software engineering underexplored. To address these gaps, we introduce SWE-Compass1, a comprehensive benchmark that unifies heterogeneous code-related evaluations into a structured and production-aligned framework. SWE-Compass spans 8 task types, 8 programming scenarios, and 10 programming languages, with 2000 high-quality instances curated from authentic GitHub pull requests and refined through systematic filtering and validation. We benchmark ten state-of-the-art LLMs under two agentic frameworks, SWE-Agent and Claude Code, revealing a clear hierarchy of difficulty across task types, languages, and scenarios. Moreover, by aligning evaluation with real-world developer practices, SWE-Compass provides a rigorous and reproducible foundation for diagnosing and advancing agentic coding capabilities in large language models.

**Link**: [arxiv](http://arxiv.org/abs/2511.05459v1),  [pdf](http://arxiv.org/pdf/2511.05459v1)

**Tags**: cs.SE cs.AI 



### Statistical parametric simulation studies based on real data
**Authors**: Christina Sauer, F. Julian D. Lange, Maria Thurow, Ina Dormuth, Anne-Laure Boulesteix

**Updated**: 2025-11-07T17:52:25Z

**Summary**: Simulation studies are indispensable for evaluating statistical methods and ubiquitous in statistical research. The most common simulation approach is parametric simulation, where the data-generating mechanism (DGM) corresponds to a parametric model from which observations are drawn. While many simulation studies aim to give practical guidance on method suitability, parametric simulations in particular are often criticized for being unrealistic. To overcome this drawback, it is sensible to employ real data for constructing the parametric DGMs. However, although real-data-based parametric DGMs are widely used, the specific ways in which DGM components are inferred vary, and their implications may not be fully understood. Additionally, researchers often rely on a limited selection of real datasets, with the rationale for their selection being unclear. This paper reviews and formally discusses how components of parametric DGMs can be inferred from real data and how dataset selection can be performed more systematically. By doing so, we aim to support researchers in conducting simulation studies with a lower risk of overgeneralization and misinterpretation. We illustrate the construction of parametric DGMs based on a systematically selected set of real datasets using two examples: one on ordinal outcomes in randomized controlled trials and one on differential gene expression analysis.

**Link**: [arxiv](http://arxiv.org/abs/2504.04864v3),  [pdf](http://arxiv.org/pdf/2504.04864v3)

**Tags**: stat.ME 



### P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication
**Authors**: Sneha Oram, Pushpak Bhattacharyya

**Updated**: 2025-11-07T17:49:33Z

**Summary**: Although explainability and interpretability have received significant attention in artificial intelligence (AI) and natural language processing (NLP) for mental health, reasoning has not been examined in the same depth. Addressing this gap is essential to bridge NLP and mental health through interpretable and reasoning-capable AI systems. To this end, we investigate the pragmatic reasoning capability of large-language models (LLMs) in the mental health domain. We introduce PRiMH dataset, and propose pragmatic reasoning tasks in mental health with pragmatic implicature and presupposition phenomena. In particular, we formulate two tasks in implicature and one task in presupposition. To benchmark the dataset and the tasks presented, we consider four models: Llama3.1, Mistral, MentaLLaMa, and Qwen. The results of the experiments suggest that Mistral and Qwen show substantial reasoning abilities in the domain. Subsequently, we study the behavior of MentaLLaMA on the proposed reasoning tasks with the rollout attention mechanism. In addition, we also propose three StiPRompts to study the stigma around mental health with the state-of-the-art LLMs, GPT4o-mini, Deepseek-chat, and Claude-3.5-haiku. Our evaluated findings show that Claude-3.5-haiku deals with stigma more responsibly compared to the other two LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2507.23247v2),  [pdf](http://arxiv.org/pdf/2507.23247v2)

**Tags**: cs.CL 



### How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?
**Authors**: Tuan Anh Tran, Duy M. H. Nguyen, Hoai-Chau Tran, Michael Barz, Khoa D. Doan, Roger Wattenhofer, Ngo Anh Vien, Mathias Niepert, Daniel Sonntag, Paul Swoboda

**Updated**: 2025-11-07T17:38:01Z

**Summary**: Recent advances in 3D point cloud transformers have led to state-of-the-art results in tasks such as semantic segmentation and reconstruction. However, these models typically rely on dense token representations, incurring high computational and memory costs during training and inference. In this work, we present the finding that tokens are remarkably redundant, leading to substantial inefficiency. We introduce gitmerge3D, a globally informed graph token merging method that can reduce the token count by up to 90-95% while maintaining competitive performance. This finding challenges the prevailing assumption that more tokens inherently yield better performance and highlights that many current models are over-tokenized and under-optimized for scalability. We validate our method across multiple 3D vision tasks and show consistent improvements in computational efficiency. This work is the first to assess redundancy in large-scale 3D transformer models, providing insights into the development of more efficient 3D foundation architectures. Our code and checkpoints are publicly available at https://gitmerge3d.github.io

**Link**: [arxiv](http://arxiv.org/abs/2511.05449v1),  [pdf](http://arxiv.org/pdf/2511.05449v1)

**Tags**: cs.CV cs.LG 



### Flashlight: PyTorch Compiler Extensions to Accelerate Attention Variants
**Authors**: Bozhi You, Irene Wang, Zelal Su Mustafaoglu, Abhinav Jangda, Angélica Moreira, Roshan Dathathri, Divya Mahajan, Keshav Pingali

**Updated**: 2025-11-07T17:26:02Z

**Summary**: Attention is a fundamental building block of large language models (LLMs), so there have been many efforts to implement it efficiently. For example, FlashAttention leverages tiling and kernel fusion to optimize attention. Recently, a number of variants of attention have been introduced to enhance model quality or efficiency. Supporting them efficiently remains difficult since they usually require specialized kernels or hand-tuned implementations. FlexAttention recently addressed part of this gap by using static programming templates to support FlashAttention-like kernels for a subset of attention variants.   In this paper, we introduce Flashlight, a compiler-native framework within the PyTorch ecosystem that automatically generates fused, FlashAttention-style kernels for arbitrary attention-based programs, without relying on static templates or predefined kernel specializations. Flashlight leverages PyTorch's compilation workflow to fuse and tile attention computations transparently, enabling efficient execution for diverse attention patterns. Not only does it support all variants expressible in the FlexAttention model but it also handles more general, data-dependent attention formulations that are beyond the capabilities of FlexAttention.   Our results show that Flashlight produces kernels with competitive or superior performance to FlexAttention, while offering the flexibility of native PyTorch code, enabling developers to rapidly explore new attention models without sacrificing performance.

**Link**: [arxiv](http://arxiv.org/abs/2511.02043v3),  [pdf](http://arxiv.org/pdf/2511.02043v3)

**Tags**: cs.LG cs.PF 



### MACO: A Multi-Agent LLM-Based Hardware/Software Co-Design Framework for   CGRAs
**Authors**: Zesong Jiang, Yuqi Sun, Qing Zhong, Mahathi Krishna, Deepak Patil, Cheng Tan, Sriram Krishnamoorthy, Jeff Zhang

**Updated**: 2025-11-07T17:21:08Z

**Summary**: Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing architecture that can deliver high-performance, energy-efficient acceleration across diverse domains. By supporting reconfiguration at the functional unit level, CGRAs efficiently adapt to varying computational patterns and optimize resource utilization. However, designing CGRAs is highly challenging due to the vast design space, independent architectural parameters, and the time-consuming nature of manual design. Fortunately, the rapid advancement of large language models (LLMs) presents new opportunities to automate this process.   In this work, we propose MACO-- an open-source multi-agent LLM-based framework for Hardware/Software (HW/SW) co-design of CGRAs. The framework employs LLM reasoning to generate CGRAs across four stages: HW/SW co-design, Design error correction, Best design selection, and Evaluation & Feedback. Furthermore, MACO iteratively optimizes the generated CGRAs, leveraging agent reasoning and feedback to achieve higher PPA (that is, power, performance, and area) design points for a given domain. In addition, we introduce an LLM self-learning mechanism that employs LLM-driven decision making to select the optimal CGRA to accelerate the design process.   We evaluate the framework with state-of-the-art LLM-based methods and manual CGRA design, in terms of performance, power consumption, and area. Experimental results show that MACO efficiently generates high-quality CGRA architectures, significantly reducing manual design effort and demonstrating the potential of our framework for real-world CGRA design.

**Link**: [arxiv](http://arxiv.org/abs/2509.13557v5),  [pdf](http://arxiv.org/pdf/2509.13557v5)

**Tags**: cs.AR 



### Large language models as uncertainty-calibrated optimizers for   experimental discovery
**Authors**: Bojana Ranković, Ryan-Rhys Griffiths, Philippe Schwaller

**Updated**: 2025-11-07T17:11:12Z

**Summary**: Scientific discovery increasingly depends on efficient experimental optimization to navigate vast design spaces under time and resource constraints. Traditional approaches often require extensive domain expertise and feature engineering. While large language models, with their vast scientific knowledge, circumvent the feature engineering limitations, they lack the calibrated uncertainty estimates required for high-stakes decision making. Hence, current optimization methods force a choice between domain knowledge and reliability, with no principled approach that affords both. In this work, we show that training language models through the uncertainty-aware objectives of traditional optimization methods enables their use as reliable optimizers guided by natural language. By teaching LLMs from experimental outcomes under uncertainty, we transform their overconfidence from a fundamental limitation into a precise calibration mechanism. Applied to Buchwald-Hartwig reactions, a cornerstone of pharmaceutical synthesis, our method nearly doubles the discovery rate of high-yielding reaction conditions, from 24% to 43% in 50 experimental iterations starting from 10 unsuccessful conditions. Across 19 diverse optimization problems spanning organic synthesis, materials science and catalysis, process chemistry, and molecular design, our approach ranks first on average, establishing a new paradigm for reliable, uncertainty-guided optimization with LLMs. Our approach can accelerate discovery by lowering the barrier to using powerful optimization methods, replacing the need for domain-specific feature engineering with more accessible natural language interfaces. These findings highlight that ensuring reliability through principled uncertainty quantification is critical for realizing the full potential of AI-guided experimentation.

**Link**: [arxiv](http://arxiv.org/abs/2504.06265v3),  [pdf](http://arxiv.org/pdf/2504.06265v3)

**Tags**: cs.LG cs.AI 



### Shared Latent Representation for Joint Text-to-Audio-Visual Synthesis
**Authors**: Dogucan Yaman, Seymanur Akti, Fevziye Irem Eyiokur, Alexander Waibel

**Updated**: 2025-11-07T17:07:56Z

**Summary**: We propose a text-to-talking-face synthesis framework leveraging latent speech representations from HierSpeech++. A Text-to-Vec module generates Wav2Vec2 embeddings from text, which jointly condition speech and face generation. To handle distribution shifts between clean and TTS-predicted features, we adopt a two-stage training: pretraining on Wav2Vec2 embeddings and finetuning on TTS outputs. This enables tight audio-visual alignment, preserves speaker identity, and produces natural, expressive speech and synchronized facial motion without ground-truth audio at inference. Experiments show that conditioning on TTS-predicted latent features outperforms cascaded pipelines, improving both lip-sync and visual realism.

**Link**: [arxiv](http://arxiv.org/abs/2511.05432v1),  [pdf](http://arxiv.org/pdf/2511.05432v1)

**Tags**: cs.CV 



### A Unified Framework for Inference with General Missingness Patterns and   Machine Learning Imputation
**Authors**: Xingran Chen, Tyler McCormick, Bhramar Mukherjee, Zhenke Wu

**Updated**: 2025-11-07T17:00:31Z

**Summary**: Pre-trained machine learning (ML) predictions have been increasingly used to complement incomplete data to enable downstream scientific inquiries, but their naive integration risks biased inferences. Recently, multiple methods have been developed to provide valid inference with ML imputations regardless of prediction quality and to enhance efficiency relative to complete-case analyses. However, existing approaches are often limited to missing outcomes under a missing-completely-at-random (MCAR) assumption, failing to handle general missingness patterns (missing in both the outcome and exposures) under the more realistic missing-at-random (MAR) assumption. This paper develops a novel method that delivers a valid statistical inference framework for general Z-estimation problems using ML imputations under the MAR assumption and for general missingness patterns. The core technical idea is to stratify observations by distinct missingness patterns and construct an estimator by appropriately weighting and aggregating pattern-specific information through a masking-and-imputation procedure on the complete cases. We provide theoretical guarantees of asymptotic normality of the proposed estimator and efficiency dominance over weighted complete-case analyses. Practically, the method affords simple implementations by leveraging existing weighted complete-case analysis software. Extensive simulations are carried out to validate theoretical results. A real data example is provided to further illustrate the practical utility of the proposed method. The paper concludes with a brief discussion on practical implications, limitations, and potential future directions.

**Link**: [arxiv](http://arxiv.org/abs/2508.15162v2),  [pdf](http://arxiv.org/pdf/2508.15162v2)

**Tags**: stat.ME stat.ML 



### Evidence of Energy Injection in the Short and Distant GRB 250221A
**Authors**: Camila Angulo-Valdez, Rosa L. Becerra, Ramandeep Gill, Noémie Globus, William H. Lee, Diego López-Cámara, Cassidy Mihalenko, Enrique Moreno-Méndez, Roberto Ricci, Karelle Siellez, Alan M. Watson, Muskan Yadav, Yu-han Yang, Dalya Akl, Sarah Antier, Jean-Luc Atteia, Stéphane Basa, Nathaniel R. Butler, Simone Dichiara, Damien Dornic, Jean-Grégoire Ducoin, Francis Fortin, Leonardo García-García, Kin Ocelotl López, Francesco Magnani, Brendan O'Connor, Margarita Pereyra, Ny Avo Rakotondrainibe, Fredd Sánchez-Álvarez, Benjamin Schneider, Eleonora Troja, Antonio de Ugarte Postigo

**Updated**: 2025-11-07T16:56:42Z

**Summary**: We present the photometric and spectroscopic analysis of the short-duration GRB 250221A ($T_{90}=1.80\pm0.32$ s), using a data set from the optical facilities COLIBR\'I, the Harlingten 50 cm Telescope, and the Very Large Telescope. We complement these observations with data from the \textit{Neil Gehrels Swift Observatory} and the \textit{Einstein Probe}, as well as radio observations from the Very Large Array. GRB 250221A is among the few short GRBs with direct afterglow spectroscopy, which gives a secure redshift determination of $z=0.768$ and allows the unambiguous identification of the host as a galaxy with a star-formation rate of $\sim3\,M_\odot\,{\rm yr}^{-1}$. The X-ray and optical light curves up to $T_0+10$ ks (where $T_0$ refers to the GRB trigger time) are well described by forward-shock synchrotron emission in the slow-cooling regime within the standard fireball framework. However, at $T_0+0.6$ days, both the X-ray and optical bands exhibit an excess over the same interval, which we interpret as evidence of energy injection into a jet with a half-opening angle of $\theta_j=11.5^{\circ}$ through a refreshed shock powered by late central engine activity or a radially stratified ejecta. The burst properties (duration, spectral hardness, peak energy, and location in the Amati plane) all favour a compact binary merger origin. However, our modelling of the afterglow suggests a dense circumburst medium ($n\sim80$ cm$^{-3}$), which is more typical of a Collapsar environment. This tension over the classification of this burst (short-hard vs. long-soft) as inferred from the prompt and afterglow emissions makes GRB~250221A an unusual event and underscores the limitations of duration-based classifications and the importance of multi-wavelength, time-resolved follow-up observations.

**Link**: [arxiv](http://arxiv.org/abs/2510.19132v4),  [pdf](http://arxiv.org/pdf/2510.19132v4)

**Tags**: astro-ph.HE 



### Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided   Medical Image Editing
**Authors**: Zhihui Chen, Mengling Feng

**Updated**: 2025-11-07T16:53:02Z

**Summary**: Medical image editing has emerged as a pivotal technology with broad applications in data augmentation, model interpretability, medical education, and treatment simulation. However, the lack of large-scale, high-quality, and openly accessible datasets tailored for medical contexts with strict anatomical and clinical constraints has significantly hindered progress in this domain. To bridge this gap, we introduce Med-Banana-50K, a comprehensive dataset of over 50k medically curated image edits spanning chest X-ray, brain MRI, and fundus photography across 23 diseases. Each sample supports bidirectional lesion editing (addition and removal) and is constructed using Gemini-2.5-Flash-Image based on real clinical images. A key differentiator of our dataset is the medically grounded quality control protocol: we employ an LLM-as-Judge evaluation framework with criteria such as instruction compliance, structural plausibility, image realism, and fidelity preservation, alongside iterative refinement over up to five rounds. Additionally, Med-Banana-50K includes around 37,000 failed editing attempts with full evaluation logs to support preference learning and alignment research. By offering a large-scale, medically rigorous, and fully documented resource, Med-Banana-50K establishes a critical foundation for developing and evaluating reliable medical image editing systems. Our dataset and code are publicly available. [https://github.com/richardChenzhihui/med-banana-50k].

**Link**: [arxiv](http://arxiv.org/abs/2511.00801v3),  [pdf](http://arxiv.org/pdf/2511.00801v3)

**Tags**: cs.CV cs.MM 



### Sharing the Learned Knowledge-base to Estimate Convolutional Filter   Parameters for Continual Image Restoration
**Authors**: Aupendu Kar, Krishnendu Ghosh, Prabir Kumar Biswas

**Updated**: 2025-11-07T16:52:42Z

**Summary**: Continual learning is an emerging topic in the field of deep learning, where a model is expected to learn continuously for new upcoming tasks without forgetting previous experiences. This field has witnessed numerous advancements, but few works have been attempted in the direction of image restoration. Handling large image sizes and the divergent nature of various degradation poses a unique challenge in the restoration domain. However, existing works require heavily engineered architectural modifications for new task adaptation, resulting in significant computational overhead. Regularization-based methods are unsuitable for restoration, as different restoration challenges require different kinds of feature processing. In this direction, we propose a simple modification of the convolution layer to adapt the knowledge from previous restoration tasks without touching the main backbone architecture. Therefore, it can be seamlessly applied to any deep architecture without any structural modifications. Unlike other approaches, we demonstrate that our model can increase the number of trainable parameters without significantly increasing computational overhead or inference time. Experimental validation demonstrates that new restoration tasks can be introduced without compromising the performance of existing tasks. We also show that performance on new restoration tasks improves by adapting the knowledge from the knowledge base created by previous restoration tasks. The code is available at https://github.com/aupendu/continual-restore.

**Link**: [arxiv](http://arxiv.org/abs/2511.05421v1),  [pdf](http://arxiv.org/pdf/2511.05421v1)

**Tags**: cs.CV 



### LimiX: Unleashing Structured-Data Modeling Capability for Generalist   Intelligence
**Authors**: Xingxuan Zhang, Gang Ren, Han Yu, Hao Yuan, Hui Wang, Jiansheng Li, Jiayun Wu, Lang Mo, Li Mao, Mingchao Hao, Ningbo Dai, Renzhe Xu, Shuyang Li, Tianyang Zhang, Yue He, Yuanrui Wang, Yunjia Zhang, Zijing Xu, Dongzhe Li, Fang Gao, Hao Zou, Jiandong Liu, Jiashuo Liu, Jiawei Xu, Kaijie Cheng, Kehan Li, Linjun Zhou, Qing Li, Shaohua Fan, Xiaoyu Lin, Xinyan Han, Xuanyue Li, Yan Lu, Yuan Xue, Yuanyuan Jiang, Zimu Wang, Zhenlei Wang, Peng Cui

**Updated**: 2025-11-07T16:49:47Z

**Summary**: We argue that progress toward general intelligence requires complementary foundation models grounded in language, the physical world, and structured data. This report presents LimiX-16M and LimiX-2M, two instantiations of our large structured-data models (LDMs). Both models treat structured data as a joint distribution over variables and missingness, thus capable of addressing a wide range of tabular tasks through query-based conditional prediction via a single model. They are pretrained using masked joint-distribution modeling with an episodic, context-conditional objective, supporting rapid, training-free adaptation at inference. We evaluate LimiX models across 11 large structured-data benchmarks with broad regimes of sample size, feature dimensionality, class number, categorical-to-numerical feature ratio, missingness, and sample-to-feature ratios. LimiX-16M consistently surpasses strong baselines, as shown in Figure 1 and Figure 2. The superiority holds across a wide range of tasks, such as classification, regression, missing value imputation, and data generation, often by substantial margins, while avoiding task-specific architectures or bespoke training per task. Notably, LimiX-2M delivers strong results under tight compute and memory budgets. We also present the first scaling law study for LDMs, revealing how data and model scaling jointly influence downstream performance and offering quantitative guidance for tabular foundation modeling. All LimiX models are publicly accessible under Apache 2.0.

**Link**: [arxiv](http://arxiv.org/abs/2509.03505v2),  [pdf](http://arxiv.org/pdf/2509.03505v2)

**Tags**: cs.LG cs.AI cs.CL 



### Inference-Time Hyper-Scaling with KV Cache Compression
**Authors**: Adrian Łańcucki, Konrad Staniszewski, Piotr Nawrot, Edoardo M. Ponti

**Updated**: 2025-11-07T16:42:30Z

**Summary**: Inference-time scaling trades efficiency for increased reasoning accuracy by generating longer or more parallel sequences. However, in Transformer LLMs, generation cost is bottlenecked by the size of the key-value (KV) cache, rather than the number of generated tokens. Hence, we explore inference-time hyper-scaling: by compressing the KV cache, we can generate more tokens within the same compute budget and further improve the accuracy of scaled inference. The success of this approach, however, hinges on the ability of compression methods to preserve accuracy even at high compression ratios. To make hyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a novel method for sparsifying KV caches that only requires 1K training steps to achieve 8$\times$ compression, while maintaining better accuracy than training-free sparse attention. Instead of prematurely discarding cached tokens, DMS delays token eviction, implicitly merging representations and preserving critical information. We demonstrate the effectiveness of inference-time hyper-scaling with DMS on multiple families of LLMs, showing that it boosts accuracy for comparable inference latency and memory load. For instance, we enhance Qwen-R1 32B by 12.0 points on AIME 24, 8.6 on GPQA, and 9.7 on LiveCodeBench on average for an equivalent number of memory reads.

**Link**: [arxiv](http://arxiv.org/abs/2506.05345v2),  [pdf](http://arxiv.org/pdf/2506.05345v2)

**Tags**: cs.LG cs.CL 



### DiskMINT: Self-Consistent Thermochemical Disk Models with Radially   Varying Gas and Dust -- Application to the Massive, CO-Rich Disk of IM Lup
**Authors**: Dingshan Deng, Uma Gorti, Ilaria Pascucci, Maxime Ruaud

**Updated**: 2025-11-07T16:41:20Z

**Summary**: Disks around young stars are the birthplaces of planets, and the spatial distribution of their gas and dust masses is critical for understanding where and what types of planets can form. We present self-consistent thermochemical disk models built with DiskMINT, which extends its initial framework to allow for spatially decoupled gas and dust distributions. DiskMINT calculates the gas temperature based on thermal equilibrium with dust grains, solves vertical gas hydrostatic equilibrium, and includes key processes for the CO chemistry, specifically selective photodissociation, and freeze-out with conversion CO/CO$_2$ ice. We apply DiskMINT to study the IM Lup disk, a large massive disk, yet with an inferred CO depletion of up to 100 based on earlier thermochemical models. By fitting the multi-wavelength SED along with the millimeter continuum, ${\rm C^{18}O}$ radial emission profiles, we find $0.02-0.08\,{\rm M_\odot}$ for the gas disk mass, which are consistent with the dynamical-based mass within the uncertainties. We further compare the derived surface densities for dust and gas and find that the outer disk is drift-dominated, with a dust-to-gas mass ratio of approximately 0.01-0.02, which is likely insufficient to meet the conditions for the streaming instability to occur. Our results suggest that when interpreted with self-consistent thermochemical models, ${\rm C^{18}O}$ alone can serve as a reliable tracer of both the total gas mass and its radial distribution. This approach enables gas mass estimates in lower-mass disks, where dynamical constraints are not available, and in fainter systems where rare species like ${\rm N_2H^+}$ are too weak to detect.

**Link**: [arxiv](http://arxiv.org/abs/2509.15487v2),  [pdf](http://arxiv.org/pdf/2509.15487v2)

**Tags**: astro-ph.EP 



### A semiparametric generalized exponential regression model with a   principled distance-based prior
**Authors**: Arijit Dey, Arnab Hazra

**Updated**: 2025-11-07T16:40:17Z

**Summary**: The generalized exponential distribution is a well-known probability model in lifetime data analysis and several other research areas, including precipitation modeling. Despite having broad applications for independently and identically distributed observations, its uses as a generalized linear model for non-identically distributed data are limited. This paper introduces a semiparametric Bayesian generalized exponential (GE) regression model. Our proposed approach involves modeling the GE rate parameter within a generalized additive model framework. An important feature is the integration of a principled distance-based prior for the GE shape parameter; this allows the model to shrink to an exponential regression model that retains the advantages of the exponential family. We draw inferences using the Markov chain Monte Carlo algorithm and discuss some theoretical results pertaining to Bayesian asymptotics. Extensive simulations demonstrate that the proposed model outperforms simpler alternatives. The Western Ghats mountain range holds critical importance in regulating monsoon rainfall across Southern India, profoundly impacting regional agriculture. Here, we analyze daily wet-day rainfall data for the monsoon months between 1901--2022 for the Northern, Middle, and Southern Western Ghats regions. Applying the proposed model to analyze the rainfall data over 122 years provides insights into model parameters, short-term temporal patterns, and the impact of climate change. We observe a significant decreasing trend in wet-day rainfall for the Southern Western Ghats region.

**Link**: [arxiv](http://arxiv.org/abs/2309.03165v2),  [pdf](http://arxiv.org/pdf/2309.03165v2)

**Tags**: stat.AP 62P12, 62F15, 62G08, 62J12 



### GUARD: Role-playing to Generate Natural-language Jailbreakings to Test   Guideline Adherence of Large Language Models
**Authors**: Haibo Jin, Ruoxi Chen, Peiyan Zhang, Andy Zhou, Haohan Wang

**Updated**: 2025-11-07T16:36:35Z

**Summary**: The discovery of "jailbreaks" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effective in inducing LLMs to generate unethical or guideline-violating responses. In addition, we also pioneer a setting in our system that will automatically follow the government-issued guidelines to generate jailbreaks to test whether LLMs follow the guidelines accordingly. We refer to our system as GUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have empirically validated the effectiveness of GUARD on three cutting-edge open-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a widely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the realm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing GUARD's versatility and contributing valuable insights for the development of safer, more reliable LLM-based applications across diverse modalities.

**Link**: [arxiv](http://arxiv.org/abs/2402.03299v6),  [pdf](http://arxiv.org/pdf/2402.03299v6)

**Tags**: cs.LG cs.CL cs.CV 



### Steering Language Models with Weight Arithmetic
**Authors**: Constanza Fierro, Fabien Roger

**Updated**: 2025-11-07T16:34:16Z

**Summary**: Providing high-quality feedback to Large Language Models (LLMs) on a diverse training distribution can be difficult and expensive, and providing feedback only on a narrow distribution can result in unintended generalizations. To better leverage narrow training data, we propose contrastive weight steering, a simple post-training method that edits the model parameters using weight arithmetic. We isolate a behavior direction in weight-space by subtracting the weight deltas from two small fine-tunes -- one that induces the desired behavior and another that induces its opposite -- and then add or remove this direction to modify the model's weights. We apply this technique to mitigate sycophancy and induce misalignment, and find that weight steering often generalizes further than activation steering, achieving stronger out-of-distribution behavioral control before degrading general capabilities. We also show that, in the context of task-specific fine-tuning, weight steering can partially mitigate undesired behavioral drift: it can reduce sycophancy and under-refusals introduced during fine-tuning while preserving task performance gains. Finally, we provide preliminary evidence that emergent misalignment can be detected by measuring the similarity between fine-tuning updates and an "evil" weight direction, suggesting that it may be possible to monitor the evolution of weights during training and detect rare misaligned behaviors that never manifest during training or evaluations.

**Link**: [arxiv](http://arxiv.org/abs/2511.05408v1),  [pdf](http://arxiv.org/pdf/2511.05408v1)

**Tags**: cs.CL cs.LG 



### Large Language Models for Explainable Threat Intelligence
**Authors**: Tiago Dinis, Miguel Correia, Roger Tavares

**Updated**: 2025-11-07T16:32:41Z

**Summary**: As cyber threats continue to grow in complexity, traditional security mechanisms struggle to keep up. Large language models (LLMs) offer significant potential in cybersecurity due to their advanced capabilities in text processing and generation. This paper explores the use of LLMs with retrieval-augmented generation (RAG) to obtain threat intelligence by combining real-time information retrieval with domain-specific data. The proposed system, RAGRecon, uses a LLM with RAG to answer questions about cybersecurity threats. Moreover, it makes this form of Artificial Intelligence (AI) explainable by generating and visually presenting to the user a knowledge graph for every reply. This increases the transparency and interpretability of the reasoning of the model, allowing analysts to better understand the connections made by the system based on the context recovered by the RAG system. We evaluated RAGRecon experimentally with two datasets and seven different LLMs and the responses matched the reference responses more than 91% of the time for the best combinations.

**Link**: [arxiv](http://arxiv.org/abs/2511.05406v1),  [pdf](http://arxiv.org/pdf/2511.05406v1)

**Tags**: cs.CL 



### Are Humans as Brittle as Large Language Models?
**Authors**: Jiahui Li, Sean Papay, Roman Klinger

**Updated**: 2025-11-07T16:21:31Z

**Summary**: The output of large language models (LLMs) is unstable, due both to non-determinism of the decoding process as well as to prompt brittleness. While the intrinsic non-determinism of LLM generation may mimic existing uncertainty in human annotations through distributional shifts in outputs, it is largely assumed, yet unexplored, that the prompt brittleness effect is unique to LLMs. This raises the question: do human annotators show similar sensitivity to prompt changes? If so, should prompt brittleness in LLMs be considered problematic? One may alternatively hypothesize that prompt brittleness correctly reflects human annotation variances. To fill this research gap, we systematically compare the effects of prompt modifications on LLMs and identical instruction modifications for human annotators, focusing on the question of whether humans are similarly sensitive to prompt perturbations. To study this, we prompt both humans and LLMs for a set of text classification tasks conditioned on prompt variations. Our findings indicate that both humans and LLMs exhibit increased brittleness in response to specific types of prompt modifications, particularly those involving the substitution of alternative label sets or label formats. However, the distribution of human judgments is less affected by typographical errors and reversed label order than that of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2509.07869v2),  [pdf](http://arxiv.org/pdf/2509.07869v2)

**Tags**: cs.CL cs.HC 



### XBreaking: Understanding how LLMs security alignment can be broken
**Authors**: Marco Arazzi, Vignesh Kumar Kembu, Antonino Nocera, Vinod P

**Updated**: 2025-11-07T16:21:06Z

**Summary**: Large Language Models are fundamental actors in the modern IT landscape dominated by AI solutions. However, security threats associated with them might prevent their reliable adoption in critical application scenarios such as government organizations and medical institutions. For this reason, commercial LLMs typically undergo a sophisticated censoring mechanism to eliminate any harmful output they could possibly produce. These mechanisms maintain the integrity of LLM alignment by guaranteeing that the models respond safely and ethically. In response to this, attacks on LLMs are a significant threat to such protections, and many previous approaches have already demonstrated their effectiveness across diverse domains. Existing LLM attacks mostly adopt a generate-and-test strategy to craft malicious input. To improve the comprehension of censoring mechanisms and design a targeted attack, we propose an Explainable-AI solution that comparatively analyzes the behavior of censored and uncensored models to derive unique exploitable alignment patterns. Then, we propose XBreaking, a novel approach that exploits these unique patterns to break the security and alignment constraints of LLMs by targeted noise injection. Our thorough experimental campaign returns important insights about the censoring mechanisms and demonstrates the effectiveness and performance of our approach.

**Link**: [arxiv](http://arxiv.org/abs/2504.21700v3),  [pdf](http://arxiv.org/pdf/2504.21700v3)

**Tags**: cs.CR cs.AI cs.LG 



### Block-structured Operator Inference for coupled multiphysics model   reduction
**Authors**: Benjamin G. Zastrow, Anirban Chaudhuri, Karen E. Willcox, Anthony Ashley, Michael Chamberlain Henson

**Updated**: 2025-11-07T16:11:18Z

**Summary**: This paper presents a block-structured formulation of Operator Inference as a way to learn structured reduced-order models for multiphysics systems. The approach specifies the governing equation structure for each physics component and the structure of the coupling terms. Once the multiphysics structure is specified, the reduced-order model is learned from snapshot data following the nonintrusive Operator Inference methodology. In addition to preserving physical system structure, which in turn permits preservation of system properties such as stability and second-order structure, the block-structured approach has the advantages of reducing the overall dimensionality of the learning problem and admitting tailored regularization for each physics component. The numerical advantages of the block-structured formulation over a monolithic Operator Inference formulation are demonstrated for aeroelastic analysis, which couples aerodynamic and structural models. For the benchmark test case of the AGARD 445.6 wing, block-structured Operator Inference provides an average 20% online prediction speedup over monolithic Operator Inference across subsonic and supersonic flow conditions in both the stable and fluttering parameter regimes while preserving the accuracy achieved with monolithic Operator Inference.

**Link**: [arxiv](http://arxiv.org/abs/2511.05389v1),  [pdf](http://arxiv.org/pdf/2511.05389v1)

**Tags**: cs.CE cs.NA math.NA 



### Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via   Self-Play and Reinforcement Learning
**Authors**: Richard Dewey, Janos Botyanszki, Ciamac C. Moallemi, Andrew T. Zheng

**Updated**: 2025-11-07T16:11:08Z

**Summary**: AI researchers have long focused on poker-like games as a testbed for environments characterized by multi-player dynamics, imperfect information, and reasoning under uncertainty. While recent breakthroughs have matched elite human play at no-limit Texas hold'em, the multi-player dynamics are subdued: most hands converge quickly with only two players engaged through multiple rounds of bidding. In this paper, we present Solly, the first AI agent to achieve elite human play in reduced-format Liar's Poker, a game characterized by extensive multi-player engagement. We trained Solly using self-play with a model-free, actor-critic, deep reinforcement learning algorithm. Solly played at an elite human level as measured by win rate (won over 50% of hands) and equity (money won) in heads-up and multi-player Liar's Poker. Solly also outperformed large language models (LLMs), including those with reasoning abilities, on the same metrics. Solly developed novel bidding strategies, randomized play effectively, and was not easily exploitable by world-class human players.

**Link**: [arxiv](http://arxiv.org/abs/2511.03724v2),  [pdf](http://arxiv.org/pdf/2511.03724v2)

**Tags**: cs.AI cs.MA 



### TeaRAG: A Token-Efficient Agentic Retrieval-Augmented Generation   Framework
**Authors**: Chao Zhang, Yuhao Wang, Derong Xu, Haoxin Zhang, Yuanjie Lyu, Yuhao Chen, Shuochen Liu, Tong Xu, Xiangyu Zhao, Yan Gao, Yao Hu, Enhong Chen

**Updated**: 2025-11-07T16:08:34Z

**Summary**: Retrieval-Augmented Generation (RAG) utilizes external knowledge to augment Large Language Models' (LLMs) reliability. For flexibility, agentic RAG employs autonomous, multi-round retrieval and reasoning to resolve queries. Although recent agentic RAG has improved via reinforcement learning, they often incur substantial token overhead from search and reasoning processes. This trade-off prioritizes accuracy over efficiency. To address this issue, this work proposes TeaRAG, a token-efficient agentic RAG framework capable of compressing both retrieval content and reasoning steps. 1) First, the retrieved content is compressed by augmenting chunk-based semantic retrieval with a graph retrieval using concise triplets. A knowledge association graph is then built from semantic similarity and co-occurrence. Finally, Personalized PageRank is leveraged to highlight key knowledge within this graph, reducing the number of tokens per retrieval. 2) Besides, to reduce reasoning steps, Iterative Process-aware Direct Preference Optimization (IP-DPO) is proposed. Specifically, our reward function evaluates the knowledge sufficiency by a knowledge matching mechanism, while penalizing excessive reasoning steps. This design can produce high-quality preference-pair datasets, supporting iterative DPO to improve reasoning conciseness. Across six datasets, TeaRAG improves the average Exact Match by 4% and 2% while reducing output tokens by 61% and 59% on Llama3-8B-Instruct and Qwen2.5-14B-Instruct, respectively. Code is available at https://github.com/Applied-Machine-Learning-Lab/TeaRAG.

**Link**: [arxiv](http://arxiv.org/abs/2511.05385v1),  [pdf](http://arxiv.org/pdf/2511.05385v1)

**Tags**: cs.IR cs.AI 



### Connectomics Informed by Large Language Models
**Authors**: Elinor Thompson, Tiantian He, Anna Schroder, Ahmed Abdulaal, Alec Sargood, Sonja Soskic, Henry F. J. Tregidgo, Daniel C. Alexander

**Updated**: 2025-11-07T16:05:21Z

**Summary**: Tractography is a unique method for mapping white matter connections in the brain, but tractography algorithms suffer from an inherent trade-off between sensitivity and specificity that limits accuracy. Incorporating prior knowledge of white matter anatomy is an effective strategy for improving accuracy and has been successful for reducing false positives and false negatives in bundle-mapping protocols. However, it is challenging to scale this approach for connectomics due to the difficulty in synthesising information relating to many thousands of possible connections. In this work, we develop and evaluate a pipeline using large language models (LLMs) to generate quantitative priors for connectomics, based on their knowledge of neuroanatomy. We benchmark our approach against an evaluation set derived from a gold-standard tractography atlas, identifying prompting techniques to elicit accurate connectivity information from the LLMs. We further identify strategies for incorporating external knowledge sources into the pipeline, which can provide grounding for the LLM and improve accuracy. Finally, we demonstrate how the LLM-derived priors can augment existing tractography filtering approaches by identifying true-positive connections to retain during the filtering process. We show that these additional connections can improve the accuracy of a connectome-based model of pathology spread, which provides supporting evidence that the connections preserved by the LLM are valid.

**Link**: [arxiv](http://arxiv.org/abs/2511.05383v1),  [pdf](http://arxiv.org/pdf/2511.05383v1)

**Tags**: cs.CE 



### A scaling relationship for non-thermal radio emission from ordered   magnetospheres - II. Investigating the efficiency of relativistic electron   production in magnetospheres of BA-type stars
**Authors**: P. Leto, S. Owocki, C. Trigilio, F. Cavallaro, B. Das, M. E. Shultz, C. S. Buemi, G. Umana, L. Fossati, R. Ignace, J. Krticka, L. M. Oskinova, I. Pillitteri, C. Bordiu, F. Bufano, L. Cerrigone, A. Ingallinera, S. Loru, S. Riggi, A. C. Ruggeri, A. ud-Doula, F. Leone

**Updated**: 2025-11-07T16:01:38Z

**Summary**: Magnetic BA stars host dipole-like magnetospheres. When detected as radio sources, their luminosities correlate with the magnetic field and rotation. Rotation is crucial because the mechanism undergirding the relativistic electron production is powered by centrifugal breakouts. CBOs occur wherever magnetic tension does not balance centrifugal force; the resulting magnetic reconnection provides particle acceleration. To investigate how physical conditions at the site of the CBOs affect the efficiency of the acceleration mechanism, we broadly explore the parameter space governing radio emission by increasing the sample of radio-loud magnetic stars. High-sensitivity VLA observations of 32 stars were performed in the hope of identifying new centrifugal magnetospheres and associated CBOs. We calculated gyro-synchrotron spectra using 3D modeling of a dipole-shaped magnetosphere. We evaluated combinations of parameters. The number of relativistic electrons was constrained by the need to produce the emission level predicted by the scaling relationship for the radio emission from magnetic BA stars. About half of the observed stars were detected, with luminosities in agreement with the expected values, reinforcing the robust nature of the scaling relationship for CBO-powered radio emission. Comparing the competing centrifugal and magnetic effects on plasma locked in a rigidly rotating magnetosphere, we located the site of CBOs and inferred the local plasma density. We then estimated the efficiency of the acceleration mechanism needed to produce enough non-thermal electrons to support the radio emission level. Given a constant acceleration efficiency, relativistic electrons represent a fixed fraction of the local thermal plasma. Thus, dense magnetospheres host more energetic particles than less dense ones; consequently, with other parameters similar, they are intrinsically brighter radio sources.

**Link**: [arxiv](http://arxiv.org/abs/2511.05378v1),  [pdf](http://arxiv.org/pdf/2511.05378v1)

**Tags**: astro-ph.SR 



### Communication-Efficient Collaborative LLM Inference via Distributed   Speculative Decoding
**Authors**: Ce Zheng, Tingting Yang

**Updated**: 2025-11-07T15:58:37Z

**Summary**: Speculative decoding is an emerging technique that accelerates large language model (LLM) inference by allowing a smaller draft model to predict multiple tokens in advance, which are then verified or corrected by a larger target model. In AI-native radio access networks (AI-RAN), this paradigm is well-suited for collaborative inference between resource-constrained end devices and more capable edge servers or base stations (BSs). However, existing distributed speculative decoding requires transmitting the full vocabulary probability distribution from the draft model on the device to the target model at the BS, which leads to prohibitive uplink communication overhead. To address this issue, we propose a ``Top-K Sparse Logits Transmission (TK-SLT)`` scheme, where the draft model transmits only the top-K token raw probabilities and the corresponding token indices instead of the entire distribution. This approach significantly reduces bandwidth consumption while maintaining inference performance. We further derive an analytical expression for the optimal draft length that maximizes inference throughput, and provide a theoretical analysis of the achievable speedup ratio under TK-SLT. Experimental results validate both the efficiency and effectiveness of the proposed method.

**Link**: [arxiv](http://arxiv.org/abs/2509.04576v2),  [pdf](http://arxiv.org/pdf/2509.04576v2)

**Tags**: eess.SP 



### Near-Efficient and Non-Asymptotic Multiway Inference
**Authors**: Oscar López, Arvind Prasadan, Carlos Llosa-Vite, Richard B. Lehoucq, Daniel M. Dunlavy

**Updated**: 2025-11-07T15:54:31Z

**Summary**: We establish non-asymptotic efficiency guarantees for tensor decomposition-based inference in count data models. Under a Poisson framework, we consider two related goals: (i) parametric inference, the estimation of the full distributional parameter tensor, and (ii) multiway analysis, the recovery of its canonical polyadic (CP) decomposition factors. Our main result shows that in the rank-one setting, a rank-constrained maximum-likelihood estimator achieves multiway analysis with variance matching the Cram\'{e}r-Rao Lower Bound (CRLB) up to absolute constants and logarithmic factors. This provides a general framework for studying "near-efficient" multiway estimators in finite-sample settings. For higher ranks, we illustrate that our multiway estimator may not attain the CRLB; nevertheless, CP-based parametric inference remains nearly minimax optimal, with error bounds that improve on prior work by offering more favorable dependence on the CP rank. Numerical experiments corroborate near-efficiency in the rank-one case and highlight the efficiency gap in higher-rank scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2511.05368v1),  [pdf](http://arxiv.org/pdf/2511.05368v1)

**Tags**: math.ST cs.NA math.NA stat.ML stat.TH 62F99 (Primary) 15A69 (Secondary) 



### Linking Warm Dark Matter to Merger Tree Histories via Deep Learning   Networks
**Authors**: Ilem Leisher, Paul Torrey, Alex M. Garcia, Jonah C. Rose, Francisco Villaescusa-Navarro, Zachary Lubberts, Arya Farahi, Stephanie O'Neil, Xuejian Shen, Olivia Mostow, Nitya Kallivayalil, Dhruv Zimmerman, Desika Narayanan, Mark Vogelsberger

**Updated**: 2025-11-07T15:53:55Z

**Summary**: Dark matter (DM) halos form hierarchically in the Universe through a series of merger events. Cosmological simulations can represent this series of mergers as a graph-like ``tree'' structure. Previous work has shown these merger trees are sensitive to cosmology simulation parameters, but as DM structures, the outstanding question of their sensitivity to DM models remains unanswered. In this work, we investigate the feasibility of deep learning methods trained on merger trees to infer Warm Dark Matter (WDM) particles masses from the DREAMS simulation suite. We organize the merger trees from 1,024 zoom-in simulations into graphs with nodes representing the merger history of galaxies and edges denoting hereditary links. We vary the complexity of the node features included in the graphs ranging from a single node feature up through an array of several galactic properties (e.g., halo mass, star formation rate, etc.). We train a Graph Neural Network (GNN) to predict the WDM mass using the graph representation of the merger tree as input. We find that the GNN can predict the mass of the WDM particle ($R^2$ from 0.07 to 0.95), with success depending on the graph complexity and node features. We extend the same methods to supernovae and active galactic nuclei feedback parameters $A_\text{SN1}$, $A_\text{SN2}$, and $A_\text{AGN}$, successfully inferring the supernovae parameters. The GNN can even infer the WDM mass from merger tree histories without any node features, indicating that the structure of merger trees alone inherits information about the cosmological parameters of the simulations from which they form.

**Link**: [arxiv](http://arxiv.org/abs/2511.05367v1),  [pdf](http://arxiv.org/pdf/2511.05367v1)

**Tags**: astro-ph.GA 



### MorphTok: Morphologically Grounded Tokenization for Indian Languages
**Authors**: Maharaj Brahma, N J Karthika, Atul Singh, Devaraj Adiga, Smruti Bhate, Ganesh Ramakrishnan, Rohit Saluja, Maunendra Sankar Desarkar

**Updated**: 2025-11-07T15:53:49Z

**Summary**: Tokenization is a crucial step in NLP, especially with the rise of large language models (LLMs), impacting downstream performance, computational cost, and efficiency. Existing LLMs rely on the classical Byte-pair Encoding (BPE) algorithm for subword tokenization that greedily merges frequent character bigrams, often leading to segmentation that does not align with linguistically meaningful units. To address this, we propose morphology-aware segmentation as a pre-tokenization step before applying BPE. To facilitate morphology-aware segmentation, we create a novel dataset for Hindi and Marathi, incorporating sandhi splitting to enhance the subword tokenization. Experiments on downstream tasks show that morphologically grounded tokenization improves machine translation and language modeling performance. Additionally, to handle the dependent vowels common in syllable-based writing systems used by Indic languages, we propose Constrained BPE (CBPE), an extension to the standard BPE algorithm incorporating script-specific constraints. In particular, CBPE handles dependent vowels to form a cohesive unit with other characters instead of occurring as a single unit. Our results show that CBPE achieves a 1.68\% reduction in fertility scores while maintaining comparable or improved downstream performance in machine translation and language modeling, offering a computationally efficient alternative to standard BPE. Moreover, to evaluate segmentation across different tokenization algorithms, we introduce a new human evaluation metric, \textit{EvalTok}, enabling more human-grounded assessment.

**Link**: [arxiv](http://arxiv.org/abs/2504.10335v2),  [pdf](http://arxiv.org/pdf/2504.10335v2)

**Tags**: cs.CL 



### Coarse-graining nonequilibrium diffusions with Markov chains
**Authors**: Ramón Nartallo-Kaluarachchi, Renaud Lambiotte, Alain Goriely

**Updated**: 2025-11-07T15:53:10Z

**Summary**: We investigate nonequilibrium steady-state dynamics in both continuous- and discrete-state stochastic processes. Our analysis focuses on planar diffusion dynamics and their coarse-grained approximations by discrete-state Markov chains. Using finite-volume approximations, we derive an approximate master equation directly from the underlying diffusion and show that this discretisation preserves key features of the nonequilibrium steady-state. In particular, we show that the entropy production rate of the approximation converges as the number of discrete states goes to the limit. These results are illustrated with analytically solvable diffusions and numerical experiments on nonlinear processes, demonstrating how this approach can be used to explore the dependence of entropy production rate on model parameters. Finally, we address the problem of inferring discrete-state Markov models from continuous stochastic trajectories. We show that discrete-state models significantly underestimate the true entropy production rate. However, we also show that they can provide tests to determine if a stationary planar diffusion is out of equilibrium. This property is illustrated with both simulated data and empirical trajectories from schooling fish.

**Link**: [arxiv](http://arxiv.org/abs/2511.05366v1),  [pdf](http://arxiv.org/pdf/2511.05366v1)

**Tags**: cond-mat.stat-mech math.DS physics.bio-ph 



### Mapping the positions of Two-Level-Systems on the surface of a   superconducting transmon qubit
**Authors**: Jürgen Lisenfeld, Alexander K. Händel, Etienne Daum, Benedikt Berlitz, Alexander Bilmes, Alexey V. Ustinov

**Updated**: 2025-11-07T15:52:53Z

**Summary**: The coherence of superconducting quantum computers is severely limited by material defects that create parasitic two-level-systems (TLS). Progress is complicated by lacking understanding how TLS are created and in which parts of a qubit circuit they are most detrimental. Here, we present a method to determine the individual positions of TLS at the surface of a transmon qubit. We employ a set of on-chip gate electrodes near the qubit to generate local DC electric fields that are used to tune the TLS' resonance frequencies. The TLS position is inferred from the strengths at which TLS couple to different electrodes and comparing them to electric field simulations. We found that the majority of detectable surface-TLS was residing on the leads of the qubit's Josephson junction, despite the dominant contribution of its coplanar capacitor to electric field energy and surface area. This indicates that the TLS density is significantly enhanced near shadow-evaporated electrodes fabricated by lift-off techniques. Our method is useful to identify critical circuit regions where TLS contribute most to decoherence, and can guide improvements in qubit design and fabrication methods.

**Link**: [arxiv](http://arxiv.org/abs/2511.05365v1),  [pdf](http://arxiv.org/pdf/2511.05365v1)

**Tags**: quant-ph cond-mat.supr-con 



### A Latent-Variable Formulation of the Poisson Canonical Polyadic Tensor   Model: Maximum Likelihood Estimation and Fisher Information
**Authors**: Carlos Llosa-Vite, Daniel M. Dunlavy, Richard B. Lehoucq, Oscar López, Arvind Prasadan

**Updated**: 2025-11-07T15:45:13Z

**Summary**: We establish parameter inference for the Poisson canonical polyadic (PCP) tensor model through a latent-variable formulation. Our approach exploits the observation that any random PCP tensor can be derived by marginalizing an unobservable random tensor of one dimension larger. The loglikelihood of this larger dimensional tensor, referred to as the "complete" loglikelihood, is comprised of multiple rank one PCP loglikelihoods. Using this methodology, we first derive non-iterative maximum likelihood estimators for the PCP model and demonstrate that several existing algorithms for fitting non-negative matrix and tensor factorizations are Expectation-Maximization algorithms. Next, we derive the observed and expected Fisher information matrices for the PCP model. The Fisher information provides us crucial insights into the well-posedness of the tensor model, such as the role that tensor rank plays in identifiability and indeterminacy. For the special case of rank one PCP models, we demonstrate that these results are greatly simplified.

**Link**: [arxiv](http://arxiv.org/abs/2511.05352v1),  [pdf](http://arxiv.org/pdf/2511.05352v1)

**Tags**: math.ST cs.NA math.NA stat.ML stat.TH 62F99 (Primary) 15A69 (Secondary) 



### Efficient CNN Inference on Ultra-Low-Power MCUs via Saturation-Aware   Convolution
**Authors**: Shiming Li, Luca Mottola, Yuan Yao, Stefanos Kaxiras

**Updated**: 2025-11-07T15:41:27Z

**Summary**: Deploying lightweight CNN inference tasks on ultra-low-power MCUs is often not limited by space constraint, thanks to the compact size of models, yet inference latency is crucial for preserving energy. We reveal that quantized CNN inference on ultra-low-power MCUs executes unnecessary computations in neurons that produce saturated output values: often times, these neurons still produce the correct output value without fully completing the computation, since the neuron value is too extreme and is eventually systematically clamped at the boundaries allowed by the neuron. We show that with carefully designed condition checks, it is possible to identify and skip these unnecessary computations without impacting the neuron output. Based on this, we present saturation-aware convolution: an inference technique whereby computations in convolution kernels are executed in an altered order to induce earlier saturation, and saturation checks are inserted to omit unnecessary computations. We integrate our implementation into MCUNet's TinyEngine, the state-of-the-art neural network code generation and inference framework, and conduct experiments on a Cortex-M0+ MCU. The result based on 7 open-source CNN models displays up to 24% inference time saving, with strictly zero impact on neural network accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2511.05347v1),  [pdf](http://arxiv.org/pdf/2511.05347v1)

**Tags**: eess.SY cs.SY 



### What Matters in Data for DPO?
**Authors**: Yu Pan, Zhongze Cai, Guanting Chen, Huaiyang Zhong, Chonghuan Wang

**Updated**: 2025-11-07T15:28:43Z

**Summary**: Direct Preference Optimization (DPO) has emerged as a simple and effective approach for aligning large language models (LLMs) with human preferences, bypassing the need for a learned reward model. Despite its growing adoption, a fundamental question remains open: what characteristics of preference data are most critical for DPO performance? In this work, we provide a systematic study of how preference data distribution influences DPO, from both theoretical and empirical perspectives. We show that the quality of chosen responses plays a dominant role in optimizing the DPO objective, while the quality of rejected responses may have relatively limited impact. Our theoretical analysis characterizes the optimal response distribution under DPO and reveals how contrastiveness between responses helps primarily by improving the chosen samples. We further study an online DPO setting and show it effectively reduces to supervised fine-tuning on the chosen responses. Extensive experiments across diverse tasks confirm our findings: improving the quality of chosen responses consistently boosts performance regardless of the quality of the rejected responses. We also investigate the benefit of mixing the on-policy data. Our results interpret the mechanism behind some widely adopted strategies and offer practical insights for constructing high-impact preference datasets for LLM alignment.

**Link**: [arxiv](http://arxiv.org/abs/2508.18312v3),  [pdf](http://arxiv.org/pdf/2508.18312v3)

**Tags**: cs.LG cs.AI 



### EMPEROR I. Exoplanet MCMC parallel tempering for RV orbit retrieval
**Authors**: Pablo A. Peña R., James S. Jenkins

**Updated**: 2025-11-07T15:28:35Z

**Summary**: We present \texttt{EMPEROR}, an open-source Python framework designed for efficient exoplanet detection and characterisation with radial velocities (RV). \texttt{EMPEROR} integrates Dynamic Nested Sampling (DNS) and Adaptive Parallel Tempering (APT) Markov Chain Monte Carlo (MCMC), supporting multiple noise models such as Gaussian Processes (GPs) and Moving Averages (MA). The framework enables systematic model comparison using statistical metrics, including Bayesian evidence ($\ln{\mathcal{Z}}$) and Bayesian Information Criterion (BIC), while providing automated, publish-ready visualisations. \texttt{EMPEROR} is evaluated across three distinct systems to assess its capabilities in different detection scenarios. Sampling performance, model selection, and the search for Earth-mass planets are evaluated in data for 51 Pegasi, HD 55693 and Barnard's Star (GJ 699). For 51 Pegasi, APT achieves an effective sampling increase over DNS by a factor 3.76, while retrieving tighter parameter estimates. For HD 55693 the stellar rotation $P_{\text{rot}}=29.72^{+0.01}_{-0.02}$ and magnetic cycle $P_{\text{mag}}=2557.0^{+70.1}_{-36.7}$ are recovered, while demonstrating the sensitivity of $\ln{\mathcal{Z}}$ to prior selection. For Barnard's star, several noise models are compared, and the confirmed planet parameters are successfully retrieved with all of them. The best model shows a period of 3.1536$\pm$0.0003~d, minimum mass of 0.38$\pm$0.03 M$_{\rm{\oplus}}$, and semi-major axis of 0.02315$\pm$0.00039~AU. Purely statistical inference might be insufficient on its own for robust exoplanet detection. Effective methodologies must integrate domain knowledge, heuristic criteria, and multi-faceted model comparisons. The versatility of \texttt{EMPEROR} in handling diverse noise structures, its systematic model selection, and its improved performance make it a valuable tool for RV exoplanetary studies.

**Link**: [arxiv](http://arxiv.org/abs/2511.05331v1),  [pdf](http://arxiv.org/pdf/2511.05331v1)

**Tags**: astro-ph.EP astro-ph.IM 



### Colored Gaussian directed acyclic graphical models
**Authors**: Tobias Boege, Kaie Kubjas, Pratik Misra, Liam Solus

**Updated**: 2025-11-07T15:23:53Z

**Summary**: We study submodels of Gaussian DAG models defined by partial homogeneity constraints imposed on the model error variances and structural coefficients. We represent these models with colored DAGs and investigate their properties for use in statistical and causal inference. Local and global Markov properties are provided and shown to characterize the colored DAG model. Additional properties relevant to causal discovery are studied, including the existence and non-existence of faithful distributions and structural identifiability. Extending prior work of Peters and B\"uhlmann and Wu and Drton, we prove structural identifiability under the assumption of homogeneous structural coefficients, as well as for a family of models with partially homogeneous structural coefficients. The latter models, termed BPEC-DAGs, capture additional causal insights by clustering the direct causes of each node into communities according to their effect on their common target. An analogue of the GES algorithm for learning BPEC-DAGs is given and evaluated on real and synthetic data. Regarding model geometry, we provide a proof of a conjecture of Sullivant which generalizes to colored DAG models, colored undirected graphical models and directed ancestral graph models. The proof yields a tool for identification of Markov properties for any rationally parameterized model with globally, rationally identifiable parameters.

**Link**: [arxiv](http://arxiv.org/abs/2404.04024v3),  [pdf](http://arxiv.org/pdf/2404.04024v3)

**Tags**: math.ST stat.TH 62H22 (primary) 62R01, 62D20, 13C70, 13P25 (secondary) 



### MultiVic: A Time-Predictable RISC-V Multi-Core Processor Optimized for   Neural Network Inference
**Authors**: Maximilian Kirschner, Konstantin Dudzik, Ben Krusekamp, Jürgen Becker

**Updated**: 2025-11-07T15:19:14Z

**Summary**: Real-time systems, particularly those used in domains like automated driving, are increasingly adopting neural networks. From this trend arises the need for high-performance hardware exhibiting predictable timing behavior. While state-of-the-art real-time hardware often suffers from limited memory and compute resources, modern AI accelerators typically lack the crucial predictability due to memory interference.   We present a new hardware architecture to bridge this gap between performance and predictability. The architecture features a multi-core vector processor with predictable cores, each equipped with local scratchpad memories. A central management core orchestrates access to shared external memory following a statically determined schedule.   To evaluate the proposed hardware architecture, we analyze different variants of our parameterized design. We compare these variants to a baseline architecture consisting of a single-core vector processor with large vector registers. We find that configurations with a larger number of smaller cores achieve better performance due to increased effective memory bandwidth and higher clock frequencies. Crucially for real-time systems, execution time fluctuation remains very low, demonstrating the platform's time predictability.

**Link**: [arxiv](http://arxiv.org/abs/2511.05321v1),  [pdf](http://arxiv.org/pdf/2511.05321v1)

**Tags**: cs.AR 



### What Are the Facts? Automated Extraction of Court-Established Facts from   Criminal-Court Opinions
**Authors**: Klára Bendová, Tomáš Knap, Jan Černý, Vojtěch Pour, Jaromir Savelka, Ivana Kvapilíková, Jakub Drápal

**Updated**: 2025-11-07T15:17:45Z

**Summary**: Criminal justice administrative data contain only a limited amount of information about the committed offense. However, there is an unused source of extensive information in continental European courts' decisions: descriptions of criminal behaviors in verdicts by which offenders are found guilty. In this paper, we study the feasibility of extracting these descriptions from publicly available court decisions from Slovakia. We use two different approaches for retrieval: regular expressions and large language models (LLMs). Our baseline was a simple method employing regular expressions to identify typical words occurring before and after the description. The advanced regular expression approach further focused on "sparing" and its normalization (insertion of spaces between individual letters), typical for delineating the description. The LLM approach involved prompting the Gemini Flash 2.0 model to extract the descriptions using predefined instructions. Although the baseline identified descriptions in only 40.5% of verdicts, both methods significantly outperformed it, achieving 97% with advanced regular expressions and 98.75% with LLMs, and 99.5% when combined. Evaluation by law students showed that both advanced methods matched human annotations in about 90% of cases, compared to just 34.5% for the baseline. LLMs fully matched human-labeled descriptions in 91.75% of instances, and a combination of advanced regular expressions with LLMs reached 92%.

**Link**: [arxiv](http://arxiv.org/abs/2511.05320v1),  [pdf](http://arxiv.org/pdf/2511.05320v1)

**Tags**: cs.CL cs.AI 



### $\mathbf{S^2LM}$: Towards Semantic Steganography via Large Language   Models
**Authors**: Huanqi Wu, Huangbiao Xu, Runfeng Xie, Jiaxin Cai, Kaixin Zhang, Xiao Ke

**Updated**: 2025-11-07T15:17:40Z

**Summary**: Although steganography has made significant advancements in recent years, it still struggles to embed semantically rich, sentence-level information into carriers. However, in the era of AIGC, the capacity of steganography is more critical than ever. In this work, we present Sentence-to-Image Steganography, an instance of Semantic Steganography, a novel task that enables the hiding of arbitrary sentence-level messages within a cover image. Furthermore, we establish a benchmark named Invisible Text (IVT), comprising a diverse set of sentence-level texts as secret messages for evaluation. Finally, we present $\mathbf{S^2LM}$: Semantic Steganographic Language Model, which utilizes large language models (LLMs) to embed high-level textual information, such as sentences or even paragraphs, into images. Unlike traditional bit-level counterparts, $\mathrm{S^2LM}$ enables the integration of semantically rich content through a newly designed pipeline in which the LLM is involved throughout the entire process. Both quantitative and qualitative experiments demonstrate that our method effectively unlocks new semantic steganographic capabilities for LLMs. The source code will be released soon.

**Link**: [arxiv](http://arxiv.org/abs/2511.05319v1),  [pdf](http://arxiv.org/pdf/2511.05319v1)

**Tags**: cs.CV cs.CR 



### Cleaning Maintenance Logs with LLM Agents for Improved Predictive   Maintenance
**Authors**: Valeriu Dimidov, Faisal Hawlader, Sasan Jafarnejad, Raphaël Frank

**Updated**: 2025-11-07T15:12:49Z

**Summary**: Economic constraints, limited availability of datasets for reproducibility and shortages of specialized expertise have long been recognized as key challenges to the adoption and advancement of predictive maintenance (PdM) in the automotive sector. Recent progress in large language models (LLMs) presents an opportunity to overcome these barriers and speed up the transition of PdM from research to industrial practice. Under these conditions, we explore the potential of LLM-based agents to support PdM cleaning pipelines. Specifically, we focus on maintenance logs, a critical data source for training well-performing machine learning (ML) models, but one often affected by errors such as typos, missing fields, near-duplicate entries, and incorrect dates. We evaluate LLM agents on cleaning tasks involving six distinct types of noise. Our findings show that LLMs are effective at handling generic cleaning tasks and offer a promising foundation for future industrial applications. While domain-specific errors remain challenging, these results highlight the potential for further improvements through specialized training and enhanced agentic capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2511.05311v1),  [pdf](http://arxiv.org/pdf/2511.05311v1)

**Tags**: cs.AI cs.LG cs.RO cs.SE 



### Code Review Automation using Retrieval Augmented Generation
**Authors**: Qianru Meng, Xiao Zhang, Zhaochen Ren, Joost Visser

**Updated**: 2025-11-07T15:02:42Z

**Summary**: Code review is essential for maintaining software quality but is labor-intensive. Automated code review generation offers a promising solution to this challenge. Both deep learning-based generative techniques and retrieval-based methods have demonstrated strong performance in this task. However, despite these advancements, there are still some limitations where generated reviews can be either off-point or overly general. To address these issues, we introduce Retrieval-Augmented Reviewer (RARe), which leverages Retrieval-Augmented Generation (RAG) to combine retrieval-based and generative methods, explicitly incorporating external domain knowledge into the code review process. RARe uses a dense retriever to select the most relevant reviews from the codebase, which then enrich the input for a neural generator, utilizing the contextual learning capacity of large language models (LLMs), to produce the final review. RARe outperforms state-of-the-art methods on two benchmark datasets, achieving BLEU-4 scores of 12.32 and 12.96, respectively. Its effectiveness is further validated through a detailed human evaluation and a case study using an interpretability tool, demonstrating its practical utility and reliability.

**Link**: [arxiv](http://arxiv.org/abs/2511.05302v1),  [pdf](http://arxiv.org/pdf/2511.05302v1)

**Tags**: cs.SE 



### QUESTER: Query Specification for Generative Retrieval
**Authors**: Arthur Satouf, Yuxuan Zong, Habiboulaye Amadou-Boubacar, Pablo Piantanida, Benjamin Piwowarski

**Updated**: 2025-11-07T15:01:38Z

**Summary**: Generative Retrieval (GR) differs from the traditional index-then-retrieve pipeline by storing relevance in model parameters and directly generating document identifiers. However, GR often struggles to generalize and is costly to scale. We introduce QUESTER (QUEry SpecificaTion gEnerative Retrieval), which reframes GR as query specification generation - in this work, a simple keyword query handled by BM25 - using a (small) LLM. The policy is trained using reinforcement learning techniques (GRPO). Across in- and out-of-domain evaluations, we show that our model is more effective than BM25, and competitive with neural IR models, while maintaining a good efficiency

**Link**: [arxiv](http://arxiv.org/abs/2511.05301v1),  [pdf](http://arxiv.org/pdf/2511.05301v1)

**Tags**: cs.IR cs.CL cs.LG 68P20, 68T50 H.3 



### LiveStar: Live Streaming Assistant for Real-World Online Video   Understanding
**Authors**: Zhenyu Yang, Kairui Zhang, Yuhang Hu, Bing Wang, Shengsheng Qian, Bin Wen, Fan Yang, Tingting Gao, Weiming Dong, Changsheng Xu

**Updated**: 2025-11-07T15:00:37Z

**Summary**: Despite significant progress in Video Large Language Models (Video-LLMs) for offline video understanding, existing online Video-LLMs typically struggle to simultaneously process continuous frame-by-frame inputs and determine optimal response timing, often compromising real-time responsiveness and narrative coherence. To address these limitations, we introduce LiveStar, a pioneering live streaming assistant that achieves always-on proactive responses through adaptive streaming decoding. Specifically, LiveStar incorporates: (1) a training strategy enabling incremental video-language alignment for variable-length video streams, preserving temporal consistency across dynamically evolving frame sequences; (2) a response-silence decoding framework that determines optimal proactive response timing via a single forward pass verification; (3) memory-aware acceleration via peak-end memory compression for online inference on 10+ minute videos, combined with streaming key-value cache to achieve 1.53x faster inference. We also construct an OmniStar dataset, a comprehensive dataset for training and benchmarking that encompasses 15 diverse real-world scenarios and 5 evaluation tasks for online video understanding. Extensive experiments across three benchmarks demonstrate LiveStar's state-of-the-art performance, achieving an average 19.5% improvement in semantic correctness with 18.1% reduced timing difference compared to existing online Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks. Our model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar.

**Link**: [arxiv](http://arxiv.org/abs/2511.05299v1),  [pdf](http://arxiv.org/pdf/2511.05299v1)

**Tags**: cs.CV cs.AI 



### Building Specialized Software-Assistant ChatBot with Graph-Based   Retrieval-Augmented Generation
**Authors**: Mohammed Hilel, Yannis Karmim, Jean De Bodinat, Reda Sarehane, Antoine Gillon

**Updated**: 2025-11-07T14:56:45Z

**Summary**: Digital Adoption Platforms (DAPs) have become essential tools for helping employees navigate complex enterprise software such as CRM, ERP, or HRMS systems. Companies like LemonLearning have shown how digital guidance can reduce training costs and accelerate onboarding. However, building and maintaining these interactive guides still requires extensive manual effort. Leveraging Large Language Models as virtual assistants is an appealing alternative, yet without a structured understanding of the target software, LLMs often hallucinate and produce unreliable answers. Moreover, most production-grade LLMs are black-box APIs, making fine-tuning impractical due to the lack of access to model weights. In this work, we introduce a Graph-based Retrieval-Augmented Generation framework that automatically converts enterprise web applications into state-action knowledge graphs, enabling LLMs to generate grounded and context-aware assistance. The framework was co-developed with the AI enterprise RAKAM, in collaboration with Lemon Learning. We detail the engineering pipeline that extracts and structures software interfaces, the design of the graph-based retrieval process, and the integration of our approach into production DAP workflows. Finally, we discuss scalability, robustness, and deployment lessons learned from industrial use cases.

**Link**: [arxiv](http://arxiv.org/abs/2511.05297v1),  [pdf](http://arxiv.org/pdf/2511.05297v1)

**Tags**: cs.SE cs.LG 



### Language Generation and Identification From Partial Enumeration: Tight   Density Bounds and Topological Characterizations
**Authors**: Jon Kleinberg, Fan Wei

**Updated**: 2025-11-07T14:56:04Z

**Summary**: The success of large language models (LLMs) has motivated formal theories of language generation and learning. We study the framework of \emph{language generation in the limit}, where an adversary enumerates strings from an unknown language $K$ drawn from a countable class, and an algorithm must generate unseen strings from $K$. Prior work showed that generation is always possible, and that some algorithms achieve positive lower density, revealing a \emph{validity--breadth} trade-off between correctness and coverage. We resolve a main open question in this line, proving a tight bound of $1/2$ on the best achievable lower density. We then strengthen the model to allow \emph{partial enumeration}, where the adversary reveals only an infinite subset $C \subseteq K$. We show that generation in the limit remains achievable, and if $C$ has lower density $\alpha$ in $K$, the algorithm's output achieves density at least $\alpha/2$, matching the upper bound. This generalizes the $1/2$ bound to the partial-information setting, where the generator must recover within a factor $1/2$ of the revealed subset's density. We further revisit the classical Gold--Angluin model of \emph{language identification} under partial enumeration. We characterize when identification in the limit is possible -- when hypotheses $M_t$ eventually satisfy $C \subseteq M \subseteq K$ -- and in the process give a new topological formulation of Angluin's characterization, showing that her condition is precisely equivalent to an appropriate topological space having the $T_D$ separation property.

**Link**: [arxiv](http://arxiv.org/abs/2511.05295v1),  [pdf](http://arxiv.org/pdf/2511.05295v1)

**Tags**: cs.DS cs.CL cs.DM cs.LG 



### What's on Your Plate? Inferring Chinese Cuisine Intake from Wearable   IMUs
**Authors**: Jiaxi Yin, Pengcheng Wang, Han Ding, Fei Wang

**Updated**: 2025-11-07T14:54:37Z

**Summary**: Accurate food intake detection is vital for dietary monitoring and chronic disease prevention. Traditional self-report methods are prone to recall bias, while camera-based approaches raise concerns about privacy. Furthermore, existing wearable-based methods primarily focus on a limited number of food types, such as hamburgers and pizza, failing to address the vast diversity of Chinese cuisine. To bridge this gap, we propose CuisineSense, a system that classifies Chinese food types by integrating hand motion cues from a smartwatch with head dynamics from smart glasses. To filter out irrelevant daily activities, we design a two-stage detection pipeline. The first stage identifies eating states by distinguishing characteristic temporal patterns from non-eating behaviors. The second stage then conducts fine-grained food type recognition based on the motions captured during food intake. To evaluate CuisineSense, we construct a dataset comprising 27.5 hours of IMU recordings across 11 food categories and 10 participants. Experiments demonstrate that CuisineSense achieves high accuracy in both eating state detection and food classification, offering a practical solution for unobtrusive, wearable-based dietary monitoring.The system code is publicly available at https://github.com/joeeeeyin/CuisineSense.git.

**Link**: [arxiv](http://arxiv.org/abs/2511.05292v1),  [pdf](http://arxiv.org/pdf/2511.05292v1)

**Tags**: cs.CV cs.LG 



### Benchmarking Retrieval-Augmented Multimodal Generation for Document   Question Answering
**Authors**: Kuicai Dong, Yujing Chang, Shijie Huang, Yasheng Wang, Ruiming Tang, Yong Liu

**Updated**: 2025-11-07T14:52:06Z

**Summary**: Document Visual Question Answering (DocVQA) faces dual challenges in processing lengthy multimodal documents (text, images, tables) and performing cross-modal reasoning. Current document retrieval-augmented generation (DocRAG) methods remain limited by their text-centric approaches, frequently missing critical visual information. The field also lacks robust benchmarks for assessing multimodal evidence selection and integration. We introduce MMDocRAG, a comprehensive benchmark featuring 4,055 expert-annotated QA pairs with multi-page, cross-modal evidence chains. Our framework introduces innovative metrics for evaluating multimodal quote selection and enables answers that interleave text with relevant visual elements. Through large-scale experiments with 60 VLM/LLM models and 14 retrieval systems, we identify persistent challenges in multimodal evidence retrieval, selection, and integration.Key findings reveal advanced proprietary LVMs show superior performance than open-sourced alternatives. Also, they show moderate advantages using multimodal inputs over text-only inputs, while open-source alternatives show significant performance degradation. Notably, fine-tuned LLMs achieve substantial improvements when using detailed image descriptions. MMDocRAG establishes a rigorous testing ground and provides actionable insights for developing more robust multimodal DocVQA systems. Our benchmark and code are available at https://mmdocrag.github.io/MMDocRAG/.

**Link**: [arxiv](http://arxiv.org/abs/2505.16470v2),  [pdf](http://arxiv.org/pdf/2505.16470v2)

**Tags**: cs.IR cs.CL cs.CV 



### Embedding-Space Data Augmentation to Prevent Membership Inference   Attacks in Clinical Time Series Forecasting
**Authors**: Marius Fracarolli, Michael Staniek, Stefan Riezler

**Updated**: 2025-11-07T14:49:18Z

**Summary**: Balancing strong privacy guarantees with high predictive performance is critical for time series forecasting (TSF) tasks involving Electronic Health Records (EHR). In this study, we explore how data augmentation can mitigate Membership Inference Attacks (MIA) on TSF models. We show that retraining with synthetic data can substantially reduce the effectiveness of loss-based MIAs by reducing the attacker's true-positive to false-positive ratio. The key challenge is generating synthetic samples that closely resemble the original training data to confuse the attacker, while also introducing enough novelty to enhance the model's ability to generalize to unseen data. We examine multiple augmentation strategies - Zeroth-Order Optimization (ZOO), a variant of ZOO constrained by Principal Component Analysis (ZOO-PCA), and MixUp - to strengthen model resilience without sacrificing accuracy. Our experimental results show that ZOO-PCA yields the best reductions in TPR/FPR ratio for MIA attacks without sacrificing performance on test data.

**Link**: [arxiv](http://arxiv.org/abs/2511.05289v1),  [pdf](http://arxiv.org/pdf/2511.05289v1)

**Tags**: cs.LG 



### Reflective Personalization Optimization: A Post-hoc Rewriting Framework   for Black-Box Large Language Models
**Authors**: Teqi Hao, Xioayu Tan, Shaojie Shi, Yinghui Xu, Xihe Qiu

**Updated**: 2025-11-07T14:48:49Z

**Summary**: The personalization of black-box large language models (LLMs) is a critical yet challenging task. Existing approaches predominantly rely on context injection, where user history is embedded into the prompt to directly guide the generation process. However, this single-step paradigm imposes a dual burden on the model: generating accurate content while simultaneously aligning with user-specific styles. This often results in a trade-off that compromises output quality and limits precise control. To address this fundamental tension, we propose Reflective Personalization Optimization (RPO), a novel framework that redefines the personalization paradigm by decoupling content generation from alignment. RPO operates in two distinct stages: first, a base model generates a high-quality, generic response; then, an external reflection module explicitly rewrites this output to align with the user's preferences. This reflection module is trained using a two-stage process. Initially, supervised fine-tuning is employed on structured rewriting trajectories to establish a core personalized reasoning policy that models the transformation from generic to user-aligned responses. Subsequently, reinforcement learning is applied to further refine and enhance the quality of the personalized outputs. Comprehensive experiments on the LaMP benchmark demonstrate that RPO, by decoupling content generation from personalization, significantly outperforms state-of-the-art baselines. These findings underscore the superiority of explicit response shaping over implicit context injection. Moreover, RPO introduces an efficient, model-agnostic personalization layer that can be seamlessly integrated with any underlying base model, paving the way for a new and effective direction in user-centric generation scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2511.05286v1),  [pdf](http://arxiv.org/pdf/2511.05286v1)

**Tags**: cs.CL 



### ExGra-Med: Extended Context Graph Alignment for Medical Vision-Language   Models
**Authors**: Duy M. H. Nguyen, Nghiem T. Diep, Trung Q. Nguyen, Hoang-Bao Le, Tai Nguyen, Tien Nguyen, TrungTin Nguyen, Nhat Ho, Pengtao Xie, Roger Wattenhofer, James Zou, Daniel Sonntag, Mathias Niepert

**Updated**: 2025-11-07T14:48:44Z

**Summary**: State-of-the-art medical multi-modal LLMs (med-MLLMs), such as LLaVA-Med and BioMedGPT, primarily depend on scaling model size and data volume, with training driven largely by autoregressive objectives. However, we reveal that this approach can lead to weak vision-language alignment, making these models overly dependent on costly instruction-following data. To address this, we introduce ExGra-Med, a novel multi-graph alignment framework that jointly aligns images, instruction responses, and extended captions in the latent space, advancing semantic grounding and cross-modal coherence. To scale to large LLMs (e.g., LLaMA-7B), we develop an efficient end-to-end training scheme using black-box gradient estimation, enabling fast and scalable optimization. Empirically, ExGra-Med matches LLaVA-Med's performance using just 10% of the pre-training data, achieving a 20.13% gain on VQA-RAD and approaching full-data performance. It also outperforms strong baselines like BioMedGPT and RadFM on visual chatbot and zero-shot classification tasks, demonstrating its promise for efficient, high-quality vision-language integration in medical AI.

**Link**: [arxiv](http://arxiv.org/abs/2410.02615v4),  [pdf](http://arxiv.org/pdf/2410.02615v4)

**Tags**: cs.LG 



### Know What You Don't Know: Uncertainty Calibration of Process Reward   Models
**Authors**: Young-Jin Park, Kristjan Greenewald, Kaveh Alim, Hao Wang, Navid Azizan

**Updated**: 2025-11-07T14:35:34Z

**Summary**: Process reward models (PRMs) play a central role in guiding inference-time scaling algorithms for large language models (LLMs). However, we observe that even state-of-the-art PRMs can be poorly calibrated. Specifically, they tend to overestimate the success probability that a partial reasoning step will lead to a correct final answer, particularly when smaller LLMs are used to complete the reasoning trajectory. To address this, we present a calibration approach -- performed via quantile regression -- that adjusts PRM outputs to better align with true success probabilities. Leveraging these calibrated success estimates and their associated confidence bounds, we introduce an \emph{instance-adaptive scaling} (IAS) framework that dynamically adjusts the compute budget based on the estimated likelihood that a partial reasoning trajectory will yield a correct final answer. Unlike conventional methods that allocate a fixed number of reasoning trajectories per query, this approach adapts to each instance and reasoning step when using our calibrated PRMs. Experiments on mathematical reasoning benchmarks show that (i) our PRM calibration method achieves small calibration error, outperforming the baseline methods, (ii) calibration is crucial for enabling effective IAS, and (iii) the proposed IAS strategy reduces inference costs while maintaining final answer accuracy, utilizing less compute on more confident problems as desired.

**Link**: [arxiv](http://arxiv.org/abs/2506.09338v2),  [pdf](http://arxiv.org/pdf/2506.09338v2)

**Tags**: stat.ML cs.AI cs.LG 



### TAMAS: Benchmarking Adversarial Risks in Multi-Agent LLM Systems
**Authors**: Ishan Kavathekar, Hemang Jain, Ameya Rathod, Ponnurangam Kumaraguru, Tanuja Ganu

**Updated**: 2025-11-07T14:30:26Z

**Summary**: Large Language Models (LLMs) have demonstrated strong capabilities as autonomous agents through tool use, planning, and decision-making abilities, leading to their widespread adoption across diverse tasks. As task complexity grows, multi-agent LLM systems are increasingly used to solve problems collaboratively. However, safety and security of these systems remains largely under-explored. Existing benchmarks and datasets predominantly focus on single-agent settings, failing to capture the unique vulnerabilities of multi-agent dynamics and co-ordination. To address this gap, we introduce $\textbf{T}$hreats and $\textbf{A}$ttacks in $\textbf{M}$ulti-$\textbf{A}$gent $\textbf{S}$ystems ($\textbf{TAMAS}$), a benchmark designed to evaluate the robustness and safety of multi-agent LLM systems. TAMAS includes five distinct scenarios comprising 300 adversarial instances across six attack types and 211 tools, along with 100 harmless tasks. We assess system performance across ten backbone LLMs and three agent interaction configurations from Autogen and CrewAI frameworks, highlighting critical challenges and failure modes in current multi-agent deployments. Furthermore, we introduce Effective Robustness Score (ERS) to assess the tradeoff between safety and task effectiveness of these frameworks. Our findings show that multi-agent systems are highly vulnerable to adversarial attacks, underscoring the urgent need for stronger defenses. TAMAS provides a foundation for systematically studying and improving the safety of multi-agent LLM systems.

**Link**: [arxiv](http://arxiv.org/abs/2511.05269v1),  [pdf](http://arxiv.org/pdf/2511.05269v1)

**Tags**: cs.MA cs.AI 



### LLM4FaaS: No-Code Application Development using LLMs and FaaS
**Authors**: Minghe Wang, Tobias Pfandzelter, Trever Schirmer, David Bermbach

**Updated**: 2025-11-07T14:25:54Z

**Summary**: Large language models (LLMs) show great capabilities in generating code from natural language descriptions, bringing programming power closer to non-technical users. However, their lack of expertise in operating the generated code remains a key barrier to realizing customized applications. Function-as-a-Service (FaaS) platforms offer a high level of abstraction for code execution and deployment, allowing users to run LLM-generated code without requiring technical expertise or incurring operational overhead.   In this paper, we present LLM4FaaS, a no-code application development approach that integrates LLMs and FaaS platforms to enable non-technical users to build and run customized applications using only natural language. By deploying LLM-generated code through FaaS, LLM4FaaS abstracts away infrastructure management and boilerplate code generation. We implement a proof-of-concept prototype based on an open-source FaaS platform, and evaluate it using real prompts from non-technical users. Experiments with GPT-4o show that LLM4FaaS can automatically build and deploy code in 71.47% of cases, outperforming a non-FaaS baseline at 43.48% and an existing LLM-based platform at 14.55%, narrowing the gap to human performance at 88.99%. Further analysis of code quality, programming language diversity, latency, and consistency demonstrates a balanced performance in terms of efficiency, maintainability and availability.

**Link**: [arxiv](http://arxiv.org/abs/2502.14450v2),  [pdf](http://arxiv.org/pdf/2502.14450v2)

**Tags**: cs.SE cs.DC 



### Statistical Inference for Extended Target Detection in mmWave Automotive   Radar
**Authors**: Vinay Kulkarni, V. V. Reddy

**Updated**: 2025-11-07T14:25:50Z

**Summary**: Millimeter-wave (mmWave) automotive radar systems offer high range resolution due to their wide bandwidth, enabling the detection of multiple spatially distributed scatterers from a single extended target, such as a vehicle. Traditional CFAR-based detection methods often treat these scatterers as independent point targets, thereby neglecting the inherent spatial structure of extended objects. To address this limitation, we propose a novel Range-Doppler (RD) segment-based statistical inference framework that captures the characteristic scattering profile of extended automotive targets. The framework employs Maximum Likelihood Estimation (MLE) for statistical parameter extraction and utilizes Gibbs sampling within a Markov Chain Monte Carlo (MCMC) scheme to model the posterior distribution of the segment features. A skewness-based test statistic, derived from the estimated distribution, is introduced for binary hypothesis testing to distinguish extended targets. Furthermore, we develop a detection pipeline incorporating Intersection over Union (IoU) metrics and peak-centric segment alignment, optimized for single-dwell radar operations. Comprehensive evaluations on both simulated and real-world datasets demonstrate the effectiveness of the proposed method, achieving enhanced detection accuracy and robustness in automotive radar scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2509.26573v2),  [pdf](http://arxiv.org/pdf/2509.26573v2)

**Tags**: eess.SP math.ST stat.TH 



### Fuzzy Neural Network Performance and Interpretability of Quantum   Wavefunction Probability Predictions
**Authors**: Pedro H. M. Zanineli, Matheus Zaia Monteiro, Vinicius Francisco Wasques, Francielle Santo Pedro Simões, Gabriel R. Schleder

**Updated**: 2025-11-07T14:24:49Z

**Summary**: Predicting quantum wavefunction probability distributions is crucial for computational chemistry and materials science, yet machine learning (ML) models often face a trade-off between accuracy and interpretability. This study compares Artificial Neural Networks (ANNs) and Adaptive Neuro-Fuzzy Inference Systems (ANFIS) in modeling quantum probability distributions for the H$_{2}^+$ ion, leveraging data generated via Physics-Informed Neural Networks (PINNs). While ANN achieved superior accuracy (R$^2$ = 0.99 vs ANFIS's 0.95 with Gaussian membership functions), it required over 50x more parameters (2,305 vs 39-45). ANFIS, however, provided unique interpretability: its Gaussian membership functions encoded spatial electron localization near proton positions ($\mu = 1.2 A$), mirroring Born probability densities, while fuzzy rules reflected quantum superposition principles. Rules prioritizing the internuclear direction revealed the system's 1D symmetry, aligning with Linear Combination of Atomic Orbitals theory--a novel data-driven perspective on orbital hybridization. Membership function variances ($\sigma$) further quantified electron delocalization trends, and peak prediction errors highlighted unresolved quantum cusps. The choice of functions critically impacted performance: Gaussian/Generalized Bell outperformed Sigmoid, with errors improving as training data increased, showing scalability. This study underscores the context-dependent value of ML: ANN for precision and ANFIS for interpretable, parameter-efficient approximations that link inputs to physical behavior. These findings advocate hybrid approaches in quantum simulations, balancing accuracy with explainability to accelerate discovery. Future work should extend ANFIS to multi-electron systems and integrate domain-specific constraints (e.g., kinetic energy terms), bridging data-driven models and fundamental physics.

**Link**: [arxiv](http://arxiv.org/abs/2511.05261v1),  [pdf](http://arxiv.org/pdf/2511.05261v1)

**Tags**: physics.chem-ph cond-mat.dis-nn cond-mat.mtrl-sci physics.data-an 



### Applying Large Language Models to Travel Satisfaction Analysis
**Authors**: Pengfei Xu, Donggen Wang

**Updated**: 2025-11-07T14:09:42Z

**Summary**: As a specific domain of subjective well-being, travel satisfaction has recently attracted much research attention. Previous studies primarily relied on statistical models and, more recently, machine learning models to explore its determinants. Both approaches,however, depend on sufficiently large sample sizes and appropriate statistical assumptions. The emergence of Large Language Models (LLMs) offers a new modeling approach that can address these limitations. Pre-trained on extensive datasets, LLMs have strongcapabilities in contextual understanding and generalization, significantly reducing their dependence on task-specific data and stringent statistical assumptions. The main challenge in applying LLMs lies in the behavioral misalignment between LLMs and humans. Using household survey data collected in Shanghai, this study identifies the existence and source of misalignment, and applies a few-shot learning method to address the misalignment issue. We find that the zero-shot LLM exhibits behavioral misalignment, leading to low prediction accuracy. With just a few samples, few-shot learning can align LLMs and enable them to outperform baseline models. Discrepancies in variable importance among machine learning model, zero-shot LLM, and few-shot LLM reveal that the misalignment arises from the gap between the general knowledge embedded in pre-trained LLMs and the specific, unique characteristics of the dataset. On these bases, we propose an LLM-based modeling approach that can be applied to model travel behavior with small sample sizes. This study highlights the potential of LLMs for modeling not only travel satisfaction but also broader aspects of travel behavior.

**Link**: [arxiv](http://arxiv.org/abs/2505.23262v2),  [pdf](http://arxiv.org/pdf/2505.23262v2)

**Tags**: cs.CY 



### Open Agent Specification (Agent Spec): A Unified Representation for AI   Agents
**Authors**: Soufiane Amini, Yassine Benajiba, Cesare Bernardis, Paul Cayet, Hassan Chafi, Abderrahim Fathan, Louis Faucon, Damien Hilloulin, Sungpack Hong, Ingo Kossyk, Tran Minh Son Le, Rhicheek Patra, Sujith Ravi, Jonas Schweizer, Jyotika Singh, Shailender Singh, Weiyi Sun, Kartik Talamadupula, Jerry Xu

**Updated**: 2025-11-07T14:02:33Z

**Summary**: The proliferation of agent frameworks has led to fragmentation in how agents are defined, executed, and evaluated. Existing systems differ in their abstractions, data flow semantics, and tool integrations, making it difficult to share or reproduce workflows. We introduce Open Agent Specification (Agent Spec), a declarative language that defines AI agents and agentic workflows in a way that is compatible across frameworks, promoting reusability, portability and interoperability of AI agents. Agent Spec defines a common set of components, control and data flow semantics, and schemas that allow an agent to be defined once and executed across different runtimes. Agent Spec also introduces a standardized Evaluation harness to assess agent behavior and agentic workflows across runtimes - analogous to how HELM and related harnesses standardized LLM evaluation - so that performance, robustness, and efficiency can be compared consistently across frameworks. We demonstrate this using four distinct runtimes (LangGraph, CrewAI, AutoGen, and WayFlow) evaluated over three different benchmarks (SimpleQA Verified, $\tau^2$-Bench and BIRD-SQL). We provide accompanying toolsets: a Python SDK (PyAgentSpec), a reference runtime (WayFlow), and adapters for popular frameworks (e.g., LangGraph, AutoGen, CrewAI). Agent Spec bridges the gap between model-centric and agent-centric standardization & evaluation, laying the groundwork for reliable, reusable, and portable agentic systems.

**Link**: [arxiv](http://arxiv.org/abs/2510.04173v4),  [pdf](http://arxiv.org/pdf/2510.04173v4)

**Tags**: cs.AI 



### Cognitive Edge Computing: A Comprehensive Survey on Optimizing Large   Models and AI Agents for Pervasive Deployment
**Authors**: Xubin Wang, Qing Li, Weijia Jia

**Updated**: 2025-11-07T13:51:28Z

**Summary**: This article surveys Cognitive Edge Computing as a practical and methodical pathway for deploying reasoning-capable Large Language Models (LLMs) and autonomous AI agents on resource-constrained devices at the network edge. We present a unified, cognition-preserving framework spanning: (1) model optimization (quantization, sparsity, low-rank adaptation, distillation) aimed at retaining multi-step reasoning under tight memory/compute budgets; (2) system architecture (on-device inference, elastic offloading, cloud-edge collaboration) that trades off latency, energy, privacy, and capacity; and (3) adaptive intelligence (context compression, dynamic routing, federated personalization) that tailors computation to task difficulty and device constraints. We synthesize advances in efficient Transformer design, multimodal integration, hardware-aware compilation, privacy-preserving learning, and agentic tool use, and map them to edge-specific operating envelopes. We further outline a standardized evaluation protocol covering latency, throughput, energy per token, accuracy, robustness, privacy, and sustainability, with explicit measurement assumptions to enhance comparability. Remaining challenges include modality-aware reasoning benchmarks, transparent and reproducible energy reporting, edge-oriented safety/alignment evaluation, and multi-agent testbeds. We conclude with practitioner guidelines for cross-layer co-design of algorithms, runtime, and hardware to deliver reliable, efficient, and privacy-preserving cognitive capabilities on edge devices.

**Link**: [arxiv](http://arxiv.org/abs/2501.03265v2),  [pdf](http://arxiv.org/pdf/2501.03265v2)

**Tags**: cs.LG cs.AI 



### Pseudo-Maximum Likelihood Theory for High-Dimensional Rank One Inference
**Authors**: Curtis Grant, Aukosh Jagannath, Justin Ko

**Updated**: 2025-11-07T13:49:52Z

**Summary**: We develop a pseudo-likelihood theory for rank one matrix estimation problems in the high dimensional limit. We prove a variational principle for the limiting pseudo-maximum likelihood which also characterizes the performance of the corresponding pseudo-maximum likelihood estimator. We show that this variational principle is universal and depends only on four parameters determined by the corresponding null model. Through this universality, we introduce a notion of equivalence for estimation problems of this type and, in particular, show that a broad class of estimation tasks, including community detection, sparse submatrix detection, and non-linear spiked matrix models, are equivalent to spiked matrix models. As an application, we obtain a complete description of the performance of the least-squares (or ``best rank one'') estimator for any rank one matrix estimation problem.

**Link**: [arxiv](http://arxiv.org/abs/2503.01708v2),  [pdf](http://arxiv.org/pdf/2503.01708v2)

**Tags**: math.ST math.PR stat.TH 



### Bilinear relational structure fixes reversal curse and enables   consistent model editing
**Authors**: Dong-Kyum Kim, Minsung Kim, Jea Kwon, Nakyeong Yang, Meeyoung Cha

**Updated**: 2025-11-07T13:49:40Z

**Summary**: The reversal curse -- a language model's (LM) inability to infer an unseen fact ``B is A'' from a learned fact ``A is B'' -- is widely considered a fundamental limitation. We show that this is not an inherent failure but an artifact of how models encode knowledge. By training LMs from scratch on a synthetic dataset of relational knowledge graphs, we demonstrate that bilinear relational structure emerges in their hidden representations. This structure substantially alleviates the reversal curse, enabling LMs to infer unseen reverse facts. Crucially, we also find that this bilinear structure plays a key role in consistent model editing. When a fact is updated in a LM with this structure, the edit correctly propagates to its reverse and other logically dependent facts. In contrast, models lacking this representation not only suffer from the reversal curse but also fail to generalize edits, further introducing logical inconsistencies. Our results establish that training on a relational knowledge dataset induces the emergence of bilinear internal representations, which in turn enable LMs to behave in a logically consistent manner after editing. This implies that the success of model editing depends critically not just on editing algorithms but on the underlying representational geometry of the knowledge being modified.

**Link**: [arxiv](http://arxiv.org/abs/2509.21993v2),  [pdf](http://arxiv.org/pdf/2509.21993v2)

**Tags**: cs.AI cs.LG 



### Translation via Annotation: A Computational Study of Translating   Classical Chinese into Japanese
**Authors**: Zilong Li, Jie Cao

**Updated**: 2025-11-07T13:46:16Z

**Summary**: Ancient people translated classical Chinese into Japanese by annotating around each character. We abstract this process as sequence tagging tasks and fit them into modern language technologies. The research of this annotation and translation system is a facing low-resource problem. We release this problem by introducing a LLM-based annotation pipeline and construct a new dataset from digitalized open-source translation data. We show that under the low-resource setting, introducing auxiliary Chinese NLP tasks has a promoting effect on the training of sequence tagging tasks. We also evaluate the performance of large language models. They achieve high scores in direct machine translation, but they are confused when being asked to annotate characters. Our method could work as a supplement of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2511.05239v1),  [pdf](http://arxiv.org/pdf/2511.05239v1)

**Tags**: cs.CL 



### Estimating Orbital Parameters of Direct Imaging Exoplanet Using Neural   Network
**Authors**: Bo Liang, Hanlin Song, Chang Liu, Tianyu Zhao, Yuxiang Xu, Zihao Xiao, Manjia Liang, Minghui Du, Wei-Liang Qian, Li-e Qiang, Peng Xu, Ziren Luo

**Updated**: 2025-11-07T13:42:16Z

**Summary**: In this work, we propose a new flow-matching Markov chain Monte Carlo (FM-MCMC) algorithm for estimating the orbital parameters of exoplanetary systems, especially for those only one exoplanet is involved. Compared to traditional methods that rely on random sampling within the Bayesian framework, our approach first leverages flow matching posterior estimation (FMPE) to efficiently constrain the prior range of physical parameters, and then employs MCMC to accurately infer the posterior distribution. For example, in the orbital parameter inference of beta Pictoris b, our model achieved a substantial speed-up while maintaining comparable accuracy-running 77.8 times faster than Parallel Tempered MCMC (PTMCMC) and 365.4 times faster than nested sampling. Moreover, our FM-MCMC method also attained the highest average log-likelihood among all approaches, demonstrating its superior sampling efficiency and accuracy. This highlights the scalability and efficiency of our approach, making it well-suited for processing the massive datasets expected from future exoplanet surveys. Beyond astrophysics, our methodology establishes a versatile paradigm for synergizing deep generative models with traditional sampling, which can be adopted to tackle complex inference problems in other fields, such as cosmology, biomedical imaging, and particle physics.

**Link**: [arxiv](http://arxiv.org/abs/2510.17459v2),  [pdf](http://arxiv.org/pdf/2510.17459v2)

**Tags**: astro-ph.EP astro-ph.GA cs.LG 



### The Causal Round Trip: Generating Authentic Counterfactuals by   Eliminating Information Loss
**Authors**: Rui Wu, Lizheng Wang, Yongjun Li

**Updated**: 2025-11-07T13:37:23Z

**Summary**: Judea Pearl's vision of Structural Causal Models (SCMs) as engines for counterfactual reasoning hinges on faithful abduction: the precise inference of latent exogenous noise. For decades, operationalizing this step for complex, non-linear mechanisms has remained a significant computational challenge. The advent of diffusion models, powerful universal function approximators, offers a promising solution. However, we argue that their standard design, optimized for perceptual generation over logical inference, introduces a fundamental flaw for this classical problem: an inherent information loss we term the Structural Reconstruction Error (SRE). To address this challenge, we formalize the principle of Causal Information Conservation (CIC) as the necessary condition for faithful abduction. We then introduce BELM-MDCM, the first diffusion-based framework engineered to be causally sound by eliminating SRE by construction through an analytically invertible mechanism. To operationalize this framework, a Targeted Modeling strategy provides structural regularization, while a Hybrid Training Objective instills a strong causal inductive bias. Rigorous experiments demonstrate that our Zero-SRE framework not only achieves state-of-the-art accuracy but, more importantly, enables the high-fidelity, individual-level counterfactuals required for deep causal inquiries. Our work provides a foundational blueprint that reconciles the power of modern generative models with the rigor of classical causal theory, establishing a new and more rigorous standard for this emerging field.

**Link**: [arxiv](http://arxiv.org/abs/2511.05236v1),  [pdf](http://arxiv.org/pdf/2511.05236v1)

**Tags**: cs.LG 



### Context-aware Learned Mesh-based Simulation via Trajectory-Level   Meta-Learning
**Authors**: Philipp Dahlinger, Niklas Freymuth, Tai Hoang, Tobias Würth, Michael Volpp, Luise Kärger, Gerhard Neumann

**Updated**: 2025-11-07T13:34:02Z

**Summary**: Simulating object deformations is a critical challenge across many scientific domains, including robotics, manufacturing, and structural mechanics. Learned Graph Network Simulators (GNSs) offer a promising alternative to traditional mesh-based physics simulators. Their speed and inherent differentiability make them particularly well suited for applications that require fast and accurate simulations, such as robotic manipulation or manufacturing optimization. However, existing learned simulators typically rely on single-step observations, which limits their ability to exploit temporal context. Without this information, these models fail to infer, e.g., material properties. Further, they rely on auto-regressive rollouts, which quickly accumulate error for long trajectories. We instead frame mesh-based simulation as a trajectory-level meta-learning problem. Using Conditional Neural Processes, our method enables rapid adaptation to new simulation scenarios from limited initial data while capturing their latent simulation properties. We utilize movement primitives to directly predict fast, stable and accurate simulations from a single model call. The resulting approach, Movement-primitive Meta-MeshGraphNet (M3GN), provides higher simulation accuracy at a fraction of the runtime cost compared to state-of-the-art GNSs across several tasks.

**Link**: [arxiv](http://arxiv.org/abs/2511.05234v1),  [pdf](http://arxiv.org/pdf/2511.05234v1)

**Tags**: cs.RO cs.LG 



### Amortized Latent Steering: Low-Cost Alternative to Test-Time   Optimization
**Authors**: Nathan Egbuna, Saatvik Gaur, Sunishchal Dev, Ashwinee Panda, Maheep Chaudhary

**Updated**: 2025-11-07T13:28:17Z

**Summary**: Test-time optimization remains impractical at scale due to prohibitive inference costs--techniques like iterative refinement and multi-step verification can require $10-100\times$ more compute per query than standard decoding. Latent space test-time optimization methods like LatentSeek offer a more direct approach by steering hidden representations, but still demand expensive per-query optimization loops with multiple backward passes. We propose Amortized Latent Steering (ALS), which collapses this iterative optimization into a single offline-computed vector applied at constant cost during inference. ALS computes the mean difference between hidden states from successful versus unsuccessful generations, then uses this direction to calibrate the model's hidden representations: when decoding drifts away from the success manifold, ALS nudges activations back toward it. Across GSM8K and MATH-500 benchmarks, ALS achieves $2-5\times$ speedup over iterative methods while matching or surpassing greedy Chain-of-Thought (CoT) and Self-Consistency baselines, yielding up to 101% improvement in efficiency--accuracy trade-off. These results show that much of latent optimization's benefit can be captured offline, making sophisticated reasoning techniques viable for production deployment. Code is available at https://github.com/negbuna/ALS.

**Link**: [arxiv](http://arxiv.org/abs/2509.18116v2),  [pdf](http://arxiv.org/pdf/2509.18116v2)

**Tags**: cs.LG cs.AI 



### Mind the Blind Spots: A Focus-Level Evaluation Framework for LLM Reviews
**Authors**: Hyungyu Shin, Jingyu Tang, Yoonjoo Lee, Nayoung Kim, Hyunseung Lim, Ji Yong Cho, Hwajung Hong, Moontae Lee, Juho Kim

**Updated**: 2025-11-07T13:27:39Z

**Summary**: Peer review underpins scientific progress, but it is increasingly strained by reviewer shortages and growing workloads. Large Language Models (LLMs) can automatically draft reviews now, but determining whether LLM-generated reviews are trustworthy requires systematic evaluation. Researchers have evaluated LLM reviews at either surface-level (e.g., BLEU and ROUGE) or content-level (e.g., specificity and factual accuracy). Yet it remains uncertain whether LLM-generated reviews attend to the same critical facets that human experts weigh -- the strengths and weaknesses that ultimately drive an accept-or-reject decision. We introduce a focus-level evaluation framework that operationalizes the focus as a normalized distribution of attention across predefined facets in paper reviews. Based on the framework, we developed an automatic focus-level evaluation pipeline based on two sets of facets: target (e.g., problem, method, and experiment) and aspect (e.g., validity, clarity, and novelty), leveraging 676 paper reviews (https://figshare.com/s/d5adf26c802527dd0f62) from OpenReview that consists of 3,657 strengths and weaknesses identified from human experts. The comparison of focus distributions between LLMs and human experts showed that the off-the-shelf LLMs consistently have a more biased focus towards examining technical validity while significantly overlooking novelty assessment when criticizing papers.

**Link**: [arxiv](http://arxiv.org/abs/2502.17086v4),  [pdf](http://arxiv.org/pdf/2502.17086v4)

**Tags**: cs.CL 



### FreeControl: Efficient, Training-Free Structural Control via One-Step   Attention Extraction
**Authors**: Jiang Lin, Xinyu Chen, Song Wu, Zhiqiu Zhang, Jizhi Zhang, Ye Wang, Qiang Tang, Qian Wang, Jian Yang, Zili Yi

**Updated**: 2025-11-07T13:17:46Z

**Summary**: Controlling the spatial and semantic structure of diffusion-generated images remains a challenge. Existing methods like ControlNet rely on handcrafted condition maps and retraining, limiting flexibility and generalization. Inversion-based approaches offer stronger alignment but incur high inference cost due to dual-path denoising. We present FreeControl, a training-free framework for semantic structural control in diffusion models. Unlike prior methods that extract attention across multiple timesteps, FreeControl performs one-step attention extraction from a single, optimally chosen key timestep and reuses it throughout denoising. This enables efficient structural guidance without inversion or retraining. To further improve quality and stability, we introduce Latent-Condition Decoupling (LCD): a principled separation of the key timestep and the noised latent used in attention extraction. LCD provides finer control over attention quality and eliminates structural artifacts. FreeControl also supports compositional control via reference images assembled from multiple sources - enabling intuitive scene layout design and stronger prompt alignment. FreeControl introduces a new paradigm for test-time control, enabling structurally and semantically aligned, visually coherent generation directly from raw images, with the flexibility for intuitive compositional design and compatibility with modern diffusion models at approximately 5 percent additional cost.

**Link**: [arxiv](http://arxiv.org/abs/2511.05219v1),  [pdf](http://arxiv.org/pdf/2511.05219v1)

**Tags**: cs.CV 



### Holistic Evaluation of Multimodal LLMs on Spatial Intelligence
**Authors**: Zhongang Cai, Yubo Wang, Qingping Sun, Ruisi Wang, Chenyang Gu, Wanqi Yin, Zhiqian Lin, Zhitao Yang, Chen Wei, Oscar Qian, Hui En Pang, Xuanke Shi, Kewang Deng, Xiaoyang Han, Zukai Chen, Jiaqi Li, Xiangyu Fan, Hanming Deng, Lewei Lu, Bo Li, Ziwei Liu, Quan Wang, Dahua Lin, Lei Yang

**Updated**: 2025-11-07T13:12:03Z

**Summary**: Multimodal models have achieved remarkable progress in recent years. Nevertheless, they continue to exhibit notable limitations in spatial understanding and reasoning, the very capability that anchors artificial general intelligence in the physical world. With the recent release of GPT-5, allegedly the most powerful AI model to date, it is timely to examine where the leading models (GPT, Gemini, Grok, Seed, Qwen, and Intern) stand on the path toward spatial intelligence. We thus propose EASI for holistic Evaluation of multimodAl LLMs on Spatial Intelligence. EASI conceptualizes a comprehensive taxonomy of spatial tasks that unifies existing benchmarks and a standardized protocol for the fair evaluation of state-of-the-art proprietary and open-source models. In this report, we conduct the study across eight key benchmarks, at a cost exceeding ten billion total tokens. Our empirical study then reveals that (1) GPT-5 demonstrates unprecedented strength in spatial intelligence (SI), yet (2) still falls short of human performance significantly across a broad spectrum of SI-tasks. Moreover, we (3) show that SI-tasks expose greater model capability deficiency than non-SI tasks, to the extent that (4) proprietary models do not exhibit a decisive advantage when facing the most difficult ones. In addition, we conduct a qualitative evaluation across a diverse set of scenarios that are intuitive for humans, yet fail even the most advanced multimodal models.

**Link**: [arxiv](http://arxiv.org/abs/2508.13142v3),  [pdf](http://arxiv.org/pdf/2508.13142v3)

**Tags**: cs.CV cs.CL cs.LG cs.MM cs.RO 



### A Composable Agentic System for Automated Visual Data Reporting
**Authors**: Péter Ferenc Gyarmati, Dominik Moritz, Torsten Möller, Laura Koesten

**Updated**: 2025-11-07T13:05:57Z

**Summary**: To address the brittleness of monolithic AI agents, our prototype for automated visual data reporting explores a Human-AI Partnership model. Its hybrid, multi-agent architecture strategically externalizes logic from LLMs to deterministic modules, leveraging the rule-based system Draco for principled visualization design. The system delivers a dual-output: an interactive Observable report with Mosaic for reader exploration, and executable Marimo notebooks for deep, analyst-facing traceability. This granular architecture yields a fully automatic yet auditable and steerable system, charting a path toward a more synergistic partnership between human experts and AI. For reproducibility, our implementation and examples are available at https://peter-gy.github.io/VISxGenAI-2025/.

**Link**: [arxiv](http://arxiv.org/abs/2509.05721v2),  [pdf](http://arxiv.org/pdf/2509.05721v2)

**Tags**: cs.HC 



### EditInfinity: Image Editing with Binary-Quantized Generative Models
**Authors**: Jiahuan Wang, Yuxin Chen, Jun Yu, Guangming Lu, Wenjie Pei

**Updated**: 2025-11-07T12:33:20Z

**Summary**: Adapting pretrained diffusion-based generative models for text-driven image editing with negligible tuning overhead has demonstrated remarkable potential. A classical adaptation paradigm, as followed by these methods, first infers the generative trajectory inversely for a given source image by image inversion, then performs image editing along the inferred trajectory guided by the target text prompts. However, the performance of image editing is heavily limited by the approximation errors introduced during image inversion by diffusion models, which arise from the absence of exact supervision in the intermediate generative steps. To circumvent this issue, we investigate the parameter-efficient adaptation of binary-quantized generative models for image editing, and leverage their inherent characteristic that the exact intermediate quantized representations of a source image are attainable, enabling more effective supervision for precise image inversion. Specifically, we propose EditInfinity, which adapts \emph{Infinity}, a binary-quantized generative model, for image editing. We propose an efficient yet effective image inversion mechanism that integrates text prompting rectification and image style preservation, enabling precise image inversion. Furthermore, we devise a holistic smoothing strategy which allows our EditInfinity to perform image editing with high fidelity to source images and precise semantic alignment to the text prompts. Extensive experiments on the PIE-Bench benchmark across `add', `change', and `delete' editing operations, demonstrate the superior performance of our model compared to state-of-the-art diffusion-based baselines. Code available at: https://github.com/yx-chen-ust/EditInfinity.

**Link**: [arxiv](http://arxiv.org/abs/2510.20217v3),  [pdf](http://arxiv.org/pdf/2510.20217v3)

**Tags**: cs.CV 



### Rethinking Approximate Gaussian Inference in Classification
**Authors**: Bálint Mucsányi, Nathaël Da Costa, Philipp Hennig

**Updated**: 2025-11-07T12:06:33Z

**Summary**: In classification tasks, softmax functions are ubiquitously used as output activations to produce predictive probabilities. Such outputs only capture aleatoric uncertainty. To capture epistemic uncertainty, approximate Gaussian inference methods have been proposed. We develop a common formalism to describe such methods, which we view as outputting Gaussian distributions over the logit space. Predictives are then obtained as the expectations of the Gaussian distributions pushed forward through the softmax. However, such softmax Gaussian integrals cannot be solved analytically, and Monte Carlo (MC) approximations can be costly and noisy. We propose to replace the softmax activation by element-wise normCDF or sigmoid, which allows for the accurate sampling-free approximation of predictives. This also enables the approximation of the Gaussian pushforwards by Dirichlet distributions with moment matching. This approach entirely eliminates the runtime and memory overhead associated with MC sampling. We evaluate it combined with several approximate Gaussian inference methods (Laplace, HET, SNGP) on large- and small-scale datasets (ImageNet, CIFAR-100, CIFAR-10), demonstrating improved uncertainty quantification capabilities compared to softmax MC sampling. Our code is available at https://github.com/bmucsanyi/probit.

**Link**: [arxiv](http://arxiv.org/abs/2502.03366v3),  [pdf](http://arxiv.org/pdf/2502.03366v3)

**Tags**: cs.LG stat.ML 



### Effectiveness of Chain-of-Thought in Distilling Reasoning Capability   from Large Language Models
**Authors**: Cong-Thanh Do, Rama Doddipatla, Kate Knill

**Updated**: 2025-11-07T12:05:39Z

**Summary**: Chain-of-Thought (CoT) prompting is a widely used method to improve the reasoning capability of Large Language Models (LLMs). More recently, CoT has been leveraged in Knowledge Distillation (KD) to transfer reasoning capability from a larger LLM to a smaller one. This paper examines the role of CoT in distilling the reasoning capability from larger LLMs to smaller LLMs using white-box KD, analysing its effectiveness in improving the performance of the distilled models for various natural language reasoning and understanding tasks. We conduct white-box KD experiments using LLMs from the Qwen and Llama2 families, employing CoT data from the CoT-Collection dataset. The distilled models are then evaluated on natural language reasoning and understanding tasks from the BIG-Bench-Hard (BBH) benchmark, which presents complex challenges for smaller LLMs. Experimental results demonstrate the role of CoT in improving white-box KD effectiveness, enabling the distilled models to achieve better average performance in natural language reasoning and understanding tasks from BBH.

**Link**: [arxiv](http://arxiv.org/abs/2511.05184v1),  [pdf](http://arxiv.org/pdf/2511.05184v1)

**Tags**: cs.CL 



### AI Through the Human Lens: Investigating Cognitive Theories in Machine   Psychology
**Authors**: Akash Kundu, Rishika Goswami

**Updated**: 2025-11-07T11:59:06Z

**Summary**: We investigate whether Large Language Models (LLMs) exhibit human-like cognitive patterns under four established frameworks from psychology: Thematic Apperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and Cognitive Dissonance. We evaluated several proprietary and open-source models using structured prompts and automated scoring. Our findings reveal that these models often produce coherent narratives, show susceptibility to positive framing, exhibit moral judgments aligned with Liberty/Oppression concerns, and demonstrate self-contradictions tempered by extensive rationalization. Such behaviors mirror human cognitive tendencies yet are shaped by their training data and alignment methods. We discuss the implications for AI transparency, ethical deployment, and future work that bridges cognitive psychology and AI safety

**Link**: [arxiv](http://arxiv.org/abs/2506.18156v2),  [pdf](http://arxiv.org/pdf/2506.18156v2)

**Tags**: cs.AI 



### Generating Software Architecture Description from Source Code using   Reverse Engineering and Large Language Model
**Authors**: Ahmad Hatahet, Christoph Knieke, Andreas Rausch

**Updated**: 2025-11-07T11:35:46Z

**Summary**: Software Architecture Descriptions (SADs) are essential for managing the inherent complexity of modern software systems. They enable high-level architectural reasoning, guide design decisions, and facilitate effective communication among diverse stakeholders. However, in practice, SADs are often missing, outdated, or poorly aligned with the system's actual implementation. Consequently, developers are compelled to derive architectural insights directly from source code-a time-intensive process that increases cognitive load, slows new developer onboarding, and contributes to the gradual degradation of clarity over the system's lifetime. To address these issues, we propose a semi-automated generation of SADs from source code by integrating reverse engineering (RE) techniques with a Large Language Model (LLM). Our approach recovers both static and behavioral architectural views by extracting a comprehensive component diagram, filtering architecturally significant elements (core components) via prompt engineering, and generating state machine diagrams to model component behavior based on underlying code logic with few-shots prompting. This resulting views representation offer a scalable and maintainable alternative to traditional manual architectural documentation. This methodology, demonstrated using C++ examples, highlights the potent capability of LLMs to: 1) abstract the component diagram, thereby reducing the reliance on human expert involvement, and 2) accurately represent complex software behaviors, especially when enriched with domain-specific knowledge through few-shot prompting. These findings suggest a viable path toward significantly reducing manual effort while enhancing system understanding and long-term maintainability.

**Link**: [arxiv](http://arxiv.org/abs/2511.05165v1),  [pdf](http://arxiv.org/pdf/2511.05165v1)

**Tags**: cs.SE cs.AI 



### Mind the Gap... or Not? How Translation Errors and Evaluation Details   Skew Multilingual Results
**Authors**: Jan-Thorsten Peter, David Vilar, Tobias Domhan, Dan Malkin, Markus Freitag

**Updated**: 2025-11-07T11:30:10Z

**Summary**: Most current large language models (LLMs) support a wide variety of languages in addition to English, including high-resource languages (e.g. German, Chinese, French), as well as low-resource ones (e.g. Swahili, Telugu). In addition they have also shown impressive capabilities in different domains, like coding, science and math. In this short paper, taking math as an example domain, we study the performance of different LLMs across languages. Experimental results show that there exists a non-negligible and consistent gap in the performance of the models across languages. Interestingly, and somewhat against expectations, the gap exists for both high- and low-resource languages. We hope that these results influence further research into cross-lingual capability generalization for next generation LLMs. If it weren't for the fact that they are false! By analyzing one of the standard multilingual math benchmarks (MGSM), we determine that several translation errors are present in the data. Furthermore, the lack of standardized answer extraction from LLM outputs further influences the final results. We propose a method for automatic quality assurance to address the first issue at scale, and give recommendations to address the second one. Combining these two approaches we show that the aforementioned language gap mostly disappears, leading to completely different conclusions from our research. We additionally release the corrected dataset to the community.

**Link**: [arxiv](http://arxiv.org/abs/2511.05162v1),  [pdf](http://arxiv.org/pdf/2511.05162v1)

**Tags**: cs.CL 



### Inverse Knowledge Search over Verifiable Reasoning: Synthesizing a   Scientific Encyclopedia from a Long Chains-of-Thought Knowledge Base
**Authors**: Yu Li, Yuan Huang, Tao Wang, Caiyu Fan, Xiansheng Cai, Sihan Hu, Xinzijian Liu, Cheng Shi, Mingjun Xu, Zhen Wang, Yan Wang, Xiangqi Jin, Tianhan Zhang, Linfeng Zhang, Lei Wang, Youjin Deng, Pan Zhang, Weijie Sun, Xingyu Li, Weinan E, Linfeng Zhang, Zhiyuan Yao, Kun Chen

**Updated**: 2025-11-07T11:11:26Z

**Summary**: Most scientific materials compress reasoning, presenting conclusions while omitting the derivational chains that justify them. This compression hinders verification by lacking explicit, step-wise justifications and inhibits cross-domain links by collapsing the very pathways that establish the logical and causal connections between concepts. We introduce a scalable framework that decompresses scientific reasoning, constructing a verifiable Long Chain-of-Thought (LCoT) knowledge base and projecting it into an emergent encyclopedia, SciencePedia. Our pipeline operationalizes an endpoint-driven, reductionist strategy: a Socratic agent, guided by a curriculum of around 200 courses, generates approximately 3 million first-principles questions. To ensure high fidelity, multiple independent solver models generate LCoTs, which are then rigorously filtered by prompt sanitization and cross-model answer consensus, retaining only those with verifiable endpoints. This verified corpus powers the Brainstorm Search Engine, which performs inverse knowledge search -- retrieving diverse, first-principles derivations that culminate in a target concept. This engine, in turn, feeds the Plato synthesizer, which narrates these verified chains into coherent articles. The initial SciencePedia comprises approximately 200,000 fine-grained entries spanning mathematics, physics, chemistry, biology, engineering, and computation. In evaluations across six disciplines, Plato-synthesized articles (conditioned on retrieved LCoTs) exhibit substantially higher knowledge-point density and significantly lower factual error rates than an equally-prompted baseline without retrieval (as judged by an external LLM). Built on this verifiable LCoT knowledge base, this reasoning-centric approach enables trustworthy, cross-domain scientific synthesis at scale and establishes the foundation for an ever-expanding encyclopedia.

**Link**: [arxiv](http://arxiv.org/abs/2510.26854v2),  [pdf](http://arxiv.org/pdf/2510.26854v2)

**Tags**: cs.AI cs.LG 



### From Linear Probing to Joint-Weighted Token Hierarchy: A Foundation   Model Bridging Global and Cellular Representations in Biomarker Detection
**Authors**: Jingsong Liu, Han Li, Nassir Navab, Peter J. Schüffler

**Updated**: 2025-11-07T11:05:36Z

**Summary**: AI-based biomarkers can infer molecular features directly from hematoxylin & eosin (H&E) slides, yet most pathology foundation models (PFMs) rely on global patch-level embeddings and overlook cell-level morphology. We present a PFM model, JWTH (Joint-Weighted Token Hierarchy), which integrates large-scale self-supervised pretraining with cell-centric post-tuning and attention pooling to fuse local and global tokens. Across four tasks involving four biomarkers and eight cohorts, JWTH achieves up to 8.3% higher balanced accuracy and 1.2% average improvement over prior PFMs, advancing interpretable and robust AI-based biomarker detection in digital pathology.

**Link**: [arxiv](http://arxiv.org/abs/2511.05150v1),  [pdf](http://arxiv.org/pdf/2511.05150v1)

**Tags**: cs.CV cs.AI 



### Inference with Mondrian Random Forests
**Authors**: Matias D. Cattaneo, Jason M. Klusowski, William G. Underwood

**Updated**: 2025-11-07T10:20:39Z

**Summary**: Random forests are popular methods for regression and classification analysis, and many different variants have been proposed in recent years. One interesting example is the Mondrian random forest, in which the underlying constituent trees are constructed via a Mondrian process. We give precise bias and variance characterizations, along with a Berry-Esseen-type central limit theorem, for the Mondrian random forest regression estimator. By combining these results with a carefully crafted debiasing approach and an accurate variance estimator, we present valid statistical inference methods for the unknown regression function. These methods come with explicitly characterized error bounds in terms of the sample size, tree complexity parameter, and number of trees in the forest, and include coverage error rates for feasible confidence interval estimators. Our debiasing procedure for the Mondrian random forest also allows it to achieve the minimax-optimal point estimation convergence rate in mean squared error for multivariate $\beta$-H\"older regression functions, for all $\beta > 0$, provided that the underlying tuning parameters are chosen appropriately. Efficient and implementable algorithms are devised for both batch and online learning settings, and we study the computational complexity of different Mondrian random forest implementations. Finally, simulations with synthetic data validate our theory and methodology, demonstrating their excellent finite-sample properties.

**Link**: [arxiv](http://arxiv.org/abs/2310.09702v4),  [pdf](http://arxiv.org/pdf/2310.09702v4)

**Tags**: math.ST stat.ME stat.ML stat.TH 62G08 (Primary), 62G05, 62G20 (Secondary) 



### Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities
**Authors**: Chung-En Sun, Xiaodong Liu, Weiwei Yang, Tsui-Wei Weng, Hao Cheng, Aidan San, Michel Galley, Jianfeng Gao

**Updated**: 2025-11-07T10:17:59Z

**Summary**: Recent research has shown that Large Language Models (LLMs) are vulnerable to automated jailbreak attacks, where adversarial suffixes crafted by algorithms appended to harmful queries bypass safety alignment and trigger unintended responses. Current methods for generating these suffixes are computationally expensive and have low Attack Success Rates (ASR), especially against well-aligned models like Llama2 and Llama3. To overcome these limitations, we introduce ADV-LLM, an iterative self-tuning process that crafts adversarial LLMs with enhanced jailbreak ability. Our framework significantly reduces the computational cost of generating adversarial suffixes while achieving nearly 100\% ASR on various open-source LLMs. Moreover, it exhibits strong attack transferability to closed-source models, achieving 99\% ASR on GPT-3.5 and 49\% ASR on GPT-4, despite being optimized solely on Llama3. Beyond improving jailbreak ability, ADV-LLM provides valuable insights for future safety alignment research through its ability to generate large datasets for studying LLM safety. Our code is available at: https://github.com/SunChungEn/ADV-LLM

**Link**: [arxiv](http://arxiv.org/abs/2410.18469v6),  [pdf](http://arxiv.org/pdf/2410.18469v6)

**Tags**: cs.CL cs.LG 



### Grounded in Reality: Learning and Deploying Proactive LLM from Offline   Logs
**Authors**: Fei Wei, Daoyuan Chen, Ce Wang, Yilun Huang, Yushuo Chen, Xuchen Pan, Yaliang Li, Bolin Ding

**Updated**: 2025-11-07T10:05:06Z

**Summary**: Large Language Models (LLMs) excel as passive responders, but teaching them to be proactive, goal-oriented partners, a critical capability in high-stakes domains, remains a major challenge. Current paradigms either myopically optimize single-turn attributes or rely on brittle, high-cost user simulators, creating a persistent ``reality gap''. To bridge this gap, we introduce \texttt{Learn-to-Ask}, a general, simulator-free framework for learning and deploying proactive dialogue agents \textit{directly from offline expert data}, bypassing the need to model complex user dynamics. Our key insight is to reframe the offline policy learning problem by leveraging the \textbf{observed future} of each expert trajectory. This allows us to infer a dense, turn-by-turn reward signal grounded in the expert's revealed strategy, decomposing the intractable long-horizon problem into a series of supervised learning tasks, and training a policy to output a structured \texttt{(action, state_assessment)} tuple, governing both \textbf{what to ask} and, crucially, \textbf{when to stop}. To ensure reward fidelity, our Automated Grader Calibration pipeline systematically purges noise from the LLM-based reward model with minimal human supervision. Empirically, we demonstrate the efficacy of \texttt{Learn-to-Ask} in a real-world medical dataset, using LLMs of varying sizes up to 32B. Our approach culminates in the successful deployment of LLMs into a live, large-scale online AI service. In rigorous in-house evaluations, our model was launched and achieved performance even superior to human experts, proving our framework's ability to translate offline data into tangible, real-world impact. We hope this work provides a practical and economically viable blueprint for transforming passive LLMs into proactive, goal-oriented LLM applications.

**Link**: [arxiv](http://arxiv.org/abs/2510.25441v2),  [pdf](http://arxiv.org/pdf/2510.25441v2)

**Tags**: cs.CL cs.AI 



### A Toolbox for Improving Evolutionary Prompt Search
**Authors**: Daniel Grießhaber, Maximilian Kimmich, Johannes Maucher, Ngoc Thang Vu

**Updated**: 2025-11-07T10:04:41Z

**Summary**: Evolutionary prompt optimization has demonstrated effectiveness in refining prompts for LLMs. However, existing approaches lack robust operators and efficient evaluation mechanisms. In this work, we propose several key improvements to evolutionary prompt optimization that can partially generalize to prompt optimization in general: 1) decomposing evolution into distinct steps to enhance the evolution and its control, 2) introducing an LLM-based judge to verify the evolutions, 3) integrating human feedback to refine the evolutionary operator, and 4) developing more efficient evaluation strategies that maintain performance while reducing computational overhead. Our approach improves both optimization quality and efficiency. We release our code, enabling prompt optimization on new tasks and facilitating further research in this area.

**Link**: [arxiv](http://arxiv.org/abs/2511.05120v1),  [pdf](http://arxiv.org/pdf/2511.05120v1)

**Tags**: cs.CL 



### Boardwalk: Towards a Framework for Creating Board Games with LLMs
**Authors**: Álvaro Guglielmin Becker, Gabriel Bauer de Oliveira, Lana Bertoldo Rossato, Anderson Rocha Tavares

**Updated**: 2025-11-07T10:02:55Z

**Summary**: Implementing board games in code can be a time-consuming task. However, Large Language Models (LLMs) have been proven effective at generating code for domain-specific tasks with simple contextual information. We aim to investigate whether LLMs can implement digital versions of board games from rules described in natural language. This would be a step towards an LLM-assisted framework for quick board game code generation. We expect to determine the main challenges for LLMs to implement the board games, and how different approaches and models compare to one another. We task three state-of-the-art LLMs (Claude, DeepSeek and ChatGPT) with coding a selection of 12 popular and obscure games in free-form and within Boardwalk, our proposed General Game Playing API. We anonymize the games and components to avoid evoking pre-trained LLM knowledge. The implementations are tested for playability and rule compliance. We evaluate success rate and common errors across LLMs and game popularity. Our approach proves viable, with the best performing model, Claude 3.7 Sonnet, yielding 55.6\% of games without any errors. While compliance with the API increases error frequency, the severity of errors is more significantly dependent on the LLM. We outline future steps for creating a framework to integrate this process, making the elaboration of board games more accessible.

**Link**: [arxiv](http://arxiv.org/abs/2508.16447v2),  [pdf](http://arxiv.org/pdf/2508.16447v2)

**Tags**: cs.LG 



### Usando LLMs para Programar Jogos de Tabuleiro e Variações
**Authors**: Álvaro Guglielmin Becker, Lana Bertoldo Rossato, Anderson Rocha Tavares

**Updated**: 2025-11-07T09:58:01Z

**Summary**: Creating programs to represent board games can be a time-consuming task. Large Language Models (LLMs) arise as appealing tools to expedite this process, given their capacity to efficiently generate code from simple contextual information. In this work, we propose a method to test how capable three LLMs (Claude, DeepSeek and ChatGPT) are at creating code for board games, as well as new variants of existing games.

**Link**: [arxiv](http://arxiv.org/abs/2511.05114v1),  [pdf](http://arxiv.org/pdf/2511.05114v1)

**Tags**: cs.LG 



### Spatially-resolved PAH_3.3 emission and stellar ages in ram pressure   stripped clumps at z~0.3
**Authors**: Pietro Benotto, Benedetta Vulcani, Peter J. Watson, Giulia Rodighiero, Bianca M. Poggianti, Marco Gullieuszik, Jacopo Fritz, Thomas S. -Y. Lai, Augusto E. Lassen, Matthew A. Malkan, Alessia Moretti

**Updated**: 2025-11-07T09:52:49Z

**Summary**: Ram pressure stripping (RPS) plays a crucial role in shaping galaxy evolution in dense environments, yet its impact on the molecular and dusty phases of the interstellar medium remains poorly understood. We present JWST/NIRCam 3.3 micrometres PAH emission maps for the nine most striking RPS galaxies in the Abell 2744 cluster at redshift z_cl = 0.306, tracing the effects of environmental processes on small dust grains. Exploiting multi-band JWST/NIRCam and HST photometry, we perform spatially-resolved UV to mid-infrared spectral energy distribution (SED) fitting, characterising stellar populations in both galactic disks and clumps detected in the stripped tails. We detect PAH_3.3 mission in eight of the nine galaxies at 5 sigma, with morphologies revealing disk truncation and elongation along the RPS direction. In three galaxies, PAH_3.3 emission is also found in star-forming clumps embedded in the stripped tails up to a distance of 40 kpc. Star formation rates inferred from PAH_3.3 emission agree with those derived from SED fitting averaged over the past 100 Myr within an intrinsic scatter of 0.4 dex, but the relation appears to be age dependent. The spatial correlation between PAH strength, stellar age, and SFR - consistent across disks and tails - demonstrates that PAH-carrying molecules can survive and be stripped by ram pressure. Finally, age gradients revealed by the SED fitting provide the first observational evidence outside the Local Universe for the fireball model of star formation in stripped clumps. This work represents the first detailed study of PAH emission in cluster galaxies, offering new insights into the fate of dust and star formation in extreme environments.

**Link**: [arxiv](http://arxiv.org/abs/2511.05113v1),  [pdf](http://arxiv.org/pdf/2511.05113v1)

**Tags**: astro-ph.GA 



### iTool: Reinforced Fine-Tuning with Dynamic Deficiency Calibration for   Advanced Tool Use
**Authors**: Yirong Zeng, Xiao Ding, Yuxian Wang, Weiwen Liu, Wu Ning, Yutai Hou, Xu Huang, Duyu Tang, Dandan Tu, Bing Qin, Ting Liu

**Updated**: 2025-11-07T09:32:05Z

**Summary**: Augmenting large language models (LLMs) with external tools is a promising approach to enhance their capabilities, especially for complex tasks. Synthesizing tool-use data through real-world simulations is an effective way to achieve this. However, our investigation reveals that training gains significantly decay as synthetic data increases. The model struggles to benefit from additional synthetic data, which fails to endow it with advanced tool-use capabilities in complex scenarios Moreover, we discovered that the above limitation usually manifests as a fragment deficiency (i.e., parameter errors) in response. To this end, we propose an iterative reinforced fine-tuning strategy designed to alleviate this limitation. This strategy involves: (1) enhancing the diversity of response for synthetic data through path exploration of Monte Carlo Tree Search. (2) iteratively pinpointing the model's deficiency by constructing fine-grained preference pairs, and then improving it by preference optimization algorithms for targeted improvement. The experiments show that our method achieves 13.11% better performance than the same-size base model. It achieves an improvement of 6.5% in complex scenarios compared to the baseline, and it also outperforms larger open-source and closed-source models.

**Link**: [arxiv](http://arxiv.org/abs/2501.09766v5),  [pdf](http://arxiv.org/pdf/2501.09766v5)

**Tags**: cs.CL cs.AI cs.LG 



### Affordance-based Robot Manipulation with Flow Matching
**Authors**: Fan Zhang, Michael Gienger

**Updated**: 2025-11-07T09:27:01Z

**Summary**: We present a framework for assistive robot manipulation, which focuses on two fundamental challenges: first, efficiently adapting large-scale models to downstream scene affordance understanding tasks, especially in daily living scenarios where gathering multi-task data involving humans requires strenuous effort; second, effectively learning robot action trajectories by grounding the visual affordance model. We tackle the first challenge by employing a parameter-efficient prompt tuning method that prepends learnable text prompts to the frozen vision model to predict manipulation affordances in multi-task scenarios. Then we propose to learn robot action trajectories guided by affordances in a supervised flow matching method. Flow matching represents a robot visuomotor policy as a conditional process of flowing random waypoints to desired robot action trajectories. Finally, we introduce a real-world dataset with 10 tasks across Activities of Daily Living to test our framework. Our extensive evaluation highlights that the proposed prompt tuning method for learning manipulation affordance achieves competitive performance and even outperforms some other finetuning protocols across data scales, while satisfying parameter efficiency. Learning multi-task robot action trajectories with flow matching leads to consistently favorable results in several robot manipulation benchmarks than some alternative behavior cloning methods. This includes more stable training and evaluation, and noticeably faster inference, while maintaining comparable generalization performance to diffusion policy, where flow matching performs marginally better in most cases. Our framework seamlessly unifies affordance learning and action generation with flow matching for robot manipulation.

**Link**: [arxiv](http://arxiv.org/abs/2409.01083v5),  [pdf](http://arxiv.org/pdf/2409.01083v5)

**Tags**: cs.RO cs.AI 



### FM4Com: Foundation Model for Scene-Adaptive Communication Strategy   Optimization
**Authors**: Zhaoyang Li, Shangzhuo Xie, Qianqian Yang

**Updated**: 2025-11-07T09:21:22Z

**Summary**: The emergence of sixth-generation (6G) networks heralds an intelligent communication ecosystem driven by AI-native air interfaces. However, current physical-layer designs-typically following modular and isolated optimization paradigms-fail to achieve global end-to-end optimality due to neglected inter-module dependencies. Although large language models (LLMs) have recently been applied to communication tasks such as beam prediction and resource allocation, existing studies remain limited to single-task or single-modality scenarios and lack the ability to jointly reason over communication states and user intents for personalized strategy adaptation. To address these limitations, this paper proposes a novel multimodal communication decision-making model based on reinforcement learning. The proposed model semantically aligns channel state information (CSI) and textual user instructions, enabling comprehensive understanding of both physical-layer conditions and communication intents. It then generates physically realizable, user-customized link construction strategies that dynamically adapt to changing environments and preference tendencies. A two-stage reinforcement learning framework is employed: the first stage expands the experience pool via heuristic exploration and behavior cloning to obtain a near-optimal initialization, while the second stage fine-tunes the model through multi-objective reinforcement learning considering bit error rate, throughput, and complexity. Experimental results demonstrate that the proposed model significantly outperforms conventional planning-based algorithms under challenging channel conditions, achieving robust, efficient, and personalized 6G link construction.

**Link**: [arxiv](http://arxiv.org/abs/2511.05094v1),  [pdf](http://arxiv.org/pdf/2511.05094v1)

**Tags**: cs.HC 



### LLM-Based Emulation of the Radio Resource Control Layer: Towards   AI-Native RAN Protocols
**Authors**: Ziming Liu, Bryan Liu, Alvaro Valcarce, Xiaoli Chu

**Updated**: 2025-11-07T09:20:34Z

**Summary**: Integrating Large AI Models (LAMs) into 6G mobile networks is a key enabler of the AI-Native Air Interface (AI-AI), where protocol intelligence must scale beyond handcrafted logic. This paper presents, to our knowledge, the first standards-compliant emulation of the Radio Resource Control (RRC) layer using a decoder-only LAM (LLAMA-class) fine-tuned with Low-Rank Adaptation (LoRA) on a multi-vendor corpus of real-world traces spanning both 5G and 4G systems. We treat RRC as a domain-specific language and construct a segmentation-safe, question--answer (Question-and-Answer (QA)) dataset that preserves Abstract Syntax Notation (ASN.1) structure through linearization prior to Byte Pair Encoding (BPE) tokenization. The proposed approach combines parameter-efficient adaptation with schema-bounded prompting to ensure syntactic and procedural fidelity. Evaluation introduces a standards-aware triad -- ASN.1 conformance, field-level coverage analysis, and uplink-to-downlink state-machine checks -- alongside semantic similarity and latency profiling across 120 configurations. On 30k 5G request--response pairs plus an additional 4.8k QA turns from 4G sessions, our 8B model achieves a median cosine similarity of 0.97, a 61% relative gain over a zero-shot baseline, while sustaining high conformance rates. These results demonstrate that LAMs, when augmented with protocol-aware reasoning, can directly orchestrate control-plane procedures, laying the foundation for the future Artificial Intelligence (AI)-native Radio Access Network (RAN).

**Link**: [arxiv](http://arxiv.org/abs/2505.16821v3),  [pdf](http://arxiv.org/pdf/2505.16821v3)

**Tags**: cs.NI cs.LG eess.SP 



### Where Is Self-admitted Code Generated by Large Language Models on   GitHub?
**Authors**: Xiao Yu, Lei Liu, Xing Hu, Jin Liu, Xin Xia

**Updated**: 2025-11-07T09:09:43Z

**Summary**: The increasing use of Large Language Models (LLMs) in software development has garnered significant attention from researchers evaluating the capabilities and limitations of LLMs for code generation. However, much of the research focuses on controlled datasets such as HumanEval, which do not adequately capture the characteristics of LLM-generated code in real-world development scenarios. To address this gap, our study investigates self-admitted code generated by LLMs on GitHub, specifically focusing on instances where developers in projects with over five stars acknowledge the use of LLMs to generate code through code comments. Our findings reveal several key insights: (1) ChatGPT and Copilot dominate code generation, with minimal contributions from other LLMs. (2) Projects containing ChatGPT/Copilot-generated code appears in small/medium-sized projects led by small teams, which are continuously evolving. (3) ChatGPT/Copilot-generated code generally is a minor project portion, primarily generating short/moderate-length, low-complexity snippets (e.g., algorithms and data structures code; text processing code). (4) ChatGPT/Copilot-generated code generally undergoes minimal modifications, with bug-related changes ranging from 4% to 12%. (5) Most code comments only state LLM use, while few include details like prompts, human edits, or code testing status. Based on these findings, we discuss the implications for researchers and practitioners.

**Link**: [arxiv](http://arxiv.org/abs/2406.19544v4),  [pdf](http://arxiv.org/pdf/2406.19544v4)

**Tags**: cs.SE 



### CompressionAttack: Exploiting Prompt Compression as a New Attack Surface   in LLM-Powered Agents
**Authors**: Zesen Liu, Zhixiang Zhang, Yuchong Xie, Dongdong She

**Updated**: 2025-11-07T09:01:26Z

**Summary**: LLM-powered agents often use prompt compression to reduce inference costs, but this introduces a new security risk. Compression modules, which are optimized for efficiency rather than safety, can be manipulated by adversarial inputs, causing semantic drift and altering LLM behavior. This work identifies prompt compression as a novel attack surface and presents CompressionAttack, the first framework to exploit it. CompressionAttack includes two strategies: HardCom, which uses discrete adversarial edits for hard compression, and SoftCom, which performs latent-space perturbations for soft compression. Experiments on multiple LLMs show up to 80% attack success and 98% preference flips, while remaining highly stealthy and transferable. Case studies in VSCode Cline and Ollama confirm real-world impact, and current defenses prove ineffective, highlighting the need for stronger protections.

**Link**: [arxiv](http://arxiv.org/abs/2510.22963v2),  [pdf](http://arxiv.org/pdf/2510.22963v2)

**Tags**: cs.CR cs.AI 



## Keyword: LLM Deployment 
 ### A Metamorphic Testing Perspective on Knowledge Distillation for Language   Models of Code: Does the Student Deeply Mimic the Teacher?
**Authors**: Md. Abdul Awal, Mrigank Rochan, Chanchal K. Roy

**Updated**: 2025-11-07T18:38:54Z

**Summary**: Transformer-based language models of code have achieved state-of-the-art performance across a wide range of software analytics tasks, but their practical deployment remains limited due to high computational costs, slow inference speeds, and significant environmental impact. To address these challenges, recent research has increasingly explored knowledge distillation as a method for compressing a large language model of code (the teacher) into a smaller model (the student) while maintaining performance. However, the degree to which a student model deeply mimics the predictive behavior and internal representations of its teacher remains largely unexplored, as current accuracy-based evaluation provides only a surface-level view of model quality and often fails to capture more profound discrepancies in behavioral fidelity between the teacher and student models. To address this gap, we empirically show that the student model often fails to deeply mimic the teacher model, resulting in up to 285% greater performance drop under adversarial attacks, which is not captured by traditional accuracy-based evaluation. Therefore, we propose MetaCompress, a metamorphic testing framework that systematically evaluates behavioral fidelity by comparing the outputs of teacher and student models under a set of behavior-preserving metamorphic relations. We evaluate MetaCompress on two widely studied tasks, using compressed versions of popular language models of code, obtained via three different knowledge distillation techniques: Compressor, AVATAR, and MORPH. The results show that MetaCompress identifies up to 62% behavioral discrepancies in student models, underscoring the need for behavioral fidelity evaluation within the knowledge distillation pipeline and establishing MetaCompress as a practical framework for testing compressed language models of code derived through knowledge distillation.

**Link**: [arxiv](http://arxiv.org/abs/2511.05476v1),  [pdf](http://arxiv.org/pdf/2511.05476v1)

**Tags**: cs.SE cs.LG 



### Enterprise Deep Research: Steerable Multi-Agent Deep Research for   Enterprise Analytics
**Authors**: Akshara Prabhakar, Roshan Ram, Zixiang Chen, Silvio Savarese, Frank Wang, Caiming Xiong, Huan Wang, Weiran Yao

**Updated**: 2025-11-07T18:10:23Z

**Summary**: As information grows exponentially, enterprises face increasing pressure to transform unstructured data into coherent, actionable insights. While autonomous agents show promise, they often struggle with domain-specific nuances, intent alignment, and enterprise integration. We present Enterprise Deep Research (EDR), a multi-agent system that integrates (1) a Master Planning Agent for adaptive query decomposition, (2) four specialized search agents (General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool ecosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a Visualization Agent for data-driven insights, and (5) a reflection mechanism that detects knowledge gaps and updates research direction with optional human-in-the-loop steering guidance. These components enable automated report generation, real-time streaming, and seamless enterprise deployment, as validated on internal datasets. On open-ended benchmarks including DeepResearch Bench and DeepConsult, EDR outperforms state-of-the-art agentic systems without any human steering. We release the EDR framework and benchmark trajectories to advance research on multi-agent reasoning applications.   Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and Dataset at https://huggingface.co/datasets/Salesforce/EDR-200

**Link**: [arxiv](http://arxiv.org/abs/2510.17797v2),  [pdf](http://arxiv.org/pdf/2510.17797v2)

**Tags**: cs.CL cs.AI 



### What is the Return on Investment of Digital Engineering for Complex   Systems Development? Findings from a Mixed-Methods Study on the   Post-production Design Change Process of Navy Assets
**Authors**: Jannatul Shefa, Taylan G. Topcu

**Updated**: 2025-11-07T18:05:11Z

**Summary**: Complex engineered systems routinely face schedule and cost overruns, along with poor post-deployment performance. Championed by both INCOSE and the U.S. Department of Defense (DoD), the systems engineering (SE) community has increasingly looked to Digital Engineering (DE) as a potential remedy. Despite this growing advocacy, most of DE's purported benefits remain anecdotal, and its return on investment (ROI) remains poorly understood. This research presents findings from a case study on a Navy SE team responsible for the preliminary design phase of post-production design change projects for Navy assets. Using a mixed-methods approach, we document why complex system sustainment projects are routinely late, where and to what extent schedule slips arise, and how a DE transformation could improve schedule adherence. This study makes three contributions. First, it identifies four archetypical inefficiency modes that drive schedule overruns and explains how these mechanisms unfold in their organizational context. Second, it quantifies the magnitude and variation of schedule slips. Third, it creates a hypothetical digitally transformed version of the current process, aligned with DoD DE policy, and compares it to the current state to estimate potential schedule gains. Our findings suggest that a DE transformation could reduce the median project duration by 50.1% and reduce the standard deviation by 41.5%, leading to faster and more predictable timelines. However, the observed gains are not uniform across task categories. Overall, this study provides initial quantitative evidence of DE's potential ROI and its value in improving the efficiency and predictability of complex system sustainment projects.

**Link**: [arxiv](http://arxiv.org/abs/2511.00077v2),  [pdf](http://arxiv.org/pdf/2511.00077v2)

**Tags**: cs.CY 



### SWE-Compass: Towards Unified Evaluation of Agentic Coding Abilities for   Large Language Models
**Authors**: Jingxuan Xu, Ken Deng, Weihao Li, Songwei Yu, Huaixi Tang, Haoyang Huang, Zhiyi Lai, Zizheng Zhan, Yanan Wu, Chenchen Zhang, Kepeng Lei, Yifan Yao, Xinping Lei, Wenqiang Zhu, Zongxian Feng, Han Li, Junqi Xiong, Dailin Li, Zuchen Gao, Kun Wu, Wen Xiang, Ziqi Zhan, Yuanxing Zhang, Wuxuan Gong, Ziyuan Gao, Guanxiang Wang, Yirong Xue, Xiaojiang Zhang, Jinghui Wang, Huiming Wang, Wenhao Zhuang, Zhaoxiang Zhang, Yuqun Zhang, Haotian Zhang, Bin Chen, Jiaheng Liu

**Updated**: 2025-11-07T18:01:32Z

**Summary**: Evaluating large language models (LLMs) for software engineering has been limited by narrow task coverage, language bias, and insufficient alignment with real-world developer workflows. Existing benchmarks often focus on algorithmic problems or Python-centric bug fixing, leaving critical dimensions of software engineering underexplored. To address these gaps, we introduce SWE-Compass1, a comprehensive benchmark that unifies heterogeneous code-related evaluations into a structured and production-aligned framework. SWE-Compass spans 8 task types, 8 programming scenarios, and 10 programming languages, with 2000 high-quality instances curated from authentic GitHub pull requests and refined through systematic filtering and validation. We benchmark ten state-of-the-art LLMs under two agentic frameworks, SWE-Agent and Claude Code, revealing a clear hierarchy of difficulty across task types, languages, and scenarios. Moreover, by aligning evaluation with real-world developer practices, SWE-Compass provides a rigorous and reproducible foundation for diagnosing and advancing agentic coding capabilities in large language models.

**Link**: [arxiv](http://arxiv.org/abs/2511.05459v1),  [pdf](http://arxiv.org/pdf/2511.05459v1)

**Tags**: cs.SE cs.AI 



### P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication
**Authors**: Sneha Oram, Pushpak Bhattacharyya

**Updated**: 2025-11-07T17:49:33Z

**Summary**: Although explainability and interpretability have received significant attention in artificial intelligence (AI) and natural language processing (NLP) for mental health, reasoning has not been examined in the same depth. Addressing this gap is essential to bridge NLP and mental health through interpretable and reasoning-capable AI systems. To this end, we investigate the pragmatic reasoning capability of large-language models (LLMs) in the mental health domain. We introduce PRiMH dataset, and propose pragmatic reasoning tasks in mental health with pragmatic implicature and presupposition phenomena. In particular, we formulate two tasks in implicature and one task in presupposition. To benchmark the dataset and the tasks presented, we consider four models: Llama3.1, Mistral, MentaLLaMa, and Qwen. The results of the experiments suggest that Mistral and Qwen show substantial reasoning abilities in the domain. Subsequently, we study the behavior of MentaLLaMA on the proposed reasoning tasks with the rollout attention mechanism. In addition, we also propose three StiPRompts to study the stigma around mental health with the state-of-the-art LLMs, GPT4o-mini, Deepseek-chat, and Claude-3.5-haiku. Our evaluated findings show that Claude-3.5-haiku deals with stigma more responsibly compared to the other two LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2507.23247v2),  [pdf](http://arxiv.org/pdf/2507.23247v2)

**Tags**: cs.CL 



### Flashlight: PyTorch Compiler Extensions to Accelerate Attention Variants
**Authors**: Bozhi You, Irene Wang, Zelal Su Mustafaoglu, Abhinav Jangda, Angélica Moreira, Roshan Dathathri, Divya Mahajan, Keshav Pingali

**Updated**: 2025-11-07T17:26:02Z

**Summary**: Attention is a fundamental building block of large language models (LLMs), so there have been many efforts to implement it efficiently. For example, FlashAttention leverages tiling and kernel fusion to optimize attention. Recently, a number of variants of attention have been introduced to enhance model quality or efficiency. Supporting them efficiently remains difficult since they usually require specialized kernels or hand-tuned implementations. FlexAttention recently addressed part of this gap by using static programming templates to support FlashAttention-like kernels for a subset of attention variants.   In this paper, we introduce Flashlight, a compiler-native framework within the PyTorch ecosystem that automatically generates fused, FlashAttention-style kernels for arbitrary attention-based programs, without relying on static templates or predefined kernel specializations. Flashlight leverages PyTorch's compilation workflow to fuse and tile attention computations transparently, enabling efficient execution for diverse attention patterns. Not only does it support all variants expressible in the FlexAttention model but it also handles more general, data-dependent attention formulations that are beyond the capabilities of FlexAttention.   Our results show that Flashlight produces kernels with competitive or superior performance to FlexAttention, while offering the flexibility of native PyTorch code, enabling developers to rapidly explore new attention models without sacrificing performance.

**Link**: [arxiv](http://arxiv.org/abs/2511.02043v3),  [pdf](http://arxiv.org/pdf/2511.02043v3)

**Tags**: cs.LG cs.PF 



### MACO: A Multi-Agent LLM-Based Hardware/Software Co-Design Framework for   CGRAs
**Authors**: Zesong Jiang, Yuqi Sun, Qing Zhong, Mahathi Krishna, Deepak Patil, Cheng Tan, Sriram Krishnamoorthy, Jeff Zhang

**Updated**: 2025-11-07T17:21:08Z

**Summary**: Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing architecture that can deliver high-performance, energy-efficient acceleration across diverse domains. By supporting reconfiguration at the functional unit level, CGRAs efficiently adapt to varying computational patterns and optimize resource utilization. However, designing CGRAs is highly challenging due to the vast design space, independent architectural parameters, and the time-consuming nature of manual design. Fortunately, the rapid advancement of large language models (LLMs) presents new opportunities to automate this process.   In this work, we propose MACO-- an open-source multi-agent LLM-based framework for Hardware/Software (HW/SW) co-design of CGRAs. The framework employs LLM reasoning to generate CGRAs across four stages: HW/SW co-design, Design error correction, Best design selection, and Evaluation & Feedback. Furthermore, MACO iteratively optimizes the generated CGRAs, leveraging agent reasoning and feedback to achieve higher PPA (that is, power, performance, and area) design points for a given domain. In addition, we introduce an LLM self-learning mechanism that employs LLM-driven decision making to select the optimal CGRA to accelerate the design process.   We evaluate the framework with state-of-the-art LLM-based methods and manual CGRA design, in terms of performance, power consumption, and area. Experimental results show that MACO efficiently generates high-quality CGRA architectures, significantly reducing manual design effort and demonstrating the potential of our framework for real-world CGRA design.

**Link**: [arxiv](http://arxiv.org/abs/2509.13557v5),  [pdf](http://arxiv.org/pdf/2509.13557v5)

**Tags**: cs.AR 



### Large language models as uncertainty-calibrated optimizers for   experimental discovery
**Authors**: Bojana Ranković, Ryan-Rhys Griffiths, Philippe Schwaller

**Updated**: 2025-11-07T17:11:12Z

**Summary**: Scientific discovery increasingly depends on efficient experimental optimization to navigate vast design spaces under time and resource constraints. Traditional approaches often require extensive domain expertise and feature engineering. While large language models, with their vast scientific knowledge, circumvent the feature engineering limitations, they lack the calibrated uncertainty estimates required for high-stakes decision making. Hence, current optimization methods force a choice between domain knowledge and reliability, with no principled approach that affords both. In this work, we show that training language models through the uncertainty-aware objectives of traditional optimization methods enables their use as reliable optimizers guided by natural language. By teaching LLMs from experimental outcomes under uncertainty, we transform their overconfidence from a fundamental limitation into a precise calibration mechanism. Applied to Buchwald-Hartwig reactions, a cornerstone of pharmaceutical synthesis, our method nearly doubles the discovery rate of high-yielding reaction conditions, from 24% to 43% in 50 experimental iterations starting from 10 unsuccessful conditions. Across 19 diverse optimization problems spanning organic synthesis, materials science and catalysis, process chemistry, and molecular design, our approach ranks first on average, establishing a new paradigm for reliable, uncertainty-guided optimization with LLMs. Our approach can accelerate discovery by lowering the barrier to using powerful optimization methods, replacing the need for domain-specific feature engineering with more accessible natural language interfaces. These findings highlight that ensuring reliability through principled uncertainty quantification is critical for realizing the full potential of AI-guided experimentation.

**Link**: [arxiv](http://arxiv.org/abs/2504.06265v3),  [pdf](http://arxiv.org/pdf/2504.06265v3)

**Tags**: cs.LG cs.AI 



### Scanning the IPv6 Internet Using Subnet-Router Anycast Probing
**Authors**: Maynard Koch, Raphael Hiesgen, Marcin Nawrocki, Thomas C. Schmidt, Matthias Wählisch

**Updated**: 2025-11-07T16:56:37Z

**Summary**: Identifying active IPv6 addresses is challenging. Various methods emerged to master the measurement challenge in this huge address space, including hitlists, new probing techniques, and AI-generated target lists. In this paper, we apply active Subnet-Router anycast (SRA) probing, a commonly unused method to explore the IPv6 address space. We compare our results with lists of active IPv6 nodes obtained from prior methods and with random probing. Our findings indicate that probing an SRA address reveals on average 10% more router IP addresses than random probing and is far less affected by ICMP rate limiting. Compared to targeting router addresses directly, SRA probing discovers 80% more addresses. We conclude that SRA probing is an important addition to the IPv6 measurement toolbox and may improve the stability of results significantly. We also find evidence that some active scans can cause harmful conditions in current IPv6 deployments, which we started to fix in collaboration with network operators.

**Link**: [arxiv](http://arxiv.org/abs/2511.05423v1),  [pdf](http://arxiv.org/pdf/2511.05423v1)

**Tags**: cs.NI 



### Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided   Medical Image Editing
**Authors**: Zhihui Chen, Mengling Feng

**Updated**: 2025-11-07T16:53:02Z

**Summary**: Medical image editing has emerged as a pivotal technology with broad applications in data augmentation, model interpretability, medical education, and treatment simulation. However, the lack of large-scale, high-quality, and openly accessible datasets tailored for medical contexts with strict anatomical and clinical constraints has significantly hindered progress in this domain. To bridge this gap, we introduce Med-Banana-50K, a comprehensive dataset of over 50k medically curated image edits spanning chest X-ray, brain MRI, and fundus photography across 23 diseases. Each sample supports bidirectional lesion editing (addition and removal) and is constructed using Gemini-2.5-Flash-Image based on real clinical images. A key differentiator of our dataset is the medically grounded quality control protocol: we employ an LLM-as-Judge evaluation framework with criteria such as instruction compliance, structural plausibility, image realism, and fidelity preservation, alongside iterative refinement over up to five rounds. Additionally, Med-Banana-50K includes around 37,000 failed editing attempts with full evaluation logs to support preference learning and alignment research. By offering a large-scale, medically rigorous, and fully documented resource, Med-Banana-50K establishes a critical foundation for developing and evaluating reliable medical image editing systems. Our dataset and code are publicly available. [https://github.com/richardChenzhihui/med-banana-50k].

**Link**: [arxiv](http://arxiv.org/abs/2511.00801v3),  [pdf](http://arxiv.org/pdf/2511.00801v3)

**Tags**: cs.CV cs.MM 



### Inference-Time Hyper-Scaling with KV Cache Compression
**Authors**: Adrian Łańcucki, Konrad Staniszewski, Piotr Nawrot, Edoardo M. Ponti

**Updated**: 2025-11-07T16:42:30Z

**Summary**: Inference-time scaling trades efficiency for increased reasoning accuracy by generating longer or more parallel sequences. However, in Transformer LLMs, generation cost is bottlenecked by the size of the key-value (KV) cache, rather than the number of generated tokens. Hence, we explore inference-time hyper-scaling: by compressing the KV cache, we can generate more tokens within the same compute budget and further improve the accuracy of scaled inference. The success of this approach, however, hinges on the ability of compression methods to preserve accuracy even at high compression ratios. To make hyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a novel method for sparsifying KV caches that only requires 1K training steps to achieve 8$\times$ compression, while maintaining better accuracy than training-free sparse attention. Instead of prematurely discarding cached tokens, DMS delays token eviction, implicitly merging representations and preserving critical information. We demonstrate the effectiveness of inference-time hyper-scaling with DMS on multiple families of LLMs, showing that it boosts accuracy for comparable inference latency and memory load. For instance, we enhance Qwen-R1 32B by 12.0 points on AIME 24, 8.6 on GPQA, and 9.7 on LiveCodeBench on average for an equivalent number of memory reads.

**Link**: [arxiv](http://arxiv.org/abs/2506.05345v2),  [pdf](http://arxiv.org/pdf/2506.05345v2)

**Tags**: cs.LG cs.CL 



### GUARD: Role-playing to Generate Natural-language Jailbreakings to Test   Guideline Adherence of Large Language Models
**Authors**: Haibo Jin, Ruoxi Chen, Peiyan Zhang, Andy Zhou, Haohan Wang

**Updated**: 2025-11-07T16:36:35Z

**Summary**: The discovery of "jailbreaks" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effective in inducing LLMs to generate unethical or guideline-violating responses. In addition, we also pioneer a setting in our system that will automatically follow the government-issued guidelines to generate jailbreaks to test whether LLMs follow the guidelines accordingly. We refer to our system as GUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have empirically validated the effectiveness of GUARD on three cutting-edge open-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a widely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the realm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing GUARD's versatility and contributing valuable insights for the development of safer, more reliable LLM-based applications across diverse modalities.

**Link**: [arxiv](http://arxiv.org/abs/2402.03299v6),  [pdf](http://arxiv.org/pdf/2402.03299v6)

**Tags**: cs.LG cs.CL cs.CV 



### Steering Language Models with Weight Arithmetic
**Authors**: Constanza Fierro, Fabien Roger

**Updated**: 2025-11-07T16:34:16Z

**Summary**: Providing high-quality feedback to Large Language Models (LLMs) on a diverse training distribution can be difficult and expensive, and providing feedback only on a narrow distribution can result in unintended generalizations. To better leverage narrow training data, we propose contrastive weight steering, a simple post-training method that edits the model parameters using weight arithmetic. We isolate a behavior direction in weight-space by subtracting the weight deltas from two small fine-tunes -- one that induces the desired behavior and another that induces its opposite -- and then add or remove this direction to modify the model's weights. We apply this technique to mitigate sycophancy and induce misalignment, and find that weight steering often generalizes further than activation steering, achieving stronger out-of-distribution behavioral control before degrading general capabilities. We also show that, in the context of task-specific fine-tuning, weight steering can partially mitigate undesired behavioral drift: it can reduce sycophancy and under-refusals introduced during fine-tuning while preserving task performance gains. Finally, we provide preliminary evidence that emergent misalignment can be detected by measuring the similarity between fine-tuning updates and an "evil" weight direction, suggesting that it may be possible to monitor the evolution of weights during training and detect rare misaligned behaviors that never manifest during training or evaluations.

**Link**: [arxiv](http://arxiv.org/abs/2511.05408v1),  [pdf](http://arxiv.org/pdf/2511.05408v1)

**Tags**: cs.CL cs.LG 



### Large Language Models for Explainable Threat Intelligence
**Authors**: Tiago Dinis, Miguel Correia, Roger Tavares

**Updated**: 2025-11-07T16:32:41Z

**Summary**: As cyber threats continue to grow in complexity, traditional security mechanisms struggle to keep up. Large language models (LLMs) offer significant potential in cybersecurity due to their advanced capabilities in text processing and generation. This paper explores the use of LLMs with retrieval-augmented generation (RAG) to obtain threat intelligence by combining real-time information retrieval with domain-specific data. The proposed system, RAGRecon, uses a LLM with RAG to answer questions about cybersecurity threats. Moreover, it makes this form of Artificial Intelligence (AI) explainable by generating and visually presenting to the user a knowledge graph for every reply. This increases the transparency and interpretability of the reasoning of the model, allowing analysts to better understand the connections made by the system based on the context recovered by the RAG system. We evaluated RAGRecon experimentally with two datasets and seven different LLMs and the responses matched the reference responses more than 91% of the time for the best combinations.

**Link**: [arxiv](http://arxiv.org/abs/2511.05406v1),  [pdf](http://arxiv.org/pdf/2511.05406v1)

**Tags**: cs.CL 



### Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement   Learning with Online Interaction
**Authors**: Yiting He, Zhishuai Liu, Weixin Wang, Pan Xu

**Updated**: 2025-11-07T16:24:22Z

**Summary**: Off-dynamics reinforcement learning (RL), where training and deployment transition dynamics are different, can be formulated as learning in a robust Markov decision process (RMDP) where uncertainties in transition dynamics are imposed. Existing literature mostly assumes access to generative models allowing arbitrary state-action queries or pre-collected datasets with a good state coverage of the deployment environment, bypassing the challenge of exploration. In this work, we study a more realistic and challenging setting where the agent is limited to online interaction with the training environment. To capture the intrinsic difficulty of exploration in online RMDPs, we introduce the supremal visitation ratio, a novel quantity that measures the mismatch between the training dynamics and the deployment dynamics. We show that if this ratio is unbounded, online learning becomes exponentially hard. We propose the first computationally efficient algorithm that achieves sublinear regret in online RMDPs with $f$-divergence based transition uncertainties. We also establish matching regret lower bounds, demonstrating that our algorithm achieves optimal dependence on both the supremal visitation ratio and the number of interaction episodes. Finally, we validate our theoretical results through comprehensive numerical experiments.

**Link**: [arxiv](http://arxiv.org/abs/2511.05396v1),  [pdf](http://arxiv.org/pdf/2511.05396v1)

**Tags**: cs.LG cs.AI cs.RO stat.ML 



### Are Humans as Brittle as Large Language Models?
**Authors**: Jiahui Li, Sean Papay, Roman Klinger

**Updated**: 2025-11-07T16:21:31Z

**Summary**: The output of large language models (LLMs) is unstable, due both to non-determinism of the decoding process as well as to prompt brittleness. While the intrinsic non-determinism of LLM generation may mimic existing uncertainty in human annotations through distributional shifts in outputs, it is largely assumed, yet unexplored, that the prompt brittleness effect is unique to LLMs. This raises the question: do human annotators show similar sensitivity to prompt changes? If so, should prompt brittleness in LLMs be considered problematic? One may alternatively hypothesize that prompt brittleness correctly reflects human annotation variances. To fill this research gap, we systematically compare the effects of prompt modifications on LLMs and identical instruction modifications for human annotators, focusing on the question of whether humans are similarly sensitive to prompt perturbations. To study this, we prompt both humans and LLMs for a set of text classification tasks conditioned on prompt variations. Our findings indicate that both humans and LLMs exhibit increased brittleness in response to specific types of prompt modifications, particularly those involving the substitution of alternative label sets or label formats. However, the distribution of human judgments is less affected by typographical errors and reversed label order than that of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2509.07869v2),  [pdf](http://arxiv.org/pdf/2509.07869v2)

**Tags**: cs.CL cs.HC 



### XBreaking: Understanding how LLMs security alignment can be broken
**Authors**: Marco Arazzi, Vignesh Kumar Kembu, Antonino Nocera, Vinod P

**Updated**: 2025-11-07T16:21:06Z

**Summary**: Large Language Models are fundamental actors in the modern IT landscape dominated by AI solutions. However, security threats associated with them might prevent their reliable adoption in critical application scenarios such as government organizations and medical institutions. For this reason, commercial LLMs typically undergo a sophisticated censoring mechanism to eliminate any harmful output they could possibly produce. These mechanisms maintain the integrity of LLM alignment by guaranteeing that the models respond safely and ethically. In response to this, attacks on LLMs are a significant threat to such protections, and many previous approaches have already demonstrated their effectiveness across diverse domains. Existing LLM attacks mostly adopt a generate-and-test strategy to craft malicious input. To improve the comprehension of censoring mechanisms and design a targeted attack, we propose an Explainable-AI solution that comparatively analyzes the behavior of censored and uncensored models to derive unique exploitable alignment patterns. Then, we propose XBreaking, a novel approach that exploits these unique patterns to break the security and alignment constraints of LLMs by targeted noise injection. Our thorough experimental campaign returns important insights about the censoring mechanisms and demonstrates the effectiveness and performance of our approach.

**Link**: [arxiv](http://arxiv.org/abs/2504.21700v3),  [pdf](http://arxiv.org/pdf/2504.21700v3)

**Tags**: cs.CR cs.AI cs.LG 



### Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via   Self-Play and Reinforcement Learning
**Authors**: Richard Dewey, Janos Botyanszki, Ciamac C. Moallemi, Andrew T. Zheng

**Updated**: 2025-11-07T16:11:08Z

**Summary**: AI researchers have long focused on poker-like games as a testbed for environments characterized by multi-player dynamics, imperfect information, and reasoning under uncertainty. While recent breakthroughs have matched elite human play at no-limit Texas hold'em, the multi-player dynamics are subdued: most hands converge quickly with only two players engaged through multiple rounds of bidding. In this paper, we present Solly, the first AI agent to achieve elite human play in reduced-format Liar's Poker, a game characterized by extensive multi-player engagement. We trained Solly using self-play with a model-free, actor-critic, deep reinforcement learning algorithm. Solly played at an elite human level as measured by win rate (won over 50% of hands) and equity (money won) in heads-up and multi-player Liar's Poker. Solly also outperformed large language models (LLMs), including those with reasoning abilities, on the same metrics. Solly developed novel bidding strategies, randomized play effectively, and was not easily exploitable by world-class human players.

**Link**: [arxiv](http://arxiv.org/abs/2511.03724v2),  [pdf](http://arxiv.org/pdf/2511.03724v2)

**Tags**: cs.AI cs.MA 



### Policy-as-Prompt: Turning AI Governance Rules into Guardrails for AI   Agents
**Authors**: Gauri Kholkar, Ratinder Ahuja

**Updated**: 2025-11-07T16:08:47Z

**Summary**: As autonomous AI agents are used in regulated and safety-critical settings, organizations need effective ways to turn policy into enforceable controls. We introduce a regulatory machine learning framework that converts unstructured design artifacts (like PRDs, TDDs, and code) into verifiable runtime guardrails. Our Policy as Prompt method reads these documents and risk controls to build a source-linked policy tree. This tree is then compiled into lightweight, prompt-based classifiers for real-time runtime monitoring. The system is built to enforce least privilege and data minimization. For conformity assessment, it provides complete provenance, traceability, and audit logging, all integrated with a human-in-the-loop review process. Evaluations show our system reduces prompt-injection risk, blocks out-of-scope requests, and limits toxic outputs. It also generates auditable rationales aligned with AI governance frameworks. By treating policies as executable prompts (a policy-as-code for agents), this approach enables secure-by-design deployment, continuous compliance, and scalable AI safety and AI security assurance for regulatable ML.

**Link**: [arxiv](http://arxiv.org/abs/2509.23994v2),  [pdf](http://arxiv.org/pdf/2509.23994v2)

**Tags**: cs.CL cs.AI 



### TeaRAG: A Token-Efficient Agentic Retrieval-Augmented Generation   Framework
**Authors**: Chao Zhang, Yuhao Wang, Derong Xu, Haoxin Zhang, Yuanjie Lyu, Yuhao Chen, Shuochen Liu, Tong Xu, Xiangyu Zhao, Yan Gao, Yao Hu, Enhong Chen

**Updated**: 2025-11-07T16:08:34Z

**Summary**: Retrieval-Augmented Generation (RAG) utilizes external knowledge to augment Large Language Models' (LLMs) reliability. For flexibility, agentic RAG employs autonomous, multi-round retrieval and reasoning to resolve queries. Although recent agentic RAG has improved via reinforcement learning, they often incur substantial token overhead from search and reasoning processes. This trade-off prioritizes accuracy over efficiency. To address this issue, this work proposes TeaRAG, a token-efficient agentic RAG framework capable of compressing both retrieval content and reasoning steps. 1) First, the retrieved content is compressed by augmenting chunk-based semantic retrieval with a graph retrieval using concise triplets. A knowledge association graph is then built from semantic similarity and co-occurrence. Finally, Personalized PageRank is leveraged to highlight key knowledge within this graph, reducing the number of tokens per retrieval. 2) Besides, to reduce reasoning steps, Iterative Process-aware Direct Preference Optimization (IP-DPO) is proposed. Specifically, our reward function evaluates the knowledge sufficiency by a knowledge matching mechanism, while penalizing excessive reasoning steps. This design can produce high-quality preference-pair datasets, supporting iterative DPO to improve reasoning conciseness. Across six datasets, TeaRAG improves the average Exact Match by 4% and 2% while reducing output tokens by 61% and 59% on Llama3-8B-Instruct and Qwen2.5-14B-Instruct, respectively. Code is available at https://github.com/Applied-Machine-Learning-Lab/TeaRAG.

**Link**: [arxiv](http://arxiv.org/abs/2511.05385v1),  [pdf](http://arxiv.org/pdf/2511.05385v1)

**Tags**: cs.IR cs.AI 



### Connectomics Informed by Large Language Models
**Authors**: Elinor Thompson, Tiantian He, Anna Schroder, Ahmed Abdulaal, Alec Sargood, Sonja Soskic, Henry F. J. Tregidgo, Daniel C. Alexander

**Updated**: 2025-11-07T16:05:21Z

**Summary**: Tractography is a unique method for mapping white matter connections in the brain, but tractography algorithms suffer from an inherent trade-off between sensitivity and specificity that limits accuracy. Incorporating prior knowledge of white matter anatomy is an effective strategy for improving accuracy and has been successful for reducing false positives and false negatives in bundle-mapping protocols. However, it is challenging to scale this approach for connectomics due to the difficulty in synthesising information relating to many thousands of possible connections. In this work, we develop and evaluate a pipeline using large language models (LLMs) to generate quantitative priors for connectomics, based on their knowledge of neuroanatomy. We benchmark our approach against an evaluation set derived from a gold-standard tractography atlas, identifying prompting techniques to elicit accurate connectivity information from the LLMs. We further identify strategies for incorporating external knowledge sources into the pipeline, which can provide grounding for the LLM and improve accuracy. Finally, we demonstrate how the LLM-derived priors can augment existing tractography filtering approaches by identifying true-positive connections to retain during the filtering process. We show that these additional connections can improve the accuracy of a connectome-based model of pathology spread, which provides supporting evidence that the connections preserved by the LLM are valid.

**Link**: [arxiv](http://arxiv.org/abs/2511.05383v1),  [pdf](http://arxiv.org/pdf/2511.05383v1)

**Tags**: cs.CE 



### Communication-Efficient Collaborative LLM Inference via Distributed   Speculative Decoding
**Authors**: Ce Zheng, Tingting Yang

**Updated**: 2025-11-07T15:58:37Z

**Summary**: Speculative decoding is an emerging technique that accelerates large language model (LLM) inference by allowing a smaller draft model to predict multiple tokens in advance, which are then verified or corrected by a larger target model. In AI-native radio access networks (AI-RAN), this paradigm is well-suited for collaborative inference between resource-constrained end devices and more capable edge servers or base stations (BSs). However, existing distributed speculative decoding requires transmitting the full vocabulary probability distribution from the draft model on the device to the target model at the BS, which leads to prohibitive uplink communication overhead. To address this issue, we propose a ``Top-K Sparse Logits Transmission (TK-SLT)`` scheme, where the draft model transmits only the top-K token raw probabilities and the corresponding token indices instead of the entire distribution. This approach significantly reduces bandwidth consumption while maintaining inference performance. We further derive an analytical expression for the optimal draft length that maximizes inference throughput, and provide a theoretical analysis of the achievable speedup ratio under TK-SLT. Experimental results validate both the efficiency and effectiveness of the proposed method.

**Link**: [arxiv](http://arxiv.org/abs/2509.04576v2),  [pdf](http://arxiv.org/pdf/2509.04576v2)

**Tags**: eess.SP 



### MorphTok: Morphologically Grounded Tokenization for Indian Languages
**Authors**: Maharaj Brahma, N J Karthika, Atul Singh, Devaraj Adiga, Smruti Bhate, Ganesh Ramakrishnan, Rohit Saluja, Maunendra Sankar Desarkar

**Updated**: 2025-11-07T15:53:49Z

**Summary**: Tokenization is a crucial step in NLP, especially with the rise of large language models (LLMs), impacting downstream performance, computational cost, and efficiency. Existing LLMs rely on the classical Byte-pair Encoding (BPE) algorithm for subword tokenization that greedily merges frequent character bigrams, often leading to segmentation that does not align with linguistically meaningful units. To address this, we propose morphology-aware segmentation as a pre-tokenization step before applying BPE. To facilitate morphology-aware segmentation, we create a novel dataset for Hindi and Marathi, incorporating sandhi splitting to enhance the subword tokenization. Experiments on downstream tasks show that morphologically grounded tokenization improves machine translation and language modeling performance. Additionally, to handle the dependent vowels common in syllable-based writing systems used by Indic languages, we propose Constrained BPE (CBPE), an extension to the standard BPE algorithm incorporating script-specific constraints. In particular, CBPE handles dependent vowels to form a cohesive unit with other characters instead of occurring as a single unit. Our results show that CBPE achieves a 1.68\% reduction in fertility scores while maintaining comparable or improved downstream performance in machine translation and language modeling, offering a computationally efficient alternative to standard BPE. Moreover, to evaluate segmentation across different tokenization algorithms, we introduce a new human evaluation metric, \textit{EvalTok}, enabling more human-grounded assessment.

**Link**: [arxiv](http://arxiv.org/abs/2504.10335v2),  [pdf](http://arxiv.org/pdf/2504.10335v2)

**Tags**: cs.CL 



### What Matters in Data for DPO?
**Authors**: Yu Pan, Zhongze Cai, Guanting Chen, Huaiyang Zhong, Chonghuan Wang

**Updated**: 2025-11-07T15:28:43Z

**Summary**: Direct Preference Optimization (DPO) has emerged as a simple and effective approach for aligning large language models (LLMs) with human preferences, bypassing the need for a learned reward model. Despite its growing adoption, a fundamental question remains open: what characteristics of preference data are most critical for DPO performance? In this work, we provide a systematic study of how preference data distribution influences DPO, from both theoretical and empirical perspectives. We show that the quality of chosen responses plays a dominant role in optimizing the DPO objective, while the quality of rejected responses may have relatively limited impact. Our theoretical analysis characterizes the optimal response distribution under DPO and reveals how contrastiveness between responses helps primarily by improving the chosen samples. We further study an online DPO setting and show it effectively reduces to supervised fine-tuning on the chosen responses. Extensive experiments across diverse tasks confirm our findings: improving the quality of chosen responses consistently boosts performance regardless of the quality of the rejected responses. We also investigate the benefit of mixing the on-policy data. Our results interpret the mechanism behind some widely adopted strategies and offer practical insights for constructing high-impact preference datasets for LLM alignment.

**Link**: [arxiv](http://arxiv.org/abs/2508.18312v3),  [pdf](http://arxiv.org/pdf/2508.18312v3)

**Tags**: cs.LG cs.AI 



### What Are the Facts? Automated Extraction of Court-Established Facts from   Criminal-Court Opinions
**Authors**: Klára Bendová, Tomáš Knap, Jan Černý, Vojtěch Pour, Jaromir Savelka, Ivana Kvapilíková, Jakub Drápal

**Updated**: 2025-11-07T15:17:45Z

**Summary**: Criminal justice administrative data contain only a limited amount of information about the committed offense. However, there is an unused source of extensive information in continental European courts' decisions: descriptions of criminal behaviors in verdicts by which offenders are found guilty. In this paper, we study the feasibility of extracting these descriptions from publicly available court decisions from Slovakia. We use two different approaches for retrieval: regular expressions and large language models (LLMs). Our baseline was a simple method employing regular expressions to identify typical words occurring before and after the description. The advanced regular expression approach further focused on "sparing" and its normalization (insertion of spaces between individual letters), typical for delineating the description. The LLM approach involved prompting the Gemini Flash 2.0 model to extract the descriptions using predefined instructions. Although the baseline identified descriptions in only 40.5% of verdicts, both methods significantly outperformed it, achieving 97% with advanced regular expressions and 98.75% with LLMs, and 99.5% when combined. Evaluation by law students showed that both advanced methods matched human annotations in about 90% of cases, compared to just 34.5% for the baseline. LLMs fully matched human-labeled descriptions in 91.75% of instances, and a combination of advanced regular expressions with LLMs reached 92%.

**Link**: [arxiv](http://arxiv.org/abs/2511.05320v1),  [pdf](http://arxiv.org/pdf/2511.05320v1)

**Tags**: cs.CL cs.AI 



### $\mathbf{S^2LM}$: Towards Semantic Steganography via Large Language   Models
**Authors**: Huanqi Wu, Huangbiao Xu, Runfeng Xie, Jiaxin Cai, Kaixin Zhang, Xiao Ke

**Updated**: 2025-11-07T15:17:40Z

**Summary**: Although steganography has made significant advancements in recent years, it still struggles to embed semantically rich, sentence-level information into carriers. However, in the era of AIGC, the capacity of steganography is more critical than ever. In this work, we present Sentence-to-Image Steganography, an instance of Semantic Steganography, a novel task that enables the hiding of arbitrary sentence-level messages within a cover image. Furthermore, we establish a benchmark named Invisible Text (IVT), comprising a diverse set of sentence-level texts as secret messages for evaluation. Finally, we present $\mathbf{S^2LM}$: Semantic Steganographic Language Model, which utilizes large language models (LLMs) to embed high-level textual information, such as sentences or even paragraphs, into images. Unlike traditional bit-level counterparts, $\mathrm{S^2LM}$ enables the integration of semantically rich content through a newly designed pipeline in which the LLM is involved throughout the entire process. Both quantitative and qualitative experiments demonstrate that our method effectively unlocks new semantic steganographic capabilities for LLMs. The source code will be released soon.

**Link**: [arxiv](http://arxiv.org/abs/2511.05319v1),  [pdf](http://arxiv.org/pdf/2511.05319v1)

**Tags**: cs.CV cs.CR 



### Cleaning Maintenance Logs with LLM Agents for Improved Predictive   Maintenance
**Authors**: Valeriu Dimidov, Faisal Hawlader, Sasan Jafarnejad, Raphaël Frank

**Updated**: 2025-11-07T15:12:49Z

**Summary**: Economic constraints, limited availability of datasets for reproducibility and shortages of specialized expertise have long been recognized as key challenges to the adoption and advancement of predictive maintenance (PdM) in the automotive sector. Recent progress in large language models (LLMs) presents an opportunity to overcome these barriers and speed up the transition of PdM from research to industrial practice. Under these conditions, we explore the potential of LLM-based agents to support PdM cleaning pipelines. Specifically, we focus on maintenance logs, a critical data source for training well-performing machine learning (ML) models, but one often affected by errors such as typos, missing fields, near-duplicate entries, and incorrect dates. We evaluate LLM agents on cleaning tasks involving six distinct types of noise. Our findings show that LLMs are effective at handling generic cleaning tasks and offer a promising foundation for future industrial applications. While domain-specific errors remain challenging, these results highlight the potential for further improvements through specialized training and enhanced agentic capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2511.05311v1),  [pdf](http://arxiv.org/pdf/2511.05311v1)

**Tags**: cs.AI cs.LG cs.RO cs.SE 



### Force-Safe Environment Maps and Real-Time Detection for Soft Robot   Manipulators
**Authors**: Akua K. Dickson, Juan C. Pacheco Garcia, Andrew P. Sabelhaus

**Updated**: 2025-11-07T15:07:01Z

**Summary**: Soft robot manipulators have the potential for deployment in delicate environments to perform complex manipulation tasks. However, existing obstacle detection and avoidance methods do not consider limits on the forces that manipulators may exert upon contact with delicate obstacles. This work introduces a framework that maps force safety criteria from task space (i.e. positions along the robot's body) to configuration space (i.e. the robot's joint angles) and enables real-time force safety detection. We incorporate limits on allowable environmental contact forces for given task-space obstacles, and map them into configuration space (C-space) through the manipulator's forward kinematics. This formulation ensures that configurations classified as safe are provably below the maximum force thresholds, thereby allowing us to determine force-safe configurations of the soft robot manipulator in real-time. We validate our approach in simulation and hardware experiments on a two-segment pneumatic soft robot manipulator. Results demonstrate that the proposed method accurately detects force safety during interactions with deformable obstacles, thereby laying the foundation for real-time safe planning of soft manipulators in delicate, cluttered environments.

**Link**: [arxiv](http://arxiv.org/abs/2511.05307v1),  [pdf](http://arxiv.org/pdf/2511.05307v1)

**Tags**: cs.RO cs.SY eess.SY 



### Code Review Automation using Retrieval Augmented Generation
**Authors**: Qianru Meng, Xiao Zhang, Zhaochen Ren, Joost Visser

**Updated**: 2025-11-07T15:02:42Z

**Summary**: Code review is essential for maintaining software quality but is labor-intensive. Automated code review generation offers a promising solution to this challenge. Both deep learning-based generative techniques and retrieval-based methods have demonstrated strong performance in this task. However, despite these advancements, there are still some limitations where generated reviews can be either off-point or overly general. To address these issues, we introduce Retrieval-Augmented Reviewer (RARe), which leverages Retrieval-Augmented Generation (RAG) to combine retrieval-based and generative methods, explicitly incorporating external domain knowledge into the code review process. RARe uses a dense retriever to select the most relevant reviews from the codebase, which then enrich the input for a neural generator, utilizing the contextual learning capacity of large language models (LLMs), to produce the final review. RARe outperforms state-of-the-art methods on two benchmark datasets, achieving BLEU-4 scores of 12.32 and 12.96, respectively. Its effectiveness is further validated through a detailed human evaluation and a case study using an interpretability tool, demonstrating its practical utility and reliability.

**Link**: [arxiv](http://arxiv.org/abs/2511.05302v1),  [pdf](http://arxiv.org/pdf/2511.05302v1)

**Tags**: cs.SE 



### QUESTER: Query Specification for Generative Retrieval
**Authors**: Arthur Satouf, Yuxuan Zong, Habiboulaye Amadou-Boubacar, Pablo Piantanida, Benjamin Piwowarski

**Updated**: 2025-11-07T15:01:38Z

**Summary**: Generative Retrieval (GR) differs from the traditional index-then-retrieve pipeline by storing relevance in model parameters and directly generating document identifiers. However, GR often struggles to generalize and is costly to scale. We introduce QUESTER (QUEry SpecificaTion gEnerative Retrieval), which reframes GR as query specification generation - in this work, a simple keyword query handled by BM25 - using a (small) LLM. The policy is trained using reinforcement learning techniques (GRPO). Across in- and out-of-domain evaluations, we show that our model is more effective than BM25, and competitive with neural IR models, while maintaining a good efficiency

**Link**: [arxiv](http://arxiv.org/abs/2511.05301v1),  [pdf](http://arxiv.org/pdf/2511.05301v1)

**Tags**: cs.IR cs.CL cs.LG 68P20, 68T50 H.3 



### LiveStar: Live Streaming Assistant for Real-World Online Video   Understanding
**Authors**: Zhenyu Yang, Kairui Zhang, Yuhang Hu, Bing Wang, Shengsheng Qian, Bin Wen, Fan Yang, Tingting Gao, Weiming Dong, Changsheng Xu

**Updated**: 2025-11-07T15:00:37Z

**Summary**: Despite significant progress in Video Large Language Models (Video-LLMs) for offline video understanding, existing online Video-LLMs typically struggle to simultaneously process continuous frame-by-frame inputs and determine optimal response timing, often compromising real-time responsiveness and narrative coherence. To address these limitations, we introduce LiveStar, a pioneering live streaming assistant that achieves always-on proactive responses through adaptive streaming decoding. Specifically, LiveStar incorporates: (1) a training strategy enabling incremental video-language alignment for variable-length video streams, preserving temporal consistency across dynamically evolving frame sequences; (2) a response-silence decoding framework that determines optimal proactive response timing via a single forward pass verification; (3) memory-aware acceleration via peak-end memory compression for online inference on 10+ minute videos, combined with streaming key-value cache to achieve 1.53x faster inference. We also construct an OmniStar dataset, a comprehensive dataset for training and benchmarking that encompasses 15 diverse real-world scenarios and 5 evaluation tasks for online video understanding. Extensive experiments across three benchmarks demonstrate LiveStar's state-of-the-art performance, achieving an average 19.5% improvement in semantic correctness with 18.1% reduced timing difference compared to existing online Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks. Our model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar.

**Link**: [arxiv](http://arxiv.org/abs/2511.05299v1),  [pdf](http://arxiv.org/pdf/2511.05299v1)

**Tags**: cs.CV cs.AI 



### Location-Informed Interference Suppression Precoding Methods for   Distributed Massive MIMO Systems
**Authors**: Emiel Vanspranghels, Raquel Marina Noguera Oishi, Franco Minucci, Sofie Pollin

**Updated**: 2025-11-07T14:57:13Z

**Summary**: The evolution of mobile networks towards user-centric cell-free distributed Massive MIMO configurations requires the development of novel signal processing techniques. More specifically, digital precoding algorithms have to be designed or adopted to enable distributed operation. Future deployments are expected to improve coexistence between cellular generations, and between mobile networks and incumbent services such as radar. In dense cell-free deployments, it might also not be possible to have full channel state information for all users at all antennas. To leverage location information in a dense deployment area, we suggest and investigate several algorithmic alterations on existing precoding methods, aimed at location-informed interference suppression, for usage in existing and emerging systems where user locations are known. The proposed algorithms are derived using a theoretical channel model and validated and numerically evaluated using an empirical dataset containing channel measurements from an indoor distributed Massive MIMO testbed. When dealing with measured CSI, the impact of the hardware, in addition to the location-based channel, needs to be compensated for. We propose a method to calibrate the hardware and achieve measurement-based evaluation of our location-based interference suppression algorithms. The results demonstrate that the proposed methods allow location-based interference suppression without explicit CSI knowledge at the transmitter, under certain realistic network conditions.

**Link**: [arxiv](http://arxiv.org/abs/2511.05298v1),  [pdf](http://arxiv.org/pdf/2511.05298v1)

**Tags**: eess.SP 



### Building Specialized Software-Assistant ChatBot with Graph-Based   Retrieval-Augmented Generation
**Authors**: Mohammed Hilel, Yannis Karmim, Jean De Bodinat, Reda Sarehane, Antoine Gillon

**Updated**: 2025-11-07T14:56:45Z

**Summary**: Digital Adoption Platforms (DAPs) have become essential tools for helping employees navigate complex enterprise software such as CRM, ERP, or HRMS systems. Companies like LemonLearning have shown how digital guidance can reduce training costs and accelerate onboarding. However, building and maintaining these interactive guides still requires extensive manual effort. Leveraging Large Language Models as virtual assistants is an appealing alternative, yet without a structured understanding of the target software, LLMs often hallucinate and produce unreliable answers. Moreover, most production-grade LLMs are black-box APIs, making fine-tuning impractical due to the lack of access to model weights. In this work, we introduce a Graph-based Retrieval-Augmented Generation framework that automatically converts enterprise web applications into state-action knowledge graphs, enabling LLMs to generate grounded and context-aware assistance. The framework was co-developed with the AI enterprise RAKAM, in collaboration with Lemon Learning. We detail the engineering pipeline that extracts and structures software interfaces, the design of the graph-based retrieval process, and the integration of our approach into production DAP workflows. Finally, we discuss scalability, robustness, and deployment lessons learned from industrial use cases.

**Link**: [arxiv](http://arxiv.org/abs/2511.05297v1),  [pdf](http://arxiv.org/pdf/2511.05297v1)

**Tags**: cs.SE cs.LG 



### Language Generation and Identification From Partial Enumeration: Tight   Density Bounds and Topological Characterizations
**Authors**: Jon Kleinberg, Fan Wei

**Updated**: 2025-11-07T14:56:04Z

**Summary**: The success of large language models (LLMs) has motivated formal theories of language generation and learning. We study the framework of \emph{language generation in the limit}, where an adversary enumerates strings from an unknown language $K$ drawn from a countable class, and an algorithm must generate unseen strings from $K$. Prior work showed that generation is always possible, and that some algorithms achieve positive lower density, revealing a \emph{validity--breadth} trade-off between correctness and coverage. We resolve a main open question in this line, proving a tight bound of $1/2$ on the best achievable lower density. We then strengthen the model to allow \emph{partial enumeration}, where the adversary reveals only an infinite subset $C \subseteq K$. We show that generation in the limit remains achievable, and if $C$ has lower density $\alpha$ in $K$, the algorithm's output achieves density at least $\alpha/2$, matching the upper bound. This generalizes the $1/2$ bound to the partial-information setting, where the generator must recover within a factor $1/2$ of the revealed subset's density. We further revisit the classical Gold--Angluin model of \emph{language identification} under partial enumeration. We characterize when identification in the limit is possible -- when hypotheses $M_t$ eventually satisfy $C \subseteq M \subseteq K$ -- and in the process give a new topological formulation of Angluin's characterization, showing that her condition is precisely equivalent to an appropriate topological space having the $T_D$ separation property.

**Link**: [arxiv](http://arxiv.org/abs/2511.05295v1),  [pdf](http://arxiv.org/pdf/2511.05295v1)

**Tags**: cs.DS cs.CL cs.DM cs.LG 



### Benchmarking Retrieval-Augmented Multimodal Generation for Document   Question Answering
**Authors**: Kuicai Dong, Yujing Chang, Shijie Huang, Yasheng Wang, Ruiming Tang, Yong Liu

**Updated**: 2025-11-07T14:52:06Z

**Summary**: Document Visual Question Answering (DocVQA) faces dual challenges in processing lengthy multimodal documents (text, images, tables) and performing cross-modal reasoning. Current document retrieval-augmented generation (DocRAG) methods remain limited by their text-centric approaches, frequently missing critical visual information. The field also lacks robust benchmarks for assessing multimodal evidence selection and integration. We introduce MMDocRAG, a comprehensive benchmark featuring 4,055 expert-annotated QA pairs with multi-page, cross-modal evidence chains. Our framework introduces innovative metrics for evaluating multimodal quote selection and enables answers that interleave text with relevant visual elements. Through large-scale experiments with 60 VLM/LLM models and 14 retrieval systems, we identify persistent challenges in multimodal evidence retrieval, selection, and integration.Key findings reveal advanced proprietary LVMs show superior performance than open-sourced alternatives. Also, they show moderate advantages using multimodal inputs over text-only inputs, while open-source alternatives show significant performance degradation. Notably, fine-tuned LLMs achieve substantial improvements when using detailed image descriptions. MMDocRAG establishes a rigorous testing ground and provides actionable insights for developing more robust multimodal DocVQA systems. Our benchmark and code are available at https://mmdocrag.github.io/MMDocRAG/.

**Link**: [arxiv](http://arxiv.org/abs/2505.16470v2),  [pdf](http://arxiv.org/pdf/2505.16470v2)

**Tags**: cs.IR cs.CL cs.CV 



### Reflective Personalization Optimization: A Post-hoc Rewriting Framework   for Black-Box Large Language Models
**Authors**: Teqi Hao, Xioayu Tan, Shaojie Shi, Yinghui Xu, Xihe Qiu

**Updated**: 2025-11-07T14:48:49Z

**Summary**: The personalization of black-box large language models (LLMs) is a critical yet challenging task. Existing approaches predominantly rely on context injection, where user history is embedded into the prompt to directly guide the generation process. However, this single-step paradigm imposes a dual burden on the model: generating accurate content while simultaneously aligning with user-specific styles. This often results in a trade-off that compromises output quality and limits precise control. To address this fundamental tension, we propose Reflective Personalization Optimization (RPO), a novel framework that redefines the personalization paradigm by decoupling content generation from alignment. RPO operates in two distinct stages: first, a base model generates a high-quality, generic response; then, an external reflection module explicitly rewrites this output to align with the user's preferences. This reflection module is trained using a two-stage process. Initially, supervised fine-tuning is employed on structured rewriting trajectories to establish a core personalized reasoning policy that models the transformation from generic to user-aligned responses. Subsequently, reinforcement learning is applied to further refine and enhance the quality of the personalized outputs. Comprehensive experiments on the LaMP benchmark demonstrate that RPO, by decoupling content generation from personalization, significantly outperforms state-of-the-art baselines. These findings underscore the superiority of explicit response shaping over implicit context injection. Moreover, RPO introduces an efficient, model-agnostic personalization layer that can be seamlessly integrated with any underlying base model, paving the way for a new and effective direction in user-centric generation scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2511.05286v1),  [pdf](http://arxiv.org/pdf/2511.05286v1)

**Tags**: cs.CL 



### ExGra-Med: Extended Context Graph Alignment for Medical Vision-Language   Models
**Authors**: Duy M. H. Nguyen, Nghiem T. Diep, Trung Q. Nguyen, Hoang-Bao Le, Tai Nguyen, Tien Nguyen, TrungTin Nguyen, Nhat Ho, Pengtao Xie, Roger Wattenhofer, James Zou, Daniel Sonntag, Mathias Niepert

**Updated**: 2025-11-07T14:48:44Z

**Summary**: State-of-the-art medical multi-modal LLMs (med-MLLMs), such as LLaVA-Med and BioMedGPT, primarily depend on scaling model size and data volume, with training driven largely by autoregressive objectives. However, we reveal that this approach can lead to weak vision-language alignment, making these models overly dependent on costly instruction-following data. To address this, we introduce ExGra-Med, a novel multi-graph alignment framework that jointly aligns images, instruction responses, and extended captions in the latent space, advancing semantic grounding and cross-modal coherence. To scale to large LLMs (e.g., LLaMA-7B), we develop an efficient end-to-end training scheme using black-box gradient estimation, enabling fast and scalable optimization. Empirically, ExGra-Med matches LLaVA-Med's performance using just 10% of the pre-training data, achieving a 20.13% gain on VQA-RAD and approaching full-data performance. It also outperforms strong baselines like BioMedGPT and RadFM on visual chatbot and zero-shot classification tasks, demonstrating its promise for efficient, high-quality vision-language integration in medical AI.

**Link**: [arxiv](http://arxiv.org/abs/2410.02615v4),  [pdf](http://arxiv.org/pdf/2410.02615v4)

**Tags**: cs.LG 



### Know What You Don't Know: Uncertainty Calibration of Process Reward   Models
**Authors**: Young-Jin Park, Kristjan Greenewald, Kaveh Alim, Hao Wang, Navid Azizan

**Updated**: 2025-11-07T14:35:34Z

**Summary**: Process reward models (PRMs) play a central role in guiding inference-time scaling algorithms for large language models (LLMs). However, we observe that even state-of-the-art PRMs can be poorly calibrated. Specifically, they tend to overestimate the success probability that a partial reasoning step will lead to a correct final answer, particularly when smaller LLMs are used to complete the reasoning trajectory. To address this, we present a calibration approach -- performed via quantile regression -- that adjusts PRM outputs to better align with true success probabilities. Leveraging these calibrated success estimates and their associated confidence bounds, we introduce an \emph{instance-adaptive scaling} (IAS) framework that dynamically adjusts the compute budget based on the estimated likelihood that a partial reasoning trajectory will yield a correct final answer. Unlike conventional methods that allocate a fixed number of reasoning trajectories per query, this approach adapts to each instance and reasoning step when using our calibrated PRMs. Experiments on mathematical reasoning benchmarks show that (i) our PRM calibration method achieves small calibration error, outperforming the baseline methods, (ii) calibration is crucial for enabling effective IAS, and (iii) the proposed IAS strategy reduces inference costs while maintaining final answer accuracy, utilizing less compute on more confident problems as desired.

**Link**: [arxiv](http://arxiv.org/abs/2506.09338v2),  [pdf](http://arxiv.org/pdf/2506.09338v2)

**Tags**: stat.ML cs.AI cs.LG 



### TAMAS: Benchmarking Adversarial Risks in Multi-Agent LLM Systems
**Authors**: Ishan Kavathekar, Hemang Jain, Ameya Rathod, Ponnurangam Kumaraguru, Tanuja Ganu

**Updated**: 2025-11-07T14:30:26Z

**Summary**: Large Language Models (LLMs) have demonstrated strong capabilities as autonomous agents through tool use, planning, and decision-making abilities, leading to their widespread adoption across diverse tasks. As task complexity grows, multi-agent LLM systems are increasingly used to solve problems collaboratively. However, safety and security of these systems remains largely under-explored. Existing benchmarks and datasets predominantly focus on single-agent settings, failing to capture the unique vulnerabilities of multi-agent dynamics and co-ordination. To address this gap, we introduce $\textbf{T}$hreats and $\textbf{A}$ttacks in $\textbf{M}$ulti-$\textbf{A}$gent $\textbf{S}$ystems ($\textbf{TAMAS}$), a benchmark designed to evaluate the robustness and safety of multi-agent LLM systems. TAMAS includes five distinct scenarios comprising 300 adversarial instances across six attack types and 211 tools, along with 100 harmless tasks. We assess system performance across ten backbone LLMs and three agent interaction configurations from Autogen and CrewAI frameworks, highlighting critical challenges and failure modes in current multi-agent deployments. Furthermore, we introduce Effective Robustness Score (ERS) to assess the tradeoff between safety and task effectiveness of these frameworks. Our findings show that multi-agent systems are highly vulnerable to adversarial attacks, underscoring the urgent need for stronger defenses. TAMAS provides a foundation for systematically studying and improving the safety of multi-agent LLM systems.

**Link**: [arxiv](http://arxiv.org/abs/2511.05269v1),  [pdf](http://arxiv.org/pdf/2511.05269v1)

**Tags**: cs.MA cs.AI 



### LLM4FaaS: No-Code Application Development using LLMs and FaaS
**Authors**: Minghe Wang, Tobias Pfandzelter, Trever Schirmer, David Bermbach

**Updated**: 2025-11-07T14:25:54Z

**Summary**: Large language models (LLMs) show great capabilities in generating code from natural language descriptions, bringing programming power closer to non-technical users. However, their lack of expertise in operating the generated code remains a key barrier to realizing customized applications. Function-as-a-Service (FaaS) platforms offer a high level of abstraction for code execution and deployment, allowing users to run LLM-generated code without requiring technical expertise or incurring operational overhead.   In this paper, we present LLM4FaaS, a no-code application development approach that integrates LLMs and FaaS platforms to enable non-technical users to build and run customized applications using only natural language. By deploying LLM-generated code through FaaS, LLM4FaaS abstracts away infrastructure management and boilerplate code generation. We implement a proof-of-concept prototype based on an open-source FaaS platform, and evaluate it using real prompts from non-technical users. Experiments with GPT-4o show that LLM4FaaS can automatically build and deploy code in 71.47% of cases, outperforming a non-FaaS baseline at 43.48% and an existing LLM-based platform at 14.55%, narrowing the gap to human performance at 88.99%. Further analysis of code quality, programming language diversity, latency, and consistency demonstrates a balanced performance in terms of efficiency, maintainability and availability.

**Link**: [arxiv](http://arxiv.org/abs/2502.14450v2),  [pdf](http://arxiv.org/pdf/2502.14450v2)

**Tags**: cs.SE cs.DC 



### Applying Large Language Models to Travel Satisfaction Analysis
**Authors**: Pengfei Xu, Donggen Wang

**Updated**: 2025-11-07T14:09:42Z

**Summary**: As a specific domain of subjective well-being, travel satisfaction has recently attracted much research attention. Previous studies primarily relied on statistical models and, more recently, machine learning models to explore its determinants. Both approaches,however, depend on sufficiently large sample sizes and appropriate statistical assumptions. The emergence of Large Language Models (LLMs) offers a new modeling approach that can address these limitations. Pre-trained on extensive datasets, LLMs have strongcapabilities in contextual understanding and generalization, significantly reducing their dependence on task-specific data and stringent statistical assumptions. The main challenge in applying LLMs lies in the behavioral misalignment between LLMs and humans. Using household survey data collected in Shanghai, this study identifies the existence and source of misalignment, and applies a few-shot learning method to address the misalignment issue. We find that the zero-shot LLM exhibits behavioral misalignment, leading to low prediction accuracy. With just a few samples, few-shot learning can align LLMs and enable them to outperform baseline models. Discrepancies in variable importance among machine learning model, zero-shot LLM, and few-shot LLM reveal that the misalignment arises from the gap between the general knowledge embedded in pre-trained LLMs and the specific, unique characteristics of the dataset. On these bases, we propose an LLM-based modeling approach that can be applied to model travel behavior with small sample sizes. This study highlights the potential of LLMs for modeling not only travel satisfaction but also broader aspects of travel behavior.

**Link**: [arxiv](http://arxiv.org/abs/2505.23262v2),  [pdf](http://arxiv.org/pdf/2505.23262v2)

**Tags**: cs.CY 



### Open Agent Specification (Agent Spec): A Unified Representation for AI   Agents
**Authors**: Soufiane Amini, Yassine Benajiba, Cesare Bernardis, Paul Cayet, Hassan Chafi, Abderrahim Fathan, Louis Faucon, Damien Hilloulin, Sungpack Hong, Ingo Kossyk, Tran Minh Son Le, Rhicheek Patra, Sujith Ravi, Jonas Schweizer, Jyotika Singh, Shailender Singh, Weiyi Sun, Kartik Talamadupula, Jerry Xu

**Updated**: 2025-11-07T14:02:33Z

**Summary**: The proliferation of agent frameworks has led to fragmentation in how agents are defined, executed, and evaluated. Existing systems differ in their abstractions, data flow semantics, and tool integrations, making it difficult to share or reproduce workflows. We introduce Open Agent Specification (Agent Spec), a declarative language that defines AI agents and agentic workflows in a way that is compatible across frameworks, promoting reusability, portability and interoperability of AI agents. Agent Spec defines a common set of components, control and data flow semantics, and schemas that allow an agent to be defined once and executed across different runtimes. Agent Spec also introduces a standardized Evaluation harness to assess agent behavior and agentic workflows across runtimes - analogous to how HELM and related harnesses standardized LLM evaluation - so that performance, robustness, and efficiency can be compared consistently across frameworks. We demonstrate this using four distinct runtimes (LangGraph, CrewAI, AutoGen, and WayFlow) evaluated over three different benchmarks (SimpleQA Verified, $\tau^2$-Bench and BIRD-SQL). We provide accompanying toolsets: a Python SDK (PyAgentSpec), a reference runtime (WayFlow), and adapters for popular frameworks (e.g., LangGraph, AutoGen, CrewAI). Agent Spec bridges the gap between model-centric and agent-centric standardization & evaluation, laying the groundwork for reliable, reusable, and portable agentic systems.

**Link**: [arxiv](http://arxiv.org/abs/2510.04173v4),  [pdf](http://arxiv.org/pdf/2510.04173v4)

**Tags**: cs.AI 



### Cognitive Edge Computing: A Comprehensive Survey on Optimizing Large   Models and AI Agents for Pervasive Deployment
**Authors**: Xubin Wang, Qing Li, Weijia Jia

**Updated**: 2025-11-07T13:51:28Z

**Summary**: This article surveys Cognitive Edge Computing as a practical and methodical pathway for deploying reasoning-capable Large Language Models (LLMs) and autonomous AI agents on resource-constrained devices at the network edge. We present a unified, cognition-preserving framework spanning: (1) model optimization (quantization, sparsity, low-rank adaptation, distillation) aimed at retaining multi-step reasoning under tight memory/compute budgets; (2) system architecture (on-device inference, elastic offloading, cloud-edge collaboration) that trades off latency, energy, privacy, and capacity; and (3) adaptive intelligence (context compression, dynamic routing, federated personalization) that tailors computation to task difficulty and device constraints. We synthesize advances in efficient Transformer design, multimodal integration, hardware-aware compilation, privacy-preserving learning, and agentic tool use, and map them to edge-specific operating envelopes. We further outline a standardized evaluation protocol covering latency, throughput, energy per token, accuracy, robustness, privacy, and sustainability, with explicit measurement assumptions to enhance comparability. Remaining challenges include modality-aware reasoning benchmarks, transparent and reproducible energy reporting, edge-oriented safety/alignment evaluation, and multi-agent testbeds. We conclude with practitioner guidelines for cross-layer co-design of algorithms, runtime, and hardware to deliver reliable, efficient, and privacy-preserving cognitive capabilities on edge devices.

**Link**: [arxiv](http://arxiv.org/abs/2501.03265v2),  [pdf](http://arxiv.org/pdf/2501.03265v2)

**Tags**: cs.LG cs.AI 



### Translation via Annotation: A Computational Study of Translating   Classical Chinese into Japanese
**Authors**: Zilong Li, Jie Cao

**Updated**: 2025-11-07T13:46:16Z

**Summary**: Ancient people translated classical Chinese into Japanese by annotating around each character. We abstract this process as sequence tagging tasks and fit them into modern language technologies. The research of this annotation and translation system is a facing low-resource problem. We release this problem by introducing a LLM-based annotation pipeline and construct a new dataset from digitalized open-source translation data. We show that under the low-resource setting, introducing auxiliary Chinese NLP tasks has a promoting effect on the training of sequence tagging tasks. We also evaluate the performance of large language models. They achieve high scores in direct machine translation, but they are confused when being asked to annotate characters. Our method could work as a supplement of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2511.05239v1),  [pdf](http://arxiv.org/pdf/2511.05239v1)

**Tags**: cs.CL 



### Amortized Latent Steering: Low-Cost Alternative to Test-Time   Optimization
**Authors**: Nathan Egbuna, Saatvik Gaur, Sunishchal Dev, Ashwinee Panda, Maheep Chaudhary

**Updated**: 2025-11-07T13:28:17Z

**Summary**: Test-time optimization remains impractical at scale due to prohibitive inference costs--techniques like iterative refinement and multi-step verification can require $10-100\times$ more compute per query than standard decoding. Latent space test-time optimization methods like LatentSeek offer a more direct approach by steering hidden representations, but still demand expensive per-query optimization loops with multiple backward passes. We propose Amortized Latent Steering (ALS), which collapses this iterative optimization into a single offline-computed vector applied at constant cost during inference. ALS computes the mean difference between hidden states from successful versus unsuccessful generations, then uses this direction to calibrate the model's hidden representations: when decoding drifts away from the success manifold, ALS nudges activations back toward it. Across GSM8K and MATH-500 benchmarks, ALS achieves $2-5\times$ speedup over iterative methods while matching or surpassing greedy Chain-of-Thought (CoT) and Self-Consistency baselines, yielding up to 101% improvement in efficiency--accuracy trade-off. These results show that much of latent optimization's benefit can be captured offline, making sophisticated reasoning techniques viable for production deployment. Code is available at https://github.com/negbuna/ALS.

**Link**: [arxiv](http://arxiv.org/abs/2509.18116v2),  [pdf](http://arxiv.org/pdf/2509.18116v2)

**Tags**: cs.LG cs.AI 



### Mind the Blind Spots: A Focus-Level Evaluation Framework for LLM Reviews
**Authors**: Hyungyu Shin, Jingyu Tang, Yoonjoo Lee, Nayoung Kim, Hyunseung Lim, Ji Yong Cho, Hwajung Hong, Moontae Lee, Juho Kim

**Updated**: 2025-11-07T13:27:39Z

**Summary**: Peer review underpins scientific progress, but it is increasingly strained by reviewer shortages and growing workloads. Large Language Models (LLMs) can automatically draft reviews now, but determining whether LLM-generated reviews are trustworthy requires systematic evaluation. Researchers have evaluated LLM reviews at either surface-level (e.g., BLEU and ROUGE) or content-level (e.g., specificity and factual accuracy). Yet it remains uncertain whether LLM-generated reviews attend to the same critical facets that human experts weigh -- the strengths and weaknesses that ultimately drive an accept-or-reject decision. We introduce a focus-level evaluation framework that operationalizes the focus as a normalized distribution of attention across predefined facets in paper reviews. Based on the framework, we developed an automatic focus-level evaluation pipeline based on two sets of facets: target (e.g., problem, method, and experiment) and aspect (e.g., validity, clarity, and novelty), leveraging 676 paper reviews (https://figshare.com/s/d5adf26c802527dd0f62) from OpenReview that consists of 3,657 strengths and weaknesses identified from human experts. The comparison of focus distributions between LLMs and human experts showed that the off-the-shelf LLMs consistently have a more biased focus towards examining technical validity while significantly overlooking novelty assessment when criticizing papers.

**Link**: [arxiv](http://arxiv.org/abs/2502.17086v4),  [pdf](http://arxiv.org/pdf/2502.17086v4)

**Tags**: cs.CL 



### ActiTect: A Generalizable Machine Learning Pipeline for REM Sleep   Behavior Disorder Screening through Standardized Actigraphy
**Authors**: David Bertram, Anja Ophey, Sinah Röttgen, Konstantin Kuffer, Gereon R. Fink, Elke Kalbe, Clint Hansen, Walter Maetzler, Maximilian Kapsecker, Lara M. Reimer, Stephan Jonas, Andreas T. Damgaard, Natasha B. Bertelsen, Casper Skjaerbaek, Per Borghammer, Karolien Groenewald, Pietro-Luca Ratti, Michele T. Hu, Noémie Moreau, Michael Sommerauer, Katarzyna Bozek

**Updated**: 2025-11-07T13:18:20Z

**Summary**: Isolated rapid eye movement sleep behavior disorder (iRBD) is a major prodromal marker of $\alpha$-synucleinopathies, often preceding the clinical onset of Parkinson's disease, dementia with Lewy bodies, or multiple system atrophy. While wrist-worn actimeters hold significant potential for detecting RBD in large-scale screening efforts by capturing abnormal nocturnal movements, they become inoperable without a reliable and efficient analysis pipeline. This study presents ActiTect, a fully automated, open-source machine learning tool to identify RBD from actigraphy recordings. To ensure generalizability across heterogeneous acquisition settings, our pipeline includes robust preprocessing and automated sleep-wake detection to harmonize multi-device data and extract physiologically interpretable motion features characterizing activity patterns. Model development was conducted on a cohort of 78 individuals, yielding strong discrimination under nested cross-validation (AUROC = 0.95). Generalization was confirmed on a blinded local test set (n = 31, AUROC = 0.86) and on two independent external cohorts (n = 113, AUROC = 0.84; n = 57, AUROC = 0.94). To assess real-world robustness, leave-one-dataset-out cross-validation across the internal and external cohorts demonstrated consistent performance (AUROC range = 0.84-0.89). A complementary stability analysis showed that key predictive features remained reproducible across datasets, supporting the final pooled multi-center model as a robust pre-trained resource for broader deployment. By being open-source and easy to use, our tool promotes widespread adoption and facilitates independent validation and collaborative improvements, thereby advancing the field toward a unified and generalizable RBD detection model using wearable devices.

**Link**: [arxiv](http://arxiv.org/abs/2511.05221v1),  [pdf](http://arxiv.org/pdf/2511.05221v1)

**Tags**: cs.LG q-bio.NC 



### Holistic Evaluation of Multimodal LLMs on Spatial Intelligence
**Authors**: Zhongang Cai, Yubo Wang, Qingping Sun, Ruisi Wang, Chenyang Gu, Wanqi Yin, Zhiqian Lin, Zhitao Yang, Chen Wei, Oscar Qian, Hui En Pang, Xuanke Shi, Kewang Deng, Xiaoyang Han, Zukai Chen, Jiaqi Li, Xiangyu Fan, Hanming Deng, Lewei Lu, Bo Li, Ziwei Liu, Quan Wang, Dahua Lin, Lei Yang

**Updated**: 2025-11-07T13:12:03Z

**Summary**: Multimodal models have achieved remarkable progress in recent years. Nevertheless, they continue to exhibit notable limitations in spatial understanding and reasoning, the very capability that anchors artificial general intelligence in the physical world. With the recent release of GPT-5, allegedly the most powerful AI model to date, it is timely to examine where the leading models (GPT, Gemini, Grok, Seed, Qwen, and Intern) stand on the path toward spatial intelligence. We thus propose EASI for holistic Evaluation of multimodAl LLMs on Spatial Intelligence. EASI conceptualizes a comprehensive taxonomy of spatial tasks that unifies existing benchmarks and a standardized protocol for the fair evaluation of state-of-the-art proprietary and open-source models. In this report, we conduct the study across eight key benchmarks, at a cost exceeding ten billion total tokens. Our empirical study then reveals that (1) GPT-5 demonstrates unprecedented strength in spatial intelligence (SI), yet (2) still falls short of human performance significantly across a broad spectrum of SI-tasks. Moreover, we (3) show that SI-tasks expose greater model capability deficiency than non-SI tasks, to the extent that (4) proprietary models do not exhibit a decisive advantage when facing the most difficult ones. In addition, we conduct a qualitative evaluation across a diverse set of scenarios that are intuitive for humans, yet fail even the most advanced multimodal models.

**Link**: [arxiv](http://arxiv.org/abs/2508.13142v3),  [pdf](http://arxiv.org/pdf/2508.13142v3)

**Tags**: cs.CV cs.CL cs.LG cs.MM cs.RO 



### A Composable Agentic System for Automated Visual Data Reporting
**Authors**: Péter Ferenc Gyarmati, Dominik Moritz, Torsten Möller, Laura Koesten

**Updated**: 2025-11-07T13:05:57Z

**Summary**: To address the brittleness of monolithic AI agents, our prototype for automated visual data reporting explores a Human-AI Partnership model. Its hybrid, multi-agent architecture strategically externalizes logic from LLMs to deterministic modules, leveraging the rule-based system Draco for principled visualization design. The system delivers a dual-output: an interactive Observable report with Mosaic for reader exploration, and executable Marimo notebooks for deep, analyst-facing traceability. This granular architecture yields a fully automatic yet auditable and steerable system, charting a path toward a more synergistic partnership between human experts and AI. For reproducibility, our implementation and examples are available at https://peter-gy.github.io/VISxGenAI-2025/.

**Link**: [arxiv](http://arxiv.org/abs/2509.05721v2),  [pdf](http://arxiv.org/pdf/2509.05721v2)

**Tags**: cs.HC 



### OptiLog: Assigning Roles in Byzantine Consensus
**Authors**: Hanish Gogada, Christian Berger, Leander Jehl, Hans P. Reiser, Hein Meling

**Updated**: 2025-11-07T13:04:52Z

**Summary**: Byzantine Fault-Tolerant (BFT) protocols play an important role in blockchains. As the deployment of such systems extends to wide-area networks, the scalability of BFT protocols becomes a critical concern. Optimizations that assign specific roles to individual replicas can significantly improve the performance of BFT systems. However, such role assignment is highly sensitive to faults, potentially undermining the optimizations' effectiveness. To address these challenges, we present OptiLog, a logging framework for collecting and analyzing measurements that help to assign roles in globally distributed systems, despite the presence of faults. OptiLog presents local measurements in global data structures, to enable consistent decisions and hold replicas accountable if they do not perform according to their reported measurements. We demonstrate OptiLog's flexibility by applying it to two BFT protocols: (1) Aware, a highly optimized PBFT-like protocol, and (2) Kauri, a tree-based protocol designed for large-scale deployments. OptiLog detects and excludes replicas that misbehave during consensus and thus enables the system to operate in an optimized, low-latency configuration, even under adverse conditions. Experiments show that for tree overlays deployed across 73 worldwide cities, trees found by OptiLog display 39% lower latency than Kauri.

**Link**: [arxiv](http://arxiv.org/abs/2502.15428v2),  [pdf](http://arxiv.org/pdf/2502.15428v2)

**Tags**: cs.DC 



### Millimeter-Scale Absolute Carrier Phase-Based Localization in Multi-Band   Systems
**Authors**: Andrea Bedin, Joerg Widmer, Melanny Davila, Marco Canil, Rafael Ruiz

**Updated**: 2025-11-07T12:43:54Z

**Summary**: Localization is a key feature of future Sixth Generation (6G) net-works with foreseen accuracy requirements down to the millimeter level, to enable novel applications in the fields of telesurgery, high-precision manufacturing, and others. Currently, such accuracy requirements are only achievable with specialized or highly resource-demanding systems, rendering them impractical for more wide-spread deployment. In this paper, we present the first system that enables low-complexity and low-bandwidth absolute 3D localization with millimeter-level accuracy in generic wireless networks. It performs a carrier phase-based wireless localization refinement of an initial location estimate based on successive location-likelihood optimization across multiple bands. Unlike previous phase unwrapping methods, our solution is one-shot. We evaluate its performance collecting ~350, 000 measurements, showing an improvement of more than one order of magnitude over classical localization techniques. Finally, we will open-source the low-cost, modular FR3 front-end that we developed for the experimental campaign.

**Link**: [arxiv](http://arxiv.org/abs/2511.05204v1),  [pdf](http://arxiv.org/pdf/2511.05204v1)

**Tags**: eess.SP 



### Procedimiento de auditoría de ciberseguridad para sistemas   autónomos: metodología, amenazas y mitigaciones
**Authors**: Adrián Campazas-Vega, Claudia Álvarez-Aparicio, David Sobrín-Hidalgo, Laura Inyesto-Alonso, Francisco Javier Rodríguez-Lera, Vicente Matellán-Olivera, Ángel Manuel Guerrero-Higueras

**Updated**: 2025-11-07T12:06:21Z

**Summary**: The deployment of autonomous systems has experienced remarkable growth in recent years, driven by their integration into sectors such as industry, medicine, logistics, and domestic environments. This expansion is accompanied by a series of security issues that entail significant risks due to the critical nature of autonomous systems, especially those operating in human-interaction environments. Furthermore, technological advancement and the high operational and architectural complexity of autonomous systems have resulted in an increased attack surface. This article presents a specific security auditing procedure for autonomous systems, based on a layer-structured methodology, a threat taxonomy adapted to the robotic context, and a set of concrete mitigation measures. The validity of the proposed approach is demonstrated through four practical case studies applied to representative robotic platforms: the Vision 60 military quadruped from Ghost Robotics, the A1 robot from Unitree Robotics, the UR3 collaborative arm from Universal Robots, and the Pepper social robot from Aldebaran Robotics.

**Link**: [arxiv](http://arxiv.org/abs/2511.05185v1),  [pdf](http://arxiv.org/pdf/2511.05185v1)

**Tags**: cs.RO cs.CR 



### Effectiveness of Chain-of-Thought in Distilling Reasoning Capability   from Large Language Models
**Authors**: Cong-Thanh Do, Rama Doddipatla, Kate Knill

**Updated**: 2025-11-07T12:05:39Z

**Summary**: Chain-of-Thought (CoT) prompting is a widely used method to improve the reasoning capability of Large Language Models (LLMs). More recently, CoT has been leveraged in Knowledge Distillation (KD) to transfer reasoning capability from a larger LLM to a smaller one. This paper examines the role of CoT in distilling the reasoning capability from larger LLMs to smaller LLMs using white-box KD, analysing its effectiveness in improving the performance of the distilled models for various natural language reasoning and understanding tasks. We conduct white-box KD experiments using LLMs from the Qwen and Llama2 families, employing CoT data from the CoT-Collection dataset. The distilled models are then evaluated on natural language reasoning and understanding tasks from the BIG-Bench-Hard (BBH) benchmark, which presents complex challenges for smaller LLMs. Experimental results demonstrate the role of CoT in improving white-box KD effectiveness, enabling the distilled models to achieve better average performance in natural language reasoning and understanding tasks from BBH.

**Link**: [arxiv](http://arxiv.org/abs/2511.05184v1),  [pdf](http://arxiv.org/pdf/2511.05184v1)

**Tags**: cs.CL 



### AI Through the Human Lens: Investigating Cognitive Theories in Machine   Psychology
**Authors**: Akash Kundu, Rishika Goswami

**Updated**: 2025-11-07T11:59:06Z

**Summary**: We investigate whether Large Language Models (LLMs) exhibit human-like cognitive patterns under four established frameworks from psychology: Thematic Apperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and Cognitive Dissonance. We evaluated several proprietary and open-source models using structured prompts and automated scoring. Our findings reveal that these models often produce coherent narratives, show susceptibility to positive framing, exhibit moral judgments aligned with Liberty/Oppression concerns, and demonstrate self-contradictions tempered by extensive rationalization. Such behaviors mirror human cognitive tendencies yet are shaped by their training data and alignment methods. We discuss the implications for AI transparency, ethical deployment, and future work that bridges cognitive psychology and AI safety

**Link**: [arxiv](http://arxiv.org/abs/2506.18156v2),  [pdf](http://arxiv.org/pdf/2506.18156v2)

**Tags**: cs.AI 



### No One-Model-Fits-All: Uncovering Spatio-Temporal Forecasting Trade-offs   with Graph Neural Networks and Foundation Models
**Authors**: Ragini Gupta, Naman Raina, Bo Chen, Li Chen, Claudiu Danilov, Josh Eckhardt, Keyshla Bernard, Klara Nahrstedt

**Updated**: 2025-11-07T11:50:39Z

**Summary**: Modern IoT deployments for environmental sensing produce high volume spatiotemporal data to support downstream tasks such as forecasting, typically powered by machine learning models. While existing filtering and strategic deployment techniques optimize collected data volume at the edge, they overlook how variations in sampling frequencies and spatial coverage affect downstream model performance. In many forecasting models, incorporating data from additional sensors denoise predictions by providing broader spatial contexts. This interplay between sampling frequency, spatial coverage and different forecasting model architectures remain underexplored. This work presents a systematic study of forecasting models - classical models (VAR), neural networks (GRU, Transformer), spatio-temporal graph neural networks (STGNNs), and time series foundation models (TSFMs: Chronos Moirai, TimesFM) under varying spatial sensor nodes density and sampling intervals using real-world temperature data in a wireless sensor network. Our results show that STGNNs are effective when sensor deployments are sparse and sampling rate is moderate, leveraging spatial correlations via encoded graph structure to compensate for limited coverage. In contrast, TSFMs perform competitively at high frequencies but degrade when spatial coverage from neighboring sensors is reduced. Crucially, the multivariate TSFM Moirai outperforms all models by natively learning cross-sensor dependencies. These findings offer actionable insights for building efficient forecasting pipelines in spatio-temporal systems. All code for model configurations, training, dataset, and logs are open-sourced for reproducibility: https://github.com/UIUC-MONET-Projects/Benchmarking-Spatiotemporal-Forecast-Models

**Link**: [arxiv](http://arxiv.org/abs/2511.05179v1),  [pdf](http://arxiv.org/pdf/2511.05179v1)

**Tags**: cs.LG cs.AI cs.NI 



### Generating Software Architecture Description from Source Code using   Reverse Engineering and Large Language Model
**Authors**: Ahmad Hatahet, Christoph Knieke, Andreas Rausch

**Updated**: 2025-11-07T11:35:46Z

**Summary**: Software Architecture Descriptions (SADs) are essential for managing the inherent complexity of modern software systems. They enable high-level architectural reasoning, guide design decisions, and facilitate effective communication among diverse stakeholders. However, in practice, SADs are often missing, outdated, or poorly aligned with the system's actual implementation. Consequently, developers are compelled to derive architectural insights directly from source code-a time-intensive process that increases cognitive load, slows new developer onboarding, and contributes to the gradual degradation of clarity over the system's lifetime. To address these issues, we propose a semi-automated generation of SADs from source code by integrating reverse engineering (RE) techniques with a Large Language Model (LLM). Our approach recovers both static and behavioral architectural views by extracting a comprehensive component diagram, filtering architecturally significant elements (core components) via prompt engineering, and generating state machine diagrams to model component behavior based on underlying code logic with few-shots prompting. This resulting views representation offer a scalable and maintainable alternative to traditional manual architectural documentation. This methodology, demonstrated using C++ examples, highlights the potent capability of LLMs to: 1) abstract the component diagram, thereby reducing the reliance on human expert involvement, and 2) accurately represent complex software behaviors, especially when enriched with domain-specific knowledge through few-shot prompting. These findings suggest a viable path toward significantly reducing manual effort while enhancing system understanding and long-term maintainability.

**Link**: [arxiv](http://arxiv.org/abs/2511.05165v1),  [pdf](http://arxiv.org/pdf/2511.05165v1)

**Tags**: cs.SE cs.AI 



### Mind the Gap... or Not? How Translation Errors and Evaluation Details   Skew Multilingual Results
**Authors**: Jan-Thorsten Peter, David Vilar, Tobias Domhan, Dan Malkin, Markus Freitag

**Updated**: 2025-11-07T11:30:10Z

**Summary**: Most current large language models (LLMs) support a wide variety of languages in addition to English, including high-resource languages (e.g. German, Chinese, French), as well as low-resource ones (e.g. Swahili, Telugu). In addition they have also shown impressive capabilities in different domains, like coding, science and math. In this short paper, taking math as an example domain, we study the performance of different LLMs across languages. Experimental results show that there exists a non-negligible and consistent gap in the performance of the models across languages. Interestingly, and somewhat against expectations, the gap exists for both high- and low-resource languages. We hope that these results influence further research into cross-lingual capability generalization for next generation LLMs. If it weren't for the fact that they are false! By analyzing one of the standard multilingual math benchmarks (MGSM), we determine that several translation errors are present in the data. Furthermore, the lack of standardized answer extraction from LLM outputs further influences the final results. We propose a method for automatic quality assurance to address the first issue at scale, and give recommendations to address the second one. Combining these two approaches we show that the aforementioned language gap mostly disappears, leading to completely different conclusions from our research. We additionally release the corrected dataset to the community.

**Link**: [arxiv](http://arxiv.org/abs/2511.05162v1),  [pdf](http://arxiv.org/pdf/2511.05162v1)

**Tags**: cs.CL 



### Follow-Me in Micro-Mobility with End-to-End Imitation Learning
**Authors**: Sahar Salimpour, Iacopo Catalano, Tomi Westerlund, Mohsen Falahi, Jorge Peña Queralta

**Updated**: 2025-11-07T11:24:20Z

**Summary**: Autonomous micro-mobility platforms face challenges from the perspective of the typical deployment environment: large indoor spaces or urban areas that are potentially crowded and highly dynamic. While social navigation algorithms have progressed significantly, optimizing user comfort and overall user experience over other typical metrics in robotics (e.g., time or distance traveled) is understudied. Specifically, these metrics are critical in commercial applications. In this paper, we show how imitation learning delivers smoother and overall better controllers, versus previously used manually-tuned controllers. We demonstrate how DAAV's autonomous wheelchair achieves state-of-the-art comfort in follow-me mode, in which it follows a human operator assisting persons with reduced mobility (PRM). This paper analyzes different neural network architectures for end-to-end control and demonstrates their usability in real-world production-level deployments.

**Link**: [arxiv](http://arxiv.org/abs/2511.05158v1),  [pdf](http://arxiv.org/pdf/2511.05158v1)

**Tags**: cs.RO cs.LG 



### Inverse Knowledge Search over Verifiable Reasoning: Synthesizing a   Scientific Encyclopedia from a Long Chains-of-Thought Knowledge Base
**Authors**: Yu Li, Yuan Huang, Tao Wang, Caiyu Fan, Xiansheng Cai, Sihan Hu, Xinzijian Liu, Cheng Shi, Mingjun Xu, Zhen Wang, Yan Wang, Xiangqi Jin, Tianhan Zhang, Linfeng Zhang, Lei Wang, Youjin Deng, Pan Zhang, Weijie Sun, Xingyu Li, Weinan E, Linfeng Zhang, Zhiyuan Yao, Kun Chen

**Updated**: 2025-11-07T11:11:26Z

**Summary**: Most scientific materials compress reasoning, presenting conclusions while omitting the derivational chains that justify them. This compression hinders verification by lacking explicit, step-wise justifications and inhibits cross-domain links by collapsing the very pathways that establish the logical and causal connections between concepts. We introduce a scalable framework that decompresses scientific reasoning, constructing a verifiable Long Chain-of-Thought (LCoT) knowledge base and projecting it into an emergent encyclopedia, SciencePedia. Our pipeline operationalizes an endpoint-driven, reductionist strategy: a Socratic agent, guided by a curriculum of around 200 courses, generates approximately 3 million first-principles questions. To ensure high fidelity, multiple independent solver models generate LCoTs, which are then rigorously filtered by prompt sanitization and cross-model answer consensus, retaining only those with verifiable endpoints. This verified corpus powers the Brainstorm Search Engine, which performs inverse knowledge search -- retrieving diverse, first-principles derivations that culminate in a target concept. This engine, in turn, feeds the Plato synthesizer, which narrates these verified chains into coherent articles. The initial SciencePedia comprises approximately 200,000 fine-grained entries spanning mathematics, physics, chemistry, biology, engineering, and computation. In evaluations across six disciplines, Plato-synthesized articles (conditioned on retrieved LCoTs) exhibit substantially higher knowledge-point density and significantly lower factual error rates than an equally-prompted baseline without retrieval (as judged by an external LLM). Built on this verifiable LCoT knowledge base, this reasoning-centric approach enables trustworthy, cross-domain scientific synthesis at scale and establishes the foundation for an ever-expanding encyclopedia.

**Link**: [arxiv](http://arxiv.org/abs/2510.26854v2),  [pdf](http://arxiv.org/pdf/2510.26854v2)

**Tags**: cs.AI cs.LG 



### Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities
**Authors**: Chung-En Sun, Xiaodong Liu, Weiwei Yang, Tsui-Wei Weng, Hao Cheng, Aidan San, Michel Galley, Jianfeng Gao

**Updated**: 2025-11-07T10:17:59Z

**Summary**: Recent research has shown that Large Language Models (LLMs) are vulnerable to automated jailbreak attacks, where adversarial suffixes crafted by algorithms appended to harmful queries bypass safety alignment and trigger unintended responses. Current methods for generating these suffixes are computationally expensive and have low Attack Success Rates (ASR), especially against well-aligned models like Llama2 and Llama3. To overcome these limitations, we introduce ADV-LLM, an iterative self-tuning process that crafts adversarial LLMs with enhanced jailbreak ability. Our framework significantly reduces the computational cost of generating adversarial suffixes while achieving nearly 100\% ASR on various open-source LLMs. Moreover, it exhibits strong attack transferability to closed-source models, achieving 99\% ASR on GPT-3.5 and 49\% ASR on GPT-4, despite being optimized solely on Llama3. Beyond improving jailbreak ability, ADV-LLM provides valuable insights for future safety alignment research through its ability to generate large datasets for studying LLM safety. Our code is available at: https://github.com/SunChungEn/ADV-LLM

**Link**: [arxiv](http://arxiv.org/abs/2410.18469v6),  [pdf](http://arxiv.org/pdf/2410.18469v6)

**Tags**: cs.CL cs.LG 



### Grounded in Reality: Learning and Deploying Proactive LLM from Offline   Logs
**Authors**: Fei Wei, Daoyuan Chen, Ce Wang, Yilun Huang, Yushuo Chen, Xuchen Pan, Yaliang Li, Bolin Ding

**Updated**: 2025-11-07T10:05:06Z

**Summary**: Large Language Models (LLMs) excel as passive responders, but teaching them to be proactive, goal-oriented partners, a critical capability in high-stakes domains, remains a major challenge. Current paradigms either myopically optimize single-turn attributes or rely on brittle, high-cost user simulators, creating a persistent ``reality gap''. To bridge this gap, we introduce \texttt{Learn-to-Ask}, a general, simulator-free framework for learning and deploying proactive dialogue agents \textit{directly from offline expert data}, bypassing the need to model complex user dynamics. Our key insight is to reframe the offline policy learning problem by leveraging the \textbf{observed future} of each expert trajectory. This allows us to infer a dense, turn-by-turn reward signal grounded in the expert's revealed strategy, decomposing the intractable long-horizon problem into a series of supervised learning tasks, and training a policy to output a structured \texttt{(action, state_assessment)} tuple, governing both \textbf{what to ask} and, crucially, \textbf{when to stop}. To ensure reward fidelity, our Automated Grader Calibration pipeline systematically purges noise from the LLM-based reward model with minimal human supervision. Empirically, we demonstrate the efficacy of \texttt{Learn-to-Ask} in a real-world medical dataset, using LLMs of varying sizes up to 32B. Our approach culminates in the successful deployment of LLMs into a live, large-scale online AI service. In rigorous in-house evaluations, our model was launched and achieved performance even superior to human experts, proving our framework's ability to translate offline data into tangible, real-world impact. We hope this work provides a practical and economically viable blueprint for transforming passive LLMs into proactive, goal-oriented LLM applications.

**Link**: [arxiv](http://arxiv.org/abs/2510.25441v2),  [pdf](http://arxiv.org/pdf/2510.25441v2)

**Tags**: cs.CL cs.AI 



### A Toolbox for Improving Evolutionary Prompt Search
**Authors**: Daniel Grießhaber, Maximilian Kimmich, Johannes Maucher, Ngoc Thang Vu

**Updated**: 2025-11-07T10:04:41Z

**Summary**: Evolutionary prompt optimization has demonstrated effectiveness in refining prompts for LLMs. However, existing approaches lack robust operators and efficient evaluation mechanisms. In this work, we propose several key improvements to evolutionary prompt optimization that can partially generalize to prompt optimization in general: 1) decomposing evolution into distinct steps to enhance the evolution and its control, 2) introducing an LLM-based judge to verify the evolutions, 3) integrating human feedback to refine the evolutionary operator, and 4) developing more efficient evaluation strategies that maintain performance while reducing computational overhead. Our approach improves both optimization quality and efficiency. We release our code, enabling prompt optimization on new tasks and facilitating further research in this area.

**Link**: [arxiv](http://arxiv.org/abs/2511.05120v1),  [pdf](http://arxiv.org/pdf/2511.05120v1)

**Tags**: cs.CL 



### Boardwalk: Towards a Framework for Creating Board Games with LLMs
**Authors**: Álvaro Guglielmin Becker, Gabriel Bauer de Oliveira, Lana Bertoldo Rossato, Anderson Rocha Tavares

**Updated**: 2025-11-07T10:02:55Z

**Summary**: Implementing board games in code can be a time-consuming task. However, Large Language Models (LLMs) have been proven effective at generating code for domain-specific tasks with simple contextual information. We aim to investigate whether LLMs can implement digital versions of board games from rules described in natural language. This would be a step towards an LLM-assisted framework for quick board game code generation. We expect to determine the main challenges for LLMs to implement the board games, and how different approaches and models compare to one another. We task three state-of-the-art LLMs (Claude, DeepSeek and ChatGPT) with coding a selection of 12 popular and obscure games in free-form and within Boardwalk, our proposed General Game Playing API. We anonymize the games and components to avoid evoking pre-trained LLM knowledge. The implementations are tested for playability and rule compliance. We evaluate success rate and common errors across LLMs and game popularity. Our approach proves viable, with the best performing model, Claude 3.7 Sonnet, yielding 55.6\% of games without any errors. While compliance with the API increases error frequency, the severity of errors is more significantly dependent on the LLM. We outline future steps for creating a framework to integrate this process, making the elaboration of board games more accessible.

**Link**: [arxiv](http://arxiv.org/abs/2508.16447v2),  [pdf](http://arxiv.org/pdf/2508.16447v2)

**Tags**: cs.LG 



### Usando LLMs para Programar Jogos de Tabuleiro e Variações
**Authors**: Álvaro Guglielmin Becker, Lana Bertoldo Rossato, Anderson Rocha Tavares

**Updated**: 2025-11-07T09:58:01Z

**Summary**: Creating programs to represent board games can be a time-consuming task. Large Language Models (LLMs) arise as appealing tools to expedite this process, given their capacity to efficiently generate code from simple contextual information. In this work, we propose a method to test how capable three LLMs (Claude, DeepSeek and ChatGPT) are at creating code for board games, as well as new variants of existing games.

**Link**: [arxiv](http://arxiv.org/abs/2511.05114v1),  [pdf](http://arxiv.org/pdf/2511.05114v1)

**Tags**: cs.LG 



### iTool: Reinforced Fine-Tuning with Dynamic Deficiency Calibration for   Advanced Tool Use
**Authors**: Yirong Zeng, Xiao Ding, Yuxian Wang, Weiwen Liu, Wu Ning, Yutai Hou, Xu Huang, Duyu Tang, Dandan Tu, Bing Qin, Ting Liu

**Updated**: 2025-11-07T09:32:05Z

**Summary**: Augmenting large language models (LLMs) with external tools is a promising approach to enhance their capabilities, especially for complex tasks. Synthesizing tool-use data through real-world simulations is an effective way to achieve this. However, our investigation reveals that training gains significantly decay as synthetic data increases. The model struggles to benefit from additional synthetic data, which fails to endow it with advanced tool-use capabilities in complex scenarios Moreover, we discovered that the above limitation usually manifests as a fragment deficiency (i.e., parameter errors) in response. To this end, we propose an iterative reinforced fine-tuning strategy designed to alleviate this limitation. This strategy involves: (1) enhancing the diversity of response for synthetic data through path exploration of Monte Carlo Tree Search. (2) iteratively pinpointing the model's deficiency by constructing fine-grained preference pairs, and then improving it by preference optimization algorithms for targeted improvement. The experiments show that our method achieves 13.11% better performance than the same-size base model. It achieves an improvement of 6.5% in complex scenarios compared to the baseline, and it also outperforms larger open-source and closed-source models.

**Link**: [arxiv](http://arxiv.org/abs/2501.09766v5),  [pdf](http://arxiv.org/pdf/2501.09766v5)

**Tags**: cs.CL cs.AI cs.LG 



### FM4Com: Foundation Model for Scene-Adaptive Communication Strategy   Optimization
**Authors**: Zhaoyang Li, Shangzhuo Xie, Qianqian Yang

**Updated**: 2025-11-07T09:21:22Z

**Summary**: The emergence of sixth-generation (6G) networks heralds an intelligent communication ecosystem driven by AI-native air interfaces. However, current physical-layer designs-typically following modular and isolated optimization paradigms-fail to achieve global end-to-end optimality due to neglected inter-module dependencies. Although large language models (LLMs) have recently been applied to communication tasks such as beam prediction and resource allocation, existing studies remain limited to single-task or single-modality scenarios and lack the ability to jointly reason over communication states and user intents for personalized strategy adaptation. To address these limitations, this paper proposes a novel multimodal communication decision-making model based on reinforcement learning. The proposed model semantically aligns channel state information (CSI) and textual user instructions, enabling comprehensive understanding of both physical-layer conditions and communication intents. It then generates physically realizable, user-customized link construction strategies that dynamically adapt to changing environments and preference tendencies. A two-stage reinforcement learning framework is employed: the first stage expands the experience pool via heuristic exploration and behavior cloning to obtain a near-optimal initialization, while the second stage fine-tunes the model through multi-objective reinforcement learning considering bit error rate, throughput, and complexity. Experimental results demonstrate that the proposed model significantly outperforms conventional planning-based algorithms under challenging channel conditions, achieving robust, efficient, and personalized 6G link construction.

**Link**: [arxiv](http://arxiv.org/abs/2511.05094v1),  [pdf](http://arxiv.org/pdf/2511.05094v1)

**Tags**: cs.HC 



### LLM-Based Emulation of the Radio Resource Control Layer: Towards   AI-Native RAN Protocols
**Authors**: Ziming Liu, Bryan Liu, Alvaro Valcarce, Xiaoli Chu

**Updated**: 2025-11-07T09:20:34Z

**Summary**: Integrating Large AI Models (LAMs) into 6G mobile networks is a key enabler of the AI-Native Air Interface (AI-AI), where protocol intelligence must scale beyond handcrafted logic. This paper presents, to our knowledge, the first standards-compliant emulation of the Radio Resource Control (RRC) layer using a decoder-only LAM (LLAMA-class) fine-tuned with Low-Rank Adaptation (LoRA) on a multi-vendor corpus of real-world traces spanning both 5G and 4G systems. We treat RRC as a domain-specific language and construct a segmentation-safe, question--answer (Question-and-Answer (QA)) dataset that preserves Abstract Syntax Notation (ASN.1) structure through linearization prior to Byte Pair Encoding (BPE) tokenization. The proposed approach combines parameter-efficient adaptation with schema-bounded prompting to ensure syntactic and procedural fidelity. Evaluation introduces a standards-aware triad -- ASN.1 conformance, field-level coverage analysis, and uplink-to-downlink state-machine checks -- alongside semantic similarity and latency profiling across 120 configurations. On 30k 5G request--response pairs plus an additional 4.8k QA turns from 4G sessions, our 8B model achieves a median cosine similarity of 0.97, a 61% relative gain over a zero-shot baseline, while sustaining high conformance rates. These results demonstrate that LAMs, when augmented with protocol-aware reasoning, can directly orchestrate control-plane procedures, laying the foundation for the future Artificial Intelligence (AI)-native Radio Access Network (RAN).

**Link**: [arxiv](http://arxiv.org/abs/2505.16821v3),  [pdf](http://arxiv.org/pdf/2505.16821v3)

**Tags**: cs.NI cs.LG eess.SP 



### Where Is Self-admitted Code Generated by Large Language Models on   GitHub?
**Authors**: Xiao Yu, Lei Liu, Xing Hu, Jin Liu, Xin Xia

**Updated**: 2025-11-07T09:09:43Z

**Summary**: The increasing use of Large Language Models (LLMs) in software development has garnered significant attention from researchers evaluating the capabilities and limitations of LLMs for code generation. However, much of the research focuses on controlled datasets such as HumanEval, which do not adequately capture the characteristics of LLM-generated code in real-world development scenarios. To address this gap, our study investigates self-admitted code generated by LLMs on GitHub, specifically focusing on instances where developers in projects with over five stars acknowledge the use of LLMs to generate code through code comments. Our findings reveal several key insights: (1) ChatGPT and Copilot dominate code generation, with minimal contributions from other LLMs. (2) Projects containing ChatGPT/Copilot-generated code appears in small/medium-sized projects led by small teams, which are continuously evolving. (3) ChatGPT/Copilot-generated code generally is a minor project portion, primarily generating short/moderate-length, low-complexity snippets (e.g., algorithms and data structures code; text processing code). (4) ChatGPT/Copilot-generated code generally undergoes minimal modifications, with bug-related changes ranging from 4% to 12%. (5) Most code comments only state LLM use, while few include details like prompts, human edits, or code testing status. Based on these findings, we discuss the implications for researchers and practitioners.

**Link**: [arxiv](http://arxiv.org/abs/2406.19544v4),  [pdf](http://arxiv.org/pdf/2406.19544v4)

**Tags**: cs.SE 



### CompressionAttack: Exploiting Prompt Compression as a New Attack Surface   in LLM-Powered Agents
**Authors**: Zesen Liu, Zhixiang Zhang, Yuchong Xie, Dongdong She

**Updated**: 2025-11-07T09:01:26Z

**Summary**: LLM-powered agents often use prompt compression to reduce inference costs, but this introduces a new security risk. Compression modules, which are optimized for efficiency rather than safety, can be manipulated by adversarial inputs, causing semantic drift and altering LLM behavior. This work identifies prompt compression as a novel attack surface and presents CompressionAttack, the first framework to exploit it. CompressionAttack includes two strategies: HardCom, which uses discrete adversarial edits for hard compression, and SoftCom, which performs latent-space perturbations for soft compression. Experiments on multiple LLMs show up to 80% attack success and 98% preference flips, while remaining highly stealthy and transferable. Case studies in VSCode Cline and Ollama confirm real-world impact, and current defenses prove ineffective, highlighting the need for stronger protections.

**Link**: [arxiv](http://arxiv.org/abs/2510.22963v2),  [pdf](http://arxiv.org/pdf/2510.22963v2)

**Tags**: cs.CR cs.AI 



### Iterative Layer-wise Distillation for Efficient Compression of Large   Language Models
**Authors**: Grigory Kovalev, Mikhail Tikhomirov

**Updated**: 2025-11-07T09:00:26Z

**Summary**: This work investigates distillation methods for large language models (LLMs) with the goal of developing compact models that preserve high performance. Several existing approaches are reviewed, with a discussion of their respective strengths and limitations. An improved method based on the ShortGPT approach has been developed, building upon the idea of incorporating iterative evaluation of layer importance. At each step, importance is assessed by measuring performance degradation when individual layers are removed, using a set of representative datasets. This process is combined with further training using a joint loss function based on KL divergence and mean squared error. Experiments on the Qwen2.5-3B model show that the number of layers can be reduced from 36 to 28 (resulting in a 2.47 billion parameter model) with only a 9.7% quality loss, and to 24 layers with an 18% loss. The findings suggest that the middle transformer layers contribute less to inference, underscoring the potential of the proposed method for creating efficient models. The results demonstrate the effectiveness of iterative distillation and fine-tuning, making the approach suitable for deployment in resource-limited settings.

**Link**: [arxiv](http://arxiv.org/abs/2511.05085v1),  [pdf](http://arxiv.org/pdf/2511.05085v1)

**Tags**: cs.CL cs.LG 



### MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems
**Authors**: Channdeth Sok, David Luz, Yacine Haddam

**Updated**: 2025-11-07T09:00:04Z

**Summary**: Large Language Models (LLMs) are increasingly deployed in enterprise applications, yet their reliability remains limited by hallucinations, i.e., confident but factually incorrect information. Existing detection approaches, such as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not address the unique challenges of Retrieval-Augmented Generation (RAG) systems, where responses must be consistent with retrieved evidence. We therefore present MetaRAG, a metamorphic testing framework for hallucination detection in Retrieval-Augmented Generation (RAG) systems. MetaRAG operates in a real-time, unsupervised, black-box setting, requiring neither ground-truth references nor access to model internals, making it suitable for proprietary and high-stakes domains. The framework proceeds in four stages: (1) decompose answers into atomic factoids, (2) generate controlled mutations of each factoid using synonym and antonym substitutions, (3) verify each variant against the retrieved context (synonyms are expected to be entailed and antonyms contradicted), and (4) aggregate penalties for inconsistencies into a response-level hallucination score. Crucially for identity-aware AI, MetaRAG localizes unsupported claims at the factoid span where they occur (e.g., pregnancy-specific precautions, LGBTQ+ refugee rights, or labor eligibility), allowing users to see flagged spans and enabling system designers to configure thresholds and guardrails for identity-sensitive queries. Experiments on a proprietary enterprise dataset illustrate the effectiveness of MetaRAG for detecting hallucinations and enabling trustworthy deployment of RAG-based conversational agents. We also outline a topic-based deployment design that translates MetaRAG's span-level scores into identity-aware safeguards; this design is discussed but not evaluated in our experiments.

**Link**: [arxiv](http://arxiv.org/abs/2509.09360v2),  [pdf](http://arxiv.org/pdf/2509.09360v2)

**Tags**: cs.CL 



### On Text Simplification Metrics and General-Purpose LLMs for Accessible   Health Information, and A Potential Architectural Advantage of The   Instruction-Tuned LLM class
**Authors**: P. Bilha Githinji, Aikaterini Meilliou, Peiwu Qin

**Updated**: 2025-11-07T08:53:39Z

**Summary**: The increasing health-seeking behavior and digital consumption of biomedical information by the general public necessitate scalable solutions for automatically adapting complex scientific and technical documents into plain language. Automatic text simplification solutions, including advanced large language models, however, continue to face challenges in reliably arbitrating the tension between optimizing readability performance and ensuring preservation of discourse fidelity. This report empirically assesses the performance of two major classes of general-purpose LLMs, demonstrating their linguistic capabilities and foundational readiness for the task compared to a human benchmark. Using a comparative analysis of the instruction-tuned Mistral 24B and the reasoning-augmented QWen2.5 32B, we identify a potential architectural advantage in the instruction-tuned LLM. Mistral exhibits a tempered lexical simplification strategy that enhances readability across a suite of metrics and the simplification-specific formula SARI (mean 42.46), while preserving human-level discourse with a BERTScore of 0.91. QWen also attains enhanced readability performance, but its operational strategy shows a disconnect in balancing between readability and accuracy, reaching a statistically significantly lower BERTScore of 0.89. Additionally, a comprehensive correlation analysis of 21 metrics spanning readability, discourse fidelity, content safety, and underlying distributional measures for mechanistic insights, confirms strong functional redundancies among five readability indices. This empirical evidence tracks baseline performance of the evolving LLMs for the task of text simplification, identifies the instruction-tuned Mistral 24B for simplification, provides necessary heuristics for metric selection, and points to lexical support as a primary domain-adaptation issue for simplification.

**Link**: [arxiv](http://arxiv.org/abs/2511.05080v1),  [pdf](http://arxiv.org/pdf/2511.05080v1)

**Tags**: cs.CL 



### A Proprietary Model-Based Safety Response Framework for AI Agents
**Authors**: Qi Li, Jianjun Xu, Pingtao Wei, Jiu Li, Peiqiang Zhao, Jiwei Shi, Xuan Zhang, Yanhui Yang, Xiaodong Hui, Peng Xu, Wenqin Shao

**Updated**: 2025-11-07T08:01:49Z

**Summary**: With the widespread application of Large Language Models (LLMs), their associated security issues have become increasingly prominent, severely constraining their trustworthy deployment in critical domains. This paper proposes a novel safety response framework designed to systematically safeguard LLMs at both the input and output levels. At the input level, the framework employs a supervised fine-tuning-based safety classification model. Through a fine-grained four-tier taxonomy (Safe, Unsafe, Conditionally Safe, Focused Attention), it performs precise risk identification and differentiated handling of user queries, significantly enhancing risk coverage and business scenario adaptability, and achieving a risk recall rate of 99.3%. At the output level, the framework integrates Retrieval-Augmented Generation (RAG) with a specifically fine-tuned interpretation model, ensuring all responses are grounded in a real-time, trustworthy knowledge base. This approach eliminates information fabrication and enables result traceability. Experimental results demonstrate that our proposed safety control model achieves a significantly higher safety score on public safety evaluation benchmarks compared to the baseline model, TinyR1-Safety-8B. Furthermore, on our proprietary high-risk test set, the framework's components attained a perfect 100% safety score, validating their exceptional protective capabilities in complex risk scenarios. This research provides an effective engineering pathway for building high-security, high-trust LLM applications.

**Link**: [arxiv](http://arxiv.org/abs/2511.03138v2),  [pdf](http://arxiv.org/pdf/2511.03138v2)

**Tags**: cs.AI 



### UMDAM: A Unified Data Layout and DRAM Address Mapping for Heterogenous   NPU-PIM
**Authors**: Hai Huang, Xuhong Qiang, Weisheng Zhao, Chenchen Liu

**Updated**: 2025-11-07T07:54:50Z

**Summary**: Large Language Models (LLMs) are increasingly deployed on edge devices with Neural Processing Units (NPUs), yet the decode phase remains memory-intensive, limiting performance. Processing-in-Memory (PIM) offers a promising solution, but co-executing NPU-PIM systems face challenges such as data layout mismatches, bandwidth loss, and redundant storage. To address these issues, we propose UMDAM, a unified memory-affinity data layout and DRAM address mapping scheme tailored for NPU-PIM co-execution. UMDAM employs a column-major, tile-based layout and a configurable DRAM mapping strategy to ensure compatibility with NPU computation while maximizing PIM efficiency -- without introducing extra memory overhead or bandwidth loss. Comprehensive evaluations on OPT models demonstrate that UMDAM reduces time-to-first-token (TTFT) by up to 3.0x and time-to-last-token (TTLT) by 2.18x, significantly improving end-to-end LLM inference efficiency on edge devices.

**Link**: [arxiv](http://arxiv.org/abs/2511.03293v2),  [pdf](http://arxiv.org/pdf/2511.03293v2)

**Tags**: cs.DC 



### UA-Code-Bench: A Competitive Programming Benchmark for Evaluating LLM   Code Generation in Ukrainian
**Authors**: Mykyta Syromiatnikov, Victoria Ruvinskaya

**Updated**: 2025-11-07T07:24:56Z

**Summary**: Evaluating the real capabilities of large language models in low-resource languages still represents a challenge, as many existing benchmarks focus on widespread tasks translated from English or evaluate only simple language understanding. This paper introduces UA-Code-Bench, a new open-source benchmark established for a thorough evaluation of language models' code generation and competitive programming problem-solving abilities in Ukrainian. The benchmark comprises 500 problems from the Eolymp platform, evenly distributed across five complexity levels from very easy to very hard. A diverse set of 13 leading proprietary and open-source models, generating Python solutions based on a one-shot prompt, was evaluated via the dedicated Eolymp environment against hidden tests, ensuring code correctness. The obtained results reveal that even top-performing models, such as OpenAI o3 and GPT-5, solve only half of the problems, highlighting the challenge of code generation in low-resource natural language. Furthermore, this research presents a comprehensive analysis of performance across various difficulty levels, as well as an assessment of solution uniqueness and computational efficiency, measured by both elapsed time and memory consumption of the generated solutions. In conclusion, this work demonstrates the value of competitive programming benchmarks in evaluating large language models, especially in underrepresented languages. It also paves the way for future research on multilingual code generation and reasoning-enhanced models. The benchmark, data parsing, preparation, code generation, and evaluation scripts are available at https://huggingface.co/datasets/NLPForUA/ua-code-bench.

**Link**: [arxiv](http://arxiv.org/abs/2511.05040v1),  [pdf](http://arxiv.org/pdf/2511.05040v1)

**Tags**: cs.CL cs.AI cs.SE 



### DRQA: Dynamic Reasoning Quota Allocation for Controlling Overthinking in   Reasoning Large Language Models
**Authors**: Kaiwen Yan, Xuanqing Shi, Hongcheng Guo, Wenxuan Wang, Zhuosheng Zhang, Chengwei Qin

**Updated**: 2025-11-07T07:16:46Z

**Summary**: Reasoning large language models (RLLMs), such as OpenAI-O3 and DeepSeek-R1, have recently demonstrated remarkable capabilities by performing structured and multi-step reasoning. However, recent studies reveal that RLLMs often suffer from overthinking, i.e., producing unnecessarily lengthy reasoning chains even for simple questions, leading to excessive token consumption and computational inefficiency. Interestingly, we observe that when processing multiple questions in batch mode, RLLMs exhibit more resource-efficient behavior by dynamically compressing reasoning steps for easier problems, due to implicit resource competition. Inspired by this, we propose Dynamic Reasoning Quota Allocation (DRQA), a novel method that transfers the benefits of resource competition from batch processing to single-question inference. Specifically, DRQA leverages batch-generated preference data and reinforcement learning to train the model to allocate reasoning resources adaptively. By encouraging the model to internalize a preference for responses that are both accurate and concise, DRQA enables it to generate concise answers for simple questions while retaining sufficient reasoning depth for more challenging ones. Extensive experiments on a wide range of mathematical and scientific reasoning benchmarks demonstrate that DRQA significantly reduces token usage while maintaining, and in many cases improving, answer accuracy. By effectively mitigating the overthinking problem, DRQA offers a promising direction for more efficient and scalable deployment of RLLMs, and we hope it inspires further exploration into fine-grained control of reasoning behaviors.

**Link**: [arxiv](http://arxiv.org/abs/2508.17803v2),  [pdf](http://arxiv.org/pdf/2508.17803v2)

**Tags**: cs.CL 



### From Observability Data to Diagnosis: An Evolving Multi-agent System for   Incident Management in Cloud Systems
**Authors**: Yu Luo, Jiamin Jiang, Jingfei Feng, Lei Tao, Qingliang Zhang, Xidao Wen, Yongqian Sun, Shenglin Zhang, Dan Pei

**Updated**: 2025-11-07T07:03:20Z

**Summary**: Incident management (IM) is central to the reliability of large-scale cloud systems. Yet manual IM, where on-call engineers examine metrics, logs, and traces is labor-intensive and error-prone in the face of massive and heterogeneous observability data. Existing automated IM approaches often struggle to generalize across systems, provide limited interpretability, and incur high deployment costs, which hinders adoption in practice. In this paper, we present OpsAgent, a lightweight, self-evolving multi-agent system for IM that employs a training-free data processor to convert heterogeneous observability data into structured textual descriptions, along with a multi-agent collaboration framework that makes diagnostic inference transparent and auditable. To support continual capability growth, OpsAgent also introduces a dual self-evolution mechanism that integrates internal model updates with external experience accumulation, thereby closing the deployment loop. Comprehensive experiments on the OPENRCA benchmark demonstrate state-of-the-art performance and show that OpsAgent is generalizable, interpretable, cost-efficient, and self-evolving, making it a practically deployable and sustainable solution for long-term operation in real-world cloud systems.

**Link**: [arxiv](http://arxiv.org/abs/2510.24145v2),  [pdf](http://arxiv.org/pdf/2510.24145v2)

**Tags**: cs.AI 



### Optimizing Anytime Reasoning via Budget Relative Policy Optimization
**Authors**: Penghui Qi, Zichen Liu, Tianyu Pang, Chao Du, Wee Sun Lee, Min Lin

**Updated**: 2025-11-07T07:01:30Z

**Summary**: Scaling test-time compute is crucial for enhancing the reasoning capabilities of large language models (LLMs). Existing approaches typically employ reinforcement learning (RL) to maximize a verifiable reward obtained at the end of reasoning traces. However, such methods optimize only the final performance under a large and fixed token budget, which hinders efficiency in both training and deployment. In this work, we present a novel framework, AnytimeReasoner, to optimize anytime reasoning performance, which aims to improve token efficiency and the flexibility of reasoning under varying token budget constraints. To achieve this, we truncate the complete thinking process to fit within sampled token budgets from a prior distribution, compelling the model to summarize the optimal answer for each truncated thinking for verification. This introduces verifiable dense rewards into the reasoning process, facilitating more effective credit assignment in RL optimization. We then optimize the thinking and summary policies in a decoupled manner to maximize the cumulative reward. Additionally, we introduce a novel variance reduction technique, Budget Relative Policy Optimization (BRPO), to enhance the robustness and efficiency of the learning process when reinforcing the thinking policy. Empirical results in mathematical reasoning tasks demonstrate that our method consistently outperforms GRPO across all thinking budgets under various prior distributions, enhancing both training and token efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2505.13438v3),  [pdf](http://arxiv.org/pdf/2505.13438v3)

**Tags**: cs.LG cs.AI cs.CL 



### String Seed of Thought: Prompting LLMs for Distribution-Faithful and   Diverse Generation
**Authors**: Kou Misaki, Takuya Akiba

**Updated**: 2025-11-07T06:59:25Z

**Summary**: We introduce String Seed of Thought (SSoT), a novel prompting method for LLMs that improves Probabilistic Instruction Following (PIF). We define PIF as a task requiring an LLM to select its answer from a predefined set of options, each associated with a specific probability, such that the empirical distribution of the generated answers aligns with the target distribution when prompted multiple times. While LLMs excel at tasks with single, deterministic answers, they often fail at PIF, exhibiting biases problematic for applications requiring non-deterministic behaviors, such as human-behavior simulation, content diversification, and multiplayer games. It also harms the diversity of generated responses, a crucial factor in test-time scaling, by causing the outputs to collapse into a limited set of answers. To address this, we propose SSoT, a simple prompting method that instructs an LLM to first output a random string to generate sufficient entropy. SSoT also instructs the LLM to extract randomness by manipulating this string to derive a final answer, thereby preserving diversity while adhering to specific constraints. We demonstrate that SSoT significantly improves the PIF performance of LLMs, approaching the ideal performance of a pseudo-random number generator. Furthermore, our experiments on NoveltyBench show SSoT's benefits extend beyond closed-set tasks to open-ended tasks by enhancing response diversity.

**Link**: [arxiv](http://arxiv.org/abs/2510.21150v2),  [pdf](http://arxiv.org/pdf/2510.21150v2)

**Tags**: cs.AI 



### Continuous-variable Measurement Device Independent MIMO Quantum Key   Distribution for THz Communications
**Authors**: Leixin Wu, Congtian Deng, Jiayu Pan, Lingtao Zhang, Yanyan Feng, Runbo Zhao, Yang Shen, Yuying Zhang, Jian Zhou

**Updated**: 2025-11-07T06:52:28Z

**Summary**: Although multiple-input multiple-output (MIMO) terahertz (THz) continuous-variable quantum key distribution (CVQKD) is theoretically secure, practical vulnerabilities may arise due to detector imperfections. This paper explores a CV measurement-device-independent (MDI) QKD system operating at THz frequencies within a MIMO framework. In this system, measurement is delegated to an untrusted third party, Charlie, rather than the receiver, eliminating all detector attacks and significantly enhancing the system's practical security. Using transmit-receive beamforming techniques, the system transforms MIMO channels into multiple parallel lossy quantum channels, enabling robust key distribution between Alice and Bob. This study examines entanglement-based and prepare-and-measure protocols, deriving secret key rates for both asymptotic and finite code scenarios. Simulations reveal the critical role of multiple antenna configurations and efficient homodyne detection in mitigating free-space path loss and maximizing key rates. Results indicate that system performance is optimized at lower THz frequencies for long-range transmissions and higher frequencies for short-range applications. The proposed protocol offers a scalable solution for secure quantum communications in next-generation wireless networks, demonstrating potential for deployment in both indoor and outdoor environments.

**Link**: [arxiv](http://arxiv.org/abs/2511.05021v1),  [pdf](http://arxiv.org/pdf/2511.05021v1)

**Tags**: quant-ph 



### Wider or Deeper? Scaling LLM Inference-Time Compute with Adaptive   Branching Tree Search
**Authors**: Yuichi Inoue, Kou Misaki, Yuki Imajuku, So Kuroki, Taishi Nakamura, Takuya Akiba

**Updated**: 2025-11-07T06:44:51Z

**Summary**: Recent advances demonstrate that increasing inference-time computation can significantly boost the reasoning capabilities of large language models (LLMs). Although repeated sampling (i.e., generating multiple candidate outputs) is a highly effective strategy, it does not leverage external feedback signals for refinement, which are often available in tasks like coding. In this work, we propose Adaptive Branching Monte Carlo Tree Search (AB-MCTS), a novel inference-time framework that generalizes repeated sampling with principled multi-turn exploration and exploitation. At each node in the search tree, AB-MCTS dynamically decides whether to "go wider" by expanding new candidate responses or "go deeper" by revisiting existing ones based on external feedback signals. We evaluate our method on complex coding and engineering tasks using frontier models. Empirical results show that AB-MCTS consistently outperforms both repeated sampling and standard MCTS, underscoring the importance of combining the response diversity of LLMs with multi-turn solution refinement for effective inference-time scaling. Code is available at https://github.com/SakanaAI/treequest .

**Link**: [arxiv](http://arxiv.org/abs/2503.04412v5),  [pdf](http://arxiv.org/pdf/2503.04412v5)

**Tags**: cs.AI 



### Pluralistic Behavior Suite: Stress-Testing Multi-Turn Adherence to   Custom Behavioral Policies
**Authors**: Prasoon Varshney, Makesh Narsimhan Sreedhar, Liwei Jiang, Traian Rebedea, Christopher Parisien

**Updated**: 2025-11-07T06:43:01Z

**Summary**: Large language models (LLMs) are typically aligned to a universal set of safety and usage principles intended for broad public acceptability. Yet, real-world applications of LLMs often take place within organizational ecosystems shaped by distinctive corporate policies, regulatory requirements, use cases, brand guidelines, and ethical commitments. This reality highlights the need for rigorous and comprehensive evaluation of LLMs with pluralistic alignment goals, an alignment paradigm that emphasizes adaptability to diverse user values and needs. In this work, we present PLURALISTIC BEHAVIOR SUITE (PBSUITE), a dynamic evaluation suite designed to systematically assess LLMs' capacity to adhere to pluralistic alignment specifications in multi-turn, interactive conversations. PBSUITE consists of (1) a diverse dataset of 300 realistic LLM behavioral policies, grounded in 30 industries; and (2) a dynamic evaluation framework for stress-testing model compliance with custom behavioral specifications under adversarial conditions. Using PBSUITE, We find that leading open- and closed-source LLMs maintain robust adherence to behavioral policies in single-turn settings (less than 4% failure rates), but their compliance weakens substantially in multi-turn adversarial interactions (up to 84% failure rates). These findings highlight that existing model alignment and safety moderation methods fall short in coherently enforcing pluralistic behavioral policies in real-world LLM interactions. Our work contributes both the dataset and analytical framework to support future research toward robust and context-aware pluralistic alignment techniques.

**Link**: [arxiv](http://arxiv.org/abs/2511.05018v1),  [pdf](http://arxiv.org/pdf/2511.05018v1)

**Tags**: cs.CL cs.AI cs.LG 



### ZERO: Industry-ready Vision Foundation Model with Multi-modal Prompts
**Authors**: Sangbum Choi, Kyeongryeol Go, Taewoong Jang

**Updated**: 2025-11-07T06:42:08Z

**Summary**: Foundation models have revolutionized AI, yet they struggle with zero-shot deployment in real-world industrial settings due to a lack of high-quality, domain-specific datasets. To bridge this gap, Superb AI introduces ZERO, an industry-ready vision foundation model that leverages multi-modal prompting (textual and visual) for generalization without retraining. Trained on a compact yet representative 0.9 million annotated samples from a proprietary billion-scale industrial dataset, ZERO demonstrates competitive performance on academic benchmarks like LVIS-Val and significantly outperforms existing models across 37 diverse industrial datasets. Furthermore, ZERO achieved 2nd place in the CVPR 2025 Object Instance Detection Challenge and 4th place in the Foundational Few-shot Object Detection Challenge, highlighting its practical deployability and generalizability with minimal adaptation and limited data. To the best of our knowledge, ZERO is the first vision foundation model explicitly built for domain-specific, zero-shot industrial applications.

**Link**: [arxiv](http://arxiv.org/abs/2507.04270v4),  [pdf](http://arxiv.org/pdf/2507.04270v4)

**Tags**: cs.CV cs.AI 



### Exploring Multimodal Perception in Large Language Models Through   Perceptual Strength Ratings
**Authors**: Jonghyun Lee, Dojun Park, Jiwoo Lee, Hoekeon Choi, Sung-Eun Lee

**Updated**: 2025-11-07T06:21:05Z

**Summary**: This study investigated whether multimodal large language models can achieve human-like sensory grounding by examining their ability to capture perceptual strength ratings across sensory modalities. We explored how model characteristics (size, multimodal capabilities, architectural generation) influence grounding performance, distributional factor dependencies (word frequency, embeddings, feature distances), and human-model processing differences. We evaluated 21 models from four families (GPT, Gemini, LLaMA, Qwen) using 3,611 words from the Lancaster Sensorimotor Norms through correlation, distance metrics, and qualitative analysis. Results showed that larger (6 out of 8 comparisons), multimodal (5 of 7), and newer models (5 of 8) generally outperformed their smaller, text-based, and older counterparts. Top models achieved 85-90% accuracy and 0.58-0.65 correlations with human ratings, demonstrating substantial similarity. Moreover, distributional factors showed minimal impact, not exceeding human dependency levels. However, despite strong alignment, models were not identical to humans, as even top performers showed differences in distance and correlation measures, with qualitative analysis revealing processing patterns related to absent sensory grounding. Additionally, it remains questionable whether introducing multimodality resolves this grounding deficit. Although multimodality improved performance, it seems to provide similar information as massive text rather than qualitatively different data, as benefits occurred across unrelated sensory dimensions and massive text-only models achieved comparable results. Our findings demonstrate that while advanced LLMs can approximate human sensory-linguistic associations through statistical learning, they still differ from human embodied cognition in processing mechanisms, even with multimodal integration.

**Link**: [arxiv](http://arxiv.org/abs/2503.06980v2),  [pdf](http://arxiv.org/pdf/2503.06980v2)

**Tags**: cs.CL 



### Query Generation Pipeline with Enhanced Answerability Assessment for   Financial Information Retrieval
**Authors**: Hyunkyu Kim, Yeeun Yoo, Youngjun Kwak

**Updated**: 2025-11-07T06:06:09Z

**Summary**: As financial applications of large language models (LLMs) gain attention, accurate Information Retrieval (IR) remains crucial for reliable AI services. However, existing benchmarks fail to capture the complex and domain-specific information needs of real-world banking scenarios. Building domain-specific IR benchmarks is costly and constrained by legal restrictions on using real customer data. To address these challenges, we propose a systematic methodology for constructing domain-specific IR benchmarks through LLM-based query generation. As a concrete implementation of this methodology, our pipeline combines single and multi-document query generation with an enhanced and reasoning-augmented answerability assessment method, achieving stronger alignment with human judgments than prior approaches. Using this methodology, we construct KoBankIR, comprising 815 queries derived from 204 official banking documents. Our experiments show that existing retrieval models struggle with the complex multi-document queries in KoBankIR, demonstrating the value of our systematic approach for domain-specific benchmark construction and underscoring the need for improved retrieval techniques in financial domains.

**Link**: [arxiv](http://arxiv.org/abs/2511.05000v1),  [pdf](http://arxiv.org/pdf/2511.05000v1)

**Tags**: cs.IR cs.AI 



### Graph Learning
**Authors**: Feng Xia, Ciyuan Peng, Jing Ren, Falih Gozi Febrinanto, Renqiang Luo, Vidya Saikrishna, Shuo Yu, Xiangjie Kong

**Updated**: 2025-11-07T05:58:26Z

**Summary**: Graph learning has rapidly evolved into a critical subfield of machine learning and artificial intelligence (AI). Its development began with early graph-theoretic methods, gaining significant momentum with the advent of graph neural networks (GNNs). Over the past decade, progress in scalable architectures, dynamic graph modeling, multimodal learning, generative AI, explainable AI (XAI), and responsible AI has broadened the applicability of graph learning to various challenging environments. Graph learning is significant due to its ability to model complex, non-Euclidean relationships that traditional machine learning struggles to capture, thus better supporting real-world applications ranging from drug discovery and fraud detection to recommender systems and scientific reasoning. However, challenges like scalability, generalization, heterogeneity, interpretability, and trustworthiness must be addressed to unlock its full potential. This survey provides a comprehensive introduction to graph learning, focusing on key dimensions including scalable, temporal, multimodal, generative, explainable, and responsible graph learning. We review state-of-the-art techniques for efficiently handling large-scale graphs, capturing dynamic temporal dependencies, integrating heterogeneous data modalities, generating novel graph samples, and enhancing interpretability to foster trust and transparency. We also explore ethical considerations, such as privacy and fairness, to ensure responsible deployment of graph learning models. Additionally, we identify and discuss emerging topics, highlighting recent integration of graph learning and other AI paradigms and offering insights into future directions. This survey serves as a valuable resource for researchers and practitioners seeking to navigate the rapidly evolving landscape of graph learning.

**Link**: [arxiv](http://arxiv.org/abs/2507.05636v2),  [pdf](http://arxiv.org/pdf/2507.05636v2)

**Tags**: cs.LG cs.AI 68T09, 68R10 I.2.6; G.2.2; E.1 



### ControlGS: Consistent Structural Compression Control for   Deployment-Aware Gaussian Splatting
**Authors**: Fengdi Zhang, Yibao Sun, Hongkun Cao, Ruqi Huang

**Updated**: 2025-11-07T05:53:48Z

**Summary**: 3D Gaussian Splatting (3DGS) is a highly deployable real-time method for novel view synthesis. In practice, it requires a universal, consistent control mechanism that adjusts the trade-off between rendering quality and model compression without scene-specific tuning, enabling automated deployment across different device performances and communication bandwidths. In this work, we present ControlGS, a control-oriented optimization framework that maps the trade-off between Gaussian count and rendering quality to a continuous, scene-agnostic, and highly responsive control axis. Extensive experiments across a wide range of scene scales and types (from small objects to large outdoor scenes) demonstrate that, by adjusting a globally unified control hyperparameter, ControlGS can flexibly generate models biased toward either structural compactness or high fidelity, regardless of the specific scene scale or complexity, while achieving markedly higher rendering quality with the same or fewer Gaussians compared to potential competing methods. Project page: https://zhang-fengdi.github.io/ControlGS/

**Link**: [arxiv](http://arxiv.org/abs/2505.10473v3),  [pdf](http://arxiv.org/pdf/2505.10473v3)

**Tags**: cs.CV 



### Enhancing Public Speaking Skills in Engineering Students Through AI
**Authors**: Amol Harsh, Brainerd Prince, Siddharth Siddharth, Deepan Raj Prabakar Muthirayan, Kabir S Bhalla, Esraaj Sarkar Gupta, Siddharth Sahu

**Updated**: 2025-11-07T05:44:15Z

**Summary**: This research-to-practice full paper was inspired by the persistent challenge in effective communication among engineering students. Public speaking is a necessary skill for future engineers as they have to communicate technical knowledge with diverse stakeholders. While universities offer courses or workshops, they are unable to offer sustained and personalized training to students. Providing comprehensive feedback on both verbal and non-verbal aspects of public speaking is time-intensive, making consistent and individualized assessment impractical. This study integrates research on verbal and non-verbal cues in public speaking to develop an AI-driven assessment model for engineering students. Our approach combines speech analysis, computer vision, and sentiment detection into a multi-modal AI system that provides assessment and feedback. The model evaluates (1) verbal communication (pitch, loudness, pacing, intonation), (2) non-verbal communication (facial expressions, gestures, posture), and (3) expressive coherence, a novel integration ensuring alignment between speech and body language. Unlike previous systems that assess these aspects separately, our model fuses multiple modalities to deliver personalized, scalable feedback. Preliminary testing demonstrated that our AI-generated feedback was moderately aligned with expert evaluations. Among the state-of-the-art AI models evaluated, all of which were Large Language Models (LLMs), including Gemini and OpenAI models, Gemini Pro emerged as the best-performing, showing the strongest agreement with human annotators. By eliminating reliance on human evaluators, this AI-driven public speaking trainer enables repeated practice, helping students naturally align their speech with body language and emotion, crucial for impactful and professional communication.

**Link**: [arxiv](http://arxiv.org/abs/2511.04995v1),  [pdf](http://arxiv.org/pdf/2511.04995v1)

**Tags**: cs.HC cs.AI cs.CL 



### Conformal Information Pursuit for Interactively Guiding Large Language   Models
**Authors**: Kwan Ho Ryan Chan, Yuyan Ge, Edgar Dobriban, Hamed Hassani, René Vidal

**Updated**: 2025-11-07T05:30:41Z

**Summary**: A significant use case of instruction-finetuned Large Language Models (LLMs) is to solve question-answering tasks interactively. In this setting, an LLM agent is tasked with making a prediction by sequentially querying relevant information from the user, as opposed to a single-turn conversation. This paper explores sequential querying strategies that aim to minimize the expected number of queries. One such strategy is Information Pursuit (IP), a greedy algorithm that at each iteration selects the query that maximizes information gain or equivalently minimizes uncertainty. However, obtaining accurate estimates of mutual information or conditional entropy for LLMs is very difficult in practice due to over- or under-confident LLM proba- bilities, which leads to suboptimal query selection and predictive performance. To better estimate the uncertainty at each iteration, we propose Conformal Information Pursuit (C-IP), an alternative approach to sequential information gain based on conformal prediction sets. More specifically, C-IP leverages a relationship between prediction sets and conditional entropy at each iteration to estimate uncertainty based on the average size of conformal prediction sets. In contrast to conditional entropy, we find that conformal prediction sets are a distribution-free and robust method of measuring uncertainty. Experiments with 20 Questions show that C-IP obtains better predictive performance and shorter query-answer chains compared to previous approaches to IP and uncertainty-based chain-of-thought methods. Furthermore, extending to an interactive medical setting between a doctor and a patient on the MediQ dataset, C-IP achieves competitive performance with direct single-turn prediction while offering greater interpretability.

**Link**: [arxiv](http://arxiv.org/abs/2507.03279v2),  [pdf](http://arxiv.org/pdf/2507.03279v2)

**Tags**: cs.LG cs.AI stat.ML 



### Acquiring Common Chinese Emotional Events Using Large Language Model
**Authors**: Ya Wang, Guangzheng Zhu, Cungen Cao, Jingjing Li, He Li, Xin Huang

**Updated**: 2025-11-07T05:27:35Z

**Summary**: Knowledge about emotional events is an important kind of knowledge which has been applied to improve the effectiveness of different applications. However, emotional events cannot be easily acquired, especially common or generalized emotional events that are context-independent. The goal of this paper is to obtain common emotional events in Chinese language such as "win a prize" and "be criticized". Our approach begins by collecting a comprehensive list of Chinese emotional event indicators. Then, we generate emotional events by prompting a Chinese large language model (LLM) using these indicators. To ensure the quality of these emotional events, we train a filter to discard invalid generated results. We also classify these emotional events as being positive events and negative events using different techniques. Finally, we harvest a total of 102,218 high-quality common emotional events with sentiment polarity labels, which is the only large-scale commonsense knowledge base of emotional events in Chinese language. Intrinsic evaluation results show that the proposed method in this paper can be effectively used to acquire common Chinese emotional events. An extrinsic use case also demonstrates the strong potential of common emotional events in the field of emotion cause extraction (ECE). Related resources including emotional event indicators and emotional events will be released after the publication of this paper.

**Link**: [arxiv](http://arxiv.org/abs/2511.04989v1),  [pdf](http://arxiv.org/pdf/2511.04989v1)

**Tags**: cs.CL 



### AIRepr: An Analyst-Inspector Framework for Evaluating Reproducibility of   LLMs in Data Science
**Authors**: Qiuhai Zeng, Claire Jin, Xinyue Wang, Yuhan Zheng, Qunhua Li

**Updated**: 2025-11-07T04:33:52Z

**Summary**: Large language models (LLMs) are increasingly used to automate data analysis through executable code generation. Yet, data science tasks often admit multiple statistically valid solutions, e.g. different modeling strategies, making it critical to understand the reasoning behind analyses, not just their outcomes. While manual review of LLM-generated code can help ensure statistical soundness, it is labor-intensive and requires expertise. A more scalable approach is to evaluate the underlying workflows-the logical plans guiding code generation. However, it remains unclear how to assess whether an LLM-generated workflow supports reproducible implementations.   To address this, we present AIRepr, an Analyst-Inspector framework for automatically evaluating and improving the reproducibility of LLM-generated data analysis workflows. Our framework is grounded in statistical principles and supports scalable, automated assessment. We introduce two novel reproducibility-enhancing prompting strategies and benchmark them against standard prompting across 15 analyst-inspector LLM pairs and 1,032 tasks from three public benchmarks. Our findings show that workflows with higher reproducibility also yield more accurate analyses, and that reproducibility-enhancing prompts substantially improve both metrics. This work provides a foundation for transparent, reliable, and efficient human-AI collaboration in data science. Our code is publicly available.

**Link**: [arxiv](http://arxiv.org/abs/2502.16395v3),  [pdf](http://arxiv.org/pdf/2502.16395v3)

**Tags**: cs.LG cs.AI cs.CL cs.HC 



### Sharing Intelligent Reflecting Surfaces in Multi-Operator Communication   Systems for Sustainable 6G Networks
**Authors**: Hiroaki Hashida, Yuichi Kawamoto, Nei Kato

**Updated**: 2025-11-07T04:12:02Z

**Summary**: In this study, we investigate the use of intelligent reflecting surfaces (IRSs) in multi-operator communication systems for 6G networks, focusing on sustainable and efficient resource management. This research is motivated by two critical challenges: limited coverage provided by mmWave frequencies and high infrastructure costs associated with current technologies. IRSs can help eliminate these issues because they can reflect electromagnetic waves to enhance signal propagation, thereby reducing blockages and extending network coverage. However, deploying a separate IRS for each mobile network operator (MNO) can result in inefficiencies, redundant infrastructure, potential conflicts over placement, and interoperator interference. To address these challenges, in this study, an IRS sharing system is proposed in which multiple MNOs collaborate to use a common IRS infrastructure. This approach not only enhances network flexibility and reduces costs but also minimizes the effect of interoperator interference. Through numerical analysis, we demonstrate that IRS sharing effectively balances performance and fairness among MNOs, outperforming MNO-specific deployment methods in multi-MNO scenarios. This study provides insights into the potential of IRS sharing to support sustainable 6G networks, thereby contributing to the efficient deployment and operation of next-generation wireless communication systems.

**Link**: [arxiv](http://arxiv.org/abs/2511.04969v1),  [pdf](http://arxiv.org/pdf/2511.04969v1)

**Tags**: cs.IT math.IT 



### Scientific judgment drifts over time in AI ideation
**Authors**: Lingyu Zhang, Mitchell Wang, Boyuan Chen

**Updated**: 2025-11-07T03:57:47Z

**Summary**: Scientific discovery begins with ideas, yet evaluating early-stage research concepts is a subtle and subjective human judgment. As large language models (LLMs) are increasingly tasked with generating scientific hypotheses, most systems assume that scientists' evaluations form a fixed gold standard, and that scientists' judgments do not change. Here we challenge this assumption. In a two-wave study with 7,182 ratings from 57 active researchers across six scientific departments, each participant repeatedly evaluated a constant "control" research idea alongside AI-generated ideas. We show that scientists' ratings of the very same idea systematically drift over time: overall quality scores increased by 0.61 points on a 0-10 scale (P = 0.005), and test-retest reliability was only moderate across core dimensions of scientific value, revealing systematic temporal drift in perceived idea quality. Yet the internal structure of judgment remained stable, such as the relative importance placed on originality, feasibility, clarity. We then aligned an LLM-based ideation system to first-wave human ratings and used it to select new ideas. Although alignment improved agreement with Wave-1 evaluations, its apparent gains disappeared once drift in human standards was accounted for. Thus, tuning to a fixed human snapshot produced improvements that were transient rather than persistent. These findings reveal that human evaluation of scientific ideas is not static but a dynamic process with stable priorities and requires shifting calibration. Treating one-time human ratings as immutable ground truth risks overstating progress in AI-assisted ideation and obscuring the challenge of co-evolving with changing expert standards. Drift-aware evaluation protocols and longitudinal benchmarks may therefore be essential for building AI systems that reliably augment, rather than overfit to, human scientific judgment.

**Link**: [arxiv](http://arxiv.org/abs/2511.04964v1),  [pdf](http://arxiv.org/pdf/2511.04964v1)

**Tags**: cs.HC 



### Too Good to be Bad: On the Failure of LLMs to Role-Play Villains
**Authors**: Zihao Yi, Qingxuan Jiang, Ruotian Ma, Xingyu Chen, Qu Yang, Mengru Wang, Fanghua Ye, Ying Shen, Zhaopeng Tu, Xiaolong Li, Linus

**Updated**: 2025-11-07T03:50:52Z

**Summary**: Large Language Models (LLMs) are increasingly tasked with creative generation, including the simulation of fictional characters. However, their ability to portray non-prosocial, antagonistic personas remains largely unexamined. We hypothesize that the safety alignment of modern LLMs creates a fundamental conflict with the task of authentically role-playing morally ambiguous or villainous characters. To investigate this, we introduce the Moral RolePlay benchmark, a new dataset featuring a four-level moral alignment scale and a balanced test set for rigorous evaluation. We task state-of-the-art LLMs with role-playing characters from moral paragons to pure villains. Our large-scale evaluation reveals a consistent, monotonic decline in role-playing fidelity as character morality decreases. We find that models struggle most with traits directly antithetical to safety principles, such as ``Deceitful'' and ``Manipulative'', often substituting nuanced malevolence with superficial aggression. Furthermore, we demonstrate that general chatbot proficiency is a poor predictor of villain role-playing ability, with highly safety-aligned models performing particularly poorly. Our work provides the first systematic evidence of this critical limitation, highlighting a key tension between model safety and creative fidelity. Our benchmark and findings pave the way for developing more nuanced, context-aware alignment methods.

**Link**: [arxiv](http://arxiv.org/abs/2511.04962v1),  [pdf](http://arxiv.org/pdf/2511.04962v1)

**Tags**: cs.CL cs.AI 



### ORCHID: Orchestrated Retrieval-Augmented Classification with   Human-in-the-Loop Intelligent Decision-Making for High-Risk Property
**Authors**: Maria Mahbub, Vanessa Lama, Sanjay Das, Brian Starks, Christopher Polchek, Saffell Silvers, Lauren Deck, Prasanna Balaprakash, Tirthankar Ghosal

**Updated**: 2025-11-07T03:48:05Z

**Summary**: High-Risk Property (HRP) classification is critical at U.S. Department of Energy (DOE) sites, where inventories include sensitive and often dual-use equipment. Compliance must track evolving rules designated by various export control policies to make transparent and auditable decisions. Traditional expert-only workflows are time-consuming, backlog-prone, and struggle to keep pace with shifting regulatory boundaries. We demo ORCHID, a modular agentic system for HRP classification that pairs retrieval-augmented generation (RAG) with human oversight to produce policy-based outputs that can be audited. Small cooperating agents, retrieval, description refiner, classifier, validator, and feedback logger, coordinate via agent-to-agent messaging and invoke tools through the Model Context Protocol (MCP) for model-agnostic on-premise operation. The interface follows an Item to Evidence to Decision loop with step-by-step reasoning, on-policy citations, and append-only audit bundles (run-cards, prompts, evidence). In preliminary tests on real HRP cases, ORCHID improves accuracy and traceability over a non-agentic baseline while deferring uncertain items to Subject Matter Experts (SMEs). The demonstration shows single item submission, grounded citations, SME feedback capture, and exportable audit artifacts, illustrating a practical path to trustworthy LLM assistance in sensitive DOE compliance workflows.

**Link**: [arxiv](http://arxiv.org/abs/2511.04956v1),  [pdf](http://arxiv.org/pdf/2511.04956v1)

**Tags**: cs.AI cs.CL 



### NVIDIA Nemotron Nano V2 VL
**Authors**: NVIDIA, :, Amala Sanjay Deshmukh, Kateryna Chumachenko, Tuomas Rintamaki, Matthieu Le, Tyler Poon, Danial Mohseni Taheri, Ilia Karmanov, Guilin Liu, Jarno Seppanen, Guo Chen, Karan Sapra, Zhiding Yu, Adi Renduchintala, Charles Wang, Peter Jin, Arushi Goel, Mike Ranzinger, Lukas Voegtle, Philipp Fischer, Timo Roman, Wei Ping, Boxin Wang, Zhuolin Yang, Nayeon Lee, Shaokun Zhang, Fuxiao Liu, Zhiqi Li, Di Zhang, Greg Heinrich, Hongxu Yin, Song Han, Pavlo Molchanov, Parth Mannan, Yao Xu, Jane Polak Scowcroft, Tom Balough, Subhashree Radhakrishnan, Paris Zhang, Sean Cha, Ratnesh Kumar, Zaid Pervaiz Bhat, Jian Zhang, Darragh Hanley, Pritam Biswas, Jesse Oliver, Kevin Vasques, Roger Waleffe, Duncan Riach, Oluwatobi Olabiyi, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Pritam Gundecha, Khanh Nguyen, Alexandre Milesi, Eugene Khvedchenia, Ran Zilberstein, Ofri Masad, Natan Bagrov, Nave Assaf, Tomer Asida, Daniel Afrimi, Amit Zuker, Netanel Haber, Zhiyu Cheng, Jingyu Xin, Di Wu, Nik Spirin, Maryam Moosaei, Roman Ageev, Vanshil Atul Shah, Yuting Wu, Daniel Korzekwa, Unnikrishnan Kizhakkemadam Sreekumar, Wanli Jiang, Padmavathy Subramanian, Alejandra Rico, Sandip Bhaskar, Saeid Motiian, Kedi Wu, Annie Surla, Chia-Chih Chen, Hayden Wolff, Matthew Feinberg, Melissa Corpuz, Marek Wawrzos, Eileen Long, Aastha Jhunjhunwala, Paul Hendricks, Farzan Memarian, Benika Hall, Xin-Yu Wang, David Mosallanezhad, Soumye Singhal, Luis Vega, Katherine Cheung, Krzysztof Pawelec, Michael Evans, Katherine Luna, Jie Lou, Erick Galinkin, Akshay Hazare, Kaustubh Purandare, Ann Guan, Anna Warno, Chen Cui, Yoshi Suhara, Shibani Likhite, Seph Mard, Meredith Price, Laya Sleiman, Saori Kaji, Udi Karpas, Kari Briski, Joey Conway, Michael Lightstone, Jan Kautz, Mohammad Shoeybi, Mostofa Patwary, Jonathen Cohen, Oleksii Kuchaiev, Andrew Tao, Bryan Catanzaro

**Updated**: 2025-11-07T03:45:07Z

**Summary**: We introduce Nemotron Nano V2 VL, the latest model of the Nemotron vision-language series designed for strong real-world document understanding, long video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers significant improvements over our previous model, Llama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major enhancements in model architecture, datasets, and training recipes. Nemotron Nano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and innovative token reduction techniques to achieve higher inference throughput in long document and video scenarios. We are releasing model checkpoints in BF16, FP8, and FP4 formats and sharing large parts of our datasets, recipes and training code.

**Link**: [arxiv](http://arxiv.org/abs/2511.03929v2),  [pdf](http://arxiv.org/pdf/2511.03929v2)

**Tags**: cs.LG cs.AI cs.CV 



### LEME: Open Large Language Models for Ophthalmology with Advanced   Reasoning and Clinical Validation
**Authors**: Hyunjae Kim, Xuguang Ai, Sahana Srinivasan, Aidan Gilson, Maxwell B. Singer, Krithi Pushpanathan, Qianqian Xie, Jungwoo Park, Serina Applebaum, Gabriel Dawei Yang, Minjie Zou, David Ziyou Chen, Ke Zou, Soshian Sarrafpour, Ji Liu, Yu Yin, Jimin Huang, Quang Ngoc Nguyen, Erping Long, Peixing Wan, Dianbo Liu, Richard Hintz, W. Jim Zheng, Sophia Y. Wang, Lucila Ohno-Machado, Hua Xu, Ron A. Adelman, Luciano V. Del Priore, Yih-Chung Tham, Qingyu Chen

**Updated**: 2025-11-07T03:19:42Z

**Summary**: The rising prevalence of eye diseases poses a growing public health burden. Large language models (LLMs) offer a promising path to reduce documentation workload and support clinical decision-making. However, few have been tailored for ophthalmology, and most evaluations focus mainly on knowledge-based QA without clinically relevant benchmarks or real-world validation. Here, we present LEME, a suite of open-weight LLMs developed through a two-stage process: (1) instruction tuning on 200,000 samples from clinical guidelines, textbooks, and case reports to enhance reasoning and task-following, and (2) reinforcement learning with ~30,000 preference labels to enhance accuracy and informativeness. LEME was evaluated on five curated zero-shot benchmarks spanning tasks such as patient QA, consultation, and treatment planning. It outperformed all seven baselines (all p < 0.004), exceeding GPT-4o by 3.32% (absolute ROUGE-L gain). It was further evaluated on three downstream tasks using deidentified patient data, reviewed by clinicians. In patient QA, LEME received the highest ratings from attending clinicians in 3 out of 4 criteria, with scores of 4.67 for factuality, 4.77 for specificity, 4.79 for completeness, and 4.88 for safety (1-5 scale). Its completeness score surpassed that of expert-written answers (4.79 vs. 4.56; p = 0.015). In visual acuity extraction, LEME achieved the highest F1, outperforming LLaMA-3 by 14.1% and Eye-LLaMA by 59.0%. In a pilot evaluation on assessment and treatment planning for diabetic retinopathy, AMD, and glaucoma, LEME received scores of 4.36 for factuality, 4.55 for specificity, 4.42 for completeness, and 4.36 for safety, approaching attending-level performance. All models, data, and code will be released to support further development and clinical translation, laying the groundwork for improved efficiency and patient care

**Link**: [arxiv](http://arxiv.org/abs/2410.03740v3),  [pdf](http://arxiv.org/pdf/2410.03740v3)

**Tags**: cs.CL 



### Generating Computational Cognitive Models using Large Language Models
**Authors**: Milena Rmus, Akshay K. Jagadish, Marvin Mathony, Tobias Ludwig, Eric Schulz

**Updated**: 2025-11-07T03:17:04Z

**Summary**: Computational cognitive models, which formalize theories of cognition, enable researchers to quantify cognitive processes and arbitrate between competing theories by fitting models to behavioral data. Traditionally, these models are handcrafted, which requires significant domain knowledge, coding expertise, and time investment. However, recent advances in machine learning offer solutions to these challenges. In particular, Large Language Models (LLMs) have demonstrated remarkable capabilities for in-context pattern recognition, leveraging knowledge from diverse domains to solve complex problems, and generating executable code that can be used to facilitate the generation of cognitive models. Building on this potential, we introduce a pipeline for Guided generation of Computational Cognitive Models (GeCCo). Given task instructions, participant data, and a template function, GeCCo prompts an LLM to propose candidate models, fits proposals to held-out data, and iteratively refines them based on feedback constructed from their predictive performance. We benchmark this approach across four different cognitive domains -- decision making, learning, planning, and memory -- using three open-source LLMs, spanning different model sizes, capacities, and families. On four human behavioral data sets, the LLM generated models that consistently matched or outperformed the best domain-specific models from the cognitive science literature. Taken together, our results suggest that LLMs can generate cognitive models with conceptually plausible theories that rival -- or even surpass -- the best models from the literature across diverse task domains.

**Link**: [arxiv](http://arxiv.org/abs/2502.00879v3),  [pdf](http://arxiv.org/pdf/2502.00879v3)

**Tags**: cs.LG 



### Black-box Context-free Grammar Inference for Readable & Natural Grammars
**Authors**: Mohammad Rifat Arefin, Shanto Rahman, Christoph Csallner

**Updated**: 2025-11-07T03:12:30Z

**Summary**: Black-box context-free grammar inference is crucial for program analysis, reverse engineering, and security, yet existing tools such as Arvada, TreeVada, and Kedavra struggle with scalability, readability, and accuracy on large, complex languages. We present NatGI, a novel LLM-guided grammar inference framework that extends TreeVada's parse tree recovery with three key innovations: bracket-guided bubble exploration, LLM-driven bubble generation and non-terminal labeling, and hierarchical delta debugging (HDD) for systematic tree simplification. Bracket-guided exploration leverages syntactic cues such as parentheses to propose well-structured grammar fragments, while LLM guidance produces meaningful non-terminal names and selects more promising merges. Finally, HDD incrementally reduces unnecessary rules, which makes the grammars both compact and interpretable. In our experiments, we evaluate NatGI on a comprehensive benchmark suite ranging from small languages to larger ones such as lua, c, and mysql. Our results show that NatGI consistently outperforms strong baselines in terms of F1 score. On average, NatGI achieves an F1 score of 0.57, which is 25pp (percentage points) higher than the best-performing baseline, TreeVada. In the case of interpretability, our generated grammars perform significantly better than those produced by existing approaches. Leveraging LLM-based node renaming and bubble exploration, NatGI produces rules with meaningful non-terminal names and compact structures that align more closely with human intuition. As a result, developers and researchers can achieve higher accuracy while still being able to easily inspect, verify, and reason about the structure and semantics of the induced grammars.

**Link**: [arxiv](http://arxiv.org/abs/2509.26616v2),  [pdf](http://arxiv.org/pdf/2509.26616v2)

**Tags**: cs.SE cs.FL cs.PL 68Q42, 68Q45 (Primary), 68T50 (Secondary) D.2.5; F.4.2 



### L2T-Tune:LLM-Guided Hybrid Database Tuning with LHS and TD3
**Authors**: Xinyue Yang, Chen Zheng, Yaoyang Hou, Renhao Zhang, Yinyan Zhang, Yanjun Wu, Heng Zhang

**Updated**: 2025-11-07T03:06:56Z

**Summary**: Configuration tuning is critical for database performance. Although recent advancements in database tuning have shown promising results in throughput and latency improvement, challenges remain. First, the vast knob space makes direct optimization unstable and slow to converge. Second, reinforcement learning pipelines often lack effective warm-start guidance and require long offline training. Third, transferability is limited: when hardware or workloads change, existing models typically require substantial retraining to recover performance.   To address these limitations, we propose L2T-Tune, a new LLM-guided hybrid database tuning framework that features a three-stage pipeline: Stage one performs a warm start that simultaneously generates uniform samples across the knob space and logs them into a shared pool; Stage two leverages a large language model to mine and prioritize tuning hints from manuals and community documents for rapid convergence. Stage three uses the warm-start sample pool to reduce the dimensionality of knobs and state features, then fine-tunes the configuration with the Twin Delayed Deep Deterministic Policy Gradient algorithm.   We conduct experiments on L2T-Tune and the state-of-the-art models. Compared with the best-performing alternative, our approach improves performance by an average of 37.1% across all workloads, and by up to 73% on TPC-C. Compared with models trained with reinforcement learning, it achieves rapid convergence in the offline tuning stage on a single server. Moreover, during the online tuning stage, it only takes 30 steps to achieve best results.

**Link**: [arxiv](http://arxiv.org/abs/2511.01602v3),  [pdf](http://arxiv.org/pdf/2511.01602v3)

**Tags**: cs.DB cs.LG 



