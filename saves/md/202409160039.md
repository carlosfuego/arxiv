# Arxiv Results
## Keyword: kv cache 
 ### Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent   Interconnects
**Authors**: Anastasiia Ruzhanskaia, Pengcheng Xu, David Cock, Timothy Roscoe

**Updated**: 2024-09-12T15:34:23Z

**Summary**: Conventional wisdom holds that an efficient interface between an OS running on a CPU and a high-bandwidth I/O device should be based on Direct Memory Access (DMA), descriptor rings, and interrupts: DMA offloads transfers from the CPU, descriptor rings provide buffering and queuing, and interrupts facilitate asynchronous interaction between cores and device with a lightweight notification mechanism. In this paper we question this wisdom in the light of modern hardware and workloads, particularly in cloud servers. We argue that the assumptions that led to this model are obsolete, and in many use-cases use of programmed I/O, where the CPU explicitly transfers data and control information to and from a device via loads and stores, actually results in a more efficient system. We quantitatively demonstrate these advantages using three use-cases: fine-grained RPC-style invocation of functions on an accelerator, offloading of operators in a streaming dataflow engine, and a network interface targeting for serverless functions. Moreover, we show that while these advantages are significant over a modern PCIe peripheral bus, a truly cache-coherent interconnect offers significant additional efficiency gains.

**Link**: [arxiv](http://arxiv.org/abs/2409.08141v1),  [pdf](http://arxiv.org/pdf/2409.08141v1)

**Tags**: cs.AR cs.OS 



### Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect
**Authors**: Priya Sharma, Alexander V. Balatsky

**Updated**: 2024-09-12T10:35:15Z

**Summary**: We present a microscopic calculation of the inverse Faraday effect in metals. We derive a static local magnetic moment induced on the application of high-frequency light, using the Eilenberger formulation of quasiclassical theory. We include the effect of disorder and formulate a theory applicable across the entire temperature range, in the absence of external applied fields. For light-induced electric fields of amplitude $\sim 100 kV/cm$, the induced fields are large, $\sim 0.1 T$ for metallic Nb! The predictions of our theory agree with recent experimental and theoretical results [1]. An extension of this approach to superconductors would open a new route of inducing orbital magnetic field and potentially vortices in superconductors.

**Link**: [arxiv](http://arxiv.org/abs/2303.01699v5),  [pdf](http://arxiv.org/pdf/2303.01699v5)

**Tags**: cond-mat.supr-con 



### Super Monotonic Alignment Search
**Authors**: Junhyeok Lee, Hyeongju Kim

**Updated**: 2024-09-12T02:13:57Z

**Summary**: Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most popular algorithm in TTS to estimate unknown alignments between text and speech. Since this algorithm needs to search for the most probable alignment with dynamic programming by caching all paths, the time complexity of the algorithm is $O(T \times S)$. The authors of Glow-TTS run this algorithm on CPU, and while they mentioned it is difficult to parallelize, we found that MAS can be parallelized in text-length dimension and CPU execution consumes an inordinate amount of time for inter-device copy. Therefore, we implemented a Triton kernel and PyTorch JIT script to accelerate MAS on GPU without inter-device copy. As a result, Super-MAS Triton kernel is up to 72 times faster in the extreme-length case. The code is available at \url{https://github.com/supertone-inc/super-monotonic-align}.

**Link**: [arxiv](http://arxiv.org/abs/2409.07704v1),  [pdf](http://arxiv.org/pdf/2409.07704v1)

**Tags**: eess.AS cs.AI 



### Learning to Compress Contexts for Efficient Knowledge-based Visual   Question Answering
**Authors**: Weixi Weng, Jieming Zhu, Hao Zhang, Xiaojun Meng, Rui Zhang, Chun Yuan

**Updated**: 2024-09-11T15:11:39Z

**Summary**: Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot performance on visual question answering (VQA). However, when it comes to knowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized domain knowledge to answer such questions and require obtaining necessary information from external knowledge sources. Previous works like Retrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input information, such as image-based textual descriptions and retrieved knowledge, as possible to improve performance, but they all overlook the issue that with the number of input tokens increasing, inference efficiency significantly decreases, which contradicts the demands of practical applications. To address this issue, we propose Retrieval-Augmented MLLM with Compressed Contexts (RACC). RACC learns to compress and aggregate retrieved contexts, from which it generates a compact modulation in the form of Key-Value (KV) cache. This modulation is then used to adapt the downstream frozen MLLM, thereby achieving effective and efficient inference. RACC achieves a state-of-the-art (SOTA) performance of 62.9% on OK-VQA. Moreover, it significantly reduces inference latency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments show RACC's broad applicability. It is compatible with various off-the-shelf MLLMs and can also handle different knowledge sources including textual and multimodal documents.

**Link**: [arxiv](http://arxiv.org/abs/2409.07331v1),  [pdf](http://arxiv.org/pdf/2409.07331v1)

**Tags**: cs.CV cs.LG 



### Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma   Generated THz Pulses
**Authors**: Benjamin Colmey, Rodrigo T. Paulino, David G. Cooke

**Updated**: 2024-09-11T11:40:23Z

**Summary**: Terahertz pulses generated by two-color laser plasmas have reported peak field strengths exceeding MV/cm, and when illuminating metal nanotips the near-field enhancement at the tip apex should result in extremely high bunch charges and electron energies via sub-cycle cold field emission. Here, electron emission from tungsten nanotips driven by THz pulses generated by a long filament air-plasma are reported. Electron energies up to 1.1 keV and bunch charges up to 2x$10^5$ electrons per pulse were detected, well below values expected for peak field calculated via the time averaged Poynting vector. Investigations revealed a failure in the use of the time-averaged Poynting vector when applied to long filament THz pulses, due to spatio-temporal restructuring of the THz pulse in the focus. Accounting for this restructuring significantly reduces the field strength to approximately 160 ~kV/cm, consistent with the observed electron bunch charges, peak energies and their dependence on the tip position in the THz focus. Despite these findings, our results surpass previous THz plasma-driven electron generation by an order of magnitude in both electron energy and bunch charge and a path to increasing these by an additional order of magnitude by modification of the THz optics is proposed.

**Link**: [arxiv](http://arxiv.org/abs/2409.07196v1),  [pdf](http://arxiv.org/pdf/2409.07196v1)

**Tags**: cond-mat.mtrl-sci physics.plasm-ph 



### In-Loop Filtering via Trained Look-Up Tables
**Authors**: Zhuoyuan Li, Jiacheng Li, Yao Li, Li Li, Dong Liu, Feng Wu

**Updated**: 2024-09-11T08:12:55Z

**Summary**: In-loop filtering (ILF) is a key technology for removing the artifacts in image/video coding standards. Recently, neural network-based in-loop filtering methods achieve remarkable coding gains beyond the capability of advanced video coding standards, which becomes a powerful coding tool candidate for future video coding standards. However, the utilization of deep neural networks brings heavy time and computational complexity, and high demands of high-performance hardware, which is challenging to apply to the general uses of coding scene. To address this limitation, inspired by explorations in image restoration, we propose an efficient and practical in-loop filtering scheme by adopting the Look-up Table (LUT). We train the DNN of in-loop filtering within a fixed filtering reference range, and cache the output values of the DNN into a LUT via traversing all possible inputs. At testing time in the coding process, the filtered pixel is generated by locating input pixels (to-be-filtered pixel with reference pixels) and interpolating cached filtered pixel values. To further enable the large filtering reference range with the limited storage cost of LUT, we introduce the enhanced indexing mechanism in the filtering process, and clipping/finetuning mechanism in the training. The proposed method is implemented into the Versatile Video Coding (VVC) reference software, VTM-11.0. Experimental results show that the ultrafast, very fast, and fast mode of the proposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39% BD-rate reduction, under the all intra (AI) and random access (RA) configurations. Especially, our method has friendly time and computational complexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel, and only 164-1148 KB storage cost for a single model. Our solution may shed light on the journey of practical neural network-based coding tool evolution.

**Link**: [arxiv](http://arxiv.org/abs/2407.10926v2),  [pdf](http://arxiv.org/pdf/2407.10926v2)

**Tags**: eess.IV cs.CV 



### Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free   Massive MIMO Systems
**Authors**: Yu Zhang, Shuaifei Chen, Jiayi Zhang

**Updated**: 2024-09-11T02:33:06Z

**Summary**: Cell-free massive multiple-input-multiple-output is promising to meet the stringent quality-of-experience (QoE) requirements of railway wireless communications by coordinating many successional access points (APs) to serve the onboard users coherently. A key challenge is how to deliver the desired contents timely due to the radical changing propagation environment caused by the growing train speed. In this paper, we propose to proactively cache the likely-requesting contents at the upcoming APs which perform the coherent transmission to reduce end-to-end delay. A long-term QoE-maximization problem is formulated and two cache placement algorithms are proposed. One is based on heuristic convex optimization (HCO) and the other exploits deep reinforcement learning (DRL) with soft actor-critic (SAC). Compared to the conventional benchmark, numerical results show the advantage of our proposed algorithms on QoE and hit probability. With the advanced DRL model, SAC outperforms HCO on QoE by predicting the user requests accurately.

**Link**: [arxiv](http://arxiv.org/abs/2208.12453v2),  [pdf](http://arxiv.org/pdf/2208.12453v2)

**Tags**: cs.IT cs.AI math.IT 



### With Greater Text Comes Greater Necessity: Inference-Time Training Helps   Long Text Generation
**Authors**: Y. Wang, D. Ma, D. Cai

**Updated**: 2024-09-11T02:22:58Z

**Summary**: Long text generation, such as novel writing and discourse-level translation with extremely long contexts, presents significant challenges to current language models. Existing methods mainly focus on extending the model's context window through strategies like length extrapolation. However, these approaches demand substantial hardware resources during the training and/or inference phases. Our proposed method, Temp-Lora, introduces an alternative concept. Instead of relying on the KV cache to store all context information, we embeds this information directly into a temporary Lora module. In the process of long text generation, this module is progressively trained with text generated previously. This approach not only efficiently preserves contextual knowledge but also prevents any permanent alteration to the model's parameters given that the module is discarded post-generation. Extensive experiments on the PG19 language modeling benchmark and the GuoFeng discourse-level translation benchmark validate the effectiveness of Temp-Lora. Our results show that: 1) Temp-Lora substantially enhances generation quality for long text, as indicated by a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3% decrease in PPL along with a 113.2% increase in BLEU score on a subset of GuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text generation methods, and 3) Temp-Lora can greatly reduce computational costs by shortening the context window. For example, we can ensure a moderate improvement in generation quality (a decrease of 3.8% in PPL) while enabling a 51.5% memory usage reduction and a 60.0% decrease in latency for inference.

**Link**: [arxiv](http://arxiv.org/abs/2401.11504v3),  [pdf](http://arxiv.org/pdf/2401.11504v3)

**Tags**: cs.CL cs.AI 



### DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online   Surgical Phase Recognition
**Authors**: Kaixiang Yang, Qiang Li, Zhiwei Wang

**Updated**: 2024-09-10T04:58:48Z

**Summary**: Surgical phase recognition has become a crucial requirement in laparoscopic surgery, enabling various clinical applications like surgical risk forecasting. Current methods typically identify the surgical phase using individual frame-wise embeddings as the fundamental unit for time modeling. However, this approach is overly sensitive to current observations, often resulting in discontinuous and erroneous predictions within a complete surgical phase. In this paper, we propose DACAT, a novel dual-stream model that adaptively learns clip-aware context information to enhance the temporal relationship. In one stream, DACAT pretrains a frame encoder, caching all historical frame-wise features. In the other stream, DACAT fine-tunes a new frame encoder to extract the frame-wise feature at the current moment. Additionally, a max clip-response read-out (Max-R) module is introduced to bridge the two streams by using the current frame-wise feature to adaptively fetch the most relevant past clip from the feature cache. The clip-aware context feature is then encoded via cross-attention between the current frame and its fetched adaptive clip, and further utilized to enhance the time modeling for accurate online surgical phase recognition. The benchmark results on three public datasets, i.e., Cholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed DACAT over existing state-of-the-art methods, with improvements in Jaccard scores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have been released at https://github.com/kk42yy/DACAT.

**Link**: [arxiv](http://arxiv.org/abs/2409.06217v1),  [pdf](http://arxiv.org/pdf/2409.06217v1)

**Tags**: cs.CV 



### Design and Implementation of Online Live Streaming System Using A 3D   Engine
**Authors**: Aizierjiang Aiersilan

**Updated**: 2024-09-10T04:24:22Z

**Summary**: With the growing demand for live video streaming, there is an increasing need for low-latency and high-quality transmission, especially with the advent of 5G networks. While 5G offers hardware-level improvements, effective software solutions for minimizing latency remain essential. Current methods, such as multi-channel streaming, fail to address latency issues fundamentally, often only adding new channels without optimizing overall performance. This thesis proposes a novel approach using a 3D engine (e.g., Unity 3D) to stream multi-input video data through a single channel with reduced latency. By leveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D Canvases, and Webcam Textures, the proposed system consolidates video streams from multiple external cameras into a unified, low-latency output. The affiliated project of this thesis demonstrates the implementation of a low-latency multi-channel live video streaming system. It employs the RTSP protocol and examines video encoding techniques, alongside a client-side application based on Unity 3D. The system architecture includes a WebSocket server for persistent connections, an HTTP server for communication, a MySQL database for storage, Redis for caching, and Nginx for load balancing. Each module operates independently, ensuring flexibility and scalability in the system's design. A key innovation of this system is its use of a 3D scene to map multiple video inputs onto a virtual canvas, recorded by an in-engine camera for transmission. This design minimizes redundant data, enabling an efficient and director-guided live streaming network. The thesis concludes by discussing challenges encountered during the project and provides solutions for future improvement.

**Link**: [arxiv](http://arxiv.org/abs/2409.06207v1),  [pdf](http://arxiv.org/pdf/2409.06207v1)

**Tags**: cs.NI cs.MM 



### Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering
**Authors**: Benjamin Attal, Dor Verbin, Ben Mildenhall, Peter Hedman, Jonathan T. Barron, Matthew O'Toole, Pratul P. Srinivasan

**Updated**: 2024-09-09T17:59:57Z

**Summary**: State-of-the-art techniques for 3D reconstruction are largely based on volumetric scene representations, which require sampling multiple points to compute the color arriving along a ray. Using these representations for more general inverse rendering -- reconstructing geometry, materials, and lighting from observed images -- is challenging because recursively path-tracing such volumetric representations is expensive. Recent works alleviate this issue through the use of radiance caches: data structures that store the steady-state, infinite-bounce radiance arriving at any point from any direction. However, these solutions rely on approximations that introduce bias into the renderings and, more importantly, into the gradients used for optimization. We present a method that avoids these approximations while remaining computationally efficient. In particular, we leverage two techniques to reduce variance for unbiased estimators of the rendering equation: (1) an occlusion-aware importance sampler for incoming illumination and (2) a fast cache architecture that can be used as a control variate for the radiance from a high-quality, but more expensive, volumetric cache. We show that by removing these biases our approach improves the generality of radiance cache based inverse rendering, as well as increasing quality in the presence of challenging light transport effects such as specular reflections.

**Link**: [arxiv](http://arxiv.org/abs/2409.05867v1),  [pdf](http://arxiv.org/pdf/2409.05867v1)

**Tags**: cs.GR cs.CV 



### WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild
**Authors**: Yuntian Deng, Wenting Zhao, Jack Hessel, Xiang Ren, Claire Cardie, Yejin Choi

**Updated**: 2024-09-09T10:04:00Z

**Summary**: The increasing availability of real-world conversation data offers exciting opportunities for researchers to study user-chatbot interactions. However, the sheer volume of this data makes manually examining individual conversations impractical. To overcome this challenge, we introduce WildVis, an interactive tool that enables fast, versatile, and large-scale conversation analysis. WildVis provides search and visualization capabilities in the text and embedding spaces based on a list of criteria. To manage million-scale datasets, we implemented optimizations including search index construction, embedding precomputation and compression, and caching to ensure responsive user interactions within seconds. We demonstrate WildVis' utility through three case studies: facilitating chatbot misuse research, visualizing and comparing topic distributions across datasets, and characterizing user-specific conversation patterns. WildVis is open-source and designed to be extendable, supporting additional datasets and customized search and visualization functionalities.

**Link**: [arxiv](http://arxiv.org/abs/2409.03753v2),  [pdf](http://arxiv.org/pdf/2409.03753v2)

**Tags**: cs.CL cs.AI cs.HC cs.IR cs.LG 



### Cooperative Learning-Based Framework for VNF Caching and Placement   Optimization over Low Earth Orbit Satellite Networks
**Authors**: Khai Doan, Marios Avgeris, Aris Leivadeas, Ioannis Lambadaris, Wonjae Shin

**Updated**: 2024-09-08T08:39:50Z

**Summary**: Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad range of modern applications, which are typically modeled as Service Function Chains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where each VNF performs a specific task. In this work, we tackle two key challenges in deploying SFCs across an LSN. Firstly, we aim to optimize the long-term system performance by minimizing the average end-to-end SFC execution delay, given that each satellite comes with a pre-installed/cached subset of VNFs. To achieve optimal SFC placement, we formulate an offline Dynamic Programming (DP) equation. To overcome the challenges associated with DP, such as its complexity, the need for probability knowledge, and centralized decision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution. Our MAQL approach addresses convergence issues in the non-stationary LSN environment by enabling satellites to share learning parameters and update their Q-tables based on distinct rules for their selected actions. Secondly, to determine the optimal VNF subsets for satellite caching, we develop a Bayesian Optimization (BO)-based learning mechanism that operates both offline and continuously in the background during runtime. Extensive experiments demonstrate that our MAQL approach achieves near-optimal performance comparable to the DP model and significantly outperforms existing baselines. Moreover, the BO-based approach effectively enhances the request serving rate over time.

**Link**: [arxiv](http://arxiv.org/abs/2409.05025v1),  [pdf](http://arxiv.org/pdf/2409.05025v1)

**Tags**: cs.IT cs.SY eess.SY math.IT 



### InstInfer: In-Storage Attention Offloading for Cost-Effective   Long-Context LLM Inference
**Authors**: Xiurui Pan, Endian Li, Qiao Li, Shengwen Liang, Yizhou Shan, Ke Zhou, Yingwei Luo, Xiaolin Wang, Jie Zhang

**Updated**: 2024-09-08T06:06:44Z

**Summary**: The widespread of Large Language Models (LLMs) marks a significant milestone in generative AI. Nevertheless, the increasing context length and batch size in offline LLM inference escalate the memory requirement of the key-value (KV) cache, which imposes a huge burden on the GPU VRAM, especially for resource-constraint scenarios (e.g., edge computing and personal devices). Several cost-effective solutions leverage host memory or SSDs to reduce storage costs for offline inference scenarios and improve the throughput. Nevertheless, they suffer from significant performance penalties imposed by intensive KV cache accesses due to limited PCIe bandwidth. To address these issues, we propose InstInfer, a novel LLM inference system that offloads the most performance-critical computation (i.e., attention in decoding phase) and data (i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize the enormous KV transfer overheads. InstInfer designs a dedicated flash-aware in-storage attention engine with KV cache management mechanisms to exploit the high internal bandwidths of CSDs instead of being limited by the PCIe bandwidth. The optimized P2P transmission between GPU and CSDs further reduces data migration overheads. Experimental results demonstrate that for a 13B model using an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence inference by up to 11.1$\times$, compared to existing SSD-based solutions such as FlexGen.

**Link**: [arxiv](http://arxiv.org/abs/2409.04992v1),  [pdf](http://arxiv.org/pdf/2409.04992v1)

**Tags**: cs.AR cs.CL 



### Training-Free Style Consistent Image Synthesis with Condition and Mask   Guidance in E-Commerce
**Authors**: Guandong Li

**Updated**: 2024-09-07T07:50:13Z

**Summary**: Generating style-consistent images is a common task in the e-commerce field, and current methods are largely based on diffusion models, which have achieved excellent results. This paper introduces the concept of the QKV (query/key/value) level, referring to modifications in the attention maps (self-attention and cross-attention) when integrating UNet with image conditions. Without disrupting the product's main composition in e-commerce images, we aim to use a train-free method guided by pre-set conditions. This involves using shared KV to enhance similarity in cross-attention and generating mask guidance from the attention map to cleverly direct the generation of style-consistent images. Our method has shown promising results in practical applications.

**Link**: [arxiv](http://arxiv.org/abs/2409.04750v1),  [pdf](http://arxiv.org/pdf/2409.04750v1)

**Tags**: cs.CV 



### MiniCache: KV Cache Compression in Depth Dimension for Large Language   Models
**Authors**: Akide Liu, Jing Liu, Zizheng Pan, Yefei He, Gholamreza Haffari, Bohan Zhuang

**Updated**: 2024-09-07T02:52:29Z

**Summary**: A critical approach for efficiently deploying computationally demanding large language models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value states of previously generated tokens, significantly reducing the need for repetitive computations and thereby lowering latency in autoregressive generation. However, the size of the KV cache grows linearly with sequence length, posing challenges for applications requiring long context input and extensive sequence generation. In this paper, we present a simple yet effective approach, called MiniCache, to compress the KV cache across layers from a novel depth perspective, significantly reducing the memory footprint for LLM inference. Our approach is based on the observation that KV cache states exhibit high similarity between the adjacent layers in the middle-to-deep portion of LLMs. To facilitate merging, we propose disentangling the states into the magnitude and direction components, interpolating the directions of the state vectors while preserving their lengths unchanged. Furthermore, we introduce a token retention strategy to keep highly distinct state pairs unmerged, thus preserving the information with minimal additional storage overhead. Our MiniCache is training-free and general, complementing existing KV cache compression strategies, such as quantization and sparsity. We conduct a comprehensive evaluation of MiniCache utilizing various models including LLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks, demonstrating its exceptional performance in achieving superior compression ratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit MiniCache achieves a remarkable compression ratio of up to 5.02x, enhances inference throughput by approximately 5x, and reduces the memory footprint by 41% compared to the FP16 full cache baseline, all while maintaining near-lossless performance.

**Link**: [arxiv](http://arxiv.org/abs/2405.14366v2),  [pdf](http://arxiv.org/pdf/2405.14366v2)

**Tags**: cs.CL cs.AI cs.LG 



### QET: Enhancing Quantized LLM Parameters and KV cache Compression through   Element Substitution and Residual Clustering
**Authors**: Yanshu Wang, Wang Li, Zhaoqian Yao, Tong Yang

**Updated**: 2024-09-06T08:28:01Z

**Summary**: The matrix quantization entails representing matrix elements in a more space-efficient form to reduce storage usage, with dequantization restoring the original matrix for use. We formulate the Quantization Error Minimization (QEM) problem as minimizing the distance between a matrix before and after quantization, under the condition that the quantized matrix occupies the same memory space. Matrix quantization is crucial in various applications, including Large Language Models (LLMs) weight quantization, vector databases, KV cache quantization, graph compression, and image compression. Recent advancements in LLMs, such as GPT-4 and BERT, have highlighted the importance of matrix compression due to the large size of parameters and KV cache, which are stored as matrices.   We propose Quantum Entanglement Trees (QET) to address the QEM problem by leveraging the local orderliness of matrix elements, involving iterative element swapping to form a locally ordered matrix. This matrix is then grouped and quantized by columns. To enhance QET, we introduce two optimizations: further quantizing residuals to reduce MSE, and using masking and batch processing to accelerate the algorithm.   Experimental results demonstrate that QET can effectively reduce MSE to 5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K cache, and V cache, respectively. Our contributions include the abstraction of the QEM problem, the design of the QET algorithm, and the proposal of two optimizations to improve accuracy and speed.

**Link**: [arxiv](http://arxiv.org/abs/2407.03637v4),  [pdf](http://arxiv.org/pdf/2407.03637v4)

**Tags**: cs.LG cs.CL 



### A First Look At Efficient And Secure On-Device LLM Inference Against KV   Leakage
**Authors**: Huan Yang, Deyu Zhang, Yudong Zhao, Yuanchun Li, Yunxin Liu

**Updated**: 2024-09-06T06:16:55Z

**Summary**: Running LLMs on end devices has garnered significant attention recently due to their advantages in privacy preservation. With the advent of lightweight LLM models and specially designed GPUs, on-device LLM inference has achieved the necessary accuracy and performance metrics. However, we have identified that LLM inference on GPUs can leak privacy-sensitive intermediate information, specifically the KV pairs. An attacker could exploit these KV pairs to reconstruct the entire user conversation, leading to significant vulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE) and Trusted Execution Environments (TEE), are either too computation-intensive or resource-limited. To address these issues, we designed KV-Shield, which operates in two phases. In the initialization phase, it permutes the weight matrices so that all KV pairs are correspondingly permuted. During the runtime phase, the attention vector is inversely permuted to ensure the correctness of the layer output. All permutation-related operations are executed within the TEE, ensuring that insecure GPUs cannot access the original KV pairs, thus preventing conversation reconstruction. Finally, we theoretically analyze the correctness of KV-Shield, along with its advantages and overhead.

**Link**: [arxiv](http://arxiv.org/abs/2409.04040v1),  [pdf](http://arxiv.org/pdf/2409.04040v1)

**Tags**: cs.CR cs.AI 



### Potential and Limitation of High-Frequency Cores and Caches
**Authors**: Kunal Pai, Anusheel Nand, Jason Lowe-Power

**Updated**: 2024-09-05T20:21:54Z

**Summary**: This paper explores the potential of cryogenic semiconductor computing and superconductor electronics as promising alternatives to traditional semiconductor devices. As semiconductor devices face challenges such as increased leakage currents and reduced performance at higher temperatures, these novel technologies offer high performance and low power computation. Conventional semiconductor electronics operating at cryogenic temperatures (below -150{\deg}C or 123.15 K) can benefit from reduced leakage currents and improved electron mobility. On the other hand, superconductor electronics, operating below 10 K, allow electrons to flow without resistance, offering the potential for ultra-low-power, high-speed computation. This study presents a comprehensive performance modeling and analysis of these technologies and provides insights into their potential benefits and limitations. We implement models of in-order and out-of-order cores operating at high clock frequencies associated with superconductor electronics and cryogenic semiconductor computing in gem5. We evaluate the performance of these components using workloads representative of real-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the potential speedups achievable by these components and the limitations posed by cache bandwidth. This work provides valuable insights into the performance implications and design trade-offs associated with cryogenic and superconductor technologies, laying the foundation for future research in this field using gem5.

**Link**: [arxiv](http://arxiv.org/abs/2408.03308v2),  [pdf](http://arxiv.org/pdf/2408.03308v2)

**Tags**: cs.AR 



### Libra: Architectural Support For Principled, Secure And Efficient   Balanced Execution On High-End Processors (Extended Version)
**Authors**: Hans Winderix, Marton Bognar, Lesly-Ann Daniel, Frank Piessens

**Updated**: 2024-09-05T17:56:19Z

**Summary**: Control-flow leakage (CFL) attacks enable an attacker to expose control-flow decisions of a victim program via side-channel observations. Linearization (i.e., elimination) of secret-dependent control flow is the main countermeasure against these attacks, yet it comes at a non-negligible cost. Conversely, balancing secret-dependent branches often incurs a smaller overhead, but is notoriously insecure on high-end processors. Hence, linearization has been widely believed to be the only effective countermeasure against CFL attacks. In this paper, we challenge this belief and investigate an unexplored alternative: how to securely balance secret-dependent branches on higher-end processors?   We propose Libra, a generic and principled hardware-software codesign to efficiently address CFL on high-end processors. We perform a systematic classification of hardware primitives leaking control flow from the literature, and provide guidelines to handle them with our design. Importantly, Libra enables secure control-flow balancing without the need to disable performance-critical hardware such as the instruction cache and the prefetcher. We formalize the semantics of Libra and propose a code transformation algorithm for securing programs, which we prove correct and secure. Finally, we implement and evaluate Libra on an out-of-order RISC-V processor, showing performance overhead on par with insecure balanced code, and outperforming state-of-the-art linearized code by 19.3%.

**Link**: [arxiv](http://arxiv.org/abs/2409.03743v1),  [pdf](http://arxiv.org/pdf/2409.03743v1)

**Tags**: cs.CR 



### Enabling Practical and Privacy-Preserving Image Processing
**Authors**: Chao Wang, Shubing Yang, Xiaoyan Sun, Jun Dai, Dongfang Zhao

**Updated**: 2024-09-05T14:22:02Z

**Summary**: Fully Homomorphic Encryption (FHE) enables computations on encrypted data, preserving confidentiality without the need for decryption. However, FHE is often hindered by significant performance overhead, particularly for high-precision and complex data like images. Due to serious efficiency issues, traditional FHE methods often encrypt images by monolithic data blocks (such as pixel rows), instead of pixels. However, this strategy compromises the advantages of homomorphic operations and disables pixel-level image processing. In this study, we address these challenges by proposing and implementing a pixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS scheme. To enhance computational efficiency, we introduce three novel caching mechanisms to pre-encrypt radix values or frequently occurring pixel values, substantially reducing redundant encryption operations. Extensive experiments demonstrate that our approach achieves up to a 19-fold improvement in encryption speed compared to the original CKKS, while maintaining high image quality. Additionally, real-world image applications such as mean filtering, brightness enhancement, image matching and watermarking are tested based on FHE, showcasing up to a 91.53% speed improvement. We also proved that our method is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure, providing strong encryption security. These results underscore the practicality and efficiency of iCHEETAH, marking a significant advancement in privacy-preserving image processing at scale.

**Link**: [arxiv](http://arxiv.org/abs/2409.03568v1),  [pdf](http://arxiv.org/pdf/2409.03568v1)

**Tags**: cs.CR C.2.0; K.6.5 



### SELCC: Coherent Caching over Compute-Limited Disaggregated Memory
**Authors**: Ruihong Wang, Jianguo Wang, Walid G. Aref

**Updated**: 2024-09-05T01:12:04Z

**Summary**: Disaggregating memory from compute offers the opportunity to better utilize stranded memory in data centers. It is important to cache data in the compute nodes and maintain cache coherence across multiple compute nodes to save on round-trip communication cost between the disaggregated memory and the compute nodes. However, the limited computing power on the disaggregated memory servers makes it challenging to maintain cache coherence among multiple compute-side caches over disaggregated shared memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache Coherence protocol that maintains cache coherence without imposing any computational burden on the remote memory side. SELCC builds on a one-sided shared-exclusive latch protocol by introducing lazy latch release and invalidation messages among the compute nodes so that it can guarantee both data access atomicity and cache coherence. SELCC minimizes communication round-trips by embedding the current cache copy holder IDs into RDMA latch words and prioritizes local concurrency control over global concurrency control. We instantiate the SELCC protocol onto compute-sided cache, forming an abstraction layer over disaggregated memory. This abstraction layer provides main-memory-like APIs to upper-level applications, and thus enabling existing data structures and algorithms to function over disaggregated memory with minimal code change. To demonstrate the usability of SELCC, we implement a B-tree and three transaction concurrency control algorithms over SELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves better performance compared to RPC-based cache-coherence protocols. Additionally, YCSB and TPC-C benchmarks indicate that applications over SELCC can achieve comparable or superior performance against competitors over disaggregated memory.

**Link**: [arxiv](http://arxiv.org/abs/2409.02088v2),  [pdf](http://arxiv.org/pdf/2409.02088v2)

**Tags**: cs.DB cs.DC cs.ET 



### Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in   Fine-tuning LLMs for Simultaneous Translation
**Authors**: Matthew Raffel, Victor Agostinelli, Lizhong Chen

**Updated**: 2024-09-05T01:06:40Z

**Summary**: Large language models (LLMs) have achieved state-of-the-art performance in various language processing tasks, motivating their adoption in simultaneous translation. Current fine-tuning methods to adapt LLMs for simultaneous translation focus on prompting optimization strategies using either data augmentation or prompt structure modifications. However, these methods suffer from several issues, such as unnecessarily expanded training sets, computational inefficiency from dumping the key and value cache, increased prompt sizes, or restriction to a single decision policy. To eliminate these issues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs for simultaneous translation. It utilizes a novel attention mask approach that models simultaneous translation during fine-tuning by masking attention for a desired decision policy. Applying the proposed SimulMask on a Falcon LLM for the IWSLT 2017 dataset, we have observed a significant translation quality improvement compared to state-of-the-art prompting optimization strategies on five language pairs while reducing the computational cost.

**Link**: [arxiv](http://arxiv.org/abs/2405.10443v3),  [pdf](http://arxiv.org/pdf/2405.10443v3)

**Tags**: cs.CL cs.LG 



### SparQ Attention: Bandwidth-Efficient LLM Inference
**Authors**: Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, Douglas Orr

**Updated**: 2024-09-04T10:04:52Z

**Summary**: The computational difficulties of large language model (LLM) inference remain a significant obstacle to their widespread deployment. The need for many applications to support long input sequences and process them in large batches typically causes token-generation to be bottlenecked by data transfer. For this reason, we introduce SparQ Attention, a technique for increasing the inference throughput of LLMs by utilising memory bandwidth more efficiently within the attention layers, through selective fetching of the cached history. Our proposed technique can be applied directly to off-the-shelf LLMs during inference, without requiring any modification to the pre-training setup or additional fine-tuning. We show that SparQ Attention brings up to 8x savings in attention data transfers without substantial drops in accuracy, by evaluating Llama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream tasks.

**Link**: [arxiv](http://arxiv.org/abs/2312.04985v6),  [pdf](http://arxiv.org/pdf/2312.04985v6)

**Tags**: cs.LG 



### A brown dwarf orbiting around the planetary-nebula central binary KV Vel
**Authors**: S. -B. Qian, L. -Y. Zhu, F. -X. Li, L. -J. Li, Z. -T. Han, J. -J. He, L. Zang, L. -F. Chang, Q. -B. Sun, M. -Y. Li, H. -T. Zhang, F. -Z. Yan

**Updated**: 2024-09-04T07:13:01Z

**Summary**: KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary containing a very hot subdwarf primary (77000 K) and a cool low-mass secondary star (3400 K) that is located at the center of the planetary nebula DS 1. The changes in the orbital period of the close binary were analyzed based on 262 new times of light maximum together with those compiled from the literature. It is discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic period variation with a period of 29.55 years. The explanation by the solar-type magnetic activity cycles of the cool component is ruled out because the required energies are much larger than the total radiant energy of this component in a whole cycle. Therefore, the cyclic variation was plausibly explained as the light-travel time effect via the presence of a tertiary component, which is supported by the periodic changes of the O-C curve and the rather symmetric and stable light curves obtained by TESS. The mass of the tertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third body is coplanar with the central binary (i.e., i' = 62.5{\deg}), the mass of the tertiary component is computed as M_3 ~ 0.068 M\sun, and thus it would be below the stable hydrogen-burning limit and is a brown dwarf. The orbital separation is shorter than 9.35 astronomical units (AU). KV Vel together with its surrounding planetary nebula and the brown-dwarf companion may be formed through the common-envelope evolution after the primary filled its Roche lobe during the early asymptotic giant branch stage.

**Link**: [arxiv](http://arxiv.org/abs/2409.02480v1),  [pdf](http://arxiv.org/pdf/2409.02480v1)

**Tags**: astro-ph.SR 



### Contemporary Model Compression on Large Language Models Inference
**Authors**: Dong Liu

**Updated**: 2024-09-03T15:35:01Z

**Summary**: Large Language Models (LLMs) have revolutionized natural language processing by achieving state-of-the-art results across a variety of tasks. However, the computational demands of LLM inference, including high memory consumption and slow processing speeds, pose significant challenges for real-world applications, particularly on resource-constrained devices. Efficient inference is crucial for scaling the deployment of LLMs to a broader range of platforms, including mobile and edge devices.   This survey explores contemporary techniques in model compression that address these challenges by reducing the size and computational requirements of LLMs while maintaining their performance. We focus on model-level compression methods, including quantization, knowledge distillation, and pruning, as well as system-level optimizations like KV cache efficient design. Each of these methodologies offers a unique approach to optimizing LLMs, from reducing numerical precision to transferring knowledge between models and structurally simplifying neural networks. Additionally, we discuss emerging trends in system-level design that further enhance the efficiency of LLM inference. This survey aims to provide a comprehensive overview of current advancements in model compression and their potential to make LLMs more accessible and practical for diverse applications.

**Link**: [arxiv](http://arxiv.org/abs/2409.01990v1),  [pdf](http://arxiv.org/pdf/2409.01990v1)

**Tags**: cs.DC cs.LG 



### A Fresh Take on Stale Embeddings: Improving Dense Retriever Training   with Corrector Networks
**Authors**: Nicholas Monath, Will Grathwohl, Michael Boratko, Rob Fergus, Andrew McCallum, Manzil Zaheer

**Updated**: 2024-09-03T13:29:13Z

**Summary**: In dense retrieval, deep encoders provide embeddings for both inputs and targets, and the softmax function is used to parameterize a distribution over a large number of candidate targets (e.g., textual passages for information retrieval). Significant challenges arise in training such encoders in the increasingly prevalent scenario of (1) a large number of targets, (2) a computationally expensive target encoder model, (3) cached target embeddings that are out-of-date due to ongoing training of target encoder parameters. This paper presents a simple and highly scalable response to these challenges by training a small parametric corrector network that adjusts stale cached target embeddings, enabling an accurate softmax approximation and thereby sampling of up-to-date high scoring "hard negatives." We theoretically investigate the generalization properties of our proposed target corrector, relating the complexity of the network, staleness of cached representations, and the amount of training data. We present experimental results on large benchmark dense retrieval datasets as well as on QA with retrieval augmented language models. Our approach matches state-of-the-art results even when no target embedding updates are made during training beyond an initial cache from the unsupervised pre-trained model, providing a 4-80x reduction in re-embedding computational cost.

**Link**: [arxiv](http://arxiv.org/abs/2409.01890v1),  [pdf](http://arxiv.org/pdf/2409.01890v1)

**Tags**: cs.LG 



### Reward Augmentation in Reinforcement Learning for Testing Distributed   Systems
**Authors**: Andrea Borgarelli, Constantin Enea, Rupak Majumdar, Srinidhi Nagendra

**Updated**: 2024-09-02T15:07:05Z

**Summary**: Bugs in popular distributed protocol implementations have been the source of many downtimes in popular internet services. We describe a randomized testing approach for distributed protocol implementations based on reinforcement learning. Since the natural reward structure is very sparse, the key to successful exploration in reinforcement learning is reward augmentation. We show two different techniques that build on one another. First, we provide a decaying exploration bonus based on the discovery of new states -- the reward decays as the same state is visited multiple times. The exploration bonus captures the intuition from coverage-guided fuzzing of prioritizing new coverage points; in contrast to other schemes, we show that taking the maximum of the bonus and the Q-value leads to more effective exploration. Second, we provide waypoints to the algorithm as a sequence of predicates that capture interesting semantic scenarios. Waypoints exploit designer insight about the protocol and guide the exploration to ``interesting'' parts of the state space. Our reward structure ensures that new episodes can reliably get to deep interesting states even without execution caching. We have implemented our algorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and RSL) shows that our algorithm can significantly outperform baseline approaches in terms of coverage and bug finding.

**Link**: [arxiv](http://arxiv.org/abs/2409.02137v1),  [pdf](http://arxiv.org/pdf/2409.02137v1)

**Tags**: cs.SE cs.DC cs.LG cs.PL 



### Learning in Hybrid Active Inference Models
**Authors**: Poppy Collis, Ryan Singh, Paul F Kinghorn, Christopher L Buckley

**Updated**: 2024-09-02T08:41:45Z

**Summary**: An open problem in artificial intelligence is how systems can flexibly learn discrete abstractions that are useful for solving inherently continuous problems. Previous work in computational neuroscience has considered this functional integration of discrete and continuous variables during decision-making under the formalism of active inference (Parr, Friston & de Vries, 2017; Parr & Friston, 2018). However, their focus is on the expressive physical implementation of categorical decisions and the hierarchical mixed generative model is assumed to be known. As a consequence, it is unclear how this framework might be extended to learning. We therefore present a novel hierarchical hybrid active inference agent in which a high-level discrete active inference planner sits above a low-level continuous active inference controller. We make use of recent work in recurrent switching linear dynamical systems (rSLDS) which implement end-to-end learning of meaningful discrete representations via the piecewise linear decomposition of complex continuous dynamics (Linderman et al., 2016). The representations learned by the rSLDS inform the structure of the hybrid decision-making agent and allow us to (1) specify temporally-abstracted sub-goals in a method reminiscent of the options framework, (2) lift the exploration into discrete space allowing us to exploit information-theoretic exploration bonuses and (3) `cache' the approximate solutions to low-level problems in the discrete planner. We apply our model to the sparse Continuous Mountain Car task, demonstrating fast system identification via enhanced exploration and successful planning through the delineation of abstract sub-goals.

**Link**: [arxiv](http://arxiv.org/abs/2409.01066v1),  [pdf](http://arxiv.org/pdf/2409.01066v1)

**Tags**: cs.AI cs.SY eess.SY 



### Throughput Optimization in Cache-aided Networks: An Opportunistic   Probing and Scheduling Approach
**Authors**: Zhou Zhang, Saman Atapattu, Yizhu Wang, Marco Di Renzo

**Updated**: 2024-09-02T02:36:22Z

**Summary**: This paper addresses the challenges of throughput optimization in wireless cache-aided cooperative networks. We propose an opportunistic cooperative probing and scheduling strategy for efficient content delivery. The strategy involves the base station probing the relaying channels and cache states of multiple cooperative nodes, thereby enabling opportunistic user scheduling for content delivery. Leveraging the theory of Sequentially Planned Decision (SPD) optimization, we dynamically formulate decisions on cooperative probing and stopping time. Our proposed Reward Expected Thresholds (RET)-based strategy optimizes opportunistic probing and scheduling. This approach significantly enhances system throughput by exploiting gains from local caching, cooperative transmission and time diversity. Simulations confirm the effectiveness and practicality of the proposed Media Access Control (MAC) strategy.

**Link**: [arxiv](http://arxiv.org/abs/2409.00905v1),  [pdf](http://arxiv.org/pdf/2409.00905v1)

**Tags**: eess.SP 



### Rapid GPU-Based Pangenome Graph Layout
**Authors**: Jiajie Li, Jan-Niklas Schmelzle, Yixiao Du, Simon Heumos, Andrea Guarracino, Giulia Guidi, Pjotr Prins, Erik Garrison, Zhiru Zhang

**Updated**: 2024-09-02T00:05:20Z

**Summary**: Computational Pangenomics is an emerging field that studies genetic variation using a graph structure encompassing multiple genomes. Visualizing pangenome graphs is vital for understanding genome diversity. Yet, handling large graphs can be challenging due to the high computational demands of the graph layout process.   In this work, we conduct a thorough performance characterization of a state-of-the-art pangenome graph layout algorithm, revealing significant data-level parallelism, which makes GPUs a promising option for compute acceleration. However, irregular data access and the algorithm's memory-bound nature present significant hurdles. To overcome these challenges, we develop a solution implementing three key optimizations: a cache-friendly data layout, coalesced random states, and warp merging. Additionally, we propose a quantitative metric for scalable evaluation of pangenome layout quality.   Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution achieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline without layout quality loss, reducing execution time from hours to minutes.

**Link**: [arxiv](http://arxiv.org/abs/2409.00876v1),  [pdf](http://arxiv.org/pdf/2409.00876v1)

**Tags**: cs.DC cs.CE cs.DS 



### Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,   Communication and Computing Systems
**Authors**: Wanming Hao, Xue Wu, Xingwang Li, Gangcan Sun, Qingqing Wu, Liang Yang

**Updated**: 2024-08-31T06:33:50Z

**Summary**: In this paper, we investigate an intelligent reflecting surface (IRS) assisted full-duplex (FD) integrated sensing, communication and computing system. Specifically, an FD base station (BS) provides service for uplink and downlink transmission, and a local cache is connected to the BS through a backhaul link to store data. Meanwhile, active sensing elements are deployed on the IRS to receive target echo signals. On this basis, in order to evaluate the overall performance of the system under consideration, we propose a system utility maximization problem while ensuring the sensing quality, expressed as the difference between the sum of communication throughput, total computation bits (offloading bits and local computation bits) and the total backhaul cost for content delivery. This makes the problem difficult to solve due to the highly non-convex coupling of the optimization variables. To effectively solve this problem, we first design the most effective caching strategy. Then, we develop an algorithm based on weighted minimum mean square error, alternative direction method of multipliers, majorization-minimization framework, semi-definite relaxation techniques, and several complex transformations to jointly solve the optimization variables. Finally, simulation results are provided to verify the utility performance of the proposed algorithm and demonstrate the advantages of the proposed scheme compared with the baseline scheme.

**Link**: [arxiv](http://arxiv.org/abs/2409.00364v1),  [pdf](http://arxiv.org/pdf/2409.00364v1)

**Tags**: cs.IT eess.SP math.IT 



### >3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction   Termination Extension and Sub-1V Turn-on
**Authors**: Advait Gilankar, Abishek Katta, Nabasindhu Das, Nidhin Kurian Kalarickal

**Updated**: 2024-08-31T04:20:58Z

**Summary**: This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction diodes (HJDs) with a 2-step space-modulated junction termination extension. Distinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown voltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a forward current density (IF) of 1 A-cm-2. The measured devices exhibit excellent turn-on characteristics achieving 100 A-cm-2 current density at a forward bias of 1.5V along with a low differential specific on-resistance (Ron,sp) of 4.4 m{\Omega}-cm2. The SM-JTE was realized using concentric NiO rings with varying widths and spacing that approximates a gradual reduction in JTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and is among the best reported for devices with a sub-1V turn-on. The fabricated devices also displayed minimal change in forward I-V characteristics post reverse bias stress of 3 kV applied during breakdown voltage testing.

**Link**: [arxiv](http://arxiv.org/abs/2409.00344v1),  [pdf](http://arxiv.org/pdf/2409.00344v1)

**Tags**: physics.app-ph 



### Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume   Visualization through Functional Approximation
**Authors**: Jianxin Sun, David Lenz, Hongfeng Yu, Tom Peterka

**Updated**: 2024-08-30T18:04:53Z

**Summary**: Functional approximation as a high-order continuous representation provides a more accurate value and gradient query compared to the traditional discrete volume representation. Volume visualization directly rendered from functional approximation generates high-quality rendering results without high-order artifacts caused by trilinear interpolations. However, querying an encoded functional approximation is computationally expensive, especially when the input dataset is large, making functional approximation impractical for interactive visualization. In this paper, we proposed a novel functional approximation multi-resolution representation, Adaptive-FAM, which is lightweight and fast to query. We also design a GPU-accelerated out-of-core multi-resolution volume visualization framework that directly utilizes the Adaptive-FAM representation to generate high-quality rendering with interactive responsiveness. Our method can not only dramatically decrease the caching time, one of the main contributors to input latency, but also effectively improve the cache hit rate through prefetching. Our approach significantly outperforms the traditional function approximation method in terms of input latency while maintaining comparable rendering quality.

**Link**: [arxiv](http://arxiv.org/abs/2409.00184v1),  [pdf](http://arxiv.org/pdf/2409.00184v1)

**Tags**: cs.GR 



### Modelling the High-Voltage Grid Using Open Data for Europe and Beyond
**Authors**: Bobby Xiong, Davide Fioriti, Fabian Neumann, Iegor Riepin, Tom Brown

**Updated**: 2024-08-30T10:26:50Z

**Summary**: This paper provides the background, methodology and validation for constructing a representation of the European high-voltage grid, including and above 200 kV, based on public data provided by OpenStreetMap. The model-independent grid dataset is published under the Open Data Commons Open Database (ODbL 1.0) licence and can be used for large-scale electricity as well as energy system modelling. The dataset and workflow are provided as part of PyPSA-Eur -- an open-source, sector-coupled optimisation model of the European energy system. By integrating with the codebase for initiatives such as PyPSA-Earth, the value of open and maintainable high-voltage grid data extends to the global context. By accessing the latest data through the the Overpass turbo API, the dataset can be easily reconstructed and updated within minutes. To assess the data quality, this paper further compares the dataset with official statistics and representative model runs using PyPSA-Eur based on different electricity grid representations.

**Link**: [arxiv](http://arxiv.org/abs/2408.17178v1),  [pdf](http://arxiv.org/pdf/2408.17178v1)

**Tags**: physics.soc-ph 



### MemLong: Memory-Augmented Retrieval for Long Text Modeling
**Authors**: Weijie Liu, Zecheng Tang, Juntao Li, Kehai Chen, Min Zhang

**Updated**: 2024-08-30T02:01:56Z

**Summary**: Recent advancements in Large Language Models (LLMs) have yielded remarkable success across diverse fields. However, handling long contexts remains a significant challenge for LLMs due to the quadratic time and space complexity of attention mechanisms and the growing memory consumption of the key-value cache during generation. This work introduces MemLong: Memory-Augmented Retrieval for Long Text Generation, a method designed to enhance the capabilities of long-context language modeling by utilizing an external retriever for historical information retrieval. MemLong combines a non-differentiable ``ret-mem'' module with a partially trainable decoder-only language model and introduces a fine-grained, controllable retrieval attention mechanism that leverages semantic-level relevant chunks. Comprehensive evaluations on multiple long-context language modeling benchmarks demonstrate that MemLong consistently outperforms other state-of-the-art LLMs. More importantly, MemLong can extend the context length on a single 3090 GPU from 4k up to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong

**Link**: [arxiv](http://arxiv.org/abs/2408.16967v1),  [pdf](http://arxiv.org/pdf/2408.16967v1)

**Tags**: cs.CL cs.AI 



### Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load
**Authors**: Hesameddin Mokhtarzadeh, Mohammed S. Al-Abiad, Md Jahangir Hossain, Julian Cheng

**Updated**: 2024-08-29T17:43:26Z

**Summary**: In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio heads (eRRHs) are connected to a macro base station (MBS) through fronthaul links. Deploying a massive number of eRRHs is not always feasible due to site constraints and the cost of fronthaul links. This paper introduces an innovative concept of using smart helpers (SHs) in F-RANs. These SHs do not require fronthaul links and listen to the nearby eRRHs' communications. Then, they smartly select and cache popular content. This capability enables SHs to serve users with frequent on-demand service requests potentially. As such, network operators have the flexibility to easily deploy SHs in various scenarios, such as dense urban areas and temporary public events, to expand their F-RANs and improve the quality of service (QoS). To study the performance of the proposed SH-aided F-RAN, we formulate an optimization problem of minimizing the average transmission delay that jointly optimizes cache resources and user scheduling. To tackle the formulated problem, we develop an innovative multi-stage algorithm that uses a reinforcement learning (RL) framework. Various performance measures, e.g., the average transmission delay, fronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated numerically and compared with those of traditional F-RANs.

**Link**: [arxiv](http://arxiv.org/abs/2309.07975v2),  [pdf](http://arxiv.org/pdf/2309.07975v2)

**Tags**: cs.IT math.IT 



### VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths   Vision Computation
**Authors**: Shiwei Wu, Joya Chen, Kevin Qinghong Lin, Qimeng Wang, Yan Gao, Qianli Xu, Tong Xu, Yao Hu, Enhong Chen, Mike Zheng Shou

**Updated**: 2024-08-29T17:21:58Z

**Summary**: A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is that while increasing the number of vision tokens generally enhances visual understanding, it also significantly raises memory and computational costs, especially in long-term, dense video frame streaming scenarios. Although learnable approaches like Q-Former and Perceiver Resampler have been developed to reduce the vision token burden, they overlook the context causally modeled by LLMs (i.e., key-value cache), potentially leading to missed visual cues when addressing user queries. In this paper, we introduce a novel approach to reduce vision compute by leveraging redundant vision tokens "skipping layers" rather than decreasing the number of vision tokens. Our method, VideoLLM-MoD, is inspired by mixture-of-depths LLMs and addresses the challenge of numerous vision tokens in long-term or streaming video. Specifically, for each transformer layer, we learn to skip the computation for a high proportion (e.g., 80\%) of vision tokens, passing them directly to the next layer. This approach significantly enhances model efficiency, achieving approximately \textasciitilde42\% time and \textasciitilde30\% memory savings for the entire training. Moreover, our method reduces the computation in the context and avoid decreasing the vision tokens, thus preserving or even improving performance compared to the vanilla model. We conduct extensive experiments to demonstrate the effectiveness of VideoLLM-MoD, showing its state-of-the-art results on multiple benchmarks, including narration, forecasting, and summarization tasks in COIN, Ego4D, and Ego-Exo4D datasets.

**Link**: [arxiv](http://arxiv.org/abs/2408.16730v1),  [pdf](http://arxiv.org/pdf/2408.16730v1)

**Tags**: cs.CV 



### GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless   Generative Inference of LLM
**Authors**: Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, Tuo Zhao

**Updated**: 2024-08-29T16:48:58Z

**Summary**: Key-value (KV) caching has become the de-facto to accelerate generation speed for large language models (LLMs) inference. However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing all entries uniformly. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient KV cache compression framework that achieves near-lossless high-ratio compression. GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision. It then employs a low rank matrix to approximate the quantization error, and a sparse matrix to remedy individual errors from outlier entries. By adeptly integrating three techniques, GEAR is able to fully exploit their synergistic potentials. Our experiments demonstrate that compared to alternatives, GEAR achieves near-lossless 4-bit KV cache compression with up to 2.38x throughput improvement, while reducing peak-memory size up to 2.29x. Our code is publicly available at https://github.com/HaoKang-Timmy/GEAR.

**Link**: [arxiv](http://arxiv.org/abs/2403.05527v3),  [pdf](http://arxiv.org/pdf/2403.05527v3)

**Tags**: cs.LG cs.AI cs.CL 



### LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through   Targeted Instruction Hardening
**Authors**: Yiming Zhu, Wenchao Huang, Yan Xiong

**Updated**: 2024-08-29T02:31:28Z

**Summary**: Several software mitigations have been proposed to defend against Spectre vulnerabilities. However, these countermeasures often suffer from high performance overhead, largely due to unnecessary protections. We propose LightSLH, designed to mitigate this overhead by hardening instructions only when they are under threat from Spectre vulnerabilities. LightSLH leverages program analysis techniques based on abstract interpretation to identify all instructions that could potentially lead to Spectre vulnerabilities and provides provable protection. To enhance analysis efficiency and precision, LightSLH employs novel taint and value domains. The taint domain enables bit-level taint tracking, while the value domain allows LightSLH to analyze complex program structures such as pointers and structures. Furthermore, LightSLH uses a two-stage abstract interpretation approach to circumvent potential analysis paralysis issues.   We demonstrate the security guarantees of LightSLH and evaluate its performance on cryptographic algorithm implementations from OpenSSL. LightSLH significantly reduces the overhead associated with speculative-load-hardening techniques. Our results show that LightSLH introduces no protection and thus no overhead on 4 out of the 7 studied algorithms, which contrasts with existing countermeasures that introduce additional overhead due to unnecessary hardening. Additionally, LightSLH performs, for the first time, a rigorous analysis of the security guarantees of RSA against Spectre v1, highlighting that the memory access patterns generated by the scatter-gather algorithm depend on secrets, even for observers at the cache line granularity, necessitating protection for such accesses.

**Link**: [arxiv](http://arxiv.org/abs/2408.16220v1),  [pdf](http://arxiv.org/pdf/2408.16220v1)

**Tags**: cs.CR 



### RIP Linked List
**Authors**: Benoît Sonntag, Dominique Colnet

**Updated**: 2024-08-28T08:41:45Z

**Summary**: Linked lists have long served as a valuable teaching tool in programming. However, the question arises: Are they truly practical for everyday program use? In most cases, it appears that array-based data structures offer distinct advantages, particularly in terms of memory efficiency and,more importantly, execution speed. While it's relatively straightforward to calculate the complexity of operations, gauging actual execution efficiency remains a challenge. This paper addresses this question by introducing a new benchmark. Our study compares various linked list implementations with several array-based alternatives. We also demonstrate the ease of incorporating memory caching for linked lists, enhancing their performance. Additionally, we introduce a new array-based data structure designed to excel in a wide range of operations.

**Link**: [arxiv](http://arxiv.org/abs/2306.06942v3),  [pdf](http://arxiv.org/pdf/2306.06942v3)

**Tags**: cs.DS 



### Efficient LLM Training and Serving with Heterogeneous Context Sharding   among Attention Heads
**Authors**: Xihui Lin, Yunan Zhang, Suyu Ge, Barun Patra, Vishrav Chaudhary, Hao Peng, Xia Song

**Updated**: 2024-08-27T22:06:20Z

**Summary**: Existing LLM training and inference frameworks struggle in boosting efficiency with sparsity while maintaining the integrity of context and model architecture. Inspired by the sharding concept in database and the fact that attention parallelizes over heads on accelerators, we propose Sparsely-Sharded (S2) Attention, an attention algorithm that allocates heterogeneous context partitions for different attention heads to divide and conquer. S2-Attention enforces each attention head to only attend to a partition of contexts following a strided sparsity pattern, while the full context is preserved as the union of all the shards. As attention heads are processed in separate thread blocks, the context reduction for each head can thus produce end-to-end speed-up and memory reduction. At inference, LLMs trained with S2-Attention can then take the KV cache reduction as free meals with guaranteed model quality preserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X wall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction in end-to-end training time and 10X inference latency, (2) on-par model training quality compared to default attention, (3)perfect needle retrieval accuracy over 32K context window. On top of the algorithm, we build DKernel, an LLM training and inference kernel library that allows users to customize sparsity patterns for their own models. We open-sourced DKerneland make it compatible with Megatron, Pytorch, and vLLM.

**Link**: [arxiv](http://arxiv.org/abs/2407.17678v2),  [pdf](http://arxiv.org/pdf/2407.17678v2)

**Tags**: cs.CL 



### Styx: Transactional Stateful Functions on Streaming Dataflows
**Authors**: Kyriakos Psarakis, George Siachamis, George Christodoulou, Marios Fragkoulis, Asterios Katsifodimos

**Updated**: 2024-08-27T17:30:41Z

**Summary**: Developing stateful cloud applications, such as low-latency workflows and microservices with strict consistency requirements, remains arduous for programmers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve these use cases. However, existing approaches either provide serializable transactional guarantees at the level of individual functions, or separate application logic from the state and use inefficient transactional protocols. These design choices increase the execution latency, limiting the adoption of SFaaS systems.   In this paper, we present Styx, a novel SFaaS runtime that executes serializable transactions across functions with exactly-once guarantees. Styx extends a deterministic transactional protocol to support an arbitrary call graph of stateful functions. It introduces a transaction-execution acknowledgment scheme that allows tracking a transactional workflow's SFaaS calls, guaranteeing atomicity and exactly-once processing. Finally, Styx features a function-execution caching mechanism and early transactional commit replies for optimized performance. Experiments with the YCSB-T, TPC-C, and Deathstar benchmarks show that Styx outperforms state-of-the-art approaches by achieving at least one order of magnitude higher throughput while exhibiting near-linear scalability and low latency.

**Link**: [arxiv](http://arxiv.org/abs/2312.06893v3),  [pdf](http://arxiv.org/pdf/2312.06893v3)

**Tags**: cs.DC cs.DB 



### Writing in the Margins: Better Inference Pattern for Long Context   Retrieval
**Authors**: Melisa Russak, Umar Jamil, Christopher Bryant, Kiran Kamble, Axel Magnuson, Mateusz Russak, Waseem AlShikh

**Updated**: 2024-08-27T09:34:38Z

**Summary**: In this paper, we introduce Writing in the Margins (WiM), a new inference pattern for Large Language Models designed to optimize the handling of long input sequences in retrieval-oriented tasks. This approach leverages the chunked prefill of the key-value cache to perform segment-wise inference, which enables efficient processing of extensive contexts along with the generation and classification of intermediate information ("margins") that guide the model towards specific tasks. This method increases computational overhead marginally while significantly enhancing the performance of off-the-shelf models without the need for fine-tuning. Specifically, we observe that WiM provides an average enhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG) and more than a 30.0% increase in the F1-score for aggregation tasks (CWE). Additionally, we show how the proposed pattern fits into an interactive retrieval design that provides end-users with ongoing updates about the progress of context processing, and pinpoints the integration of relevant information into the final response. We release our implementation of WiM using Hugging Face Transformers library at https://github.com/writer/writing-in-the-margins.

**Link**: [arxiv](http://arxiv.org/abs/2408.14906v1),  [pdf](http://arxiv.org/pdf/2408.14906v1)

**Tags**: cs.CL cs.IR 



### PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework   with Correlated Differential Privacy
**Authors**: Xianzhi Zhang, Yipeng Zhou, Di Wu, Quan Z. Sheng, Miao Hu, Linchang Xiao

**Updated**: 2024-08-27T02:03:36Z

**Summary**: Online video streaming has evolved into an integral component of the contemporary Internet landscape. Yet, the disclosure of user requests presents formidable privacy challenges. As users stream their preferred online videos, their requests are automatically seized by video content providers, potentially leaking users' privacy.   Unfortunately, current protection methods are not well-suited to preserving user request privacy from content providers while maintaining high-quality online video services. To tackle this challenge, we introduce a novel Privacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge devices to pre-fetch and cache videos, ensuring the privacy of users' requests while optimizing the efficiency of edge caching. More specifically, we design PPVF with three core components: (1) \textit{Online privacy budget scheduler}, which employs a theoretically guaranteed online algorithm to select non-requested videos as candidates with assigned privacy budgets. Alternative videos are chosen by an online algorithm that is theoretically guaranteed to consider both video utilities and available privacy budgets. (2) \textit{Noisy video request generator}, which generates redundant video requests (in addition to original ones) utilizing correlated differential privacy to obfuscate request privacy. (3) \textit{Online video utility predictor}, which leverages federated learning to collaboratively evaluate video utility in an online fashion, aiding in video selection in (1) and noise generation in (2). Finally, we conduct extensive experiments using real-world video request traces from Tencent Video. The results demonstrate that PPVF effectively safeguards user request privacy while upholding high video caching performance.

**Link**: [arxiv](http://arxiv.org/abs/2408.14735v1),  [pdf](http://arxiv.org/pdf/2408.14735v1)

**Tags**: cs.MM cs.CR cs.DC 



### Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference
**Authors**: Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, Song Han

**Updated**: 2024-08-26T21:01:02Z

**Summary**: As the demand for long-context large language models (LLMs) increases, models with context windows of up to 128K or 1M tokens are becoming increasingly prevalent. However, long-context LLM inference is challenging since the inference speed decreases significantly as the sequence length grows. This slowdown is primarily caused by loading a large KV cache during self-attention. Previous works have shown that a small portion of critical tokens will dominate the attention outcomes. However, we observe the criticality of a token highly depends on the query. To this end, we propose Quest, a query-aware KV cache selection algorithm. Quest keeps track of the minimal and maximal Key values in KV cache pages and estimates the criticality of a given page using Query vectors. By only loading the Top-K critical KV cache pages for attention, Quest significantly speeds up self-attention without sacrificing accuracy. We show that Quest can achieve up to 2.23x self-attention speedup, which reduces inference latency by 7.03x while performing well on tasks with long dependencies with negligible accuracy loss. Code is available at http://github.com/mit-han-lab/Quest .

**Link**: [arxiv](http://arxiv.org/abs/2406.10774v2),  [pdf](http://arxiv.org/pdf/2406.10774v2)

**Tags**: cs.CL cs.LG 



### Employing Artificial Intelligence to Steer Exascale Workflows with   Colmena
**Authors**: Logan Ward, J. Gregory Pauloski, Valerie Hayot-Sasson, Yadu Babuji, Alexander Brace, Ryan Chard, Kyle Chard, Rajeev Thakur, Ian Foster

**Updated**: 2024-08-26T17:21:19Z

**Summary**: Computational workflows are a common class of application on supercomputers, yet the loosely coupled and heterogeneous nature of workflows often fails to take full advantage of their capabilities. We created Colmena to leverage the massive parallelism of a supercomputer by using Artificial Intelligence (AI) to learn from and adapt a workflow as it executes. Colmena allows scientists to define how their application should respond to events (e.g., task completion) as a series of cooperative agents. In this paper, we describe the design of Colmena, the challenges we overcame while deploying applications on exascale systems, and the science workflows we have enhanced through interweaving AI. The scaling challenges we discuss include developing steering strategies that maximize node utilization, introducing data fabrics that reduce communication overhead of data-intensive tasks, and implementing workflow tasks that cache costly operations between invocations. These innovations coupled with a variety of application patterns accessible through our agent-based steering model have enabled science advances in chemistry, biophysics, and materials science using different types of AI. Our vision is that Colmena will spur creative solutions that harness AI across many domains of scientific computing.

**Link**: [arxiv](http://arxiv.org/abs/2408.14434v1),  [pdf](http://arxiv.org/pdf/2408.14434v1)

**Tags**: cs.DC cs.LG 



### Decision-Focused Learning to Predict Action Costs for Planning
**Authors**: Jayanta Mandi, Marco Foschini, Daniel Holler, Sylvie Thiebaux, Jorg Hoffmann, Tias Guns

**Updated**: 2024-08-26T11:29:07Z

**Summary**: In many automated planning applications, action costs can be hard to specify. An example is the time needed to travel through a certain road segment, which depends on many factors, such as the current weather conditions. A natural way to address this issue is to learn to predict these parameters based on input features (e.g., weather forecasts) and use the predicted action costs in automated planning afterward. Decision-Focused Learning (DFL) has been successful in learning to predict the parameters of combinatorial optimization problems in a way that optimizes solution quality rather than prediction quality. This approach yields better results than treating prediction and optimization as separate tasks. In this paper, we investigate for the first time the challenges of implementing DFL for automated planning in order to learn to predict the action costs. There are two main challenges to overcome: (1) planning systems are called during gradient descent learning, to solve planning problems with negative action costs, which are not supported in planning. We propose novel methods for gradient computation to avoid this issue. (2) DFL requires repeated planner calls during training, which can limit the scalability of the method. We experiment with different methods approximating the optimal plan as well as an easy-to-implement caching mechanism to speed up the learning process. As the first work that addresses DFL for automated planning, we demonstrate that the proposed gradient computation consistently yields significantly better plans than predictions aimed at minimizing prediction error; and that caching can temper the computation requirements.

**Link**: [arxiv](http://arxiv.org/abs/2408.06876v2),  [pdf](http://arxiv.org/pdf/2408.06876v2)

**Tags**: cs.AI cs.RO 



### Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems
**Authors**: Yiwei Li, Boyu Tian, Mingyu Gao

**Updated**: 2024-08-26T07:26:27Z

**Summary**: Hybrid main memory systems combine both performance and capacity advantages from heterogeneous memory technologies. With larger capacities, higher associativities, and finer granularities, hybrid memory systems currently exhibit significant metadata storage and lookup overheads for flexibly remapping data blocks between the two memory tiers. To alleviate the inefficiencies of existing designs, we propose Trimma, the combination of a multi-level metadata structure and an efficient metadata cache design. Trimma uses a multi-level metadata table to only track truly necessary address remap entries. The saved memory space is effectively utilized as extra DRAM cache capacity to improve performance. Trimma also uses separate formats to store the entries with non-identity and identity address mappings. This improves the overall remap cache hit rate, further boosting the performance. Trimma is transparent to software and compatible with various types of hybrid memory systems. When evaluated on a representative hybrid memory system with HBM3 and DDR5, Trimma achieves up to 1.68$\times$ and on average 1.33$\times$ speedup benefits, compared to state-of-the-art hybrid memory designs. These results show that Trimma effectively addresses metadata management overheads, especially for future scalable large-scale hybrid memory architectures.

**Link**: [arxiv](http://arxiv.org/abs/2402.16343v2),  [pdf](http://arxiv.org/pdf/2402.16343v2)

**Tags**: cs.AR 



### RollingCache: Using Runtime Behavior to Defend Against Cache Side   Channel Attacks
**Authors**: Divya Ojha, Sandhya Dwarkadas

**Updated**: 2024-08-26T04:32:56Z

**Summary**: Shared caches are vulnerable to side channel attacks through contention in cache sets. Besides being a simple source of information leak, these side channels form useful gadgets for more sophisticated attacks that compromise the security of shared systems.   The fundamental design aspect that contention attacks exploit is the deterministic nature of the set of addresses contending for a cache set. In this paper, we present RollingCache, a cache design that defends against contention attacks by dynamically changing the set of addresses contending for cache sets. Unlike prior defenses, RollingCache does not rely on address encryption/decryption, data relocation, or cache partitioning. We use one level of indirection to implement dynamic mapping controlled by the whole-cache runtime behavior. Our solution does not depend on having defined security domains, and can defend against an attacker running on the same or another core.   We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our security evaluation shows that our dynamic mapping removes the deterministic ability to identify the source of contention. The performance evaluation shows an impact of 1.67\% over a mix of workloads, with a corresponding

**Link**: [arxiv](http://arxiv.org/abs/2408.08795v2),  [pdf](http://arxiv.org/pdf/2408.08795v2)

**Tags**: cs.CR cs.AR 



### Decentralized Federated Learning with Model Caching on Mobile Agents
**Authors**: Xiaoyu Wang, Guojun Xiong, Houwei Cao, Jian Li, Yong Liu

**Updated**: 2024-08-26T03:58:20Z

**Summary**: Federated Learning (FL) aims to train a shared model using data and computation power on distributed agents coordinated by a central server. Decentralized FL (DFL) utilizes local model exchange and aggregation between agents to reduce the communication and computation overheads on the central server. However, when agents are mobile, the communication opportunity between agents can be sporadic, largely hindering the convergence and accuracy of DFL. In this paper, we study delay-tolerant model spreading and aggregation enabled by model caching on mobile agents. Each agent stores not only its own model, but also models of agents encountered in the recent past. When two agents meet, they exchange their own models as well as the cached models. Local model aggregation works on all models in the cache. We theoretically analyze the convergence of DFL with cached models, explicitly taking into account the model staleness introduced by caching. We design and compare different model caching algorithms for different DFL and mobility scenarios. We conduct detailed case studies in a vehicular network to systematically investigate the interplay between agent mobility, cache staleness, and model convergence. In our experiments, cached DFL converges quickly, and significantly outperforms DFL without caching.

**Link**: [arxiv](http://arxiv.org/abs/2408.14001v1),  [pdf](http://arxiv.org/pdf/2408.14001v1)

**Tags**: cs.LG cs.DC 



### Mobile Edge Computing Networks: Online Low-Latency and Fresh Service   Provisioning
**Authors**: Yuhan Yi, Guanglin Zhang, Hai Jiang

**Updated**: 2024-08-24T15:23:32Z

**Summary**: Edge service caching can significantly mitigate latency and reduce communication and computing overhead by fetching and initializing services (applications) from clouds. The freshness of cached service data is critical when providing satisfactory services to users, but has been overlooked in existing research efforts. In this paper, we study the online low-latency and fresh service provisioning in mobile edge computing (MEC) networks. Specifically, we jointly optimize the service caching, task offloading, and resource allocation without knowledge of future system information, which is formulated as a joint online long-term optimization problem. This problem is NP-hard. To solve the problem, we design a Lyapunov-based online framework that decouples the problem at temporal level into a series of per-time-slot subproblems. For each subproblem, we propose an online integrated optimization-deep reinforcement learning (OIODRL) method, which contains an optimization stage including a quadratically constrained quadratic program (QCQP) transformation and a semidefinite relaxation (SDR) method, and a learning stage including a deep reinforcement learning (DRL) algorithm. Extensive simulations show that the proposed OIODRL method achieves a near-optimal solution and outperforms other benchmark methods.

**Link**: [arxiv](http://arxiv.org/abs/2408.13605v1),  [pdf](http://arxiv.org/pdf/2408.13605v1)

**Tags**: cs.IT math.IT 



### MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context   Generation with Speculative Decoding
**Authors**: Jian Chen, Vashisth Tiwari, Ranajoy Sadhukhan, Zhuoming Chen, Jinyuan Shi, Ian En-Hsu Yen, Beidi Chen

**Updated**: 2024-08-23T17:54:34Z

**Summary**: Large Language Models (LLMs) have become more prevalent in long-context applications such as interactive chatbots, document analysis, and agent workflows, but it is challenging to serve long-context requests with low latency and high throughput. Speculative decoding (SD) is a widely used technique to reduce latency without sacrificing performance but the conventional wisdom suggests that its efficacy is limited to small batch sizes. In MagicDec, we show that surprisingly SD can achieve speedup even for a high throughput inference regime for moderate to long sequences. More interestingly, an intelligent drafting strategy can achieve better speedup with increasing batch size based on our rigorous analysis. MagicDec first identifies the bottleneck shifts with increasing batch size and sequence length, and uses these insights to deploy speculative decoding more effectively for high throughput inference. Then, it leverages draft models with sparse KV cache to address the KV bottleneck that scales with both sequence length and batch size. This finding underscores the broad applicability of speculative decoding in long-context serving, as it can enhance throughput and reduce latency without compromising accuracy. For moderate to long sequences, we demonstrate up to 2x speedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving batch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available at https://github.com/Infini-AI-Lab/MagicDec/.

**Link**: [arxiv](http://arxiv.org/abs/2408.11049v3),  [pdf](http://arxiv.org/pdf/2408.11049v3)

**Tags**: cs.CL 



### Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches
**Authors**: Dhruv Pratap Singh, Anjana A. Mahesh, B. Sundar Rajan

**Updated**: 2024-08-23T15:39:20Z

**Summary**: We consider a variant of the coded caching problem where users connect to two types of caches, called private caches and access caches. The problem setting consists of a server having a library of files and a set of access caches. Every user, equipped with a private cache, connects to $L$ neighboring access caches in a cyclic wrap-around fashion. The server populates the private and access caches with file contents in either coded or uncoded format. For this setting, we derive a lower bound on the optimal worst-case transmission rate using cut-set arguments. This lower bound applies to both coded and uncoded placements. We then provide an achievable scheme with uncoded placement and show that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for the dedicated cache network in the absence of access caches. Finally, we show that the proposed scheme achieves optimality in large memory regimes and provide numerical plots comparing the rate of the proposed scheme with the derived lower bound, demonstrating the optimality of our scheme.

**Link**: [arxiv](http://arxiv.org/abs/2408.13165v1),  [pdf](http://arxiv.org/pdf/2408.13165v1)

**Tags**: cs.IT math.IT 



### Fundamental Limits of Multi-Message Private Computation
**Authors**: Ali Gholami, Kai Wan, Tayyebeh Jahani-Nezhad, Hua Sun, Mingyue Ji, Giuseppe Caire

**Updated**: 2024-08-23T13:25:07Z

**Summary**: In a typical formulation of the private information retrieval (PIR) problem, a single user wishes to retrieve one out of $ K$ files from $N$ servers without revealing the demanded file index to any server. This paper formulates an extended model of PIR, referred to as multi-message private computation (MM-PC), where instead of retrieving a single file, the user wishes to retrieve $P>1$ linear combinations of files while preserving the privacy of the demand information. The MM-PC problem is a generalization of the private computation (PC) problem (where the user requests one linear combination of the files), and the multi-message private information retrieval (MM-PIR) problem (where the user requests $P>1$ files). A baseline achievable scheme repeats the optimal PC scheme by Sun and Jafar $P$ times, or treats each possible demanded linear combination as an independent file and then uses the near optimal MM-PIR scheme by Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that significantly improves upon the baseline schemes. In doing so, we design the queries inspired by the structure in the cache-aided scalar linear function retrieval scheme by Wan {\it et al.}, which leverages the dependency between linear functions to reduce the amount of communications. To ensure the decodability of our scheme, we propose a new method to benefit from the existing dependency, referred to as the sign assignment step. In the end, we use Maximum Distance Separable matrices to code the queries, which allows the reduction of download from the servers, while preserving privacy. By the proposed schemes, we characterize the capacity within a multiplicative factor of $2$.

**Link**: [arxiv](http://arxiv.org/abs/2305.05332v5),  [pdf](http://arxiv.org/pdf/2305.05332v5)

**Tags**: cs.IT math.IT 



### Which Part of the Heap is Useful? Improving Heap Liveness Analysis
**Authors**: Vini Kanvar, Uday P. Khedker

**Updated**: 2024-08-23T09:54:22Z

**Summary**: With the growing sizes of data structures allocated in heap, understanding the actual use of heap memory is critically important for minimizing cache misses and reclaiming unused memory. A static analysis aimed at this is difficult because the heap locations are unnamed. Using allocation sites to name them creates very few distinctions making it difficult to identify allocated heap locations that are not used. Heap liveness analysis using access graphs solves this problem by (a) using a storeless model of heap memory by naming the locations with access paths, and (b) representing the unbounded sets of access paths (which are regular languages) as finite automata.   We improve the scalability and efficiency of heap liveness analysis, and reduce the amount of computed heap liveness information by using deterministic automata and by minimizing the inclusion of aliased access paths in the language. Practically, our field-, flow-, context-sensitive liveness analysis on SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5 kLoC) and improves efficiency even up to 99%. For some of the benchmarks, our technique shows multifold reduction in the computed liveness information, ranging from 2 to 100 times (in terms of the number of live access paths), without compromising on soundness.

**Link**: [arxiv](http://arxiv.org/abs/2408.12947v1),  [pdf](http://arxiv.org/pdf/2408.12947v1)

**Tags**: cs.PL 



### Exposing Shadow Branches
**Authors**: Chrysanthos Pepi, Bhargav Reddy Godala, Krishnam Tibrewala, Gino Chacon, Paul V. Gratz, Daniel A. Jiménez, Gilles A. Pokam, David I. August

**Updated**: 2024-08-22T17:56:29Z

**Summary**: Modern processors implement a decoupled front-end in the form of Fetch Directed Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is driven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and branch target tracking structures to speculatively fetch instructions into the Instruction Cache (L1I). As data center applications become more complex, their code footprints also grow, resulting in an increase in Branch Target Buffer (BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB miss, the BPU may not identify the current instruction as a branch to FDIP. This can prevent FDIP from prefetching or cause it to speculate down the wrong path, further polluting the L1I cache. We observe that the vast majority, 75%, of BTB-missing, unidentified branches are actually present in instruction cache lines that FDIP has previously fetched but, these missing branches have not yet been decoded and inserted into the BTB. This is because the instruction line is decoded from an entry point (which is the target of the previous taken branch) till an exit point (the taken branch). Branch instructions present in the ignored portion of the cache line we call them "Shadow Branches". Here we present Skeia, a novel shadow branch decoding technique that identifies and decodes unused bytes in cache lines fetched by FDIP, inserting them into a Shadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB, allowing FDIP to speculate despite a BTB miss. With a minimal storage state of 12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB) and ~2% versus adding an equal amount of state to the BTB across 16 front-end bound applications. Since many branches stored in the SBB are unique compared to those in a similarly sized BTB, we consistently observe greater performance gains with Skeia across all examined sizes until saturation.

**Link**: [arxiv](http://arxiv.org/abs/2408.12592v1),  [pdf](http://arxiv.org/pdf/2408.12592v1)

**Tags**: cs.AR 



### Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties
**Authors**: Simon Hettler, Kankona Singha Roy, Raul Arenal, Leela S. Panchakarla

**Updated**: 2024-08-22T17:47:49Z

**Summary**: Layered CoO$_2$ is of great interest for its promising properties but is meta-stable in its bulk form. CoO$_2$ was synthesized by converting the quasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a hydrothermal treatment. The resulting nanostructures were predominantly nanoscrolls with very thin walls, which exhibit long-term stability. A detailed structural investigation reveals that the CoO$_2$ is found to crystallize in monoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure. Individual nanoscrolls are characterized electrically and show a p-type semiconducting nature with a high current-carrying capacity of 4$\cdot$10$^5$ A cm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The results demonstrate the possibility to stabilize meta-stable materials in low-dimensional forms and a promising application of the nanoscrolls as interconnect in high-voltage electronic circuitry.

**Link**: [arxiv](http://arxiv.org/abs/2309.14533v2),  [pdf](http://arxiv.org/pdf/2309.14533v2)

**Tags**: cond-mat.mtrl-sci 



### Rheological behavior of molybdenum disulfide (MoS2) inks under electric   fields: influence of concentration and voltage
**Authors**: Pedro C Rijo, Francisco J. Galindo-Rosales

**Updated**: 2024-08-21T10:26:26Z

**Summary**: This work provides a complete rheological characterization of molybdenum disulfide (MoS2) inks in the presence of electric fields. Several concentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The lubrication effects are present in the ink when the MoS2 concentration is higher than 0.10% w/w. The dielectric properties show the impossibility of a positive electrorheological effect for all MoS2-inks studied. The formation of vortices and electromigration of MoS2 particles occur under the influence of an external electric field. These two phenomena affect the rheological behavior of MoS2-inks under shear flow condition. Relatively to the extensional rheology experiments, the particle migration and the vortex formation promote anisotropy on the rheological properties of the inks which affects the relaxation time, the formation of beads-on-a-string and the uniaxial elongational flow condition is no longer valid. When the electric field strength is 1.5 kV/mm, the formation of Taylor's cone is observed and independent of MoS2 concentration.

**Link**: [arxiv](http://arxiv.org/abs/2408.11506v1),  [pdf](http://arxiv.org/pdf/2408.11506v1)

**Tags**: physics.flu-dyn cond-mat.soft 



### Towards End-to-End GPS Localization with Neural Pseudorange Correction
**Authors**: Xu Weng, KV Ling, Haochen Liu, Kun Cao

**Updated**: 2024-08-21T06:10:02Z

**Summary**: The pseudorange error is one of the root causes of localization inaccuracy in GPS. Previous data-driven methods regress and eliminate pseudorange errors using handcrafted intermediate labels. Unlike them, we propose an end-to-end GPS localization framework, E2E-PrNet, to train a neural network for pseudorange correction (PrNet) directly using the final task loss calculated with the ground truth of GPS receiver states. The gradients of the loss with respect to learnable parameters are backpropagated through a Differentiable Nonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing the data-driven neural network and the model-based DNLS module is verified with GPS data collected by Android phones, showing that E2E-PrNet outperforms the baseline weighted least squares method and the state-of-the-art end-to-end data-driven approach. Finally, we discuss the explainability of E2E-PrNet.

**Link**: [arxiv](http://arxiv.org/abs/2401.10685v2),  [pdf](http://arxiv.org/pdf/2401.10685v2)

**Tags**: cs.LG cs.AI eess.SP 



### Telepathic Datacenters: Fast RPCs using Shared CXL Memory
**Authors**: Suyash Mahar, Ehsan Hajyjasini, Seungjin Lee, Zifeng Zhang, Mingyao Shen, Steven Swanson

**Updated**: 2024-08-21T04:16:49Z

**Summary**: Datacenter applications often rely on remote procedure calls (RPCs) for fast, efficient, and secure communication. However, RPCs are slow, inefficient, and hard to use as they require expensive serialization and compression to communicate over a packetized serial network link. Compute Express Link 3.0 (CXL) offers an alternative solution, allowing applications to share data using a cache-coherent, shared-memory interface across clusters of machines.   RPCool is a new framework that exploits CXL's shared memory capabilities. RPCool avoids serialization by passing pointers to data structures in shared memory. While avoiding serialization is useful, directly sharing pointer-rich data eliminates the isolation that copying data over traditional networks provides, leaving the receiver vulnerable to invalid pointers and concurrent updates to shared data by the sender. RPCool restores this safety with careful and efficient management of memory permissions. Another significant challenge with CXL shared memory capabilities is that they are unlikely to scale to an entire datacenter. RPCool addresses this by falling back to RDMA-based communication.   Overall, RPCool reduces the round-trip latency by 1.93$\times$ and 7.2$\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms, respectively. Moreover, RPCool performs either comparably or better than other RPC mechanisms across a range of workloads.

**Link**: [arxiv](http://arxiv.org/abs/2408.11325v1),  [pdf](http://arxiv.org/pdf/2408.11325v1)

**Tags**: cs.DC cs.OS 



### Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical   Planning and Control
**Authors**: Poppy Collis, Ryan Singh, Paul F Kinghorn, Christopher L Buckley

**Updated**: 2024-08-20T16:02:54Z

**Summary**: An open problem in artificial intelligence is how systems can flexibly learn discrete abstractions that are useful for solving inherently continuous problems. Previous work has demonstrated that a class of hybrid state-space model known as recurrent switching linear dynamical systems (rSLDS) discover meaningful behavioural units via the piecewise linear decomposition of complex continuous dynamics (Linderman et al., 2016). Furthermore, they model how the underlying continuous states drive these discrete mode switches. We propose that the rich representations formed by an rSLDS can provide useful abstractions for planning and control. We present a novel hierarchical model-based algorithm inspired by Active Inference in which a discrete MDP sits above a low-level linear-quadratic controller. The recurrent transition dynamics learned by the rSLDS allow us to (1) specify temporally-abstracted sub-goals in a method reminiscent of the options framework, (2) lift the exploration into discrete space allowing us to exploit information-theoretic exploration bonuses and (3) `cache' the approximate solutions to low-level problems in the discrete planner. We successfully apply our model to the sparse Continuous Mountain Car task, demonstrating fast system identification via enhanced exploration and non-trivial planning through the delineation of abstract sub-goals.

**Link**: [arxiv](http://arxiv.org/abs/2408.10970v1),  [pdf](http://arxiv.org/pdf/2408.10970v1)

**Tags**: cs.AI cs.SY eess.SY 



### Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI   Framework for Personal LLMs Fine-Tuning
**Authors**: Bei Ouyang, Shengyuan Ye, Liekang Zeng, Tianyi Qian, Jingyi Li, Xu Chen

**Updated**: 2024-08-20T11:30:12Z

**Summary**: Large language models (LLMs) have unlocked a plethora of powerful applications at the network edge, such as intelligent personal assistants. Data privacy and security concerns have prompted a shift towards edge-based fine-tuning of personal LLMs, away from cloud reliance. However, this raises issues of computational intensity and resource scarcity, hindering training efficiency and feasibility. While current studies investigate parameter-efficient fine-tuning (PEFT) techniques to mitigate resource constraints, our analysis indicates that these techniques are not sufficiently resource-efficient for edge devices. To tackle these challenges, we propose Pluto and Charon (PAC), a time and memory efficient collaborative edge AI framework for personal LLMs fine-tuning. PAC breaks the resource wall of personal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1) Algorithmically, PAC implements a personal LLMs fine-tuning technique that is efficient in terms of parameters, time, and memory. It utilizes Parallel Adapters to circumvent the need for a full backward pass through the LLM backbone. Additionally, an activation cache mechanism further streamlining the process by negating the necessity for repeated forward passes across multiple epochs. (2) Systematically, PAC leverages edge devices in close proximity, pooling them as a collective resource for in-situ personal LLMs fine-tuning, utilizing a hybrid data and pipeline parallelism to orchestrate distributed training. The use of the activation cache eliminates the need for forward pass through the LLM backbone,enabling exclusive fine-tuning of the Parallel Adapters using data parallelism. Extensive evaluation based on prototype implementation demonstrates that PAC remarkably outperforms state-of-the-art approaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction in memory footprint.

**Link**: [arxiv](http://arxiv.org/abs/2408.10746v1),  [pdf](http://arxiv.org/pdf/2408.10746v1)

**Tags**: cs.DC cs.AI cs.LG cs.NI 



### Heta: Distributed Training of Heterogeneous Graph Neural Networks
**Authors**: Yuchen Zhong, Junwei Su, Chuan Wu, Minjie Wang

**Updated**: 2024-08-20T04:46:18Z

**Summary**: Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic relationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable learning performance in various applications. However, current distributed GNN training systems often overlook unique characteristics of HetGs, such as varying feature dimensions and the prevalence of missing features among nodes, leading to suboptimal performance or even incompatibility with distributed HGNN training. We introduce Heta, a framework designed to address the communication bottleneck in distributed HGNN training. Heta leverages the inherent structure of HGNNs - independent relation-specific aggregations for each relation, followed by a cross-relation aggregation - and advocates for a novel Relation-Aggregation-First computation paradigm. It performs relation-specific aggregations within graph partitions and then exchanges partial aggregations. This design, coupled with a new graph partitioning method that divides a HetG based on its graph schema and HGNN computation dependency, substantially reduces communication overhead. Heta further incorporates an innovative GPU feature caching strategy that accounts for the different cache miss-penalties associated with diverse node types. Comprehensive evaluations of various HGNN models and large heterogeneous graph datasets demonstrate that Heta outperforms state-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in end-to-end epoch time, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2408.09697v2),  [pdf](http://arxiv.org/pdf/2408.09697v2)

**Tags**: cs.DC 



### Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory
**Authors**: Olena Tkach, Gerd Schoenhense

**Updated**: 2024-08-19T15:47:17Z

**Summary**: The strong electric field between the sample and the extractor is the core of cathode lenses and a pivotal determinant of high resolution. Nevertheless, fields in the range of 3-8 kV/mm can be a source of complications. Local field enhancement at sharp edges or microscopic protrusions of cleaved samples may result in field emission or flashovers. Moreover, slow background electrons are drawn into the microscope column, where they contribute to space charge effects. A novel front lens configuration, optimized through ray-tracing simulations, significantly reduces the field at the sample and allows even for zero field or retarding field, which serves to suppress space charge effects. One or several annular electrodes, situated in a concentric position relative to the extractor, serve to form an additional lens within the gap between the sample and the extractor. The refractory power of this lens, and consequently the field at the sample surface, can be modified by adjusting the potentials of the annular electrodes. The imaging properties and aberrations of this gap lens have been investigated with regard to momentum imaging and XPEEM. The study encompasses the energy range from the few-eV level for laser-ARPES to 6 keV, for hard X-ray ARPES. The additional converging lens situated in close proximity to the sample exhibits a reduced field curvature of the k-image in the backfocal plane. This allows for the acquisition of larger fields of view in both momentum and real-space imaging.

**Link**: [arxiv](http://arxiv.org/abs/2408.10104v1),  [pdf](http://arxiv.org/pdf/2408.10104v1)

**Tags**: physics.app-ph cond-mat.mtrl-sci physics.ins-det 



### Abstract Environment Trimming
**Authors**: Daniel Jurjo-Rivas, Jose F. Morales, Pedro López-García, Manuel V. Hermenegildo

**Updated**: 2024-08-19T09:50:35Z

**Summary**: Variable sharing is a fundamental property in the static analysis of logic programs, since it is instrumental for ensuring correctness and increasing precision while inferring many useful program properties. Such properties include modes, determinacy, non-failure, cost, etc. This has motivated significant work on developing abstract domains to improve the precision and performance of sharing analyses. Much of this work has centered around the family of set-sharing domains, because of the high precision they offer. However, this comes at a price: their scalability to a wide set of realistic programs remains challenging and this hinders their wider adoption. In this work, rather than defining new sharing abstract domains, we focus instead on developing techniques which can be incorporated in the analyzers to address aspects that are known to affect the efficiency of these domains, such as the number of variables, without affecting precision. These techniques are inspired in others used in the context of compiler optimizations, such as expression reassociation and variable trimming. We present several such techniques and provide an extensive experimental evaluation of over 1100 program modules taken from both production code and classical benchmarks. This includes the Spectector cache analyzer, the s(CASP) system, the libraries of the Ciao system, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental results are quite encouraging: we have obtained significant speed-ups, and, more importantly, the number of modules that require a timeout was cut in half. As a result, many more programs can be analyzed precisely in reasonable times.

**Link**: [arxiv](http://arxiv.org/abs/2408.09848v1),  [pdf](http://arxiv.org/pdf/2408.09848v1)

**Tags**: cs.PL 



### AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for   Efficient MoE Inference
**Authors**: Shuzhang Zhong, Ling Liang, Yuan Wang, Runsheng Wang, Ru Huang, Meng Li

**Updated**: 2024-08-19T03:27:15Z

**Summary**: Mixture-of-Experts (MoE) models are designed to enhance the efficiency of large language models (LLMs) without proportionally increasing the computational demands. However, their deployment on edge devices still faces significant challenges due to high on-demand loading overheads from managing sparsely activated experts. This paper introduces AdapMoE, an algorithm-system co-design framework for efficient MoE inference. AdapMoE features adaptive expert gating and management to reduce the on-demand loading overheads. We observe the heterogeneity of experts loading across layers and tokens, based on which we propose a sensitivity-based strategy to adjust the number of activated experts dynamically. Meanwhile, we also integrate advanced prefetching and cache management techniques to further reduce the loading latency. Through comprehensive evaluations on various platforms, we demonstrate AdapMoE consistently outperforms existing techniques, reducing the average number of activated experts by 25% and achieving a 1.35x speedup without accuracy degradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE.

**Link**: [arxiv](http://arxiv.org/abs/2408.10284v1),  [pdf](http://arxiv.org/pdf/2408.10284v1)

**Tags**: cs.LG 



### Post-Training Sparse Attention with Double Sparsity
**Authors**: Shuo Yang, Ying Sheng, Joseph E. Gonzalez, Ion Stoica, Lianmin Zheng

**Updated**: 2024-08-18T17:27:17Z

**Summary**: The inference process for large language models is slow and memory-intensive, with one of the most critical bottlenecks being excessive Key-Value (KV) cache accesses. This paper introduces "Double Sparsity," a novel post-training sparse attention technique designed to alleviate this bottleneck by reducing KV cache access. Double Sparsity combines token sparsity, which focuses on utilizing only the important tokens for computing self-attention, with channel sparsity, an approach that uses important feature channels for identifying important tokens. Our key insight is that the pattern of channel sparsity is relatively static, allowing us to use offline calibration to make it efficient at runtime, thereby enabling accurate and efficient identification of important tokens. Moreover, this method can be combined with offloading to achieve significant memory usage reduction. Experimental results demonstrate that Double Sparsity can achieve $\frac{1}{16}$ token and channel sparsity with minimal impact on accuracy across various tasks, including wiki-2 perplexity, key-value retrieval, and long context benchmarks with models including Llama-2-7B, Llama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\times$ acceleration in attention operations and a 1.9$\times$ improvement in end-to-end inference on GPUs. With offloading, it achieves a decoding speed acceleration of 16.3$\times$ compared to state-of-the-art solutions at a sequence length of 256K. Our code is publicly available at https://github.com/andy-yang-1/DoubleSparse.

**Link**: [arxiv](http://arxiv.org/abs/2408.07092v2),  [pdf](http://arxiv.org/pdf/2408.07092v2)

**Tags**: cs.LG cs.AI cs.CL 



### CMD: A Cache-assisted GPU Memory Deduplication Architecture
**Authors**: Wei Zhao, Dan Feng, Wei Tong, Xueliang Wei, Bing Wu

**Updated**: 2024-08-18T13:54:46Z

**Summary**: Massive off-chip accesses in GPUs are the main performance bottleneck, and we divided these accesses into three types: (1) Write, (2) Data-Read, and (3) Read-Only. Besides, We find that many writes are duplicate, and the duplication can be inter-dup and intra-dup. While inter-dup means different memory blocks are identical, and intra-dup means all the 4B elements in a line are the same. In this work, we propose a cache-assisted GPU memory deduplication architecture named CMD to reduce the off-chip accesses via utilizing the data duplication in GPU applications. CMD includes three key design contributions which aim to reduce the three kinds of accesses: (1) A novel GPU memory deduplication architecture that removes the inter-dup and inter-dup lines. As for the inter-dup detection, we reduce the extra read requests caused by the traditional read-verify hash process. Besides, we design several techniques to manage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce the reads to duplicate data. When an L2 cache miss wants to read the duplicate block, if the reference block has been fetched to L2 and it is clean, we can copy it to the L2 missed block without accessing off-chip DRAM. As for the reads to intra-dup data, CMD uses the on-chip metadata cache to get the data. (3) When a cache line is evicted, the clean sectors in the line are invalidated while the dirty sectors are written back. However, most read-only victims are re-referenced from DRAM more than twice. Therefore, we add a full-associate FIFO to accommodate the read-only (it is also clean) victims to reduce the re-reference counts. Experiments show that CMD can decrease the off-chip accesses by 31.01%, reduce the energy by 32.78% and improve performance by 37.79%. Besides, CMD can improve the performance of memory-intensive workloads by 50.18%.

**Link**: [arxiv](http://arxiv.org/abs/2408.09483v1),  [pdf](http://arxiv.org/pdf/2408.09483v1)

**Tags**: cs.AR 



### Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for   Efficient LLM Inference
**Authors**: Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, S. Kevin Zhou

**Updated**: 2024-08-16T08:46:33Z

**Summary**: Large Language Models have excelled in various fields but encounter challenges in memory and time efficiency due to the expanding Key-Value (KV) cache required for long-sequence inference. Recent efforts try to reduce KV cache size to a given memory budget by evicting vast non-critical cache elements during runtime, while preserving generation quality. Our revisiting of current eviction methods reveals that they fundamentally minimize an upper bound of the $L_1$ eviction loss between the pre- and post-eviction outputs of multi-head self-attention mechanisms. Moreover, our analysis indicates that the common practices of uniformly assigning budgets across attention heads harm their post-eviction generation quality. In light of these findings, we propose a simple yet effective adaptive budget allocation algorithm. This algorithm not only optimizes the theoretical loss upper bound but also reduces the $L_1$ eviction loss in practice by aligning with the varied characteristics across different heads. By integrating this algorithm into two state-of-the-art methods, we demonstrate the effectiveness of using adaptive budget allocation to optimize KV cache eviction. Extensive evaluations on 16 datasets and the Needle-in-a-Haystack test confirm significant performance improvements across various tasks.

**Link**: [arxiv](http://arxiv.org/abs/2407.11550v3),  [pdf](http://arxiv.org/pdf/2407.11550v3)

**Tags**: cs.CL cs.AI 



### SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language   Models
**Authors**: Kaushal Kumar Maurya, KV Aditya Srivatsa, Ekaterina Kochmar

**Updated**: 2024-08-16T06:11:21Z

**Summary**: Large language models (LLMs) have gained increased popularity due to their remarkable success across various tasks, which has led to the active development of a large set of diverse LLMs. However, individual LLMs have limitations when applied to complex tasks because of such factors as training biases, model sizes, and the datasets used. A promising approach is to efficiently harness the diverse capabilities of LLMs to overcome these individual limitations. Towards this goal, we introduce a novel LLM selection algorithm called SelectLLM. This algorithm directs input queries to the most suitable subset of LLMs from a large pool, ensuring they collectively provide the correct response efficiently. SelectLLM uses a multi-label classifier, utilizing the classifier's predictions and confidence scores to design optimal policies for selecting an optimal, query-aware, and lightweight subset of LLMs. Our findings show that the proposed model outperforms individual LLMs and achieves competitive performance compared to similarly sized, computationally expensive top-performing LLM subsets. Specifically, with a similarly sized top-performing LLM subset, we achieve a significant reduction in latency on two standard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower latency for MMLU. Additionally, we conduct comprehensive analyses and ablation studies, which validate the robustness of the proposed model.

**Link**: [arxiv](http://arxiv.org/abs/2408.08545v1),  [pdf](http://arxiv.org/pdf/2408.08545v1)

**Tags**: cs.CL 



### Symmetric Locality: Definition and Initial Results
**Authors**: Giordan Escalona, Dylan McKellips, Chen Ding

**Updated**: 2024-08-16T04:12:25Z

**Summary**: In this short paper, we characterize symmetric locality. In designing algorithms, compilers, and systems, data movement is a common bottleneck in high-performance computation, in which we improve cache and memory performance. We study a special type of data reuse in the form of repeated traversals, or re-traversals, which are based on the symmetric group. The cyclic and sawtooth traces are previously known results in symmetric locality, and in this work, we would like to generalize this result for any re-traversal. Then, we also provide an abstract framework for applications in compiler design and machine learning models to improve the memory performance of certain programs.

**Link**: [arxiv](http://arxiv.org/abs/2407.19291v2),  [pdf](http://arxiv.org/pdf/2407.19291v2)

**Tags**: eess.SY cs.SY 



### ConfusedPilot: Confused Deputy Risks in RAG-based LLMs
**Authors**: Ayush RoyChowdhury, Mulong Luo, Prateek Sahu, Sarbartha Banerjee, Mohit Tiwari

**Updated**: 2024-08-15T05:24:19Z

**Summary**: Retrieval augmented generation (RAG) is a process where a large language model (LLM) retrieves useful information from a database and then generates the responses. It is becoming popular in enterprise settings for daily business operations. For example, Copilot for Microsoft 365 has accumulated millions of businesses. However, the security implications of adopting such RAG-based systems are unclear.   In this paper, we introduce ConfusedPilot, a class of security vulnerabilities of RAG systems that confuse Copilot and cause integrity and confidentiality violations in its responses. First, we investigate a vulnerability that embeds malicious text in the modified prompt in RAG, corrupting the responses generated by the LLM. Second, we demonstrate a vulnerability that leaks secret data, which leverages the caching mechanism during retrieval. Third, we investigate how both vulnerabilities can be exploited to propagate misinformation within the enterprise and ultimately impact its operations, such as sales and manufacturing. We also discuss the root cause of these attacks by investigating the architecture of a RAG-based system. This study highlights the security vulnerabilities in today's RAG-based systems and proposes design guidelines to secure future RAG-based systems.

**Link**: [arxiv](http://arxiv.org/abs/2408.04870v3),  [pdf](http://arxiv.org/pdf/2408.04870v3)

**Tags**: cs.CR cs.AI 



### A Case for Enabling Delegation of 5G Core Decisions to the RAN
**Authors**: Lucas Vancina, Geoffrey Xie

**Updated**: 2024-08-14T23:42:46Z

**Summary**: Under conventional 5G system design, the authentication and continuous monitoring of user equipment (UE) demands a reliable backhaul connection between the radio access network (RAN) and the core network functions (AMF, AUSF, UDM, etc.). This is not a given, especially in disaster response and military operations. We propose that, in these scenarios, decisions made by core functions can be effectively delegated to the RAN by leveraging the RAN's computing resources and the micro-service programmability of the O-RAN system architecture. This paper presents several concrete designs of core-RAN decision delegation, including caching of core decisions and replicating some of the core decision logic. Each design has revealed interesting performance and security trade-offs that warrant further investigation.

**Link**: [arxiv](http://arxiv.org/abs/2408.07853v1),  [pdf](http://arxiv.org/pdf/2408.07853v1)

**Tags**: cs.NI 



### The Bicameral Cache: a split cache for vector architectures
**Authors**: Susana Rebolledo, Borja Perez, Jose Luis Bosque, Peter Hsu

**Updated**: 2024-08-14T09:18:02Z

**Summary**: The Bicameral Cache is a cache organization proposal for a vector architecture that segregates data according to their access type, distinguishing scalar from vector references. Its aim is to avoid both types of references from interfering in each other's data locality, with a special focus on prioritizing the performance on vector references. The proposed system incorporates an additional, non-polluting prefetching mechanism to help populate the long vector cache lines in advance to increase the hit rate by further exploiting the spatial locality on vector data. Its evaluation was conducted on the Cavatools simulator, comparing the performance to a standard conventional cache, over different typical vector benchmarks for several vector lengths. The results proved the proposed cache speeds up performance on stride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition, the prefetching feature consistently provided an additional value.

**Link**: [arxiv](http://arxiv.org/abs/2407.15440v2),  [pdf](http://arxiv.org/pdf/2407.15440v2)

**Tags**: cs.AR cs.PF 



### At Least Factor-of-Two Optimization for RWLE-Based Homomorphic   Encryption
**Authors**: Jonathan Ly

**Updated**: 2024-08-14T05:42:35Z

**Summary**: Many modern applications that deal with sensitive data, such as healthcare and government services, outsource computation to cloud platforms. In such untrusted environments, privacy is of vital importance. One solution to this problem is homomorphic encryption (HE), a family of cryptographic schemes that support certain algebraic operations on encrypted data without the need for decryption. However, despite major advancements, encryption in modern HE schemes still comes with a non-trivial computational overhead that can hamper data-intensive workloads. To resolve this, recent research has shown that leveraging caching techniques, such as Rache, can significantly enhance the performance of HE schemes while maintaining security. Rache unfortunately displays a key limitation in the time complexity of its caching procedure, which scales with the size of the plaintext space. Smuche is another caching scheme that simultaneously improves the scalability of the caching procedure and turns the encryption process into a constant-time operation, utilizing only a single scalar multiplication. Even still, more can be done. In this paper, we present an encryption method we call ``Zinc" which entirely forgoes the multiple caching process, replacing it with a single scalar addition, and then injecting randomness that takes constant time with respect to the plaintext space. This injection of randomness is similar to Smuche, and a great improvement from Rache, allowing Zinc to achieve efficiency without compromising security. We implement the scheme using Microsoft SEAL and compare its performance to vanilla CKKS.

**Link**: [arxiv](http://arxiv.org/abs/2408.07304v1),  [pdf](http://arxiv.org/pdf/2408.07304v1)

**Tags**: cs.CR 



### Cache-Aided MIMO Communications: DoF Analysis and Transmitter   Optimization
**Authors**: Mohammad NaseriTehrani, MohammadJavad Salehi, Antti Tölli

**Updated**: 2024-08-13T13:56:14Z

**Summary**: Cache-aided MIMO communications aims to jointly exploit both coded caching~(CC) and spatial multiplexing gains to enhance communication efficiency. In this paper, we first analyze the achievable degrees of freedom~(DoF) in a MIMO-CC system with CC gain \(t\), where a server with \(L\) transmit antennas communicates with \(K\) users, each equipped with \(G\) receive antennas. We demonstrate that the enhanced achievable DoF is \(\max_{\beta, \Omega} \Omega \beta\), where the number of users \(\Omega\) served in each transmission is fine-tuned to maximize DoF, and \(\beta \le \min\big(G, \nicefrac{L \binom{\Omega-1}{t}}{1 + (\Omega - t - 1)\binom{\Omega-1}{t}}\big)\) represents the number of parallel streams decoded by each user. Second, we introduce an effective transmit covariance matrix design aimed at maximizing the symmetric rate, solved iteratively via successive convex approximation. Third, we propose a new class of MIMO-CC schemes using a novel scheduling mechanism leveraging maximal multicasting opportunities to maximize delivery rates at given SNR levels while adhering to linear processing constraints. Lastly, we devise linear multicast beamforming strategies tailored for the flexible scheduling schemes in MIMO-CC systems and present an iterative solution for the efficient design of beamformers. Extensive numerical simulations are used to verify the results of the paper.

**Link**: [arxiv](http://arxiv.org/abs/2407.15743v2),  [pdf](http://arxiv.org/pdf/2407.15743v2)

**Tags**: cs.IT eess.SP math.IT 



### Ownership in low-level intermediate representation
**Authors**: Siddharth Priya, Arie Gurfinkel

**Updated**: 2024-08-13T13:31:34Z

**Summary**: The concept of ownership in high level languages can aid both the programmer and the compiler to reason about the validity of memory operations. Previously, ownership semantics has been used successfully in high level automatic program verification to model a reference to data by a first order logic (FOL) representation of data instead of maintaining an address map. However, ownership semantics is not used in low level program verification. We have identified two challenges. First, ownership information is lost when a program is compiled to a low level intermediate representation (e.g., in LLVM IR). Second, pointers in low level programs point to bytes using an address map (e.g., in unsafe Rust) and thus the verification condition (VC) cannot always replace a pointer by its FOL abstraction. To remedy the situation, we develop ownership semantics for an LLVM like low level intermediate representation. Using these semantics, the VC can opportunistically model some memory accesses by a direct access of a pointer cache that stores byte representation of data. This scheme reduces instances where an address map must be maintained, especially for mostly safe programs that follow ownership semantics. For unsafe functionality, memory accesses are modelled by operations on an address map and we provide mechanisms to keep the address map and pointer cache in sync. We implement these semantics in SEABMC, a bit precise bounded model checker for LLVM. For evaluation, the source programs are assumed to be written in C. Since C does not have ownership built in, suitable macros are added that introduce and preserve ownership during translation to LLVM like IR for verification. This approach is evaluated on mature open source C code. For both handcrafted benchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT solving.

**Link**: [arxiv](http://arxiv.org/abs/2408.04043v3),  [pdf](http://arxiv.org/pdf/2408.04043v3)

**Tags**: cs.PL cs.SE D.2.4 



### Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache   Consumption
**Authors**: Luohe Shi, Hongyi Zhang, Yao Yao, Zuchao Li, Hai Zhao

**Updated**: 2024-08-13T09:55:43Z

**Summary**: Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022, have revolutionized various industries with their advanced language comprehension. However, their efficiency is challenged by the Transformer architecture' s struggle with handling long texts. KV-Cache has emerged as a pivotal solution to this issue, converting the time complexity of token generation from quadratic to linear, albeit with increased GPU memory overhead proportional to conversation length. With the development of the LLM community and academia, various KV-Cache compression methods have been proposed. In this review, we dissect the various properties of KV-Cache and elaborate on various methods currently used to optimize the KV-Cache space usage of LLMs. These methods span the pre-training phase, deployment phase, and inference phase, and we summarize the commonalities and differences among these methods. Additionally, we list some metrics for evaluating the long-text capabilities of large language models, from both efficiency and capability perspectives. Our review thus sheds light on the evolving landscape of LLM optimization, offering insights into future advancements in this dynamic field.

**Link**: [arxiv](http://arxiv.org/abs/2407.18003v3),  [pdf](http://arxiv.org/pdf/2407.18003v3)

**Tags**: cs.CL 



### Finch: Prompt-guided Key-Value Cache Compression
**Authors**: Giulio Corallo, Paolo Papotti

**Updated**: 2024-08-13T09:08:55Z

**Summary**: Recent large language model applications, such as Retrieval-Augmented Generation and chatbots, have led to an increased need to process longer input contexts. However, this requirement is hampered by inherent limitations. Architecturally, models are constrained by a context window defined during training. Additionally, processing extensive texts requires substantial GPU memory. We propose a novel approach, Finch, to compress the input context by leveraging the pre-trained model weights of the self-attention. Given a prompt and a long text, Finch iteratively identifies the most relevant Key (K) and Value (V) pairs over chunks of the text conditioned on the prompt. Only such pairs are stored in the KV cache, which, within the space constrained by the context window, ultimately contains a compressed version of the long text. Our proposal enables models to consume large inputs even with high compression (up to 93x) while preserving semantic integrity without the need for fine-tuning.

**Link**: [arxiv](http://arxiv.org/abs/2408.00167v2),  [pdf](http://arxiv.org/pdf/2408.00167v2)

**Tags**: cs.AI 



### Value-based Proactive Caching for Sensing Data in Internet of Vehicles
**Authors**: Yantong Wang, Ke Liu, Hui Ji, Jiande Sun

**Updated**: 2024-08-12T08:46:30Z

**Summary**: Sensing data (SD) plays an important role in safe-related applications for Internet of Vehicles. Proactively caching required sensing data (SD) is a pivotal strategy for alleviating network congestion and improving data accessibility. Despite merits, existing studies predominantly address SD caching within a single time slot, which may not be scalable to scenarios involving multi-slots. Furthermore, the oversight of service capacity at caching nodes could lead to significant queuing delays in SD reception. To tackle these limitations, we jointly consider the problem of anchoring caching placement and requests allocation for SD. A value model incorporating both temporal and spacial characteristics is first proposed to estimate the significance of different caching decisions. Subsequently, a stochastic integer nonlinear programming model is provided to optimize the long-term system performance, which is converted into a series of online optimization problem by leveraging the Lyapunov method and linearized via introducing auxiliary variables. To expedite the solution, we provide a binary quantum particle swarm optimization based algorithm with quadratic time complexity. Numerical investigations demonstrate the superiority of proposed algorithms compared with other schemes in terms of energy consumption, response latency, and cache-hit ratio.

**Link**: [arxiv](http://arxiv.org/abs/2408.05996v1),  [pdf](http://arxiv.org/pdf/2408.05996v1)

**Tags**: cs.NI 



### Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open   Source RISC-V application processor
**Authors**: Riccardo Tedeschi, Luca Valente, Gianmarco Ottavi, Enrico Zelioli, Nils Wistoff, Massimiliano Giacometti, Abdul Basit Sajjad, Luca Benini, Davide Rossi

**Updated**: 2024-08-12T07:47:28Z

**Summary**: Symmetric Multi-Processing (SMP) based on cache coherency is crucial for high-end embedded systems like automotive applications. RISC-V is gaining traction, and open-source hardware (OSH) platforms offer solutions to issues such as IP costs and vendor dependency. Existing multi-core cache-coherent RISC-V platforms are complex and not efficient for small embedded core clusters. We propose an open-source SystemVerilog implementation of a lightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our design uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with Splash-3 benchmarks, our solution shows up to 32.87% faster performance in a dual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized using GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of the system area.

**Link**: [arxiv](http://arxiv.org/abs/2407.19895v2),  [pdf](http://arxiv.org/pdf/2407.19895v2)

**Tags**: eess.SY cs.SY 



### Correct Wrong Path
**Authors**: Bhargav Reddy Godala, Sankara Prasad Ramesh, Krishnam Tibrewala, Chrysanthos Pepi, Gino Chacon, Svilen Kanev, Gilles A. Pokam, Daniel A. Jiménez, Paul V. Gratz, David I. August

**Updated**: 2024-08-12T03:53:51Z

**Summary**: Modern OOO CPUs have very deep pipelines with large branch misprediction recovery penalties. Speculatively executed instructions on the wrong path can significantly change cache state, depending on speculation levels. Architects often employ trace-driven simulation models in the design exploration stage, which sacrifice precision for speed. Trace-driven simulators are orders of magnitude faster than execution-driven models, reducing the often hundreds of thousands of simulation hours needed to explore new micro-architectural ideas. Despite this strong benefit of trace-driven simulation, these often fail to adequately model the consequences of wrong path because obtaining them is nontrivial. Prior works consider either a positive or negative impact of wrong path but not both. Here, we examine wrong path execution in simulation results and design a set of infrastructure for enabling wrong-path execution in a trace driven simulator. Our analysis shows the wrong path affects structures on both the instruction and data sides extensively, resulting in performance variations ranging from $-3.05$\% to $20.9$\% when ignoring wrong path. To benefit the research community and enhance the accuracy of simulators, we opened our traces and tracing utility in the hopes that industry can provide wrong-path traces generated by their internal simulators, enabling academic simulation without exposing industry IP.

**Link**: [arxiv](http://arxiv.org/abs/2408.05912v1),  [pdf](http://arxiv.org/pdf/2408.05912v1)

**Tags**: cs.AR 



### Hierarchical Coded Caching with Low Subpacketization and Coding Delay
**Authors**: Rashid Ummer N. T., B. Sundar Rajan

**Updated**: 2024-08-11T16:35:10Z

**Summary**: Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN) considered a broadcast network consisting of a single server connected to a set of users each having a cache memory. Motivated by practical scenarios, Karamchandani \textit{et al.} in [16] proposed a coded caching scheme for a two-layer hierarchical network consisting of a single server connected to multiple mirror sites and each mirror site connected to a distinct set of users, in which both mirror sites and users having cache memories. Low subpacketization level coded caching schemes are desirable for practical implementations. Placement delivery array (PDA) was proposed as a tool to design coded caching schemes with reduced subpacketization level by Yan \textit{et al.} in [4]. Schemes with reduced subpacketization levels are studied extensively in the literature for single-layer networks. Kong \textit{et al.} in [17] proposed a structure called hierarchical placement delivery arrays (HPDA), which characterizes a hierarchical coded caching system and also proposed a class of HPDAs that gives low subpacketization level schemes by using two PDAs. Low subpacketization level hierarchical schemes using combinatorial $t$-designs is proposed in [20]. Apart from that there is no other existing work that discusses the subpacketization problem in a hierarchical network. This paper proposes a class of HPDA construction that gives low subpacketization level hierarchical coded caching schemes, by first constructing a new class of PDAs. Compared with the existing schemes, in cases where the system parameters and subpacketization level are the same, the proposed hierarchical scheme has a better coding delay. Further, the new class of PDAs constructed either subsumes several known PDA constructions or achieves better transmission load for the same system parameters.

**Link**: [arxiv](http://arxiv.org/abs/2405.12747v2),  [pdf](http://arxiv.org/pdf/2405.12747v2)

**Tags**: cs.IT math.IT 



### Genie: Smart ROS-based Caching for Connected Autonomous Robots
**Authors**: Zexin Li, Soroush Bateni, Cong Liu

**Updated**: 2024-08-11T08:07:28Z

**Summary**: Despite the promising future of autonomous robots, several key issues currently remain that can lead to compromised performance and safety. One such issue is latency, where we find that even the latest embedded platforms from NVIDIA fail to execute intelligence tasks (e.g., object detection) of autonomous vehicles in a real-time fashion. One remedy to this problem is the promising paradigm of edge computing. Through collaboration with our industry partner, we identify key prohibitive limitations of the current edge mindset: (1) servers are not distributed enough and thus, are not close enough to vehicles, (2) current proposed edge solutions do not provide substantially better performance and extra information specific to autonomous vehicles to warrant their cost to the user, and (3) the state-of-the-art solutions are not compatible with popular frameworks used in autonomous systems, particularly the Robot Operating System (ROS).   To remedy these issues, we provide Genie, an encapsulation technique that can enable transparent caching in ROS in a non-intrusive way (i.e., without modifying the source code), can build the cache in a distributed manner (in contrast to traditional central caching methods), and can construct a collective three-dimensional object map to provide substantially better latency (even on low-power edge servers) and higher quality data to all vehicles in a certain locality. We fully implement our design on state-of-the-art industry-adopted embedded and edge platforms, using the prominent autonomous driving software Autoware, and find that Genie can enhance the latency of Autoware Vision Detector by 82% on average, enable object reusability 31% of the time on average and as much as 67% for the incoming requests, and boost the confidence in its object map considerably over time.

**Link**: [arxiv](http://arxiv.org/abs/2402.19410v2),  [pdf](http://arxiv.org/pdf/2402.19410v2)

**Tags**: cs.RO cs.SY eess.SY 



### Eigen Attention: Attention in Low-Rank Space for KV Cache Compression
**Authors**: Utkarsh Saxena, Gobinda Saha, Sakshi Choudhary, Kaushik Roy

**Updated**: 2024-08-10T22:47:12Z

**Summary**: Large language models (LLMs) represent a groundbreaking advancement in the domain of natural language processing due to their impressive reasoning abilities. Recently, there has been considerable interest in increasing the context lengths for these models to enhance their applicability to complex tasks. However, at long context lengths and large batch sizes, the key-value (KV) cache, which stores the attention keys and values, emerges as the new bottleneck in memory usage during inference. To address this, we propose Eigen Attention, which performs the attention operation in a low-rank space, thereby reducing the KV cache memory overhead. Our proposed approach is orthogonal to existing KV cache compression techniques and can be used synergistically with them. Through extensive experiments over OPT, MPT, and Llama model families, we demonstrate that Eigen Attention results in up to 40% reduction in KV cache sizes and up to 60% reduction in attention operation latency with minimal drop in performance.

**Link**: [arxiv](http://arxiv.org/abs/2408.05646v1),  [pdf](http://arxiv.org/pdf/2408.05646v1)

**Tags**: cs.LG cs.AI cs.CL 



### ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using   Gaussian Mixture Model
**Authors**: Hanqiu Chen, Yitu Wang, Luis Vitorio Cargnini, Mohammadreza Soltaniyeh, Dongyang Li, Gongjin Sun, Pradeep Subedi, Andrew Chang, Yiran Chen, Cong Hao

**Updated**: 2024-08-10T19:17:46Z

**Summary**: Compute Express Link (CXL) emerges as a solution for wide gap between computational speed and data communication rates among host and multiple devices. It fosters a unified and coherent memory space between host and CXL storage devices such as such as Solid-state drive (SSD) for memory expansion, with a corresponding DRAM implemented as the device cache. However, this introduces challenges such as substantial cache miss penalties, sub-optimal caching due to data access granularity mismatch between the DRAM "cache" and SSD "memory", and inefficient hardware cache management. To address these issues, we propose a novel solution, named ICGMM, which optimizes caching and eviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based approach. We prototype our solution on an FPGA board, which demonstrates a noteworthy improvement compared to the classic Least Recently Used (LRU) cache strategy. We observe a decrease in the cache miss rate ranging from 0.32% to 6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD access latency. Furthermore, when compared to the state-of-the-art Long Short-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA showcases an impressive latency reduction of over 10,000 times. Remarkably, this is achieved while demanding much fewer hardware resources.

**Link**: [arxiv](http://arxiv.org/abs/2408.05614v1),  [pdf](http://arxiv.org/pdf/2408.05614v1)

**Tags**: cs.AR cs.ET cs.SY eess.SY 



### Time-resolved measurement of neutron energy isotropy in a   sheared-flow-stabilized Z pinch
**Authors**: R. A. Ryan, P. E. Tsai, A. R. Johansen, A. Youmans, D. P. Higginson, J. M. Mitrani, C. S. Adams, D. A. Sutherland, B. Levitt, U. Shumlak

**Updated**: 2024-08-09T16:48:01Z

**Summary**: Previous measurements of neutron energy using fast plastic scintillators while operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of any yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been operated at increasingly higher input power, resulting in increased plasma current and larger fusion neutron yields. A detailed experimental study of the neutron energy isotropy in these regimes applies more stringent limits to possible contributions from beam-target fusion. The FuZE device operated at $-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and D-D fusion neutron yields of $4\times10^7$ neutrons per discharge. Measurements of the neutron energy isotropy under these operating conditions demonstrates the energy of deuteron beams is less than $7.4 \pm 5.6^\mathrm{(stat)} \pm 3.7^\mathrm{(syst)}~keV$. Characterization of the detector response has reduced the number of free parameters in the fit of the neutron energy distribution, improving the confidence in the forward-fit method. Gamma backgrounds have been measured and the impact of these contributions on the isotropy results have been studied. Additionally, a time dependent measurement of the isotropy has been resolved for the first time, indicating increases to possible deuteron beam energies at late times. This suggests the possible growth of $m$=0 instabilities at the end of the main radiation event but confirms that the majority of the neutron production exhibits isotropy consistent with thermonuclear origin.

**Link**: [arxiv](http://arxiv.org/abs/2408.05171v1),  [pdf](http://arxiv.org/pdf/2408.05171v1)

**Tags**: physics.plasm-ph nucl-ex 



### NACL: A General and Effective KV Cache Eviction Framework for LLMs at   Inference Time
**Authors**: Yilong Chen, Guoxia Wang, Junyuan Shang, Shiyao Cui, Zhenyu Zhang, Tingwen Liu, Shuohuan Wang, Yu Sun, Dianhai Yu, Hua Wu

**Updated**: 2024-08-08T01:20:13Z

**Summary**: Large Language Models (LLMs) have ignited an innovative surge of AI applications, marking a new era of exciting possibilities equipped with extended context windows. However, hosting these models is cost-prohibitive mainly due to the extensive memory consumption of KV Cache involving long-context modeling. Despite several works proposing to evict unnecessary tokens from the KV Cache, most of them rely on the biased local statistics of accumulated attention scores and report performance using unconvincing metric like perplexity on inadequate short-text evaluation. In this paper, we propose NACL, a general framework for long-context KV cache eviction that achieves more optimal and efficient eviction in a single operation during the encoding phase. Due to NACL's efficiency, we combine more accurate attention score statistics in PROXY TOKENS EVICTION with the diversified random eviction strategy of RANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance the robustness in maintaining pivotal tokens for long-context modeling tasks. Notably, our method significantly improves the performance on short- and long-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50% with over 95% performance maintenance. The code is available at https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.

**Link**: [arxiv](http://arxiv.org/abs/2408.03675v2),  [pdf](http://arxiv.org/pdf/2408.03675v2)

**Tags**: cs.CL 



### A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals   and Future Trends
**Authors**: Yao Zhao, Youyang Qu, Yong Xiang, Md Palash Uddin, Dezhong Peng, Longxiang Gao

**Updated**: 2024-08-07T23:48:59Z

**Summary**: Recent advances in edge computing~(EC) have pushed cloud-based data caching services to edge, however, such emerging edge storage comes with numerous challenging and unique security issues. One of them is the problem of edge data integrity verification (EDIV) which coordinates multiple participants (e.g., data owners and edge nodes) to inspect whether data cached on edge is authentic. To date, various solutions have been proposed to address the EDIV problem, while there is no systematic review. Thus, we offer a comprehensive survey for the first time, aiming to show current research status, open problems, and potentially promising insights for readers to further investigate this under-explored field. Specifically, we begin by stating the significance of the EDIV problem, the integrity verification difference between data cached on cloud and edge, and three typical system models with corresponding inspection processes. To thoroughly assess prior research efforts, we synthesize a universal criteria framework that an effective verification approach should satisfy. On top of it, a schematic development timeline is developed to reveal the research advance on EDIV in a sequential manner, followed by a detailed review of the existing EDIV solutions. Finally, we highlight intriguing research challenges and possible directions for future work, along with a discussion on how forthcoming technology, e.g., machine learning and context-aware security, can augment security in EC. Given our findings, some major observations are: there is a noticeable trend to equip EDIV solutions with various functions and diversify study scenarios; completing EDIV within two types of participants (i.e., data owner and edge nodes) is garnering escalating interest among researchers; although the majority of existing methods rely on cryptography, emerging technology is being explored to handle the EDIV problem.

**Link**: [arxiv](http://arxiv.org/abs/2210.10978v2),  [pdf](http://arxiv.org/pdf/2210.10978v2)

**Tags**: cs.CR 



### Zero-Delay QKV Compression for Mitigating KV Cache and Network   Bottlenecks in LLM Inference
**Authors**: Zeyu Zhang, Haiying Shen

**Updated**: 2024-08-07T22:10:26Z

**Summary**: In large-language models, memory constraints in the key-value cache (KVC) pose a challenge during inference, especially with long prompts. In this work, we observed that compressing KV values is more effective than compressing the model regarding accuracy and job completion time (JCT). However, quantizing KV values and dropping less-important tokens incur significant runtime computational time overhead, delaying JCT. These methods also cannot reduce computation time or high network communication time overhead in sequence-parallelism (SP) frameworks for long prompts. To tackle these issues, based on our insightful observations from experimental analysis, we propose ZeroC, a Zero-delay QKV Compression system that eliminates time overhead and even reduces computation and communication time of the model operations. ZeroC innovatively embeds compression and decompression operations within model operations and adaptively determines compression ratios at a hybrid layer-token level. Further, it enables a communication-efficient SP inference framework. Trace-driven experiments demonstrate that ZeroC achieves up to 80% lower average JCT, 35% lower average perplexity, and 2.8x higher throughput with the same latency compared to state-of-the-art compression methods. ZeroC also reduces the average JCT of current LLM serving systems by up to 91% with the constraint of 0.1 perplexity increase. We open-sourced the code.

**Link**: [arxiv](http://arxiv.org/abs/2408.04107v1),  [pdf](http://arxiv.org/pdf/2408.04107v1)

**Tags**: cs.LG cs.DC 



### Temporal Feature Matters: A Framework for Diffusion Model Quantization
**Authors**: Yushi Huang, Ruihao Gong, Xianglong Liu, Jing Liu, Yuhang Li, Jiwen Lu, Dacheng Tao

**Updated**: 2024-08-07T20:43:10Z

**Summary**: The Diffusion models, widely used for image generation, face significant challenges related to their broad applicability due to prolonged inference times and high memory demands. Efficient Post-Training Quantization (PTQ) is crucial to address these issues. However, unlike traditional models, diffusion models critically rely on the time-step for the multi-round denoising. Typically, each time-step is encoded into a hypersensitive temporal feature by several modules. Despite this, existing PTQ methods do not optimize these modules individually. Instead, they employ unsuitable reconstruction objectives and complex calibration methods, leading to significant disturbances in the temporal feature and denoising trajectory, as well as reduced compression efficiency. To address these challenges, we introduce a novel quantization framework that includes three strategies: 1) TIB-based Maintenance: Based on our innovative Temporal Information Block (TIB) definition, Temporal Information-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are developed to efficiently align original temporal features. 2) Cache-based Maintenance: Instead of indirect and complex optimization for the related modules, pre-computing and caching quantized counterparts of temporal features are developed to minimize errors. 3) Disturbance-aware Selection: Employ temporal feature errors to guide a fine-grained selection between the two maintenance strategies for further disturbance reduction. This framework preserves most of the temporal information and ensures high-quality end-to-end generation. Extensive testing on various datasets, diffusion models and hardware confirms our superior performance and acceleration..

**Link**: [arxiv](http://arxiv.org/abs/2407.19547v2),  [pdf](http://arxiv.org/pdf/2407.19547v2)

**Tags**: cs.CV 



### mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest   Neighbor Search
**Authors**: Ahmed Abdou, Tasneem Mohsen

**Updated**: 2024-08-07T09:34:55Z

**Summary**: Named Entity Recognition (NER) is a task in Natural Language Processing (NLP) that aims to identify and classify entities in text into predefined categories. However, when applied to Arabic data, NER encounters unique challenges stemming from the language's rich morphological inflections, absence of capitalization cues, and spelling variants, where a single word can comprise multiple morphemes. In this paper, we introduce Arabic KNN-NER, our submission to the Wojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the shared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained flat-entity recognition for Arabic text, where we identify a single main entity and possibly zero or multiple sub-entities for each word. Arabic KNN-NER augments the probability distribution of a fine-tuned model with another label probability distribution derived from performing a KNN search over the cached training data. Our submission achieved 91% on the test set on the WojoodFine dataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.

**Link**: [arxiv](http://arxiv.org/abs/2408.03652v1),  [pdf](http://arxiv.org/pdf/2408.03652v1)

**Tags**: cs.CL cs.LG 



### LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning
**Authors**: Lekai Chen, Ashutosh Trivedi, Alvaro Velasquez

**Updated**: 2024-08-06T07:12:09Z

**Summary**: The emergence of intelligence in large language models (LLMs) has inspired investigations into their integration into automata learning. This paper introduces the probabilistic Minimally Adequate Teacher (pMAT) formulation, which leverages a probabilistic oracle that could give persistent errors randomly during answering the membership queries for deterministic finite automata (DFA) learning. Given the tendency of LLMs to produce hallucinatory content, we have developed techniques to improve answer accuracy and ensure the correctness of the learned automata. We propose the $\mathtt{Discrimination}$ prompt as well as the $\mathtt{Verification}$ prompt and explore their advantages over common prompts. Additionally, we compare DFA learning performance between the TTT algorithm and common active learning algorithms. To address the exponential number of persistent errors, we implement a dynamic query cache refinement algorithm that identifies and corrects conflicting queries by combining the active and passive learning algorithms. The empirical results demonstrate the robustness and efficiency of our approach, providing a theoretical foundation for automata learning with LLMs in the loop.

**Link**: [arxiv](http://arxiv.org/abs/2408.02999v1),  [pdf](http://arxiv.org/pdf/2408.02999v1)

**Tags**: cs.FL cs.AI 



### NVPC: A Transparent NVM Page Cache
**Authors**: Guoyu Wang, Xilong Che, Haoyang Wei, Shuo Chen, Puyi He, Juncheng Hu

**Updated**: 2024-08-06T02:51:22Z

**Summary**: Towards a compatible utilization of NVM, NVM-specialized kernel file systems and NVM-based disk file system accelerators have been proposed. However, these studies only focus on one or several characteristics of NVM, while failing to exploit its best practice by putting NVM in the proper position of the whole storage stack. In this paper, we present NVPC, a transparent acceleration to existing kernel file systems with an NVM-enhanced page cache. The acceleration lies in two aspects, respectively matching the desperate needs of existing disk file systems: sync writes and cache-missed operations. Besides, the fast DRAM page cache is preserved for cache-hit operations. For sync writes, a high-performance log-based sync absorbing area is provided to redirect data destination from the slow disk to the fast NVM. Meanwhile, the byte-addressable feature of NVM is used to prevent write amplification. For cache-missed operations, NVPC makes use of the idle space on NVM to extend the DRAM page cache, so that more and larger workloads can fit into the cache. NVPC is entirely implemented as a page cache, thus can provide efficient speed-up to disk file systems with full transparency to users and full compatibility to lower file systems.   In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x faster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger than DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and SPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in 62.5% of the tested cases in our read/write/sync mixed evaluation, demonstrating that NVPC is more balanced and adaptive to complex real-world workloads. Experimental results also show that NVPC is the only method that accelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to any other use cases.

**Link**: [arxiv](http://arxiv.org/abs/2408.02911v1),  [pdf](http://arxiv.org/pdf/2408.02911v1)

**Tags**: cs.OS 



### Electron-beam-induced modification of gold microparticles in an SEM
**Authors**: Kristina Weinel, Marc Benjamin Hahn, Axel Lubk, Wen Feng, Ignacio Gonzalez Martinez, Bernd Büchner, Leonardo Agudo Jácome

**Updated**: 2024-08-05T12:09:50Z

**Summary**: Electron-beam-induced conversion of materials in a transmission electron microscope uses the high power density of a localized electron beam of acceleration voltages above 100 kV as an energy source to transform matter at the sub-micron scale. Here, the e-beam-induced transformation of precursor microparticles employing a low-energy e-beam with an acceleration voltage of 30 kV in a scanning electron microscope is developed to increase the versatility and efficiency of the technique. Under these conditions, the technique can be classified between e-beam lithography, where the e-beam is used to mill holes in or grow some different material onto a substrate, and e-beam welding, where matter can be welded together when overcoming the melting phase. Modifying gold microparticles on an amorphous SiOx substrate reveals the dominant role of inelastic electron-matter interaction and subsequent localized heating for the observed melting and vaporization of the precursor microparticles under the electron beam. Monte-Carlo scattering simulations and thermodynamic modeling further support the findings.

**Link**: [arxiv](http://arxiv.org/abs/2408.02409v1),  [pdf](http://arxiv.org/pdf/2408.02409v1)

**Tags**: cond-mat.mtrl-sci 



### SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference   Serving
**Authors**: Andreas Kosmas Kakolyris, Dimosthenis Masouros, Petros Vavaroutsos, Sotirios Xydis, Dimitrios Soudris

**Updated**: 2024-08-05T09:07:06Z

**Summary**: As Large Language Models (LLMs) gain traction, their reliance on power-hungry GPUs places ever-increasing energy demands, raising environmental and monetary concerns. Inference dominates LLM workloads, presenting a critical challenge for providers: minimizing energy costs under Service-Level Objectives (SLOs) that ensure optimal user experience. In this paper, we present \textit{throttLL'eM}, a framework that reduces energy consumption while meeting SLOs through the use of instance and GPU frequency scaling. \textit{throttLL'eM} features mechanisms that project future KV cache usage and batch size. Leveraging a Machine-Learning (ML) model that receives these projections as inputs, \textit{throttLL'eM} manages performance at the iteration level to satisfy SLOs with reduced frequencies and instance sizes. We show that the proposed ML model achieves $R^2$ scores greater than 0.97 and miss-predicts performance by less than 1 iteration per second on average. Experimental results on LLM inference traces show that \textit{throttLL'eM} achieves up to 43.8\% lower energy consumption and an energy efficiency improvement of at least $1.71\times$ under SLOs, when compared to NVIDIA's Triton server.

**Link**: [arxiv](http://arxiv.org/abs/2408.05235v1),  [pdf](http://arxiv.org/pdf/2408.05235v1)

**Tags**: cs.DC cs.AI cs.AR cs.LG 



### TriForce: Lossless Acceleration of Long Sequence Generation with   Hierarchical Speculative Decoding
**Authors**: Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, Beidi Chen

**Updated**: 2024-08-04T00:58:04Z

**Summary**: With large language models (LLMs) widely deployed in long content generation recently, there has emerged an increasing demand for efficient long-sequence inference support. However, key-value (KV) cache, which is stored to avoid re-computation, has emerged as a critical bottleneck by growing linearly in size with the sequence length. Due to the auto-regressive nature of LLMs, the entire KV cache will be loaded for every generated token, resulting in low utilization of computational cores and high latency. While various compression methods for KV cache have been proposed to alleviate this issue, they suffer from degradation in generation quality. We introduce TriForce, a hierarchical speculative decoding system that is scalable for long sequence generation. This approach leverages the original model weights and dynamic sparse KV cache via retrieval as a draft model, which serves as an intermediate layer in the hierarchy and is further speculated by a smaller model to reduce its drafting latency. TriForce not only facilitates impressive speedups for Llama2-7B-128K, achieving up to 2.31$\times$ on an A100 GPU but also showcases scalability in handling even longer contexts. For the offloading setting on two RTX 4090 GPUs, TriForce achieves 0.108s/token$\unicode{x2014}$only half as slow as the auto-regressive baseline on an A100, which attains 7.78$\times$ on our optimized offloading system. Additionally, TriForce performs 4.86$\times$ than DeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is highlighted by its consistently outstanding performance across various temperatures. The code is available at https://github.com/Infini-AI-Lab/TriForce.

**Link**: [arxiv](http://arxiv.org/abs/2404.11912v3),  [pdf](http://arxiv.org/pdf/2404.11912v3)

**Tags**: cs.CL cs.LG 



### Cross-layer Attention Sharing for Large Language Models
**Authors**: Yongyu Mu, Yuzhang Wu, Yuchun Fan, Chenglong Wang, Hengyu Li, Qiaozhi He, Murun Yang, Tong Xiao, Jingbo Zhu

**Updated**: 2024-08-04T00:38:34Z

**Summary**: As large language models (LLMs) evolve, the increase in model depth and parameter number leads to substantial redundancy. To enhance the efficiency of the attention mechanism, previous works primarily compress the KV cache or group attention heads, while largely overlooking redundancy between layers. Our comprehensive analyses across various LLMs show that highly similar attention patterns persist within most layers. It's intuitive to save the computation by sharing attention weights across layers. However, further analysis reveals two challenges: (1) Directly sharing the weight matrix without carefully rearranging the attention heads proves to be ineffective; (2) Shallow layers are vulnerable to small deviations in attention weights. Driven by these insights, we introduce LiSA, a lightweight substitute for self-attention in well-trained LLMs. LiSA employs tiny feed-forward networks to align attention heads between adjacent layers and low-rank matrices to approximate differences in layer-wise attention weights. Evaluations encompassing 13 typical benchmarks demonstrate that LiSA maintains high response quality in terms of accuracy and perplexity while reducing redundant attention calculations within 53-84% of the total layers. Our implementations of LiSA achieve a 6X compression of Q and K, with maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for LLaMA2-7B.

**Link**: [arxiv](http://arxiv.org/abs/2408.01890v1),  [pdf](http://arxiv.org/pdf/2408.01890v1)

**Tags**: cs.CL 



### Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling
**Authors**: Xiao Jiang, Grace J. Gang, J. Webster Stayman

**Updated**: 2024-08-02T18:25:57Z

**Summary**: Many spectral CT applications require accurate material decomposition. Existing material decomposition algorithms are often susceptible to significant noise magnification or, in the case of one-step model-based approaches, hampered by slow convergence rates and large computational requirements. In this work, we proposed a novel framework - spectral diffusion posterior sampling (spectral DPS) - for one-step reconstruction and multi-material decomposition, which combines sophisticated prior information captured by one-time unsupervised learning and an arbitrary analytic physical system model. Spectral DPS is built upon a general DPS framework for nonlinear inverse problems. Several strategies developed in previous work, including jumpstart sampling, Jacobian approximation, and multi-step likelihood updates are applied facilitate stable and accurate decompositions. The effectiveness of spectral DPS was evaluated on a simulated dual-layer and a kV-switching spectral system as well as on a physical cone-beam CT (CBCT) test bench. In simulation studies, spectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53% to 57.30% over MBMD, depending on the the region of interest. In physical phantom study, spectral DPS achieved a <1% error in estimating the mean density in a homogeneous region. Compared with baseline DPS, spectral DPS effectively avoided generating false structures in the homogeneous phantom and reduced the variability around edges. Both simulation and physical phantom studies demonstrated the superior performance of spectral DPS for stable and accurate material decomposition.

**Link**: [arxiv](http://arxiv.org/abs/2408.01519v1),  [pdf](http://arxiv.org/pdf/2408.01519v1)

**Tags**: physics.med-ph 



## Keyword: LLM Inference 
 ### Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale
**Authors**: Rogerio Bonatti, Dan Zhao, Francesco Bonacci, Dillon Dupont, Sara Abdali, Yinheng Li, Justin Wagle, Kazuhito Koishida, Arthur Bucker, Lawrence Jang, Zack Hui

**Updated**: 2024-09-12T17:56:43Z

**Summary**: Large language models (LLMs) show remarkable potential to act as computer agents, enhancing human productivity and software accessibility in multi-modal tasks that require planning and reasoning. However, measuring agent performance in realistic environments remains a challenge since: (i) most benchmarks are limited to specific modalities or domains (e.g. text-only, web navigation, Q&A, coding) and (ii) full benchmark evaluations are slow (on order of magnitude of days) given the multi-step sequential nature of tasks. To address these challenges, we introduce the Windows Agent Arena: a reproducible, general environment focusing exclusively on the Windows operating system (OS) where agents can operate freely within a real Windows OS and use the same wide range of applications, tools, and web browsers available to human users when solving tasks. We adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverse Windows tasks across representative domains that require agent abilities in planning, screen understanding, and tool usage. Our benchmark is scalable and can be seamlessly parallelized in Azure for a full benchmark evaluation in as little as 20 minutes. To demonstrate Windows Agent Arena's capabilities, we also introduce a new multi-modal agent, Navi. Our agent achieves a success rate of 19.5% in the Windows domain, compared to 74.5% performance of an unassisted human. Navi also demonstrates strong performance on another popular web-based benchmark, Mind2Web. We offer extensive quantitative and qualitative analysis of Navi's performance, and provide insights into the opportunities for future research in agent development and data generation using Windows Agent Arena.   Webpage: https://microsoft.github.io/WindowsAgentArena   Code: https://github.com/microsoft/WindowsAgentArena

**Link**: [arxiv](http://arxiv.org/abs/2409.08264v1),  [pdf](http://arxiv.org/pdf/2409.08264v1)

**Tags**: cs.AI 



### Derivative-Free Guidance in Continuous and Discrete Diffusion Models   with Soft Value-Based Decoding
**Authors**: Xiner Li, Yulai Zhao, Chenyu Wang, Gabriele Scalia, Gokcen Eraslan, Surag Nair, Tommaso Biancalani, Aviv Regev, Sergey Levine, Masatoshi Uehara

**Updated**: 2024-09-12T17:56:40Z

**Summary**: Diffusion models excel at capturing the natural design spaces of images, molecules, DNA, RNA, and protein sequences. However, rather than merely generating designs that are natural, we often aim to optimize downstream reward functions while preserving the naturalness of these design spaces. Existing methods for achieving this goal often require ``differentiable'' proxy models (\textit{e.g.}, classifier guidance or DPS) or involve computationally expensive fine-tuning of diffusion models (\textit{e.g.}, classifier-free guidance, RL-based fine-tuning). In our work, we propose a new method to address these challenges. Our algorithm is an iterative sampling method that integrates soft value functions, which looks ahead to how intermediate noisy states lead to high rewards in the future, into the standard inference procedure of pre-trained diffusion models. Notably, our approach avoids fine-tuning generative models and eliminates the need to construct differentiable models. This enables us to (1) directly utilize non-differentiable features/reward feedback, commonly used in many scientific domains, and (2) apply our method to recent discrete diffusion models in a principled way. Finally, we demonstrate the effectiveness of our algorithm across several domains, including image generation, molecule generation, and DNA/RNA sequence generation. The code is available at \href{https://github.com/masa-ue/SVDD}{https://github.com/masa-ue/SVDD}.

**Link**: [arxiv](http://arxiv.org/abs/2408.08252v3),  [pdf](http://arxiv.org/pdf/2408.08252v3)

**Tags**: cs.LG cs.AI q-bio.GN stat.ML 



### Improving Text-guided Object Inpainting with Semantic Pre-inpainting
**Authors**: Yifu Chen, Jingwen Chen, Yingwei Pan, Yehao Li, Ting Yao, Zhineng Chen, Tao Mei

**Updated**: 2024-09-12T17:55:37Z

**Summary**: Recent years have witnessed the success of large text-to-image diffusion models and their remarkable potential to generate high-quality images. The further pursuit of enhancing the editability of images has sparked significant interest in the downstream task of inpainting a novel object described by a text prompt within a designated region in the image. Nevertheless, the problem is not trivial from two aspects: 1) Solely relying on one single U-Net to align text prompt and visual object across all the denoising timesteps is insufficient to generate desired objects; 2) The controllability of object generation is not guaranteed in the intricate sampling space of diffusion model. In this paper, we propose to decompose the typical single-stage object inpainting into two cascaded processes: 1) semantic pre-inpainting that infers the semantic features of desired objects in a multi-modal feature space; 2) high-fieldity object generation in diffusion latent space that pivots on such inpainted semantic features. To achieve this, we cascade a Transformer-based semantic inpainter and an object inpainting diffusion model, leading to a novel CAscaded Transformer-Diffusion (CAT-Diffusion) framework for text-guided object inpainting. Technically, the semantic inpainter is trained to predict the semantic features of the target object conditioning on unmasked context and text prompt. The outputs of the semantic inpainter then act as the informative visual prompts to guide high-fieldity object generation through a reference adapter layer, leading to controllable object inpainting. Extensive evaluations on OpenImages-V6 and MSCOCO validate the superiority of CAT-Diffusion against the state-of-the-art methods. Code is available at \url{https://github.com/Nnn-s/CATdiffusion}.

**Link**: [arxiv](http://arxiv.org/abs/2409.08260v1),  [pdf](http://arxiv.org/pdf/2409.08260v1)

**Tags**: cs.CV cs.MM 



### OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable   Personal Question Answering
**Authors**: Jiahao Nick Li, Zhuohao Jerry Zhang, Jiaju Ma

**Updated**: 2024-09-12T17:48:08Z

**Summary**: People often capture memories through photos, screenshots, and videos. While existing AI-based tools enable querying this data using natural language, they mostly only support retrieving individual pieces of information like certain objects in photos and struggle with answering more complex queries that involve interpreting interconnected memories like event sequences. We conducted a one-month diary study to collect realistic user queries and generated a taxonomy of necessary contextual information for integrating with captured memories. We then introduce OmniQuery, a novel system that is able to answer complex personal memory-related questions that require extracting and inferring contextual information. OmniQuery augments single captured memories through integrating scattered contextual information from multiple interconnected memories, retrieves relevant memories, and uses a large language model (LLM) to comprehensive answers. In human evaluations, we show the effectiveness of OmniQuery with an accuracy of 71.5%, and it outperformed a conventional RAG system, winning or tying in 74.5% of the time.

**Link**: [arxiv](http://arxiv.org/abs/2409.08250v1),  [pdf](http://arxiv.org/pdf/2409.08250v1)

**Tags**: cs.HC cs.AI 



### E2 TTS: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS
**Authors**: Sefik Emre Eskimez, Xiaofei Wang, Manthan Thakker, Canrun Li, Chung-Hsien Tsai, Zhen Xiao, Hemin Yang, Zirun Zhu, Min Tang, Xu Tan, Yanqing Liu, Sheng Zhao, Naoyuki Kanda

**Updated**: 2024-09-12T17:45:37Z

**Summary**: This paper introduces Embarrassingly Easy Text-to-Speech (E2 TTS), a fully non-autoregressive zero-shot text-to-speech system that offers human-level naturalness and state-of-the-art speaker similarity and intelligibility. In the E2 TTS framework, the text input is converted into a character sequence with filler tokens. The flow-matching-based mel spectrogram generator is then trained based on the audio infilling task. Unlike many previous works, it does not require additional components (e.g., duration model, grapheme-to-phoneme) or complex techniques (e.g., monotonic alignment search). Despite its simplicity, E2 TTS achieves state-of-the-art zero-shot TTS capabilities that are comparable to or surpass previous works, including Voicebox and NaturalSpeech 3. The simplicity of E2 TTS also allows for flexibility in the input representation. We propose several variants of E2 TTS to improve usability during inference. See https://aka.ms/e2tts/ for demo samples.

**Link**: [arxiv](http://arxiv.org/abs/2406.18009v2),  [pdf](http://arxiv.org/pdf/2406.18009v2)

**Tags**: eess.AS cs.SD 



### Reasoning Around Paradox with Grounded Deduction
**Authors**: Bryan Ford

**Updated**: 2024-09-12T17:43:27Z

**Summary**: How can we reason around logical paradoxes without falling into them? This paper introduces grounded deduction or GD, a Kripke-inspired approach to first-order logic and arithmetic that is neither classical nor intuitionistic, but nevertheless appears both pragmatically usable and intuitively justifiable. GD permits the direct expression of unrestricted recursive definitions - including paradoxical ones such as 'L := not L' - while adding dynamic typing premises to certain inference rules so that such paradoxes do not lead to inconsistency. This paper constitutes a preliminary development and investigation of grounded deduction, to be extended with further elaboration and deeper analysis of its intriguing properties.

**Link**: [arxiv](http://arxiv.org/abs/2409.08243v1),  [pdf](http://arxiv.org/pdf/2409.08243v1)

**Tags**: math.LO cs.LO 03B60 F.4.1 



### Source2Synth: Synthetic Data Generation and Curation Grounded in Real   Data Sources
**Authors**: Alisia Lupidi, Carlos Gemmell, Nicola Cancedda, Jane Dwivedi-Yu, Jason Weston, Jakob Foerster, Roberta Raileanu, Maria Lomeli

**Updated**: 2024-09-12T17:39:08Z

**Summary**: Large Language Models still struggle in challenging scenarios that leverage structured data, complex reasoning, or tool usage. In this paper, we propose Source2Synth: a new method that can be used for teaching LLMs new skills without relying on costly human annotations. Source2Synth takes as input a custom data source and produces synthetic data points with intermediate reasoning steps grounded in real-world sources. Source2Synth improves the dataset quality by discarding low-quality generations based on their answerability. We demonstrate the generality of this approach by applying it to two challenging domains: we test reasoning abilities in multi-hop question answering (MHQA), and tool usage in tabular question answering (TQA). Our method improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on HotPotQA compared to the fine-tuned baselines.

**Link**: [arxiv](http://arxiv.org/abs/2409.08239v1),  [pdf](http://arxiv.org/pdf/2409.08239v1)

**Tags**: cs.CL cs.AI 



### Tracking Network Dynamics using Probabilistic State-Space Models
**Authors**: Victor M. Tenorio, Elvin Isufi, Geert Leus, Antonio G. Marques

**Updated**: 2024-09-12T17:38:11Z

**Summary**: This paper introduces a probabilistic approach for tracking the dynamics of unweighted and directed graphs using state-space models (SSMs). Unlike conventional topology inference methods that assume static graphs and generate point-wise estimates, our method accounts for dynamic changes in the network structure over time. We model the network at each timestep as the state of the SSM, and use observations to update beliefs that quantify the probability of the network being in a particular state. Then, by considering the dynamics of transition and observation models through the update and prediction steps, respectively, the proposed method can incorporate the information of real-time graph signals into the beliefs. These beliefs provide a probability distribution of the network at each timestep, being able to provide both an estimate for the network and the uncertainty it entails. Our approach is evaluated through experiments with synthetic and real-world networks. The results demonstrate that our method effectively estimates network states and accounts for the uncertainty in the data, outperforming traditional techniques such as recursive least squares.

**Link**: [arxiv](http://arxiv.org/abs/2409.08238v1),  [pdf](http://arxiv.org/pdf/2409.08238v1)

**Tags**: eess.SP 



### Validating full-spectrum fitting with a synthetic integral-field   spectroscopic observation of the Milky Way
**Authors**: Zixian Wang, Michael R. Hayden, Sanjib Sharma, Jesse van de Sande, Joss Bland-Hawthorn, Sam Vaughan, Marie Martig, Francesca Pinna

**Updated**: 2024-09-12T17:38:03Z

**Summary**: Ongoing deep IFS observations of disk galaxies provide opportunities for comparison with the Milky Way (MW) to understand galaxy evolution. However, such comparisons are marred by many challenges such as selection effects, differences in observations and methodology, and proper validation of full-spectrum fitting methods. In this study, we present a novel code GalCraft to address these challenges by generating mock IFS data cubes of the MW using simple stellar population models and a mock MW stellar catalog derived from E-Galaxia. We use the widely adopted full-spectrum fitting code pPXF to investigate the ability to recover kinematics and stellar populations for an edge-on mock MW IFS observation. We confirm that differences in kinematics, mean age, [M/H], and [$\alpha$/Fe] between thin and thick disks can be distinguished. However, the age distribution is overestimated in the ranges between 2 - 4 and 12 - 14 Gyr compared to the expected values. This is likely due to the age spacing and degeneracy of SSP templates. We find systematic offsets in the recovered kinematics due to insufficient spectral resolution and the variation of line-of-sight velocity distribution with age and [M/H]. With future higher resolution and multi-[$\alpha$/Fe] SSP templates, GalCraft will be useful to validate key signatures such as [$\alpha$/Fe]-[M/H] distribution at different $R$ and $|z|$ and potentially infer radial migration and kinematic heating efficiency to study detailed chemodynamical evolution of MW-like galaxies.

**Link**: [arxiv](http://arxiv.org/abs/2310.18258v2),  [pdf](http://arxiv.org/pdf/2310.18258v2)

**Tags**: astro-ph.GA astro-ph.IM 



### LLM Honeypot: Leveraging Large Language Models as Advanced Interactive   Honeypot Systems
**Authors**: Hakan T. Otal, M. Abdullah Canbaz

**Updated**: 2024-09-12T17:33:06Z

**Summary**: The rapid evolution of cyber threats necessitates innovative solutions for detecting and analyzing malicious activity. Honeypots, which are decoy systems designed to lure and interact with attackers, have emerged as a critical component in cybersecurity. In this paper, we present a novel approach to creating realistic and interactive honeypot systems using Large Language Models (LLMs). By fine-tuning a pre-trained open-source language model on a diverse dataset of attacker-generated commands and responses, we developed a honeypot capable of sophisticated engagement with attackers. Our methodology involved several key steps: data collection and processing, prompt engineering, model selection, and supervised fine-tuning to optimize the model's performance. Evaluation through similarity metrics and live deployment demonstrated that our approach effectively generates accurate and informative responses. The results highlight the potential of LLMs to revolutionize honeypot technology, providing cybersecurity professionals with a powerful tool to detect and analyze malicious activity, thereby enhancing overall security infrastructure.

**Link**: [arxiv](http://arxiv.org/abs/2409.08234v1),  [pdf](http://arxiv.org/pdf/2409.08234v1)

**Tags**: cs.CR cs.AI cs.CL cs.LG cs.NI 68T50, 68M10 I.2.7; D.4.6; K.6.5 



### Private Private Information
**Authors**: Kevin He, Fedor Sandomirskiy, Omer Tamuz

**Updated**: 2024-09-12T16:56:19Z

**Summary**: Private signals model noisy information about an unknown state. Although these signals are called "private," they may still carry information about each other. Our paper introduces the concept of private private signals, which contain information about the state but not about other signals. To achieve privacy, signal quality may need to be sacrificed. We study the informativeness of private private signals and characterize those that are optimal in the sense that they cannot be made more informative without violating privacy. We discuss implications for privacy in recommendation systems, information design, causal inference, and mechanism design.

**Link**: [arxiv](http://arxiv.org/abs/2112.14356v4),  [pdf](http://arxiv.org/pdf/2112.14356v4)

**Tags**: econ.TH cs.GT math.PR 



### Adaptive Language-Guided Abstraction from Contrastive Explanations
**Authors**: Andi Peng, Belinda Z. Li, Ilia Sucholutsky, Nishanth Kumar, Julie A. Shah, Jacob Andreas, Andreea Bobu

**Updated**: 2024-09-12T16:51:58Z

**Summary**: Many approaches to robot learning begin by inferring a reward function from a set of human demonstrations. To learn a good reward, it is necessary to determine which features of the environment are relevant before determining how these features should be used to compute reward. End-to-end methods for joint feature and reward learning (e.g., using deep networks or program synthesis techniques) often yield brittle reward functions that are sensitive to spurious state features. By contrast, humans can often generalizably learn from a small number of demonstrations by incorporating strong priors about what features of a demonstration are likely meaningful for a task of interest. How do we build robots that leverage this kind of background knowledge when learning from new demonstrations? This paper describes a method named ALGAE (Adaptive Language-Guided Abstraction from [Contrastive] Explanations) which alternates between using language models to iteratively identify human-meaningful features needed to explain demonstrated behavior, then standard inverse reinforcement learning techniques to assign weights to these features. Experiments across a variety of both simulated and real-world robot environments show that ALGAE learns generalizable reward functions defined on interpretable features using only small numbers of demonstrations. Importantly, ALGAE can recognize when features are missing, then extract and define those features without any human input -- making it possible to quickly and efficiently acquire rich representations of user behavior.

**Link**: [arxiv](http://arxiv.org/abs/2409.08212v1),  [pdf](http://arxiv.org/pdf/2409.08212v1)

**Tags**: cs.RO cs.LG 



### Understanding the concerns and choices of public when using large   language models for healthcare
**Authors**: Yunpeng Xiao, Kyrie Zhixuan Zhou, Yueqing Liang, Kai Shu

**Updated**: 2024-09-12T16:40:18Z

**Summary**: Large language models (LLMs) have shown their potential in biomedical fields. However, how the public uses them for healthcare purposes such as medical Q\&A, self-diagnosis, and daily healthcare information seeking is under-investigated. This paper adopts a mixed-methods approach, including surveys (N=214) and interviews (N=17) to investigate how and why the public uses LLMs for healthcare. We found that participants generally believed LLMs as a healthcare tool have gained popularity, and are often used in combination with other information channels such as search engines and online health communities to optimize information quality. Based on the findings, we reflect on the ethical and effective use of LLMs for healthcare and propose future research directions.

**Link**: [arxiv](http://arxiv.org/abs/2401.09090v2),  [pdf](http://arxiv.org/pdf/2401.09090v2)

**Tags**: cs.CY J.4; K.4.2 



### Quantifying the breakdown scale of pionless effective field theory
**Authors**: Andreas Ekström, Lucas Platter

**Updated**: 2024-09-12T16:35:57Z

**Summary**: We use Bayesian statistics to infer the breakdown scale of pionless effective field theory in its standard power counting and with renormalization of observables carried out using the power-divergence subtraction scheme and cutoff regularization. We condition our inference on predictions of the total neutron-proton scattering cross section up next-to-next-to leading order. We quantify a median breakdown scale of approximately 1.4$m_\pi$. The 68% degree of belief interval is $[0.96,1.69]m_\pi$. This result confirms the canonical expectation that the pion mass is a relevant scale in low-energy nuclear physics.

**Link**: [arxiv](http://arxiv.org/abs/2409.08197v1),  [pdf](http://arxiv.org/pdf/2409.08197v1)

**Tags**: nucl-th hep-ph 



### Trading Devil Final: Backdoor attack via Stock market and Bayesian   Optimization
**Authors**: Orson Mengara

**Updated**: 2024-09-12T16:22:52Z

**Summary**: Since the advent of generative artificial intelligence, every company and researcher has been rushing to develop their own generative models, whether commercial or not. Given the large number of users of these powerful new tools, there is currently no intrinsically verifiable way to explain from the ground up what happens when LLMs (large language models) learn. For example, those based on automatic speech recognition systems, which have to rely on huge and astronomical amounts of data collected from all over the web to produce fast and efficient results, In this article, we develop a backdoor attack called MarketBackFinal 2.0, based on acoustic data poisoning, MarketBackFinal 2.0 is mainly based on modern stock market models. In order to show the possible vulnerabilities of speech-based transformers that may rely on LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2407.14573v5),  [pdf](http://arxiv.org/pdf/2407.14573v5)

**Tags**: cs.LG cs.CR q-fin.CP q-fin.PR q-fin.ST 



### Fine-tuning Large Language Models for Entity Matching
**Authors**: Aaron Steiner, Ralph Peeters, Christian Bizer

**Updated**: 2024-09-12T16:20:57Z

**Summary**: Generative large language models (LLMs) are a promising alternative to pre-trained language models for entity matching due to their high zero-shot performance and their ability to generalize to unseen entities. Existing research on using LLMs for entity matching has focused on prompt engineering and in-context learning. This paper explores the potential of fine-tuning LLMs for entity matching. We analyze fine-tuning along two dimensions: 1) The representation of training examples, where we experiment with adding different types of LLM-generated explanations to the training set, and 2) the selection and generation of training examples using LLMs. In addition to the matching performance on the source dataset, we investigate how fine-tuning affects the model's ability to generalize to other in-domain datasets as well as across topical domains. Our experiments show that fine-tuning significantly improves the performance of the smaller models while the results for the larger models are mixed. Fine-tuning also improves the generalization to in-domain datasets while hurting cross-domain transfer. We show that adding structured explanations to the training set has a positive impact on the performance of three out of four LLMs, while the proposed example selection and generation methods only improve the performance of Llama 3.1 8B while decreasing the performance of GPT-4o Mini.

**Link**: [arxiv](http://arxiv.org/abs/2409.08185v1),  [pdf](http://arxiv.org/pdf/2409.08185v1)

**Tags**: cs.CL cs.AI cs.LG 68T50 I.2.7 



### Faster Speech-LLaMA Inference with Multi-token Prediction
**Authors**: Desh Raj, Gil Keren, Junteng Jia, Jay Mahadeokar, Ozlem Kalinli

**Updated**: 2024-09-12T15:43:10Z

**Summary**: Large language models (LLMs) have become proficient at solving a wide variety of tasks, including those involving multi-modal inputs. In particular, instantiating an LLM (such as LLaMA) with a speech encoder and training it on paired data imparts speech recognition (ASR) abilities to the decoder-only model, hence called Speech-LLaMA. Nevertheless, due to the sequential nature of auto-regressive inference and the relatively large decoder, Speech-LLaMA models require relatively high inference time. In this work, we propose to speed up Speech-LLaMA inference by predicting multiple tokens in the same decoding step. We explore several model architectures that enable this, and investigate their performance using threshold-based and verification-based inference strategies. We also propose a prefix-based beam search decoding method that allows efficient minimum word error rate (MWER) training for such models. We evaluate our models on a variety of public benchmarks, where they reduce the number of decoder calls by ~3.2x while maintaining or improving WER performance.

**Link**: [arxiv](http://arxiv.org/abs/2409.08148v1),  [pdf](http://arxiv.org/pdf/2409.08148v1)

**Tags**: eess.AS cs.SD 



### LLM-POTUS Score: A Framework of Analyzing Presidential Debates with   Large Language Models
**Authors**: Zhengliang Liu, Yiwei Li, Oleksandra Zolotarevych, Rongwei Yang, Tianming Liu

**Updated**: 2024-09-12T15:40:45Z

**Summary**: Large language models have demonstrated remarkable capabilities in natural language processing, yet their application to political discourse analysis remains underexplored. This paper introduces a novel approach to evaluating presidential debate performances using LLMs, addressing the longstanding challenge of objectively assessing debate outcomes. We propose a framework that analyzes candidates' "Policies, Persona, and Perspective" (3P) and how they resonate with the "Interests, Ideologies, and Identity" (3I) of four key audience groups: voters, businesses, donors, and politicians. Our method employs large language models to generate the LLM-POTUS Score, a quantitative measure of debate performance based on the alignment between 3P and 3I. We apply this framework to analyze transcripts from recent U.S. presidential debates, demonstrating its ability to provide nuanced, multi-dimensional assessments of candidate performances. Our results reveal insights into the effectiveness of different debating strategies and their impact on various audience segments. This study not only offers a new tool for political analysis but also explores the potential and limitations of using LLMs as impartial judges in complex social contexts. In addition, this framework provides individual citizens with an independent tool to evaluate presidential debate performances, which enhances democratic engagement and reduces reliance on potentially biased media interpretations and institutional influence, thereby strengthening the foundation of informed civic participation.

**Link**: [arxiv](http://arxiv.org/abs/2409.08147v1),  [pdf](http://arxiv.org/pdf/2409.08147v1)

**Tags**: cs.CL 



### Gravity.jl: fast and accurate gravitational lens modeling in Julia
**Authors**: Marco Lombardi

**Updated**: 2024-09-12T15:13:55Z

**Summary**: We present Gravity.jl, a new software for the modeling of gravitational lens systems. Gravity.jl is written in the Julia programming language, and is designed to be fast, accurate, and flexible. It can be used to model gravitational lens systems composed of multiple lensing planes, and to perform Bayesian inference on the lens model parameters. In this paper we present the theoretical and statistical ideas behind the code, and we describe its main features. In this first paper of the series, we focus on the modeling of point-like and small extended sources, for which we can linearize the lens equation. We show a practical use of Gravity.jl on a galaxy-scale lens, and we compare the results with those obtained with other codes. We also show how Gravity.jl can be used to perform Bayesian inference on cosmological parameters.

**Link**: [arxiv](http://arxiv.org/abs/2406.15280v2),  [pdf](http://arxiv.org/pdf/2406.15280v2)

**Tags**: astro-ph.IM 



### GAZEploit: Remote Keystroke Inference Attack by Gaze Estimation from   Avatar Views in VR/MR Devices
**Authors**: Hanqiu Wang, Zihao Zhan, Haoqi Shan, Siqi Dai, Max Panoff, Shuo Wang

**Updated**: 2024-09-12T15:11:35Z

**Summary**: The advent and growing popularity of Virtual Reality (VR) and Mixed Reality (MR) solutions have revolutionized the way we interact with digital platforms. The cutting-edge gaze-controlled typing methods, now prevalent in high-end models of these devices, e.g., Apple Vision Pro, have not only improved user experience but also mitigated traditional keystroke inference attacks that relied on hand gestures, head movements and acoustic side-channels. However, this advancement has paradoxically given birth to a new, potentially more insidious cyber threat, GAZEploit.   In this paper, we unveil GAZEploit, a novel eye-tracking based attack specifically designed to exploit these eye-tracking information by leveraging the common use of virtual appearances in VR applications. This widespread usage significantly enhances the practicality and feasibility of our attack compared to existing methods. GAZEploit takes advantage of this vulnerability to remotely extract gaze estimations and steal sensitive keystroke information across various typing scenarios-including messages, passwords, URLs, emails, and passcodes. Our research, involving 30 participants, achieved over 80% accuracy in keystroke inference. Alarmingly, our study also identified over 15 top-rated apps in the Apple Store as vulnerable to the GAZEploit attack, emphasizing the urgent need for bolstered security measures for this state-of-the-art VR/MR text entry method.

**Link**: [arxiv](http://arxiv.org/abs/2409.08122v1),  [pdf](http://arxiv.org/pdf/2409.08122v1)

**Tags**: cs.HC cs.CV 



### JWST ice band profiles reveal mixed ice compositions in the HH 48 NE   disk
**Authors**: Jennifer B. Bergner, J. A. Sturm, Elettra L. Piacentino, M. K. McClure, Karin I. Oberg, A. C. A. Boogert, E. Dartois, M. N. Drozdovskaya, H. J. Fraser, Daniel Harsono, Sergio Ioppolo, Charles J. Law, Dariusz C. Lis, Brett A. McGuire, Gary J. Melnick, Jennifer A. Noble, M. E. Palumbo, Yvonne J. Pendleton, Giulia Perotti, Danna Qasim, W. R. M. Rocha, E. F. van Dishoeck

**Updated**: 2024-09-12T15:08:15Z

**Summary**: Planet formation is strongly influenced by the composition and distribution of volatiles within protoplanetary disks. With JWST, it is now possible to obtain direct observational constraints on disk ices, as recently demonstrated by the detection of ice absorption features towards the edge-on HH 48 NE disk as part of the Ice Age Early Release Science program. Here, we introduce a new radiative transfer modeling framework designed to retrieve the composition and mixing status of disk ices using their band profiles, and apply it to interpret the H2O, CO2, and CO ice bands observed towards the HH 48 NE disk. We show that the ices are largely present as mixtures, with strong evidence for CO trapping in both H2O and CO2 ice. The HH 48 NE disk ice composition (pure vs. polar vs. apolar fractions) is markedly different from earlier protostellar stages, implying thermal and/or chemical reprocessing during the formation or evolution of the disk. We infer low ice-phase C/O ratios around 0.1 throughout the disk, and also demonstrate that the mixing and entrapment of disk ices can dramatically affect the radial dependence of the C/O ratio. It is therefore imperative that realistic disk ice compositions are considered when comparing planetary compositions with potential formation scenarios, which will fortunately be possible for an increasing number of disks with JWST.

**Link**: [arxiv](http://arxiv.org/abs/2409.08117v1),  [pdf](http://arxiv.org/pdf/2409.08117v1)

**Tags**: astro-ph.EP astro-ph.IM 



### What is the Role of Small Models in the LLM Era: A Survey
**Authors**: Lihu Chen, Gaël Varoquaux

**Updated**: 2024-09-12T15:04:57Z

**Summary**: Large Language Models (LLMs) have made significant progress in advancing artificial general intelligence (AGI), leading to the development of increasingly large models such as GPT-4 and LLaMA-405B. However, scaling up model sizes results in exponentially higher computational costs and energy consumption, making these models impractical for academic researchers and businesses with limited resources. At the same time, Small Models (SMs) are frequently used in practical settings, although their significance is currently underestimated. This raises important questions about the role of small models in the era of LLMs, a topic that has received limited attention in prior research. In this work, we systematically examine the relationship between LLMs and SMs from two key perspectives: Collaboration and Competition. We hope this survey provides valuable insights for practitioners, fostering a deeper understanding of the contribution of small models and promoting more efficient use of computational resources. The code is available at https://github.com/tigerchen52/role_of_small_models

**Link**: [arxiv](http://arxiv.org/abs/2409.06857v2),  [pdf](http://arxiv.org/pdf/2409.06857v2)

**Tags**: cs.CL 



### WhisperNER: Unified Open Named Entity and Speech Recognition
**Authors**: Gil Ayache, Menachem Pirchi, Aviv Navon, Aviv Shamsian, Gill Hetz, Joseph Keshet

**Updated**: 2024-09-12T15:00:56Z

**Summary**: Integrating named entity recognition (NER) with automatic speech recognition (ASR) can significantly enhance transcription accuracy and informativeness. In this paper, we introduce WhisperNER, a novel model that allows joint speech transcription and entity recognition. WhisperNER supports open-type NER, enabling recognition of diverse and evolving entities at inference. Building on recent advancements in open NER research, we augment a large synthetic dataset with synthetic speech samples. This allows us to train WhisperNER on a large number of examples with diverse NER tags. During training, the model is prompted with NER labels and optimized to output the transcribed utterance along with the corresponding tagged entities. To evaluate WhisperNER, we generate synthetic speech for commonly used NER benchmarks and annotate existing ASR datasets with open NER tags. Our experiments demonstrate that WhisperNER outperforms natural baselines on both out-of-domain open type NER and supervised finetuning.

**Link**: [arxiv](http://arxiv.org/abs/2409.08107v1),  [pdf](http://arxiv.org/pdf/2409.08107v1)

**Tags**: cs.CL cs.LG 



### Large Language Models and Cognitive Science: A Comprehensive Review of   Similarities, Differences, and Challenges
**Authors**: Qian Niu, Junyu Liu, Ziqian Bi, Pohsun Feng, Benji Peng, Keyu Chen, Ming Li

**Updated**: 2024-09-12T14:56:35Z

**Summary**: This comprehensive review explores the intersection of Large Language Models (LLMs) and cognitive science, examining similarities and differences between LLMs and human cognitive processes. We analyze methods for evaluating LLMs cognitive abilities and discuss their potential as cognitive models. The review covers applications of LLMs in various cognitive fields, highlighting insights gained for cognitive science research. We assess cognitive biases and limitations of LLMs, along with proposed methods for improving their performance. The integration of LLMs with cognitive architectures is examined, revealing promising avenues for enhancing artificial intelligence (AI) capabilities. Key challenges and future research directions are identified, emphasizing the need for continued refinement of LLMs to better align with human cognition. This review provides a balanced perspective on the current state and future potential of LLMs in advancing our understanding of both artificial and human intelligence.

**Link**: [arxiv](http://arxiv.org/abs/2409.02387v3),  [pdf](http://arxiv.org/pdf/2409.02387v3)

**Tags**: cs.AI cs.CL 



### Bayesian Self-Training for Semi-Supervised 3D Segmentation
**Authors**: Ozan Unal, Christos Sakaridis, Luc Van Gool

**Updated**: 2024-09-12T14:54:31Z

**Summary**: 3D segmentation is a core problem in computer vision and, similarly to many other dense prediction tasks, it requires large amounts of annotated data for adequate training. However, densely labeling 3D point clouds to employ fully-supervised training remains too labor intensive and expensive. Semi-supervised training provides a more practical alternative, where only a small set of labeled data is given, accompanied by a larger unlabeled set. This area thus studies the effective use of unlabeled data to reduce the performance gap that arises due to the lack of annotations. In this work, inspired by Bayesian deep learning, we first propose a Bayesian self-training framework for semi-supervised 3D semantic segmentation. Employing stochastic inference, we generate an initial set of pseudo-labels and then filter these based on estimated point-wise uncertainty. By constructing a heuristic $n$-partite matching algorithm, we extend the method to semi-supervised 3D instance segmentation, and finally, with the same building blocks, to dense 3D visual grounding. We demonstrate state-of-the-art results for our semi-supervised method on SemanticKITTI and ScribbleKITTI for 3D semantic segmentation and on ScanNet and S3DIS for 3D instance segmentation. We further achieve substantial improvements in dense 3D visual grounding over supervised-only baselines on ScanRefer. Our project page is available at ouenal.github.io/bst/.

**Link**: [arxiv](http://arxiv.org/abs/2409.08102v1),  [pdf](http://arxiv.org/pdf/2409.08102v1)

**Tags**: cs.CV 



### The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK   Employment Tribunal
**Authors**: Huiyuan Xie, Felix Steffek, Joana Ribeiro de Faria, Christine Carter, Jonathan Rutherford

**Updated**: 2024-09-12T14:51:43Z

**Summary**: This paper explores the intersection of technological innovation and access to justice by developing a benchmark for predicting case outcomes in the UK Employment Tribunal (UKET). To address the challenge of extensive manual annotation, the study employs a large language model (LLM) for automatic annotation, resulting in the creation of the CLC-UKET dataset. The dataset consists of approximately 19,000 UKET cases and their metadata. Comprehensive legal annotations cover facts, claims, precedent references, statutory references, case outcomes, reasons and jurisdiction codes. Facilitated by the CLC-UKET data, we examine a multi-class case outcome prediction task in the UKET. Human predictions are collected to establish a performance reference for model comparison. Empirical results from baseline models indicate that finetuned transformer models outperform zero-shot and few-shot LLMs on the UKET prediction task. The performance of zero-shot LLMs can be enhanced by integrating task-related information into few-shot examples. We hope that the CLC-UKET dataset, along with human annotations and empirical findings, can serve as a valuable benchmark for employment-related dispute resolution.

**Link**: [arxiv](http://arxiv.org/abs/2409.08098v1),  [pdf](http://arxiv.org/pdf/2409.08098v1)

**Tags**: cs.CL cs.AI 



### Securing Large Language Models: Addressing Bias, Misinformation, and   Prompt Attacks
**Authors**: Benji Peng, Keyu Chen, Ming Li, Pohsun Feng, Ziqian Bi, Junyu Liu, Qian Niu

**Updated**: 2024-09-12T14:42:08Z

**Summary**: Large Language Models (LLMs) demonstrate impressive capabilities across various fields, yet their increasing use raises critical security concerns. This article reviews recent literature addressing key issues in LLM security, with a focus on accuracy, bias, content detection, and vulnerability to attacks. Issues related to inaccurate or misleading outputs from LLMs is discussed, with emphasis on the implementation from fact-checking methodologies to enhance response reliability. Inherent biases within LLMs are critically examined through diverse evaluation techniques, including controlled input studies and red teaming exercises. A comprehensive analysis of bias mitigation strategies is presented, including approaches from pre-processing interventions to in-training adjustments and post-processing refinements. The article also probes the complexity of distinguishing LLM-generated content from human-produced text, introducing detection mechanisms like DetectGPT and watermarking techniques while noting the limitations of machine learning enabled classifiers under intricate circumstances. Moreover, LLM vulnerabilities, including jailbreak attacks and prompt injection exploits, are analyzed by looking into different case studies and large-scale competitions like HackAPrompt. This review is concluded by retrospecting defense mechanisms to safeguard LLMs, accentuating the need for more extensive research into the LLM security field.

**Link**: [arxiv](http://arxiv.org/abs/2409.08087v1),  [pdf](http://arxiv.org/pdf/2409.08087v1)

**Tags**: cs.CR 



### TravelAgent: An AI Assistant for Personalized Travel Planning
**Authors**: Aili Chen, Xuyang Ge, Ziquan Fu, Yanghua Xiao, Jiangjie Chen

**Updated**: 2024-09-12T14:24:45Z

**Summary**: As global tourism expands and artificial intelligence technology advances, intelligent travel planning services have emerged as a significant research focus. Within dynamic real-world travel scenarios with multi-dimensional constraints, services that support users in automatically creating practical and customized travel itineraries must address three key objectives: Rationality, Comprehensiveness, and Personalization. However, existing systems with rule-based combinations or LLM-based planning methods struggle to fully satisfy these criteria. To overcome the challenges, we introduce TravelAgent, a travel planning system powered by large language models (LLMs) designed to provide reasonable, comprehensive, and personalized travel itineraries grounded in dynamic scenarios. TravelAgent comprises four modules: Tool-usage, Recommendation, Planning, and Memory Module. We evaluate TravelAgent's performance with human and simulated users, demonstrating its overall effectiveness in three criteria and confirming the accuracy of personalized recommendations.

**Link**: [arxiv](http://arxiv.org/abs/2409.08069v1),  [pdf](http://arxiv.org/pdf/2409.08069v1)

**Tags**: cs.AI cs.CL 



### NITRO-D: Native Integer-only Training of Deep Convolutional Neural   Networks
**Authors**: Alberto Pirillo, Luca Colombo, Manuel Roveri

**Updated**: 2024-09-12T14:18:22Z

**Summary**: Quantization has become increasingly pivotal in addressing the steadily increasing computational and memory requirements of Deep Neural Networks (DNNs). By reducing the number of bits used to represent weights and activations (typically from 32-bit floating-point to 16-bit or 8-bit integers), quantization reduces the memory footprint, energy consumption, and execution time of DNN models. However, traditional quantization methods typically focus on the inference of DNNs, while the training process still relies on floating-point operations. To date, only one work in the literature has addressed integer-only training for Multi-Layer Perceptron (MLP) architectures. This work introduces NITRO-D, a new framework for training arbitrarily deep integer-only Convolutional Neural Networks (CNNs) that operate entirely in the integer-only domain for both training and inference. NITRO-D is the first framework in the literature enabling the training of integer-only CNNs without the need to introduce a quantization scheme. Specifically, NITRO-D introduces a novel architecture integrating multiple integer local-loss blocks, which include the proposed NITRO Scaling Layer and the NITRO-ReLU activation function. Additionally, it introduces a novel integer-only learning algorithm derived from Local Error Signals (LES), utilizing IntegerSGD, an optimizer specifically designed to operate in an integer-only context. NITRO-D is implemented in an open-source Python library. Extensive experimental evaluations demonstrate its effectiveness across several state-of-the-art image recognition datasets. Results show significant performance improvements from 2.47% to 5.96% for integer-only MLP architectures over the state-of-the-art solution, and the capability of training integer-only CNN architectures with minimal accuracy degradation from -0.15% to -4.22% compared to floating-point LES.

**Link**: [arxiv](http://arxiv.org/abs/2407.11698v2),  [pdf](http://arxiv.org/pdf/2407.11698v2)

**Tags**: cs.LG cs.AI cs.CV cs.NE I.2.6 



### HD 222237 b: a long period super-Jupiter around a nearby star revealed   by radial-velocity and Hipparcos-Gaia astrometry
**Authors**: Guang-Yao Xiao, Fabo Feng, Stephen A. Shectman, C. G. Tinney, Johanna K. Teske, B. D. Carter, H. R. A. Jones, Robert A. Wittenmyer, Matías R. Díaz, Jeffrey D. Crane, Sharon X. Wang, J. Bailey, S. J. O'Toole, Adina D. Feinstein, Malena Rice, Zahra Essack, Benjamin T. Montet, Avi Shporer, R. Paul Butler

**Updated**: 2024-09-12T14:17:40Z

**Summary**: Giant planets on long period orbits around the nearest stars are among the easiest to directly image. Unfortunately these planets are difficult to fully constrain by indirect methods, e.g., transit and radial velocity (RV). In this study, we present the discovery of a super-Jupiter, HD 222237 b, orbiting a star located $11.445\pm0.002$ pc away. By combining RV data, Hipparcos and multi-epoch Gaia astrometry, we estimate the planetary mass to be ${5.19}_{-0.58}^{+0.58}\,M_{\rm Jup}$, with an eccentricity of ${0.56}_{-0.03}^{+0.03}$ and a period of ${40.8}_{-4.5}^{+5.8}$ yr, making HD 222237 b a promising target for imaging using the Mid-Infrared Instrument (MIRI) of JWST. A comparative analysis suggests that our method can break the inclination degeneracy and thus differentiate between prograde and retrograde orbits of a companion. We further find that the inferred contrast ratio between the planet and the host star in the F1550C filter ($15.50\,\mu \rm m$) is approximately $1.9\times10^{-4}$, which is comparable with the measured limit of the MIRI coronagraphs. The relatively low metallicity of the host star ($\rm-0.32\,dex$) combined with the unique orbital architecture of this system presents an excellent opportunity to probe the planet-metallicity correlation and the formation scenarios of giant planets.

**Link**: [arxiv](http://arxiv.org/abs/2409.08067v1),  [pdf](http://arxiv.org/pdf/2409.08067v1)

**Tags**: astro-ph.EP 



### Causal inference and racial bias in policing: New estimands and the   importance of mobility data
**Authors**: Zhuochao Huang, Brenden Beck, Joseph Antonelli

**Updated**: 2024-09-12T14:08:10Z

**Summary**: Studying racial bias in policing is a critically important problem, but one that comes with a number of inherent difficulties due to the nature of the available data. In this manuscript we tackle multiple key issues in the causal analysis of racial bias in policing. First, we formalize race and place policing, the idea that individuals of one race are policed differently when they are in neighborhoods primarily made up of individuals of other races. We develop an estimand to study this question rigorously, show the assumptions necessary for causal identification, and develop sensitivity analyses to assess robustness to violations of key assumptions. Additionally, we investigate difficulties with existing estimands targeting racial bias in policing. We show for these estimands, and the estimands developed in this manuscript, that estimation can benefit from incorporating mobility data into analyses. We apply these ideas to a study in New York City, where we find a large amount of racial bias, as well as race and place policing, and that these findings are robust to large violations of untestable assumptions. We additionally show that mobility data can make substantial impacts on the resulting estimates, suggesting it should be used whenever possible in subsequent studies.

**Link**: [arxiv](http://arxiv.org/abs/2409.08059v1),  [pdf](http://arxiv.org/pdf/2409.08059v1)

**Tags**: stat.AP stat.ME 



### Noiseless Privacy-Preserving Decentralized Learning
**Authors**: Sayan Biswas, Mathieu Even, Anne-Marie Kermarrec, Laurent Massoulie, Rafael Pires, Rishi Sharma, Martijn de Vos

**Updated**: 2024-09-12T13:53:31Z

**Summary**: Decentralized learning (DL) enables collaborative learning without a server and without training data leaving the users' devices. However, the models shared in DL can still be used to infer training data. Conventional defenses such as differential privacy and secure aggregation fall short in effectively safeguarding user privacy in DL, either sacrificing model utility or efficiency. We introduce Shatter, a novel DL approach in which nodes create virtual nodes (VNs) to disseminate chunks of their full model on their behalf. This enhances privacy by (i) preventing attackers from collecting full models from other nodes, and (ii) hiding the identity of the original node that produced a given model chunk. We theoretically prove the convergence of Shatter and provide a formal analysis demonstrating how Shatter reduces the efficacy of attacks compared to when exchanging full models between nodes. We evaluate the convergence and attack resilience of Shatter with existing DL algorithms, with heterogeneous datasets, and against three standard privacy attacks. Our evaluation shows that Shatter not only renders these privacy attacks infeasible when each node operates 16 VNs but also exhibits a positive impact on model utility compared to standard DL. In summary, Shatter enhances the privacy of DL while maintaining the utility and efficiency of the model.

**Link**: [arxiv](http://arxiv.org/abs/2404.09536v2),  [pdf](http://arxiv.org/pdf/2404.09536v2)

**Tags**: cs.DC cs.AI cs.CR cs.LG 



### Unleashing Worms and Extracting Data: Escalating the Outcome of Attacks   against RAG-based Inference in Scale and Severity Using Jailbreaking
**Authors**: Stav Cohen, Ron Bitton, Ben Nassi

**Updated**: 2024-09-12T13:50:22Z

**Summary**: In this paper, we show that with the ability to jailbreak a GenAI model, attackers can escalate the outcome of attacks against RAG-based GenAI-powered applications in severity and scale. In the first part of the paper, we show that attackers can escalate RAG membership inference attacks and RAG entity extraction attacks to RAG documents extraction attacks, forcing a more severe outcome compared to existing attacks. We evaluate the results obtained from three extraction methods, the influence of the type and the size of five embeddings algorithms employed, the size of the provided context, and the GenAI engine. We show that attackers can extract 80%-99.8% of the data stored in the database used by the RAG of a Q&A chatbot. In the second part of the paper, we show that attackers can escalate the scale of RAG data poisoning attacks from compromising a single GenAI-powered application to compromising the entire GenAI ecosystem, forcing a greater scale of damage. This is done by crafting an adversarial self-replicating prompt that triggers a chain reaction of a computer worm within the ecosystem and forces each affected application to perform a malicious activity and compromise the RAG of additional applications. We evaluate the performance of the worm in creating a chain of confidential data extraction about users within a GenAI ecosystem of GenAI-powered email assistants and analyze how the performance of the worm is affected by the size of the context, the adversarial self-replicating prompt used, the type and size of the embeddings algorithm employed, and the number of hops in the propagation. Finally, we review and analyze guardrails to protect RAG-based inference and discuss the tradeoffs.

**Link**: [arxiv](http://arxiv.org/abs/2409.08045v1),  [pdf](http://arxiv.org/pdf/2409.08045v1)

**Tags**: cs.CR cs.AI 



### MAPF-GPT: Imitation Learning for Multi-Agent Pathfinding at Scale
**Authors**: Anton Andreychuk, Konstantin Yakovlev, Aleksandr Panov, Alexey Skrynnik

**Updated**: 2024-09-12T13:49:00Z

**Summary**: Multi-agent pathfinding (MAPF) is a challenging computational problem that typically requires to find collision-free paths for multiple agents in a shared environment. Solving MAPF optimally is NP-hard, yet efficient solutions are critical for numerous applications, including automated warehouses and transportation systems. Recently, learning-based approaches to MAPF have gained attention, particularly those leveraging deep reinforcement learning. Following current trends in machine learning, we have created a foundation model for the MAPF problems called MAPF-GPT. Using imitation learning, we have trained a policy on a set of pre-collected sub-optimal expert trajectories that can generate actions in conditions of partial observability without additional heuristics, reward functions, or communication with other agents. The resulting MAPF-GPT model demonstrates zero-shot learning abilities when solving the MAPF problem instances that were not present in the training dataset. We show that MAPF-GPT notably outperforms the current best-performing learnable-MAPF solvers on a diverse range of problem instances and is efficient in terms of computation (in the inference mode).

**Link**: [arxiv](http://arxiv.org/abs/2409.00134v2),  [pdf](http://arxiv.org/pdf/2409.00134v2)

**Tags**: cs.MA cs.AI cs.LG 



### From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework   for Student Performance Feedback
**Authors**: Vinitra Swamy, Davide Romano, Bhargav Srinivasa Desikan, Oana-Maria Camburu, Tanja Käser

**Updated**: 2024-09-12T13:18:41Z

**Summary**: Recent advances in eXplainable AI (XAI) for education have highlighted a critical challenge: ensuring that explanations for state-of-the-art AI models are understandable for non-technical users such as educators and students. In response, we introduce iLLuMinaTE, a zero-shot, chain-of-prompts LLM-XAI pipeline inspired by Miller's cognitive model of explanation. iLLuMinaTE is designed to deliver theory-driven, actionable feedback to students in online courses. iLLuMinaTE navigates three main stages - causal connection, explanation selection, and explanation presentation - with variations drawing from eight social science theories (e.g. Abnormal Conditions, Pearl's Model of Explanation, Necessity and Robustness Selection, Contrastive Explanation). We extensively evaluate 21,915 natural language explanations of iLLuMinaTE extracted from three LLMs (GPT-4o, Gemma2-9B, Llama3-70B), with three different underlying XAI methods (LIME, Counterfactuals, MC-LIME), across students from three diverse online courses. Our evaluation involves analyses of explanation alignment to the social science theory, understandability of the explanation, and a real-world user preference study with 114 university students containing a novel actionability simulation. We find that students prefer iLLuMinaTE explanations over traditional explainers 89.52% of the time. Our work provides a robust, ready-to-use framework for effectively communicating hybrid XAI-driven insights in education, with significant generalization potential for other human-centric fields.

**Link**: [arxiv](http://arxiv.org/abs/2409.08027v1),  [pdf](http://arxiv.org/pdf/2409.08027v1)

**Tags**: cs.CY cs.HC cs.LG 



### Edge-Wise Graph-Instructed Neural Networks
**Authors**: Francesco Della Santa, Antonio Mastropietro, Sandra Pieraccini, Francesco Vaccarino

**Updated**: 2024-09-12T13:05:28Z

**Summary**: The problem of multi-task regression over graph nodes has been recently approached through Graph-Instructed Neural Network (GINN), which is a promising architecture belonging to the subset of message-passing graph neural networks. In this work, we discuss the limitations of the Graph-Instructed (GI) layer, and we formalize a novel edge-wise GI (EWGI) layer. We discuss the advantages of the EWGI layer and we provide numerical evidence that EWGINNs perform better than GINNs over graph-structured input data with chaotic connectivity, like the ones inferred from the Erdos-R\'enyi graph.

**Link**: [arxiv](http://arxiv.org/abs/2409.08023v1),  [pdf](http://arxiv.org/pdf/2409.08023v1)

**Tags**: cs.LG cs.AI cs.NA math.NA 05C21, 65D15, 68T07, 90C35 



### SparseGrow: Addressing Growth-Induced Forgetting in Task-Agnostic   Continual Learning
**Authors**: Yuqing Zhao, Divya Saxena, Jiannong Cao, Xiaoyun Liu, Changlin Song

**Updated**: 2024-09-12T12:57:25Z

**Summary**: In continual learning (CL), model growth enhances adaptability over new data, improving knowledge retention for more tasks. However, improper model growth can lead to severe degradation of previously learned knowledge, an issue we name as growth-induced forgetting (GIFt), especially in task-agnostic CL using entire grown model for inference. Existing works, despite adopting model growth and random initialization for better adaptability, often fail to recognize the presence of GIFt caused by improper model growth. This oversight limits comprehensive control of forgetting and hinders full utilization of model growth. We are the first in CL to identify this issue and conduct an in-depth study on root cause of GIFt, where layer expansion stands out among model growth strategies, widening layers without affecting model functionality. Yet, direct adoption of layer expansion presents challenges. It lacks data-driven control and initialization of expanded parameters to balance adaptability and knowledge retention. This paper presents a novel SparseGrow approach to overcome the issue of GIFt while enhancing adaptability over new data. SparseGrow employs data-driven sparse layer expansion to control efficient parameter usage during growth, reducing GIFt from excessive growth and functionality changes. It also combines sparse growth with on-data initialization at training late-stage to create partially 0-valued expansions that fit learned distribution, enhancing retention and adaptability. To further minimize forgetting, freezing is applied by calculating the sparse mask, allowing data-driven preservation of important parameters. Through experiments across datasets with various settings, cases and task numbers, we demonstrate the necessity of layer expansion and showcase the effectiveness of SparseGrow in overcoming GIFt, highlighting its adaptability and knowledge retention for incremental tasks.

**Link**: [arxiv](http://arxiv.org/abs/2408.10566v3),  [pdf](http://arxiv.org/pdf/2408.10566v3)

**Tags**: cs.LG cs.AI 



### An Evaluation Framework for Attributed Information Retrieval using Large   Language Models
**Authors**: Hanane Djeddal, Pierre Erbacher, Raouf Toukal, Laure Soulier, Karen Pinel-Sauvagnat, Sophia Katrenko, Lynda Tamine

**Updated**: 2024-09-12T12:57:08Z

**Summary**: With the growing success of Large Language models (LLMs) in information-seeking scenarios, search engines are now adopting generative approaches to provide answers along with in-line citations as attribution. While existing work focuses mainly on attributed question answering, in this paper, we target information-seeking scenarios which are often more challenging due to the open-ended nature of the queries and the size of the label space in terms of the diversity of candidate-attributed answers per query. We propose a reproducible framework to evaluate and benchmark attributed information seeking, using any backbone LLM, and different architectural designs: (1) Generate (2) Retrieve then Generate, and (3) Generate then Retrieve. Experiments using HAGRID, an attributed information-seeking dataset, show the impact of different scenarios on both the correctness and attributability of answers.

**Link**: [arxiv](http://arxiv.org/abs/2409.08014v1),  [pdf](http://arxiv.org/pdf/2409.08014v1)

**Tags**: cs.IR 



### SPARK: Self-supervised Personalized Real-time Monocular Face Capture
**Authors**: Kelian Baert, Shrisha Bharadwaj, Fabien Castan, Benoit Maujean, Marc Christie, Victoria Abrevaya, Adnane Boukhayma

**Updated**: 2024-09-12T12:30:04Z

**Summary**: Feedforward monocular face capture methods seek to reconstruct posed faces from a single image of a person. Current state of the art approaches have the ability to regress parametric 3D face models in real-time across a wide range of identities, lighting conditions and poses by leveraging large image datasets of human faces. These methods however suffer from clear limitations in that the underlying parametric face model only provides a coarse estimation of the face shape, thereby limiting their practical applicability in tasks that require precise 3D reconstruction (aging, face swapping, digital make-up, ...). In this paper, we propose a method for high-precision 3D face capture taking advantage of a collection of unconstrained videos of a subject as prior information. Our proposal builds on a two stage approach. We start with the reconstruction of a detailed 3D face avatar of the person, capturing both precise geometry and appearance from a collection of videos. We then use the encoder from a pre-trained monocular face reconstruction method, substituting its decoder with our personalized model, and proceed with transfer learning on the video collection. Using our pre-estimated image formation model, we obtain a more precise self-supervision objective, enabling improved expression and pose alignment. This results in a trained encoder capable of efficiently regressing pose and expression parameters in real-time from previously unseen images, which combined with our personalized geometry model yields more accurate and high fidelity mesh inference. Through extensive qualitative and quantitative evaluation, we showcase the superiority of our final model as compared to state-of-the-art baselines, and demonstrate its generalization ability to unseen pose, expression and lighting.

**Link**: [arxiv](http://arxiv.org/abs/2409.07984v1),  [pdf](http://arxiv.org/pdf/2409.07984v1)

**Tags**: cs.CV cs.LG 



### Constraints on the Primordial Black Hole Abundance through   Scalar-Induced Gravitational Waves from Advanced LIGO and Virgo's First Three   Observing Runs
**Authors**: Yang Jiang, Chen Yuan, Chong-Zhi Li, Qing-Guo Huang

**Updated**: 2024-09-12T12:16:00Z

**Summary**: As a promising dark matter candidate, primordial black holes (PBHs) lighter than $\sim10^{-18}M_{\odot}$ are supposed to have evaporated by today through Hawking radiation. This scenario is challenged by the memory burden effect, which suggests that the evaporation of black holes may slow down significantly after they have emitted about half of their initial mass. We explore the astrophysical implications of the memory burden effect on the PBH abundance by today and the possibility for PBHs lighter than $\sim10^{-18}M_{\odot}$ to persist as dark matter. Our analysis utilizes current LIGO-Virgo-KAGRA data to constrain the primordial power spectrum and infers the PBH abundance. We find a null detection of scalar-induced gravitational waves that accompanied the formation of the PBHs. Then we place an upper limit on the primordial power spectrum and the PBH abundance to be $f_{\mathrm{pbh}}\simeq0.3$ for PBHs with masses $\sim10^{-24}M_{\odot}$. Furthermore, we expect that next-generation gravitational wave detectors, such as the Einstein Telescope and the Cosmic Explorer, will provide even more stringent constraints. Our results indicate that future detectors can reach sensitivities that could rule out PBH as dark matter within $\sim[10^{-29}M_{\odot},10^{-19}M_{\odot}]$ in the null detection of scalar-induced gravitational waves.

**Link**: [arxiv](http://arxiv.org/abs/2409.07976v1),  [pdf](http://arxiv.org/pdf/2409.07976v1)

**Tags**: astro-ph.CO gr-qc hep-th 



### Unlocking Sustainability Compliance: Characterizing the EU Taxonomy for   Business Process Management
**Authors**: Finn Klessascheck, Stephan A. Fahrenkrog-Petersen, Jan Mendling, Luise Pufahl

**Updated**: 2024-09-12T12:08:04Z

**Summary**: To promote sustainable business practices, and to achieve climate neutrality by 2050, the EU has developed the taxonomy of sustainable activities, which describes when exactly business practices can be considered sustainable. While the taxonomy has only been recently established, progressively more companies will have to report how much of their revenue was created via sustainably executed business processes. To help companies prepare to assess whether their business processes comply with the constraints outlined in the taxonomy, we investigate in how far these criteria can be used for conformance checking, that is, assessing in a data-driven manner, whether business process executions adhere to regulatory constraints. For this, we develop a few-shot learning pipeline to characterize the constraints of the taxonomy with the help of an LLM as to the process dimensions they relate to. We find that many constraints of the taxonomy are useable for conformance checking, particularly in the sectors of energy, manufacturing, and transport. This will aid companies in preparing to monitor regulatory compliance with the taxonomy automatically, by characterizing what kind of information they need to extract, and by providing a better understanding of sectors where such an assessment is feasible and where it is not.

**Link**: [arxiv](http://arxiv.org/abs/2408.11386v2),  [pdf](http://arxiv.org/pdf/2408.11386v2)

**Tags**: cs.CY cs.DB 



### Localized Schrödinger Bridge Sampler
**Authors**: Georg A. Gottwald, Sebastian Reich

**Updated**: 2024-09-12T12:02:51Z

**Summary**: We consider the generative problem of sampling from an unknown distribution for which only a sufficiently large number of training samples are available. In this paper, we build on previous work combining Schr\"odinger bridges and Langevin dynamics. A key bottleneck of this approach is the exponential dependence of the required training samples on the dimension, $d$, of the ambient state space. We propose a localization strategy which exploits conditional independence of conditional expectation values. Localization thus replaces a single high-dimensional Schr\"odinger bridge problem by $d$ low-dimensional Schr\"odinger bridge problems over the available training samples. As for the original approach, the localized sampler is stable and geometric ergodic. The sampler also naturally extends to conditional sampling and to Bayesian inference. We demonstrate the performance of our proposed scheme through experiments on a Gaussian problem with increasing dimensions and on a stochastic subgrid-scale parametrization conditional sampling problem.

**Link**: [arxiv](http://arxiv.org/abs/2409.07968v1),  [pdf](http://arxiv.org/pdf/2409.07968v1)

**Tags**: stat.ML cs.LG cs.NA math.NA stat.CO 60H10, 62F15, 62F30, 65C05, 65C40 



### Exploring Parameter-Efficient Fine-Tuning of Large Language Model on   Automated Program Repair
**Authors**: Guochang Li, Chen Zhi, Jialiang Chen, Junxiao Han, Shuiguang Deng

**Updated**: 2024-09-12T12:00:26Z

**Summary**: Automated Program Repair (APR) aims to fix bugs by generating patches. And existing work has demonstrated that "pre-training and fine-tuning" paradigm enables Large Language Models (LLMs) improve fixing capabilities on APR. However, existing work mainly focuses on Full-Model Fine-Tuning (FMFT) for APR and limited research has been conducted on the execution-based evaluation of Parameter-Efficient Fine-Tuning (PEFT) for APR. Comparing to FMFT, PEFT can reduce computing resource consumption without compromising performance and has been widely adopted to other software engineering tasks.   To fill this gap, we enhance the existing APR dataset by employing prompt engineering to create an instruction dataset, APR-INSTRUCTION, at first. Secondly, we fine-tune four pre-trained LLMs using four different PEFT methods with APR-INSTRUCTION. The best fine-tuned model fixes 58% more bugs than the state-of-the-art LLM-based APR techniques. The results also show that $(IA)^3$ improves the creativity of LLMs more effectively through fine-tuning and achieves the highest fixing capability compared to the other three PEFT methods. Thirdly, we explore the optimal configuration of PEFT hyperparameters, and assess the impact of instruction dataset size, showing that a larger number of parameters and a larger training dataset do not necessarily result in better performance for PEFT. Lastly, we analyze peak memory usage and trainable parameters to show the efficiency of PEFT.   This work provides a comprehensive exploration of PEFT on APR and suggests potentially promising directions for extension to other software engineering downstream tasks. APR-INSTRUCTION, PEFT weights, and the fine-tuning code are publicly available as open-source resources.

**Link**: [arxiv](http://arxiv.org/abs/2406.05639v2),  [pdf](http://arxiv.org/pdf/2406.05639v2)

**Tags**: cs.SE 



### How Easily do Irrelevant Inputs Skew the Responses of Large Language   Models?
**Authors**: Siye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, Yanghua Xiao

**Updated**: 2024-09-12T11:51:51Z

**Summary**: By leveraging the retrieval of information from external knowledge databases, Large Language Models (LLMs) exhibit enhanced capabilities for accomplishing many knowledge-intensive tasks. However, due to the inherent flaws of current retrieval systems, there might exist irrelevant information within those retrieving top-ranked passages. In this work, we present a comprehensive investigation into the robustness of LLMs to different types of irrelevant information under various conditions. We initially introduce a framework to construct high-quality irrelevant information that ranges from semantically unrelated, partially related, and related to questions. Furthermore, our analysis demonstrates that the constructed irrelevant information not only scores highly on similarity metrics, being highly retrieved by existing systems, but also bears semantic connections to the context. Our investigation reveals that current LLMs still face challenges in discriminating highly semantically related information and can be easily distracted by these irrelevant yet misleading content. Besides, we also find that current solutions for handling irrelevant information have limitations in improving the robustness of LLMs to such distractions. All the resources are available on GitHub at https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information.

**Link**: [arxiv](http://arxiv.org/abs/2404.03302v4),  [pdf](http://arxiv.org/pdf/2404.03302v4)

**Tags**: cs.CL 



### WirelessAgent: Large Language Model Agents for Intelligent Wireless   Networks
**Authors**: Jingwen Tong, Jiawei Shao, Qiong Wu, Wei Guo, Zijian Li, Zehong Lin, Jun Zhang

**Updated**: 2024-09-12T11:48:01Z

**Summary**: Wireless networks are increasingly facing challenges due to their expanding scale and complexity. These challenges underscore the need for advanced AI-driven strategies, particularly in the upcoming 6G networks. In this article, we introduce WirelessAgent, a novel approach leveraging large language models (LLMs) to develop AI agents capable of managing complex tasks in wireless networks. It can effectively improve network performance through advanced reasoning, multimodal data processing, and autonomous decision making. Thereafter, we demonstrate the practical applicability and benefits of WirelessAgent for network slicing management. The experimental results show that WirelessAgent is capable of accurately understanding user intent, effectively allocating slice resources, and consistently maintaining optimal performance.

**Link**: [arxiv](http://arxiv.org/abs/2409.07964v1),  [pdf](http://arxiv.org/pdf/2409.07964v1)

**Tags**: cs.NI cs.AI cs.LG 



### Iterative CT Reconstruction via Latent Variable Optimization of Shallow   Diffusion Models
**Authors**: Sho Ozaki, Shizuo Kaji, Toshikazu Imae, Kanabu Nawa, Hideomi Yamashita, Keiichi Nakagawa

**Updated**: 2024-09-12T11:28:07Z

**Summary**: Image-generative artificial intelligence (AI) has garnered significant attention in recent years. In particular, the diffusion model, a core component of generative AI, produces high-quality images with rich diversity. In this study, we proposed a novel computed tomography (CT) reconstruction method by combining the denoising diffusion probabilistic model with iterative CT reconstruction. In sharp contrast to previous studies, we optimized the fidelity loss of CT reconstruction with respect to the latent variable of the diffusion model, instead of the image and model parameters. To suppress the changes in anatomical structures produced by the diffusion model, we shallowed the diffusion and reverse processes and fixed a set of added noises in the reverse process to make it deterministic during the inference. We demonstrated the effectiveness of the proposed method through the sparse-projection CT reconstruction of 1/10 projection data. Despite the simplicity of the implementation, the proposed method has the potential to reconstruct high-quality images while preserving the patient's anatomical structures and was found to outperform existing methods, including iterative reconstruction, iterative reconstruction with total variation, and the diffusion model alone in terms of quantitative indices such as the structural similarity index and peak signal-to-noise ratio. We also explored further sparse-projection CT reconstruction using 1/20 projection data with the same trained diffusion model. As the number of iterations increased, the image quality improved comparable to that of 1/10 sparse-projection CT reconstruction. In principle, this method can be widely applied not only to CT but also to other imaging modalities.

**Link**: [arxiv](http://arxiv.org/abs/2408.03156v2),  [pdf](http://arxiv.org/pdf/2408.03156v2)

**Tags**: cs.CV cs.LG physics.med-ph 



### Data-efficient multi-fidelity training for high-fidelity machine   learning interatomic potentials
**Authors**: Jaesun Kim, Jisu Kim, Jaehoon Kim, Jiho Lee, Yutack Park, Youngho Kang, Seungwu Han

**Updated**: 2024-09-12T11:18:00Z

**Summary**: Machine learning interatomic potentials (MLIPs) are used to estimate potential energy surfaces (PES) from ab initio calculations, providing near quantum-level accuracy with reduced computational costs. However, the high cost of assembling high-fidelity databases hampers the application of MLIPs to systems that require high chemical accuracy. Utilizing an equivariant graph neural network, we present an MLIP framework that trains on multi-fidelity databases simultaneously. This approach enables the accurate learning of high-fidelity PES with minimal high-fidelity data. We test this framework on the Li$_6$PS$_5$Cl and In$_x$Ga$_{1-x}$N systems. The computational results indicate that geometric and compositional spaces not covered by the high-fidelity meta-gradient generalized approximation (meta-GGA) database can be effectively inferred from low-fidelity GGA data, thus enhancing accuracy and molecular dynamics stability. We also develop a general-purpose MLIP that utilizes both GGA and meta-GGA data from the Materials Project, significantly enhancing MLIP performance for high-accuracy tasks such as predicting energies above hull for crystals in general. Furthermore, we demonstrate that the present multi-fidelity learning is more effective than transfer learning or $\Delta$-learning an d that it can also be applied to learn higher-fidelity up to the coupled-cluster level. We believe this methodology holds promise for creating highly accurate bespoke or universal MLIPs by effectively expanding the high-fidelity dataset.

**Link**: [arxiv](http://arxiv.org/abs/2409.07947v1),  [pdf](http://arxiv.org/pdf/2409.07947v1)

**Tags**: cond-mat.mtrl-sci 



### Collaborative Automatic Modulation Classification via Deep Edge   Inference for Hierarchical Cognitive Radio Networks
**Authors**: Chaowei He, Peihao Dong, Fuhui Zhou, Qihui Wu

**Updated**: 2024-09-12T11:14:25Z

**Summary**: In hierarchical cognitive radio networks, edge or cloud servers utilize the data collected by edge devices for modulation classification, which, however, is faced with problems of the transmission overhead, data privacy, and computation load. In this article, an edge learning (EL) based framework jointly mobilizing the edge device and the edge server for intelligent co-inference is proposed to realize the collaborative automatic modulation classification (C-AMC) between them. A spectrum semantic compression neural network (SSCNet) with the lightweight structure is designed for the edge device to compress the collected raw data into a compact semantic message that is then sent to the edge server via the wireless channel. On the edge server side, a modulation classification neural network (MCNet) combining bidirectional long short-term memory (Bi?LSTM) and multi-head attention layers is elaborated to deter?mine the modulation type from the noisy semantic message. By leveraging the computation resources of both the edge device and the edge server, high transmission overhead and risks of data privacy leakage are avoided. The simulation results verify the effectiveness of the proposed C-AMC framework, significantly reducing the model size and computational complexity.

**Link**: [arxiv](http://arxiv.org/abs/2409.07946v1),  [pdf](http://arxiv.org/pdf/2409.07946v1)

**Tags**: cs.IR 



### AIC MLLM: Autonomous Interactive Correction MLLM for Robust Robotic   Manipulation
**Authors**: Chuyan Xiong, Chengyu Shen, Xiaoqi Li, Kaichen Zhou, Jiaming Liu, Ruiping Wang, Hao Dong

**Updated**: 2024-09-12T10:48:30Z

**Summary**: The ability to reflect on and correct failures is crucial for robotic systems to interact stably with real-life objects.Observing the generalization and reasoning capabilities of Multimodal Large Language Models (MLLMs), previous approaches have aimed to utilize these models to enhance robotic systems accordingly.However, these methods typically focus on high-level planning corrections using an additional MLLM, with limited utilization of failed samples to correct low-level contact poses. To address this gap, we propose an Autonomous Interactive Correction (AIC) MLLM, which makes use of previous low-level interaction experiences to correct SE(3) pose predictions. Specifically, AIC MLLM is initially fine-tuned to acquire both pose prediction and feedback prompt comprehension abilities.We carefully design two types of prompt instructions through interactions with objects: 1) visual masks to highlight unmovable parts for position correction, and 2)textual descriptions to indicate potential directions for rotation correction.During inference, a Feedback Information Extraction module is introduced to recognize the failure cause, allowing AIC MLLM to adaptively correct the pose prediction using the corresponding prompts. To further enhance manipulation stability, we devise a Test Time Adaptation strategy that enables AIC MLLM to better adapt to the current scene configuration.Finally, extensive experiments are conducted in both simulated and real-world environments to evaluate the proposed method. The results demonstrate that our AIC MLLM can efficiently correct failure samples by leveraging interaction experience prompts.Real-world demonstration can be found at https://sites.google.com/view/aic-mllm

**Link**: [arxiv](http://arxiv.org/abs/2406.11548v3),  [pdf](http://arxiv.org/pdf/2406.11548v3)

**Tags**: cs.RO cs.AI cs.CV 



### Parameter constraints for accreting millisecond pulsars with synthetic   NICER data
**Authors**: Bas Dorsman, Tuomo Salmi, Anna L. Watts, Mason Ng, Satish Kamath, Anna Bobrikova, Juri Poutanen, Vladislav Loktev, Yves Kini, Devarshi Choudhury, Serena Vinciguerra, Slavko Bogdanov, Deepto Chakrabarty

**Updated**: 2024-09-12T10:24:01Z

**Summary**: Pulse profile modelling (PPM) is a technique for inferring mass, radius and hotspot properties of millisecond pulsars. PPM is now regularly used for analysis of rotation-powered millisecond pulsars (RMPs) with data from the Neutron Star Interior Composition ExploreR (NICER). Extending PPM to accreting millisecond pulsars (AMPs) is attractive, because they are a different source class featuring bright X-ray radiation from hotspots powered by accretion. In this paper, we present a modification of one of the PPM codes, X-PSI, so that it can be used for AMPs. In particular, we implement a model of an accretion disc and atmosphere model appropriate for the hotspots of AMPs, and improve the overall computational efficiency. We then test parameter recovery with synthetic NICER data in two scenarios with reasonable parameters for AMPs. We find in the first scenario, where the hotspot is large, that we are able to tightly and accurately constrain all parameters including mass and radius. In the second scenario, which is a high inclination system with a smaller hotspot, we find degeneracy between a subset of model parameters and a slight bias in the inferred mass and radius. This analysis of synthetic data lays the ground work for future analysis of AMPs with NICER data. Such an analysis could be complemented by future (joint) analysis of polarization data from the Imaging X-ray Polarimetry Explorer (IXPE).

**Link**: [arxiv](http://arxiv.org/abs/2409.07908v1),  [pdf](http://arxiv.org/pdf/2409.07908v1)

**Tags**: astro-ph.HE 



### Identifiable causal inference with noisy treatment and no side   information
**Authors**: Antti Pöllänen, Pekka Marttinen

**Updated**: 2024-09-12T10:20:43Z

**Summary**: In some causal inference scenarios, the treatment variable is measured inaccurately, for instance in epidemiology or econometrics. Failure to correct for the effect of this measurement error can lead to biased causal effect estimates. Previous research has not studied methods that address this issue from a causal viewpoint while allowing for complex nonlinear dependencies and without assuming access to side information. For such a scenario, this study proposes a model that assumes a continuous treatment variable that is inaccurately measured. Building on existing results for measurement error models, we prove that our model's causal effect estimates are identifiable, even without side information and knowledge of the measurement error variance. Our method relies on a deep latent variable model in which Gaussian conditionals are parameterized by neural networks, and we develop an amortized importance-weighted variational objective for training the model. Empirical results demonstrate the method's good performance with unknown measurement error. More broadly, our work extends the range of applications in which reliable causal inference can be conducted.

**Link**: [arxiv](http://arxiv.org/abs/2306.10614v3),  [pdf](http://arxiv.org/pdf/2306.10614v3)

**Tags**: cs.LG stat.ME stat.ML 68T37 



### Conformal Distributed Remote Inference in Sensor Networks Under   Reliability and Communication Constraints
**Authors**: Meiyi Zhu, Matteo Zecchin, Sangwoo Park, Caili Guo, Chunyan Feng, Petar Popovski, Osvaldo Simeone

**Updated**: 2024-09-12T10:12:43Z

**Summary**: This paper presents communication-constrained distributed conformal risk control (CD-CRC) framework, a novel decision-making framework for sensor networks under communication constraints. Targeting multi-label classification problems, such as segmentation, CD-CRC dynamically adjusts local and global thresholds used to identify significant labels with the goal of ensuring a target false negative rate (FNR), while adhering to communication capacity limits. CD-CRC builds on online exponentiated gradient descent to estimate the relative quality of the observations of different sensors, and on online conformal risk control (CRC) as a mechanism to control local and global thresholds. CD-CRC is proved to offer deterministic worst-case performance guarantees in terms of FNR and communication overhead, while the regret performance in terms of false positive rate (FPR) is characterized as a function of the key hyperparameters. Simulation results highlight the effectiveness of CD-CRC, particularly in communication resource-constrained environments, making it a valuable tool for enhancing the performance and reliability of distributed sensor networks.

**Link**: [arxiv](http://arxiv.org/abs/2409.07902v1),  [pdf](http://arxiv.org/pdf/2409.07902v1)

**Tags**: eess.SP cs.IT cs.LG math.IT 



### Genuine Retrieval of the AGN Host Stellar Population (GRAHSP)
**Authors**: Johannes Buchner, Hattie Starck, Mara Salvato, Hagai Netzer, Zsofi Igo, Brivael Laloux, Antonis Georgakakis, Isabelle Gauger, Anna Olechowska, Nicolas Lopez, Suraj D Shankar, Junyao Li, Kirpal Nandra, Andrea Merloni

**Updated**: 2024-09-12T10:04:08Z

**Summary**: The assembly and co-evolution of supermassive black holes (SMBH) and their host galaxy stellar population is a key open questions in galaxy evolution. Stellar mass ($M_\star$) and star formation rate (SFR), are inferred by modeling the spectral energy distribution (SED). For galaxies triggering SMBH activity, the active galactic nucleus (AGN) contaminates the light at all wavelengths, hampering the inference of galaxy parameters. Incomplete AGN templates can lead to systematic overestimates of the stellar mass, biasing our understanding of AGN-galaxy co-evolution. This challenge has gained further impetus with the advent of sensitive wide-area surveys with millions of luminous AGN, including by eROSITA, Euclid and LSST. We aim to estimate the accuracy and bias of AGN host galaxy parameters and improve upon existing techniques. This work makes two contributions: 1) a new SED fitting code, GRAHSP, with a flexible, empirically motivated AGN model including a power law continuum emission lines, a FeII forest and a flexible infrared torus. We verify that our model reproduces published X-ray to infrared SEDs of AGN to better than 20\% accuracy. A fully Bayesian fit with nested sampling includes uncertainties in the model and the data, making the inference highly robust. 2) we created a benchmark photometric dataset where pure quasars are merged with non-AGN pure galaxies into a hybrid (Chimera) object but with known galaxy and AGN properties. Comparing the true and retrieved $M_\star$, SFR and AGN luminosities shows that previous codes systematically over-estimate $M_\star$ and SFR by 0.5 dex with a wide scatter of 0.7 dex, at AGN luminosities above 10^44 erg/s. In contrast, GRAHSP shows no bias on $M_\star$ and SFR. GRAHSP also estimates more realistic uncertainties. GRAHSP enables characterization of the environmental conditions conducive to black hole growth. (abridged)

**Link**: [arxiv](http://arxiv.org/abs/2405.19297v2),  [pdf](http://arxiv.org/pdf/2405.19297v2)

**Tags**: astro-ph.GA astro-ph.HE 



### Objection Overruled! Lay People can Distinguish Large Language Models   from Lawyers, but still Favour Advice from an LLM
**Authors**: Eike Schneiders, Tina Seabrooke, Joshua Krook, Richard Hyde, Natalie Leesakul, Jeremie Clos, Joel Fischer

**Updated**: 2024-09-12T09:28:34Z

**Summary**: Large Language Models (LLMs) are seemingly infiltrating every domain, and the legal context is no exception. In this paper, we present the results of three experiments (total N=288) that investigated lay people's willingness to act upon, and their ability to discriminate between, LLM- and lawyer-generated legal advice. In Experiment 1, participants judged their willingness to act on legal advice when the source of the advice was either known or unknown. When the advice source was unknown, participants indicated that they were significantly more willing to act on the LLM-generated advice. This result was replicated in Experiment 2. Intriguingly, despite participants indicating higher willingness to act on LLM-generated advice in Experiments 1 and 2, participants discriminated between the LLM- and lawyer-generated texts significantly above chance-level in Experiment 3. Lastly, we discuss potential explanations and risks of our findings, limitations and future work, and the importance of language complexity and real-world comparability.

**Link**: [arxiv](http://arxiv.org/abs/2409.07871v1),  [pdf](http://arxiv.org/pdf/2409.07871v1)

**Tags**: cs.HC cs.CY 



### Sliding-Window Thompson Sampling for Non-Stationary Settings
**Authors**: Marco Fiandri, Alberto Maria Metelli, Francesco Trovò

**Updated**: 2024-09-12T09:08:56Z

**Summary**: $\textit{Restless Bandits}$ describe sequential decision-making problems in which the rewards evolve with time independently from the actions taken by the policy-maker. It has been shown that classical Bandit algorithms fail when the underlying environment is changing, making clear that in order to tackle more challenging scenarios specifically crafted algorithms are needed. In this paper, extending and correcting the work by \cite{trovo2020sliding}, we analyze two Thompson-Sampling inspired algorithms, namely $\texttt{BETA-SWTS}$ and $\texttt{$\gamma$-SWGTS}$, introduced to face the additional complexity given by the non-stationary nature of the settings; in particular we derive a general formulation for the regret in $\textit{any}$ arbitrary restless environment for both Bernoulli and Subgaussian rewards, and, through the introduction of new quantities, we delve in what contribution lays the deeper foundations of the error made by the algorithms. Finally, we infer from the general formulation the regret for two of the most common non-stationary settings: the $\textit{Abruptly Changing}$ and the $\textit{Smoothly Changing}$ environments.

**Link**: [arxiv](http://arxiv.org/abs/2409.05181v2),  [pdf](http://arxiv.org/pdf/2409.05181v2)

**Tags**: stat.ML cs.LG 



### Bootstrap Adaptive Lasso Solution Path Unit Root Tests
**Authors**: Martin C. Arnold, Thilo Reinschlüssel

**Updated**: 2024-09-12T09:05:40Z

**Summary**: We propose sieve wild bootstrap analogues to the adaptive Lasso solution path unit root tests of Arnold and Reinschl\"ussel (2024) arXiv:2404.06205 to improve finite sample properties and extend their applicability to a generalised framework, allowing for non-stationary volatility. Numerical evidence shows the bootstrap to improve the tests' precision for error processes that promote spurious rejections of the unit root null, depending on the detrending procedure. The bootstrap mitigates finite-sample size distortions and restores asymptotically valid inference when the data features time-varying unconditional variance. We apply the bootstrap tests to real residential property prices of the top six Eurozone economies and find evidence of stationarity to be period-specific, supporting the conjecture that exuberance in the housing market characterises the development of Euro-era residential property prices in the recent past.

**Link**: [arxiv](http://arxiv.org/abs/2409.07859v1),  [pdf](http://arxiv.org/pdf/2409.07859v1)

**Tags**: stat.ME econ.EM 



### GOPT: Generalizable Online 3D Bin Packing via Transformer-based Deep   Reinforcement Learning
**Authors**: Heng Xiong, Changrong Guo, Jian Peng, Kai Ding, Wenjie Chen, Xuchong Qiu, Long Bai, Jianfeng Xu

**Updated**: 2024-09-12T08:49:20Z

**Summary**: Robotic object packing has broad practical applications in the logistics and automation industry, often formulated by researchers as the online 3D Bin Packing Problem (3D-BPP). However, existing DRL-based methods primarily focus on enhancing performance in limited packing environments while neglecting the ability to generalize across multiple environments characterized by different bin dimensions. To this end, we propose GOPT, a generalizable online 3D Bin Packing approach via Transformer-based deep reinforcement learning (DRL). First, we design a Placement Generator module to yield finite subspaces as placement candidates and the representation of the bin. Second, we propose a Packing Transformer, which fuses the features of the items and bin, to identify the spatial correlation between the item to be packed and available sub-spaces within the bin. Coupling these two components enables GOPT's ability to perform inference on bins of varying dimensions. We conduct extensive experiments and demonstrate that GOPT not only achieves superior performance against the baselines, but also exhibits excellent generalization capabilities. Furthermore, the deployment with a robot showcases the practical applicability of our method in the real world. The source code will be publicly available at https://github.com/Xiong5Heng/GOPT.

**Link**: [arxiv](http://arxiv.org/abs/2409.05344v2),  [pdf](http://arxiv.org/pdf/2409.05344v2)

**Tags**: cs.RO 



### Real-time Multi-view Omnidirectional Depth Estimation System for Robots   and Autonomous Driving on Real Scenes
**Authors**: Ming Li, Xiong Yang, Chaofan Wu, Jiaheng Li, Pinzhi Wang, Xuejiao Hu, Sidan Du, Yang Li

**Updated**: 2024-09-12T08:44:35Z

**Summary**: Omnidirectional Depth Estimation has broad application prospects in fields such as robotic navigation and autonomous driving. In this paper, we propose a robotic prototype system and corresponding algorithm designed to validate omnidirectional depth estimation for navigation and obstacle avoidance in real-world scenarios for both robots and vehicles. The proposed HexaMODE system captures 360$^\circ$ depth maps using six surrounding arranged fisheye cameras. We introduce a combined spherical sweeping method and optimize the model architecture for proposed RtHexa-OmniMVS algorithm to achieve real-time omnidirectional depth estimation. To ensure high accuracy, robustness, and generalization in real-world environments, we employ a teacher-student self-training strategy, utilizing large-scale unlabeled real-world data for model training. The proposed algorithm demonstrates high accuracy in various complex real-world scenarios, both indoors and outdoors, achieving an inference speed of 15 fps on edge computing platforms.

**Link**: [arxiv](http://arxiv.org/abs/2409.07843v1),  [pdf](http://arxiv.org/pdf/2409.07843v1)

**Tags**: cs.CV cs.RO 



### What Matters to Enhance Traffic Rule Compliance of Imitation Learning   for End-to-End Autonomous Driving
**Authors**: Hongkuan Zhou, Wei Cao, Aifen Sui, Zhenshan Bing

**Updated**: 2024-09-12T08:38:39Z

**Summary**: End-to-end autonomous driving, where the entire driving pipeline is replaced with a single neural network, has recently gained research attention because of its simpler structure and faster inference time. Despite this appealing approach largely reducing the complexity in the driving pipeline, it also leads to safety issues because the trained policy is not always compliant with the traffic rules. In this paper, we proposed P-CSG, a penalty-based imitation learning approach with contrastive-based cross semantics generation sensor fusion technologies to increase the overall performance of end-to-end autonomous driving. In this method, we introduce three penalties - red light, stop sign, and curvature speed penalty to make the agent more sensitive to traffic rules. The proposed cross semantics generation helps to align the shared information of different input modalities. We assessed our model's performance using the CARLA Leaderboard - Town 05 Long Benchmark and Longest6 Benchmark, achieving 8.5% and 2.0% driving score improvement compared to the baselines. Furthermore, we conducted robustness evaluations against adversarial attacks like FGSM and Dot attacks, revealing a substantial increase in robustness compared to other baseline models. More detailed information can be found at https://hk-zh.github.io/p-csg-plus.

**Link**: [arxiv](http://arxiv.org/abs/2309.07808v3),  [pdf](http://arxiv.org/pdf/2309.07808v3)

**Tags**: cs.CV cs.AI cs.LG cs.RO 



### Do These LLM Benchmarks Agree? Fixing Benchmark Evaluation with   BenchBench
**Authors**: Yotam Perlitz, Ariel Gera, Ofir Arviv, Asaf Yehudai, Elron Bandel, Eyal Shnarch, Michal Shmueli-Scheuer, Leshem Choshen

**Updated**: 2024-09-12T08:36:47Z

**Summary**: Recent advancements in Language Models (LMs) have catalyzed the creation of multiple benchmarks, designed to assess these models' general capabilities. A crucial task, however, is assessing the validity of the benchmarks themselves. This is most commonly done via Benchmark Agreement Testing (BAT), where new benchmarks are validated against established ones using some agreement metric (e.g., rank correlation). Despite the crucial role of BAT for benchmark builders and consumers, there are no standardized procedures for such agreement testing. This deficiency can lead to invalid conclusions, fostering mistrust in benchmarks and upending the ability to properly choose the appropriate benchmark to use. By analyzing over 40 prominent benchmarks, we demonstrate how some overlooked methodological choices can significantly influence BAT results, potentially undermining the validity of conclusions. To address these inconsistencies, we propose a set of best practices for BAT and demonstrate how utilizing these methodologies greatly improves BAT robustness and validity. To foster adoption and facilitate future research,, we introduce BenchBench, a python package for BAT, and release the BenchBench-leaderboard, a meta-benchmark designed to evaluate benchmarks using their peers. Our findings underscore the necessity for standardized BAT, ensuring the robustness and validity of benchmark evaluations in the evolving landscape of language model research.   BenchBench Package: github.com/IBM/BenchBench   Leaderboard: hf.co/spaces/IBM/BenchBench

**Link**: [arxiv](http://arxiv.org/abs/2407.13696v2),  [pdf](http://arxiv.org/pdf/2407.13696v2)

**Tags**: cs.CL 



### Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs:   A Case Study in WeChat
**Authors**: Sidong Feng, Haochuan Lu, Jianqin Jiang, Ting Xiong, Likun Huang, Yinglin Liang, Xiaoqin Li, Yuetang Deng, Aldeida Aleti

**Updated**: 2024-09-12T08:25:33Z

**Summary**: UI automation tests play a crucial role in ensuring the quality of mobile applications. Despite the growing popularity of machine learning techniques to generate these tests, they still face several challenges, such as the mismatch of UI elements. The recent advances in Large Language Models (LLMs) have addressed these issues by leveraging their semantic understanding capabilities. However, a significant gap remains in applying these models to industrial-level app testing, particularly in terms of cost optimization and knowledge limitation. To address this, we introduce CAT to create cost-effective UI automation tests for industry apps by combining machine learning and LLMs with best practices. Given the task description, CAT employs Retrieval Augmented Generation (RAG) to source examples of industrial app usage as the few-shot learning context, assisting LLMs in generating the specific sequence of actions. CAT then employs machine learning techniques, with LLMs serving as a complementary optimizer, to map the target element on the UI screen. Our evaluations on the WeChat testing dataset demonstrate the CAT's performance and cost-effectiveness, achieving 90% UI automation with $0.34 cost, outperforming the state-of-the-art. We have also integrated our approach into the real-world WeChat testing platform, demonstrating its usefulness in detecting 141 bugs and enhancing the developers' testing process.

**Link**: [arxiv](http://arxiv.org/abs/2409.07829v1),  [pdf](http://arxiv.org/pdf/2409.07829v1)

**Tags**: cs.SE 



### Graph of Graphs: From Nodes to Supernodes in Graphical Models
**Authors**: Maria De Iorio, Willem van den Boom, Alexandros Beskos, Ajay Jasra, Andrea Cremaschi

**Updated**: 2024-09-12T07:53:17Z

**Summary**: High-dimensional data analysis typically focuses on low-dimensional structure, often to aid interpretation and computational efficiency. Graphical models provide a powerful methodology for learning the conditional independence structure in multivariate data by representing variables as nodes and dependencies as edges. Inference is often focused on individual edges in the latent graph. Nonetheless, there is increasing interest in determining more complex structures, such as communities of nodes, for multiple reasons, including more effective information retrieval and better interpretability. In this work, we propose a hierarchical graphical model where we first cluster nodes and then, at the higher level, investigate the relationships among groups of nodes. Specifically, nodes are partitioned into supernodes with a data-coherent size-biased tessellation prior which combines ideas from Bayesian nonparametrics and Voronoi tessellations. This construct also allows accounting for the dependence of nodes within supernodes. At the higher level, dependence structure among supernodes is modeled through a Gaussian graphical model, where the focus of inference is on superedges. We provide theoretical justification for our modeling choices. We design tailored Markov chain Monte Carlo schemes, which also enable parallel computations. We demonstrate the effectiveness of our approach for large-scale structure learning in simulations and a transcriptomics application.

**Link**: [arxiv](http://arxiv.org/abs/2310.11741v2),  [pdf](http://arxiv.org/pdf/2310.11741v2)

**Tags**: stat.ME 



### What is YOLOv9: An In-Depth Exploration of the Internal Features of the   Next-Generation Object Detector
**Authors**: Muhammad Yaseen

**Updated**: 2024-09-12T07:46:58Z

**Summary**: This study provides a comprehensive analysis of the YOLOv9 object detection model, focusing on its architectural innovations, training methodologies, and performance improvements over its predecessors. Key advancements, such as the Generalized Efficient Layer Aggregation Network GELAN and Programmable Gradient Information PGI, significantly enhance feature extraction and gradient flow, leading to improved accuracy and efficiency. By incorporating Depthwise Convolutions and the lightweight C3Ghost architecture, YOLOv9 reduces computational complexity while maintaining high precision. Benchmark tests on Microsoft COCO demonstrate its superior mean Average Precision mAP and faster inference times, outperforming YOLOv8 across multiple metrics. The model versatility is highlighted by its seamless deployment across various hardware platforms, from edge devices to high performance GPUs, with built in support for PyTorch and TensorRT integration. This paper provides the first in depth exploration of YOLOv9s internal features and their real world applicability, establishing it as a state of the art solution for real time object detection across industries, from IoT devices to large scale industrial applications.

**Link**: [arxiv](http://arxiv.org/abs/2409.07813v1),  [pdf](http://arxiv.org/pdf/2409.07813v1)

**Tags**: cs.CV 



### Trustworthy, Responsible, and Safe AI: A Comprehensive Architectural   Framework for AI Safety with Challenges and Mitigations
**Authors**: Chen Chen, Ziyao Liu, Weifeng Jiang, Si Qi Goh, Kwok-Yan Lam

**Updated**: 2024-09-12T07:45:50Z

**Summary**: AI Safety is an emerging area of critical importance to the safe adoption and deployment of AI systems. With the rapid proliferation of AI and especially with the recent advancement of Generative AI (or GAI), the technology ecosystem behind the design, development, adoption, and deployment of AI systems has drastically changed, broadening the scope of AI Safety to address impacts on public safety and national security. In this paper, we propose a novel architectural framework for understanding and analyzing AI Safety; defining its characteristics from three perspectives: Trustworthy AI, Responsible AI, and Safe AI. We provide an extensive review of current research and advancements in AI safety from these perspectives, highlighting their key challenges and mitigation approaches. Through examples from state-of-the-art technologies, particularly Large Language Models (LLMs), we present innovative mechanism, methodologies, and techniques for designing and testing AI safety. Our goal is to promote advancement in AI safety research, and ultimately enhance people's trust in digital transformation.

**Link**: [arxiv](http://arxiv.org/abs/2408.12935v2),  [pdf](http://arxiv.org/pdf/2408.12935v2)

**Tags**: cs.AI 



### Extremely Dense Gas around Little Red Dots and High-redshift AGNs: A   Non-stellar Origin of the Balmer Break and Absorption Features
**Authors**: Kohei Inayoshi, Roberto Maiolino

**Updated**: 2024-09-12T07:32:24Z

**Summary**: The James Webb Space Telescope (JWST) has uncovered low-luminosity active galactic nuclei (AGNs) at high redshifts of $z\gtrsim 4-7$, powered by accreting black holes (BHs) with masses of $\sim 10^{6-8}~M_\odot$. These AGN populations are considered crucial for understanding early BH assembly and coevolution with their host galaxies. One remarkable distinction of these JWST-identified AGNs, compared to their low-redshift counterparts, is that at least $\sim 20\%$ of them present H$\alpha$ and/or H$\beta$ absorption, which must be associated with extremely dense ($\gtrsim 10^9$ cm$^{-3}$) gas along the line of sight. These Balmer absorption features unavoidably imply the presence of a Balmer break caused by the same dense gas. In this Letter, we quantitatively demonstrate that a Balmer-break feature can form in AGN spectra without stellar components, when the accretion disk is heavily embedded in dense neutral gas clumps with densities of $\sim 10^{9-11}$ cm$^{-3}$, where hydrogen atoms are collisionally excited to the $n=2$ states and effectively absorb the AGN continuum at the bluer side of the Balmer limit. The non-stellar origin of a Balmer break offers a potential solution to the large stellar masses and densities inferred for little red dots (LRDs) when assuming that their continuum is primarily due to stellar light. Our calculations of hydrogen-level populations indicate that the observed Balmer absorption blueshifted by a few hundreds km s$^{-1}$ suggests the presence of dense outflows at parsec scales in the nucleus. The outflow rate likely exceeds the Eddington accretion rate, driven by powerful radiation from a super-Eddington accretion disk. Other spectral features such as higher equivalent widths of broad H$\alpha$ emission and presence of OI lines observed in high-redshift AGNs including LRDs align with the predicted signatures of a dense super-Eddington accretion disk.

**Link**: [arxiv](http://arxiv.org/abs/2409.07805v1),  [pdf](http://arxiv.org/pdf/2409.07805v1)

**Tags**: astro-ph.GA 



### Memory-Efficient 3D Denoising Diffusion Models for Medical Image   Processing
**Authors**: Florentin Bieder, Julia Wolleb, Alicia Durrer, Robin Sandkühler, Philippe C. Cattin

**Updated**: 2024-09-12T07:26:21Z

**Summary**: Denoising diffusion models have recently achieved state-of-the-art performance in many image-generation tasks. They do, however, require a large amount of computational resources. This limits their application to medical tasks, where we often deal with large 3D volumes, like high-resolution three-dimensional data. In this work, we present a number of different ways to reduce the resource consumption for 3D diffusion models and apply them to a dataset of 3D images. The main contribution of this paper is the memory-efficient patch-based diffusion model \textit{PatchDDM}, which can be applied to the total volume during inference while the training is performed only on patches. While the proposed diffusion model can be applied to any image generation tasks, we evaluate the method on the tumor segmentation task of the BraTS2020 dataset and demonstrate that we can generate meaningful three-dimensional segmentations.

**Link**: [arxiv](http://arxiv.org/abs/2303.15288v2),  [pdf](http://arxiv.org/pdf/2303.15288v2)

**Tags**: cs.CV cs.LG 



### LLM-enhanced Scene Graph Learning for Household Rearrangement
**Authors**: Wenhao Li, Zhiyuan Yu, Qijin She, Zhinan Yu, Yuqing Lan, Chenyang Zhu, Ruizhen Hu, Kai Xu

**Updated**: 2024-09-12T07:18:00Z

**Summary**: The household rearrangement task involves spotting misplaced objects in a scene and accommodate them with proper places. It depends both on common-sense knowledge on the objective side and human user preference on the subjective side. In achieving such task, we propose to mine object functionality with user preference alignment directly from the scene itself, without relying on human intervention. To do so, we work with scene graph representation and propose LLM-enhanced scene graph learning which transforms the input scene graph into an affordance-enhanced graph (AEG) with information-enhanced nodes and newly discovered edges (relations). In AEG, the nodes corresponding to the receptacle objects are augmented with context-induced affordance which encodes what kind of carriable objects can be placed on it. New edges are discovered with newly discovered non-local relations. With AEG, we perform task planning for scene rearrangement by detecting misplaced carriables and determining a proper placement for each of them. We test our method by implementing a tiding robot in simulator and perform evaluation on a new benchmark we build. Extensive evaluations demonstrate that our method achieves state-of-the-art performance on misplacement detection and the following rearrangement planning.

**Link**: [arxiv](http://arxiv.org/abs/2408.12093v2),  [pdf](http://arxiv.org/pdf/2408.12093v2)

**Tags**: cs.RO cs.CV 



### In-Situ Fine-Tuning of Wildlife Models in IoT-Enabled Camera Traps for   Efficient Adaptation
**Authors**: Mohammad Mehdi Rastikerdar, Jin Huang, Hui Guan, Deepak Ganesan

**Updated**: 2024-09-12T06:56:52Z

**Summary**: Wildlife monitoring via camera traps has become an essential tool in ecology, but the deployment of machine learning models for on-device animal classification faces significant challenges due to domain shifts and resource constraints. This paper introduces WildFit, a novel approach that reconciles the conflicting goals of achieving high domain generalization performance and ensuring efficient inference for camera trap applications. WildFit leverages continuous background-aware model fine-tuning to deploy ML models tailored to the current location and time window, allowing it to maintain robust classification accuracy in the new environment without requiring significant computational resources. This is achieved by background-aware data synthesis, which generates training images representing the new domain by blending background images with animal images from the source domain. We further enhance fine-tuning effectiveness through background drift detection and class distribution drift detection, which optimize the quality of synthesized data and improve generalization performance. Our extensive evaluation across multiple camera trap datasets demonstrates that WildFit achieves significant improvements in classification accuracy and computational efficiency compared to traditional approaches.

**Link**: [arxiv](http://arxiv.org/abs/2409.07796v1),  [pdf](http://arxiv.org/pdf/2409.07796v1)

**Tags**: cs.CV cs.AI cs.LG 



### Full-text Error Correction for Chinese Speech Recognition with Large   Language Model
**Authors**: Zhiyuan Tang, Dong Wang, Shen Huang, Shidong Shang

**Updated**: 2024-09-12T06:50:45Z

**Summary**: Large Language Models (LLMs) have demonstrated substantial potential for error correction in Automatic Speech Recognition (ASR). However, most research focuses on utterances from short-duration speech recordings, which are the predominant form of speech data for supervised ASR training. This paper investigates the effectiveness of LLMs for error correction in full-text generated by ASR systems from longer speech recordings, such as transcripts from podcasts, news broadcasts, and meetings. First, we develop a Chinese dataset for full-text error correction, named ChFT, utilizing a pipeline that involves text-to-speech synthesis, ASR, and error-correction pair extractor. This dataset enables us to correct errors across contexts, including both full-text and segment, and to address a broader range of error types, such as punctuation restoration and inverse text normalization, thus making the correction process comprehensive. Second, we fine-tune a pre-trained LLM on the constructed dataset using a diverse set of prompts and target formats, and evaluate its performance on full-text error correction. Specifically, we design prompts based on full-text and segment, considering various output formats, such as directly corrected text and JSON-based error-correction pairs. Through various test settings, including homogeneous, up-to-date, and hard test sets, we find that the fine-tuned LLMs perform well in the full-text setting with different prompts, each presenting its own strengths and weaknesses. This establishes a promising baseline for further research. The dataset is available on the website.

**Link**: [arxiv](http://arxiv.org/abs/2409.07790v1),  [pdf](http://arxiv.org/pdf/2409.07790v1)

**Tags**: cs.CL eess.AS 



### Alignment with Preference Optimization Is All You Need for LLM Safety
**Authors**: Reda Alami, Ali Khalifa Almansoori, Ahmed Alzubaidi, Mohamed El Amine Seddik, Mugariya Farooq, Hakim Hacid

**Updated**: 2024-09-12T06:10:15Z

**Summary**: We demonstrate that preference optimization methods can effectively enhance LLM safety. Applying various alignment techniques to the Falcon 11B model using safety datasets, we achieve a significant boost in global safety score (from $57.64\%$ to $99.90\%$) as measured by LlamaGuard 3 8B, competing with state-of-the-art models. On toxicity benchmarks, average scores in adversarial settings dropped from over $0.6$ to less than $0.07$. However, this safety improvement comes at the cost of reduced general capabilities, particularly in math, suggesting a trade-off. We identify noise contrastive alignment (Safe-NCA) as an optimal method for balancing safety and performance. Our study ultimately shows that alignment techniques can be sufficient for building safe and robust models.

**Link**: [arxiv](http://arxiv.org/abs/2409.07772v1),  [pdf](http://arxiv.org/pdf/2409.07772v1)

**Tags**: cs.LG 



### DiTAS: Quantizing Diffusion Transformers via Enhanced Activation   Smoothing
**Authors**: Zhenyuan Dong, Sai Qian Zhang

**Updated**: 2024-09-12T05:18:57Z

**Summary**: Diffusion Transformers (DiTs) have recently attracted significant interest from both industry and academia due to their enhanced capabilities in visual generation, surpassing the performance of traditional diffusion models that employ U-Net. However, the improved performance of DiTs comes at the expense of higher parameter counts and implementation costs, which significantly limits their deployment on resource-constrained devices like mobile phones. We propose DiTAS, a data-free post-training quantization (PTQ) method for efficient DiT inference. DiTAS relies on the proposed temporal-aggregated smoothing techniques to mitigate the impact of the channel-wise outliers within the input activations, leading to much lower quantization error under extremely low bitwidth. To further enhance the performance of the quantized DiT, we adopt the layer-wise grid search strategy to optimize the smoothing factor. Experimental results demonstrate that our approach enables 4-bit weight, 8-bit activation (W4A8) quantization for DiTs while maintaining comparable performance as the full-precision model.

**Link**: [arxiv](http://arxiv.org/abs/2409.07756v1),  [pdf](http://arxiv.org/pdf/2409.07756v1)

**Tags**: cs.CV 



### Efficient Privacy-Preserving KAN Inference Using Homomorphic Encryption
**Authors**: Zhizheng Lai, Yufei Zhou, Peijia Zheng, Lin Chen

**Updated**: 2024-09-12T04:51:27Z

**Summary**: The recently proposed Kolmogorov-Arnold Networks (KANs) offer enhanced interpretability and greater model expressiveness. However, KANs also present challenges related to privacy leakage during inference. Homomorphic encryption (HE) facilitates privacy-preserving inference for deep learning models, enabling resource-limited users to benefit from deep learning services while ensuring data security. Yet, the complex structure of KANs, incorporating nonlinear elements like the SiLU activation function and B-spline functions, renders existing privacy-preserving inference techniques inadequate. To address this issue, we propose an accurate and efficient privacy-preserving inference scheme tailored for KANs. Our approach introduces a task-specific polynomial approximation for the SiLU activation function, dynamically adjusting the approximation range to ensure high accuracy on real-world datasets. Additionally, we develop an efficient method for computing B-spline functions within the HE domain, leveraging techniques such as repeat packing, lazy combination, and comparison functions. We evaluate the effectiveness of our privacy-preserving KAN inference scheme on both symbolic formula evaluation and image classification. The experimental results show that our model achieves accuracy comparable to plaintext KANs across various datasets and outperforms plaintext MLPs. Additionally, on the CIFAR-10 dataset, our inference latency achieves over 7 times speedup compared to the naive method.

**Link**: [arxiv](http://arxiv.org/abs/2409.07751v1),  [pdf](http://arxiv.org/pdf/2409.07751v1)

**Tags**: cs.LG cs.CR 



### On Leveraging Large Language Models for Enhancing Entity Resolution: A   Cost-efficient Approach
**Authors**: Huahang Li, Longyu Feng, Shuangyin Li, Fei Hao, Chen Jason Zhang, Yuanfeng Song

**Updated**: 2024-09-12T04:47:33Z

**Summary**: Entity resolution, the task of identifying and merging records that refer to the same real-world entity, is crucial in sectors like e-commerce, healthcare, and law enforcement. Large Language Models (LLMs) introduce an innovative approach to this task, capitalizing on their advanced linguistic capabilities and a ``pay-as-you-go'' model that provides significant advantages to those without extensive data science expertise. However, current LLMs are costly due to per-API request billing. Existing methods often either lack quality or become prohibitively expensive at scale. To address these problems, we propose an uncertainty reduction framework using LLMs to improve entity resolution results. We first initialize possible partitions of the entity cluster, refer to the same entity, and define the uncertainty of the result. Then, we reduce the uncertainty by selecting a few valuable matching questions for LLM verification. Upon receiving the answers, we update the probability distribution of the possible partitions. To further reduce costs, we design an efficient algorithm to judiciously select the most valuable matching pairs to query. Additionally, we create error-tolerant techniques to handle LLM mistakes and a dynamic adjustment method to reach truly correct partitions. Experimental results show that our method is efficient and effective, offering promising applications in real-world tasks.

**Link**: [arxiv](http://arxiv.org/abs/2401.03426v2),  [pdf](http://arxiv.org/pdf/2401.03426v2)

**Tags**: cs.CL cs.AI 



### LOCKEY: A Novel Approach to Model Authentication and Deepfake Tracking
**Authors**: Mayank Kumar Singh, Naoya Takahashi, Wei-Hsiang Liao, Yuki Mitsufuji

**Updated**: 2024-09-12T04:28:22Z

**Summary**: This paper presents a novel approach to deter unauthorized deepfakes and enable user tracking in generative models, even when the user has full access to the model parameters, by integrating key-based model authentication with watermarking techniques. Our method involves providing users with model parameters accompanied by a unique, user-specific key. During inference, the model is conditioned upon the key along with the standard input. A valid key results in the expected output, while an invalid key triggers a degraded output, thereby enforcing key-based model authentication. For user tracking, the model embeds the user's unique key as a watermark within the generated content, facilitating the identification of the user's ID. We demonstrate the effectiveness of our approach on two types of models, audio codecs and vocoders, utilizing the SilentCipher watermarking method. Additionally, we assess the robustness of the embedded watermarks against various distortions, validating their reliability in various scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2409.07743v1),  [pdf](http://arxiv.org/pdf/2409.07743v1)

**Tags**: cs.CR 



### Handling expression evaluation under interference
**Authors**: Ian J. Hayes, Cliff B. Jones, Larissa A. Meinicke

**Updated**: 2024-09-12T04:16:22Z

**Summary**: Hoare-style inference rules for program constructs permit the copying of expressions and tests from program text into logical contexts. It is known that this requires care even for sequential programs but further issues arise for concurrent programs because of potential interference to the values of variables. The "rely-guarantee" approach does tackle the issue of recording acceptable interference and offers a way to provide safe inference rules. This paper shows how the algebraic presentation of rely-guarantee ideas can clarify and formalise the conditions for safely re-using expressions and tests from program text in logical contexts for reasoning about programs.

**Link**: [arxiv](http://arxiv.org/abs/2409.07741v1),  [pdf](http://arxiv.org/pdf/2409.07741v1)

**Tags**: cs.LO cs.SE F.3.1; D.1.3 



### A model-based approach for clustering binned data
**Authors**: Asael Fabian Martínez, Carlos Díaz-Avalos

**Updated**: 2024-09-12T04:09:09Z

**Summary**: Binned data often appears in different fields of research, and it is generated after summarizing the original data in a sequence of pairs of bins (or their midpoints) and frequencies. There may exist different reasons to only provide this summary, but more importantly, it is necessary being able to perform statistical analyses based only on it. We present a Bayesian nonparametric model for clustering applicable for binned data. Clusters are modeled via random partitions, and within them a model-based approach is assumed. Inferences are performed by a Markov chain Monte Carlo method and the complete proposal is tested using simulated and real data. Having particular interest in studying marine populations, we analyze samples of Lobatus (Strobus) gigas' lengths and found the presence of up to three cohorts along the year.

**Link**: [arxiv](http://arxiv.org/abs/2409.07738v1),  [pdf](http://arxiv.org/pdf/2409.07738v1)

**Tags**: stat.ME math.ST stat.TH 



### Ruri: Japanese General Text Embeddings
**Authors**: Hayato Tsukagoshi, Ryohei Sasano

**Updated**: 2024-09-12T04:06:31Z

**Summary**: We report the development of Ruri, a series of Japanese general text embedding models. While the development of general-purpose text embedding models in English and multilingual contexts has been active in recent years, model development in Japanese remains insufficient. The primary reasons for this are the lack of datasets and the absence of necessary expertise. In this report, we provide a detailed account of the development process of Ruri. Specifically, we discuss the training of embedding models using synthesized datasets generated by LLMs, the construction of the reranker for dataset filtering and knowledge distillation, and the performance evaluation of the resulting general-purpose text embedding models.

**Link**: [arxiv](http://arxiv.org/abs/2409.07737v1),  [pdf](http://arxiv.org/pdf/2409.07737v1)

**Tags**: cs.CL 



### Large Language Models are Pattern Matchers: Editing Semi-Structured and   Structured Documents with ChatGPT
**Authors**: Irene Weber

**Updated**: 2024-09-12T03:41:39Z

**Summary**: Large Language Models (LLMs) offer numerous applications, the full extent of which is not yet understood. This paper investigates if LLMs can be applied for editing structured and semi-structured documents with minimal effort. Using a qualitative research approach, we conduct two case studies with ChatGPT and thoroughly analyze the results. Our experiments indicate that LLMs can effectively edit structured and semi-structured documents when provided with basic, straightforward prompts. ChatGPT demonstrates a strong ability to recognize and process the structure of annotated documents. This suggests that explicitly structuring tasks and data in prompts might enhance an LLM's ability to understand and solve tasks. Furthermore, the experiments also reveal impressive pattern matching skills in ChatGPT. This observation deserves further investigation, as it may contribute to understanding the processes leading to hallucinations in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2409.07732v1),  [pdf](http://arxiv.org/pdf/2409.07732v1)

**Tags**: cs.LG cs.AI cs.CL I.2 



### Identification and multiply robust estimation in causal mediation   analysis across principal strata
**Authors**: Chao Cheng, Fan Li

**Updated**: 2024-09-12T03:17:18Z

**Summary**: We consider assessing causal mediation in the presence of a post-treatment event (examples include noncompliance, a clinical event, or death). We identify natural mediation effects for the entire study population and for each principal stratum characterized by the joint potential values of the post-treatment event. We derive the efficient influence function for each mediation estimand, which motivates a set of multiply robust estimators for inference. The multiply robust estimators are consistent under four types of misspecifications and are efficient when all nuisance models are correctly specified. We also develop a nonparametric efficient estimator that leverages data-adaptive machine learners to achieve efficient inference and discuss sensitivity methods to address key identification assumptions. We illustrate our methods via simulations and two real data examples.

**Link**: [arxiv](http://arxiv.org/abs/2304.10025v4),  [pdf](http://arxiv.org/pdf/2304.10025v4)

**Tags**: stat.ME stat.ML 



### LLM4VG: Large Language Models Evaluation for Video Grounding
**Authors**: Wei Feng, Xin Wang, Hong Chen, Zeyang Zhang, Houlun Chen, Zihan Song, Yuwei Zhou, Yuekui Yang, Haiyang Wu, Wenwu Zhu

**Updated**: 2024-09-12T02:57:07Z

**Summary**: Recently, researchers have attempted to investigate the capability of LLMs in handling videos and proposed several video LLM models. However, the ability of LLMs to handle video grounding (VG), which is an important time-related video task requiring the model to precisely locate the start and end timestamps of temporal moments in videos that match the given textual queries, still remains unclear and unexplored in literature. To fill the gap, in this paper, we propose the LLM4VG benchmark, which systematically evaluates the performance of different LLMs on video grounding tasks. Based on our proposed LLM4VG, we design extensive experiments to examine two groups of video LLM models on video grounding: (i) the video LLMs trained on the text-video pairs (denoted as VidLLM), and (ii) the LLMs combined with pretrained visual description models such as the video/image captioning model. We propose prompt methods to integrate the instruction of VG and description from different kinds of generators, including caption-based generators for direct visual description and VQA-based generators for information enhancement. We also provide comprehensive comparisons of various VidLLMs and explore the influence of different choices of visual models, LLMs, prompt designs, etc, as well. Our experimental evaluations lead to two conclusions: (i) the existing VidLLMs are still far away from achieving satisfactory video grounding performance, and more time-related video tasks should be included to further fine-tune these models, and (ii) the combination of LLMs and visual models shows preliminary abilities for video grounding with considerable potential for improvement by resorting to more reliable models and further guidance of prompt instructions.

**Link**: [arxiv](http://arxiv.org/abs/2312.14206v3),  [pdf](http://arxiv.org/pdf/2312.14206v3)

**Tags**: cs.CV 



### Experimenting with Legal AI Solutions: The Case of Question-Answering   for Access to Justice
**Authors**: Jonathan Li, Rohan Bhambhoria, Samuel Dahan, Xiaodan Zhu

**Updated**: 2024-09-12T02:40:28Z

**Summary**: Generative AI models, such as the GPT and Llama series, have significant potential to assist laypeople in answering legal questions. However, little prior work focuses on the data sourcing, inference, and evaluation of these models in the context of laypersons. To this end, we propose a human-centric legal NLP pipeline, covering data sourcing, inference, and evaluation. We introduce and release a dataset, LegalQA, with real and specific legal questions spanning from employment law to criminal law, corresponding answers written by legal experts, and citations for each answer. We develop an automatic evaluation protocol for this dataset, then show that retrieval-augmented generation from only 850 citations in the train set can match or outperform internet-wide retrieval, despite containing 9 orders of magnitude less data. Finally, we propose future directions for open-sourced efforts, which fall behind closed-sourced models.

**Link**: [arxiv](http://arxiv.org/abs/2409.07713v1),  [pdf](http://arxiv.org/pdf/2409.07713v1)

**Tags**: cs.CL 



### Attack End-to-End Autonomous Driving through Module-Wise Noise
**Authors**: Lu Wang, Tianyuan Zhang, Yikai Han, Muyang Fang, Ting Jin, Jiaqi Kang

**Updated**: 2024-09-12T02:19:16Z

**Summary**: With recent breakthroughs in deep neural networks, numerous tasks within autonomous driving have exhibited remarkable performance. However, deep learning models are susceptible to adversarial attacks, presenting significant security risks to autonomous driving systems. Presently, end-to-end architectures have emerged as the predominant solution for autonomous driving, owing to their collaborative nature across different tasks. Yet, the implications of adversarial attacks on such models remain relatively unexplored. In this paper, we conduct comprehensive adversarial security research on the modular end-to-end autonomous driving model for the first time. We thoroughly consider the potential vulnerabilities in the model inference process and design a universal attack scheme through module-wise noise injection. We conduct large-scale experiments on the full-stack autonomous driving model and demonstrate that our attack method outperforms previous attack methods. We trust that our research will offer fresh insights into ensuring the safety and reliability of autonomous driving systems.

**Link**: [arxiv](http://arxiv.org/abs/2409.07706v1),  [pdf](http://arxiv.org/pdf/2409.07706v1)

**Tags**: cs.LG cs.AI 



### DSBench: How Far Are Data Science Agents to Becoming Data Science   Experts?
**Authors**: Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, Dong Yu

**Updated**: 2024-09-12T02:08:00Z

**Summary**: Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have demonstrated impressive language/vision reasoning abilities, igniting the recent trend of building agents for targeted applications such as shopping assistants or AI software engineers. Recently, many data science benchmarks have been proposed to investigate their performance in the data science domain. However, existing data science benchmarks still fall short when compared to real-world data science applications due to their simplified settings. To bridge this gap, we introduce DSBench, a comprehensive benchmark designed to evaluate data science agents with realistic tasks. This benchmark includes 466 data analysis tasks and 74 data modeling tasks, sourced from Eloquence and Kaggle competitions. DSBench offers a realistic setting by encompassing long contexts, multimodal task backgrounds, reasoning with large data files and multi-table structures, and performing end-to-end data modeling tasks. Our evaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle with most tasks, with the best agent solving only 34.12% of data analysis tasks and achieving a 34.74% Relative Performance Gap (RPG). These findings underscore the need for further advancements in developing more practical, intelligent, and autonomous data science agents.

**Link**: [arxiv](http://arxiv.org/abs/2409.07703v1),  [pdf](http://arxiv.org/pdf/2409.07703v1)

**Tags**: cs.AI cs.CL 



### Cooperative Inference with Interleaved Operator Partitioning for CNNs
**Authors**: Zhibang Liu, Chaonong Xu, Zhizhuo Liu, Lekai Huang, Jiachen Wei, Chao Li

**Updated**: 2024-09-12T01:55:08Z

**Summary**: Deploying deep learning models on Internet of Things (IoT) devices often faces challenges due to limited memory resources and computing capabilities. Cooperative inference is an important method for addressing this issue, requiring the partitioning and distributive deployment of an intelligent model. To perform horizontal partitions, existing cooperative inference methods take either the output channel of operators or the height and width of feature maps as the partition dimensions. In this manner, since the activation of operators is distributed, they have to be concatenated together before being fed to the next operator, which incurs the delay for cooperative inference. In this paper, we propose the Interleaved Operator Partitioning (IOP) strategy for CNN models. By partitioning an operator based on the output channel dimension and its successive operator based on the input channel dimension, activation concatenation becomes unnecessary, thereby reducing the number of communication connections, which consequently reduces cooperative inference de-lay. Based on IOP, we further present a model segmentation algorithm for minimizing cooperative inference time, which greedily selects operators for IOP pairing based on the inference delay benefit harvested. Experimental results demonstrate that compared with the state-of-the-art partition approaches used in CoEdge, the IOP strategy achieves 6.39% ~ 16.83% faster acceleration and reduces peak memory footprint by 21.22% ~ 49.98% for three classical image classification models.

**Link**: [arxiv](http://arxiv.org/abs/2409.07693v1),  [pdf](http://arxiv.org/pdf/2409.07693v1)

**Tags**: cs.DC 



### Representational Analysis of Binding in Large Language Models
**Authors**: Qin Dai, Benjamin Heinzerling, Kentaro Inui

**Updated**: 2024-09-12T01:32:25Z

**Summary**: Entity tracking is essential for complex reasoning. To perform in-context entity tracking, language models (LMs) must bind an entity to its attribute (e.g., bind a container to its content) to recall attribute for a given entity. For example, given a context mentioning ``The coffee is in Box Z, the stone is in Box M, the map is in Box H'', to infer ``Box Z contains the coffee'' later, LMs must bind ``Box Z'' to ``coffee''. To explain the binding behaviour of LMs, Feng and Steinhardt (2023) introduce a Binding ID mechanism and state that LMs use a abstract concept called Binding ID (BI) to internally mark entity-attribute pairs. However, they have not directly captured the BI determinant information from entity activations. In this work, we provide a novel view of the Binding ID mechanism by localizing the prototype of BI information. Specifically, we discover that there exists a low-rank subspace in the hidden state (or activation) of LMs, that primarily encodes the order of entity and attribute and which is used as the prototype of BI to causally determine the binding. To identify this subspace, we choose principle component analysis as our first attempt and it is empirically proven to be effective. Moreover, we also discover that when editing representations along directions in the subspace, LMs tend to bind a given entity to other attributes accordingly. For example, by patching activations along the BI encoding direction we can make the LM to infer ``Box Z contains the stone'' and ``Box Z contains the map''.

**Link**: [arxiv](http://arxiv.org/abs/2409.05448v2),  [pdf](http://arxiv.org/pdf/2409.05448v2)

**Tags**: cs.CL 



### How to achieve model-robust inference in stepped wedge trials with   model-based methods?
**Authors**: Bingkai Wang, Xueqi Wang, Fan Li

**Updated**: 2024-09-12T00:47:06Z

**Summary**: A stepped wedge design is a unidirectional crossover design where clusters are randomized to distinct treatment sequences. While model-based analysis of stepped wedge designs is standard practice to evaluate treatment effects accounting for clustering and adjusting for covariates, their properties under misspecification have not been systematically explored. In this article, we focus on model-based methods, including linear mixed models and generalized estimating equations with an independence, simple exchangeable, or nested exchangeable working correlation structure. We study when a potentially misspecified working model can offer consistent estimation of the marginal treatment effect estimands, which are defined nonparametrically with potential outcomes and may be functions of calendar time and/or exposure time. We prove a central result that consistency for nonparametric estimands usually requires a correctly specified treatment effect structure, but generally not the remaining aspects of the working model (functional form of covariates, random effects, and residual distribution), and valid inference is obtained via the sandwich variance estimator. Furthermore, an additional g-computation step is required to achieve model-robust inference under non-identity link functions or for ratio estimands. The theoretical results are illustrated via several simulation experiments and re-analysis of a completed stepped wedge cluster randomized trial.

**Link**: [arxiv](http://arxiv.org/abs/2401.15680v4),  [pdf](http://arxiv.org/pdf/2401.15680v4)

**Tags**: stat.ME 



### A Survey of Backdoor Attacks and Defenses on Large Language Models:   Implications for Security Measures
**Authors**: Shuai Zhao, Meihuizi Jia, Zhongliang Guo, Leilei Gan, Xiaoyu Xu, Xiaobao Wu, Jie Fu, Yichao Feng, Fengjun Pan, Luu Anh Tuan

**Updated**: 2024-09-12T00:27:06Z

**Summary**: Large Language Models (LLMs), which bridge the gap between human language understanding and complex problem-solving, achieve state-of-the-art performance on several NLP tasks, particularly in few-shot and zero-shot settings. Despite the demonstrable efficacy of LLMs, due to constraints on computational resources, users have to engage with open-source language models or outsource the entire training process to third-party platforms. However, research has demonstrated that language models are susceptible to potential security vulnerabilities, particularly in backdoor attacks. Backdoor attacks are designed to introduce targeted vulnerabilities into language models by poisoning training samples or model weights, allowing attackers to manipulate model responses through malicious triggers. While existing surveys on backdoor attacks provide a comprehensive overview, they lack an in-depth examination of backdoor attacks specifically targeting LLMs. To bridge this gap and grasp the latest trends in the field, this paper presents a novel perspective on backdoor attacks for LLMs by focusing on fine-tuning methods. Specifically, we systematically classify backdoor attacks into three categories: full-parameter fine-tuning, parameter-efficient fine-tuning, and no fine-tuning Based on insights from a substantial review, we also discuss crucial issues for future research on backdoor attacks, such as further exploring attack algorithms that do not require fine-tuning, or developing more covert attack algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2406.06852v4),  [pdf](http://arxiv.org/pdf/2406.06852v4)

**Tags**: cs.CR cs.AI cs.CL 



### Estimation and Inference for Three-Dimensional Panel Data Models
**Authors**: Guohua Feng, Jiti Gao, Fei Liu, Bin Peng

**Updated**: 2024-09-12T00:19:19Z

**Summary**: Hierarchical panel data models have recently garnered significant attention. This study contributes to the relevant literature by introducing a novel three-dimensional (3D) hierarchical panel data model, which integrates panel regression with three sets of latent factor structures: one set of global factors and two sets of local factors. Instead of aggregating latent factors from various nodes, as seen in the literature of distributed principal component analysis (PCA), we propose an estimation approach capable of recovering the parameters of interest and disentangling latent factors at different levels and across different dimensions. We establish an asymptotic theory and provide a bootstrap procedure to obtain inference for the parameters of interest while accommodating various types of cross-sectional dependence and time series autocorrelation. Finally, we demonstrate the applicability of our framework by examining productivity convergence in manufacturing industries worldwide.

**Link**: [arxiv](http://arxiv.org/abs/2404.08365v2),  [pdf](http://arxiv.org/pdf/2404.08365v2)

**Tags**: econ.EM 



### Unsupervised anomaly detection in spatio-temporal stream network sensor   data
**Authors**: Edgar Santos-Fernandez, Jay M. Ver Hoef, Erin E. Peterson, James McGree, Cesar A. Villa, Catherine Leigh, Ryan Turner, Cameron Roberts, Kerrie Mengersen

**Updated**: 2024-09-11T23:59:59Z

**Summary**: The use of in-situ digital sensors for water quality monitoring is becoming increasingly common worldwide. While these sensors provide near real-time data for science, the data are prone to technical anomalies that can undermine the trustworthiness of the data and the accuracy of statistical inferences, particularly in spatial and temporal analyses. Here we propose a framework for detecting anomalies in sensor data recorded in stream networks, which takes advantage of spatial and temporal autocorrelation to improve detection rates. The proposed framework involves the implementation of effective data imputation to handle missing data, alignment of time-series to address temporal disparities, and the identification of water quality events. We explore the effectiveness of a suite of state-of-the-art statistical methods including posterior predictive distributions, finite mixtures, and Hidden Markov Models (HMM). We showcase the practical implementation of automated anomaly detection in near-real time by employing a Bayesian recursive approach. This demonstration is conducted through a comprehensive simulation study and a practical application to a substantive case study situated in the Herbert River, located in Queensland, Australia, which flows into the Great Barrier Reef. We found that methods such as posterior predictive distributions and HMM produce the best performance in detecting multiple types of anomalies. Utilizing data from multiple sensors deployed relatively near one another enhances the ability to distinguish between water quality events and technical anomalies, thereby significantly improving the accuracy of anomaly detection. Thus, uncertainty and biases in water quality reporting, interpretation, and modelling are reduced, and the effectiveness of subsequent management actions improved.

**Link**: [arxiv](http://arxiv.org/abs/2409.07667v1),  [pdf](http://arxiv.org/pdf/2409.07667v1)

**Tags**: stat.AP 



### SparseCoder: Advancing Source Code Analysis with Sparse Attention and   Learned Token Pruning
**Authors**: Xueqi Yang, Mariusz Jakubowski, Li Kang, Haojie Yu, Tim Menzies

**Updated**: 2024-09-11T23:15:44Z

**Summary**: As software projects rapidly evolve, software artifacts become more complex and defects behind get harder to identify. The emerging Transformer-based approaches, though achieving remarkable performance, struggle with long code sequences due to their self-attention mechanism, which scales quadratically with the sequence length. This paper introduces SparseCoder, an innovative approach incorporating sparse attention and learned token pruning (LTP) method (adapted from natural language processing) to address this limitation. Compared to previous state-of-the-art models CodeBERT, RoBERTa, and CodeT5, our experiments demonstrate that SparseCoder can handle significantly longer input sequences--at least twice as long, within the limits of our hardware resources and data statistics. Additionally, SparseCoder is four times faster than other methods measured in runtime, achieving a 50% reduction in floating point operations per second (FLOPs) with a negligible performance drop of less than 1% compared to Transformers using sparse attention (Sparse Atten). Plotting FLOPs of model inference against token lengths reveals that SparseCoder scales linearly, whereas other methods, including the current state-of-the-art model CodeT5, scale quadratically. Moreover, SparseCoder enhances interpretability by visualizing non-trivial tokens layer-wise.

**Link**: [arxiv](http://arxiv.org/abs/2310.07109v2),  [pdf](http://arxiv.org/pdf/2310.07109v2)

**Tags**: cs.SE 



### VidLPRO: A $\underline{Vid}$eo-$\underline{L}$anguage   $\underline{P}$re-training Framework for $\underline{Ro}$botic and   Laparoscopic Surgery
**Authors**: Mohammadmahdi Honarmand, Muhammad Abdullah Jamal, Omid Mohareri

**Updated**: 2024-09-11T23:12:53Z

**Summary**: We introduce VidLPRO, a novel video-language (VL) pre-training framework designed specifically for robotic and laparoscopic surgery. While existing surgical VL models primarily rely on contrastive learning, we propose a more comprehensive approach to capture the intricate temporal dynamics and align video with language. VidLPRO integrates video-text contrastive learning, video-text matching, and masked language modeling objectives to learn rich VL representations. To support this framework, we present GenSurg+, a carefully curated dataset derived from GenSurgery, comprising 17k surgical video clips paired with captions generated by GPT-4 using transcripts extracted by the Whisper model. This dataset addresses the need for large-scale, high-quality VL data in the surgical domain. Extensive experiments on benchmark datasets, including Cholec80 and AutoLaparo, demonstrate the efficacy of our approach. VidLPRO achieves state-of-the-art performance in zero-shot surgical phase recognition, significantly outperforming existing surgical VL models such as SurgVLP and HecVL. Our model demonstrates improvements of up to 21.5\% in accuracy and 15.7% in F1 score, setting a new benchmark in the field. Notably, VidLPRO exhibits robust performance even with single-frame inference, while effectively scaling with increased temporal context. Ablation studies reveal the impact of frame sampling strategies on model performance and computational efficiency. These results underscore VidLPRO's potential as a foundation model for surgical video understanding.

**Link**: [arxiv](http://arxiv.org/abs/2409.04732v2),  [pdf](http://arxiv.org/pdf/2409.04732v2)

**Tags**: cs.CV cs.AI 



### SimulBench: Evaluating Language Models with Creative Simulation Tasks
**Authors**: Qi Jia, Xiang Yue, Tianyu Zheng, Jie Huang, Bill Yuchen Lin

**Updated**: 2024-09-11T21:53:20Z

**Summary**: We introduce SimulBench, a benchmark designed to evaluate large language models (LLMs) across a diverse collection of creative simulation scenarios, such as acting as a Linux terminal or playing text games with users. While these simulation tasks serve as effective measures of an LLM's general intelligence, they are seldom incorporated into existing benchmarks. A major challenge is to develop an evaluation framework for testing different LLMs fairly while preserving the multi-round interactive nature of simulation tasks between users and AI. To tackle this issue, we suggest using a fixed LLM as a user agent to engage with an LLM to collect dialogues first under different tasks. Then, challenging dialogue scripts are extracted for evaluating different target LLMs. To facilitate automatic assessment on \DataName{}, GPT-4 is employed as the evaluator, tasked with reviewing the quality of the final response generated by the target LLMs given multi-turn dialogue scripts. Our comprehensive experiments indicate that these simulation tasks continue to pose a significant challenge with their unique natures and show the gap between proprietary models and the most advanced open LLMs. For example, GPT-4-turbo outperforms LLaMA-3-70b-Chat on 18.55\% more cases.

**Link**: [arxiv](http://arxiv.org/abs/2409.07641v1),  [pdf](http://arxiv.org/pdf/2409.07641v1)

**Tags**: cs.CL 



### Continual Skill and Task Learning via Dialogue
**Authors**: Weiwei Gu, Suresh Kondepudi, Lixiao Huang, Nakul Gopalan

**Updated**: 2024-09-11T21:52:22Z

**Summary**: Continual and interactive robot learning is a challenging problem as the robot is present with human users who expect the robot to learn novel skills to solve novel tasks perpetually with sample efficiency. In this work we present a framework for robots to query and learn visuo-motor robot skills and task relevant information via natural language dialog interactions with human users. Previous approaches either focus on improving the performance of instruction following agents, or passively learn novel skills or concepts. Instead, we used dialog combined with a language-skill grounding embedding to query or confirm skills and/or tasks requested by a user. To achieve this goal, we developed and integrated three different components for our agent. Firstly, we propose a novel visual-motor control policy ACT with Low Rank Adaptation (ACT-LoRA), which enables the existing SoTA ACT model to perform few-shot continual learning. Secondly, we develop an alignment model that projects demonstrations across skill embodiments into a shared embedding allowing us to know when to ask questions and/or demonstrations from users. Finally, we integrated an existing LLM to interact with a human user to perform grounded interactive continual skill learning to solve a task. Our ACT-LoRA model learns novel fine-tuned skills with a 100% accuracy when trained with only five demonstrations for a novel skill while still maintaining a 74.75% accuracy on pre-trained skills in the RLBench dataset where other models fall significantly short. We also performed a human-subjects study with 8 subjects to demonstrate the continual learning capabilities of our combined framework. We achieve a success rate of 75% in the task of sandwich making with the real robot learning from participant data demonstrating that robots can learn novel skills or task knowledge from dialogue with non-expert users using our approach.

**Link**: [arxiv](http://arxiv.org/abs/2409.03166v2),  [pdf](http://arxiv.org/pdf/2409.03166v2)

**Tags**: cs.RO cs.AI cs.CL 



### Can We Count on LLMs? The Fixed-Effect Fallacy and Claims of GPT-4   Capabilities
**Authors**: Thomas Ball, Shuo Chen, Cormac Herley

**Updated**: 2024-09-11T21:48:33Z

**Summary**: In this paper we explore evaluation of LLM capabilities. We present measurements of GPT-4 performance on several deterministic tasks; each task involves a basic calculation and takes as input parameter some element drawn from a large well-defined population (e.g., count elements in a list, multiply two k-digit numbers, etc). We examine several conditions per-task and perform enough trials so that statistically significant differences can be detected. This allows us to investigate the sensitivity of task-accuracy both to query phrasing and input parameter population. We find that seemingly trivial modifications in the task-prompt or input population can yield differences far larger than can be explained by sampling effects. For example, performance on a simple list-counting task varies with query-phrasing and list-length, but also with list composition (i.e., the thing-to-be-counted) and object frequency (e.g., success when an element accounts for $\approx$ 50\% of a list is different from when it accounts for $\approx$ 70\% etc).   We conclude that efforts to quantify LLM capabilities easily succumb to the language-as-fixed-effect fallacy, where experimental observations are improperly generalized beyond what the data supports. A consequence appears to be that intuitions that have been formed based on interactions with humans form a very unreliable guide as to which input modifications should ``make no difference'' to LLM performance.

**Link**: [arxiv](http://arxiv.org/abs/2409.07638v1),  [pdf](http://arxiv.org/pdf/2409.07638v1)

**Tags**: cs.AI cs.CL cs.LG 



### Can ChatGPT Forecast Stock Price Movements? Return Predictability and   Large Language Models
**Authors**: Alejandro Lopez-Lira, Yuehua Tang

**Updated**: 2024-09-11T21:23:04Z

**Summary**: We document the capability of large language models (LLMs) like ChatGPT to predict stock price movements using news headlines, even without direct financial training. ChatGPT scores significantly predict out-of-sample daily stock returns, subsuming traditional methods, and predictability is stronger among smaller stocks and following negative news. To explain these findings, we develop a theoretical model incorporating information capacity constraints, underreaction, limits-to-arbitrage, and LLMs. The model generates several key predictions, which we empirically test: (i) it establishes a critical threshold in AI capabilities necessary for profitable predictions, (ii) it demonstrates that only advanced LLMs can effectively interpret complex information, and (iii) it predicts that widespread LLM adoption can enhance market efficiency. Our results suggest that sophisticated return forecasting is an emerging capability of AI systems and that these technologies can alter information diffusion and decision-making processes in financial markets. Finally, we introduce an interpretability framework to evaluate LLMs' reasoning, contributing to AI transparency and economic decision-making.

**Link**: [arxiv](http://arxiv.org/abs/2304.07619v5),  [pdf](http://arxiv.org/pdf/2304.07619v5)

**Tags**: q-fin.ST cs.CL 



### A Training Rate and Survival Heuristic for Inference and Robustness   Evaluation (TRASHFIRE)
**Authors**: Charles Meyers, Mohammad Reza Saleh Sedghpour, Tommy Löfstedt, Erik Elmroth

**Updated**: 2024-09-11T20:55:32Z

**Summary**: Machine learning models -- deep neural networks in particular -- have performed remarkably well on benchmark datasets across a wide variety of domains. However, the ease of finding adversarial counter-examples remains a persistent problem when training times are measured in hours or days and the time needed to find a successful adversarial counter-example is measured in seconds. Much work has gone into generating and defending against these adversarial counter-examples, however the relative costs of attacks and defences are rarely discussed. Additionally, machine learning research is almost entirely guided by test/train metrics, but these would require billions of samples to meet industry standards. The present work addresses the problem of understanding and predicting how particular model hyper-parameters influence the performance of a model in the presence of an adversary. The proposed approach uses survival models, worst-case examples, and a cost-aware analysis to precisely and accurately reject a particular model change during routine model training procedures rather than relying on real-world deployment, expensive formal verification methods, or accurate simulations of very complicated systems (\textit{e.g.}, digitally recreating every part of a car or a plane). Through an evaluation of many pre-processing techniques, adversarial counter-examples, and neural network configurations, the conclusion is that deeper models do offer marginal gains in survival times compared to more shallow counterparts. However, we show that those gains are driven more by the model inference time than inherent robustness properties. Using the proposed methodology, we show that ResNet is hopelessly insecure against even the simplest of white box attacks.

**Link**: [arxiv](http://arxiv.org/abs/2401.13751v2),  [pdf](http://arxiv.org/pdf/2401.13751v2)

**Tags**: cs.LG cs.AI cs.CV stat.ML 



### Zero-Shot Machine-Generated Text Detection Using Mixture of Large   Language Models
**Authors**: Matthieu Dubois, François Yvon, Pablo Piantanida

**Updated**: 2024-09-11T20:55:12Z

**Summary**: The dissemination of Large Language Models (LLMs), trained at scale, and endowed with powerful text-generating abilities has vastly increased the threats posed by generative AI technologies by reducing the cost of producing harmful, toxic, faked or forged content. In response, various proposals have been made to automatically discriminate artificially generated from human-written texts, typically framing the problem as a classification problem. Most approaches evaluate an input document by a well-chosen detector LLM, assuming that low-perplexity scores reliably signal machine-made content. As using one single detector can induce brittleness of performance, we instead consider several and derive a new, theoretically grounded approach to combine their respective strengths. Our experiments, using a variety of generator LLMs, suggest that our method effectively increases the robustness of detection.

**Link**: [arxiv](http://arxiv.org/abs/2409.07615v1),  [pdf](http://arxiv.org/pdf/2409.07615v1)

**Tags**: cs.CL 



### FlowSep: Language-Queried Sound Separation with Rectified Flow Matching
**Authors**: Yi Yuan, Xubo Liu, Haohe Liu, Mark D. Plumbley, Wenwu Wang

**Updated**: 2024-09-11T20:54:23Z

**Summary**: Language-queried audio source separation (LASS) focuses on separating sounds using textual descriptions of the desired sources. Current methods mainly use discriminative approaches, such as time-frequency masking, to separate target sounds and minimize interference from other sources. However, these models face challenges when separating overlapping soundtracks, which may lead to artifacts such as spectral holes or incomplete separation. Rectified flow matching (RFM), a generative model that establishes linear relations between the distribution of data and noise, offers superior theoretical properties and simplicity, but has not yet been explored in sound separation. In this work, we introduce FlowSep, a new generative model based on RFM for LASS tasks. FlowSep learns linear flow trajectories from noise to target source features within the variational autoencoder (VAE) latent space. During inference, the RFM-generated latent features are reconstructed into a mel-spectrogram via the pre-trained VAE decoder, followed by a pre-trained vocoder to synthesize the waveform. Trained on 1,680 hours of audio data, FlowSep outperforms the state-of-the-art models across multiple benchmarks, as evaluated with subjective and objective metrics. Additionally, our results show that FlowSep surpasses a diffusion-based LASS model in both separation quality and inference efficiency, highlighting its strong potential for audio source separation tasks. Code, pre-trained models and demos can be found at: https://audio-agi.github.io/FlowSep_demo/.

**Link**: [arxiv](http://arxiv.org/abs/2409.07614v1),  [pdf](http://arxiv.org/pdf/2409.07614v1)

**Tags**: cs.SD eess.AS 



### Token Turing Machines are Efficient Vision Models
**Authors**: Purvish Jajal, Nick John Eliopoulos, Benjamin Shiue-Hal Chou, George K. Thiravathukal, James C. Davis, Yung-Hsiang Lu

**Updated**: 2024-09-11T20:50:41Z

**Summary**: We propose Vision Token Turing Machines (ViTTM), an efficient, low-latency, memory-augmented Vision Transformer (ViT). Our approach builds on Neural Turing Machines and Token Turing Machines, which were applied to NLP and sequential visual understanding tasks. ViTTMs are designed for non-sequential computer vision tasks such as image classification and segmentation. Our model creates two sets of tokens: process tokens and memory tokens; process tokens pass through encoder blocks and read-write from memory tokens at each encoder block in the network, allowing them to store and retrieve information from memory. By ensuring that there are fewer process tokens than memory tokens, we are able to reduce the inference time of the network while maintaining its accuracy. On ImageNet-1K, the state-of-the-art ViT-B has median latency of 529.5ms and 81.0% accuracy, while our ViTTM-B is 56% faster (234.1ms), with 2.4 times fewer FLOPs, with an accuracy of 82.9%. On ADE20K semantic segmentation, ViT-B achieves 45.65mIoU at 13.8 frame-per-second (FPS) whereas our ViTTM-B model acheives a 45.17 mIoU with 26.8 FPS (+94%).

**Link**: [arxiv](http://arxiv.org/abs/2409.07613v1),  [pdf](http://arxiv.org/pdf/2409.07613v1)

**Tags**: cs.CV cs.LG 



### Dual scale Residual-Network for turbulent flow sub grid scale resolving:   A prior analysis
**Authors**: Omar Sallam, Mirjam Fürth

**Updated**: 2024-09-11T20:33:05Z

**Summary**: This paper introduces generative Residual Networks (ResNet) as a surrogate Machine Learning (ML) tool for Large Eddy Simulation (LES) Sub Grid Scale (SGS) resolving. The study investigates the impact of incorporating Dual Scale Residual Blocks (DS-RB) within the ResNet architecture. Two LES SGS resolving models are proposed and tested for prior analysis test cases: a super-resolution model (SR-ResNet) and a SGS stress tensor inference model (SGS-ResNet). The SR-ResNet model task is to upscale LES solutions from coarse to finer grids by inferring unresolved SGS velocity fluctuations, exhibiting success in preserving high-frequency velocity fluctuation information, and aligning with higher-resolution LES solutions' energy spectrum. Furthermore, employing DS-RB enhances prediction accuracy and precision of high-frequency velocity fields compared to Single Scale Residual Blocks (SS-RB), evident in both spatial and spectral domains. The SR-ResNet model is tested and trained on filtered/downsampled 2-D LES planar jet injection problems at two Reynolds numbers, two jet configurations, and two upscale ratios. In the case of SGS stress tensor inference, both SS-RB and DS-RB exhibit higher prediction accuracy over the Smagorinsky model with reference to the true DNS SGS stress tensor, with DS-RB-based SGS-ResNet showing stronger statistical alignment with DNS data. The SGS-ResNet model is tested on a filtered/downsampled 2-D DNS isotropic homogenous decay turbulence problem. The adoption of DS-RB incurs notable increases in network size, training time, and forward inference time, with the network size expanding by over tenfold, and training and forward inference times increasing by approximately 0.5 and 3 times, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2409.07605v1),  [pdf](http://arxiv.org/pdf/2409.07605v1)

**Tags**: physics.flu-dyn cs.NA math.NA 



## Keyword: LLM Deployment 
 ### Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale
**Authors**: Rogerio Bonatti, Dan Zhao, Francesco Bonacci, Dillon Dupont, Sara Abdali, Yinheng Li, Justin Wagle, Kazuhito Koishida, Arthur Bucker, Lawrence Jang, Zack Hui

**Updated**: 2024-09-12T17:56:43Z

**Summary**: Large language models (LLMs) show remarkable potential to act as computer agents, enhancing human productivity and software accessibility in multi-modal tasks that require planning and reasoning. However, measuring agent performance in realistic environments remains a challenge since: (i) most benchmarks are limited to specific modalities or domains (e.g. text-only, web navigation, Q&A, coding) and (ii) full benchmark evaluations are slow (on order of magnitude of days) given the multi-step sequential nature of tasks. To address these challenges, we introduce the Windows Agent Arena: a reproducible, general environment focusing exclusively on the Windows operating system (OS) where agents can operate freely within a real Windows OS and use the same wide range of applications, tools, and web browsers available to human users when solving tasks. We adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverse Windows tasks across representative domains that require agent abilities in planning, screen understanding, and tool usage. Our benchmark is scalable and can be seamlessly parallelized in Azure for a full benchmark evaluation in as little as 20 minutes. To demonstrate Windows Agent Arena's capabilities, we also introduce a new multi-modal agent, Navi. Our agent achieves a success rate of 19.5% in the Windows domain, compared to 74.5% performance of an unassisted human. Navi also demonstrates strong performance on another popular web-based benchmark, Mind2Web. We offer extensive quantitative and qualitative analysis of Navi's performance, and provide insights into the opportunities for future research in agent development and data generation using Windows Agent Arena.   Webpage: https://microsoft.github.io/WindowsAgentArena   Code: https://github.com/microsoft/WindowsAgentArena

**Link**: [arxiv](http://arxiv.org/abs/2409.08264v1),  [pdf](http://arxiv.org/pdf/2409.08264v1)

**Tags**: cs.AI 



### OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable   Personal Question Answering
**Authors**: Jiahao Nick Li, Zhuohao Jerry Zhang, Jiaju Ma

**Updated**: 2024-09-12T17:48:08Z

**Summary**: People often capture memories through photos, screenshots, and videos. While existing AI-based tools enable querying this data using natural language, they mostly only support retrieving individual pieces of information like certain objects in photos and struggle with answering more complex queries that involve interpreting interconnected memories like event sequences. We conducted a one-month diary study to collect realistic user queries and generated a taxonomy of necessary contextual information for integrating with captured memories. We then introduce OmniQuery, a novel system that is able to answer complex personal memory-related questions that require extracting and inferring contextual information. OmniQuery augments single captured memories through integrating scattered contextual information from multiple interconnected memories, retrieves relevant memories, and uses a large language model (LLM) to comprehensive answers. In human evaluations, we show the effectiveness of OmniQuery with an accuracy of 71.5%, and it outperformed a conventional RAG system, winning or tying in 74.5% of the time.

**Link**: [arxiv](http://arxiv.org/abs/2409.08250v1),  [pdf](http://arxiv.org/pdf/2409.08250v1)

**Tags**: cs.HC cs.AI 



### Source2Synth: Synthetic Data Generation and Curation Grounded in Real   Data Sources
**Authors**: Alisia Lupidi, Carlos Gemmell, Nicola Cancedda, Jane Dwivedi-Yu, Jason Weston, Jakob Foerster, Roberta Raileanu, Maria Lomeli

**Updated**: 2024-09-12T17:39:08Z

**Summary**: Large Language Models still struggle in challenging scenarios that leverage structured data, complex reasoning, or tool usage. In this paper, we propose Source2Synth: a new method that can be used for teaching LLMs new skills without relying on costly human annotations. Source2Synth takes as input a custom data source and produces synthetic data points with intermediate reasoning steps grounded in real-world sources. Source2Synth improves the dataset quality by discarding low-quality generations based on their answerability. We demonstrate the generality of this approach by applying it to two challenging domains: we test reasoning abilities in multi-hop question answering (MHQA), and tool usage in tabular question answering (TQA). Our method improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on HotPotQA compared to the fine-tuned baselines.

**Link**: [arxiv](http://arxiv.org/abs/2409.08239v1),  [pdf](http://arxiv.org/pdf/2409.08239v1)

**Tags**: cs.CL cs.AI 



### LLM Honeypot: Leveraging Large Language Models as Advanced Interactive   Honeypot Systems
**Authors**: Hakan T. Otal, M. Abdullah Canbaz

**Updated**: 2024-09-12T17:33:06Z

**Summary**: The rapid evolution of cyber threats necessitates innovative solutions for detecting and analyzing malicious activity. Honeypots, which are decoy systems designed to lure and interact with attackers, have emerged as a critical component in cybersecurity. In this paper, we present a novel approach to creating realistic and interactive honeypot systems using Large Language Models (LLMs). By fine-tuning a pre-trained open-source language model on a diverse dataset of attacker-generated commands and responses, we developed a honeypot capable of sophisticated engagement with attackers. Our methodology involved several key steps: data collection and processing, prompt engineering, model selection, and supervised fine-tuning to optimize the model's performance. Evaluation through similarity metrics and live deployment demonstrated that our approach effectively generates accurate and informative responses. The results highlight the potential of LLMs to revolutionize honeypot technology, providing cybersecurity professionals with a powerful tool to detect and analyze malicious activity, thereby enhancing overall security infrastructure.

**Link**: [arxiv](http://arxiv.org/abs/2409.08234v1),  [pdf](http://arxiv.org/pdf/2409.08234v1)

**Tags**: cs.CR cs.AI cs.CL cs.LG cs.NI 68T50, 68M10 I.2.7; D.4.6; K.6.5 



### Understanding the concerns and choices of public when using large   language models for healthcare
**Authors**: Yunpeng Xiao, Kyrie Zhixuan Zhou, Yueqing Liang, Kai Shu

**Updated**: 2024-09-12T16:40:18Z

**Summary**: Large language models (LLMs) have shown their potential in biomedical fields. However, how the public uses them for healthcare purposes such as medical Q\&A, self-diagnosis, and daily healthcare information seeking is under-investigated. This paper adopts a mixed-methods approach, including surveys (N=214) and interviews (N=17) to investigate how and why the public uses LLMs for healthcare. We found that participants generally believed LLMs as a healthcare tool have gained popularity, and are often used in combination with other information channels such as search engines and online health communities to optimize information quality. Based on the findings, we reflect on the ethical and effective use of LLMs for healthcare and propose future research directions.

**Link**: [arxiv](http://arxiv.org/abs/2401.09090v2),  [pdf](http://arxiv.org/pdf/2401.09090v2)

**Tags**: cs.CY J.4; K.4.2 



### Trading Devil Final: Backdoor attack via Stock market and Bayesian   Optimization
**Authors**: Orson Mengara

**Updated**: 2024-09-12T16:22:52Z

**Summary**: Since the advent of generative artificial intelligence, every company and researcher has been rushing to develop their own generative models, whether commercial or not. Given the large number of users of these powerful new tools, there is currently no intrinsically verifiable way to explain from the ground up what happens when LLMs (large language models) learn. For example, those based on automatic speech recognition systems, which have to rely on huge and astronomical amounts of data collected from all over the web to produce fast and efficient results, In this article, we develop a backdoor attack called MarketBackFinal 2.0, based on acoustic data poisoning, MarketBackFinal 2.0 is mainly based on modern stock market models. In order to show the possible vulnerabilities of speech-based transformers that may rely on LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2407.14573v5),  [pdf](http://arxiv.org/pdf/2407.14573v5)

**Tags**: cs.LG cs.CR q-fin.CP q-fin.PR q-fin.ST 



### Fine-tuning Large Language Models for Entity Matching
**Authors**: Aaron Steiner, Ralph Peeters, Christian Bizer

**Updated**: 2024-09-12T16:20:57Z

**Summary**: Generative large language models (LLMs) are a promising alternative to pre-trained language models for entity matching due to their high zero-shot performance and their ability to generalize to unseen entities. Existing research on using LLMs for entity matching has focused on prompt engineering and in-context learning. This paper explores the potential of fine-tuning LLMs for entity matching. We analyze fine-tuning along two dimensions: 1) The representation of training examples, where we experiment with adding different types of LLM-generated explanations to the training set, and 2) the selection and generation of training examples using LLMs. In addition to the matching performance on the source dataset, we investigate how fine-tuning affects the model's ability to generalize to other in-domain datasets as well as across topical domains. Our experiments show that fine-tuning significantly improves the performance of the smaller models while the results for the larger models are mixed. Fine-tuning also improves the generalization to in-domain datasets while hurting cross-domain transfer. We show that adding structured explanations to the training set has a positive impact on the performance of three out of four LLMs, while the proposed example selection and generation methods only improve the performance of Llama 3.1 8B while decreasing the performance of GPT-4o Mini.

**Link**: [arxiv](http://arxiv.org/abs/2409.08185v1),  [pdf](http://arxiv.org/pdf/2409.08185v1)

**Tags**: cs.CL cs.AI cs.LG 68T50 I.2.7 



### A Tutorial on 5G Positioning
**Authors**: Lorenzo Italiano, Bernardo Camajori Tedeschini, Mattia Brambilla, Huiping Huang, Monica Nicoli, Henk Wymeersch

**Updated**: 2024-09-12T15:43:59Z

**Summary**: The widespread adoption of the fifth generation (5G) of cellular networks has brought new opportunities for the development of localization-based services. High-accuracy positioning use cases and functionalities defined by the standards are drawing the interest of vertical industries. In the transition towards the deployment, this paper aims to provide an in-depth tutorial on 5G positioning, summarizing the evolutionary path that led to the standardization of cellular-based positioning, describing the localization elements in current and forthcoming releases of the Third Generation Partnership Project (3GPP) standard, and the major research trends. By providing fundamental notions on wireless localization, comprehensive definitions of measurements and architectures, examples of algorithms, and details on simulation approaches, this paper is intended to represent an exhaustive guide for researchers and practitioners. Our approach aims to merge practical aspects of enabled use cases and related requirements with theoretical methodologies and fundamental bounds, allowing to understand the trade-off between system complexity and achievable, i.e., tangible, benefits of 5G positioning services. We analyze the performance of 3GPP Rel-16 positioning by standard-compliant simulations in realistic outdoor and indoor propagation environments, investigating the impact of the system configuration and the limitations to be resolved for delivering accurate positioning solutions.

**Link**: [arxiv](http://arxiv.org/abs/2311.10551v3),  [pdf](http://arxiv.org/pdf/2311.10551v3)

**Tags**: eess.SP 



### Faster Speech-LLaMA Inference with Multi-token Prediction
**Authors**: Desh Raj, Gil Keren, Junteng Jia, Jay Mahadeokar, Ozlem Kalinli

**Updated**: 2024-09-12T15:43:10Z

**Summary**: Large language models (LLMs) have become proficient at solving a wide variety of tasks, including those involving multi-modal inputs. In particular, instantiating an LLM (such as LLaMA) with a speech encoder and training it on paired data imparts speech recognition (ASR) abilities to the decoder-only model, hence called Speech-LLaMA. Nevertheless, due to the sequential nature of auto-regressive inference and the relatively large decoder, Speech-LLaMA models require relatively high inference time. In this work, we propose to speed up Speech-LLaMA inference by predicting multiple tokens in the same decoding step. We explore several model architectures that enable this, and investigate their performance using threshold-based and verification-based inference strategies. We also propose a prefix-based beam search decoding method that allows efficient minimum word error rate (MWER) training for such models. We evaluate our models on a variety of public benchmarks, where they reduce the number of decoder calls by ~3.2x while maintaining or improving WER performance.

**Link**: [arxiv](http://arxiv.org/abs/2409.08148v1),  [pdf](http://arxiv.org/pdf/2409.08148v1)

**Tags**: eess.AS cs.SD 



### LLM-POTUS Score: A Framework of Analyzing Presidential Debates with   Large Language Models
**Authors**: Zhengliang Liu, Yiwei Li, Oleksandra Zolotarevych, Rongwei Yang, Tianming Liu

**Updated**: 2024-09-12T15:40:45Z

**Summary**: Large language models have demonstrated remarkable capabilities in natural language processing, yet their application to political discourse analysis remains underexplored. This paper introduces a novel approach to evaluating presidential debate performances using LLMs, addressing the longstanding challenge of objectively assessing debate outcomes. We propose a framework that analyzes candidates' "Policies, Persona, and Perspective" (3P) and how they resonate with the "Interests, Ideologies, and Identity" (3I) of four key audience groups: voters, businesses, donors, and politicians. Our method employs large language models to generate the LLM-POTUS Score, a quantitative measure of debate performance based on the alignment between 3P and 3I. We apply this framework to analyze transcripts from recent U.S. presidential debates, demonstrating its ability to provide nuanced, multi-dimensional assessments of candidate performances. Our results reveal insights into the effectiveness of different debating strategies and their impact on various audience segments. This study not only offers a new tool for political analysis but also explores the potential and limitations of using LLMs as impartial judges in complex social contexts. In addition, this framework provides individual citizens with an independent tool to evaluate presidential debate performances, which enhances democratic engagement and reduces reliance on potentially biased media interpretations and institutional influence, thereby strengthening the foundation of informed civic participation.

**Link**: [arxiv](http://arxiv.org/abs/2409.08147v1),  [pdf](http://arxiv.org/pdf/2409.08147v1)

**Tags**: cs.CL 



### Robust Robot Walker: Learning Agile Locomotion over Tiny Traps
**Authors**: Shaoting Zhu, Runhan Huang, Linzhan Mou, Hang Zhao

**Updated**: 2024-09-12T15:35:49Z

**Summary**: Quadruped robots must exhibit robust walking capabilities in practical applications. In this work, we propose a novel approach that enables quadruped robots to pass various small obstacles, or "tiny traps". Existing methods often rely on exteroceptive sensors, which can be unreliable for detecting such tiny traps. To overcome this limitation, our approach focuses solely on proprioceptive inputs. We introduce a two-stage training framework incorporating a contact encoder and a classification head to learn implicit representations of different traps. Additionally, we design a set of tailored reward functions to improve both the stability of training and the ease of deployment for goal-tracking tasks. To benefit further research, we design a new benchmark for tiny trap task. Extensive experiments in both simulation and real-world settings demonstrate the effectiveness and robustness of our method. Project Page: https://robust-robot-walker.github.io/

**Link**: [arxiv](http://arxiv.org/abs/2409.07409v2),  [pdf](http://arxiv.org/pdf/2409.07409v2)

**Tags**: cs.RO cs.AI 



### What is the Role of Small Models in the LLM Era: A Survey
**Authors**: Lihu Chen, Gaël Varoquaux

**Updated**: 2024-09-12T15:04:57Z

**Summary**: Large Language Models (LLMs) have made significant progress in advancing artificial general intelligence (AGI), leading to the development of increasingly large models such as GPT-4 and LLaMA-405B. However, scaling up model sizes results in exponentially higher computational costs and energy consumption, making these models impractical for academic researchers and businesses with limited resources. At the same time, Small Models (SMs) are frequently used in practical settings, although their significance is currently underestimated. This raises important questions about the role of small models in the era of LLMs, a topic that has received limited attention in prior research. In this work, we systematically examine the relationship between LLMs and SMs from two key perspectives: Collaboration and Competition. We hope this survey provides valuable insights for practitioners, fostering a deeper understanding of the contribution of small models and promoting more efficient use of computational resources. The code is available at https://github.com/tigerchen52/role_of_small_models

**Link**: [arxiv](http://arxiv.org/abs/2409.06857v2),  [pdf](http://arxiv.org/pdf/2409.06857v2)

**Tags**: cs.CL 



### Large Language Models and Cognitive Science: A Comprehensive Review of   Similarities, Differences, and Challenges
**Authors**: Qian Niu, Junyu Liu, Ziqian Bi, Pohsun Feng, Benji Peng, Keyu Chen, Ming Li

**Updated**: 2024-09-12T14:56:35Z

**Summary**: This comprehensive review explores the intersection of Large Language Models (LLMs) and cognitive science, examining similarities and differences between LLMs and human cognitive processes. We analyze methods for evaluating LLMs cognitive abilities and discuss their potential as cognitive models. The review covers applications of LLMs in various cognitive fields, highlighting insights gained for cognitive science research. We assess cognitive biases and limitations of LLMs, along with proposed methods for improving their performance. The integration of LLMs with cognitive architectures is examined, revealing promising avenues for enhancing artificial intelligence (AI) capabilities. Key challenges and future research directions are identified, emphasizing the need for continued refinement of LLMs to better align with human cognition. This review provides a balanced perspective on the current state and future potential of LLMs in advancing our understanding of both artificial and human intelligence.

**Link**: [arxiv](http://arxiv.org/abs/2409.02387v3),  [pdf](http://arxiv.org/pdf/2409.02387v3)

**Tags**: cs.AI cs.CL 



### The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK   Employment Tribunal
**Authors**: Huiyuan Xie, Felix Steffek, Joana Ribeiro de Faria, Christine Carter, Jonathan Rutherford

**Updated**: 2024-09-12T14:51:43Z

**Summary**: This paper explores the intersection of technological innovation and access to justice by developing a benchmark for predicting case outcomes in the UK Employment Tribunal (UKET). To address the challenge of extensive manual annotation, the study employs a large language model (LLM) for automatic annotation, resulting in the creation of the CLC-UKET dataset. The dataset consists of approximately 19,000 UKET cases and their metadata. Comprehensive legal annotations cover facts, claims, precedent references, statutory references, case outcomes, reasons and jurisdiction codes. Facilitated by the CLC-UKET data, we examine a multi-class case outcome prediction task in the UKET. Human predictions are collected to establish a performance reference for model comparison. Empirical results from baseline models indicate that finetuned transformer models outperform zero-shot and few-shot LLMs on the UKET prediction task. The performance of zero-shot LLMs can be enhanced by integrating task-related information into few-shot examples. We hope that the CLC-UKET dataset, along with human annotations and empirical findings, can serve as a valuable benchmark for employment-related dispute resolution.

**Link**: [arxiv](http://arxiv.org/abs/2409.08098v1),  [pdf](http://arxiv.org/pdf/2409.08098v1)

**Tags**: cs.CL cs.AI 



### Securing Large Language Models: Addressing Bias, Misinformation, and   Prompt Attacks
**Authors**: Benji Peng, Keyu Chen, Ming Li, Pohsun Feng, Ziqian Bi, Junyu Liu, Qian Niu

**Updated**: 2024-09-12T14:42:08Z

**Summary**: Large Language Models (LLMs) demonstrate impressive capabilities across various fields, yet their increasing use raises critical security concerns. This article reviews recent literature addressing key issues in LLM security, with a focus on accuracy, bias, content detection, and vulnerability to attacks. Issues related to inaccurate or misleading outputs from LLMs is discussed, with emphasis on the implementation from fact-checking methodologies to enhance response reliability. Inherent biases within LLMs are critically examined through diverse evaluation techniques, including controlled input studies and red teaming exercises. A comprehensive analysis of bias mitigation strategies is presented, including approaches from pre-processing interventions to in-training adjustments and post-processing refinements. The article also probes the complexity of distinguishing LLM-generated content from human-produced text, introducing detection mechanisms like DetectGPT and watermarking techniques while noting the limitations of machine learning enabled classifiers under intricate circumstances. Moreover, LLM vulnerabilities, including jailbreak attacks and prompt injection exploits, are analyzed by looking into different case studies and large-scale competitions like HackAPrompt. This review is concluded by retrospecting defense mechanisms to safeguard LLMs, accentuating the need for more extensive research into the LLM security field.

**Link**: [arxiv](http://arxiv.org/abs/2409.08087v1),  [pdf](http://arxiv.org/pdf/2409.08087v1)

**Tags**: cs.CR 



### MosquitoMiner: A Light Weight Rover for Detecting and Eliminating   Mosquito Breeding Sites
**Authors**: Md. Adnanul Islam, Md. Faiyaz Abdullah Sayeedi, Jannatul Ferdous Deepti, Shahanur Rahman Bappy, Safrin Sanzida Islam, Fahim Hafiz

**Updated**: 2024-09-12T14:31:02Z

**Summary**: In this paper, we present a novel approach to the development and deployment of an autonomous mosquito breeding place detector rover with the object and obstacle detection capabilities to control mosquitoes. Mosquito-borne diseases continue to pose significant health threats globally, with conventional control methods proving slow and inefficient. Amidst rising concerns over the rapid spread of these diseases, there is an urgent need for innovative and efficient strategies to manage mosquito populations and prevent disease transmission. To mitigate the limitations of manual labor and traditional methods, our rover employs autonomous control strategies. Leveraging our own custom dataset, the rover can autonomously navigate along a pre-defined path, identifying and mitigating potential breeding grounds with precision. It then proceeds to eliminate these breeding grounds by spraying a chemical agent, effectively eradicating mosquito habitats. Our project demonstrates the effectiveness that is absent in traditional ways of controlling and safeguarding public health. The code for this project is available on GitHub at - https://github.com/faiyazabdullah/MosquitoMiner

**Link**: [arxiv](http://arxiv.org/abs/2409.08078v1),  [pdf](http://arxiv.org/pdf/2409.08078v1)

**Tags**: cs.RO 



### TravelAgent: An AI Assistant for Personalized Travel Planning
**Authors**: Aili Chen, Xuyang Ge, Ziquan Fu, Yanghua Xiao, Jiangjie Chen

**Updated**: 2024-09-12T14:24:45Z

**Summary**: As global tourism expands and artificial intelligence technology advances, intelligent travel planning services have emerged as a significant research focus. Within dynamic real-world travel scenarios with multi-dimensional constraints, services that support users in automatically creating practical and customized travel itineraries must address three key objectives: Rationality, Comprehensiveness, and Personalization. However, existing systems with rule-based combinations or LLM-based planning methods struggle to fully satisfy these criteria. To overcome the challenges, we introduce TravelAgent, a travel planning system powered by large language models (LLMs) designed to provide reasonable, comprehensive, and personalized travel itineraries grounded in dynamic scenarios. TravelAgent comprises four modules: Tool-usage, Recommendation, Planning, and Memory Module. We evaluate TravelAgent's performance with human and simulated users, demonstrating its overall effectiveness in three criteria and confirming the accuracy of personalized recommendations.

**Link**: [arxiv](http://arxiv.org/abs/2409.08069v1),  [pdf](http://arxiv.org/pdf/2409.08069v1)

**Tags**: cs.AI cs.CL 



### Embodied Neuromorphic Artificial Intelligence for Robotics:   Perspectives, Challenges, and Research Development Stack
**Authors**: Rachmad Vidya Wicaksana Putra, Alberto Marchisio, Fakhreddine Zayer, Jorge Dias, Muhammad Shafique

**Updated**: 2024-09-12T14:18:26Z

**Summary**: Robotic technologies have been an indispensable part for improving human productivity since they have been helping humans in completing diverse, complex, and intensive tasks in a fast yet accurate and efficient way. Therefore, robotic technologies have been deployed in a wide range of applications, ranging from personal to industrial use-cases. However, current robotic technologies and their computing paradigm still lack embodied intelligence to efficiently interact with operational environments, respond with correct/expected actions, and adapt to changes in the environments. Toward this, recent advances in neuromorphic computing with Spiking Neural Networks (SNN) have demonstrated the potential to enable the embodied intelligence for robotics through bio-plausible computing paradigm that mimics how the biological brain works, known as "neuromorphic artificial intelligence (AI)". However, the field of neuromorphic AI-based robotics is still at an early stage, therefore its development and deployment for solving real-world problems expose new challenges in different design aspects, such as accuracy, adaptability, efficiency, reliability, and security. To address these challenges, this paper will discuss how we can enable embodied neuromorphic AI for robotic systems through our perspectives: (P1) Embodied intelligence based on effective learning rule, training mechanism, and adaptability; (P2) Cross-layer optimizations for energy-efficient neuromorphic computing; (P3) Representative and fair benchmarks; (P4) Low-cost reliability and safety enhancements; (P5) Security and privacy for neuromorphic computing; and (P6) A synergistic development for energy-efficient and robust neuromorphic-based robotics. Furthermore, this paper identifies research challenges and opportunities, as well as elaborates our vision for future research development toward embodied neuromorphic AI for robotics.

**Link**: [arxiv](http://arxiv.org/abs/2404.03325v2),  [pdf](http://arxiv.org/pdf/2404.03325v2)

**Tags**: cs.RO cs.AI cs.LG cs.NE 



### External Memories of PDP Switches for In-Network Implementable Functions   Placement: Deep Learning Based Reconfiguration of SFCs
**Authors**: Somayeh Kianpisheh, Tarik Taleb

**Updated**: 2024-09-12T13:47:18Z

**Summary**: Network function virtualization leverages programmable data plane switches to deploy in-network implementable functions, to improve QoS. The memories of switches can be extended through remote direct memory access to access external memories. This paper exploits the switches external memories to place VNFs at time intervals with ultra-low latency and high bandwidth demands. The reconfiguration decision is modeled as an optimization to minimize the deployment and reconfiguration cost, while meeting the SFCs deadlines. A DRL based method is proposed to reconfigure service chains adoptable with dynamic network and traffic characteristics. To deal with slow convergence due to the complexity of deployment scenarios, static and dynamic filters are used in policy networks construction to diminish unfeasible placement exploration. Results illustrate improvement in convergence, acceptance ratio and cost.

**Link**: [arxiv](http://arxiv.org/abs/2409.08043v1),  [pdf](http://arxiv.org/pdf/2409.08043v1)

**Tags**: cs.NI 



### Towards Scalable Quantum Key Distribution: A Machine Learning-Based   Cascade Protocol Approach
**Authors**: Hasan Abbas Al-Mohammed, Saif Al-Kuwari, Hashir Kuniyil, Ahmed Farouk

**Updated**: 2024-09-12T13:40:08Z

**Summary**: Quantum Key Distribution (QKD) is a pivotal technology in the quest for secure communication, harnessing the power of quantum mechanics to ensure robust data protection. However, scaling QKD to meet the demands of high-speed, real-world applications remains a significant challenge. Traditional key rate determination methods, dependent on complex mathematical models, often fall short in efficiency and scalability. In this paper, we propose an approach that involves integrating machine learning (ML) techniques with the Cascade error correction protocol to enhance the scalability and efficiency of QKD systems. Our ML-based approach utilizes an autoencoder framework to predict the Quantum Bit Error Rate (QBER) and final key length with over 99\% accuracy. This method significantly reduces error correction time, maintaining a consistently low computation time even with large input sizes, such as data rates up to 156 Mbps. In contrast, traditional methods exhibit exponentially increasing computation times as input sizes grow, highlighting the superior scalability of our ML-based solution. Through comprehensive simulations, we demonstrate that our method not only accelerates the error correction process but also optimizes resource utilization, making it more cost-effective and practical for real-world deployment. The Cascade protocol's integration further enhances system security by dynamically adjusting error correction based on real-time QBER observations, providing robust protection against potential eavesdropping.   Our research establishes a new benchmark for scalable, high-throughput QKD systems, proving that machine learning can significantly advance the field of quantum cryptography. This work continues the evolution towards truly scalable quantum communication.

**Link**: [arxiv](http://arxiv.org/abs/2409.08038v1),  [pdf](http://arxiv.org/pdf/2409.08038v1)

**Tags**: quant-ph 



### From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework   for Student Performance Feedback
**Authors**: Vinitra Swamy, Davide Romano, Bhargav Srinivasa Desikan, Oana-Maria Camburu, Tanja Käser

**Updated**: 2024-09-12T13:18:41Z

**Summary**: Recent advances in eXplainable AI (XAI) for education have highlighted a critical challenge: ensuring that explanations for state-of-the-art AI models are understandable for non-technical users such as educators and students. In response, we introduce iLLuMinaTE, a zero-shot, chain-of-prompts LLM-XAI pipeline inspired by Miller's cognitive model of explanation. iLLuMinaTE is designed to deliver theory-driven, actionable feedback to students in online courses. iLLuMinaTE navigates three main stages - causal connection, explanation selection, and explanation presentation - with variations drawing from eight social science theories (e.g. Abnormal Conditions, Pearl's Model of Explanation, Necessity and Robustness Selection, Contrastive Explanation). We extensively evaluate 21,915 natural language explanations of iLLuMinaTE extracted from three LLMs (GPT-4o, Gemma2-9B, Llama3-70B), with three different underlying XAI methods (LIME, Counterfactuals, MC-LIME), across students from three diverse online courses. Our evaluation involves analyses of explanation alignment to the social science theory, understandability of the explanation, and a real-world user preference study with 114 university students containing a novel actionability simulation. We find that students prefer iLLuMinaTE explanations over traditional explainers 89.52% of the time. Our work provides a robust, ready-to-use framework for effectively communicating hybrid XAI-driven insights in education, with significant generalization potential for other human-centric fields.

**Link**: [arxiv](http://arxiv.org/abs/2409.08027v1),  [pdf](http://arxiv.org/pdf/2409.08027v1)

**Tags**: cs.CY cs.HC cs.LG 



### An Evaluation Framework for Attributed Information Retrieval using Large   Language Models
**Authors**: Hanane Djeddal, Pierre Erbacher, Raouf Toukal, Laure Soulier, Karen Pinel-Sauvagnat, Sophia Katrenko, Lynda Tamine

**Updated**: 2024-09-12T12:57:08Z

**Summary**: With the growing success of Large Language models (LLMs) in information-seeking scenarios, search engines are now adopting generative approaches to provide answers along with in-line citations as attribution. While existing work focuses mainly on attributed question answering, in this paper, we target information-seeking scenarios which are often more challenging due to the open-ended nature of the queries and the size of the label space in terms of the diversity of candidate-attributed answers per query. We propose a reproducible framework to evaluate and benchmark attributed information seeking, using any backbone LLM, and different architectural designs: (1) Generate (2) Retrieve then Generate, and (3) Generate then Retrieve. Experiments using HAGRID, an attributed information-seeking dataset, show the impact of different scenarios on both the correctness and attributability of answers.

**Link**: [arxiv](http://arxiv.org/abs/2409.08014v1),  [pdf](http://arxiv.org/pdf/2409.08014v1)

**Tags**: cs.IR 



### Games for AI Control: Models of Safety Evaluations of AI Deployment   Protocols
**Authors**: Charlie Griffin, Louis Thomson, Buck Shlegeris, Alessandro Abate

**Updated**: 2024-09-12T12:30:07Z

**Summary**: To evaluate the safety and usefulness of deployment protocols for untrusted AIs, AI Control uses a red-teaming exercise played between a protocol designer and an adversary. This paper introduces AI-Control Games, a formal decision-making model of the red-teaming exercise as a multi-objective, partially observable, stochastic game. We also introduce methods for finding optimal protocols in AI-Control Games, by reducing them to a set of zero-sum partially observable stochastic games. We apply our formalism to model, evaluate and synthesise protocols for deploying untrusted language models as programming assistants, focusing on Trusted Monitoring protocols, which use weaker language models and limited human assistance. Finally, we demonstrate the utility of our formalism by showcasing improvements over empirical studies in existing settings, evaluating protocols in new settings, and analysing how modelling assumptions affect the safety and usefulness of protocols.

**Link**: [arxiv](http://arxiv.org/abs/2409.07985v1),  [pdf](http://arxiv.org/pdf/2409.07985v1)

**Tags**: cs.AI cs.LG 



### Unlocking Sustainability Compliance: Characterizing the EU Taxonomy for   Business Process Management
**Authors**: Finn Klessascheck, Stephan A. Fahrenkrog-Petersen, Jan Mendling, Luise Pufahl

**Updated**: 2024-09-12T12:08:04Z

**Summary**: To promote sustainable business practices, and to achieve climate neutrality by 2050, the EU has developed the taxonomy of sustainable activities, which describes when exactly business practices can be considered sustainable. While the taxonomy has only been recently established, progressively more companies will have to report how much of their revenue was created via sustainably executed business processes. To help companies prepare to assess whether their business processes comply with the constraints outlined in the taxonomy, we investigate in how far these criteria can be used for conformance checking, that is, assessing in a data-driven manner, whether business process executions adhere to regulatory constraints. For this, we develop a few-shot learning pipeline to characterize the constraints of the taxonomy with the help of an LLM as to the process dimensions they relate to. We find that many constraints of the taxonomy are useable for conformance checking, particularly in the sectors of energy, manufacturing, and transport. This will aid companies in preparing to monitor regulatory compliance with the taxonomy automatically, by characterizing what kind of information they need to extract, and by providing a better understanding of sectors where such an assessment is feasible and where it is not.

**Link**: [arxiv](http://arxiv.org/abs/2408.11386v2),  [pdf](http://arxiv.org/pdf/2408.11386v2)

**Tags**: cs.CY cs.DB 



### Exploring Parameter-Efficient Fine-Tuning of Large Language Model on   Automated Program Repair
**Authors**: Guochang Li, Chen Zhi, Jialiang Chen, Junxiao Han, Shuiguang Deng

**Updated**: 2024-09-12T12:00:26Z

**Summary**: Automated Program Repair (APR) aims to fix bugs by generating patches. And existing work has demonstrated that "pre-training and fine-tuning" paradigm enables Large Language Models (LLMs) improve fixing capabilities on APR. However, existing work mainly focuses on Full-Model Fine-Tuning (FMFT) for APR and limited research has been conducted on the execution-based evaluation of Parameter-Efficient Fine-Tuning (PEFT) for APR. Comparing to FMFT, PEFT can reduce computing resource consumption without compromising performance and has been widely adopted to other software engineering tasks.   To fill this gap, we enhance the existing APR dataset by employing prompt engineering to create an instruction dataset, APR-INSTRUCTION, at first. Secondly, we fine-tune four pre-trained LLMs using four different PEFT methods with APR-INSTRUCTION. The best fine-tuned model fixes 58% more bugs than the state-of-the-art LLM-based APR techniques. The results also show that $(IA)^3$ improves the creativity of LLMs more effectively through fine-tuning and achieves the highest fixing capability compared to the other three PEFT methods. Thirdly, we explore the optimal configuration of PEFT hyperparameters, and assess the impact of instruction dataset size, showing that a larger number of parameters and a larger training dataset do not necessarily result in better performance for PEFT. Lastly, we analyze peak memory usage and trainable parameters to show the efficiency of PEFT.   This work provides a comprehensive exploration of PEFT on APR and suggests potentially promising directions for extension to other software engineering downstream tasks. APR-INSTRUCTION, PEFT weights, and the fine-tuning code are publicly available as open-source resources.

**Link**: [arxiv](http://arxiv.org/abs/2406.05639v2),  [pdf](http://arxiv.org/pdf/2406.05639v2)

**Tags**: cs.SE 



### How Easily do Irrelevant Inputs Skew the Responses of Large Language   Models?
**Authors**: Siye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, Yanghua Xiao

**Updated**: 2024-09-12T11:51:51Z

**Summary**: By leveraging the retrieval of information from external knowledge databases, Large Language Models (LLMs) exhibit enhanced capabilities for accomplishing many knowledge-intensive tasks. However, due to the inherent flaws of current retrieval systems, there might exist irrelevant information within those retrieving top-ranked passages. In this work, we present a comprehensive investigation into the robustness of LLMs to different types of irrelevant information under various conditions. We initially introduce a framework to construct high-quality irrelevant information that ranges from semantically unrelated, partially related, and related to questions. Furthermore, our analysis demonstrates that the constructed irrelevant information not only scores highly on similarity metrics, being highly retrieved by existing systems, but also bears semantic connections to the context. Our investigation reveals that current LLMs still face challenges in discriminating highly semantically related information and can be easily distracted by these irrelevant yet misleading content. Besides, we also find that current solutions for handling irrelevant information have limitations in improving the robustness of LLMs to such distractions. All the resources are available on GitHub at https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information.

**Link**: [arxiv](http://arxiv.org/abs/2404.03302v4),  [pdf](http://arxiv.org/pdf/2404.03302v4)

**Tags**: cs.CL 



### WirelessAgent: Large Language Model Agents for Intelligent Wireless   Networks
**Authors**: Jingwen Tong, Jiawei Shao, Qiong Wu, Wei Guo, Zijian Li, Zehong Lin, Jun Zhang

**Updated**: 2024-09-12T11:48:01Z

**Summary**: Wireless networks are increasingly facing challenges due to their expanding scale and complexity. These challenges underscore the need for advanced AI-driven strategies, particularly in the upcoming 6G networks. In this article, we introduce WirelessAgent, a novel approach leveraging large language models (LLMs) to develop AI agents capable of managing complex tasks in wireless networks. It can effectively improve network performance through advanced reasoning, multimodal data processing, and autonomous decision making. Thereafter, we demonstrate the practical applicability and benefits of WirelessAgent for network slicing management. The experimental results show that WirelessAgent is capable of accurately understanding user intent, effectively allocating slice resources, and consistently maintaining optimal performance.

**Link**: [arxiv](http://arxiv.org/abs/2409.07964v1),  [pdf](http://arxiv.org/pdf/2409.07964v1)

**Tags**: cs.NI cs.AI cs.LG 



### A Lagrangian shape and topology optimization framework based on   semi-discrete optimal transport
**Authors**: Charles Dapogny, Bruno Levy, Edouard Oudet

**Updated**: 2024-09-12T09:32:08Z

**Summary**: This article revolves around shape and topology optimization, in the applicative context where the objective and constraint functionals depend on the solution to a physical boundary value problem posed on the optimized domain. We introduce a novel framework based on modern concepts from computational geometry, optimal transport and numerical analysis. Its pivotal feature is a representation of the optimized shape by the cells of an adapted version of a Laguerre diagram. Although such objects are originally described by a collection of seed points and weights, recent results from optimal transport theory suggest a more intuitive parametrization in terms of the seed points and measures of the associated cells. The polygonal mesh of the shape induced by this diagram serves as support for the deployment of the Virtual Element Method for the numerical solution of the physical boundary value problem at play and the calculation of the objective and constraint functionals. The sensitivities of the latter are derived next; at first, we calculate their derivatives with respect to the positions of the vertices of the Laguerre diagram by shape calculus techniques; a suitable adjoint methodology is then developed to express them in terms of the seed points and cell measures of the diagram. The evolution of the shape is realized by first updating the design variables according to these sensitivities and then reconstructing the diagram with efficient algorithms from computational geometry. Our shape optimization strategy is versatile: it can be applied to a wide gammut of physical situations. It is Lagrangian by essence, and it thereby benefits from all the assets of a consistently meshed representation of the shape. Yet, it naturally handles dramatic motions, including topological changes, in a very robust fashion. These features, among others, are illustrated by a series of 2d numerical examples.

**Link**: [arxiv](http://arxiv.org/abs/2409.07873v1),  [pdf](http://arxiv.org/pdf/2409.07873v1)

**Tags**: math.OC 



### Objection Overruled! Lay People can Distinguish Large Language Models   from Lawyers, but still Favour Advice from an LLM
**Authors**: Eike Schneiders, Tina Seabrooke, Joshua Krook, Richard Hyde, Natalie Leesakul, Jeremie Clos, Joel Fischer

**Updated**: 2024-09-12T09:28:34Z

**Summary**: Large Language Models (LLMs) are seemingly infiltrating every domain, and the legal context is no exception. In this paper, we present the results of three experiments (total N=288) that investigated lay people's willingness to act upon, and their ability to discriminate between, LLM- and lawyer-generated legal advice. In Experiment 1, participants judged their willingness to act on legal advice when the source of the advice was either known or unknown. When the advice source was unknown, participants indicated that they were significantly more willing to act on the LLM-generated advice. This result was replicated in Experiment 2. Intriguingly, despite participants indicating higher willingness to act on LLM-generated advice in Experiments 1 and 2, participants discriminated between the LLM- and lawyer-generated texts significantly above chance-level in Experiment 3. Lastly, we discuss potential explanations and risks of our findings, limitations and future work, and the importance of language complexity and real-world comparability.

**Link**: [arxiv](http://arxiv.org/abs/2409.07871v1),  [pdf](http://arxiv.org/pdf/2409.07871v1)

**Tags**: cs.HC cs.CY 



### AnomalyDINO: Boosting Patch-based Few-shot Anomaly Detection with DINOv2
**Authors**: Simon Damm, Mike Laszkiewicz, Johannes Lederer, Asja Fischer

**Updated**: 2024-09-12T09:23:32Z

**Summary**: Recent advances in multimodal foundation models have set new standards in few-shot anomaly detection. This paper explores whether high-quality visual features alone are sufficient to rival existing state-of-the-art vision-language models. We affirm this by adapting DINOv2 for one-shot and few-shot anomaly detection, with a focus on industrial applications. We show that this approach does not only rival existing techniques but can even outmatch them in many settings. Our proposed vision-only approach, AnomalyDINO, is based on patch similarities and enables both image-level anomaly prediction and pixel-level anomaly segmentation. The approach is methodologically simple and training-free and, thus, does not require any additional data for fine-tuning or meta-learning. Despite its simplicity, AnomalyDINO achieves state-of-the-art results in one- and few-shot anomaly detection (e.g., pushing the one-shot performance on MVTec-AD from an AUROC of 93.1% to 96.6%). The reduced overhead, coupled with its outstanding few-shot performance, makes AnomalyDINO a strong candidate for fast deployment, e.g., in industrial contexts.

**Link**: [arxiv](http://arxiv.org/abs/2405.14529v2),  [pdf](http://arxiv.org/pdf/2405.14529v2)

**Tags**: cs.CV 



### GOPT: Generalizable Online 3D Bin Packing via Transformer-based Deep   Reinforcement Learning
**Authors**: Heng Xiong, Changrong Guo, Jian Peng, Kai Ding, Wenjie Chen, Xuchong Qiu, Long Bai, Jianfeng Xu

**Updated**: 2024-09-12T08:49:20Z

**Summary**: Robotic object packing has broad practical applications in the logistics and automation industry, often formulated by researchers as the online 3D Bin Packing Problem (3D-BPP). However, existing DRL-based methods primarily focus on enhancing performance in limited packing environments while neglecting the ability to generalize across multiple environments characterized by different bin dimensions. To this end, we propose GOPT, a generalizable online 3D Bin Packing approach via Transformer-based deep reinforcement learning (DRL). First, we design a Placement Generator module to yield finite subspaces as placement candidates and the representation of the bin. Second, we propose a Packing Transformer, which fuses the features of the items and bin, to identify the spatial correlation between the item to be packed and available sub-spaces within the bin. Coupling these two components enables GOPT's ability to perform inference on bins of varying dimensions. We conduct extensive experiments and demonstrate that GOPT not only achieves superior performance against the baselines, but also exhibits excellent generalization capabilities. Furthermore, the deployment with a robot showcases the practical applicability of our method in the real world. The source code will be publicly available at https://github.com/Xiong5Heng/GOPT.

**Link**: [arxiv](http://arxiv.org/abs/2409.05344v2),  [pdf](http://arxiv.org/pdf/2409.05344v2)

**Tags**: cs.RO 



### Do These LLM Benchmarks Agree? Fixing Benchmark Evaluation with   BenchBench
**Authors**: Yotam Perlitz, Ariel Gera, Ofir Arviv, Asaf Yehudai, Elron Bandel, Eyal Shnarch, Michal Shmueli-Scheuer, Leshem Choshen

**Updated**: 2024-09-12T08:36:47Z

**Summary**: Recent advancements in Language Models (LMs) have catalyzed the creation of multiple benchmarks, designed to assess these models' general capabilities. A crucial task, however, is assessing the validity of the benchmarks themselves. This is most commonly done via Benchmark Agreement Testing (BAT), where new benchmarks are validated against established ones using some agreement metric (e.g., rank correlation). Despite the crucial role of BAT for benchmark builders and consumers, there are no standardized procedures for such agreement testing. This deficiency can lead to invalid conclusions, fostering mistrust in benchmarks and upending the ability to properly choose the appropriate benchmark to use. By analyzing over 40 prominent benchmarks, we demonstrate how some overlooked methodological choices can significantly influence BAT results, potentially undermining the validity of conclusions. To address these inconsistencies, we propose a set of best practices for BAT and demonstrate how utilizing these methodologies greatly improves BAT robustness and validity. To foster adoption and facilitate future research,, we introduce BenchBench, a python package for BAT, and release the BenchBench-leaderboard, a meta-benchmark designed to evaluate benchmarks using their peers. Our findings underscore the necessity for standardized BAT, ensuring the robustness and validity of benchmark evaluations in the evolving landscape of language model research.   BenchBench Package: github.com/IBM/BenchBench   Leaderboard: hf.co/spaces/IBM/BenchBench

**Link**: [arxiv](http://arxiv.org/abs/2407.13696v2),  [pdf](http://arxiv.org/pdf/2407.13696v2)

**Tags**: cs.CL 



### ReGentS: Real-World Safety-Critical Driving Scenario Generation Made   Stable
**Authors**: Yuan Yin, Pegah Khayatan, Éloi Zablocki, Alexandre Boulch, Matthieu Cord

**Updated**: 2024-09-12T08:26:33Z

**Summary**: Machine learning based autonomous driving systems often face challenges with safety-critical scenarios that are rare in real-world data, hindering their large-scale deployment. While increasing real-world training data coverage could address this issue, it is costly and dangerous. This work explores generating safety-critical driving scenarios by modifying complex real-world regular scenarios through trajectory optimization. We propose ReGentS, which stabilizes generated trajectories and introduces heuristics to avoid obvious collisions and optimization problems. Our approach addresses unrealistic diverging trajectories and unavoidable collision scenarios that are not useful for training robust planner. We also extend the scenario generation framework to handle real-world data with up to 32 agents. Additionally, by using a differentiable simulator, our approach simplifies gradient descent-based optimization involving a simulator, paving the way for future advancements. The code is available at https://github.com/valeoai/ReGentS.

**Link**: [arxiv](http://arxiv.org/abs/2409.07830v1),  [pdf](http://arxiv.org/pdf/2409.07830v1)

**Tags**: cs.LG cs.CV cs.RO 



### Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs:   A Case Study in WeChat
**Authors**: Sidong Feng, Haochuan Lu, Jianqin Jiang, Ting Xiong, Likun Huang, Yinglin Liang, Xiaoqin Li, Yuetang Deng, Aldeida Aleti

**Updated**: 2024-09-12T08:25:33Z

**Summary**: UI automation tests play a crucial role in ensuring the quality of mobile applications. Despite the growing popularity of machine learning techniques to generate these tests, they still face several challenges, such as the mismatch of UI elements. The recent advances in Large Language Models (LLMs) have addressed these issues by leveraging their semantic understanding capabilities. However, a significant gap remains in applying these models to industrial-level app testing, particularly in terms of cost optimization and knowledge limitation. To address this, we introduce CAT to create cost-effective UI automation tests for industry apps by combining machine learning and LLMs with best practices. Given the task description, CAT employs Retrieval Augmented Generation (RAG) to source examples of industrial app usage as the few-shot learning context, assisting LLMs in generating the specific sequence of actions. CAT then employs machine learning techniques, with LLMs serving as a complementary optimizer, to map the target element on the UI screen. Our evaluations on the WeChat testing dataset demonstrate the CAT's performance and cost-effectiveness, achieving 90% UI automation with $0.34 cost, outperforming the state-of-the-art. We have also integrated our approach into the real-world WeChat testing platform, demonstrating its usefulness in detecting 141 bugs and enhancing the developers' testing process.

**Link**: [arxiv](http://arxiv.org/abs/2409.07829v1),  [pdf](http://arxiv.org/pdf/2409.07829v1)

**Tags**: cs.SE 



### What is YOLOv9: An In-Depth Exploration of the Internal Features of the   Next-Generation Object Detector
**Authors**: Muhammad Yaseen

**Updated**: 2024-09-12T07:46:58Z

**Summary**: This study provides a comprehensive analysis of the YOLOv9 object detection model, focusing on its architectural innovations, training methodologies, and performance improvements over its predecessors. Key advancements, such as the Generalized Efficient Layer Aggregation Network GELAN and Programmable Gradient Information PGI, significantly enhance feature extraction and gradient flow, leading to improved accuracy and efficiency. By incorporating Depthwise Convolutions and the lightweight C3Ghost architecture, YOLOv9 reduces computational complexity while maintaining high precision. Benchmark tests on Microsoft COCO demonstrate its superior mean Average Precision mAP and faster inference times, outperforming YOLOv8 across multiple metrics. The model versatility is highlighted by its seamless deployment across various hardware platforms, from edge devices to high performance GPUs, with built in support for PyTorch and TensorRT integration. This paper provides the first in depth exploration of YOLOv9s internal features and their real world applicability, establishing it as a state of the art solution for real time object detection across industries, from IoT devices to large scale industrial applications.

**Link**: [arxiv](http://arxiv.org/abs/2409.07813v1),  [pdf](http://arxiv.org/pdf/2409.07813v1)

**Tags**: cs.CV 



### Trustworthy, Responsible, and Safe AI: A Comprehensive Architectural   Framework for AI Safety with Challenges and Mitigations
**Authors**: Chen Chen, Ziyao Liu, Weifeng Jiang, Si Qi Goh, Kwok-Yan Lam

**Updated**: 2024-09-12T07:45:50Z

**Summary**: AI Safety is an emerging area of critical importance to the safe adoption and deployment of AI systems. With the rapid proliferation of AI and especially with the recent advancement of Generative AI (or GAI), the technology ecosystem behind the design, development, adoption, and deployment of AI systems has drastically changed, broadening the scope of AI Safety to address impacts on public safety and national security. In this paper, we propose a novel architectural framework for understanding and analyzing AI Safety; defining its characteristics from three perspectives: Trustworthy AI, Responsible AI, and Safe AI. We provide an extensive review of current research and advancements in AI safety from these perspectives, highlighting their key challenges and mitigation approaches. Through examples from state-of-the-art technologies, particularly Large Language Models (LLMs), we present innovative mechanism, methodologies, and techniques for designing and testing AI safety. Our goal is to promote advancement in AI safety research, and ultimately enhance people's trust in digital transformation.

**Link**: [arxiv](http://arxiv.org/abs/2408.12935v2),  [pdf](http://arxiv.org/pdf/2408.12935v2)

**Tags**: cs.AI 



### Detecting Wildfires on UAVs with Real-time Segmentation Trained by   Larger Teacher Models
**Authors**: Julius Pesonen, Teemu Hakala, Väinö Karjalainen, Niko Koivumäki, Lauri Markelin, Anna-Maria Raita-Hakola, Juha Suomalainen, Ilkka Pölönen, Eija Honkavaara

**Updated**: 2024-09-12T07:39:49Z

**Summary**: Early detection of wildfires is essential to prevent large-scale fires resulting in extensive environmental, structural, and societal damage. Uncrewed aerial vehicles (UAVs) can cover large remote areas effectively with quick deployment requiring minimal infrastructure and equipping them with small cameras and computers enables autonomous real-time detection. In remote areas, however, the UAVs are limited to on-board computing for detection due to the lack of high-bandwidth mobile networks. This limits the detection to methods which are light enough for the on-board computer alone. For accurate camera-based localisation, segmentation of the detected smoke is essential but training data for deep learning-based wildfire smoke segmentation is limited. This study shows how small specialised segmentation models can be trained using only bounding box labels, leveraging zero-shot foundation model supervision. The method offers the advantages of needing only fairly easily obtainable bounding box labels and requiring training solely for the smaller student network. The proposed method achieved 63.3% mIoU on a manually annotated and diverse wildfire dataset. The used model can perform in real-time at ~25 fps with a UAV-carried NVIDIA Jetson Orin NX computer while reliably recognising smoke, demonstrated at real-world forest burning events. Code is available at https://gitlab.com/fgi_nls/public/wildfire-real-time-segmentation

**Link**: [arxiv](http://arxiv.org/abs/2408.10843v2),  [pdf](http://arxiv.org/pdf/2408.10843v2)

**Tags**: cs.CV cs.AI I.4.6 



### LLM-enhanced Scene Graph Learning for Household Rearrangement
**Authors**: Wenhao Li, Zhiyuan Yu, Qijin She, Zhinan Yu, Yuqing Lan, Chenyang Zhu, Ruizhen Hu, Kai Xu

**Updated**: 2024-09-12T07:18:00Z

**Summary**: The household rearrangement task involves spotting misplaced objects in a scene and accommodate them with proper places. It depends both on common-sense knowledge on the objective side and human user preference on the subjective side. In achieving such task, we propose to mine object functionality with user preference alignment directly from the scene itself, without relying on human intervention. To do so, we work with scene graph representation and propose LLM-enhanced scene graph learning which transforms the input scene graph into an affordance-enhanced graph (AEG) with information-enhanced nodes and newly discovered edges (relations). In AEG, the nodes corresponding to the receptacle objects are augmented with context-induced affordance which encodes what kind of carriable objects can be placed on it. New edges are discovered with newly discovered non-local relations. With AEG, we perform task planning for scene rearrangement by detecting misplaced carriables and determining a proper placement for each of them. We test our method by implementing a tiding robot in simulator and perform evaluation on a new benchmark we build. Extensive evaluations demonstrate that our method achieves state-of-the-art performance on misplacement detection and the following rearrangement planning.

**Link**: [arxiv](http://arxiv.org/abs/2408.12093v2),  [pdf](http://arxiv.org/pdf/2408.12093v2)

**Tags**: cs.RO cs.CV 



### In-Situ Fine-Tuning of Wildlife Models in IoT-Enabled Camera Traps for   Efficient Adaptation
**Authors**: Mohammad Mehdi Rastikerdar, Jin Huang, Hui Guan, Deepak Ganesan

**Updated**: 2024-09-12T06:56:52Z

**Summary**: Wildlife monitoring via camera traps has become an essential tool in ecology, but the deployment of machine learning models for on-device animal classification faces significant challenges due to domain shifts and resource constraints. This paper introduces WildFit, a novel approach that reconciles the conflicting goals of achieving high domain generalization performance and ensuring efficient inference for camera trap applications. WildFit leverages continuous background-aware model fine-tuning to deploy ML models tailored to the current location and time window, allowing it to maintain robust classification accuracy in the new environment without requiring significant computational resources. This is achieved by background-aware data synthesis, which generates training images representing the new domain by blending background images with animal images from the source domain. We further enhance fine-tuning effectiveness through background drift detection and class distribution drift detection, which optimize the quality of synthesized data and improve generalization performance. Our extensive evaluation across multiple camera trap datasets demonstrates that WildFit achieves significant improvements in classification accuracy and computational efficiency compared to traditional approaches.

**Link**: [arxiv](http://arxiv.org/abs/2409.07796v1),  [pdf](http://arxiv.org/pdf/2409.07796v1)

**Tags**: cs.CV cs.AI cs.LG 



### Full-text Error Correction for Chinese Speech Recognition with Large   Language Model
**Authors**: Zhiyuan Tang, Dong Wang, Shen Huang, Shidong Shang

**Updated**: 2024-09-12T06:50:45Z

**Summary**: Large Language Models (LLMs) have demonstrated substantial potential for error correction in Automatic Speech Recognition (ASR). However, most research focuses on utterances from short-duration speech recordings, which are the predominant form of speech data for supervised ASR training. This paper investigates the effectiveness of LLMs for error correction in full-text generated by ASR systems from longer speech recordings, such as transcripts from podcasts, news broadcasts, and meetings. First, we develop a Chinese dataset for full-text error correction, named ChFT, utilizing a pipeline that involves text-to-speech synthesis, ASR, and error-correction pair extractor. This dataset enables us to correct errors across contexts, including both full-text and segment, and to address a broader range of error types, such as punctuation restoration and inverse text normalization, thus making the correction process comprehensive. Second, we fine-tune a pre-trained LLM on the constructed dataset using a diverse set of prompts and target formats, and evaluate its performance on full-text error correction. Specifically, we design prompts based on full-text and segment, considering various output formats, such as directly corrected text and JSON-based error-correction pairs. Through various test settings, including homogeneous, up-to-date, and hard test sets, we find that the fine-tuned LLMs perform well in the full-text setting with different prompts, each presenting its own strengths and weaknesses. This establishes a promising baseline for further research. The dataset is available on the website.

**Link**: [arxiv](http://arxiv.org/abs/2409.07790v1),  [pdf](http://arxiv.org/pdf/2409.07790v1)

**Tags**: cs.CL eess.AS 



### Alignment with Preference Optimization Is All You Need for LLM Safety
**Authors**: Reda Alami, Ali Khalifa Almansoori, Ahmed Alzubaidi, Mohamed El Amine Seddik, Mugariya Farooq, Hakim Hacid

**Updated**: 2024-09-12T06:10:15Z

**Summary**: We demonstrate that preference optimization methods can effectively enhance LLM safety. Applying various alignment techniques to the Falcon 11B model using safety datasets, we achieve a significant boost in global safety score (from $57.64\%$ to $99.90\%$) as measured by LlamaGuard 3 8B, competing with state-of-the-art models. On toxicity benchmarks, average scores in adversarial settings dropped from over $0.6$ to less than $0.07$. However, this safety improvement comes at the cost of reduced general capabilities, particularly in math, suggesting a trade-off. We identify noise contrastive alignment (Safe-NCA) as an optimal method for balancing safety and performance. Our study ultimately shows that alignment techniques can be sufficient for building safe and robust models.

**Link**: [arxiv](http://arxiv.org/abs/2409.07772v1),  [pdf](http://arxiv.org/pdf/2409.07772v1)

**Tags**: cs.LG 



### DiTAS: Quantizing Diffusion Transformers via Enhanced Activation   Smoothing
**Authors**: Zhenyuan Dong, Sai Qian Zhang

**Updated**: 2024-09-12T05:18:57Z

**Summary**: Diffusion Transformers (DiTs) have recently attracted significant interest from both industry and academia due to their enhanced capabilities in visual generation, surpassing the performance of traditional diffusion models that employ U-Net. However, the improved performance of DiTs comes at the expense of higher parameter counts and implementation costs, which significantly limits their deployment on resource-constrained devices like mobile phones. We propose DiTAS, a data-free post-training quantization (PTQ) method for efficient DiT inference. DiTAS relies on the proposed temporal-aggregated smoothing techniques to mitigate the impact of the channel-wise outliers within the input activations, leading to much lower quantization error under extremely low bitwidth. To further enhance the performance of the quantized DiT, we adopt the layer-wise grid search strategy to optimize the smoothing factor. Experimental results demonstrate that our approach enables 4-bit weight, 8-bit activation (W4A8) quantization for DiTs while maintaining comparable performance as the full-precision model.

**Link**: [arxiv](http://arxiv.org/abs/2409.07756v1),  [pdf](http://arxiv.org/pdf/2409.07756v1)

**Tags**: cs.CV 



### On Leveraging Large Language Models for Enhancing Entity Resolution: A   Cost-efficient Approach
**Authors**: Huahang Li, Longyu Feng, Shuangyin Li, Fei Hao, Chen Jason Zhang, Yuanfeng Song

**Updated**: 2024-09-12T04:47:33Z

**Summary**: Entity resolution, the task of identifying and merging records that refer to the same real-world entity, is crucial in sectors like e-commerce, healthcare, and law enforcement. Large Language Models (LLMs) introduce an innovative approach to this task, capitalizing on their advanced linguistic capabilities and a ``pay-as-you-go'' model that provides significant advantages to those without extensive data science expertise. However, current LLMs are costly due to per-API request billing. Existing methods often either lack quality or become prohibitively expensive at scale. To address these problems, we propose an uncertainty reduction framework using LLMs to improve entity resolution results. We first initialize possible partitions of the entity cluster, refer to the same entity, and define the uncertainty of the result. Then, we reduce the uncertainty by selecting a few valuable matching questions for LLM verification. Upon receiving the answers, we update the probability distribution of the possible partitions. To further reduce costs, we design an efficient algorithm to judiciously select the most valuable matching pairs to query. Additionally, we create error-tolerant techniques to handle LLM mistakes and a dynamic adjustment method to reach truly correct partitions. Experimental results show that our method is efficient and effective, offering promising applications in real-world tasks.

**Link**: [arxiv](http://arxiv.org/abs/2401.03426v2),  [pdf](http://arxiv.org/pdf/2401.03426v2)

**Tags**: cs.CL cs.AI 



### Ruri: Japanese General Text Embeddings
**Authors**: Hayato Tsukagoshi, Ryohei Sasano

**Updated**: 2024-09-12T04:06:31Z

**Summary**: We report the development of Ruri, a series of Japanese general text embedding models. While the development of general-purpose text embedding models in English and multilingual contexts has been active in recent years, model development in Japanese remains insufficient. The primary reasons for this are the lack of datasets and the absence of necessary expertise. In this report, we provide a detailed account of the development process of Ruri. Specifically, we discuss the training of embedding models using synthesized datasets generated by LLMs, the construction of the reranker for dataset filtering and knowledge distillation, and the performance evaluation of the resulting general-purpose text embedding models.

**Link**: [arxiv](http://arxiv.org/abs/2409.07737v1),  [pdf](http://arxiv.org/pdf/2409.07737v1)

**Tags**: cs.CL 



### Large Language Models are Pattern Matchers: Editing Semi-Structured and   Structured Documents with ChatGPT
**Authors**: Irene Weber

**Updated**: 2024-09-12T03:41:39Z

**Summary**: Large Language Models (LLMs) offer numerous applications, the full extent of which is not yet understood. This paper investigates if LLMs can be applied for editing structured and semi-structured documents with minimal effort. Using a qualitative research approach, we conduct two case studies with ChatGPT and thoroughly analyze the results. Our experiments indicate that LLMs can effectively edit structured and semi-structured documents when provided with basic, straightforward prompts. ChatGPT demonstrates a strong ability to recognize and process the structure of annotated documents. This suggests that explicitly structuring tasks and data in prompts might enhance an LLM's ability to understand and solve tasks. Furthermore, the experiments also reveal impressive pattern matching skills in ChatGPT. This observation deserves further investigation, as it may contribute to understanding the processes leading to hallucinations in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2409.07732v1),  [pdf](http://arxiv.org/pdf/2409.07732v1)

**Tags**: cs.LG cs.AI cs.CL I.2 



### LLM4VG: Large Language Models Evaluation for Video Grounding
**Authors**: Wei Feng, Xin Wang, Hong Chen, Zeyang Zhang, Houlun Chen, Zihan Song, Yuwei Zhou, Yuekui Yang, Haiyang Wu, Wenwu Zhu

**Updated**: 2024-09-12T02:57:07Z

**Summary**: Recently, researchers have attempted to investigate the capability of LLMs in handling videos and proposed several video LLM models. However, the ability of LLMs to handle video grounding (VG), which is an important time-related video task requiring the model to precisely locate the start and end timestamps of temporal moments in videos that match the given textual queries, still remains unclear and unexplored in literature. To fill the gap, in this paper, we propose the LLM4VG benchmark, which systematically evaluates the performance of different LLMs on video grounding tasks. Based on our proposed LLM4VG, we design extensive experiments to examine two groups of video LLM models on video grounding: (i) the video LLMs trained on the text-video pairs (denoted as VidLLM), and (ii) the LLMs combined with pretrained visual description models such as the video/image captioning model. We propose prompt methods to integrate the instruction of VG and description from different kinds of generators, including caption-based generators for direct visual description and VQA-based generators for information enhancement. We also provide comprehensive comparisons of various VidLLMs and explore the influence of different choices of visual models, LLMs, prompt designs, etc, as well. Our experimental evaluations lead to two conclusions: (i) the existing VidLLMs are still far away from achieving satisfactory video grounding performance, and more time-related video tasks should be included to further fine-tune these models, and (ii) the combination of LLMs and visual models shows preliminary abilities for video grounding with considerable potential for improvement by resorting to more reliable models and further guidance of prompt instructions.

**Link**: [arxiv](http://arxiv.org/abs/2312.14206v3),  [pdf](http://arxiv.org/pdf/2312.14206v3)

**Tags**: cs.CV 



### DSBench: How Far Are Data Science Agents to Becoming Data Science   Experts?
**Authors**: Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, Dong Yu

**Updated**: 2024-09-12T02:08:00Z

**Summary**: Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have demonstrated impressive language/vision reasoning abilities, igniting the recent trend of building agents for targeted applications such as shopping assistants or AI software engineers. Recently, many data science benchmarks have been proposed to investigate their performance in the data science domain. However, existing data science benchmarks still fall short when compared to real-world data science applications due to their simplified settings. To bridge this gap, we introduce DSBench, a comprehensive benchmark designed to evaluate data science agents with realistic tasks. This benchmark includes 466 data analysis tasks and 74 data modeling tasks, sourced from Eloquence and Kaggle competitions. DSBench offers a realistic setting by encompassing long contexts, multimodal task backgrounds, reasoning with large data files and multi-table structures, and performing end-to-end data modeling tasks. Our evaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle with most tasks, with the best agent solving only 34.12% of data analysis tasks and achieving a 34.74% Relative Performance Gap (RPG). These findings underscore the need for further advancements in developing more practical, intelligent, and autonomous data science agents.

**Link**: [arxiv](http://arxiv.org/abs/2409.07703v1),  [pdf](http://arxiv.org/pdf/2409.07703v1)

**Tags**: cs.AI cs.CL 



### Learn from Balance: Rectifying Knowledge Transfer for Long-Tailed   Scenarios
**Authors**: Xinlei Huang, Jialiang Tang, Xubin Zheng, Jinjia Zhou, Wenxin Yu, Ning Jiang

**Updated**: 2024-09-12T01:58:06Z

**Summary**: Knowledge Distillation (KD) transfers knowledge from a large pre-trained teacher network to a compact and efficient student network, making it suitable for deployment on resource-limited media terminals. However, traditional KD methods require balanced data to ensure robust training, which is often unavailable in practical applications. In such scenarios, a few head categories occupy a substantial proportion of examples. This imbalance biases the trained teacher network towards the head categories, resulting in severe performance degradation on the less represented tail categories for both the teacher and student networks. In this paper, we propose a novel framework called Knowledge Rectification Distillation (KRDistill) to address the imbalanced knowledge inherited in the teacher network through the incorporation of the balanced category priors. Furthermore, we rectify the biased predictions produced by the teacher network, particularly focusing on the tail categories. Consequently, the teacher network can provide balanced and accurate knowledge to train a reliable student network. Intensive experiments conducted on various long-tailed datasets demonstrate that our KRDistill can effectively train reliable student networks in realistic scenarios of data imbalance.

**Link**: [arxiv](http://arxiv.org/abs/2409.07694v1),  [pdf](http://arxiv.org/pdf/2409.07694v1)

**Tags**: cs.CV 



### Cooperative Inference with Interleaved Operator Partitioning for CNNs
**Authors**: Zhibang Liu, Chaonong Xu, Zhizhuo Liu, Lekai Huang, Jiachen Wei, Chao Li

**Updated**: 2024-09-12T01:55:08Z

**Summary**: Deploying deep learning models on Internet of Things (IoT) devices often faces challenges due to limited memory resources and computing capabilities. Cooperative inference is an important method for addressing this issue, requiring the partitioning and distributive deployment of an intelligent model. To perform horizontal partitions, existing cooperative inference methods take either the output channel of operators or the height and width of feature maps as the partition dimensions. In this manner, since the activation of operators is distributed, they have to be concatenated together before being fed to the next operator, which incurs the delay for cooperative inference. In this paper, we propose the Interleaved Operator Partitioning (IOP) strategy for CNN models. By partitioning an operator based on the output channel dimension and its successive operator based on the input channel dimension, activation concatenation becomes unnecessary, thereby reducing the number of communication connections, which consequently reduces cooperative inference de-lay. Based on IOP, we further present a model segmentation algorithm for minimizing cooperative inference time, which greedily selects operators for IOP pairing based on the inference delay benefit harvested. Experimental results demonstrate that compared with the state-of-the-art partition approaches used in CoEdge, the IOP strategy achieves 6.39% ~ 16.83% faster acceleration and reduces peak memory footprint by 21.22% ~ 49.98% for three classical image classification models.

**Link**: [arxiv](http://arxiv.org/abs/2409.07693v1),  [pdf](http://arxiv.org/pdf/2409.07693v1)

**Tags**: cs.DC 



### A Survey of Backdoor Attacks and Defenses on Large Language Models:   Implications for Security Measures
**Authors**: Shuai Zhao, Meihuizi Jia, Zhongliang Guo, Leilei Gan, Xiaoyu Xu, Xiaobao Wu, Jie Fu, Yichao Feng, Fengjun Pan, Luu Anh Tuan

**Updated**: 2024-09-12T00:27:06Z

**Summary**: Large Language Models (LLMs), which bridge the gap between human language understanding and complex problem-solving, achieve state-of-the-art performance on several NLP tasks, particularly in few-shot and zero-shot settings. Despite the demonstrable efficacy of LLMs, due to constraints on computational resources, users have to engage with open-source language models or outsource the entire training process to third-party platforms. However, research has demonstrated that language models are susceptible to potential security vulnerabilities, particularly in backdoor attacks. Backdoor attacks are designed to introduce targeted vulnerabilities into language models by poisoning training samples or model weights, allowing attackers to manipulate model responses through malicious triggers. While existing surveys on backdoor attacks provide a comprehensive overview, they lack an in-depth examination of backdoor attacks specifically targeting LLMs. To bridge this gap and grasp the latest trends in the field, this paper presents a novel perspective on backdoor attacks for LLMs by focusing on fine-tuning methods. Specifically, we systematically classify backdoor attacks into three categories: full-parameter fine-tuning, parameter-efficient fine-tuning, and no fine-tuning Based on insights from a substantial review, we also discuss crucial issues for future research on backdoor attacks, such as further exploring attack algorithms that do not require fine-tuning, or developing more covert attack algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2406.06852v4),  [pdf](http://arxiv.org/pdf/2406.06852v4)

**Tags**: cs.CR cs.AI cs.CL 



### Analytical Optimized Traffic Flow Recovery for Large-scale Urban   Transportation Network
**Authors**: Sicheng Fu, Haotian Shi, Shixiao Liang, Xin Wang, Bin Ran

**Updated**: 2024-09-11T23:37:25Z

**Summary**: The implementation of intelligent transportation systems (ITS) has enhanced data collection in urban transportation through advanced traffic sensing devices. However, the high costs associated with installation and maintenance result in sparse traffic data coverage. To obtain complete, accurate, and high-resolution network-wide traffic flow data, this study introduces the Analytical Optimized Recovery (AOR) approach that leverages abundant GPS speed data alongside sparse flow data to estimate traffic flow in large-scale urban networks. The method formulates a constrained optimization framework that utilizes a quadratic objective function with l2 norm regularization terms to address the traffic flow recovery problem effectively and incorporates a Lagrangian relaxation technique to maintain non-negativity constraints. The effectiveness of this approach was validated in a large urban network in Shenzhen's Futian District using the Simulation of Urban MObility (SUMO) platform. Analytical results indicate that the method achieves low estimation errors, affirming its suitability for comprehensive traffic analysis in urban settings with limited sensor deployment.

**Link**: [arxiv](http://arxiv.org/abs/2409.03906v2),  [pdf](http://arxiv.org/pdf/2409.03906v2)

**Tags**: eess.SY cs.SY 



### Pruning One More Token is Enough: Leveraging Latency-Workload   Non-Linearities for Vision Transformers on the Edge
**Authors**: Nick John Eliopoulos, Purvish Jajal, James Davis, Gaowen Liu, George K. Thiravathukal, Yung-Hsiang Lu

**Updated**: 2024-09-11T21:53:59Z

**Summary**: This paper investigates how to efficiently deploy vision transformers on edge devices for small workloads. Recent methods reduce the latency of transformer neural networks by removing or merging tokens, with small accuracy degradation. However, these methods are not designed with edge device deployment in mind: they do not leverage information about the latency-workload trends to improve efficiency. We address this shortcoming in our work. First, we identify factors that affect ViT latency-workload relationships. Second, we determine token pruning schedule by leveraging non-linear latency-workload relationships. Third, we demonstrate a training-free, token pruning method utilizing this schedule. We show other methods may increase latency by 2-30%, while we reduce latency by 9-26%. For similar latency (within 5.2% or 7ms) across devices we achieve 78.6%-84.5% ImageNet1K accuracy, while the state-of-the-art, Token Merging, achieves 45.8%-85.4%.

**Link**: [arxiv](http://arxiv.org/abs/2407.05941v3),  [pdf](http://arxiv.org/pdf/2407.05941v3)

**Tags**: cs.LG cs.CV 



### SimulBench: Evaluating Language Models with Creative Simulation Tasks
**Authors**: Qi Jia, Xiang Yue, Tianyu Zheng, Jie Huang, Bill Yuchen Lin

**Updated**: 2024-09-11T21:53:20Z

**Summary**: We introduce SimulBench, a benchmark designed to evaluate large language models (LLMs) across a diverse collection of creative simulation scenarios, such as acting as a Linux terminal or playing text games with users. While these simulation tasks serve as effective measures of an LLM's general intelligence, they are seldom incorporated into existing benchmarks. A major challenge is to develop an evaluation framework for testing different LLMs fairly while preserving the multi-round interactive nature of simulation tasks between users and AI. To tackle this issue, we suggest using a fixed LLM as a user agent to engage with an LLM to collect dialogues first under different tasks. Then, challenging dialogue scripts are extracted for evaluating different target LLMs. To facilitate automatic assessment on \DataName{}, GPT-4 is employed as the evaluator, tasked with reviewing the quality of the final response generated by the target LLMs given multi-turn dialogue scripts. Our comprehensive experiments indicate that these simulation tasks continue to pose a significant challenge with their unique natures and show the gap between proprietary models and the most advanced open LLMs. For example, GPT-4-turbo outperforms LLaMA-3-70b-Chat on 18.55\% more cases.

**Link**: [arxiv](http://arxiv.org/abs/2409.07641v1),  [pdf](http://arxiv.org/pdf/2409.07641v1)

**Tags**: cs.CL 



### Continual Skill and Task Learning via Dialogue
**Authors**: Weiwei Gu, Suresh Kondepudi, Lixiao Huang, Nakul Gopalan

**Updated**: 2024-09-11T21:52:22Z

**Summary**: Continual and interactive robot learning is a challenging problem as the robot is present with human users who expect the robot to learn novel skills to solve novel tasks perpetually with sample efficiency. In this work we present a framework for robots to query and learn visuo-motor robot skills and task relevant information via natural language dialog interactions with human users. Previous approaches either focus on improving the performance of instruction following agents, or passively learn novel skills or concepts. Instead, we used dialog combined with a language-skill grounding embedding to query or confirm skills and/or tasks requested by a user. To achieve this goal, we developed and integrated three different components for our agent. Firstly, we propose a novel visual-motor control policy ACT with Low Rank Adaptation (ACT-LoRA), which enables the existing SoTA ACT model to perform few-shot continual learning. Secondly, we develop an alignment model that projects demonstrations across skill embodiments into a shared embedding allowing us to know when to ask questions and/or demonstrations from users. Finally, we integrated an existing LLM to interact with a human user to perform grounded interactive continual skill learning to solve a task. Our ACT-LoRA model learns novel fine-tuned skills with a 100% accuracy when trained with only five demonstrations for a novel skill while still maintaining a 74.75% accuracy on pre-trained skills in the RLBench dataset where other models fall significantly short. We also performed a human-subjects study with 8 subjects to demonstrate the continual learning capabilities of our combined framework. We achieve a success rate of 75% in the task of sandwich making with the real robot learning from participant data demonstrating that robots can learn novel skills or task knowledge from dialogue with non-expert users using our approach.

**Link**: [arxiv](http://arxiv.org/abs/2409.03166v2),  [pdf](http://arxiv.org/pdf/2409.03166v2)

**Tags**: cs.RO cs.AI cs.CL 



### Can We Count on LLMs? The Fixed-Effect Fallacy and Claims of GPT-4   Capabilities
**Authors**: Thomas Ball, Shuo Chen, Cormac Herley

**Updated**: 2024-09-11T21:48:33Z

**Summary**: In this paper we explore evaluation of LLM capabilities. We present measurements of GPT-4 performance on several deterministic tasks; each task involves a basic calculation and takes as input parameter some element drawn from a large well-defined population (e.g., count elements in a list, multiply two k-digit numbers, etc). We examine several conditions per-task and perform enough trials so that statistically significant differences can be detected. This allows us to investigate the sensitivity of task-accuracy both to query phrasing and input parameter population. We find that seemingly trivial modifications in the task-prompt or input population can yield differences far larger than can be explained by sampling effects. For example, performance on a simple list-counting task varies with query-phrasing and list-length, but also with list composition (i.e., the thing-to-be-counted) and object frequency (e.g., success when an element accounts for $\approx$ 50\% of a list is different from when it accounts for $\approx$ 70\% etc).   We conclude that efforts to quantify LLM capabilities easily succumb to the language-as-fixed-effect fallacy, where experimental observations are improperly generalized beyond what the data supports. A consequence appears to be that intuitions that have been formed based on interactions with humans form a very unreliable guide as to which input modifications should ``make no difference'' to LLM performance.

**Link**: [arxiv](http://arxiv.org/abs/2409.07638v1),  [pdf](http://arxiv.org/pdf/2409.07638v1)

**Tags**: cs.AI cs.CL cs.LG 



### Can ChatGPT Forecast Stock Price Movements? Return Predictability and   Large Language Models
**Authors**: Alejandro Lopez-Lira, Yuehua Tang

**Updated**: 2024-09-11T21:23:04Z

**Summary**: We document the capability of large language models (LLMs) like ChatGPT to predict stock price movements using news headlines, even without direct financial training. ChatGPT scores significantly predict out-of-sample daily stock returns, subsuming traditional methods, and predictability is stronger among smaller stocks and following negative news. To explain these findings, we develop a theoretical model incorporating information capacity constraints, underreaction, limits-to-arbitrage, and LLMs. The model generates several key predictions, which we empirically test: (i) it establishes a critical threshold in AI capabilities necessary for profitable predictions, (ii) it demonstrates that only advanced LLMs can effectively interpret complex information, and (iii) it predicts that widespread LLM adoption can enhance market efficiency. Our results suggest that sophisticated return forecasting is an emerging capability of AI systems and that these technologies can alter information diffusion and decision-making processes in financial markets. Finally, we introduce an interpretability framework to evaluate LLMs' reasoning, contributing to AI transparency and economic decision-making.

**Link**: [arxiv](http://arxiv.org/abs/2304.07619v5),  [pdf](http://arxiv.org/pdf/2304.07619v5)

**Tags**: q-fin.ST cs.CL 



### A Training Rate and Survival Heuristic for Inference and Robustness   Evaluation (TRASHFIRE)
**Authors**: Charles Meyers, Mohammad Reza Saleh Sedghpour, Tommy Löfstedt, Erik Elmroth

**Updated**: 2024-09-11T20:55:32Z

**Summary**: Machine learning models -- deep neural networks in particular -- have performed remarkably well on benchmark datasets across a wide variety of domains. However, the ease of finding adversarial counter-examples remains a persistent problem when training times are measured in hours or days and the time needed to find a successful adversarial counter-example is measured in seconds. Much work has gone into generating and defending against these adversarial counter-examples, however the relative costs of attacks and defences are rarely discussed. Additionally, machine learning research is almost entirely guided by test/train metrics, but these would require billions of samples to meet industry standards. The present work addresses the problem of understanding and predicting how particular model hyper-parameters influence the performance of a model in the presence of an adversary. The proposed approach uses survival models, worst-case examples, and a cost-aware analysis to precisely and accurately reject a particular model change during routine model training procedures rather than relying on real-world deployment, expensive formal verification methods, or accurate simulations of very complicated systems (\textit{e.g.}, digitally recreating every part of a car or a plane). Through an evaluation of many pre-processing techniques, adversarial counter-examples, and neural network configurations, the conclusion is that deeper models do offer marginal gains in survival times compared to more shallow counterparts. However, we show that those gains are driven more by the model inference time than inherent robustness properties. Using the proposed methodology, we show that ResNet is hopelessly insecure against even the simplest of white box attacks.

**Link**: [arxiv](http://arxiv.org/abs/2401.13751v2),  [pdf](http://arxiv.org/pdf/2401.13751v2)

**Tags**: cs.LG cs.AI cs.CV stat.ML 



### Zero-Shot Machine-Generated Text Detection Using Mixture of Large   Language Models
**Authors**: Matthieu Dubois, François Yvon, Pablo Piantanida

**Updated**: 2024-09-11T20:55:12Z

**Summary**: The dissemination of Large Language Models (LLMs), trained at scale, and endowed with powerful text-generating abilities has vastly increased the threats posed by generative AI technologies by reducing the cost of producing harmful, toxic, faked or forged content. In response, various proposals have been made to automatically discriminate artificially generated from human-written texts, typically framing the problem as a classification problem. Most approaches evaluate an input document by a well-chosen detector LLM, assuming that low-perplexity scores reliably signal machine-made content. As using one single detector can induce brittleness of performance, we instead consider several and derive a new, theoretically grounded approach to combine their respective strengths. Our experiments, using a variety of generator LLMs, suggest that our method effectively increases the robustness of detection.

**Link**: [arxiv](http://arxiv.org/abs/2409.07615v1),  [pdf](http://arxiv.org/pdf/2409.07615v1)

**Tags**: cs.CL 



### A Cost-Aware Approach to Adversarial Robustness in Neural Networks
**Authors**: Charles Meyers, Mohammad Reza Saleh Sedghpour, Tommy Löfstedt, Erik Elmroth

**Updated**: 2024-09-11T20:43:59Z

**Summary**: Considering the growing prominence of production-level AI and the threat of adversarial attacks that can evade a model at run-time, evaluating the robustness of models to these evasion attacks is of critical importance. Additionally, testing model changes likely means deploying the models to (e.g. a car or a medical imaging device), or a drone to see how it affects performance, making un-tested changes a public problem that reduces development speed, increases cost of development, and makes it difficult (if not impossible) to parse cause from effect. In this work, we used survival analysis as a cloud-native, time-efficient and precise method for predicting model performance in the presence of adversarial noise. For neural networks in particular, the relationships between the learning rate, batch size, training time, convergence time, and deployment cost are highly complex, so researchers generally rely on benchmark datasets to assess the ability of a model to generalize beyond the training data. To address this, we propose using accelerated failure time models to measure the effect of hardware choice, batch size, number of epochs, and test-set accuracy by using adversarial attacks to induce failures on a reference model architecture before deploying the model to the real world. We evaluate several GPU types and use the Tree Parzen Estimator to maximize model robustness and minimize model run-time simultaneously. This provides a way to evaluate the model and optimise it in a single step, while simultaneously allowing us to model the effect of model parameters on training time, prediction time, and accuracy. Using this technique, we demonstrate that newer, more-powerful hardware does decrease the training time, but with a monetary and power cost that far outpaces the marginal gains in accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2409.07609v1),  [pdf](http://arxiv.org/pdf/2409.07609v1)

**Tags**: cs.CR cs.CV cs.LG stat.AP 



### Multilingual Prompts in LLM-Based Recommenders: Performance Across   Languages
**Authors**: Makbule Gulcin Ozsoy

**Updated**: 2024-09-11T20:31:42Z

**Summary**: Large language models (LLMs) are increasingly used in natural language processing tasks. Recommender systems traditionally use methods such as collaborative filtering and matrix factorization, as well as advanced techniques like deep learning and reinforcement learning. Although language models have been applied in recommendation, the recent trend have focused on leveraging the generative capabilities of LLMs for more personalized suggestions. While current research focuses on English due to its resource richness, this work explores the impact of non-English prompts on recommendation performance. Using OpenP5, a platform for developing and evaluating LLM-based recommendations, we expanded its English prompt templates to include Spanish and Turkish. Evaluation on three real-world datasets, namely ML1M, LastFM, and Amazon-Beauty, showed that usage of non-English prompts generally reduce performance, especially in less-resourced languages like Turkish. We also retrained an LLM-based recommender model with multilingual prompts to analyze performance variations. Retraining with multilingual prompts resulted in more balanced performance across languages, but slightly reduced English performance. This work highlights the need for diverse language support in LLM-based recommenders and suggests future research on creating evaluation datasets, using newer models and additional languages.

**Link**: [arxiv](http://arxiv.org/abs/2409.07604v1),  [pdf](http://arxiv.org/pdf/2409.07604v1)

**Tags**: cs.IR 



### Jäger: Automated Telephone Call Traceback
**Authors**: David Adei, Varun Madathil, Sathvik Prasad, Bradley Reaves, Alessandra Scafuro

**Updated**: 2024-09-11T20:06:26Z

**Summary**: Unsolicited telephone calls that facilitate fraud or unlawful telemarketing continue to overwhelm network users and the regulators who prosecute them. The first step in prosecuting phone abuse is traceback -- identifying the call originator. This fundamental investigative task currently requires hours of manual effort per call. In this paper, we introduce J\"ager, a distributed secure call traceback system. J\"ager can trace a call in a few seconds, even with partial deployment, while cryptographically preserving the privacy of call parties, carrier trade secrets like peers and call volume, and limiting the threat of bulk analysis. We establish definitions and requirements of secure traceback, then develop a suite of protocols that meet these requirements using witness encryption, oblivious pseudorandom functions, and group signatures. We prove these protocols secure in the universal composibility framework. We then demonstrate that J\"ager has low compute and bandwidth costs per call, and these costs scale linearly with call volume. J\"ager provides an efficient, secure, privacy-preserving system to revolutionize telephone abuse investigation with minimal costs to operators.

**Link**: [arxiv](http://arxiv.org/abs/2409.02839v3),  [pdf](http://arxiv.org/pdf/2409.02839v3)

**Tags**: cs.CR cs.CY cs.NI 



### CAVERNAUTE: a design and manufacturing pipeline of a rigid but foldable   indoor airship aerial system for cave exploration
**Authors**: Catar Louis, Tabiai Ilyass, St-Onge David

**Updated**: 2024-09-11T19:47:02Z

**Summary**: Airships, best recognized for their unique quality of payload/energy ratio, present a fascinating challenge for the field of engineering. Their construction and operation require a delicate balance of materials and rules, making them a compelling object of study. They embody a distinct intersection of physics, design, and innovation, offering a wide array of possibilities for future transportation and exploration. Thanks to their long-flight endurance, they are suited for long-term missions. To operate in complex environments such as indoor cluttered spaces, their membrane and mechatronics need to be protected from impacts. This paper presents a new indoor airship design inspired by origami and the Kresling pattern. The airship structure combines a carbon fiber exoskeleton and UV resin micro-lattices for shock absorption. Our design strengthens the robot while granting the ability to access narrow spaces by folding the structure - up to a volume expansion ratio of 19.8. To optimize the numerous parameters of the airship, we present a pipeline for design, manufacture, and assembly. It takes into account manufacturing constraints, dimensions of the target deployment area, and aerostatics, allowing for easy and quick testing of new configurations. We also present unique features made possible by combining origami with airship design, which reduces the chances of mission-compromising failures. We demonstrate the potential of the design with a complete simulation including an effective control strategy leveraging lightweight mechatronics to optimize flight autonomy in exploration missions of unstructured environments.

**Link**: [arxiv](http://arxiv.org/abs/2409.07591v1),  [pdf](http://arxiv.org/pdf/2409.07591v1)

**Tags**: cs.RO cond-mat.other 



### Exploring LLMs for Malware Detection: Review, Framework Design, and   Countermeasure Approaches
**Authors**: Jamal Al-Karaki, Muhammad Al-Zafar Khan, Marwan Omar

**Updated**: 2024-09-11T19:33:44Z

**Summary**: The rising use of Large Language Models (LLMs) to create and disseminate malware poses a significant cybersecurity challenge due to their ability to generate and distribute attacks with ease. A single prompt can initiate a wide array of malicious activities. This paper addresses this critical issue through a multifaceted approach. First, we provide a comprehensive overview of LLMs and their role in malware detection from diverse sources. We examine five specific applications of LLMs: Malware honeypots, identification of text-based threats, code analysis for detecting malicious intent, trend analysis of malware, and detection of non-standard disguised malware. Our review includes a detailed analysis of the existing literature and establishes guiding principles for the secure use of LLMs. We also introduce a classification scheme to categorize the relevant literature. Second, we propose performance metrics to assess the effectiveness of LLMs in these contexts. Third, we present a risk mitigation framework designed to prevent malware by leveraging LLMs. Finally, we evaluate the performance of our proposed risk mitigation strategies against various factors and demonstrate their effectiveness in countering LLM-enabled malware. The paper concludes by suggesting future advancements and areas requiring deeper exploration in this fascinating field of artificial intelligence.

**Link**: [arxiv](http://arxiv.org/abs/2409.07587v1),  [pdf](http://arxiv.org/pdf/2409.07587v1)

**Tags**: cs.CR 



### Exploring the Ecosystem of DNS HTTPS Resource Records: An End-to-End   Perspective
**Authors**: Hongying Dong, Yizhe Zhang, Hyeonmin Lee, Shumon Huque, Yixin Sun

**Updated**: 2024-09-11T19:33:11Z

**Summary**: The DNS HTTPS resource record is a new DNS record type designed for the delivery of configuration information and parameters required to initiate connections to HTTPS network services. In addition, it is a key enabler for TLS Encrypted ClientHello (ECH) by providing the cryptographic keying material needed to encrypt the initial exchange. To understand the adoption of this new DNS HTTPS record, we perform a longitudinal study on the server-side deployment of DNS HTTPS for Tranco top million domains, as well as an analysis of the client-side support for DNS HTTPS through snapshots from major browsers. To the best of our knowledge, our work is the first longitudinal study on DNS HTTPS server deployment, and the first known study on client-side support for DNS HTTPS. Despite the rapidly growing trend of DNS HTTPS adoption, our study highlights challenges and concerns in the deployment by both servers and clients, such as the complexity in properly maintaining HTTPS records and connection failure in browsers when the HTTPS record is not properly configured.

**Link**: [arxiv](http://arxiv.org/abs/2403.15672v2),  [pdf](http://arxiv.org/pdf/2403.15672v2)

**Tags**: cs.NI 



### A Novel Mathematical Framework for Objective Evaluation of Ideas using a   Conversational AI (CAI) System
**Authors**: B. Sankar, Dibakar Sen

**Updated**: 2024-09-11T19:10:29Z

**Summary**: The demand for innovation in product design necessitates a prolific ideation phase. Conversational AI (CAI) systems that use Large Language Models (LLMs) such as GPT (Generative Pre-trained Transformer) have been shown to be fruitful in augmenting human creativity, providing numerous novel and diverse ideas. Despite the success in ideation quantity, the qualitative assessment of these ideas remains challenging and traditionally reliant on expert human evaluation. This method suffers from limitations such as human judgment errors, bias, and oversight. Addressing this gap, our study introduces a comprehensive mathematical framework for automated analysis to objectively evaluate the plethora of ideas generated by CAI systems and/or humans. This framework is particularly advantageous for novice designers who lack experience in selecting promising ideas. By converting the ideas into higher dimensional vectors and quantitatively measuring the diversity between them using tools such as UMAP, DBSCAN and PCA, the proposed method provides a reliable and objective way of selecting the most promising ideas, thereby enhancing the efficiency of the ideation phase.

**Link**: [arxiv](http://arxiv.org/abs/2409.07578v1),  [pdf](http://arxiv.org/pdf/2409.07578v1)

**Tags**: cs.AI 53A45 I.2.7; G.3 



### Initial performance of the Radar Echo Telescope for Cosmic Rays, RET-CR
**Authors**: P. Allison, J. Beatty, D. Besson, A. Connolly, A. Cummings, C. Deaconu, S. De Kockere, K. D. de Vries, D. Frikken, C. Hast, E. Huesca Santiago, C. -Y. Kuo, A. Kyriacou, U. A. Latif, J. Loonen, I. Loudon, V. Lukic, C. McLennan, K. Mulrey, J. Nam, K. Nivedita, A. Nozdrina, E. Oberla, S. Prohira, J. P. Ralston, M. F. H. Seikh, R. S. Stanley, S. Toscano, D. Van den Broeck, N. van Eijndhoven, S. Wissel

**Updated**: 2024-09-11T18:00:00Z

**Summary**: The Radar Echo Telescope for Cosmic Rays (RET-CR), a pathfinder instrument for the radar echo method of ultrahigh energy (UHE) neutrino detection, was initially deployed near Summit Station, Greenland, in May 2023. After a 4 week commissioning period, 9 days of data were taken before the instrument went offline. In this article, we describe the instrument as it was deployed, and the initial performance of the detector. We show that the technical aspects of running a radar based particle cascade detector in the ice have been demonstrated. Analysis of the 2023 data informed improvements that were incorporated into the May-August 2024 deployment, which has just concluded at time of writing. Results from the 2024 run will be presented in forthcoming publications.

**Link**: [arxiv](http://arxiv.org/abs/2409.07511v1),  [pdf](http://arxiv.org/pdf/2409.07511v1)

**Tags**: hep-ex astro-ph.HE 



### "My Grade is Wrong!": A Contestable AI Framework for Interactive   Feedback in Evaluating Student Essays
**Authors**: Shengxin Hong, Chang Cai, Sixuan Du, Haiyue Feng, Siyuan Liu, Xiuyi Fan

**Updated**: 2024-09-11T17:59:01Z

**Summary**: Interactive feedback, where feedback flows in both directions between teacher and student, is more effective than traditional one-way feedback. However, it is often too time-consuming for widespread use in educational practice. While Large Language Models (LLMs) have potential for automating feedback, they struggle with reasoning and interaction in an interactive setting. This paper introduces CAELF, a Contestable AI Empowered LLM Framework for automating interactive feedback. CAELF allows students to query, challenge, and clarify their feedback by integrating a multi-agent system with computational argumentation. Essays are first assessed by multiple Teaching-Assistant Agents (TA Agents), and then a Teacher Agent aggregates the evaluations through formal reasoning to generate feedback and grades. Students can further engage with the feedback to refine their understanding. A case study on 500 critical thinking essays with user studies demonstrates that CAELF significantly improves interactive feedback, enhancing the reasoning and interaction capabilities of LLMs. This approach offers a promising solution to overcoming the time and resource barriers that have limited the adoption of interactive feedback in educational settings.

**Link**: [arxiv](http://arxiv.org/abs/2409.07453v1),  [pdf](http://arxiv.org/pdf/2409.07453v1)

**Tags**: cs.AI cs.HC 



### SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research   Repositories
**Authors**: Ben Bogin, Kejuan Yang, Shashank Gupta, Kyle Richardson, Erin Bransom, Peter Clark, Ashish Sabharwal, Tushar Khot

**Updated**: 2024-09-11T17:37:48Z

**Summary**: Given that Large Language Models (LLMs) have made significant progress in writing code, can they now be used to autonomously reproduce results from research repositories? Such a capability would be a boon to the research community, helping researchers validate, understand, and extend prior work. To advance towards this goal, we introduce SUPER, the first benchmark designed to evaluate the capability of LLMs in setting up and executing tasks from research repositories. SUPERaims to capture the realistic challenges faced by researchers working with Machine Learning (ML) and Natural Language Processing (NLP) research repositories. Our benchmark comprises three distinct problem sets: 45 end-to-end problems with annotated expert solutions, 152 sub problems derived from the expert set that focus on specific challenges (e.g., configuring a trainer), and 602 automatically generated problems for larger-scale development. We introduce various evaluation measures to assess both task success and progress, utilizing gold solutions when available or approximations otherwise. We show that state-of-the-art approaches struggle to solve these problems with the best model (GPT-4o) solving only 16.3% of the end-to-end set, and 46.1% of the scenarios. This illustrates the challenge of this task, and suggests that SUPER can serve as a valuable resource for the community to make and measure progress.

**Link**: [arxiv](http://arxiv.org/abs/2409.07440v1),  [pdf](http://arxiv.org/pdf/2409.07440v1)

**Tags**: cs.AI cs.CL cs.SE 



### Towards Fairer Health Recommendations: finding informative unbiased   samples via Word Sense Disambiguation
**Authors**: Gavin Butts, Pegah Emdad, Jethro Lee, Shannon Song, Chiman Salavati, Willmar Sosa Diaz, Shiri Dori-Hacohen, Fabricio Murai

**Updated**: 2024-09-11T17:10:20Z

**Summary**: There have been growing concerns around high-stake applications that rely on models trained with biased data, which consequently produce biased predictions, often harming the most vulnerable. In particular, biased medical data could cause health-related applications and recommender systems to create outputs that jeopardize patient care and widen disparities in health outcomes. A recent framework titled Fairness via AI posits that, instead of attempting to correct model biases, researchers must focus on their root causes by using AI to debias data. Inspired by this framework, we tackle bias detection in medical curricula using NLP models, including LLMs, and evaluate them on a gold standard dataset containing 4,105 excerpts annotated by medical experts for bias from a large corpus. We build on previous work by coauthors which augments the set of negative samples with non-annotated text containing social identifier terms. However, some of these terms, especially those related to race and ethnicity, can carry different meanings (e.g., "white matter of spinal cord"). To address this issue, we propose the use of Word Sense Disambiguation models to refine dataset quality by removing irrelevant sentences. We then evaluate fine-tuned variations of BERT models as well as GPT models with zero- and few-shot prompting. We found LLMs, considered SOTA on many NLP tasks, unsuitable for bias detection, while fine-tuned BERT models generally perform well across all evaluated metrics.

**Link**: [arxiv](http://arxiv.org/abs/2409.07424v1),  [pdf](http://arxiv.org/pdf/2409.07424v1)

**Tags**: cs.CL cs.CY cs.LG I.2.7; J.3; K.4 



### Moderating Model Marketplaces: Platform Governance Puzzles for AI   Intermediaries
**Authors**: Robert Gorwa, Michael Veale

**Updated**: 2024-09-11T16:52:44Z

**Summary**: The AI development community is increasingly making use of hosting intermediaries such as Hugging Face provide easy access to user-uploaded models and training data. These model marketplaces lower technical deployment barriers for hundreds of thousands of users, yet can be used in numerous potentially harmful and illegal ways. In this article, we explain ways in which AI systems, which can both `contain' content and be open-ended tools, present one of the trickiest platform governance challenges seen to date. We provide case studies of several incidents across three illustrative platforms -- Hugging Face, GitHub and Civitai -- to examine how model marketplaces moderate models. Building on this analysis, we outline important (and yet nevertheless limited) practices that industry has been developing to respond to moderation demands: licensing, access and use restrictions, automated content moderation, and open policy development. While the policy challenge at hand is a considerable one, we conclude with some ideas as to how platforms could better mobilize resources to act as a careful, fair, and proportionate regulatory access point.

**Link**: [arxiv](http://arxiv.org/abs/2311.12573v3),  [pdf](http://arxiv.org/pdf/2311.12573v3)

**Tags**: cs.CY cs.AI cs.LG 



### CLNX: Bridging Code and Natural Language for C/C++   Vulnerability-Contributing Commits Identification
**Authors**: Zeqing Qin, Yiwei Wu, Lansheng Han

**Updated**: 2024-09-11T16:49:46Z

**Summary**: Large Language Models (LLMs) have shown great promise in vulnerability identification. As C/C++ comprises half of the Open-Source Software (OSS) vulnerabilities over the past decade and updates in OSS mainly occur through commits, enhancing LLMs' ability to identify C/C++ Vulnerability-Contributing Commits (VCCs) is essential. However, current studies primarily focus on further pre-training LLMs on massive code datasets, which is resource-intensive and poses efficiency challenges. In this paper, we enhance the ability of BERT-based LLMs to identify C/C++ VCCs in a lightweight manner. We propose CodeLinguaNexus (CLNX) as a bridge facilitating communication between C/C++ programs and LLMs. Based on commits, CLNX efficiently converts the source code into a more natural representation while preserving key details. Specifically, CLNX first applies structure-level naturalization to decompose complex programs, followed by token-level naturalization to interpret complex symbols. We evaluate CLNX on public datasets of 25,872 C/C++ functions with their commits. The results show that CLNX significantly enhances the performance of LLMs on identifying C/C++ VCCs. Moreover, CLNX-equipped CodeBERT achieves new state-of-the-art and identifies 38 OSS vulnerabilities in the real world.

**Link**: [arxiv](http://arxiv.org/abs/2409.07407v1),  [pdf](http://arxiv.org/pdf/2409.07407v1)

**Tags**: cs.CR cs.AI 68M25 



### AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and   Parametric Knowledge
**Authors**: Han Wang, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal

**Updated**: 2024-09-11T16:35:18Z

**Summary**: Knowledge conflict arises from discrepancies between information in the context of a large language model (LLM) and the knowledge stored in its parameters. This can hurt performance when using standard decoding techniques, which tend to ignore the context. Existing test-time contrastive methods seek to address this by comparing the LLM's output distribution with and without the context and adjust the model according to the contrast between them. However, we find that these methods frequently misjudge the degree of conflict and struggle to handle instances that vary in their amount of conflict, with static methods over-adjusting when conflict is absent. We propose a fine-grained, instance-level approach called AdaCAD, which dynamically infers the weight of adjustment based on the degree of conflict, as measured by the Jensen-Shannon divergence between distributions representing contextual and parametric knowledge. Our experiments across four models on six diverse question-answering (QA) datasets and three summarization tasks demonstrate that our training-free adaptive method consistently outperforms other decoding methods on QA, with average accuracy gains of 14.21% (absolute) over a static contrastive baseline, and improves the factuality of summaries by 5.59 (AlignScore). Furthermore, our analysis shows that while decoding with contrastive baselines hurts performance when conflict is absent, AdaCAD mitigates these losses, making it more applicable to real-world datasets in which some examples have conflict and others do not.

**Link**: [arxiv](http://arxiv.org/abs/2409.07394v1),  [pdf](http://arxiv.org/pdf/2409.07394v1)

**Tags**: cs.CL 



### LongGenbench: Benchmarking Long-Form Generation in Long Context LLMs
**Authors**: Yuhao Wu, Ming Shan Hee, Zhiqing Hu, Roy Ka-Wei Lee

**Updated**: 2024-09-11T16:35:00Z

**Summary**: The abilities of long-context language models (LMs) are often evaluated using the "Needle-in-a-Haystack" (NIAH) test, which comprises tasks designed to assess a model's ability to identify specific information ("needle") within large text sequences ("haystack"). While these benchmarks measure how well models understand long-context input sequences, they do not effectively gauge the quality of long-form text generation--a critical aspect for applications such as design proposals and creative writing. To address this gap, we have introduced a new long-form text evaluation benchmark, LongGenbench, which tests models' ability to identify specific events within generated long text sequences. In this benchmark, we prompt long-context LMs to create long-form text that must include particular events or constraints and evaluate their ability to incorporate these elements. We evaluated ten long-context LMs across four distinct scenarios, three types of prompt instructions, and two different generation-length settings (16K and 32K). Although these models perform well on NIAH benchmarks, none demonstrated satisfactory performance on the LongGenbench, raising concerns about their ability to generate coherent long-form text that follows instructions. Additionally, as the length of the generated text increases, all models exhibit a significant drop in performance.

**Link**: [arxiv](http://arxiv.org/abs/2409.02076v3),  [pdf](http://arxiv.org/pdf/2409.02076v3)

**Tags**: cs.CL 



### Awaking the Slides: A Tuning-free and Knowledge-regulated AI Tutoring   System via Language Model Coordination
**Authors**: Daniel Zhang-Li, Zheyuan Zhang, Jifan Yu, Joy Lim Jia Yin, Shangqing Tu, Linlu Gong, Haohua Wang, Zhiyuan Liu, Huiqin Liu, Lei Hou, Juanzi Li

**Updated**: 2024-09-11T16:03:09Z

**Summary**: The vast pre-existing slides serve as rich and important materials to carry lecture knowledge. However, effectively leveraging lecture slides to serve students is difficult due to the multi-modal nature of slide content and the heterogeneous teaching actions. We study the problem of discovering effective designs that convert a slide into an interactive lecture. We develop Slide2Lecture, a tuning-free and knowledge-regulated intelligent tutoring system that can (1) effectively convert an input lecture slide into a structured teaching agenda consisting of a set of heterogeneous teaching actions; (2) create and manage an interactive lecture that generates responsive interactions catering to student learning demands while regulating the interactions to follow teaching actions. Slide2Lecture contains a complete pipeline for learners to obtain an interactive classroom experience to learn the slide. For teachers and developers, Slide2Lecture enables customization to cater to personalized demands. The evaluation rated by annotators and students shows that Slide2Lecture is effective in outperforming the remaining implementation. Slide2Lecture's online deployment has made more than 200K interaction with students in the 3K lecture sessions. We open source Slide2Lecture's implementation in https://anonymous.4open.science/r/slide2lecture-4210/.

**Link**: [arxiv](http://arxiv.org/abs/2409.07372v1),  [pdf](http://arxiv.org/pdf/2409.07372v1)

**Tags**: cs.CL cs.AI cs.HC 



### Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation   of Code
**Authors**: Khiem Ton, Nhi Nguyen, Mahmoud Nazzal, Abdallah Khreishah, Cristian Borcea, NhatHai Phan, Ruoming Jin, Issa Khalil, Yelong Shen

**Updated**: 2024-09-11T15:56:15Z

**Summary**: This paper introduces SGCode, a flexible prompt-optimizing system to generate secure code with large language models (LLMs). SGCode integrates recent prompt-optimization approaches with LLMs in a unified system accessible through front-end and back-end APIs, enabling users to 1) generate secure code, which is free of vulnerabilities, 2) review and share security analysis, and 3) easily switch from one prompt optimization approach to another, while providing insights on model and system performance. We populated SGCode on an AWS server with PromSec, an approach that optimizes prompts by combining an LLM and security tools with a lightweight generative adversarial graph neural network to detect and fix security vulnerabilities in the generated code. Extensive experiments show that SGCode is practical as a public tool to gain insights into the trade-offs between model utility, secure code generation, and system cost. SGCode has only a marginal cost compared with prompting LLMs. SGCode is available at: http://3.131.141.63:8501/.

**Link**: [arxiv](http://arxiv.org/abs/2409.07368v1),  [pdf](http://arxiv.org/pdf/2409.07368v1)

**Tags**: cs.CR cs.AI 



### CriticEval: Evaluating Large Language Model as Critic
**Authors**: Tian Lan, Wenwei Zhang, Chen Xu, Heyan Huang, Dahua Lin, Kai Chen, Xian-ling Mao

**Updated**: 2024-09-11T15:47:11Z

**Summary**: Critique ability, i.e., the capability of Large Language Models (LLMs) to identify and rectify flaws in responses, is crucial for their applications in self-improvement and scalable oversight. While numerous studies have been proposed to evaluate critique ability of LLMs, their comprehensiveness and reliability are still limited. To overcome this problem, we introduce CriticEval, a novel benchmark designed to comprehensively and reliably evaluate critique ability of LLMs. Specifically, to ensure the comprehensiveness, CriticEval evaluates critique ability from four dimensions across nine diverse task scenarios. It evaluates both scalar-valued and textual critiques, targeting responses of varying quality. To ensure the reliability, a large number of critiques are annotated to serve as references, enabling GPT-4 to evaluate textual critiques reliably. Extensive evaluations of open-source and closed-source LLMs first validate the reliability of evaluation in CriticEval. Then, experimental results demonstrate the promising potential of open-source LLMs, the effectiveness of critique datasets and several intriguing relationships between the critique ability and some critical factors, including task types, response qualities and critique dimensions. Datasets and evaluation toolkit for CriticEval will be publicly released.

**Link**: [arxiv](http://arxiv.org/abs/2402.13764v4),  [pdf](http://arxiv.org/pdf/2402.13764v4)

**Tags**: cs.CL cs.AI 



### Think Together and Work Better: Combining Humans' and LLMs' Think-Aloud   Outcomes for Effective Text Evaluation
**Authors**: SeongYeub Chu, JongWoo Kim, MunYong Yi

**Updated**: 2024-09-11T15:40:07Z

**Summary**: This study introduces \textbf{InteractEval}, a framework that integrates human expertise and Large Language Models (LLMs) using the Think-Aloud (TA) method to generate attributes for checklist-based text evaluation. By combining human flexibility and reasoning with LLM consistency, InteractEval outperforms traditional non-LLM-based and LLM-based baselines across four distinct dimensions, consisting of Coherence, Fluency, Consistency, and Relevance. The experiment also investigates the effectiveness of the TA method, showing that it promotes divergent thinking in both humans and LLMs, leading to the generation of a wider range of relevant attributes and enhance text evaluation performance. Comparative analysis reveals that humans excel at identifying attributes related to internal quality (Coherence and Fluency), but LLMs perform better at those attributes related to external alignment (Consistency and Relevance). Consequently, leveraging both humans and LLMs together produces the best evaluation outcomes. In other words, this study emphasizes the necessity of effectively combining humans and LLMs in an automated checklist-based text evaluation framework. The code is available at \textbf{\url{https://github.com/BBeeChu/InteractEval.git}}.

**Link**: [arxiv](http://arxiv.org/abs/2409.07355v1),  [pdf](http://arxiv.org/pdf/2409.07355v1)

**Tags**: cs.CL 



### Securing Vision-Language Models with a Robust Encoder Against Jailbreak   and Adversarial Attacks
**Authors**: Md Zarif Hossain, Ahmed Imteaj

**Updated**: 2024-09-11T15:39:42Z

**Summary**: Large Vision-Language Models (LVLMs), trained on multimodal big datasets, have significantly advanced AI by excelling in vision-language tasks. However, these models remain vulnerable to adversarial attacks, particularly jailbreak attacks, which bypass safety protocols and cause the model to generate misleading or harmful responses. This vulnerability stems from both the inherent susceptibilities of LLMs and the expanded attack surface introduced by the visual modality. We propose Sim-CLIP+, a novel defense mechanism that adversarially fine-tunes the CLIP vision encoder by leveraging a Siamese architecture. This approach maximizes cosine similarity between perturbed and clean samples, facilitating resilience against adversarial manipulations. Sim-CLIP+ offers a plug-and-play solution, allowing seamless integration into existing LVLM architectures as a robust vision encoder. Unlike previous defenses, our method requires no structural modifications to the LVLM and incurs minimal computational overhead. Sim-CLIP+ demonstrates effectiveness against both gradient-based adversarial attacks and various jailbreak techniques. We evaluate Sim-CLIP+ against three distinct jailbreak attack strategies and perform clean evaluations using standard downstream datasets, including COCO for image captioning and OKVQA for visual question answering. Extensive experiments demonstrate that Sim-CLIP+ maintains high clean accuracy while substantially improving robustness against both gradient-based adversarial attacks and jailbreak techniques. Our code and robust vision encoders are available at https://github.com/speedlab-git/Robust-Encoder-against-Jailbreak-attack.git.

**Link**: [arxiv](http://arxiv.org/abs/2409.07353v1),  [pdf](http://arxiv.org/pdf/2409.07353v1)

**Tags**: cs.CV cs.AI 



### MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical   Applications
**Authors**: Praveen K Kanithi, Clément Christophe, Marco AF Pimentel, Tathagata Raha, Nada Saadi, Hamza Javed, Svetlana Maslenkova, Nasir Hayat, Ronnie Rajan, Shadab Khan

**Updated**: 2024-09-11T14:44:51Z

**Summary**: The rapid development of Large Language Models (LLMs) for healthcare applications has spurred calls for holistic evaluation beyond frequently-cited benchmarks like USMLE, to better reflect real-world performance. While real-world assessments are valuable indicators of utility, they often lag behind the pace of LLM evolution, likely rendering findings obsolete upon deployment. This temporal disconnect necessitates a comprehensive upfront evaluation that can guide model selection for specific clinical applications. We introduce MEDIC, a framework assessing LLMs across five critical dimensions of clinical competence: medical reasoning, ethics and bias, data and language understanding, in-context learning, and clinical safety. MEDIC features a novel cross-examination framework quantifying LLM performance across areas like coverage and hallucination detection, without requiring reference outputs. We apply MEDIC to evaluate LLMs on medical question-answering, safety, summarization, note generation, and other tasks. Our results show performance disparities across model sizes, baseline vs medically finetuned models, and have implications on model selection for applications requiring specific model strengths, such as low hallucination or lower cost of inference. MEDIC's multifaceted evaluation reveals these performance trade-offs, bridging the gap between theoretical capabilities and practical implementation in healthcare settings, ensuring that the most promising models are identified and adapted for diverse healthcare applications.

**Link**: [arxiv](http://arxiv.org/abs/2409.07314v1),  [pdf](http://arxiv.org/pdf/2409.07314v1)

**Tags**: cs.CL cs.AI 



### AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM   Experts
**Authors**: Shaona Ghosh, Prasoon Varshney, Erick Galinkin, Christopher Parisien

**Updated**: 2024-09-11T14:42:29Z

**Summary**: As Large Language Models (LLMs) and generative AI become more widespread, the content safety risks associated with their use also increase. We find a notable deficiency in high-quality content safety datasets and benchmarks that comprehensively cover a wide range of critical safety areas. To address this, we define a broad content safety risk taxonomy, comprising 13 critical risk and 9 sparse risk categories. Additionally, we curate AEGISSAFETYDATASET, a new dataset of approximately 26, 000 human-LLM interaction instances, complete with human annotations adhering to the taxonomy. We plan to release this dataset to the community to further research and to help benchmark LLM models for safety. To demonstrate the effectiveness of the dataset, we instruction-tune multiple LLM-based safety models. We show that our models (named AEGISSAFETYEXPERTS), not only surpass or perform competitively with the state-of-the-art LLM-based safety models and general purpose LLMs, but also exhibit robustness across multiple jail-break attack categories. We also show how using AEGISSAFETYDATASET during the LLM alignment phase does not negatively impact the performance of the aligned models on MT Bench scores. Furthermore, we propose AEGIS, a novel application of a no-regret online adaptation framework with strong theoretical guarantees, to perform content moderation with an ensemble of LLM content safety experts in deployment

**Link**: [arxiv](http://arxiv.org/abs/2404.05993v2),  [pdf](http://arxiv.org/pdf/2404.05993v2)

**Tags**: cs.LG cs.CL cs.CY 



### TLD-READY: Traffic Light Detection -- Relevance Estimation and   Deployment Analysis
**Authors**: Nikolai Polley, Svetlana Pavlitska, Yacin Boualili, Patrick Rohrbeck, Paul Stiller, Ashok Kumar Bangaru, J. Marius Zöllner

**Updated**: 2024-09-11T14:12:44Z

**Summary**: Effective traffic light detection is a critical component of the perception stack in autonomous vehicles. This work introduces a novel deep-learning detection system while addressing the challenges of previous work. Utilizing a comprehensive dataset amalgamation, including the Bosch Small Traffic Lights Dataset, LISA, the DriveU Traffic Light Dataset, and a proprietary dataset from Karlsruhe, we ensure a robust evaluation across varied scenarios. Furthermore, we propose a relevance estimation system that innovatively uses directional arrow markings on the road, eliminating the need for prior map creation. On the DriveU dataset, this approach results in 96% accuracy in relevance estimation. Finally, a real-world evaluation is performed to evaluate the deployment and generalizing abilities of these models. For reproducibility and to facilitate further research, we provide the model weights and code: https://github.com/KASTEL-MobilityLab/traffic-light-detection.

**Link**: [arxiv](http://arxiv.org/abs/2409.07284v1),  [pdf](http://arxiv.org/pdf/2409.07284v1)

**Tags**: cs.CV cs.LG 



### STORE: Streamlining Semantic Tokenization and Generative Recommendation   with A Single LLM
**Authors**: Qijiong Liu, Jieming Zhu, Lu Fan, Zhou Zhao, Xiao-Ming Wu

**Updated**: 2024-09-13T04:16:55Z

**Summary**: Traditional recommendation models often rely on unique item identifiers (IDs) to distinguish between items, which can hinder their ability to effectively leverage item content information and generalize to long-tail or cold-start items. Recently, semantic tokenization has been proposed as a promising solution that aims to tokenize each item's semantic representation into a sequence of discrete tokens. In this way, it preserves the item's semantics within these tokens and ensures that semantically similar items are represented by similar tokens. These semantic tokens have become fundamental in training generative recommendation models. However, existing generative recommendation methods typically involve multiple sub-models for embedding, quantization, and recommendation, leading to an overly complex system. In this paper, we propose to streamline the semantic tokenization and generative recommendation process with a unified framework, dubbed STORE, which leverages a single large language model (LLM) for both tasks. Specifically, we formulate semantic tokenization as a text-to-token task and generative recommendation as a token-to-token task, supplemented by a token-to-text reconstruction task and a text-to-token auxiliary task. All these tasks are framed in a generative manner and trained using a single LLM backbone. Extensive experiments have been conducted to validate the effectiveness of our STORE framework across various recommendation tasks and datasets. We will release the source code and configurations for reproducible research.

**Link**: [arxiv](http://arxiv.org/abs/2409.07276v2),  [pdf](http://arxiv.org/pdf/2409.07276v2)

**Tags**: cs.IR 



### MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D   Features as Text Tokens for Autonomous Driving
**Authors**: Enming Zhang, Xingyuan Dai, Yisheng Lv, Qianghai Miao

**Updated**: 2024-09-11T13:43:01Z

**Summary**: Vision-language models (VLMs) serve as general-purpose end-to-end models in autonomous driving, performing subtasks such as prediction, planning, and perception through question-and-answer interactions. However, most existing methods rely on computationally expensive visual encoders and large language models (LLMs), making them difficult to deploy in real-world scenarios and real-time applications. Meanwhile, most existing VLMs lack the ability to process multiple images, making it difficult to adapt to multi-camera perception in autonomous driving. To address these issues, we propose a novel framework called MiniDrive, which incorporates our proposed Feature Engineering Mixture of Experts (FE-MoE) module and Dynamic Instruction Adapter (DI-Adapter). The FE-MoE effectively maps 2D features into visual token embeddings before being input into the language model. The DI-Adapter enables the visual token embeddings to dynamically change with the instruction text embeddings, resolving the issue of static visual token embeddings for the same image in previous approaches. Compared to previous works, MiniDrive achieves state-of-the-art performance in terms of parameter size, floating point operations, and response efficiency, with the smallest version containing only 83M parameters.

**Link**: [arxiv](http://arxiv.org/abs/2409.07267v1),  [pdf](http://arxiv.org/pdf/2409.07267v1)

**Tags**: cs.CV 



### SECURE: Benchmarking Large Language Models for Cybersecurity Advisory
**Authors**: Dipkamal Bhusal, Md Tanvirul Alam, Le Nguyen, Ashim Mahara, Zachary Lightcap, Rodney Frazier, Romy Fieblinger, Grace Long Torales, Nidhi Rastogi

**Updated**: 2024-09-11T13:11:16Z

**Summary**: Large Language Models (LLMs) have demonstrated potential in cybersecurity applications but have also caused lower confidence due to problems like hallucinations and a lack of truthfulness. Existing benchmarks provide general evaluations but do not sufficiently address the practical and applied aspects of LLM performance in cybersecurity-specific tasks. To address this gap, we introduce the SECURE (Security Extraction, Understanding \& Reasoning Evaluation), a benchmark designed to assess LLMs performance in realistic cybersecurity scenarios. SECURE includes six datasets focussed on the Industrial Control System sector to evaluate knowledge extraction, understanding, and reasoning based on industry-standard sources. Our study evaluates seven state-of-the-art models on these tasks, providing insights into their strengths and weaknesses in cybersecurity contexts, and offer recommendations for improving LLMs reliability as cyber advisory tools.

**Link**: [arxiv](http://arxiv.org/abs/2405.20441v2),  [pdf](http://arxiv.org/pdf/2405.20441v2)

**Tags**: cs.CR cs.AI cs.HC 



### Propaganda to Hate: A Multimodal Analysis of Arabic Memes with   Multi-Agent LLMs
**Authors**: Firoj Alam, Md. Rafiul Biswas, Uzair Shah, Wajdi Zaghouani, Georgios Mikros

**Updated**: 2024-09-11T13:04:34Z

**Summary**: In the past decade, social media platforms have been used for information dissemination and consumption. While a major portion of the content is posted to promote citizen journalism and public awareness, some content is posted to mislead users. Among different content types such as text, images, and videos, memes (text overlaid on images) are particularly prevalent and can serve as powerful vehicles for propaganda, hate, and humor. In the current literature, there have been efforts to individually detect such content in memes. However, the study of their intersection is very limited. In this study, we explore the intersection between propaganda and hate in memes using a multi-agent LLM-based approach. We extend the propagandistic meme dataset with coarse and fine-grained hate labels. Our finding suggests that there is an association between propaganda and hate in memes. We provide detailed experimental results that can serve as a baseline for future studies. We will make the experimental resources publicly available to the community.

**Link**: [arxiv](http://arxiv.org/abs/2409.07246v1),  [pdf](http://arxiv.org/pdf/2409.07246v1)

**Tags**: cs.CL cs.AI 68T50 F.2.2; I.2.7 



### PiTe: Pixel-Temporal Alignment for Large Video-Language Model
**Authors**: Yang Liu, Pengxiang Ding, Siteng Huang, Min Zhang, Han Zhao, Donglin Wang

**Updated**: 2024-09-11T12:53:07Z

**Summary**: Fueled by the Large Language Models (LLMs) wave, Large Visual-Language Models (LVLMs) have emerged as a pivotal advancement, bridging the gap between image and text. However, video making it challenging for LVLMs to perform adequately due to the complexity of the relationship between language and spatial-temporal data structure. Recent Large Video-Language Models (LVidLMs) align feature of static visual data like image into latent space of language feature, by general multi-modal tasks to leverage abilities of LLMs sufficiently. In this paper, we explore fine-grained alignment approach via object trajectory for different modalities across both spatial and temporal dimensions simultaneously. Thus, we propose a novel LVidLM by trajectory-guided Pixel-Temporal Alignment, dubbed PiTe, that exhibits promising applicable model property. To achieve fine-grained video-language alignment, we curate a multi-modal pre-training dataset PiTe-143k, the dataset provision of moving trajectories in pixel level for all individual objects, that appear and mention in the video and caption both, by our automatic annotation pipeline. Meanwhile, PiTe demonstrates astounding capabilities on myriad video-related multi-modal tasks through beat the state-of-the-art methods by a large margin.

**Link**: [arxiv](http://arxiv.org/abs/2409.07239v1),  [pdf](http://arxiv.org/pdf/2409.07239v1)

**Tags**: cs.CV 



### The Philosopher's Stone: Trojaning Plugins of Large Language Models
**Authors**: Tian Dong, Minhui Xue, Guoxing Chen, Rayne Holland, Yan Meng, Shaofeng Li, Zhen Liu, Haojin Zhu

**Updated**: 2024-09-11T12:48:42Z

**Summary**: Open-source Large Language Models (LLMs) have recently gained popularity because of their comparable performance to proprietary LLMs. To efficiently fulfill domain-specialized tasks, open-source LLMs can be refined, without expensive accelerators, using low-rank adapters. However, it is still unknown whether low-rank adapters can be exploited to control LLMs. To address this gap, we demonstrate that an infected adapter can induce, on specific triggers,an LLM to output content defined by an adversary and to even maliciously use tools. To train a Trojan adapter, we propose two novel attacks, POLISHED and FUSION, that improve over prior approaches. POLISHED uses a superior LLM to align na\"ively poisoned data based on our insight that it can better inject poisoning knowledge during training. In contrast, FUSION leverages a novel over-poisoning procedure to transform a benign adapter into a malicious one by magnifying the attention between trigger and target in model weights. In our experiments, we first conduct two case studies to demonstrate that a compromised LLM agent can use malware to control the system (e.g., a LLM-driven robot) or to launch a spear-phishing attack. Then, in terms of targeted misinformation, we show that our attacks provide higher attack effectiveness than the existing baseline and, for the purpose of attracting downloads, preserve or improve the adapter's utility. Finally, we designed and evaluated three potential defenses. However, none proved entirely effective in safeguarding against our attacks, highlighting the need for more robust defenses supporting a secure LLM supply chain.

**Link**: [arxiv](http://arxiv.org/abs/2312.00374v3),  [pdf](http://arxiv.org/pdf/2312.00374v3)

**Tags**: cs.CR 



### Traceable LLM-based validation of statements in knowledge graphs
**Authors**: Daniel Adam, Tomáš Kliegr

**Updated**: 2024-09-11T12:27:41Z

**Summary**: This article presents a method for verifying RDF triples using LLMs, with an emphasis on providing traceable arguments. Because the LLMs cannot currently reliably identify the origin of the information used to construct the response to the user query, our approach is to avoid using internal LLM factual knowledge altogether. Instead, verified RDF statements are compared to chunks of external documents retrieved through a web search or Wikipedia. To assess the possible application of this workflow on biosciences content, we evaluated 1,719 positive statements from the BioRED dataset and the same number of newly generated negative statements. The resulting precision is 88%, and recall is 44%. This indicates that the method requires human oversight. We demonstrate the method on Wikidata, where a SPARQL query is used to automatically retrieve statements needing verification. Overall, the results suggest that LLMs could be used for large-scale verification of statements in KGs, a task previously unfeasible due to human annotation costs.

**Link**: [arxiv](http://arxiv.org/abs/2409.07507v1),  [pdf](http://arxiv.org/pdf/2409.07507v1)

**Tags**: cs.AI cs.LG 



### BiLD: Bi-directional Logits Difference Loss for Large Language Model   Distillation
**Authors**: Minchong Li, Feng Zhou, Xiaohui Song

**Updated**: 2024-09-11T12:19:14Z

**Summary**: In recent years, large language models (LLMs) have shown exceptional capabilities across various natural language processing (NLP) tasks. However, such impressive performance often comes with the trade-off of an increased parameter size, posing significant challenges for widespread deployment. Knowledge distillation (KD) provides a solution by transferring knowledge from a large teacher model to a smaller student model. In this paper, we explore the task-specific distillation of LLMs at the logit level. Our investigation reveals that the logits of fine-tuned LLMs exhibit a more extreme long-tail distribution than those from vision models, with hidden "noise" in the long tail affecting distillation performance. Furthermore, existing logits distillation methods often struggle to effectively utilize the internal ranking information from the logits. To address these, we propose the Bi-directional Logits Difference (BiLD) loss. The BiLD loss filters out the long-tail noise by utilizing only top-$k$ teacher and student logits, and leverages the internal logits ranking information by constructing logits differences. To evaluate BiLD loss, we conduct comprehensive experiments on 13 datasets using two types of LLMs. Our results show that the BiLD loss, with only the top-8 logits, outperforms supervised fine-tuning (SFT), vanilla KL loss, and five other distillation methods from both NLP and CV fields.

**Link**: [arxiv](http://arxiv.org/abs/2406.13555v2),  [pdf](http://arxiv.org/pdf/2406.13555v2)

**Tags**: cs.CL cs.AI 



### Adaptive Layer Splitting for Wireless LLM Inference in Edge Computing: A   Model-Based Reinforcement Learning Approach
**Authors**: Yuxuan Chen, Rongpeng Li, Xiaoxue Yu, Zhifeng Zhao, Honggang Zhang

**Updated**: 2024-09-11T11:59:25Z

**Summary**: Optimizing the deployment of large language models (LLMs) in edge computing environments is critical for enhancing privacy and computational efficiency. Toward efficient wireless LLM inference in edge computing, this study comprehensively analyzes the impact of different splitting points in mainstream open-source LLMs. On this basis, this study introduces a framework taking inspiration from model-based reinforcement learning (MBRL) to determine the optimal splitting point across the edge and user equipment (UE). By incorporating a reward surrogate model, our approach significantly reduces the computational cost of frequent performance evaluations. Extensive simulations demonstrate that this method effectively balances inference performance and computational load under varying network conditions, providing a robust solution for LLM deployment in decentralized settings.

**Link**: [arxiv](http://arxiv.org/abs/2406.02616v5),  [pdf](http://arxiv.org/pdf/2406.02616v5)

**Tags**: cs.LG cs.AI 



### Identify Design Problems Through Questioning: Exploring Role-playing   Interactions with Large Language Models to Foster Design Questioning Skills
**Authors**: Hyunseung Lim, Dasom Choi, Hwajung Hong

**Updated**: 2024-09-11T10:41:05Z

**Summary**: Identifying design problems is a crucial step for creating plausible solutions, but it is challenging for design novices due to their limited knowledge and experience. Questioning is a promising skill that enables students to independently identify design problems without being passive or relying on instructors. This study explores role-playing interactions with Large Language Model (LLM)-powered Conversational Agents (CAs) to foster the questioning skills of novice design students. We proposed an LLM-powered CA prototype and conducted a preliminary study with 16 novice design students engaged in a real-world design class to observe the interactions between students and the LLM-powered CAs. Our findings indicate that while the CAs stimulated questioning and reduced pressure to ask questions, it also inadvertently led to over-reliance on LLM responses. We proposed design considerations and future works for LLM-powered CA to foster questioning skills.

**Link**: [arxiv](http://arxiv.org/abs/2409.07178v1),  [pdf](http://arxiv.org/pdf/2409.07178v1)

**Tags**: cs.HC 



### Linear Time Complexity Conformers with SummaryMixing for Streaming   Speech Recognition
**Authors**: Titouan Parcollet, Rogier van Dalen, Shucong Zhang, Sourav Batthacharya

**Updated**: 2024-09-11T10:24:43Z

**Summary**: Automatic speech recognition (ASR) with an encoder equipped with self-attention, whether streaming or non-streaming, takes quadratic time in the length of the speech utterance. This slows down training and decoding, increase their cost, and limit the deployment of the ASR in constrained devices. SummaryMixing is a promising linear-time complexity alternative to self-attention for non-streaming speech recognition that, for the first time, preserves or outperforms the accuracy of self-attention models. Unfortunately, the original definition of SummaryMixing is not suited to streaming speech recognition. Hence, this work extends SummaryMixing to a Conformer Transducer that works in both a streaming and an offline mode. It shows that this new linear-time complexity speech encoder outperforms self-attention in both scenarios while requiring less compute and memory during training and decoding.

**Link**: [arxiv](http://arxiv.org/abs/2409.07165v1),  [pdf](http://arxiv.org/pdf/2409.07165v1)

**Tags**: cs.SD cs.AI eess.AS 



### A Fine-grained Sentiment Analysis of App Reviews using Large Language   Models: An Evaluation Study
**Authors**: Faiz Ali Shah, Ahmed Sabir, Rajesh Sharma

**Updated**: 2024-09-11T10:21:13Z

**Summary**: Analyzing user reviews for sentiment towards app features can provide valuable insights into users' perceptions of app functionality and their evolving needs. Given the volume of user reviews received daily, an automated mechanism to generate feature-level sentiment summaries of user reviews is needed. Recent advances in Large Language Models (LLMs) such as ChatGPT have shown impressive performance on several new tasks without updating the model's parameters i.e. using zero or a few labeled examples. Despite these advancements, LLMs' capabilities to perform feature-specific sentiment analysis of user reviews remain unexplored. This study compares the performance of state-of-the-art LLMs, including GPT-4, ChatGPT, and LLama-2-chat variants, for extracting app features and associated sentiments under 0-shot, 1-shot, and 5-shot scenarios. Results indicate the best-performing GPT-4 model outperforms rule-based approaches by 23.6% in f1-score with zero-shot feature extraction; 5-shot further improving it by 6%. GPT-4 achieves a 74% f1-score for predicting positive sentiment towards correctly predicted app features, with 5-shot enhancing it by 7%. Our study suggests that LLM models are promising for generating feature-specific sentiment summaries of user reviews.

**Link**: [arxiv](http://arxiv.org/abs/2409.07162v1),  [pdf](http://arxiv.org/pdf/2409.07162v1)

**Tags**: cs.CL cs.SE 



### From the Beginning: Key Transitions in the First 15 Years of DNSSEC
**Authors**: Eric Osterweil, Pouyan Fotouhi Tehrani, Thomas C. Schmidt, Matthias Wählisch

**Updated**: 2024-09-11T10:10:13Z

**Summary**: When the global rollout of the DNS Security Extensions (DNSSEC) began in 2005, a first-of-its-kind trial started: The complexity of a core Internet protocol was magnified in favor of better security for the overall Internet. Thereby, the scale of the loosely-federated delegation in DNS became an unprecedented cryptographic key management challenge. Though fundamental for current and future operational success, our community lacks a clear notion of how to empirically evaluate the process of securely transitioning keys.   In this paper, we propose two building blocks to formally characterize and assess key transitions. First, the anatomy of key transitions, i.e., measurable and well-defined properties of key changes; and second, a novel classification model based on this anatomy for describing key transition practices in abstract terms. This abstraction allows for classifying operational behavior. We apply our proposed transition anatomy and transition classes to describe the global DNSSEC deployment. Specifically, we use measurements from the first 15 years of the DNSSEC rollout to detect and understand which key transitions have been used to what degree and which rates of errors and warnings occurred. In contrast to prior work, we consider all possible transitions and not only 1:1 key rollovers. Our results show measurable gaps between prescribed key management processes and key transitions in the wild. We also find evidence that such noncompliant transitions are needed in operations.

**Link**: [arxiv](http://arxiv.org/abs/2109.08783v2),  [pdf](http://arxiv.org/pdf/2109.08783v2)

**Tags**: cs.CR cs.NI C.2 



### WaDec: Decompiling WebAssembly Using Large Language Model
**Authors**: Xinyu She, Yanjie Zhao, Haoyu Wang

**Updated**: 2024-09-11T10:05:37Z

**Summary**: WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web development, offering a compact binary format that allows high-performance applications to run at near-native speeds in web browsers. Despite its advantages, Wasm's binary nature presents significant challenges for developers and researchers, particularly regarding readability when debugging or analyzing web applications. Therefore, effective decompilation becomes crucial. Unfortunately, traditional decompilers often struggle with producing readable outputs. While some large language model (LLM)-based decompilers have shown good compatibility with general binary files, they still face specific challenges when dealing with Wasm. In this paper, we introduce a novel approach, WaDec, which is the first use of a fine-tuned LLM to interpret and decompile Wasm binary code into a higher-level, more comprehensible source code representation. The LLM was meticulously fine-tuned using a specialized dataset of wat-c code snippets, employing self-supervised learning techniques. This enables WaDec to effectively decompile not only complete wat functions but also finer-grained wat code snippets. Our experiments demonstrate that WaDec markedly outperforms current state-of-the-art tools, offering substantial improvements across several metrics. It achieves a code inflation rate of only 3.34%, a dramatic 97% reduction compared to the state-of-the-art's 116.94%. Unlike baselines' output that cannot be directly compiled or executed, WaDec maintains a recompilability rate of 52.11%, a re-execution rate of 43.55%, and an output consistency of 27.15%. Additionally, it significantly exceeds state-of-the-art performance in AST edit distance similarity by 185%, cyclomatic complexity by 8%, and cosine similarity by 41%, achieving an average code similarity above 50%.

**Link**: [arxiv](http://arxiv.org/abs/2406.11346v3),  [pdf](http://arxiv.org/pdf/2406.11346v3)

**Tags**: cs.SE 



### ZKFault: Fault attack analysis on zero-knowledge based post-quantum   digital signature schemes
**Authors**: Puja Mondal, Supriya Adhikary, Suparna Kundu, Angshuman Karmakar

**Updated**: 2024-09-11T09:54:45Z

**Summary**: Computationally hard problems based on coding theory, such as the syndrome decoding problem, have been used for constructing secure cryptographic schemes for a long time. Schemes based on these problems are also assumed to be secure against quantum computers. However, these schemes are often considered impractical for real-world deployment due to large key sizes and inefficient computation time. In the recent call for standardization of additional post-quantum digital signatures by the National Institute of Standards and Technology, several code-based candidates have been proposed, including LESS, CROSS, and MEDS. These schemes are designed on the relatively new zero-knowledge framework. Although several works analyze the hardness of these schemes, there is hardly any work that examines the security of these schemes in the presence of physical attacks.   In this work, we analyze these signature schemes from the perspective of fault attacks. All these schemes use a similar tree-based construction to compress the signature size. We attack this component of these schemes. Therefore, our attack is applicable to all of these schemes. In this work, we first analyze the LESS signature scheme and devise our attack. Furthermore, we showed how this attack can be extended to the CROSS signature scheme. Our attacks are built on very simple fault assumptions. Our results show that we can recover the entire secret key of LESS and CROSS using as little as a single fault. Finally, we propose various countermeasures to prevent these kinds of attacks and discuss their efficiency and shortcomings.

**Link**: [arxiv](http://arxiv.org/abs/2409.07150v1),  [pdf](http://arxiv.org/pdf/2409.07150v1)

**Tags**: cs.CR E.3.3 



### Leveraging Unstructured Text Data for Federated Instruction Tuning of   Large Language Models
**Authors**: Rui Ye, Rui Ge, Yuchi Fengting, Jingyi Chai, Yanfeng Wang, Siheng Chen

**Updated**: 2024-09-11T09:31:44Z

**Summary**: Federated instruction tuning enables multiple clients to collaboratively fine-tune a shared large language model (LLM) that can follow humans' instructions without directly sharing raw data. However, existing literature impractically requires that all the clients readily hold instruction-tuning data (i.e., structured instruction-response pairs), which necessitates massive human annotations since clients' data is usually unstructured text instead. Addressing this, we propose a novel and flexible framework FedIT-U2S, which can automatically transform unstructured corpus into structured data for federated instruction tuning. FedIT-U2S consists two key steps: (1) few-shot instruction-tuning data generation, where each unstructured data piece together with several examples is combined to prompt an LLM in generating an instruction-response pair. To further enhance the flexibility, a retrieval-based example selection technique is proposed, where the examples are automatically selected based on the relatedness between the client's data piece and example pool, bypassing the need of determining examples in advance. (2) A typical federated instruction tuning process based on the generated data. Overall, FedIT-U2S can be applied to diverse scenarios as long as the client holds valuable text corpus, broadening the application scope of federated instruction tuning. We conduct a series of experiments on three domains (medicine, knowledge, and math), showing that our proposed FedIT-U2S can consistently and significantly brings improvement over the base LLM.

**Link**: [arxiv](http://arxiv.org/abs/2409.07136v1),  [pdf](http://arxiv.org/pdf/2409.07136v1)

**Tags**: cs.CL cs.AI cs.MA 



### LLM-based feature generation from text for interpretable machine   learning
**Authors**: Vojtěch Balek, Lukáš Sýkora, Vilém Sklenák, Tomáš Kliegr

**Updated**: 2024-09-11T09:29:28Z

**Summary**: Existing text representations such as embeddings and bag-of-words are not suitable for rule learning due to their high dimensionality and absent or questionable feature-level interpretability. This article explores whether large language models (LLMs) could address this by extracting a small number of interpretable features from text. We demonstrate this process on two datasets (CORD-19 and M17+) containing several thousand scientific articles from multiple disciplines and a target being a proxy for research impact. An evaluation based on testing for the statistically significant correlation with research impact has shown that LLama 2-generated features are semantically meaningful. We consequently used these generated features in text classification to predict the binary target variable representing the citation rate for the CORD-19 dataset and the ordinal 5-class target representing an expert-awarded grade in the M17+ dataset. Machine-learning models trained on the LLM-generated features provided similar predictive performance to the state-of-the-art embedding model SciBERT for scientific text. The LLM used only 62 features compared to 768 features in SciBERT embeddings, and these features were directly interpretable, corresponding to notions such as article methodological rigor, novelty, or grammatical correctness. As the final step, we extract a small number of well-interpretable action rules. Consistently competitive results obtained with the same LLM feature set across both thematically diverse datasets show that this approach generalizes across domains.

**Link**: [arxiv](http://arxiv.org/abs/2409.07132v1),  [pdf](http://arxiv.org/pdf/2409.07132v1)

**Tags**: cs.LG cs.CL 



### Reranking Laws for Language Generation: A Communication-Theoretic   Perspective
**Authors**: António Farinhas, Haau-Sing Li, André F. T. Martins

**Updated**: 2024-09-11T09:27:50Z

**Summary**: To ensure large language models (LLMs) are used safely, one must reduce their propensity to hallucinate or to generate unacceptable answers. A simple and often used strategy is to first let the LLM generate multiple hypotheses and then employ a reranker to choose the best one. In this paper, we draw a parallel between this strategy and the use of redundancy to decrease the error rate in noisy communication channels. We conceptualize the generator as a sender transmitting multiple descriptions of a message through parallel noisy channels. The receiver decodes the message by ranking the (potentially corrupted) descriptions and selecting the one found to be most reliable. We provide conditions under which this protocol is asymptotically error-free (i.e., yields an acceptable answer almost surely) even in scenarios where the reranker is imperfect (governed by Mallows or Zipf-Mandelbrot models) and the channel distributions are statistically dependent. We use our framework to obtain reranking laws which we validate empirically on two real-world tasks using LLMs: text-to-code generation with DeepSeek-Coder 7B and machine translation of medical data with TowerInstruct 13B.

**Link**: [arxiv](http://arxiv.org/abs/2409.07131v1),  [pdf](http://arxiv.org/pdf/2409.07131v1)

**Tags**: cs.CL cs.LG stat.ML 



### Cross-Refine: Improving Natural Language Explanation Generation by   Learning in Tandem
**Authors**: Qianli Wang, Tatiana Anikina, Nils Feldhus, Simon Ostermann, Sebastian Möller, Vera Schmitt

**Updated**: 2024-09-11T09:21:20Z

**Summary**: Natural language explanations (NLEs) are vital for elucidating the reasoning behind large language model (LLM) decisions. Many techniques have been developed to generate NLEs using LLMs. However, like humans, LLMs might not always produce optimal NLEs on first attempt. Inspired by human learning processes, we introduce Cross-Refine, which employs role modeling by deploying two LLMs as generator and critic, respectively. The generator outputs a first NLE and then refines this initial explanation using feedback and suggestions provided by the critic. Cross-Refine does not require any supervised training data or additional training. We validate Cross-Refine across three NLP tasks using three state-of-the-art open-source LLMs through automatic and human evaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which only utilizes self-feedback to refine the explanations. Our findings from automatic evaluation and a user study indicate that Cross-Refine outperforms Self-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful LLMs, whereas Self-Refine only yields strong results with ChatGPT. Additionally, we conduct an ablation study to assess the importance of feedback and suggestions. Both of them play an important role in refining explanations. We further evaluate Cross-Refine on a bilingual dataset in English and German.

**Link**: [arxiv](http://arxiv.org/abs/2409.07123v1),  [pdf](http://arxiv.org/pdf/2409.07123v1)

**Tags**: cs.CL cs.LG 



