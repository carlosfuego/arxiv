# Arxiv Results
## Keyword: kv cache 
 ### ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for   Scalable Recommendation System
**Authors**: Youngsuk Kim, Junghwan Lim, Hyuk-Jae Lee, Chae Eun Rhee

**Updated**: 2024-11-21T05:55:43Z

**Summary**: The model size growth of personalized recommendation systems poses new challenges for inference. Weight-sharing algorithms have been proposed for size reduction, but they increase memory access. Recent advancements in processing-in-memory (PIM) enhanced the model throughput by exploiting memory parallelism, but such algorithms introduce massive CPU-PIM communication into prior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing recommendation system acceleration. ProactivePIM integrates a cache within the PIM with a prefetching scheme to leverage a unique locality of the algorithm and eliminate communication overhead through a subtable mapping strategy. ProactivePIM achieves a 4.8x speedup compared to prior works.

**Link**: [arxiv](http://arxiv.org/abs/2402.04032v5),  [pdf](http://arxiv.org/pdf/2402.04032v5)

**Tags**: cs.AR cs.AI 



### Static Reuse Profile Estimation for Array Applications
**Authors**: Abdur Razzak, Atanu Barai, Nandakishore Santhi, Abdel-Hameed A. Badawy

**Updated**: 2024-11-21T05:26:57Z

**Summary**: Reuse distance analysis is a widely recognized method for application characterization that illustrates cache locality. Although there are various techniques to calculate the reuse profile from dynamic memory traces, it is both time and space-consuming due to the requirement to collect dynamic memory traces at runtime. In contrast, static analysis reuse profile estimation is a promisingly faster approach since it is calculated at compile time without running the program or collecting memory traces. This work presents a static analysis technique to estimate the reuse profile of loop-based programs. For an input program, we generate a basic block-level control flow graph and the execution count by analyzing the LLVM IR of the program. We present the memory accesses of the application kernel in a compact bracketed format and use a recursive algorithm to predict the reuse distance histogram. We deploy a separate predictor that unrolls the loop(s) for smaller bounds and generates a temporary reuse distance profile for those small cases. Using these smaller profiles, the reuse profile is extrapolated for the actual loop bound(s). We use this reuse profile to predict the cache hit rate. Results show that our model can predict cache hit rates with an average accuracy of 95% relative to the dynamic reuse profile methods.

**Link**: [arxiv](http://arxiv.org/abs/2411.13854v1),  [pdf](http://arxiv.org/pdf/2411.13854v1)

**Tags**: cs.PF 



### Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification
**Authors**: Jintao Rong, Hao Chen, Linlin Ou, Tianxiao Chen, Xinyi Yu, Yifan Liu

**Updated**: 2024-11-21T04:12:53Z

**Summary**: The Contrastive Language-Image Pretraining (CLIP) model has been widely used in various downstream vision tasks. The few-shot learning paradigm has been widely adopted to augment its capacity for these tasks. However, current paradigms may struggle with fine-grained classification, such as satellite image recognition, due to widening domain gaps. To address this limitation, we propose retrieval-enhanced visual prompt learning (RePrompt), which introduces retrieval mechanisms to cache and reuse the knowledge of downstream tasks. RePrompt constructs a retrieval database from either training examples or external data if available, and uses a retrieval mechanism to enhance multiple stages of a simple prompt learning baseline, thus narrowing the domain gap. During inference, our enhanced model can reference similar samples brought by retrieval to make more accurate predictions. A detailed analysis reveals that retrieval helps to improve the distribution of late features, thus, improving generalization for downstream tasks. Reprompt attains state-of-the-art performance on a wide range of vision datasets, including 11 image datasets, 3 video datasets, 1 multi-view dataset, and 4 domain generalization benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2306.02243v3),  [pdf](http://arxiv.org/pdf/2306.02243v3)

**Tags**: cs.CV 



### InstCache: A Predictive Cache for LLM Serving
**Authors**: Longwei Zou, Tingfeng Liu, Kai Chen, Jiangang Kong, Yangdong Deng

**Updated**: 2024-11-21T03:52:41Z

**Summary**: Large language models are revolutionizing every aspect of human life. However, the unprecedented power comes at the cost of significant computing intensity, suggesting long latency and large energy footprint. Key-Value Cache and Semantic Cache have been proposed as a solution to the above problem, but both suffer from limited scalability due to significant memory cost for each token or instruction embeddings. Motivated by the observations that most instructions are short, repetitive and predictable by LLMs, we propose to predict user-instructions by an instruction-aligned LLM and store them in a predictive cache, so-called InstCache. We introduce an instruction pre-population algorithm based on the negative log likelihood of instructions, determining the cache size with regard to the hit rate. The proposed InstCache is efficiently implemented as a hash table with minimal lookup latency for deployment. Experimental results show that InstCache can achieve up to 51.34% hit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost of only 4.5GB.

**Link**: [arxiv](http://arxiv.org/abs/2411.13820v1),  [pdf](http://arxiv.org/pdf/2411.13820v1)

**Tags**: cs.CL cs.DC 



### WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series   Forecasting
**Authors**: Aobo Liang, Yan Sun, Nadra Guizani

**Updated**: 2024-11-21T03:34:44Z

**Summary**: In recent years, Transformer-based models (Transformers) have achieved significant success in multivariate time series forecasting (MTSF). However, previous works focus on extracting features either from the time domain or the frequency domain, which inadequately captures the trends and periodic characteristics. To address this issue, we propose a wavelet learning framework to model complex temporal dependencies of the time series data. The wavelet domain integrates both time and frequency information, allowing for the analysis of local characteristics of signals at different scales. Additionally, the Softmax self-attention mechanism used by Transformers has quadratic complexity, which leads to excessive computational costs when capturing long-term dependencies. Therefore, we propose a novel attention mechanism: Rotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary position embeddings to inject relative positional information to sequence tokens and introduces a small number of routing tokens $r$ to aggregate information from the $KV$ matrices and redistribute it to the $Q$ matrix, offering linear complexity. We further propose WaveRoRA, which leverages RoRA to capture inter-series dependencies in the wavelet domain. We conduct extensive experiments on eight real-world datasets. The results indicate that WaveRoRA outperforms existing state-of-the-art models while maintaining lower computational costs. Our code is available at https://github.com/Leopold2333/WaveRoRA.

**Link**: [arxiv](http://arxiv.org/abs/2410.22649v2),  [pdf](http://arxiv.org/pdf/2410.22649v2)

**Tags**: cs.LG 



### Adaptable Embeddings Network (AEN)
**Authors**: Stan Loosmore, Alexander Titus

**Updated**: 2024-11-21T02:15:52Z

**Summary**: Modern day Language Models see extensive use in text classification, yet this comes at significant computational cost. Compute-effective classification models are needed for low-resource environments, most notably on edge devices. We introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder architecture using Kernel Density Estimation (KDE). This architecture allows for runtime adaptation of classification criteria without retraining and is non-autoregressive. Through thorough synthetic data experimentation, we demonstrate our model outputs comparable and in certain cases superior results to that of autoregressive models an order of magnitude larger than AEN's size. The architecture's ability to preprocess and cache condition embeddings makes it ideal for edge computing applications and real-time monitoring systems.

**Link**: [arxiv](http://arxiv.org/abs/2411.13786v1),  [pdf](http://arxiv.org/pdf/2411.13786v1)

**Tags**: cs.LG cs.CL 



### Hymba: A Hybrid-head Architecture for Small Language Models
**Authors**: Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil Mahabaleshwarkar, Shih-Yang Liu, Matthijs Van Keirsbilck, Min-Hung Chen, Yoshi Suhara, Yingyan Lin, Jan Kautz, Pavlo Molchanov

**Updated**: 2024-11-20T19:51:25Z

**Summary**: We propose Hymba, a family of small language models featuring a hybrid-head parallel architecture that integrates transformer attention mechanisms with state space models (SSMs) for enhanced efficiency. Attention heads provide high-resolution recall, while SSM heads enable efficient context summarization. Additionally, we introduce learnable meta tokens that are prepended to prompts, storing critical information and alleviating the "forced-to-attend" burden associated with attention mechanisms. This model is further optimized by incorporating cross-layer key-value (KV) sharing and partial sliding window attention, resulting in a compact cache size. During development, we conducted a controlled study comparing various architectures under identical settings and observed significant advantages of our proposed architecture. Notably, Hymba achieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model surpasses all sub-2B public models in performance and even outperforms Llama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size reduction, and 3.49x throughput.

**Link**: [arxiv](http://arxiv.org/abs/2411.13676v1),  [pdf](http://arxiv.org/pdf/2411.13676v1)

**Tags**: cs.CL cs.AI cs.LG 



### A Distributed-memory Tridiagonal Solver Based on a Specialised Data   Structure Optimised for CPU and GPU Architectures
**Authors**: Semih Akkurt, Sébastien Lemaire, Paul Bartholomew, Sylvain Laizet

**Updated**: 2024-11-20T18:31:39Z

**Summary**: Various numerical methods used for solving partial differential equations (PDE) result in tridiagonal systems. Solving tridiagonal systems on distributed-memory environments is not straightforward, and often requires significant amount of communication. In this article, we present a novel distributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a specialised data structure. DistD2-TDS algorithm takes advantage of the diagonal dominance in tridiagonal systems to reduce the communications in distributed-memory environments. The underlying data structure plays a crucial role for the performance of the algorithm. First, the data structure improves data localities and makes it possible to minimise data movements via cache blocking and kernel fusion strategies. Second, data continuity enables a contiguous data access pattern and results in efficient utilisation of the available memory bandwidth. Finally, the data layout supports vectorisation on CPUs and thread level parallelisation on GPUs for improved performance. In order to demonstrate the robustness of the algorithm, we implemented and benchmarked the algorithm on CPUs and GPUs. We investigated the single rank performance and compared against existing algorithms. Furthermore, we analysed the strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to 8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the algorithm by using compact finite difference schemes to solve a 3D non-linear PDE. The results demonstrate that DistD2 algorithm can sustain around 66% of the theoretical peak bandwidth at scale on CPU and GPU based supercomputers.

**Link**: [arxiv](http://arxiv.org/abs/2411.13532v1),  [pdf](http://arxiv.org/pdf/2411.13532v1)

**Tags**: cs.DC physics.comp-ph 



### Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2   experiment
**Authors**: Marco Barbisan, Marco Boldrin, Luca Cinnirella, Bruno Laterza, Alberto Maistrello, Lionello Marrelli, Federico Molon, Simone Peruzzo, Cesare Taliercio, Marco Valisa, Enrico Zampiva

**Updated**: 2024-11-20T14:52:36Z

**Summary**: Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge Exchange Recombination Spectroscopy (CHERS) and Motional Stark effect diagnostics (MSE), are a well-known tool to access important information about magnetically confined plasmas, such as radial profiles of ion temperature, ion flow, impurity content and intensity and direction of the magnetic field. For this purpose, a DNBI was installed and operated in the RFX-mod experiment, which was designed to confine plasma mainly through the Reversed Field Pinch configuration. The DNBI, designed and built by the Budker Institute of Plasma Physics, was based on a source of positive hydrogen ions, accelerated to 50 keV and for an equivalent neutral beam current of about 5 A at the source. The beam could be modulated and the maximum overall duration was 50 ms. With the upgrade of RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to solve several plant faults and improve the overall reliability of the system. The 50 kV power supply is being improved, as well as the power supplies in the high voltage deck and its insulation transformer. The control system, originally based on CAMAC technology, was redesigned to be fully replaced. This contribution reviews the technical criticalities emerged in the DNBI check-up and the new solutions adopted to make the DNBI operative and more reliable.

**Link**: [arxiv](http://arxiv.org/abs/2411.13373v1),  [pdf](http://arxiv.org/pdf/2411.13373v1)

**Tags**: physics.plasm-ph 



### Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache   Consumption
**Authors**: Luohe Shi, Hongyi Zhang, Yao Yao, Zuchao Li, Hai Zhao

**Updated**: 2024-11-20T02:04:10Z

**Summary**: Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022, have revolutionized various industries with their advanced language comprehension. However, their efficiency is challenged by the Transformer architecture's struggle with handling long texts. KV Cache has emerged as a pivotal solution to this issue, converting the time complexity of token generation from quadratic to linear, albeit with increased GPU memory overhead proportional to conversation length. With the development of the LLM community and academia, various KV Cache compression methods have been proposed. In this review, we dissect the various properties of KV Cache and elaborate on various methods currently used to optimize the KV Cache space usage of LLMs. These methods span the pre-training phase, deployment phase, and inference phase, and we summarize the commonalities and differences among these methods. Additionally, we list some metrics for evaluating the long-text capabilities of large language models, from both efficiency and capability perspectives. Our review thus sheds light on the evolving landscape of LLM optimization, offering insights into future advancements in this dynamic field. Links to the papers mentioned in this review can be found in our Github Repo https://github.com/zcli-charlie/Awesome-KV-Cache.

**Link**: [arxiv](http://arxiv.org/abs/2407.18003v4),  [pdf](http://arxiv.org/pdf/2407.18003v4)

**Tags**: cs.CL 



### GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and   Retrieval
**Authors**: Dong Liu, Roger Waleffe, Meng Jiang, Shivaram Venkataraman

**Updated**: 2024-11-19T18:24:03Z

**Summary**: In our recent research, we have developed a framework called GraphSnapShot, which has been proven an useful tool for graph learning acceleration. GraphSnapShot is a framework for fast cache, storage, retrieval and computation for graph learning. It can quickly store and update the local topology of graph structure and allows us to track patterns in the structure of graph networks, just like take snapshots of the graphs. In experiments, GraphSnapShot shows efficiency, it can achieve up to 30% training acceleration and 73% memory reduction for lossless graph ML training compared to current baselines such as dgl.This technique is particular useful for large dynamic graph learning tasks such as social media analysis and recommendation systems to process complex relationships between entities.   The code for GraphSnapShot is publicly available at https://github.com/NoakLiu/GraphSnapShot.

**Link**: [arxiv](http://arxiv.org/abs/2406.17918v3),  [pdf](http://arxiv.org/pdf/2406.17918v3)

**Tags**: cs.LG cs.DC cs.SI 



### An Eulerian approach to regularized JKO scheme with low-rank tensor   decompositions for Bayesian inversion
**Authors**: Vitalii Aksenov, Martin Eigel

**Updated**: 2024-11-19T11:40:56Z

**Summary**: The possibility of using the Eulerian discretization for the problem of modelling high-dimensional distributions and sampling, is studied. The problem is posed as a minimization problem over the space of probability measures with respect to the Wasserstein distance and solved with entropy-regularized JKO scheme. Each proximal step can be formulated as a fixed-point equation and solved with accelerated methods, such as Anderson's. The usage of low-rank Tensor Train format allows to overcome the \emph{curse of dimensionality}, i.e. the exponential growth of degrees of freedom with dimension, inherent to Eulerian approaches. The resulting method requires only pointwise computations of the unnormalized posterior and is, in particular, gradient-free. Fixed Eulerian grid allows to employ a caching strategy, significally reducing the expensive evaluations of the posterior. When the Eulerian model of the target distribution is fitted, the passage back to the Lagrangian perspective can also be made, allowing to approximately sample from it. We test our method both for synthetic target distributions and particular Bayesian inverse problems and report comparable or better performance than the baseline Metropolis-Hastings MCMC with same amount of resources. Finally, the fitted model can be modified to facilitate the solution of certain associated problems, which we demonstrate by fitting an importance distribution for a particular quantity of interest.

**Link**: [arxiv](http://arxiv.org/abs/2411.12430v1),  [pdf](http://arxiv.org/pdf/2411.12430v1)

**Tags**: math.NA cs.NA math.OC 46E27, 49Q22, 62F15, 68W25 



### Adaptive Cache Management for Complex Storage Systems Using   CNN-LSTM-Based Spatiotemporal Prediction
**Authors**: Xiaoye Wang, Xuan Li, Linji Wang, Tingyi Ruan, Pochun Li

**Updated**: 2024-11-19T01:55:26Z

**Summary**: This paper proposes an intelligent cache management strategy based on CNN-LSTM to improve the performance and cache hit rate of storage systems. Through comparative experiments with traditional algorithms (such as LRU and LFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the results show that the CNN-LSTM model has significant advantages in cache demand prediction. The MSE and MAE values of this model are significantly reduced, proving its effectiveness under complex data access patterns. This study not only verifies the potential of deep learning technology in storage system optimization, but also provides direction and reference for further optimizing and improving cache management strategies. This intelligent cache management strategy performs well in complex storage environments. By combining the spatial feature extraction capabilities of convolutional neural networks and the time series modeling capabilities of long short-term memory networks, the CNN-LSTM model can more accurately predict cache needs, thereby Dynamically optimize cache allocation to improve system response speed and resource utilization. This research provides theoretical support and practical reference for cache optimization under large-scale data access modes, and is of great significance to improving the performance of future storage systems.

**Link**: [arxiv](http://arxiv.org/abs/2411.12161v1),  [pdf](http://arxiv.org/pdf/2411.12161v1)

**Tags**: cs.DC 



### Bi-Mamba: Towards Accurate 1-Bit State Space Models
**Authors**: Shengkun Tang, Liqun Ma, Haonan Li, Mingjie Sun, Zhiqiang Shen

**Updated**: 2024-11-18T18:59:15Z

**Summary**: The typical selective state-space model (SSM) of Mamba addresses several limitations of Transformers, such as quadratic computational complexity with sequence length and significant inference-time memory requirements due to the key-value cache. However, the growing size of Mamba models continues to pose training and deployment challenges and raises environmental concerns due to considerable energy consumption. In this work, we introduce Bi-Mamba, a scalable and powerful 1-bit Mamba architecture designed for more efficient large language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba models are trained from scratch on data volume as regular LLM pertaining using an autoregressive distillation loss. Extensive experimental results on language modeling demonstrate that Bi-Mamba achieves performance comparable to its full-precision counterparts (e.g., FP16 or BF16) and much better accuracy than post-training-binarization (PTB) Mamba baselines, while significantly reducing memory footprint and energy consumption compared to the original Mamba model. Our study pioneers a new linear computational complexity LLM framework under low-bit representation and facilitates the future design of specialized hardware tailored for efficient 1-bit Mamba-based LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2411.11843v1),  [pdf](http://arxiv.org/pdf/2411.11843v1)

**Tags**: cs.CL cs.AI 



### QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou
**Authors**: Xinchen Luo, Jiangxia Cao, Tianyu Sun, Jinkai Yu, Rui Huang, Wei Yuan, Hezheng Lin, Yichen Zheng, Shiyao Wang, Qigen Hu, Changqing Qiu, Jiaqi Zhang, Xu Zhang, Zhiheng Yan, Jingming Zhang, Simin Zhang, Mingxing Wen, Zhaojie Liu, Kun Gai, Guorui Zhou

**Updated**: 2024-11-18T17:08:35Z

**Summary**: In recent years, with the significant evolution of multi-modal large models, many recommender researchers realized the potential of multi-modal information for user interest modeling. In industry, a wide-used modeling architecture is a cascading paradigm: (1) first pre-training a multi-modal model to provide omnipotent representations for downstream services; (2) The downstream recommendation model takes the multi-modal representation as additional input to fit real user-item behaviours. Although such paradigm achieves remarkable improvements, however, there still exist two problems that limit model performance: (1) Representation Unmatching: The pre-trained multi-modal model is always supervised by the classic NLP/CV tasks, while the recommendation models are supervised by real user-item interaction. As a result, the two fundamentally different tasks' goals were relatively separate, and there was a lack of consistent objective on their representations; (2) Representation Unlearning: The generated multi-modal representations are always stored in cache store and serve as extra fixed input of recommendation model, thus could not be updated by recommendation model gradient, further unfriendly for downstream training. Inspired by the two difficulties challenges in downstream tasks usage, we introduce a quantitative multi-modal framework to customize the specialized and trainable multi-modal information for different downstream models.

**Link**: [arxiv](http://arxiv.org/abs/2411.11739v1),  [pdf](http://arxiv.org/pdf/2411.11739v1)

**Tags**: cs.IR cs.AI N/A 



### Deegen: A JIT-Capable VM Generator for Dynamic Languages
**Authors**: Haoran Xu, Fredrik Kjolstad

**Updated**: 2024-11-18T11:12:57Z

**Summary**: Building a high-performance JIT-capable VM for a dynamic language has traditionally required a tremendous amount of time, money, and expertise. We present Deegen, a meta-compiler that allows users to generate a high-performance JIT-capable VM for their own language at an engineering cost similar to writing a simple interpreter. Deegen takes in the execution semantics of the bytecodes implemented as C++ functions, and automatically generates a two-tier VM execution engine with a state-of-the-art interpreter, a state-of-the-art baseline JIT, and the tier-switching logic that connects them into a self-adaptive system.   We are the first to demonstrate the automatic generation of a JIT compiler, and the automatic generation of an interpreter that outperforms the state of the art. Our performance comes from a long list of optimizations supported by Deegen, including bytecode specialization and quickening, register pinning, tag register optimization, call inline caching, generic inline caching, JIT polymorphic IC, JIT IC inline slab, type-check removal and strength reduction, type-based slow-path extraction and outlining, JIT hot-cold code splitting, and JIT OSR-entry. These optimizations are either employed automatically, or guided by the language implementer through intuitive APIs. As a result, the disassembly of the Deegen-generated interpreter, baseline JIT, and the generated JIT code rivals the assembly code hand-written by experts in state-of-the-art VMs.   We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using Deegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than the official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter. LJR's baseline JIT has negligible startup delay, and its execution performance is on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44 benchmarks) than LuaJIT's optimizing JIT.

**Link**: [arxiv](http://arxiv.org/abs/2411.11469v1),  [pdf](http://arxiv.org/pdf/2411.11469v1)

**Tags**: cs.PL 



### Accelerating spherical K-means clustering for large-scale sparse   document data
**Authors**: Kazuo Aoyama, Kazumi Saito

**Updated**: 2024-11-18T05:50:58Z

**Summary**: This paper presents an accelerated spherical K-means clustering algorithm for large-scale and high-dimensional sparse document data sets. We design an algorithm working in an architecture-friendly manner (AFM), which is a procedure of suppressing performance-degradation factors such as the numbers of instructions, branch mispredictions, and cache misses in CPUs of a modern computer system. For the AFM operation, we leverage unique universal characteristics (UCs) of a data-object and a cluster's mean set, which are skewed distributions on data relationships such as Zipf's law and a feature-value concentration phenomenon. The UCs indicate that the most part of the number of multiplications for similarity calculations is executed regarding terms with high document frequencies (df) and the most part of a similarity between an object- and a mean-feature vector is obtained by the multiplications regarding a few high mean-feature values. Our proposed algorithm applies an inverted-index data structure to a mean set, extracts the specific region with high-df terms and high mean-feature values in the mean-inverted index by newly introduced two structural parameters, and exploits the index divided into three parts for efficient pruning. The algorithm determines the two structural parameters by minimizing the approximate number of multiplications related to that of instructions, reduces the branch mispredictions by sharing the index structure including the two parameters with all the objects, and suppressing the cache misses by keeping in the caches the frequently used data in the foregoing specific region, resulting in working in the AFM. We experimentally demonstrate that our algorithm efficiently achieves superior speed performance in large-scale documents compared with algorithms using the state-of-the-art techniques.

**Link**: [arxiv](http://arxiv.org/abs/2411.11300v1),  [pdf](http://arxiv.org/pdf/2411.11300v1)

**Tags**: stat.ML cs.LG 



### Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic   Study
**Authors**: Xibo Sun, Jiarui Fang, Aoyu Li, Jinzhe Pan

**Updated**: 2024-11-18T02:49:23Z

**Summary**: The increased model capacity of Diffusion Transformers (DiTs) and the demand for generating higher resolutions of images and videos have led to a significant rise in inference latency, impacting real-time performance adversely. While prior research has highlighted the presence of high similarity in activation values between adjacent diffusion steps (referred to as redundancy) and proposed various caching mechanisms to mitigate computational overhead, the exploration of redundancy in existing literature remains limited, with findings often not generalizable across different DiT models. This study aims to address this gap by conducting a comprehensive investigation into redundancy across a broad spectrum of mainstream DiT models. Our experimental analysis reveals substantial variations in the distribution of redundancy across diffusion steps among different DiT models. Interestingly, within a single model, the redundancy distribution remains stable regardless of variations in input prompts, step counts, or scheduling strategies. Given the lack of a consistent pattern across diverse models, caching strategies designed for a specific group of models may not easily transfer to others. To overcome this challenge, we introduce a tool for analyzing the redundancy of individual models, enabling subsequent research to develop tailored caching strategies for specific model architectures. The project is publicly available at https://github.com/xdit-project/DiTCacheAnalysis.

**Link**: [arxiv](http://arxiv.org/abs/2411.13588v1),  [pdf](http://arxiv.org/pdf/2411.13588v1)

**Tags**: cs.CV cs.AI 



### LSMGraph: A High-Performance Dynamic Graph Storage System with   Multi-Level CSR
**Authors**: Song Yu, Shufeng Gong, Qian Tao, Sijie Shen, Yanfeng Zhang, Wenyuan Yu, Pengxi Liu, Zhixin Zhang, Hongfu Li, Xiaojian Luo, Ge Yu, Jingren Zhou

**Updated**: 2024-11-18T02:10:28Z

**Summary**: The growing volume of graph data may exhaust the main memory. It is crucial to design a disk-based graph storage system to ingest updates and analyze graphs efficiently. However, existing dynamic graph storage systems suffer from read or write amplification and face the challenge of optimizing both read and write performance simultaneously. To address this challenge, we propose LSMGraph, a novel dynamic graph storage system that combines the write-friendly LSM-tree and the read-friendly CSR. It leverages the multi-level structure of LSM-trees to optimize write performance while utilizing the compact CSR structures embedded in the LSM-trees to boost read performance. LSMGraph uses a new memory structure, MemGraph, to efficiently cache graph updates and uses a multi-level index to speed up reads within the multi-level structure. Furthermore, LSMGraph incorporates a vertex-grained version control mechanism to mitigate the impact of LSM-tree compaction on read performance and ensure the correctness of concurrent read and write operations. Our evaluation shows that LSMGraph significantly outperforms state-of-the-art (graph) storage systems on both graph update and graph analytical workloads.

**Link**: [arxiv](http://arxiv.org/abs/2411.06392v2),  [pdf](http://arxiv.org/pdf/2411.06392v2)

**Tags**: cs.DB 



### KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage   Engines
**Authors**: Edward Bortnikov, Michael Azran, Asa Bornstein, Shmuel Dashevsky, Dennis Huang, Omer Kepten, Michael Pan, Gali Sheffi, Moshe Twitto, Tamar Weiss Orzech, Idit Keidar, Guy Gueta, Roey Maor, Niv Dayan

**Updated**: 2024-11-17T14:47:15Z

**Summary**: We present~\emph{KV-Tandem}, a modular architecture for building LSM-based storage engines on top of simple, non-ordered persistent key-value stores (KVSs). KV-Tandem enables advanced functionalities such as range queries and snapshot reads, while maintaining the native KVS performance for random reads and writes. Its modular design offers better performance trade-offs compared to previous KV-separation solutions, which struggle to decompose the monolithic LSM structure. Central to KV-Tandem is~\emph{LSM bypass} -- a novel algorithm that offers a fast path to basic operations while ensuring the correctness of advanced APIs.   We implement KV-Tandem in \emph{XDP-Rocks}, a RocksDB-compatible storage engine that leverages the XDP KVS and incorporates practical design optimizations for real-world deployment. Through extensive microbenchmark and system-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x performance improvements over RocksDB across various workloads. XDP-Rocks is already deployed in production, delivering significant operator cost savings consistent with these performance gains.

**Link**: [arxiv](http://arxiv.org/abs/2411.11091v1),  [pdf](http://arxiv.org/pdf/2411.11091v1)

**Tags**: cs.DB 



### Breaking the Low-Rank Dilemma of Linear Attention
**Authors**: Qihang Fan, Huaibo Huang, Ran He

**Updated**: 2024-11-17T12:56:16Z

**Summary**: The Softmax attention mechanism in Transformer models is notoriously computationally expensive, particularly due to its quadratic complexity, posing significant challenges in vision applications. In contrast, linear attention provides a far more efficient solution by reducing the complexity to linear levels. However, compared to Softmax attention, linear attention often experiences significant performance degradation. Our experiments indicate that this performance drop is due to the low-rank nature of linear attention's feature map, which hinders its ability to adequately model complex spatial information. In this paper, to break the low-rank dilemma of linear attention, we conduct rank analysis from two perspectives: the KV buffer and the output features. Consequently, we introduce Rank-Augmented Linear Attention (RALA), which rivals the performance of Softmax attention while maintaining linear complexity and high efficiency. Based on RALA, we construct the Rank-Augmented Vision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT achieves excellent performance across various vision tasks. Specifically, without using any additional labels, data, or supervision during training, RAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters and 4.6G FLOPs. This result significantly surpasses previous linear attention mechanisms, fully illustrating the potential of RALA. Code will be available at https://github.com/qhfan/RALA.

**Link**: [arxiv](http://arxiv.org/abs/2411.07635v3),  [pdf](http://arxiv.org/pdf/2411.07635v3)

**Tags**: cs.CV 



### I Know What You Sync: Covert and Side Channel Attacks on File Systems   via syncfs
**Authors**: Cheng Gu, Yicheng Zhang, Nael Abu-Ghazaleh

**Updated**: 2024-11-16T20:40:08Z

**Summary**: Operating Systems enforce logical isolation using abstractions such as processes, containers, and isolation technologies to protect a system from malicious or buggy code. In this paper, we show new types of side channels through the file system that break this logical isolation. The file system plays a critical role in the operating system, managing all I/O activities between the application layer and the physical storage device. We observe that the file system implementation is shared, leading to timing leakage when using common I/O system calls. Specifically, we found that modern operating systems take advantage of any flush operation (which saves cached blocks in memory to the SSD or disk) to flush all of the I/O buffers, even those used by other isolation domains. Thus, by measuring the delay of syncfs, the attacker can infer the I/O behavior of victim programs. We then demonstrate a syncfs covert channel attack on multiple file systems, including both Linux native file systems and the Windows file system, achieving a maximum bandwidth of 5 Kbps with an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on Windows. In addition, we construct three side-channel attacks targeting both Linux and Android devices. On Linux devices, we implement a website fingerprinting attack and a video fingerprinting attack by tracking the write patterns of temporary buffering files. On Android devices, we design an application fingerprinting attack that leaks application write patterns during boot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally, we demonstrate that these attacks can be exploited across containers implementing a container detection technique and a cross-container covert channel attack.

**Link**: [arxiv](http://arxiv.org/abs/2411.10883v1),  [pdf](http://arxiv.org/pdf/2411.10883v1)

**Tags**: cs.CR 



### Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks
**Authors**: Bo-Ru Lu, Nikita Haduong, Chien-Yu Lin, Hao Cheng, Noah A. Smith, Mari Ostendorf

**Updated**: 2024-11-16T20:39:46Z

**Summary**: Transformer-based NLP models are powerful but have high computational costs that limit deployment. Finetuned encoder-decoder models are popular in specialized domains and can outperform larger more generalized decoder-only models, such as GPT-4. We introduce a new configuration for encoder-decoder models that improves efficiency on structured output and decomposable tasks where multiple outputs are required for a single shared input. Our method, prompt-in-decoder (PiD), encodes the input once and decodes the output in parallel, boosting both training and inference efficiency by avoiding duplicate input encoding and increasing the operational intensity (ratio of numbers of arithmetic operation to memory access) of decoding process by sharing the input key-value cache. We achieve computation reduction that roughly scales with the number of subtasks, gaining up to 4.6x speed-up over state-of-the-art models for dialogue state tracking, summarization, and question-answering tasks, with comparable or better performance.

**Link**: [arxiv](http://arxiv.org/abs/2403.13112v3),  [pdf](http://arxiv.org/pdf/2403.13112v3)

**Tags**: cs.CL 



### Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large   Language Model
**Authors**: Ting Liu, Liangtao Shi, Richang Hong, Yue Hu, Quanjun Yin, Linfeng Zhang

**Updated**: 2024-11-16T13:45:33Z

**Summary**: The vision tokens in multimodal large language models usually exhibit significant spatial and temporal redundancy and take up most of the input tokens, which harms their inference efficiency. To solve this problem, some recent works were introduced to drop the unimportant tokens during inference where the importance of each token is decided only by the information in either the vision encoding stage or the prefilling stage. In this paper, we propose Multi-stage Token Dropping (MustDrop) to measure the importance of each token from the whole lifecycle, including the vision encoding stage, prefilling stage, and decoding stage. Concretely, in the visual encoding stage, MustDrop merges spatially adjacent tokens with high similarity, and establishes a key token set to retain the most vision-critical tokens, preventing them from being discarded in later stages. In the prefilling stage, MustDrop further compresses vision tokens by the guidance of text semantics, with a dual-attention filtering strategy. In the decoding stage, an output-aware cache policy is proposed to further reduce the size of the KV cache. By leveraging tailored strategies in the multi-stage process, MustDrop can more precisely recognize the important and redundant tokens, thus achieving an optimal balance between performance and efficiency. For instance, MustDrop reduces about 88.5\% FLOPs on LLaVA with a compression ratio of 92.2\% while maintaining comparable accuracy. Our codes are available at \url{https://github.com/liuting20/MustDrop}.

**Link**: [arxiv](http://arxiv.org/abs/2411.10803v1),  [pdf](http://arxiv.org/pdf/2411.10803v1)

**Tags**: cs.CV 



### Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching
**Authors**: Xinyin Ma, Gongfan Fang, Michael Bi Mi, Xinchao Wang

**Updated**: 2024-11-16T07:43:28Z

**Summary**: Diffusion Transformers have recently demonstrated unprecedented generative capabilities for various tasks. The encouraging results, however, come with the cost of slow inference, since each denoising step requires inference on a transformer model with a large scale of parameters. In this study, we make an interesting and somehow surprising observation: the computation of a large proportion of layers in the diffusion transformer, through introducing a caching mechanism, can be readily removed even without updating the model parameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68% of the computation in the cache steps (46.84% for all steps), with less than 0.01 drop in FID. To achieve this, we introduce a novel scheme, named Learning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for diffusion transformers. Specifically, by leveraging the identical structure of layers in transformers and the sequential nature of diffusion, we explore redundant computations between timesteps by treating each layer as the fundamental unit for caching. To address the challenge of the exponential search space in deep models for identifying layers to cache and remove, we propose a novel differentiable optimization objective. An input-invariant yet timestep-variant router is then optimized, which can finally produce a static computation graph. Experimental results show that L2C largely outperforms samplers such as DDIM and DPM-Solver, alongside prior cache-based methods at the same inference speed. Code is available at https://github.com/horseee/learning-to-cache

**Link**: [arxiv](http://arxiv.org/abs/2406.01733v2),  [pdf](http://arxiv.org/pdf/2406.01733v2)

**Tags**: cs.LG cs.CV 



### Spineless Traversal for Layout Invalidation
**Authors**: Marisa Kirisame, Tiezhi Wang, Pavel Panchekha

**Updated**: 2024-11-16T01:39:44Z

**Summary**: Latency is a major concern for web rendering engines like those in Chrome, Safari, and Firefox. These engines reduce latency by using an incremental layout algorithm to redraw the page when the user interacts with it. In such an algorithm, elements that change frame-to-frame are marked dirty; only the dirty elements need be processed to draw the next frame, dramatically reducing latency. However, the standard incremental layout algorithm must search the page for dirty elements, accessing a number of auxiliary elements in the process. These auxiliary elements add cache misses and stalled cycles, and are responsible for a sizable fraction of all layout latency. We introduce a new, faster incremental layout algorithm called Spineless Traversal. Spineless Traversal uses a more computationally demanding priority queue algorithm to avoid the need to access auxiliary nodes and thus reduces cache traffic and stalls. This leads to dramatic speedups on the most latency-critical interactions such as hovering, typing, or animations. Moreover, thanks to numerous low-level optimizations, we are able to make Spineless Traversal competitive across the whole spectrum of incremental layout workloads. As a result, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the benchmark, with a mean speedup of 3.23x concentrated in the most latency-critical interactions such as hovering, typing, and animations.

**Link**: [arxiv](http://arxiv.org/abs/2411.10659v1),  [pdf](http://arxiv.org/pdf/2411.10659v1)

**Tags**: cs.PL 



### Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)
**Authors**: Ziyue Qiu, Juncheng Yang, Mor Harchol-Balter

**Updated**: 2024-11-15T22:37:48Z

**Summary**: Software caches are an intrinsic component of almost every computer system. Consequently, caching algorithms, particularly eviction policies, are the topic of many papers. Almost all these prior papers evaluate the caching algorithm based on its hit ratio, namely the fraction of requests that are found in the cache, as opposed to disk. The hit ratio is viewed as a proxy for traditional performance metrics like system throughput or response time. Intuitively it makes sense that higher hit ratio should lead to higher throughput (and lower response time), since more requests are found in the cache (low access time) as opposed to the disk (high access time).   This paper challenges this intuition. We show that increasing the hit ratio can actually hurt the throughput (and response time) for many caching algorithms. Our investigation follows a three-pronged approach involving (i) queueing modeling and analysis, (ii) implementation and measurement, and (iii) simulation to validate the accuracy of the queueing model. We also show that the phenomenon of throughput decreasing at higher hit ratios is likely to be more pronounced in future systems, where the trend is towards faster disks and higher numbers of cores per CPU.

**Link**: [arxiv](http://arxiv.org/abs/2404.16219v4),  [pdf](http://arxiv.org/pdf/2404.16219v4)

**Tags**: cs.PF 



### Forecasting GPU Performance for Deep Learning Training and Inference
**Authors**: Seonho Lee, Amar Phanishayee, Divya Mahajan

**Updated**: 2024-11-15T22:30:38Z

**Summary**: Deep learning kernels exhibit predictable memory accesses and compute patterns, making GPUs' parallel architecture well-suited for their execution. Software and runtime systems for GPUs are optimized to better utilize the stream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As deep learning models and GPUs evolve, access to newer GPUs is often limited, raising questions about the performance of new model architectures on existing GPUs, existing models on new GPUs, and new model architectures on new GPUs. To address these questions, we introduce NeuSight, a framework to predict the performance of various deep learning models, for both training and inference, on unseen GPUs without requiring actual execution. The framework leverages both GPU hardware behavior and software library optimizations to estimate end-to-end performance. Previous work uses regression models that capture linear trends or multilayer perceptrons to predict the overall latency of deep learning kernels on GPUs. These approaches suffer from higher error percentages when forecasting performance on unseen models and new GPUs. Instead, NeuSight decomposes the prediction problem into smaller problems, bounding the prediction through fundamental performance laws. NeuSight decomposes a single deep learning kernel prediction into smaller working sets called tiles, which are executed independently on the GPU. Tile-granularity predictions are determined using a machine learning approach and aggregated to estimate end-to-end latency. NeuSight outperforms prior work across various deep learning workloads and the latest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in predicting the latency of GPT3 model for training and inference on H100, compared to state-of-the-art prior works, where both GPT3 and H100 were not used to train the framework.

**Link**: [arxiv](http://arxiv.org/abs/2407.13853v2),  [pdf](http://arxiv.org/pdf/2407.13853v2)

**Tags**: cs.LG cs.PF 



### SmoothCache: A Universal Inference Acceleration Technique for Diffusion   Transformers
**Authors**: Joseph Liu, Joshua Geddes, Ziyu Guo, Haomiao Jiang, Mahesh Kumar Nandwana

**Updated**: 2024-11-15T16:24:02Z

**Summary**: Diffusion Transformers (DiT) have emerged as powerful generative models for various tasks, including image, video, and speech synthesis. However, their inference process remains computationally expensive due to the repeated evaluation of resource-intensive attention and feed-forward modules. To address this, we introduce SmoothCache, a model-agnostic inference acceleration technique for DiT architectures. SmoothCache leverages the observed high similarity between layer outputs across adjacent diffusion timesteps. By analyzing layer-wise representation errors from a small calibration set, SmoothCache adaptively caches and reuses key features during inference. Our experiments demonstrate that SmoothCache achieves 8% to 71% speed up while maintaining or even improving generation quality across diverse modalities. We showcase its effectiveness on DiT-XL for image generation, Open-Sora for text-to-video, and Stable Audio Open for text-to-audio, highlighting its potential to enable real-time applications and broaden the accessibility of powerful DiT models.

**Link**: [arxiv](http://arxiv.org/abs/2411.10510v1),  [pdf](http://arxiv.org/pdf/2411.10510v1)

**Tags**: cs.LG 



### ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression
**Authors**: Rui Xie, Linsen Ma, Alex Zhong, Feng Chen, Tong Zhang

**Updated**: 2024-11-15T07:25:54Z

**Summary**: As a core component in modern data centers, key-value cache provides high-throughput and low-latency services for high-speed data processing. The effectiveness of a key-value cache relies on its ability of accommodating the needed data. However, expanding the cache capacity is often more difficult than commonly expected because of many practical constraints, such as server costs, cooling issues, rack space, and even human resource expenses. A potential solution is compression, which virtually extends the cache capacity by condensing data in cache. In practice, this seemingly simple idea has not gained much traction in key-value cache system design, due to several critical issues: the compression-unfriendly index structure, severe read/write amplification, wasteful decompression operations, and heavy computing cost. This paper presents a hybrid DRAM-SSD cache design to realize a systematic integration of data compression in key-value cache. By treating compression as an essential component, we have redesigned the indexing structure, data management, and leveraged the emerging computational SSD hardware for collaborative optimizations. We have developed a prototype, called ZipCache. Our experimental results show that ZipCache can achieve up to 72.4% higher throughput and 42.4% lower latency, while reducing the write amplification by up to 26.2 times.

**Link**: [arxiv](http://arxiv.org/abs/2411.03174v2),  [pdf](http://arxiv.org/pdf/2411.03174v2)

**Tags**: cs.DB 



### Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures
**Authors**: Ishna Satyarth, Chao Yin, RuQing G. Xu, Devin A. Matthews

**Updated**: 2024-11-15T00:37:31Z

**Summary**: The factorization of skew-symmetric matrices is a critically understudied area of dense linear algebra (DLA), particularly in comparison to that of symmetric matrices. While some algorithms can be adapted from the symmetric case, the cost of algorithms can be reduced by exploiting skew-symmetry. A motivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix $X$, which is used in practical applications as a means of determining the determinant of $X$ as the square of the (cheaply-computed) Pfaffian of the skew-symmetric tridiagonal matrix $T$, for example in fields such as quantum electronic structure and machine learning. Such applications also often require pivoting in order to improve numerical stability. In this work we explore a combination of known literature algorithms and new algorithms recently derived using formal methods. High-performance parallel CPU implementations are created, leveraging the concept of fusion at multiple levels in order to reduce memory traffic overhead, as well as the BLIS framework which provides high-performance GEMM kernels, hierarchical parallelism, and cache blocking. We find that operation fusion and improved use of available bandwidth via parallelization of bandwidth-bound (level-2 BLAS) operations are essential for obtaining high performance, while a concise C++ implementation provides a clear and close connection to the formal derivation process without sacrificing performance.

**Link**: [arxiv](http://arxiv.org/abs/2411.09859v1),  [pdf](http://arxiv.org/pdf/2411.09859v1)

**Tags**: cs.MS 



### Edge Caching Optimization with PPO and Transfer Learning for Dynamic   Environments
**Authors**: Farnaz Niknia, Ping Wang

**Updated**: 2024-11-14T21:01:29Z

**Summary**: This paper addresses the challenge of edge caching in dynamic environments, where rising traffic loads strain backhaul links and core networks. We propose a Proximal Policy Optimization (PPO)-based caching strategy that fully incorporates key file attributes such as size, lifetime, importance, and popularity, while also considering random file request arrivals, reflecting more realistic edge caching scenarios. In dynamic environments, changes such as shifts in content popularity and variations in request rates frequently occur, making previously learned policies less effective as they were optimized for earlier conditions. Without adaptation, caching efficiency and response times can degrade. While learning a new policy from scratch in a new environment is an option, it is highly inefficient and computationally expensive. Thus, adapting an existing policy to these changes is critical. To address this, we develop a mechanism that detects changes in content popularity and request rates, ensuring timely adjustments to the caching strategy. We also propose a transfer learning-based PPO algorithm that accelerates convergence in new environments by leveraging prior knowledge. Simulation results demonstrate the significant effectiveness of our approach, outperforming a recent Deep Reinforcement Learning (DRL)-based method.

**Link**: [arxiv](http://arxiv.org/abs/2411.09812v1),  [pdf](http://arxiv.org/pdf/2411.09812v1)

**Tags**: cs.NI cs.LG cs.SY eess.SY 



### Squeezed Attention: Accelerating Long Context Length LLM Inference
**Authors**: Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Monishwaran Maheswaran, June Paik, Michael W. Mahoney, Kurt Keutzer, Amir Gholami

**Updated**: 2024-11-14T18:54:19Z

**Summary**: Emerging Large Language Model (LLM) applications require long input prompts to perform complex downstream tasks like document analysis and code generation. For these long context length applications, the length of the input prompt poses a significant challenge in terms of inference efficiency since the inference costs increase linearly with sequence length. However, for many of these applications, much of the context in the prompt is fixed across different user inputs, thereby providing the opportunity to perform offline optimizations to process user inputs quickly, as they are received. In this work, we propose Squeezed Attention as a mechanism to accelerate LLM applications where a large portion of the input prompt is fixed. We first leverage K-means clustering offline to group the keys for the fixed context based on semantic similarity and represent each cluster with a single centroid value. During inference, we compare query tokens from the user input with the centroids to predict which of the keys from the fixed context are semantically relevant and need to be loaded during inference. We then compute exact attention using only these important keys from the fixed context, thereby reducing bandwidth and computational costs. We also extend our method to use a hierarchical centroid lookup to identify important keys, which can reduce the complexity of attention from linear to logarithmic with respect to the context length. We implement optimized Triton kernels for centroid comparison and sparse FlashAttention with important keys, achieving more than 4x speedups during both the prefill and generation phases for long-context inference. Furthermore, we have extensively evaluated our method on various long-context benchmarks including LongBench, where it achieves a 3x reduction in KV cache budget without accuracy loss and up to an 8x reduction with <0.5 point accuracy gap for various models.

**Link**: [arxiv](http://arxiv.org/abs/2411.09688v1),  [pdf](http://arxiv.org/pdf/2411.09688v1)

**Tags**: cs.CL 



### Value Residual Learning For Alleviating Attention Concentration In   Transformers
**Authors**: Zhanchao Zhou, Tianyi Wu, Zhiyun Jiang, Zhenzhong Lan

**Updated**: 2024-11-14T17:46:04Z

**Summary**: Transformers can capture long-range dependencies using self-attention, allowing tokens to attend to all others directly. However, stacking multiple attention layers leads to attention concentration. One natural way to address this issue is to use cross-layer attention, allowing information from earlier layers to be directly accessible to later layers. However, this approach is computationally expensive. To address this problem, we propose Transformer with residual value (ResFormer) which approximates cross-layer attention through adding a residual connection from the values of the the first layer to all subsequent layers. Based on this method, one variant is the Transformer with single layer value (SVFormer), where all layers share the same value embedding from first layer, reducing the $KV$ cache by nearly 50\%. Comprehensive empirical evidence demonstrates that ResFormer mitigates attention concentration problem in deeper layers and enhances representation across most layers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as downstream tasks. Further visualization results suggest that Resformer alleviates attention sinks through avoiding value-state drains. SVFormer trains significantly faster than the vanilla Transformer and performs better than other methods like GQA and CLA, with performance influenced by sequence length and cumulative learning rate.

**Link**: [arxiv](http://arxiv.org/abs/2410.17897v2),  [pdf](http://arxiv.org/pdf/2410.17897v2)

**Tags**: cs.CL 



### Architectural Exploration of Application-Specific Resonant SRAM   Compute-in-Memory (rCiM)
**Authors**: Dhandeep Challagundla, Ignatius Bezzam, Riadul Islam

**Updated**: 2024-11-14T16:01:05Z

**Summary**: While general-purpose computing follows Von Neumann's architecture, the data movement between memory and processor elements dictates the processor's performance. The evolving compute-in-memory (CiM) paradigm tackles this issue by facilitating simultaneous processing and storage within static random-access memory (SRAM) elements. Numerous design decisions taken at different levels of hierarchy affect the figure of merits (FoMs) of SRAM, such as power, performance, area, and yield. The absence of a rapid assessment mechanism for the impact of changes at different hierarchy levels on global FoMs poses a challenge to accurately evaluating innovative SRAM designs. This paper presents an automation tool designed to optimize the energy and latency of SRAM designs incorporating diverse implementation strategies for executing logic operations within the SRAM. The tool structure allows easy comparison across different array topologies and various design strategies to result in energy-efficient implementations. Our study involves a comprehensive comparison of over 6900+ distinct design implementation strategies for EPFL combinational benchmark circuits on the energy-recycling resonant compute-in-memory (rCiM) architecture designed using TSMC 28 nm technology. When provided with a combinational circuit, the tool aims to generate an energy-efficient implementation strategy tailored to the specified input memory and latency constraints. The tool reduces 80.9% of energy consumption on average across all benchmarks while using the six-topology implementation compared to baseline implementation of single-macro topology by considering the parallel processing capability of rCiM cache size ranging from 4KB to 192KB.

**Link**: [arxiv](http://arxiv.org/abs/2411.09546v1),  [pdf](http://arxiv.org/pdf/2411.09546v1)

**Tags**: cs.AR cs.CY cs.ET cs.SY eess.SY 



### Enhancing Scalability and Performance in Influence Maximization with   Optimized Parallel Processing
**Authors**: Hanjiang Wu, Huan Xu, Joongun Park, Jesmin Jahan Tithi, Fabio Checconi, Jordi Wolfson-Pou, Fabrizio Petrini, Tushar Krishna

**Updated**: 2024-11-14T14:28:31Z

**Summary**: Influence Maximization (IM) is vital in viral marketing and biological network analysis for identifying key influencers. Given its NP-hard nature, approximate solutions are employed. This paper addresses scalability challenges in scale-out shared memory system by focusing on the state-of-the-art Influence Maximization via Martingales (IMM) benchmark. To enhance the work efficiency of the current IMM implementation, we propose EFFICIENTIMM with key strategies, including new parallelization scheme, NUMA-aware memory usage, dynamic load balancing and fine-grained adaptive data structures. Benchmarking on a 128-core CPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance improvements, achieving an average 5.9x speedup over Ripples across 8 diverse SNAP datasets, when compared to the best execution times of the original Ripples framework. Additionally, on the Youtube graph, EFFICIENTIMM demonstrates a better memory access pattern with 357.4x reduction in L1+L2 cache misses as compared to Ripples.

**Link**: [arxiv](http://arxiv.org/abs/2411.09473v1),  [pdf](http://arxiv.org/pdf/2411.09473v1)

**Tags**: cs.DC cs.DS 



### MARM: Unlocking the Future of Recommendation Systems through Memory   Augmentation and Scalable Complexity
**Authors**: Xiao Lv, Jiangxia Cao, Shijie Guan, Xiaoyou Zhou, Zhiguang Qi, Yaqiang Zang, Ming Li, Ben Wang, Kun Gai, Guorui Zhou

**Updated**: 2024-11-14T13:22:41Z

**Summary**: Scaling-law has guided the language model designing for past years, however, it is worth noting that the scaling laws of NLP cannot be directly applied to RecSys due to the following reasons: (1) The amount of training samples and model parameters is typically not the bottleneck for the model. Our recommendation system can generate over 50 billion user samples daily, and such a massive amount of training data can easily allow our model parameters to exceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the stability and robustness of the recommendation system, it is essential to control computational complexity FLOPs carefully. Considering the above differences with LLM, we can draw a conclusion that: for a RecSys model, compared to model parameters, the computational complexity FLOPs is a more expensive factor that requires careful control. In this paper, we propose our milestone work, MARM (Memory Augmented Recommendation Model), which explores a new cache scaling-laws successfully.

**Link**: [arxiv](http://arxiv.org/abs/2411.09425v1),  [pdf](http://arxiv.org/pdf/2411.09425v1)

**Tags**: cs.IR N/A 



### Pie: Pooling CPU Memory for LLM Inference
**Authors**: Yi Xu, Ziming Mao, Xiangxi Mo, Shu Liu, Ion Stoica

**Updated**: 2024-11-14T09:50:41Z

**Summary**: The rapid growth of LLMs has revolutionized natural language processing and AI analysis, but their increasing size and memory demands present significant challenges. A common solution is to spill over to CPU memory; however, traditional GPU-CPU memory swapping often results in higher latency and lower throughput.   This paper introduces Pie, an LLM inference framework that addresses these challenges with performance-transparent swapping and adaptive expansion. By leveraging predictable memory access patterns and the high bandwidth of modern hardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent data swapping without affecting foreground computation, expanding effective memory without added latency. Adaptive expansion dynamically adjusts CPU memory allocation based on real-time information, optimizing memory usage and performance under varying conditions.   Pie maintains low computation latency, high throughput, and high elasticity. Our experimental evaluation demonstrates that Pie achieves optimal swapping policy during cache warmup and effectively balances increased memory capacity with negligible impact on computation. With its extended capacity, Pie outperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally, Pie can reduce GPU memory usage by up to 1.67X while maintaining the same performance. Compared to FlexGen, an offline profiling-based swapping solution, Pie achieves magnitudes lower latency and 9.4X higher throughput.

**Link**: [arxiv](http://arxiv.org/abs/2411.09317v1),  [pdf](http://arxiv.org/pdf/2411.09317v1)

**Tags**: cs.LG cs.DC 



### Pkd-tree: Parallel $k$d-tree with Batch Updates
**Authors**: Ziyang Men, Zheqi Shen, Yan Gu, Yihan Sun

**Updated**: 2024-11-14T08:25:31Z

**Summary**: The $k$d-tree is one of the most widely used data structures to manage multi-dimensional data. Due to the ever-growing data volume, it is imperative to consider parallelism in $k$d-trees. However, we observed challenges in existing parallel kd-tree implementations, for both constructions and updates.   The goal of this paper is to develop efficient in-memory $k$d-trees by supporting high parallelism and cache-efficiency. We propose the Pkd-tree (Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and in practice. The Pkd-tree supports parallel tree construction, batch update (insertion and deletion), and various queries including k-nearest neighbor search, range query, and range count. We proved that our algorithms have strong theoretical bounds in work (sequential time complexity), span (parallelism), and cache complexity. Our key techniques include 1) an efficient construction algorithm that optimizes work, span, and cache complexity simultaneously, and 2) reconstruction-based update algorithms that guarantee the tree to be weight-balanced. With the new algorithmic insights and careful engineering effort, we achieved a highly optimized implementation of the Pkd-tree.   We tested Pkd-tree with various synthetic and real-world datasets, including both uniform and highly skewed data. We compare the Pkd-tree with state-of-the-art parallel $k$d-tree implementations. In all tests, with better or competitive query performance, Pkd-tree is much faster in construction and updates consistently than all baselines. We released our code.

**Link**: [arxiv](http://arxiv.org/abs/2411.09275v1),  [pdf](http://arxiv.org/pdf/2411.09275v1)

**Tags**: cs.DS cs.DB cs.DC cs.PF 



### Not All Heads Matter: A Head-Level KV Cache Compression Method with   Integrated Retrieval and Reasoning
**Authors**: Yu Fu, Zefan Cai, Abedelkadir Asi, Wayne Xiong, Yue Dong, Wen Xiao

**Updated**: 2024-11-14T01:56:11Z

**Summary**: Key-Value (KV) caching is a common technique to enhance the computational efficiency of Large Language Models (LLMs), but its memory overhead grows rapidly with input length. Prior work has shown that not all tokens are equally important for text generation, proposing layer-level KV cache compression to selectively retain key information. Recognizing the distinct roles of attention heads in generation, we propose HeadKV, a head-level KV cache compression method, and HeadKV-R2, which leverages a novel contextual reasoning ability estimation for compression. Our approach operates at the level of individual heads, estimating their importance for contextual QA tasks that require both retrieval and reasoning capabilities. Extensive experiments across diverse benchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct, Mistral-7B-Instruct), and long-context abilities tests demonstrate that our head-level KV cache compression significantly outperforms strong baselines, particularly in low-resource settings (KV size = 64 & 128). Notably, our method retains just 1.5% of the KV cache while achieving 97% of the performance of the full KV cache on the contextual question answering benchmark.Codes are available at https://github.com/FYYFU/HeadKV

**Link**: [arxiv](http://arxiv.org/abs/2410.19258v3),  [pdf](http://arxiv.org/pdf/2410.19258v3)

**Tags**: cs.CL cs.AI 



### Joint Model Caching and Resource Allocation in Generative AI-Enabled   Wireless Edge Networks
**Authors**: Zhang Liu, Hongyang Du, Lianfen Huang, Zhibin Gao, Dusit Niyato

**Updated**: 2024-11-13T15:07:15Z

**Summary**: With the rapid advancement of artificial intelligence (AI), generative AI (GenAI) has emerged as a transformative tool, enabling customized and personalized AI-generated content (AIGC) services. However, GenAI models with billions of parameters require substantial memory capacity and computational power for deployment and execution, presenting significant challenges to resource-limited edge networks. In this paper, we address the joint model caching and resource allocation problem in GenAI-enabled wireless edge networks. Our objective is to balance the trade-off between delivering high-quality AIGC and minimizing the delay in AIGC service provisioning. To tackle this problem, we employ a deep deterministic policy gradient (DDPG)-based reinforcement learning approach, capable of efficiently determining optimal model caching and resource allocation decisions for AIGC services in response to user mobility and time-varying channel conditions. Numerical results demonstrate that DDPG achieves a higher model hit ratio and provides superior-quality, lower-latency AIGC services compared to other benchmark solutions.

**Link**: [arxiv](http://arxiv.org/abs/2411.08672v1),  [pdf](http://arxiv.org/pdf/2411.08672v1)

**Tags**: cs.NI eess.SP 



### A Novel Extensible Simulation Framework for CXL-Enabled Systems
**Authors**: Yuda An, Shushu Yi, Bo Mao, Qiao Li, Mingzhe Zhang, Ke Zhou, Nong Xiao, Guangyu Sun, Xiaolin Wang, Yingwei Luo, Jie Zhang

**Updated**: 2024-11-13T03:28:44Z

**Summary**: Compute Express Link (CXL) serves as a rising industry standard, delivering high-speed cache-coherent links to a variety of devices, including host CPUs, computational accelerators, and memory devices. It is designed to promote system scalability, enable peer-to-peer exchanges, and accelerate data transmissions. To achieve these objectives, the most recent CXL protocol has brought forth several innovative features, such as port-focused routing, device-handled coherence, and PCIe 6.0 compatibility. However, due to the limited availability of hardware prototypes and simulators compatible with CXL, earlier CXL research has largely depended on emulating CXL devices using remote NUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in accurately representing the new features due to fundamental differences in hardware and protocols. Moreover, the absence of support for non-tree topology and PCIe links makes it complex to merely adapt existing simulators for CXL simulation. To overcome these problems, we introduce ESF, a simulation framework specifically designed for CXL systems. ESF has been developed to accurately reflect the unique features of the latest CXL protocol from the ground up. It uses a specialized interconnect layer to facilitate connections within a wide range of system topologies and also includes key components to carry out specific functions required by these features. By utilizing ESF, we thoroughly investigate various aspects of CXL systems, including system topology, device-handled coherence, and the effects of PCIe characteristics, leading to important findings that can guide the creation of high-performance CXL systems. The ESF source codes are fully open-source and can be accessed at https://anonymous.4open.science/r/ESF-1CE3.

**Link**: [arxiv](http://arxiv.org/abs/2411.08312v1),  [pdf](http://arxiv.org/pdf/2411.08312v1)

**Tags**: cs.AR 



### FaaS and Furious: abstractions and differential caching for efficient   data pre-processing
**Authors**: Jacopo Tagliabue, Ryan Curtin, Ciro Greco

**Updated**: 2024-11-12T21:50:03Z

**Summary**: Data pre-processing pipelines are the bread and butter of any successful AI project. We introduce a novel programming model for pipelines in a data lakehouse, allowing users to interact declaratively with assets in object storage. Motivated by real-world industry usage patterns, we exploit these new abstractions with a columnar and differential cache to maximize iteration speed for data scientists, who spent most of their time in pre-processing - adding or removing features, restricting or relaxing time windows, wrangling current or older datasets. We show how the new cache works transparently across programming languages, schemas and time windows, and provide preliminary evidence on its efficiency on standard data workloads.

**Link**: [arxiv](http://arxiv.org/abs/2411.08203v1),  [pdf](http://arxiv.org/pdf/2411.08203v1)

**Tags**: cs.DB 



### SKVQ: Sliding-window Key and Value Cache Quantization for Large Language   Models
**Authors**: Haojie Duanmu, Zhihang Yuan, Xiuhong Li, Jiangfei Duan, Xingcheng Zhang, Dahua Lin

**Updated**: 2024-11-12T08:18:45Z

**Summary**: Large language models (LLMs) can now handle longer sequences of tokens, enabling complex tasks like book understanding and generating lengthy novels. However, the key-value (KV) cache required for LLMs consumes substantial memory as context length increasing, becoming the bottleneck for deployment. In this paper, we present a strategy called SKVQ, which stands for sliding-window KV cache quantization, to address the issue of extremely low bitwidth KV cache quantization. To achieve this, SKVQ rearranges the channels of the KV cache in order to improve the similarity of channels in quantization groups, and applies clipped dynamic quantization at the group level. Additionally, SKVQ ensures that the most recent window tokens in the KV cache are preserved with high precision. This helps maintain the accuracy of a small but important portion of the KV cache.SKVQ achieves high compression ratios while maintaining accuracy. Our evaluation on LLMs demonstrates that SKVQ surpasses previous quantization approaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit values with minimal loss of accuracy. With SKVQ, it is possible to process context lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7 times faster decoding.

**Link**: [arxiv](http://arxiv.org/abs/2405.06219v3),  [pdf](http://arxiv.org/pdf/2405.06219v3)

**Tags**: cs.LG cs.CL 



### Leveraging Previous Steps: A Training-free Fast Solver for Flow   Diffusion
**Authors**: Kaiyu Song, Hanjiang Lai

**Updated**: 2024-11-12T08:17:15Z

**Summary**: Flow diffusion models (FDMs) have recently shown potential in generation tasks due to the high generation quality. However, the current ordinary differential equation (ODE) solver for FDMs, e.g., the Euler solver, still suffers from slow generation since ODE solvers need many number function evaluations (NFE) to keep high-quality generation. In this paper, we propose a novel training-free flow-solver to reduce NFE while maintaining high-quality generation. The key insight for the flow-solver is to leverage the previous steps to reduce the NFE, where a cache is created to reuse these results from the previous steps. Specifically, the Taylor expansion is first used to approximate the ODE. To calculate the high-order derivatives of Taylor expansion, the flow-solver proposes to use the previous steps and a polynomial interpolation to approximate it, where the number of orders we could approximate equals the number of previous steps we cached. We also prove that the flow-solver has a more minor approximation error and faster generation speed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom, LSUN-Church, ImageNet, and real text-to-image generation prove the efficiency of the flow-solver. Specifically, the flow-solver improves the FID-30K from 13.79 to 6.75, from 46.64 to 19.49 with $\text{NFE}=10$ on CIFAR-10 and LSUN-Church, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2411.07627v1),  [pdf](http://arxiv.org/pdf/2411.07627v1)

**Tags**: cs.CV 



### WDMoE: Wireless Distributed Mixture of Experts for Large Language Models
**Authors**: Nan Xue, Yaping Sun, Zhiyong Chen, Meixia Tao, Xiaodong Xu, Liang Qian, Shuguang Cui, Wenjun Zhang, Ping Zhang

**Updated**: 2024-11-11T02:48:00Z

**Summary**: Large Language Models (LLMs) have achieved significant success in various natural language processing tasks, but the role of wireless networks in supporting LLMs has not been thoroughly explored. In this paper, we propose a wireless distributed Mixture of Experts (WDMoE) architecture to enable collaborative deployment of LLMs across edge servers at the base station (BS) and mobile devices in wireless networks. Specifically, we decompose the MoE layer in LLMs by placing the gating network and the preceding neural network layer at BS, while distributing the expert networks among the devices. This deployment leverages the parallel inference capabilities of expert networks on mobile devices, effectively utilizing the limited computing and caching resources of these devices. Accordingly, we develop a performance metric for WDMoE-based LLMs, which accounts for both model capability and latency. To minimize the latency while maintaining accuracy, we jointly optimize expert selection and bandwidth allocation based on the performance metric. Moreover, we build a hardware testbed using NVIDIA Jetson kits to validate the effectiveness of WDMoE. Both theoretical simulations and practical hardware experiments demonstrate that the proposed method can significantly reduce the latency without compromising LLM performance.

**Link**: [arxiv](http://arxiv.org/abs/2411.06681v1),  [pdf](http://arxiv.org/pdf/2411.06681v1)

**Tags**: cs.LG cs.AI cs.DC cs.IT math.IT 



### Anchor Attention, Small Cache: Code Generation with Large Language   Models
**Authors**: Xiangyu Zhang, Yu Zhou, Guang Yang, Harald C. Gall, Taolue Chen

**Updated**: 2024-11-11T02:47:05Z

**Summary**: The development of large language models (LLMs) has revolutionized automated code generation. However, their high demand of computation resources has hindered a broader deployment and raised environmental concerns. A common strategy for diminishing computational demands is to cache Key-Value (KV) states from the attention mechanism which is adopted predominately by mainstream LLMs. It can mitigate the need of repeated attention computations, but brings significant memory overhead. Current practices in NLP often use sparse attention which may, unfortunately, lead to substantial inaccuracies, or hallucinations, in code generation tasks. In this paper, we analyze the attention weights distribution within code generation models via an empirical study, uncovering a sparsity pattern, i.e., the aggregation of information at specific anchor points. Based on this observation, we propose a novel approach, AnchorCoder, which features token-wise anchor attention designed to extract and compress the contextual information, and layer-wise anchor attention enabling cross-layer communication to mitigate the issue of excessive superposition caused by the compression. The extensive experiments across multiple benchmark datasets confirm the effectiveness of AnchorCoder, which can consistently achieve a significant (at least 70%) reduction in KV cache requirements, while preserving the majority of model's performance.

**Link**: [arxiv](http://arxiv.org/abs/2411.06680v1),  [pdf](http://arxiv.org/pdf/2411.06680v1)

**Tags**: cs.SE 68N19 D.2.3 



### An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning
**Authors**: Dong Li, Aijia Zhang, Junqi Gao, Biqing Qi

**Updated**: 2024-11-11T01:53:14Z

**Summary**: Incremental graph learning has gained significant attention for its ability to address the catastrophic forgetting problem in graph representation learning. However, traditional methods often rely on a large number of labels for node classification, which is impractical in real-world applications. This makes few-shot incremental learning on graphs a pressing need. Current methods typically require extensive training samples from meta-learning to build memory and perform intensive fine-tuning of GNN parameters, leading to high memory consumption and potential loss of previously learned knowledge. To tackle these challenges, we introduce Mecoin, an efficient method for building and maintaining memory. Mecoin employs Structured Memory Units to cache prototypes of learned categories, as well as Memory Construction Modules to update these prototypes for new categories through interactions between the nodes and the cached prototypes. Additionally, we have designed a Memory Representation Adaptation Module to store probabilities associated with each class prototype, reducing the need for parameter fine-tuning and lowering the forgetting rate. When a sample matches its corresponding class prototype, the relevant probabilities are retrieved from the MRaM. Knowledge is then distilled back into the GNN through a Graph Knowledge Distillation Module, preserving the model's memory. We analyze the effectiveness of Mecoin in terms of generalization error and explore the impact of different distillation strategies on model performance through experiments and VC-dimension analysis. Compared to other related works, Mecoin shows superior performance in accuracy and forgetting rate. Our code is publicly available on the https://github.com/Arvin0313/Mecoin-GFSCIL.git .

**Link**: [arxiv](http://arxiv.org/abs/2411.06659v1),  [pdf](http://arxiv.org/pdf/2411.06659v1)

**Tags**: cs.LG cs.AI 



### Context Parallelism for Scalable Million-Token Inference
**Authors**: Amy Yang, Jingyi Yang, Aya Ibrahim, Xinfeng Xie, Bangsheng Tang, Grigory Sizov, Jeremy Reizenstein, Jongsoo Park, Jianyu Huang

**Updated**: 2024-11-10T23:04:12Z

**Summary**: We present context parallelism for long-context large language model inference, which achieves near-linear scaling for long-context prefill latency with up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M context prefill with Llama3 405B model in 77s (93% parallelization efficiency, 63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two lossless exact ring attention variants: pass-KV and pass-Q to cover a wide range of use cases with the state-of-the-art performance: full prefill, persistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected with RDMA and TCP both show similar scalability for long-context prefill, demonstrating that our method scales well using common commercial data center with medium-to-low inter-host bandwidth.

**Link**: [arxiv](http://arxiv.org/abs/2411.01783v2),  [pdf](http://arxiv.org/pdf/2411.01783v2)

**Tags**: cs.DC cs.AI cs.LG 



### GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for   Dynamic Graph Processing
**Authors**: Hongfu Li

**Updated**: 2024-11-10T15:58:07Z

**Summary**: An efficient data structure is fundamental to meeting the growing demands in dynamic graph processing. However, the dual requirements for graph computation efficiency (with contiguous structures) and graph update efficiency (with linked list-like structures) present a conflict in the design principles of graph structures. After experimental studies of existing state-of-the-art dynamic graph structures, we observe that the overhead of cache misses accounts for a major portion of the graph computation time. This paper presents GastCoCo, a system with graph storage and coroutine-based prefetch co-design. By employing software prefetching via stackless coroutines and introducing a prefetch-friendly data structure CBList, GastCoCo significantly alleviates the performance degradation caused by cache misses. Our results show that GastCoCo outperforms state-of-the-art graph storage systems by 1.3x - 180x in graph updates and 1.4x - 41.1x in graph computation.

**Link**: [arxiv](http://arxiv.org/abs/2312.14396v4),  [pdf](http://arxiv.org/pdf/2312.14396v4)

**Tags**: cs.DB 



### Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion   Prior
**Authors**: Tanvir Mahmud, Mustafa Munir, Radu Marculescu, Diana Marculescu

**Updated**: 2024-11-10T10:08:37Z

**Summary**: Video-to-video synthesis poses significant challenges in maintaining character consistency, smooth temporal transitions, and preserving visual quality during fast motion. While recent fully cross-frame self-attention mechanisms have improved character consistency across multiple frames, they come with high computational costs and often include redundant operations, especially for videos with higher frame rates. To address these inefficiencies, we propose an adaptive motion-guided cross-frame attention mechanism that selectively reduces redundant computations. This enables a greater number of cross-frame attentions over more frames within the same computational budget, thereby enhancing both video quality and temporal coherence. Our method leverages optical flow to focus on moving regions while sparsely attending to stationary areas, allowing for the joint editing of more frames without increasing computational demands. Traditional frame interpolation techniques struggle with motion blur and flickering in intermediate frames, which compromises visual fidelity. To mitigate this, we introduce KV-caching for jointly edited frames, reusing keys and values across intermediate frames to preserve visual quality and maintain temporal consistency throughout the video. With our adaptive cross-frame self-attention approach, we achieve a threefold increase in the number of keyframes processed compared to existing methods, all within the same computational budget as fully cross-frame attention baselines. This results in significant improvements in prediction accuracy and temporal consistency, outperforming state-of-the-art approaches. Code will be made publicly available at https://github.com/tanvir-utexas/AdaVE/tree/main

**Link**: [arxiv](http://arxiv.org/abs/2406.04873v2),  [pdf](http://arxiv.org/pdf/2406.04873v2)

**Tags**: cs.CV cs.AI 



### EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in   LLM Serving
**Authors**: Haiying Shen, Tanmoy Sen

**Updated**: 2024-11-10T05:12:51Z

**Summary**: As Large Language Models (LLMs) continue to grow, reducing costs and alleviating GPU demands has become increasingly critical. However, existing schedulers primarily target either GPU compute or Key-Value Cache (KVC) utilization, failing to fully optimize both GPU compute and KVC usage during each iteration or guarantee timely KVC allocations when needed. To address these challenges, we conducted a trace-based experimental analysis and made insightful observations, leading to the design of a system called EcoServe. EcoServe maximizes multi-resource utilization while ensuring service-level objective (SLO) guarantees in LLM serving. To enable adding prompts to a batch to maximize GPU utilization in each iteration, EcoServe maintains separate waiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It batches GTs with the same predicted response lengths (RL) to save scheduling time and allocates KVC space for the predicted RL to avoid KVC allocation failures. It further has a novel KVC pipelining method, allowing sharing allocated but unused KVC space to enhance KVC utilization. In addition, it prioritizes queued requests that occupy more KVC to release KVC earlier and satisfy request service-level-objective (SLO). Experimental results demonstrate that EcoServe increases throughput by up to 4$\times$ with the same level of latency, generates up to 91\% lower job completion time and up to 91\% higher SLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs used in DistServe by up to 78\% while maintaining the same level of goodput.

**Link**: [arxiv](http://arxiv.org/abs/2411.06364v1),  [pdf](http://arxiv.org/pdf/2411.06364v1)

**Tags**: cs.DC 



### Eigen Attention: Attention in Low-Rank Space for KV Cache Compression
**Authors**: Utkarsh Saxena, Gobinda Saha, Sakshi Choudhary, Kaushik Roy

**Updated**: 2024-11-08T16:29:33Z

**Summary**: Large language models (LLMs) represent a groundbreaking advancement in the domain of natural language processing due to their impressive reasoning abilities. Recently, there has been considerable interest in increasing the context lengths for these models to enhance their applicability to complex tasks. However, at long context lengths and large batch sizes, the key-value (KV) cache, which stores the attention keys and values, emerges as the new bottleneck in memory usage during inference. To address this, we propose Eigen Attention, which performs the attention operation in a low-rank space, thereby reducing the KV cache memory overhead. Our proposed approach is orthogonal to existing KV cache compression techniques and can be used synergistically with them. Through extensive experiments over OPT, MPT, and Llama model families, we demonstrate that Eigen Attention results in up to 40% reduction in KV cache sizes and up to 60% reduction in attention operation latency with minimal drop in performance. Code is available at https://github.com/UtkarshSaxena1/EigenAttn.

**Link**: [arxiv](http://arxiv.org/abs/2408.05646v2),  [pdf](http://arxiv.org/pdf/2408.05646v2)

**Tags**: cs.LG cs.AI cs.CL 



### AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing   and Data Locality
**Authors**: Ilias Bournias, Lukas Cavigelli, Georgios Zacharopoulos

**Updated**: 2024-11-08T13:24:01Z

**Summary**: Large Language Model (LLM) inference on large-scale systems is expected to dominate future cloud infrastructures. Efficient LLM inference in cloud environments with numerous AI accelerators is challenging, necessitating extensive optimizations for optimal performance. Current systems batch prefill and decoding to boost throughput but encounter latency issues, while others disaggregate these phases, leading to resource underutilization. We propose AcceLLM, a novel method addressing latency and load balancing, inspired by the cache data management. It strategically utilizes redundant data to enhance inference via load balancing and optimal hardware use. Simulated evaluations on Nvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art systems up to 30% in latency and efficiency, handling diverse workloads effectively.

**Link**: [arxiv](http://arxiv.org/abs/2411.05555v1),  [pdf](http://arxiv.org/pdf/2411.05555v1)

**Tags**: cs.DC 



### GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic   Embedding Caching
**Authors**: Sajal Regmi, Chetan Phakami Pun

**Updated**: 2024-11-08T02:21:19Z

**Summary**: Large Language Models (LLMs), such as GPT (Radford et al., 2019), have significantly advanced artificial intelligence by enabling sophisticated natural language understanding and generation. However, the high computational and financial costs associated with frequent API calls to these models present a substantial bottleneck, especially for applications like customer service chatbots that handle repetitive queries. In this paper, we introduce GPT Semantic Cache, a method that leverages semantic caching of query embeddings in in-memory storage (Redis). By storing embeddings of user queries, our approach efficiently identifies semantically similar questions, allowing for the retrieval of pre-generated responses without redundant API calls to the LLM. This technique reduces operational costs and improves response times, enhancing the efficiency of LLM-powered applications.

**Link**: [arxiv](http://arxiv.org/abs/2411.05276v1),  [pdf](http://arxiv.org/pdf/2411.05276v1)

**Tags**: cs.LG 



### Loki: Low-rank Keys for Efficient Sparse Attention
**Authors**: Prajwal Singhania, Siddharth Singh, Shwai He, Soheil Feizi, Abhinav Bhatele

**Updated**: 2024-11-07T18:58:50Z

**Summary**: Inference on large language models (LLMs) can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in LLM inference contributes significantly to these costs, which has sparked an interest in approximating the self-attention computation to reduce such costs. In this work, we propose to approximate self-attention by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to speed up the attention computation due to reduced data movement (load/store) and compute costs while maintaining the efficacy of the models better than other popular approximation methods.

**Link**: [arxiv](http://arxiv.org/abs/2406.02542v2),  [pdf](http://arxiv.org/pdf/2406.02542v2)

**Tags**: cs.LG 



### BitNet a4.8: 4-bit Activations for 1-bit LLMs
**Authors**: Hongyu Wang, Shuming Ma, Furu Wei

**Updated**: 2024-11-07T18:41:50Z

**Summary**: Recent research on the 1-bit Large Language Models (LLMs), such as BitNet b1.58, presents a promising direction for reducing the inference cost of LLMs while maintaining their performance. In this work, we introduce BitNet a4.8, enabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid quantization and sparsification strategy to mitigate the quantization errors introduced by the outlier channels. Specifically, we utilize 4-bit activations for inputs to the attention and feed-forward network layers, while sparsifying intermediate states followed with 8-bit quantization. Extensive experiments demonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58 with equivalent training costs, while being faster in inference with enabling 4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of parameters and supports 3-bit KV cache, further enhancing the efficiency of large-scale LLM deployment and inference.

**Link**: [arxiv](http://arxiv.org/abs/2411.04965v1),  [pdf](http://arxiv.org/pdf/2411.04965v1)

**Tags**: cs.CL cs.LG 



### Adaptive Caching for Faster Video Generation with Diffusion Transformers
**Authors**: Kumara Kahatapitiya, Haozhe Liu, Sen He, Ding Liu, Menglin Jia, Chenyang Zhang, Michael S. Ryoo, Tian Xie

**Updated**: 2024-11-07T17:06:32Z

**Summary**: Generating temporally-consistent high-fidelity videos can be computationally expensive, especially over longer temporal spans. More-recent Diffusion Transformers (DiTs) -- despite making significant headway in this context -- have only heightened such challenges as they rely on larger models and heavier attention mechanisms, resulting in slower inference speeds. In this paper, we introduce a training-free method to accelerate video DiTs, termed Adaptive Caching (AdaCache), which is motivated by the fact that "not all videos are created equal": meaning, some videos require fewer denoising steps to attain a reasonable quality than others. Building on this, we not only cache computations through the diffusion process, but also devise a caching schedule tailored to each video generation, maximizing the quality-latency trade-off. We further introduce a Motion Regularization (MoReg) scheme to utilize video information within AdaCache, essentially controlling the compute allocation based on motion content. Altogether, our plug-and-play contributions grant significant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video generation) without sacrificing the generation quality, across multiple video DiT baselines.

**Link**: [arxiv](http://arxiv.org/abs/2411.02397v2),  [pdf](http://arxiv.org/pdf/2411.02397v2)

**Tags**: cs.CV 



### JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial   Cyber-Physical Systems
**Authors**: Geng Sun, Jiaxu Wu, Long He, Jiacheng Wang, Dusit Niyato, Abbas Jamalipour, Shiwen Mao

**Updated**: 2024-11-07T14:59:44Z

**Summary**: In the era of the sixth generation (6G) and industrial Internet of Things (IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of sensor devices and computing-intensive tasks. To address the limited resources of IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) has emerged as a promising solution, providing flexible and cost-effective services in close proximity of IIoT sensor devices (ISDs). However, leveraging aerial MEC to meet the delay-sensitive and computation-intensive requirements of the ISDs could face several challenges, including the limited communication, computation and caching (3C) resources, stringent offloading requirements for 3C services, and constrained on-board energy of UAVs. To address these issues, we first present a collaborative aerial MEC-assisted ICPS architecture by incorporating the computing capabilities of the macro base station (MBS) and UAVs. We then formulate a service delay minimization optimization problem (SDMOP). Since the SDMOP is proved to be an NP-hard problem, we propose a joint computation offloading, caching, communication resource allocation, computation resource allocation, and UAV trajectory control approach (JC5A). Specifically, JC5A consists of a block successive upper bound minimization method of multipliers (BSUMM) for computation offloading and service caching, a convex optimization-based method for communication and computation resource allocation, and a successive convex approximation (SCA)-based method for UAV trajectory control. Moreover, we theoretically prove the convergence and polynomial complexity of JC5A. Simulation results demonstrate that the proposed approach can achieve superior system performance compared to the benchmark approaches and algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2411.04762v1),  [pdf](http://arxiv.org/pdf/2411.04762v1)

**Tags**: cs.NI eess.SP 



### CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot   Classification
**Authors**: Qijie Wang, Guandu Liu, Bin Wang

**Updated**: 2024-11-07T09:33:40Z

**Summary**: Recent advances in vision-language foundational models, such as CLIP, have demonstrated significant strides in zero-shot classification. However, the extensive parameterization of models like CLIP necessitates a resource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have introduced training-free methods aimed at bolstering the efficacy of downstream tasks. While these approaches incorporate support sets to maintain data distribution consistency between knowledge cache and test sets, they often fall short in terms of generalization on the test set, particularly when faced with test data exhibiting substantial distributional variations. In this work, we present CapS-Adapter, an innovative method that employs a caption-based support set, effectively harnessing both image and caption features to exceed existing state-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly constructs support sets that closely mirror target distributions, utilizing instance-level distribution features extracted from multimodal large models. By leveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances predictive accuracy through the use of multimodal support sets. Our method achieves outstanding zero-shot classification results across 19 benchmark datasets, improving accuracy by 2.19\% over the previous leading method. Our contributions are substantiated through extensive validation on multiple benchmark datasets, demonstrating superior performance and robust generalization capabilities. Our code is made publicly available at https://github.com/WLuLi/CapS-Adapter.

**Link**: [arxiv](http://arxiv.org/abs/2405.16591v2),  [pdf](http://arxiv.org/pdf/2405.16591v2)

**Tags**: cs.CV 



### HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO   Computation Redundancy
**Authors**: Shuqing Luo, Jie Peng, Pingzhi Li, Tianlong Chen

**Updated**: 2024-11-07T06:40:40Z

**Summary**: Mixture-of-Experts (MoE) has emerged as a practical approach to scale up parameters for the Transformer model to achieve better generalization while maintaining a sub-linear increase in computation overhead. Current MoE models are mainly built with expert parallelism on distributed devices. However, it usually depends on homogeneous devices to deploy and suffers from heavy communication overhead and computation redundancy. In this paper, we explore developing a \texttt{H}eterogeneous-aware \texttt{EX}pert \texttt{A}llocation framework, \textbf{\texttt{HEXA-MoE}}, with significantly enhanced computing efficiency. It contains two components: ($1$) \textit{Expert-Specific Operators}. We replace the typical general matrix multiplication or grouped matrix multiplication interfaces with our operators, which allows the computing to be performed in an in-place manner with \textbf{ZERO} redundancy. ($2$) \textit{Adaptive Data- and Model-Centric Configurations} for different workload scales. Specifically, we introduce a pipeline-shared cache on each device to tackle the heavy memory consumption in the existing data-centric MoE library. Comprehensive experiments on the Swin-MoE benchmark consistently reveal the effectiveness of our \texttt{HEXA-MoE} framework, i.e., reducing $10\%\sim48\%$ memory consumption and achieving $0.5\sim4.3\times$ speed up compared to current state-of-the-art MoE libraries. Furthermore, we examine our \texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric settings. Promising results show that employing optimal parallel configuration with \texttt{HEXA-MoE} on heterogeneous devices can substantially minimize overall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.

**Link**: [arxiv](http://arxiv.org/abs/2411.01288v2),  [pdf](http://arxiv.org/pdf/2411.01288v2)

**Tags**: cs.DC 



### Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated   Parameters by Tencent
**Authors**: Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang, Jonny Han, Xiaobo Shu, Jiahao Bu, Zhongzhi Chen, Xuemeng Huang, Fengzong Lian, Saiyong Yang, Jianfeng Yan, Yuyuan Zeng, Xiaoqin Ren, Chao Yu, Lulu Wu, Yue Mao, Jun Xia, Tao Yang, Suncong Zheng, Kan Wu, Dian Jiao, Jinbao Xue, Xipeng Zhang, Decheng Wu, Kai Liu, Dengpeng Wu, Guanghui Xu, Shaohua Chen, Shuang Chen, Xiao Feng, Yigeng Hong, Junqiang Zheng, Chengcheng Xu, Zongwei Li, Xiong Kuang, Jianglu Hu, Yiqi Chen, Yuchi Deng, Guiyang Li, Ao Liu, Chenchen Zhang, Shihui Hu, Zilong Zhao, Zifan Wu, Yao Ding, Weichao Wang, Han Liu, Roberts Wang, Hao Fei, Peijie Yu, Ze Zhao, Xun Cao, Hai Wang, Fusheng Xiang, Mengyuan Huang, Zhiyuan Xiong, Bin Hu, Xuebin Hou, Lei Jiang, Jianqiang Ma, Jiajia Wu, Yaping Deng, Yi Shen, Qian Wang, Weijie Liu, Jie Liu, Meng Chen, Liang Dong, Weiwen Jia, Hu Chen, Feifei Liu, Rui Yuan, Huilin Xu, Zhenxiang Yan, Tengfei Cao, Zhichao Hu, Xinhua Feng, Dong Du, Tinghao Yu, Yangyu Tao, Feng Zhang, Jianchen Zhu, Chengzhong Xu, Xirui Li, Chong Zha, Wen Ouyang, Yinben Xia, Xiang Li, Zekun He, Rongpeng Chen, Jiawei Song, Ruibin Chen, Fan Jiang, Chongqing Zhao, Bo Wang, Hao Gong, Rong Gan, Winston Hu, Zhanhui Kang, Yong Yang, Yuhong Liu, Di Wang, Jie Jiang

**Updated**: 2024-11-06T09:15:27Z

**Summary**: In this paper, we introduce Hunyuan-Large, which is currently the largest open-source Transformer-based mixture of experts model, with a total of 389 billion parameters and 52 billion activation parameters, capable of handling up to 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior performance across various benchmarks including language understanding and generation, logical reasoning, mathematical problem-solving, coding, long-context, and aggregated tasks, where it outperforms LLama3.1-70B and exhibits comparable performance when compared to the significantly larger LLama3.1-405B model. Key practice of Hunyuan-Large include large-scale synthetic data that is orders larger than in previous literature, a mixed expert routing strategy, a key-value cache compression technique, and an expert-specific learning rate strategy. Additionally, we also investigate the scaling laws and learning rate schedule of mixture of experts models, providing valuable insights and guidances for future model development and optimization. The code and checkpoints of Hunyuan-Large are released to facilitate future innovations and applications.   Codes: https://github.com/Tencent/Hunyuan-Large   Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large

**Link**: [arxiv](http://arxiv.org/abs/2411.02265v3),  [pdf](http://arxiv.org/pdf/2411.02265v3)

**Tags**: cs.CL cs.AI 



### Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model   Training Pipelines via Memoization-Awareness
**Authors**: Abdelmajid Essofi, Ridwan Salahuddeen, Munachiso Nwadike, Elnura Zhalieva, Kun Zhang, Eric Xing, Willie Neiswanger, Qirong Ho

**Updated**: 2024-11-06T07:53:04Z

**Summary**: The training or fine-tuning of machine learning, vision, and language models is often implemented as a pipeline: a sequence of stages encompassing data preparation, model training and evaluation. In this paper, we exploit pipeline structures to reduce the cost of hyperparameter tuning for model training/fine-tuning, which is particularly valuable for language models given their high costs in GPU-days. We propose a "memoization-aware" Bayesian Optimization (BO) algorithm, EEIPU, that works in tandem with a pipeline caching system, allowing it to evaluate significantly more hyperparameter candidates per GPU-day than other tuning algorithms. The result is better-quality hyperparameters in the same amount of search time, or equivalently, reduced search time to reach the same hyperparameter quality. In our benchmarks on machine learning (model ensembles), vision (convolutional architecture) and language (T5 architecture) pipelines, we compare EEIPU against recent BO algorithms: EEIPU produces an average of $103\%$ more hyperparameter candidates (within the same budget), and increases the validation metric by an average of $108\%$ more than other algorithms (where the increase is measured starting from the end of warm-up iterations).

**Link**: [arxiv](http://arxiv.org/abs/2411.03731v1),  [pdf](http://arxiv.org/pdf/2411.03731v1)

**Tags**: cs.LG stat.ML 



### The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM   Serving Systems
**Authors**: Linke Song, Zixuan Pang, Wenhao Wang, Zihao Wang, XiaoFeng Wang, Hongbo Chen, Wei Song, Yier Jin, Dan Meng, Rui Hou

**Updated**: 2024-11-06T07:12:55Z

**Summary**: The wide deployment of Large Language Models (LLMs) has given rise to strong demands for optimizing their inference performance. Today's techniques serving this purpose primarily focus on reducing latency and improving throughput through algorithmic and hardware enhancements, while largely overlooking their privacy side effects, particularly in a multi-user environment. In our research, for the first time, we discovered a set of new timing side channels in LLM systems, arising from shared caches and GPU memory allocations, which can be exploited to infer both confidential system prompts and those issued by other users. These vulnerabilities echo security challenges observed in traditional computing systems, highlighting an urgent need to address potential information leakage in LLM serving infrastructures. In this paper, we report novel attack strategies designed to exploit such timing side channels inherent in LLM deployments, specifically targeting the Key-Value (KV) cache and semantic cache widely used to enhance LLM inference performance. Our approach leverages timing measurements and classification models to detect cache hits, allowing an adversary to infer private prompts with high accuracy. We also propose a token-by-token search algorithm to efficiently recover shared prompt prefixes in the caches, showing the feasibility of stealing system prompts and those produced by peer users. Our experimental studies on black-box testing of popular online LLM services demonstrate that such privacy risks are completely realistic, with significant consequences. Our findings underscore the need for robust mitigation to protect LLM systems against such emerging threats.

**Link**: [arxiv](http://arxiv.org/abs/2409.20002v2),  [pdf](http://arxiv.org/pdf/2409.20002v2)

**Tags**: cs.CR 



### HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE   Inference
**Authors**: Peng Tang, Jiacheng Liu, Xiaofeng Hou, Yifei Pu, Jing Wang, Pheng-Ann Heng, Chao Li, Minyi Guo

**Updated**: 2024-11-06T01:49:45Z

**Summary**: The Mixture-of-Experts (MoE) architecture has demonstrated significant advantages in the era of Large Language Models (LLMs), offering enhanced capabilities with reduced inference costs. However, deploying MoE-based LLMs on memoryconstrained edge devices remains challenging due to their substantial memory requirements. While existing expertoffloading methods alleviate the memory requirements, they often incur significant expert-loading costs or compromise model accuracy. We present HOBBIT, a mixed precision expert offloading system to enable flexible and efficient MoE inference. Our key insight is that dynamically replacing less critical cache-miss experts with low precision versions can substantially reduce expert-loading latency while preserving model accuracy. HOBBIT introduces three innovative techniques that map the natural hierarchy of MoE computation: (1) a token-level dynamic expert loading mechanism, (2) a layer-level adaptive expert prefetching technique, and (3) a sequence-level multidimensional expert caching policy. These innovations fully leverage the benefits of mixedprecision expert inference. By implementing HOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate its performance across different edge devices with representative MoE models. The results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding compared to state-of-the-art MoE offloading systems.

**Link**: [arxiv](http://arxiv.org/abs/2411.01433v2),  [pdf](http://arxiv.org/pdf/2411.01433v2)

**Tags**: cs.LG cs.DC 



### Wireless Edge Content Broadcast via Integrated Terrestrial and   Non-terrestrial Networks
**Authors**: Feng Wang, Giovanni Geraci, Lingxiang Li, Peng Wang, Tony Q. S. Quek

**Updated**: 2024-11-05T08:34:44Z

**Summary**: Non-terrestrial networks (NTN) have emerged as a transformative solution to bridge the digital divide and deliver essential services to remote and underserved areas. In this context, low Earth orbit (LEO) satellite constellations offer remarkable potential for efficient cache content broadcast in remote regions, thereby extending the reach of digital services. In this paper, we introduce a novel approach to optimize wireless edge content placement using NTN. Despite wide coverage, the varying NTN transmission capabilities must be carefully aligned with each content placement to maximize broadcast efficiency. In this paper, we introduce a novel approach to optimize wireless edge content placement using NTN, positioning NTN as a complement to TN for achieving optimal content broadcasting. Specifically, we dynamically select content for placement via NTN links. This selection is based on popularity and suitability for delivery through NTN, while considering the orbital motion of LEO satellites. Our system-level case studies, based on a practical LEO constellation, demonstrate the significant improvement in placement speed compared to existing methods, which neglect network mobility. We also demonstrate that NTN links significantly outperform standalone wireless TN solutions, particularly in the early stages of content delivery. This advantage is amplified when there is a higher correlation of content popularity across geographical regions.

**Link**: [arxiv](http://arxiv.org/abs/2308.05591v3),  [pdf](http://arxiv.org/pdf/2308.05591v3)

**Tags**: eess.SY cs.IT cs.NI cs.SY eess.SP math.IT 



### TokenSelect: Efficient Long-Context Inference and Length Extrapolation   for LLMs via Dynamic Token-Level KV Cache Selection
**Authors**: Wei Wu, Zhuoshi Pan, Chao Wang, Liyi Chen, Yunchu Bai, Kun Fu, Zheng Wang, Hui Xiong

**Updated**: 2024-11-05T07:56:24Z

**Summary**: With the development of large language models (LLMs), the ability to handle longer contexts has become a key capability for Web applications such as cross-document understanding and LLM-powered search systems. However, this progress faces two major challenges: performance degradation due to sequence lengths out-of-distribution, and excessively long inference times caused by the quadratic computational complexity of attention. These issues hinder the application of LLMs in long-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic, training-free method for efficient and accurate long-context inference. TokenSelect builds upon the observation of non-contiguous attention sparsity, using Query-Key dot products to measure per-head KV Cache criticality at token-level. By per-head soft voting mechanism, TokenSelect selectively involves a small number of critical KV cache tokens in the attention calculation without sacrificing accuracy. To further accelerate TokenSelect, we designed the Selection Cache based on observations of consecutive Query similarity and implemented efficient dot product kernel, significantly reducing the overhead of token selection. A comprehensive evaluation of TokenSelect demonstrates up to 23.84x speedup in attention computation and up to 2.28x acceleration in end-to-end latency, while providing superior performance compared to state-of-the-art long-context inference methods.

**Link**: [arxiv](http://arxiv.org/abs/2411.02886v1),  [pdf](http://arxiv.org/pdf/2411.02886v1)

**Tags**: cs.CL cs.AI cs.LG 



### DroidSpeak: Enhancing Cross-LLM Communication
**Authors**: Yuhan Liu, Esha Choukse, Shan Lu, Junchen Jiang, Madan Musuvathi

**Updated**: 2024-11-05T05:41:41Z

**Summary**: In multi-agent systems utilizing Large Language Models (LLMs), communication between agents traditionally relies on natural language. This communication often includes the full context of the query so far, which can introduce significant prefill-phase latency, especially with long contexts.   We introduce DroidSpeak, a novel framework to target this cross-LLM communication by leveraging the reuse of intermediate data, such as input embeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the need to reprocess entire contexts for fine-tuned versions of the same foundational model. This approach allows faster context integration while maintaining the quality of task performance. Experimental evaluations demonstrate DroidSpeak's ability to significantly accelerate inter-agent communication, achieving up to a 2.78x speedup in prefill latency with negligible loss in accuracy. Our findings underscore the potential to create more efficient and scalable multi-agent systems.

**Link**: [arxiv](http://arxiv.org/abs/2411.02820v1),  [pdf](http://arxiv.org/pdf/2411.02820v1)

**Tags**: cs.MA cs.AI cs.CL cs.LG 



### Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation   With Fluidic Heating
**Authors**: Di Ni, Ved Gund, Landon Ivy, Amit Lal

**Updated**: 2024-11-04T17:21:58Z

**Summary**: Integrated micro power generators are crucial components for micro robotic platforms to demonstrate untethered operation and to achieve autonomy. Current micro robotic electrostatic actuators typically require hundreds to thousands of voltages to output sufficient work. Pyroelectricity is one such source of high voltages that can be scaled to small form factors. This paper demonstrates a distributed pyroelectric high voltage generation mechanism to power kV actuators using alternating exposure of crystals to hot and cold water (300C to 900C water temperature). Using this fluidic temperature control, a pyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage capacitor yielding a 6.10 {\mu}J stored energy. A maximum energy of 17.46 {\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can be used to heat a distributed array of converters to generate electricity in distant robotic actuator sections. The development of this distributed system would enable untethered micro-robot to be operated with a flexible body and free of battery recharging, which advances its applications in the real world.

**Link**: [arxiv](http://arxiv.org/abs/2411.02295v1),  [pdf](http://arxiv.org/pdf/2411.02295v1)

**Tags**: cs.RO cs.SY eess.SY 



### TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory   Encryption
**Authors**: Martin Unterguggenberger, Lukas Lamster, David Schrammel, Martin Schwarzl, Stefan Mangard

**Updated**: 2024-11-04T12:14:07Z

**Summary**: Efficient cloud computing relies on in-process isolation to optimize performance by running workloads within a single process. Without heavy-weight process isolation, memory safety errors pose a significant security threat by allowing an adversary to extract or corrupt the private data of other co-located tenants. Existing in-process isolation mechanisms are not suitable for modern cloud requirements, e.g., MPK's 16 protection domains are insufficient to isolate thousands of cloud workers per process. Consequently, cloud service providers have a strong need for lightweight in-process isolation on commodity x86 machines.   This paper presents TME-Box, a novel isolation technique that enables fine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing Intel TME-MK, which is intended for the encryption of virtual machines, TME-Box offers lightweight and efficient in-process isolation. TME-Box enforces that sandboxes use their designated encryption keys for memory interactions through compiler instrumentation. This cryptographic isolation enables fine-grained access control, from single cache lines to full pages, and supports flexible data relocation. In addition, the design of TME-Box allows the efficient isolation of up to 32K concurrent sandboxes. We present a performance-optimized TME-Box prototype, utilizing x86 segment-based addressing, that showcases geomean performance overheads of 5.2 % for data isolation and 9.7 % for code and data isolation, evaluated with the SPEC CPU2017 benchmark suite.

**Link**: [arxiv](http://arxiv.org/abs/2407.10740v2),  [pdf](http://arxiv.org/pdf/2407.10740v2)

**Tags**: cs.CR 



### Diversity in Network-Friendly Recommendations
**Authors**: Evangelia Tzimpimpaki, Thrasyvoulos Spyropoulos

**Updated**: 2024-11-04T09:40:27Z

**Summary**: In recent years, the Internet has been dominated by content-rich platforms, employing recommendation systems to provide users with more appealing content (e.g., videos in YouTube, movies in Netflix). While traditional content recommendations are oblivious to network conditions, the paradigm of Network-Friendly Recommendations (NFR) has recently emerged, favoring content that improves network performance (e.g. cached near the user), while still being appealing to the user. However, NFR algorithms sometimes achieve their goal by shrinking the pool of content recommended to users. The undesirable side-effect is reduced content diversity, a phenomenon known as ``content/filter bubble''. This reduced diversity is problematic for both users, who are prevented from exploring a broader range of content, and content creators (e.g. YouTubers) whose content may be recommended less frequently, leading to perceived unfairness. In this paper, we first investigate - using real data and state-of-the-art NFR schemes - the extent of this phenomenon. We then formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly recommendations with - sufficient - content diversity), and through a series of transformation steps, we manage to reduce it to a linear program that can be solved fast and optimally. Our findings show that Diverse-NFR can achieve high network gains (comparable to non-diverse NFR) while maintaining diversity constraints. To our best knowledge, this is the first work that incorporates diversity issues into network-friendly recommendation algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2411.00601v2),  [pdf](http://arxiv.org/pdf/2411.00601v2)

**Tags**: cs.PF 



### Experimental demonstration of dark current mitigation by an   over-inserted plug in a normal conducting VHF gun
**Authors**: X. -H. Wang, G. Shu, H. Qian, X. Li, Z. Liu, Z. Jiang, H. Meng, C. Xing, Q. Zhou, H. Deng

**Updated**: 2024-11-04T02:35:03Z

**Summary**: The room temperature continuous wave (CW) very-high-frequency (VHF) gun is one of the candidates for the electron gun of the high-repetition-rate free-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~ 20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission leads to beam loss along the FEL machine, therefore is a critical parameter for the performance of the CW gun. In this paper, we presents a systematic study of the dark current reduction of the VHF gun, including cathode region optimizations, dark current tracking simulations and measurements. Over-inserted cathode plugs were tested in two VHF guns of different acceleration gap sizes, and both demonstrated significant dark current reduction ratios of more than two orders of magnitude.

**Link**: [arxiv](http://arxiv.org/abs/2411.01754v1),  [pdf](http://arxiv.org/pdf/2411.01754v1)

**Tags**: physics.acc-ph 



### Palu: Compressing KV-Cache with Low-Rank Projection
**Authors**: Chi-Chih Chang, Wei-Cheng Lin, Chien-Yu Lin, Chong-Yan Chen, Yu-Fang Hu, Pei-Shuo Wang, Ning-Chi Huang, Luis Ceze, Mohamed S. Abdelfattah, Kai-Chiang Wu

**Updated**: 2024-11-04T02:08:55Z

**Summary**: Post-training KV-Cache compression methods typically either sample a subset of effectual tokens or quantize the data into lower numerical bit width. However, these methods cannot exploit redundancy in the hidden dimension of the KV tensors. This paper presents a hidden dimension compression approach called Palu, a KV-Cache compression framework that utilizes low-rank projection to reduce inference-time LLM memory usage. Palu decomposes the linear layers into low-rank matrices, caches compressed intermediate states, and reconstructs the full keys and values on the fly. To improve accuracy, compression rate, and efficiency, Palu further encompasses (1) a medium-grained low-rank decomposition scheme, (2) an efficient rank search algorithm, (3) low-rank-aware quantization compatibility enhancements, and (4) optimized GPU kernels with operators fusion. Extensive experiments with popular LLMs show that Palu compresses KV-Cache by 50% while maintaining strong accuracy and delivering up to 1.89x on the RoPE-based attention module. When combined with quantization, Palu's inherent quantization-friendly design yields small to negligible extra accuracy degradation while saving additional memory than quantization-only methods and achieving up to 2.91x speedup for the RoPE-based attention. Moreover, it maintains comparable or even better accuracy (up to 1.19 lower perplexity) compared to quantization-only methods. These results demonstrate Palu's superior capability to effectively address the efficiency and memory challenges of LLM inference posed by KV-Cache. Our code is publicly available at: https://github.com/shadowpa0327/Palu

**Link**: [arxiv](http://arxiv.org/abs/2407.21118v2),  [pdf](http://arxiv.org/pdf/2407.21118v2)

**Tags**: cs.AI cs.LG 



### A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache   Compression
**Authors**: Alessio Devoto, Yu Zhao, Simone Scardapane, Pasquale Minervini

**Updated**: 2024-11-03T09:42:35Z

**Summary**: The deployment of large language models (LLMs) is often hindered by the extensive memory requirements of the Key-Value (KV) cache, especially as context lengths increase. Existing approaches to reduce the KV cache size involve either fine-tuning the model to learn a compression strategy or leveraging attention scores to reduce the sequence length. We analyse the attention distributions in decoder-only Transformers-based models and observe that attention allocation patterns stay consistent across most layers. Surprisingly, we find a clear correlation between the $L_2$ and the attention scores over cached KV pairs, where a low $L_2$ of a key embedding usually leads to a high attention score during decoding. This finding indicates that the influence of a KV pair is potentially determined by the key embedding itself before being queried. Based on this observation, we compress the KV cache based on the $L_2$ of key embeddings. Our experimental results show that this simple strategy can reduce the KV cache size by 50% on language modelling and needle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing accuracy. Moreover, without relying on the attention scores, this approach remains compatible with FlashAttention, enabling broader applicability.

**Link**: [arxiv](http://arxiv.org/abs/2406.11430v4),  [pdf](http://arxiv.org/pdf/2406.11430v4)

**Tags**: cs.CL cs.AI 



### Two-Timescale Model Caching and Resource Allocation for Edge-Enabled   AI-Generated Content Services
**Authors**: Zhang Liu, Hongyang Du, Xiangwang Hou, Lianfen Huang, Seyyedali Hosseinalipour, Dusit Niyato, Khaled Ben Letaief

**Updated**: 2024-11-03T07:01:13Z

**Summary**: Generative AI (GenAI) has emerged as a transformative technology, enabling customized and personalized AI-generated content (AIGC) services. In this paper, we address challenges of edge-enabled AIGC service provisioning, which remain underexplored in the literature. These services require executing GenAI models with billions of parameters, posing significant obstacles to resource-limited wireless edge. We subsequently introduce the formulation of joint model caching and resource allocation for AIGC services to balance a trade-off between AIGC quality and latency metrics. We obtain mathematical relationships of these metrics with the computational resources required by GenAI models via experimentation. Afterward, we decompose the formulation into a model caching subproblem on a long-timescale and a resource allocation subproblem on a short-timescale. Since the variables to be solved are discrete and continuous, respectively, we leverage a double deep Q-network (DDQN) algorithm to solve the former subproblem and propose a diffusion-based deep deterministic policy gradient (D3PG) algorithm to solve the latter. The proposed D3PG algorithm makes an innovative use of diffusion models as the actor network to determine optimal resource allocation decisions. Consequently, we integrate these two learning methods within the overarching two-timescale deep reinforcement learning (T2DRL) algorithm, the performance of which is studied through comparative numerical simulations.

**Link**: [arxiv](http://arxiv.org/abs/2411.01458v1),  [pdf](http://arxiv.org/pdf/2411.01458v1)

**Tags**: cs.LG cs.AI cs.DC 



### Disaggregated Database Management Systems
**Authors**: Shahram Ghandeharizadeh, Philip A. Bernstein, Dhruba Borthakur, Haoyu Huang, Jai Menon, Sumit Puri

**Updated**: 2024-11-02T14:40:36Z

**Summary**: Modern applications demand high performance and cost efficient database management systems (DBMSs). Their workloads may be diverse, ranging from online transaction processing to analytics and decision support. The cloud infrastructure enables disaggregation of monolithic DBMSs into components that facilitate software-hardware co-design. This is realized using pools of hardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using high-speed networks. This disaggregation trend is being adopted by cloud DBMSs because hardware re-provisioning can be achieved by simply invoking software APIs. Disaggregated DBMSs separate processing from storage, enabling each to scale elastically and independently. They may disaggregate compute usage based on functionality, e.g., compute needed for writes from compute needed for queries and compute needed for compaction. They may also use disaggregated memory, e.g., for intermediate results in a shuffle or for remote caching. The DBMS monitors the characteristics of a workload and dynamically assembles its components that are most efficient and cost effective for the workload. This paper is a summary of a panel session that discussed the capability, challenges, and opportunities of these emerging DBMSs and disaggregated hardware systems.

**Link**: [arxiv](http://arxiv.org/abs/2411.01269v1),  [pdf](http://arxiv.org/pdf/2411.01269v1)

**Tags**: cs.DB 



### CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores
**Authors**: Shahram Ghandeharizadeh, Sandy Irani, Jenny Lam, Jason Yap

**Updated**: 2024-11-02T13:52:49Z

**Summary**: Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a general purpose key-value store (KVS) that manages key-value pairs computed by applications with different access patterns, key-value sizes, and varying costs for each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS) algorithm that can be implemented as efficiently as LRU. In particular, CAMP's eviction policies are as effective as those of GDS but require only a small fraction of the updates to an internal data structure in order to make those decisions. Similar to an implementation of LRU using queues, it adapts to changing workload patterns based on the history of requests for different key-value pairs. It is superior to LRU because it considers both the size and cost of key-value pairs to maximize the utility of the available memory across competing applications. We compare CAMP with both LRU and an alternative that requires human intervention to partition memory into pools and assign grouping of key-value pairs to different pools. The results demonstrate CAMP is as fast as LRU while outperforming both LRU and the pooled alternative. We also present results from an implementation of CAMP using Twitter's version of memcached.

**Link**: [arxiv](http://arxiv.org/abs/2411.01246v1),  [pdf](http://arxiv.org/pdf/2411.01246v1)

**Tags**: cs.DB cs.DS cs.PF 



### NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM   Inference
**Authors**: Xuanlin Jiang, Yang Zhou, Shiyi Cao, Ion Stoica, Minlan Yu

**Updated**: 2024-11-02T05:15:44Z

**Summary**: Online LLM inference powers many exciting applications such as intelligent chatbots and autonomous agents. Modern LLM inference engines widely rely on request batching to improve inference throughput, aiming to make it cost-efficient when running on expensive GPU accelerators. However, the limited GPU memory has largely limited the batch size achieved in practice, leaving significant GPU compute resources wasted.   We present NEO, an online LLM inference system that offloads part of attention compute and KV cache states from the GPU to the local host CPU, effectively increasing the GPU batch size and thus inference throughput. To this end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling to balance GPU and CPU loads and fully utilize their compute and memory resources. We evaluate NEO on a wide range of workloads (i.e., code generation, text summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B, 70B). NEO achieves up to 7.5$\times$, 26%, and 14% higher throughput compared to GPU-only approach on T4, A10G, and H100 GPUs, respectively, while maintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3% throughput gain on A10G GPU.

**Link**: [arxiv](http://arxiv.org/abs/2411.01142v1),  [pdf](http://arxiv.org/pdf/2411.01142v1)

**Tags**: cs.DC cs.AI cs.LG 



### XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference
**Authors**: João Monteiro, Étienne Marcotte, Pierre-André Noël, Valentina Zantedeschi, David Vázquez, Nicolas Chapados, Christopher Pal, Perouz Taslakian

**Updated**: 2024-11-01T14:56:52Z

**Summary**: In-context learning (ICL) approaches typically leverage prompting to condition decoder-only language model generation on reference information. Just-in-time processing of a context is inefficient due to the quadratic cost of self-attention operations, and caching is desirable. However, caching transformer states can easily require almost as much space as the model parameters. When the right context isn't known in advance, caching ICL can be challenging. This work addresses these limitations by introducing models that, inspired by the encoder-decoder architecture, use cross-attention to condition generation on reference text without the prompt. More precisely, we leverage pre-trained decoder-only models and only train a small number of added layers. We use Question-Answering (QA) as a testbed to evaluate the ability of our models to perform conditional generation and observe that they outperform ICL, are comparable to fine-tuned prompted LLMs, and drastically reduce the space footprint relative to standard KV caching by two orders of magnitude.

**Link**: [arxiv](http://arxiv.org/abs/2404.15420v3),  [pdf](http://arxiv.org/pdf/2404.15420v3)

**Tags**: cs.CL cs.AI 



### Block Transformer: Global-to-Local Language Modeling for Fast Inference
**Authors**: Namgyu Ho, Sangmin Bae, Taehyeon Kim, Hyunjik Jo, Yireun Kim, Tal Schuster, Adam Fisch, James Thorne, Se-Young Yun

**Updated**: 2024-11-01T08:52:18Z

**Summary**: We introduce the Block Transformer which adopts hierarchical global-to-local modeling to autoregressive transformers to mitigate the inference bottlenecks associated with self-attention. Self-attention requires the key-value (KV) cache of all previous sequences to be retrieved from memory at every decoding step to retrieve context information, leading to two primary bottlenecks during batch inference. First, there is a significant delay in obtaining the first token, as the information of the entire prompt must first be processed to prefill the KV cache. Second, computation of subsequent tokens is bottlenecked by the high memory I/O demand of fetching the entire KV cache, which grows linearly with sequence length, incurring quadratic memory reads overall. We design the Block Transformer to strategically mitigate these costs, by incorporating coarsity and locality into an integrated global-to-local architecture. At the lower layers, we aggregate tokens into fixed size blocks to apply attention across the entire sequence at coarse-grained detail, to capture the global context while minimizing KV cache overhead. At upper layers, we apply attention within each block to decode individual tokens, to model fine-grained details with a lightweight local KV cache. We pretrain vanilla and Block Transformers from scratch and demonstrate that Block Transformers reach 10--20x inference throughput compared to vanilla transformers with equivalent perplexity and zero-shot task performance. Code is available at https://github.com/itsnamgyu/block-transformer.

**Link**: [arxiv](http://arxiv.org/abs/2406.02657v2),  [pdf](http://arxiv.org/pdf/2406.02657v2)

**Tags**: cs.CL cs.AI cs.LG 



### Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence
**Authors**: John Whitington

**Updated**: 2024-10-31T18:31:13Z

**Summary**: We describe a hidden surface removal algorithm for two-dimensional layered scenes built from arbitrary primitives, particularly suited to interaction and animation in rich scenes (for example, in illustration). The method makes use of a set-based raster representation to implement a front-to-back rendering model which analyses and dramatically reduces the amount of rasterization and composition required to render a scene. The method is extended to add frame-to-frame coherence analysis and caching for interactive or animated scenes. A powerful system of primitive-combiners called filters is described, which preserves the efficiencies of the algorithm in highly complicated scenes. The set representation is extended to solve the problem of correlated mattes, leading to an efficient solution for high quality antialiasing. A prototype implementation has been prepared.

**Link**: [arxiv](http://arxiv.org/abs/2411.00131v1),  [pdf](http://arxiv.org/pdf/2411.00131v1)

**Tags**: cs.GR 



### Novel Architecture for Distributed Travel Data Integration and Service   Provision Using Microservices
**Authors**: Biman Barua, M. Shamim Kaiser

**Updated**: 2024-10-31T17:41:14Z

**Summary**: This paper introduces a microservices architecture for the purpose of enhancing the flexibility and performance of an airline reservation system. The architectural design incorporates Redis cache technologies, two different messaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and PostgreSQL). It also introduces authorization techniques, including secure communication through OAuth2 and JWT which is essential with the management of high-demand travel services. According to selected indicators, the architecture provides an impressive level of data consistency at 99.5% and a latency of data propagation of less than 75 ms allowing rapid and reliable intercommunication between microservices. A system throughput of 1050 events per second was achieved so that the acceptability level was maintained even during peak time. Redis caching reduced a 92% cache hit ratio on the database thereby lowering the burden on the database and increasing the speed of response. Further improvement of the systems scalability was done through the use of Docker and Kubernetes which enabled services to be expanded horizontally to cope with the changes in demand. The error rates were very low, at 0.2% further enhancing the efficiency of the system in handling real-time data integration. This approach is suggested to meet the specific needs of the airline reservation system. It is secure, fast, scalable, all serving to improve the user experience as well as the efficiency of operations. The low latency and high data integration levels and prevaiing efficient usage of the resources demonstrates the architecture ability to offer continued support in the ever growing high demand situations.

**Link**: [arxiv](http://arxiv.org/abs/2410.24174v1),  [pdf](http://arxiv.org/pdf/2410.24174v1)

**Tags**: cs.CE cs.CL cs.DC 



### MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM   Hardware
**Authors**: Sitian Chen, Amelie Chi Zhou, Yucheng Shi, Yusen Li, Xin Yao

**Updated**: 2024-10-31T10:45:02Z

**Summary**: In numerous production environments, Approximate Nearest Neighbor Search (ANNS) plays an indispensable role, particularly when dealing with massive datasets that can contain billions of entries. The necessity for rapid response times in these applications makes the efficiency of ANNS algorithms crucial. However, traditional ANNS approaches encounter substantial challenges at the billion-scale level. CPU-based methods are hindered by the limitations of memory bandwidth, while GPU-based methods struggle with memory capacity and resource utilization efficiency. This paper introduces MemANNS, an innovative framework that utilizes UPMEM PIM architecture to address the memory bottlenecks in ANNS algorithms at scale. We concentrate on optimizing a well-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques. First, we introduce an architecture-aware strategy for data placement and query scheduling that ensures an even distribution of workload across PIM chips, thereby maximizing the use of aggregated memory bandwidth. Additionally, we have developed an efficient thread scheduling mechanism that capitalizes on PIM's multi-threading capabilities and enhances memory management to boost cache efficiency. Moreover, we have recognized that real-world datasets often feature vectors with frequently co-occurring items. To address this, we propose a novel encoding method for IVFPQ that minimizes memory accesses during query processing. Our comprehensive evaluation using actual PIM hardware and real-world datasets at the billion-scale, show that MemANNS offers a significant 4.3x increase in QPS over CPU-based Faiss, and it matches the performance of GPU-based Faiss implementations. Additionally, MemANNS improves energy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU solutions.

**Link**: [arxiv](http://arxiv.org/abs/2410.23805v1),  [pdf](http://arxiv.org/pdf/2410.23805v1)

**Tags**: cs.AR 



### ALISE: Accelerating Large Language Model Serving with Speculative   Scheduling
**Authors**: Youpeng Zhao, Jun Wang

**Updated**: 2024-10-31T00:58:11Z

**Summary**: Large Language Models (LLMs) represent a revolutionary advancement in the contemporary landscape of artificial general intelligence (AGI). As exemplified by ChatGPT, LLM-based applications necessitate minimal response latency and maximal throughput for inference serving. However, due to the unpredictability of LLM execution, the first-come-first-serve (FCFS) scheduling policy employed by current LLM serving systems suffers from head-of-line (HoL) blocking issues and long job response times.   In this paper, we propose a new efficient LLM inference serving framework, named ALISE. The key design paradigm of ALISE is to leverage a novel speculative scheduler by estimating the execution time for each job and exploiting such prior knowledge to assign appropriate job priority orders, thus minimizing potential queuing delays for heterogeneous workloads. Furthermore, to mitigate the memory overhead of the intermediate key-value (KV) cache, we employ a priority-based adaptive memory management protocol and quantization-based compression techniques. Evaluations demonstrate that in comparison to the state-of-the-art solution vLLM, ALISE improves the throughput of inference serving by up to 1.8x and 2.1x under the same latency constraint on the Alpaca and ShareGPT datasets, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2410.23537v1),  [pdf](http://arxiv.org/pdf/2410.23537v1)

**Tags**: cs.PF cs.AI 



### Superposed Decoding: Multiple Generations from a Single Autoregressive   Inference Pass
**Authors**: Ethan Shen, Alan Fan, Sarah M. Pratt, Jae Sung Park, Matthew Wallingford, Sham M. Kakade, Ari Holtzman, Ranjay Krishna, Ali Farhadi, Aditya Kusupati

**Updated**: 2024-10-30T21:22:54Z

**Summary**: Many applications today provide users with multiple auto-complete drafts as they type, including GitHub's code completion, Gmail's smart compose, and Apple's messaging auto-suggestions. Under the hood, language models support this by running an autoregressive inference pass to provide a draft. Consequently, providing $k$ drafts to the user requires running an expensive language model $k$ times. To alleviate the computation cost of running $k$ inference passes, we propose Superposed Decoding, a new decoding algorithm that generates $k$ drafts at the computation cost of one autoregressive inference pass. We achieve this by feeding a superposition of the most recent token embeddings from the $k$ drafts as input to the next decoding step of the language model. At every inference step we combine the $k$ drafts with the top-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options, using an n-gram interpolation with minimal compute overhead to filter out incoherent generations. Our experiments show that $k$ drafts from Superposed Decoding are at least as coherent and factual as Nucleus Sampling and Greedy Decoding respectively, while being at least $2.44\times$ faster for $k\ge3$. In a compute-normalized setting, user evaluations demonstrably favor text generated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can also be combined with other decoding strategies, resulting in universal coverage gains when scaling inference time compute. Code and more examples open-sourced at https://github.com/RAIVNLab/SuperposedDecoding.

**Link**: [arxiv](http://arxiv.org/abs/2405.18400v6),  [pdf](http://arxiv.org/pdf/2405.18400v6)

**Tags**: cs.CL cs.LG 



### Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive   Edge Caching
**Authors**: Farnaz Niknia, Ping Wang, Zixu Wang, Aakash Agarwal, Adib S. Rezaei

**Updated**: 2024-10-30T16:06:21Z

**Summary**: This paper tackles the growing issue of excessive data transmission in networks. With increasing traffic, backhaul links and core networks are under significant traffic, leading to the investigation of caching solutions at edge routers. Many existing studies utilize Markov Decision Processes (MDP) to tackle caching problems, often assuming decision points at fixed intervals; however, real-world environments are characterized by random request arrivals. Additionally, critical file attributes such as lifetime, size, and priority significantly impact the effectiveness of caching policies, yet existing research fails to integrate all these attributes in policy design. In this work, we model the caching problem using a Semi-Markov Decision Process (SMDP) to better capture the continuous-time nature of real-world applications, enabling caching decisions to be triggered by random file requests. We then introduce a Proximal Policy Optimization (PPO)--based caching strategy that fully considers file attributes like lifetime, size, and priority. Simulations show that our method outperforms a recent Deep Reinforcement Learning-based technique. To further advance our research, we improved the convergence rate of PPO by prioritizing transitions within the replay buffer through an attention mechanism. This mechanism evaluates the similarity between the current state and all stored transitions, assigning higher priorities to transitions that exhibit greater similarity.

**Link**: [arxiv](http://arxiv.org/abs/2402.14576v3),  [pdf](http://arxiv.org/pdf/2402.14576v3)

**Tags**: cs.NI cs.LG cs.SY eess.SY 



### BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters   for Efficient LLM Inference
**Authors**: Junqi Zhao, Zhijin Fang, Shu Li, Shaohui Yang, Shichao He

**Updated**: 2024-10-30T14:53:37Z

**Summary**: Large language models (LLMs) are essential in natural language processing but often struggle with inference speed and computational efficiency, limiting real-time deployment. The key-value (KV) cache mechanism reduces computational overhead in transformer models, but challenges in maintaining contextual understanding remain. In this paper, we propose BUZZ, a novel KV caching algorithm that leverages structured contextual information to minimize cache memory usage while enhancing inference speed. BUZZ employs a beehive-structured sparse cache, incorporating a sliding window to capture recent information and dynamically segmenting historical tokens into chunks to prioritize important tokens in local neighborhoods. We evaluate BUZZ on four real-world datasets: CNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ (1) reduces cache memory usage by $\textbf{2.5}\times$ in LLM inference while maintaining over 99% accuracy in long-text summarization, and (2) surpasses state-of-the-art performance in multi-document question answering by $\textbf{7.69%}$ under the same memory limit, where full cache methods encounter out-of-memory issues. Additionally, BUZZ achieves significant inference speedup with a $\log{n}$ time complexity. The code is available at https://github.com/JunqiZhao888/buzz-llm.

**Link**: [arxiv](http://arxiv.org/abs/2410.23079v1),  [pdf](http://arxiv.org/pdf/2410.23079v1)

**Tags**: cs.CL cs.AI 



### Training-Free Exponential Context Extension via Cascading KV Cache
**Authors**: Jeffrey Willette, Heejun Lee, Youngwan Lee, Myeongjae Jeon, Sung Ju Hwang

**Updated**: 2024-10-30T03:31:09Z

**Summary**: The transformer's context window is vital for tasks such as few-shot learning and conditional generation as it preserves previous tokens for active memory. However, as the context lengths increase, the computational costs grow quadratically, hindering the deployment of large language models (LLMs) in real-world, long sequence scenarios. Although some recent key-value caching (KV Cache) methods offer linear inference complexity, they naively manage the stored context, prematurely evicting tokens and losing valuable information. Moreover, they lack an optimized prefill/prompt stage strategy, resulting in higher latency than even quadratic attention for realistic context sizes. In response, we introduce a novel mechanism that leverages cascading sub-cache buffers to selectively retain the most relevant tokens, enabling the model to maintain longer context histories without increasing the cache size. Our approach outperforms linear caching baselines across key benchmarks, including streaming perplexity, question answering, book summarization, and passkey retrieval, where it retains better retrieval accuracy at 1M tokens after four doublings of the cache size of 65K. Additionally, our method reduces prefill stage latency by a factor of 6.8 when compared to flash attention on 1M tokens. These innovations not only enhance the computational efficiency of LLMs but also pave the way for their effective deployment in resource-constrained environments, enabling large-scale, real-time applications with significantly reduced latency.

**Link**: [arxiv](http://arxiv.org/abs/2406.17808v2),  [pdf](http://arxiv.org/pdf/2406.17808v2)

**Tags**: cs.CL cs.AI cs.LG 



### Graph-GIC: A Smart and Parallelized Geomagnetically Induced Current   Modelling Algorithm Based on Graph Theory for Space Weather Applications
**Authors**: Wen Chen, Ding Yuan, Xueshang Feng, Stefaan Poedts, Zhengyang Zou, Song Feng, Yuxuan Zhu, Tong Yin

**Updated**: 2024-10-30T02:18:59Z

**Summary**: Geomagnetically Induced Current (GIC) refers to the electromagnetic response of the Earth and its conductive modern infrastructures to space weather and would pose a significant threat to high-voltage power grids designed for the alternative current operation. To assess the impact of space weather on the power grid, one needs to calculate the GIC on a national or continental scale. In this study, we developed a smart and parallelized GIC modelling algorithm, Graph GIC. This algorithm deploys a graph representing a power grid in a single-line diagram, in which substations/transformers act as nodes and transmission lines as edges. With these denotations, a power grid and its electric parameters are mathematically represented with an adjacency matrix and an admittance matrix. We used sparse matrix and parallelisation techniques to expedite the intensive computation in cases of large-scale power grids. The Graph GIC was validated with a benchmark grid, applied to the GIC calculation of the 500 kV power grid of Guangdong, China, and conducted preliminary analysis on the grid's susceptibility to geomagnetic storms. The Graph GIC algorithm has the advantage of an intuitive and highly scalable graph representation of a power grid at any scale. It achieves high-accuracy calculation and a speedup of about 18 times after parallelisation. This algorithm could be applied to assess the impact of space weather on a power grid up to continental scales and could be incorporated into global space weather modelling frameworks.

**Link**: [arxiv](http://arxiv.org/abs/2411.08043v1),  [pdf](http://arxiv.org/pdf/2411.08043v1)

**Tags**: physics.space-ph physics.geo-ph 



### VL-Cache: Sparsity and Modality-Aware KV Cache Compression for   Vision-Language Model Inference Acceleration
**Authors**: Dezhan Tu, Danylo Vashchilenko, Yuzhe Lu, Panpan Xu

**Updated**: 2024-10-29T20:04:34Z

**Summary**: Vision-Language Models (VLMs) have demonstrated impressive performance across a versatile set of tasks. A key challenge in accelerating VLMs is storing and accessing the large Key-Value (KV) cache that encodes long visual contexts, such as images or videos. While existing KV cache compression methods are effective for Large Language Models (LLMs), directly migrating them to VLMs yields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache, a novel KV cache compression recipe tailored for accelerating VLM inference. In this paper, we first investigate the unique sparsity pattern of VLM attention by distinguishing visual and text tokens in prefill and decoding phases. Based on these observations, we introduce a layer-adaptive sparsity-aware cache budget allocation method that effectively distributes the limited cache budget across different layers, further reducing KV cache size without compromising accuracy. Additionally, we develop a modality-aware token scoring policy to better evaluate the token importance. Empirical results on multiple benchmark datasets demonstrate that retaining only 10% of KV cache achieves accuracy comparable to that with full cache. In a speed benchmark, our method accelerates end-to-end latency of generating 100 tokens by up to 2.33x and speeds up decoding by up to 7.08x, while reducing the memory footprint of KV cache in GPU by 90%.

**Link**: [arxiv](http://arxiv.org/abs/2410.23317v1),  [pdf](http://arxiv.org/pdf/2410.23317v1)

**Tags**: cs.CV cs.AI cs.CL cs.DC cs.PF 



### Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs
**Authors**: Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, Jianfeng Gao

**Updated**: 2024-10-29T18:26:09Z

**Summary**: In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss. We will release our code and the compatible CUDA kernel for reproducibility.

**Link**: [arxiv](http://arxiv.org/abs/2310.01801v4),  [pdf](http://arxiv.org/pdf/2310.01801v4)

**Tags**: cs.CL 



### Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware   Neuron Management
**Authors**: Tuowei Wang, Ruwen Fan, Minxing Huang, Zixu Hao, Kun Li, Ting Cao, Youyou Lu, Yaoxue Zhang, Ju Ren

**Updated**: 2024-10-29T17:33:19Z

**Summary**: Large Language Models (LLMs) have achieved remarkable success across various domains, yet deploying them on mobile devices remains an arduous challenge due to their extensive computational and memory demands. While lightweight LLMs have been developed to fit mobile environments, they suffer from degraded model accuracy. In contrast, sparsity-based techniques minimize DRAM usage by selectively transferring only relevant neurons to DRAM while retaining the full model in external storage, such as flash. However, such approaches are critically limited by numerous I/O operations, particularly on smartphones with severe IOPS constraints.   In this paper, we propose Ripple, a novel approach that accelerates LLM inference on smartphones by optimizing neuron placement in flash memory. Ripple leverages the concept of Neuron Co-Activation, where neurons frequently activated together are linked to facilitate continuous read access and optimize data transfer efficiency. Our approach incorporates a two-stage solution: an offline stage that reorganizes neuron placement based on co-activation patterns, and an online stage that employs tailored data access and caching strategies to align well with hardware characteristics. Evaluations conducted on a variety of smartphones and LLMs demonstrate that Ripple achieves up to 5.93x improvements in I/O latency compared to the state-of-the-art. As the first solution to optimize storage placement under sparsity, Ripple explores a new optimization space at the intersection of sparsity-driven algorithm and storage-level system co-design in LLM inference.

**Link**: [arxiv](http://arxiv.org/abs/2410.19274v2),  [pdf](http://arxiv.org/pdf/2410.19274v2)

**Tags**: cs.LG cs.AI cs.OS cs.PF 



### Modeling and Monitoring of Indoor Populations using Sparse Positioning   Data (Extension)
**Authors**: Xiao Li, Huan Li, Hua Lu, Christian S. Jensen

**Updated**: 2024-10-29T16:55:23Z

**Summary**: In large venues like shopping malls and airports, knowledge on the indoor populations fuels applications such as business analytics, venue management, and safety control. In this work, we provide means of modeling populations in partitions of indoor space offline and of monitoring indoor populations continuously, by using indoor positioning data. However, the low-sampling rates of indoor positioning render the data temporally and spatially sparse, which in turn renders the offline capture of indoor populations challenging. It is even more challenging to continuously monitor indoor populations, as positioning data may be missing or not ready yet at the current moment. To address these challenges, we first enable probabilistic modeling of populations in indoor space partitions as Normal distributions. Based on that, we propose two learning-based estimators for on-the-fly prediction of population distributions. Leveraging the prediction-based schemes, we provide a unified continuous query processing framework for a type of query that enables continuous monitoring of populated partitions. The framework encompasses caching and result validity mechanisms to reduce cost and maintain monitoring effectiveness. Extensive experiments on two real data sets show that the proposed estimators are able to outperform the state-of-the-art alternatives and that the query processing framework is effective and efficient.

**Link**: [arxiv](http://arxiv.org/abs/2410.21142v2),  [pdf](http://arxiv.org/pdf/2410.21142v2)

**Tags**: cs.DB 



### ProMoE: Fast MoE-based LLM Serving using Proactive Caching
**Authors**: Xiaoniu Song, Zihang Zhong, Rong Chen

**Updated**: 2024-10-29T15:31:27Z

**Summary**: The promising applications of large language models are often constrained by the limited GPU memory capacity available on edge devices. Mixture-of-Experts (MoE) models help mitigate this issue by activating only a subset of the model's parameters during computation, allowing the unused parameters to be offloaded to host memory and reducing overall GPU memory demand. However, existing cache-based offloading solutions handle cache misses reactively and significantly impact system performance. In this paper, we propose ProMoE, a novel proactive caching system that leverages intermediate model results to predict subsequent parameter usage. By proactively fetching experts in advance, ProMoE removes the loading time from the critical path and diminishes the performance overhead of offloading. Our evaluations demonstrate that ProMoE achieves an average speedup of 2.13x and 2.84x in the prefill and decode stages respectively, compared to existing offloading solutions.

**Link**: [arxiv](http://arxiv.org/abs/2410.22134v1),  [pdf](http://arxiv.org/pdf/2410.22134v1)

**Tags**: cs.DC cs.AI 



### The Impact of Inference Acceleration Strategies on Bias of LLMs
**Authors**: Elisabeth Kirsten, Ivan Habernal, Vedant Nanda, Muhammad Bilal Zafar

**Updated**: 2024-10-29T15:19:13Z

**Summary**: Last few years have seen unprecedented advances in capabilities of Large Language Models (LLMs). These advancements promise to deeply benefit a vast array of application domains. However, due to their immense size, performing inference with LLMs is both costly and slow. Consequently, a plethora of recent work has proposed strategies to enhance inference efficiency, e.g., quantization, pruning, and caching. These acceleration strategies reduce the inference cost and latency, often by several factors, while maintaining much of the predictive performance measured via common benchmarks. In this work, we explore another critical aspect of LLM performance: demographic bias in model generations due to inference acceleration optimizations. Using a wide range of metrics, we probe bias in model outputs from a number of angles. Analysis of outputs before and after inference acceleration shows significant change in bias. Worryingly, these bias effects are complex and unpredictable. A combination of an acceleration strategy and bias type may show little bias change in one model but may lead to a large effect in another. Our results highlight a need for in-depth and case-by-case evaluation of model bias after it has been modified to accelerate inference.

**Link**: [arxiv](http://arxiv.org/abs/2410.22118v1),  [pdf](http://arxiv.org/pdf/2410.22118v1)

**Tags**: cs.CL cs.AI cs.LG 



### LoongServe: Efficiently Serving Long-Context Large Language Models with   Elastic Sequence Parallelism
**Authors**: Bingyang Wu, Shengyu Liu, Yinmin Zhong, Peng Sun, Xuanzhe Liu, Xin Jin

**Updated**: 2024-10-29T13:04:42Z

**Summary**: The context window of large language models (LLMs) is rapidly increasing, leading to a huge variance in resource usage between different requests as well as between different phases of the same request. Restricted by static parallelism strategies, existing LLM serving systems cannot efficiently utilize the underlying resources to serve variable-length requests in different phases. To address this problem, we propose a new parallelism paradigm, elastic sequence parallelism (ESP), to elastically adapt to the variance between different requests and phases. Based on ESP, we design and build LoongServe, an LLM serving system that (1) improves computation efficiency by elastically adjusting the degree of parallelism in real-time, (2) improves communication efficiency by reducing key-value cache migration overhead and overlapping partial decoding communication with computation, and (3) improves GPU memory efficiency by reducing key-value cache fragmentation across instances. Our evaluation under diverse real-world datasets shows that LoongServe improves the maximum throughput by up to 3.85$\times$ compared to the chunked prefill and 5.81$\times$ compared to the prefill-decoding disaggregation.

**Link**: [arxiv](http://arxiv.org/abs/2404.09526v2),  [pdf](http://arxiv.org/pdf/2404.09526v2)

**Tags**: cs.DC cs.LG 



### ASVD: Activation-aware Singular Value Decomposition for Compressing   Large Language Models
**Authors**: Zhihang Yuan, Yuzhang Shang, Yue Song, Qiang Wu, Yan Yan, Guangyu Sun

**Updated**: 2024-10-29T12:28:58Z

**Summary**: In this paper, we introduce a new post-training compression paradigm for Large Language Models (LLMs) to facilitate their wider adoption. We delve into LLM weight low-rank decomposition, and find that the challenges of this task stem from the distribution variance in the LLM activations and the sensitivity difference among various kinds of layers. To address these issues, we propose a training-free approach called Activation-aware Singular Value Decomposition (ASVD). Specifically, ASVD manages activation outliers by transforming the weight matrix based on the activation distribution. This transformation allows the outliers in the activation matrix to be absorbed into the transformed weight matrix, thereby enhancing decomposition accuracy. Additionally, we propose an efficient iterative calibration process to optimize layer-specific decomposition by addressing the varying sensitivity of different LLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the success of the low-rank decomposition of projection matrices in the self-attention module, we further introduce ASVD to compress the KV cache. By reducing the channel dimension of KV activations, memory requirements for KV cache can be largely reduced. ASVD can further achieve 50% KV cache reductions without performance drop in a training-free manner. Code is anonymously available in supplementary materials.

**Link**: [arxiv](http://arxiv.org/abs/2312.05821v4),  [pdf](http://arxiv.org/pdf/2312.05821v4)

**Tags**: cs.CL 



### Dynamic Content Caching with Waiting Costs via Restless Multi-Armed   Bandits
**Authors**: Ankita Koley, Chandramani Singh

**Updated**: 2024-10-29T12:03:14Z

**Summary**: We consider a system with a local cache connected to a backend server and an end user population. A set of contents are stored at the the server where they continuously get updated. The local cache keeps copies, potentially stale, of a subset of the contents. The users make content requests to the local cache which either can serve the local version if available or can fetch a fresh version or can wait for additional requests before fetching and serving a fresh version. Serving a stale version of a content incurs an age-of-version(AoV) dependent ageing cost, fetching it from the server incurs a fetching cost, and making a request wait incurs a per unit time waiting cost. We focus on the optimal actions subject to the cache capacity constraint at each decision epoch, aiming at minimizing the long term average cost. We pose the problem as a Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based policy which is known to be asymptotically optimal. We explicitly characterize the Whittle indices. We numerically evaluate the proposed policy and also compare it to a greedy policy. We show that it is close to the optimal policy and substantially outperforms the greedy policy.

**Link**: [arxiv](http://arxiv.org/abs/2410.18627v2),  [pdf](http://arxiv.org/pdf/2410.18627v2)

**Tags**: cs.NI 



### QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs
**Authors**: Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li, Pashmina Cameron, Martin Jaggi, Dan Alistarh, Torsten Hoefler, James Hensman

**Updated**: 2024-10-29T11:09:12Z

**Summary**: We introduce QuaRot, a new Quantization scheme based on Rotations, which is able to quantize LLMs end-to-end, including all weights, activations, and KV cache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the hidden state without changing the output, making quantization easier. This computational invariance is applied to the hidden state (residual) of the LLM, as well as to the activations of the feed-forward components, aspects of the attention mechanism, and to the KV cache. The result is a quantized model where all matrix multiplications are performed in 4 bits, without any channels identified for retention in higher precision. Our 4-bit quantized LLaMa2-70B model has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the zero-shot performance. We also show that QuaRot can provide lossless 6 and 8 bit LLaMa2 models without any calibration data using round-to-nearest quantization. Code is available at: https://github.com/spcl/QuaRot.

**Link**: [arxiv](http://arxiv.org/abs/2404.00456v2),  [pdf](http://arxiv.org/pdf/2404.00456v2)

**Tags**: cs.LG 



### Unleashing the Potential of the Diffusion Model in Few-shot Semantic   Segmentation
**Authors**: Muzhi Zhu, Yang Liu, Zekai Luo, Chenchen Jing, Hao Chen, Guangkai Xu, Xinlong Wang, Chunhua Shen

**Updated**: 2024-10-29T04:21:30Z

**Summary**: The Diffusion Model has not only garnered noteworthy achievements in the realm of image generation but has also demonstrated its potential as an effective pretraining method utilizing unlabeled data. Drawing from the extensive potential unveiled by the Diffusion Model in both semantic correspondence and open vocabulary segmentation, our work initiates an investigation into employing the Latent Diffusion Model for Few-shot Semantic Segmentation. Recently, inspired by the in-context learning ability of large language models, Few-shot Semantic Segmentation has evolved into In-context Segmentation tasks, morphing into a crucial element in assessing generalist segmentation models. In this context, we concentrate on Few-shot Semantic Segmentation, establishing a solid foundation for the future development of a Diffusion-based generalist model for segmentation. Our initial focus lies in understanding how to facilitate interaction between the query image and the support image, resulting in the proposal of a KV fusion method within the self-attention framework. Subsequently, we delve deeper into optimizing the infusion of information from the support mask and simultaneously re-evaluating how to provide reasonable supervision from the query mask. Based on our analysis, we establish a simple and effective framework named DiffewS, maximally retaining the original Latent Diffusion Model's generative framework and effectively utilizing the pre-training prior. Experimental results demonstrate that our method significantly outperforms the previous SOTA models in multiple settings.

**Link**: [arxiv](http://arxiv.org/abs/2410.02369v3),  [pdf](http://arxiv.org/pdf/2410.02369v3)

**Tags**: cs.CV 



## Keyword: LLM Inference 
 ### Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large   Language Models
**Authors**: Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, Ziwei Liu

**Updated**: 2024-11-21T18:59:55Z

**Summary**: Large Language Models (LLMs) demonstrate enhanced capabilities and reliability by reasoning more, evolving from Chain-of-Thought prompting to product-level solutions like OpenAI o1. Despite various efforts to improve LLM reasoning, high-quality long-chain reasoning data and optimized training pipelines still remain inadequately explored in vision-language tasks. In this paper, we present Insight-V, an early effort to 1) scalably produce long and robust reasoning data for complex multi-modal tasks, and 2) an effective training pipeline to enhance the reasoning capabilities of multi-modal large language models (MLLMs). Specifically, to create long and structured reasoning data without human labor, we design a two-step pipeline with a progressive strategy to generate sufficiently long and diverse reasoning paths and a multi-granularity assessment method to ensure data quality. We observe that directly supervising MLLMs with such long and complex reasoning data will not yield ideal reasoning ability. To tackle this problem, we design a multi-agent system consisting of a reasoning agent dedicated to performing long-chain reasoning and a summary agent trained to judge and summarize reasoning results. We further incorporate an iterative DPO algorithm to enhance the reasoning agent's generation stability and quality. Based on the popular LLaVA-NeXT model and our stronger base MLLM, we demonstrate significant performance gains across challenging multi-modal benchmarks requiring visual reasoning. Benefiting from our multi-agent system, Insight-V can also easily maintain or improve performance on perception-focused multi-modal tasks.

**Link**: [arxiv](http://arxiv.org/abs/2411.14432v1),  [pdf](http://arxiv.org/pdf/2411.14432v1)

**Tags**: cs.CV 



### Unleashing the Potential of Multi-modal Foundation Models and Video   Diffusion for 4D Dynamic Physical Scene Simulation
**Authors**: Zhuoman Liu, Weicai Ye, Yan Luximon, Pengfei Wan, Di Zhang

**Updated**: 2024-11-21T18:55:23Z

**Summary**: Realistic simulation of dynamic scenes requires accurately capturing diverse material properties and modeling complex object interactions grounded in physical principles. However, existing methods are constrained to basic material types with limited predictable parameters, making them insufficient to represent the complexity of real-world materials. We introduce a novel approach that leverages multi-modal foundation models and video diffusion to achieve enhanced 4D dynamic scene simulation. Our method utilizes multi-modal models to identify material types and initialize material parameters through image queries, while simultaneously inferring 3D Gaussian splats for detailed scene representation. We further refine these material parameters using video diffusion with a differentiable Material Point Method (MPM) and optical flow guidance rather than render loss or Score Distillation Sampling (SDS) loss. This integrated framework enables accurate prediction and realistic simulation of dynamic interactions in real-world scenarios, advancing both accuracy and flexibility in physics-based simulations.

**Link**: [arxiv](http://arxiv.org/abs/2411.14423v1),  [pdf](http://arxiv.org/pdf/2411.14423v1)

**Tags**: cs.CV 



### Coarse Correspondences Boost Spatial-Temporal Reasoning in Multimodal   Language Model
**Authors**: Benlin Liu, Yuhao Dong, Yiqin Wang, Zixian Ma, Yansong Tang, Luming Tang, Yongming Rao, Wei-Chiu Ma, Ranjay Krishna

**Updated**: 2024-11-21T18:52:31Z

**Summary**: Multimodal language models (MLLMs) are increasingly being applied in real-world environments, necessitating their ability to interpret 3D spaces and comprehend temporal dynamics. Current methods often rely on specialized architectural designs or task-specific fine-tuning to achieve this. We introduce Coarse Correspondences, a simple lightweight method that enhances MLLMs' spatial-temporal reasoning with 2D images as input, without modifying the architecture or requiring task-specific fine-tuning. Our method uses a lightweight tracking model to identify primary object correspondences between frames in a video or across different image viewpoints, and then conveys this information to MLLMs through visual prompting. We demonstrate that this simple training-free approach brings substantial gains to GPT4-V/O consistently on four benchmarks that require spatial-temporal reasoning, including +20.5\% improvement on ScanQA, +9.7\% on OpenEQA's episodic memory subset, +6.0\% on the long-form video benchmark EgoSchema, and +11\% on the R2R navigation benchmark. Additionally, we show that Coarse Correspondences can also enhance open-source MLLMs' spatial reasoning (by +6.9\% on ScanQA) when applied in both training and inference and that the improvement can generalize to unseen datasets such as SQA3D (+3.1\%). Taken together, we show that Coarse Correspondences effectively and efficiently boosts models' performance on downstream tasks requiring spatial-temporal reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2408.00754v2),  [pdf](http://arxiv.org/pdf/2408.00754v2)

**Tags**: cs.CV cs.LG 



### Combining summary statistics with simulation-based inference for the 21   cm signal from the Epoch of Reionization
**Authors**: Benoit Semelin, Romain Mériot, Ashutosh Mishra, David Cornu

**Updated**: 2024-11-21T18:52:29Z

**Summary**: The 21 cm signal from the Epoch of Reionization will be observed with the up-coming Square Kilometer Array (SKA). SKA should yield a full tomography of the signal which opens the possibility to explore its non-Gaussian properties. How can we extract the maximum information from the tomography and derive the tightest constraint on the signal? In this work, instead of looking for the most informative summary statistics, we investigate how to combine the information from two sets of summary statistics using simulation-based inference. To this purpose, we train Neural Density Estimators (NDE) to fit the implicit likelihood of our model, the LICORICE code, using the Loreli II database. We train three different NDEs: one to perform Bayesian inference on the power spectrum, one to do it on the linear moments of the Pixel Distribution Function (PDF) and one to work with the combination of the two. We perform $\sim 900$ inferences at different points in our parameter space and use them to assess both the validity of our posteriors with Simulation-based Calibration (SBC) and the typical gain obtained by combining summary statistics. We find that our posteriors are biased by no more than $\sim 20 \%$ of their standard deviation and under-confident by no more than $\sim 15 \%$. Then, we establish that combining summary statistics produces a contraction of the 4-D volume of the posterior (derived from the generalized variance) in 91.5 % of our cases, and in 70 to 80 % of the cases for the marginalized 1-D posteriors. The median volume variation is a contraction of a factor of a few for the 4D posteriors and a contraction of 20 to 30 % in the case of the marginalized 1D posteriors. This shows that our approach is a possible alternative to looking for sufficient statistics in the theoretical sense.

**Link**: [arxiv](http://arxiv.org/abs/2411.14419v1),  [pdf](http://arxiv.org/pdf/2411.14419v1)

**Tags**: astro-ph.CO 



### Lightweight Safety Guardrails Using Fine-tuned BERT Embeddings
**Authors**: Aaron Zheng, Mansi Rana, Andreas Stolcke

**Updated**: 2024-11-21T18:27:25Z

**Summary**: With the recent proliferation of large language models (LLMs), enterprises have been able to rapidly develop proof-of-concepts and prototypes. As a result, there is a growing need to implement robust guardrails that monitor, quantize and control an LLM's behavior, ensuring that the use is reliable, safe, accurate and also aligned with the users' expectations. Previous approaches for filtering out inappropriate user prompts or system outputs, such as LlamaGuard and OpenAI's MOD API, have achieved significant success by fine-tuning existing LLMs. However, using fine-tuned LLMs as guardrails introduces increased latency and higher maintenance costs, which may not be practical or scalable for cost-efficient deployments. We take a different approach, focusing on fine-tuning a lightweight architecture: Sentence-BERT. This method reduces the model size from LlamaGuard's 7 billion parameters to approximately 67 million, while maintaining comparable performance on the AEGIS safety benchmark.

**Link**: [arxiv](http://arxiv.org/abs/2411.14398v1),  [pdf](http://arxiv.org/pdf/2411.14398v1)

**Tags**: cs.CL 



### Convolutional Vision Transformer for Cosmology Parameter Inference
**Authors**: Yash Gondhalekar, Kana Moriwaki

**Updated**: 2024-11-21T18:25:18Z

**Summary**: Parameter inference is a crucial task in modern cosmology that requires accurate and fast computational methods to handle the high precision and volume of observational datasets. In this study, we explore a hybrid vision transformer, the Convolution vision Transformer (CvT), which combines the benefits of vision transformers and convolutional neural networks. We use this approach to infer the $\Omega_m$ and $\sigma_8$ cosmological parameters from simulated dark matter and halo fields. Our experiments indicate that the constraints on $\Omega_m$ and $\sigma_8$ obtained using CvT are better than the traditional vision transformer (ViT) and CNN, using either dark matter or halo fields. For CvT, pretraining on dark matter fields proves advantageous for improving constraints using halo fields compared to training a model from the beginning. However, ViT and CNN do not show these benefits. The CvT is more efficient than ViT since, despite having more parameters, it requires a training time similar to that of ViT and has similar inference times. The code is available at \url{https://github.com/Yash-10/cvt-cosmo-inference/}.

**Link**: [arxiv](http://arxiv.org/abs/2411.14392v1),  [pdf](http://arxiv.org/pdf/2411.14392v1)

**Tags**: astro-ph.IM 



### Learning Humanoid Locomotion with Perceptive Internal Model
**Authors**: Junfeng Long, Junli Ren, Moji Shi, Zirui Wang, Tao Huang, Ping Luo, Jiangmiao Pang

**Updated**: 2024-11-21T18:21:59Z

**Summary**: In contrast to quadruped robots that can navigate diverse terrains using a "blind" policy, humanoid robots require accurate perception for stable locomotion due to their high degrees of freedom and inherently unstable morphology. However, incorporating perceptual signals often introduces additional disturbances to the system, potentially reducing its robustness, generalizability, and efficiency. This paper presents the Perceptive Internal Model (PIM), which relies on onboard, continuously updated elevation maps centered around the robot to perceive its surroundings. We train the policy using ground-truth obstacle heights surrounding the robot in simulation, optimizing it based on the Hybrid Internal Model (HIM), and perform inference with heights sampled from the constructed elevation map. Unlike previous methods that directly encode depth maps or raw point clouds, our approach allows the robot to perceive the terrain beneath its feet clearly and is less affected by camera movement or noise. Furthermore, since depth map rendering is not required in simulation, our method introduces minimal additional computational costs and can train the policy in 3 hours on an RTX 4090 GPU. We verify the effectiveness of our method across various humanoid robots, various indoor and outdoor terrains, stairs, and various sensor configurations. Our method can enable a humanoid robot to continuously climb stairs and has the potential to serve as a foundational algorithm for the development of future humanoid control methods.

**Link**: [arxiv](http://arxiv.org/abs/2411.14386v1),  [pdf](http://arxiv.org/pdf/2411.14386v1)

**Tags**: cs.RO 



### Exchangeability, prediction and predictive modeling in Bayesian   statistics
**Authors**: Sandra Fortini, Sonia Petrone

**Updated**: 2024-11-21T18:19:21Z

**Summary**: There is currently a renewed interest in the Bayesian predictive approach to statistics. This paper offers a review on foundational concepts and focuses on predictive modeling, which by directly reasoning on prediction, bypasses inferential models or may characterize them. We detail predictive characterizations in exchangeable and partially exchangeable settings, for a large variety of data structures, and hint at new directions. The underlying concept is that Bayesian predictive rules are probabilistic learning rules, formalizing through conditional probability how we learn on future events given the available information. This concept has implications in any statistical problem and in inference, from classic contexts to less explored challenges, such as providing Bayesian uncertainty quantification to predictive algorithms in data science, as we show in the last part of the paper. The paper gives a historical overview, but also includes a few new results, presents some recent developments and poses some open questions.

**Link**: [arxiv](http://arxiv.org/abs/2402.10126v2),  [pdf](http://arxiv.org/pdf/2402.10126v2)

**Tags**: math.ST stat.TH 



### Localizing Events in Videos with Multimodal Queries
**Authors**: Gengyuan Zhang, Mang Ling Ada Fok, Jialu Ma, Yan Xia, Daniel Cremers, Philip Torr, Volker Tresp, Jindong Gu

**Updated**: 2024-11-21T17:58:55Z

**Summary**: Localizing events in videos based on semantic queries is a pivotal task in video understanding, with the growing significance of user-oriented applications like video search. Yet, current research predominantly relies on natural language queries (NLQs), overlooking the potential of using multimodal queries (MQs) that integrate images to more flexibly represent semantic queries -- especially when it is difficult to express non-verbal or unfamiliar concepts in words. To bridge this gap, we introduce ICQ, a new benchmark designed for localizing events in videos with MQs, alongside an evaluation dataset ICQ-Highlight. To accommodate and evaluate existing video localization models for this new task, we propose 3 Multimodal Query Adaptation methods and a novel Surrogate Fine-tuning on pseudo-MQs strategy. ICQ systematically benchmarks 12 state-of-the-art backbone models, spanning from specialized video localization models to Video LLMs, across diverse application domains. Our experiments highlight the high potential of MQs in real-world applications. We believe this benchmark is a first step toward advancing MQs in video event localization.

**Link**: [arxiv](http://arxiv.org/abs/2406.10079v3),  [pdf](http://arxiv.org/pdf/2406.10079v3)

**Tags**: cs.CV cs.AI 



### LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with   LLM Token Embeddings
**Authors**: Duo Wang, Yuan Zuo, Fengzhi Li, Junjie Wu

**Updated**: 2024-11-21T17:48:25Z

**Summary**: Zero-shot graph machine learning, especially with graph neural networks (GNNs), has garnered significant interest due to the challenge of scarce labeled data. While methods like self-supervised learning and graph prompt learning have been extensively explored, they often rely on fine-tuning with task-specific labels, limiting their effectiveness in zero-shot scenarios. Inspired by the zero-shot capabilities of instruction-fine-tuned large language models (LLMs), we introduce a novel framework named Token Embedding-Aligned Graph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and cross-task zero-shot learners for graph machine learning. Concretely, we pretrain a GNN, aligning its representations with token embeddings of an LLM. We then train a linear projector that transforms the GNN's representations into a fixed number of graph token embeddings without tuning the LLM. A unified instruction is designed for various graph tasks at different levels, such as node classification (node-level) and link prediction (edge-level). These design choices collectively enhance our method's effectiveness in zero-shot learning, setting it apart from existing methods. Experiments show that our graph token embeddings help the LLM predictor achieve state-of-the-art performance on unseen datasets and tasks compared to other methods using LLMs as predictors.

**Link**: [arxiv](http://arxiv.org/abs/2408.14512v2),  [pdf](http://arxiv.org/pdf/2408.14512v2)

**Tags**: cs.LG cs.AI cs.CL 



### Indiscriminate Disruption of Conditional Inference on Multivariate   Gaussians
**Authors**: William N. Caballero, Matthew LaRosa, Alexander Fisher, Vahid Tarokh

**Updated**: 2024-11-21T17:46:55Z

**Summary**: The multivariate Gaussian distribution underpins myriad operations-research, decision-analytic, and machine-learning models (e.g., Bayesian optimization, Gaussian influence diagrams, and variational autoencoders). However, despite recent advances in adversarial machine learning (AML), inference for Gaussian models in the presence of an adversary is notably understudied. Therefore, we consider a self-interested attacker who wishes to disrupt a decisionmaker's conditional inference and subsequent actions by corrupting a set of evidentiary variables. To avoid detection, the attacker also desires the attack to appear plausible wherein plausibility is determined by the density of the corrupted evidence. We consider white- and grey-box settings such that the attacker has complete and incomplete knowledge about the decisionmaker's underlying multivariate Gaussian distribution, respectively. Select instances are shown to reduce to quadratic and stochastic quadratic programs, and structural properties are derived to inform solution methods. We assess the impact and efficacy of these attacks in three examples, including, real estate evaluation, interest rate estimation and signals processing. Each example leverages an alternative underlying model, thereby highlighting the attacks' broad applicability. Through these applications, we also juxtapose the behavior of the white- and grey-box attacks to understand how uncertainty and structure affect attacker behavior.

**Link**: [arxiv](http://arxiv.org/abs/2411.14351v1),  [pdf](http://arxiv.org/pdf/2411.14351v1)

**Tags**: stat.ML cs.CR cs.LG stat.AP 



### UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptation of LLMs   on Low-Resource Languages
**Authors**: Bethel Melesse Tessema, Akhil Kedia, Tae-Sun Chung

**Updated**: 2024-11-21T17:41:08Z

**Summary**: Large language models (LLMs) under-perform on low-resource languages due to limited training data. We present a method to efficiently collect text data for low-resource languages from the entire Common Crawl corpus. Our approach, UnifiedCrawl, filters and extracts common crawl using minimal compute resources, yielding mono-lingual datasets much larger than previously available sources. We demonstrate that leveraging this data to fine-tuning multilingual LLMs via efficient adapter methods (QLoRA) significantly boosts performance on the low-resource language, while minimizing VRAM usage. Our experiments show large improvements in language modeling perplexity and an increase in few-shot prompting scores. Our work and released source code provide an affordable approach to improve LLMs for low-resource languages using consumer hardware. Our source code is available here at https://github.com/bethelmelesse/unifiedcrawl.

**Link**: [arxiv](http://arxiv.org/abs/2411.14343v1),  [pdf](http://arxiv.org/pdf/2411.14343v1)

**Tags**: cs.CL cs.AI 68T50 I.2.7 



### Logarithmic Neyman Regret for Adaptive Estimation of the Average   Treatment Effect
**Authors**: Ojash Neopane, Aaditya Ramdas, Aarti Singh

**Updated**: 2024-11-21T17:38:49Z

**Summary**: Estimation of the Average Treatment Effect (ATE) is a core problem in causal inference with strong connections to Off-Policy Evaluation in Reinforcement Learning. This paper considers the problem of adaptively selecting the treatment allocation probability in order to improve estimation of the ATE. The majority of prior work on adaptive ATE estimation focus on asymptotic guarantees, and in turn overlooks important practical considerations such as the difficulty of learning the optimal treatment allocation as well as hyper-parameter selection. Existing non-asymptotic methods are limited by poor empirical performance and exponential scaling of the Neyman regret with respect to problem parameters. In order to address these gaps, we propose and analyze the Clipped Second Moment Tracking (ClipSMT) algorithm, a variant of an existing algorithm with strong asymptotic optimality guarantees, and provide finite sample bounds on its Neyman regret. Our analysis shows that ClipSMT achieves exponential improvements in Neyman regret on two fronts: improving the dependence on $T$ from $O(\sqrt{T})$ to $O(\log T)$, as well as reducing the exponential dependence on problem parameters to a polynomial dependence. Finally, we conclude with simulations which show the marked improvement of ClipSMT over existing approaches.

**Link**: [arxiv](http://arxiv.org/abs/2411.14341v1),  [pdf](http://arxiv.org/pdf/2411.14341v1)

**Tags**: stat.ML cs.LG 



### On the Morphology of Relativistically Broadened Line Emission from   Axisymmetric Equatorial Accretion Disks
**Authors**: Delilah E. A. Gates, Chau Truong, Amrita Sahu, Alejandro Cárdenas-Avendaño

**Updated**: 2024-11-21T17:34:25Z

**Summary**: Single-frequency emission from an accretion disk around a black hole is broadened into a line profile due to gravitational redshift and the motion of the disk's particles relative to the observer. The ensemble of relativistically broadened emission frequencies from the disk elements forms the spectrum viewed by an observer. Over the past decades, the broadened spectra of accreting systems have been used to constrain the spin of the black hole, the observer's inclination, and the astrophysical model parameters of the system. These inferences are usually made under the assumption that the accretion disk consists of particles orbiting around the black hole on stable circular orbits in the equatorial plane. Under this Standard disk model, in this work, we revisit line profile morphology, i.e., its extent, kinks, and fall-off. We provide a unified analytical explanation for these line profile morphological features, which encode the black hole spin, viewing inclination, and locations of the disk's inner and outer edges. We then show that these features, however, are model-dependent, by parametrically relaxing some of the astrophysical assumptions. In particular, we explore how allowing the disk particles to deviate from stable circular orbits rapidly degenerates the characteristic features of the line profile under the Standard disk model. Our results further demonstrate how sensitive our understanding of black hole and system properties can be to assumptions we make when interpreting these types of measurements.

**Link**: [arxiv](http://arxiv.org/abs/2411.14338v1),  [pdf](http://arxiv.org/pdf/2411.14338v1)

**Tags**: astro-ph.HE gr-qc 



### Hamiltonian Monte Carlo Inference of Marginalized Linear Mixed-Effects   Models
**Authors**: Jinlin Lai, Justin Domke, Daniel Sheldon

**Updated**: 2024-11-21T17:33:18Z

**Summary**: Bayesian reasoning in linear mixed-effects models (LMMs) is challenging and often requires advanced sampling techniques like Markov chain Monte Carlo (MCMC). A common approach is to write the model in a probabilistic programming language and then sample via Hamiltonian Monte Carlo (HMC). However, there are many ways a user can transform a model that make inference more or less efficient. In particular, marginalizing some variables can greatly improve inference but is difficult for users to do manually. We develop an algorithm to easily marginalize random effects in LMMs. A naive approach introduces cubic time operations within an inference algorithm like HMC, but we reduce the running time to linear using fast linear algebra techniques. We show that marginalization is always beneficial when applicable and highlight improvements in various models, especially ones from cognitive sciences.

**Link**: [arxiv](http://arxiv.org/abs/2410.24079v2),  [pdf](http://arxiv.org/pdf/2410.24079v2)

**Tags**: cs.LG stat.ML 



### Finding the root in random nearest neighbor trees
**Authors**: Anna Brandenberger, Cassandra Marcussen, Elchanan Mossel, Madhu Sudan

**Updated**: 2024-11-21T17:30:49Z

**Summary**: We study the inference of network archaeology in growing random geometric graphs. We consider the root finding problem for a random nearest neighbor tree in dimension $d \in \mathbb{N}$, generated by sequentially embedding vertices uniformly at random in the $d$-dimensional torus and connecting each new vertex to the nearest existing vertex. More precisely, given an error parameter $\varepsilon > 0$ and the unlabeled tree, we want to efficiently find a small set of candidate vertices, such that the root is included in this set with probability at least $1 - \varepsilon$. We call such a candidate set a $\textit{confidence set}$. We define several variations of the root finding problem in geometric settings -- embedded, metric, and graph root finding -- which differ based on the nature of the type of metric information provided in addition to the graph structure (torus embedding, edge lengths, or no additional information, respectively).   We show that there exist efficient root finding algorithms for embedded and metric root finding. For embedded root finding, we derive upper and lower bounds (uniformly bounded in $n$) on the size of the confidence set: the upper bound is subpolynomial in $1/\varepsilon$ and stems from an explicit efficient algorithm, and the information-theoretic lower bound is polylogarithmic in $1/\varepsilon$. In particular, in $d=1$, we obtain matching upper and lower bounds for a confidence set of size $\Theta\left(\frac{\log(1/\varepsilon)}{\log \log(1/\varepsilon)} \right)$.

**Link**: [arxiv](http://arxiv.org/abs/2411.14336v1),  [pdf](http://arxiv.org/pdf/2411.14336v1)

**Tags**: math.PR cs.DS cs.SI 



### Automated Generation of Code Debugging Exercises
**Authors**: Victor-Alexandru Pădurean, Paul Denny, Adish Singla

**Updated**: 2024-11-21T16:56:33Z

**Summary**: Debugging is an essential skill when learning to program, yet its instruction and emphasis often vary widely across introductory courses. In the era of code-generating large language models (LLMs), the ability for students to reason about code and identify errors is increasingly important. However, students frequently resort to trial-and-error methods to resolve bugs without fully understanding the underlying issues. Developing the ability to identify and hypothesize the cause of bugs is crucial but can be time-consuming to teach effectively through traditional means. This paper introduces BugSpotter, an innovative tool that leverages an LLM to generate buggy code from a problem description and verify the synthesized bugs via a test suite. Students interact with BugSpotter by designing failing test cases, where the buggy code's output differs from the expected result as defined by the problem specification. This not only provides opportunities for students to enhance their debugging skills, but also to practice reading and understanding problem specifications. We deployed BugSpotter in a large classroom setting and compared the debugging exercises it generated to exercises hand-crafted by an instructor for the same problems. We found that the LLM-generated exercises produced by BugSpotter varied in difficulty and were well-matched to the problem specifications. Importantly, the LLM-generated exercises were comparable to those manually created by instructors with respect to student performance, suggesting that BugSpotter could be an effective and efficient aid for learning debugging.

**Link**: [arxiv](http://arxiv.org/abs/2411.14303v1),  [pdf](http://arxiv.org/pdf/2411.14303v1)

**Tags**: cs.SE cs.AI 



### Auto-SPICE: Leveraging LLMs for Dataset Creation via Automated SPICE   Netlist Extraction from Analog Circuit Diagrams
**Authors**: Jitendra Bhandari, Vineet Bhat, Yuheng He, Siddharth Garg, Hamed Rahmani, Ramesh Karri

**Updated**: 2024-11-21T16:50:11Z

**Summary**: Auto-SPICE is the first fully automated framework leveraging large language models (LLMs) to generate Simulation Programs with Integrated Circuit Emphasis (SPICE) netlists. It addresses a long-standing challenge in automating netlist generation for analog circuits within circuit design automation. Automating this workflow could accelerate the creation of finetuned LLMs for analog circuit design and verification. We identify key challenges in this automation and evaluate the multi-modal capabilities of state-of-the-art LLMs, particularly GPT-4, to address these issues. We propose a three-step workflow to overcome current limitations: labeling analog circuits, prompt tuning, and netlist verification. This approach aims to create an end-to-end SPICE netlist generator from circuit schematic images, tackling the long-standing hurdle of accurate netlist generation. Our framework demonstrates significant performance improvements, tested on approximately 2,100 schematics of varying complexity. We open-source this solution for community-driven development.

**Link**: [arxiv](http://arxiv.org/abs/2411.14299v1),  [pdf](http://arxiv.org/pdf/2411.14299v1)

**Tags**: cs.AR 



### LLMSteer: Improving Long-Context LLM Inference by Steering Attention on   Reused Contexts
**Authors**: Zhuohan Gu, Jiayi Yao, Kuntai Du, Junchen Jiang

**Updated**: 2024-11-21T16:49:51Z

**Summary**: As large language models (LLMs) show impressive performance on complex tasks, they still struggle with longer contextual understanding and high computational costs. To balance efficiency and quality, we introduce LLMSteer, a fine-tuning-free framework that enhances LLMs through query-independent attention steering. Tested on popular LLMs and datasets, LLMSteer narrows the performance gap with baselines by 65.9% and reduces the runtime delay by up to 4.8x compared to recent attention steering methods.

**Link**: [arxiv](http://arxiv.org/abs/2411.13009v2),  [pdf](http://arxiv.org/pdf/2411.13009v2)

**Tags**: cs.LG cs.CL 



### A Calculus for Scoped Effects & Handlers
**Authors**: Roger Bosman, Birthe van den Berg, Wenhao Tang, Tom Schrijvers

**Updated**: 2024-11-21T16:45:56Z

**Summary**: Algebraic effects & handlers have become a standard approach for side-effects in functional programming. Their modular composition with other effects and clean separation of syntax and semantics make them attractive to a wide audience. However, not all effects can be classified as algebraic; some need a more sophisticated handling. In particular, effects that have or create a delimited scope need special care, as their continuation consists of two parts-in and out of the scope-and their modular composition introduces additional complexity. These effects are called scoped and have gained attention by their growing applicability and adoption in popular libraries. While calculi have been designed with algebraic effects & handlers built in to facilitate their use, a calculus that supports scoped effects & handlers in a similar manner does not yet exist. This work fills this gap: we present $\lambda_{\mathit{sc}}$, a calculus with native support for both algebraic and scoped effects & handlers. It addresses the need for polymorphic handlers and explicit clauses for forwarding unknown scoped operations to other handlers. Our calculus is based on Eff, an existing calculus for algebraic effects, extended with Koka-style row polymorphism, and consists of a formal grammar, operational semantics, a (type-safe) type-and-effect system and type inference. We demonstrate $\lambda_{\mathit{sc}}$ on a range of examples.

**Link**: [arxiv](http://arxiv.org/abs/2304.09697v7),  [pdf](http://arxiv.org/pdf/2304.09697v7)

**Tags**: cs.PL 



### AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context
**Authors**: Naba Rizvi, Harper Strickland, Daniel Gitelman, Tristan Cooper, Alexis Morales-Flores, Michael Golden, Aekta Kallepalli, Akshat Alurkar, Haaset Owens, Saleha Ahmedi, Isha Khirwadkar, Imani Munyaka, Nedjma Ousidhoum

**Updated**: 2024-11-21T16:43:06Z

**Summary**: As our understanding of autism and ableism continues to increase, so does our understanding of ableist language towards autistic people. Such language poses a significant challenge in NLP research due to its subtle and context-dependent nature. Yet, detecting anti-autistic ableist language remains underexplored, with existing NLP tools often failing to capture its nuanced expressions. We present AUTALIC, the first benchmark dataset dedicated to the detection of anti-autistic ableist language in context, addressing a significant gap in the field. The dataset comprises 2,400 autism-related sentences collected from Reddit, accompanied by surrounding context, and is annotated by trained experts with backgrounds in neurodiversity. Our comprehensive evaluation reveals that current language models, including state-of-the-art LLMs, struggle to reliably identify anti-autistic ableism and align with human judgments, underscoring their limitations in this domain. We publicly release AUTALIC along with the individual annotations which serve as a valuable resource to researchers working on ableism, neurodiversity, and also studying disagreements in annotation tasks. This dataset serves as a crucial step towards developing more inclusive and context-aware NLP systems that better reflect diverse perspectives.

**Link**: [arxiv](http://arxiv.org/abs/2410.16520v2),  [pdf](http://arxiv.org/pdf/2410.16520v2)

**Tags**: cs.CL cs.AI 



### White-box Membership Inference Attacks against Diffusion Models
**Authors**: Yan Pang, Tianhao Wang, Xuhui Kang, Mengdi Huai, Yang Zhang

**Updated**: 2024-11-21T16:42:56Z

**Summary**: Diffusion models have begun to overshadow GANs and other generative models in industrial applications due to their superior image generation performance. The complex architecture of these models furnishes an extensive array of attack features. In light of this, we aim to design membership inference attacks (MIAs) catered to diffusion models. We first conduct an exhaustive analysis of existing MIAs on diffusion models, taking into account factors such as black-box/white-box models and the selection of attack features. We found that white-box attacks are highly applicable in real-world scenarios, and the most effective attacks presently are white-box. Departing from earlier research, which employs model loss as the attack feature for white-box MIAs, we employ model gradients in our attack, leveraging the fact that these gradients provide a more profound understanding of model responses to various samples. We subject these models to rigorous testing across a range of parameters, including training steps, sampling frequency, diffusion steps, and data variance. Across all experimental settings, our method consistently demonstrated near-flawless attack performance, with attack success rate approaching 100% and attack AUCROC near 1.0. We also evaluate our attack against common defense mechanisms, and observe our attacks continue to exhibit commendable performance.

**Link**: [arxiv](http://arxiv.org/abs/2308.06405v3),  [pdf](http://arxiv.org/pdf/2308.06405v3)

**Tags**: cs.CR 



### Hypothesis testing of symmetry in quantum dynamics
**Authors**: Yu-Ao Chen, Chenghong Zhu, Keming He, Yingjian Liu, Xin Wang

**Updated**: 2024-11-21T16:39:35Z

**Summary**: Symmetry plays a crucial role in quantum physics, dictating the behavior and dynamics of physical systems. In this paper, We develop a hypothesis-testing framework for quantum dynamics symmetry using a limited number of queries to the unknown unitary operation and establish the quantum max-relative entropy lower bound for the type-II error. We construct optimal ancilla-free protocols that achieve optimal type-II error probability for testing time-reversal symmetry (T-symmetry) and diagonal symmetry (Z-symmetry) with limited queries. Contrasting with the advantages of indefinite causal order strategies in various quantum information processing tasks, we show that parallel, adaptive, and indefinite causal order strategies have equal power for our tasks. We establish optimal protocols for T-symmetry testing and Z-symmetry testing for 6 and 5 queries, respectively, from which we infer that the type-II error exhibits a decay rate of $\mathcal{O}(m^{-2})$ with respect to the number of queries $m$. This represents a significant improvement over the basic repetition protocols without using global entanglement, where the error decays at a slower rate of $\mathcal{O}(m^{-1})$.

**Link**: [arxiv](http://arxiv.org/abs/2411.14292v1),  [pdf](http://arxiv.org/pdf/2411.14292v1)

**Tags**: quant-ph cs.IT math.IT 



### Stochastic interventions, sensitivity analysis, and optimal transport
**Authors**: Alexander W. Levis, Edward H. Kennedy, Alec McClean, Sivaraman Balakrishnan, Larry Wasserman

**Updated**: 2024-11-21T16:35:03Z

**Summary**: Recent methodological research in causal inference has focused on effects of stochastic interventions, which assign treatment randomly, often according to subject-specific covariates. In this work, we demonstrate that the usual notion of stochastic interventions have a surprising property: when there is unmeasured confounding, bounds on their effects do not collapse when the policy approaches the observational regime. As an alternative, we propose to study generalized policies, treatment rules that can depend on covariates, the natural value of treatment, and auxiliary randomness. We show that certain generalized policy formulations can resolve the "non-collapsing" bound issue: bounds narrow to a point when the target treatment distribution approaches that in the observed data. Moreover, drawing connections to the theory of optimal transport, we characterize generalized policies that minimize worst-case bound width in various sensitivity analysis models, as well as corresponding sharp bounds on their causal effects. These optimal policies are new, and can have a more parsimonious interpretation compared to their usual stochastic policy analogues. Finally, we develop flexible, efficient, and robust estimators for the sharp nonparametric bounds that emerge from the framework.

**Link**: [arxiv](http://arxiv.org/abs/2411.14285v1),  [pdf](http://arxiv.org/pdf/2411.14285v1)

**Tags**: stat.ME math.ST stat.TH 



### CAIP: Detecting Router Misconfigurations with Context-Aware Iterative   Prompting of LLMs
**Authors**: Xi Jiang, Aaron Gember-Jacobson, Nick Feamster

**Updated**: 2024-11-21T16:34:52Z

**Summary**: Model checkers and consistency checkers detect critical errors in router configurations, but these tools require significant manual effort to develop and maintain. LLM-based Q&A models have emerged as a promising alternative, allowing users to query partitions of configurations through prompts and receive answers based on learned patterns, thanks to transformer models pre-trained on vast datasets that provide generic configuration context for interpreting router configurations. Yet, current methods of partition-based prompting often do not provide enough network-specific context from the actual configurations to enable accurate inference. We introduce a Context-Aware Iterative Prompting (CAIP) framework that automates network-specific context extraction and optimizes LLM prompts for more precise router misconfiguration detection. CAIP addresses three challenges: (1) efficiently mining relevant context from complex configuration files, (2) accurately distinguishing between pre-defined and user-defined parameter values to prevent irrelevant context from being introduced, and (3) managing prompt context overload with iterative, guided interactions with the model. Our evaluations on synthetic and real-world configurations show that CAIP improves misconfiguration detection accuracy by more than 30% compared to partition-based LLM approaches, model checkers, and consistency checkers, uncovering over 20 previously undetected misconfigurations in real-world configurations.

**Link**: [arxiv](http://arxiv.org/abs/2411.14283v1),  [pdf](http://arxiv.org/pdf/2411.14283v1)

**Tags**: cs.NI 



### Looking Beyond Text: Reducing Language bias in Large Vision-Language   Models via Multimodal Dual-Attention and Soft-Image Guidance
**Authors**: Haozhe Zhao, Shuzheng Si, Liang Chen, Yichi Zhang, Maosong Sun, Mingjia Zhang, Baobao Chang

**Updated**: 2024-11-21T16:33:30Z

**Summary**: Large vision-language models (LVLMs) have achieved impressive results in various vision-language tasks. However, despite showing promising performance, LVLMs suffer from hallucinations caused by language bias, leading to diminished focus on images and ineffective visual comprehension. We identify two primary reasons for this bias: 1. Different scales of training data between the pretraining stage of LLM and multimodal alignment stage. 2. The learned inference bias due to short-term dependency of text data. Therefore, we propose LACING, a systemic framework designed to address the language bias of LVLMs with muLtimodal duAl-attention meChanIsm (MDA) aNd soft-image Guidance (IFG). Specifically, MDA introduces a parallel dual-attention mechanism that enhances the integration of visual inputs across the model. IFG introduces a learnable soft visual prompt during training and inference to replace visual inputs, designed to compel LVLMs to prioritize text inputs. Then, IFG further proposes a novel decoding strategy using the soft visual prompt to mitigate the model's over-reliance on adjacent text inputs. Comprehensive experiments demonstrate that our method effectively debiases LVLMs from their language bias, enhancing visual comprehension and reducing hallucinations without requiring additional training resources or data. The code and model are available at [lacing-lvlm.github.io](https://lacing-lvlm.github.io).

**Link**: [arxiv](http://arxiv.org/abs/2411.14279v1),  [pdf](http://arxiv.org/pdf/2411.14279v1)

**Tags**: cs.CV cs.CL 



### Accelerating Gaussian Variational Inference for Motion Planning Under   Uncertainty
**Authors**: Zinuo Chang, Hongzhe Yu, Patricio Vela, Yongxin Chen

**Updated**: 2024-11-21T16:32:25Z

**Summary**: This work addresses motion planning under uncertainty as a stochastic optimal control problem. The path distribution induced by the optimal controller corresponds to a posterior path distribution with a known form. To approximate this posterior, we frame an optimization problem in the space of Gaussian distributions, which aligns with the Gaussian Variational Inference Motion Planning (GVIMP) paradigm introduced in \cite{yu2023gaussian}. In this framework, the computation bottleneck lies in evaluating the expectation of collision costs over a dense discretized trajectory and computing the marginal covariances. This work exploits the sparse motion planning factor graph, which allows for parallel computing collision costs and Gaussian Belief Propagation (GBP) marginal covariance computation, to introduce a computationally efficient approach to solving GVIMP. We term the novel paradigm as the Parallel Gaussian Variational Inference Motion Planning (P-GVIMP). We validate the proposed framework on various robotic systems, demonstrating significant speed acceleration achieved by leveraging Graphics Processing Units (GPUs) for parallel computation. An open-sourced implementation is presented at https://github.com/hzyu17/VIMP.

**Link**: [arxiv](http://arxiv.org/abs/2411.03416v2),  [pdf](http://arxiv.org/pdf/2411.03416v2)

**Tags**: cs.RO 



### Dual Attention Model with Reinforcement Learning for Classification of   Histology Whole-Slide Images
**Authors**: Manahil Raza, Ruqayya Awan, Raja Muhammad Saad Bashir, Talha Qaiser, Nasir M. Rajpoot

**Updated**: 2024-11-21T16:29:08Z

**Summary**: Digital whole slide images (WSIs) are generally captured at microscopic resolution and encompass extensive spatial data. Directly feeding these images to deep learning models is computationally intractable due to memory constraints, while downsampling the WSIs risks incurring information loss. Alternatively, splitting the WSIs into smaller patches may result in a loss of important contextual information. In this paper, we propose a novel dual attention approach, consisting of two main components, both inspired by the visual examination process of a pathologist: The first soft attention model processes a low magnification view of the WSI to identify relevant regions of interest, followed by a custom sampling method to extract diverse and spatially distinct image tiles from the selected ROIs. The second component, the hard attention classification model further extracts a sequence of multi-resolution glimpses from each tile for classification. Since hard attention is non-differentiable, we train this component using reinforcement learning to predict the location of the glimpses. This approach allows the model to focus on essential regions instead of processing the entire tile, thereby aligning with a pathologist's way of diagnosis. The two components are trained in an end-to-end fashion using a joint loss function to demonstrate the efficacy of the model. The proposed model was evaluated on two WSI-level classification problems: Human epidermal growth factor receptor 2 scoring on breast cancer histology images and prediction of Intact/Loss status of two Mismatch Repair biomarkers from colorectal cancer histology images. We show that the proposed model achieves performance better than or comparable to the state-of-the-art methods while processing less than 10% of the WSI at the highest magnification and reducing the time required to infer the WSI-level label by more than 75%.

**Link**: [arxiv](http://arxiv.org/abs/2302.09682v2),  [pdf](http://arxiv.org/pdf/2302.09682v2)

**Tags**: eess.IV cs.CV 



### Efficient Aspect-Based Summarization of Climate Change Reports with   Small Language Models
**Authors**: Iacopo Ghinassi, Leonardo Catalano, Tommaso Colella

**Updated**: 2024-11-21T16:28:32Z

**Summary**: The use of Natural Language Processing (NLP) for helping decision-makers with Climate Change action has recently been highlighted as a use case aligning with a broader drive towards NLP technologies for social good. In this context, Aspect-Based Summarization (ABS) systems that extract and summarize relevant information are particularly useful as they provide stakeholders with a convenient way of finding relevant information in expert-curated reports. In this work, we release a new dataset for ABS of Climate Change reports and we employ different Large Language Models (LLMs) and so-called Small Language Models (SLMs) to tackle this problem in an unsupervised way. Considering the problem at hand, we also show how SLMs are not significantly worse for the problem while leading to reduced carbon footprint; we do so by applying for the first time an existing framework considering both energy efficiency and task performance to the evaluation of zero-shot generative models for ABS. Overall, our results show that modern language models, both big and small, can effectively tackle ABS for Climate Change reports but more research is needed when we frame the problem as a Retrieval Augmented Generation (RAG) problem and our work and dataset will help foster efforts in this direction.

**Link**: [arxiv](http://arxiv.org/abs/2411.14272v1),  [pdf](http://arxiv.org/pdf/2411.14272v1)

**Tags**: cs.CL I.2.7 



### The properties of the interstellar medium in dusty, star-forming   galaxies at $z \sim 2-4$: The shape of the CO spectral line energy   distributions
**Authors**: Dominic J. Taylor, A. M. Swinbank, Ian Smail, Annagrazia Puglisi, Jack E. Birkin, Ugne Dudzeviciute, Chian-Chou Chen, S. Ikarashi, Marta Frias Castillo, Axel Weiss, Zefeng Li, Scott C. Chapman, Jasper Jansen, E. F. Jimenez-Andrade, Leah K. Morabito, Eric J. Murphy, Matus Rybak, P. P. van der Werf

**Updated**: 2024-11-21T16:28:27Z

**Summary**: The molecular gas in the interstellar medium (ISM) of star-forming galaxy populations exhibits diverse physical properties. We investigate the $^{12}$CO excitation of twelve dusty, luminous star-forming galaxies at $z \sim 2-4$ by combining observations of the $^{12}$CO from $J_{\rm up} = 1$ to $J_{\rm up} = 8$. The spectral line energy distribution (SLED) has a similar shape to NGC 253, M82, and local ULIRGs, with much stronger excitation than the Milky Way inner disc. By combining with resolved dust continuum sizes from high-resolution $870$-$\mu$m ALMA observations and dust mass measurements determined from multi-wavelength SED fitting, we measure the relationship between the $^{12}$CO SLED and probable physical drivers of excitation: star-formation efficiency, the average intensity of the radiation field $\langle U\rangle$, and the star-formation rate surface density. The primary driver of high-$J_{\rm up}$ $^{12}$CO excitation in star-forming galaxies is star-formation rate surface density. We use the ratio of the CO($3-2$) and CO($6-5$) line fluxes to infer the CO excitation in each source and find that the average ratios for our sample are elevated compared to observations of low-redshift, less actively star-forming galaxies and agree well with predictions from numerical models that relate the ISM excitation to the star-formation rate surface density. The significant scatter in the line ratios of a factor $\approx 3$ within our sample likely reflects intrinsic variations in the ISM properties which may be caused by other effects on the excitation of the molecular gas, such as cosmic ray ionization rates and mechanical heating through turbulence dissipation.

**Link**: [arxiv](http://arxiv.org/abs/2411.14271v1),  [pdf](http://arxiv.org/pdf/2411.14271v1)

**Tags**: astro-ph.GA astro-ph.CO 



### Linguacodus: A Synergistic Framework for Transformative Code Generation   in Machine Learning Pipelines
**Authors**: Ekaterina Trofimova, Emil Sataev, Andrey E. Ustyuzhanin

**Updated**: 2024-11-21T16:28:03Z

**Summary**: In the ever-evolving landscape of machine learning, seamless translation of natural language descriptions into executable code remains a formidable challenge. This paper introduces Linguacodus, an innovative framework designed to tackle this challenge by deploying a dynamic pipeline that iteratively transforms natural language task descriptions into code through high-level data-shaping instructions. The core of Linguacodus is a fine-tuned large language model (LLM), empowered to evaluate diverse solutions for various problems and select the most fitting one for a given task. This paper details the fine-tuning process, and sheds light on how natural language descriptions can be translated into functional code. Linguacodus represents a substantial leap towards automated code generation, effectively bridging the gap between task descriptions and executable code. It holds great promise for advancing machine learning applications across diverse domains. Additionally, we propose an algorithm capable of transforming a natural description of an ML task into code with minimal human interaction. In extensive experiments on a vast machine learning code dataset originating from Kaggle, we showcase the effectiveness of Linguacodus. The investigations highlight its potential applications across diverse domains, emphasizing its impact on applied machine learning in various scientific fields.

**Link**: [arxiv](http://arxiv.org/abs/2403.11585v3),  [pdf](http://arxiv.org/pdf/2403.11585v3)

**Tags**: cs.LG cs.AI cs.CL cs.PL cs.SE 



### Finite-Choice Logic Programming
**Authors**: Chris Martens, Robert J. Simmons, Michael Arntzenius

**Updated**: 2024-11-21T16:27:59Z

**Summary**: Logic programming, as exemplified by datalog, defines the meaning of a program as its unique smallest model: the deductive closure of its inference rules. However, many problems call for an enumeration of models that vary along some set of choices while maintaining structural and logical constraints -- there is no single canonical model. The notion of stable models for logic programs with negation has successfully captured programmer intuition about the set of valid solutions for such problems, giving rise to a family of programming languages and associated solvers known as answer set programming. Unfortunately, the definition of a stable model is frustratingly indirect, especially in the presence of rules containing free variables.   We propose a new formalism, finite-choice logic programming, that uses choice, not negation, to admit multiple solutions. Finite-choice logic programming contains all the expressive power of the stable model semantics, gives meaning to a new and useful class of programs, and enjoys a least-fixed-point interpretation over a novel domain. We present an algorithm for exploring the solution space and prove it correct with respect to our semantics. Our implementation, the Dusa logic programming language, has performance that compares favorably with state-of-the-art answer set solvers and exhibits more predictable scaling with problem size.

**Link**: [arxiv](http://arxiv.org/abs/2405.19040v3),  [pdf](http://arxiv.org/pdf/2405.19040v3)

**Tags**: cs.PL cs.LO 



### Probabilistically Correct Language-based Multi-Robot Planning using   Conformal Prediction
**Authors**: Jun Wang, Guocheng He, Yiannis Kantaros

**Updated**: 2024-11-21T16:19:35Z

**Summary**: This paper addresses task planning problems for language-instructed robot teams. Tasks are expressed in natural language (NL), requiring the robots to apply their capabilities at various locations and semantic objects. Several recent works have addressed similar planning problems by leveraging pre-trained Large Language Models (LLMs) to design effective multi-robot plans. However, these approaches lack performance guarantees. To address this challenge, we introduce a new distributed LLM-based planner, called S-ATLAS for Safe plAnning for Teams of Language-instructed AgentS, that is capable of achieving user-defined mission success rates. This is accomplished by leveraging conformal prediction (CP), a distribution-free uncertainty quantification tool in black-box models. CP allows the proposed multi-robot planner to reason about its inherent uncertainty in a distributed fashion, enabling robots to make individual decisions when they are sufficiently certain and seek help otherwise. We show, both theoretically and empirically, that the proposed planner can achieve user-specified task success rates, assuming successful plan execution, while minimizing the overall number of help requests. We provide comparative experiments against related works showing that our method is significantly more computational efficient and achieves lower help rates. The advantage of our algorithm over baselines becomes more pronounced with increasing robot team size.

**Link**: [arxiv](http://arxiv.org/abs/2402.15368v4),  [pdf](http://arxiv.org/pdf/2402.15368v4)

**Tags**: cs.RO cs.AI 



### EoRA: Training-free Compensation for Compressed LLM with Eigenspace   Low-Rank Approximation
**Authors**: Shih-Yang Liu, Huck Yang, Chien-Yi Wang, Nai Chit Fung, Hongxu Yin, Charbel Sakr, Saurav Muralidharan, Kwang-Ting Cheng, Jan Kautz, Yu-Chiang Frank Wang, Pavlo Molchanov, Min-Hung Chen

**Updated**: 2024-11-21T16:12:34Z

**Summary**: In this work, we re-formulate the model compression problem into the customized compensation problem: Given a compressed model, we aim to introduce residual low-rank paths to compensate for compression errors under customized requirements from users (e.g., tasks, compression ratios), resulting in greater flexibility in adjusting overall capacity without being constrained by specific compression formats. However, naively applying SVD to derive residual paths causes suboptimal utilization of the low-rank representation capacity. Instead, we propose Training-free Eigenspace Low-Rank Approximation (EoRA), a method that directly minimizes compression-induced errors without requiring gradient-based training, achieving fast optimization in minutes using a small amount of calibration data. EoRA projects compression errors into the eigenspace of input activations, leveraging eigenvalues to effectively prioritize the reconstruction of high-importance error components. Moreover, EoRA can be seamlessly integrated with fine-tuning and quantization to further improve effectiveness and efficiency. EoRA consistently outperforms previous methods in compensating errors for compressed LLaMA2/3 models on various tasks, such as language generation, commonsense reasoning, and math reasoning tasks (e.g., 31.31%/12.88% and 9.69% improvements on ARC-Easy/ARC-Challenge and MathQA when compensating LLaMA3-8B that is quantized to 4-bit and pruned to 2:4 sparsity). EoRA offers a scalable, training-free solution to compensate for compression errors, making it a powerful tool to deploy LLMs in various capacity and efficiency requirements.

**Link**: [arxiv](http://arxiv.org/abs/2410.21271v2),  [pdf](http://arxiv.org/pdf/2410.21271v2)

**Tags**: cs.CL cs.AI 



### Knowledge Graphs, Large Language Models, and Hallucinations: An NLP   Perspective
**Authors**: Ernests Lavrinovics, Russa Biswas, Johannes Bjerva, Katja Hose

**Updated**: 2024-11-21T16:09:05Z

**Summary**: Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) based applications including automated text generation, question answering, chatbots, and others. However, they face a significant challenge: hallucinations, where models produce plausible-sounding but factually incorrect responses. This undermines trust and limits the applicability of LLMs in different domains. Knowledge Graphs (KGs), on the other hand, provide a structured collection of interconnected facts represented as entities (nodes) and their relationships (edges). In recent research, KGs have been leveraged to provide context that can fill gaps in an LLM understanding of certain topics offering a promising approach to mitigate hallucinations in LLMs, enhancing their reliability and accuracy while benefiting from their wide applicability. Nonetheless, it is still a very active area of research with various unresolved open problems. In this paper, we discuss these open challenges covering state-of-the-art datasets and benchmarks as well as methods for knowledge integration and evaluating hallucinations. In our discussion, we consider the current use of KGs in LLM systems and identify future directions within each of these challenges.

**Link**: [arxiv](http://arxiv.org/abs/2411.14258v1),  [pdf](http://arxiv.org/pdf/2411.14258v1)

**Tags**: cs.CL cs.AI 68-02 I.2.7 



### Generalizing End-To-End Autonomous Driving In Real-World Environments   Using Zero-Shot LLMs
**Authors**: Zeyu Dong, Yimin Zhu, Yansong Li, Kevin Mahon, Yu Sun

**Updated**: 2024-11-21T16:04:10Z

**Summary**: Traditional autonomous driving methods adopt a modular design, decomposing tasks into sub-tasks. In contrast, end-to-end autonomous driving directly outputs actions from raw sensor data, avoiding error accumulation. However, training an end-to-end model requires a comprehensive dataset; otherwise, the model exhibits poor generalization capabilities. Recently, large language models (LLMs) have been applied to enhance the generalization capabilities of end-to-end driving models. Most studies explore LLMs in an open-loop manner, where the output actions are compared to those of experts without direct feedback from the real world, while others examine closed-loop results only in simulations. This paper proposes an efficient architecture that integrates multimodal LLMs into end-to-end driving models operating in closed-loop settings in real-world environments. In our architecture, the LLM periodically processes raw sensor data to generate high-level driving instructions, effectively guiding the end-to-end model, even at a slower rate than the raw sensor data. This architecture relaxes the trade-off between the latency and inference quality of the LLM. It also allows us to choose from a wide variety of LLMs to improve high-level driving instructions and minimize fine-tuning costs. Consequently, our architecture reduces data collection requirements because the LLMs do not directly output actions; we only need to train a simple imitation learning model to output actions. In our experiments, the training data for the end-to-end model in a real-world environment consists of only simple obstacle configurations with one traffic cone, while the test environment is more complex and contains multiple obstacles placed in various positions. Experiments show that the proposed architecture enhances the generalization capabilities of the end-to-end model even without fine-tuning the LLM.

**Link**: [arxiv](http://arxiv.org/abs/2411.14256v1),  [pdf](http://arxiv.org/pdf/2411.14256v1)

**Tags**: cs.RO 



### Intent-Aware Dialogue Generation and Multi-Task Contrastive Learning for   Multi-Turn Intent Classification
**Authors**: Junhua Liu, Yong Keat Tan, Bin Fu, Kwan Hui Lim

**Updated**: 2024-11-21T15:59:29Z

**Summary**: Generating large-scale, domain-specific, multilingual multi-turn dialogue datasets remains a significant hurdle for training effective Multi-Turn Intent Classification models in chatbot systems. In this paper, we introduce Chain-of-Intent, a novel mechanism that combines Hidden Markov Models with Large Language Models (LLMs) to generate contextually aware, intent-driven conversations through self-play. By extracting domain-specific knowledge from e-commerce chat logs, we estimate conversation turns and intent transitions, which guide the generation of coherent dialogues. Leveraging LLMs to enhance emission probabilities, our approach produces natural and contextually consistent questions and answers. We also propose MINT-CL, a framework for multi-turn intent classification using multi-task contrastive learning, improving classification accuracy without the need for extensive annotated data. Evaluations show that our methods outperform baselines in dialogue quality and intent classification accuracy, especially in multilingual settings, while significantly reducing data generation efforts. Furthermore, we release MINT-E, a multilingual, intent-aware multi-turn e-commerce dialogue corpus to support future research in this area.

**Link**: [arxiv](http://arxiv.org/abs/2411.14252v1),  [pdf](http://arxiv.org/pdf/2411.14252v1)

**Tags**: cs.CL cs.AI 



### Natural Language Reinforcement Learning
**Authors**: Xidong Feng, Ziyu Wan, Haotian Fu, Bo Liu, Mengyue Yang, Girish A. Koushik, Zhiyuan Hu, Ying Wen, Jun Wang

**Updated**: 2024-11-21T15:57:02Z

**Summary**: Reinforcement Learning (RL) mathematically formulates decision-making with Markov Decision Process (MDP). With MDPs, researchers have achieved remarkable breakthroughs across various domains, including games, robotics, and language models. This paper seeks a new possibility, Natural Language Reinforcement Learning (NLRL), by extending traditional MDP to natural language-based representation space. Specifically, NLRL innovatively redefines RL principles, including task objectives, policy, value function, Bellman equation, and policy iteration, into their language counterparts. With recent advancements in large language models (LLMs), NLRL can be practically implemented to achieve RL-like policy and value improvement by either pure prompting or gradient-based training. Experiments over Maze, Breakthrough, and Tic-Tac-Toe games demonstrate the effectiveness, efficiency, and interpretability of the NLRL framework among diverse use cases. Our code will be released at https://github.com/waterhorse1/Natural-language-RL.

**Link**: [arxiv](http://arxiv.org/abs/2411.14251v1),  [pdf](http://arxiv.org/pdf/2411.14251v1)

**Tags**: cs.LG cs.AI cs.CL 



### VeriGraph: Scene Graphs for Execution Verifiable Robot Planning
**Authors**: Daniel Ekpo, Mara Levy, Saksham Suri, Chuong Huynh, Abhinav Shrivastava

**Updated**: 2024-11-21T15:56:48Z

**Summary**: Recent advancements in vision-language models (VLMs) offer potential for robot task planning, but challenges remain due to VLMs' tendency to generate incorrect action sequences. To address these limitations, we propose VeriGraph, a novel framework that integrates VLMs for robotic planning while verifying action feasibility. VeriGraph employs scene graphs as an intermediate representation, capturing key objects and spatial relationships to improve plan verification and refinement. The system generates a scene graph from input images and uses it to iteratively check and correct action sequences generated by an LLM-based task planner, ensuring constraints are respected and actions are executable. Our approach significantly enhances task completion rates across diverse manipulation scenarios, outperforming baseline methods by 58% for language-based tasks and 30% for image-based tasks.

**Link**: [arxiv](http://arxiv.org/abs/2411.10446v2),  [pdf](http://arxiv.org/pdf/2411.10446v2)

**Tags**: cs.RO cs.AI 



### AnywhereDoor: Multi-Target Backdoor Attacks on Object Detection
**Authors**: Jialin Lu, Junjie Shan, Ziqi Zhao, Ka-Ho Chow

**Updated**: 2024-11-21T15:50:59Z

**Summary**: As object detection becomes integral to many safety-critical applications, understanding its vulnerabilities is essential. Backdoor attacks, in particular, pose a significant threat by implanting hidden backdoor in a victim model, which adversaries can later exploit to trigger malicious behaviors during inference. However, current backdoor techniques are limited to static scenarios where attackers must define a malicious objective before training, locking the attack into a predetermined action without inference-time adaptability. Given the expressive output space in object detection, including object existence detection, bounding box estimation, and object classification, the feasibility of implanting a backdoor that provides inference-time control with a high degree of freedom remains unexplored. This paper introduces AnywhereDoor, a flexible backdoor attack tailored for object detection. Once implanted, AnywhereDoor enables adversaries to specify different attack types (object vanishing, fabrication, or misclassification) and configurations (untargeted or targeted with specific classes) to dynamically control detection behavior. This flexibility is achieved through three key innovations: (i) objective disentanglement to support a broader range of attack combinations well beyond what existing methods allow; (ii) trigger mosaicking to ensure backdoor activations are robust, even against those object detectors that extract localized regions from the input image for recognition; and (iii) strategic batching to address object-level data imbalances that otherwise hinders a balanced manipulation. Extensive experiments demonstrate that AnywhereDoor provides attackers with a high degree of control, achieving an attack success rate improvement of nearly 80% compared to adaptations of existing methods for such flexible control.

**Link**: [arxiv](http://arxiv.org/abs/2411.14243v1),  [pdf](http://arxiv.org/pdf/2411.14243v1)

**Tags**: cs.CR cs.AI cs.CV 



### FocusLLaVA: A Coarse-to-Fine Approach for Efficient and Effective Visual   Token Compression
**Authors**: Yuke Zhu, Chi Xie, Shuang Liang, Bo Zheng, Sheng Guo

**Updated**: 2024-11-21T15:37:52Z

**Summary**: Recent advances on Multi-modal Large Language Models have demonstrated that high-resolution image input is crucial for model capabilities, especially for fine-grained tasks. However, high-resolution images lead to a quadratic increase in the number of visual tokens input into LLMs, resulting in significant computational costs. Current work develop visual token compression methods to achieve efficiency improvements, often at the expense of performance. We argue that removing visual redundancy can simultaneously improve both efficiency and performance. We build a coarse-to-fine visual token compression method, with a vision-guided sampler for compressing redundant regions with low information density, and a text-guided sampler for selecting visual tokens that are strongly correlated with the user instructions.With these two modules, the proposed FocusLLaVA achieves improvements in both efficiency and performance. We validate the effectiveness of our approach on a wide range of evaluation datasets.

**Link**: [arxiv](http://arxiv.org/abs/2411.14228v1),  [pdf](http://arxiv.org/pdf/2411.14228v1)

**Tags**: cs.CV 



### Evaluating the Robustness of Analogical Reasoning in Large Language   Models
**Authors**: Martha Lewis, Melanie Mitchell

**Updated**: 2024-11-21T15:25:08Z

**Summary**: LLMs have performed well on several reasoning benchmarks, including ones that test analogical reasoning abilities. However, there is debate on the extent to which they are performing general abstract reasoning versus employing non-robust processes, e.g., that overly rely on similarity to pre-training data. Here we investigate the robustness of analogy-making abilities previously claimed for LLMs on three of four domains studied by Webb, Holyoak, and Lu (2023): letter-string analogies, digit matrices, and story analogies. For each domain we test humans and GPT models on robustness to variants of the original analogy problems that test the same abstract reasoning abilities but are likely dissimilar from tasks in the pre-training data. The performance of a system that uses robust abstract reasoning should not decline substantially on these variants.   On simple letter-string analogies, we find that while the performance of humans remains high for two types of variants we tested, the GPT models' performance declines sharply. This pattern is less pronounced as the complexity of these problems is increased, as both humans and GPT models perform poorly on both the original and variant problems requiring more complex analogies. On digit-matrix problems, we find a similar pattern but only on one out of the two types of variants we tested. On story-based analogy problems, we find that, unlike humans, the performance of GPT models are susceptible to answer-order effects, and that GPT models also may be more sensitive than humans to paraphrasing.   This work provides evidence that LLMs often lack the robustness of zero-shot human analogy-making, exhibiting brittleness on most of the variations we tested. More generally, this work points to the importance of carefully evaluating AI systems not only for accuracy but also robustness when testing their cognitive capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2411.14215v1),  [pdf](http://arxiv.org/pdf/2411.14215v1)

**Tags**: cs.CL cs.AI cs.LG 



### Physics-Informed LLM-Agent for Automated Modulation Design in Power   Electronics Systems
**Authors**: Junhua Liu, Fanfan Lin, Xinze Li, Kwan Hui Lim, Shuai Zhao

**Updated**: 2024-11-21T15:24:41Z

**Summary**: LLM-based autonomous agents have demonstrated outstanding performance in solving complex industrial tasks. However, in the pursuit of carbon neutrality and high-performance renewable energy systems, existing AI-assisted design automation faces significant limitations in explainability, scalability, and usability. To address these challenges, we propose LP-COMDA, an LLM-based, physics-informed autonomous agent that automates the modulation design of power converters in Power Electronics Systems with minimal human supervision. Unlike traditional AI-assisted approaches, LP-COMDA contains an LLM-based planner that gathers and validates design specifications through a user-friendly chat interface. The planner then coordinates with physics-informed design and optimization tools to iteratively generate and refine modulation designs autonomously. Through the chat interface, LP-COMDA provides an explainable design process, presenting explanations and charts. Experiments show that LP-COMDA outperforms all baseline methods, achieving a 63.2% reduction in error compared to the second-best benchmark method in terms of standard mean absolute error. Furthermore, empirical studies with 20 experts conclude that design time with LP-COMDA is over 33 times faster than conventional methods, showing its significant improvement on design efficiency over the current processes.

**Link**: [arxiv](http://arxiv.org/abs/2411.14214v1),  [pdf](http://arxiv.org/pdf/2411.14214v1)

**Tags**: cs.AI cs.ET 



### Revised Regularization for Efficient Continual Learning through   Correlation-Based Parameter Update in Bayesian Neural Networks
**Authors**: Sanchar Palit, Biplab Banerjee, Subhasis Chaudhuri

**Updated**: 2024-11-21T15:11:02Z

**Summary**: We propose a Bayesian neural network-based continual learning algorithm using Variational Inference, aiming to overcome several drawbacks of existing methods. Specifically, in continual learning scenarios, storing network parameters at each step to retain knowledge poses challenges. This is compounded by the crucial need to mitigate catastrophic forgetting, particularly given the limited access to past datasets, which complicates maintaining correspondence between network parameters and datasets across all sessions. Current methods using Variational Inference with KL divergence risk catastrophic forgetting during uncertain node updates and coupled disruptions in certain nodes. To address these challenges, we propose the following strategies. To reduce the storage of the dense layer parameters, we propose a parameter distribution learning method that significantly reduces the storage requirements. In the continual learning framework employing variational inference, our study introduces a regularization term that specifically targets the dynamics and population of the mean and variance of the parameters. This term aims to retain the benefits of KL divergence while addressing related challenges. To ensure proper correspondence between network parameters and the data, our method introduces an importance-weighted Evidence Lower Bound term to capture data and parameter correlations. This enables storage of common and distinctive parameter hyperspace bases. The proposed method partitions the parameter space into common and distinctive subspaces, with conditions for effective backward and forward knowledge transfer, elucidating the network-parameter dataset correspondence. The experimental results demonstrate the effectiveness of our method across diverse datasets and various combinations of sequential datasets, yielding superior performance compared to existing approaches.

**Link**: [arxiv](http://arxiv.org/abs/2411.14202v1),  [pdf](http://arxiv.org/pdf/2411.14202v1)

**Tags**: cs.LG cs.CV 



### OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented   LMs
**Authors**: Akari Asai, Jacqueline He, Rulin Shao, Weijia Shi, Amanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Sergey Feldman, Mike D'arcy, David Wadden, Matt Latzke, Minyang Tian, Pan Ji, Shengyan Liu, Hao Tong, Bohao Wu, Yanyu Xiong, Luke Zettlemoyer, Graham Neubig, Dan Weld, Doug Downey, Wen-tau Yih, Pang Wei Koh, Hannaneh Hajishirzi

**Updated**: 2024-11-21T15:07:42Z

**Summary**: Scientific progress depends on researchers' ability to synthesize the growing body of literature. Can large language models (LMs) assist scientists in this task? We introduce OpenScholar, a specialized retrieval-augmented LM that answers scientific queries by identifying relevant passages from 45 million open-access papers and synthesizing citation-backed responses. To evaluate OpenScholar, we develop ScholarQABench, the first large-scale multi-domain benchmark for literature search, comprising 2,967 expert-written queries and 208 long-form answers across computer science, physics, neuroscience, and biomedicine. On ScholarQABench, OpenScholar-8B outperforms GPT-4o by 5% and PaperQA2 by 7% in correctness, despite being a smaller, open model. While GPT4o hallucinates citations 78 to 90% of the time, OpenScholar achieves citation accuracy on par with human experts. OpenScholar's datastore, retriever, and self-feedback inference loop also improves off-the-shelf LMs: for instance, OpenScholar-GPT4o improves GPT-4o's correctness by 12%. In human evaluations, experts preferred OpenScholar-8B and OpenScholar-GPT4o responses over expert-written ones 51% and 70% of the time, respectively, compared to GPT4o's 32%. We open-source all of our code, models, datastore, data and a public demo.

**Link**: [arxiv](http://arxiv.org/abs/2411.14199v1),  [pdf](http://arxiv.org/pdf/2411.14199v1)

**Tags**: cs.CL cs.AI cs.DL cs.IR cs.LG 



### Learning Pore-scale Multi-phase Flow from Experimental Data with Graph   Neural Network
**Authors**: Yuxuan Gu, Catherine Spurin, Gege Wen

**Updated**: 2024-11-21T15:01:17Z

**Summary**: Understanding the process of multiphase fluid flow through porous media is crucial for many climate change mitigation technologies, including CO$_2$ geological storage, hydrogen storage, and fuel cells. However, current numerical models are often incapable of accurately capturing the complex pore-scale physics observed in experiments. In this study, we address this challenge using a graph neural network-based approach and directly learn pore-scale fluid flow using micro-CT experimental data. We propose a Long-Short-Edge MeshGraphNet (LSE-MGN) that predicts the state of each node in the pore space at each time step. During inference, given an initial state, the model can autoregressively predict the evolution of the multiphase flow process over time. This approach successfully captures the physics from the high-resolution experimental data while maintaining computational efficiency, providing a promising direction for accurate and efficient pore-scale modeling of complex multiphase fluid flow dynamics.

**Link**: [arxiv](http://arxiv.org/abs/2411.14192v1),  [pdf](http://arxiv.org/pdf/2411.14192v1)

**Tags**: physics.flu-dyn cs.LG 



### TaQ-DiT: Time-aware Quantization for Diffusion Transformers
**Authors**: Xinyan Liu, Huihong Shi, Yang Xu, Zhongfeng Wang

**Updated**: 2024-11-21T14:34:46Z

**Summary**: Transformer-based diffusion models, dubbed Diffusion Transformers (DiTs), have achieved state-of-the-art performance in image and video generation tasks. However, their large model size and slow inference speed limit their practical applications, calling for model compression methods such as quantization. Unfortunately, existing DiT quantization methods overlook (1) the impact of reconstruction and (2) the varying quantization sensitivities across different layers, which hinder their achievable performance. To tackle these issues, we propose innovative time-aware quantization for DiTs (TaQ-DiT). Specifically, (1) we observe a non-convergence issue when reconstructing weights and activations separately during quantization and introduce a joint reconstruction method to resolve this problem. (2) We discover that Post-GELU activations are particularly sensitive to quantization due to their significant variability across different denoising steps as well as extreme asymmetries and variations within each step. To address this, we propose time-variance-aware transformations to facilitate more effective quantization. Experimental results show that when quantizing DiTs' weights to 4-bit and activations to 8-bit (W4A8), our method significantly surpasses previous quantization methods.

**Link**: [arxiv](http://arxiv.org/abs/2411.14172v1),  [pdf](http://arxiv.org/pdf/2411.14172v1)

**Tags**: eess.IV 



### Spatiotemporal Decoupling for Efficient Vision-Based Occupancy   Forecasting
**Authors**: Jingyi Xu, Xieyuanli Chen, Junyi Ma, Jiawei Huang, Jintao Xu, Yue Wang, Ling Pei

**Updated**: 2024-11-21T14:27:15Z

**Summary**: The task of occupancy forecasting (OCF) involves utilizing past and present perception data to predict future occupancy states of autonomous vehicle surrounding environments, which is critical for downstream tasks such as obstacle avoidance and path planning. Existing 3D OCF approaches struggle to predict plausible spatial details for movable objects and suffer from slow inference speeds due to neglecting the bias and uneven distribution of changing occupancy states in both space and time. In this paper, we propose a novel spatiotemporal decoupling vision-based paradigm to explicitly tackle the bias and achieve both effective and efficient 3D OCF. To tackle spatial bias in empty areas, we introduce a novel spatial representation that decouples the conventional dense 3D format into 2D bird's-eye view (BEV) occupancy with corresponding height values, enabling 3D OCF derived only from 2D predictions thus enhancing efficiency. To reduce temporal bias on static voxels, we design temporal decoupling to improve end-to-end OCF by temporally associating instances via predicted flows. We develop an efficient multi-head network EfficientOCF to achieve 3D OCF with our devised spatiotemporally decoupled representation. A new metric, conditional IoU (C-IoU), is also introduced to provide a robust 3D OCF performance assessment, especially in datasets with missing or incomplete annotations. The experimental results demonstrate that EfficientOCF surpasses existing baseline methods on accuracy and efficiency, achieving state-of-the-art performance with a fast inference time of 82.33ms with a single GPU. Our code will be released as open source.

**Link**: [arxiv](http://arxiv.org/abs/2411.14169v1),  [pdf](http://arxiv.org/pdf/2411.14169v1)

**Tags**: cs.CV 



### RRADistill: Distilling LLMs' Passage Ranking Ability for Long-Tail   Queries Document Re-Ranking on a Search Engine
**Authors**: Nayoung Choi, Youngjune Lee, Gyu-Hwung Cho, Haeyu Jeong, Jungmin Kong, Saehun Kim, Keunchan Park, Sarah Cho, Inchang Jeong, Gyohee Nam, Sunghoon Han, Wonil Yang, Jaeho Choi

**Updated**: 2024-11-21T14:23:49Z

**Summary**: Large Language Models (LLMs) excel at understanding the semantic relationships between queries and documents, even with lengthy and complex long-tail queries. These queries are challenging for feedback-based rankings due to sparse user engagement and limited feedback, making LLMs' ranking ability highly valuable. However, the large size and slow inference of LLMs necessitate the development of smaller, more efficient models (sLLMs). Recently, integrating ranking label generation into distillation techniques has become crucial, but existing methods underutilize LLMs' capabilities and are cumbersome. Our research, RRADistill: Re-Ranking Ability Distillation, propose an efficient label generation pipeline and novel sLLM training methods for both encoder and decoder models. We introduce an encoder-based method using a Term Control Layer to capture term matching signals and a decoder-based model with a ranking layer for enhanced understanding. A/B testing on a Korean-based search platform, validates the effectiveness of our approach in improving re-ranking for long-tail queries.

**Link**: [arxiv](http://arxiv.org/abs/2410.18097v3),  [pdf](http://arxiv.org/pdf/2410.18097v3)

**Tags**: cs.IR cs.AI cs.LG 



### FoPru: Focal Pruning for Efficient Large Vision-Language Models
**Authors**: Lei Jiang, Weizhe Huang, Tongxuan Liu, Yuting Zeng, Jing Li, Lechao Cheng, Xiaohua Xu

**Updated**: 2024-11-21T14:22:38Z

**Summary**: Large Vision-Language Models (LVLMs) represent a significant advancement toward achieving superior multimodal capabilities by enabling powerful Large Language Models (LLMs) to understand visual input. Typically, LVLMs utilize visual encoders, such as CLIP, to transform images into visual tokens, which are then aligned with textual tokens through projection layers before being input into the LLM for inference. Although existing LVLMs have achieved significant success, their inference efficiency is still limited by the substantial number of visual tokens and the potential redundancy among them. To mitigate this issue, we propose Focal Pruning (FoPru), a training-free method that prunes visual tokens based on the attention-based token significance derived from the vision encoder. Specifically, we introduce two alternative pruning strategies: 1) the rank strategy, which leverages all token significance scores to retain more critical tokens in a global view; 2) the row strategy, which focuses on preserving continuous key information in images from a local perspective. Finally, the selected tokens are reordered to maintain their original positional relationships. Extensive experiments across various LVLMs and multimodal datasets demonstrate that our method can prune a large number of redundant tokens while maintaining high accuracy, leading to significant improvements in inference efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2411.14164v1),  [pdf](http://arxiv.org/pdf/2411.14164v1)

**Tags**: cs.CV cs.AI 



### OmniGen: Unified Image Generation
**Authors**: Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, Zheng Liu

**Updated**: 2024-11-21T14:09:12Z

**Summary**: The emergence of Large Language Models (LLMs) has unified language generation tasks and revolutionized human-machine interaction. However, in the realm of image generation, a unified model capable of handling various tasks within a single framework remains largely unexplored. In this work, we introduce OmniGen, a new diffusion model for unified image generation. OmniGen is characterized by the following features: 1) Unification: OmniGen not only demonstrates text-to-image generation capabilities but also inherently supports various downstream tasks, such as image editing, subject-driven generation, and visual-conditional generation. 2) Simplicity: The architecture of OmniGen is highly simplified, eliminating the need for additional plugins. Moreover, compared to existing diffusion models, it is more user-friendly and can complete complex tasks end-to-end through instructions without the need for extra intermediate steps, greatly simplifying the image generation workflow. 3) Knowledge Transfer: Benefit from learning in a unified format, OmniGen effectively transfers knowledge across different tasks, manages unseen tasks and domains, and exhibits novel capabilities. We also explore the model's reasoning capabilities and potential applications of the chain-of-thought mechanism. This work represents the first attempt at a general-purpose image generation model, and we will release our resources at https://github.com/VectorSpaceLab/OmniGen to foster future advancements.

**Link**: [arxiv](http://arxiv.org/abs/2409.11340v2),  [pdf](http://arxiv.org/pdf/2409.11340v2)

**Tags**: cs.CV cs.AI 



### GASP: Efficient Black-Box Generation of Adversarial Suffixes for   Jailbreaking LLMs
**Authors**: Advik Raj Basani, Xiao Zhang

**Updated**: 2024-11-21T14:00:01Z

**Summary**: Large Language Models (LLMs) have shown impressive proficiency across a range of natural language processing tasks yet remain vulnerable to adversarial prompts, known as jailbreak attacks, carefully designed to elicit harmful responses from LLMs. Traditional methods rely on manual heuristics, which suffer from limited generalizability. While being automatic, optimization-based attacks often produce unnatural jailbreak prompts that are easy to detect by safety filters or require high computational overhead due to discrete token optimization. Witnessing the limitations of existing jailbreak methods, we introduce Generative Adversarial Suffix Prompter (GASP), a novel framework that combines human-readable prompt generation with Latent Bayesian Optimization (LBO) to improve adversarial suffix creation in a fully black-box setting. GASP leverages LBO to craft adversarial suffixes by efficiently exploring continuous embedding spaces, gradually optimizing the model to improve attack efficacy while balancing prompt coherence through a targeted iterative refinement procedure. Our experiments show that GASP can generate natural jailbreak prompts, significantly improving attack success rates, reducing training times, and accelerating inference speed, thus making it an efficient and scalable solution for red-teaming LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2411.14133v1),  [pdf](http://arxiv.org/pdf/2411.14133v1)

**Tags**: cs.LG cs.AI cs.CR cs.CV 



### SemanticDraw: Towards Real-Time Interactive Content Creation from Image   Diffusion Models
**Authors**: Jaerin Lee, Daniel Sungho Jung, Kanggeon Lee, Kyoung Mu Lee

**Updated**: 2024-11-21T13:57:27Z

**Summary**: We introduce SemanticDraw, a new paradigm of interactive content creation where high-quality images are generated in near real-time from given multiple hand-drawn regions, each encoding prescribed semantic meaning. In order to maximize the productivity of content creators and to fully realize their artistic imagination, it requires both quick interactive interfaces and fine-grained regional controls in their tools. Despite astonishing generation quality from recent diffusion models, we find that existing approaches for regional controllability are very slow (52 seconds for $512 \times 512$ image) while not compatible with acceleration methods such as LCM, blocking their huge potential in interactive content creation. From this observation, we build our solution for interactive content creation in two steps: (1) we establish compatibility between region-based controls and acceleration techniques for diffusion models, maintaining high fidelity of multi-prompt image generation with $\times 10$ reduced number of inference steps, (2) we increase the generation throughput with our new multi-prompt stream batch pipeline, enabling low-latency generation from multiple, region-based text prompts on a single RTX 2080 Ti GPU. Our proposed framework is generalizable to any existing diffusion models and acceleration schedulers, allowing sub-second (0.64 seconds) image content creation application upon well-established image diffusion models. Our project page is: https://jaerinlee.com/research/semantic-draw.

**Link**: [arxiv](http://arxiv.org/abs/2403.09055v3),  [pdf](http://arxiv.org/pdf/2403.09055v3)

**Tags**: cs.CV 



### Learning from "Silly" Questions Improves Large Language Models, But Only   Slightly
**Authors**: Tingyuan Zhu, Shudong Liu, Yidong Wang, Derek F. Wong, Han Yu, Takahiro Shinozaki, Jindong Wang

**Updated**: 2024-11-21T13:45:40Z

**Summary**: Constructing high-quality Supervised Fine-Tuning (SFT) datasets is critical for the training of large language models (LLMs). Recent studies have shown that using data from a specific source, Ruozhiba, a Chinese website where users ask "silly" questions to better understand certain topics, can lead to better fine-tuning performance. This paper aims to explore some hidden factors: the potential interpretations of its success and a large-scale evaluation of the performance. First, we leverage GPT-4 to analyze the successful cases of Ruozhiba questions from the perspective of education, psychology, and cognitive science, deriving a set of explanatory rules. Then, we construct fine-tuning datasets by applying these rules to the MMLU training set. Surprisingly, our results indicate that rules can significantly improve model performance in certain tasks, while potentially diminishing performance on others. For example, SFT data generated following the "Counterintuitive Thinking" rule can achieve approximately a 5% improvement on the "Global Facts" task, whereas the "Blurring the Conceptual Boundaries" rule leads to a performance drop of 6.14% on the "Econometrics" task. In addition, for specific tasks, different rules tend to have a consistent impact on model performance. This suggests that the differences between the extracted rules are not as significant, and the effectiveness of the rules is relatively consistent across tasks. Our research highlights the importance of considering task diversity and rule applicability when constructing SFT datasets to achieve more comprehensive performance improvements.

**Link**: [arxiv](http://arxiv.org/abs/2411.14121v1),  [pdf](http://arxiv.org/pdf/2411.14121v1)

**Tags**: cs.CL 



### RAG-Thief: Scalable Extraction of Private Data from Retrieval-Augmented   Generation Applications with Agent-based Attacks
**Authors**: Changyue Jiang, Xudong Pan, Geng Hong, Chenfu Bao, Min Yang

**Updated**: 2024-11-21T13:18:03Z

**Summary**: While large language models (LLMs) have achieved notable success in generative tasks, they still face limitations, such as lacking up-to-date knowledge and producing hallucinations. Retrieval-Augmented Generation (RAG) enhances LLM performance by integrating external knowledge bases, providing additional context which significantly improves accuracy and knowledge coverage. However, building these external knowledge bases often requires substantial resources and may involve sensitive information. In this paper, we propose an agent-based automated privacy attack called RAG-Thief, which can extract a scalable amount of private data from the private database used in RAG applications. We conduct a systematic study on the privacy risks associated with RAG applications, revealing that the vulnerability of LLMs makes the private knowledge bases suffer significant privacy risks. Unlike previous manual attacks which rely on traditional prompt injection techniques, RAG-Thief starts with an initial adversarial query and learns from model responses, progressively generating new queries to extract as many chunks from the knowledge base as possible. Experimental results show that our RAG-Thief can extract over 70% information from the private knowledge bases within customized RAG applications deployed on local machines and real-world platforms, including OpenAI's GPTs and ByteDance's Coze. Our findings highlight the privacy vulnerabilities in current RAG applications and underscore the pressing need for stronger safeguards.

**Link**: [arxiv](http://arxiv.org/abs/2411.14110v1),  [pdf](http://arxiv.org/pdf/2411.14110v1)

**Tags**: cs.CR 



### Exploring HOD-dependent systematics for the DESI 2024 Full-Shape galaxy   clustering analysis
**Authors**: N. Findlay, S. Nadathur, W. J. Percival, A. de Mattia, P. Zarrouk, H. Gil-Marín, O. Alves, J. Mena-Fernández, C. Garcia-Quintero, A. Rocher, S. Ahlen, D. Bianchi, D. Brooks, T. Claybaugh, S. Cole, A. de la Macorra, Arjun Dey, P. Doel, K. Fanning, A. Font-Ribera, J. E. Forero-Romero, E. Gaztañaga, G. Gutierrez, C. Hahn, K. Honscheid, C. Howlett, S. Juneau, M. E. Levi, A. Meisner, R. Miquel, J. Moustakas, N. Palanque-Delabrouille, I. Pérez-Ràfols, G. Rossi, E. Sanchez, D. Schlegel, M. Schubnell, H. Seo, D. Sprayberry, G. Tarlé, M. Vargas-Magaña, B. A. Weaver

**Updated**: 2024-11-21T13:10:39Z

**Summary**: We analyse the robustness of the DESI 2024 cosmological inference from fits to the full shape of the galaxy power spectrum to uncertainties in the Halo Occupation Distribution (HOD) model of the galaxy-halo connection and the choice of priors on nuisance parameters. We assess variations in the recovered cosmological parameters across a range of mocks populated with different HOD models and find that shifts are often greater than 20% of the expected statistical uncertainties from the DESI data. We encapsulate the effect of such shifts in terms of a systematic covariance term, $\mathsf{C}_{\rm HOD}$, and an additional diagonal contribution quantifying the impact of our choice of nuisance parameter priors on the ability of the effective field theory (EFT) model to correctly recover the cosmological parameters of the simulations. These two covariance contributions are designed to be added to the usual covariance term, $\mathsf{C}_{\rm stat}$, describing the statistical uncertainty in the power spectrum measurement, in order to fairly represent these sources of systematic uncertainty. This approach is more general and robust to choices of model free parameters or additional external datasets used in cosmological fits than the alternative approach of adding systematic uncertainties at the level of the recovered marginalised parameter posteriors. We compare the approaches within the context of a fixed $\Lambda$CDM model and demonstrate that our method gives conservative estimates of the systematic uncertainty that nevertheless have little impact on the final posteriors obtained from DESI data.

**Link**: [arxiv](http://arxiv.org/abs/2411.12023v2),  [pdf](http://arxiv.org/pdf/2411.12023v2)

**Tags**: astro-ph.CO 



### Lost in Inference: Rediscovering the Role of Natural Language Inference   for Large Language Models
**Authors**: Lovish Madaan, David Esiobu, Pontus Stenetorp, Barbara Plank, Dieuwke Hupkes

**Updated**: 2024-11-21T13:09:36Z

**Summary**: In the recent past, a popular way of evaluating natural language understanding (NLU), was to consider a model's ability to perform natural language inference (NLI) tasks. In this paper, we investigate if NLI tasks, that are rarely used for LLM evaluation, can still be informative for evaluating LLMs. Focusing on five different NLI benchmarks across six models of different scales, we investigate if they are able to discriminate models of different size and quality and how their accuracies develop during training. Furthermore, we investigate the extent to which the softmax distributions of models align with human distributions in cases where statements are ambiguous or vague. Overall, our results paint a positive picture for the NLI tasks: we find that they are able to discriminate well between models at various stages of training, yet are not (all) saturated. Furthermore, we find that while the similarity of model distributions with human label distributions increases with scale, it is still much higher than the similarity between two populations of humans, making it a potentially interesting statistic to consider.

**Link**: [arxiv](http://arxiv.org/abs/2411.14103v1),  [pdf](http://arxiv.org/pdf/2411.14103v1)

**Tags**: cs.CL 



### Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years   of German Parliamentary Debates
**Authors**: Aida Kostikova, Benjamin Paassen, Dominik Beese, Ole Pütz, Gregor Wiedemann, Steffen Eger

**Updated**: 2024-11-21T13:06:53Z

**Summary**: Solidarity is a crucial concept to understand social relations in societies. In this paper, we explore fine-grained solidarity frames to study solidarity towards women and migrants in German parliamentary debates between 1867 and 2022. Using 2,864 manually annotated text snippets (with a cost exceeding 18k Euro), we evaluate large language models (LLMs) like Llama 3, GPT-3.5, and GPT-4. We find that GPT-4 outperforms other LLMs, approaching human annotation quality. Using GPT-4, we automatically annotate more than 18k further instances (with a cost of around 500 Euro) across 155 years and find that solidarity with migrants outweighs anti-solidarity but that frequencies and solidarity types shift over time. Most importantly, group-based notions of (anti-)solidarity fade in favor of compassionate solidarity, focusing on the vulnerability of migrant groups, and exchange-based anti-solidarity, focusing on the lack of (economic) contribution. Our study highlights the interplay of historical events, socio-economic needs, and political ideologies in shaping migration discourse and social cohesion. We also show that powerful LLMs, if carefully prompted, can be cost-effective alternatives to human annotation for hard social scientific tasks.

**Link**: [arxiv](http://arxiv.org/abs/2210.04359v3),  [pdf](http://arxiv.org/pdf/2210.04359v3)

**Tags**: cs.CL cs.LG cs.SI 



### Data Augmentation for Surgical Scene Segmentation with Anatomy-Aware   Diffusion Models
**Authors**: Danush Kumar Venkatesh, Dominik Rivoir, Micha Pfeiffer, Fiona Kolbinger, Stefanie Speidel

**Updated**: 2024-11-21T12:47:53Z

**Summary**: In computer-assisted surgery, automatically recognizing anatomical organs is crucial for understanding the surgical scene and providing intraoperative assistance. While machine learning models can identify such structures, their deployment is hindered by the need for labeled, diverse surgical datasets with anatomical annotations. Labeling multiple classes (i.e., organs) in a surgical scene is time-intensive, requiring medical experts. Although synthetically generated images can enhance segmentation performance, maintaining both organ structure and texture during generation is challenging. We introduce a multi-stage approach using diffusion models to generate multi-class surgical datasets with annotations. Our framework improves anatomy awareness by training organ specific models with an inpainting objective guided by binary segmentation masks. The organs are generated with an inference pipeline using pre-trained ControlNet to maintain the organ structure. The synthetic multi-class datasets are constructed through an image composition step, ensuring structural and textural consistency. This versatile approach allows the generation of multi-class datasets from real binary datasets and simulated surgical masks. We thoroughly evaluate the generated datasets on image quality and downstream segmentation, achieving a $15\%$ improvement in segmentation scores when combined with real images. The code is available at https://gitlab.com/nct_tso_public/muli-class-image-synthesis

**Link**: [arxiv](http://arxiv.org/abs/2410.07753v2),  [pdf](http://arxiv.org/pdf/2410.07753v2)

**Tags**: cs.CV cs.LG 



### PatchScaler: An Efficient Patch-Independent Diffusion Model for Image   Super-Resolution
**Authors**: Yong Liu, Hang Dong, Jinshan Pan, Qingji Dong, Kai Chen, Rongxiang Zhang, Lean Fu, Fei Wang

**Updated**: 2024-11-21T12:35:18Z

**Summary**: While diffusion models significantly improve the perceptual quality of super-resolved images, they usually require a large number of sampling steps, resulting in high computational costs and long inference times. Recent efforts have explored reasonable acceleration schemes by reducing the number of sampling steps. However, these approaches treat all regions of the image equally, overlooking the fact that regions with varying levels of reconstruction difficulty require different sampling steps. To address this limitation, we propose PatchScaler, an efficient patch-independent diffusion pipeline for single image super-resolution. Specifically, PatchScaler introduces a Patch-adaptive Group Sampling (PGS) strategy that groups feature patches by quantifying their reconstruction difficulty and establishes shortcut paths with different sampling configurations for each group. To further optimize the patch-level reconstruction process of PGS, we propose a texture prompt that provides rich texture conditional information to the diffusion model. The texture prompt adaptively retrieves texture priors for the target patch from a common reference texture memory. Extensive experiments show that our PatchScaler achieves superior performance in both quantitative and qualitative evaluations, while significantly speeding up inference. Our code will be available at \url{https://github.com/yongliuy/PatchScaler}.

**Link**: [arxiv](http://arxiv.org/abs/2405.17158v4),  [pdf](http://arxiv.org/pdf/2405.17158v4)

**Tags**: cs.CV 



### Multi LoRA Meets Vision: Merging multiple adapters to create a multi   task model
**Authors**: Ege Kesim, Selahattin Serdar Helli

**Updated**: 2024-11-21T12:26:33Z

**Summary**: Parameter efficient finetuning (PEFT) methods are widely used in LLMs and generative models in computer vision. Especially one can use multiple of these during inference to change the behavior of the base model. In this paper we investigated whether multiple LoRA adapters trained on computer vision tasks can be merged together and used during inference without loss in performance. By achieving this, multitask models can be created just by merging different LoRAs. Merging these will reduce inference time and it will not require any additional retraining. We have trained adapters on six different tasks and evaluated their performance when they are merged together. For comparison we used a model with a frozen backbone and finetuned its head. Our results show that even with simple merging techniques creating a multitask model by merging adapters is achievable by slightly loosing performance in some cases. In our experiments we merged up to three adapters together. Depending on the task and the similarity of the data adapters were trained on, merges can outperform head finetuning. We have observed that LoRAs trained with dissimilar datasets tend to perform better compared to model trained on similar datasets.

**Link**: [arxiv](http://arxiv.org/abs/2411.14064v1),  [pdf](http://arxiv.org/pdf/2411.14064v1)

**Tags**: cs.CV cs.AI 



### Is Less More? Exploring Token Condensation as Training-free Adaptation   for CLIP
**Authors**: Zixin Wang, Dong Gong, Sen Wang, Zi Huang, Yadan Luo

**Updated**: 2024-11-21T12:17:29Z

**Summary**: Contrastive language-image pre-training (CLIP) has shown remarkable generalization ability in image classification. However, CLIP sometimes encounters performance drops on downstream datasets during zero-shot inference. Test-time adaptation methods attempt to mitigate this by adjusting normalization layers or tuning context prompts with large batch sizes and extensive augmentations; yet, these methods are computationally intensive. This raises an important question: Is there a training-free approach that can efficiently address CLIP's performance drop in such cases? To explore this, we benchmark token condensation techniques, originally designed to enhance the efficiency of vision transformers, on CLIP zero-shot inference tasks. We observe that although token condensation may compromise in-domain accuracy, it surprisingly enhances CLIP's performance on certain cross-dataset benchmarks. This motivates two key inquiries: (1) Can token condensation serve as a "free-lunch" solution for CLIP zero-shot inference? (2) What criteria should guide condensation -- how can essential tokens be identified and redundant ones eliminated? To address these questions, we propose Token Condensation as Adaptation (TCA), a training-free adaptation method for CLIP by pruning class-irrelevant visual tokens while merging class-ambiguous tokens. As the first approach for CLIP's token efficiency, TCA demonstrates superior performance across cross-dataset tasks, achieving up to a 21.4\% improvement over the strongest baseline while reducing GFLOPs by 12.2\% to 48.9\%, with minimized hyperparameter dependency.

**Link**: [arxiv](http://arxiv.org/abs/2410.14729v2),  [pdf](http://arxiv.org/pdf/2410.14729v2)

**Tags**: cs.CV cs.AI cs.CL cs.LG 



### Characterizing and Transforming DAGs within the I-LCA Framework
**Authors**: Marc Hellmuth, Anna Lindeberg

**Updated**: 2024-11-21T12:09:21Z

**Summary**: We explore the connections between clusters and least common ancestors (LCAs) in directed acyclic graphs (DAGs), focusing on DAGs with unique LCAs for specific subsets of their leaves. These DAGs are important in modeling phylogenetic networks that account for reticulate processes or horizontal gene transfer. Phylogenetic DAGs inferred from genomic data are often complex, obscuring evolutionary insights, especially when vertices lack support as LCAs for any subset of taxa. To address this, we focus on $I$-lca-relevant DAGs, where each vertex serves as the unique LCA for a subset $A$ of leaves of specific size $|A|\in I$. We characterize DAGs with the so-called $I$-lca-property and establish their close relationship to pre-$I$-ary and $I$-ary set systems. Moreover, we build upon recently established results that use a simple operator $\ominus$, enabling the transformation of arbitrary DAGs into $I$-lca-relevant DAGs. This process reduces unnecessary complexity while preserving the key structural properties of the original DAG. The set $C_G$ consists of all clusters in a DAG $G$, where clusters correspond to the descendant leaves of vertices. While in some cases $C_H = C_G$ when transforming $G$ into an $I$-lca-relevant DAG $H$, it often happens that certain clusters in $C_G$ do not appear as clusters in $H$. To understand this phenomenon in detail, we characterize the subset of clusters in $C_G$ that remain in $H$ for DAGs $G$ with the $I$-lca-property. Furthermore, we show that the set $W$ of vertices required to transform $G$ into $H = G \ominus W$ is uniquely determined for such DAGs. This, in turn, allows us to show that the transformed DAG $H$ is always a tree or a galled-tree whenever $C_G$ represents the clustering system of a tree or galled-tree and $G$ has the $I$-lca-property. In the latter case $C_H = C_G$ always holds.

**Link**: [arxiv](http://arxiv.org/abs/2411.14057v1),  [pdf](http://arxiv.org/pdf/2411.14057v1)

**Tags**: math.CO cs.DM 



### DRPruning: Efficient Large Language Model Pruning through   Distributionally Robust Optimization
**Authors**: Hexuan Deng, Wenxiang Jiao, Xuebo Liu, Min Zhang, Zhaopeng Tu

**Updated**: 2024-11-21T12:02:39Z

**Summary**: Large language models (LLMs) deliver impressive results but face challenges from increasing model sizes and computational costs. Structured pruning reduces model size and speeds up inference but often causes uneven degradation across domains, leading to biased performance. To address this, we propose DRPruning, which incorporates distributionally robust optimization to restore balanced performance across domains, along with further improvements to enhance robustness. Experiments in monolingual and multilingual settings show that our method surpasses similarly sized models in pruning and continued pretraining over perplexity, downstream tasks, and instruction tuning. We further provide analysis demonstrating the robustness of our method towards various domains and distribution shifts. Furthermore, our method automatically determines optimal reference losses and data ratios, suggesting potential for broader applications. Our code is available at https://github.com/hexuandeng/DRPruning.

**Link**: [arxiv](http://arxiv.org/abs/2411.14055v1),  [pdf](http://arxiv.org/pdf/2411.14055v1)

**Tags**: cs.CL 



### How Does A Text Preprocessing Pipeline Affect Ontology Syntactic   Matching?
**Authors**: Zhangcheng Qiang, Kerry Taylor, Weiqing Wang

**Updated**: 2024-11-21T12:00:23Z

**Summary**: The generic text preprocessing pipeline, comprising Tokenisation, Normalisation, Stop Words Removal, and Stemming/Lemmatisation, has been implemented in many ontology matching (OM) systems. However, the lack of standardisation in text preprocessing creates diversity in mapping results. In this paper, we investigate the effect of the text preprocessing pipeline on OM tasks at syntactic levels. Our experiments on 8 Ontology Alignment Evaluation Initiative (OAEI) track repositories with 49 distinct alignments indicate: (1) Tokenisation and Normalisation are currently more effective than Stop Words Removal and Stemming/Lemmatisation; and (2) The selection of Lemmatisation and Stemming is task-specific. We recommend standalone Lemmatisation or Stemming with post-hoc corrections. We find that (3) Porter Stemmer and Snowball Stemmer perform better than Lancaster Stemmer; and that (4) Part-of-Speech (POS) Tagging does not help Lemmatisation. To repair less effective Stop Words Removal and Stemming/Lemmatisation used in OM tasks, we propose a novel context-based pipeline repair approach that significantly improves matching correctness and overall matching performance. We also discuss the use of text preprocessing pipeline in the new era of large language models (LLMs).

**Link**: [arxiv](http://arxiv.org/abs/2411.03962v2),  [pdf](http://arxiv.org/pdf/2411.03962v2)

**Tags**: cs.CL 



### Improving Controllability and Editability for Pretrained Text-to-Music   Generation Models
**Authors**: Yixiao Zhang

**Updated**: 2024-11-21T11:46:13Z

**Summary**: The field of AI-assisted music creation has made significant strides, yet existing systems often struggle to meet the demands of iterative and nuanced music production. These challenges include providing sufficient control over the generated content and allowing for flexible, precise edits. This thesis tackles these issues by introducing a series of advancements that progressively build upon each other, enhancing the controllability and editability of text-to-music generation models.   First, we introduce Loop Copilot, a system that tries to address the need for iterative refinement in music creation. Loop Copilot leverages a large language model (LLM) to coordinate multiple specialised AI models, enabling users to generate and refine music interactively through a conversational interface. Central to this system is the Global Attribute Table, which records and maintains key musical attributes throughout the iterative process, ensuring that modifications at any stage preserve the overall coherence of the music. While Loop Copilot excels in orchestrating the music creation process, it does not directly address the need for detailed edits to the generated content.   To overcome this limitation, MusicMagus is presented as a further solution for editing AI-generated music. MusicMagus introduces a zero-shot text-to-music editing approach that allows for the modification of specific musical attributes, such as genre, mood, and instrumentation, without the need for retraining. By manipulating the latent space within pre-trained diffusion models, MusicMagus ensures that these edits are stylistically coherent and that non-targeted attributes remain unchanged. This system is particularly effective in maintaining the structural integrity of the music during edits, but it encounters challenges with more complex and real-world audio scenarios.   ...

**Link**: [arxiv](http://arxiv.org/abs/2411.12641v2),  [pdf](http://arxiv.org/pdf/2411.12641v2)

**Tags**: cs.SD eess.AS 



### Forecasting Future International Events: A Reliable Dataset for   Text-Based Event Modeling
**Authors**: Daehoon Gwak, Junwoo Park, Minho Park, Chaehun Park, Hyunchan Lee, Edward Choi, Jaegul Choo

**Updated**: 2024-11-21T11:44:23Z

**Summary**: Predicting future international events from textual information, such as news articles, has tremendous potential for applications in global policy, strategic decision-making, and geopolitics. However, existing datasets available for this task are often limited in quality, hindering the progress of related research. In this paper, we introduce WORLDREP (WORLD Relationship and Event Prediction), a novel dataset designed to address these limitations by leveraging the advanced reasoning capabilities of large-language models (LLMs). Our dataset features high-quality scoring labels generated through advanced prompt modeling and rigorously validated by domain experts in political science. We showcase the quality and utility of WORLDREP for real-world event prediction tasks, demonstrating its effectiveness through extensive experiments and analysis. Furthermore, we publicly release our dataset along with the full automation source code for data collection, labeling, and benchmarking, aiming to support and advance research in text-based event prediction.

**Link**: [arxiv](http://arxiv.org/abs/2411.14042v1),  [pdf](http://arxiv.org/pdf/2411.14042v1)

**Tags**: cs.CL cs.AI 



### Teaching MLPs to Master Heterogeneous Graph-Structured Knowledge for   Efficient and Accurate Inference
**Authors**: Yunhui Liu, Xinyi Gao, Tieke He, Jianhua Zhao, Hongzhi Yin

**Updated**: 2024-11-21T11:39:09Z

**Summary**: Heterogeneous Graph Neural Networks (HGNNs) have achieved promising results in various heterogeneous graph learning tasks, owing to their superiority in capturing the intricate relationships and diverse relational semantics inherent in heterogeneous graph structures. However, the neighborhood-fetching latency incurred by structure dependency in HGNNs makes it challenging to deploy for latency-constrained applications that require fast inference. Inspired by recent GNN-to-MLP knowledge distillation frameworks, we introduce HG2M and HG2M+ to combine both HGNN's superior performance and MLP's efficient inference. HG2M directly trains student MLPs with node features as input and soft labels from teacher HGNNs as targets, and HG2M+ further distills reliable and heterogeneous semantic knowledge into student MLPs through reliable node distillation and reliable meta-path distillation. Experiments conducted on six heterogeneous graph datasets show that despite lacking structural dependencies, HG2Ms can still achieve competitive or even better performance than HGNNs and significantly outperform vanilla MLPs. Moreover, HG2Ms demonstrate a 379.24$\times$ speedup in inference over HGNNs on the large-scale IGB-3M-19 dataset, showcasing their ability for latency-sensitive deployments.

**Link**: [arxiv](http://arxiv.org/abs/2411.14035v1),  [pdf](http://arxiv.org/pdf/2411.14035v1)

**Tags**: cs.LG 



### Multi-LLM-Agent Systems: Techniques and Business Perspectives
**Authors**: Yingxuan Yang, Qiuying Peng, Jun Wang, Weinan Zhang

**Updated**: 2024-11-21T11:36:29Z

**Summary**: In the era of (multi-modal) large language models, most operational processes can be reformulated and reproduced using LLM agents. The LLM agents can perceive, control, and get feedback from the environment so as to accomplish the given tasks in an autonomous manner. Besides the environment-interaction property, the LLM agents can call various external tools to ease the task completion process. The tools can be regarded as a predefined operational process with private or real-time knowledge that does not exist in the parameters of LLMs. As a natural trend of development, the tools for calling are becoming autonomous agents, thus the full intelligent system turns out to be a multi-LLM-agent system (MLAS). This paper discusses the technical and business landscapes of MLAS. Compared to the previous single-LLM-agent system, a MLAS has the advantages of i) higher potential of task-solving performance, ii) higher flexibility for system changing, iii) proprietary data preserving for each participating entity, and iv) feasibility of monetization for each entity. To support the ecosystem of MLAS, we provide a preliminary version of such MLAS protocol considering technical requirements, data privacy, and business incentives. As such, MLAS would be a practical solution to achieve artificial collective intelligence in the near future.

**Link**: [arxiv](http://arxiv.org/abs/2411.14033v1),  [pdf](http://arxiv.org/pdf/2411.14033v1)

**Tags**: cs.AI 



### OASIS: Open Agents Social Interaction Simulations on One Million Agents
**Authors**: Ziyi Yang, Zaibin Zhang, Zirui Zheng, Yuxian Jiang, Ziyue Gan, Zhiyu Wang, Zijian Ling, Jinsong Chen, Martz Ma, Bowen Dong, Prateek Gupta, Shuyue Hu, Zhenfei Yin, Guohao Li, Xu Jia, Lijun Wang, Bernard Ghanem, Huchuan Lu, Wanli Ouyang, Yu Qiao, Philip Torr, Jing Shao

**Updated**: 2024-11-21T11:27:34Z

**Summary**: There has been a growing interest in enhancing rule-based agent-based models (ABMs) for social media platforms (i.e., X, Reddit) with more realistic large language model (LLM) agents, thereby allowing for a more nuanced study of complex systems. As a result, several LLM-based ABMs have been proposed in the past year. While they hold promise, each simulator is specifically designed to study a particular scenario, making it time-consuming and resource-intensive to explore other phenomena using the same ABM. Additionally, these models simulate only a limited number of agents, whereas real-world social media platforms involve millions of users. To this end, we propose OASIS, a generalizable and scalable social media simulator. OASIS is designed based on real-world social media platforms, incorporating dynamically updated environments (i.e., dynamic social networks and post information), diverse action spaces (i.e., following, commenting), and recommendation systems (i.e., interest-based and hot-score-based). Additionally, OASIS supports large-scale user simulations, capable of modeling up to one million users. With these features, OASIS can be easily extended to different social media platforms to study large-scale group phenomena and behaviors. We replicate various social phenomena, including information spreading, group polarization, and herd effects across X and Reddit platforms. Moreover, we provide observations of social phenomena at different agent group scales. We observe that the larger agent group scale leads to more enhanced group dynamics and more diverse and helpful agents' opinions. These findings demonstrate OASIS's potential as a powerful tool for studying complex systems in digital environments.

**Link**: [arxiv](http://arxiv.org/abs/2411.11581v2),  [pdf](http://arxiv.org/pdf/2411.11581v2)

**Tags**: cs.CL 



### Logic Augmented Generation
**Authors**: Aldo Gangemi, Andrea Giovanni Nuzzolese

**Updated**: 2024-11-21T10:54:35Z

**Summary**: Semantic Knowledge Graphs (SKG) face challenges with scalability, flexibility, contextual understanding, and handling unstructured or ambiguous information. However, they offer formal and structured knowledge enabling highly interpretable and reliable results by means of reasoning and querying. Large Language Models (LLMs) overcome those limitations making them suitable in open-ended tasks and unstructured environments. Nevertheless, LLMs are neither interpretable nor reliable. To solve the dichotomy between LLMs and SKGs we envision Logic Augmented Generation (LAG) that combines the benefits of the two worlds. LAG uses LLMs as Reactive Continuous Knowledge Graphs that can generate potentially infinite relations and tacit knowledge on-demand. SKGs are key for injecting a discrete heuristic dimension with clear logical and factual boundaries. We exemplify LAG in two tasks of collective intelligence, i.e., medical diagnostics and climate projections. Understanding the properties and limitations of LAG, which are still mostly unknown, is of utmost importance for enabling a variety of tasks involving tacit knowledge in order to provide interpretable and effective results.

**Link**: [arxiv](http://arxiv.org/abs/2411.14012v1),  [pdf](http://arxiv.org/pdf/2411.14012v1)

**Tags**: cs.AI cs.CL 



### CulturePark: Boosting Cross-cultural Understanding in Large Language   Models
**Authors**: Cheng Li, Damien Teney, Linyi Yang, Qingsong Wen, Xing Xie, Jindong Wang

**Updated**: 2024-11-21T10:52:29Z

**Summary**: Cultural bias is pervasive in many large language models (LLMs), largely due to the deficiency of data representative of different cultures. Typically, cultural datasets and benchmarks are constructed either by extracting subsets of existing datasets or by aggregating from platforms such as Wikipedia and social media. However, these approaches are highly dependent on real-world data and human annotations, making them costly and difficult to scale. Inspired by cognitive theories on social communication, this paper introduces CulturePark, an LLM-powered multi-agent communication framework for cultural data collection. CulturePark simulates cross-cultural human communication with LLM-based agents playing roles in different cultures. It generates high-quality cross-cultural dialogues encapsulating human beliefs, norms, and customs. Using CulturePark, we generated 41,000 cultural samples to fine-tune eight culture-specific LLMs. We evaluated these models across three downstream tasks: content moderation, cultural alignment, and cultural education. Results show that for content moderation, our GPT-3.5-based models either match or outperform GPT-4 on datasets. Regarding cultural alignment, our models surpass GPT-4 on Hofstede's VSM 13 framework. Furthermore, for cultural education of human participants, our models demonstrate superior outcomes in both learning efficacy and user experience compared to GPT-4. CulturePark proves an important step in addressing cultural bias and advancing the democratization of AI, highlighting the critical role of culturally inclusive data in model training. Code is released at https://github.com/Scarelette/CulturePark.

**Link**: [arxiv](http://arxiv.org/abs/2405.15145v3),  [pdf](http://arxiv.org/pdf/2405.15145v3)

**Tags**: cs.AI cs.CL cs.MA 



### Spectral domain likelihoods for Bayesian inference in time-varying   parameter models
**Authors**: Oskar Gustafsson, Mattias Villani, Robert Kohn

**Updated**: 2024-11-21T10:46:41Z

**Summary**: Inference for locally stationary processes is often based on some local Whittle-type approximation of the likelihood function defined in the frequency domain. The main reasons for using such a likelihood approximation is that i) it has substantially lower computational cost and better scalability to long time series compared to the time domain likelihood, particularly when used for Bayesian inference via Markov Chain Monte Carlo (MCMC), ii) convenience when the model itself is specified in the frequency domain, and iii) it provides access to bootstrap and subsampling MCMC which exploits the asymptotic independence of Fourier transformed data. Most of the existing literature compares the asymptotic performance of the maximum likelihood estimator (MLE) from such frequency domain likelihood approximation with the exact time domain MLE. Our article uses three simulation studies to assess the finite-sample accuracy of several frequency domain likelihood functions when used to approximate the posterior distribution in time-varying parameter models. The methods are illustrated on an application to egg price data.

**Link**: [arxiv](http://arxiv.org/abs/2411.14010v1),  [pdf](http://arxiv.org/pdf/2411.14010v1)

**Tags**: stat.ME 



### GPT versus Humans: Uncovering Ethical Concerns in Conversational   Generative AI-empowered Multi-Robot Systems
**Authors**: Rebekah Rousi, Niko Makitalo, Hooman Samani, Kai-Kristian Kemell, Jose Siqueira de Cerqueira, Ville Vakkuri, Tommi Mikkonen, Pekka Abrahamsson

**Updated**: 2024-11-21T10:45:44Z

**Summary**: The emergence of generative artificial intelligence (GAI) and large language models (LLMs) such ChatGPT has enabled the realization of long-harbored desires in software and robotic development. The technology however, has brought with it novel ethical challenges. These challenges are compounded by the application of LLMs in other machine learning systems, such as multi-robot systems. The objectives of the study were to examine novel ethical issues arising from the application of LLMs in multi-robot systems. Unfolding ethical issues in GPT agent behavior (deliberation of ethical concerns) was observed, and GPT output was compared with human experts. The article also advances a model for ethical development of multi-robot systems. A qualitative workshop-based method was employed in three workshops for the collection of ethical concerns: two human expert workshops (N=16 participants) and one GPT-agent-based workshop (N=7 agents; two teams of 6 agents plus one judge). Thematic analysis was used to analyze the qualitative data. The results reveal differences between the human-produced and GPT-based ethical concerns. Human experts placed greater emphasis on new themes related to deviance, data privacy, bias and unethical corporate conduct. GPT agents emphasized concerns present in existing AI ethics guidelines. The study contributes to a growing body of knowledge in context-specific AI ethics and GPT application. It demonstrates the gap between human expert thinking and LLM output, while emphasizing new ethical concerns emerging in novel technology.

**Link**: [arxiv](http://arxiv.org/abs/2411.14009v1),  [pdf](http://arxiv.org/pdf/2411.14009v1)

**Tags**: cs.RO cs.HC cs.MA 



### Generative Intervention Models for Causal Perturbation Modeling
**Authors**: Nora Schneider, Lars Lorch, Niki Kilbertus, Bernhard Schölkopf, Andreas Krause

**Updated**: 2024-11-21T10:37:57Z

**Summary**: We consider the problem of predicting perturbation effects via causal models. In many applications, it is a priori unknown which mechanisms of a system are modified by an external perturbation, even though the features of the perturbation are available. For example, in genomics, some properties of a drug may be known, but not their causal effects on the regulatory pathways of cells. We propose a generative intervention model (GIM) that learns to map these perturbation features to distributions over atomic interventions in a jointly-estimated causal model. Contrary to prior approaches, this enables us to predict the distribution shifts of unseen perturbation features while gaining insights about their mechanistic effects in the underlying data-generating process. On synthetic data and scRNA-seq drug perturbation data, GIMs achieve robust out-of-distribution predictions on par with unstructured approaches, while effectively inferring the underlying perturbation mechanisms, often better than other causal inference methods.

**Link**: [arxiv](http://arxiv.org/abs/2411.14003v1),  [pdf](http://arxiv.org/pdf/2411.14003v1)

**Tags**: cs.LG stat.ML 



### SEMPose: A Single End-to-end Network for Multi-object Pose Estimation
**Authors**: Xin Liu, Hao Wang, Shibei Xue, Dezong Zhao

**Updated**: 2024-11-21T10:37:54Z

**Summary**: In computer vision, estimating the six-degree-of-freedom pose from an RGB image is a fundamental task. However, this task becomes highly challenging in multi-object scenes. Currently, the best methods typically employ an indirect strategy, which identifies 2D and 3D correspondences, and then solves with the Perspective-n-Points method. Yet, this approach cannot be trained end-to-end. Direct methods, on the other hand, suffer from lower accuracy due to challenges such as varying object sizes and occlusions. To address these issues, we propose SEMPose, an end-to-end multi-object pose estimation network. SEMPose utilizes a well-designed texture-shape guided feature pyramid network, effectively tackling the challenge of object size variations. Additionally, it employs an iterative refinement head structure, progressively regressing rotation and translation separately to enhance estimation accuracy. During training, we alleviate the impact of occlusion by selecting positive samples from visible parts. Experimental results demonstrate that SEMPose can perform inference at 32 FPS without requiring inputs other than the RGB image. It can accurately estimate the poses of multiple objects in real time, with inference time unaffected by the number of target objects. On the LM-O and YCB-V datasets, our method outperforms other RGB-based single-model methods, achieving higher accuracy. Even when compared with multi-model methods and approaches that use additional refinement, our results remain competitive.

**Link**: [arxiv](http://arxiv.org/abs/2411.14002v1),  [pdf](http://arxiv.org/pdf/2411.14002v1)

**Tags**: cs.CV 



### Repository-level Code Translation Benchmark Targeting Rust
**Authors**: Guangsheng Ou, Mingwei Liu, Yuxuan Chen, Xing Peng, Zibin Zheng

**Updated**: 2024-11-21T10:00:52Z

**Summary**: Recent advances in large language models (LLMs) have shown significant capabilities in code translation, often evaluated using benchmarks like CodeTransOcean. However, these evaluations typically focus on simple, function-level translations without considering dependencies, which does not reflect the complexities of real-world software development. Further, their effectiveness in translating to newer, lower-resource languages like Rust in realistic scenarios is still under-explored. To address this gap, we introduce first repository-level code translation benchmark comprising 375 tasks targeting Rust, complete with relevant dependencies. Using this benchmark, we study four state-of-the-art LLMs, analyzing their erroneous outputs to understand their performance in more complex translation scenarios. Our findings reveal that LLMs exhibit substantially worse performance (41.5%-56.2% Pass@1 drop of GPT-4) on repository-level translations compared to simpler tasks, highlighting limitations in existing evaluation methods. The model that performed the best is Claude-3.5, demonstrating the strongest translation capabilities in both basic functionality accuracy and several relevant additional abilities. Additionally, we discover that LLMs struggle with identifying language differences in complex tasks, and that increased dependencies correlate with greater translation difficulty.

**Link**: [arxiv](http://arxiv.org/abs/2411.13990v1),  [pdf](http://arxiv.org/pdf/2411.13990v1)

**Tags**: cs.SE 



### Integration of Active Learning and MCMC Sampling for Efficient Bayesian   Calibration of Mechanical Properties
**Authors**: Leon Riccius, Iuri B. C. M. Rocha, Joris Bierkens, Hanne Kekkonen, Frans P. van der Meer

**Updated**: 2024-11-21T09:51:22Z

**Summary**: Recent advancements in Markov chain Monte Carlo (MCMC) sampling and surrogate modelling have significantly enhanced the feasibility of Bayesian analysis across engineering fields. However, the selection and integration of surrogate models and cutting-edge MCMC algorithms, often depend on ad-hoc decisions. A systematic assessment of their combined influence on analytical accuracy and efficiency is notably lacking. The present work offers a comprehensive comparative study, employing a scalable case study in computational mechanics focused on the inference of spatially varying material parameters, that sheds light on the impact of methodological choices for surrogate modelling and sampling. We show that a priori training of the surrogate model introduces large errors in the posterior estimation even in low to moderate dimensions. We introduce a simple active learning strategy based on the path of the MCMC algorithm that is superior to all a priori trained models, and determine its training data requirements. We demonstrate that the choice of the MCMC algorithm has only a small influence on the amount of training data but no significant influence on the accuracy of the resulting surrogate model. Further, we show that the accuracy of the posterior estimation largely depends on the surrogate model, but not even a tailored surrogate guarantees convergence of the MCMC.Finally, we identify the forward model as the bottleneck in the inference process, not the MCMC algorithm. While related works focus on employing advanced MCMC algorithms, we demonstrate that the training data requirements render the surrogate modelling approach infeasible before the benefits of these gradient-based MCMC algorithms on cheap models can be reaped.

**Link**: [arxiv](http://arxiv.org/abs/2411.13361v2),  [pdf](http://arxiv.org/pdf/2411.13361v2)

**Tags**: physics.comp-ph stat.AP stat.ML 74S60 



### Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model   with Frozen LLM
**Authors**: Xiong Wang, Yangze Li, Chaoyou Fu, Yunhang Shen, Lei Xie, Ke Li, Xing Sun, Long Ma

**Updated**: 2024-11-21T09:19:28Z

**Summary**: Rapidly developing large language models (LLMs) have brought tremendous intelligent applications. Especially, the GPT-4o's excellent duplex speech interaction ability has brought impressive experience to users. Researchers have recently proposed several multi-modal LLMs in this direction that can achieve user-agent speech-to-speech conversations. This paper proposes a novel speech-text multimodal LLM architecture called Freeze-Omni. Our main contribution is that the speech input and output modalities can be easily connected to a textual LLM while keeping the LLM's parameters frozen throughout the training process. We design a three-stage training strategy for modeling both the speech input and output, enabling Freeze-Omni to obtain speech-to-speech conversation ability using text-speech paired data (such as ASR and TTS data) and only 60,000 multi-round text Q&A data on 8 GPUs. Moreover, we can effectively ensure that the intelligence of the Freeze-Omni in the speech modality is at the same level compared with that in the text modality of its backbone LLM, while achieving low latency end-to-end spoken response. In addition, we also designed a method to achieve duplex dialogue ability through multi-task training, giving Freeze-Omni a more natural style of dialogue ability between users and agents. In summary, Freeze-Omni holds great potential to conduct speech-to-speech dialogue based on a multimodal LLM under the condition of a frozen LLM, avoiding the catastrophic forgetting problem caused by limited data and training resources.

**Link**: [arxiv](http://arxiv.org/abs/2411.00774v3),  [pdf](http://arxiv.org/pdf/2411.00774v3)

**Tags**: cs.SD cs.AI cs.CL eess.AS 



### Learning the Universe: Cosmological and Astrophysical Parameter   Inference with Galaxy Luminosity Functions and Colours
**Authors**: Christopher C. Lovell, Tjitske Starkenburg, Matthew Ho, Daniel Anglés-Alcázar, Romeel Davé, Austen Gabrielpillai, Kartheik Iyer, Alice E. Matthews, William J. Roper, Rachel Somerville, Laura Sommovigo, Francisco Villaescusa-Navarro

**Updated**: 2024-11-21T09:15:51Z

**Summary**: We perform the first direct cosmological and astrophysical parameter inference from the combination of galaxy luminosity functions and colours using a simulation based inference approach. Using the Synthesizer code we simulate the dust attenuated ultraviolet--near infrared stellar emission from galaxies in thousands of cosmological hydrodynamic simulations from the CAMELS suite, including the Swift-EAGLE, Illustris-TNG, Simba & Astrid galaxy formation models. For each galaxy we calculate the rest-frame luminosity in a number of photometric bands, including the SDSS $\textit{ugriz}$ and GALEX FUV & NUV filters; this dataset represents the largest catalogue of synthetic photometry based on hydrodynamic galaxy formation simulations produced to date, totalling >200 million sources. From these we compile luminosity functions and colour distributions, and find clear dependencies on both cosmology and feedback. We then perform simulation based (likelihood-free) inference using these distributions, and obtain constraints on both cosmological and astrophysical parameters. Both colour distributions and luminosity functions provide complementary information on certain parameters when performing inference. Most interestingly we achieve constraints on $\sigma_8$, describing the clustering of matter. This is attributable to the fact that the photometry encodes the star formation--metal enrichment history of each galaxy; galaxies in a universe with a higher $\sigma_8$ tend to form earlier and have higher metallicities, which leads to redder colours. We find that a model trained on one galaxy formation simulation generalises poorly when applied to another, and attribute this to differences in the subgrid prescriptions, and lack of flexibility in our emission modelling. The photometric catalogues are publicly available at: https://camels.readthedocs.io/ .

**Link**: [arxiv](http://arxiv.org/abs/2411.13960v1),  [pdf](http://arxiv.org/pdf/2411.13960v1)

**Tags**: astro-ph.GA astro-ph.CO 



### Privacy-Aware Data Acquisition under Data Similarity in Regression   Markets
**Authors**: Shashi Raj Pandey, Pierre Pinson, Petar Popovski

**Updated**: 2024-11-21T08:57:44Z

**Summary**: Data markets facilitate decentralized data exchange for applications such as prediction, learning, or inference. The design of these markets is challenged by varying privacy preferences as well as data similarity among data owners. Related works have often overlooked how data similarity impacts pricing and data value through statistical information leakage. We demonstrate that data similarity and privacy preferences are integral to market design and propose a query-response protocol using local differential privacy for a two-party data acquisition mechanism. In our regression data market model, we analyze strategic interactions between privacy-aware owners and the learner as a Stackelberg game over the asked price and privacy factor. Finally, we numerically evaluate how data similarity affects market participation and traded data value.

**Link**: [arxiv](http://arxiv.org/abs/2312.02611v2),  [pdf](http://arxiv.org/pdf/2312.02611v2)

**Tags**: cs.LG cs.CR cs.GT 



### Engagement-Driven Content Generation with Large Language Models
**Authors**: Erica Coppolillo, Federico Cinus, Marco Minici, Francesco Bonchi, Giuseppe Manco

**Updated**: 2024-11-21T08:56:57Z

**Summary**: Large Language Models (LLMs) exhibit significant persuasion capabilities in one-on-one interactions, but their influence within social networks remains underexplored. This study investigates the potential social impact of LLMs in these environments, where interconnected users and complex opinion dynamics pose unique challenges. In particular, we address the following research question: can LLMs learn to generate meaningful content that maximizes user engagement on social networks?   To answer this question, we define a pipeline to guide the LLM-based content generation which employs reinforcement learning with simulated feedback. In our framework, the reward is based on an engagement model borrowed from the literature on opinion dynamics and information propagation. Moreover, we force the text generated by the LLM to be aligned with a given topic and to satisfy a minimum fluency requirement.   Using our framework, we analyze the capabilities and limitations of LLMs in tackling the given task, specifically considering the relative positions of the LLM as an agent within the social network and the distribution of opinions in the network on the given topic. Our findings show the full potential of LLMs in creating social engagement. Notable properties of our approach are that the learning procedure is adaptive to the opinion distribution of the underlying network and agnostic to the specifics of the engagement model, which is embedded as a plug-and-play component. In this regard, our approach can be easily refined for more complex engagement tasks and interventions in computational social science.   The code used for the experiments is publicly available at https://anonymous.4open.science/r/EDCG/.

**Link**: [arxiv](http://arxiv.org/abs/2411.13187v2),  [pdf](http://arxiv.org/pdf/2411.13187v2)

**Tags**: cs.LG cs.AI 



### LLMs as Continuous Learners: Improving the Reproduction of Defective   Code in Software Issues
**Authors**: Yalan Lin, Yingwei Ma, Rongyu Cao, Binhua Li, Fei Huang, Xiaodong Gu, Yongbin Li

**Updated**: 2024-11-21T08:49:23Z

**Summary**: Reproducing buggy code is the first and crucially important step in issue resolving, as it aids in identifying the underlying problems and validating that generated patches resolve the problem. While numerous approaches have been proposed for this task, they primarily address common, widespread errors and struggle to adapt to unique, evolving errors specific to individual code repositories. To fill this gap, we propose EvoCoder, a multi-agent continuous learning framework for issue code reproduction. EvoCoder adopts a reflection mechanism that allows the LLM to continuously learn from previously resolved problems and dynamically refine its strategies to new emerging challenges. To prevent experience bloating, EvoCoder introduces a novel hierarchical experience pool that enables the model to adaptively update common and repo-specific experiences. Our experimental results show a 20\% improvement in issue reproduction rates over existing SOTA methods. Furthermore, integrating our reproduction mechanism significantly boosts the overall accuracy of the existing issue-resolving pipeline.

**Link**: [arxiv](http://arxiv.org/abs/2411.13941v1),  [pdf](http://arxiv.org/pdf/2411.13941v1)

**Tags**: cs.SE cs.AI 



### REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain   Question Answering
**Authors**: Yuhao Wang, Ruiyang Ren, Junyi Li, Wayne Xin Zhao, Jing Liu, Ji-Rong Wen

**Updated**: 2024-11-21T08:44:20Z

**Summary**: Considering the limited internal parametric knowledge, retrieval-augmented generation (RAG) has been widely used to extend the knowledge scope of large language models (LLMs). Despite the extensive efforts on RAG research, in existing methods, LLMs cannot precisely assess the relevance of retrieved documents, thus likely leading to misleading or even incorrect utilization of external knowledge (eg., retrieved documents). To address this issue, in this paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for open-domain question answering (QA). As the key motivation, we aim to enhance the self-awareness regarding the reliability of external knowledge for LLMs, so as to adaptively utilize external knowledge in RAG systems. Specially, we develop a novel architecture for LLM-based RAG systems, by incorporating a specially designed assessment module that precisely assesses the relevance of retrieved documents. Furthermore, we propose an improved training method based on bi-granularity relevance fusion and noise-resistant training. By combining the improvements in both architecture and training, our proposed REAR can better utilize external knowledge by effectively perceiving the relevance of retrieved documents. Experiments on four open-domain QA tasks show that REAR significantly outperforms previous a number of competitive RAG approaches. Our codes can be accessed at https://github.com/RUCAIBox/REAR.

**Link**: [arxiv](http://arxiv.org/abs/2402.17497v2),  [pdf](http://arxiv.org/pdf/2402.17497v2)

**Tags**: cs.CL cs.IR 



### F2PAD: A General Optimization Framework for Feature-Level to Pixel-Level   Anomaly Detection
**Authors**: Chengyu Tao, Hao Xu, Juan Du

**Updated**: 2024-11-21T08:31:49Z

**Summary**: Image-based inspection systems have been widely deployed in manufacturing production lines. Due to the scarcity of defective samples, unsupervised anomaly detection that only leverages normal samples during training to detect various defects is popular. Existing feature-based methods, utilizing deep features from pretrained neural networks, show their impressive performance in anomaly localization and the low demand for the sample size for training. However, the detected anomalous regions of these methods always exhibit inaccurate boundaries, which impedes the downstream tasks. This deficiency is caused: (i) The decreased resolution of high-level features compared with the original image, and (ii) The mixture of adjacent normal and anomalous pixels during feature extraction. To address them, we propose a novel unified optimization framework (F2PAD) that leverages the Feature-level information to guide the optimization process for Pixel-level Anomaly Detection in the inference stage. The proposed framework is universal and plug-and-play, which can enhance various feature-based methods with limited assumptions. Case studies are provided to demonstrate the effectiveness of our strategy, particularly when applied to three popular backbone methods: PaDiM, CFLOW-AD, and PatchCore.

**Link**: [arxiv](http://arxiv.org/abs/2407.06519v2),  [pdf](http://arxiv.org/pdf/2407.06519v2)

**Tags**: eess.IV 



### XAgents: A Framework for Interpretable Rule-Based Multi-Agents   Cooperation
**Authors**: Hailong Yang, Mingxian Gu, Renhuo Zhao, Fuping Hu, Zhaohong Deng, Yitang Chen

**Updated**: 2024-11-21T08:28:27Z

**Summary**: Extracting implicit knowledge and logical reasoning abilities from large language models (LLMs) has consistently been a significant challenge. The advancement of multi-agent systems has further en-hanced the capabilities of LLMs. Inspired by the structure of multi-polar neurons (MNs), we propose the XAgents framework, an in-terpretable multi-agent cooperative framework based on the IF-THEN rule-based system. The IF-Parts of the rules are responsible for logical reasoning and domain membership calculation, while the THEN-Parts are comprised of domain expert agents that generate domain-specific contents. Following the calculation of the member-ship, XAgetns transmits the task to the disparate domain rules, which subsequently generate the various responses. These re-sponses are analogous to the answers provided by different experts to the same question. The final response is reached at by eliminat-ing the hallucinations and erroneous knowledge of the LLM through membership computation and semantic adversarial genera-tion of the various domain rules. The incorporation of rule-based interpretability serves to bolster user confidence in the XAgents framework. We evaluate the efficacy of XAgents through a com-parative analysis with the latest AutoAgents, in which XAgents demonstrated superior performance across three distinct datasets. We perform post-hoc interpretable studies with SHAP algorithm and case studies, proving the interpretability of XAgent in terms of input-output feature correlation and rule-based semantics.

**Link**: [arxiv](http://arxiv.org/abs/2411.13932v1),  [pdf](http://arxiv.org/pdf/2411.13932v1)

**Tags**: cs.AI cs.MA 



### Quantization without Tears
**Authors**: Minghao Fu, Hao Yu, Jie Shao, Junjie Zhou, Ke Zhu, Jianxin Wu

**Updated**: 2024-11-21T08:13:24Z

**Summary**: Deep neural networks, while achieving remarkable success across diverse tasks, demand significant resources, including computation, GPU memory, bandwidth, storage, and energy. Network quantization, as a standard compression and acceleration technique, reduces storage costs and enables potential inference acceleration by discretizing network weights and activations into a finite set of integer values. However, current quantization methods are often complex and sensitive, requiring extensive task-specific hyperparameters, where even a single misconfiguration can impair model performance, limiting generality across different models and tasks. In this paper, we propose Quantization without Tears (QwT), a method that simultaneously achieves quantization speed, accuracy, simplicity, and generality. The key insight of QwT is to incorporate a lightweight additional structure into the quantized network to mitigate information loss during quantization. This structure consists solely of a small set of linear layers, keeping the method simple and efficient. More importantly, it provides a closed-form solution, allowing us to improve accuracy effortlessly under 2 minutes. Extensive experiments across various vision, language, and multimodal tasks demonstrate that QwT is both highly effective and versatile. In fact, our approach offers a robust solution for network quantization that combines simplicity, accuracy, and adaptability, which provides new insights for the design of novel quantization paradigms.

**Link**: [arxiv](http://arxiv.org/abs/2411.13918v1),  [pdf](http://arxiv.org/pdf/2411.13918v1)

**Tags**: cs.CV 



### Wireless 6G Connectivity for Massive Number of Devices and Critical   Services
**Authors**: Anders E. Kalør, Giuseppe Durisi, Sinem Coleri, Stefan Parkvall, Wei Yu, Andreas Mueller, Petar Popovski

**Updated**: 2024-11-21T07:58:13Z

**Summary**: Compared to the generations up to 4G, whose main focus was on broadband and coverage aspects, 5G has expanded the scope of wireless cellular systems towards embracing two new types of connectivity: massive machine-type communication (mMTC) and ultra-reliable low-latency communications (URLLC). This paper discusses the possible evolution of these two types of connectivity within the umbrella of 6G wireless systems. The paper consists of three parts. The first part deals with the connectivity for a massive number of devices. While mMTC research in 5G predominantly focuses on the problem of uncoordinated access in the uplink for a large number of devices, the traffic patterns in 6G may become more symmetric, leading to closed-loop massive connectivity. One of the drivers for this is distributed learning/inference. The second part of the paper discusses the evolution of wireless connectivity for critical services. While latency and reliability are tightly coupled in 5G, 6G will support a variety of safety critical control applications with different types of timing requirements, as evidenced by the emergence of metrics related to information freshness and information value. Additionally, ensuring ultra-high reliability for safety critical control applications requires modeling and estimation of the tail statistics of the wireless channel, queue length, and delay. The fulfillment of these stringent requirements calls for the development of novel AI-based techniques, incorporating optimization theory, explainable AI, generative AI and digital twins. The third part analyzes the coexistence of massive connectivity and critical services. We will consider scenarios in which a massive number of devices need to support traffic patterns of mixed criticality. This is followed by a discussion about the management of wireless resources shared by services with different criticality.

**Link**: [arxiv](http://arxiv.org/abs/2401.01127v5),  [pdf](http://arxiv.org/pdf/2401.01127v5)

**Tags**: cs.IT math.IT 



### Panther: Illuminate the Sight of Multimodal LLMs with Instruction-Guided   Visual Prompts
**Authors**: Honglin Li, Yuting Gao, Chenglu Zhu, Jingdong Chen, Ming Yang, Lin Yang

**Updated**: 2024-11-21T07:47:27Z

**Summary**: Multimodal large language models (MLLMs) are closing the gap to human visual perception capability rapidly, while, still lag behind on attending to subtle images details or locating small objects precisely, etc. Common schemes to tackle these issues include deploying multiple vision encoders or operating on original high-resolution images. Few studies have concentrated on taking the textual instruction into improving visual representation, resulting in losing focus in some vision-centric tasks, a phenomenon we herein termed as Amblyopia. In this work, we introduce Panther, a MLLM that closely adheres to user instruction and locates targets of interests precisely, with the finesse of a black panther. Specifically, Panther comprises three integral components: Panther-VE, Panther-Bridge, and Panther-Decoder. Panther-VE integrates user instruction information at the early stages of the vision encoder, thereby extracting the most relevant and useful visual representations. The Panther-Bridge module, equipped with powerful filtering capabilities, significantly reduces redundant visual information, leading to a substantial savings in training costs. The Panther-Decoder is versatile and can be employed with any decoder-only architecture of LLMs without discrimination. Experimental results, particularly on vision-centric benchmarks, have demonstrated the effectiveness of Panther.

**Link**: [arxiv](http://arxiv.org/abs/2411.13909v1),  [pdf](http://arxiv.org/pdf/2411.13909v1)

**Tags**: cs.CV 



### Towards Full Delegation: Designing Ideal Agentic Behaviors for Travel   Planning
**Authors**: Song Jiang, Da JU, Andrew Cohen, Sasha Mitts, Aaron Foss, Justine T Kao, Xian Li, Yuandong Tian

**Updated**: 2024-11-21T07:30:02Z

**Summary**: How are LLM-based agents used in the future? While many of the existing work on agents has focused on improving the performance of a specific family of objective and challenging tasks, in this work, we take a different perspective by thinking about full delegation: agents take over humans' routine decision-making processes and are trusted by humans to find solutions that fit people's personalized needs and are adaptive to ever-changing context. In order to achieve such a goal, the behavior of the agents, i.e., agentic behaviors, should be evaluated not only on their achievements (i.e., outcome evaluation), but also how they achieved that (i.e., procedure evaluation). For this, we propose APEC Agent Constitution, a list of criteria that an agent should follow for good agentic behaviors, including Accuracy, Proactivity, Efficiency and Credibility. To verify whether APEC aligns with human preferences, we develop APEC-Travel, a travel planning agent that proactively extracts hidden personalized needs via multi-round dialog with travelers. APEC-Travel is constructed purely from synthetic data generated by Llama3.1-405B-Instruct with a diverse set of travelers' persona to simulate rich distribution of dialogs. Iteratively fine-tuned to follow APEC Agent Constitution, APEC-Travel surpasses baselines by 20.7% on rule-based metrics and 9.1% on LLM-as-a-Judge scores across the constitution axes.

**Link**: [arxiv](http://arxiv.org/abs/2411.13904v1),  [pdf](http://arxiv.org/pdf/2411.13904v1)

**Tags**: cs.CL 



### PIORS: Personalized Intelligent Outpatient Reception based on Large   Language Model with Multi-Agents Medical Scenario Simulation
**Authors**: Zhijie Bao, Qingyun Liu, Ying Guo, Zhengqiang Ye, Jun Shen, Shirong Xie, Jiajie Peng, Xuanjing Huang, Zhongyu Wei

**Updated**: 2024-11-21T07:28:07Z

**Summary**: In China, receptionist nurses face overwhelming workloads in outpatient settings, limiting their time and attention for each patient and ultimately reducing service quality. In this paper, we present the Personalized Intelligent Outpatient Reception System (PIORS). This system integrates an LLM-based reception nurse and a collaboration between LLM and hospital information system (HIS) into real outpatient reception setting, aiming to deliver personalized, high-quality, and efficient reception services. Additionally, to enhance the performance of LLMs in real-world healthcare scenarios, we propose a medical conversational data generation framework named Service Flow aware Medical Scenario Simulation (SFMSS), aiming to adapt the LLM to the real-world environments and PIORS settings. We evaluate the effectiveness of PIORS and SFMSS through automatic and human assessments involving 15 users and 15 clinical experts. The results demonstrate that PIORS-Nurse outperforms all baselines, including the current state-of-the-art model GPT-4o, and aligns with human preferences and clinical needs. Further details and demo can be found at https://github.com/FudanDISC/PIORS

**Link**: [arxiv](http://arxiv.org/abs/2411.13902v1),  [pdf](http://arxiv.org/pdf/2411.13902v1)

**Tags**: cs.CL cs.AI 



### Structure-Based Molecule Optimization via Gradient-Guided Bayesian   Update
**Authors**: Keyue Qiu, Yuxuan Song, Jie Yu, Hongbo Ma, Ziyao Cao, Zhilong Zhang, Yushuai Wu, Mingyue Zheng, Hao Zhou, Wei-Ying Ma

**Updated**: 2024-11-21T07:23:37Z

**Summary**: Structure-based molecule optimization (SBMO) aims to optimize molecules with both continuous coordinates and discrete types against protein targets. A promising direction is to exert gradient guidance on generative models given its remarkable success in images, but it is challenging to guide discrete data and risks inconsistencies between modalities. To this end, we leverage a continuous and differentiable space derived through Bayesian inference, presenting Molecule Joint Optimization (MolJO), the first gradient-based SBMO framework that facilitates joint guidance signals across different modalities while preserving SE(3)-equivariance. We introduce a novel backward correction strategy that optimizes within a sliding window of the past histories, allowing for a seamless trade-off between explore-and-exploit during optimization. Our proposed MolJO achieves state-of-the-art performance on CrossDocked2020 benchmark (Success Rate 51.3% , Vina Dock -9.05 and SA 0.78), more than 4x improvement in Success Rate compared to the gradient-based counterpart, and 2x "Me-Better" Ratio as much as 3D baselines. Furthermore, we extend MolJO to a wide range of optimization settings, including multi-objective optimization and challenging tasks in drug design such as R-group optimization and scaffold hopping, further underscoring its versatility and potential.

**Link**: [arxiv](http://arxiv.org/abs/2411.13280v2),  [pdf](http://arxiv.org/pdf/2411.13280v2)

**Tags**: q-bio.BM cs.AI 



### Schemato -- An LLM for Netlist-to-Schematic Conversion
**Authors**: Ryoga Matsuo, Stefan Uhlich, Arun Venkitaraman, Andrea Bonetti, Chia-Yu Hsieh, Ali Momeni, Lukas Mauch, Augusto Capone, Eisaku Ohbuchi, Lorenzo Servadei

**Updated**: 2024-11-21T07:21:59Z

**Summary**: Machine learning models are advancing circuit design, particularly in analog circuits. They typically generate netlists that lack human interpretability. This is a problem as human designers heavily rely on the interpretability of circuit diagrams or schematics to intuitively understand, troubleshoot, and develop designs. Hence, to integrate domain knowledge effectively, it is crucial to translate ML-generated netlists into interpretable schematics quickly and accurately. We propose Schemato, a large language model (LLM) for netlist-to-schematic conversion. In particular, we consider our approach in the two settings of converting netlists to .asc files for LTSpice and LATEX files for CircuiTikz schematics. Experiments on our circuit dataset show that Schemato achieves up to 93% compilation success rate for the netlist-to-LaTeX conversion task, surpassing the 26% rate scored by the state-of-the-art LLMs. Furthermore, our experiments show that Schemato generates schematics with a mean structural similarity index measure that is 3xhigher than the best performing LLMs, therefore closer to the reference human design.

**Link**: [arxiv](http://arxiv.org/abs/2411.13899v1),  [pdf](http://arxiv.org/pdf/2411.13899v1)

**Tags**: cs.LG 



### LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level   Mathematical Reasoning
**Authors**: Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco Pavone, Yuqiang Li, Wanli Ouyang, Dongzhan Zhou

**Updated**: 2024-11-21T07:07:59Z

**Summary**: This paper presents an advanced mathematical problem-solving framework, LLaMA-Berry, for enhancing the mathematical reasoning ability of Large Language Models (LLMs). The framework combines Monte Carlo Tree Search (MCTS) with iterative Self-Refine to optimize the reasoning path and utilizes a pairwise reward model to evaluate different paths globally. By leveraging the self-critic and rewriting capabilities of LLMs, Self-Refine applied to MCTS (SR-MCTS) overcomes the inefficiencies and limitations of conventional step-wise and greedy search algorithms by fostering a more efficient exploration of solution spaces. Pairwise Preference Reward Model~(PPRM), inspired by Reinforcement Learning from Human Feedback (RLHF), is then used to model pairwise preferences between solutions, utilizing an Enhanced Borda Count (EBC) method to synthesize these preferences into a global ranking score to find better answers. This approach addresses the challenges of scoring variability and non-independent distributions in mathematical reasoning tasks. The framework has been tested on general and advanced benchmarks, showing superior performance in terms of search efficiency and problem-solving capability compared to existing methods like ToT and rStar, particularly in complex Olympiad-level benchmarks, including GPQA, AIME24 and AMC23.

**Link**: [arxiv](http://arxiv.org/abs/2410.02884v2),  [pdf](http://arxiv.org/pdf/2410.02884v2)

**Tags**: cs.AI cs.CL 



### PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity   Recognition
**Authors**: Jinghui Lu, Ziwei Yang, Yanjie Wang, Xuejing Liu, Brian Mac Namee, Can Huang

**Updated**: 2024-11-21T06:52:02Z

**Summary**: In this study, we aim to reduce generation latency for Named Entity Recognition (NER) with Large Language Models (LLMs). The main cause of high latency in LLMs is the sequential decoding process, which autoregressively generates all labels and mentions for NER, significantly increase the sequence length. To this end, we introduce Parallel Decoding in LLM for NE} (PaDeLLM-NER), a approach that integrates seamlessly into existing generative model frameworks without necessitating additional modules or architectural modifications. PaDeLLM-NER allows for the simultaneous decoding of all mentions, thereby reducing generation latency. Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese. Simultaneously it maintains the quality of predictions as evidenced by the performance that is on par with the state-of-the-art across various datasets.

**Link**: [arxiv](http://arxiv.org/abs/2402.04838v5),  [pdf](http://arxiv.org/pdf/2402.04838v5)

**Tags**: cs.CL cs.AI 



### t-READi: Transformer-Powered Robust and Efficient Multimodal Inference   for Autonomous Driving
**Authors**: Pengfei Hu, Yuhang Qian, Tianyue Zheng, Ang Li, Zhe Chen, Yue Gao, Xiuzhen Cheng, Jun Luo

**Updated**: 2024-11-21T06:46:57Z

**Summary**: Given the wide adoption of multimodal sensors (e.g., camera, lidar, radar) by autonomous vehicles (AVs), deep analytics to fuse their outputs for a robust perception become imperative. However, existing fusion methods often make two assumptions rarely holding in practice: i) similar data distributions for all inputs and ii) constant availability for all sensors. Because, for example, lidars have various resolutions and failures of radars may occur, such variability often results in significant performance degradation in fusion. To this end, we present tREADi, an adaptive inference system that accommodates the variability of multimodal sensory data and thus enables robust and efficient perception. t-READi identifies variation-sensitive yet structure-specific model parameters; it then adapts only these parameters while keeping the rest intact. t-READi also leverages a cross-modality contrastive learning method to compensate for the loss from missing modalities. Both functions are implemented to maintain compatibility with existing multimodal deep fusion methods. The extensive experiments evidently demonstrate that compared with the status quo approaches, t-READi not only improves the average inference accuracy by more than 6% but also reduces the inference latency by almost 15x with the cost of only 5% extra memory overhead in the worst case under realistic data and modal variations.

**Link**: [arxiv](http://arxiv.org/abs/2410.09747v3),  [pdf](http://arxiv.org/pdf/2410.09747v3)

**Tags**: cs.CV cs.AI cs.DC cs.LG cs.RO 



### Interaction of an outflow with surrounding gaseous clouds as the origin   of the late-time radio flares in TDEs
**Authors**: Jialun Zhuang, Rong-Feng Shen, Guobin Mou, Wenbin Lu

**Updated**: 2024-11-21T06:38:54Z

**Summary**: Close encounter between a star and a supermassive black hole (SMBH) results in the tidal disruption of the star, known as a tidal disruption event (TDE). Recently, a few TDEs, e.g., ASASSN-15oi and AT2018hyz, have shown late-time (hundreds of days after their UV/optical peaks) radio flares with radio luminosities of $10^{38\sim39}$ erg/s. The super-Eddington fallback or accretion in a TDE may generate a mass outflow. Here we investigate a scenario that the late-time radio flares come from the interaction of the outflow with the circum-nuclear gaseous clouds, in addition to the slow-evolving emission component due to the outflow-diffuse medium interaction. We calculate the associated radio temporal and spectral signatures and find that they reproduce well the observations. The outflows have the inferred velocity of 0.2$c\sim0.6$$c$, the total mass of $10^{-3}\sim10^{-1}$ $\mathrm{M_{\odot}}$ and the ejection duration of a month to a year. The distances of the clouds to the SMBH are $0.1\sim1$ pc. This scenario has advantages in explaining the long delay, sharpness of the rise and the multiplicity of the late radio flares. Future observations may build up a much larger sample of late-time radio flares and enable their use as a probe of the TDE physics and the host circumnuclear environment.

**Link**: [arxiv](http://arxiv.org/abs/2406.08012v3),  [pdf](http://arxiv.org/pdf/2406.08012v3)

**Tags**: astro-ph.HE 



### Next-Generation Phishing: How LLM Agents Empower Cyber Attackers
**Authors**: Khalifa Afane, Wenqi Wei, Ying Mao, Junaid Farooq, Juntao Chen

**Updated**: 2024-11-21T06:20:29Z

**Summary**: The escalating threat of phishing emails has become increasingly sophisticated with the rise of Large Language Models (LLMs). As attackers exploit LLMs to craft more convincing and evasive phishing emails, it is crucial to assess the resilience of current phishing defenses. In this study we conduct a comprehensive evaluation of traditional phishing detectors, such as Gmail Spam Filter, Apache SpamAssassin, and Proofpoint, as well as machine learning models like SVM, Logistic Regression, and Naive Bayes, in identifying both traditional and LLM-rephrased phishing emails. We also explore the emerging role of LLMs as phishing detection tools, a method already adopted by companies like NTT Security Holdings and JPMorgan Chase. Our results reveal notable declines in detection accuracy for rephrased emails across all detectors, highlighting critical weaknesses in current phishing defenses. As the threat landscape evolves, our findings underscore the need for stronger security controls and regulatory oversight on LLM-generated content to prevent its misuse in creating advanced phishing attacks. This study contributes to the development of more effective Cyber Threat Intelligence (CTI) by leveraging LLMs to generate diverse phishing variants that can be used for data augmentation, harnessing the power of LLMs to enhance phishing detection, and paving the way for more robust and adaptable threat detection systems.

**Link**: [arxiv](http://arxiv.org/abs/2411.13874v1),  [pdf](http://arxiv.org/pdf/2411.13874v1)

**Tags**: cs.CR cs.AI 



### Design2Code: Benchmarking Multimodal Code Generation for Automated   Front-End Engineering
**Authors**: Chenglei Si, Yanzhe Zhang, Ryan Li, Zhengyuan Yang, Ruibo Liu, Diyi Yang

**Updated**: 2024-11-21T06:18:07Z

**Summary**: Generative AI has made rapid advancements in recent years, achieving unprecedented capabilities in multimodal understanding and code generation. This can enable a new paradigm of front-end development in which multimodal large language models (MLLMs) directly convert visual designs into code implementations. In this work, we construct Design2Code - the first real-world benchmark for this task. Specifically, we manually curate 484 diverse real-world webpages as test cases and develop a set of automatic evaluation metrics to assess how well current multimodal LLMs can generate the code implementations that directly render into the given reference webpages, given the screenshots as input. We also complement automatic metrics with comprehensive human evaluations to validate the performance ranking. To rigorously benchmark MLLMs, we test various multimodal prompting methods on frontier models such as GPT-4o, GPT-4V, Gemini, and Claude. Our fine-grained break-down metrics indicate that models mostly lag in recalling visual elements from the input webpages and generating correct layout designs.

**Link**: [arxiv](http://arxiv.org/abs/2403.03163v2),  [pdf](http://arxiv.org/pdf/2403.03163v2)

**Tags**: cs.CL cs.CV cs.CY 



### High Risk of Political Bias in Black Box Emotion Inference Models
**Authors**: Hubert Plisiecki, Paweł Lenartowicz, Maria Flakus, Artur Pokropek

**Updated**: 2024-11-21T06:07:13Z

**Summary**: This paper investigates the presence of political bias in emotion inference models used for sentiment analysis (SA) in social science research. Machine learning models often reflect biases in their training data, impacting the validity of their outcomes. While previous research has highlighted gender and race biases, our study focuses on political bias - an underexplored yet pervasive issue that can skew the interpretation of text data across a wide array of studies. We conducted a bias audit on a Polish sentiment analysis model developed in our lab. By analyzing valence predictions for names and sentences involving Polish politicians, we uncovered systematic differences influenced by political affiliations. Our findings indicate that annotations by human raters propagate political biases into the model's predictions. To mitigate this, we pruned the training dataset of texts mentioning these politicians and observed a reduction in bias, though not its complete elimination. Given the significant implications of political bias in SA, our study emphasizes caution in employing these models for social science research. We recommend a critical examination of SA results and propose using lexicon-based systems as a more ideologically neutral alternative. This paper underscores the necessity for ongoing scrutiny and methodological adjustments to ensure the reliability and impartiality of the use of machine learning in academic and applied contexts.

**Link**: [arxiv](http://arxiv.org/abs/2407.13891v2),  [pdf](http://arxiv.org/pdf/2407.13891v2)

**Tags**: cs.CL cs.AI 



## Keyword: LLM Deployment 
 ### Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large   Language Models
**Authors**: Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, Ziwei Liu

**Updated**: 2024-11-21T18:59:55Z

**Summary**: Large Language Models (LLMs) demonstrate enhanced capabilities and reliability by reasoning more, evolving from Chain-of-Thought prompting to product-level solutions like OpenAI o1. Despite various efforts to improve LLM reasoning, high-quality long-chain reasoning data and optimized training pipelines still remain inadequately explored in vision-language tasks. In this paper, we present Insight-V, an early effort to 1) scalably produce long and robust reasoning data for complex multi-modal tasks, and 2) an effective training pipeline to enhance the reasoning capabilities of multi-modal large language models (MLLMs). Specifically, to create long and structured reasoning data without human labor, we design a two-step pipeline with a progressive strategy to generate sufficiently long and diverse reasoning paths and a multi-granularity assessment method to ensure data quality. We observe that directly supervising MLLMs with such long and complex reasoning data will not yield ideal reasoning ability. To tackle this problem, we design a multi-agent system consisting of a reasoning agent dedicated to performing long-chain reasoning and a summary agent trained to judge and summarize reasoning results. We further incorporate an iterative DPO algorithm to enhance the reasoning agent's generation stability and quality. Based on the popular LLaVA-NeXT model and our stronger base MLLM, we demonstrate significant performance gains across challenging multi-modal benchmarks requiring visual reasoning. Benefiting from our multi-agent system, Insight-V can also easily maintain or improve performance on perception-focused multi-modal tasks.

**Link**: [arxiv](http://arxiv.org/abs/2411.14432v1),  [pdf](http://arxiv.org/pdf/2411.14432v1)

**Tags**: cs.CV 



### Lightweight Safety Guardrails Using Fine-tuned BERT Embeddings
**Authors**: Aaron Zheng, Mansi Rana, Andreas Stolcke

**Updated**: 2024-11-21T18:27:25Z

**Summary**: With the recent proliferation of large language models (LLMs), enterprises have been able to rapidly develop proof-of-concepts and prototypes. As a result, there is a growing need to implement robust guardrails that monitor, quantize and control an LLM's behavior, ensuring that the use is reliable, safe, accurate and also aligned with the users' expectations. Previous approaches for filtering out inappropriate user prompts or system outputs, such as LlamaGuard and OpenAI's MOD API, have achieved significant success by fine-tuning existing LLMs. However, using fine-tuned LLMs as guardrails introduces increased latency and higher maintenance costs, which may not be practical or scalable for cost-efficient deployments. We take a different approach, focusing on fine-tuning a lightweight architecture: Sentence-BERT. This method reduces the model size from LlamaGuard's 7 billion parameters to approximately 67 million, while maintaining comparable performance on the AEGIS safety benchmark.

**Link**: [arxiv](http://arxiv.org/abs/2411.14398v1),  [pdf](http://arxiv.org/pdf/2411.14398v1)

**Tags**: cs.CL 



### Localizing Events in Videos with Multimodal Queries
**Authors**: Gengyuan Zhang, Mang Ling Ada Fok, Jialu Ma, Yan Xia, Daniel Cremers, Philip Torr, Volker Tresp, Jindong Gu

**Updated**: 2024-11-21T17:58:55Z

**Summary**: Localizing events in videos based on semantic queries is a pivotal task in video understanding, with the growing significance of user-oriented applications like video search. Yet, current research predominantly relies on natural language queries (NLQs), overlooking the potential of using multimodal queries (MQs) that integrate images to more flexibly represent semantic queries -- especially when it is difficult to express non-verbal or unfamiliar concepts in words. To bridge this gap, we introduce ICQ, a new benchmark designed for localizing events in videos with MQs, alongside an evaluation dataset ICQ-Highlight. To accommodate and evaluate existing video localization models for this new task, we propose 3 Multimodal Query Adaptation methods and a novel Surrogate Fine-tuning on pseudo-MQs strategy. ICQ systematically benchmarks 12 state-of-the-art backbone models, spanning from specialized video localization models to Video LLMs, across diverse application domains. Our experiments highlight the high potential of MQs in real-world applications. We believe this benchmark is a first step toward advancing MQs in video event localization.

**Link**: [arxiv](http://arxiv.org/abs/2406.10079v3),  [pdf](http://arxiv.org/pdf/2406.10079v3)

**Tags**: cs.CV cs.AI 



### LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with   LLM Token Embeddings
**Authors**: Duo Wang, Yuan Zuo, Fengzhi Li, Junjie Wu

**Updated**: 2024-11-21T17:48:25Z

**Summary**: Zero-shot graph machine learning, especially with graph neural networks (GNNs), has garnered significant interest due to the challenge of scarce labeled data. While methods like self-supervised learning and graph prompt learning have been extensively explored, they often rely on fine-tuning with task-specific labels, limiting their effectiveness in zero-shot scenarios. Inspired by the zero-shot capabilities of instruction-fine-tuned large language models (LLMs), we introduce a novel framework named Token Embedding-Aligned Graph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and cross-task zero-shot learners for graph machine learning. Concretely, we pretrain a GNN, aligning its representations with token embeddings of an LLM. We then train a linear projector that transforms the GNN's representations into a fixed number of graph token embeddings without tuning the LLM. A unified instruction is designed for various graph tasks at different levels, such as node classification (node-level) and link prediction (edge-level). These design choices collectively enhance our method's effectiveness in zero-shot learning, setting it apart from existing methods. Experiments show that our graph token embeddings help the LLM predictor achieve state-of-the-art performance on unseen datasets and tasks compared to other methods using LLMs as predictors.

**Link**: [arxiv](http://arxiv.org/abs/2408.14512v2),  [pdf](http://arxiv.org/pdf/2408.14512v2)

**Tags**: cs.LG cs.AI cs.CL 



### UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptation of LLMs   on Low-Resource Languages
**Authors**: Bethel Melesse Tessema, Akhil Kedia, Tae-Sun Chung

**Updated**: 2024-11-21T17:41:08Z

**Summary**: Large language models (LLMs) under-perform on low-resource languages due to limited training data. We present a method to efficiently collect text data for low-resource languages from the entire Common Crawl corpus. Our approach, UnifiedCrawl, filters and extracts common crawl using minimal compute resources, yielding mono-lingual datasets much larger than previously available sources. We demonstrate that leveraging this data to fine-tuning multilingual LLMs via efficient adapter methods (QLoRA) significantly boosts performance on the low-resource language, while minimizing VRAM usage. Our experiments show large improvements in language modeling perplexity and an increase in few-shot prompting scores. Our work and released source code provide an affordable approach to improve LLMs for low-resource languages using consumer hardware. Our source code is available here at https://github.com/bethelmelesse/unifiedcrawl.

**Link**: [arxiv](http://arxiv.org/abs/2411.14343v1),  [pdf](http://arxiv.org/pdf/2411.14343v1)

**Tags**: cs.CL cs.AI 68T50 I.2.7 



### Automated Generation of Code Debugging Exercises
**Authors**: Victor-Alexandru Pădurean, Paul Denny, Adish Singla

**Updated**: 2024-11-21T16:56:33Z

**Summary**: Debugging is an essential skill when learning to program, yet its instruction and emphasis often vary widely across introductory courses. In the era of code-generating large language models (LLMs), the ability for students to reason about code and identify errors is increasingly important. However, students frequently resort to trial-and-error methods to resolve bugs without fully understanding the underlying issues. Developing the ability to identify and hypothesize the cause of bugs is crucial but can be time-consuming to teach effectively through traditional means. This paper introduces BugSpotter, an innovative tool that leverages an LLM to generate buggy code from a problem description and verify the synthesized bugs via a test suite. Students interact with BugSpotter by designing failing test cases, where the buggy code's output differs from the expected result as defined by the problem specification. This not only provides opportunities for students to enhance their debugging skills, but also to practice reading and understanding problem specifications. We deployed BugSpotter in a large classroom setting and compared the debugging exercises it generated to exercises hand-crafted by an instructor for the same problems. We found that the LLM-generated exercises produced by BugSpotter varied in difficulty and were well-matched to the problem specifications. Importantly, the LLM-generated exercises were comparable to those manually created by instructors with respect to student performance, suggesting that BugSpotter could be an effective and efficient aid for learning debugging.

**Link**: [arxiv](http://arxiv.org/abs/2411.14303v1),  [pdf](http://arxiv.org/pdf/2411.14303v1)

**Tags**: cs.SE cs.AI 



### Auto-SPICE: Leveraging LLMs for Dataset Creation via Automated SPICE   Netlist Extraction from Analog Circuit Diagrams
**Authors**: Jitendra Bhandari, Vineet Bhat, Yuheng He, Siddharth Garg, Hamed Rahmani, Ramesh Karri

**Updated**: 2024-11-21T16:50:11Z

**Summary**: Auto-SPICE is the first fully automated framework leveraging large language models (LLMs) to generate Simulation Programs with Integrated Circuit Emphasis (SPICE) netlists. It addresses a long-standing challenge in automating netlist generation for analog circuits within circuit design automation. Automating this workflow could accelerate the creation of finetuned LLMs for analog circuit design and verification. We identify key challenges in this automation and evaluate the multi-modal capabilities of state-of-the-art LLMs, particularly GPT-4, to address these issues. We propose a three-step workflow to overcome current limitations: labeling analog circuits, prompt tuning, and netlist verification. This approach aims to create an end-to-end SPICE netlist generator from circuit schematic images, tackling the long-standing hurdle of accurate netlist generation. Our framework demonstrates significant performance improvements, tested on approximately 2,100 schematics of varying complexity. We open-source this solution for community-driven development.

**Link**: [arxiv](http://arxiv.org/abs/2411.14299v1),  [pdf](http://arxiv.org/pdf/2411.14299v1)

**Tags**: cs.AR 



### LLMSteer: Improving Long-Context LLM Inference by Steering Attention on   Reused Contexts
**Authors**: Zhuohan Gu, Jiayi Yao, Kuntai Du, Junchen Jiang

**Updated**: 2024-11-21T16:49:51Z

**Summary**: As large language models (LLMs) show impressive performance on complex tasks, they still struggle with longer contextual understanding and high computational costs. To balance efficiency and quality, we introduce LLMSteer, a fine-tuning-free framework that enhances LLMs through query-independent attention steering. Tested on popular LLMs and datasets, LLMSteer narrows the performance gap with baselines by 65.9% and reduces the runtime delay by up to 4.8x compared to recent attention steering methods.

**Link**: [arxiv](http://arxiv.org/abs/2411.13009v2),  [pdf](http://arxiv.org/pdf/2411.13009v2)

**Tags**: cs.LG cs.CL 



### AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context
**Authors**: Naba Rizvi, Harper Strickland, Daniel Gitelman, Tristan Cooper, Alexis Morales-Flores, Michael Golden, Aekta Kallepalli, Akshat Alurkar, Haaset Owens, Saleha Ahmedi, Isha Khirwadkar, Imani Munyaka, Nedjma Ousidhoum

**Updated**: 2024-11-21T16:43:06Z

**Summary**: As our understanding of autism and ableism continues to increase, so does our understanding of ableist language towards autistic people. Such language poses a significant challenge in NLP research due to its subtle and context-dependent nature. Yet, detecting anti-autistic ableist language remains underexplored, with existing NLP tools often failing to capture its nuanced expressions. We present AUTALIC, the first benchmark dataset dedicated to the detection of anti-autistic ableist language in context, addressing a significant gap in the field. The dataset comprises 2,400 autism-related sentences collected from Reddit, accompanied by surrounding context, and is annotated by trained experts with backgrounds in neurodiversity. Our comprehensive evaluation reveals that current language models, including state-of-the-art LLMs, struggle to reliably identify anti-autistic ableism and align with human judgments, underscoring their limitations in this domain. We publicly release AUTALIC along with the individual annotations which serve as a valuable resource to researchers working on ableism, neurodiversity, and also studying disagreements in annotation tasks. This dataset serves as a crucial step towards developing more inclusive and context-aware NLP systems that better reflect diverse perspectives.

**Link**: [arxiv](http://arxiv.org/abs/2410.16520v2),  [pdf](http://arxiv.org/pdf/2410.16520v2)

**Tags**: cs.CL cs.AI 



### CAIP: Detecting Router Misconfigurations with Context-Aware Iterative   Prompting of LLMs
**Authors**: Xi Jiang, Aaron Gember-Jacobson, Nick Feamster

**Updated**: 2024-11-21T16:34:52Z

**Summary**: Model checkers and consistency checkers detect critical errors in router configurations, but these tools require significant manual effort to develop and maintain. LLM-based Q&A models have emerged as a promising alternative, allowing users to query partitions of configurations through prompts and receive answers based on learned patterns, thanks to transformer models pre-trained on vast datasets that provide generic configuration context for interpreting router configurations. Yet, current methods of partition-based prompting often do not provide enough network-specific context from the actual configurations to enable accurate inference. We introduce a Context-Aware Iterative Prompting (CAIP) framework that automates network-specific context extraction and optimizes LLM prompts for more precise router misconfiguration detection. CAIP addresses three challenges: (1) efficiently mining relevant context from complex configuration files, (2) accurately distinguishing between pre-defined and user-defined parameter values to prevent irrelevant context from being introduced, and (3) managing prompt context overload with iterative, guided interactions with the model. Our evaluations on synthetic and real-world configurations show that CAIP improves misconfiguration detection accuracy by more than 30% compared to partition-based LLM approaches, model checkers, and consistency checkers, uncovering over 20 previously undetected misconfigurations in real-world configurations.

**Link**: [arxiv](http://arxiv.org/abs/2411.14283v1),  [pdf](http://arxiv.org/pdf/2411.14283v1)

**Tags**: cs.NI 



### Looking Beyond Text: Reducing Language bias in Large Vision-Language   Models via Multimodal Dual-Attention and Soft-Image Guidance
**Authors**: Haozhe Zhao, Shuzheng Si, Liang Chen, Yichi Zhang, Maosong Sun, Mingjia Zhang, Baobao Chang

**Updated**: 2024-11-21T16:33:30Z

**Summary**: Large vision-language models (LVLMs) have achieved impressive results in various vision-language tasks. However, despite showing promising performance, LVLMs suffer from hallucinations caused by language bias, leading to diminished focus on images and ineffective visual comprehension. We identify two primary reasons for this bias: 1. Different scales of training data between the pretraining stage of LLM and multimodal alignment stage. 2. The learned inference bias due to short-term dependency of text data. Therefore, we propose LACING, a systemic framework designed to address the language bias of LVLMs with muLtimodal duAl-attention meChanIsm (MDA) aNd soft-image Guidance (IFG). Specifically, MDA introduces a parallel dual-attention mechanism that enhances the integration of visual inputs across the model. IFG introduces a learnable soft visual prompt during training and inference to replace visual inputs, designed to compel LVLMs to prioritize text inputs. Then, IFG further proposes a novel decoding strategy using the soft visual prompt to mitigate the model's over-reliance on adjacent text inputs. Comprehensive experiments demonstrate that our method effectively debiases LVLMs from their language bias, enhancing visual comprehension and reducing hallucinations without requiring additional training resources or data. The code and model are available at [lacing-lvlm.github.io](https://lacing-lvlm.github.io).

**Link**: [arxiv](http://arxiv.org/abs/2411.14279v1),  [pdf](http://arxiv.org/pdf/2411.14279v1)

**Tags**: cs.CV cs.CL 



### Efficient Aspect-Based Summarization of Climate Change Reports with   Small Language Models
**Authors**: Iacopo Ghinassi, Leonardo Catalano, Tommaso Colella

**Updated**: 2024-11-21T16:28:32Z

**Summary**: The use of Natural Language Processing (NLP) for helping decision-makers with Climate Change action has recently been highlighted as a use case aligning with a broader drive towards NLP technologies for social good. In this context, Aspect-Based Summarization (ABS) systems that extract and summarize relevant information are particularly useful as they provide stakeholders with a convenient way of finding relevant information in expert-curated reports. In this work, we release a new dataset for ABS of Climate Change reports and we employ different Large Language Models (LLMs) and so-called Small Language Models (SLMs) to tackle this problem in an unsupervised way. Considering the problem at hand, we also show how SLMs are not significantly worse for the problem while leading to reduced carbon footprint; we do so by applying for the first time an existing framework considering both energy efficiency and task performance to the evaluation of zero-shot generative models for ABS. Overall, our results show that modern language models, both big and small, can effectively tackle ABS for Climate Change reports but more research is needed when we frame the problem as a Retrieval Augmented Generation (RAG) problem and our work and dataset will help foster efforts in this direction.

**Link**: [arxiv](http://arxiv.org/abs/2411.14272v1),  [pdf](http://arxiv.org/pdf/2411.14272v1)

**Tags**: cs.CL I.2.7 



### Linguacodus: A Synergistic Framework for Transformative Code Generation   in Machine Learning Pipelines
**Authors**: Ekaterina Trofimova, Emil Sataev, Andrey E. Ustyuzhanin

**Updated**: 2024-11-21T16:28:03Z

**Summary**: In the ever-evolving landscape of machine learning, seamless translation of natural language descriptions into executable code remains a formidable challenge. This paper introduces Linguacodus, an innovative framework designed to tackle this challenge by deploying a dynamic pipeline that iteratively transforms natural language task descriptions into code through high-level data-shaping instructions. The core of Linguacodus is a fine-tuned large language model (LLM), empowered to evaluate diverse solutions for various problems and select the most fitting one for a given task. This paper details the fine-tuning process, and sheds light on how natural language descriptions can be translated into functional code. Linguacodus represents a substantial leap towards automated code generation, effectively bridging the gap between task descriptions and executable code. It holds great promise for advancing machine learning applications across diverse domains. Additionally, we propose an algorithm capable of transforming a natural description of an ML task into code with minimal human interaction. In extensive experiments on a vast machine learning code dataset originating from Kaggle, we showcase the effectiveness of Linguacodus. The investigations highlight its potential applications across diverse domains, emphasizing its impact on applied machine learning in various scientific fields.

**Link**: [arxiv](http://arxiv.org/abs/2403.11585v3),  [pdf](http://arxiv.org/pdf/2403.11585v3)

**Tags**: cs.LG cs.AI cs.CL cs.PL cs.SE 



### Probabilistically Correct Language-based Multi-Robot Planning using   Conformal Prediction
**Authors**: Jun Wang, Guocheng He, Yiannis Kantaros

**Updated**: 2024-11-21T16:19:35Z

**Summary**: This paper addresses task planning problems for language-instructed robot teams. Tasks are expressed in natural language (NL), requiring the robots to apply their capabilities at various locations and semantic objects. Several recent works have addressed similar planning problems by leveraging pre-trained Large Language Models (LLMs) to design effective multi-robot plans. However, these approaches lack performance guarantees. To address this challenge, we introduce a new distributed LLM-based planner, called S-ATLAS for Safe plAnning for Teams of Language-instructed AgentS, that is capable of achieving user-defined mission success rates. This is accomplished by leveraging conformal prediction (CP), a distribution-free uncertainty quantification tool in black-box models. CP allows the proposed multi-robot planner to reason about its inherent uncertainty in a distributed fashion, enabling robots to make individual decisions when they are sufficiently certain and seek help otherwise. We show, both theoretically and empirically, that the proposed planner can achieve user-specified task success rates, assuming successful plan execution, while minimizing the overall number of help requests. We provide comparative experiments against related works showing that our method is significantly more computational efficient and achieves lower help rates. The advantage of our algorithm over baselines becomes more pronounced with increasing robot team size.

**Link**: [arxiv](http://arxiv.org/abs/2402.15368v4),  [pdf](http://arxiv.org/pdf/2402.15368v4)

**Tags**: cs.RO cs.AI 



### EoRA: Training-free Compensation for Compressed LLM with Eigenspace   Low-Rank Approximation
**Authors**: Shih-Yang Liu, Huck Yang, Chien-Yi Wang, Nai Chit Fung, Hongxu Yin, Charbel Sakr, Saurav Muralidharan, Kwang-Ting Cheng, Jan Kautz, Yu-Chiang Frank Wang, Pavlo Molchanov, Min-Hung Chen

**Updated**: 2024-11-21T16:12:34Z

**Summary**: In this work, we re-formulate the model compression problem into the customized compensation problem: Given a compressed model, we aim to introduce residual low-rank paths to compensate for compression errors under customized requirements from users (e.g., tasks, compression ratios), resulting in greater flexibility in adjusting overall capacity without being constrained by specific compression formats. However, naively applying SVD to derive residual paths causes suboptimal utilization of the low-rank representation capacity. Instead, we propose Training-free Eigenspace Low-Rank Approximation (EoRA), a method that directly minimizes compression-induced errors without requiring gradient-based training, achieving fast optimization in minutes using a small amount of calibration data. EoRA projects compression errors into the eigenspace of input activations, leveraging eigenvalues to effectively prioritize the reconstruction of high-importance error components. Moreover, EoRA can be seamlessly integrated with fine-tuning and quantization to further improve effectiveness and efficiency. EoRA consistently outperforms previous methods in compensating errors for compressed LLaMA2/3 models on various tasks, such as language generation, commonsense reasoning, and math reasoning tasks (e.g., 31.31%/12.88% and 9.69% improvements on ARC-Easy/ARC-Challenge and MathQA when compensating LLaMA3-8B that is quantized to 4-bit and pruned to 2:4 sparsity). EoRA offers a scalable, training-free solution to compensate for compression errors, making it a powerful tool to deploy LLMs in various capacity and efficiency requirements.

**Link**: [arxiv](http://arxiv.org/abs/2410.21271v2),  [pdf](http://arxiv.org/pdf/2410.21271v2)

**Tags**: cs.CL cs.AI 



### Knowledge Graphs, Large Language Models, and Hallucinations: An NLP   Perspective
**Authors**: Ernests Lavrinovics, Russa Biswas, Johannes Bjerva, Katja Hose

**Updated**: 2024-11-21T16:09:05Z

**Summary**: Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) based applications including automated text generation, question answering, chatbots, and others. However, they face a significant challenge: hallucinations, where models produce plausible-sounding but factually incorrect responses. This undermines trust and limits the applicability of LLMs in different domains. Knowledge Graphs (KGs), on the other hand, provide a structured collection of interconnected facts represented as entities (nodes) and their relationships (edges). In recent research, KGs have been leveraged to provide context that can fill gaps in an LLM understanding of certain topics offering a promising approach to mitigate hallucinations in LLMs, enhancing their reliability and accuracy while benefiting from their wide applicability. Nonetheless, it is still a very active area of research with various unresolved open problems. In this paper, we discuss these open challenges covering state-of-the-art datasets and benchmarks as well as methods for knowledge integration and evaluating hallucinations. In our discussion, we consider the current use of KGs in LLM systems and identify future directions within each of these challenges.

**Link**: [arxiv](http://arxiv.org/abs/2411.14258v1),  [pdf](http://arxiv.org/pdf/2411.14258v1)

**Tags**: cs.CL cs.AI 68-02 I.2.7 



### Generalizing End-To-End Autonomous Driving In Real-World Environments   Using Zero-Shot LLMs
**Authors**: Zeyu Dong, Yimin Zhu, Yansong Li, Kevin Mahon, Yu Sun

**Updated**: 2024-11-21T16:04:10Z

**Summary**: Traditional autonomous driving methods adopt a modular design, decomposing tasks into sub-tasks. In contrast, end-to-end autonomous driving directly outputs actions from raw sensor data, avoiding error accumulation. However, training an end-to-end model requires a comprehensive dataset; otherwise, the model exhibits poor generalization capabilities. Recently, large language models (LLMs) have been applied to enhance the generalization capabilities of end-to-end driving models. Most studies explore LLMs in an open-loop manner, where the output actions are compared to those of experts without direct feedback from the real world, while others examine closed-loop results only in simulations. This paper proposes an efficient architecture that integrates multimodal LLMs into end-to-end driving models operating in closed-loop settings in real-world environments. In our architecture, the LLM periodically processes raw sensor data to generate high-level driving instructions, effectively guiding the end-to-end model, even at a slower rate than the raw sensor data. This architecture relaxes the trade-off between the latency and inference quality of the LLM. It also allows us to choose from a wide variety of LLMs to improve high-level driving instructions and minimize fine-tuning costs. Consequently, our architecture reduces data collection requirements because the LLMs do not directly output actions; we only need to train a simple imitation learning model to output actions. In our experiments, the training data for the end-to-end model in a real-world environment consists of only simple obstacle configurations with one traffic cone, while the test environment is more complex and contains multiple obstacles placed in various positions. Experiments show that the proposed architecture enhances the generalization capabilities of the end-to-end model even without fine-tuning the LLM.

**Link**: [arxiv](http://arxiv.org/abs/2411.14256v1),  [pdf](http://arxiv.org/pdf/2411.14256v1)

**Tags**: cs.RO 



### Intent-Aware Dialogue Generation and Multi-Task Contrastive Learning for   Multi-Turn Intent Classification
**Authors**: Junhua Liu, Yong Keat Tan, Bin Fu, Kwan Hui Lim

**Updated**: 2024-11-21T15:59:29Z

**Summary**: Generating large-scale, domain-specific, multilingual multi-turn dialogue datasets remains a significant hurdle for training effective Multi-Turn Intent Classification models in chatbot systems. In this paper, we introduce Chain-of-Intent, a novel mechanism that combines Hidden Markov Models with Large Language Models (LLMs) to generate contextually aware, intent-driven conversations through self-play. By extracting domain-specific knowledge from e-commerce chat logs, we estimate conversation turns and intent transitions, which guide the generation of coherent dialogues. Leveraging LLMs to enhance emission probabilities, our approach produces natural and contextually consistent questions and answers. We also propose MINT-CL, a framework for multi-turn intent classification using multi-task contrastive learning, improving classification accuracy without the need for extensive annotated data. Evaluations show that our methods outperform baselines in dialogue quality and intent classification accuracy, especially in multilingual settings, while significantly reducing data generation efforts. Furthermore, we release MINT-E, a multilingual, intent-aware multi-turn e-commerce dialogue corpus to support future research in this area.

**Link**: [arxiv](http://arxiv.org/abs/2411.14252v1),  [pdf](http://arxiv.org/pdf/2411.14252v1)

**Tags**: cs.CL cs.AI 



### Natural Language Reinforcement Learning
**Authors**: Xidong Feng, Ziyu Wan, Haotian Fu, Bo Liu, Mengyue Yang, Girish A. Koushik, Zhiyuan Hu, Ying Wen, Jun Wang

**Updated**: 2024-11-21T15:57:02Z

**Summary**: Reinforcement Learning (RL) mathematically formulates decision-making with Markov Decision Process (MDP). With MDPs, researchers have achieved remarkable breakthroughs across various domains, including games, robotics, and language models. This paper seeks a new possibility, Natural Language Reinforcement Learning (NLRL), by extending traditional MDP to natural language-based representation space. Specifically, NLRL innovatively redefines RL principles, including task objectives, policy, value function, Bellman equation, and policy iteration, into their language counterparts. With recent advancements in large language models (LLMs), NLRL can be practically implemented to achieve RL-like policy and value improvement by either pure prompting or gradient-based training. Experiments over Maze, Breakthrough, and Tic-Tac-Toe games demonstrate the effectiveness, efficiency, and interpretability of the NLRL framework among diverse use cases. Our code will be released at https://github.com/waterhorse1/Natural-language-RL.

**Link**: [arxiv](http://arxiv.org/abs/2411.14251v1),  [pdf](http://arxiv.org/pdf/2411.14251v1)

**Tags**: cs.LG cs.AI cs.CL 



### VeriGraph: Scene Graphs for Execution Verifiable Robot Planning
**Authors**: Daniel Ekpo, Mara Levy, Saksham Suri, Chuong Huynh, Abhinav Shrivastava

**Updated**: 2024-11-21T15:56:48Z

**Summary**: Recent advancements in vision-language models (VLMs) offer potential for robot task planning, but challenges remain due to VLMs' tendency to generate incorrect action sequences. To address these limitations, we propose VeriGraph, a novel framework that integrates VLMs for robotic planning while verifying action feasibility. VeriGraph employs scene graphs as an intermediate representation, capturing key objects and spatial relationships to improve plan verification and refinement. The system generates a scene graph from input images and uses it to iteratively check and correct action sequences generated by an LLM-based task planner, ensuring constraints are respected and actions are executable. Our approach significantly enhances task completion rates across diverse manipulation scenarios, outperforming baseline methods by 58% for language-based tasks and 30% for image-based tasks.

**Link**: [arxiv](http://arxiv.org/abs/2411.10446v2),  [pdf](http://arxiv.org/pdf/2411.10446v2)

**Tags**: cs.RO cs.AI 



### FocusLLaVA: A Coarse-to-Fine Approach for Efficient and Effective Visual   Token Compression
**Authors**: Yuke Zhu, Chi Xie, Shuang Liang, Bo Zheng, Sheng Guo

**Updated**: 2024-11-21T15:37:52Z

**Summary**: Recent advances on Multi-modal Large Language Models have demonstrated that high-resolution image input is crucial for model capabilities, especially for fine-grained tasks. However, high-resolution images lead to a quadratic increase in the number of visual tokens input into LLMs, resulting in significant computational costs. Current work develop visual token compression methods to achieve efficiency improvements, often at the expense of performance. We argue that removing visual redundancy can simultaneously improve both efficiency and performance. We build a coarse-to-fine visual token compression method, with a vision-guided sampler for compressing redundant regions with low information density, and a text-guided sampler for selecting visual tokens that are strongly correlated with the user instructions.With these two modules, the proposed FocusLLaVA achieves improvements in both efficiency and performance. We validate the effectiveness of our approach on a wide range of evaluation datasets.

**Link**: [arxiv](http://arxiv.org/abs/2411.14228v1),  [pdf](http://arxiv.org/pdf/2411.14228v1)

**Tags**: cs.CV 



### Evaluating the Robustness of Analogical Reasoning in Large Language   Models
**Authors**: Martha Lewis, Melanie Mitchell

**Updated**: 2024-11-21T15:25:08Z

**Summary**: LLMs have performed well on several reasoning benchmarks, including ones that test analogical reasoning abilities. However, there is debate on the extent to which they are performing general abstract reasoning versus employing non-robust processes, e.g., that overly rely on similarity to pre-training data. Here we investigate the robustness of analogy-making abilities previously claimed for LLMs on three of four domains studied by Webb, Holyoak, and Lu (2023): letter-string analogies, digit matrices, and story analogies. For each domain we test humans and GPT models on robustness to variants of the original analogy problems that test the same abstract reasoning abilities but are likely dissimilar from tasks in the pre-training data. The performance of a system that uses robust abstract reasoning should not decline substantially on these variants.   On simple letter-string analogies, we find that while the performance of humans remains high for two types of variants we tested, the GPT models' performance declines sharply. This pattern is less pronounced as the complexity of these problems is increased, as both humans and GPT models perform poorly on both the original and variant problems requiring more complex analogies. On digit-matrix problems, we find a similar pattern but only on one out of the two types of variants we tested. On story-based analogy problems, we find that, unlike humans, the performance of GPT models are susceptible to answer-order effects, and that GPT models also may be more sensitive than humans to paraphrasing.   This work provides evidence that LLMs often lack the robustness of zero-shot human analogy-making, exhibiting brittleness on most of the variations we tested. More generally, this work points to the importance of carefully evaluating AI systems not only for accuracy but also robustness when testing their cognitive capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2411.14215v1),  [pdf](http://arxiv.org/pdf/2411.14215v1)

**Tags**: cs.CL cs.AI cs.LG 



### Physics-Informed LLM-Agent for Automated Modulation Design in Power   Electronics Systems
**Authors**: Junhua Liu, Fanfan Lin, Xinze Li, Kwan Hui Lim, Shuai Zhao

**Updated**: 2024-11-21T15:24:41Z

**Summary**: LLM-based autonomous agents have demonstrated outstanding performance in solving complex industrial tasks. However, in the pursuit of carbon neutrality and high-performance renewable energy systems, existing AI-assisted design automation faces significant limitations in explainability, scalability, and usability. To address these challenges, we propose LP-COMDA, an LLM-based, physics-informed autonomous agent that automates the modulation design of power converters in Power Electronics Systems with minimal human supervision. Unlike traditional AI-assisted approaches, LP-COMDA contains an LLM-based planner that gathers and validates design specifications through a user-friendly chat interface. The planner then coordinates with physics-informed design and optimization tools to iteratively generate and refine modulation designs autonomously. Through the chat interface, LP-COMDA provides an explainable design process, presenting explanations and charts. Experiments show that LP-COMDA outperforms all baseline methods, achieving a 63.2% reduction in error compared to the second-best benchmark method in terms of standard mean absolute error. Furthermore, empirical studies with 20 experts conclude that design time with LP-COMDA is over 33 times faster than conventional methods, showing its significant improvement on design efficiency over the current processes.

**Link**: [arxiv](http://arxiv.org/abs/2411.14214v1),  [pdf](http://arxiv.org/pdf/2411.14214v1)

**Tags**: cs.AI cs.ET 



### RRADistill: Distilling LLMs' Passage Ranking Ability for Long-Tail   Queries Document Re-Ranking on a Search Engine
**Authors**: Nayoung Choi, Youngjune Lee, Gyu-Hwung Cho, Haeyu Jeong, Jungmin Kong, Saehun Kim, Keunchan Park, Sarah Cho, Inchang Jeong, Gyohee Nam, Sunghoon Han, Wonil Yang, Jaeho Choi

**Updated**: 2024-11-21T14:23:49Z

**Summary**: Large Language Models (LLMs) excel at understanding the semantic relationships between queries and documents, even with lengthy and complex long-tail queries. These queries are challenging for feedback-based rankings due to sparse user engagement and limited feedback, making LLMs' ranking ability highly valuable. However, the large size and slow inference of LLMs necessitate the development of smaller, more efficient models (sLLMs). Recently, integrating ranking label generation into distillation techniques has become crucial, but existing methods underutilize LLMs' capabilities and are cumbersome. Our research, RRADistill: Re-Ranking Ability Distillation, propose an efficient label generation pipeline and novel sLLM training methods for both encoder and decoder models. We introduce an encoder-based method using a Term Control Layer to capture term matching signals and a decoder-based model with a ranking layer for enhanced understanding. A/B testing on a Korean-based search platform, validates the effectiveness of our approach in improving re-ranking for long-tail queries.

**Link**: [arxiv](http://arxiv.org/abs/2410.18097v3),  [pdf](http://arxiv.org/pdf/2410.18097v3)

**Tags**: cs.IR cs.AI cs.LG 



### FoPru: Focal Pruning for Efficient Large Vision-Language Models
**Authors**: Lei Jiang, Weizhe Huang, Tongxuan Liu, Yuting Zeng, Jing Li, Lechao Cheng, Xiaohua Xu

**Updated**: 2024-11-21T14:22:38Z

**Summary**: Large Vision-Language Models (LVLMs) represent a significant advancement toward achieving superior multimodal capabilities by enabling powerful Large Language Models (LLMs) to understand visual input. Typically, LVLMs utilize visual encoders, such as CLIP, to transform images into visual tokens, which are then aligned with textual tokens through projection layers before being input into the LLM for inference. Although existing LVLMs have achieved significant success, their inference efficiency is still limited by the substantial number of visual tokens and the potential redundancy among them. To mitigate this issue, we propose Focal Pruning (FoPru), a training-free method that prunes visual tokens based on the attention-based token significance derived from the vision encoder. Specifically, we introduce two alternative pruning strategies: 1) the rank strategy, which leverages all token significance scores to retain more critical tokens in a global view; 2) the row strategy, which focuses on preserving continuous key information in images from a local perspective. Finally, the selected tokens are reordered to maintain their original positional relationships. Extensive experiments across various LVLMs and multimodal datasets demonstrate that our method can prune a large number of redundant tokens while maintaining high accuracy, leading to significant improvements in inference efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2411.14164v1),  [pdf](http://arxiv.org/pdf/2411.14164v1)

**Tags**: cs.CV cs.AI 



### OmniGen: Unified Image Generation
**Authors**: Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, Zheng Liu

**Updated**: 2024-11-21T14:09:12Z

**Summary**: The emergence of Large Language Models (LLMs) has unified language generation tasks and revolutionized human-machine interaction. However, in the realm of image generation, a unified model capable of handling various tasks within a single framework remains largely unexplored. In this work, we introduce OmniGen, a new diffusion model for unified image generation. OmniGen is characterized by the following features: 1) Unification: OmniGen not only demonstrates text-to-image generation capabilities but also inherently supports various downstream tasks, such as image editing, subject-driven generation, and visual-conditional generation. 2) Simplicity: The architecture of OmniGen is highly simplified, eliminating the need for additional plugins. Moreover, compared to existing diffusion models, it is more user-friendly and can complete complex tasks end-to-end through instructions without the need for extra intermediate steps, greatly simplifying the image generation workflow. 3) Knowledge Transfer: Benefit from learning in a unified format, OmniGen effectively transfers knowledge across different tasks, manages unseen tasks and domains, and exhibits novel capabilities. We also explore the model's reasoning capabilities and potential applications of the chain-of-thought mechanism. This work represents the first attempt at a general-purpose image generation model, and we will release our resources at https://github.com/VectorSpaceLab/OmniGen to foster future advancements.

**Link**: [arxiv](http://arxiv.org/abs/2409.11340v2),  [pdf](http://arxiv.org/pdf/2409.11340v2)

**Tags**: cs.CV cs.AI 



### GASP: Efficient Black-Box Generation of Adversarial Suffixes for   Jailbreaking LLMs
**Authors**: Advik Raj Basani, Xiao Zhang

**Updated**: 2024-11-21T14:00:01Z

**Summary**: Large Language Models (LLMs) have shown impressive proficiency across a range of natural language processing tasks yet remain vulnerable to adversarial prompts, known as jailbreak attacks, carefully designed to elicit harmful responses from LLMs. Traditional methods rely on manual heuristics, which suffer from limited generalizability. While being automatic, optimization-based attacks often produce unnatural jailbreak prompts that are easy to detect by safety filters or require high computational overhead due to discrete token optimization. Witnessing the limitations of existing jailbreak methods, we introduce Generative Adversarial Suffix Prompter (GASP), a novel framework that combines human-readable prompt generation with Latent Bayesian Optimization (LBO) to improve adversarial suffix creation in a fully black-box setting. GASP leverages LBO to craft adversarial suffixes by efficiently exploring continuous embedding spaces, gradually optimizing the model to improve attack efficacy while balancing prompt coherence through a targeted iterative refinement procedure. Our experiments show that GASP can generate natural jailbreak prompts, significantly improving attack success rates, reducing training times, and accelerating inference speed, thus making it an efficient and scalable solution for red-teaming LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2411.14133v1),  [pdf](http://arxiv.org/pdf/2411.14133v1)

**Tags**: cs.LG cs.AI cs.CR cs.CV 



### Learning from "Silly" Questions Improves Large Language Models, But Only   Slightly
**Authors**: Tingyuan Zhu, Shudong Liu, Yidong Wang, Derek F. Wong, Han Yu, Takahiro Shinozaki, Jindong Wang

**Updated**: 2024-11-21T13:45:40Z

**Summary**: Constructing high-quality Supervised Fine-Tuning (SFT) datasets is critical for the training of large language models (LLMs). Recent studies have shown that using data from a specific source, Ruozhiba, a Chinese website where users ask "silly" questions to better understand certain topics, can lead to better fine-tuning performance. This paper aims to explore some hidden factors: the potential interpretations of its success and a large-scale evaluation of the performance. First, we leverage GPT-4 to analyze the successful cases of Ruozhiba questions from the perspective of education, psychology, and cognitive science, deriving a set of explanatory rules. Then, we construct fine-tuning datasets by applying these rules to the MMLU training set. Surprisingly, our results indicate that rules can significantly improve model performance in certain tasks, while potentially diminishing performance on others. For example, SFT data generated following the "Counterintuitive Thinking" rule can achieve approximately a 5% improvement on the "Global Facts" task, whereas the "Blurring the Conceptual Boundaries" rule leads to a performance drop of 6.14% on the "Econometrics" task. In addition, for specific tasks, different rules tend to have a consistent impact on model performance. This suggests that the differences between the extracted rules are not as significant, and the effectiveness of the rules is relatively consistent across tasks. Our research highlights the importance of considering task diversity and rule applicability when constructing SFT datasets to achieve more comprehensive performance improvements.

**Link**: [arxiv](http://arxiv.org/abs/2411.14121v1),  [pdf](http://arxiv.org/pdf/2411.14121v1)

**Tags**: cs.CL 



### RAG-Thief: Scalable Extraction of Private Data from Retrieval-Augmented   Generation Applications with Agent-based Attacks
**Authors**: Changyue Jiang, Xudong Pan, Geng Hong, Chenfu Bao, Min Yang

**Updated**: 2024-11-21T13:18:03Z

**Summary**: While large language models (LLMs) have achieved notable success in generative tasks, they still face limitations, such as lacking up-to-date knowledge and producing hallucinations. Retrieval-Augmented Generation (RAG) enhances LLM performance by integrating external knowledge bases, providing additional context which significantly improves accuracy and knowledge coverage. However, building these external knowledge bases often requires substantial resources and may involve sensitive information. In this paper, we propose an agent-based automated privacy attack called RAG-Thief, which can extract a scalable amount of private data from the private database used in RAG applications. We conduct a systematic study on the privacy risks associated with RAG applications, revealing that the vulnerability of LLMs makes the private knowledge bases suffer significant privacy risks. Unlike previous manual attacks which rely on traditional prompt injection techniques, RAG-Thief starts with an initial adversarial query and learns from model responses, progressively generating new queries to extract as many chunks from the knowledge base as possible. Experimental results show that our RAG-Thief can extract over 70% information from the private knowledge bases within customized RAG applications deployed on local machines and real-world platforms, including OpenAI's GPTs and ByteDance's Coze. Our findings highlight the privacy vulnerabilities in current RAG applications and underscore the pressing need for stronger safeguards.

**Link**: [arxiv](http://arxiv.org/abs/2411.14110v1),  [pdf](http://arxiv.org/pdf/2411.14110v1)

**Tags**: cs.CR 



### Lost in Inference: Rediscovering the Role of Natural Language Inference   for Large Language Models
**Authors**: Lovish Madaan, David Esiobu, Pontus Stenetorp, Barbara Plank, Dieuwke Hupkes

**Updated**: 2024-11-21T13:09:36Z

**Summary**: In the recent past, a popular way of evaluating natural language understanding (NLU), was to consider a model's ability to perform natural language inference (NLI) tasks. In this paper, we investigate if NLI tasks, that are rarely used for LLM evaluation, can still be informative for evaluating LLMs. Focusing on five different NLI benchmarks across six models of different scales, we investigate if they are able to discriminate models of different size and quality and how their accuracies develop during training. Furthermore, we investigate the extent to which the softmax distributions of models align with human distributions in cases where statements are ambiguous or vague. Overall, our results paint a positive picture for the NLI tasks: we find that they are able to discriminate well between models at various stages of training, yet are not (all) saturated. Furthermore, we find that while the similarity of model distributions with human label distributions increases with scale, it is still much higher than the similarity between two populations of humans, making it a potentially interesting statistic to consider.

**Link**: [arxiv](http://arxiv.org/abs/2411.14103v1),  [pdf](http://arxiv.org/pdf/2411.14103v1)

**Tags**: cs.CL 



### Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years   of German Parliamentary Debates
**Authors**: Aida Kostikova, Benjamin Paassen, Dominik Beese, Ole Pütz, Gregor Wiedemann, Steffen Eger

**Updated**: 2024-11-21T13:06:53Z

**Summary**: Solidarity is a crucial concept to understand social relations in societies. In this paper, we explore fine-grained solidarity frames to study solidarity towards women and migrants in German parliamentary debates between 1867 and 2022. Using 2,864 manually annotated text snippets (with a cost exceeding 18k Euro), we evaluate large language models (LLMs) like Llama 3, GPT-3.5, and GPT-4. We find that GPT-4 outperforms other LLMs, approaching human annotation quality. Using GPT-4, we automatically annotate more than 18k further instances (with a cost of around 500 Euro) across 155 years and find that solidarity with migrants outweighs anti-solidarity but that frequencies and solidarity types shift over time. Most importantly, group-based notions of (anti-)solidarity fade in favor of compassionate solidarity, focusing on the vulnerability of migrant groups, and exchange-based anti-solidarity, focusing on the lack of (economic) contribution. Our study highlights the interplay of historical events, socio-economic needs, and political ideologies in shaping migration discourse and social cohesion. We also show that powerful LLMs, if carefully prompted, can be cost-effective alternatives to human annotation for hard social scientific tasks.

**Link**: [arxiv](http://arxiv.org/abs/2210.04359v3),  [pdf](http://arxiv.org/pdf/2210.04359v3)

**Tags**: cs.CL cs.LG cs.SI 



### MetaCropFollow: Few-Shot Adaptation with Meta-Learning for Under-Canopy   Navigation
**Authors**: Thomas Woehrle, Arun N. Sivakumar, Naveen Uppalapati, Girish Chowdhary

**Updated**: 2024-11-21T12:58:09Z

**Summary**: Autonomous under-canopy navigation faces additional challenges compared to over-canopy settings - for example the tight spacing between the crop rows, degraded GPS accuracy and excessive clutter. Keypoint-based visual navigation has been shown to perform well in these conditions, however the differences between agricultural environments in terms of lighting, season, soil and crop type mean that a domain shift will likely be encountered at some point of the robot deployment. In this paper, we explore the use of Meta-Learning to overcome this domain shift using a minimal amount of data. We train a base-learner that can quickly adapt to new conditions, enabling more robust navigation in low-data regimes.

**Link**: [arxiv](http://arxiv.org/abs/2411.14092v1),  [pdf](http://arxiv.org/pdf/2411.14092v1)

**Tags**: cs.RO cs.AI cs.CV cs.LG 



### Channel Customization for Low-Complexity CSI Acquisition in   Multi-RIS-Assisted MIMO Systems
**Authors**: Weicong Chen, Yu Han, Chao-Kai Wen, Xiao Li, Shi Jin

**Updated**: 2024-11-21T12:53:39Z

**Summary**: The deployment of multiple reconfigurable intelligent surfaces (RISs) enhances the propagation environment by improving channel quality, but it also complicates channel estimation. Following the conventional wireless communication system design, which involves full channel state information (CSI) acquisition followed by RIS configuration, can reduce transmission efficiency due to substantial pilot overhead and computational complexity. This study introduces an innovative approach that integrates CSI acquisition and RIS configuration, leveraging the channel-altering capabilities of the RIS to reduce both the overhead and complexity of CSI acquisition. The focus is on multi-RIS-assisted systems, featuring both direct and reflected propagation paths. By applying a fast-varying reflection sequence during RIS configuration for channel training, the complex problem of channel estimation is decomposed into simpler, independent tasks. These fast-varying reflections effectively isolate transmit signals from different paths, streamlining the CSI acquisition process for both uplink and downlink communications with reduced complexity. In uplink scenarios, a positioning-based algorithm derives partial CSI, informing the adjustment of RIS parameters to create a sparse reflection channel, enabling precise reconstruction of the uplink channel. Downlink communication benefits from this strategically tailored reflection channel, allowing effective CSI acquisition with fewer pilot signals. Simulation results highlight the proposed methodology's ability to accurately reconstruct the reflection channel with minimal impact on the normalized mean square error while simultaneously enhancing spectral efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2411.14088v1),  [pdf](http://arxiv.org/pdf/2411.14088v1)

**Tags**: cs.IT eess.SP math.IT 



### Data Augmentation for Surgical Scene Segmentation with Anatomy-Aware   Diffusion Models
**Authors**: Danush Kumar Venkatesh, Dominik Rivoir, Micha Pfeiffer, Fiona Kolbinger, Stefanie Speidel

**Updated**: 2024-11-21T12:47:53Z

**Summary**: In computer-assisted surgery, automatically recognizing anatomical organs is crucial for understanding the surgical scene and providing intraoperative assistance. While machine learning models can identify such structures, their deployment is hindered by the need for labeled, diverse surgical datasets with anatomical annotations. Labeling multiple classes (i.e., organs) in a surgical scene is time-intensive, requiring medical experts. Although synthetically generated images can enhance segmentation performance, maintaining both organ structure and texture during generation is challenging. We introduce a multi-stage approach using diffusion models to generate multi-class surgical datasets with annotations. Our framework improves anatomy awareness by training organ specific models with an inpainting objective guided by binary segmentation masks. The organs are generated with an inference pipeline using pre-trained ControlNet to maintain the organ structure. The synthetic multi-class datasets are constructed through an image composition step, ensuring structural and textural consistency. This versatile approach allows the generation of multi-class datasets from real binary datasets and simulated surgical masks. We thoroughly evaluate the generated datasets on image quality and downstream segmentation, achieving a $15\%$ improvement in segmentation scores when combined with real images. The code is available at https://gitlab.com/nct_tso_public/muli-class-image-synthesis

**Link**: [arxiv](http://arxiv.org/abs/2410.07753v2),  [pdf](http://arxiv.org/pdf/2410.07753v2)

**Tags**: cs.CV cs.LG 



### Multi LoRA Meets Vision: Merging multiple adapters to create a multi   task model
**Authors**: Ege Kesim, Selahattin Serdar Helli

**Updated**: 2024-11-21T12:26:33Z

**Summary**: Parameter efficient finetuning (PEFT) methods are widely used in LLMs and generative models in computer vision. Especially one can use multiple of these during inference to change the behavior of the base model. In this paper we investigated whether multiple LoRA adapters trained on computer vision tasks can be merged together and used during inference without loss in performance. By achieving this, multitask models can be created just by merging different LoRAs. Merging these will reduce inference time and it will not require any additional retraining. We have trained adapters on six different tasks and evaluated their performance when they are merged together. For comparison we used a model with a frozen backbone and finetuned its head. Our results show that even with simple merging techniques creating a multitask model by merging adapters is achievable by slightly loosing performance in some cases. In our experiments we merged up to three adapters together. Depending on the task and the similarity of the data adapters were trained on, merges can outperform head finetuning. We have observed that LoRAs trained with dissimilar datasets tend to perform better compared to model trained on similar datasets.

**Link**: [arxiv](http://arxiv.org/abs/2411.14064v1),  [pdf](http://arxiv.org/pdf/2411.14064v1)

**Tags**: cs.CV cs.AI 



### DRPruning: Efficient Large Language Model Pruning through   Distributionally Robust Optimization
**Authors**: Hexuan Deng, Wenxiang Jiao, Xuebo Liu, Min Zhang, Zhaopeng Tu

**Updated**: 2024-11-21T12:02:39Z

**Summary**: Large language models (LLMs) deliver impressive results but face challenges from increasing model sizes and computational costs. Structured pruning reduces model size and speeds up inference but often causes uneven degradation across domains, leading to biased performance. To address this, we propose DRPruning, which incorporates distributionally robust optimization to restore balanced performance across domains, along with further improvements to enhance robustness. Experiments in monolingual and multilingual settings show that our method surpasses similarly sized models in pruning and continued pretraining over perplexity, downstream tasks, and instruction tuning. We further provide analysis demonstrating the robustness of our method towards various domains and distribution shifts. Furthermore, our method automatically determines optimal reference losses and data ratios, suggesting potential for broader applications. Our code is available at https://github.com/hexuandeng/DRPruning.

**Link**: [arxiv](http://arxiv.org/abs/2411.14055v1),  [pdf](http://arxiv.org/pdf/2411.14055v1)

**Tags**: cs.CL 



### How Does A Text Preprocessing Pipeline Affect Ontology Syntactic   Matching?
**Authors**: Zhangcheng Qiang, Kerry Taylor, Weiqing Wang

**Updated**: 2024-11-21T12:00:23Z

**Summary**: The generic text preprocessing pipeline, comprising Tokenisation, Normalisation, Stop Words Removal, and Stemming/Lemmatisation, has been implemented in many ontology matching (OM) systems. However, the lack of standardisation in text preprocessing creates diversity in mapping results. In this paper, we investigate the effect of the text preprocessing pipeline on OM tasks at syntactic levels. Our experiments on 8 Ontology Alignment Evaluation Initiative (OAEI) track repositories with 49 distinct alignments indicate: (1) Tokenisation and Normalisation are currently more effective than Stop Words Removal and Stemming/Lemmatisation; and (2) The selection of Lemmatisation and Stemming is task-specific. We recommend standalone Lemmatisation or Stemming with post-hoc corrections. We find that (3) Porter Stemmer and Snowball Stemmer perform better than Lancaster Stemmer; and that (4) Part-of-Speech (POS) Tagging does not help Lemmatisation. To repair less effective Stop Words Removal and Stemming/Lemmatisation used in OM tasks, we propose a novel context-based pipeline repair approach that significantly improves matching correctness and overall matching performance. We also discuss the use of text preprocessing pipeline in the new era of large language models (LLMs).

**Link**: [arxiv](http://arxiv.org/abs/2411.03962v2),  [pdf](http://arxiv.org/pdf/2411.03962v2)

**Tags**: cs.CL 



### Out-Of-Distribution Detection with Diversification (Provably)
**Authors**: Haiyun Yao, Zongbo Han, Huazhu Fu, Xi Peng, Qinghua Hu, Changqing Zhang

**Updated**: 2024-11-21T11:56:32Z

**Summary**: Out-of-distribution (OOD) detection is crucial for ensuring reliable deployment of machine learning models. Recent advancements focus on utilizing easily accessible auxiliary outliers (e.g., data from the web or other datasets) in training. However, we experimentally reveal that these methods still struggle to generalize their detection capabilities to unknown OOD data, due to the limited diversity of the auxiliary outliers collected. Therefore, we thoroughly examine this problem from the generalization perspective and demonstrate that a more diverse set of auxiliary outliers is essential for enhancing the detection capabilities. However, in practice, it is difficult and costly to collect sufficiently diverse auxiliary outlier data. Therefore, we propose a simple yet practical approach with a theoretical guarantee, termed Diversity-induced Mixup for OOD detection (diverseMix), which enhances the diversity of auxiliary outlier set for training in an efficient way. Extensive experiments show that diverseMix achieves superior performance on commonly used and recent challenging large-scale benchmarks, which further confirm the importance of the diversity of auxiliary outliers.

**Link**: [arxiv](http://arxiv.org/abs/2411.14049v1),  [pdf](http://arxiv.org/pdf/2411.14049v1)

**Tags**: cs.LG cs.CV 



### Improving Controllability and Editability for Pretrained Text-to-Music   Generation Models
**Authors**: Yixiao Zhang

**Updated**: 2024-11-21T11:46:13Z

**Summary**: The field of AI-assisted music creation has made significant strides, yet existing systems often struggle to meet the demands of iterative and nuanced music production. These challenges include providing sufficient control over the generated content and allowing for flexible, precise edits. This thesis tackles these issues by introducing a series of advancements that progressively build upon each other, enhancing the controllability and editability of text-to-music generation models.   First, we introduce Loop Copilot, a system that tries to address the need for iterative refinement in music creation. Loop Copilot leverages a large language model (LLM) to coordinate multiple specialised AI models, enabling users to generate and refine music interactively through a conversational interface. Central to this system is the Global Attribute Table, which records and maintains key musical attributes throughout the iterative process, ensuring that modifications at any stage preserve the overall coherence of the music. While Loop Copilot excels in orchestrating the music creation process, it does not directly address the need for detailed edits to the generated content.   To overcome this limitation, MusicMagus is presented as a further solution for editing AI-generated music. MusicMagus introduces a zero-shot text-to-music editing approach that allows for the modification of specific musical attributes, such as genre, mood, and instrumentation, without the need for retraining. By manipulating the latent space within pre-trained diffusion models, MusicMagus ensures that these edits are stylistically coherent and that non-targeted attributes remain unchanged. This system is particularly effective in maintaining the structural integrity of the music during edits, but it encounters challenges with more complex and real-world audio scenarios.   ...

**Link**: [arxiv](http://arxiv.org/abs/2411.12641v2),  [pdf](http://arxiv.org/pdf/2411.12641v2)

**Tags**: cs.SD eess.AS 



### Forecasting Future International Events: A Reliable Dataset for   Text-Based Event Modeling
**Authors**: Daehoon Gwak, Junwoo Park, Minho Park, Chaehun Park, Hyunchan Lee, Edward Choi, Jaegul Choo

**Updated**: 2024-11-21T11:44:23Z

**Summary**: Predicting future international events from textual information, such as news articles, has tremendous potential for applications in global policy, strategic decision-making, and geopolitics. However, existing datasets available for this task are often limited in quality, hindering the progress of related research. In this paper, we introduce WORLDREP (WORLD Relationship and Event Prediction), a novel dataset designed to address these limitations by leveraging the advanced reasoning capabilities of large-language models (LLMs). Our dataset features high-quality scoring labels generated through advanced prompt modeling and rigorously validated by domain experts in political science. We showcase the quality and utility of WORLDREP for real-world event prediction tasks, demonstrating its effectiveness through extensive experiments and analysis. Furthermore, we publicly release our dataset along with the full automation source code for data collection, labeling, and benchmarking, aiming to support and advance research in text-based event prediction.

**Link**: [arxiv](http://arxiv.org/abs/2411.14042v1),  [pdf](http://arxiv.org/pdf/2411.14042v1)

**Tags**: cs.CL cs.AI 



### Teaching MLPs to Master Heterogeneous Graph-Structured Knowledge for   Efficient and Accurate Inference
**Authors**: Yunhui Liu, Xinyi Gao, Tieke He, Jianhua Zhao, Hongzhi Yin

**Updated**: 2024-11-21T11:39:09Z

**Summary**: Heterogeneous Graph Neural Networks (HGNNs) have achieved promising results in various heterogeneous graph learning tasks, owing to their superiority in capturing the intricate relationships and diverse relational semantics inherent in heterogeneous graph structures. However, the neighborhood-fetching latency incurred by structure dependency in HGNNs makes it challenging to deploy for latency-constrained applications that require fast inference. Inspired by recent GNN-to-MLP knowledge distillation frameworks, we introduce HG2M and HG2M+ to combine both HGNN's superior performance and MLP's efficient inference. HG2M directly trains student MLPs with node features as input and soft labels from teacher HGNNs as targets, and HG2M+ further distills reliable and heterogeneous semantic knowledge into student MLPs through reliable node distillation and reliable meta-path distillation. Experiments conducted on six heterogeneous graph datasets show that despite lacking structural dependencies, HG2Ms can still achieve competitive or even better performance than HGNNs and significantly outperform vanilla MLPs. Moreover, HG2Ms demonstrate a 379.24$\times$ speedup in inference over HGNNs on the large-scale IGB-3M-19 dataset, showcasing their ability for latency-sensitive deployments.

**Link**: [arxiv](http://arxiv.org/abs/2411.14035v1),  [pdf](http://arxiv.org/pdf/2411.14035v1)

**Tags**: cs.LG 



### Multi-LLM-Agent Systems: Techniques and Business Perspectives
**Authors**: Yingxuan Yang, Qiuying Peng, Jun Wang, Weinan Zhang

**Updated**: 2024-11-21T11:36:29Z

**Summary**: In the era of (multi-modal) large language models, most operational processes can be reformulated and reproduced using LLM agents. The LLM agents can perceive, control, and get feedback from the environment so as to accomplish the given tasks in an autonomous manner. Besides the environment-interaction property, the LLM agents can call various external tools to ease the task completion process. The tools can be regarded as a predefined operational process with private or real-time knowledge that does not exist in the parameters of LLMs. As a natural trend of development, the tools for calling are becoming autonomous agents, thus the full intelligent system turns out to be a multi-LLM-agent system (MLAS). This paper discusses the technical and business landscapes of MLAS. Compared to the previous single-LLM-agent system, a MLAS has the advantages of i) higher potential of task-solving performance, ii) higher flexibility for system changing, iii) proprietary data preserving for each participating entity, and iv) feasibility of monetization for each entity. To support the ecosystem of MLAS, we provide a preliminary version of such MLAS protocol considering technical requirements, data privacy, and business incentives. As such, MLAS would be a practical solution to achieve artificial collective intelligence in the near future.

**Link**: [arxiv](http://arxiv.org/abs/2411.14033v1),  [pdf](http://arxiv.org/pdf/2411.14033v1)

**Tags**: cs.AI 



### Performance Analysis of STAR-RIS-Assisted Cell-Free Massive MIMO Systems   with Electromagnetic Interference and Phase Errors
**Authors**: Jun Qian, Ross Murch, Khaled B. Letaief

**Updated**: 2024-11-21T11:30:24Z

**Summary**: Simultaneous Transmitting and Reflecting Reconfigurable Intelligent Surfaces (STAR-RISs) are being explored for the next generation of sixth-generation (6G) networks. A promising configuration for their deployment is within cell-free massive multiple-input multiple-output (MIMO) systems. However, despite the advantages that STAR-RISs could bring, challenges such as electromagnetic interference (EMI) and phase errors may lead to significant performance degradation. In this paper, we investigate the impact of EMI and phase errors on STAR-RIS-assisted cell-free massive MIMO systems and propose techniques to mitigate these effects. We introduce a novel projected gradient descent (GD) algorithm for STAR-RIS coefficient matrix design by minimizing the local channel estimation normalised mean square error. We also derive the closed-form expressions of the uplink and downlink spectral efficiency (SE) to analyze system performance with EMI and phase errors, in which fractional power control methods are applied for performance improvement. The results reveal that the projected GD algorithm can effectively tackle EMI and phase errors to improve estimation accuracy and compensate for performance degradation with nearly $10\%\sim20\%$ SE improvement. Moreover, increasing access points (APs), antennas per AP, and STAR-RIS elements can also improve SE performance. Applying STAR-RIS in the proposed system achieves a larger $25\%$-likely SE than conventional RISs. However, the advantages of employing more STAR-RIS elements are reduced when EMI is severe.

**Link**: [arxiv](http://arxiv.org/abs/2411.14030v1),  [pdf](http://arxiv.org/pdf/2411.14030v1)

**Tags**: cs.IT eess.SP math.IT 



### OASIS: Open Agents Social Interaction Simulations on One Million Agents
**Authors**: Ziyi Yang, Zaibin Zhang, Zirui Zheng, Yuxian Jiang, Ziyue Gan, Zhiyu Wang, Zijian Ling, Jinsong Chen, Martz Ma, Bowen Dong, Prateek Gupta, Shuyue Hu, Zhenfei Yin, Guohao Li, Xu Jia, Lijun Wang, Bernard Ghanem, Huchuan Lu, Wanli Ouyang, Yu Qiao, Philip Torr, Jing Shao

**Updated**: 2024-11-21T11:27:34Z

**Summary**: There has been a growing interest in enhancing rule-based agent-based models (ABMs) for social media platforms (i.e., X, Reddit) with more realistic large language model (LLM) agents, thereby allowing for a more nuanced study of complex systems. As a result, several LLM-based ABMs have been proposed in the past year. While they hold promise, each simulator is specifically designed to study a particular scenario, making it time-consuming and resource-intensive to explore other phenomena using the same ABM. Additionally, these models simulate only a limited number of agents, whereas real-world social media platforms involve millions of users. To this end, we propose OASIS, a generalizable and scalable social media simulator. OASIS is designed based on real-world social media platforms, incorporating dynamically updated environments (i.e., dynamic social networks and post information), diverse action spaces (i.e., following, commenting), and recommendation systems (i.e., interest-based and hot-score-based). Additionally, OASIS supports large-scale user simulations, capable of modeling up to one million users. With these features, OASIS can be easily extended to different social media platforms to study large-scale group phenomena and behaviors. We replicate various social phenomena, including information spreading, group polarization, and herd effects across X and Reddit platforms. Moreover, we provide observations of social phenomena at different agent group scales. We observe that the larger agent group scale leads to more enhanced group dynamics and more diverse and helpful agents' opinions. These findings demonstrate OASIS's potential as a powerful tool for studying complex systems in digital environments.

**Link**: [arxiv](http://arxiv.org/abs/2411.11581v2),  [pdf](http://arxiv.org/pdf/2411.11581v2)

**Tags**: cs.CL 



### Positioning of a Next Generation Mobile Cell to Maximise Aggregate   Network Capacity
**Authors**: Paulo Furtado Correia, Andre Coelho, Manuel Ricardo

**Updated**: 2024-11-21T10:55:25Z

**Summary**: In wireless communications, the need to cover operation areas, such as seaports, is at the forefront of discussion, especially regarding network capacity provisioning. Radio network planning typically involves determining the number of fixed cells, considering link budgets and deploying them geometrically centered across targeted areas. This paper proposes a solution to determine the optimal position for a mobile cell, considering 3GPP path loss models. The optimal position for the mobile cell maximises the aggregate network capacity offered to a set of User Equipments (UEs), with gains up to 187% compared to the positioning of the mobile cell at the UEs geometrical center. The proposed solution can be used by network planners and integrated into network optimisation tools. This has the potential to reduce costs associated with the Radio Access Network (RAN) planning by enhancing flexibility for on-demand deployments.

**Link**: [arxiv](http://arxiv.org/abs/2409.06098v2),  [pdf](http://arxiv.org/pdf/2409.06098v2)

**Tags**: cs.NI 



### Logic Augmented Generation
**Authors**: Aldo Gangemi, Andrea Giovanni Nuzzolese

**Updated**: 2024-11-21T10:54:35Z

**Summary**: Semantic Knowledge Graphs (SKG) face challenges with scalability, flexibility, contextual understanding, and handling unstructured or ambiguous information. However, they offer formal and structured knowledge enabling highly interpretable and reliable results by means of reasoning and querying. Large Language Models (LLMs) overcome those limitations making them suitable in open-ended tasks and unstructured environments. Nevertheless, LLMs are neither interpretable nor reliable. To solve the dichotomy between LLMs and SKGs we envision Logic Augmented Generation (LAG) that combines the benefits of the two worlds. LAG uses LLMs as Reactive Continuous Knowledge Graphs that can generate potentially infinite relations and tacit knowledge on-demand. SKGs are key for injecting a discrete heuristic dimension with clear logical and factual boundaries. We exemplify LAG in two tasks of collective intelligence, i.e., medical diagnostics and climate projections. Understanding the properties and limitations of LAG, which are still mostly unknown, is of utmost importance for enabling a variety of tasks involving tacit knowledge in order to provide interpretable and effective results.

**Link**: [arxiv](http://arxiv.org/abs/2411.14012v1),  [pdf](http://arxiv.org/pdf/2411.14012v1)

**Tags**: cs.AI cs.CL 



### CulturePark: Boosting Cross-cultural Understanding in Large Language   Models
**Authors**: Cheng Li, Damien Teney, Linyi Yang, Qingsong Wen, Xing Xie, Jindong Wang

**Updated**: 2024-11-21T10:52:29Z

**Summary**: Cultural bias is pervasive in many large language models (LLMs), largely due to the deficiency of data representative of different cultures. Typically, cultural datasets and benchmarks are constructed either by extracting subsets of existing datasets or by aggregating from platforms such as Wikipedia and social media. However, these approaches are highly dependent on real-world data and human annotations, making them costly and difficult to scale. Inspired by cognitive theories on social communication, this paper introduces CulturePark, an LLM-powered multi-agent communication framework for cultural data collection. CulturePark simulates cross-cultural human communication with LLM-based agents playing roles in different cultures. It generates high-quality cross-cultural dialogues encapsulating human beliefs, norms, and customs. Using CulturePark, we generated 41,000 cultural samples to fine-tune eight culture-specific LLMs. We evaluated these models across three downstream tasks: content moderation, cultural alignment, and cultural education. Results show that for content moderation, our GPT-3.5-based models either match or outperform GPT-4 on datasets. Regarding cultural alignment, our models surpass GPT-4 on Hofstede's VSM 13 framework. Furthermore, for cultural education of human participants, our models demonstrate superior outcomes in both learning efficacy and user experience compared to GPT-4. CulturePark proves an important step in addressing cultural bias and advancing the democratization of AI, highlighting the critical role of culturally inclusive data in model training. Code is released at https://github.com/Scarelette/CulturePark.

**Link**: [arxiv](http://arxiv.org/abs/2405.15145v3),  [pdf](http://arxiv.org/pdf/2405.15145v3)

**Tags**: cs.AI cs.CL cs.MA 



### GPT versus Humans: Uncovering Ethical Concerns in Conversational   Generative AI-empowered Multi-Robot Systems
**Authors**: Rebekah Rousi, Niko Makitalo, Hooman Samani, Kai-Kristian Kemell, Jose Siqueira de Cerqueira, Ville Vakkuri, Tommi Mikkonen, Pekka Abrahamsson

**Updated**: 2024-11-21T10:45:44Z

**Summary**: The emergence of generative artificial intelligence (GAI) and large language models (LLMs) such ChatGPT has enabled the realization of long-harbored desires in software and robotic development. The technology however, has brought with it novel ethical challenges. These challenges are compounded by the application of LLMs in other machine learning systems, such as multi-robot systems. The objectives of the study were to examine novel ethical issues arising from the application of LLMs in multi-robot systems. Unfolding ethical issues in GPT agent behavior (deliberation of ethical concerns) was observed, and GPT output was compared with human experts. The article also advances a model for ethical development of multi-robot systems. A qualitative workshop-based method was employed in three workshops for the collection of ethical concerns: two human expert workshops (N=16 participants) and one GPT-agent-based workshop (N=7 agents; two teams of 6 agents plus one judge). Thematic analysis was used to analyze the qualitative data. The results reveal differences between the human-produced and GPT-based ethical concerns. Human experts placed greater emphasis on new themes related to deviance, data privacy, bias and unethical corporate conduct. GPT agents emphasized concerns present in existing AI ethics guidelines. The study contributes to a growing body of knowledge in context-specific AI ethics and GPT application. It demonstrates the gap between human expert thinking and LLM output, while emphasizing new ethical concerns emerging in novel technology.

**Link**: [arxiv](http://arxiv.org/abs/2411.14009v1),  [pdf](http://arxiv.org/pdf/2411.14009v1)

**Tags**: cs.RO cs.HC cs.MA 



### Mirror Target YOLO: An Improved YOLOv8 Method with Indirect Vision for   Heritage Buildings Fire Detection
**Authors**: Jian Liang, JunSheng Cheng

**Updated**: 2024-11-21T10:23:00Z

**Summary**: Fires can cause severe damage to heritage buildings, making timely fire detection essential. Traditional dense cabling and drilling can harm these structures, so reducing the number of cameras to minimize such impact is challenging. Additionally, avoiding false alarms due to noise sensitivity and preserving the expertise of managers in fire-prone areas is crucial. To address these needs, we propose a fire detection method based on indirect vision, called Mirror Target YOLO (MITA-YOLO). MITA-YOLO integrates indirect vision deployment and an enhanced detection module. It uses mirror angles to achieve indirect views, solving issues with limited visibility in irregular spaces and aligning each indirect view with the target monitoring area. The Target-Mask module is designed to automatically identify and isolate the indirect vision areas in each image, filtering out non-target areas. This enables the model to inherit managers' expertise in assessing fire-risk zones, improving focus and resistance to interference in fire detection.In our experiments, we created an 800-image fire dataset with indirect vision. Results show that MITA-YOLO significantly reduces camera requirements while achieving superior detection performance compared to other mainstream models.

**Link**: [arxiv](http://arxiv.org/abs/2411.13997v1),  [pdf](http://arxiv.org/pdf/2411.13997v1)

**Tags**: cs.CV cs.AI 



### Repository-level Code Translation Benchmark Targeting Rust
**Authors**: Guangsheng Ou, Mingwei Liu, Yuxuan Chen, Xing Peng, Zibin Zheng

**Updated**: 2024-11-21T10:00:52Z

**Summary**: Recent advances in large language models (LLMs) have shown significant capabilities in code translation, often evaluated using benchmarks like CodeTransOcean. However, these evaluations typically focus on simple, function-level translations without considering dependencies, which does not reflect the complexities of real-world software development. Further, their effectiveness in translating to newer, lower-resource languages like Rust in realistic scenarios is still under-explored. To address this gap, we introduce first repository-level code translation benchmark comprising 375 tasks targeting Rust, complete with relevant dependencies. Using this benchmark, we study four state-of-the-art LLMs, analyzing their erroneous outputs to understand their performance in more complex translation scenarios. Our findings reveal that LLMs exhibit substantially worse performance (41.5%-56.2% Pass@1 drop of GPT-4) on repository-level translations compared to simpler tasks, highlighting limitations in existing evaluation methods. The model that performed the best is Claude-3.5, demonstrating the strongest translation capabilities in both basic functionality accuracy and several relevant additional abilities. Additionally, we discover that LLMs struggle with identifying language differences in complex tasks, and that increased dependencies correlate with greater translation difficulty.

**Link**: [arxiv](http://arxiv.org/abs/2411.13990v1),  [pdf](http://arxiv.org/pdf/2411.13990v1)

**Tags**: cs.SE 



### Towards Smart Fronthauling Management: Experimental Insights from a 5G   Testbed
**Authors**: Marcello Morini, Eugenio Moro, Ilario Filippini, Danilo De Donno, Salvatore Moscato, Antonio Capone

**Updated**: 2024-11-21T10:00:05Z

**Summary**: The fronthaul connection is a key component of Centralized RAN (C-RAN) architectures, consistently required to handle high capacity demands. However, this critical feature is at risk when the transport link relies on wireless technology. Fortunately, solutions exist to enhance the reliability of wireless links. In this paper, we recall the theoretical fronthaul model, present a dynamic reconfiguration strategy and perform a conclusive experiment. Specifically, we showcase the setup of a wireless fronthaul testbed and discuss the resulting measurements. For this task, we leveraged the commercial hardware provided by the High-Frequency Campus Lab (HFCL), a private 5G network with millimeter wave (mmWave) radio access interface. Our experiments provide original data on the fronthaul utilization in this real deployment, demonstrating both a good accordance with the theoretical model discussed in [1] and the viability of one stabilizing solution.

**Link**: [arxiv](http://arxiv.org/abs/2411.13989v1),  [pdf](http://arxiv.org/pdf/2411.13989v1)

**Tags**: cs.NI 



### Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model   with Frozen LLM
**Authors**: Xiong Wang, Yangze Li, Chaoyou Fu, Yunhang Shen, Lei Xie, Ke Li, Xing Sun, Long Ma

**Updated**: 2024-11-21T09:19:28Z

**Summary**: Rapidly developing large language models (LLMs) have brought tremendous intelligent applications. Especially, the GPT-4o's excellent duplex speech interaction ability has brought impressive experience to users. Researchers have recently proposed several multi-modal LLMs in this direction that can achieve user-agent speech-to-speech conversations. This paper proposes a novel speech-text multimodal LLM architecture called Freeze-Omni. Our main contribution is that the speech input and output modalities can be easily connected to a textual LLM while keeping the LLM's parameters frozen throughout the training process. We design a three-stage training strategy for modeling both the speech input and output, enabling Freeze-Omni to obtain speech-to-speech conversation ability using text-speech paired data (such as ASR and TTS data) and only 60,000 multi-round text Q&A data on 8 GPUs. Moreover, we can effectively ensure that the intelligence of the Freeze-Omni in the speech modality is at the same level compared with that in the text modality of its backbone LLM, while achieving low latency end-to-end spoken response. In addition, we also designed a method to achieve duplex dialogue ability through multi-task training, giving Freeze-Omni a more natural style of dialogue ability between users and agents. In summary, Freeze-Omni holds great potential to conduct speech-to-speech dialogue based on a multimodal LLM under the condition of a frozen LLM, avoiding the catastrophic forgetting problem caused by limited data and training resources.

**Link**: [arxiv](http://arxiv.org/abs/2411.00774v3),  [pdf](http://arxiv.org/pdf/2411.00774v3)

**Tags**: cs.SD cs.AI cs.CL eess.AS 



### Engagement-Driven Content Generation with Large Language Models
**Authors**: Erica Coppolillo, Federico Cinus, Marco Minici, Francesco Bonchi, Giuseppe Manco

**Updated**: 2024-11-21T08:56:57Z

**Summary**: Large Language Models (LLMs) exhibit significant persuasion capabilities in one-on-one interactions, but their influence within social networks remains underexplored. This study investigates the potential social impact of LLMs in these environments, where interconnected users and complex opinion dynamics pose unique challenges. In particular, we address the following research question: can LLMs learn to generate meaningful content that maximizes user engagement on social networks?   To answer this question, we define a pipeline to guide the LLM-based content generation which employs reinforcement learning with simulated feedback. In our framework, the reward is based on an engagement model borrowed from the literature on opinion dynamics and information propagation. Moreover, we force the text generated by the LLM to be aligned with a given topic and to satisfy a minimum fluency requirement.   Using our framework, we analyze the capabilities and limitations of LLMs in tackling the given task, specifically considering the relative positions of the LLM as an agent within the social network and the distribution of opinions in the network on the given topic. Our findings show the full potential of LLMs in creating social engagement. Notable properties of our approach are that the learning procedure is adaptive to the opinion distribution of the underlying network and agnostic to the specifics of the engagement model, which is embedded as a plug-and-play component. In this regard, our approach can be easily refined for more complex engagement tasks and interventions in computational social science.   The code used for the experiments is publicly available at https://anonymous.4open.science/r/EDCG/.

**Link**: [arxiv](http://arxiv.org/abs/2411.13187v2),  [pdf](http://arxiv.org/pdf/2411.13187v2)

**Tags**: cs.LG cs.AI 



### LLMs as Continuous Learners: Improving the Reproduction of Defective   Code in Software Issues
**Authors**: Yalan Lin, Yingwei Ma, Rongyu Cao, Binhua Li, Fei Huang, Xiaodong Gu, Yongbin Li

**Updated**: 2024-11-21T08:49:23Z

**Summary**: Reproducing buggy code is the first and crucially important step in issue resolving, as it aids in identifying the underlying problems and validating that generated patches resolve the problem. While numerous approaches have been proposed for this task, they primarily address common, widespread errors and struggle to adapt to unique, evolving errors specific to individual code repositories. To fill this gap, we propose EvoCoder, a multi-agent continuous learning framework for issue code reproduction. EvoCoder adopts a reflection mechanism that allows the LLM to continuously learn from previously resolved problems and dynamically refine its strategies to new emerging challenges. To prevent experience bloating, EvoCoder introduces a novel hierarchical experience pool that enables the model to adaptively update common and repo-specific experiences. Our experimental results show a 20\% improvement in issue reproduction rates over existing SOTA methods. Furthermore, integrating our reproduction mechanism significantly boosts the overall accuracy of the existing issue-resolving pipeline.

**Link**: [arxiv](http://arxiv.org/abs/2411.13941v1),  [pdf](http://arxiv.org/pdf/2411.13941v1)

**Tags**: cs.SE cs.AI 



### REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain   Question Answering
**Authors**: Yuhao Wang, Ruiyang Ren, Junyi Li, Wayne Xin Zhao, Jing Liu, Ji-Rong Wen

**Updated**: 2024-11-21T08:44:20Z

**Summary**: Considering the limited internal parametric knowledge, retrieval-augmented generation (RAG) has been widely used to extend the knowledge scope of large language models (LLMs). Despite the extensive efforts on RAG research, in existing methods, LLMs cannot precisely assess the relevance of retrieved documents, thus likely leading to misleading or even incorrect utilization of external knowledge (eg., retrieved documents). To address this issue, in this paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for open-domain question answering (QA). As the key motivation, we aim to enhance the self-awareness regarding the reliability of external knowledge for LLMs, so as to adaptively utilize external knowledge in RAG systems. Specially, we develop a novel architecture for LLM-based RAG systems, by incorporating a specially designed assessment module that precisely assesses the relevance of retrieved documents. Furthermore, we propose an improved training method based on bi-granularity relevance fusion and noise-resistant training. By combining the improvements in both architecture and training, our proposed REAR can better utilize external knowledge by effectively perceiving the relevance of retrieved documents. Experiments on four open-domain QA tasks show that REAR significantly outperforms previous a number of competitive RAG approaches. Our codes can be accessed at https://github.com/RUCAIBox/REAR.

**Link**: [arxiv](http://arxiv.org/abs/2402.17497v2),  [pdf](http://arxiv.org/pdf/2402.17497v2)

**Tags**: cs.CL cs.IR 



### XAgents: A Framework for Interpretable Rule-Based Multi-Agents   Cooperation
**Authors**: Hailong Yang, Mingxian Gu, Renhuo Zhao, Fuping Hu, Zhaohong Deng, Yitang Chen

**Updated**: 2024-11-21T08:28:27Z

**Summary**: Extracting implicit knowledge and logical reasoning abilities from large language models (LLMs) has consistently been a significant challenge. The advancement of multi-agent systems has further en-hanced the capabilities of LLMs. Inspired by the structure of multi-polar neurons (MNs), we propose the XAgents framework, an in-terpretable multi-agent cooperative framework based on the IF-THEN rule-based system. The IF-Parts of the rules are responsible for logical reasoning and domain membership calculation, while the THEN-Parts are comprised of domain expert agents that generate domain-specific contents. Following the calculation of the member-ship, XAgetns transmits the task to the disparate domain rules, which subsequently generate the various responses. These re-sponses are analogous to the answers provided by different experts to the same question. The final response is reached at by eliminat-ing the hallucinations and erroneous knowledge of the LLM through membership computation and semantic adversarial genera-tion of the various domain rules. The incorporation of rule-based interpretability serves to bolster user confidence in the XAgents framework. We evaluate the efficacy of XAgents through a com-parative analysis with the latest AutoAgents, in which XAgents demonstrated superior performance across three distinct datasets. We perform post-hoc interpretable studies with SHAP algorithm and case studies, proving the interpretability of XAgent in terms of input-output feature correlation and rule-based semantics.

**Link**: [arxiv](http://arxiv.org/abs/2411.13932v1),  [pdf](http://arxiv.org/pdf/2411.13932v1)

**Tags**: cs.AI cs.MA 



### Panther: Illuminate the Sight of Multimodal LLMs with Instruction-Guided   Visual Prompts
**Authors**: Honglin Li, Yuting Gao, Chenglu Zhu, Jingdong Chen, Ming Yang, Lin Yang

**Updated**: 2024-11-21T07:47:27Z

**Summary**: Multimodal large language models (MLLMs) are closing the gap to human visual perception capability rapidly, while, still lag behind on attending to subtle images details or locating small objects precisely, etc. Common schemes to tackle these issues include deploying multiple vision encoders or operating on original high-resolution images. Few studies have concentrated on taking the textual instruction into improving visual representation, resulting in losing focus in some vision-centric tasks, a phenomenon we herein termed as Amblyopia. In this work, we introduce Panther, a MLLM that closely adheres to user instruction and locates targets of interests precisely, with the finesse of a black panther. Specifically, Panther comprises three integral components: Panther-VE, Panther-Bridge, and Panther-Decoder. Panther-VE integrates user instruction information at the early stages of the vision encoder, thereby extracting the most relevant and useful visual representations. The Panther-Bridge module, equipped with powerful filtering capabilities, significantly reduces redundant visual information, leading to a substantial savings in training costs. The Panther-Decoder is versatile and can be employed with any decoder-only architecture of LLMs without discrimination. Experimental results, particularly on vision-centric benchmarks, have demonstrated the effectiveness of Panther.

**Link**: [arxiv](http://arxiv.org/abs/2411.13909v1),  [pdf](http://arxiv.org/pdf/2411.13909v1)

**Tags**: cs.CV 



### A Survey on Small-Scale Testbeds for Connected and Automated Vehicles   and Robot Swarms
**Authors**: Armin Mokhtarian, Jianye Xu, Patrick Scheffe, Maximilian Kloock, Simon Schäfer, Heeseung Bang, Viet-Anh Le, Sangeet Ulhas, Johannes Betz, Sean Wilson, Spring Berman, Liam Paull, Amanda Prorok, Bassam Alrifaee

**Updated**: 2024-11-21T07:38:12Z

**Summary**: Connected and automated vehicles and robot swarms hold transformative potential for enhancing safety, efficiency, and sustainability in the transportation and manufacturing sectors. Extensive testing and validation of these technologies is crucial for their deployment in the real world. While simulations are essential for initial testing, they often have limitations in capturing the complex dynamics of real-world interactions. This limitation underscores the importance of small-scale testbeds. These testbeds provide a realistic, cost-effective, and controlled environment for testing and validating algorithms, acting as an essential intermediary between simulation and full-scale experiments. This work serves to facilitate researchers' efforts in identifying existing small-scale testbeds suitable for their experiments and provide insights for those who want to build their own. In addition, it delivers a comprehensive survey of the current landscape of these testbeds. We derive 62 characteristics of testbeds based on the well-known sense-plan-act paradigm and offer an online table comparing 23 small-scale testbeds based on these characteristics. The online table is hosted on our designated public webpage https://bassamlab.github.io/testbeds-survey, and we invite testbed creators and developers to contribute to it. We closely examine nine testbeds in this paper, demonstrating how the derived characteristics can be used to present testbeds. Furthermore, we discuss three ongoing challenges concerning small-scale testbeds that we identified, i.e., small-scale to full-scale transition, sustainability, and power and resource management.

**Link**: [arxiv](http://arxiv.org/abs/2408.14199v2),  [pdf](http://arxiv.org/pdf/2408.14199v2)

**Tags**: cs.RO cs.MA 



### Towards Full Delegation: Designing Ideal Agentic Behaviors for Travel   Planning
**Authors**: Song Jiang, Da JU, Andrew Cohen, Sasha Mitts, Aaron Foss, Justine T Kao, Xian Li, Yuandong Tian

**Updated**: 2024-11-21T07:30:02Z

**Summary**: How are LLM-based agents used in the future? While many of the existing work on agents has focused on improving the performance of a specific family of objective and challenging tasks, in this work, we take a different perspective by thinking about full delegation: agents take over humans' routine decision-making processes and are trusted by humans to find solutions that fit people's personalized needs and are adaptive to ever-changing context. In order to achieve such a goal, the behavior of the agents, i.e., agentic behaviors, should be evaluated not only on their achievements (i.e., outcome evaluation), but also how they achieved that (i.e., procedure evaluation). For this, we propose APEC Agent Constitution, a list of criteria that an agent should follow for good agentic behaviors, including Accuracy, Proactivity, Efficiency and Credibility. To verify whether APEC aligns with human preferences, we develop APEC-Travel, a travel planning agent that proactively extracts hidden personalized needs via multi-round dialog with travelers. APEC-Travel is constructed purely from synthetic data generated by Llama3.1-405B-Instruct with a diverse set of travelers' persona to simulate rich distribution of dialogs. Iteratively fine-tuned to follow APEC Agent Constitution, APEC-Travel surpasses baselines by 20.7% on rule-based metrics and 9.1% on LLM-as-a-Judge scores across the constitution axes.

**Link**: [arxiv](http://arxiv.org/abs/2411.13904v1),  [pdf](http://arxiv.org/pdf/2411.13904v1)

**Tags**: cs.CL 



### PIORS: Personalized Intelligent Outpatient Reception based on Large   Language Model with Multi-Agents Medical Scenario Simulation
**Authors**: Zhijie Bao, Qingyun Liu, Ying Guo, Zhengqiang Ye, Jun Shen, Shirong Xie, Jiajie Peng, Xuanjing Huang, Zhongyu Wei

**Updated**: 2024-11-21T07:28:07Z

**Summary**: In China, receptionist nurses face overwhelming workloads in outpatient settings, limiting their time and attention for each patient and ultimately reducing service quality. In this paper, we present the Personalized Intelligent Outpatient Reception System (PIORS). This system integrates an LLM-based reception nurse and a collaboration between LLM and hospital information system (HIS) into real outpatient reception setting, aiming to deliver personalized, high-quality, and efficient reception services. Additionally, to enhance the performance of LLMs in real-world healthcare scenarios, we propose a medical conversational data generation framework named Service Flow aware Medical Scenario Simulation (SFMSS), aiming to adapt the LLM to the real-world environments and PIORS settings. We evaluate the effectiveness of PIORS and SFMSS through automatic and human assessments involving 15 users and 15 clinical experts. The results demonstrate that PIORS-Nurse outperforms all baselines, including the current state-of-the-art model GPT-4o, and aligns with human preferences and clinical needs. Further details and demo can be found at https://github.com/FudanDISC/PIORS

**Link**: [arxiv](http://arxiv.org/abs/2411.13902v1),  [pdf](http://arxiv.org/pdf/2411.13902v1)

**Tags**: cs.CL cs.AI 



### Schemato -- An LLM for Netlist-to-Schematic Conversion
**Authors**: Ryoga Matsuo, Stefan Uhlich, Arun Venkitaraman, Andrea Bonetti, Chia-Yu Hsieh, Ali Momeni, Lukas Mauch, Augusto Capone, Eisaku Ohbuchi, Lorenzo Servadei

**Updated**: 2024-11-21T07:21:59Z

**Summary**: Machine learning models are advancing circuit design, particularly in analog circuits. They typically generate netlists that lack human interpretability. This is a problem as human designers heavily rely on the interpretability of circuit diagrams or schematics to intuitively understand, troubleshoot, and develop designs. Hence, to integrate domain knowledge effectively, it is crucial to translate ML-generated netlists into interpretable schematics quickly and accurately. We propose Schemato, a large language model (LLM) for netlist-to-schematic conversion. In particular, we consider our approach in the two settings of converting netlists to .asc files for LTSpice and LATEX files for CircuiTikz schematics. Experiments on our circuit dataset show that Schemato achieves up to 93% compilation success rate for the netlist-to-LaTeX conversion task, surpassing the 26% rate scored by the state-of-the-art LLMs. Furthermore, our experiments show that Schemato generates schematics with a mean structural similarity index measure that is 3xhigher than the best performing LLMs, therefore closer to the reference human design.

**Link**: [arxiv](http://arxiv.org/abs/2411.13899v1),  [pdf](http://arxiv.org/pdf/2411.13899v1)

**Tags**: cs.LG 



### LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level   Mathematical Reasoning
**Authors**: Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco Pavone, Yuqiang Li, Wanli Ouyang, Dongzhan Zhou

**Updated**: 2024-11-21T07:07:59Z

**Summary**: This paper presents an advanced mathematical problem-solving framework, LLaMA-Berry, for enhancing the mathematical reasoning ability of Large Language Models (LLMs). The framework combines Monte Carlo Tree Search (MCTS) with iterative Self-Refine to optimize the reasoning path and utilizes a pairwise reward model to evaluate different paths globally. By leveraging the self-critic and rewriting capabilities of LLMs, Self-Refine applied to MCTS (SR-MCTS) overcomes the inefficiencies and limitations of conventional step-wise and greedy search algorithms by fostering a more efficient exploration of solution spaces. Pairwise Preference Reward Model~(PPRM), inspired by Reinforcement Learning from Human Feedback (RLHF), is then used to model pairwise preferences between solutions, utilizing an Enhanced Borda Count (EBC) method to synthesize these preferences into a global ranking score to find better answers. This approach addresses the challenges of scoring variability and non-independent distributions in mathematical reasoning tasks. The framework has been tested on general and advanced benchmarks, showing superior performance in terms of search efficiency and problem-solving capability compared to existing methods like ToT and rStar, particularly in complex Olympiad-level benchmarks, including GPQA, AIME24 and AMC23.

**Link**: [arxiv](http://arxiv.org/abs/2410.02884v2),  [pdf](http://arxiv.org/pdf/2410.02884v2)

**Tags**: cs.AI cs.CL 



### PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity   Recognition
**Authors**: Jinghui Lu, Ziwei Yang, Yanjie Wang, Xuejing Liu, Brian Mac Namee, Can Huang

**Updated**: 2024-11-21T06:52:02Z

**Summary**: In this study, we aim to reduce generation latency for Named Entity Recognition (NER) with Large Language Models (LLMs). The main cause of high latency in LLMs is the sequential decoding process, which autoregressively generates all labels and mentions for NER, significantly increase the sequence length. To this end, we introduce Parallel Decoding in LLM for NE} (PaDeLLM-NER), a approach that integrates seamlessly into existing generative model frameworks without necessitating additional modules or architectural modifications. PaDeLLM-NER allows for the simultaneous decoding of all mentions, thereby reducing generation latency. Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese. Simultaneously it maintains the quality of predictions as evidenced by the performance that is on par with the state-of-the-art across various datasets.

**Link**: [arxiv](http://arxiv.org/abs/2402.04838v5),  [pdf](http://arxiv.org/pdf/2402.04838v5)

**Tags**: cs.CL cs.AI 



### Brain-Inspired Efficient Pruning: Exploiting Criticality in Spiking   Neural Networks
**Authors**: Shuo Chen, Boxiao Liu, Zeshi Liu, Haihang You

**Updated**: 2024-11-21T06:20:46Z

**Summary**: Spiking Neural Networks (SNNs) have gained significant attention due to the energy-efficient and multiplication-free characteristics. Despite these advantages, deploying large-scale SNNs on edge hardware is challenging due to limited resource availability. Network pruning offers a viable approach to compress the network scale and reduce hardware resource requirements for model deployment. However, existing SNN pruning methods cause high pruning costs and performance loss because they lack efficiency in processing the sparse spike representation of SNNs. In this paper, inspired by the critical brain hypothesis in neuroscience and the high biological plausibility of SNNs, we explore and leverage criticality to facilitate efficient pruning in deep SNNs. We firstly explain criticality in SNNs from the perspective of maximizing feature information entropy. Second, We propose a low-cost metric for assess neuron criticality in feature transmission and design a pruning-regeneration method that incorporates this criticality into the pruning process. Experimental results demonstrate that our method achieves higher performance than the current state-of-the-art (SOTA) method with up to 95.26\% reduction of pruning cost. The criticality-based regeneration process efficiently selects potential structures and facilitates consistent feature representation.

**Link**: [arxiv](http://arxiv.org/abs/2311.16141v3),  [pdf](http://arxiv.org/pdf/2311.16141v3)

**Tags**: cs.NE cs.AI cs.CV cs.LG 



### Next-Generation Phishing: How LLM Agents Empower Cyber Attackers
**Authors**: Khalifa Afane, Wenqi Wei, Ying Mao, Junaid Farooq, Juntao Chen

**Updated**: 2024-11-21T06:20:29Z

**Summary**: The escalating threat of phishing emails has become increasingly sophisticated with the rise of Large Language Models (LLMs). As attackers exploit LLMs to craft more convincing and evasive phishing emails, it is crucial to assess the resilience of current phishing defenses. In this study we conduct a comprehensive evaluation of traditional phishing detectors, such as Gmail Spam Filter, Apache SpamAssassin, and Proofpoint, as well as machine learning models like SVM, Logistic Regression, and Naive Bayes, in identifying both traditional and LLM-rephrased phishing emails. We also explore the emerging role of LLMs as phishing detection tools, a method already adopted by companies like NTT Security Holdings and JPMorgan Chase. Our results reveal notable declines in detection accuracy for rephrased emails across all detectors, highlighting critical weaknesses in current phishing defenses. As the threat landscape evolves, our findings underscore the need for stronger security controls and regulatory oversight on LLM-generated content to prevent its misuse in creating advanced phishing attacks. This study contributes to the development of more effective Cyber Threat Intelligence (CTI) by leveraging LLMs to generate diverse phishing variants that can be used for data augmentation, harnessing the power of LLMs to enhance phishing detection, and paving the way for more robust and adaptable threat detection systems.

**Link**: [arxiv](http://arxiv.org/abs/2411.13874v1),  [pdf](http://arxiv.org/pdf/2411.13874v1)

**Tags**: cs.CR cs.AI 



### Design2Code: Benchmarking Multimodal Code Generation for Automated   Front-End Engineering
**Authors**: Chenglei Si, Yanzhe Zhang, Ryan Li, Zhengyuan Yang, Ruibo Liu, Diyi Yang

**Updated**: 2024-11-21T06:18:07Z

**Summary**: Generative AI has made rapid advancements in recent years, achieving unprecedented capabilities in multimodal understanding and code generation. This can enable a new paradigm of front-end development in which multimodal large language models (MLLMs) directly convert visual designs into code implementations. In this work, we construct Design2Code - the first real-world benchmark for this task. Specifically, we manually curate 484 diverse real-world webpages as test cases and develop a set of automatic evaluation metrics to assess how well current multimodal LLMs can generate the code implementations that directly render into the given reference webpages, given the screenshots as input. We also complement automatic metrics with comprehensive human evaluations to validate the performance ranking. To rigorously benchmark MLLMs, we test various multimodal prompting methods on frontier models such as GPT-4o, GPT-4V, Gemini, and Claude. Our fine-grained break-down metrics indicate that models mostly lag in recalling visual elements from the input webpages and generating correct layout designs.

**Link**: [arxiv](http://arxiv.org/abs/2403.03163v2),  [pdf](http://arxiv.org/pdf/2403.03163v2)

**Tags**: cs.CL cs.CV cs.CY 



### Robust Detection of Watermarks for Large Language Models Under Human   Edits
**Authors**: Xiang Li, Feng Ruan, Huiyuan Wang, Qi Long, Weijie J. Su

**Updated**: 2024-11-21T06:06:04Z

**Summary**: Watermarking has offered an effective approach to distinguishing text generated by large language models (LLMs) from human-written text. However, the pervasive presence of human edits on LLM-generated text dilutes watermark signals, thereby significantly degrading detection performance of existing methods. In this paper, by modeling human edits through mixture model detection, we introduce a new method in the form of a truncated goodness-of-fit test for detecting watermarked text under human edits, which we refer to as Tr-GoF. We prove that the Tr-GoF test achieves optimality in robust detection of the Gumbel-max watermark in a certain asymptotic regime of substantial text modifications and vanishing watermark signals. Importantly, Tr-GoF achieves this optimality \textit{adaptively} as it does not require precise knowledge of human edit levels or probabilistic specifications of the LLMs, in contrast to the optimal but impractical (Neyman--Pearson) likelihood ratio test. Moreover, we establish that the Tr-GoF test attains the highest detection efficiency rate in a certain regime of moderate text modifications. In stark contrast, we show that sum-based detection rules, as employed by existing methods, fail to achieve optimal robustness in both regimes because the additive nature of their statistics is less resilient to edit-induced noise. Finally, we demonstrate the competitive and sometimes superior empirical performance of the Tr-GoF test on both synthetic data and open-source LLMs in the OPT and LLaMA families.

**Link**: [arxiv](http://arxiv.org/abs/2411.13868v1),  [pdf](http://arxiv.org/pdf/2411.13868v1)

**Tags**: cs.LG cs.CL math.ST stat.ME stat.ML stat.TH 



### Generative Fuzzy System for Sequence Generation
**Authors**: Hailong Yang, Zhaohong Deng, Wei Zhang, Zhuangzhuang Zhao, Guanjin Wang, Kup-sze Choi

**Updated**: 2024-11-21T06:03:25Z

**Summary**: Generative Models (GMs), particularly Large Language Models (LLMs), have garnered significant attention in machine learning and artificial intelligence for their ability to generate new data by learning the statistical properties of training data and creating data that resemble the original. This capability offers a wide range of applications across various domains. However, the complex structures and numerous model parameters of GMs make the input-output processes opaque, complicating the understanding and control of outputs. Moreover, the purely data-driven learning mechanism limits GM's ability to acquire broader knowledge. There remains substantial potential for enhancing the robustness and generalization capabilities of GMs. In this work, we introduce the fuzzy system, a classical modeling method that combines data and knowledge-driven mechanisms, to generative tasks. We propose a novel Generative Fuzzy System framework, named GenFS, which integrates the deep learning capabilities of GM with the interpretability and dual-driven mechanisms of fuzzy systems. Specifically, we propose an end-to-end GenFS-based model for sequence generation, called FuzzyS2S. A series of experimental studies were conducted on 12 datasets, covering three distinct categories of generative tasks: machine translation, code generation, and summary generation. The results demonstrate that FuzzyS2S outperforms the Transformer in terms of accuracy and fluency. Furthermore, it exhibits better performance on some datasets compared to state-of-the-art models T5 and CodeT5.

**Link**: [arxiv](http://arxiv.org/abs/2411.13867v1),  [pdf](http://arxiv.org/pdf/2411.13867v1)

**Tags**: cs.AI cs.LG 



### HARec: Hyperbolic Graph-LLM Alignment for Exploration and Exploitation   in Recommender Systems
**Authors**: Qiyao Ma, Menglin Yang, Mingxuan Ju, Tong Zhao, Neil Shah, Rex Ying

**Updated**: 2024-11-21T06:01:47Z

**Summary**: Modern recommendation systems often create information cocoons, limiting users' exposure to diverse content. To enhance user experience, a crucial challenge is developing systems that can balance content exploration and exploitation, allowing users to adjust their recommendation preferences. Intuitively, this balance can be achieved through a tree-structured representation, where depth search facilitates exploitation and breadth search enables exploration. However, current works face two challenges to achieve this target: (1) Euclidean methods fail to fully capture hierarchical structures and lack flexibility in balancing exploration-exploitation, while (2) hyperbolic approaches, despite better hierarchical modeling, suffer from insufficient semantic alignment due to their reliance on Euclidean text encoders. To address these challenges, we propose HARec, a hyperbolic representation learning framework that jointly aligns user-item collaborative information with textual descriptions in hyperbolic space. Our framework introduces two key technique novelty: (1) a hierarchical-aware graph-llm alignment mechanism that enables better hierarchical representation, and (2) a hyperbolic hierarchical tree structure that facilitates user-adjustable exploration-exploitation trade-offs. Extensive experiments demonstrate that HARec consistently outperforms both Euclidean and hyperbolic baselines, achieving up to 5.49% improvement in utility metrics and 11.39% increase in diversity metrics.

**Link**: [arxiv](http://arxiv.org/abs/2411.13865v1),  [pdf](http://arxiv.org/pdf/2411.13865v1)

**Tags**: cs.IR cs.AI cs.CL cs.LG 



### Instruction-Guided Editing Controls for Images and Multimedia: A Survey   in LLM era
**Authors**: Thanh Tam Nguyen, Zhao Ren, Trinh Pham, Thanh Trung Huynh, Phi Le Nguyen, Hongzhi Yin, Quoc Viet Hung Nguyen

**Updated**: 2024-11-21T05:28:10Z

**Summary**: The rapid advancement of large language models (LLMs) and multimodal learning has transformed digital content creation and manipulation. Traditional visual editing tools require significant expertise, limiting accessibility. Recent strides in instruction-based editing have enabled intuitive interaction with visual content, using natural language as a bridge between user intent and complex editing operations. This survey provides an overview of these techniques, focusing on how LLMs and multimodal models empower users to achieve precise visual modifications without deep technical knowledge. By synthesizing over 100 publications, we explore methods from generative adversarial networks to diffusion models, examining multimodal integration for fine-grained content control. We discuss practical applications across domains such as fashion, 3D scene manipulation, and video synthesis, highlighting increased accessibility and alignment with human intuition. Our survey compares existing literature, emphasizing LLM-empowered editing, and identifies key challenges to stimulate further research. We aim to democratize powerful visual editing across various industries, from entertainment to education. Interested readers are encouraged to access our repository at https://github.com/tamlhp/awesome-instruction-editing.

**Link**: [arxiv](http://arxiv.org/abs/2411.09955v2),  [pdf](http://arxiv.org/pdf/2411.09955v2)

**Tags**: cs.CV cs.AI cs.HC cs.LG cs.MM 



### A Multimodal Approach to The Detection and Classification of Skin   Diseases
**Authors**: Allen Yang, Edward Yang

**Updated**: 2024-11-21T05:27:42Z

**Summary**: According to PBS, nearly one-third of Americans lack access to primary care services, and another forty percent delay going to avoid medical costs. As a result, many diseases are left undiagnosed and untreated, even if the disease shows many physical symptoms on the skin. With the rise of AI, self-diagnosis and improved disease recognition have become more promising than ever; in spite of that, existing methods suffer from a lack of large-scale patient databases and outdated methods of study, resulting in studies being limited to only a few diseases or modalities. This study incorporates readily available and easily accessible patient information via image and text for skin disease classification on a new dataset of 26 skin disease types that includes both skin disease images (37K) and associated patient narratives. Using this dataset, baselines for various image models were established that outperform existing methods. Initially, the Resnet-50 model was only able to achieve an accuracy of 70% but, after various optimization techniques, the accuracy was improved to 80%. In addition, this study proposes a novel fine-tuning strategy for sequence classification Large Language Models (LLMs), Chain of Options, which breaks down a complex reasoning task into intermediate steps at training time instead of inference. With Chain of Options and preliminary disease recommendations from the image model, this method achieves state of the art accuracy 91% in diagnosing patient skin disease given just an image of the afflicted area as well as a patient description of the symptoms (such as itchiness or dizziness). Through this research, an earlier diagnosis of skin diseases can occur, and clinicians can work with deep learning models to give a more accurate diagnosis, improving quality of life and saving lives.

**Link**: [arxiv](http://arxiv.org/abs/2411.13855v1),  [pdf](http://arxiv.org/pdf/2411.13855v1)

**Tags**: eess.IV cs.CV cs.LG 



### Graph Knowledge Distillation to Mixture of Experts
**Authors**: Pavel Rumiantsev, Mark Coates

**Updated**: 2024-11-21T05:24:27Z

**Summary**: In terms of accuracy, Graph Neural Networks (GNNs) are the best architectural choice for the node classification task. Their drawback in real-world deployment is the latency that emerges from the neighbourhood processing operation. One solution to the latency issue is to perform knowledge distillation from a trained GNN to a Multi-Layer Perceptron (MLP), where the MLP processes only the features of the node being classified (and possibly some pre-computed structural information). However, the performance of such MLPs in both transductive and inductive settings remains inconsistent for existing knowledge distillation techniques. We propose to address the performance concerns by using a specially-designed student model instead of an MLP. Our model, named Routing-by-Memory (RbM), is a form of Mixture-of-Experts (MoE), with a design that enforces expert specialization. By encouraging each expert to specialize on a certain region on the hidden representation space, we demonstrate experimentally that it is possible to derive considerably more consistent performance across multiple datasets. Code available at https://github.com/Rufaim/routing-by-memory.

**Link**: [arxiv](http://arxiv.org/abs/2406.11919v2),  [pdf](http://arxiv.org/pdf/2406.11919v2)

**Tags**: cs.LG cs.AI stat.ML 



### Disentangling Memory and Reasoning Ability in Large Language Models
**Authors**: Mingyu Jin, Weidi Luo, Sitao Cheng, Xinyi Wang, Wenyue Hua, Ruixiang Tang, William Yang Wang, Yongfeng Zhang

**Updated**: 2024-11-21T05:19:56Z

**Summary**: Large Language Models (LLMs) have demonstrated strong performance in handling complex tasks requiring both extensive knowledge and reasoning abilities. However, the existing LLM inference pipeline operates as an opaque process without explicit separation between knowledge retrieval and reasoning steps, making the model's decision-making process unclear and disorganized. This ambiguity can lead to issues such as hallucinations and knowledge forgetting, which significantly impact the reliability of LLMs in high-stakes domains. In this paper, we propose a new inference paradigm that decomposes the complex inference process into two distinct and clear actions: (1) memory recall: which retrieves relevant knowledge, and (2) reasoning: which performs logical steps based on the recalled knowledge. To facilitate this decomposition, we introduce two special tokens memory and reason, guiding the model to distinguish between steps that require knowledge retrieval and those that involve reasoning. Our experiment results show that this decomposition not only improves model performance but also enhances the interpretability of the inference process, enabling users to identify sources of error and refine model responses effectively. The code is available at https://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2411.13504v2),  [pdf](http://arxiv.org/pdf/2411.13504v2)

**Tags**: cs.CL 



### A Closer Look at Machine Unlearning for Large Language Models
**Authors**: Xiaojian Yuan, Tianyu Pang, Chao Du, Kejiang Chen, Weiming Zhang, Min Lin

**Updated**: 2024-11-21T04:39:13Z

**Summary**: Large language models (LLMs) may memorize sensitive or copyrighted content, raising privacy and legal concerns. Due to the high cost of retraining from scratch, researchers attempt to employ machine unlearning to remove specific content from LLMs while preserving the overall performance. In this paper, we discuss several issues in machine unlearning for LLMs and provide our insights on possible approaches. To address the issue of inadequate evaluation of model outputs after unlearning, we introduce three additional metrics to evaluate token diversity, sentence semantics, and factual correctness. We then categorize unlearning methods into untargeted and targeted, and discuss their issues respectively. Specifically, the behavior that untargeted unlearning attempts to approximate is unpredictable and may involve hallucinations, and existing regularization is insufficient for targeted unlearning. To alleviate these issues, we propose using the objective of maximizing entropy (ME) for untargeted unlearning and incorporate answer preservation (AP) loss as regularization for targeted unlearning. Experimental results across three scenarios, i.e., fictitious unlearning, continual unlearning, and real-world unlearning, demonstrate the effectiveness of our approaches. The code is available at https://github.com/sail-sg/closer-look-LLM-unlearning.

**Link**: [arxiv](http://arxiv.org/abs/2410.08109v2),  [pdf](http://arxiv.org/pdf/2410.08109v2)

**Tags**: cs.CL cs.AI cs.LG 



### Fixing Security Vulnerabilities with AI in OSS-Fuzz
**Authors**: Yuntong Zhang, Jiawei Wang, Dominic Berzin, Martin Mirchev, Dongge Liu, Abhishek Arya, Oliver Chang, Abhik Roychoudhury

**Updated**: 2024-11-21T04:35:46Z

**Summary**: Critical open source software systems undergo significant validation in the form of lengthy fuzz campaigns. The fuzz campaigns typically conduct a biased random search over the domain of program inputs, to find inputs which crash the software system. Such fuzzing is useful to enhance the security of software systems in general since even closed source software may use open source components. Hence testing open source software is of paramount importance. Currently OSS-Fuzz is the most significant and widely used infrastructure for continuous validation of open source systems. Unfortunately even though OSS-Fuzz has identified more than 10,000 vulnerabilities across 1000 or more software projects, the detected vulnerabilities may remain unpatched, as vulnerability fixing is often manual in practice. In this work, we rely on the recent progress in Large Language Model (LLM) agents for autonomous program improvement including bug fixing. We customise the well-known AutoCodeRover agent for fixing security vulnerabilities. This is because LLM agents like AutoCodeRover fix bugs from issue descriptions via code search. Instead for security patching, we rely on the test execution of the exploit input to extract code elements relevant to the fix. Our experience with OSS-Fuzz vulnerability data shows that LLM agent autonomy is useful for successful security patching, as opposed to approaches like Agentless where the control flow is fixed. More importantly our findings show that we cannot measure quality of patches by code similarity of the patch with reference codes (as in CodeBLEU scores used in VulMaster), since patches with high CodeBLEU scores still fail to pass given the given exploit input. Our findings indicate that security patch correctness needs to consider dynamic attributes like test executions as opposed to relying of standard text/code similarity metrics.

**Link**: [arxiv](http://arxiv.org/abs/2411.03346v2),  [pdf](http://arxiv.org/pdf/2411.03346v2)

**Tags**: cs.CR cs.SE 



### Interactive and Expressive Code-Augmented Planning with Large Language   Models
**Authors**: Anthony Z. Liu, Xinhe Wang, Jacob Sansom, Yao Fu, Jongwook Choi, Sungryull Sohn, Jaekyeom Kim, Honglak Lee

**Updated**: 2024-11-21T04:23:17Z

**Summary**: Large Language Models (LLMs) demonstrate strong abilities in common-sense reasoning and interactive decision-making, but often struggle with complex, long-horizon planning tasks. Recent techniques have sought to structure LLM outputs using control flow and other code-adjacent techniques to improve planning performance. These techniques include using variables (to track important information) and functions (to divide complex tasks into smaller re-usable sub-tasks). However, purely code-based approaches can be error-prone and insufficient for handling ambiguous or unstructured data. To address these challenges, we propose REPL-Plan, an LLM planning approach that is fully code-expressive (it can utilize all the benefits of code) while also being dynamic (it can flexibly adapt from errors and use the LLM for fuzzy situations). In REPL-Plan, an LLM solves tasks by interacting with a Read-Eval-Print Loop (REPL), which iteratively executes and evaluates code, similar to language shells or interactive code notebooks, allowing the model to flexibly correct errors and handle tasks dynamically. We demonstrate that REPL-Plan achieves strong results across various planning domains compared to previous methods.

**Link**: [arxiv](http://arxiv.org/abs/2411.13826v1),  [pdf](http://arxiv.org/pdf/2411.13826v1)

**Tags**: cs.CL cs.LG 



### InstCache: A Predictive Cache for LLM Serving
**Authors**: Longwei Zou, Tingfeng Liu, Kai Chen, Jiangang Kong, Yangdong Deng

**Updated**: 2024-11-21T03:52:41Z

**Summary**: Large language models are revolutionizing every aspect of human life. However, the unprecedented power comes at the cost of significant computing intensity, suggesting long latency and large energy footprint. Key-Value Cache and Semantic Cache have been proposed as a solution to the above problem, but both suffer from limited scalability due to significant memory cost for each token or instruction embeddings. Motivated by the observations that most instructions are short, repetitive and predictable by LLMs, we propose to predict user-instructions by an instruction-aligned LLM and store them in a predictive cache, so-called InstCache. We introduce an instruction pre-population algorithm based on the negative log likelihood of instructions, determining the cache size with regard to the hit rate. The proposed InstCache is efficiently implemented as a hash table with minimal lookup latency for deployment. Experimental results show that InstCache can achieve up to 51.34% hit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost of only 4.5GB.

**Link**: [arxiv](http://arxiv.org/abs/2411.13820v1),  [pdf](http://arxiv.org/pdf/2411.13820v1)

**Tags**: cs.CL cs.DC 



### InterControl: Zero-shot Human Interaction Generation by Controlling   Every Joint
**Authors**: Zhenzhi Wang, Jingbo Wang, Yixuan Li, Dahua Lin, Bo Dai

**Updated**: 2024-11-21T03:51:58Z

**Summary**: Text-conditioned motion synthesis has made remarkable progress with the emergence of diffusion models. However, the majority of these motion diffusion models are primarily designed for a single character and overlook multi-human interactions. In our approach, we strive to explore this problem by synthesizing human motion with interactions for a group of characters of any size in a zero-shot manner. The key aspect of our approach is the adaptation of human-wise interactions as pairs of human joints that can be either in contact or separated by a desired distance. In contrast to existing methods that necessitate training motion generation models on multi-human motion datasets with a fixed number of characters, our approach inherently possesses the flexibility to model human interactions involving an arbitrary number of individuals, thereby transcending the limitations imposed by the training data. We introduce a novel controllable motion generation method, InterControl, to encourage the synthesized motions maintaining the desired distance between joint pairs. It consists of a motion controller and an inverse kinematics guidance module that realistically and accurately aligns the joints of synthesized characters to the desired location. Furthermore, we demonstrate that the distance between joint pairs for human-wise interactions can be generated using an off-the-shelf Large Language Model (LLM). Experimental results highlight the capability of our framework to generate interactions with multiple human characters and its potential to work with off-the-shelf physics-based character simulators. Code is available at https://github.com/zhenzhiwang/intercontrol

**Link**: [arxiv](http://arxiv.org/abs/2311.15864v4),  [pdf](http://arxiv.org/pdf/2311.15864v4)

**Tags**: cs.CV 



### Dynamic Structural Clustering Unleashed: Flexible Similarities,   Versatile Updates and for All Parameters
**Authors**: Zhuowei Zhao, Junhao Gan, Boyu Ruan, Zhifeng Bao, Jianzhong Qi, Sibo Wang

**Updated**: 2024-11-21T03:48:03Z

**Summary**: We study structural clustering on graphs in dynamic scenarios, where the graphs can be updated by arbitrary insertions or deletions of edges/vertices. The goal is to efficiently compute structural clustering results for any clustering parameters $\epsilon$ and $\mu$ given on the fly, for arbitrary graph update patterns, and for all typical similarity measurements. Specifically, we adopt the idea of update affordability and propose an a-lot-simpler yet more efficient (both theoretically and practically) algorithm (than state of the art), named VD-STAR to handle graph updates. First, with a theoretical clustering result quality guarantee, VD-STAR can output high-quality clustering results with up to 99.9% accuracy. Second, our VD-STAR is easy to implement as it just needs to maintain certain sorted linked lists and hash tables, and hence, effectively enhances its deployment in practice. Third and most importantly, by careful analysis, VD-STAR improves the per-update time bound of the state-of-the-art from $O(\log^2 n)$ expected with certain update pattern assumption to $O(\log n)$ amortized in expectation without any update pattern assumption. We further design two variants of VD-STAR to enhance its empirical performance. Experimental results show that our algorithms consistently outperform the state-of-the-art competitors by up to 9,315 times in update time across nine real datasets.

**Link**: [arxiv](http://arxiv.org/abs/2411.13817v1),  [pdf](http://arxiv.org/pdf/2411.13817v1)

**Tags**: cs.DS 



### AutoMixQ: Self-Adjusting Quantization for High Performance   Memory-Efficient Fine-Tuning
**Authors**: Changhai Zhou, Shiyang Zhang, Yuhua Zhou, Zekai Liu, Shichao Weng

**Updated**: 2024-11-21T03:35:07Z

**Summary**: Fine-tuning large language models (LLMs) under resource constraints is a significant challenge in deep learning. Low-Rank Adaptation (LoRA), pruning, and quantization are all effective methods for improving resource efficiency. However, combining them directly often results in suboptimal performance, especially with uniform quantization across all model layers. This is due to the complex, uneven interlayer relationships introduced by pruning, necessitating more refined quantization strategies. To address this, we propose AutoMixQ, an end-to-end optimization framework that selects optimal quantization configurations for each LLM layer. AutoMixQ leverages lightweight performance models to guide the selection process, significantly reducing time and computational resources compared to exhaustive search methods. By incorporating Pareto optimality, AutoMixQ balances memory usage and performance, approaching the upper bounds of model capability under strict resource constraints. Our experiments on widely used benchmarks show that AutoMixQ reduces memory consumption while achieving superior performance. For example, at a 30\% pruning rate in LLaMA-7B, AutoMixQ achieved 66.21\% on BoolQ compared to 62.45\% for LoRA and 58.96\% for LoftQ, while reducing memory consumption by 35.5\% compared to LoRA and 27.5\% compared to LoftQ.

**Link**: [arxiv](http://arxiv.org/abs/2411.13814v1),  [pdf](http://arxiv.org/pdf/2411.13814v1)

**Tags**: cs.LG cs.AI 



### DCSim: Computing and Networking Integration based Container Scheduling   Simulator for Data Centers
**Authors**: Jinlong Hu, Zhizhe Rao, Xingchen Liu, Lihao Deng, Shoubin Dong

**Updated**: 2024-11-21T03:16:05Z

**Summary**: The increasing prevalence of cloud-native technologies, particularly containers, has led to the widespread adoption of containerized deployments in data centers. The advancement of deep neural network models has increased the demand for container-based distributed model training and inference, where frequent data transmission among nodes has emerged as a significant performance bottleneck. However, traditional container scheduling simulators often overlook the influence of network modeling on the efficiency of container scheduling, primarily concentrating on modeling computational resources. In this paper, we focus on a container scheduling simulator based on collaboration between computing and networking within data centers. We propose a new container scheduling simulator for data centers, named DCSim. The simulator consists of several modules: a data center module, a network simulation module, a container scheduling module, a discrete event-driven module, and a data collection and analysis module. Together, these modules provide heterogeneous computing power modeling and dynamic network simulation capabilities. We design a discrete event model using SimPy to represent various aspects of container processing, including container requests, scheduling, execution, pauses, communication, migration, and termination within data centers. Among these, lightweight virtualization technology based on Mininet is employed to construct a software-defined network. An experimental environment for container scheduling simulation was established, and functional and performance tests were conducted on the simulator to validate its scheduling simulation capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2411.13809v1),  [pdf](http://arxiv.org/pdf/2411.13809v1)

**Tags**: cs.DC 



### GPAI Evaluations Standards Taskforce: Towards Effective AI Governance
**Authors**: Patricia Paskov, Lukas Berglund, Everett Smith, Lisa Soder

**Updated**: 2024-11-21T03:14:31Z

**Summary**: General-purpose AI evaluations have been proposed as a promising way of identifying and mitigating systemic risks posed by AI development and deployment. While GPAI evaluations play an increasingly central role in institutional decision- and policy-making -- including by way of the European Union AI Act's mandate to conduct evaluations on GPAI models presenting systemic risk -- no standards exist to date to promote their quality or legitimacy. To strengthen GPAI evaluations in the EU, which currently constitutes the first and only jurisdiction that mandates GPAI evaluations, we outline four desiderata for GPAI evaluations: internal validity, external validity, reproducibility, and portability. To uphold these desiderata in a dynamic environment of continuously evolving risks, we propose a dedicated EU GPAI Evaluation Standards Taskforce, to be housed within the bodies established by the EU AI Act. We outline the responsibilities of the Taskforce, specify the GPAI provider commitments that would facilitate Taskforce success, discuss the potential impact of the Taskforce on global AI governance, and address potential sources of failure that policymakers should heed.

**Link**: [arxiv](http://arxiv.org/abs/2411.13808v1),  [pdf](http://arxiv.org/pdf/2411.13808v1)

**Tags**: cs.CY 



### SemiKong: Curating, Training, and Evaluating A Semiconductor   Industry-Specific Large Language Model
**Authors**: Christopher Nguyen, William Nguyen, Atsushi Suzuki, Daisuke Oku, Hong An Phan, Sang Dinh, Zooey Nguyen, Anh Ha, Shruti Raghavan, Huy Vo, Thang Nguyen, Lan Nguyen, Yoshikuni Hirayama

**Updated**: 2024-11-21T03:05:38Z

**Summary**: Large Language Models (LLMs) have demonstrated the potential to address some issues within the semiconductor industry. However, they are often general-purpose models that lack the specialized knowledge needed to tackle the unique challenges of this sector, such as the intricate physics and chemistry of semiconductor devices and processes. SemiKong, the first industry-specific LLM for the semiconductor domain, provides a foundation that can be used to develop tailored proprietary models. With SemiKong 1.0, we aim to develop a foundational model capable of understanding etching problems at an expert level. Our key contributions include (a) curating a comprehensive corpus of semiconductor-related texts, (b) creating a foundational model with in-depth semiconductor knowledge, and (c) introducing a framework for integrating expert knowledge, thereby advancing the evaluation process of domain-specific AI models. Through fine-tuning a pre-trained LLM using our curated dataset, we have shown that SemiKong outperforms larger, general-purpose LLMs in various semiconductor manufacturing and design tasks. Our extensive experiments underscore the importance of developing domain-specific LLMs as a foundation for company- or tool-specific proprietary models, paving the way for further research and applications in the semiconductor domain. Code and dataset will be available at https://github.com/aitomatic/semikong

**Link**: [arxiv](http://arxiv.org/abs/2411.13802v1),  [pdf](http://arxiv.org/pdf/2411.13802v1)

**Tags**: cs.CL 



### Unconsidered Installations: Discovering IoT Deployments in the IPv6   Internet
**Authors**: Markus Dahlmanns, Felix Heidenreich, Johannes Lohmöller, Jan Pennekamp, Klaus Wehrle, Martin Henze

**Updated**: 2024-11-21T02:57:59Z

**Summary**: Internet-wide studies provide extremely valuable insight into how operators manage their Internet of Things (IoT) deployments in reality and often reveal grievances, e.g., significant security issues. However, while IoT devices often use IPv6, past studies resorted to comprehensively scan the IPv4 address space. To fully understand how the IoT and all its services and devices is operated, including IPv6-reachable deployments is inevitable-although scanning the entire IPv6 address space is infeasible. In this paper, we close this gap and examine how to best discover IPv6-reachable IoT deployments. To this end, we propose a methodology that allows combining various IPv6 scan direction approaches to understand the findability and prevalence of IPv6-reachable IoT deployments. Using three sources of active IPv6 addresses and eleven address generators, we discovered 6658 IoT deployments. We derive that the available address sources are a good starting point for finding IoT deployments. Additionally, we show that using two address generators is sufficient to cover most found deployments and save time as well as resources. Assessing the security of the deployments, we surprisingly find similar issues as in the IPv4 Internet, although IPv6 deployments might be newer and generally more up-to-date: Only 39% of deployments have access control in place and only 6.2% make use of TLS inviting attackers, e.g., to eavesdrop sensitive data.

**Link**: [arxiv](http://arxiv.org/abs/2411.13799v1),  [pdf](http://arxiv.org/pdf/2411.13799v1)

**Tags**: cs.NI 



### When Context Leads but Parametric Memory Follows in Large Language   Models
**Authors**: Yufei Tao, Adam Hiatt, Erik Haake, Antonie J. Jetter, Ameeta Agrawal

**Updated**: 2024-11-21T02:31:15Z

**Summary**: Large language models (LLMs) have demonstrated remarkable progress in leveraging diverse knowledge sources. This study investigates how nine widely used LLMs allocate knowledge between local context and global parameters when answering open-ended questions in knowledge-consistent scenarios. We introduce a novel dataset, WikiAtomic, and systematically vary context sizes to analyze how LLMs prioritize and utilize the provided information and their parametric knowledge in knowledge-consistent scenarios. Additionally, we also study their tendency to hallucinate under varying context sizes. Our findings reveal consistent patterns across models, including a consistent reliance on both contextual (around 70%) and parametric (around 30%) knowledge, and a decrease in hallucinations with increasing context. These insights highlight the importance of more effective context organization and developing models that use input more deterministically for robust performance.

**Link**: [arxiv](http://arxiv.org/abs/2409.08435v4),  [pdf](http://arxiv.org/pdf/2409.08435v4)

**Tags**: cs.CL cs.AI 



### LEADRE: Multi-Faceted Knowledge Enhanced LLM Empowered Display   Advertisement Recommender System
**Authors**: Fengxin Li, Yi Li, Yue Liu, Chao Zhou, Yuan Wang, Xiaoxiang Deng, Wei Xue, Dapeng Liu, Lei Xiao, Haijie Gu, Jie Jiang, Hongyan Liu, Biao Qin, Jun He

**Updated**: 2024-11-21T02:22:35Z

**Summary**: Display advertising provides significant value to advertisers, publishers, and users. Traditional display advertising systems utilize a multi-stage architecture consisting of retrieval, coarse ranking, and final ranking. However, conventional retrieval methods rely on ID-based learning to rank mechanisms and fail to adequately utilize the content information of ads, which hampers their ability to provide diverse recommendation lists.   To address this limitation, we propose leveraging the extensive world knowledge of LLMs. However, three key challenges arise when attempting to maximize the effectiveness of LLMs: "How to capture user interests", "How to bridge the knowledge gap between LLMs and advertising system", and "How to efficiently deploy LLMs". To overcome these challenges, we introduce a novel LLM-based framework called LLM Empowered Display ADvertisement REcommender system (LEADRE). LEADRE consists of three core modules: (1) The Intent-Aware Prompt Engineering introduces multi-faceted knowledge and designs intent-aware <Prompt, Response> pairs that fine-tune LLMs to generate ads tailored to users' personal interests. (2) The Advertising-Specific Knowledge Alignment incorporates auxiliary fine-tuning tasks and Direct Preference Optimization (DPO) to align LLMs with ad semantic and business value. (3) The Efficient System Deployment deploys LEADRE in an online environment by integrating both latency-tolerant and latency-sensitive service. Extensive offline experiments demonstrate the effectiveness of LEADRE and validate the contributions of individual modules. Online A/B test shows that LEADRE leads to a 1.57% and 1.17% GMV lift for serviced users on WeChat Channels and Moments separately. LEADRE has been deployed on both platforms, serving tens of billions of requests each day.

**Link**: [arxiv](http://arxiv.org/abs/2411.13789v1),  [pdf](http://arxiv.org/pdf/2411.13789v1)

**Tags**: cs.IR 



### Edge-Cloud Routing for Text-to-Image Model with Token-Level Multi-Metric   Prediction
**Authors**: Zewei Xin, Qinya Li, Chaoyue Niu, Fan Wu

**Updated**: 2024-11-21T02:18:06Z

**Summary**: Large text-to-image models demonstrate impressive generation capabilities; however, their substantial size necessitates expensive cloud servers for deployment. Conversely, light-weight models can be deployed on edge devices at lower cost but often with inferior generation quality for complex user prompts. To strike a balance between performance and cost, we propose a routing framework, called \texttt{RouteT2I}, which dynamically selects either the large cloud model or the light-weight edge model for each user prompt. Since generated image quality is challenging to measure directly, \texttt{RouteT2I} establishes multi-dimensional quality metrics, particularly, by evaluating the similarity between the generated images and both positive and negative texts that describe each specific quality metric. \texttt{RouteT2I} then predicts the expected quality of the generated images by identifying key tokens in the prompt and comparing their impact on the quality. \texttt{RouteT2I} further introduces the Pareto relative superiority to compare the multi-metric quality of the generated images. Based on this comparison and predefined cost constraints, \texttt{RouteT2I} allocates prompts to either the edge or the cloud. Evaluation reveals that \texttt{RouteT2I} significantly reduces the number of requesting large cloud model while maintaining high-quality image generation.

**Link**: [arxiv](http://arxiv.org/abs/2411.13787v1),  [pdf](http://arxiv.org/pdf/2411.13787v1)

**Tags**: cs.CV cs.LG 



### TransLinkGuard: Safeguarding Transformer Models Against Model Stealing   in Edge Deployment
**Authors**: Qinfeng Li, Zhiqiang Shen, Zhenghan Qin, Yangfan Xie, Xuhong Zhang, Tianyu Du, Jianwei Yin

**Updated**: 2024-11-21T02:16:53Z

**Summary**: Proprietary large language models (LLMs) have been widely applied in various scenarios. Additionally, deploying LLMs on edge devices is trending for efficiency and privacy reasons. However, edge deployment of proprietary LLMs introduces new security challenges: edge-deployed models are exposed as white-box accessible to users, enabling adversaries to conduct effective model stealing (MS) attacks. Unfortunately, existing defense mechanisms fail to provide effective protection. Specifically, we identify four critical protection properties that existing methods fail to simultaneously satisfy: (1) maintaining protection after a model is physically copied; (2) authorizing model access at request level; (3) safeguarding runtime reverse engineering; (4) achieving high security with negligible runtime overhead. To address the above issues, we propose TransLinkGuard, a plug-and-play model protection approach against model stealing on edge devices. The core part of TransLinkGuard is a lightweight authorization module residing in a secure environment, e.g., TEE. The authorization module can freshly authorize each request based on its input. Extensive experiments show that TransLinkGuard achieves the same security protection as the black-box security guarantees with negligible overhead.

**Link**: [arxiv](http://arxiv.org/abs/2404.11121v2),  [pdf](http://arxiv.org/pdf/2404.11121v2)

**Tags**: cs.CR cs.AI 



### Physical Adversarial Attack meets Computer Vision: A Decade Survey
**Authors**: Hui Wei, Hao Tang, Xuemei Jia, Zhixiang Wang, Hanxun Yu, Zhubo Li, Shin'ichi Satoh, Luc Van Gool, Zheng Wang

**Updated**: 2024-11-21T01:47:32Z

**Summary**: Despite the impressive achievements of Deep Neural Networks (DNNs) in computer vision, their vulnerability to adversarial attacks remains a critical concern. Extensive research has demonstrated that incorporating sophisticated perturbations into input images can lead to a catastrophic degradation in DNNs' performance. This perplexing phenomenon not only exists in the digital space but also in the physical world. Consequently, it becomes imperative to evaluate the security of DNNs-based systems to ensure their safe deployment in real-world scenarios, particularly in security-sensitive applications. To facilitate a profound understanding of this topic, this paper presents a comprehensive overview of physical adversarial attacks. Firstly, we distill four general steps for launching physical adversarial attacks. Building upon this foundation, we uncover the pervasive role of artifacts carrying adversarial perturbations in the physical world. These artifacts influence each step. To denote them, we introduce a new term: adversarial medium. Then, we take the first step to systematically evaluate the performance of physical adversarial attacks, taking the adversarial medium as a first attempt. Our proposed evaluation metric, hiPAA, comprises six perspectives: Effectiveness, Stealthiness, Robustness, Practicability, Aesthetics, and Economics. We also provide comparative results across task categories, together with insightful observations and suggestions for future research directions.

**Link**: [arxiv](http://arxiv.org/abs/2209.15179v4),  [pdf](http://arxiv.org/pdf/2209.15179v4)

**Tags**: cs.CV 



### NewsInterview: a Dataset and a Playground to Evaluate LLMs' Ground Gap   via Informational Interviews
**Authors**: Michael Lu, Hyundong Justin Cho, Weiyan Shi, Jonathan May, Alexander Spangher

**Updated**: 2024-11-21T01:37:38Z

**Summary**: Large Language Models (LLMs) have demonstrated impressive capabilities in generating coherent text but often struggle with grounding language and strategic dialogue. To address this gap, we focus on journalistic interviews, a domain rich in grounding communication and abundant in data. We curate a dataset of 40,000 two-person informational interviews from NPR and CNN, and reveal that LLMs are significantly less likely than human interviewers to use acknowledgements and to pivot to higher-level questions. Realizing that a fundamental deficit exists in multi-turn planning and strategic thinking, we develop a realistic simulated environment, incorporating source personas and persuasive elements, in order to facilitate the development of agents with longer-horizon rewards. Our experiments show that while source LLMs mimic human behavior in information sharing, interviewer LLMs struggle with recognizing when questions are answered and engaging persuasively, leading to suboptimal information extraction across model size and capability. These findings underscore the need for enhancing LLMs' strategic dialogue capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2411.13779v1),  [pdf](http://arxiv.org/pdf/2411.13779v1)

**Tags**: cs.CL cs.AI cs.LG 



### Benchmarking GPT-4 against Human Translators: A Comprehensive Evaluation   Across Languages, Domains, and Expertise Levels
**Authors**: Jianhao Yan, Pingchuan Yan, Yulong Chen, Jing Li, Xianchao Zhu, Yue Zhang

**Updated**: 2024-11-21T01:12:46Z

**Summary**: This study presents a comprehensive evaluation of GPT-4's translation capabilities compared to human translators of varying expertise levels. Through systematic human evaluation using the MQM schema, we assess translations across three language pairs (Chinese$\longleftrightarrow$English, Russian$\longleftrightarrow$English, and Chinese$\longleftrightarrow$Hindi) and three domains (News, Technology, and Biomedical). Our findings reveal that GPT-4 achieves performance comparable to junior-level translators in terms of total errors, while still lagging behind senior translators. Unlike traditional Neural Machine Translation systems, which show significant performance degradation in resource-poor language directions, GPT-4 maintains consistent translation quality across all evaluated language pairs. Through qualitative analysis, we identify distinctive patterns in translation approaches: GPT-4 tends toward overly literal translations and exhibits lexical inconsistency, while human translators sometimes over-interpret context and introduce hallucinations. This study represents the first systematic comparison between LLM and human translators across different proficiency levels, providing valuable insights into the current capabilities and limitations of LLM-based translation systems.

**Link**: [arxiv](http://arxiv.org/abs/2411.13775v1),  [pdf](http://arxiv.org/pdf/2411.13775v1)

**Tags**: cs.CL cs.AI 



### Chat Bankman-Fried: an Exploration of LLM Alignment in Finance
**Authors**: Claudia Biancotti, Carolina Camassa, Andrea Coletta, Oliver Giudice, Aldo Glielmo

**Updated**: 2024-11-21T01:10:30Z

**Summary**: Advancements in large language models (LLMs) have renewed concerns about AI alignment - the consistency between human and AI goals and values. As various jurisdictions enact legislation on AI safety, the concept of alignment must be defined and measured across different domains. This paper proposes an experimental framework to assess whether LLMs adhere to ethical and legal standards in the relatively unexplored context of finance. We prompt nine LLMs to impersonate the CEO of a financial institution and test their willingness to misuse customer assets to repay outstanding corporate debt. Beginning with a baseline configuration, we adjust preferences, incentives and constraints, analyzing the impact of each adjustment with logistic regression. Our findings reveal significant heterogeneity in the baseline propensity for unethical behavior of LLMs. Factors such as risk aversion, profit expectations, and regulatory environment consistently influence misalignment in ways predicted by economic theory, although the magnitude of these effects varies across LLMs. This paper highlights both the benefits and limitations of simulation-based, ex post safety testing. While it can inform financial authorities and institutions aiming to ensure LLM safety, there is a clear trade-off between generality and cost.

**Link**: [arxiv](http://arxiv.org/abs/2411.11853v2),  [pdf](http://arxiv.org/pdf/2411.11853v2)

**Tags**: cs.CY cs.AI cs.CL q-fin.GN 



### FastRAG: Retrieval Augmented Generation for Semi-structured Data
**Authors**: Amar Abane, Anis Bekri, Abdella Battou

**Updated**: 2024-11-21T01:00:25Z

**Summary**: Efficiently processing and interpreting network data is critical for the operation of increasingly complex networks. Recent advances in Large Language Models (LLM) and Retrieval-Augmented Generation (RAG) techniques have improved data processing in network management. However, existing RAG methods like VectorRAG and GraphRAG struggle with the complexity and implicit nature of semi-structured technical data, leading to inefficiencies in time, cost, and retrieval. This paper introduces FastRAG, a novel RAG approach designed for semi-structured data. FastRAG employs schema learning and script learning to extract and structure data without needing to submit entire data sources to an LLM. It integrates text search with knowledge graph (KG) querying to improve accuracy in retrieving context-rich information. Evaluation results demonstrate that FastRAG provides accurate question answering, while improving up to 90% in time and 85% in cost compared to GraphRAG.

**Link**: [arxiv](http://arxiv.org/abs/2411.13773v1),  [pdf](http://arxiv.org/pdf/2411.13773v1)

**Tags**: cs.NI cs.AI 



### Multi-Modal Forecaster: Jointly Predicting Time Series and Textual Data
**Authors**: Kai Kim, Howard Tsai, Rajat Sen, Abhimanyu Das, Zihao Zhou, Abhishek Tanpure, Mathew Luo, Rose Yu

**Updated**: 2024-11-21T00:52:53Z

**Summary**: Current forecasting approaches are largely unimodal and ignore the rich textual data that often accompany the time series due to lack of well-curated multimodal benchmark dataset. In this work, we develop TimeText Corpus (TTC), a carefully curated, time-aligned text and time dataset for multimodal forecasting. Our dataset is composed of sequences of numbers and text aligned to timestamps, and includes data from two different domains: climate science and healthcare. Our data is a significant contribution to the rare selection of available multimodal datasets. We also propose the Hybrid Multi-Modal Forecaster (Hybrid-MMF), a multimodal LLM that jointly forecasts both text and time series data using shared embeddings. However, contrary to our expectations, our Hybrid-MMF model does not outperform existing baselines in our experiments. This negative result highlights the challenges inherent in multimodal forecasting. Our code and data are available at https://github.com/Rose-STL-Lab/Multimodal_ Forecasting.

**Link**: [arxiv](http://arxiv.org/abs/2411.06735v2),  [pdf](http://arxiv.org/pdf/2411.06735v2)

**Tags**: cs.AI 



### An Evaluation-Driven Approach to Designing LLM Agents: Process and   Architecture
**Authors**: Boming Xia, Qinghua Lu, Liming Zhu, Zhenchang Xing, Dehai Zhao, Hao Zhang

**Updated**: 2024-11-21T00:34:30Z

**Summary**: The advent of Large Language Models (LLMs) has enabled the development of LLM agents capable of autonomously achieving under-specified goals and continuously evolving through post-deployment improvement, sometimes without requiring code or model updates. Conventional approaches, such as pre-defined test cases and code/model redevelopment pipelines, are inadequate for addressing the unique challenges of LLM agent development, particularly in terms of quality and risk control. This paper introduces an evaluation-driven design approach, inspired by test-driven development, to address these challenges. Through a multivocal literature review (MLR), we synthesize existing LLM evaluation methods and propose a novel process model and reference architecture specifically designed for LLM agents. The proposed approach integrates online and offline evaluations to support adaptive runtime adjustments and systematic offline redevelopment, improving runtime pipelines, artifacts, system architecture, and LLMs by continuously incorporating evaluation results, including fine-grained feedback from human and AI evaluators.

**Link**: [arxiv](http://arxiv.org/abs/2411.13768v1),  [pdf](http://arxiv.org/pdf/2411.13768v1)

**Tags**: cs.SE cs.AI 



### Tiny-Align: Bridging Automatic Speech Recognition and Large Language   Model on the Edge
**Authors**: Ruiyang Qin, Dancheng Liu, Gelei Xu, Zheyu Yan, Chenhui Xu, Yuting Hu, X. Sharon Hu, Jinjun Xiong, Yiyu Shi

**Updated**: 2024-11-21T00:29:58Z

**Summary**: The combination of Large Language Models (LLM) and Automatic Speech Recognition (ASR), when deployed on edge devices (called edge ASR-LLM), can serve as a powerful personalized assistant to enable audio-based interaction for users. Compared to text-based interaction, edge ASR-LLM allows accessible and natural audio interactions. Unfortunately, existing ASR-LLM models are mainly trained in high-performance computing environments and produce substantial model weights, making them difficult to deploy on edge devices. More importantly, to better serve users' personalized needs, the ASR-LLM must be able to learn from each distinct user, given that audio input often contains highly personalized characteristics that necessitate personalized on-device training. Since individually fine-tuning the ASR or LLM often leads to suboptimal results due to modality-specific limitations, end-to-end training ensures seamless integration of audio features and language understanding (cross-modal alignment), ultimately enabling a more personalized and efficient adaptation on edge devices. However, due to the complex training requirements and substantial computational demands of existing approaches, cross-modal alignment between ASR audio and LLM can be challenging on edge devices. In this work, we propose a resource-efficient cross-modal alignment framework that bridges ASR and LLMs on edge devices to handle personalized audio input. Our framework enables efficient ASR-LLM alignment on resource-constrained devices like NVIDIA Jetson Orin (8GB RAM), achieving 50x training time speedup while improving the alignment quality by more than 50\%. To the best of our knowledge, this is the first work to study efficient ASR-LLM alignment on resource-constrained edge devices.

**Link**: [arxiv](http://arxiv.org/abs/2411.13766v1),  [pdf](http://arxiv.org/pdf/2411.13766v1)

**Tags**: cs.SD cs.AI eess.AS 



### A dataset of questions on decision-theoretic reasoning in Newcomb-like   problems
**Authors**: Caspar Oesterheld, Emery Cooper, Miles Kodama, Linh Chi Nguyen, Ethan Perez

**Updated**: 2024-11-21T00:24:36Z

**Summary**: We introduce a dataset of natural-language questions in the decision theory of so-called Newcomb-like problems. Newcomb-like problems include, for instance, decision problems in which an agent interacts with a similar other agent, and thus has to reason about the fact that the other agent will likely reason in similar ways. Evaluating LLM reasoning about Newcomb-like problems is important because interactions between foundation-model-based agents will often be Newcomb-like. Some ways of reasoning about Newcomb-like problems may allow for greater cooperation between models.   Our dataset contains both capabilities questions (i.e., questions with a unique, uncontroversially correct answer) and attitude questions (i.e., questions about which decision theorists would disagree). We use our dataset for an investigation of decision-theoretical capabilities and expressed attitudes and their interplay in existing models (different models by OpenAI, Anthropic, Meta, GDM, Reka, etc.), as well as models under simple prompt-based interventions. We find, among other things, that attitudes vary significantly between existing models; that high capabilities are associated with attitudes more favorable toward so-called evidential decision theory; and that attitudes are consistent across different types of questions.

**Link**: [arxiv](http://arxiv.org/abs/2411.10588v2),  [pdf](http://arxiv.org/pdf/2411.10588v2)

**Tags**: cs.CL cs.AI I.2.7 



### A Framework for Evaluating LLMs Under Task Indeterminacy
**Authors**: Luke Guerdan, Hanna Wallach, Solon Barocas, Alexandra Chouldechova

**Updated**: 2024-11-21T00:15:44Z

**Summary**: Large language model (LLM) evaluations often assume there is a single correct response -- a gold label -- for each item in the evaluation corpus. However, some tasks can be ambiguous -- i.e., they provide insufficient information to identify a unique interpretation -- or vague -- i.e., they do not clearly indicate where to draw the line when making a determination. Both ambiguity and vagueness can cause task indeterminacy -- the condition where some items in the evaluation corpus have more than one correct response. In this paper, we develop a framework for evaluating LLMs under task indeterminacy. Our framework disentangles the relationships between task specification, human ratings, and LLM responses in the LLM evaluation pipeline. Using our framework, we conduct a synthetic experiment showing that evaluations that use the "gold label" assumption underestimate the true performance. We also provide a method for estimating an error-adjusted performance interval given partial knowledge about indeterminate items in the evaluation corpus. We conclude by outlining implications of our work for the research community.

**Link**: [arxiv](http://arxiv.org/abs/2411.13760v1),  [pdf](http://arxiv.org/pdf/2411.13760v1)

**Tags**: cs.LG cs.CL cs.HC 



### AttentionBreaker: Adaptive Evolutionary Optimization for Unmasking   Vulnerabilities in LLMs through Bit-Flip Attacks
**Authors**: Sanjay Das, Swastik Bhattacharya, Souvik Kundu, Shamik Kundu, Anand Menon, Arnab Raha, Kanad Basu

**Updated**: 2024-11-21T00:01:51Z

**Summary**: Large Language Models (LLMs) have revolutionized natural language processing (NLP), excelling in tasks like text generation and summarization. However, their increasing adoption in mission-critical applications raises concerns about hardware-based threats, particularly bit-flip attacks (BFAs). BFAs, enabled by fault injection methods such as Rowhammer, target model parameters in memory, compromising both integrity and performance. Identifying critical parameters for BFAs in the vast parameter space of LLMs poses significant challenges. While prior research suggests transformer-based architectures are inherently more robust to BFAs compared to traditional deep neural networks, we challenge this assumption. For the first time, we demonstrate that as few as three bit-flips can cause catastrophic performance degradation in an LLM with billions of parameters. Current BFA techniques are inadequate for exploiting this vulnerability due to the difficulty of efficiently identifying critical parameters within the immense parameter space. To address this, we propose AttentionBreaker, a novel framework tailored for LLMs that enables efficient traversal of the parameter space to identify critical parameters. Additionally, we introduce GenBFA, an evolutionary optimization strategy designed to refine the search further, isolating the most critical bits for an efficient and effective attack. Empirical results reveal the profound vulnerability of LLMs to AttentionBreaker. For example, merely three bit-flips (4.129 x 10^-9% of total parameters) in the LLaMA3-8B-Instruct 8-bit quantized (W8) model result in a complete performance collapse: accuracy on MMLU tasks drops from 67.3% to 0%, and Wikitext perplexity skyrockets from 12.6 to 4.72 x 10^5. These findings underscore the effectiveness of AttentionBreaker in uncovering and exploiting critical vulnerabilities within LLM architectures.

**Link**: [arxiv](http://arxiv.org/abs/2411.13757v1),  [pdf](http://arxiv.org/pdf/2411.13757v1)

**Tags**: cs.CR cs.AI cs.LG 



### Establishing Quantum-Secured Channels in Large-Scale Optical Networks
**Authors**: Farzam Toudeh-Fallah

**Updated**: 2024-11-20T23:56:12Z

**Summary**: Quantum-secured optical channels based on Quantum Key Distribution technology have generated a significant global interest. Although the maturity level of the short distance (less than 100 km) quantum-secured channels is at a deployment level, instituting such channels over long distance faces technological challenges, which is the subject of a world-wide research. In this article an industry perspective on establishing quantum-secured channels in large-scale optical networks in operational environments will be discussed, including the vision, requirements, and technical analysis of different approaches for establishing such channels.

**Link**: [arxiv](http://arxiv.org/abs/2408.06587v2),  [pdf](http://arxiv.org/pdf/2408.06587v2)

**Tags**: quant-ph 



