# Arxiv Results
## Keyword: kv cache 
 ### Efficient Prompt Compression with Evaluator Heads for Long-Context   Transformer Inference
**Authors**: Weizhi Fei, Xueyan Niu, Guoqing Xie, Yingqing Liu, Bo Bai, Wei Han

**Updated**: 2025-02-05T09:35:38Z

**Summary**: Although applications involving long-context inputs are crucial for the effective utilization of large language models (LLMs), they also result in increased computational costs and reduced performance. To address this challenge, we propose an efficient, training-free prompt compression method that retains key information within compressed prompts. We identify specific attention heads in transformer-based LLMs, which we designate as evaluator heads, that are capable of selecting tokens in long inputs that are most significant for inference. Building on this discovery, we develop EHPC, an Evaluator Head-based Prompt Compression method, which enables LLMs to rapidly "skim through" input prompts by leveraging only the first few layers with evaluator heads during the pre-filling stage, subsequently passing only the important tokens to the model for inference. EHPC achieves state-of-the-art results across two mainstream benchmarks: prompt compression and long-context inference acceleration. Consequently, it effectively reduces the complexity and costs associated with commercial API calls. We further demonstrate that EHPC attains competitive results compared to key-value cache-based acceleration methods, thereby highlighting its potential to enhance the efficiency of LLMs for long-context tasks.

**Link**: [arxiv](http://arxiv.org/abs/2501.12959v2),  [pdf](http://arxiv.org/pdf/2501.12959v2)

**Tags**: cs.CL 



### A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference
**Authors**: You Wu, Haoyi Wu, Kewei Tu

**Updated**: 2025-02-05T08:22:05Z

**Summary**: Recently, sharing key-value (KV) cache across layers has been found effective in efficient inference of large language models (LLMs). To systematically investigate different techniques of cross-layer KV sharing, we propose a unified framework that covers several recent methods and their novel variants. We conduct comprehensive experiments on all the configurations of the framework, evaluating their generation throughput and performance in language modeling and downstream tasks. We find that when reducing the size of the KV cache by 2$\times$, most configurations can achieve higher throughput than standard transformers while maintaining competitive performance. When further reducing the size of the KV cache, however, pairing queries of all layers with KVs of upper layers performs better, at the expense of additional training cost and prefilling latency. We hope that this work will help users make more informed choices of cross-layer KV sharing approaches and facilitate future research on efficient LLM inference.

**Link**: [arxiv](http://arxiv.org/abs/2410.14442v2),  [pdf](http://arxiv.org/pdf/2410.14442v2)

**Tags**: cs.CL 



### Qrazor: Reliable and Effortless 4-bit LLM Quantization by Significant   Data Razoring
**Authors**: Dongyoung Lee, Seungkyu Choi, Ik Joon Chang

**Updated**: 2025-02-05T08:10:45Z

**Summary**: Large-scale language models (LLMs) excel in language processing tasks but face deployment challenges due to high memory and computational demands. While low-bit quantization, such as 4-bit techniques, offers a potential solution, these methods often suffer from significant accuracy loss or require considerable effort for implementation such as reordering, rotation, etc. To address these challenges, we propose QRazor, a simple yet effective quantization scheme that enables 4-bit quantization of weights, activations, and KV cache in transformer-based LLMs. QRazor operates in two stages: first, quantizing data using 8 or 16-bit integers as a basis with absolute max scaling to preserve accuracy close to full-precision models, and second, compressing the quantized data to 4-bit using our significant data razoring (SDR) technique, which retains only the four most salient bits. Without any additional requirment of fine-tuning or additional training, QRazor achieves performance similar or better compared to state-of-the-art in 4-bit quantization method, surpassing Smoothquant and QLLM by over 12 points and Quarot(RTN) by more than 2.9 points in zero-shot reasoning task accuracy on the LLaMA2-7B model. Additionally, we introduce an integer-based arithmetic unit optimized for QRazor, allowing direct low-precision operations on SDR data without decompression.

**Link**: [arxiv](http://arxiv.org/abs/2501.13331v2),  [pdf](http://arxiv.org/pdf/2501.13331v2)

**Tags**: cs.LG 



### Accessible and Portable LLM Inference by Compiling Computational Graphs   into SQL
**Authors**: Wenbo Sun, Qiming Guo, Wenlu Wang, Rihan Hai

**Updated**: 2025-02-05T01:36:40Z

**Summary**: Serving large language models (LLMs) often demands specialized hardware, dedicated frameworks, and substantial development efforts, which restrict their accessibility, especially for edge devices and organizations with limited technical resources. We propose a novel compiler that translates LLM inference graphs into SQL queries, enabling relational databases, one of the most widely used and mature software systems globally, to serve as the runtime. By mapping neural operators such as matrix multiplication and attention into relational primitives like joins and aggregations, our approach leverages database capabilities, including disk-based data management and native caching. Supporting key transformer components, such as attention mechanisms and key-value caching, our system generates SQL pipelines for end-to-end LLM inference. Using the Llama3 family as a case study, we demonstrate up to 30x speedup in token generation for memory-constrained scenarios comparable to competitive CPU-based frameworks. Our work offers an accessible, portable, and efficient solution, facilitating the serving of LLMs across diverse deployment environments.

**Link**: [arxiv](http://arxiv.org/abs/2502.02818v1),  [pdf](http://arxiv.org/pdf/2502.02818v1)

**Tags**: cs.DB cs.LG 



### Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning
**Authors**: Chaofan Lin, Jiaming Tang, Shuo Yang, Hanshuo Wang, Tian Tang, Boyu Tian, Ion Stoica, Song Han, Mingyu Gao

**Updated**: 2025-02-04T23:26:10Z

**Summary**: Leveraging attention sparsity to accelerate long-context large language models (LLMs) has been a hot research topic. However, current algorithms such as sparse attention or key-value (KV) cache compression tend to use a fixed budget, which presents a significant challenge during deployment because it fails to account for the dynamic nature of real-world scenarios, where the optimal balance between accuracy and efficiency can vary greatly. In this paper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse attention can surprisingly achieve adaptive budgeting. Based on this, we propose Twilight, a framework to bring adaptive sparsity to any existing sparse attention algorithm without sacrificing their accuracy. Empirical results show that Twilight can adaptively prune at most 98% of redundant tokens, leading to $15.4\times$ acceleration in self-attention operations and $3.9\times$ acceleration in end-to-end per token latency in long context LLM decoding.

**Link**: [arxiv](http://arxiv.org/abs/2502.02770v1),  [pdf](http://arxiv.org/pdf/2502.02770v1)

**Tags**: cs.LG cs.CL 



### Cache is King: Smart Page Eviction with eBPF
**Authors**: Tal Zussman, Ioannis Zarkadas, Jeremy Carin, Andrew Cheng, Hubertus Franke, Jonas Pfefferle, Asaf Cidon

**Updated**: 2025-02-04T22:37:17Z

**Summary**: The page cache is a central part of an OS. It reduces repeated accesses to storage by deciding which pages to retain in memory. As a result, the page cache has a significant impact on the performance of many applications. However, its one-size-fits-all eviction policy performs poorly in many workloads. While the systems community has experimented with a plethora of new and adaptive eviction policies in non-OS settings (e.g., key-value stores, CDNs), it is very difficult to implement such policies in the page cache, due to the complexity of modifying kernel code. To address these shortcomings, we design a novel eBPF-based framework for the Linux page cache, called $\texttt{cachebpf}$, that allows developers to customize the page cache without modifying the kernel. $\texttt{cachebpf}$ enables applications to customize the page cache policy for their specific needs, while also ensuring that different applications' policies do not interfere with each other and preserving the page cache's ability to share memory across different processes. We demonstrate the flexibility of $\texttt{cachebpf}$'s interface by using it to implement several eviction policies. Our evaluation shows that it is indeed beneficial for applications to customize the page cache to match their workloads' unique properties, and that they can achieve up to 70% higher throughput and 58% lower tail latency.

**Link**: [arxiv](http://arxiv.org/abs/2502.02750v1),  [pdf](http://arxiv.org/pdf/2502.02750v1)

**Tags**: cs.OS 



### CReIS: Computation Reuse through Image Similarity in ICN-Based Edge   Computing
**Authors**: Atiyeh Javaheri, Ali Bohlooli, Kamal Jamshidi

**Updated**: 2025-02-04T18:39:10Z

**Summary**: At the edge, there is a high level of similarity in computing. One approach that has been proposed to enhance the efficiency of edge computing is computation reuse, which eliminates redundant computations. Edge computing is integrated with the ICN architecture, capitalizing on its inherent intelligence to facilitate computation reuse and reduce redundancies in computing operations. In many past works, ICN's ability to enable computation reuse through caching has been limited. In this context, a new approach is proposed that considers computation requests with similar input data, which yield identical results, as equivalent. This method facilitates computation reuse through caching in ICN. The use of approximate results to reduce redundant computations without requiring high accuracy in input matching is provided. This concept is termed the Similarity Index, which effectively considers images to be similar despite minor changes in the angle of photography. The Similarity Index is determined through an algorithm known as HNSW and utilizes the SIFT descriptor to identify similar data. This approach helps reduce user latency times by providing quick access to results. The evaluation, simulated using the ndnSIM tool, showed an 86% improvement in completion time compared to scenarios without computation reuse, whereas previous works reported only a 70% improvement. To strengthen this method, an analytical model for computing request transfer considering computation reuse in ICN-based edge computing is provided. To assess the accuracy of the model, several evaluations have been conducted in the simulator by varying the parameters, resulting in a maximum error percentage of approximately 16%.

**Link**: [arxiv](http://arxiv.org/abs/2502.02564v1),  [pdf](http://arxiv.org/pdf/2502.02564v1)

**Tags**: cs.NI 



### Decentralized Federated Learning with Model Caching on Mobile Agents
**Authors**: Xiaoyu Wang, Guojun Xiong, Houwei Cao, Jian Li, Yong Liu

**Updated**: 2025-02-04T17:14:22Z

**Summary**: Federated Learning (FL) trains a shared model using data and computation power on distributed agents coordinated by a central server. Decentralized FL (DFL) utilizes local model exchange and aggregation between agents to reduce the communication and computation overheads on the central server. However, when agents are mobile, the communication opportunity between agents can be sporadic, largely hindering the convergence and accuracy of DFL. In this paper, we propose Cached Decentralized Federated Learning (Cached-DFL) to investigate delay-tolerant model spreading and aggregation enabled by model caching on mobile agents. Each agent stores not only its own model, but also models of agents encountered in the recent past. When two agents meet, they exchange their own models as well as the cached models. Local model aggregation utilizes all models stored in the cache. We theoretically analyze the convergence of Cached-DFL, explicitly taking into account the model staleness introduced by caching. We design and compare different model caching algorithms for different DFL and mobility scenarios. We conduct detailed case studies in a vehicular network to systematically investigate the interplay between agent mobility, cache staleness, and model convergence. In our experiments, Cached-DFL converges quickly, and significantly outperforms DFL without caching.

**Link**: [arxiv](http://arxiv.org/abs/2408.14001v2),  [pdf](http://arxiv.org/pdf/2408.14001v2)

**Tags**: cs.LG cs.DC 



### EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU   Utilization
**Authors**: Yize Wu, Ke Gao, Yanjun Wu

**Updated**: 2025-02-04T17:09:21Z

**Summary**: Speculative decoding is an effective and lossless method for Large Language Model (LLM) inference acceleration. It employs a smaller model to generate a draft token sequence, which is then verified by the original base model. In multi-GPU systems, inference latency can be further reduced through tensor parallelism (TP), while the optimal TP size of the draft model is typically smaller than that of the base model, leading to GPU idling during the drafting stage. To solve this problem, we propose EasySpec, a layer-parallel speculation strategy that optimizes the efficiency of multi-GPU utilization.EasySpec breaks the sequential execution order of layers in the drafting model, enabling multi-layer parallelization across devices, albeit with some induced approximation errors. After each drafting-and-verification iteration, the draft model's key-value (KV) cache is calibrated in a single forward pass, preventing long-term error accumulation at minimal additional latency. We evaluated EasySpec on several mainstream open-source LLMs, using smaller versions of models from the same series as drafters. The results demonstrate that EasySpec can achieve a peak speedup of 4.17x compared to vanilla decoding, while preserving the original distribution of the base LLMs. Specifically, the drafting stage can be accelerated by up to 1.62x with a maximum accuracy drop of only 7%, requiring no training or fine-tuning on the draft models.

**Link**: [arxiv](http://arxiv.org/abs/2502.02493v1),  [pdf](http://arxiv.org/pdf/2502.02493v1)

**Tags**: cs.LG I.2.11 



### H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed   Criticality Systems
**Authors**: Afonso Oliveira, Diogo Costa, Gonçalo Moreira, José Martins, Sandro Pinto

**Updated**: 2025-02-04T16:03:52Z

**Summary**: Recent advancements in fields such as automotive and aerospace have driven a growing demand for robust computational resources. Applications that were once designed for basic MCUs are now deployed on highly heterogeneous SoC platforms. While these platforms deliver the necessary computational performance, they also present challenges related to resource sharing and predictability. These challenges are particularly pronounced when consolidating safety and non-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to adhere to strict SWaP-C requirements. MCS consolidation on shared platforms requires stringent spatial and temporal isolation to comply with functional safety standards. Virtualization, mainly leveraged by hypervisors, is a key technology that ensures spatial isolation across multiple OSes and applications; however, ensuring temporal isolation remains challenging due to contention on shared hardwar resources, which impacts real-time performance and predictability. To mitigate this problem, several strategies as cache coloring and memory bandwidth reservation have been proposed. Although cache coloring is typically implemented on state-of-the-art hypervisors, memory bandwidth reservation approaches are commonly implemented at the Linux kernel level or rely on dedicated hardware and typically do not consider the concept of VMs that can run different OSes. To fill the gap between current memory bandwidth reservation solutions and the deployment of MCSs that operate on a hypervisor, this work introduces H-MBR, an open-source VM-centric memory bandwidth reservation mechanism. H-MBR features (i) VM-centric bandwidth reservation, (ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results evidenced no overhead on non-regulated workloads, and negligible overhead (<1%) for regulated workloads for regulation periods of 2 us or higher.

**Link**: [arxiv](http://arxiv.org/abs/2502.02437v1),  [pdf](http://arxiv.org/pdf/2502.02437v1)

**Tags**: cs.DC cs.SY eess.SY 



### A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals
**Authors**: Róbert Busa-Fekete, Julian Zimmert, András György, Linhai Qiu, Tzu-Wei Sung, Hao Shen, Hyomin Choi, Sharmila Subramaniam, Li Xiao

**Updated**: 2025-02-04T15:55:10Z

**Summary**: Web refresh crawling is the problem of keeping a cache of web pages fresh, that is, having the most recent copy available when a page is requested, given a limited bandwidth available to the crawler. Under the assumption that the change and request events, resp., to each web page follow independent Poisson processes, the optimal scheduling policy was derived by Azar et al. 2018. In this paper, we study an extension of this problem where side information indicating content changes, such as various types of web pings, for example, signals from sitemaps, content delivery networks, etc., is available. Incorporating such side information into the crawling policy is challenging, because (i) the signals can be noisy with false positive events and with missing change events; and (ii) the crawler should achieve a fair performance over web pages regardless of the quality of the side information, which might differ from web page to web page. We propose a scalable crawling algorithm which (i) uses the noisy side information in an optimal way under mild assumptions; (ii) can be deployed without heavy centralized computation; (iii) is able to crawl web pages at a constant total rate without spikes in the total bandwidth usage over any time interval, and automatically adapt to the new optimal solution when the total bandwidth changes without centralized computation. Experiments clearly demonstrate the versatility of our approach.

**Link**: [arxiv](http://arxiv.org/abs/2502.02430v1),  [pdf](http://arxiv.org/pdf/2502.02430v1)

**Tags**: stat.ML cs.IR cs.LG 



### Random Adaptive Cache Placement Policy
**Authors**: Vrushank Ahire, Pranav Menon, Aniruddh Muley, Abhinandan S. Prasad

**Updated**: 2025-02-04T14:33:44Z

**Summary**: This paper presents a new hybrid cache replacement algorithm that combines random allocation with a modified V-Way cache implementation. Our RAC adapts to complex cache access patterns and optimizes cache usage by improving the utilization of cache sets, unlike traditional cache policies. The algorithm utilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic allocation and flexible tag management. RAC extends the V-Way cache design and its variants by optimizing tag and data storage for enhanced efficiency.   We evaluated the algorithm using the ChampSim simulator with four diverse benchmark traces and observed significant improvements in cache hit rates up to 80.82% hit rate. Although the improvements in the instructions per cycle (IPC) were moderate, our findings emphasize the algorithm's potential to enhance cache utilization and reduce memory access times.

**Link**: [arxiv](http://arxiv.org/abs/2502.02349v1),  [pdf](http://arxiv.org/pdf/2502.02349v1)

**Tags**: cs.AR cs.DC cs.OS cs.PF 



### LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with   Effortless Adaptation
**Authors**: Xuan Zhang, Fengzhuo Zhang, Cunxiao Du, Chao Du, Tianyu Pang, Wei Gao, Min Lin

**Updated**: 2025-02-04T13:45:37Z

**Summary**: Scaling language models to handle longer contexts introduces substantial memory challenges due to the growing cost of key-value (KV) caches. Motivated by the efficiency gains of hybrid models and the broad availability of pretrained large transformer backbones, we explore transitioning transformer models into hybrid architectures for a more efficient generation. In this work, we propose LightTransfer, a lightweight method that transforms models such as LLaMA into hybrid variants. Our approach identifies lazy layers -- those focusing on recent or initial tokens -- and replaces their full attention with streaming attention. This transformation can be performed without any training for long-context understanding tasks or with minimal fine-tuning for o1-like long reasoning generation tasks that require stronger reasoning capabilities. Experiments across diverse benchmarks and models (e.g., LLaMA, Mistral, QwQ-STILL) demonstrate that, even when half of the layers are identified as lazy, LightTransfer achieves up to 2.17$\times$ throughput improvement with minimal performance loss ($<1.5\%$ on LongBench) and achieves 53.3\% on math benchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL.

**Link**: [arxiv](http://arxiv.org/abs/2410.13846v2),  [pdf](http://arxiv.org/pdf/2410.13846v2)

**Tags**: cs.CL cs.AI cs.LG 



### VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive   Token Caching in Robotic Manipulation
**Authors**: Siyu Xu, Yunke Wang, Chenghao Xia, Dihao Zhu, Tao Huang, Chang Xu

**Updated**: 2025-02-04T09:48:14Z

**Summary**: Vision-Language-Action (VLA) model can process instructions and visual perception to directly generate actions as output in an end-to-end fashion due to its strong multi-modal reasoning capabilities. While the performance of VLA models is promising, their computational cost can be substantial. This raises challenge for applying them on robotics tasks, which requires real-time decision-making to respond quickly to environmental changes. Since robotic control involves sequential decision-making, the visual input often exhibits minimal variation between successive steps. A natural idea is to reuse the computational results of unchanged visual tokens from the last step. Motivated by this idea, we propose VLA-Cache, an efficient vision-language-action model. VLA-Cache incorporates a token-selection mechanism that compares the visual input at each step with the input from the previous step, adaptively identifying visual tokens with minimal changes. The computational results for these unchanged tokens are then reused in subsequent steps via KV-cache, thereby significantly improving the efficiency of the VLA-Cache model. Experimental results on both simulation (e.g., LIBERO benchmark and SIMPLER) and real-world robot valid VLA-Cache can achieve practical acceleration with minimal sacrifice in success rate.

**Link**: [arxiv](http://arxiv.org/abs/2502.02175v1),  [pdf](http://arxiv.org/pdf/2502.02175v1)

**Tags**: cs.RO cs.CV cs.LG 



### PolarQuant: Quantizing KV Caches with Polar Transformation
**Authors**: Insu Han, Praneeth Kacham, Amin Karbasi, Vahab Mirrokni, Amir Zandieh

**Updated**: 2025-02-04T08:52:13Z

**Summary**: Large language models (LLMs) require significant memory to store Key-Value (KV) embeddings in their KV cache, especially when handling long-range contexts. Quantization of these KV embeddings is a common technique to reduce memory consumption. This work introduces PolarQuant, a novel quantization method employing random preconditioning and polar transformation. Our method transforms the KV embeddings into polar coordinates using an efficient recursive algorithm and then quantizes resulting angles. Our key insight is that, after random preconditioning, the angles in the polar representation exhibit a tightly bounded and highly concentrated distribution with an analytically computable form. This nice distribution eliminates the need for explicit normalization, a step required by traditional quantization methods which introduces significant memory overhead because quantization parameters (e.g., zero point and scale) must be stored in full precision per each data block. PolarQuant bypasses this normalization step, enabling substantial memory savings. The long-context evaluation demonstrates that PolarQuant compresses the KV cache by over x4.2 while achieving the best quality scores compared to the state-of-the-art methods.

**Link**: [arxiv](http://arxiv.org/abs/2502.02617v1),  [pdf](http://arxiv.org/pdf/2502.02617v1)

**Tags**: cs.LG cs.AI 



### SepLLM: Accelerate Large Language Models by Compressing One Segment into   One Separator
**Authors**: Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xiaozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li, Weiyang Liu, Chao Huang

**Updated**: 2025-02-04T08:16:31Z

**Summary**: Large Language Models (LLMs) have exhibited exceptional performance across a spectrum of natural language processing tasks. However, their substantial sizes pose considerable challenges, particularly in computational demands and inference speed, due to their quadratic complexity. In this work, we have identified a key pattern: certain seemingly meaningless special tokens (i.e., separators) contribute disproportionately to attention scores compared to semantically meaningful tokens. This observation suggests that information of the segments between these separator tokens can be effectively condensed into the separator tokens themselves without significant information loss. Guided by this insight, we introduce SepLLM, a plug-and-play framework that accelerates inference by compressing these segments and eliminating redundant tokens. Additionally, we implement efficient kernels for training acceleration. Experimental results across training-free, training-from-scratch, and post-training settings demonstrate SepLLM's effectiveness. Notably, using the Llama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the GSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in streaming settings, SepLLM effectively processes sequences of up to 4 million tokens or more while maintaining consistent language modeling capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2412.12094v4),  [pdf](http://arxiv.org/pdf/2412.12094v4)

**Tags**: cs.CL cs.AI cs.LG 



### LoRA-TTT: Low-Rank Test-Time Training for Vision-Language Models
**Authors**: Yuto Kojima, Jiarui Xu, Xueyan Zou, Xiaolong Wang

**Updated**: 2025-02-04T07:40:26Z

**Summary**: The rapid advancements in vision-language models (VLMs), such as CLIP, have intensified the need to address distribution shifts between training and testing datasets. Although prior Test-Time Training (TTT) techniques for VLMs have demonstrated robust performance, they predominantly rely on tuning text prompts, a process that demands substantial computational resources and is heavily dependent on entropy-based loss. In this paper, we propose LoRA-TTT, a novel TTT method that leverages Low-Rank Adaptation (LoRA), applied exclusively to the image encoder of VLMs. By introducing LoRA and updating only its parameters during test time, our method offers a simple yet effective TTT approach, retaining the model's initial generalization capability while achieving substantial performance gains with minimal memory and runtime overhead. Additionally, we introduce a highly efficient reconstruction loss tailored for TTT. Our method can adapt to diverse domains by combining these two losses, without increasing memory consumption or runtime. Extensive experiments on two benchmarks, covering 15 datasets, demonstrate that our method improves the zero-shot top-1 accuracy of CLIP-ViT-B/16 by an average of 5.79% on the OOD benchmark and 1.36% on the fine-grained benchmark, efficiently surpassing test-time prompt tuning, without relying on any external models or cache.

**Link**: [arxiv](http://arxiv.org/abs/2502.02069v1),  [pdf](http://arxiv.org/pdf/2502.02069v1)

**Tags**: cs.CV 



### MPIC: Position-Independent Multimodal Context Caching System for   Efficient MLLM Serving
**Authors**: Shiju Zhao, Junhao Hu, Rongxiao Huang, Jiaqi Zheng, Guihai Chen

**Updated**: 2025-02-04T03:13:09Z

**Summary**: The context caching technique is employed to accelerate the Multimodal Large Language Model (MLLM) inference by prevailing serving platforms currently. However, this approach merely reuses the Key-Value (KV) cache of the initial sequence of prompt, resulting in full KV cache recomputation even if the prefix differs slightly. This becomes particularly inefficient in the context of interleaved text and images, as well as multimodal retrieval-augmented generation. This paper proposes position-independent caching as a more effective approach for multimodal information management. We have designed and implemented a caching system, named MPIC, to address both system-level and algorithm-level challenges. MPIC stores the KV cache on local or remote disks when receiving multimodal data, and calculates and loads the KV cache in parallel during inference. To mitigate accuracy degradation, we have incorporated integrated reuse and recompute mechanisms within the system. The experimental results demonstrate that MPIC can achieve up to 54% reduction in response time compared to existing context caching systems, while maintaining negligible or no accuracy loss.

**Link**: [arxiv](http://arxiv.org/abs/2502.01960v1),  [pdf](http://arxiv.org/pdf/2502.01960v1)

**Tags**: cs.LG 



### Can LLMs Maintain Fundamental Abilities under KV Cache Compression?
**Authors**: Xiang Liu, Zhenheng Tang, Hong Chen, Peijie Dong, Zeyu Li, Xiuze Zhou, Bo Li, Xuming Hu, Xiaowen Chu

**Updated**: 2025-02-04T02:23:06Z

**Summary**: This paper investigates an under-explored challenge in large language models (LLMs): the impact of KV cache compression methods on LLMs' fundamental capabilities. While existing methods achieve impressive compression ratios on long-context benchmarks, their effects on core model capabilities remain understudied. We present a comprehensive empirical study evaluating prominent KV cache compression methods across diverse tasks, spanning world knowledge, commonsense reasoning, arithmetic reasoning, code generation, safety, and long-context understanding and generation.Our analysis reveals that KV cache compression methods exhibit task-specific performance degradation. Arithmetic reasoning tasks prove particularly sensitive to aggressive compression, with different methods showing performance drops of $17.4\%$-$43.3\%$. Notably, the DeepSeek R1 Distill model exhibits more robust compression tolerance compared to instruction-tuned models, showing only $9.67\%$-$25.53\%$ performance degradation. Based on our analysis of attention patterns and cross-task compression performance, we propose ShotKV, a novel compression approach that distinctly handles prefill and decoding phases while maintaining shot-level semantic coherence. Empirical results show that ShotKV achieves $9\%$-$18\%$ performance improvements on long-context generation tasks under aggressive compression ratios.

**Link**: [arxiv](http://arxiv.org/abs/2502.01941v1),  [pdf](http://arxiv.org/pdf/2502.01941v1)

**Tags**: cs.CL cs.AI 



### ResQ: Mixed-Precision Quantization of Large Language Models with   Low-Rank Residuals
**Authors**: Utkarsh Saxena, Sayeh Sharify, Kaushik Roy, Xin Wang

**Updated**: 2025-02-03T21:45:32Z

**Summary**: Post-training quantization (PTQ) of large language models (LLMs) holds the promise in reducing the prohibitive computational cost at inference time. Quantization of all weight, activation and key-value (KV) cache tensors to 4-bit without significantly degrading generalizability is challenging, due to the high quantization error caused by extreme outliers in activations. To tackle this problem, we propose ResQ, a PTQ method that pushes further the state-of-the-art. By means of principal component analysis (PCA), it identifies a low-rank subspace (in practice 1/8 of the hidden dimension) in which activation variances are highest, and keep the coefficients within this subspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit. Within each subspace, invariant random rotation is applied to further suppress outliers. We show that this is a provably optimal mixed precision quantization scheme that minimizes error. With the Llama and Qwen2.5 families of models, we demonstrate that ResQ outperforms recent uniform and mixed precision PTQ methods on a variety of benchmarks, achieving up to 33\% lower perplexity on Wikitext than the next best method SpinQuant, and upto 3\times speedup over 16-bit baseline. Code is available at https://github.com/utkarsh-dmx/project-resq.

**Link**: [arxiv](http://arxiv.org/abs/2412.14363v2),  [pdf](http://arxiv.org/pdf/2412.14363v2)

**Tags**: cs.LG cs.CL 



### General kinetic ion induced electron emission model for metallic walls   applied to biased Z-pinch electrodes
**Authors**: Chirag R. Skolar, Kolter Bradshaw, Manaure Francisquez, Lucio Murillo, Vignesh Krishna Kumar, Bhuvana Srinivasan

**Updated**: 2025-02-03T20:30:25Z

**Summary**: A generalized kinetic ion induced electron emission (IIEE) model is developed to obtain the emitted electron energy spectrum for a distribution of ion impacts on a metallic surface. This framework is implemented as a boundary condition for the continuum kinetic Boltzmann equation. The IIEE model is used to study how emissions affect sheath formation near biased Z-pinch electrodes. 1X-1V (one spatial and one velocity dimension) Boltzmann-Poisson simulations are performed for a proton-electron plasma doubly bounded by two biased copper electrodes with and without IIEE at bias potentials from 0 kV to 9 kV. The ions are accelerated to higher energies by the sheath potentials at the electrodes inducing electron emission. The secondary electron yield (SEY), defined as the ratio of the flux of emitted electrons to impacting ions, increases with bias potential at both electrodes, but more significantly at the cathode. Despite the SEY crossing 1 at 7 kV, a classical sheath, rather than a space-charge limited or inverse sheath, forms for all cases. The emitted electrons present as a beam that is accelerated by the sheath potential into the domain resulting in increased electron temperatures due to collisions. For bias potentials greater than 2 kV, the potential difference at the cathode is sufficiently strong for emissive heating to increase the plasma potential compared to emissionless simulations. The emitted electrons increase the current in the domain from 130 kA to 199 kA closely matching the experimental value of 200 kA.

**Link**: [arxiv](http://arxiv.org/abs/2502.01802v1),  [pdf](http://arxiv.org/pdf/2502.01802v1)

**Tags**: physics.plasm-ph 



### Scaling Embedding Layers in Language Models
**Authors**: Da Yu, Edith Cohen, Badih Ghazi, Yangsibo Huang, Pritish Kamath, Ravi Kumar, Daogao Liu, Chiyuan Zhang

**Updated**: 2025-02-03T18:59:32Z

**Summary**: We propose SCONE ($\textbf{S}$calable, $\textbf{C}$ontextualized, $\textbf{O}$ffloaded, $\textbf{N}$-gram $\textbf{E}$mbedding), a method for extending input embedding layers to enhance language model performance as layer size scales. To avoid increased decoding costs, SCONE retains the original vocabulary while introducing embeddings for a set of frequent $n$-grams. These embeddings provide contextualized representation for each input token and are learned with a separate model during training. During inference, they are precomputed and stored in off-accelerator memory with minimal impact on inference speed. SCONE enables two new scaling strategies: increasing the number of cached $n$-gram embeddings and scaling the model used to learn them, all while maintaining fixed inference-time FLOPS. We show that scaling both aspects allows SCONE to outperform a 1.9B parameter baseline across diverse corpora, while using only half the inference-time FLOPS.

**Link**: [arxiv](http://arxiv.org/abs/2502.01637v1),  [pdf](http://arxiv.org/pdf/2502.01637v1)

**Tags**: cs.CL cs.LG 



### PlaceIT: Placement-based Inter-Chiplet Interconnect Topologies
**Authors**: Patrick Iff, Benigna Bruggmann, Maciej Besta, Luca Benini, Torsten Hoefler

**Updated**: 2025-02-03T15:38:53Z

**Summary**: 2.5D integration technology is gaining traction as it copes with the exponentially growing design cost of modern integrated circuits. A crucial part of a 2.5D stacked chip is a low-latency and high-throughput inter-chiplet interconnect (ICI). Two major factors affecting the latency and throughput are the topology of links between chiplets and the chiplet placement. In this work, we present PlaceIT, a novel methodology to jointly optimize the ICI topology and the chiplet placement. While state-of-the-art methods optimize the chiplet placement for a predetermined ICI topology, or they select one topology out of a set of candidates, we generate a completely new topology for each placement. Our process of inferring placement-based ICI topologies connects chiplets that are in close proximity to each other, making it particularly attractive for chips with silicon bridges or passive silicon interposers with severely limited link lengths. We provide an open-source implementation of our method that optimizes the placement of homogeneously or heterogeneously shaped chiplets and the ICI topology connecting them for a user-defined mix of four different traffic types. We evaluate our methodology using synthetic traffic and traces, and we compare our results to a 2D mesh baseline. PlaceIT reduces the latency of synthetic L1-to-L2 and L2-to-memory traffic, the two most important types for cache coherency traffic, by up to 28% and 62%, respectively. It also achieve an average packet latency reduction of up to 18% on traffic traces. PlaceIT enables the construction of 2.5D stacked chips with low-latency ICIs.

**Link**: [arxiv](http://arxiv.org/abs/2502.01449v1),  [pdf](http://arxiv.org/pdf/2502.01449v1)

**Tags**: cs.AR 



### The "Huh?" Button: Improving Understanding in Educational Videos with   Large Language Models
**Authors**: Boris Ruf, Marcin Detyniecki

**Updated**: 2025-02-03T15:15:58Z

**Summary**: We propose a simple way to use large language models (LLMs) in education. Specifically, our method aims to improve individual comprehension by adding a novel feature to online videos. We combine the low threshold for interactivity in digital experiences with the benefits of rephrased and elaborated explanations typical of face-to-face interactions, thereby supporting to close knowledge gaps at scale. To demonstrate the technical feasibility of our approach, we conducted a proof-of-concept experiment and implemented a prototype which is available for testing online. Through the use case, we also show how caching can be applied in LLM-powered applications to reduce their carbon footprint.

**Link**: [arxiv](http://arxiv.org/abs/2412.14201v2),  [pdf](http://arxiv.org/pdf/2412.14201v2)

**Tags**: cs.HC cs.CY 



### FastKV: KV Cache Compression for Fast Long-Context Processing with   Token-Selective Propagation
**Authors**: Dongwon Jo, Jiwon Song, Yulhwa Kim, Jae-Joon Kim

**Updated**: 2025-02-03T05:25:09Z

**Summary**: While large language models (LLMs) excel at handling long-context sequences, they require substantial key-value (KV) caches to store contextual information, which can heavily burden computational efficiency and memory usage. Previous efforts to compress these KV caches primarily focused on reducing memory demands but were limited in enhancing latency. To address this issue, we introduce FastKV, a KV cache compression method designed to enhance latency for long-context sequences. To enhance processing speeds while maintaining accuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that retains the full context information in the initial layers of LLMs and selectively propagates only a portion of this information in deeper layers even in the prefill stage. Additionally, FastKV incorporates grouped-query attention (GQA)-aware KV cache compression to exploit the advantages of GQA in both memory and computational efficiency. Our experimental results show that FastKV achieves 2.00$\times$ and 1.40$\times$ improvements in time-to-first-token (TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art KV cache compression method. Moreover, FastKV successfully maintains accuracy on long-context benchmarks at levels comparable to the baselines. Our code is available at https://github.com/dongwonjo/FastKV.

**Link**: [arxiv](http://arxiv.org/abs/2502.01068v1),  [pdf](http://arxiv.org/pdf/2502.01068v1)

**Tags**: cs.LG cs.CL 



### Implicit Shape and Appearance Priors for Few-Shot Full Head   Reconstruction
**Authors**: Pol Caselles, Eduard Ramon, Jaime Garcia, Gil Triginer, Francesc Moreno-Noguer

**Updated**: 2025-02-02T14:38:15Z

**Summary**: Recent advancements in learning techniques that employ coordinate-based neural representations have yielded remarkable results in multi-view 3D reconstruction tasks. However, these approaches often require a substantial number of input views (typically several tens) and computationally intensive optimization procedures to achieve their effectiveness. In this paper, we address these limitations specifically for the problem of few-shot full 3D head reconstruction. We accomplish this by incorporating a probabilistic shape and appearance prior into coordinate-based representations, enabling faster convergence and improved generalization when working with only a few input images (even as low as a single image). During testing, we leverage this prior to guide the fitting process of a signed distance function using a differentiable renderer. By incorporating the statistical prior alongside parallelizable ray tracing and dynamic caching strategies, we achieve an efficient and accurate approach to few-shot full 3D head reconstruction. Moreover, we extend the H3DS dataset, which now comprises 60 high-resolution 3D full head scans and their corresponding posed images and masks, which we use for evaluation purposes. By leveraging this dataset, we demonstrate the remarkable capabilities of our approach in achieving state-of-the-art results in geometry reconstruction while being an order of magnitude faster than previous approaches.

**Link**: [arxiv](http://arxiv.org/abs/2310.08784v2),  [pdf](http://arxiv.org/pdf/2310.08784v2)

**Tags**: cs.CV 



### RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via   Outlier-Aware Adaptive Rotations
**Authors**: Zunhai Su, Zhe Chen, Wang Shen, Hanyu Wei, Linge Li, Huangqi Yu, Kehong Yuan

**Updated**: 2025-02-02T03:04:54Z

**Summary**: Key-Value (KV) cache facilitates efficient large language models (LLMs) inference by avoiding recomputation of past KVs. As the batch size and context length increase, the oversized KV caches become a significant memory bottleneck, highlighting the need for efficient compression. Existing KV quantization rely on fine-grained quantization or the retention of a significant portion of high bit-widths caches, both of which compromise compression ratio and often fail to maintain robustness at extremely low average bit-widths. In this work, we explore the potential of rotation technique for 2-bit KV quantization and propose RotateKV, which achieves accurate and robust performance through the following innovations: (i) Outlier-Aware Rotation, which utilizes channel-reordering to adapt the rotations to varying channel-wise outlier distributions without sacrificing the computational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii) Pre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position embedding (RoPE) on proposed outlier-aware rotation and further smooths outliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages the massive activations to precisely identify and protect attention sinks. RotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit quantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning and long-context capabilities, with less than 1.7\% degradation on GSM8K, outperforming existing methods even at lower average bit-widths. RotateKV also showcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch sizes, and achieves a 2.32x speedup in decoding stage.

**Link**: [arxiv](http://arxiv.org/abs/2501.16383v2),  [pdf](http://arxiv.org/pdf/2501.16383v2)

**Tags**: cs.LG cs.AI cs.CL 



### PolarQuant: Leveraging Polar Transformation for Efficient Key Cache   Quantization and Decoding Acceleration
**Authors**: Songhao Wu, Ang Lv, Xiao Feng, Yufei Zhang, Xun Zhang, Guojun Yin, Wei Lin, Rui Yan

**Updated**: 2025-02-01T18:59:03Z

**Summary**: The KV cache in large language models is a dominant factor in memory usage, limiting their broader applicability. Quantizing the cache to lower bit widths is an effective way to reduce computational costs; however, previous methods struggle with quantizing key vectors due to outliers, resulting in excessive overhead. We propose a novel quantization approach called PolarQuant, which efficiently addresses the outlier challenge. We observe that outliers typically appear in only one of two dimensions, which are rotated together by a specific angle when rotary position embeddings are applied. When represented as two-dimensional vectors, these dimensions exhibit well-structured patterns, with radii and angles smoothly distributed in polar coordinates. This alleviates the challenge of outliers on per-channel quantization, making them well-suited for quantization. Thus, PolarQuant divides key vectors into groups of two-dimensional sub-vectors, encoding them as the corresponding quantized radius and the polar angle, rather than quantizing original key vectors directly. PolarQuant achieves the superior efficiency in KV cache quantization and accelerates the decoding process by turning the query-key inner product into a table lookup, all while maintaining the downstream performance of full-precision models.

**Link**: [arxiv](http://arxiv.org/abs/2502.00527v1),  [pdf](http://arxiv.org/pdf/2502.00527v1)

**Tags**: cs.LG cs.CL 



### QMDB: Quick Merkle Database
**Authors**: Isaac Zhang, Ryan Zarick, Daniel Wong, Thomas Kim, Bryan Pellegrino, Mignon Li, Kelvin Wong

**Updated**: 2025-02-01T16:00:50Z

**Summary**: Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain state management by integrating key-value (KV) and Merkle tree storage into a single unified architecture. QMDB delivers a significant throughput improvement over existing architectures, achieving up to 6X over the widely used RocksDB and 8X over NOMT, a leading verifiable database. Its novel append-only twig-based design enables one SSD read per state access, O(1) IOs for updates, and in-memory Merkleization on a memory footprint as small as 2.3 bytes per entry, enabling it to run on even modest consumer-grade PCs. QMDB scales seamlessly across both commodity and enterprise hardware, achieving up to 2.28 million state updates per second. This performance enables support for 1 million token transfers per second (TPS), marking QMDB as the first solution achieving such a milestone. QMDB has been benchmarked with workloads exceeding 15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to scale to 280 billion entries on a single server. Furthermore, QMDB introduces historical proofs, unlocking the ability to query its blockchain's historical state at the latest block. QMDB not only meets the demands of current blockchains but also provides a robust foundation for building scalable, efficient, and verifiable decentralized applications across diverse use cases.

**Link**: [arxiv](http://arxiv.org/abs/2501.05262v3),  [pdf](http://arxiv.org/pdf/2501.05262v3)

**Tags**: cs.NI cs.DB 



### UniAttn: Reducing Inference Costs via Softmax Unification for   Post-Training LLMs
**Authors**: Yizhe Xiong, Wei Huang, Xin Ye, Hui Chen, Zijia Lin, Haoran Lian, Zhenpeng Su, Jungong Han, Guiguang Ding

**Updated**: 2025-02-01T14:16:31Z

**Summary**: Post-training is essential for adapting Large Language Models (LLMs) to real-world applications. Deploying post-trained models faces significant challenges due to substantial memory overhead and noticeable inference latency. Existing work has identified significant redundancies in LLMs and proposed efficient architectures, namely intra-layer KV sharing and cross-layer KV sharing. However, intra-layer KV sharing still results in high inference costs, while cross-layer KV sharing leads to significant performance degradation. As a result, both methods remain suboptimal for post-training pre-trained LLMs. In this paper, we identify that the \texttt{Softmax} operation is a primary bottleneck for LLM inference and discover that it is actually highly redundant during post-training. We propose Softmax \textbf{Uni}fication in \textbf{Att}e\textbf{n}tion (\textbf{UniAttn}), a novel post-training method that unifies Softmax activations across transformer blocks to reduce LLM inference costs. Additionally, UniAttn adopts a linear projection to compensate for the errors induced by Softmax unification. Experiments show that UniAttn matches the performance of standard post-training while significantly reducing inference costs, outperforming existing efficient architectures during post-training. Our code will be available at \url{https://github.com/Bostoncake/UniAttn}.

**Link**: [arxiv](http://arxiv.org/abs/2502.00439v1),  [pdf](http://arxiv.org/pdf/2502.00439v1)

**Tags**: cs.CL 



### CAT Pruning: Cluster-Aware Token Pruning For Text-to-Image Diffusion   Models
**Authors**: Xinle Cheng, Zhuoming Chen, Zhihao Jia

**Updated**: 2025-02-01T13:46:02Z

**Summary**: Diffusion models have revolutionized generative tasks, especially in the domain of text-to-image synthesis; however, their iterative denoising process demands substantial computational resources. In this paper, we present a novel acceleration strategy that integrates token-level pruning with caching techniques to tackle this computational challenge. By employing noise relative magnitude, we identify significant token changes across denoising iterations. Additionally, we enhance token selection by incorporating spatial clustering and ensuring distributional balance. Our experiments demonstrate reveal a 50%-60% reduction in computational costs while preserving the performance of the model, thereby markedly increasing the efficiency of diffusion models. The code is available at https://github.com/ada-cheng/CAT-Pruning

**Link**: [arxiv](http://arxiv.org/abs/2502.00433v1),  [pdf](http://arxiv.org/pdf/2502.00433v1)

**Tags**: cs.CV 



### Masked Generative Nested Transformers with Decode Time Scaling
**Authors**: Sahil Goyal, Debapriya Tula, Gagan Jain, Pradeep Shenoy, Prateek Jain, Sujoy Paul

**Updated**: 2025-02-01T09:41:01Z

**Summary**: Recent advances in visual generation have made significant strides in producing content of exceptional quality. However, most methods suffer from a fundamental problem - a bottleneck of inference computational efficiency. Most of these algorithms involve multiple passes over a transformer model to generate tokens or denoise inputs. However, the model size is kept consistent throughout all iterations, which makes it computationally expensive. In this work, we aim to address this issue primarily through two key ideas - (a) not all parts of the generation process need equal compute, and we design a decode time model scaling schedule to utilize compute effectively, and (b) we can cache and reuse some of the computation. Combining these two ideas leads to using smaller models to process more tokens while large models process fewer tokens. These different-sized models do not increase the parameter size, as they share parameters. We rigorously experiment with ImageNet256$\times$256 , UCF101, and Kinetics600 to showcase the efficacy of the proposed method for image/video generation and frame prediction. Our experiments show that with almost $3\times$ less compute than baseline, our model obtains competitive performance.

**Link**: [arxiv](http://arxiv.org/abs/2502.00382v1),  [pdf](http://arxiv.org/pdf/2502.00382v1)

**Tags**: cs.CV cs.AI cs.LG 



### QSpec: Speculative Decoding with Complementary Quantization Schemes
**Authors**: Juntao Zhao, Wenhao Lu, Sheng Wang, Lingpeng Kong, Chuan Wu

**Updated**: 2025-02-01T04:24:16Z

**Summary**: Quantization has been substantially adopted to accelerate inference and reduce memory consumption of large language models (LLMs). While activation-weight joint quantization speeds up the inference process through low-precision kernels, we demonstrate that it suffers severe performance degradation on multi-step reasoning tasks, rendering it ineffective. We propose a novel quantization paradigm called QSPEC, which seamlessly integrates two complementary quantization schemes for speculative decoding. Leveraging nearly cost-free execution switching, QSPEC drafts tokens with low-precision, fast activation-weight quantization, and verifies them with high-precision weight-only quantization, effectively combining the strengths of both quantization schemes. Compared to high-precision quantization methods, QSPEC empirically boosts token generation throughput by up to 1.64x without any quality compromise, distinguishing it from other low-precision quantization approaches. This enhancement is also consistent across various serving tasks, model sizes, quantization methods, and batch sizes. Compared to state-of-art speculative decoding methods, our approach reuses weights and the KV cache, avoiding extra memory overhead while achieving up to 1.55x speedup in batched serving with a high acceptance rate. Furthermore, QSPEC offers a plug-and-play advantage without requiring any training. We believe that QSPEC demonstrates unique strengths for future deployment of high-fidelity quantization schemes, particularly in memory-constrained scenarios (e.g., edge devices).

**Link**: [arxiv](http://arxiv.org/abs/2410.11305v2),  [pdf](http://arxiv.org/pdf/2410.11305v2)

**Tags**: cs.LG cs.AI 



### ChunkKV: Semantic-Preserving KV Cache Compression for Efficient   Long-Context LLM Inference
**Authors**: Xiang Liu, Zhenheng Tang, Peijie Dong, Zeyu Li, Bo Li, Xuming Hu, Xiaowen Chu

**Updated**: 2025-02-01T03:49:47Z

**Summary**: To reduce memory costs in long-context inference with Large Language Models (LLMs), many recent works focus on compressing the key-value (KV) cache of different tokens. However, we identify that the previous KV cache compression methods measure token importance individually, neglecting the dependency between different tokens in the real-world language characterics. In light of this, we introduce ChunkKV, grouping the tokens in a chunk as a basic compressing unit, and retaining the most informative semantic chunks while discarding the less important ones. Furthermore, observing that ChunkKV exhibits higher similarity in the preserved indices across different layers, we propose layer-wise index reuse to further reduce computational overhead. We evaluated ChunkKV on cutting-edge long-context benchmarks including LongBench and Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context learning benchmark. Our experiments with instruction tuning and multi-step reasoning (O1 and R1) LLMs, achieve up to 10\% performance improvement under aggressive compression ratios compared to existing methods.

**Link**: [arxiv](http://arxiv.org/abs/2502.00299v1),  [pdf](http://arxiv.org/pdf/2502.00299v1)

**Tags**: cs.CL 



### Learning to Compress Contexts for Efficient Knowledge-based Visual   Question Answering
**Authors**: Weixi Weng, Jieming Zhu, Xiaojun Meng, Hao Zhang, Rui Zhang, Chun Yuan

**Updated**: 2025-02-01T03:40:37Z

**Summary**: Multimodal large language models (MLLMs) have demonstrated great performance on visual question answering (VQA). When it comes to knowledge-based Visual Question Answering (KB-VQA), MLLMs may lack the specialized domain knowledge needed to answer questions, necessitating the retrieval of necessary information from external knowledge sources. Previous works like Retrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input information, such as image-based textual descriptions and retrieved knowledge, as possible to improve performance, but they all overlook the issue that with the number of input tokens increasing, inference efficiency significantly decreases, which contradicts the demands of practical applications. To address this issue, we propose \textbf{R}etrieval-\textbf{A}ugmented MLLMs with Compressed Contexts (RACC). RACC learns to compress and aggregate retrieved knowledge for a given image-question pair, generating a compact modulation in the form of Key-Value (KV) cache to adapt the downstream frozen MLLM, thereby achieving effective and efficient inference. RACC achieves a state-of-the-art (SOTA) performance of 63.92\% on OK-VQA. Moreover, it significantly reduces inference latency by 22.0\%-59.7\% compared to the prominent RAVQA-v2. Abundant experiments show RACC's broad applicability. It is compatible with various off-the-shelf MLLMs and can also handle different knowledge sources including textual and multimodal documents.

**Link**: [arxiv](http://arxiv.org/abs/2409.07331v2),  [pdf](http://arxiv.org/pdf/2409.07331v2)

**Tags**: cs.CV cs.LG 



### Activation Sparsity Opportunities for Compressing General Large Language   Models
**Authors**: Nobel Dhar, Bobin Deng, Md Romyull Islam, Kazi Fahim Ahmad Nasif, Liang Zhao, Kun Suo

**Updated**: 2025-01-31T19:09:19Z

**Summary**: Deploying local AI models, such as Large Language Models (LLMs), to edge devices can substantially enhance devices' independent capabilities, alleviate the server's burden, and lower the response time. Owing to these tremendous potentials, many big tech companies have released several lightweight Small Language Models (SLMs) to bridge this gap. However, we still have huge motivations to deploy more powerful (LLMs) AI models on edge devices and enhance their smartness level. Unlike the conventional approaches for AI model compression, we investigate activation sparsity. The activation sparsity method is orthogonal and combinable with existing techniques to maximize the compression rate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN) components, which typically comprise a large proportion of parameters (around 2/3), ensure that our FFN optimizations would have a better chance of achieving effective compression. Moreover, our findings are beneficial to general LLMs and are not restricted to ReLU-based models. This work systematically investigates the tradeoff between enforcing activation sparsity and perplexity (accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that we can obtain around 50% of main memory and computing reductions for critical FFN components with negligible accuracy degradation. This extra 50% sparsity does not naturally exist in the current LLMs, which require tuning LLMs' activation outputs by injecting zero-enforcing thresholds. To obtain the benefits of activation sparsity, we provide a guideline for the system architect for LLM prediction and prefetching. The success prediction allows the system to prefetch the necessary weights while omitting the inactive ones and their successors, therefore lowering cache and memory pollution and reducing LLM execution time on resource-constrained edge devices.

**Link**: [arxiv](http://arxiv.org/abs/2412.12178v2),  [pdf](http://arxiv.org/pdf/2412.12178v2)

**Tags**: cs.LG cs.AI 



### Cache Me If You Must: Adaptive Key-Value Quantization for Large Language   Models
**Authors**: Alina Shutova, Vladimir Malinovskii, Vage Egiazarian, Denis Kuznedelev, Denis Mazur, Nikita Surkov, Ivan Ermakov, Dan Alistarh

**Updated**: 2025-01-31T18:47:42Z

**Summary**: Efficient real-world deployments of large language models (LLMs) rely on Key-Value (KV) caching for processing and generating long outputs, reducing the need for repetitive computation. For large contexts, Key-Value caches can take up tens of gigabytes of device memory, as they store vector representations for each token and layer. Recent work has shown that the cached vectors can be compressed through quantization, pruning or merging, but these techniques often compromise quality towards higher compression rates. In this work, we aim to improve Key & Value compression by exploiting two observations: 1) the inherent dependencies between keys and values across different layers, and 2) high-compression mechanisms for internal network states. We propose AQUA-KV, an adaptive quantization for Key-Value caches that relies on compact adapters to exploit existing dependencies between Keys and Values, and aims to "optimally" compress the information that cannot be predicted. AQUA-KV significantly improves compression rates, while maintaining high accuracy on state-of-the-art LLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5 bits per value with under $1\%$ relative error in perplexity and LongBench scores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a single GPU within 1-6 hours, even for 70B models.

**Link**: [arxiv](http://arxiv.org/abs/2501.19392v1),  [pdf](http://arxiv.org/pdf/2501.19392v1)

**Tags**: cs.LG 



### Offline Learning for Combinatorial Multi-armed Bandits
**Authors**: Xutong Liu, Xiangxiang Dai, Jinhang Zuo, Siwei Wang, Carlee-Joe Wong, John C. S. Lui, Wei Chen

**Updated**: 2025-01-31T16:56:18Z

**Summary**: The combinatorial multi-armed bandit (CMAB) is a fundamental sequential decision-making framework, extensively studied over the past decade. However, existing work primarily focuses on the online setting, overlooking the substantial costs of online interactions and the readily available offline datasets. To overcome these limitations, we introduce Off-CMAB, the first offline learning framework for CMAB. Central to our framework is the combinatorial lower confidence bound (CLCB) algorithm, which combines pessimistic reward estimations with combinatorial solvers. To characterize the quality of offline datasets, we propose two novel data coverage conditions and prove that, under these conditions, CLCB achieves a near-optimal suboptimality gap, matching the theoretical lower bound up to a logarithmic factor. We validate Off-CMAB through practical applications, including learning to rank, large language model (LLM) caching, and social influence maximization, showing its ability to handle nonlinear reward functions, general feedback models, and out-of-distribution action samples that excludes optimal or even feasible actions. Extensive experiments on synthetic and real-world datasets further highlight the superior performance of CLCB.

**Link**: [arxiv](http://arxiv.org/abs/2501.19300v1),  [pdf](http://arxiv.org/pdf/2501.19300v1)

**Tags**: cs.LG 



### Efficient Beam Search for Large Language Models Using Trie-Based   Decoding
**Authors**: Brian J Chan, Jui-Hung Cheng, Mao Xun Huang, Chao-Ting Chen, Hen-Hsen Huang

**Updated**: 2025-01-31T16:22:36Z

**Summary**: In Transformer-based sequence-to-sequence generation, beam search has proven effective in enhancing the quality of generated sequences compared to greedy decoding. Conventional beam search methods typically adopt either a sequential or batch-based approach. The sequential approach, while memory-efficient, requires multiple decoding passes to construct a complete search tree, leading to significantly slower inference. On the other hand, the batch-based approach enables parallel computation across beams, but at the expense of high memory consumption due to the need to maintain separate key-value (KV) caches for each beam. In this study, we introduce a novel trie (prefix-tree)-based parallel decoding method that addresses the memory inefficiency of batch-based beam search. By sharing a single KV cache among all beams that share the same prefix, the proposed method not only reduces memory consumption dramatically but also enables parallel decoding across all branches. This innovative use of a prefix tree offers an efficient alternative for beam search, achieving significant memory savings while preserving inference speed, making it particularly well-suited for memory-constrained environments or large-scale model deployments.

**Link**: [arxiv](http://arxiv.org/abs/2502.00085v1),  [pdf](http://arxiv.org/pdf/2502.00085v1)

**Tags**: cs.CL 



### Accelerating Diffusion Transformer via Error-Optimized Cache
**Authors**: Junxiang Qiu, Shuo Wang, Jinda Lu, Lin Liu, Houcheng Jiang, Yanbin Hao

**Updated**: 2025-01-31T15:58:15Z

**Summary**: Diffusion Transformer (DiT) is a crucial method for content generation. However, it needs a lot of time to sample. Many studies have attempted to use caching to reduce the time consumption of sampling. Existing caching methods accelerate generation by reusing DiT features from the previous time step and skipping calculations in the next, but they tend to locate and cache low-error modules without focusing on reducing caching-induced errors, resulting in a sharp decline in generated content quality when increasing caching intensity. To solve this problem, we propose the Error-Optimized Cache (EOC). This method introduces three key improvements: (1) Prior knowledge extraction: Extract and process the caching differences; (2) A judgment method for cache optimization: Determine whether certain caching steps need to be optimized; (3) Cache optimization: reduce caching errors. Experiments show that this algorithm significantly reduces the error accumulation caused by caching (especially over-caching). On the ImageNet dataset, without significantly increasing the computational burden, this method improves the quality of the generated images under the over-caching, rule-based, and training-based methods. Specifically, the Fr\'echet Inception Distance (FID) values are improved as follows: from 6.857 to 5.821, from 3.870 to 3.692 and form 3.539 to 3.451 respectively.

**Link**: [arxiv](http://arxiv.org/abs/2501.19243v1),  [pdf](http://arxiv.org/pdf/2501.19243v1)

**Tags**: cs.CV 



### HarmoniCa: Harmonizing Training and Inference for Better Feature Caching   in Diffusion Transformer Acceleration
**Authors**: Yushi Huang, Zining Wang, Ruihao Gong, Jing Liu, Xinjie Zhang, Jinyang Guo, Xianglong Liu, Jun Zhang

**Updated**: 2025-01-31T14:26:05Z

**Summary**: Diffusion Transformers (DiTs) excel in generative tasks but face practical deployment challenges due to high inference costs. Feature caching, which stores and retrieves redundant computations, offers the potential for acceleration. Existing learning-based caching, though adaptive, overlooks the impact of the prior timestep. It also suffers from misaligned objectives--aligned predicted noise vs. high-quality images--between training and inference. These two discrepancies compromise both performance and efficiency. To this end, we harmonize training and inference with a novel learning-based caching framework dubbed HarmoniCa. It first incorporates Step-Wise Denoising Training (SDT) to ensure the continuity of the denoising process, where prior steps can be leveraged. In addition, an Image Error Proxy-Guided Objective (IEPO) is applied to balance image quality against cache utilization through an efficient proxy to approximate the image error. Extensive experiments across $8$ models, $4$ samplers, and resolutions from $256\times256$ to $2K$ demonstrate superior performance and speedup of our framework. For instance, it achieves over $40\%$ latency reduction (i.e., $2.07\times$ theoretical speedup) and improved performance on PixArt-$\alpha$. Remarkably, our image-free approach reduces training time by $25\%$ compared with the previous method.

**Link**: [arxiv](http://arxiv.org/abs/2410.01723v3),  [pdf](http://arxiv.org/pdf/2410.01723v3)

**Tags**: cs.CV 



### CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning
**Authors**: Fanxu Meng, Pingzhi Tang, Fan jiang, Muhan Zhang

**Updated**: 2025-01-31T14:13:49Z

**Summary**: Decoder-only models generate tokens autoregressively by caching key/value vectors, but as the cache grows, inference becomes memory-bound. To address this issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel approach that treats pairs of attention layers as a set of low-rank decompositions. CLOVER applies Singular Value Decomposition (SVD) to the \( Q \)-\( K \) and \( V \)-\( O \) pairs within each attention head. The resulting singular values can either guide pruning or serve as trainable parameters for efficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning, these values are reintegrated into the model without increasing its parameter count. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite, Whisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results demonstrate that CLOVER significantly improves pruning efficiency. For instance, the perplexity of pruning 70\% of the \( Q \)-\( K \) pairs in GPT-2 XL is similar to that of pruning just 8\% with vanilla methods. Fine-tuning the singular values further results in a full-rank update, outperforming state-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\%, 5.5\%, 3.8\%, and 0.7\%, respectively, on eight commonsense tasks for LLaMA-2 7B.

**Link**: [arxiv](http://arxiv.org/abs/2411.17426v3),  [pdf](http://arxiv.org/pdf/2411.17426v3)

**Tags**: cs.LG cs.AI 



### Swift: Rethinking RDMA Control Plane for Elastic Computing
**Authors**: Junxue Zhang, Han Tian, Xinyang Huang, Wenxue Li, Kaiqiang Xu, Dian Shen, Yong Wang, Kai Chen

**Updated**: 2025-01-31T11:25:40Z

**Summary**: Elastic computing enables dynamic scaling to meet workload demands, and Remote Direct Memory Access (RDMA) enhances this by providing high-throughput, low-latency network communication. However, integrating RDMA into elastic computing remains a challenge, particularly in control plane operations for RDMA connection setup.   This paper revisits the assumptions of prior work on high-performance RDMA for elastic computing, and reveals that extreme microsecond-level control plane optimizations are often unnecessary. By challenging the conventional beliefs on the slowness of user-space RDMA control plane and the difficulty of user-space RDMA resource sharing, we uncover new design opportunities. Our key insight is that user-space RDMA connection setup can be significantly improved with caching, while RDMA resources can be efficiently shared among processes using fork. In light of this, we propose Swift, a simple yet effective solution that co-designs RDMA with a serverless framework to optimize performance for elastic computing. At its very core, Swift handles cold and warm serverless requests by swiftly initializing the RDMA control plane with cache-optimized libibverbs, and manages fork requests by leveraging the RDMA's fork capability. Implemented with OpenWhisk, Swift delivers 30.56-46.50% higher average throughput and 18.55-37.21% lower latency, at a cost of 6.5% control plane overhead, compared to prior solutions.

**Link**: [arxiv](http://arxiv.org/abs/2501.19051v1),  [pdf](http://arxiv.org/pdf/2501.19051v1)

**Tags**: cs.NI 



### The development of IBIC microscopy at the 100 kV ion implanter of the   University of Torino (LIUTo) and the application for the assessment of the   radiation hardness of a silicon photodiode
**Authors**: Emilio Corte, Alberto Bortone, Elena Nieto Hernández, Carlo Ceresa, Georgios Provatas, Karla Ivanković Nizić, Milko Jaksić, Ettore Vittone, Sviatoslav Ditalia Tchernij

**Updated**: 2025-01-31T10:43:00Z

**Summary**: The Ion Beam Induced Charge (IBIC) technique is widely used to characterize the electronic properties of semiconductor materials and devices. Its main advantage over other charge collection microscopies stems in the use of MeV ion probes, which provide both measurable induced charge signals from single ions, and high spatial resolution, which is maintained along the ion range. It is a fact, however, that the use of low-energy ions in the keV range can provide the IBIC technique with complementary analytical capabilities, that are not available with MeV ions, for example the higher sensitivity to the status, contamination and morphology of the surface and the fact that the induced signal depends on the transport of only one type of charge carrier. This paper outlines the upgrade that was made at the 100 kV ion implanter of the University of Torino, originally installed for material and surface modification, to explore the rather unexplored keV-IBIC field and to assess its potential to characterize semiconductor devices. Finally, we report the first IBIC application of our apparatus, which regards the assessment of the radiation damage of a commercially available silicon photodiode, adopting the IAEA experimental protocol and the relevant interpretative model.

**Link**: [arxiv](http://arxiv.org/abs/2501.19021v1),  [pdf](http://arxiv.org/pdf/2501.19021v1)

**Tags**: physics.ins-det 



### Memory-Efficient Fine-Tuning of Transformers via Token Selection
**Authors**: Antoine Simoulin, Namyong Park, Xiaoyi Liu, Grey Yang

**Updated**: 2025-01-31T00:43:50Z

**Summary**: Fine-tuning provides an effective means to specialize pre-trained models for various downstream tasks. However, fine-tuning often incurs high memory overhead, especially for large transformer-based models, such as LLMs. While existing methods may reduce certain parts of the memory required for fine-tuning, they still require caching all intermediate activations computed in the forward pass to update weights during the backward pass. In this work, we develop TokenTune, a method to reduce memory usage, specifically the memory to store intermediate activations, in the fine-tuning of transformer-based models. During the backward pass, TokenTune approximates the gradient computation by backpropagating through just a subset of input tokens. Thus, with TokenTune, only a subset of intermediate activations are cached during the forward pass. Also, TokenTune can be easily combined with existing methods like LoRA, further reducing the memory cost. We evaluate our approach on pre-trained transformer models with up to billions of parameters, considering the performance on multiple downstream tasks such as text classification and question answering in a few-shot learning setup. Overall, TokenTune achieves performance on par with full fine-tuning or representative memory-efficient fine-tuning methods, while greatly reducing the memory footprint, especially when combined with other methods with complementary memory reduction mechanisms. We hope that our approach will facilitate the fine-tuning of large transformers, in specializing them for specific domains or co-training them with other neural components from a larger system. Our code is available at https://github.com/facebookresearch/tokentune.

**Link**: [arxiv](http://arxiv.org/abs/2501.18824v1),  [pdf](http://arxiv.org/pdf/2501.18824v1)

**Tags**: cs.CL cs.LG 



### REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and   Failure Mitigation
**Authors**: Tommaso Bonato, Abdul Kabbani, Ahmad Ghalayini, Michael Papamichael, Mohammad Dohadwala, Lukas Gianinazzi, Mikhail Khalilov, Elias Achermann, Daniele De Sensi, Torsten Hoefler

**Updated**: 2025-01-30T18:23:46Z

**Summary**: Next-generation datacenters require highly efficient network load balancing to manage the growing scale of artificial intelligence (AI) training and general datacenter traffic. Existing solutions designed for Ethernet, such as Equal Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to maintain high network utilizations as datacenter topologies (and network failures as a consequence) continue to grow. To address these limitations, we propose REPS, a lightweight decentralized per-packet adaptive load balancing algorithm designed to optimize network utilization while ensuring rapid recovery from link failures. REPS adapts to network conditions by caching good-performing paths. In case of a network failure, REPS re-routes traffic away from it in less than 100 microseconds. REPS is designed to be deployed with next-generation out-of-order transports, such as Ultra Ethernet, and introduces less than 25 bytes of per-connection state. We extensively evaluate REPS in large-scale simulations and FPGA-based NICs.

**Link**: [arxiv](http://arxiv.org/abs/2407.21625v3),  [pdf](http://arxiv.org/pdf/2407.21625v3)

**Tags**: cs.NI 



### State Stream Transformer (SST) : Emergent Metacognitive Behaviours   Through Latent State Persistence
**Authors**: Thea Aviss

**Updated**: 2025-01-30T14:03:36Z

**Summary**: We introduce the State Stream Transformer (SST), a novel LLM architecture that reveals emergent reasoning behaviours and capabilities latent in pretrained weights through addressing a fundamental limitation in traditional transformer models: the lack of latent computational continuity across autoregressive generations in the state space. SST introduces a sliding window latent state (FFN) cache with weighted decay that maintains and evolves persistent latent processes throughout autoregressive generations. Through controlled experiments comparing base and SST architectures using the same frozen weights, we demonstrate that this architectural modification alone enables enhanced reasoning capabilities which appear best explained by some form of potential higher-order processing, as evidenced by emergent metacognitive behaviours. These behaviours persist under controlled conditions designed to eliminate confounding factors such as stochastic variation or learned response patterns. Analysis of latent state distributions and processing dynamics provides evidence that it is solely the 'state stream' that is responsible for these phenomena. In quantitative evaluations, the SST achieves substantial performance improvements over the base model on two reasoning benchmarks, reaching 89.01\% accuracy on GSM-8K (0-shot) and 91.04\% on ARC Challenge (0-shot CoT). These findings indicate that persistent computation in the latent state space enables fundamentally different information processing and internal reasoning strategies, with implications for our understanding of artificial intelligence systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.18356v1),  [pdf](http://arxiv.org/pdf/2501.18356v1)

**Tags**: cs.LG cs.AI cs.CL 



### Locret: Enhancing Eviction in Long-Context LLM Inference with Trained   Retaining Heads on Consumer-Grade Devices
**Authors**: Yuxiang Huang, Binhang Yuan, Xu Han, Chaojun Xiao, Zhiyuan Liu

**Updated**: 2025-01-30T13:07:37Z

**Summary**: Scaling the input context length of a large language model (LLM) incurs a significant increase in computation cost and memory footprint to maintain the attention key-value (KV) cache. Existing KV cache compression methods suffer from inefficient compression strategies and limited memory reduction effects, making it difficult for LLMs to conduct long-context inference on consumer-grade devices, especially when inferring long-context stream input. Such obstacles prevent consumer-grade devices from supporting more complex applications, creating challenges for the democratization of LLMs. To overcome this, we propose Locret, the first framework to create an eviction policy compatible with chunked prefill. By evaluating the causal importance of KV cache units by learnable retaining heads, Locret enables precise eviction of cache units, facilitating efficient long-context inference. In our extensive empirical studies, Locret outperforms the recent popular and competitive approaches in terms of memory efficiency and generation quality -- Locret achieves up to 20x of KV cache compression ratio within less than 10% performance loss. Furthermore, Locret achieves 128K+ long-context inference on a single NVIDIA 4090 GPU without compromising generation quality and only costs <1 GPU hour of additional training.

**Link**: [arxiv](http://arxiv.org/abs/2410.01805v2),  [pdf](http://arxiv.org/pdf/2410.01805v2)

**Tags**: cs.CL 



### Systematic Evaluation of Randomized Cache Designs against Cache   Occupancy
**Authors**: Anirban Chakraborty, Nimish Mishra, Sayandeep Saha, Sarani Bhattacharya, Debdeep Mukhopadhyay

**Updated**: 2025-01-30T06:02:11Z

**Summary**: Randomizing the address-to-set mapping and partitioning of the cache has been shown to be an effective mechanism in designing secured caches. Several designs have been proposed on a variety of rationales: (1) randomized design, (2) randomized-and-partitioned design, and (3) psuedo-fully associative design. This work fills in a crucial gap in current literature on randomized caches: currently most randomized cache designs defend only contention-based attacks, and leave out considerations of cache occupancy. We perform a systematic evaluation of 5 randomized cache designs- CEASER, CEASER-S, MIRAGE, Scatter-Cache, and Sass-cache against cache occupancy wrt. both performance as well as security.   With respect to performance, we first establish that benchmarking strategies used by contemporary designs are unsuitable for a fair evaluation (because of differing cache configurations, choice of benchmarking suites, additional implementation-specific assumptions). We thus propose a uniform benchmarking strategy, which allows us to perform a fair and comparative analysis across all designs under various replacement policies. Likewise, with respect to security against cache occupancy attacks, we evaluate the cache designs against various threat assumptions: (1) covert channels, (2) process fingerprinting, and (3) AES key recovery (to the best of our knowledge, this work is the first to demonstrate full AES key recovery on a randomized cache design using cache occupancy attack). Our results establish the need to also consider cache occupancy side-channel in randomized cache design considerations.

**Link**: [arxiv](http://arxiv.org/abs/2310.05172v2),  [pdf](http://arxiv.org/pdf/2310.05172v2)

**Tags**: cs.CR cs.AR 



### Reciprocating Locks
**Authors**: Dave Dice, Alex Kogan

**Updated**: 2025-01-29T16:44:27Z

**Summary**: We present "Reciprocating Locks", a novel mutual exclusion locking algorithm, targeting cache-coherent shared memory (CC), that enjoys a number of desirable properties. The doorway arrival phase and the release operation both run in constant-time. Waiting threads use local spinning and only a single waiting element is required per thread, regardless of the number of locks a thread might hold at a given time. While our lock does not provide strict FIFO admission, it bounds bypass and has strong anti-starvation properties. The lock is compact, space efficient, and has been intentionally designed to be readily usable in real-world general purpose computing environments such as the linux kernel, pthreads, or C++. We show the lock exhibits high throughput under contention and low latency in the uncontended case. The performance of Reciprocating Locks is competitive with and often better than the best state-of-the-art scalable spin locks.

**Link**: [arxiv](http://arxiv.org/abs/2501.02380v5),  [pdf](http://arxiv.org/pdf/2501.02380v5)

**Tags**: cs.DC D.4.1 



### vAttention: Dynamic Memory Management for Serving LLMs without   PagedAttention
**Authors**: Ramya Prabhu, Ajay Nayak, Jayashree Mohan, Ramachandran Ramjee, Ashish Panwar

**Updated**: 2025-01-29T04:10:41Z

**Summary**: PagedAttention is a popular approach for dynamic memory allocation in LLM serving systems. It enables on-demand allocation of GPU memory to mitigate KV cache fragmentation -- a phenomenon that crippled the batch size (and consequently throughput) in prior systems. However, in trying to allocate physical memory at runtime, PagedAttention ends up changing the virtual memory layout of the KV cache from contiguous to non-contiguous. Such a design leads to non-trivial programming and performance overheads.   We present vAttention -- an approach that mitigates fragmentation in physical memory while retaining the contiguity of KV cache in virtual memory. We achieve this by decoupling the allocation of virtual and physical memory using CUDA virtual memory management APIs. We also introduce various LLM-specific optimizations to address the limitations of CUDA virtual memory support. Overall, vAttention is a simpler, portable, and performant alternative to PagedAttention: it supports various attention kernels out-of-the-box and improves LLM serving throughput by up to 1.23x compared to the use of PagedAttention-based kernels of FlashAttention and FlashInfer.

**Link**: [arxiv](http://arxiv.org/abs/2405.04437v3),  [pdf](http://arxiv.org/pdf/2405.04437v3)

**Tags**: cs.LG cs.OS 



### Optimizing SSD Caches for Cloud Block Storage Systems Using Machine   Learning Approaches
**Authors**: Chiyu Cheng, Chang Zhou, Yang Zhao, Jin Cao

**Updated**: 2025-01-28T20:35:23Z

**Summary**: The growing demand for efficient cloud storage solutions has led to the widespread adoption of Solid-State Drives (SSDs) for caching in cloud block storage systems. The management of data writes to SSD caches plays a crucial role in improving overall system performance, reducing latency, and extending the lifespan of storage devices. A critical challenge arises from the large volume of write-only data, which significantly impacts the performance of SSD caches when handled inefficiently. Specifically, writes that have not been read for a certain period may introduce unnecessary write traffic to the SSD cache without offering substantial benefits for cache performance. This paper proposes a novel approach to mitigate this issue by leveraging machine learning techniques to dynamically optimize the write policy in cloud-based storage systems. The proposed method identifies write-only data and selectively filters it out in real-time, thereby minimizing the number of unnecessary write operations and improving the overall performance of the cache system. Experimental results demonstrate that the proposed machine learning-based policy significantly outperforms traditional approaches by reducing the number of harmful writes and optimizing cache utilization. This solution is particularly suitable for cloud environments with varying and unpredictable workloads, where traditional cache management strategies often fall short.

**Link**: [arxiv](http://arxiv.org/abs/2501.14770v2),  [pdf](http://arxiv.org/pdf/2501.14770v2)

**Tags**: cs.DC cs.LG cs.OS 



### Dynamic Adaptation in Data Storage: Real-Time Machine Learning for   Enhanced Prefetching
**Authors**: Chiyu Cheng, Chang Zhou, Yang Zhao, Jin Cao

**Updated**: 2025-01-28T20:33:43Z

**Summary**: The exponential growth of data storage demands has necessitated the evolution of hierarchical storage management strategies [1]. This study explores the application of streaming machine learning [3] to revolutionize data prefetching within multi-tiered storage systems. Unlike traditional batch-trained models, streaming machine learning [5] offers adaptability, real-time insights, and computational efficiency, responding dynamically to workload variations. This work designs and validates an innovative framework that integrates streaming classification models for predicting file access patterns, specifically the next file offset. Leveraging comprehensive feature engineering and real-time evaluation over extensive production traces, the proposed methodology achieves substantial improvements in prediction accuracy, memory efficiency, and system adaptability. The results underscore the potential of streaming models in real-time storage management, setting a precedent for advanced caching and tiering strategies.

**Link**: [arxiv](http://arxiv.org/abs/2501.14771v2),  [pdf](http://arxiv.org/pdf/2501.14771v2)

**Tags**: cs.DC cs.LG cs.OS 



### Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks   Detection: A Comparative Analysis
**Authors**: Tejal Joshi, Aarya Kawalay, Anvi Jamkhande, Amit Joshi

**Updated**: 2025-01-28T18:14:43Z

**Summary**: Cache side channel attacks are a sophisticated and persistent threat that exploit vulnerabilities in modern processors to extract sensitive information. These attacks leverage weaknesses in shared computational resources, particularly the last level cache, to infer patterns in data access and execution flows, often bypassing traditional security defenses. Such attacks are especially dangerous as they can be executed remotely without requiring physical access to the victim's device. This study focuses on a specific class of these threats: fingerprinting attacks, where an adversary monitors and analyzes the behavior of co-located processes via cache side channels. This can potentially reveal confidential information, such as encryption keys or user activity patterns. A comprehensive threat model illustrates how attackers sharing computational resources with target systems exploit these side channels to compromise sensitive data. To mitigate such risks, a hybrid deep learning model is proposed for detecting cache side channel attacks. Its performance is compared with five widely used deep learning models: Multi-Layer Perceptron, Convolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term Memory, and Gated Recurrent Unit. The experimental results demonstrate that the hybrid model achieves a detection rate of up to 99.96%. These findings highlight the limitations of existing models, the need for enhanced defensive mechanisms, and directions for future research to secure sensitive data against evolving side channel threats.

**Link**: [arxiv](http://arxiv.org/abs/2501.17123v1),  [pdf](http://arxiv.org/pdf/2501.17123v1)

**Tags**: cs.CR cs.NE 



### Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications
**Authors**: Mohammad NaseriTehrani, MohammadJavad Salehi, Antti Tölli

**Updated**: 2025-01-28T16:19:24Z

**Summary**: Integrating coded caching (CC) into multiple-input multiple-output (MIMO) communications can significantly enhance the achievable degrees of freedom (DoF) in wireless networks. This paper investigates a practical cache-aided asymmetric MIMO configuration with cache ratio $\gamma$, where a server equipped with $L$ transmit antennas communicates with $K$ users, each having $G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the \emph{min-G} scheme, which treats the system as symmetric by assuming all users have the same number of antennas, equal to the smallest among them; the \emph{Grouping} scheme, which maximizes spatial multiplexing gain separately within each user subset at the cost of some global caching gain; and the \emph{Phantom} scheme, which dynamically redistributes spatial resources using virtual or ``phantom'' antennas at the users, bridging the performance gains of the min-$G$ and Grouping schemes. These strategies jointly optimize the number of users, $\Omega$, and the parallel streams decoded by each user, $\beta_k$, ensuring linear decodability for all target users. Analytical and numerical results confirm that the proposed schemes achieve significant DoF improvements across various system configurations.

**Link**: [arxiv](http://arxiv.org/abs/2501.10854v2),  [pdf](http://arxiv.org/pdf/2501.10854v2)

**Tags**: cs.IT eess.SP math.IT 



### Measuring GPU utilization one level deeper
**Authors**: Paul Elvinger, Foteini Strati, Natalie Enright Jerger, Ana Klimovic

**Updated**: 2025-01-28T12:57:53Z

**Summary**: GPU hardware is vastly underutilized. Even resource-intensive AI applications have diverse resource profiles that often leave parts of GPUs idle. While colocating applications can improve utilization, current spatial sharing systems lack performance guarantees. Providing predictable performance guarantees requires a deep understanding of how applications contend for shared GPU resources such as block schedulers, compute units, L1/L2 caches, and memory bandwidth. We propose a methodology to profile resource interference of GPU kernels across these dimensions and discuss how to build GPU schedulers that provide strict performance guarantees while colocating applications to minimize cost.

**Link**: [arxiv](http://arxiv.org/abs/2501.16909v1),  [pdf](http://arxiv.org/pdf/2501.16909v1)

**Tags**: cs.DC 



### Optimizing Smart Helper Placement for Enhanced Cache Efficiency in   F-RANs
**Authors**: Hesameddin Mokhtarzadeh, Mohammed Saif, Md. Jahangir Hossain, Julian Cheng

**Updated**: 2025-01-28T00:22:34Z

**Summary**: Smart helpers (SHs) have been proposed to improve content delivery delays and alleviate high fronthaul loads in fog radio access networks (F-RANs). They offer an alternative to deploying additional enhanced remote radio heads (RRHs), which are often infeasible due to site constraints.} The optimal placement of SHs can significantly increase the number of users they serve which leads to enhanced cache efficiency and improved content delivery delay. In this letter, we optimize SH placement within an F-RAN to maximize the cache hit rate and further reduce the content delivery latency. We model the SH cache hit rate as a function of outage probability and user density distribution. We develop a function to estimate user density distribution leveraging the radial basis functions (RBFs) method and optimize SH placement utilizing the particle swarm optimization (PSO) algorithm. \an{Our} numerical results confirm the effectiveness of the proposed approach in maximizing the \an{SH cache hit rate}, thereby improving delivery delays and fronthaul loads of the network.

**Link**: [arxiv](http://arxiv.org/abs/2501.16597v1),  [pdf](http://arxiv.org/pdf/2501.16597v1)

**Tags**: eess.SP 



### Latency Guarantees for Caching with Delayed Hits
**Authors**: Keerthana Gurushankar, Noah G. Singer, Bernardo Subercaseaux

**Updated**: 2025-01-27T22:14:43Z

**Summary**: In the classical caching problem, when a requested page is not present in the cache (i.e., a "miss"), it is assumed to travel from the backing store into the cache "before" the next request arrives. However, in many real-life applications, such as content delivery networks, this assumption is unrealistic.   The "delayed-hits" model for caching, introduced by Atre, Sherry, Wang, and Berger, accounts for the latency between a missed cache request and the corresponding arrival from the backing store. This theoretical model has two parameters: the "delay" $Z$, representing the ratio between the retrieval delay and the inter-request delay in an application, and the "cache size" $k$, as in classical caching. Classical caching corresponds to $Z=1$, whereas larger values of $Z$ model applications where retrieving missed requests is expensive. Despite the practical relevance of the delayed-hits model, its theoretical underpinnings are still poorly understood.   We present the first tight theoretical guarantee for optimizing delayed-hits caching: The "Least Recently Used" algorithm, a natural, deterministic, online algorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at most $O(Zk)$ times more latency than the (offline) optimal schedule. Our result extends to any so-called "marking" algorithm.

**Link**: [arxiv](http://arxiv.org/abs/2501.16535v1),  [pdf](http://arxiv.org/pdf/2501.16535v1)

**Tags**: cs.DS 



### SP-IMPact: A Framework for Static Partitioning Interference Mitigation   and Performance Analysis
**Authors**: Diogo Costa, Gonçalo Moreira, Afonso Oliveira, José Martins, Sandro Pinto

**Updated**: 2025-01-27T17:42:20Z

**Summary**: Modern embedded systems are evolving toward complex, heterogeneous architectures to accommodate increasingly demanding applications. Driven by SWAP-C constraints, this shift has led to consolidating multiple systems onto single hardware platforms. Static Partitioning Hypervisors offer a promising solution to partition hardware resources and provide spatial isolation between critical workloads. However, shared resources like the Last-Level Cache and system bus can introduce temporal interference between virtual machines (VMs), negatively impacting performance and predictability. Over the past decade, academia and industry have developed interference mitigation techniques, such as cache partitioning and memory bandwidth reservation. However, configuring these techniques is complex and time-consuming. Cache partitioning requires balancing cache sections across VMs, while memory bandwidth reservation needs tuning bandwidth budgets and periods. Testing all configurations is impractical and often leads to suboptimal results. Moreover, understanding how these techniques interact is limited, as their combined use can produce compounded or conflicting effects on performance. Static analysis tools estimating worst-case execution times offer guidance for configuring mitigation techniques but often fail to capture the complexity of modern multi-core systems. They typically focus on limited shared resources while neglecting others, such as IOMMUs and interrupt controllers. To address these challenges, we present SP-IMPact, an open-source framework for analyzing and guiding interference mitigation configurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth reservation, while evaluating their interactions and cumulative impact. By providing insights on real hardware, SP-IMPact helps optimize configurations for mixed-criticality systems, ensuring performance and predictability.

**Link**: [arxiv](http://arxiv.org/abs/2501.16245v1),  [pdf](http://arxiv.org/pdf/2501.16245v1)

**Tags**: cs.DC cs.PF cs.SY eess.SY 



### Recommenadation aided Caching using Combinatorial Multi-armed Bandits
**Authors**: Pavamana K J, Chandramani Kishore Singh

**Updated**: 2025-01-27T14:55:40Z

**Summary**: We study content caching with recommendations in a wireless network where the users are connected through a base station equipped with a finite-capacity cache. We assume a fixed set of contents with unknown user preferences and content popularities. The base station can cache a subset of the contents and can also recommend subsets of the contents to different users in order to encourage them to request the recommended contents. Recommendations, depending on their acceptability, can thus be used to increase cache hits. We first assume that the users' recommendation acceptabilities are known and formulate the cache hit optimization problem as a combinatorial multi-armed bandit (CMAB). We propose a UCB-based algorithm to decide which contents to cache and recommend and provide an upper bound on the regret of this algorithm. Subsequently, we consider a more general scenario where the users' recommendation acceptabilities are also unknown and propose another UCB-based algorithm that learns these as well. We numerically demonstrate the performance of our algorithms and compare these to state-of-the-art algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2405.00080v4),  [pdf](http://arxiv.org/pdf/2405.00080v4)

**Tags**: cs.LG cs.IR cs.NI 



### SIC-free Multicast Scheduling for Multi-antenna Coded Caching
**Authors**: MohammadJavad Sojdeh, MohammadJavad Salehi, Antti Tölli

**Updated**: 2025-01-27T14:37:24Z

**Summary**: Multi-antenna coded caching (CC) with multicast beamforming typically relies on a complex successive interference cancellation (SIC) structure to decode a superposition of multiple streams received by each user. Signal-level CC schemes require the regeneration and cancellation of interfering signals at the physical layer of each receiver, which complicates practical implementations. To address this, we propose a bit-level multicast scheduling scheme enabling linear, SIC-free decoding of parallel streams by repeatedly transmitting data terms with linearly independent coefficients. Two reference strategies and a novel sparse strategy are considered for constructing the coefficient matrix. The reference cases include the random strategy, which lacks control over matrix construction, and the equal-distant strategy, which balances users' interference and data terms equally. In contrast, the sparse strategy minimizes the number of multicast streams transmitted in parallel during each interval. This approach simplifies both the decoding process and the beamforming design by decoupling the desired data terms for each user and reducing the number of SINR constraints, respectively. To further enhance the symmetric rate, a successive projection algorithm is applied to exploit channel properties and optimize user ordering. With the coefficient matrix and optimized user ordering in place, multicast beamformers are devised to aggregate desired data from relevant multicast streams. Numerical simulations validate the effectiveness of the sparse strategy and user scheduling, demonstrating significant gains in symmetric rate.

**Link**: [arxiv](http://arxiv.org/abs/2501.11126v2),  [pdf](http://arxiv.org/pdf/2501.11126v2)

**Tags**: cs.IT math.IT 



### Random Reshuffling for Stochastic Gradient Langevin Dynamics
**Authors**: Luke Shaw, Peter A. Whalley

**Updated**: 2025-01-27T13:53:12Z

**Summary**: We examine the use of different randomisation policies for stochastic gradient algorithms used in sampling, based on first-order (or overdamped) Langevin dynamics, the most popular of which is known as Stochastic Gradient Langevin Dynamics. Conventionally, this algorithm is combined with a specific stochastic gradient strategy, called Robbins-Monro. In this work, we study an alternative strategy, Random Reshuffling, and show convincingly that it leads to improved performance via: a) a proof of reduced bias in the Wasserstein metric for strongly convex, gradient Lipschitz potentials; b) an analytical demonstration of reduced bias for a Gaussian model problem; and c) an empirical demonstration of reduced bias in numerical experiments for some logistic regression problems. This is especially important since Random Reshuffling is typically more efficient due to memory access and cache reasons. Such acceleration for the Random Reshuffling policy is familiar from the optimisation literature on stochastic gradient descent.

**Link**: [arxiv](http://arxiv.org/abs/2501.16055v1),  [pdf](http://arxiv.org/pdf/2501.16055v1)

**Tags**: math.NA cs.NA math.PR stat.ML 65C05, 82C31, 62F15 



### PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language   Models Quantization
**Authors**: Mengzhao Chen, Yi Liu, Jiahao Wang, Yi Bin, Wenqi Shao, Ping Luo

**Updated**: 2025-01-27T13:39:25Z

**Summary**: Existing weight-activation quantization methods for Large Language Models (LLMs) primarily address channel-wise outliers but often neglect token-wise outliers, which limits the accuracy of quantized models. In this work, we propose PrefixQuant, a novel quantization method that achieves state-of-the-art performance across various precision levels (W4A4KV4 and W4A8KV4) and granularities (dynamic and static quantization) by effectively isolating token-wise outliers. First, PrefixQuant eliminates token-wise outliers by prefixing outlier tokens in the KV cache, a process that is training-free and highly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant introduces new trainable parameters for block-wise training to compensate for quantization error. Our experiments show that PrefixQuant significantly outperforms existing dynamic quantization methods, even under coarser static quantization settings. For instance, PrefixQuant achieves an average accuracy improvement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on five zero-shot reasoning tasks under dynamic and static quantization settings, respectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x prefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant. Our code is available at https://github.com/ChenMnZ/PrefixQuant.

**Link**: [arxiv](http://arxiv.org/abs/2410.05265v2),  [pdf](http://arxiv.org/pdf/2410.05265v2)

**Tags**: cs.LG cs.CL 



### Dynamic Content Caching with Waiting Costs via Restless Multi-Armed   Bandits
**Authors**: Ankita Koley, Chandramani Singh

**Updated**: 2025-01-27T06:47:20Z

**Summary**: We consider a system with a local cache connected to a backend server and an end user population. A set of contents are stored at the the server where they continuously get updated. The local cache keeps copies, potentially stale, of a subset of the contents. The users make content requests to the local cache which either can serve the local version if available or can fetch a fresh version or can wait for additional requests before fetching and serving a fresh version. Serving a stale version of a content incurs an age-of-version(AoV) dependent ageing cost, fetching it from the server incurs a fetching cost, and making a request wait incurs a per unit time waiting cost. We focus on the optimal actions subject to the cache capacity constraint at each decision epoch, aiming at minimizing the long term average cost. We pose the problem as a Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based policy which is known to be asymptotically optimal. We explicitly characterize the Whittle indices. We numerically evaluate the proposed policy and also compare it to a greedy policy. We show that it is close to the optimal policy and substantially outperforms the exising policies.

**Link**: [arxiv](http://arxiv.org/abs/2410.18627v3),  [pdf](http://arxiv.org/pdf/2410.18627v3)

**Tags**: cs.NI 



### Online Allocation with Multi-Class Arrivals: Group Fairness vs   Individual Welfare
**Authors**: Faraz Zargari, Hossein Nekouyan Jazi, Bo Sun, Xiaoqi Tan

**Updated**: 2025-01-27T05:02:05Z

**Summary**: We introduce and study a multi-class online resource allocation problem with group fairness guarantees. The problem involves allocating a fixed amount of resources to a sequence of agents, each belonging to a specific group. The primary objective is to ensure fairness across different groups in an online setting. We focus on three fairness notions: one based on quantity and two based on utility. To achieve fair allocations, we develop two threshold-based online algorithms, proving their optimality under two fairness notions and near-optimality for the more challenging one. Additionally, we demonstrate a fundamental trade-off between group fairness and individual welfare using a novel representative function-based approach. To address this trade-off, we propose a set-aside multi-threshold algorithm that reserves a portion of the resource to ensure fairness across groups while utilizing the remaining resource to optimize efficiency under utility-based fairness notions. This algorithm is proven to achieve the Pareto-optimal trade-off. We also demonstrate that our problem can model a wide range of real-world applications, including network caching and cloud computing, and empirically evaluate our proposed algorithms in the network caching problem using real datasets.

**Link**: [arxiv](http://arxiv.org/abs/2501.15782v1),  [pdf](http://arxiv.org/pdf/2501.15782v1)

**Tags**: cs.GT cs.DS 



### ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language   Model Born from Transformer
**Authors**: Lin Yueyu, Li Zhiyuan, Peter Yue, Liu Xiao

**Updated**: 2025-01-26T15:56:56Z

**Summary**: As is known, hybrid quadratic and subquadratic attention models in multi-head architectures have surpassed both Transformer and Linear RNN models , with these works primarily focusing on reducing KV complexity and improving efficiency. For further research on expressiveness, we introduce our series of models distilled from Qwen 2.5, based on pure native RWKV-7 attention, which aims to make RNN more expressive and demonstrates state tracking ability beyond transformers. We work with QRWK 32B based on RWKV-6 architecture, another approach that reduces the entire knowledge processing time to just 8 hours using 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the distillation process can utilize any LLM, not just Qwen, and enables knowledge transfer from larger LLMs to smaller ones with more fewer tokens. We will explain the detailed process and share our insights on building more powerful foundation models. Please note that this is an ongoing work that will be updated continuously. The model checkpoints and source code are available at \href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside}, \href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}.

**Link**: [arxiv](http://arxiv.org/abs/2501.15570v1),  [pdf](http://arxiv.org/pdf/2501.15570v1)

**Tags**: cs.CL 



### Query-based versus resource-based cache strategies in tag-based browsing   systems
**Authors**: Joaquín Gayoso-Cabada, Mercedes Gómez-Albarrán, José-Luis Sierra

**Updated**: 2025-01-26T11:01:10Z

**Summary**: Tag-based browsing is a popular interaction model for navigating digital libraries. According to this model, users select descriptive tags to filter resources in the collections. Typical implementations of the model are based on inverted indexes. However, these implementations can require a considerable amount of set operations to update the browsing state. To palliate this inconven-ience, it is possible to adopt suitable cache strategies. In this paper we describe and compare two of these strategies: (i) a query-based strategy, according to which previously computed browsing states are indexed by sets of selected tags; and (ii) a resource-based strategy, according to which browsing states are in-dexed by sets of filtered resources. Our comparison focused on runtime perfor-mance, and was carried out empirically, using a real-world web-based collec-tion in the field of digital humanities. The results obtained show that the re-source-based strategy clearly outperforms the query-based one.

**Link**: [arxiv](http://arxiv.org/abs/2501.15481v1),  [pdf](http://arxiv.org/pdf/2501.15481v1)

**Tags**: cs.CL 



### Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for   Efficient LLM Inference
**Authors**: Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, S. Kevin Zhou

**Updated**: 2025-01-26T07:29:06Z

**Summary**: Large Language Models have excelled in various domains but face efficiency challenges due to the growing Key-Value (KV) cache required for long-sequence inference. Recent efforts aim to reduce KV cache size by evicting vast non-critical cache elements during runtime while preserving generation quality. However, these methods typically allocate compression budgets uniformly across all attention heads, ignoring the unique attention patterns of each head. In this paper, we establish a theoretical loss upper bound between pre- and post-eviction attention output, explaining the optimization target of prior cache eviction methods, while guiding the optimization of adaptive budget allocation. Base on this, we propose {\it Ada-KV}, the first head-wise adaptive budget allocation strategy. It offers plug-and-play benefits, enabling seamless integration with prior cache eviction methods. Extensive evaluations on 13 datasets from Ruler and 16 datasets from LongBench, all conducted under both question-aware and question-agnostic scenarios, demonstrate substantial quality improvements over existing methods.

**Link**: [arxiv](http://arxiv.org/abs/2407.11550v4),  [pdf](http://arxiv.org/pdf/2407.11550v4)

**Tags**: cs.CL cs.AI 



### Collaborative Coded Caching for Partially Connected Networks
**Authors**: Kagan Akcay, Eleftherios Lampiris, MohammadJavad Salehi, Giuseppe Caire

**Updated**: 2025-01-26T01:43:46Z

**Summary**: Coded caching leverages the differences in user cache memories to achieve gains that scale with the total cache size, alleviating network congestion due to high-quality content requests. Additionally, distributing transmitters over a wide area can mitigate the adverse effects of path loss. In this work, we consider a partially connected network where the channel between distributed transmitters (helpers) and users is modeled as a distributed MIMO Gaussian broadcast channel. We propose a novel delivery scheme consisting of two phases: partitioning and transmission. In the partitioning phase, users with identical cache profiles are partitioned into the minimum number of sets, such that users within each set can successfully decode their desired message from a joint transmission enabled by MIMO precoding. To optimally partition the users, we employ the branch and bound method. In the transmission phase, each partition is treated as a single entity, and codewords are multicast to partitions with distinct cache profiles. The proposed delivery scheme is applicable to any partially connected network, and while the partitioning is optimal, the overall delivery scheme, including transmission, is heuristic. Interestingly, simulation results show that its performance closely approximates that of the fully connected optimal solution.

**Link**: [arxiv](http://arxiv.org/abs/2501.13298v2),  [pdf](http://arxiv.org/pdf/2501.13298v2)

**Tags**: cs.IT math.IT 



### ReInc: Scaling Training of Dynamic Graph Neural Networks
**Authors**: Mingyu Guan, Saumia Singhal, Taesoo Kim, Anand Padmanabha Iyer

**Updated**: 2025-01-25T23:16:03Z

**Summary**: Dynamic Graph Neural Networks (DGNNs) have gained widespread attention due to their applicability in diverse domains such as traffic network prediction, epidemiological forecasting, and social network analysis. In this paper, we present ReInc, a system designed to enable efficient and scalable training of DGNNs on large-scale graphs. ReInc introduces key innovations that capitalize on the unique combination of Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs) inherent in DGNNs. By reusing intermediate results and incrementally computing aggregations across consecutive graph snapshots, ReInc significantly enhances computational efficiency. To support these optimizations, ReInc incorporates a novel two-level caching mechanism with a specialized caching policy aligned to the DGNN execution workflow. Additionally, ReInc addresses the challenges of managing structural and temporal dependencies in dynamic graphs through a new distributed training strategy. This approach eliminates communication overheads associated with accessing remote features and redistributing intermediate results. Experimental results demonstrate that ReInc achieves up to an order of magnitude speedup compared to state-of-the-art frameworks, tested across various dynamic GNN architectures and real-world graph datasets.

**Link**: [arxiv](http://arxiv.org/abs/2501.15348v1),  [pdf](http://arxiv.org/pdf/2501.15348v1)

**Tags**: cs.LG cs.DC 



### Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle
**Authors**: KVS Chaithanya, Sumesh P. Thampi

**Updated**: 2025-01-25T12:17:41Z

**Summary**: Understanding the hydrodynamics of microswimmers in viscoelastic fluids and confined environments is crucial for interpreting their behaviour in natural settings and designing synthetic microswimmers for practical applications like cargo transport. In this study, we explore the hydrodynamics of a concentric active compound particle - a model microswimmer (a squirmer) positioned at the centre of a viscoelastic fluid droplet (a model cargo) suspended in another viscoelastic medium. We consider the Oldroyd-B constitutive model to characterize the fluids and employ a perturbative approach in the Deborah number to analyze viscoelastic effects analytically, assuming a small Capillary number so that the droplet remains spherical and does not deform. We examine three cases: (i) a squirmer confined within a viscoelastic fluid droplet suspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian fluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined within a viscoelastic fluid droplet suspended in another viscoelastic fluid. Our findings reveal that the swimming speeds of the squirmer and the droplet are determined by the complex interplay of viscoelasticity, the size ratio of the droplet to the squirmer (confinement strength), and the viscosity ratio of the surrounding fluid to the droplet fluid. A critical aspect of this interaction is the positioning of stagnation points within the fluid flow, which governs the distribution of polymeric stress. This distribution, in turn, plays a crucial role in determining the influence of viscoelasticity on the squirmer's dynamics. Our analysis suggests that viscoelastic effects can either enhance or hinder the swimming speed of the squirmer when confined in a droplet, depending on the specific configuration of the system.

**Link**: [arxiv](http://arxiv.org/abs/2410.09479v2),  [pdf](http://arxiv.org/pdf/2410.09479v2)

**Tags**: physics.flu-dyn cond-mat.soft 



### The Selection Problem in Multi-Query Optimization: a Comprehensive   Survey
**Authors**: Sergey Zinchenko, Denis Ponomaryov

**Updated**: 2025-01-25T10:38:11Z

**Summary**: View materialization, index selection, and plan caching are well-known techniques for optimization of query processing in database systems. The essence of these tasks is to select and save a subset of the most useful candidates (views/indexes/plans) for reuse within given space/time budget constraints. In this paper, we propose a unified view on these selection problems. We make a detailed analysis of the root causes of their complexity and summarize techniques to address them. Our survey provides a modern classification of selection algorithms known in the literature, including the latest ones based on Machine Learning. We provide a ground for reuse of the selection techniques between different optimization scenarios and highlight challenges and promising directions in the field. Based on our analysis we derive a method to exponentially accelerate some of the state-of-the-art selection algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2412.11828v2),  [pdf](http://arxiv.org/pdf/2412.11828v2)

**Tags**: cs.DB cs.DM 



### Fully-Automated Code Generation for Efficient Computation of Sparse   Matrix Permanents on GPUs
**Authors**: Deniz Elbek, Kamer Kaya

**Updated**: 2025-01-25T08:27:26Z

**Summary**: Registers are the fastest memory components within the GPU's complex memory hierarchy, accessed by names rather than addresses. They are managed entirely by the compiler through a process called register allocation, during which the compiler attempts to cache predictable data from thread-local memory into thread-private registers. Computing the permanent of a sparse matrix poses a challenge for compilers, as optimizing this process is hindered by the unpredictable distribution of nonzero elements, which only become known at runtime. In this work, we employ fully-automated code generation to address this, producing highly optimized kernels tailored to the matrix's sparsity pattern. State-of-the-art permanent computation algorithms require each thread to store a private array, denoted x, of size n. We first propose a technique that fully stores these arrays in registers, with inclusion and exclusion kernels generated for each column. To minimize control divergence and reduce the number of unique kernels within a warp, we exploit the internal structure of Gray codes, which are also used in the state-of-the-art algorithm. Our second technique reduces register pressure by utilizing both registers and global memory and introduces a matrix ordering and partitioning strategy for greater efficiency. On synthetic matrices, this approach achieves a 31x speedup over state-of-the-art CPU implementations on 112 cores, and an 8x speedup compared to our traditional GPU implementation. For real-world matrices, these speedups are 24.9x and 4.9x.

**Link**: [arxiv](http://arxiv.org/abs/2501.15126v1),  [pdf](http://arxiv.org/pdf/2501.15126v1)

**Tags**: cs.DC cs.DM cs.NA math.NA 



### Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation   of Attention Heads
**Authors**: Xingyang He, Jie Liu, Shaowei Chen

**Updated**: 2025-01-25T07:28:13Z

**Summary**: KV cache is a widely used acceleration technique for large language models (LLMs) inference. However, its memory requirement grows rapidly with input length. Previous studies have reduced the size of KV cache by either removing the same number of unimportant tokens for all attention heads or by allocating differentiated KV cache budgets for pre-identified attention heads. However, due to the importance of attention heads varies across different tasks, the pre-identified attention heads fail to adapt effectively to various downstream tasks. To address this issue, we propose Task-KV, a method that leverages the semantic differentiation of attention heads to allocate differentiated KV cache budgets across various tasks. We demonstrate that attention heads far from the semantic center (called heterogeneous heads) make an significant contribution to task outputs and semantic understanding. In contrast, other attention heads play the role of aggregating important information and focusing reasoning. Task-KV allocates full KV cache budget to heterogeneous heads to preserve comprehensive semantic information, while reserving a small number of recent tokens and attention sinks for non-heterogeneous heads. Furthermore, we innovatively introduce middle activations to preserve key contextual information aggregated from non-heterogeneous heads. To dynamically perceive semantic differences among attention heads, we design a semantic separator to distinguish heterogeneous heads from non-heterogeneous ones based on their distances from the semantic center. Experimental results on multiple benchmarks and different model architectures demonstrate that Task-KV significantly outperforms existing baseline methods.

**Link**: [arxiv](http://arxiv.org/abs/2501.15113v1),  [pdf](http://arxiv.org/pdf/2501.15113v1)

**Tags**: cs.CL 



### A New Construction Structure on Coded Caching with Linear   Subpacketization: Non-Half-Sum Disjoint Packing
**Authors**: Minquan Cheng, Huimei Wei, Kai Wan, Giuseppe Caire

**Updated**: 2025-01-25T04:21:57Z

**Summary**: Coded caching is a promising technique to effectively reduce peak traffic by using local caches and the multicast gains generated by these local caches. We prefer to design a coded caching scheme with the subpacketization $F$ and transmission load $R$ as small as possible since these are the key metrics for evaluating the implementation complexity and transmission efficiency of the scheme, respectively. However, most of the existing coded caching schemes have large subpacketizations which grow exponentially with the number of users $K$, and there are a few schemes with linear subpacketizations which have large transmission loads. In this paper, we focus on studying the linear subpacketization, i.e., $K=F$, coded caching scheme with low transmission load. Specifically, we first introduce a new combinatorial structure called non-half-sum disjoint packing (NHSDP) which can be used to generate a coded caching scheme with $K=F$. Then a class of new schemes is obtained by constructing NHSDP. Theoretical and numerical comparisons show that (i) compared to the existing schemes with linear subpacketization (to the number of users), the proposed scheme achieves a lower load; (ii) compared to some existing schemes with polynomial subpacketization, the proposed scheme can also achieve a lower load in some cases; (iii) compared to some existing schemes with exponential subpacketization, the proposed scheme has loads close to those of these schemes in some cases. Moreover, the new concept of NHSDP is closely related to the classical combinatorial structures such as cyclic difference packing (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash family (PHF). These connections indicate that NHSDP is an important combinatorial structure in the field of combinatorial design.

**Link**: [arxiv](http://arxiv.org/abs/2501.11855v2),  [pdf](http://arxiv.org/pdf/2501.11855v2)

**Tags**: cs.IT math.IT 



### AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for   Vision-Language Models
**Authors**: Zunhai Su, Wang Shen, Linge Li, Zhe Chen, Hanyu Wei, Huangqi Yu, Kehong Yuan

**Updated**: 2025-01-25T02:01:56Z

**Summary**: Vision-language models (VLMs) show remarkable performance in multimodal tasks. However, excessively long multimodal inputs lead to oversized Key-Value (KV) caches, resulting in significant memory consumption and I/O bottlenecks. Previous KV quantization methods for Large Language Models (LLMs) may alleviate these issues but overlook the attention saliency differences of multimodal tokens, resulting in suboptimal performance. In this paper, we investigate the attention-aware token saliency patterns in VLM and propose AKVQ-VL. AKVQ-VL leverages the proposed Text-Salient Attention (TSA) and Pivot-Token-Salient Attention (PSA) patterns to adaptively allocate bit budgets. Moreover, achieving extremely low-bit quantization requires effectively addressing outliers in KV tensors. AKVQ-VL utilizes the Walsh-Hadamard transform (WHT) to construct outlier-free KV caches, thereby reducing quantization difficulty. Evaluations of 2-bit quantization on 12 long-context and multimodal tasks demonstrate that AKVQ-VL maintains or even improves accuracy, outperforming LLM-oriented methods. AKVQ-VL can reduce peak memory usage by 2.13x, support up to 3.25x larger batch sizes and 2.46x throughput.

**Link**: [arxiv](http://arxiv.org/abs/2501.15021v1),  [pdf](http://arxiv.org/pdf/2501.15021v1)

**Tags**: cs.CL 



### EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation
**Authors**: Yifan Yu, Yu Gan, Lillian Tsai, Nikhil Sarda, Jiaming Shen, Yanqi Zhou, Arvind Krishnamurthy, Fan Lai, Henry M. Levy, David Culler

**Updated**: 2025-01-24T19:13:12Z

**Summary**: Large language models (LLMs) have excelled in various applications, yet serving them at scale is challenging due to their substantial resource demands and high latency. Our real-world studies reveal that over 60% of user requests to LLMs have semantically similar counterparts, suggesting the potential for knowledge sharing among requests. However, naively caching and reusing past responses leads to large quality degradation. In this paper, we introduce EchoLM, an in-context caching system that leverages historical requests as examples to guide response generation, enabling selective offloading of requests to more efficient LLMs. However, enabling this real-time knowledge transfer leads to intricate tradeoffs between response quality, latency, and system throughput at scale. For a new request, EchoLM identifies similar, high-utility examples and efficiently prepends them to the input for better response. At scale, EchoLM adaptively routes requests to LLMs of varying capabilities, accounting for response quality and serving loads. EchoLM employs a cost-aware cache replay mechanism to improve example quality and coverage offline, maximizing cache utility and runtime efficiency. Evaluations on millions of open-source requests demonstrate that EchoLM has a throughput improvement of 1.4-5.9x while reducing latency by 28-71% without hurting response quality on average.

**Link**: [arxiv](http://arxiv.org/abs/2501.12689v2),  [pdf](http://arxiv.org/pdf/2501.12689v2)

**Tags**: cs.LG 



### Language-Queried Target Sound Extraction Without Parallel Training Data
**Authors**: Hao Ma, Zhiyuan Peng, Xu Li, Yukai Li, Mingjie Shao, Qiuqiang Kong, Ju Liu

**Updated**: 2025-01-24T15:16:48Z

**Summary**: Language-queried target sound extraction (TSE) aims to extract specific sounds from mixtures based on language queries. Traditional fully-supervised training schemes require extensively annotated parallel audio-text data, which are labor-intensive. We introduce a parallel-data-free training scheme, requiring only unlabelled audio clips for TSE model training by utilizing the contrastive language-audio pre-trained model (CLAP). In a vanilla parallel-data-free training stage, target audio is encoded using the pre-trained CLAP audio encoder to form a condition embedding, while during testing, user language queries are encoded by CLAP text encoder as the condition embedding. This vanilla approach assumes perfect alignment between text and audio embeddings, which is unrealistic. Two major challenges arise from training-testing mismatch: the persistent modality gap between text and audio and the risk of overfitting due to the exposure of rich acoustic details in target audio embedding during training. To address this, we propose a retrieval-augmented strategy. Specifically, we create an embedding cache using audio captions generated by a large language model (LLM). During training, target audio embeddings retrieve text embeddings from this cache to use as condition embeddings, ensuring consistent modalities between training and testing and eliminating information leakage. Extensive experiment results show that our retrieval-augmented approach achieves consistent and notable performance improvements over existing state-of-the-art with better generalizability.

**Link**: [arxiv](http://arxiv.org/abs/2409.09398v2),  [pdf](http://arxiv.org/pdf/2409.09398v2)

**Tags**: eess.AS cs.SD 



### A Programming Model for Disaggregated Memory over CXL
**Authors**: Gal Assa, Lucas Bürgi, Michal Friedman, Ori Lahav

**Updated**: 2025-01-24T14:32:34Z

**Summary**: CXL (Compute Express Link) is an emerging open industry-standard interconnect between processing and memory devices that is expected to revolutionize the way systems are designed in the near future. It enables cache-coherent shared memory pools in a disaggregated fashion at unprecedented scales, allowing algorithms to interact with a variety of storage devices using simple loads and stores. Alongside unleashing unique opportunities for a wide range of applications, CXL introduces new challenges of data management and crash consistency. Alas, CXL lacks an adequate programming model, which makes reasoning about the correctness and expected behaviors of algorithms and systems on top of it nearly impossible.   In this work, we present CXL0, the first programming model for concurrent programs running on top of CXL. We propose a high-level abstraction for CXL memory accesses and formally define operational semantics on top of that abstraction. We perform initial measurements that provide practical insight into CXL0. We provide a set of general transformations that adapt concurrent algorithms to the new disruptive technology. These transformations enhance linearizable algorithms with durability under a general partial-failure model. We provide an additional transformation for algorithms designed for persistent main memory and full-system crashes. We believe that this work will serve as a stepping stone for systems design and modeling on top of CXL, and support the development of future models as software and hardware evolve.

**Link**: [arxiv](http://arxiv.org/abs/2407.16300v2),  [pdf](http://arxiv.org/pdf/2407.16300v2)

**Tags**: cs.DC cs.ET 



### Application-Aware Resource Allocation and Data Management for   MEC-assisted IoT Service Providers
**Authors**: Simone Bolettieri, Raffaele Bruno, Enzo Mingozzi

**Updated**: 2025-01-24T10:39:45Z

**Summary**: To support the growing demand for data-intensive and low-latency IoT applications, Multi-Access Edge Computing (MEC) is emerging as an effective edge-computing approach enabling the execution of delay-sensitive processing tasks close to end-users. However, most of the existing works on resource allocation and service placement in MEC systems overlook the unique characteristics of new IoT use cases. For instance, many IoT applications require the periodic execution of computing tasks on real-time data streams that originate from devices dispersed over a wide area. Thus, users requesting IoT services are typically distant from the data producers. To fill this gap, the contribution of this work is two-fold. Firstly, we propose a MEC-compliant architectural solution to support the operation of multiple IoT service providers over a common MEC platform deployment, which enables the steering and shaping of IoT data transport within the platform. Secondly, we model the problem of service placement and data management in the proposed MEC-based solution taking into account the dependencies at the data level between IoT services and sensing resources. Our model also considers that caches can be deployed on MEC hosts, to allow the sharing of the same data between different IoT services with overlapping geographical scope, and provides support for IoT services with heterogeneous QoS requirements, such as different frequencies of periodic task execution. Due to the complexity of the optimisation problem, a heuristic algorithm is proposed using linear relaxation and rounding techniques. Extensive simulation results demonstrate the efficiency of the proposed approach, especially when traffic demands generated by the service requests are not uniform.

**Link**: [arxiv](http://arxiv.org/abs/2501.14387v1),  [pdf](http://arxiv.org/pdf/2501.14387v1)

**Tags**: cs.NI 



### Joint System Latency and Data Freshness Optimization for Cache-enabled   Mobile Crowdsensing Networks
**Authors**: Kexin Shi, Yaru Fu, Yongna Guo, Fu Lee Wang, Yan Zhang

**Updated**: 2025-01-24T10:00:21Z

**Summary**: Mobile crowdsensing (MCS) networks enable large-scale data collection by leveraging the ubiquity of mobile devices. However, frequent sensing and data transmission can lead to significant resource consumption. To mitigate this issue, edge caching has been proposed as a solution for storing recently collected data. Nonetheless, this approach may compromise data freshness. In this paper, we investigate the trade-off between re-using cached task results and re-sensing tasks in cache-enabled MCS networks, aiming to minimize system latency while maintaining information freshness. To this end, we formulate a weighted delay and age of information (AoI) minimization problem, jointly optimizing sensing decisions, user selection, channel selection, task allocation, and caching strategies. The problem is a mixed-integer non-convex programming problem which is intractable. Therefore, we decompose the long-term problem into sequential one-shot sub-problems and design a framework that optimizes system latency, task sensing decision, and caching strategy subproblems. When one task is re-sensing, the one-shot problem simplifies to the system latency minimization problem, which can be solved optimally. The task sensing decision is then made by comparing the system latency and AoI. Additionally, a Bayesian update strategy is developed to manage the cached task results. Building upon this framework, we propose a lightweight and time-efficient algorithm that makes real-time decisions for the long-term optimization problem. Extensive simulation results validate the effectiveness of our approach.

**Link**: [arxiv](http://arxiv.org/abs/2501.14367v1),  [pdf](http://arxiv.org/pdf/2501.14367v1)

**Tags**: cs.NI eess.SP 



### Locality-aware Fair Scheduling in LLM Serving
**Authors**: Shiyi Cao, Yichuan Wang, Ziming Mao, Pin-Lun Hsu, Liangsheng Yin, Tian Xia, Dacheng Li, Shu Liu, Yineng Zhang, Yang Zhou, Ying Sheng, Joseph Gonzalez, Ion Stoica

**Updated**: 2025-01-24T08:12:47Z

**Summary**: Large language model (LLM) inference workload dominates a wide variety of modern AI applications, ranging from multi-turn conversation to document analysis. Balancing fairness and efficiency is critical for managing diverse client workloads with varying prefix patterns. Unfortunately, existing fair scheduling algorithms for LLM serving, such as Virtual Token Counter (VTC), fail to take prefix locality into consideration and thus suffer from poor performance. On the other hand, locality-aware scheduling algorithms in existing LLM serving frameworks tend to maximize the prefix cache hit rate without considering fair sharing among clients.   This paper introduces the first locality-aware fair scheduling algorithm, Deficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix locality with a fairness guarantee. We also introduce a novel algorithm, Double Deficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find a balance point among fairness, locality, and load-balancing. Our extensive evaluation demonstrates the superior performance of DLPM and D$^2$LPM in ensuring fairness while maintaining high throughput (up to 2.87$\times$ higher than VTC) and low per-client (up to 7.18$\times$ lower than state-of-the-art distributed LLM serving system) latency.

**Link**: [arxiv](http://arxiv.org/abs/2501.14312v1),  [pdf](http://arxiv.org/pdf/2501.14312v1)

**Tags**: cs.DC cs.LG 



### Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement   Learning-based Model Caching and Inference Offloading
**Authors**: Minrui Xu, Dusit Niyato, Christopher G. Brinton

**Updated**: 2025-01-24T03:21:20Z

**Summary**: Large Language Models (LLMs) can perform zero-shot learning on unseen tasks and few-shot learning on complex reasoning tasks. However, resource-limited mobile edge networks struggle to support long-context LLM serving for LLM agents during multi-round interactions with users. Unlike stateless computation offloading and static service offloading in edge computing, optimizing LLM serving at edge servers is challenging because LLMs continuously learn from context which raises accuracy, latency, and resource consumption dynamics. In this paper, we propose a joint model caching and inference offloading framework that utilizes test-time deep reinforcement learning (T2DRL) to optimize deployment and execution strategies for long-context LLM serving. In this framework, we analyze the performance convergence and design an optimization problem considering the utilization of context windows in LLMs. Furthermore, the T2DRL algorithm can learn in both the training phase and the testing phase to proactively manage cached models and service requests and adapt to context changes and usage patterns during execution. To further enhance resource allocation efficiency, we propose a double Dutch auction (DDA) mechanism, which dynamically matches supply and demand while maximizing social welfare. Finally, experimental results demonstrate that the T2DRL algorithm can reduce system costs by at least 30% compared to baselines while guaranteeing the performance of LLM agents in real-world perception and reasoning tasks.

**Link**: [arxiv](http://arxiv.org/abs/2501.14205v1),  [pdf](http://arxiv.org/pdf/2501.14205v1)

**Tags**: cs.NI 



### Sigma: Differential Rescaling of Query, Key and Value for Efficient   Language Models
**Authors**: Zhenghao Lin, Zihao Tang, Xiao Liu, Yeyun Gong, Yi Cheng, Qi Chen, Hang Li, Ying Xin, Ziyue Yang, Kailai Yang, Yu Yan, Xiao Liang, Shuai Lu, Yiming Huang, Zheheng Luo, Lei Qu, Xuan Feng, Yaoxiang Wang, Yuqing Xia, Feiyang Chen, Yuting Jiang, Yasen Hu, Hao Ni, Binyang Li, Guoshuai Zhao, Jui-Hao Chiang, Zhongxin Guo, Chen Lin, Kun Kuang, Wenjie Li, Yelong Shen, Jian Jiao, Peng Cheng, Mao Yang

**Updated**: 2025-01-23T12:58:14Z

**Summary**: We introduce Sigma, an efficient large language model specialized for the system domain, empowered by a novel architecture including DiffQKV attention, and pre-trained on our meticulously collected system domain data. DiffQKV attention significantly enhances the inference efficiency of Sigma by optimizing the Query (Q), Key (K), and Value (V) components in the attention mechanism differentially, based on their varying impacts on the model performance and efficiency indicators. Specifically, we (1) conduct extensive experiments that demonstrate the model's varying sensitivity to the compression of K and V components, leading to the development of differentially compressed KV, and (2) propose augmented Q to expand the Q head dimension, which enhances the model's representation capacity with minimal impacts on the inference speed. Rigorous theoretical and empirical analyses reveal that DiffQKV attention significantly enhances efficiency, achieving up to a 33.36% improvement in inference speed over the conventional grouped-query attention (GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various sources, including 19.5B system domain data that we carefully collect and 1T tokens of synthesized and rewritten data. In general domains, Sigma achieves comparable performance to other state-of-arts models. In the system domain, we introduce the first comprehensive benchmark AIMicius, where Sigma demonstrates remarkable performance across all tasks, significantly outperforming GPT-4 with an absolute improvement up to 52.5%.

**Link**: [arxiv](http://arxiv.org/abs/2501.13629v1),  [pdf](http://arxiv.org/pdf/2501.13629v1)

**Tags**: cs.CL 



### Characterisation of the plutonium isotopic composition of a sediment   core from Palomares, Spain, by low-energy AMS and alpha-spectrometry
**Authors**: E. Chamizo, M. C. Jiménez-Ramos, S. M. Enamorado, M. García-León, R. García-Tenorio, J. L. Mas, P. Masqué, J. Merino, J. A. Sanchez-Cabeza

**Updated**: 2025-01-23T11:18:42Z

**Summary**: The measurement of plutonium isotopes, 239Pu and 240Pu, at 670 kV on the compact accelerator mass spectrometry (AMS) system at the Centro Nacional de Aceleradores (CNA) in Seville, Spain, is now a reality. In this work, we present first Pu AMS results for environmental samples: a sediment core collected in a submarine canyon in the Mediterranean coast of the Spanish region of Palomares, affected by a nuclear accident in 1966. From the study of the 240Pu/239Pu atomic ratio profile, showing on average levels lower than 11%, we confirm that the weapon-grade plutonium released on land during the accident, with a characteristic 240Pu/239Pu atomic ratio of 5.8%, has found its way into the marine environment. A two-plutonium sources mixture model (Palomares and fallout) is used to elucidate the percentage of the plutonium coming from the accident. As a validation exercise of the Pu AMS measuring technique and in order to obtain the 238Pu/(239+240)Pu activity ratios, samples were also studied by alpha-spectrometry (AS). The obtained AS 239+240Pu activity concentration results fit in with the AMS ones in a wide dynamic range, thus validating the AMS technique.

**Link**: [arxiv](http://arxiv.org/abs/2501.13998v1),  [pdf](http://arxiv.org/pdf/2501.13998v1)

**Tags**: physics.ins-det physics.ao-ph 



### POPS: From History to Mitigation of DNS Cache Poisoning Attacks
**Authors**: Yehuda Afek, Harel Berger, Anat Bremler-Barr

**Updated**: 2025-01-23T10:40:09Z

**Summary**: We present a novel yet simple and comprehensive DNS cache POisoning Prevention System (POPS), designed to integrate as a module in Intrusion Prevention Systems (IPS). POPS addresses statistical DNS poisoning attacks, including those documented from 2002 to the present, and offers robust protection against similar future threats. It consists of two main components: a detection module that employs three simple rules, and a mitigation module that leverages the TC flag in the DNS header to enhance security. Once activated, the mitigation module has zero false positives or negatives, correcting any such errors on the side of the detection module.   We first analyze POPS against historical DNS services and attacks, showing that it would have mitigated all network-based statistical poisoning attacks, yielding a success rate of only 0.0076% for the adversary. We then simulate POPS on traffic benchmarks (PCAPs) incorporating current potential network-based statistical poisoning attacks, and benign PCAPs; the simulated attacks still succeed with a probability of 0.0076%. This occurs because five malicious packets go through before POPS detects the attack and activates the mitigation module. In addition, POPS completes its task using only 20%-50% of the time required by other tools (e.g., Suricata or Snort), and after examining just 5%-10% as many packets. Furthermore, it successfully identifies DNS cache poisoning attacks-such as fragmentation attacks-that both Suricata and Snort fail to detect, underscoring its superiority in providing comprehensive DNS protection.

**Link**: [arxiv](http://arxiv.org/abs/2501.13540v1),  [pdf](http://arxiv.org/pdf/2501.13540v1)

**Tags**: cs.CR cs.NI 



### A Training-free Sub-quadratic Cost Transformer Model Serving Framework   With Hierarchically Pruned Attention
**Authors**: Heejun Lee, Geon Park, Youngwan Lee, Jaduk Suh, Jina Kim, Wonyoung Jeong, Bumsik Kim, Hyemin Lee, Myeongjae Jeon, Sung Ju Hwang

**Updated**: 2025-01-23T07:25:28Z

**Summary**: In modern large language models (LLMs), increasing the context length is crucial for improving comprehension and coherence in long-context, multi-modal, and retrieval-augmented language generation. While many recent transformer models attempt to extend their context length over a million tokens, they remain impractical due to the quadratic time and space complexities. Although recent works on linear and sparse attention mechanisms can achieve this goal, their real-world applicability is often limited by the need to re-train from scratch and significantly worse performance. In response, we propose a novel approach, Hierarchically Pruned Attention (HiP), which reduces the time complexity of the attention mechanism to $O(T \log T)$ and the space complexity to $O(T)$, where $T$ is the sequence length. We notice a pattern in the attention scores of pretrained LLMs where tokens close together tend to have similar scores, which we call ``attention locality''. Based on this observation, we utilize a novel tree-search-like algorithm that estimates the top-$k$ key tokens for a given query on the fly, which is mathematically guaranteed to have better performance than random attention pruning. In addition to improving the time complexity of the attention mechanism, we further optimize GPU memory usage by implementing KV cache offloading, which stores only $O(\log T)$ tokens on the GPU while maintaining similar decoding throughput. Experiments on benchmarks show that HiP, with its training-free nature, significantly reduces both prefill and decoding latencies, as well as memory usage, while maintaining high-quality generation with minimal degradation. HiP enables pretrained LLMs to scale up to millions of tokens on commodity GPUs, potentially unlocking long-context LLM applications previously deemed infeasible.

**Link**: [arxiv](http://arxiv.org/abs/2406.09827v3),  [pdf](http://arxiv.org/pdf/2406.09827v3)

**Tags**: cs.CL cs.CV cs.DC cs.LG 



### Parallel Key-Value Cache Fusion for Position Invariant RAG
**Authors**: Philhoon Oh, Jinwoo Shin, James Thorne

**Updated**: 2025-01-23T06:48:22Z

**Summary**: Recent advancements in Large Language Models (LLMs) underscore the necessity of Retrieval Augmented Generation (RAG) to leverage external information. However, LLMs are sensitive to the position of relevant information within contexts and tend to generate incorrect responses when such information is placed in the middle, known as `Lost in the Middle' phenomenon. In this paper, we introduce a framework that generates consistent outputs for decoder-only models, irrespective of the input context order. Experimental results for three open domain question answering tasks demonstrate position invariance, where the model is not sensitive to input context order, and superior robustness to irrelevent passages compared to prevailing approaches for RAG pipelines.

**Link**: [arxiv](http://arxiv.org/abs/2501.07523v2),  [pdf](http://arxiv.org/pdf/2501.07523v2)

**Tags**: cs.AI cs.CL 



### Personalized Federated Learning for Cellular VR: Online Learning and   Dynamic Caching
**Authors**: Krishnendu S. Tharakan, Hayssam Dahrouj, Nour Kouzayha, Hesham ElSawy, Tareq Y. Al-Naffouri

**Updated**: 2025-01-22T16:25:47Z

**Summary**: Delivering an immersive experience to virtual reality (VR) users through wireless connectivity offers the freedom to engage from anywhere at any time. Nevertheless, it is challenging to ensure seamless wireless connectivity that delivers real-time and high-quality videos to the VR users. This paper proposes a field of view (FoV) aware caching for mobile edge computing (MEC)-enabled wireless VR network. In particular, the FoV of each VR user is cached/prefetched at the base stations (BSs) based on the caching strategies tailored to each BS. Specifically, decentralized and personalized federated learning (DP-FL) based caching strategies with guarantees are presented. Considering VR systems composed of multiple VR devices and BSs, a DP-FL caching algorithm is implemented at each BS to personalize content delivery for VR users. The utilized DP-FL algorithm guarantees a probably approximately correct (PAC) bound on the conditional average cache hit. Further, to reduce the cost of communicating gradients, one-bit quantization of the stochastic gradient descent (OBSGD) is proposed, and a convergence guarantee of $\mathcal{O}(1/\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is the number of iterations. Additionally, to better account for the wireless channel dynamics, the FoVs are grouped into multicast or unicast groups based on the number of requesting VR users. The performance of the proposed DP-FL algorithm is validated through realistic VR head-tracking dataset, and the proposed algorithm is shown to have better performance in terms of average delay and cache hit as compared to baseline algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2501.11745v2),  [pdf](http://arxiv.org/pdf/2501.11745v2)

**Tags**: cs.IT cs.LG math.IT 



### Yi-Lightning Technical Report
**Authors**: Alan Wake, Bei Chen, C. X. Lv, Chao Li, Chengen Huang, Chenglin Cai, Chujie Zheng, Daniel Cooper, Fan Zhou, Feng Hu, Ge Zhang, Guoyin Wang, Heng Ji, Howard Qiu, Jiangcheng Zhu, Jun Tian, Katherine Su, Lihuan Zhang, Liying Li, Ming Song, Mou Li, Peng Liu, Qicheng Hu, Shawn Wang, Shijun Zhou, Shiming Yang, Shiyong Li, Tianhang Zhu, Wen Xie, Wenhao Huang, Xiang He, Xiaobo Chen, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Yanpeng Li, Yongke Zhao, Yongzhen Luo, Yuchi Xu, Yuxuan Sha, Zhaodong Yan, Zhiyuan Liu, Zirui Zhang, Zonghong Dai

**Updated**: 2025-01-22T15:09:58Z

**Summary**: This technical report presents Yi-Lightning, our latest flagship large language model (LLM). It achieves exceptional performance, ranking 6th overall on Chatbot Arena, with particularly strong results (2nd to 4th place) in specialized categories including Chinese, Math, Coding, and Hard Prompts. Yi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture, featuring advanced expert segmentation and routing mechanisms coupled with optimized KV-caching techniques. Our development process encompasses comprehensive pre-training, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF), where we devise deliberate strategies for multi-stage training, synthetic data construction, and reward modeling. Furthermore, we implement RAISE (Responsible AI Safety Engine), a four-component framework to address safety issues across pre-training, post-training, and serving phases. Empowered by our scalable super-computing infrastructure, all these innovations substantially reduce training, deployment and inference costs while maintaining high-performance standards. With further evaluations on public academic benchmarks, Yi-Lightning demonstrates competitive performance against top-tier LLMs, while we observe a notable disparity between traditional, static benchmark results and real-world, dynamic human preferences. This observation prompts a critical reassessment of conventional benchmarks' utility in guiding the development of more intelligent and powerful AI systems for practical applications. Yi-Lightning is now available through our developer platform at https://platform.lingyiwanwu.com.

**Link**: [arxiv](http://arxiv.org/abs/2412.01253v5),  [pdf](http://arxiv.org/pdf/2412.01253v5)

**Tags**: cs.CL cs.AI cs.LG 



### Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic   Wrap-Around
**Authors**: Elizabath Peter, K. K. Krishnan Namboodiri, B. Sundar Rajan

**Updated**: 2025-01-22T15:05:08Z

**Summary**: This work explores a multiple transmit antenna setting in a multi-access coded caching (MACC) network where each user accesses more than one cache. A MACC network has $K$ users and $K$ caches, and each user has access to $r < K$ consecutive caches in a cyclic wrap-around manner. There are $L$ antennas at the server, and each cache has a normalized size of $M/N \leq 1$. The cyclic wrap-around MACC network with a single antenna at the server has been a well-investigated topic, and several coded caching schemes and improved lower bounds on the performance are known for the same. However, this MACC network has not yet been studied under multi-antenna settings in the coded caching literature. We study the multi-antenna MACC problem and propose a solution for the same by constructing a pair of arrays called caching and delivery arrays. We present three constructions of caching and delivery arrays for different scenarios and obtain corresponding multi-antenna MACC schemes for the same. Two schemes resulting from the above constructions achieve optimal performance under uncoded placement and one-shot delivery. The optimality is shown by matching the performance of the multi-antenna MACC scheme to that of an optimal multi-antenna scheme for a dedicated cache network having an identical number of users, and each user has a normalized cache size of $rM/N$. Further, as a special case, one of the proposed schemes subsumes an existing optimal MACC scheme for the single-antenna setting.

**Link**: [arxiv](http://arxiv.org/abs/2310.08894v3),  [pdf](http://arxiv.org/pdf/2310.08894v3)

**Tags**: cs.IT math.IT 



### Not all tokens are created equal: Perplexity Attention Weighted Networks   for AI generated text detection
**Authors**: Pablo Miralles-González, Javier Huertas-Tato, Alejandro Martín, David Camacho

**Updated**: 2025-01-22T10:39:50Z

**Summary**: The rapid advancement in large language models (LLMs) has significantly enhanced their ability to generate coherent and contextually relevant text, raising concerns about the misuse of AI-generated content and making it critical to detect it. However, the task remains challenging, particularly in unseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution outputs offers a theoretically appealing approach for detection, as they encapsulate insights from the models' extensive pre-training on diverse corpora. Despite its promise, zero-shot methods that attempt to operationalize these outputs have met with limited success. We hypothesize that one of the problems is that they use the mean to aggregate next-token distribution metrics across tokens, when some tokens are naturally easier or harder to predict and should be weighted differently. Based on this idea, we propose the Perplexity Attention Weighted Network (PAWN), which uses the last hidden states of the LLM and positions to weight the sum of a series of features based on metrics from the next-token distribution across the sequence length. Although not zero-shot, our method allows us to cache the last hidden states and next-token distribution metrics on disk, greatly reducing the training resource requirements. PAWN shows competitive and even better performance in-distribution than the strongest baselines (fine-tuned LMs) with a fraction of their trainable parameters. Our model also generalizes better to unseen domains and source models, with smaller variability in the decision boundary across distribution shifts. It is also more robust to adversarial attacks, and if the backbone has multilingual capabilities, it presents decent generalization to languages not seen during supervised training, with LLaMA3-1B reaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine languages.

**Link**: [arxiv](http://arxiv.org/abs/2501.03940v2),  [pdf](http://arxiv.org/pdf/2501.03940v2)

**Tags**: cs.CL cs.AI 



### Bright single-photon source in a silicon chip by nanoscale positioning   of a color center in a microcavity
**Authors**: Baptiste Lefaucher, Yoann Baron, Jean-Baptiste Jager, Vincent Calvo, Christian Elsässer, Giuliano Coppola, Frédéric Mazen, Sébastien Kerdilès, Félix Cache, Anaïs Dréau, Jean-Michel Gérard

**Updated**: 2025-01-22T09:25:29Z

**Summary**: We present an all-silicon source of near-infrared linearly-polarized single photons, fabricated by nanoscale positioning of a color center in a silicon-on-insulator microcavity. The color center consists of a single W center, created at a well-defined position by Si$^{+}$ ion implantation through a 150 nm-diameter nanohole in a mask. A circular Bragg grating cavity resonant with the W's zero-phonon line at 1217 nm is fabricated at the same location as the nanohole. Under above-gap continuous-wave excitation, a very clean photon antibunching behavior ($g{^2} \leq 0.06$) is observed over the entire power range, which highlights the absence of parasitic emitters. Purcell-enhancement of W's zero-phonon emission provides both a record-high photoluminescence count rate among Si color centers (ca $1.2 \times 10^{6}$ counts/s) and apparent Debye-Waller factor around 99%. We also demonstrate the triggered emission of single photons with 93% purity under weak pulsed laser excitation. At high pulsed laser power, we reveal a detrimental effect of repumping processes, that could be mitigated using selective pumping schemes in the future. These results represent a major step towards on-demand sources of indistinguishable near-infrared single photons within silicon photonics chips.

**Link**: [arxiv](http://arxiv.org/abs/2501.12744v1),  [pdf](http://arxiv.org/pdf/2501.12744v1)

**Tags**: physics.optics quant-ph 



### Improved Coded Caching Scheme for Multi-User Information Retrieval   System
**Authors**: Junyi Wang, Quan Zang, Jinyu Wang, Minquan Cheng

**Updated**: 2025-01-21T22:33:15Z

**Summary**: In this paper, we study the coded caching scheme for the $(L, K, M, N)$ multi-user information retrieval (MIR) system, which consists of a content library containing $N$ files, a base station (BS) with $L$ antennas that cannot access the library, and $K$ single-antenna users, each of which can cache at most $M$ files from the library. The users communicate with the others assisted by the BS to decode their required files. In this paper, we focus on designing a coded caching scheme with low communication latency measured by normalized delivery time (NDT), computational complexity, and subpacketizations. When $\frac{KM}{N}\geq L$ we first simply the precoding matrix in the downlink step to an identity matrix and use the multiple-antenna placement delivery array (MAPDA), which was originally proposed for the multiple-input single-output networks, to generate several new schemes for MIR system. Compared to the existing schemes, both the theoretical and numerical analyses show that our new schemes achieve much lower computational complexity and smaller subpacketizations with the same NDT.

**Link**: [arxiv](http://arxiv.org/abs/2501.12528v1),  [pdf](http://arxiv.org/pdf/2501.12528v1)

**Tags**: cs.IT math.IT 



### Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and   Multiple Level Analysis
**Authors**: Weile Luo, Ruibo Fan, Zeyu Li, Dayou Du, Hongyuan Liu, Qiang Wang, Xiaowen Chu

**Updated**: 2025-01-21T12:19:02Z

**Summary**: Modern GPUs, with their specialized hardware like tensor cores, are essential for demanding AI and deep learning applications. This study presents a comprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU architecture, delving into its performance characteristics and novel features. We benchmark Hopper's memory subsystem latency and throughput, comparing its L2 partitioned cache behavior and global memory access patterns against recent GPU generations, Ampere and Ada Lovelace. Our analysis reveals significant performance differences and architectural improvements in Hopper. A core contribution of this work is a detailed evaluation of Hopper's fourth-generation tensor cores, including their FP8 precision support and the novel asynchronous wgmma instructions, assessing their impact on matrix multiply-accumulate operations. We further investigate the performance implications of other key Hopper innovations: DPX instructions for accelerating dynamic programming algorithms, distributed shared memory (DSM) for inter-SM communication, and the Tensor Memory Accelerator (TMA) for asynchronous data movement. This multi-level approach encompasses instruction-level microbenchmarks, library-level analysis of the Transformer Engine, and application-level benchmarks of tensor core performance within large language models. Our findings provide valuable, in-depth insights for software developers seeking to optimize performance and develop accurate performance models for the Hopper architecture, ultimately contributing to a deeper understanding of its potential for accelerating AI and other computationally intensive workloads.

**Link**: [arxiv](http://arxiv.org/abs/2501.12084v1),  [pdf](http://arxiv.org/pdf/2501.12084v1)

**Tags**: cs.DC cs.AR cs.PF 



### Build Optimization: A Systematic Literature Review
**Authors**: Henri Aïdasso, Mohammed Sayagh, Francis Bordeleau

**Updated**: 2025-01-21T07:32:06Z

**Summary**: Continuous Integration (CI) consists of an automated build process involving continuous compilation, testing, and packaging of the software system. While CI comes up with several advantages related to quality and time to delivery, CI also presents several challenges addressed by a large body of research. To better understand the literature so as to help practitioners find solutions for their problems and guide future research, we conduct a systematic review of 97 studies on build optimization published between 2006 and 2024, which we summarized according to their goals, methodologies, used datasets, and leveraged metrics. The identified build optimization studies focus on two main challenges: (1) long build durations, and (2) build failures. To meet the first challenge, existing studies have developed a range of techniques, including predicting build outcome and duration, selective build execution, and build acceleration using caching or repairing performance smells. The causes of build failures have been the subject of several studies, leading to the development of techniques for predicting build script maintenance and automating repair. Recent studies have also focused on predicting flaky build failures caused by environmental issues. The majority of these techniques use machine learning algorithms and leverage build metrics, which we classify into five categories. Additionally, we identify eight publicly available build datasets for build optimization research.

**Link**: [arxiv](http://arxiv.org/abs/2501.11940v1),  [pdf](http://arxiv.org/pdf/2501.11940v1)

**Tags**: cs.SE 



### PDA Construction via Union of Cartesian Product Cache Configurations for   Coded Caching
**Authors**: Jinyu Wang, Minquan Cheng, Kai Wan, Giuseppe Caire

**Updated**: 2025-01-21T02:35:31Z

**Summary**: Caching is an efficient technique to reduce peak traffic by storing popular content in local caches. Placement delivery array (PDA) proposed by Yan et al. is a combinatorial structure to design coded caching schemes with uncoded placement and one-shot linear delivery. By taking the $m$-fold Cartesian product of a small base PDA, Wang et al. constructed a big PDA while maintaining the memory ratio and transmission load unchanged, which achieves linear growth in both the number of users and coded caching gain. In order to achieve exponential growth in both the number of users and coded caching gain, in this paper we propose a PDA construction by taking the union operation of the cache configurations from the $m$-fold Cartesian product of a base PDA. The resulting PDA leads to a coded caching scheme with subpacketization increasing sub-exponentially with the number of users while keeping the load constant for fixed memory ratio. By applying the proposed construction to existing base PDAs, three new coded caching schemes are obtained, which cover some existing schemes as special cases and can achieve lower load with simultaneously lower subpacketization for some memory ratios.

**Link**: [arxiv](http://arxiv.org/abs/2501.11834v1),  [pdf](http://arxiv.org/pdf/2501.11834v1)

**Tags**: cs.IT math.IT 



### Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference
**Authors**: Pouya Hamadanian, Sadjad Fouladi

**Updated**: 2025-01-20T23:10:13Z

**Summary**: Large Language Models (LLM) have revolutionized natural language processing, but their inference demands substantial resources, while under-utilizing high-end accelerators like GPUs. A major bottleneck arises from the attention mechanism, which requires storing large key-value caches, limiting the maximum achievable throughput way below the available computing resources. Current approaches attempt to mitigate this issue through memory-efficient attention and paging mechanisms, but remained constrained by the assumption that all operations must be performed on high-end accelerators.   In this work, we propose Glinthawk, a two-tiered architecture that decouples the attention mechanism from the rest of the Transformer model. This approach allows the memory requirements for attention to scale independently, enabling larger batch sizes and more efficient use of the high-end accelerators. We prototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the other. Compared to a traditional single-tier setup, it improves throughput by $5.9\times$ and reduces cost of generation by $2.8\times$. For longer sequence lengths, it achieves $16.3\times$ throughput improvement at $2.4\times$ less cost. Our evaluation shows that this architecture can tolerate moderate network latency with minimal performance degradation, making it highly effective for latency-tolerant, throughput-oriented applications such as batch processing. We shared our prototype publicly at \url{https://github.com/microsoft/glinthawk}.

**Link**: [arxiv](http://arxiv.org/abs/2501.11779v1),  [pdf](http://arxiv.org/pdf/2501.11779v1)

**Tags**: cs.LG cs.DC cs.PF 



### Hierarchical Coded Caching in High Memory Regime with Coded Placement
**Authors**: Rajlaxmi Pandey, Charul Rajput, B. Sundar Rajan

**Updated**: 2025-01-20T14:19:48Z

**Summary**: We consider a two-layer hierarchical coded caching network where a server with a library of $N$ files is connected to $K_1$ mirrors, each having a cache memory of size $M_1$. Each mirror is further connected to $K_2$ users, each equipped with a dedicated cache of size $M_2$. In this paper, we propose two distinct coded caching schemes based on coded placement, corresponding to two distinct memory pairs, \( (M_1, M_2) \). We show that the proposed schemes outperform the existing schemes at these memory points given by the proposed schemes for smaller values of $K_2$. In setups where mirrors are positioned near each other, avoiding signal interference is crucial. This can be ensured by having all mirrors transmit using orthogonal carrier frequencies. To compare our schemes with existing ones, we used the composite rate metric, which accurately represents the total bandwidth utilized in such setups. The composite rate is given by $\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the rate from the server to the mirrors, and $R_2$ is the rate from the mirrors to the users, with respect to $M_1$ and $M_2$.

**Link**: [arxiv](http://arxiv.org/abs/2501.11502v1),  [pdf](http://arxiv.org/pdf/2501.11502v1)

**Tags**: cs.IT math.IT 



### Spineless Traversal for Layout Invalidation
**Authors**: Marisa Kirisame, Tiezhi Wang, Pavel Panchekha

**Updated**: 2025-01-20T08:44:01Z

**Summary**: Latency is a major concern for web rendering engines like those in Chrome, Safari, and Firefox. These engines reduce latency by using an incremental layout algorithm to redraw the page when the user interacts with it. In such an algorithm, elements that change frame-to-frame are marked dirty; only the dirty elements need be processed to draw the next frame, dramatically reducing latency. However, the standard incremental layout algorithm must search the page for dirty elements, accessing a number of auxiliary elements in the process. These auxiliary elements add cache misses and stalled cycles, and are responsible for a sizable fraction of all layout latency. We introduce a new, faster incremental layout algorithm called Spineless Traversal. Spineless Traversal uses a more computationally demanding priority queue algorithm to avoid the need to access auxiliary nodes and thus reduces cache traffic and stalls. This leads to dramatic speedups on the most latency-critical interactions such as hovering, typing, or animations. Moreover, thanks to numerous low-level optimizations, we are able to make Spineless Traversal competitive across the whole spectrum of incremental layout workloads. As a result, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the benchmark, with a mean speedup of 3.23x concentrated in the most latency-critical interactions such as hovering, typing, and animations.

**Link**: [arxiv](http://arxiv.org/abs/2411.10659v3),  [pdf](http://arxiv.org/pdf/2411.10659v3)

**Tags**: cs.PL 



## Keyword: LLM Inference 
 ### Cosmic Calipers: Precise and Accurate Neutron Star Radius Measurements   with Next-Generation Gravitational Wave Detectors
**Authors**: Sanika Khadkikar, Ish Gupta, Rahul Kashyap, Koustav Chandra, Rossella Gamba, Bangalore Sathyaprakash

**Updated**: 2025-02-05T18:59:08Z

**Summary**: Gravitational waves from merging binary neutron stars carry characteristic information about their astrophysical properties, including masses and tidal deformabilities, that are needed to infer their radii. In this study, we use Bayesian inference to quantify the precision with which radius can inferred with upgrades in the current gravitational wave detectors and next-generation observatories such as the Einstein Telescope and Cosmic Explorer. We assign evidences for a set of plausible equations of state, which are then used as weights to obtain radius posteriors. We find that prior choices and the loudness of observed signals limit the precision and accuracy of inferred radii by current detectors. In contrast, next-generation observatories can resolve the radius precisely and accurately, across most of the mass range to within $\lesssim 5\%$ for both soft and stiff equations of state. We also explore how the choice of the neutron star mass prior can influence the inferred masses and potentially affect radii measurements, finding that choosing an astrophysically motivated prior does not notably impact an individual neutron star's radius measurements.

**Link**: [arxiv](http://arxiv.org/abs/2502.03463v1),  [pdf](http://arxiv.org/pdf/2502.03463v1)

**Tags**: astro-ph.HE gr-qc 



### Bayesian learning with Gaussian processes for low-dimensional   representations of time-dependent nonlinear systems
**Authors**: Shane A. McQuarrie, Anirban Chaudhuri, Karen E. Willcox, Mengwu Guo

**Updated**: 2025-02-05T18:58:56Z

**Summary**: This work presents a data-driven method for learning low-dimensional time-dependent physics-based surrogate models whose predictions are endowed with uncertainty estimates. We use the operator inference approach to model reduction that poses the problem of learning low-dimensional model terms as a regression of state space data and corresponding time derivatives by minimizing the residual of reduced system equations. Standard operator inference models perform well with accurate training data that are dense in time, but producing stable and accurate models when the state data are noisy and/or sparse in time remains a challenge. Another challenge is the lack of uncertainty estimation for the predictions from the operator inference models. Our approach addresses these challenges by incorporating Gaussian process surrogates into the operator inference framework to (1) probabilistically describe uncertainties in the state predictions and (2) procure analytical time derivative estimates with quantified uncertainties. The formulation leads to a generalized least-squares regression and, ultimately, reduced-order models that are described probabilistically with a closed-form expression for the posterior distribution of the operators. The resulting probabilistic surrogate model propagates uncertainties from the observed state data to reduced-order predictions. We demonstrate the method is effective for constructing low-dimensional models of two nonlinear partial differential equations representing a compressible flow and a nonlinear diffusion-reaction process, as well as for estimating the parameters of a low-dimensional system of nonlinear ordinary differential equations representing compartmental models in epidemiology.

**Link**: [arxiv](http://arxiv.org/abs/2408.03455v2),  [pdf](http://arxiv.org/pdf/2408.03455v2)

**Tags**: math.NA cs.NA 62F15, 60G15, 65C05, 35B30 G.3 



### Do Large Language Model Benchmarks Test Reliability?
**Authors**: Joshua Vendrow, Edward Vendrow, Sara Beery, Aleksander Madry

**Updated**: 2025-02-05T18:58:19Z

**Summary**: When deploying large language models (LLMs), it is important to ensure that these models are not only capable, but also reliable. Many benchmarks have been created to track LLMs' growing capabilities, however there has been no similar focus on measuring their reliability. To understand the potential ramifications of this gap, we investigate how well current benchmarks quantify model reliability. We find that pervasive label errors can compromise these evaluations, obscuring lingering model failures and hiding unreliable behavior.   Motivated by this gap in the evaluation of reliability, we then propose the concept of so-called platinum benchmarks, i.e., benchmarks carefully curated to minimize label errors and ambiguity. As a first attempt at constructing such benchmarks, we revise examples from fifteen existing popular benchmarks. We evaluate a wide range of models on these platinum benchmarks and find that, indeed, frontier LLMs still exhibit failures on simple tasks such as elementary-level math word problems. Analyzing these failures further reveals previously unidentified patterns of problems on which frontier models consistently struggle. We provide code at https://github.com/MadryLab/platinum-benchmarks

**Link**: [arxiv](http://arxiv.org/abs/2502.03461v1),  [pdf](http://arxiv.org/pdf/2502.03461v1)

**Tags**: cs.LG cs.CL 



### Adapt-Pruner: Adaptive Structural Pruning for Efficient Small Language   Model Training
**Authors**: Boyao Wang, Rui Pan, Shizhe Diao, Xingyuan Pan, Jipeng Zhang, Renjie Pi, Tong Zhang

**Updated**: 2025-02-05T18:57:40Z

**Summary**: Small language models (SLMs) have attracted considerable attention from both academia and industry due to their broad range of applications in edge devices. To obtain SLMs with strong performance, conventional approaches either pre-train the models from scratch, which incurs substantial computational costs, or compress/prune existing large language models (LLMs), which results in performance drops and falls short in comparison to pre-training. In this paper, we investigate the family of acceleration methods that involve both structured pruning and model training. We found 1) layer-wise adaptive pruning (Adapt-Pruner) is extremely effective in LLMs and yields significant improvements over existing pruning techniques, 2) adaptive pruning equipped with further training leads to models comparable to those pre-training from scratch, 3) incremental pruning brings non-trivial performance gain by interleaving pruning with training and only removing a small portion of neurons ($\sim$5%) at a time. Experimental results on LLaMA-3.1-8B demonstrate that Adapt-Pruner outperforms conventional pruning methods, such as LLM-Pruner, FLAP, and SliceGPT, by an average of 1%-7% in accuracy on commonsense benchmarks. Additionally, Adapt-Pruner restores the performance of MobileLLM-125M to 600M on the MMLU benchmark with 200$\times$ fewer tokens via pruning from its larger counterparts, and discovers a new 1B model that surpasses LLaMA-3.2-1B in multiple benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2502.03460v1),  [pdf](http://arxiv.org/pdf/2502.03460v1)

**Tags**: cs.LG cs.AI cs.CL 



### SKI Models: Skeleton Induced Vision-Language Embeddings for   Understanding Activities of Daily Living
**Authors**: Arkaprava Sinha, Dominick Reilly, Francois Bremond, Pu Wang, Srijan Das

**Updated**: 2025-02-05T18:57:04Z

**Summary**: The introduction of vision-language models like CLIP has enabled the development of foundational video models capable of generalizing to unseen videos and human actions. However, these models are typically trained on web videos, which often fail to capture the challenges present in Activities of Daily Living (ADL) videos. Existing works address ADL-specific challenges, such as similar appearances, subtle motion patterns, and multiple viewpoints, by combining 3D skeletons and RGB videos. However, these approaches are not integrated with language, limiting their ability to generalize to unseen action classes. In this paper, we introduce SKI models, which integrate 3D skeletons into the vision-language embedding space. SKI models leverage a skeleton-language model, SkeletonCLIP, to infuse skeleton information into Vision Language Models (VLMs) and Large Vision Language Models (LVLMs) through collaborative training. Notably, SKI models do not require skeleton data during inference, enhancing their robustness for real-world applications. The effectiveness of SKI models is validated on three popular ADL datasets for zero-shot action recognition and video caption generation tasks.

**Link**: [arxiv](http://arxiv.org/abs/2502.03459v1),  [pdf](http://arxiv.org/pdf/2502.03459v1)

**Tags**: cs.CV 



### Unconventional anomalous Hall effect in hexagonal polar magnet   Y_3Co_8Sn_4
**Authors**: Afsar Ahmed, Jyoti Sharma, Arnab Bhattacharya, Anis Biswas, Tukai Singha, Yaroslav Mudryk, Aftab Alam, I. Das

**Updated**: 2025-02-05T18:51:45Z

**Summary**: We report a rare realization of unconventional anomalous Hall effect (UAHE) both below and above the magnetic transition temperature (T_C) in a hexagonal noncentrosymmetric magnet Y_3Co_8Sn_4, using a combined experimental and ab-initio calculations. Occurrence of such UAHE is mainly attributed to the reciprocal (KS) topology (i.e. the presence of topological Weyl points at/near the Fermi level), along with some contribution from the topological magnetic texture, as inferred from the measured field-dependent ac susceptibility. The effect of UAHE on the measured transport behavior however evolves differently with temperature above and below T_C, suggesting different physical mechanism responsible in the two phases. A unique planar ferrimagnetic ordering is found to be the most stable state with ab-plane as the easy plane below TC, as observed experimentally. The simulated net magnetization and the moment per Co atom agrees fairly well with the measured values. A reasonably large AHC is also observed in both the phases (above and below and T_C) of the present compound, which is again not so ubiquitous. Our results underscore the family of R_3Co_8Sn_4 (R= rare earth) polar magnets as a compelling backdrop for exploring the synergy of topological magnetism and non-trivial electronic bands, pivotal for spintronic applications.

**Link**: [arxiv](http://arxiv.org/abs/2502.03452v1),  [pdf](http://arxiv.org/pdf/2502.03452v1)

**Tags**: cond-mat.mtrl-sci 



### A Schema-Guided Reason-while-Retrieve framework for Reasoning on Scene   Graphs with Large-Language-Models (LLMs)
**Authors**: Yiye Chen, Harpreet Sawhney, Nicholas Gydé, Yanan Jian, Jack Saunders, Patricio Vela, Ben Lundell

**Updated**: 2025-02-05T18:50:38Z

**Summary**: Scene graphs have emerged as a structured and serializable environment representation for grounded spatial reasoning with Large Language Models (LLMs). In this work, we propose SG-RwR, a Schema-Guided Retrieve-while-Reason framework for reasoning and planning with scene graphs. Our approach employs two cooperative, code-writing LLM agents: a (1) Reasoner for task planning and information queries generation, and a (2) Retriever for extracting corresponding graph information following the queries. Two agents collaborate iteratively, enabling sequential reasoning and adaptive attention to graph information. Unlike prior works, both agents are prompted only with the scene graph schema rather than the full graph data, which reduces the hallucination by limiting input tokens, and drives the Reasoner to generate reasoning trace abstractly.Following the trace, the Retriever programmatically query the scene graph data based on the schema understanding, allowing dynamic and global attention on the graph that enhances alignment between reasoning and retrieval. Through experiments in multiple simulation environments, we show that our framework surpasses existing LLM-based approaches in numerical Q\&A and planning tasks, and can benefit from task-level few-shot examples, even in the absence of agent-level demonstrations. Project code will be released.

**Link**: [arxiv](http://arxiv.org/abs/2502.03450v1),  [pdf](http://arxiv.org/pdf/2502.03450v1)

**Tags**: cs.LG cs.AI cs.MA cs.RO 



### Designing LLM-simulated Immersive Spaces to Enhance Autistic Children's   Social Affordances Understanding
**Authors**: Yancheng Cao, Yangyang HE, Yonglin Chen, Menghan Chen, Shanhe You, Yulin Qiu, Min Liu, Chuan Luo, Chen Zheng, Xin Tong, Jing Liang, Jiangtao Gong

**Updated**: 2025-02-05T18:45:38Z

**Summary**: One of the key challenges faced by autistic children is understanding social affordances in complex environments, which further impacts their ability to respond appropriately to social signals. In traffic scenarios, this impairment can even lead to safety concerns. In this paper, we introduce an LLM-simulated immersive projection environment designed to improve this ability in autistic children while ensuring their safety. We first propose 17 design considerations across four major categories, derived from a comprehensive review of previous research. Next, we developed a system called AIroad, which leverages LLMs to simulate drivers with varying social intents, expressed through explicit multimodal social signals. AIroad helps autistic children bridge the gap in recognizing the intentions behind behaviors and learning appropriate responses through various stimuli. A user study involving 14 participants demonstrated that this technology effectively engages autistic children and leads to significant improvements in their comprehension of social affordances in traffic scenarios. Additionally, parents reported high perceived usability of the system. These findings highlight the potential of combining LLM technology with immersive environments for the functional rehabilitation of autistic children in the future.

**Link**: [arxiv](http://arxiv.org/abs/2502.03447v1),  [pdf](http://arxiv.org/pdf/2502.03447v1)

**Tags**: cs.HC 



### Masked Autoencoders Are Effective Tokenizers for Diffusion Models
**Authors**: Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong Wang, Jindong Wang, Ze Wang, Zicheng Liu, Difan Zou, Bhiksha Raj

**Updated**: 2025-02-05T18:42:04Z

**Summary**: Recent advances in latent diffusion models have demonstrated their effectiveness for high-resolution image synthesis. However, the properties of the latent space from tokenizer for better learning and generation of diffusion models remain under-explored. Theoretically and empirically, we find that improved generation quality is closely tied to the latent distributions with better structure, such as the ones with fewer Gaussian Mixture modes and more discriminative features. Motivated by these insights, we propose MAETok, an autoencoder (AE) leveraging mask modeling to learn semantically rich latent space while maintaining reconstruction fidelity. Extensive experiments validate our analysis, demonstrating that the variational form of autoencoders is not necessary, and a discriminative latent space from AE alone enables state-of-the-art performance on ImageNet generation using only 128 tokens. MAETok achieves significant practical improvements, enabling a gFID of 1.69 with 76x faster training and 31x higher inference throughput for 512x512 generation. Our findings show that the structure of the latent space, rather than variational constraints, is crucial for effective diffusion models. Code and trained models are released.

**Link**: [arxiv](http://arxiv.org/abs/2502.03444v1),  [pdf](http://arxiv.org/pdf/2502.03444v1)

**Tags**: cs.CV cs.AI cs.LG 



### S2-Attention: Hardware-Aware Context Sharding Among Attention Heads
**Authors**: Xihui Lin, Yunan Zhang, Suyu Ge, Liliang Ren, Barun Patra, Vishrav Chaudhary, Hao Peng, Xia Song

**Updated**: 2025-02-05T18:39:43Z

**Summary**: Sparse attention, which selectively attends to a subset of tokens in the context was supposed to be efficient. However, its theoretical reduction in FLOPs has rarely translated into wall-clock speed-up over its dense attention counterparts due to the lack of hardware-aware optimizations like FlashAttention. Meanwhile, it remains unclear whether sparse attention can maintain the model's quality at a scale of today's large language models (LLMs) and how. This paper presents Sparsely-Sharded(S2) Attention, a Triton library that provides kernel optimization for sparse attention customizable at both per-head and per-context-range levels. S2-Attention enables the exploration of novel and high-performance sparse attention techniques, which we demonstrate through extensive ablations across a wide range of sparse attention designs at various model scales. From these insights, we present several basic guidelines to design sparse attention that can achieve not only practical efficiency improvements, but also strong downstream performance. To achieve high parallelization and optimized memory IO, sparse attention should shard the context heterogeneously across attention heads, where each head attends to a different subset of tokens while collectively covering the full context. Meanwhile, we find hybrid architectures combining sparse and dense attention particularly beneficial in practice. S2-Attention achieves wall-clock speedup of 8.79X, 15.87X, 25.3X compared to the strong FlashAttention-2 baseline with strong downstream performance on-par with full attention and perfect retrieval performance at a 128k context length. At inference, for 7B models, our model, with the help of our S2-Attention kernel, achieves 4.5x speed-up compared to dense counterparts. S2-Attention is released with easy-to-customize APIs for direct usage in Megatron and vLLM.

**Link**: [arxiv](http://arxiv.org/abs/2407.17678v7),  [pdf](http://arxiv.org/pdf/2407.17678v7)

**Tags**: cs.CL 



### BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic   Theorem Proving
**Authors**: Ran Xin, Chenguang Xi, Jie Yang, Feng Chen, Hang Wu, Xia Xiao, Yifan Sun, Shen Zheng, Kai Shen

**Updated**: 2025-02-05T18:33:36Z

**Summary**: Recent advancements in large language models (LLMs) have spurred growing interest in automatic theorem proving using Lean4, where effective tree search methods are crucial for navigating proof search spaces. While the existing approaches primarily rely on value functions and Monte Carlo Tree Search (MCTS), the potential of simpler methods like Best-First Search (BFS) remains underexplored. This paper investigates whether BFS can achieve competitive performance in large-scale theorem proving tasks. We present \texttt{BFS-Prover}, a scalable expert iteration framework, featuring three key innovations. First, we implement strategic data filtering at each expert iteration round, excluding problems solvable via beam search node expansion to focus on harder cases. Second, we improve the sample efficiency of BFS through Direct Preference Optimization (DPO) applied to state-tactic pairs automatically annotated with compiler error feedback, refining the LLM's policy to prioritize productive expansions. Third, we employ length normalization in BFS to encourage exploration of deeper proof paths. \texttt{BFS-Prover} achieves a score of $71.31$ on the MiniF2F test set and therefore challenges the perceived necessity of complex tree search methods, demonstrating that BFS can achieve competitive performance when properly scaled.

**Link**: [arxiv](http://arxiv.org/abs/2502.03438v1),  [pdf](http://arxiv.org/pdf/2502.03438v1)

**Tags**: cs.AI 



### Causal survival analysis under competing risks using longitudinal   modified treatment policies
**Authors**: Iván Díaz, Katherine L Hoffman, Nima S. Hejazi

**Updated**: 2025-02-05T18:29:55Z

**Summary**: Longitudinal modified treatment policies (LMTP) have been recently developed as a novel method to define and estimate causal parameters that depend on the natural value of treatment. LMTPs represent an important advancement in causal inference for longitudinal studies as they allow the non-parametric definition and estimation of the joint effect of multiple categorical, numerical, or continuous exposures measured at several time points. We extend the LMTP methodology to problems in which the outcome is a time-to-event variable subject to right-censoring and competing risks. We present identification results and non-parametric locally efficient estimators that use flexible data-adaptive regression techniques to alleviate model misspecification bias, while retaining important asymptotic properties such as $\sqrt{n}$-consistency. We present an application to the estimation of the effect of the time-to-intubation on acute kidney injury amongst COVID-19 hospitalized patients, where death by other causes is taken to be the competing event.

**Link**: [arxiv](http://arxiv.org/abs/2202.03513v3),  [pdf](http://arxiv.org/pdf/2202.03513v3)

**Tags**: stat.ME 



### Harnessing Large Language Models for Curated Code Reviews
**Authors**: Oussama Ben Sghaier, Martin Weyssow, Houari Sahraoui

**Updated**: 2025-02-05T18:15:09Z

**Summary**: In code review, generating structured and relevant comments is crucial for identifying code issues and facilitating accurate code changes that ensure an efficient code review process. Well-crafted comments not only streamline the code review itself but are also essential for subsequent tasks like code refinement, where the code is modified to satisfy the input review comment. Although various AI-based approaches aimed to automate comment generation, their effectiveness remains limited by the quality of the training data. Existing code review datasets are often noisy and unrefined, posing limitations to the learning potential of AI models and hindering the automation process.   To address these challenges, we propose a curation pipeline designed to enhance the quality of the largest publicly available code review dataset. We begin by establishing an evaluation framework, incorporating specific criteria and categories to empirically study the initial quality of the dataset. Using a large language model (LLM)-driven approach, we then apply our curation pipeline to refine the dataset. A comparative analysis of the newly curated dataset, based on the same evaluation framework, demonstrates substantial improvements in the clarity and conciseness of the comments. Additionally, we assess the impact of the curated dataset on automating downstream tasks, specifically comment generation and code refinement. Our findings show that the curated dataset leads to enhanced model performance in generating more accurate comments. Curated comments are also more useful as they lead to more accurate code refinement.

**Link**: [arxiv](http://arxiv.org/abs/2502.03425v1),  [pdf](http://arxiv.org/pdf/2502.03425v1)

**Tags**: cs.SE 



### L-PR: Exploiting LiDAR Fiducial Marker for Unordered Low Overlap   Multiview Point Cloud Registration
**Authors**: Yibo Liu, Jinjun Shan, Amaldev Haridevan, Shuo Zhang

**Updated**: 2025-02-05T18:13:41Z

**Summary**: Point cloud registration is a prerequisite for many applications in computer vision and robotics. Most existing methods focus on pairwise registration of two point clouds with high overlap. Although there have been some methods for low overlap cases, they struggle in degraded scenarios. This paper introduces a novel framework dubbed L-PR, designed to register unordered low overlap multiview point clouds leveraging LiDAR fiducial markers. We refer to them as LiDAR fiducial markers, but they are the same as the popular AprilTag and ArUco markers, thin sheets of paper that do not affect the 3D geometry of the environment. We first propose an improved adaptive threshold marker detection method to provide robust detection results when the viewpoints among point clouds change dramatically. Then, we formulate the unordered multiview point cloud registration problem as a maximum a-posteriori (MAP) problem and develop a framework consisting of two levels of graphs to address it. The first-level graph, constructed as a weighted graph, is designed to efficiently and optimally infer initial values of scan poses from the unordered set. The second-level graph is constructed as a factor graph. By globally optimizing the variables on the graph, including scan poses, marker poses, and marker corner positions, we tackle the MAP problem. We conduct both qualitative and quantitative experiments to demonstrate that the proposed method surpasses previous state-of-the-art (SOTA) methods and to showcase that L-PR can serve as a low-cost and efficient tool for 3D asset collection and training data collection. In particular, we collect a new dataset named Livox-3DMatch using L-PR and incorporate it into the training of the SOTA learning-based method, SGHR, which brings evident improvements for SGHR on various benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2406.03298v3),  [pdf](http://arxiv.org/pdf/2406.03298v3)

**Tags**: cs.CV cs.RO 



### Think or Step-by-Step? UnZIPping the Black Box in Zero-Shot Prompts
**Authors**: Nikta Gohari Sadr, Sangmitra Madhusudan, Ali Emami

**Updated**: 2025-02-05T18:04:29Z

**Summary**: Zero-shot prompting techniques have significantly improved the performance of Large Language Models (LLMs). However, we lack a clear understanding of why zero-shot prompts are so effective. For example, in the prompt "Let's think step-by-step," is "think" or "step-by-step" more crucial to its success? Existing interpretability methods, such as gradient-based and attention-based approaches, are computationally intensive and restricted to open-source models. We introduce the ZIP score (Zero-shot Importance of Perturbation score), a versatile metric applicable to both open and closed-source models, based on systematic input word perturbations. Our experiments across four recent LLMs, seven widely-used prompts, and several tasks, reveal interesting patterns in word importance. For instance, while both 'step-by-step' and 'think' show high ZIP scores, which one is more influential depends on the model and task. We validate our method using controlled experiments and compare our results with human judgments, finding that proprietary models align more closely with human intuition regarding word significance. These findings enhance our understanding of LLM behavior and contribute to developing more effective zero-shot prompts and improved model analysis.

**Link**: [arxiv](http://arxiv.org/abs/2502.03418v1),  [pdf](http://arxiv.org/pdf/2502.03418v1)

**Tags**: cs.CL 



### From Features to Transformers: Redefining Ranking for Scalable Impact
**Authors**: Fedor Borisyuk, Lars Hertel, Ganesh Parameswaran, Gaurav Srivastava, Sudarshan Srinivasa Ramanujam, Borja Ocejo, Peng Du, Andrei Akterskii, Neil Daftary, Shao Tang, Daqi Sun, Qiang Charles Xiao, Deepesh Nathani, Mohit Kothari, Yun Dai, Aman Gupta

**Updated**: 2025-02-05T18:02:01Z

**Summary**: We present LiGR, a large-scale ranking framework developed at LinkedIn that brings state-of-the-art transformer-based modeling architectures into production. We introduce a modified transformer architecture that incorporates learned normalization and simultaneous set-wise attention to user history and ranked items. This architecture enables several breakthrough achievements, including: (1) the deprecation of most manually designed feature engineering, outperforming the prior state-of-the-art system using only few features (compared to hundreds in the baseline), (2) validation of the scaling law for ranking systems, showing improved performance with larger models, more training data, and longer context sequences, and (3) simultaneous joint scoring of items in a set-wise manner, leading to automated improvements in diversity. To enable efficient serving of large ranking models, we describe techniques to scale inference effectively using single-pass processing of user history and set-wise attention. We also summarize key insights from various ablation studies and A/B tests, highlighting the most impactful technical approaches.

**Link**: [arxiv](http://arxiv.org/abs/2502.03417v1),  [pdf](http://arxiv.org/pdf/2502.03417v1)

**Tags**: cs.LG 



### OverThink: Slowdown Attacks on Reasoning LLMs
**Authors**: Abhinav Kumar, Jaechul Roh, Ali Naseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr, Eugene Bagdasarian

**Updated**: 2025-02-05T17:58:46Z

**Summary**: We increase overhead for applications that rely on reasoning LLMs-we force models to spend an amplified number of reasoning tokens, i.e., "overthink", to respond to the user query while providing contextually correct answers. The adversary performs an OVERTHINK attack by injecting decoy reasoning problems into the public content that is used by the reasoning LLM (e.g., for RAG applications) during inference time. Due to the nature of our decoy problems (e.g., a Markov Decision Process), modified texts do not violate safety guardrails. We evaluated our attack across closed-(OpenAI o1, o1-mini, o3-mini) and open-(DeepSeek R1) weights reasoning models on the FreshQA and SQuAD datasets. Our results show up to 18x slowdown on FreshQA dataset and 46x slowdown on SQuAD dataset. The attack also shows high transferability across models. To protect applications, we discuss and implement defenses leveraging LLM-based and system design approaches. Finally, we discuss societal, financial, and energy impacts of OVERTHINK attack which could amplify the costs for third-party applications operating reasoning models.

**Link**: [arxiv](http://arxiv.org/abs/2502.02542v2),  [pdf](http://arxiv.org/pdf/2502.02542v2)

**Tags**: cs.LG cs.CR 



### Estimating causal effects using difference-in-differences under network   dependency and interference
**Authors**: Michael Jetsupphasuk, Didong Li, Michael G. Hudgens

**Updated**: 2025-02-05T17:55:22Z

**Summary**: Differences-in-differences (DiD) is a causal inference method for observational longitudinal data that assumes parallel expected outcome trajectories between treatment groups under the (possible) counterfactual of receiving a specific treatment. In this paper DiD is extended to allow for (i) network dependency where outcomes, treatments, and covariates may exhibit between-unit latent correlation, and (ii) interference, where treatments can affect outcomes in neighboring units. In this setting, the causal estimand of interest is the average exposure effect among units with a specific exposure level, where the exposure is a function of treatments from potentially many units. Under a conditional parallel trends assumption and suitable network dependency conditions, a doubly robust estimator allowing for data-adaptive nuisance function estimation is proposed and shown to be consistent and asymptotically normal with variance reaching the semiparametric efficiency bound. The proposed methods are evaluated in simulations and applied to study the effects of adopting emission control technologies in coal power plants on county-level mortality due to cardiovascular disease.

**Link**: [arxiv](http://arxiv.org/abs/2502.03414v1),  [pdf](http://arxiv.org/pdf/2502.03414v1)

**Tags**: stat.ME 



### Simple Is Effective: The Roles of Graphs and Large Language Models in   Knowledge-Graph-Based Retrieval-Augmented Generation
**Authors**: Mufei Li, Siqi Miao, Pan Li

**Updated**: 2025-02-05T17:45:24Z

**Summary**: Large Language Models (LLMs) demonstrate strong reasoning abilities but face limitations such as hallucinations and outdated knowledge. Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by grounding LLM outputs in structured external knowledge from KGs. However, current KG-based RAG frameworks still struggle to optimize the trade-off between retrieval effectiveness and efficiency in identifying a suitable amount of relevant graph information for the LLM to digest. We introduce SubgraphRAG, extending the KG-based RAG framework that retrieves subgraphs and leverages LLMs for reasoning and answer prediction. Our approach innovatively integrates a lightweight multilayer perceptron with a parallel triple-scoring mechanism for efficient and flexible subgraph retrieval while encoding directional structural distances to enhance retrieval effectiveness. The size of retrieved subgraphs can be flexibly adjusted to match the query's need and the downstream LLM's capabilities. This design strikes a balance between model complexity and reasoning power, enabling scalable and generalizable retrieval processes. Notably, based on our retrieved subgraphs, smaller LLMs like Llama3.1-8B-Instruct deliver competitive results with explainable reasoning, while larger models like GPT-4o achieve state-of-the-art accuracy compared with previous baselines -- all without fine-tuning. Extensive evaluations on the WebQSP and CWQ benchmarks highlight SubgraphRAG's strengths in efficiency, accuracy, and reliability by reducing hallucinations and improving response grounding.

**Link**: [arxiv](http://arxiv.org/abs/2410.20724v4),  [pdf](http://arxiv.org/pdf/2410.20724v4)

**Tags**: cs.CL cs.LG 



### ExploreSelf: Fostering User-driven Exploration and Reflection on   Personal Challenges with Adaptive Guidance by Large Language Models
**Authors**: Inhwa Song, SoHyun Park, Sachin R. Pendse, Jessica Lee Schleider, Munmun De Choudhury, Young-Ho Kim

**Updated**: 2025-02-05T17:41:42Z

**Summary**: Expressing stressful experiences in words is proven to improve mental and physical health, but individuals often disengage with writing interventions as they struggle to organize their thoughts and emotions. Reflective prompts have been used to provide direction, and large language models (LLMs) have demonstrated the potential to provide tailored guidance. However, current systems often limit users' flexibility to direct their reflections. We thus present ExploreSelf, an LLM-driven application designed to empower users to control their reflective journey, providing adaptive support through dynamically generated questions. Through an exploratory study with 19 participants, we examine how participants explore and reflect on personal challenges using ExploreSelf. Our findings demonstrate that participants valued the flexible navigation of adaptive guidance to control their reflective journey, leading to deeper engagement and insight. Building on our findings, we discuss the implications of designing LLM-driven tools that facilitate user-driven and effective reflection of personal challenges.

**Link**: [arxiv](http://arxiv.org/abs/2409.09662v3),  [pdf](http://arxiv.org/pdf/2409.09662v3)

**Tags**: cs.HC cs.AI cs.CL H.5.2; I.2.7 



### SPRI: Aligning Large Language Models with Context-Situated Principles
**Authors**: Hongli Zhan, Muneeza Azmat, Raya Horesh, Junyi Jessy Li, Mikhail Yurochkin

**Updated**: 2025-02-05T17:32:29Z

**Summary**: Aligning Large Language Models to integrate and reflect human values, especially for tasks that demand intricate human oversight, is arduous since it is resource-intensive and time-consuming to depend on human expertise for context-specific guidance. Prior work has utilized predefined sets of rules or principles to steer the behavior of models (Bai et al., 2022; Sun et al., 2023). However, these principles tend to be generic, making it challenging to adapt them to each individual input query or context. In this work, we present Situated-PRInciples (SPRI), a framework requiring minimal or no human effort that is designed to automatically generate guiding principles in real-time for each input query and utilize them to align each response. We evaluate SPRI on three tasks, and show that 1) SPRI can derive principles in a complex domain-specific task that leads to on-par performance as expert-crafted ones; 2) SPRI-generated principles lead to instance-specific rubrics that outperform prior LLM-as-a-judge frameworks; 3) using SPRI to generate synthetic SFT data leads to substantial improvement on truthfulness. We release our code and model generations at https://github.com/honglizhan/SPRI-public.

**Link**: [arxiv](http://arxiv.org/abs/2502.03397v1),  [pdf](http://arxiv.org/pdf/2502.03397v1)

**Tags**: cs.CL cs.AI 



### Benchmarking Time Series Forecasting Models: From Statistical Techniques   to Foundation Models in Real-World Applications
**Authors**: Issar Arab, Rodrigo Benitez

**Updated**: 2025-02-05T17:30:31Z

**Summary**: Time series forecasting is essential for operational intelligence in the hospitality industry, and particularly challenging in large-scale, distributed systems. This study evaluates the performance of statistical, machine learning (ML), deep learning, and foundation models in forecasting hourly sales over a 14-day horizon using real-world data from a network of thousands of restaurants across Germany. The forecasting solution includes features such as weather conditions, calendar events, and time-of-day patterns. Results demonstrate the strong performance of ML-based meta-models and highlight the emerging potential of foundation models like Chronos and TimesFM, which deliver competitive performance with minimal feature engineering, leveraging only the pre-trained model (zero-shot inference). Additionally, a hybrid PySpark-Pandas approach proves to be a robust solution for achieving horizontal scalability in large-scale deployments.

**Link**: [arxiv](http://arxiv.org/abs/2502.03395v1),  [pdf](http://arxiv.org/pdf/2502.03395v1)

**Tags**: cs.LG cs.AI 



### CITER: Collaborative Inference for Efficient Large Language Model   Decoding with Token-Level Routing
**Authors**: Wenhao Zheng, Yixiao Chen, Weitong Zhang, Souvik Kundu, Yun Li, Zhengzhong Liu, Eric P. Xing, Hongyi Wang, Huaxiu Yao

**Updated**: 2025-02-05T17:26:35Z

**Summary**: Large language models have achieved remarkable success in various tasks but suffer from high computational costs during inference, limiting their deployment in resource-constrained applications. To address this issue, we propose a novel CITER (\textbf{C}ollaborative \textbf{I}nference with \textbf{T}oken-l\textbf{E}vel \textbf{R}outing) framework that enables efficient collaboration between small and large language models (SLMs & LLMs) through a token-level routing strategy. Specifically, CITER routes non-critical tokens to an SLM for efficiency and routes critical tokens to an LLM for generalization quality. We formulate router training as a policy optimization, where the router receives rewards based on both the quality of predictions and the inference costs of generation. This allows the router to learn to predict token-level routing scores and make routing decisions based on both the current token and the future impact of its decisions. To further accelerate the reward evaluation process, we introduce a shortcut which significantly reduces the costs of the reward estimation and improving the practicality of our approach. Extensive experiments on five benchmark datasets demonstrate that CITER reduces the inference costs while preserving high-quality generation, offering a promising solution for real-time and resource-constrained applications.

**Link**: [arxiv](http://arxiv.org/abs/2502.01976v2),  [pdf](http://arxiv.org/pdf/2502.01976v2)

**Tags**: cs.CL cs.AI cs.LG cs.PF 



### High-Fidelity Simultaneous Speech-To-Speech Translation
**Authors**: Tom Labiausse, Laurent Mazaré, Edouard Grave, Patrick Pérez, Alexandre Défossez, Neil Zeghidour

**Updated**: 2025-02-05T17:18:55Z

**Summary**: We introduce Hibiki, a decoder-only model for simultaneous speech translation. Hibiki leverages a multistream language model to synchronously process source and target speech, and jointly produces text and audio tokens to perform speech-to-text and speech-to-speech translation. We furthermore address the fundamental challenge of simultaneous interpretation, which unlike its consecutive counterpart, where one waits for the end of the source utterance to start translating, adapts its flow to accumulate just enough context to produce a correct translation in real-time, chunk by chunk. To do so, we introduce a weakly-supervised method that leverages the perplexity of an off-the-shelf text translation system to identify optimal delays on a per-word basis and create aligned synthetic data. After supervised training, Hibiki performs adaptive, simultaneous speech translation with vanilla temperature sampling. On a French-English simultaneous speech translation task, Hibiki demonstrates state-of-the-art performance in translation quality, speaker fidelity and naturalness. Moreover, the simplicity of its inference process makes it compatible with batched translation and even real-time on-device deployment. We provide examples as well as models and inference code.

**Link**: [arxiv](http://arxiv.org/abs/2502.03382v1),  [pdf](http://arxiv.org/pdf/2502.03382v1)

**Tags**: cs.CL cs.SD eess.AS 



### Demystifying Long Chain-of-Thought Reasoning in LLMs
**Authors**: Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, Xiang Yue

**Updated**: 2025-02-05T17:13:32Z

**Summary**: Scaling inference compute enhances reasoning in large language models (LLMs), with long chains-of-thought (CoTs) enabling strategies like backtracking and error correction. Reinforcement learning (RL) has emerged as a crucial method for developing these capabilities, yet the conditions under which long CoTs emerge remain unclear, and RL training requires careful design choices. In this study, we systematically investigate the mechanics of long CoT reasoning, identifying the key factors that enable models to generate long CoT trajectories. Through extensive supervised fine-tuning (SFT) and RL experiments, we present four main findings: (1) While SFT is not strictly necessary, it simplifies training and improves efficiency; (2) Reasoning capabilities tend to emerge with increased training compute, but their development is not guaranteed, making reward shaping crucial for stabilizing CoT length growth; (3) Scaling verifiable reward signals is critical for RL. We find that leveraging noisy, web-extracted solutions with filtering mechanisms shows strong potential, particularly for out-of-distribution (OOD) tasks such as STEM reasoning; and (4) Core abilities like error correction are inherently present in base models, but incentivizing these skills effectively for complex tasks via RL demands significant compute, and measuring their emergence requires a nuanced approach. These insights provide practical guidance for optimizing training strategies to enhance long CoT reasoning in LLMs. Our code is available at: https://github.com/eddycmu/demystify-long-cot.

**Link**: [arxiv](http://arxiv.org/abs/2502.03373v1),  [pdf](http://arxiv.org/pdf/2502.03373v1)

**Tags**: cs.CL cs.LG 



### Agent-OM: Leveraging LLM Agents for Ontology Matching
**Authors**: Zhangcheng Qiang, Weiqing Wang, Kerry Taylor

**Updated**: 2025-02-05T17:08:17Z

**Summary**: Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM (Agent for Ontology Matching), consisting of two Siamese agents for retrieval and matching, with a set of OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM systems show that our system can achieve results very close to the long-standing best performance on simple OM tasks and can significantly improve the performance on complex and few-shot OM tasks.

**Link**: [arxiv](http://arxiv.org/abs/2312.00326v8),  [pdf](http://arxiv.org/pdf/2312.00326v8)

**Tags**: cs.AI cs.CL cs.IR 



### PalimpChat: Declarative and Interactive AI analytics
**Authors**: Chunwei Liu, Gerardo Vitagliano, Brandon Rose, Matt Prinz, David Andrew Samson, Michael Cafarella

**Updated**: 2025-02-05T17:06:59Z

**Summary**: Thanks to the advances in generative architectures and large language models, data scientists can now code pipelines of machine-learning operations to process large collections of unstructured data. Recent progress has seen the rise of declarative AI frameworks (e.g., Palimpzest, Lotus, and DocETL) to build optimized and increasingly complex pipelines, but these systems often remain accessible only to expert programmers. In this demonstration, we present PalimpChat, a chat-based interface to Palimpzest that bridges this gap by letting users create and run sophisticated AI pipelines through natural language alone. By integrating Archytas, a ReAct-based reasoning agent, and Palimpzest's suite of relational and LLM-based operators, PalimpChat provides a practical illustration of how a chat interface can make declarative AI frameworks truly accessible to non-experts.   Our demo system is publicly available online. At SIGMOD'25, participants can explore three real-world scenarios--scientific discovery, legal discovery, and real estate search--or apply PalimpChat to their own datasets. In this paper, we focus on how PalimpChat, supported by the Palimpzest optimizer, simplifies complex AI workflows such as extracting and analyzing biomedical data.

**Link**: [arxiv](http://arxiv.org/abs/2502.03368v1),  [pdf](http://arxiv.org/pdf/2502.03368v1)

**Tags**: cs.AI cs.DB cs.IR 



### Rethinking Approximate Gaussian Inference in Classification
**Authors**: Bálint Mucsányi, Nathaël Da Costa, Philipp Hennig

**Updated**: 2025-02-05T17:03:49Z

**Summary**: In classification tasks, softmax functions are ubiquitously used as output activations to produce predictive probabilities. Such outputs only capture aleatoric uncertainty. To capture epistemic uncertainty, approximate Gaussian inference methods have been proposed, which output Gaussian distributions over the logit space. Predictives are then obtained as the expectations of the Gaussian distributions pushed forward through the softmax. However, such softmax Gaussian integrals cannot be solved analytically, and Monte Carlo (MC) approximations can be costly and noisy. We propose a simple change in the learning objective which allows the exact computation of predictives and enjoys improved training dynamics, with no runtime or memory overhead. This framework is compatible with a family of output activation functions that includes the softmax, as well as element-wise normCDF and sigmoid. Moreover, it allows for approximating the Gaussian pushforwards with Dirichlet distributions by analytic moment matching. We evaluate our approach combined with several approximate Gaussian inference methods (Laplace, HET, SNGP) on large- and small-scale datasets (ImageNet, CIFAR-10), demonstrating improved uncertainty quantification capabilities compared to softmax MC sampling. Code is available at https://github.com/bmucsanyi/probit.

**Link**: [arxiv](http://arxiv.org/abs/2502.03366v1),  [pdf](http://arxiv.org/pdf/2502.03366v1)

**Tags**: cs.LG stat.ML 



### A Beam's Eye View to Fluence Maps 3D Network for Ultra Fast VMAT   Radiotherapy Planning
**Authors**: Simon Arberet, Florin C. Ghesu, Riqiang Gao, Martin Kraus, Jonathan Sackett, Esa Kuusela, Ali Kamen

**Updated**: 2025-02-05T16:56:17Z

**Summary**: Volumetric Modulated Arc Therapy (VMAT) revolutionizes cancer treatment by precisely delivering radiation while sparing healthy tissues. Fluence maps generation, crucial in VMAT planning, traditionally involves complex and iterative, and thus time consuming processes. These fluence maps are subsequently leveraged for leaf-sequence. The deep-learning approach presented in this article aims to expedite this by directly predicting fluence maps from patient data. We developed a 3D network which we trained in a supervised way using a combination of L1 and L2 losses, and RT plans generated by Eclipse and from the REQUITE dataset, taking the RT dose map as input and the fluence maps computed from the corresponding RT plans as target. Our network predicts jointly the 180 fluence maps corresponding to the 180 control points (CP) of single arc VMAT plans. In order to help the network, we pre-process the input dose by computing the projections of the 3D dose map to the beam's eye view (BEV) of the 180 CPs, in the same coordinate system as the fluence maps. We generated over 2000 VMAT plans using Eclipse to scale up the dataset size. Additionally, we evaluated various network architectures and analyzed the impact of increasing the dataset size. We are measuring the performance in the 2D fluence maps domain using image metrics (PSNR, SSIM), as well as in the 3D dose domain using the dose-volume histogram (DVH) on a validation dataset. The network inference, which does not include the data loading and processing, is less than 20ms. Using our proposed 3D network architecture as well as increasing the dataset size using Eclipse improved the fluence map reconstruction performance by approximately 8 dB in PSNR compared to a U-Net architecture trained on the original REQUITE dataset. The resulting DVHs are very close to the one of the input target dose.

**Link**: [arxiv](http://arxiv.org/abs/2502.03360v1),  [pdf](http://arxiv.org/pdf/2502.03360v1)

**Tags**: eess.IV cs.AI physics.med-ph 



### Distilling Implicit Multimodal Knowledge into Large Language Models for   Zero-Resource Dialogue Generation
**Authors**: Bo Zhang, Hui Ma, Jian Ding, Jian Wang, Bo Xu, Hongfei Lin

**Updated**: 2025-02-05T16:54:42Z

**Summary**: Integrating multimodal knowledge into large language models (LLMs) represents a significant advancement in dialogue generation capabilities. However, the effective incorporation of such knowledge in zero-resource scenarios remains a substantial challenge due to the scarcity of diverse, high-quality dialogue datasets. To address this, we propose the Visual Implicit Knowledge Distillation Framework (VIKDF), an innovative approach aimed at enhancing LLMs for enriched dialogue generation in zero-resource contexts by leveraging implicit multimodal knowledge. VIKDF comprises two main stages: knowledge distillation, using an Implicit Query Transformer to extract and encode visual implicit knowledge from image-text pairs into knowledge vectors; and knowledge integration, employing a novel Bidirectional Variational Information Fusion technique to seamlessly integrate these distilled vectors into LLMs. This enables the LLMs to generate dialogues that are not only coherent and engaging but also exhibit a deep understanding of the context through implicit multimodal cues, effectively overcoming the limitations of zero-resource scenarios. Our extensive experimentation across two dialogue datasets shows that VIKDF outperforms existing state-of-the-art models in generating high-quality dialogues. The code is available at https://github.com/zhangbo-nlp/VIKDF.

**Link**: [arxiv](http://arxiv.org/abs/2405.10121v2),  [pdf](http://arxiv.org/pdf/2405.10121v2)

**Tags**: cs.CL cs.MM 



### Minerva: A Programmable Memory Test Benchmark for Language Models
**Authors**: Menglin Xia, Victor Ruehle, Saravan Rajmohan, Reza Shokri

**Updated**: 2025-02-05T16:53:45Z

**Summary**: How effectively can LLM-based AI assistants utilize their memory (context) to perform various tasks? Traditional data benchmarks, which are often manually crafted, suffer from several limitations: they are static, susceptible to overfitting, difficult to interpret, and lack actionable insights--failing to pinpoint the specific capabilities a model lacks when it does not pass a test. In this paper, we present a framework for automatically generating a comprehensive set of tests to evaluate models' abilities to use their memory effectively. Our framework extends the range of capability tests beyond the commonly explored (passkey, key-value, needle in the haystack) search, a dominant focus in the literature. Specifically, we evaluate models on atomic tasks such as searching, recalling, editing, matching, comparing information in context memory, and performing basic operations when inputs are structured into distinct blocks, simulating real-world data. Additionally, we design composite tests to investigate the models' ability to maintain state while operating on memory. Our benchmark enables an interpretable, detailed assessment of memory capabilities of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2502.03358v1),  [pdf](http://arxiv.org/pdf/2502.03358v1)

**Tags**: cs.CL 



### Inverse Mixed Strategy Games with Generative Trajectory Models
**Authors**: Max Muchen Sun, Pete Trautman, Todd Murphey

**Updated**: 2025-02-05T16:53:34Z

**Summary**: Game-theoretic models are effective tools for modeling multi-agent interactions, especially when robots need to coordinate with humans. However, applying these models requires inferring their specifications from observed behaviors -- a challenging task known as the inverse game problem. Existing inverse game approaches often struggle to account for behavioral uncertainty and measurement noise, and leverage both offline and online data. To address these limitations, we propose an inverse game method that integrates a generative trajectory model into a differentiable mixed-strategy game framework. By representing the mixed strategy with a conditional variational autoencoder (CVAE), our method can infer high-dimensional, multi-modal behavior distributions from noisy measurements while adapting in real-time to new observations. We extensively evaluate our method in a simulated navigation benchmark, where the observations are generated by an unknown game model. Despite the model mismatch, our method can infer Nash-optimal actions comparable to those of the ground-truth model and the oracle inverse game baseline, even in the presence of uncertain agent objectives and noisy measurements.

**Link**: [arxiv](http://arxiv.org/abs/2502.03356v1),  [pdf](http://arxiv.org/pdf/2502.03356v1)

**Tags**: cs.RO 



### Constraints on Ultra-light Axion Dark Matter through Galaxy Cluster   Number Counts
**Authors**: S. Zelmer, E. Artis, E. Bulbul, S. Grandis, V. Ghirardini, A. von der Linden, Y. E. Bahar, F. Balzer, M. Brüggen, I. Chiu, N. Clerc, J. Comparat, F. Kleinebreil, M. Kluge, S. Krippendorf, A. Liu, N. Malavasi, A. Merloni, H. Miyatake, S. Miyazaki, K. Nandra, N. Okabe, M. E. Ramos-Ceja, J. S. Sanders, T. Schrabback, R. Seppi, J. Weller, X. Zhang

**Updated**: 2025-02-05T16:48:42Z

**Summary**: Ultra-light axions are hypothetical scalar particles that influence the evolution of large-scale structures of the Universe. Depending on their mass, they can potentially be part of the dark matter component of the Universe, as candidates commonly referred to as fuzzy dark matter. While strong constraints have been established for pure fuzzy dark matter models, the more general scenario where ultra-light axions constitute only a fraction of the dark matter has been limited to a few observational probes. In this work, we use the galaxy cluster number counts obtained from the first All-Sky Survey (eRASS1) of the SRG/eROSITA mission together with gravitational weak lensing data from the Dark Energy Survey, the Kilo-Degree Survey, and the Hyper Suprime-Cam, to constrain the fraction of ultra-light axions in the mass range $10^{-32}$ eV to $10^{-24}$ eV. We put upper bounds on the ultra-light axion relic density in independent logarithmic axion mass bins by performing a full cosmological parameter inference. We find an exclusion region in the intermediate ultra-light axion mass regime with the tightest bounds reported so far in the mass bins around $m_\mathrm{a}=10^{-27}$ eV with $\Omega_\mathrm{a} < 0.0036$ and $m_\mathrm{a}=10^{-26}$ eV with $\Omega_\mathrm{a} < 0.0084$, both at 95% confidence level. When combining with CMB probes, these bounds are tightened to $\Omega_\mathrm{a} < 0.0030$ in the $m_\mathrm{a}=10^{27}$ eV mass bin and $\Omega_\mathrm{a} < 0.0058$ in the $m_\mathrm{a}=10^{-26}$ eV mass bin, both at 95% confidence level. This is the first time that constraints on ultra-light axions have been obtained using the growth of structure measured by galaxy cluster number counts. These results pave the way for large surveys, which can be utilized to obtain tight constraints on the mass and relic density of ultra-light axions with better theoretical modeling of the abundance of halos.

**Link**: [arxiv](http://arxiv.org/abs/2502.03353v1),  [pdf](http://arxiv.org/pdf/2502.03353v1)

**Tags**: astro-ph.CO hep-ph 



### Reconstructing the Magnetic Field in an Arbitrary Domain via Data-driven   Bayesian Methods and Numerical Simulations
**Authors**: Georgios E. Pavlou, Vasiliki Pavlidou, Vagelis Harmandaris

**Updated**: 2025-02-05T16:43:21Z

**Summary**: Inverse problems are prevalent in numerous scientific and engineering disciplines, where the objective is to determine unknown parameters within a physical system using indirect measurements or observations. The inherent challenge lies in deducing the most probable parameter values that align with the collected data. This study introduces an algorithm for reconstructing parameters by addressing an inverse problem formulated through differential equations underpinned by uncertain boundary conditions or variant parameters. We adopt a Bayesian approach for parameter inference, delineating the establishment of prior, likelihood, and posterior distributions, and the subsequent resolution of the maximum a posteriori problem via numerical optimization techniques. The proposed algorithm is applied to the task of magnetic field reconstruction within a conical domain, demonstrating precise recovery of the true parameter values.

**Link**: [arxiv](http://arxiv.org/abs/2404.15745v3),  [pdf](http://arxiv.org/pdf/2404.15745v3)

**Tags**: physics.comp-ph astro-ph.HE cs.NA math.NA 



### Prepending or Cross-Attention for Speech-to-Text? An Empirical   Comparison
**Authors**: Tsz Kin Lam, Marco Gaido, Sara Papi, Luisa Bentivogli, Barry Haddow

**Updated**: 2025-02-05T16:40:51Z

**Summary**: Following the remarkable success of Large Language Models (LLMs) in NLP tasks, there is increasing interest in extending their capabilities to speech -- the most common form of communication. The most widespread approach to integrating speech into LLMs is dense feature prepending (DFP), which prepends the projected speech representations to the textual representations, allowing end-to-end training with a speech encoder. This raises questions about the need for a sophisticated speech encoder for DFP and how its performance compares with a standard encoder-decoder (i.e., cross-attention) architecture. We compare DFP and cross-attention under a variety of configurations, such as CTC compression, sequence-level knowledge distillation, on monolingual, bilingual, and multilingual models. To perform a controlled architectural comparison, we train all models from scratch rather than using large pretrained models and use comparable data and parameter settings, testing speech-to-text recognition (ASR) and translation (ST) on MuST-C v1.0 and CoVoST2 datasets. Despite the wide adoption of DFP, our results do not indicate a clear advantage of DFP over cross-attention.

**Link**: [arxiv](http://arxiv.org/abs/2501.02370v2),  [pdf](http://arxiv.org/pdf/2501.02370v2)

**Tags**: cs.CL cs.SD eess.AS 



### Implicit Communication in Human-Robot Collaborative Transport
**Authors**: Elvin Yang, Christoforos Mavrogiannis

**Updated**: 2025-02-05T16:39:26Z

**Summary**: We focus on human-robot collaborative transport, in which a robot and a user collaboratively move an object to a goal pose. In the absence of explicit communication, this problem is challenging because it demands tight implicit coordination between two heterogeneous agents, who have very different sensing, actuation, and reasoning capabilities. Our key insight is that the two agents can coordinate fluently by encoding subtle, communicative signals into actions that affect the state of the transported object. To this end, we design an inference mechanism that probabilistically maps observations of joint actions executed by the two agents to a set of joint strategies of workspace traversal. Based on this mechanism, we define a cost representing the human's uncertainty over the unfolding traversal strategy and introduce it into a model predictive controller that balances between uncertainty minimization and efficiency maximization. We deploy our framework on a mobile manipulator (Hello Robot Stretch) and evaluate it in a within-subjects lab study (N=24). We show that our framework enables greater team performance and empowers the robot to be perceived as a significantly more fluent and competent partner compared to baselines lacking a communicative mechanism.

**Link**: [arxiv](http://arxiv.org/abs/2502.03346v1),  [pdf](http://arxiv.org/pdf/2502.03346v1)

**Tags**: cs.RO 



### Adaptive Variational Inference in Probabilistic Graphical Models: Beyond   Bethe, Tree-Reweighted, and Convex Free Energies
**Authors**: Harald Leisenberger, Franz Pernkopf

**Updated**: 2025-02-05T16:33:59Z

**Summary**: Variational inference in probabilistic graphical models aims to approximate fundamental quantities such as marginal distributions and the partition function. Popular approaches are the Bethe approximation, tree-reweighted, and other types of convex free energies. These approximations are efficient but can fail if the model is complex and highly interactive. In this work, we analyze two classes of approximations that include the above methods as special cases: first, if the model parameters are changed; and second, if the entropy approximation is changed. We discuss benefits and drawbacks of either approach, and deduce from this analysis how a free energy approximation should ideally be constructed. Based on our observations, we propose approximations that automatically adapt to a given model and demonstrate their effectiveness for a range of difficult problems.

**Link**: [arxiv](http://arxiv.org/abs/2502.03341v1),  [pdf](http://arxiv.org/pdf/2502.03341v1)

**Tags**: stat.ML cs.AI cs.LG 



### A Mixture-Based Framework for Guiding Diffusion Models
**Authors**: Yazid Janati, Badr Moufad, Mehdi Abou El Qassime, Alain Durmus, Eric Moulines, Jimmy Olsson

**Updated**: 2025-02-05T16:26:06Z

**Summary**: Denoising diffusion models have driven significant progress in the field of Bayesian inverse problems. Recent approaches use pre-trained diffusion models as priors to solve a wide range of such problems, only leveraging inference-time compute and thereby eliminating the need to retrain task-specific models on the same dataset. To approximate the posterior of a Bayesian inverse problem, a diffusion model samples from a sequence of intermediate posterior distributions, each with an intractable likelihood function. This work proposes a novel mixture approximation of these intermediate distributions. Since direct gradient-based sampling of these mixtures is infeasible due to intractable terms, we propose a practical method based on Gibbs sampling. We validate our approach through extensive experiments on image inverse problems, utilizing both pixel- and latent-space diffusion priors, as well as on source separation with an audio diffusion model. The code is available at https://www.github.com/badr-moufad/mgdm

**Link**: [arxiv](http://arxiv.org/abs/2502.03332v1),  [pdf](http://arxiv.org/pdf/2502.03332v1)

**Tags**: stat.ML cs.LG 



### ECM: A Unified Electronic Circuit Model for Explaining the Emergence of   In-Context Learning and Chain-of-Thought in Large Language Model
**Authors**: Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiaqi Wang, Mengkang Hu, Zhi Chen, Wanxiang Che, Ting Liu

**Updated**: 2025-02-05T16:22:33Z

**Summary**: Recent advancements in large language models (LLMs) have led to significant successes across various applications, where the most noticeable is to a series of emerging capabilities, particularly in the areas of In-Context Learning (ICL) and Chain-of-Thought (CoT). To better understand and control model performance, many studies have begun investigating the underlying causes of these phenomena and their impact on task outcomes. However, existing explanatory frameworks predominantly focus on isolating and explaining ICL and CoT independently, leading to an incomplete understanding of their combined influence on model performance. To address this gap, we propose the Electronic Circuit Model (ECM), which provides a foundation for developing scalable, learnable policies and improving the management of AI-generated content. Specifically, ECM conceptualizes model behavior as an electronic circuit: ICL is represented as semantic magnetic field to providing an additional voltage following Faraday's Law, while CoT is modeled as series resistors to constrain the model output performance following Ohm's Law. Experimental results demonstrate that the ECM effectively predicts and explains LLM performance across a variety of prompting strategies. Furthermore, we apply ECM to advanced reasoning strategy optimization on a series of tasks, such as the International Olympiad in Informatics (IOI) and the International Mathematical Olympiad (IMO), achieving competitive performance that surpasses nearly 80% of top human competitors.

**Link**: [arxiv](http://arxiv.org/abs/2502.03325v1),  [pdf](http://arxiv.org/pdf/2502.03325v1)

**Tags**: cs.CL cs.AI 



### Out-of-Distribution Detection using Synthetic Data Generation
**Authors**: Momin Abbas, Muneeza Azmat, Raya Horesh, Mikhail Yurochkin

**Updated**: 2025-02-05T16:22:09Z

**Summary**: Distinguishing in- and out-of-distribution (OOD) inputs is crucial for reliable deployment of classification systems. However, OOD data is typically unavailable or difficult to collect, posing a significant challenge for accurate OOD detection. In this work, we present a method that harnesses the generative capabilities of Large Language Models (LLMs) to create high-quality synthetic OOD proxies, eliminating the dependency on any external OOD data source. We study the efficacy of our method on classical text classification tasks such as toxicity detection and sentiment classification as well as classification tasks arising in LLM development and deployment, such as training a reward model for RLHF and detecting misaligned generations. Extensive experiments on nine InD-OOD dataset pairs and various model sizes show that our approach dramatically lowers false positive rates (achieving a perfect zero in some cases) while maintaining high accuracy on in-distribution tasks, outperforming baseline methods by a significant margin.

**Link**: [arxiv](http://arxiv.org/abs/2502.03323v1),  [pdf](http://arxiv.org/pdf/2502.03323v1)

**Tags**: cs.CL cs.AI cs.LG 



### Conversation Routines: A Prompt Engineering Framework for Task-Oriented   Dialog Systems
**Authors**: Giorgio Robino

**Updated**: 2025-02-05T16:21:05Z

**Summary**: This study introduces Conversation Routines (CR), a structured prompt engineering framework for developing task-oriented dialog systems using Large Language Models (LLMs). While LLMs demonstrate remarkable natural language understanding capabilities, engineering them to reliably execute complex business workflows remains challenging. The proposed CR framework enables the development of Conversation Agentic Systems (CAS) through natural language specifications, embedding task-oriented logic within LLM prompts. This approach provides a systematic methodology for designing and implementing complex conversational workflows while maintaining behavioral consistency. We demonstrate the framework's effectiveness through two proof-of-concept implementations: a Train Ticket Booking System and an Interactive Troubleshooting Copilot. These case studies validate CR's capability to encode sophisticated behavioral patterns and decision logic while preserving natural conversational flexibility. Results show that CR enables domain experts to design conversational workflows in natural language while leveraging custom functions (tools) developed by software engineers, creating an efficient division of responsibilities where developers focus on core API implementation and domain experts handle conversation design. While the framework shows promise in accessibility and adaptability, we identify key challenges including computational overhead, non-deterministic behavior, and domain-specific logic optimization. Future research directions include CR evaluation methods based on prompt engineering frameworks driven by goal-oriented grading criteria, improving scalability for complex multi-agent interactions, and enhancing system robustness to address the identified limitations across diverse business applications.

**Link**: [arxiv](http://arxiv.org/abs/2501.11613v3),  [pdf](http://arxiv.org/pdf/2501.11613v3)

**Tags**: cs.CL cs.AI cs.ET cs.HC cs.PL 



### GP-GS: Gaussian Processes for Enhanced Gaussian Splatting
**Authors**: Zhihao Guo, Jingxuan Su, Shenglin Wang, Jinlong Fan, Jing Zhang, Liangxiu Han, Peng Wang

**Updated**: 2025-02-05T16:09:26Z

**Summary**: 3D Gaussian Splatting has emerged as an efficient photorealistic novel view synthesis method. However, its reliance on sparse Structure-from-Motion (SfM) point clouds consistently compromises the scene reconstruction quality. To address these limitations, this paper proposes a novel 3D reconstruction framework Gaussian Processes Gaussian Splatting (GP-GS), where a multi-output Gaussian Process model is developed to achieve adaptive and uncertainty-guided densification of sparse SfM point clouds. Specifically, we propose a dynamic sampling and filtering pipeline that adaptively expands the SfM point clouds by leveraging GP-based predictions to infer new candidate points from the input 2D pixels and depth maps. The pipeline utilizes uncertainty estimates to guide the pruning of high-variance predictions, ensuring geometric consistency and enabling the generation of dense point clouds. The densified point clouds provide high-quality initial 3D Gaussians to enhance reconstruction performance. Extensive experiments conducted on synthetic and real-world datasets across various scales validate the effectiveness and practicality of the proposed framework.

**Link**: [arxiv](http://arxiv.org/abs/2502.02283v2),  [pdf](http://arxiv.org/pdf/2502.02283v2)

**Tags**: cs.CV cs.AI 68T45 



### Intent Representation Learning with Large Language Model for   Recommendation
**Authors**: Yu Wang, Lei Sang, Yi Zhang, Yiwen Zhang

**Updated**: 2025-02-05T16:08:05Z

**Summary**: Intent-based recommender systems have garnered significant attention for uncovering latent fine-grained preferences. Intents, as underlying factors of interactions, are crucial for improving recommendation interpretability. Most methods define intents as learnable parameters updated alongside interactions. However, existing frameworks often overlook textual information (e.g., user reviews, item descriptions), which is crucial for alleviating the sparsity of interaction intents. Exploring these multimodal intents, especially the inherent differences in representation spaces, poses two key challenges: i) How to align multimodal intents and effectively mitigate noise issues; ii) How to extract and match latent key intents across modalities. To tackle these challenges, we propose a model-agnostic framework, Intent Representation Learning with Large Language Model (IRLLRec), which leverages large language models (LLMs) to construct multimodal intents and enhance recommendations. Specifically, IRLLRec employs a dual-tower architecture to learn multimodal intent representations. Next, we propose pairwise and translation alignment to eliminate inter-modal differences and enhance robustness against noisy input features. Finally, to better match textual and interaction-based intents, we employ momentum distillation to perform teacher-student learning on fused intent representations. Empirical evaluations on three datasets show that our IRLLRec framework outperforms baselines. The implementation is available at https://github.com/wangyu0627/IRLLRec.

**Link**: [arxiv](http://arxiv.org/abs/2502.03307v1),  [pdf](http://arxiv.org/pdf/2502.03307v1)

**Tags**: cs.IR 



### Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient   Zeroth-order LLM Fine-tuning
**Authors**: Qitao Tan, Jun Liu, Zheng Zhan, Caiwei Ding, Yanzhi Wang, Jin Lu, Geng Yuan

**Updated**: 2025-02-05T16:03:17Z

**Summary**: Large language models (LLMs) excel across various tasks, but standard first-order (FO) fine-tuning demands considerable memory, significantly limiting real-world deployment. Recently, zeroth-order (ZO) optimization stood out as a promising memory-efficient training paradigm, avoiding backward passes and relying solely on forward passes for gradient estimation, making it attractive for resource-constrained scenarios. However, ZO method lags far behind FO method in both convergence speed and accuracy. To bridge the gap, we introduce a novel layer-wise divergence analysis that uncovers the distinct update pattern of FO and ZO optimization. Aiming to resemble the learning capacity of FO method from the findings, we propose \textbf{Di}vergence-driven \textbf{Z}eroth-\textbf{O}rder (\textbf{DiZO}) optimization. DiZO conducts divergence-driven layer adaptation by incorporating projections to ZO updates, generating diverse-magnitude updates precisely scaled to layer-wise individual optimization needs. Our results demonstrate that DiZO significantly reduces the needed iterations for convergence without sacrificing throughput, cutting training GPU hours by up to 48\% on various datasets. Moreover, DiZO consistently outperforms the representative ZO baselines in fine-tuning RoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some cases, even surpasses memory-intensive FO fine-tuning.

**Link**: [arxiv](http://arxiv.org/abs/2502.03304v1),  [pdf](http://arxiv.org/pdf/2502.03304v1)

**Tags**: cs.LG cs.AI cs.CL 



### ScNeuGM: Scalable Neural Graph Modeling for Coloring-Based Contention   and Interference Management in Wi-Fi 7
**Authors**: Zhouyou Gu, Jihong Park, Jinho Choi

**Updated**: 2025-02-05T15:58:44Z

**Summary**: Carrier-sense multiple access with collision avoidance in Wi-Fi often leads to contention and interference, thereby increasing packet losses. These challenges have traditionally been modeled as a graph, with stations (STAs) represented as vertices and contention or interference as edges. Graph coloring assigns orthogonal transmission slots to STAs, managing contention and interference, e.g., using the restricted target wake time (RTWT) mechanism introduced in Wi-Fi 7 standards. However, legacy graph models lack flexibility in optimizing these assignments, often failing to minimize slot usage while maintaining reliable transmissions. To address this issue, we propose ScNeuGM, a neural graph modeling (NGM) framework that flexibly trains a neural network (NN) to construct optimal graph models whose coloring corresponds to optimal slot assignments. ScNeuGM is highly scalable to large Wi-Fi networks with massive STA pairs: 1) it utilizes an evolution strategy (ES) to directly optimize the NN parameters based on one network-wise reward signal, avoiding exhaustive edge-wise feedback estimations in all STA pairs; 2) ScNeuGM also leverages a deep hashing function (DHF) to group contending or interfering STA pairs and restricts NGM NN training and inference to pairs within these groups, significantly reducing complexity. Simulations show that the ES-trained NN in ScNeuGM returns near-optimal graphs 4-10 times more often than algorithms requiring edge-wise feedback and reduces 25\% slots than legacy graph constructions. Furthermore, the DHF in ScNeuGM reduces the training and the inference time of NGM by 4 and 8 times, respectively, and the online slot assignment time by 3 times in large networks, and up to 30\% fewer packet losses in dynamic scenarios due to the timely assignments.

**Link**: [arxiv](http://arxiv.org/abs/2502.03300v1),  [pdf](http://arxiv.org/pdf/2502.03300v1)

**Tags**: eess.SP 



### MeDiSumQA: Patient-Oriented Question-Answer Generation from Discharge   Letters
**Authors**: Amin Dada, Osman Alperen Koras, Marie Bauer, Amanda Butler, Kaleb E. Smith, Jens Kleesiek, Julian Friedrich

**Updated**: 2025-02-05T15:56:37Z

**Summary**: While increasing patients' access to medical documents improves medical care, this benefit is limited by varying health literacy levels and complex medical terminology. Large language models (LLMs) offer solutions by simplifying medical information. However, evaluating LLMs for safe and patient-friendly text generation is difficult due to the lack of standardized evaluation resources. To fill this gap, we developed MeDiSumQA. MeDiSumQA is a dataset created from MIMIC-IV discharge summaries through an automated pipeline combining LLM-based question-answer generation with manual quality checks. We use this dataset to evaluate various LLMs on patient-oriented question-answering. Our findings reveal that general-purpose LLMs frequently surpass biomedical-adapted models, while automated metrics correlate with human judgment. By releasing MeDiSumQA on PhysioNet, we aim to advance the development of LLMs to enhance patient understanding and ultimately improve care outcomes.

**Link**: [arxiv](http://arxiv.org/abs/2502.03298v1),  [pdf](http://arxiv.org/pdf/2502.03298v1)

**Tags**: cs.CL cs.AI cs.LG 



### SymAgent: A Neural-Symbolic Self-Learning Agent Framework for Complex   Reasoning over Knowledge Graphs
**Authors**: Ben Liu, Jihai Zhang, Fangquan Lin, Cheng Yang, Min Peng, Wotao Yin

**Updated**: 2025-02-05T15:37:05Z

**Summary**: Recent advancements have highlighted that Large Language Models (LLMs) are prone to hallucinations when solving complex reasoning problems, leading to erroneous results. To tackle this issue, researchers incorporate Knowledge Graphs (KGs) to improve the reasoning ability of LLMs. However, existing methods face two limitations: 1) they typically assume that all answers to the questions are contained in KGs, neglecting the incompleteness issue of KGs, and 2) they treat the KG as a static repository and overlook the implicit logical reasoning structures inherent in KGs. In this paper, we introduce SymAgent, an innovative neural-symbolic agent framework that achieves collaborative augmentation between KGs and LLMs. We conceptualize KGs as dynamic environments and transform complex reasoning tasks into a multi-step interactive process, enabling KGs to participate deeply in the reasoning process. SymAgent consists of two modules: Agent-Planner and Agent-Executor. The Agent-Planner leverages LLM's inductive reasoning capability to extract symbolic rules from KGs, guiding efficient question decomposition. The Agent-Executor autonomously invokes predefined action tools to integrate information from KGs and external documents, addressing the issues of KG incompleteness. Furthermore, we design a self-learning framework comprising online exploration and offline iterative policy updating phases, enabling the agent to automatically synthesize reasoning trajectories and improve performance. Experimental results demonstrate that SymAgent with weak LLM backbones (i.e., 7B series) yields better or comparable performance compared to various strong baselines. Further analysis reveals that our agent can identify missing triples, facilitating automatic KG updates.

**Link**: [arxiv](http://arxiv.org/abs/2502.03283v1),  [pdf](http://arxiv.org/pdf/2502.03283v1)

**Tags**: cs.AI cs.CL cs.LG 



### Posterior SBC: Simulation-Based Calibration Checking Conditional on Data
**Authors**: Teemu Säilynoja, Marvin Schmitt, Paul Bürkner, Aki Vehtari

**Updated**: 2025-02-05T15:35:06Z

**Summary**: Simulation-based calibration checking (SBC) refers to the validation of an inference algorithm and model implementation through repeated inference on data simulated from a generative model. In the original and commonly used approach, the generative model uses parameters drawn from the prior, and thus the approach is testing whether the inference works for simulated data generated with parameter values plausible under that prior. This approach is natural and desirable when we want to test whether the inference works for a wide range of datasets we might observe. However, after observing data, we are interested in answering whether the inference works conditional on that particular data. In this paper, we propose posterior SBC and demonstrate how it can be used to validate the inference conditionally on observed data. We illustrate the utility of posterior SBC in three case studies: (1) A simple multilevel model; (2) a model that is governed by differential equations; and (3) a joint integrative neuroscience model which is approximated via amortized Bayesian inference with neural networks.

**Link**: [arxiv](http://arxiv.org/abs/2502.03279v1),  [pdf](http://arxiv.org/pdf/2502.03279v1)

**Tags**: stat.ME stat.CO stat.ML 



### Token Assorted: Mixing Latent and Text Tokens for Improved Language   Model Reasoning
**Authors**: DiJia Su, Hanlin Zhu, Yingchen Xu, Jiantao Jiao, Yuandong Tian, Qinqing Zheng

**Updated**: 2025-02-05T15:33:00Z

**Summary**: Large Language Models (LLMs) excel at reasoning and planning when trained on chainof-thought (CoT) data, where the step-by-step thought process is explicitly outlined by text tokens. However, this results in lengthy inputs where many words support textual coherence rather than core reasoning information, and processing these inputs consumes substantial computation resources. In this work, we propose a hybrid representation of the reasoning process, where we partially abstract away the initial reasoning steps using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces. We explore the use of latent trace abstractions in two scenarios: 1) training the model from scratch for the Keys-Finding Maze problem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary including unseen latent tokens, for both logical and mathematical reasoning problems. To facilitate effective learning, we introduce a simple training procedure that randomly mixes latent and text tokens, which enables fast adaptation to new latent tokens. Our approach consistently outperforms the baselines methods in various benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2502.03275v1),  [pdf](http://arxiv.org/pdf/2502.03275v1)

**Tags**: cs.CL cs.AI cs.LG cs.LO 



### Kolmogorov-Arnold Networks for Time Series Granger Causality Inference
**Authors**: Meiliang Liu, Yunfang Xu, Zijin Li, Zhengye Si, Xiaoxiao Yang, Xinyue Yang, Zhiwen Zhao

**Updated**: 2025-02-05T15:26:49Z

**Summary**: We propose the Granger causality inference Kolmogorov-Arnold Networks (KANGCI), a novel architecture that extends the recently proposed Kolmogorov-Arnold Networks (KAN) to the domain of causal inference. By extracting base weights from KAN layers and incorporating the sparsity-inducing penalty and ridge regularization, KANGCI effectively infers the Granger causality from time series. Additionally, we propose an algorithm based on time-reversed Granger causality that automatically selects causal relationships with better inference performance from the original or time-reversed time series or integrates the results to mitigate spurious connectivities. Comprehensive experiments conducted on Lorenz-96, Gene regulatory networks, fMRI BOLD signals, VAR, and real-world EEG datasets demonstrate that the proposed model achieves competitive performance to state-of-the-art methods in inferring Granger causality from nonlinear, high-dimensional, and limited-sample time series.

**Link**: [arxiv](http://arxiv.org/abs/2501.08958v2),  [pdf](http://arxiv.org/pdf/2501.08958v2)

**Tags**: cs.LG cs.AI 



### The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically   Justify Replacing Human Annotators with LLMs
**Authors**: Nitay Calderon, Roi Reichart, Rotem Dror

**Updated**: 2025-02-05T15:24:26Z

**Summary**: The "LLM-as-a-judge" paradigm employs Large Language Models (LLMs) as annotators and evaluators in tasks traditionally performed by humans. LLM annotations are widely used, not only in NLP research but also in fields like medicine, psychology, and social science. Despite their role in shaping study results and insights, there is no standard or rigorous procedure to determine whether LLMs can replace human annotators. In this paper, we propose a novel statistical procedure -- the Alternative Annotator Test (alt-test) -- that requires only a modest subset of annotated examples to justify using LLM annotations. Additionally, we introduce a versatile and interpretable measure for comparing LLM judges. To demonstrate our procedure, we curated a diverse collection of ten datasets, consisting of language and vision-language tasks, and conducted experiments with six LLMs and four prompting techniques. Our results show that LLMs can sometimes replace humans with closed-source LLMs (such as GPT-4o), outperforming open-source LLMs, and that prompting techniques yield judges of varying quality. We hope this study encourages more rigorous and reliable practices.

**Link**: [arxiv](http://arxiv.org/abs/2501.10970v2),  [pdf](http://arxiv.org/pdf/2501.10970v2)

**Tags**: cs.CL cs.AI cs.HC 



### CARROT: A Cost Aware Rate Optimal Router
**Authors**: Seamus Somerstep, Felipe Maia Polo, Allysson Flavio Melo de Oliveira, Prattyush Mangal, Mírian Silva, Onkar Bhardwaj, Mikhail Yurochkin, Subha Maity

**Updated**: 2025-02-05T15:17:25Z

**Summary**: With the rapid growth in the number of Large Language Models (LLMs), there has been a recent interest in LLM routing, or directing queries to the cheapest LLM that can deliver a suitable response. Following this line of work, we introduce CARROT, a Cost AwaRe Rate Optimal rouTer that can select models based on any desired trade-off between performance and cost. Given a query, CARROT selects a model based on estimates of models' cost and performance. Its simplicity lends CARROT computational efficiency, while our theoretical analysis demonstrates minimax rate-optimality in its routing performance. Alongside CARROT, we also introduce the Smart Price-aware Routing (SPROUT) dataset to facilitate routing on a wide spectrum of queries with the latest state-of-the-art LLMs. Using SPROUT and prior benchmarks such as Routerbench and open-LLM-leaderboard-v2 we empirically validate CARROT's performance against several alternative routers.

**Link**: [arxiv](http://arxiv.org/abs/2502.03261v1),  [pdf](http://arxiv.org/pdf/2502.03261v1)

**Tags**: stat.ML cs.LG cs.NI math.ST stat.TH 



### Should Audio Front-ends be Adaptive? Comparing Learnable and Adaptive   Front-ends
**Authors**: Qiquan Zhang, Buddhi Wickramasinghe, Eliathamby Ambikairajah, Vidhyasaharan Sethu, Haizhou Li

**Updated**: 2025-02-05T15:16:52Z

**Summary**: Hand-crafted features, such as Mel-filterbanks, have traditionally been the choice for many audio processing applications. Recently, there has been a growing interest in learnable front-ends that extract representations directly from the raw audio waveform. \textcolor{black}{However, both hand-crafted filterbanks and current learnable front-ends lead to fixed computation graphs at inference time, failing to dynamically adapt to varying acoustic environments, a key feature of human auditory systems.} To this end, we explore the question of whether audio front-ends should be adaptive by comparing the Ada-FE front-end (a recently developed adaptive front-end that employs a neural adaptive feedback controller to dynamically adjust the Q-factors of its spectral decomposition filters) to established learnable front-ends. Specifically, we systematically investigate learnable front-ends and Ada-FE across two commonly used back-end backbones and a wide range of audio benchmarks including speech, sound event, and music. The comprehensive results show that our Ada-FE outperforms advanced learnable front-ends, and more importantly, it exhibits impressive stability or robustness on test samples over various training epochs.

**Link**: [arxiv](http://arxiv.org/abs/2502.03260v1),  [pdf](http://arxiv.org/pdf/2502.03260v1)

**Tags**: eess.AS cs.SD 



### Learning Ordinality in Semantic Segmentation
**Authors**: Ricardo P. M. Cruz, Rafael Cristino, Jaime S. Cardoso

**Updated**: 2025-02-05T15:16:08Z

**Summary**: Semantic segmentation consists of predicting a semantic label for each image pixel. While existing deep learning approaches achieve high accuracy, they often overlook the ordinal relationships between classes, which can provide critical domain knowledge (e.g., the pupil lies within the iris, and lane markings are part of the road). This paper introduces novel methods for spatial ordinal segmentation that explicitly incorporate these inter-class dependencies. By treating each pixel as part of a structured image space rather than as an independent observation, we propose two regularization terms and a new metric to enforce ordinal consistency between neighboring pixels. Two loss regularization terms and one metric are proposed for structural ordinal segmentation, which penalizes predictions of non-ordinal adjacent classes. Five biomedical datasets and multiple configurations of autonomous driving datasets demonstrate the efficacy of the proposed methods. Our approach achieves improvements in ordinal metrics and enhances generalization, with up to a 15.7% relative increase in the Dice coefficient. Importantly, these benefits come without additional inference time costs. This work highlights the significance of spatial ordinal relationships in semantic segmentation and provides a foundation for further exploration in structured image representations.

**Link**: [arxiv](http://arxiv.org/abs/2407.20959v2),  [pdf](http://arxiv.org/pdf/2407.20959v2)

**Tags**: cs.CV cs.LG 



### PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs
**Authors**: Max Zimmer, Megi Andoni, Christoph Spiegel, Sebastian Pokutta

**Updated**: 2025-02-05T15:10:23Z

**Summary**: Neural Networks can be effectively compressed through pruning, significantly reducing storage and compute demands while maintaining predictive performance. Simple yet effective methods like magnitude pruning remove less important parameters and typically require a costly retraining procedure to restore performance. However, with the rise of LLMs, full retraining has become infeasible due to memory and compute constraints. This study challenges the practice of retraining all parameters by showing that updating a small subset of highly expressive parameters can suffice to recover or even enhance performance after pruning. Surprisingly, retraining just 0.01%-0.05% of the parameters in GPT-architectures can match the performance of full retraining across various sparsity levels, significantly reducing compute and memory requirements, and enabling retraining of models with up to 30 billion parameters on a single GPU in minutes. To bridge the gap to full retraining in the high sparsity regime, we introduce two novel LoRA variants that, unlike standard LoRA, allow merging adapters back without compromising sparsity. Going a step further, we show that these methods can be applied for memory-efficient layer-wise reconstruction, significantly enhancing state-of-the-art retraining-free methods like Wanda (Sun et al., 2023) and SparseGPT (Frantar & Alistarh, 2023). Our findings present a promising alternative to avoiding retraining.

**Link**: [arxiv](http://arxiv.org/abs/2312.15230v3),  [pdf](http://arxiv.org/pdf/2312.15230v3)

**Tags**: cs.LG cs.AI 



### How do Humans and Language Models Reason About Creativity? A Comparative   Analysis
**Authors**: Antonio Laverghetta Jr., Tuhin Chakrabarty, Tom Hope, Jimmy Pronchick, Krupa Bhawsar, Roger E. Beaty

**Updated**: 2025-02-05T15:08:43Z

**Summary**: Creativity assessment in science and engineering is increasingly based on both human and AI judgment, but the cognitive processes and biases behind these evaluations remain poorly understood. We conducted two experiments examining how including example solutions with ratings impact creativity evaluation, using a finegrained annotation protocol where raters were tasked with explaining their originality scores and rating for the facets of remoteness (whether the response is "far" from everyday ideas), uncommonness (whether the response is rare), and cleverness. In Study 1, we analyzed creativity ratings from 72 experts with formal science or engineering training, comparing those who received example solutions with ratings (example) to those who did not (no example). Computational text analysis revealed that, compared to experts with examples, no-example experts used more comparative language (e.g., "better/worse") and emphasized solution uncommonness, suggesting they may have relied more on memory retrieval for comparisons. In Study 2, parallel analyses with state-of-the-art LLMs revealed that models prioritized uncommonness and remoteness of ideas when rating originality, suggesting an evaluative process rooted around the semantic similarity of ideas. In the example condition, while LLM accuracy in predicting the true originality scores improved, the correlations of remoteness, uncommonness, and cleverness with originality also increased substantially - to upwards of 0.99 - suggesting a homogenization in the LLMs evaluation of the individual facets. These findings highlight important implications for how humans and AI reason about creativity and suggest diverging preferences for what different populations prioritize when rating.

**Link**: [arxiv](http://arxiv.org/abs/2502.03253v1),  [pdf](http://arxiv.org/pdf/2502.03253v1)

**Tags**: cs.CL 



### Exploring the Security Threats of Knowledge Base Poisoning in   Retrieval-Augmented Code Generation
**Authors**: Bo Lin, Shangwen Wang, Liqian Chen, Xiaoguang Mao

**Updated**: 2025-02-05T14:49:12Z

**Summary**: The integration of Large Language Models (LLMs) into software development has revolutionized the field, particularly through the use of Retrieval-Augmented Code Generation (RACG) systems that enhance code generation with information from external knowledge bases. However, the security implications of RACG systems, particularly the risks posed by vulnerable code examples in the knowledge base, remain largely unexplored. This risk is particularly concerning given that public code repositories, which often serve as the sources for knowledge base collection in RACG systems, are usually accessible to anyone in the community. Malicious attackers can exploit this accessibility to inject vulnerable code into the knowledge base, making it toxic. Once these poisoned samples are retrieved and incorporated into the generated code, they can propagate security vulnerabilities into the final product. This paper presents the first comprehensive study on the security risks associated with RACG systems, focusing on how vulnerable code in the knowledge base compromises the security of generated code. We investigate the LLM-generated code security across different settings through extensive experiments using four major LLMs, two retrievers, and two poisoning scenarios. Our findings highlight the significant threat of knowledge base poisoning, where even a single poisoned code example can compromise up to 48% of generated code. Our findings provide crucial insights into vulnerability introduction in RACG systems and offer practical mitigation recommendations, thereby helping improve the security of LLM-generated code in future works.

**Link**: [arxiv](http://arxiv.org/abs/2502.03233v1),  [pdf](http://arxiv.org/pdf/2502.03233v1)

**Tags**: cs.CR cs.SE 



### PSC: Posterior Sampling-Based Compression
**Authors**: Noam Elata, Tomer Michaeli, Michael Elad

**Updated**: 2025-02-05T14:33:13Z

**Summary**: Diffusion models have transformed the landscape of image generation and now show remarkable potential for image compression. Most of the recent diffusion-based compression methods require training and are tailored for a specific bit-rate. In this work, we propose Posterior Sampling-based Compression (PSC) - a zero-shot compression method that leverages a pre-trained diffusion model as its sole neural network component, thus enabling the use of diverse, publicly available models without additional training. Our approach is inspired by transform coding methods, which encode the image in some pre-chosen transform domain. However, PSC constructs a transform that is adaptive to the image. This is done by employing a zero-shot diffusion-based posterior sampler so as to progressively construct the rows of the transform matrix. Each new chunk of rows is chosen to reduce the uncertainty about the image given the quantized measurements collected thus far. Importantly, the same adaptive scheme can be replicated at the decoder, thus avoiding the need to encode the transform itself. We demonstrate that even with basic quantization and entropy coding, PSC's performance is comparable to established training-based methods in terms of rate, distortion, and perceptual quality. This is while providing greater flexibility, allowing to choose at inference time any desired rate or distortion.

**Link**: [arxiv](http://arxiv.org/abs/2407.09896v3),  [pdf](http://arxiv.org/pdf/2407.09896v3)

**Tags**: cs.CV eess.IV 



### Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large   Language Models
**Authors**: Jialiang Wu, Yi Shen, Sijia Liu, Yi Tang, Sen Song, Xiaoyi Wang, Longjun Cai

**Updated**: 2025-02-05T14:19:52Z

**Summary**: Despite their impressive capacities, Large language models (LLMs) often struggle with the hallucination issue of generating inaccurate or fabricated content even when they possess correct knowledge. In this paper, we extend the exploration of the correlation between hidden-state prediction changes and output factuality into a deeper, token-wise level. Based on the insights , we propose cross-layer Entropy eNhanced Decoding (END), a decoding method that mitigates hallucinations without requiring extra training. END leverages inner probability changes across layers to individually quantify the factual knowledge required for each candidate token, and adjusts the final predicting distribution to prioritize tokens with higher factuality. Experiments on both hallucination and QA benchmarks demonstrate that END significantly enhances the truthfulness and informativeness of generated content while maintaining robust QA accuracy. Moreover, our work provides a deeper perspective on understanding the correlations between inherent knowledge and output factuality.

**Link**: [arxiv](http://arxiv.org/abs/2502.03199v1),  [pdf](http://arxiv.org/pdf/2502.03199v1)

**Tags**: cs.CL cs.AI 



### Applications of emulation and Bayesian methods in heavy-ion physics
**Authors**: Jean-François Paquet

**Updated**: 2025-02-05T14:09:33Z

**Summary**: Heavy-ion collisions provide a window into the properties of many-body systems of deconfined quarks and gluons. Understanding the collective properties of quarks and gluons is possible by comparing models of heavy-ion collisions to measurements of the distribution of particles produced at the end of the collisions. These model-to-data comparisons are extremely challenging, however, because of the complexity of the models, the large amount of experimental data, and their uncertainties. Bayesian inference provides a rigorous statistical framework to constrain the properties of nuclear matter by systematically comparing models and measurements.   This review covers model emulation and Bayesian methods as applied to model-to-data comparisons in heavy-ion collisions. Replacing the model outputs (observables) with Gaussian process emulators is key to the Bayesian approach currently used in the field, and both current uses of emulators and related recent developments are reviewed. The general principles of Bayesian inference are then discussed along with other Bayesian methods, followed by a systematic comparison of seven recent Bayesian analyses that studied quark-gluon plasma properties, such as the shear and bulk viscosities. The latter comparison is used to illustrate sources of differences in analyses, and what it can teach us for future studies.

**Link**: [arxiv](http://arxiv.org/abs/2310.17618v2),  [pdf](http://arxiv.org/pdf/2310.17618v2)

**Tags**: nucl-th 



### LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence
**Authors**: Zhuoling Li, Xiaogang Xu, Zhenhua Xu, SerNam Lim, Hengshuang Zhao

**Updated**: 2025-02-05T14:06:21Z

**Summary**: Recent embodied agents are primarily built based on reinforcement learning (RL) or large language models (LLMs). Among them, RL agents are efficient for deployment but only perform very few tasks. By contrast, giant LLM agents (often more than 1000B parameters) present strong generalization while demanding enormous computing resources. In this work, we combine their advantages while avoiding the drawbacks by conducting the proposed referee RL on our developed large auto-regressive model (LARM). Specifically, LARM is built upon a lightweight LLM (fewer than 5B parameters) and directly outputs the next action to execute rather than text. We mathematically reveal that classic RL feedbacks vanish in long-horizon embodied exploration and introduce a giant LLM based referee to handle this reward vanishment during training LARM. In this way, LARM learns to complete diverse open-world tasks without human intervention. Especially, LARM successfully harvests enchanted diamond equipment in Minecraft, which demands significantly longer decision-making chains than the highest achievements of prior best methods.

**Link**: [arxiv](http://arxiv.org/abs/2405.17424v2),  [pdf](http://arxiv.org/pdf/2405.17424v2)

**Tags**: cs.CV 



### SuperCorrect: Supervising and Correcting Language Models with   Error-Driven Insights
**Authors**: Ling Yang, Zhaochen Yu, Tianjun Zhang, Minkai Xu, Joseph E. Gonzalez, Bin Cui, Shuicheng Yan

**Updated**: 2025-02-05T13:47:11Z

**Summary**: Large language models (LLMs) like GPT-4, PaLM, and LLaMA have shown significant improvements in various reasoning tasks. However, smaller models such as Llama-3-8B and DeepSeekMath-Base still struggle with complex mathematical reasoning because they fail to effectively identify and correct reasoning errors. Recent reflection-based methods aim to address these issues by enabling self-reflection and self-correction, but they still face challenges in independently detecting errors in their reasoning steps. To overcome these limitations, we propose SuperCorrect, a novel two-stage framework that uses a large teacher model to supervise and correct both the reasoning and reflection processes of a smaller student model. In the first stage, we extract hierarchical high-level and detailed thought templates from the teacher model to guide the student model in eliciting more fine-grained reasoning thoughts. In the second stage, we introduce cross-model collaborative direct preference optimization (DPO) to enhance the self-correction abilities of the student model by following the teacher's correction traces during training. This cross-model DPO approach teaches the student model to effectively locate and resolve erroneous thoughts with error-driven insights from the teacher model, breaking the bottleneck of its thoughts and acquiring new skills and knowledge to tackle challenging problems. Extensive experiments consistently demonstrate our superiority over previous methods. Notably, our SuperCorrect-7B model significantly surpasses powerful DeepSeekMath-7B by 7.8%/5.3% and Qwen2.5-Math-7B by 15.1%/6.3% on MATH/GSM8K benchmarks, achieving new SOTA performance among all 7B models. Code: https://github.com/YangLing0818/SuperCorrect-llm

**Link**: [arxiv](http://arxiv.org/abs/2410.09008v2),  [pdf](http://arxiv.org/pdf/2410.09008v2)

**Tags**: cs.CL 



### Thermal spin wave noise as a probe for the Dzyaloshinkii-Moriya   interaction
**Authors**: Aurore Finco, Pawan Kumar, Van Tuong Pham, Joseba Urrestarazu-Larrañaga, Rodrigo Guedas Garcia, Maxime Rollo, Olivier Boulle, Joo-Von Kim, Vincent Jacques

**Updated**: 2025-02-05T13:39:25Z

**Summary**: Interfacial Dzyaloshinkii-Moriya interaction (DMI) is a key ingredient in the stabilization of chiral magnetic states in thin films. Its sign and strength often determine crucial properties of magnetic objects, like their topology or how they can be manipulated with currents. A few experimental techniques are currently available to measure DMI quantitatively, based on the study of domain walls, spin waves, or spin-orbit torques. In this work, we propose a qualitative variant of spin wave methods. We rely on magnetic noise from confined thermal spin waves in domain walls and skyrmions in perpendicularly magnetized thin films, which we probe with scanning NV center relaxometry. We show both numerically and experimentally that the sign of the DMI can be inferred from the amplitude of the detected noise, which is affected by the non-reciprocity in the spin wave dispersion. Furthermore, we also demonstrate that the noise distribution around the contour of magnetic skyrmions reveals their N\'eel/Bloch nature, giving therefore also insight into the strength of DMI involved in their stabilization.

**Link**: [arxiv](http://arxiv.org/abs/2502.03166v1),  [pdf](http://arxiv.org/pdf/2502.03166v1)

**Tags**: cond-mat.mes-hall 



### PICBench: Benchmarking LLMs for Photonic Integrated Circuits Design
**Authors**: Yuchao Wu, Xiaofei Yu, Hao Chen, Yang Luo, Yeyu Tong, Yuzhe Ma

**Updated**: 2025-02-05T13:32:29Z

**Summary**: While large language models (LLMs) have shown remarkable potential in automating various tasks in digital chip design, the field of Photonic Integrated Circuits (PICs)-a promising solution to advanced chip designs-remains relatively unexplored in this context. The design of PICs is time-consuming and prone to errors due to the extensive and repetitive nature of code involved in photonic chip design. In this paper, we introduce PICBench, the first benchmarking and evaluation framework specifically designed to automate PIC design generation using LLMs, where the generated output takes the form of a netlist. Our benchmark consists of dozens of meticulously crafted PIC design problems, spanning from fundamental device designs to more complex circuit-level designs. It automatically evaluates both the syntax and functionality of generated PIC designs by comparing simulation outputs with expert-written solutions, leveraging an open-source simulator. We evaluate a range of existing LLMs, while also conducting comparative tests on various prompt engineering techniques to enhance LLM performance in automated PIC design. The results reveal the challenges and potential of LLMs in the PIC design domain, offering insights into the key areas that require further research and development to optimize automation in this field. Our benchmark and evaluation code is available at https://github.com/PICDA/PICBench.

**Link**: [arxiv](http://arxiv.org/abs/2502.03159v1),  [pdf](http://arxiv.org/pdf/2502.03159v1)

**Tags**: cs.LG 



### Strategizing with AI: Insights from a Beauty Contest Experiment
**Authors**: Iuliia Alekseenko, Dmitry Dagaev, Sofia Paklina, Petr Parshakov

**Updated**: 2025-02-05T13:31:38Z

**Summary**: A Keynesian beauty contest is a wide class of games of guessing the most popular strategy among other players. In particular, guessing a fraction of a mean of numbers chosen by all players is a classic behavioral experiment designed to test iterative reasoning patterns among various groups of people. The previous literature reveals that the level of sophistication of the opponents is an important factor affecting the outcome of the game. Smarter decision makers choose strategies that are closer to theoretical Nash equilibrium and demonstrate faster convergence to equilibrium in iterated contests with information revelation. We replicate a series of classic experiments by running virtual experiments with modern large language models (LLMs) who play against various groups of virtual players. We test how advanced the LLMs' behavior is compared to the behavior of human players. We show that LLMs typically take into account the opponents' level of sophistication and adapt by changing the strategy. In various settings, most LLMs (with the exception of Llama) are more sophisticated and play lower numbers compared to human players. Our results suggest that LLMs (except Llama) are rather successful in identifying the underlying strategic environment and adopting the strategies to the changing set of parameters of the game in the same way that human players do. All LLMs still fail to play dominant strategies in a two-player game. Our results contribute to the discussion on the accuracy of modeling human economic agents by artificial intelligence.

**Link**: [arxiv](http://arxiv.org/abs/2502.03158v1),  [pdf](http://arxiv.org/pdf/2502.03158v1)

**Tags**: econ.GN q-fin.EC 



### AlphaAdam:Asynchronous Masked Optimization with Dynamic Alpha for   Selective Updates
**Authors**: Da Chang, Yu Li, Ganzhao Yuan

**Updated**: 2025-02-05T13:23:18Z

**Summary**: In the training of large language models (LLMs), updating parameters more efficiently and stably has always been an important challenge. To achieve efficient parameter updates, existing methods usually achieve performance comparable to full parameter updates through methods such as low-dimensional decomposition or layer-wise selective updates. In this work, we propose AlphaAdam, an optimization framework for LLM from the perspective of intra-layer parameter updates. By decoupling parameter updates and dynamically adjusting their strength, AlphaAdam accelerates convergence and improves training stability. We construct parameter masks based on the consistency of historical momentum and gradient direction and combine them with an adaptive mask strength strategy to ensure efficient optimization and theoretical convergence guarantees, which is also applicable to most momentum-based optimizers. Extensive experiments show that AlphaAdam outperforms state-of-the-art methods such as AdamW in terms of convergence speed and computational efficiency across tasks, including GPT-2 pre-trained and fine-tuned RoBERTa and Llama-7B. Our AlphaAdam implements an optimizer enhancement framework for LLMs through intra-layer asynchronous masked adaptive updates. Our code is available in this https://github.com/MaeChd/AlphaAdam.

**Link**: [arxiv](http://arxiv.org/abs/2501.18094v2),  [pdf](http://arxiv.org/pdf/2501.18094v2)

**Tags**: cs.LG 



### ImgTrojan: Jailbreaking Vision-Language Models with ONE Image
**Authors**: Xijia Tao, Shuai Zhong, Lei Li, Qi Liu, Lingpeng Kong

**Updated**: 2025-02-05T13:20:24Z

**Summary**: There has been an increasing interest in the alignment of large language models (LLMs) with human values. However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain relatively underexplored. In this paper, we propose a novel jailbreaking attack against VLMs, aiming to bypass their safety barrier when a user inputs harmful instructions. A scenario where our poisoned (image, text) data pairs are included in the training data is assumed. By replacing the original textual captions with malicious jailbreak prompts, our method can perform jailbreak attacks with the poisoned images. Moreover, we analyze the effect of poison ratios and positions of trainable parameters on our attack's success rate. For evaluation, we design two metrics to quantify the success rate and the stealthiness of our attack. Together with a list of curated harmful instructions, a benchmark for measuring attack efficacy is provided. We demonstrate the efficacy of our attack by comparing it with baseline methods.

**Link**: [arxiv](http://arxiv.org/abs/2403.02910v3),  [pdf](http://arxiv.org/pdf/2403.02910v3)

**Tags**: cs.CV cs.AI 



### TwinMarket: A Scalable Behavioral and Social Simulation for Financial   Markets
**Authors**: Yuzhe Yang, Yifei Zhang, Minghao Wu, Kaidi Zhang, Yunmiao Zhang, Honghai Yu, Yan Hu, Benyou Wang

**Updated**: 2025-02-05T13:18:13Z

**Summary**: The study of social emergence has long been a central focus in social science. Traditional modeling approaches, such as rule-based Agent-Based Models (ABMs), struggle to capture the diversity and complexity of human behavior, particularly the irrational factors emphasized in behavioral economics. Recently, large language model (LLM) agents have gained traction as simulation tools for modeling human behavior in social science and role-playing applications. Studies suggest that LLMs can account for cognitive biases, emotional fluctuations, and other non-rational influences, enabling more realistic simulations of socio-economic dynamics. In this work, we introduce TwinMarket, a novel multi-agent framework that leverages LLMs to simulate socio-economic systems. Specifically, we examine how individual behaviors, through interactions and feedback mechanisms, give rise to collective dynamics and emergent phenomena. Through experiments in a simulated stock market environment, we demonstrate how individual actions can trigger group behaviors, leading to emergent outcomes such as financial bubbles and recessions. Our approach provides valuable insights into the complex interplay between individual decision-making and collective socio-economic patterns.

**Link**: [arxiv](http://arxiv.org/abs/2502.01506v2),  [pdf](http://arxiv.org/pdf/2502.01506v2)

**Tags**: cs.CE cs.CY 



### Scalable In-Context Learning on Tabular Data via Retrieval-Augmented   Large Language Models
**Authors**: Xumeng Wen, Shun Zheng, Zhen Xu, Yiming Sun, Jiang Bian

**Updated**: 2025-02-05T13:16:41Z

**Summary**: Recent studies have shown that large language models (LLMs), when customized with post-training on tabular data, can acquire general tabular in-context learning (TabICL) capabilities. These models are able to transfer effectively across diverse data schemas and different task domains. However, existing LLM-based TabICL approaches are constrained to few-shot scenarios due to the sequence length limitations of LLMs, as tabular instances represented in plain text consume substantial tokens. To address this limitation and enable scalable TabICL for any data size, we propose retrieval-augmented LLMs tailored to tabular data. Our approach incorporates a customized retrieval module, combined with retrieval-guided instruction-tuning for LLMs. This enables LLMs to effectively leverage larger datasets, achieving significantly improved performance across 69 widely recognized datasets and demonstrating promising scaling behavior. Extensive comparisons with state-of-the-art tabular models reveal that, while LLM-based TabICL still lags behind well-tuned numeric models in overall performance, it uncovers powerful algorithms under limited contexts, enhances ensemble diversity, and excels on specific datasets. These unique properties underscore the potential of language as a universal and accessible interface for scalable tabular data learning.

**Link**: [arxiv](http://arxiv.org/abs/2502.03147v1),  [pdf](http://arxiv.org/pdf/2502.03147v1)

**Tags**: cs.CL cs.AI 



### Ensemble noise properties of the European Pulsar Timing Array
**Authors**: Boris Goncharov, Shubhit Sardana

**Updated**: 2025-02-05T13:06:07Z

**Summary**: The null hypothesis in Pulsar Timing Array (PTA) analyses includes assumptions about ensemble properties of pulsar time-correlated noise. These properties are encoded in prior probabilities for the amplitude and the spectral index of the power-law power spectral density of temporal correlations of the noise. Because multiple realizations of time-correlated noise processes are found in pulsars, these ensemble noise properties could and should be modelled in the full-PTA observations by parameterising the respective prior distributions using the so-called hyperparameters. This approach is known as the hierarchical Bayesian inference. In this work, we introduce a new procedure for numerical marginalisation over hyperparameters. The procedure may be used in searches for nanohertz gravitational waves and other PTA analyses to resolve prior misspecification at negligible computational cost. Furthermore, we infer the distribution of amplitudes and spectral indices of the power spectral density of spin noise and dispersion measure variation noise based on the observation of 25 millisecond pulsars by the European Pulsar Timing Array (EPTA). Our results may be used for the simulation of realistic noise in PTAs.

**Link**: [arxiv](http://arxiv.org/abs/2409.03661v3),  [pdf](http://arxiv.org/pdf/2409.03661v3)

**Tags**: astro-ph.HE astro-ph.IM 



### Fast Sampling of Cosmological Initial Conditions with Gaussian Neural   Posterior Estimation
**Authors**: Oleg Savchenko, Guillermo Franco Abellán, Florian List, Noemi Anau Montel, Christoph Weniger

**Updated**: 2025-02-05T13:02:14Z

**Summary**: Knowledge of the primordial matter density field from which the large-scale structure of the Universe emerged over cosmic time is of fundamental importance for cosmology. However, reconstructing these cosmological initial conditions from late-time observations is a notoriously difficult task, which requires advanced cosmological simulators and sophisticated statistical methods to explore a multi-million-dimensional parameter space. We show how simulation-based inference (SBI) can be used to tackle this problem and to obtain data-constrained realisations of the primordial dark matter density field in a simulation-efficient way with general non-differentiable simulators. Our method is applicable to full high-resolution dark matter $N$-body simulations and is based on modelling the posterior distribution of the constrained initial conditions to be Gaussian with a diagonal covariance matrix in Fourier space. As a result, we can generate thousands of posterior samples within seconds on a single GPU, orders of magnitude faster than existing methods, paving the way for sequential SBI for cosmological fields. Furthermore, we perform an analytical fit of the estimated dependence of the covariance on the wavenumber, effectively transforming any point-estimator of initial conditions into a fast sampler. We test the validity of our obtained samples by comparing them to the true values with summary statistics and performing a Bayesian consistency test.

**Link**: [arxiv](http://arxiv.org/abs/2502.03139v1),  [pdf](http://arxiv.org/pdf/2502.03139v1)

**Tags**: astro-ph.CO astro-ph.IM cs.LG 



### Compressing Large Language Models with Automated Sub-Network Search
**Authors**: Rhea Sanjay Sukthanker, Benedikt Staffler, Frank Hutter, Aaron Klein

**Updated**: 2025-02-05T12:50:46Z

**Summary**: Large Language Models (LLMs) demonstrate exceptional reasoning abilities, enabling strong generalization across diverse tasks such as commonsense reasoning and instruction following. However, as LLMs scale, inference costs become increasingly prohibitive, accumulating significantly over their life cycle. In this paper we consider model compression for LLMs to reduce model size while improving downstream task performance. We phrase this as a neural architecture search problem that automatically prunes structural components, such as attention heads, neurons, and layers by searching for the Pareto-optimal set of sub-networks balancing between performance and on-device latency. Compared to state-of-the-art structural pruning approaches and fine-tuned smaller sub-networks extracted from the pre-trained model, our method achieves upto 9.85% improvement on average on 11 diverse downstream tasks, while achieving up to 22% improvement of on-device latency.

**Link**: [arxiv](http://arxiv.org/abs/2410.06479v3),  [pdf](http://arxiv.org/pdf/2410.06479v3)

**Tags**: cs.CL 



### Teaching Large Language Models Number-Focused Headline Generation With   Key Element Rationales
**Authors**: Zhen Qian, Xiuzhen Zhang, Xiaofei Xu, Feng Xia

**Updated**: 2025-02-05T12:39:07Z

**Summary**: Number-focused headline generation is a summarization task requiring both high textual quality and precise numerical accuracy, which poses a unique challenge for Large Language Models (LLMs). Existing studies in the literature focus only on either textual quality or numerical reasoning and thus are inadequate to address this challenge. In this paper, we propose a novel chain-of-thought framework for using rationales comprising key elements of the Topic, Entities, and Numerical reasoning (TEN) in news articles to enhance the capability for LLMs to generate topic-aligned high-quality texts with precise numerical accuracy. Specifically, a teacher LLM is employed to generate TEN rationales as supervision data, which are then used to teach and fine-tune a student LLM. Our approach teaches the student LLM automatic generation of rationales with enhanced capability for numerical reasoning and topic-aligned numerical headline generation. Experiments show that our approach achieves superior performance in both textual quality and numerical accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2502.03129v1),  [pdf](http://arxiv.org/pdf/2502.03129v1)

**Tags**: cs.CL cs.LG 



### Investigating Privacy Bias in Training Data of Language Models
**Authors**: Yan Shvartzshnaider, Vasisht Duddu

**Updated**: 2025-02-05T12:31:01Z

**Summary**: As LLMs are integrated into sociotechnical systems, it is crucial to examine the privacy biases they exhibit. A privacy bias refers to the skew in the appropriateness of information flows within a given context that LLMs acquire from large amounts of non-publicly available training data. This skew may either align with existing expectations or signal a symptom of systemic issues reflected in the training datasets.   We formulate a novel research question: how can we examine privacy biases in the training data of LLMs? We present a novel approach to assess the privacy biases using a contextual integrity-based methodology to evaluate the responses from different LLMs. Our approach accounts for the sensitivity of responses across prompt variations, which hinders the evaluation of privacy biases. We investigate how privacy biases are affected by model capacities and optimizations.

**Link**: [arxiv](http://arxiv.org/abs/2409.03735v2),  [pdf](http://arxiv.org/pdf/2409.03735v2)

**Tags**: cs.LG cs.AI cs.CR cs.CY 



### Code-Optimise: Self-Generated Preference Data for Correctness and   Efficiency
**Authors**: Leonidas Gee, Milan Gritta, Gerasimos Lampouras, Ignacio Iacobacci

**Updated**: 2025-02-05T12:29:01Z

**Summary**: Code Language Models have been trained to generate accurate solutions, typically with no regard for runtime. On the other hand, previous works that explored execution optimisation have observed corresponding drops in functional correctness. To that end, we introduce Code-Optimise, a framework that incorporates both correctness (passed, failed) and runtime (quick, slow) as learning signals via self-generated preference data. Our framework is both lightweight and robust as it dynamically selects solutions to reduce overfitting while avoiding a reliance on larger models for learning signals. Code-Optimise achieves significant improvements in pass@k while decreasing the competitive baseline runtimes by an additional 6% for in-domain data and up to 3% for out-of-domain data. As a by-product, the average length of the generated solutions is reduced by up to 48% on MBPP and 23% on HumanEval, resulting in faster and cheaper inference. The generated data and codebase is open-sourced at https://github.com/huawei-noah/HEBO/tree/Code_Optimise.

**Link**: [arxiv](http://arxiv.org/abs/2406.12502v2),  [pdf](http://arxiv.org/pdf/2406.12502v2)

**Tags**: cs.CL 



### Can Large Language Models Predict the Outcome of Judicial Decisions?
**Authors**: Mohamed Bayan Kmainasi, Ali Ezzat Shahroor, Amani Al-Ghraibah

**Updated**: 2025-02-05T12:17:36Z

**Summary**: Large Language Models (LLMs) have shown exceptional capabilities in Natural Language Processing (NLP) across diverse domains. However, their application in specialized tasks such as Legal Judgment Prediction (LJP) for low-resource languages like Arabic remains underexplored. In this work, we address this gap by developing an Arabic LJP dataset, collected and preprocessed from Saudi commercial court judgments. We benchmark state-of-the-art open-source LLMs, including LLaMA-3.2-3B and LLaMA-3.1-8B, under varying configurations such as zero-shot, one-shot, and fine-tuning using QLoRA. Additionally, we used a comprehensive evaluation framework combining quantitative metrics (BLEU and ROUGE) and qualitative assessments (Coherence, legal language, clarity). Our results demonstrate that fine-tuned smaller models achieve comparable performance to larger models in task-specific contexts while offering significant resource efficiency. Furthermore, we investigate the effects of prompt engineering and fine-tuning on model outputs, providing insights into performance variability and instruction sensitivity. By making the dataset, implementation code, and models publicly available, we establish a robust foundation for future research in Arabic legal NLP.

**Link**: [arxiv](http://arxiv.org/abs/2501.09768v2),  [pdf](http://arxiv.org/pdf/2501.09768v2)

**Tags**: cs.CL cs.AI 



### Structured Token Retention and Computational Memory Paths in Large   Language Models
**Authors**: Jonathan Delena, Augustin Moreau, Dominic Ravensdale, Frederick Chatterton

**Updated**: 2025-02-05T11:59:22Z

**Summary**: Memory retention mechanisms play a central role in determining the efficiency of computational architectures designed for processing extended sequences. Conventional methods for token management often impose fixed retention thresholds or rely on uniform attention weight distributions, leading to inefficient memory utilization and premature information loss in extended sequence modeling. Structured Token Retention (STR) introduces a probabilistic selection framework that dynamically adjusts token persistence based on contextual significance, ensuring that computational resources are allocated to semantically relevant elements. Computational Memory Paths (CMP) extend this framework through hierarchical memory allocation, refining retention efficiency through structured reallocation of token embeddings. Comparative assessments against baseline models demonstrate that STR and CMP improve token survival rates across long input sequences while reducing cumulative error propagation across processing layers. Experimental results further indicate reductions in computational overhead, improving inference speed without degrading contextual coherence. Token distribution analyses reveal that structured memory allocation prevents excessive redundancy in attention weight calculations, optimizing information retrieval efficiency in large-scale generative architectures. The integration of STR and CMP into an open-source model illustrates the adaptability of structured memory retention methodologies, highlighting their applicability in generative text processing, long-context comprehension, and scalable sequence modeling.

**Link**: [arxiv](http://arxiv.org/abs/2502.03102v1),  [pdf](http://arxiv.org/pdf/2502.03102v1)

**Tags**: cs.CL 



### Changes over time in the 100-year return value of climate model   variables
**Authors**: Callum Leach, Kevin Ewans, Philip Jonathan

**Updated**: 2025-02-05T11:55:55Z

**Summary**: We assess evidence for changes in tail characteristics of wind, solar irradiance and temperature variables output from CMIP6 global climate models (GCMs) due to climate forcing. We estimate global and climate zone annual maximum and annual means for period (2015, 2100) from daily output of seven GCMs for daily wind speed, maximum wind speed, solar irradiance and near-surface temperature. We calculate corresponding annualised data for individual locations within neighbourhoods of the North Atlantic and Celtic Sea region. We consider output for three climate scenarios and multiple climate ensembles. We estimate non-stationary extreme value models for annual extremes, and non-homogeneous Gaussian regressions for annual means, using Bayesian inference. We use estimated statistical models to quantify the distribution of (i) the change in 100-year return value for annual extremes, and (2) the change in annual mean, over the period (2025, 2125). To summarise results, we estimate linear mixed effects models for observed variation of (i) and (ii). Evidence for changes in the 100-year return value for annual maxima of solar irradiance and temperature is much stronger than for wind variables over time and with climate scenario.

**Link**: [arxiv](http://arxiv.org/abs/2501.11650v2),  [pdf](http://arxiv.org/pdf/2501.11650v2)

**Tags**: stat.AP physics.ao-ph 



### Reveal the Mystery of DPO: The Connection between DPO and RL Algorithms
**Authors**: Xuerui Su, Yue Wang, Jinhua Zhu, Mingyang Yi, Feng Xu, Zhiming Ma, Yuting Liu

**Updated**: 2025-02-05T11:41:43Z

**Summary**: With the rapid development of Large Language Models (LLMs), numerous Reinforcement Learning from Human Feedback (RLHF) algorithms have been introduced to improve model safety and alignment with human preferences. These algorithms can be divided into two main frameworks based on whether they require an explicit reward (or value) function for training: actor-critic-based Proximal Policy Optimization (PPO) and alignment-based Direct Preference Optimization (DPO). The mismatch between DPO and PPO, such as DPO's use of a classification loss driven by human-preferred data, has raised confusion about whether DPO should be classified as a Reinforcement Learning (RL) algorithm. To address these ambiguities, we focus on three key aspects related to DPO, RL, and other RLHF algorithms: (1) the construction of the loss function; (2) the target distribution at which the algorithm converges; (3) the impact of key components within the loss function. Specifically, we first establish a unified framework named UDRRA connecting these algorithms based on the construction of their loss functions. Next, we uncover their target policy distributions within this framework. Finally, we investigate the critical components of DPO to understand their impact on the convergence rate. Our work provides a deeper understanding of the relationship between DPO, RL, and other RLHF algorithms, offering new insights for improving existing algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2502.03095v1),  [pdf](http://arxiv.org/pdf/2502.03095v1)

**Tags**: cs.LG 



### UGPhysics: A Comprehensive Benchmark for Undergraduate Physics Reasoning   with Large Language Models
**Authors**: Xin Xu, Qiyun Xu, Tong Xiao, Tianhao Chen, Yuchen Yan, Jiaxin Zhang, Shizhe Diao, Can Yang, Yang Wang

**Updated**: 2025-02-05T11:36:53Z

**Summary**: Large language models (LLMs) have demonstrated remarkable capabilities in solving complex reasoning tasks, particularly in mathematics. However, the domain of physics reasoning presents unique challenges that have received significantly less attention. Existing benchmarks often fall short in evaluating LLMs' abilities on the breadth and depth of undergraduate-level physics, underscoring the need for a comprehensive evaluation. To fill this gap, we introduce UGPhysics, a large-scale and comprehensive benchmark specifically designed to evaluate UnderGraduate-level Physics (UGPhysics) reasoning with LLMs. UGPhysics includes 5,520 undergraduate-level physics problems in both English and Chinese, covering 13 subjects with seven different answer types and four distinct physics reasoning skills, all rigorously screened for data leakage. Additionally, we develop a Model-Assistant Rule-based Judgment (MARJ) pipeline specifically tailored for assessing answer correctness of physics problems, ensuring accurate evaluation. Our evaluation of 31 leading LLMs shows that the highest overall accuracy, 49.8% (achieved by OpenAI-o1-mini), emphasizes the necessity for models with stronger physics reasoning skills, beyond math abilities. We hope UGPhysics, along with MARJ, will drive future advancements in AI for physics reasoning. Codes and data are available at https://github.com/YangLabHKUST/UGPhysics .

**Link**: [arxiv](http://arxiv.org/abs/2502.00334v2),  [pdf](http://arxiv.org/pdf/2502.00334v2)

**Tags**: cs.CL cs.AI 



### Changing disc compositions via internal photoevaporation I: Solar-mass   stars
**Authors**: Julia Lena Lienert, Bertram Bitsch, Thomas Henning

**Updated**: 2025-02-05T11:33:20Z

**Summary**: The chemical evolution of protoplanetary discs is not fully understood, several factors influence the final distribution of disc material. One such factor are inward drifting and evaporating pebbles that enrich the inner disc with vapour. In particular, it is first enriched with water vapour, resulting in a low C/O ratio, before carbon-rich gas from the outer disc is transported inwards elevating the C/O ratio again. However, it is unclear how internal photoevaporation, which carries away gas and opens gaps that block inward drifting pebbles, affects the chemical composition of the disc. We aim to study these effects in discs around solar-like stars, where we especially focus on the C/O ratio and the water content. The simulations are carried out using a semi-analytical 1D disc model. Our code chemcomp includes viscous evolution and heating, pebble growth and drift, pebble evaporation and condensation, and a simple chemical partitioning model. We show that internal photoevaporation plays a major role in the (chemical) evolution of protoplanetary discs: As it opens a gap, inward drifting pebbles are stopped and cannot contribute to the volatile content any more. In addition, gas from the outer disc is carried away by photoevaporative winds. Consequently, the C/O ratio in the inner disc is low. In contrast, gaps opened by giant planets allow the gas to pass, resulting in an elevated C/O ratio, similar to viscous discs without internal photoevaporation. This will enable us to distinguish observationally between these two scenarios when measuring the C/O ratio, implying that we can infer the cause of gap structures in disc observations. In the case of a photoevaporative disc, we additionally find an elevated water content in the inner disc as the water vapour and ice undergo a cycle of evaporation/re-condensation, preventing its inward accretion onto the star.

**Link**: [arxiv](http://arxiv.org/abs/2402.09342v3),  [pdf](http://arxiv.org/pdf/2402.09342v3)

**Tags**: astro-ph.EP 



### Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of   Experts
**Authors**: Xiaoming Shi, Shiyu Wang, Yuqi Nie, Dianqi Li, Zhou Ye, Qingsong Wen, Ming Jin

**Updated**: 2025-02-05T11:32:34Z

**Summary**: Deep learning for time series forecasting has seen significant advancements over the past decades. However, despite the success of large-scale pre-training in language and vision domains, pre-trained time series models remain limited in scale and operate at a high cost, hindering the development of larger capable forecasting models in real-world applications. In response, we introduce Time-MoE, a scalable and unified architecture designed to pre-train larger, more capable forecasting foundation models while reducing inference costs. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE enhances computational efficiency by activating only a subset of networks for each prediction, reducing computational load while maintaining high model capacity. This allows Time-MoE to scale effectively without a corresponding increase in inference costs. Time-MoE comprises a family of decoder-only transformer models that operate in an auto-regressive manner and support flexible forecasting horizons with varying input context lengths. We pre-trained these models on our newly introduced large-scale data Time-300B, which spans over 9 domains and encompassing over 300 billion time points. For the first time, we scaled a time series foundation model up to 2.4 billion parameters, achieving significantly improved forecasting precision. Our results validate the applicability of scaling laws for training tokens and model size in the context of time series forecasting. Compared to dense models with the same number of activated parameters or equivalent computation budgets, our models consistently outperform them by large margin. These advancements position Time-MoE as a state-of-the-art solution for tackling real-world time series forecasting challenges with superior capability, efficiency, and flexibility.

**Link**: [arxiv](http://arxiv.org/abs/2409.16040v3),  [pdf](http://arxiv.org/pdf/2409.16040v3)

**Tags**: cs.LG cs.AI 



### Inference on varying coefficients in spatial autoregressions
**Authors**: Abhimanyu Gupta, Xi Qu, Sorawoot Srisuma, Jiajun Zhang

**Updated**: 2025-02-05T11:21:04Z

**Summary**: We present simple to implement Wald-type statistics that deliver a general nonparametric inference theory for linear restrictions on varying coefficients in a range of spatial autoregressive models. Our theory covers error dependence of a general form, allows for a degree of misspecification robustness via nonparametric spatial weights and permits inference on both varying regression and spatial coefficients. One application of our method finds evidence for constant returns to scale in the production function of the Chinese nonmetal mineral industry, while another finds a nonlinear impact of the distance to the employment center on housing prices in Boston. A simulation study confirms that our tests perform well in finite-samples.

**Link**: [arxiv](http://arxiv.org/abs/2502.03084v1),  [pdf](http://arxiv.org/pdf/2502.03084v1)

**Tags**: econ.EM 



### IAO Prompting: Making Knowledge Flow Explicit in LLMs through Structured   Reasoning Templates
**Authors**: Aissatou Diallo, Antonis Bikakis, Luke Dickens, Anthony Hunter, Rob Miller

**Updated**: 2025-02-05T11:14:20Z

**Summary**: While Large Language Models (LLMs) demonstrate impressive reasoning capabilities, understanding and validating their knowledge utilization remains challenging. Chain-of-thought (CoT) prompting partially addresses this by revealing intermediate reasoning steps, but the knowledge flow and application remain implicit. We introduce IAO (Input-Action-Output) prompting, a structured template-based method that explicitly models how LLMs access and apply their knowledge during complex reasoning tasks. IAO decomposes problems into sequential steps, each clearly identifying the input knowledge being used, the action being performed, and the resulting output. This structured decomposition enables us to trace knowledge flow, verify factual consistency, and identify potential knowledge gaps or misapplications. Through experiments across diverse reasoning tasks, we demonstrate that IAO not only improves zero-shot performance but also provides transparency in how LLMs leverage their stored knowledge. Human evaluation confirms that this structured approach enhances our ability to verify knowledge utilization and detect potential hallucinations or reasoning errors. Our findings provide insights into both knowledge representation within LLMs and methods for more reliable knowledge application.

**Link**: [arxiv](http://arxiv.org/abs/2502.03080v1),  [pdf](http://arxiv.org/pdf/2502.03080v1)

**Tags**: cs.CL 



### Optimizing Electric Vehicles Charging using Large Language Models and   Graph Neural Networks
**Authors**: Stavros Orfanoudakis, Peter Palensky, Pedro P. Vergara

**Updated**: 2025-02-05T11:00:51Z

**Summary**: Maintaining grid stability amid widespread electric vehicle (EV) adoption is vital for sustainable transportation. Traditional optimization methods and Reinforcement Learning (RL) approaches often struggle with the high dimensionality and dynamic nature of real-time EV charging, leading to sub-optimal solutions. To address these challenges, this study demonstrates that combining Large Language Models (LLMs), for sequence modeling, with Graph Neural Networks (GNNs), for relational information extraction, not only outperforms conventional EV smart charging methods, but also paves the way for entirely new research directions and innovative solutions.

**Link**: [arxiv](http://arxiv.org/abs/2502.03067v1),  [pdf](http://arxiv.org/pdf/2502.03067v1)

**Tags**: eess.SY cs.LG cs.SY 



### Time Series Anomaly Detection in the Frequency Domain with Statistical   Reliability
**Authors**: Akifumi Yamada, Tomohiro Shiraishi, Shuichi Nishino, Teruyuki Katsuoka, Kouichi Taji, Ichiro Takeuchi

**Updated**: 2025-02-05T10:48:12Z

**Summary**: Effective anomaly detection in complex systems requires identifying change points (CPs) in the frequency domain, as abnormalities often arise across multiple frequencies. This paper extends recent advancements in statistically significant CP detection, based on Selective Inference (SI), to the frequency domain. The proposed SI method quantifies the statistical significance of detected CPs in the frequency domain using $p$-values, ensuring that the detected changes reflect genuine structural shifts in the target system. We address two major technical challenges to achieve this. First, we extend the existing SI framework to the frequency domain by appropriately utilizing the properties of discrete Fourier transform (DFT). Second, we develop an SI method that provides valid $p$-values for CPs where changes occur across multiple frequencies. Experimental results demonstrate that the proposed method reliably identifies genuine CPs with strong statistical guarantees, enabling more accurate root-cause analysis in the frequency domain of complex systems.

**Link**: [arxiv](http://arxiv.org/abs/2502.03062v1),  [pdf](http://arxiv.org/pdf/2502.03062v1)

**Tags**: stat.ML cs.LG 



### Almost Surely Safe Alignment of Large Language Models at Inference-Time
**Authors**: Xiaotong Ji, Shyam Sundhar Ramesh, Matthieu Zimmer, Ilija Bogunovic, Jun Wang, Haitham Bou Ammar

**Updated**: 2025-02-05T10:47:19Z

**Summary**: Even highly capable large language models (LLMs) can produce biased or unsafe responses, and alignment techniques, such as RLHF, aimed at mitigating this issue, are expensive and prone to overfitting as they retrain the LLM. This paper introduces a novel inference-time alignment approach that ensures LLMs generate safe responses almost surely, i.e., with a probability approaching one. We achieve this by framing the safe generation of inference-time responses as a constrained Markov decision process within the LLM's latent space. Crucially, we augment a safety state that tracks the evolution of safety constraints and enables us to demonstrate formal safety guarantees upon solving the MDP in the latent space. Building on this foundation, we propose InferenceGuard, a practical implementation that safely aligns LLMs without modifying the model weights. Empirically, we demonstrate InferenceGuard effectively balances safety and task performance, outperforming existing inference-time alignment methods in generating safe and aligned responses.

**Link**: [arxiv](http://arxiv.org/abs/2502.01208v2),  [pdf](http://arxiv.org/pdf/2502.01208v2)

**Tags**: cs.LG cs.CL 



### Text-to-CAD Generation Through Infusing Visual Feedback in Large   Language Models
**Authors**: Ruiyu Wang, Yu Yuan, Shizhao Sun, Jiang Bian

**Updated**: 2025-02-05T10:43:26Z

**Summary**: Creating Computer-Aided Design (CAD) models requires significant expertise and effort. Text-to-CAD, which converts textual descriptions into CAD parametric sequences, is crucial in streamlining this process. Recent studies have utilized ground-truth parametric sequences, known as sequential signals, as supervision to achieve this goal. However, CAD models are inherently multimodal, comprising parametric sequences and corresponding rendered visual objects. Besides,the rendering process from parametric sequences to visual objects is many-to-one. Therefore, both sequential and visual signals are critical for effective training. In this work, we introduce CADFusion, a framework that uses Large Language Models (LLMs) as the backbone and alternates between two training stages: the sequential learning (SL) stage and the visual feedback (VF) stage. In the SL stage, we train LLMs using ground-truth parametric sequences, enabling the generation of logically coherent parametric sequences. In the VF stage, we reward parametric sequences that render into visually preferred objects and penalize those that do not, allowing LLMs to learn how rendered visual objects are perceived and evaluated. These two stages alternate throughout the training, ensuring balanced learning and preserving benefits of both signals. Experiments demonstrate that CADFusion significantly improves performance, both qualitatively and quantitatively.

**Link**: [arxiv](http://arxiv.org/abs/2501.19054v2),  [pdf](http://arxiv.org/pdf/2501.19054v2)

**Tags**: cs.CV cs.LG 



### Understanding and Enhancing the Transferability of Jailbreaking Attacks
**Authors**: Runqi Lin, Bo Han, Fengwang Li, Tongling Liu

**Updated**: 2025-02-05T10:29:54Z

**Summary**: Jailbreaking attacks can effectively manipulate open-source large language models (LLMs) to produce harmful responses. However, these attacks exhibit limited transferability, failing to disrupt proprietary LLMs consistently. To reliably identify vulnerabilities in proprietary LLMs, this work investigates the transferability of jailbreaking attacks by analysing their impact on the model's intent perception. By incorporating adversarial sequences, these attacks can redirect the source LLM's focus away from malicious-intent tokens in the original input, thereby obstructing the model's intent recognition and eliciting harmful responses. Nevertheless, these adversarial sequences fail to mislead the target LLM's intent perception, allowing the target LLM to refocus on malicious-intent tokens and abstain from responding. Our analysis further reveals the inherent distributional dependency within the generated adversarial sequences, whose effectiveness stems from overfitting the source LLM's parameters, resulting in limited transferability to target LLMs. To this end, we propose the Perceived-importance Flatten (PiF) method, which uniformly disperses the model's focus across neutral-intent tokens in the original input, thus obscuring malicious-intent tokens without relying on overfitted adversarial sequences. Extensive experiments demonstrate that PiF provides an effective and efficient red-teaming evaluation for proprietary LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2502.03052v1),  [pdf](http://arxiv.org/pdf/2502.03052v1)

**Tags**: cs.LG cs.CR 



### SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a   Practical Manner
**Authors**: Xunguang Wang, Daoyuan Wu, Zhenlan Ji, Zongjie Li, Pingchuan Ma, Shuai Wang, Yingjiu Li, Yang Liu, Ning Liu, Juergen Rahmel

**Updated**: 2025-02-05T10:29:07Z

**Summary**: Jailbreaking is an emerging adversarial attack that bypasses the safety alignment deployed in off-the-shelf large language models (LLMs) and has evolved into multiple categories: human-based, optimization-based, generation-based, and the recent indirect and multilingual jailbreaks. However, delivering a practical jailbreak defense is challenging because it needs to not only handle all the above jailbreak attacks but also incur negligible delays to user prompts, as well as be compatible with both open-source and closed-source LLMs. Inspired by how the traditional security concept of shadow stacks defends against memory overflow attacks, this paper introduces a generic LLM jailbreak defense framework called SelfDefend, which establishes a shadow LLM as a defense instance (in detection state) to concurrently protect the target LLM instance (in normal answering state) in the normal stack and collaborate with it for checkpoint-based access control. The effectiveness of SelfDefend builds upon our observation that existing LLMs can identify harmful prompts or intentions in user queries, which we empirically validate using mainstream GPT-3.5/4 models against major jailbreak attacks. To further improve the defense's robustness and minimize costs, we employ a data distillation approach to tune dedicated open-source defense models. When deployed to protect GPT-3.5/4, Claude, Llama-2-7b/13b, and Mistral, these models outperform seven state-of-the-art defenses and match the performance of GPT-4-based SelfDefend, with significantly lower extra delays. Further experiments show that the tuned models are robust to adaptive jailbreaks and prompt injections.

**Link**: [arxiv](http://arxiv.org/abs/2406.05498v3),  [pdf](http://arxiv.org/pdf/2406.05498v3)

**Tags**: cs.CR cs.AI 



### Energy Diffusion and Advection Coefficients in Kinetic Simulations of   Relativistic Plasma Turbulence
**Authors**: Kai W. Wong, Vladimir Zhdankin, Dmitri A. Uzdensky, Gregory R. Werner, Mitchell C. Begelman

**Updated**: 2025-02-05T09:59:28Z

**Summary**: Turbulent, relativistic nonthermal plasmas are ubiquitous in high-energy astrophysical systems, as inferred from broadband nonthermal emission spectra. The underlying turbulent nonthermal particle acceleration (NTPA) processes have traditionally been modelled with a Fokker-Planck (FP) diffusion-advection equation for the particle energy distribution. We test FP-type NTPA theories by performing and analysing particle-in-cell (PIC) simulations of turbulence in collisionless relativistic pair plasma. By tracking large numbers of particles in simulations with different initial magnetisation and system size, we first test and confirm the applicability of the FP framework. We then measure the FP energy diffusion ($D$) and advection ($A$) coefficients as functions of particle energy $\gamma m c^2$, and compare their dependence to theoretical predictions. At high energies, we robustly find $D \sim \gamma^2$ for all cases. Hence, we fit $D = D_0 \gamma^2$ and find a scaling consistent with $D_0 \sim \sigma^{3/2}$ at low instantaneous magnetisation $\sigma(t)$, flattening to $D_0 \sim \sigma$ at higher $\sigma \sim 1$. We also find that the power-law index $\alpha(t)$ of the particle energy distribution converges exponentially in time. We build and test an analytic model connecting the FP coefficients and $\alpha(t)$, predicting $A(\gamma) \sim \gamma \log \gamma$. We confirm this functional form in our measurements of $A(\gamma,t)$, which allows us to predict $\alpha(t)$ through the model relations. Our results suggest that the basic second-order Fermi acceleration model, which predicts $D_0 \sim \sigma$, may not be a complete description of NTPA in turbulent plasmas. These findings encourage further application of tracked particles and FP coefficients as a diagnostic in kinetic simulations of various astrophysically relevant plasma processes like collisionless shocks and magnetic reconnection.

**Link**: [arxiv](http://arxiv.org/abs/2502.03042v1),  [pdf](http://arxiv.org/pdf/2502.03042v1)

**Tags**: astro-ph.HE physics.plasm-ph 



### Large Language Models Are Universal Recommendation Learners
**Authors**: Junguang Jiang, Yanwen Huang, Bin Liu, Xiaoyu Kong, Ziru Xu, Han Zhu, Jian Xu, Bo Zheng

**Updated**: 2025-02-05T09:56:52Z

**Summary**: In real-world recommender systems, different tasks are typically addressed using supervised learning on task-specific datasets with carefully designed model architectures. We demonstrate that large language models (LLMs) can function as universal recommendation learners, capable of handling multiple tasks within a unified input-output framework, eliminating the need for specialized model designs. To improve the recommendation performance of LLMs, we introduce a multimodal fusion module for item representation and a sequence-in-set-out approach for efficient candidate generation. When applied to industrial-scale data, our LLM achieves competitive results with expert models elaborately designed for different recommendation tasks. Furthermore, our analysis reveals that recommendation outcomes are highly sensitive to text input, highlighting the potential of prompt engineering in optimizing industrial-scale recommender systems.

**Link**: [arxiv](http://arxiv.org/abs/2502.03041v1),  [pdf](http://arxiv.org/pdf/2502.03041v1)

**Tags**: cs.IR cs.LG 



### On the Minimum Attainable Risk in Permutation Invariant Problems
**Authors**: Asaf Weinstein

**Updated**: 2025-02-05T09:53:31Z

**Summary**: We consider a broad class of permutation invariant statistical problems by extending the standard decision theoretic definition to allow also selective inference tasks, where the target is specified only after seeing the data. For any such problem we show that, among all permutation invariant procedures, the minimizer of the risk at $\boldsymbol{\theta}$ is precisely the rule that minimizes the Bayes risk under a (postulated) discrete prior assigning equal probability to every permutation of $\boldsymbol{\theta}$. This gives an explicit characterization of the greatest lower bound on the risk of every sensible procedure in a wide range of problems. Furthermore, in a permutation invariant problem of estimating the parameter of a selected population under squared loss, we prove that this lower bound coincides asymptotically with a simpler lower bound, attained by the Bayes solution that replaces the aforementioned uniform prior on all permutations of $\boldsymbol{\theta}$ by the i.i.d. prior with the same marginals. This has important algorithmic implications because it suggests that our greatest lower bound is asymptotically attainable uniformly in $\boldsymbol{\theta}$ by an empirical Bayes procedure. Altogether, the above extends theory that has been established in the existing literature only for the very special case of compound decision problems.

**Link**: [arxiv](http://arxiv.org/abs/2110.06250v2),  [pdf](http://arxiv.org/pdf/2110.06250v2)

**Tags**: math.ST stat.TH 62C05, 62C12, 62C25 



### Knowledge Distillation from Large Language Models for Household Energy   Modeling
**Authors**: Mohannad Takrouri, Nicolás M. Cuadrado, Martin Takáč

**Updated**: 2025-02-05T09:43:14Z

**Summary**: Machine learning (ML) is increasingly vital for smart-grid research, yet restricted access to realistic, diverse data - often due to privacy concerns - slows progress and fuels doubts within the energy sector about adopting ML-based strategies. We propose integrating Large Language Models (LLMs) in energy modeling to generate realistic, culturally sensitive, and behavior-specific data for household energy usage across diverse geographies. In this study, we employ and compare five different LLMs to systematically produce family structures, weather patterns, and daily consumption profiles for households in six distinct countries. A four-stage methodology synthesizes contextual daily data, including culturally nuanced activities, realistic weather ranges, HVAC operations, and distinct `energy signatures' that capture unique consumption footprints. Additionally, we explore an alternative strategy where external weather datasets can be directly integrated, bypassing intermediate weather modeling stages while ensuring physically consistent data inputs. The resulting dataset provides insights into how cultural, climatic, and behavioral factors converge to shape carbon emissions, offering a cost-effective avenue for scenario-based energy optimization. This approach underscores how prompt engineering, combined with knowledge distillation, can advance sustainable energy research and climate mitigation efforts. Source code is available at https://github.com/Singularity-AI-Lab/LLM-Energy-Knowledge-Distillation .

**Link**: [arxiv](http://arxiv.org/abs/2502.03034v1),  [pdf](http://arxiv.org/pdf/2502.03034v1)

**Tags**: cs.CL cs.LG 



### Stochastic Variational Inference for Structured Additive Distributional   Regression
**Authors**: Gianmarco Callegher, Thomas Kneib, Johannes Söding, Paul Wiemann

**Updated**: 2025-02-05T09:40:05Z

**Summary**: In structured additive distributional regression, the conditional distribution of the response variables given the covariate information and the vector of model parameters is modelled using a P-parametric probability density function where each parameter is modelled through a linear predictor and a bijective response function that maps the domain of the predictor into the domain of the parameter. We present a method to perform inference in structured additive distributional regression using stochastic variational inference. We propose two strategies for constructing a multivariate Gaussian variational distribution to estimate the posterior distribution of the regression coefficients. The first strategy leverages covariate information and hyperparameters to learn both the location vector and the precision matrix. The second strategy tackles the complexity challenges of the first by initially assuming independence among all smooth terms and then introducing correlations through an additional set of variational parameters. Furthermore, we present two approaches for estimating the smoothing parameters. The first treats them as free parameters and provides point estimates, while the second accounts for uncertainty by applying a variational approximation to the posterior distribution. Our model was benchmarked against state-of-the-art competitors in logistic and gamma regression simulation studies. Finally, we validated our approach by comparing its posterior estimates to those obtained using Markov Chain Monte Carlo on a dataset of patents from the biotechnology/pharmaceutics and semiconductor/computer sectors.

**Link**: [arxiv](http://arxiv.org/abs/2412.10038v2),  [pdf](http://arxiv.org/pdf/2412.10038v2)

**Tags**: stat.CO 62 G.3 



### Efficient Prompt Compression with Evaluator Heads for Long-Context   Transformer Inference
**Authors**: Weizhi Fei, Xueyan Niu, Guoqing Xie, Yingqing Liu, Bo Bai, Wei Han

**Updated**: 2025-02-05T09:35:38Z

**Summary**: Although applications involving long-context inputs are crucial for the effective utilization of large language models (LLMs), they also result in increased computational costs and reduced performance. To address this challenge, we propose an efficient, training-free prompt compression method that retains key information within compressed prompts. We identify specific attention heads in transformer-based LLMs, which we designate as evaluator heads, that are capable of selecting tokens in long inputs that are most significant for inference. Building on this discovery, we develop EHPC, an Evaluator Head-based Prompt Compression method, which enables LLMs to rapidly "skim through" input prompts by leveraging only the first few layers with evaluator heads during the pre-filling stage, subsequently passing only the important tokens to the model for inference. EHPC achieves state-of-the-art results across two mainstream benchmarks: prompt compression and long-context inference acceleration. Consequently, it effectively reduces the complexity and costs associated with commercial API calls. We further demonstrate that EHPC attains competitive results compared to key-value cache-based acceleration methods, thereby highlighting its potential to enhance the efficiency of LLMs for long-context tasks.

**Link**: [arxiv](http://arxiv.org/abs/2501.12959v2),  [pdf](http://arxiv.org/pdf/2501.12959v2)

**Tags**: cs.CL 



### Rank-Based Identification of High-dimensional Surrogate Markers:   Application to Vaccinology
**Authors**: Arthur Hughes, Layla Parast, Rodolphe Thiébaut, Boris P. Hejblum

**Updated**: 2025-02-05T09:33:42Z

**Summary**: In vaccine trials with long-term participant follow-up, it is of great importance to identify surrogate markers that accurately infer long-term immune responses. These markers offer practical advantages such as providing early, indirect evidence of vaccine efficacy, and can accelerate vaccine development while identifying potential biomarkers. High-throughput technologies like RNA-sequencing have emerged as promising tools for understanding complex biological systems and informing new treatment strategies. However, these data are high-dimensional, presenting unique statistical challenges for existing surrogate marker identification methods. We introduce Rank-based Identification of high-dimensional SurrogatE Markers (RISE), a novel approach designed for small sample, high-dimensional settings typical in modern vaccine experiments. RISE employs a non-parametric univariate test to screen variables for promising candidates, followed by surrogate evaluation on independent data. Our simulation studies demonstrate RISE's desirable properties, including type one error rate control and empirical power under various conditions. Applying RISE to a clinical trial for inactivated influenza vaccination, we sought to identify genes whose post-vaccination expression could serve as a surrogate for the induced immune response. This analysis revealed a signature of genes whose combined expression at 1 day post-injection appears to be a reasonable surrogate for the neutralising antibody titres at 28 days after vaccination. Pathways related to innate antiviral signalling and interferon stimulation were strongly represented in this derived surrogate, providing a clear immunological interpretation.

**Link**: [arxiv](http://arxiv.org/abs/2502.03030v1),  [pdf](http://arxiv.org/pdf/2502.03030v1)

**Tags**: stat.ME stat.AP 



### On Zero-Initialized Attention: Optimal Prompt and Gating Factor   Estimation
**Authors**: Nghiem T. Diep, Huy Nguyen, Chau Nguyen, Minh Le, Duy M. H. Nguyen, Daniel Sonntag, Mathias Niepert, Nhat Ho

**Updated**: 2025-02-05T09:31:27Z

**Summary**: The LLaMA-Adapter has recently emerged as an efficient fine-tuning technique for LLaMA models, leveraging zero-initialized attention to stabilize training and enhance performance. However, despite its empirical success, the theoretical foundations of zero-initialized attention remain largely unexplored. In this paper, we provide a rigorous theoretical analysis, establishing a connection between zero-initialized attention and mixture-of-expert models. We prove that both linear and non-linear prompts, along with gating functions, can be optimally estimated, with non-linear prompts offering greater flexibility for future applications. Empirically, we validate our findings on the open LLM benchmarks, demonstrating that non-linear prompts outperform linear ones. Notably, even with limited training data, both prompt types consistently surpass vanilla attention, highlighting the robustness and adaptability of zero-initialized attention.

**Link**: [arxiv](http://arxiv.org/abs/2502.03029v1),  [pdf](http://arxiv.org/pdf/2502.03029v1)

**Tags**: cs.LG 



### More is better: Strong constraints on the stellar properties of LEGA-C z   ~ 1 galaxies with Prospector
**Authors**: Angelos Nersesian, Arjen van der Wel, Anna R. Gallazzi, Yasha Kaushal, Rachel Bezanson, Stefano Zibetti, Eric F. Bell, Francesco D'Eugenio, Joel Leja, Marco Martorano, Po-Feng Wu

**Updated**: 2025-02-05T09:24:53Z

**Summary**: We present the stellar properties of 2908 galaxies at 0.6 < z < 1.0 from the LEGA-C survey. We emphasize the importance of high signal-to-noise, high spectral resolution spectroscopy in the inference of stellar population properties of galaxies. We estimate the galaxy properties with the SED fitting code Prospector, by fitting spectroscopy and broadband photometry together, drawn from the LEGA-C DR3 and UltraVISTA catalogs respectively. We report a positive correlation between light-weighted ages and stellar velocity dispersion ($\sigma_\star$). The trend with $\sigma_\star$ is weaker for the mass-weighted ages and stellar metallicity ($Z_\star$). On average, quiescent galaxies are characterized by high $Z_\star$, they are \sim 1.1 Gyr older, less dusty, with steeper dust attenuation slopes compared to star-forming galaxies. Conversely, star-forming galaxies are characterized by significantly higher dust optical depths and shallower (grayer) attenuation slopes. Low mass (high mass) star-forming galaxies have lower (higher) $Z_\star$, while their stellar populations are on average younger (older). A key pragmatic result of our study is that a linear-space metallicity prior is preferable to a logarithmic-space one when using photometry alone, as the latter biases the posteriors downward. Spectroscopy greatly improves stellar population measurements and is required to provide meaningful constraints on age, metallicity, and other properties. Pairing spectroscopy with photometry helps resolving the dust-age-metallicity degeneracy, yielding more accurate mass- and light-weighted ages, with ages inferred from photometry alone suffering such large uncertainties. Stellar metallicities are constrained by our spectroscopy, but precise measurements remain challenging (and impossible with photometry alone), particularly in the absence of Mg and Fe lines redward of 5000 $\AA$ in the observed spectrum.

**Link**: [arxiv](http://arxiv.org/abs/2502.03021v1),  [pdf](http://arxiv.org/pdf/2502.03021v1)

**Tags**: astro-ph.GA 



### Panel Data Estimation and Inference: Homogeneity versus Heterogeneity
**Authors**: Jiti Gao, Fei Liu, Bin Peng, Yayi Yan

**Updated**: 2025-02-05T09:22:33Z

**Summary**: In this paper, we define an underlying data generating process that allows for different magnitudes of cross-sectional dependence, along with time series autocorrelation. This is achieved via high-dimensional moving average processes of infinite order (HDMA($\infty$)). Our setup and investigation integrates and enhances homogenous and heterogeneous panel data estimation and testing in a unified way. To study HDMA($\infty$), we extend the Beveridge-Nelson decomposition to a high-dimensional time series setting, and derive a complete toolkit set. We exam homogeneity versus heterogeneity using Gaussian approximation, a prevalent technique for establishing uniform inference. For post-testing inference, we derive central limit theorems through Edgeworth expansions for both homogenous and heterogeneous settings. Additionally, we showcase the practical relevance of the established asymptotic properties by revisiting the common correlated effects (CCE) estimators, and a classic nonstationary panel data process. Finally, we verify our theoretical findings via extensive numerical studies using both simulated and real datasets.

**Link**: [arxiv](http://arxiv.org/abs/2502.03019v1),  [pdf](http://arxiv.org/pdf/2502.03019v1)

**Tags**: econ.EM 



## Keyword: LLM Deployment 
 ### Do Large Language Model Benchmarks Test Reliability?
**Authors**: Joshua Vendrow, Edward Vendrow, Sara Beery, Aleksander Madry

**Updated**: 2025-02-05T18:58:19Z

**Summary**: When deploying large language models (LLMs), it is important to ensure that these models are not only capable, but also reliable. Many benchmarks have been created to track LLMs' growing capabilities, however there has been no similar focus on measuring their reliability. To understand the potential ramifications of this gap, we investigate how well current benchmarks quantify model reliability. We find that pervasive label errors can compromise these evaluations, obscuring lingering model failures and hiding unreliable behavior.   Motivated by this gap in the evaluation of reliability, we then propose the concept of so-called platinum benchmarks, i.e., benchmarks carefully curated to minimize label errors and ambiguity. As a first attempt at constructing such benchmarks, we revise examples from fifteen existing popular benchmarks. We evaluate a wide range of models on these platinum benchmarks and find that, indeed, frontier LLMs still exhibit failures on simple tasks such as elementary-level math word problems. Analyzing these failures further reveals previously unidentified patterns of problems on which frontier models consistently struggle. We provide code at https://github.com/MadryLab/platinum-benchmarks

**Link**: [arxiv](http://arxiv.org/abs/2502.03461v1),  [pdf](http://arxiv.org/pdf/2502.03461v1)

**Tags**: cs.LG cs.CL 



### Adapt-Pruner: Adaptive Structural Pruning for Efficient Small Language   Model Training
**Authors**: Boyao Wang, Rui Pan, Shizhe Diao, Xingyuan Pan, Jipeng Zhang, Renjie Pi, Tong Zhang

**Updated**: 2025-02-05T18:57:40Z

**Summary**: Small language models (SLMs) have attracted considerable attention from both academia and industry due to their broad range of applications in edge devices. To obtain SLMs with strong performance, conventional approaches either pre-train the models from scratch, which incurs substantial computational costs, or compress/prune existing large language models (LLMs), which results in performance drops and falls short in comparison to pre-training. In this paper, we investigate the family of acceleration methods that involve both structured pruning and model training. We found 1) layer-wise adaptive pruning (Adapt-Pruner) is extremely effective in LLMs and yields significant improvements over existing pruning techniques, 2) adaptive pruning equipped with further training leads to models comparable to those pre-training from scratch, 3) incremental pruning brings non-trivial performance gain by interleaving pruning with training and only removing a small portion of neurons ($\sim$5%) at a time. Experimental results on LLaMA-3.1-8B demonstrate that Adapt-Pruner outperforms conventional pruning methods, such as LLM-Pruner, FLAP, and SliceGPT, by an average of 1%-7% in accuracy on commonsense benchmarks. Additionally, Adapt-Pruner restores the performance of MobileLLM-125M to 600M on the MMLU benchmark with 200$\times$ fewer tokens via pruning from its larger counterparts, and discovers a new 1B model that surpasses LLaMA-3.2-1B in multiple benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2502.03460v1),  [pdf](http://arxiv.org/pdf/2502.03460v1)

**Tags**: cs.LG cs.AI cs.CL 



### A Schema-Guided Reason-while-Retrieve framework for Reasoning on Scene   Graphs with Large-Language-Models (LLMs)
**Authors**: Yiye Chen, Harpreet Sawhney, Nicholas Gydé, Yanan Jian, Jack Saunders, Patricio Vela, Ben Lundell

**Updated**: 2025-02-05T18:50:38Z

**Summary**: Scene graphs have emerged as a structured and serializable environment representation for grounded spatial reasoning with Large Language Models (LLMs). In this work, we propose SG-RwR, a Schema-Guided Retrieve-while-Reason framework for reasoning and planning with scene graphs. Our approach employs two cooperative, code-writing LLM agents: a (1) Reasoner for task planning and information queries generation, and a (2) Retriever for extracting corresponding graph information following the queries. Two agents collaborate iteratively, enabling sequential reasoning and adaptive attention to graph information. Unlike prior works, both agents are prompted only with the scene graph schema rather than the full graph data, which reduces the hallucination by limiting input tokens, and drives the Reasoner to generate reasoning trace abstractly.Following the trace, the Retriever programmatically query the scene graph data based on the schema understanding, allowing dynamic and global attention on the graph that enhances alignment between reasoning and retrieval. Through experiments in multiple simulation environments, we show that our framework surpasses existing LLM-based approaches in numerical Q\&A and planning tasks, and can benefit from task-level few-shot examples, even in the absence of agent-level demonstrations. Project code will be released.

**Link**: [arxiv](http://arxiv.org/abs/2502.03450v1),  [pdf](http://arxiv.org/pdf/2502.03450v1)

**Tags**: cs.LG cs.AI cs.MA cs.RO 



### Designing LLM-simulated Immersive Spaces to Enhance Autistic Children's   Social Affordances Understanding
**Authors**: Yancheng Cao, Yangyang HE, Yonglin Chen, Menghan Chen, Shanhe You, Yulin Qiu, Min Liu, Chuan Luo, Chen Zheng, Xin Tong, Jing Liang, Jiangtao Gong

**Updated**: 2025-02-05T18:45:38Z

**Summary**: One of the key challenges faced by autistic children is understanding social affordances in complex environments, which further impacts their ability to respond appropriately to social signals. In traffic scenarios, this impairment can even lead to safety concerns. In this paper, we introduce an LLM-simulated immersive projection environment designed to improve this ability in autistic children while ensuring their safety. We first propose 17 design considerations across four major categories, derived from a comprehensive review of previous research. Next, we developed a system called AIroad, which leverages LLMs to simulate drivers with varying social intents, expressed through explicit multimodal social signals. AIroad helps autistic children bridge the gap in recognizing the intentions behind behaviors and learning appropriate responses through various stimuli. A user study involving 14 participants demonstrated that this technology effectively engages autistic children and leads to significant improvements in their comprehension of social affordances in traffic scenarios. Additionally, parents reported high perceived usability of the system. These findings highlight the potential of combining LLM technology with immersive environments for the functional rehabilitation of autistic children in the future.

**Link**: [arxiv](http://arxiv.org/abs/2502.03447v1),  [pdf](http://arxiv.org/pdf/2502.03447v1)

**Tags**: cs.HC 



### S2-Attention: Hardware-Aware Context Sharding Among Attention Heads
**Authors**: Xihui Lin, Yunan Zhang, Suyu Ge, Liliang Ren, Barun Patra, Vishrav Chaudhary, Hao Peng, Xia Song

**Updated**: 2025-02-05T18:39:43Z

**Summary**: Sparse attention, which selectively attends to a subset of tokens in the context was supposed to be efficient. However, its theoretical reduction in FLOPs has rarely translated into wall-clock speed-up over its dense attention counterparts due to the lack of hardware-aware optimizations like FlashAttention. Meanwhile, it remains unclear whether sparse attention can maintain the model's quality at a scale of today's large language models (LLMs) and how. This paper presents Sparsely-Sharded(S2) Attention, a Triton library that provides kernel optimization for sparse attention customizable at both per-head and per-context-range levels. S2-Attention enables the exploration of novel and high-performance sparse attention techniques, which we demonstrate through extensive ablations across a wide range of sparse attention designs at various model scales. From these insights, we present several basic guidelines to design sparse attention that can achieve not only practical efficiency improvements, but also strong downstream performance. To achieve high parallelization and optimized memory IO, sparse attention should shard the context heterogeneously across attention heads, where each head attends to a different subset of tokens while collectively covering the full context. Meanwhile, we find hybrid architectures combining sparse and dense attention particularly beneficial in practice. S2-Attention achieves wall-clock speedup of 8.79X, 15.87X, 25.3X compared to the strong FlashAttention-2 baseline with strong downstream performance on-par with full attention and perfect retrieval performance at a 128k context length. At inference, for 7B models, our model, with the help of our S2-Attention kernel, achieves 4.5x speed-up compared to dense counterparts. S2-Attention is released with easy-to-customize APIs for direct usage in Megatron and vLLM.

**Link**: [arxiv](http://arxiv.org/abs/2407.17678v7),  [pdf](http://arxiv.org/pdf/2407.17678v7)

**Tags**: cs.CL 



### BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic   Theorem Proving
**Authors**: Ran Xin, Chenguang Xi, Jie Yang, Feng Chen, Hang Wu, Xia Xiao, Yifan Sun, Shen Zheng, Kai Shen

**Updated**: 2025-02-05T18:33:36Z

**Summary**: Recent advancements in large language models (LLMs) have spurred growing interest in automatic theorem proving using Lean4, where effective tree search methods are crucial for navigating proof search spaces. While the existing approaches primarily rely on value functions and Monte Carlo Tree Search (MCTS), the potential of simpler methods like Best-First Search (BFS) remains underexplored. This paper investigates whether BFS can achieve competitive performance in large-scale theorem proving tasks. We present \texttt{BFS-Prover}, a scalable expert iteration framework, featuring three key innovations. First, we implement strategic data filtering at each expert iteration round, excluding problems solvable via beam search node expansion to focus on harder cases. Second, we improve the sample efficiency of BFS through Direct Preference Optimization (DPO) applied to state-tactic pairs automatically annotated with compiler error feedback, refining the LLM's policy to prioritize productive expansions. Third, we employ length normalization in BFS to encourage exploration of deeper proof paths. \texttt{BFS-Prover} achieves a score of $71.31$ on the MiniF2F test set and therefore challenges the perceived necessity of complex tree search methods, demonstrating that BFS can achieve competitive performance when properly scaled.

**Link**: [arxiv](http://arxiv.org/abs/2502.03438v1),  [pdf](http://arxiv.org/pdf/2502.03438v1)

**Tags**: cs.AI 



### Harnessing Large Language Models for Curated Code Reviews
**Authors**: Oussama Ben Sghaier, Martin Weyssow, Houari Sahraoui

**Updated**: 2025-02-05T18:15:09Z

**Summary**: In code review, generating structured and relevant comments is crucial for identifying code issues and facilitating accurate code changes that ensure an efficient code review process. Well-crafted comments not only streamline the code review itself but are also essential for subsequent tasks like code refinement, where the code is modified to satisfy the input review comment. Although various AI-based approaches aimed to automate comment generation, their effectiveness remains limited by the quality of the training data. Existing code review datasets are often noisy and unrefined, posing limitations to the learning potential of AI models and hindering the automation process.   To address these challenges, we propose a curation pipeline designed to enhance the quality of the largest publicly available code review dataset. We begin by establishing an evaluation framework, incorporating specific criteria and categories to empirically study the initial quality of the dataset. Using a large language model (LLM)-driven approach, we then apply our curation pipeline to refine the dataset. A comparative analysis of the newly curated dataset, based on the same evaluation framework, demonstrates substantial improvements in the clarity and conciseness of the comments. Additionally, we assess the impact of the curated dataset on automating downstream tasks, specifically comment generation and code refinement. Our findings show that the curated dataset leads to enhanced model performance in generating more accurate comments. Curated comments are also more useful as they lead to more accurate code refinement.

**Link**: [arxiv](http://arxiv.org/abs/2502.03425v1),  [pdf](http://arxiv.org/pdf/2502.03425v1)

**Tags**: cs.SE 



### Calibrating Wireless AI via Meta-Learned Context-Dependent Conformal   Prediction
**Authors**: Seonghoon Yoo, Sangwoo Park, Petar Popovski, Joonhyuk Kang, Osvaldo Simeone

**Updated**: 2025-02-05T18:12:52Z

**Summary**: Modern software-defined networks, such as Open Radio Access Network (O-RAN) systems, rely on artificial intelligence (AI)-powered applications running on controllers interfaced with the radio access network. To ensure that these AI applications operate reliably at runtime, they must be properly calibrated before deployment. A promising and theoretically grounded approach to calibration is conformal prediction (CP), which enhances any AI model by transforming it into a provably reliable set predictor that provides error bars for estimates and decisions. CP requires calibration data that matches the distribution of the environment encountered during runtime. However, in practical scenarios, network controllers often have access only to data collected under different contexts -- such as varying traffic patterns and network conditions -- leading to a mismatch between the calibration and runtime distributions. This paper introduces a novel methodology to address this calibration-test distribution shift. The approach leverages meta-learning to develop a zero-shot estimator of distribution shifts, relying solely on contextual information. The proposed method, called meta-learned context-dependent weighted conformal prediction (ML-WCP), enables effective calibration of AI applications without requiring data from the current context. Additionally, it can incorporate data from multiple contexts to further enhance calibration reliability.

**Link**: [arxiv](http://arxiv.org/abs/2501.14566v3),  [pdf](http://arxiv.org/pdf/2501.14566v3)

**Tags**: eess.SP 



### Think or Step-by-Step? UnZIPping the Black Box in Zero-Shot Prompts
**Authors**: Nikta Gohari Sadr, Sangmitra Madhusudan, Ali Emami

**Updated**: 2025-02-05T18:04:29Z

**Summary**: Zero-shot prompting techniques have significantly improved the performance of Large Language Models (LLMs). However, we lack a clear understanding of why zero-shot prompts are so effective. For example, in the prompt "Let's think step-by-step," is "think" or "step-by-step" more crucial to its success? Existing interpretability methods, such as gradient-based and attention-based approaches, are computationally intensive and restricted to open-source models. We introduce the ZIP score (Zero-shot Importance of Perturbation score), a versatile metric applicable to both open and closed-source models, based on systematic input word perturbations. Our experiments across four recent LLMs, seven widely-used prompts, and several tasks, reveal interesting patterns in word importance. For instance, while both 'step-by-step' and 'think' show high ZIP scores, which one is more influential depends on the model and task. We validate our method using controlled experiments and compare our results with human judgments, finding that proprietary models align more closely with human intuition regarding word significance. These findings enhance our understanding of LLM behavior and contribute to developing more effective zero-shot prompts and improved model analysis.

**Link**: [arxiv](http://arxiv.org/abs/2502.03418v1),  [pdf](http://arxiv.org/pdf/2502.03418v1)

**Tags**: cs.CL 



### OverThink: Slowdown Attacks on Reasoning LLMs
**Authors**: Abhinav Kumar, Jaechul Roh, Ali Naseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr, Eugene Bagdasarian

**Updated**: 2025-02-05T17:58:46Z

**Summary**: We increase overhead for applications that rely on reasoning LLMs-we force models to spend an amplified number of reasoning tokens, i.e., "overthink", to respond to the user query while providing contextually correct answers. The adversary performs an OVERTHINK attack by injecting decoy reasoning problems into the public content that is used by the reasoning LLM (e.g., for RAG applications) during inference time. Due to the nature of our decoy problems (e.g., a Markov Decision Process), modified texts do not violate safety guardrails. We evaluated our attack across closed-(OpenAI o1, o1-mini, o3-mini) and open-(DeepSeek R1) weights reasoning models on the FreshQA and SQuAD datasets. Our results show up to 18x slowdown on FreshQA dataset and 46x slowdown on SQuAD dataset. The attack also shows high transferability across models. To protect applications, we discuss and implement defenses leveraging LLM-based and system design approaches. Finally, we discuss societal, financial, and energy impacts of OVERTHINK attack which could amplify the costs for third-party applications operating reasoning models.

**Link**: [arxiv](http://arxiv.org/abs/2502.02542v2),  [pdf](http://arxiv.org/pdf/2502.02542v2)

**Tags**: cs.LG cs.CR 



### Simple Is Effective: The Roles of Graphs and Large Language Models in   Knowledge-Graph-Based Retrieval-Augmented Generation
**Authors**: Mufei Li, Siqi Miao, Pan Li

**Updated**: 2025-02-05T17:45:24Z

**Summary**: Large Language Models (LLMs) demonstrate strong reasoning abilities but face limitations such as hallucinations and outdated knowledge. Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by grounding LLM outputs in structured external knowledge from KGs. However, current KG-based RAG frameworks still struggle to optimize the trade-off between retrieval effectiveness and efficiency in identifying a suitable amount of relevant graph information for the LLM to digest. We introduce SubgraphRAG, extending the KG-based RAG framework that retrieves subgraphs and leverages LLMs for reasoning and answer prediction. Our approach innovatively integrates a lightweight multilayer perceptron with a parallel triple-scoring mechanism for efficient and flexible subgraph retrieval while encoding directional structural distances to enhance retrieval effectiveness. The size of retrieved subgraphs can be flexibly adjusted to match the query's need and the downstream LLM's capabilities. This design strikes a balance between model complexity and reasoning power, enabling scalable and generalizable retrieval processes. Notably, based on our retrieved subgraphs, smaller LLMs like Llama3.1-8B-Instruct deliver competitive results with explainable reasoning, while larger models like GPT-4o achieve state-of-the-art accuracy compared with previous baselines -- all without fine-tuning. Extensive evaluations on the WebQSP and CWQ benchmarks highlight SubgraphRAG's strengths in efficiency, accuracy, and reliability by reducing hallucinations and improving response grounding.

**Link**: [arxiv](http://arxiv.org/abs/2410.20724v4),  [pdf](http://arxiv.org/pdf/2410.20724v4)

**Tags**: cs.CL cs.LG 



### ExploreSelf: Fostering User-driven Exploration and Reflection on   Personal Challenges with Adaptive Guidance by Large Language Models
**Authors**: Inhwa Song, SoHyun Park, Sachin R. Pendse, Jessica Lee Schleider, Munmun De Choudhury, Young-Ho Kim

**Updated**: 2025-02-05T17:41:42Z

**Summary**: Expressing stressful experiences in words is proven to improve mental and physical health, but individuals often disengage with writing interventions as they struggle to organize their thoughts and emotions. Reflective prompts have been used to provide direction, and large language models (LLMs) have demonstrated the potential to provide tailored guidance. However, current systems often limit users' flexibility to direct their reflections. We thus present ExploreSelf, an LLM-driven application designed to empower users to control their reflective journey, providing adaptive support through dynamically generated questions. Through an exploratory study with 19 participants, we examine how participants explore and reflect on personal challenges using ExploreSelf. Our findings demonstrate that participants valued the flexible navigation of adaptive guidance to control their reflective journey, leading to deeper engagement and insight. Building on our findings, we discuss the implications of designing LLM-driven tools that facilitate user-driven and effective reflection of personal challenges.

**Link**: [arxiv](http://arxiv.org/abs/2409.09662v3),  [pdf](http://arxiv.org/pdf/2409.09662v3)

**Tags**: cs.HC cs.AI cs.CL H.5.2; I.2.7 



### SPRI: Aligning Large Language Models with Context-Situated Principles
**Authors**: Hongli Zhan, Muneeza Azmat, Raya Horesh, Junyi Jessy Li, Mikhail Yurochkin

**Updated**: 2025-02-05T17:32:29Z

**Summary**: Aligning Large Language Models to integrate and reflect human values, especially for tasks that demand intricate human oversight, is arduous since it is resource-intensive and time-consuming to depend on human expertise for context-specific guidance. Prior work has utilized predefined sets of rules or principles to steer the behavior of models (Bai et al., 2022; Sun et al., 2023). However, these principles tend to be generic, making it challenging to adapt them to each individual input query or context. In this work, we present Situated-PRInciples (SPRI), a framework requiring minimal or no human effort that is designed to automatically generate guiding principles in real-time for each input query and utilize them to align each response. We evaluate SPRI on three tasks, and show that 1) SPRI can derive principles in a complex domain-specific task that leads to on-par performance as expert-crafted ones; 2) SPRI-generated principles lead to instance-specific rubrics that outperform prior LLM-as-a-judge frameworks; 3) using SPRI to generate synthetic SFT data leads to substantial improvement on truthfulness. We release our code and model generations at https://github.com/honglizhan/SPRI-public.

**Link**: [arxiv](http://arxiv.org/abs/2502.03397v1),  [pdf](http://arxiv.org/pdf/2502.03397v1)

**Tags**: cs.CL cs.AI 



### Benchmarking Time Series Forecasting Models: From Statistical Techniques   to Foundation Models in Real-World Applications
**Authors**: Issar Arab, Rodrigo Benitez

**Updated**: 2025-02-05T17:30:31Z

**Summary**: Time series forecasting is essential for operational intelligence in the hospitality industry, and particularly challenging in large-scale, distributed systems. This study evaluates the performance of statistical, machine learning (ML), deep learning, and foundation models in forecasting hourly sales over a 14-day horizon using real-world data from a network of thousands of restaurants across Germany. The forecasting solution includes features such as weather conditions, calendar events, and time-of-day patterns. Results demonstrate the strong performance of ML-based meta-models and highlight the emerging potential of foundation models like Chronos and TimesFM, which deliver competitive performance with minimal feature engineering, leveraging only the pre-trained model (zero-shot inference). Additionally, a hybrid PySpark-Pandas approach proves to be a robust solution for achieving horizontal scalability in large-scale deployments.

**Link**: [arxiv](http://arxiv.org/abs/2502.03395v1),  [pdf](http://arxiv.org/pdf/2502.03395v1)

**Tags**: cs.LG cs.AI 



### CITER: Collaborative Inference for Efficient Large Language Model   Decoding with Token-Level Routing
**Authors**: Wenhao Zheng, Yixiao Chen, Weitong Zhang, Souvik Kundu, Yun Li, Zhengzhong Liu, Eric P. Xing, Hongyi Wang, Huaxiu Yao

**Updated**: 2025-02-05T17:26:35Z

**Summary**: Large language models have achieved remarkable success in various tasks but suffer from high computational costs during inference, limiting their deployment in resource-constrained applications. To address this issue, we propose a novel CITER (\textbf{C}ollaborative \textbf{I}nference with \textbf{T}oken-l\textbf{E}vel \textbf{R}outing) framework that enables efficient collaboration between small and large language models (SLMs & LLMs) through a token-level routing strategy. Specifically, CITER routes non-critical tokens to an SLM for efficiency and routes critical tokens to an LLM for generalization quality. We formulate router training as a policy optimization, where the router receives rewards based on both the quality of predictions and the inference costs of generation. This allows the router to learn to predict token-level routing scores and make routing decisions based on both the current token and the future impact of its decisions. To further accelerate the reward evaluation process, we introduce a shortcut which significantly reduces the costs of the reward estimation and improving the practicality of our approach. Extensive experiments on five benchmark datasets demonstrate that CITER reduces the inference costs while preserving high-quality generation, offering a promising solution for real-time and resource-constrained applications.

**Link**: [arxiv](http://arxiv.org/abs/2502.01976v2),  [pdf](http://arxiv.org/pdf/2502.01976v2)

**Tags**: cs.CL cs.AI cs.LG cs.PF 



### High-Fidelity Simultaneous Speech-To-Speech Translation
**Authors**: Tom Labiausse, Laurent Mazaré, Edouard Grave, Patrick Pérez, Alexandre Défossez, Neil Zeghidour

**Updated**: 2025-02-05T17:18:55Z

**Summary**: We introduce Hibiki, a decoder-only model for simultaneous speech translation. Hibiki leverages a multistream language model to synchronously process source and target speech, and jointly produces text and audio tokens to perform speech-to-text and speech-to-speech translation. We furthermore address the fundamental challenge of simultaneous interpretation, which unlike its consecutive counterpart, where one waits for the end of the source utterance to start translating, adapts its flow to accumulate just enough context to produce a correct translation in real-time, chunk by chunk. To do so, we introduce a weakly-supervised method that leverages the perplexity of an off-the-shelf text translation system to identify optimal delays on a per-word basis and create aligned synthetic data. After supervised training, Hibiki performs adaptive, simultaneous speech translation with vanilla temperature sampling. On a French-English simultaneous speech translation task, Hibiki demonstrates state-of-the-art performance in translation quality, speaker fidelity and naturalness. Moreover, the simplicity of its inference process makes it compatible with batched translation and even real-time on-device deployment. We provide examples as well as models and inference code.

**Link**: [arxiv](http://arxiv.org/abs/2502.03382v1),  [pdf](http://arxiv.org/pdf/2502.03382v1)

**Tags**: cs.CL cs.SD eess.AS 



### Learning to Identify Conflicts in RPKI
**Authors**: Haya Schulmann, Shujie Zhao

**Updated**: 2025-02-05T17:16:44Z

**Summary**: The long history of misconfigurations and errors in RPKI indicates that they cannot be easily avoided and will most probably persist also in the future. These errors create conflicts between BGP announcements and their covering ROAs, causing the RPKI validation to result in status invalid. Networks that enforce RPKI filtering with Route Origin Validation (ROV) would block such conflicting BGP announcements and as a result lose traffic from the corresponding origins. Since the business incentives of networks are tightly coupled with the traffic they relay, filtering legitimate traffic leads to a loss of revenue, reducing the motivation to filter invalid announcements with ROV.   In this work, we introduce a new mechanism, LOV, designed for whitelisting benign conflicts on an Internet scale. The resulting whitelist is made available to RPKI supporting ASes to avoid filtering RPKI-invalid but benign routes. Saving legitimate traffic resolves one main obstacle towards RPKI deployment. We measure live BGP updates using LOV during a period of half a year and whitelist 52,846 routes with benign origin errors.

**Link**: [arxiv](http://arxiv.org/abs/2502.03378v1),  [pdf](http://arxiv.org/pdf/2502.03378v1)

**Tags**: cs.CR 



### Energy-Efficient Flying LoRa Gateways: A Multi-Agent Reinforcement   Learning Approach
**Authors**: Abdullahi Isa Ahmed, El Mehdi Amhoud

**Updated**: 2025-02-05T17:16:40Z

**Summary**: With the rapid development of next-generation Internet of Things (NG-IoT) networks, the increasing number of connected devices has led to a surge in power consumption. This rise in energy demand poses significant challenges to resource availability and raises sustainability concerns for large-scale IoT deployments. Efficient energy utilization in communication networks, particularly for power-constrained IoT devices, has thus become a critical area of research. In this paper, we deployed flying LoRa gateways (GWs) mounted on unmanned aerial vehicles (UAVs) to collect data from LoRa end devices (EDs) and transmit it to a central server. Our primary objective is to maximize the global system energy efficiency (EE) of wireless LoRa networks by joint optimization of transmission power (TP), spreading factor (SF), bandwidth (W), and ED association. To solve this challenging problem, we model the problem as a partially observable Markov decision process (POMDP), where each flying LoRa GW acts as a learning agent using a cooperative Multi-Agent Reinforcement Learning (MARL) approach under centralized training and decentralized execution (CTDE). Simulation results demonstrate that our proposed method, based on the multi-agent proximal policy optimization (MAPPO) algorithm, significantly improves the global system EE and surpasses the conventional MARL schemes.

**Link**: [arxiv](http://arxiv.org/abs/2502.03377v1),  [pdf](http://arxiv.org/pdf/2502.03377v1)

**Tags**: cs.NI cs.LG 



### Demystifying Long Chain-of-Thought Reasoning in LLMs
**Authors**: Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, Xiang Yue

**Updated**: 2025-02-05T17:13:32Z

**Summary**: Scaling inference compute enhances reasoning in large language models (LLMs), with long chains-of-thought (CoTs) enabling strategies like backtracking and error correction. Reinforcement learning (RL) has emerged as a crucial method for developing these capabilities, yet the conditions under which long CoTs emerge remain unclear, and RL training requires careful design choices. In this study, we systematically investigate the mechanics of long CoT reasoning, identifying the key factors that enable models to generate long CoT trajectories. Through extensive supervised fine-tuning (SFT) and RL experiments, we present four main findings: (1) While SFT is not strictly necessary, it simplifies training and improves efficiency; (2) Reasoning capabilities tend to emerge with increased training compute, but their development is not guaranteed, making reward shaping crucial for stabilizing CoT length growth; (3) Scaling verifiable reward signals is critical for RL. We find that leveraging noisy, web-extracted solutions with filtering mechanisms shows strong potential, particularly for out-of-distribution (OOD) tasks such as STEM reasoning; and (4) Core abilities like error correction are inherently present in base models, but incentivizing these skills effectively for complex tasks via RL demands significant compute, and measuring their emergence requires a nuanced approach. These insights provide practical guidance for optimizing training strategies to enhance long CoT reasoning in LLMs. Our code is available at: https://github.com/eddycmu/demystify-long-cot.

**Link**: [arxiv](http://arxiv.org/abs/2502.03373v1),  [pdf](http://arxiv.org/pdf/2502.03373v1)

**Tags**: cs.CL cs.LG 



### Agent-OM: Leveraging LLM Agents for Ontology Matching
**Authors**: Zhangcheng Qiang, Weiqing Wang, Kerry Taylor

**Updated**: 2025-02-05T17:08:17Z

**Summary**: Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM (Agent for Ontology Matching), consisting of two Siamese agents for retrieval and matching, with a set of OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM systems show that our system can achieve results very close to the long-standing best performance on simple OM tasks and can significantly improve the performance on complex and few-shot OM tasks.

**Link**: [arxiv](http://arxiv.org/abs/2312.00326v8),  [pdf](http://arxiv.org/pdf/2312.00326v8)

**Tags**: cs.AI cs.CL cs.IR 



### PalimpChat: Declarative and Interactive AI analytics
**Authors**: Chunwei Liu, Gerardo Vitagliano, Brandon Rose, Matt Prinz, David Andrew Samson, Michael Cafarella

**Updated**: 2025-02-05T17:06:59Z

**Summary**: Thanks to the advances in generative architectures and large language models, data scientists can now code pipelines of machine-learning operations to process large collections of unstructured data. Recent progress has seen the rise of declarative AI frameworks (e.g., Palimpzest, Lotus, and DocETL) to build optimized and increasingly complex pipelines, but these systems often remain accessible only to expert programmers. In this demonstration, we present PalimpChat, a chat-based interface to Palimpzest that bridges this gap by letting users create and run sophisticated AI pipelines through natural language alone. By integrating Archytas, a ReAct-based reasoning agent, and Palimpzest's suite of relational and LLM-based operators, PalimpChat provides a practical illustration of how a chat interface can make declarative AI frameworks truly accessible to non-experts.   Our demo system is publicly available online. At SIGMOD'25, participants can explore three real-world scenarios--scientific discovery, legal discovery, and real estate search--or apply PalimpChat to their own datasets. In this paper, we focus on how PalimpChat, supported by the Palimpzest optimizer, simplifies complex AI workflows such as extracting and analyzing biomedical data.

**Link**: [arxiv](http://arxiv.org/abs/2502.03368v1),  [pdf](http://arxiv.org/pdf/2502.03368v1)

**Tags**: cs.AI cs.DB cs.IR 



### Distilling Implicit Multimodal Knowledge into Large Language Models for   Zero-Resource Dialogue Generation
**Authors**: Bo Zhang, Hui Ma, Jian Ding, Jian Wang, Bo Xu, Hongfei Lin

**Updated**: 2025-02-05T16:54:42Z

**Summary**: Integrating multimodal knowledge into large language models (LLMs) represents a significant advancement in dialogue generation capabilities. However, the effective incorporation of such knowledge in zero-resource scenarios remains a substantial challenge due to the scarcity of diverse, high-quality dialogue datasets. To address this, we propose the Visual Implicit Knowledge Distillation Framework (VIKDF), an innovative approach aimed at enhancing LLMs for enriched dialogue generation in zero-resource contexts by leveraging implicit multimodal knowledge. VIKDF comprises two main stages: knowledge distillation, using an Implicit Query Transformer to extract and encode visual implicit knowledge from image-text pairs into knowledge vectors; and knowledge integration, employing a novel Bidirectional Variational Information Fusion technique to seamlessly integrate these distilled vectors into LLMs. This enables the LLMs to generate dialogues that are not only coherent and engaging but also exhibit a deep understanding of the context through implicit multimodal cues, effectively overcoming the limitations of zero-resource scenarios. Our extensive experimentation across two dialogue datasets shows that VIKDF outperforms existing state-of-the-art models in generating high-quality dialogues. The code is available at https://github.com/zhangbo-nlp/VIKDF.

**Link**: [arxiv](http://arxiv.org/abs/2405.10121v2),  [pdf](http://arxiv.org/pdf/2405.10121v2)

**Tags**: cs.CL cs.MM 



### Minerva: A Programmable Memory Test Benchmark for Language Models
**Authors**: Menglin Xia, Victor Ruehle, Saravan Rajmohan, Reza Shokri

**Updated**: 2025-02-05T16:53:45Z

**Summary**: How effectively can LLM-based AI assistants utilize their memory (context) to perform various tasks? Traditional data benchmarks, which are often manually crafted, suffer from several limitations: they are static, susceptible to overfitting, difficult to interpret, and lack actionable insights--failing to pinpoint the specific capabilities a model lacks when it does not pass a test. In this paper, we present a framework for automatically generating a comprehensive set of tests to evaluate models' abilities to use their memory effectively. Our framework extends the range of capability tests beyond the commonly explored (passkey, key-value, needle in the haystack) search, a dominant focus in the literature. Specifically, we evaluate models on atomic tasks such as searching, recalling, editing, matching, comparing information in context memory, and performing basic operations when inputs are structured into distinct blocks, simulating real-world data. Additionally, we design composite tests to investigate the models' ability to maintain state while operating on memory. Our benchmark enables an interpretable, detailed assessment of memory capabilities of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2502.03358v1),  [pdf](http://arxiv.org/pdf/2502.03358v1)

**Tags**: cs.CL 



### Prepending or Cross-Attention for Speech-to-Text? An Empirical   Comparison
**Authors**: Tsz Kin Lam, Marco Gaido, Sara Papi, Luisa Bentivogli, Barry Haddow

**Updated**: 2025-02-05T16:40:51Z

**Summary**: Following the remarkable success of Large Language Models (LLMs) in NLP tasks, there is increasing interest in extending their capabilities to speech -- the most common form of communication. The most widespread approach to integrating speech into LLMs is dense feature prepending (DFP), which prepends the projected speech representations to the textual representations, allowing end-to-end training with a speech encoder. This raises questions about the need for a sophisticated speech encoder for DFP and how its performance compares with a standard encoder-decoder (i.e., cross-attention) architecture. We compare DFP and cross-attention under a variety of configurations, such as CTC compression, sequence-level knowledge distillation, on monolingual, bilingual, and multilingual models. To perform a controlled architectural comparison, we train all models from scratch rather than using large pretrained models and use comparable data and parameter settings, testing speech-to-text recognition (ASR) and translation (ST) on MuST-C v1.0 and CoVoST2 datasets. Despite the wide adoption of DFP, our results do not indicate a clear advantage of DFP over cross-attention.

**Link**: [arxiv](http://arxiv.org/abs/2501.02370v2),  [pdf](http://arxiv.org/pdf/2501.02370v2)

**Tags**: cs.CL cs.SD eess.AS 



### ECM: A Unified Electronic Circuit Model for Explaining the Emergence of   In-Context Learning and Chain-of-Thought in Large Language Model
**Authors**: Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiaqi Wang, Mengkang Hu, Zhi Chen, Wanxiang Che, Ting Liu

**Updated**: 2025-02-05T16:22:33Z

**Summary**: Recent advancements in large language models (LLMs) have led to significant successes across various applications, where the most noticeable is to a series of emerging capabilities, particularly in the areas of In-Context Learning (ICL) and Chain-of-Thought (CoT). To better understand and control model performance, many studies have begun investigating the underlying causes of these phenomena and their impact on task outcomes. However, existing explanatory frameworks predominantly focus on isolating and explaining ICL and CoT independently, leading to an incomplete understanding of their combined influence on model performance. To address this gap, we propose the Electronic Circuit Model (ECM), which provides a foundation for developing scalable, learnable policies and improving the management of AI-generated content. Specifically, ECM conceptualizes model behavior as an electronic circuit: ICL is represented as semantic magnetic field to providing an additional voltage following Faraday's Law, while CoT is modeled as series resistors to constrain the model output performance following Ohm's Law. Experimental results demonstrate that the ECM effectively predicts and explains LLM performance across a variety of prompting strategies. Furthermore, we apply ECM to advanced reasoning strategy optimization on a series of tasks, such as the International Olympiad in Informatics (IOI) and the International Mathematical Olympiad (IMO), achieving competitive performance that surpasses nearly 80% of top human competitors.

**Link**: [arxiv](http://arxiv.org/abs/2502.03325v1),  [pdf](http://arxiv.org/pdf/2502.03325v1)

**Tags**: cs.CL cs.AI 



### Out-of-Distribution Detection using Synthetic Data Generation
**Authors**: Momin Abbas, Muneeza Azmat, Raya Horesh, Mikhail Yurochkin

**Updated**: 2025-02-05T16:22:09Z

**Summary**: Distinguishing in- and out-of-distribution (OOD) inputs is crucial for reliable deployment of classification systems. However, OOD data is typically unavailable or difficult to collect, posing a significant challenge for accurate OOD detection. In this work, we present a method that harnesses the generative capabilities of Large Language Models (LLMs) to create high-quality synthetic OOD proxies, eliminating the dependency on any external OOD data source. We study the efficacy of our method on classical text classification tasks such as toxicity detection and sentiment classification as well as classification tasks arising in LLM development and deployment, such as training a reward model for RLHF and detecting misaligned generations. Extensive experiments on nine InD-OOD dataset pairs and various model sizes show that our approach dramatically lowers false positive rates (achieving a perfect zero in some cases) while maintaining high accuracy on in-distribution tasks, outperforming baseline methods by a significant margin.

**Link**: [arxiv](http://arxiv.org/abs/2502.03323v1),  [pdf](http://arxiv.org/pdf/2502.03323v1)

**Tags**: cs.CL cs.AI cs.LG 



### Conversation Routines: A Prompt Engineering Framework for Task-Oriented   Dialog Systems
**Authors**: Giorgio Robino

**Updated**: 2025-02-05T16:21:05Z

**Summary**: This study introduces Conversation Routines (CR), a structured prompt engineering framework for developing task-oriented dialog systems using Large Language Models (LLMs). While LLMs demonstrate remarkable natural language understanding capabilities, engineering them to reliably execute complex business workflows remains challenging. The proposed CR framework enables the development of Conversation Agentic Systems (CAS) through natural language specifications, embedding task-oriented logic within LLM prompts. This approach provides a systematic methodology for designing and implementing complex conversational workflows while maintaining behavioral consistency. We demonstrate the framework's effectiveness through two proof-of-concept implementations: a Train Ticket Booking System and an Interactive Troubleshooting Copilot. These case studies validate CR's capability to encode sophisticated behavioral patterns and decision logic while preserving natural conversational flexibility. Results show that CR enables domain experts to design conversational workflows in natural language while leveraging custom functions (tools) developed by software engineers, creating an efficient division of responsibilities where developers focus on core API implementation and domain experts handle conversation design. While the framework shows promise in accessibility and adaptability, we identify key challenges including computational overhead, non-deterministic behavior, and domain-specific logic optimization. Future research directions include CR evaluation methods based on prompt engineering frameworks driven by goal-oriented grading criteria, improving scalability for complex multi-agent interactions, and enhancing system robustness to address the identified limitations across diverse business applications.

**Link**: [arxiv](http://arxiv.org/abs/2501.11613v3),  [pdf](http://arxiv.org/pdf/2501.11613v3)

**Tags**: cs.CL cs.AI cs.ET cs.HC cs.PL 



### A Systematic Literature Review on Explainability for Machine/Deep   Learning-based Software Engineering Research
**Authors**: Sicong Cao, Xiaobing Sun, Ratnadira Widyasari, David Lo, Xiaoxue Wu, Lili Bo, Jiale Zhang, Bin Li, Wei Liu, Di Wu, Yixin Chen

**Updated**: 2025-02-05T16:10:05Z

**Summary**: The remarkable achievements of Artificial Intelligence (AI) algorithms, particularly in Machine Learning (ML) and Deep Learning (DL), have fueled their extensive deployment across multiple sectors, including Software Engineering (SE). However, due to their black-box nature, these promising AI-driven SE models are still far from being deployed in practice. This lack of explainability poses unwanted risks for their applications in critical tasks, such as vulnerability detection, where decision-making transparency is of paramount importance. This paper endeavors to elucidate this interdisciplinary domain by presenting a systematic literature review of approaches that aim to improve the explainability of AI models within the context of SE. The review canvasses work appearing in the most prominent SE & AI conferences and journals, and spans 108 papers across 23 unique SE tasks. Based on three key Research Questions (RQs), we aim to (1) summarize the SE tasks where XAI techniques have shown success to date; (2) classify and analyze different XAI techniques; and (3) investigate existing evaluation approaches. Based on our findings, we identified a set of challenges remaining to be addressed in existing studies, together with a set of guidelines highlighting potential opportunities we deemed appropriate and important for future work.

**Link**: [arxiv](http://arxiv.org/abs/2401.14617v2),  [pdf](http://arxiv.org/pdf/2401.14617v2)

**Tags**: cs.SE cs.AI 



### Intent Representation Learning with Large Language Model for   Recommendation
**Authors**: Yu Wang, Lei Sang, Yi Zhang, Yiwen Zhang

**Updated**: 2025-02-05T16:08:05Z

**Summary**: Intent-based recommender systems have garnered significant attention for uncovering latent fine-grained preferences. Intents, as underlying factors of interactions, are crucial for improving recommendation interpretability. Most methods define intents as learnable parameters updated alongside interactions. However, existing frameworks often overlook textual information (e.g., user reviews, item descriptions), which is crucial for alleviating the sparsity of interaction intents. Exploring these multimodal intents, especially the inherent differences in representation spaces, poses two key challenges: i) How to align multimodal intents and effectively mitigate noise issues; ii) How to extract and match latent key intents across modalities. To tackle these challenges, we propose a model-agnostic framework, Intent Representation Learning with Large Language Model (IRLLRec), which leverages large language models (LLMs) to construct multimodal intents and enhance recommendations. Specifically, IRLLRec employs a dual-tower architecture to learn multimodal intent representations. Next, we propose pairwise and translation alignment to eliminate inter-modal differences and enhance robustness against noisy input features. Finally, to better match textual and interaction-based intents, we employ momentum distillation to perform teacher-student learning on fused intent representations. Empirical evaluations on three datasets show that our IRLLRec framework outperforms baselines. The implementation is available at https://github.com/wangyu0627/IRLLRec.

**Link**: [arxiv](http://arxiv.org/abs/2502.03307v1),  [pdf](http://arxiv.org/pdf/2502.03307v1)

**Tags**: cs.IR 



### Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient   Zeroth-order LLM Fine-tuning
**Authors**: Qitao Tan, Jun Liu, Zheng Zhan, Caiwei Ding, Yanzhi Wang, Jin Lu, Geng Yuan

**Updated**: 2025-02-05T16:03:17Z

**Summary**: Large language models (LLMs) excel across various tasks, but standard first-order (FO) fine-tuning demands considerable memory, significantly limiting real-world deployment. Recently, zeroth-order (ZO) optimization stood out as a promising memory-efficient training paradigm, avoiding backward passes and relying solely on forward passes for gradient estimation, making it attractive for resource-constrained scenarios. However, ZO method lags far behind FO method in both convergence speed and accuracy. To bridge the gap, we introduce a novel layer-wise divergence analysis that uncovers the distinct update pattern of FO and ZO optimization. Aiming to resemble the learning capacity of FO method from the findings, we propose \textbf{Di}vergence-driven \textbf{Z}eroth-\textbf{O}rder (\textbf{DiZO}) optimization. DiZO conducts divergence-driven layer adaptation by incorporating projections to ZO updates, generating diverse-magnitude updates precisely scaled to layer-wise individual optimization needs. Our results demonstrate that DiZO significantly reduces the needed iterations for convergence without sacrificing throughput, cutting training GPU hours by up to 48\% on various datasets. Moreover, DiZO consistently outperforms the representative ZO baselines in fine-tuning RoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some cases, even surpasses memory-intensive FO fine-tuning.

**Link**: [arxiv](http://arxiv.org/abs/2502.03304v1),  [pdf](http://arxiv.org/pdf/2502.03304v1)

**Tags**: cs.LG cs.AI cs.CL 



### MeDiSumQA: Patient-Oriented Question-Answer Generation from Discharge   Letters
**Authors**: Amin Dada, Osman Alperen Koras, Marie Bauer, Amanda Butler, Kaleb E. Smith, Jens Kleesiek, Julian Friedrich

**Updated**: 2025-02-05T15:56:37Z

**Summary**: While increasing patients' access to medical documents improves medical care, this benefit is limited by varying health literacy levels and complex medical terminology. Large language models (LLMs) offer solutions by simplifying medical information. However, evaluating LLMs for safe and patient-friendly text generation is difficult due to the lack of standardized evaluation resources. To fill this gap, we developed MeDiSumQA. MeDiSumQA is a dataset created from MIMIC-IV discharge summaries through an automated pipeline combining LLM-based question-answer generation with manual quality checks. We use this dataset to evaluate various LLMs on patient-oriented question-answering. Our findings reveal that general-purpose LLMs frequently surpass biomedical-adapted models, while automated metrics correlate with human judgment. By releasing MeDiSumQA on PhysioNet, we aim to advance the development of LLMs to enhance patient understanding and ultimately improve care outcomes.

**Link**: [arxiv](http://arxiv.org/abs/2502.03298v1),  [pdf](http://arxiv.org/pdf/2502.03298v1)

**Tags**: cs.CL cs.AI cs.LG 



### Context in Public Health for Underserved Communities: A Bayesian   Approach to Online Restless Bandits
**Authors**: Biyonka Liang, Lily Xu, Aparna Taneja, Milind Tambe, Lucas Janson

**Updated**: 2025-02-05T15:41:15Z

**Summary**: Public health programs often provide interventions to encourage program adherence, and effectively allocating interventions is vital for producing the greatest overall health outcomes, especially in underserved communities where resources are limited. Such resource allocation problems are often modeled as restless multi-armed bandits (RMABs) with unknown underlying transition dynamics, hence requiring online reinforcement learning (RL). We present Bayesian Learning for Contextual RMABs (BCoR), an online RL approach for RMABs that novelly combines techniques in Bayesian modeling with Thompson sampling to flexibly model the complex RMAB settings present in public health program adherence problems, namely context and non-stationarity. BCoR's key strength is the ability to leverage shared information within and between arms to learn the unknown RMAB transition dynamics quickly in intervention-scarce settings with relatively short time horizons, which is common in public health applications. Empirically, BCoR achieves substantially higher finite-sample performance over a range of experimental settings, including a setting using real-world adherence data that was developed in collaboration with ARMMAN, an NGO in India which runs a large-scale maternal mHealth program, showcasing BCoR practical utility and potential for real-world deployment.

**Link**: [arxiv](http://arxiv.org/abs/2402.04933v3),  [pdf](http://arxiv.org/pdf/2402.04933v3)

**Tags**: cs.LG stat.AP 



### SymAgent: A Neural-Symbolic Self-Learning Agent Framework for Complex   Reasoning over Knowledge Graphs
**Authors**: Ben Liu, Jihai Zhang, Fangquan Lin, Cheng Yang, Min Peng, Wotao Yin

**Updated**: 2025-02-05T15:37:05Z

**Summary**: Recent advancements have highlighted that Large Language Models (LLMs) are prone to hallucinations when solving complex reasoning problems, leading to erroneous results. To tackle this issue, researchers incorporate Knowledge Graphs (KGs) to improve the reasoning ability of LLMs. However, existing methods face two limitations: 1) they typically assume that all answers to the questions are contained in KGs, neglecting the incompleteness issue of KGs, and 2) they treat the KG as a static repository and overlook the implicit logical reasoning structures inherent in KGs. In this paper, we introduce SymAgent, an innovative neural-symbolic agent framework that achieves collaborative augmentation between KGs and LLMs. We conceptualize KGs as dynamic environments and transform complex reasoning tasks into a multi-step interactive process, enabling KGs to participate deeply in the reasoning process. SymAgent consists of two modules: Agent-Planner and Agent-Executor. The Agent-Planner leverages LLM's inductive reasoning capability to extract symbolic rules from KGs, guiding efficient question decomposition. The Agent-Executor autonomously invokes predefined action tools to integrate information from KGs and external documents, addressing the issues of KG incompleteness. Furthermore, we design a self-learning framework comprising online exploration and offline iterative policy updating phases, enabling the agent to automatically synthesize reasoning trajectories and improve performance. Experimental results demonstrate that SymAgent with weak LLM backbones (i.e., 7B series) yields better or comparable performance compared to various strong baselines. Further analysis reveals that our agent can identify missing triples, facilitating automatic KG updates.

**Link**: [arxiv](http://arxiv.org/abs/2502.03283v1),  [pdf](http://arxiv.org/pdf/2502.03283v1)

**Tags**: cs.AI cs.CL cs.LG 



### Token Assorted: Mixing Latent and Text Tokens for Improved Language   Model Reasoning
**Authors**: DiJia Su, Hanlin Zhu, Yingchen Xu, Jiantao Jiao, Yuandong Tian, Qinqing Zheng

**Updated**: 2025-02-05T15:33:00Z

**Summary**: Large Language Models (LLMs) excel at reasoning and planning when trained on chainof-thought (CoT) data, where the step-by-step thought process is explicitly outlined by text tokens. However, this results in lengthy inputs where many words support textual coherence rather than core reasoning information, and processing these inputs consumes substantial computation resources. In this work, we propose a hybrid representation of the reasoning process, where we partially abstract away the initial reasoning steps using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces. We explore the use of latent trace abstractions in two scenarios: 1) training the model from scratch for the Keys-Finding Maze problem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary including unseen latent tokens, for both logical and mathematical reasoning problems. To facilitate effective learning, we introduce a simple training procedure that randomly mixes latent and text tokens, which enables fast adaptation to new latent tokens. Our approach consistently outperforms the baselines methods in various benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2502.03275v1),  [pdf](http://arxiv.org/pdf/2502.03275v1)

**Tags**: cs.CL cs.AI cs.LG cs.LO 



### A Scalable Approach to Probabilistic Neuro-Symbolic Verification
**Authors**: Vasileios Manginas, Nikolaos Manginas, Edward Stevinson, Sherwin Varghese, Nikos Katzouris, Georgios Paliouras, Alessio Lomuscio

**Updated**: 2025-02-05T15:29:41Z

**Summary**: Neuro-Symbolic Artificial Intelligence (NeSy AI) has emerged as a promising direction for integrating neural learning with symbolic reasoning. In the probabilistic variant of such systems, a neural network first extracts a set of symbols from sub-symbolic input, which are then used by a symbolic component to reason in a probabilistic manner towards answering a query. In this work, we address the problem of formally verifying the robustness of such NeSy probabilistic reasoning systems, therefore paving the way for their safe deployment in critical domains. We analyze the complexity of solving this problem exactly, and show that it is $\mathrm{NP}^{\# \mathrm{P}}$-hard. To overcome this issue, we propose the first approach for approximate, relaxation-based verification of probabilistic NeSy systems. We demonstrate experimentally that the proposed method scales exponentially better than solver-based solutions and apply our technique to a real-world autonomous driving dataset, where we verify a safety property under large input dimensionalities and network sizes.

**Link**: [arxiv](http://arxiv.org/abs/2502.03274v1),  [pdf](http://arxiv.org/pdf/2502.03274v1)

**Tags**: cs.AI 



### The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically   Justify Replacing Human Annotators with LLMs
**Authors**: Nitay Calderon, Roi Reichart, Rotem Dror

**Updated**: 2025-02-05T15:24:26Z

**Summary**: The "LLM-as-a-judge" paradigm employs Large Language Models (LLMs) as annotators and evaluators in tasks traditionally performed by humans. LLM annotations are widely used, not only in NLP research but also in fields like medicine, psychology, and social science. Despite their role in shaping study results and insights, there is no standard or rigorous procedure to determine whether LLMs can replace human annotators. In this paper, we propose a novel statistical procedure -- the Alternative Annotator Test (alt-test) -- that requires only a modest subset of annotated examples to justify using LLM annotations. Additionally, we introduce a versatile and interpretable measure for comparing LLM judges. To demonstrate our procedure, we curated a diverse collection of ten datasets, consisting of language and vision-language tasks, and conducted experiments with six LLMs and four prompting techniques. Our results show that LLMs can sometimes replace humans with closed-source LLMs (such as GPT-4o), outperforming open-source LLMs, and that prompting techniques yield judges of varying quality. We hope this study encourages more rigorous and reliable practices.

**Link**: [arxiv](http://arxiv.org/abs/2501.10970v2),  [pdf](http://arxiv.org/pdf/2501.10970v2)

**Tags**: cs.CL cs.AI cs.HC 



### CARROT: A Cost Aware Rate Optimal Router
**Authors**: Seamus Somerstep, Felipe Maia Polo, Allysson Flavio Melo de Oliveira, Prattyush Mangal, Mírian Silva, Onkar Bhardwaj, Mikhail Yurochkin, Subha Maity

**Updated**: 2025-02-05T15:17:25Z

**Summary**: With the rapid growth in the number of Large Language Models (LLMs), there has been a recent interest in LLM routing, or directing queries to the cheapest LLM that can deliver a suitable response. Following this line of work, we introduce CARROT, a Cost AwaRe Rate Optimal rouTer that can select models based on any desired trade-off between performance and cost. Given a query, CARROT selects a model based on estimates of models' cost and performance. Its simplicity lends CARROT computational efficiency, while our theoretical analysis demonstrates minimax rate-optimality in its routing performance. Alongside CARROT, we also introduce the Smart Price-aware Routing (SPROUT) dataset to facilitate routing on a wide spectrum of queries with the latest state-of-the-art LLMs. Using SPROUT and prior benchmarks such as Routerbench and open-LLM-leaderboard-v2 we empirically validate CARROT's performance against several alternative routers.

**Link**: [arxiv](http://arxiv.org/abs/2502.03261v1),  [pdf](http://arxiv.org/pdf/2502.03261v1)

**Tags**: stat.ML cs.LG cs.NI math.ST stat.TH 



### PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs
**Authors**: Max Zimmer, Megi Andoni, Christoph Spiegel, Sebastian Pokutta

**Updated**: 2025-02-05T15:10:23Z

**Summary**: Neural Networks can be effectively compressed through pruning, significantly reducing storage and compute demands while maintaining predictive performance. Simple yet effective methods like magnitude pruning remove less important parameters and typically require a costly retraining procedure to restore performance. However, with the rise of LLMs, full retraining has become infeasible due to memory and compute constraints. This study challenges the practice of retraining all parameters by showing that updating a small subset of highly expressive parameters can suffice to recover or even enhance performance after pruning. Surprisingly, retraining just 0.01%-0.05% of the parameters in GPT-architectures can match the performance of full retraining across various sparsity levels, significantly reducing compute and memory requirements, and enabling retraining of models with up to 30 billion parameters on a single GPU in minutes. To bridge the gap to full retraining in the high sparsity regime, we introduce two novel LoRA variants that, unlike standard LoRA, allow merging adapters back without compromising sparsity. Going a step further, we show that these methods can be applied for memory-efficient layer-wise reconstruction, significantly enhancing state-of-the-art retraining-free methods like Wanda (Sun et al., 2023) and SparseGPT (Frantar & Alistarh, 2023). Our findings present a promising alternative to avoiding retraining.

**Link**: [arxiv](http://arxiv.org/abs/2312.15230v3),  [pdf](http://arxiv.org/pdf/2312.15230v3)

**Tags**: cs.LG cs.AI 



### How do Humans and Language Models Reason About Creativity? A Comparative   Analysis
**Authors**: Antonio Laverghetta Jr., Tuhin Chakrabarty, Tom Hope, Jimmy Pronchick, Krupa Bhawsar, Roger E. Beaty

**Updated**: 2025-02-05T15:08:43Z

**Summary**: Creativity assessment in science and engineering is increasingly based on both human and AI judgment, but the cognitive processes and biases behind these evaluations remain poorly understood. We conducted two experiments examining how including example solutions with ratings impact creativity evaluation, using a finegrained annotation protocol where raters were tasked with explaining their originality scores and rating for the facets of remoteness (whether the response is "far" from everyday ideas), uncommonness (whether the response is rare), and cleverness. In Study 1, we analyzed creativity ratings from 72 experts with formal science or engineering training, comparing those who received example solutions with ratings (example) to those who did not (no example). Computational text analysis revealed that, compared to experts with examples, no-example experts used more comparative language (e.g., "better/worse") and emphasized solution uncommonness, suggesting they may have relied more on memory retrieval for comparisons. In Study 2, parallel analyses with state-of-the-art LLMs revealed that models prioritized uncommonness and remoteness of ideas when rating originality, suggesting an evaluative process rooted around the semantic similarity of ideas. In the example condition, while LLM accuracy in predicting the true originality scores improved, the correlations of remoteness, uncommonness, and cleverness with originality also increased substantially - to upwards of 0.99 - suggesting a homogenization in the LLMs evaluation of the individual facets. These findings highlight important implications for how humans and AI reason about creativity and suggest diverging preferences for what different populations prioritize when rating.

**Link**: [arxiv](http://arxiv.org/abs/2502.03253v1),  [pdf](http://arxiv.org/pdf/2502.03253v1)

**Tags**: cs.CL 



### SkyOctopus: Enabling Low-Latency Mobile Satellite Network through   Multiple Anchors
**Authors**: Shaojie Su, Jiasheng Wu, Zijie Ying, Zhiyuan Zhao, Xiangyu Jia, Wenjun Zhu, Yue Gao

**Updated**: 2025-02-05T15:05:05Z

**Summary**: The rapid deployment of low earth orbit (LEO) satellite constellations has drawn attention to the potential of nonterrestrial networks (NTN) in providing global communication services. Telecom operators are attempting to collaborate with satellite network providers to develop mobile satellite networks, which serve as an effective supplement to terrestrial networks. However, current mobile satellite network architectures still employ the single-anchor design of terrestrial mobile networks, leading to severely circuitous routing for users and significantly impacting their service experience. To reduce unnecessary latency caused by circuitous routing and provide users with low-latency global internet services, this paper presents SkyOctopus, an advanced multi-anchor mobile satellite network architecture. SkyOctopus innovatively deploys traffic classifiers on satellites to enable connections between users and multiple anchor points distributed globally. It guarantees optimal anchor point selection for each user's target server by monitoring multiple end-to-end paths. We build a prototype of SkyOctopus using enhanced Open5GS and UERANSIM, which is driven by actual LEO satellite constellations such as Starlink, Kuiper, and OneWeb. We conducted extensive experiments, and the results demonstrate that, compared to standard 5G NTN and two other existing schemes, SkyOctopus can reduce end-to-end latency by up to 53\%.

**Link**: [arxiv](http://arxiv.org/abs/2502.03250v1),  [pdf](http://arxiv.org/pdf/2502.03250v1)

**Tags**: cs.NI 



### Exploring the Security Threats of Knowledge Base Poisoning in   Retrieval-Augmented Code Generation
**Authors**: Bo Lin, Shangwen Wang, Liqian Chen, Xiaoguang Mao

**Updated**: 2025-02-05T14:49:12Z

**Summary**: The integration of Large Language Models (LLMs) into software development has revolutionized the field, particularly through the use of Retrieval-Augmented Code Generation (RACG) systems that enhance code generation with information from external knowledge bases. However, the security implications of RACG systems, particularly the risks posed by vulnerable code examples in the knowledge base, remain largely unexplored. This risk is particularly concerning given that public code repositories, which often serve as the sources for knowledge base collection in RACG systems, are usually accessible to anyone in the community. Malicious attackers can exploit this accessibility to inject vulnerable code into the knowledge base, making it toxic. Once these poisoned samples are retrieved and incorporated into the generated code, they can propagate security vulnerabilities into the final product. This paper presents the first comprehensive study on the security risks associated with RACG systems, focusing on how vulnerable code in the knowledge base compromises the security of generated code. We investigate the LLM-generated code security across different settings through extensive experiments using four major LLMs, two retrievers, and two poisoning scenarios. Our findings highlight the significant threat of knowledge base poisoning, where even a single poisoned code example can compromise up to 48% of generated code. Our findings provide crucial insights into vulnerability introduction in RACG systems and offer practical mitigation recommendations, thereby helping improve the security of LLM-generated code in future works.

**Link**: [arxiv](http://arxiv.org/abs/2502.03233v1),  [pdf](http://arxiv.org/pdf/2502.03233v1)

**Tags**: cs.CR cs.SE 



### Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large   Language Models
**Authors**: Jialiang Wu, Yi Shen, Sijia Liu, Yi Tang, Sen Song, Xiaoyi Wang, Longjun Cai

**Updated**: 2025-02-05T14:19:52Z

**Summary**: Despite their impressive capacities, Large language models (LLMs) often struggle with the hallucination issue of generating inaccurate or fabricated content even when they possess correct knowledge. In this paper, we extend the exploration of the correlation between hidden-state prediction changes and output factuality into a deeper, token-wise level. Based on the insights , we propose cross-layer Entropy eNhanced Decoding (END), a decoding method that mitigates hallucinations without requiring extra training. END leverages inner probability changes across layers to individually quantify the factual knowledge required for each candidate token, and adjusts the final predicting distribution to prioritize tokens with higher factuality. Experiments on both hallucination and QA benchmarks demonstrate that END significantly enhances the truthfulness and informativeness of generated content while maintaining robust QA accuracy. Moreover, our work provides a deeper perspective on understanding the correlations between inherent knowledge and output factuality.

**Link**: [arxiv](http://arxiv.org/abs/2502.03199v1),  [pdf](http://arxiv.org/pdf/2502.03199v1)

**Tags**: cs.CL cs.AI 



### LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence
**Authors**: Zhuoling Li, Xiaogang Xu, Zhenhua Xu, SerNam Lim, Hengshuang Zhao

**Updated**: 2025-02-05T14:06:21Z

**Summary**: Recent embodied agents are primarily built based on reinforcement learning (RL) or large language models (LLMs). Among them, RL agents are efficient for deployment but only perform very few tasks. By contrast, giant LLM agents (often more than 1000B parameters) present strong generalization while demanding enormous computing resources. In this work, we combine their advantages while avoiding the drawbacks by conducting the proposed referee RL on our developed large auto-regressive model (LARM). Specifically, LARM is built upon a lightweight LLM (fewer than 5B parameters) and directly outputs the next action to execute rather than text. We mathematically reveal that classic RL feedbacks vanish in long-horizon embodied exploration and introduce a giant LLM based referee to handle this reward vanishment during training LARM. In this way, LARM learns to complete diverse open-world tasks without human intervention. Especially, LARM successfully harvests enchanted diamond equipment in Minecraft, which demands significantly longer decision-making chains than the highest achievements of prior best methods.

**Link**: [arxiv](http://arxiv.org/abs/2405.17424v2),  [pdf](http://arxiv.org/pdf/2405.17424v2)

**Tags**: cs.CV 



### SuperCorrect: Supervising and Correcting Language Models with   Error-Driven Insights
**Authors**: Ling Yang, Zhaochen Yu, Tianjun Zhang, Minkai Xu, Joseph E. Gonzalez, Bin Cui, Shuicheng Yan

**Updated**: 2025-02-05T13:47:11Z

**Summary**: Large language models (LLMs) like GPT-4, PaLM, and LLaMA have shown significant improvements in various reasoning tasks. However, smaller models such as Llama-3-8B and DeepSeekMath-Base still struggle with complex mathematical reasoning because they fail to effectively identify and correct reasoning errors. Recent reflection-based methods aim to address these issues by enabling self-reflection and self-correction, but they still face challenges in independently detecting errors in their reasoning steps. To overcome these limitations, we propose SuperCorrect, a novel two-stage framework that uses a large teacher model to supervise and correct both the reasoning and reflection processes of a smaller student model. In the first stage, we extract hierarchical high-level and detailed thought templates from the teacher model to guide the student model in eliciting more fine-grained reasoning thoughts. In the second stage, we introduce cross-model collaborative direct preference optimization (DPO) to enhance the self-correction abilities of the student model by following the teacher's correction traces during training. This cross-model DPO approach teaches the student model to effectively locate and resolve erroneous thoughts with error-driven insights from the teacher model, breaking the bottleneck of its thoughts and acquiring new skills and knowledge to tackle challenging problems. Extensive experiments consistently demonstrate our superiority over previous methods. Notably, our SuperCorrect-7B model significantly surpasses powerful DeepSeekMath-7B by 7.8%/5.3% and Qwen2.5-Math-7B by 15.1%/6.3% on MATH/GSM8K benchmarks, achieving new SOTA performance among all 7B models. Code: https://github.com/YangLing0818/SuperCorrect-llm

**Link**: [arxiv](http://arxiv.org/abs/2410.09008v2),  [pdf](http://arxiv.org/pdf/2410.09008v2)

**Tags**: cs.CL 



### PICBench: Benchmarking LLMs for Photonic Integrated Circuits Design
**Authors**: Yuchao Wu, Xiaofei Yu, Hao Chen, Yang Luo, Yeyu Tong, Yuzhe Ma

**Updated**: 2025-02-05T13:32:29Z

**Summary**: While large language models (LLMs) have shown remarkable potential in automating various tasks in digital chip design, the field of Photonic Integrated Circuits (PICs)-a promising solution to advanced chip designs-remains relatively unexplored in this context. The design of PICs is time-consuming and prone to errors due to the extensive and repetitive nature of code involved in photonic chip design. In this paper, we introduce PICBench, the first benchmarking and evaluation framework specifically designed to automate PIC design generation using LLMs, where the generated output takes the form of a netlist. Our benchmark consists of dozens of meticulously crafted PIC design problems, spanning from fundamental device designs to more complex circuit-level designs. It automatically evaluates both the syntax and functionality of generated PIC designs by comparing simulation outputs with expert-written solutions, leveraging an open-source simulator. We evaluate a range of existing LLMs, while also conducting comparative tests on various prompt engineering techniques to enhance LLM performance in automated PIC design. The results reveal the challenges and potential of LLMs in the PIC design domain, offering insights into the key areas that require further research and development to optimize automation in this field. Our benchmark and evaluation code is available at https://github.com/PICDA/PICBench.

**Link**: [arxiv](http://arxiv.org/abs/2502.03159v1),  [pdf](http://arxiv.org/pdf/2502.03159v1)

**Tags**: cs.LG 



### Strategizing with AI: Insights from a Beauty Contest Experiment
**Authors**: Iuliia Alekseenko, Dmitry Dagaev, Sofia Paklina, Petr Parshakov

**Updated**: 2025-02-05T13:31:38Z

**Summary**: A Keynesian beauty contest is a wide class of games of guessing the most popular strategy among other players. In particular, guessing a fraction of a mean of numbers chosen by all players is a classic behavioral experiment designed to test iterative reasoning patterns among various groups of people. The previous literature reveals that the level of sophistication of the opponents is an important factor affecting the outcome of the game. Smarter decision makers choose strategies that are closer to theoretical Nash equilibrium and demonstrate faster convergence to equilibrium in iterated contests with information revelation. We replicate a series of classic experiments by running virtual experiments with modern large language models (LLMs) who play against various groups of virtual players. We test how advanced the LLMs' behavior is compared to the behavior of human players. We show that LLMs typically take into account the opponents' level of sophistication and adapt by changing the strategy. In various settings, most LLMs (with the exception of Llama) are more sophisticated and play lower numbers compared to human players. Our results suggest that LLMs (except Llama) are rather successful in identifying the underlying strategic environment and adopting the strategies to the changing set of parameters of the game in the same way that human players do. All LLMs still fail to play dominant strategies in a two-player game. Our results contribute to the discussion on the accuracy of modeling human economic agents by artificial intelligence.

**Link**: [arxiv](http://arxiv.org/abs/2502.03158v1),  [pdf](http://arxiv.org/pdf/2502.03158v1)

**Tags**: econ.GN q-fin.EC 



### AlphaAdam:Asynchronous Masked Optimization with Dynamic Alpha for   Selective Updates
**Authors**: Da Chang, Yu Li, Ganzhao Yuan

**Updated**: 2025-02-05T13:23:18Z

**Summary**: In the training of large language models (LLMs), updating parameters more efficiently and stably has always been an important challenge. To achieve efficient parameter updates, existing methods usually achieve performance comparable to full parameter updates through methods such as low-dimensional decomposition or layer-wise selective updates. In this work, we propose AlphaAdam, an optimization framework for LLM from the perspective of intra-layer parameter updates. By decoupling parameter updates and dynamically adjusting their strength, AlphaAdam accelerates convergence and improves training stability. We construct parameter masks based on the consistency of historical momentum and gradient direction and combine them with an adaptive mask strength strategy to ensure efficient optimization and theoretical convergence guarantees, which is also applicable to most momentum-based optimizers. Extensive experiments show that AlphaAdam outperforms state-of-the-art methods such as AdamW in terms of convergence speed and computational efficiency across tasks, including GPT-2 pre-trained and fine-tuned RoBERTa and Llama-7B. Our AlphaAdam implements an optimizer enhancement framework for LLMs through intra-layer asynchronous masked adaptive updates. Our code is available in this https://github.com/MaeChd/AlphaAdam.

**Link**: [arxiv](http://arxiv.org/abs/2501.18094v2),  [pdf](http://arxiv.org/pdf/2501.18094v2)

**Tags**: cs.LG 



### ImgTrojan: Jailbreaking Vision-Language Models with ONE Image
**Authors**: Xijia Tao, Shuai Zhong, Lei Li, Qi Liu, Lingpeng Kong

**Updated**: 2025-02-05T13:20:24Z

**Summary**: There has been an increasing interest in the alignment of large language models (LLMs) with human values. However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain relatively underexplored. In this paper, we propose a novel jailbreaking attack against VLMs, aiming to bypass their safety barrier when a user inputs harmful instructions. A scenario where our poisoned (image, text) data pairs are included in the training data is assumed. By replacing the original textual captions with malicious jailbreak prompts, our method can perform jailbreak attacks with the poisoned images. Moreover, we analyze the effect of poison ratios and positions of trainable parameters on our attack's success rate. For evaluation, we design two metrics to quantify the success rate and the stealthiness of our attack. Together with a list of curated harmful instructions, a benchmark for measuring attack efficacy is provided. We demonstrate the efficacy of our attack by comparing it with baseline methods.

**Link**: [arxiv](http://arxiv.org/abs/2403.02910v3),  [pdf](http://arxiv.org/pdf/2403.02910v3)

**Tags**: cs.CV cs.AI 



### TwinMarket: A Scalable Behavioral and Social Simulation for Financial   Markets
**Authors**: Yuzhe Yang, Yifei Zhang, Minghao Wu, Kaidi Zhang, Yunmiao Zhang, Honghai Yu, Yan Hu, Benyou Wang

**Updated**: 2025-02-05T13:18:13Z

**Summary**: The study of social emergence has long been a central focus in social science. Traditional modeling approaches, such as rule-based Agent-Based Models (ABMs), struggle to capture the diversity and complexity of human behavior, particularly the irrational factors emphasized in behavioral economics. Recently, large language model (LLM) agents have gained traction as simulation tools for modeling human behavior in social science and role-playing applications. Studies suggest that LLMs can account for cognitive biases, emotional fluctuations, and other non-rational influences, enabling more realistic simulations of socio-economic dynamics. In this work, we introduce TwinMarket, a novel multi-agent framework that leverages LLMs to simulate socio-economic systems. Specifically, we examine how individual behaviors, through interactions and feedback mechanisms, give rise to collective dynamics and emergent phenomena. Through experiments in a simulated stock market environment, we demonstrate how individual actions can trigger group behaviors, leading to emergent outcomes such as financial bubbles and recessions. Our approach provides valuable insights into the complex interplay between individual decision-making and collective socio-economic patterns.

**Link**: [arxiv](http://arxiv.org/abs/2502.01506v2),  [pdf](http://arxiv.org/pdf/2502.01506v2)

**Tags**: cs.CE cs.CY 



### Scalable In-Context Learning on Tabular Data via Retrieval-Augmented   Large Language Models
**Authors**: Xumeng Wen, Shun Zheng, Zhen Xu, Yiming Sun, Jiang Bian

**Updated**: 2025-02-05T13:16:41Z

**Summary**: Recent studies have shown that large language models (LLMs), when customized with post-training on tabular data, can acquire general tabular in-context learning (TabICL) capabilities. These models are able to transfer effectively across diverse data schemas and different task domains. However, existing LLM-based TabICL approaches are constrained to few-shot scenarios due to the sequence length limitations of LLMs, as tabular instances represented in plain text consume substantial tokens. To address this limitation and enable scalable TabICL for any data size, we propose retrieval-augmented LLMs tailored to tabular data. Our approach incorporates a customized retrieval module, combined with retrieval-guided instruction-tuning for LLMs. This enables LLMs to effectively leverage larger datasets, achieving significantly improved performance across 69 widely recognized datasets and demonstrating promising scaling behavior. Extensive comparisons with state-of-the-art tabular models reveal that, while LLM-based TabICL still lags behind well-tuned numeric models in overall performance, it uncovers powerful algorithms under limited contexts, enhances ensemble diversity, and excels on specific datasets. These unique properties underscore the potential of language as a universal and accessible interface for scalable tabular data learning.

**Link**: [arxiv](http://arxiv.org/abs/2502.03147v1),  [pdf](http://arxiv.org/pdf/2502.03147v1)

**Tags**: cs.CL cs.AI 



### Compressing Large Language Models with Automated Sub-Network Search
**Authors**: Rhea Sanjay Sukthanker, Benedikt Staffler, Frank Hutter, Aaron Klein

**Updated**: 2025-02-05T12:50:46Z

**Summary**: Large Language Models (LLMs) demonstrate exceptional reasoning abilities, enabling strong generalization across diverse tasks such as commonsense reasoning and instruction following. However, as LLMs scale, inference costs become increasingly prohibitive, accumulating significantly over their life cycle. In this paper we consider model compression for LLMs to reduce model size while improving downstream task performance. We phrase this as a neural architecture search problem that automatically prunes structural components, such as attention heads, neurons, and layers by searching for the Pareto-optimal set of sub-networks balancing between performance and on-device latency. Compared to state-of-the-art structural pruning approaches and fine-tuned smaller sub-networks extracted from the pre-trained model, our method achieves upto 9.85% improvement on average on 11 diverse downstream tasks, while achieving up to 22% improvement of on-device latency.

**Link**: [arxiv](http://arxiv.org/abs/2410.06479v3),  [pdf](http://arxiv.org/pdf/2410.06479v3)

**Tags**: cs.CL 



### SPARK: A Modular Benchmark for Humanoid Robot Safety
**Authors**: Yifan Sun, Rui Chen, Kai S. Yun, Yikuan Fang, Sebin Jung, Feihan Li, Bowei Li, Weiye Zhao, Changliu Liu

**Updated**: 2025-02-05T12:49:26Z

**Summary**: This paper introduces the Safe Protective and Assistive Robot Kit (SPARK), a comprehensive benchmark designed to ensure safety in humanoid autonomy and teleoperation. Humanoid robots pose significant safety risks due to their physical capabilities of interacting with complex environments. The physical structures of humanoid robots further add complexity to the design of general safety solutions. To facilitate the safe deployment of complex robot systems, SPARK can be used as a toolbox that comes with state-of-the-art safe control algorithms in a modular and composable robot control framework. Users can easily configure safety criteria and sensitivity levels to optimize the balance between safety and performance. To accelerate humanoid safety research and development, SPARK provides a simulation benchmark that compares safety approaches in a variety of environments, tasks, and robot models. Furthermore, SPARK allows quick deployment of synthesized safe controllers on real robots. For hardware deployment, SPARK supports Apple Vision Pro (AVP) or a Motion Capture System as external sensors, while also offering interfaces for seamless integration with alternative hardware setups. This paper demonstrates SPARK's capability with both simulation experiments and case studies with a Unitree G1 humanoid robot. Leveraging these advantages of SPARK, users and researchers can significantly improve the safety of their humanoid systems as well as accelerate relevant research. The open-source code is available at https://github.com/intelligent-control-lab/spark.

**Link**: [arxiv](http://arxiv.org/abs/2502.03132v1),  [pdf](http://arxiv.org/pdf/2502.03132v1)

**Tags**: cs.RO cs.SY eess.SY 



### Teaching Large Language Models Number-Focused Headline Generation With   Key Element Rationales
**Authors**: Zhen Qian, Xiuzhen Zhang, Xiaofei Xu, Feng Xia

**Updated**: 2025-02-05T12:39:07Z

**Summary**: Number-focused headline generation is a summarization task requiring both high textual quality and precise numerical accuracy, which poses a unique challenge for Large Language Models (LLMs). Existing studies in the literature focus only on either textual quality or numerical reasoning and thus are inadequate to address this challenge. In this paper, we propose a novel chain-of-thought framework for using rationales comprising key elements of the Topic, Entities, and Numerical reasoning (TEN) in news articles to enhance the capability for LLMs to generate topic-aligned high-quality texts with precise numerical accuracy. Specifically, a teacher LLM is employed to generate TEN rationales as supervision data, which are then used to teach and fine-tune a student LLM. Our approach teaches the student LLM automatic generation of rationales with enhanced capability for numerical reasoning and topic-aligned numerical headline generation. Experiments show that our approach achieves superior performance in both textual quality and numerical accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2502.03129v1),  [pdf](http://arxiv.org/pdf/2502.03129v1)

**Tags**: cs.CL cs.LG 



### Investigating Privacy Bias in Training Data of Language Models
**Authors**: Yan Shvartzshnaider, Vasisht Duddu

**Updated**: 2025-02-05T12:31:01Z

**Summary**: As LLMs are integrated into sociotechnical systems, it is crucial to examine the privacy biases they exhibit. A privacy bias refers to the skew in the appropriateness of information flows within a given context that LLMs acquire from large amounts of non-publicly available training data. This skew may either align with existing expectations or signal a symptom of systemic issues reflected in the training datasets.   We formulate a novel research question: how can we examine privacy biases in the training data of LLMs? We present a novel approach to assess the privacy biases using a contextual integrity-based methodology to evaluate the responses from different LLMs. Our approach accounts for the sensitivity of responses across prompt variations, which hinders the evaluation of privacy biases. We investigate how privacy biases are affected by model capacities and optimizations.

**Link**: [arxiv](http://arxiv.org/abs/2409.03735v2),  [pdf](http://arxiv.org/pdf/2409.03735v2)

**Tags**: cs.LG cs.AI cs.CR cs.CY 



### Can Large Language Models Predict the Outcome of Judicial Decisions?
**Authors**: Mohamed Bayan Kmainasi, Ali Ezzat Shahroor, Amani Al-Ghraibah

**Updated**: 2025-02-05T12:17:36Z

**Summary**: Large Language Models (LLMs) have shown exceptional capabilities in Natural Language Processing (NLP) across diverse domains. However, their application in specialized tasks such as Legal Judgment Prediction (LJP) for low-resource languages like Arabic remains underexplored. In this work, we address this gap by developing an Arabic LJP dataset, collected and preprocessed from Saudi commercial court judgments. We benchmark state-of-the-art open-source LLMs, including LLaMA-3.2-3B and LLaMA-3.1-8B, under varying configurations such as zero-shot, one-shot, and fine-tuning using QLoRA. Additionally, we used a comprehensive evaluation framework combining quantitative metrics (BLEU and ROUGE) and qualitative assessments (Coherence, legal language, clarity). Our results demonstrate that fine-tuned smaller models achieve comparable performance to larger models in task-specific contexts while offering significant resource efficiency. Furthermore, we investigate the effects of prompt engineering and fine-tuning on model outputs, providing insights into performance variability and instruction sensitivity. By making the dataset, implementation code, and models publicly available, we establish a robust foundation for future research in Arabic legal NLP.

**Link**: [arxiv](http://arxiv.org/abs/2501.09768v2),  [pdf](http://arxiv.org/pdf/2501.09768v2)

**Tags**: cs.CL cs.AI 



### REED: Chiplet-Based Accelerator for Fully Homomorphic Encryption
**Authors**: Aikata Aikata, Ahmet Can Mert, Sunmin Kwon, Maxim Deryabin, Sujoy Sinha Roy

**Updated**: 2025-02-05T12:16:00Z

**Summary**: Fully Homomorphic Encryption (FHE) enables privacy-preserving computation and has many applications. However, its practical implementation faces massive computation and memory overheads. To address this bottleneck, several Application-Specific Integrated Circuit (ASIC) FHE accelerators have been proposed. All these prior works put every component needed for FHE onto one chip (monolithic), hence offering high performance. However, they suffer from practical problems associated with large-scale chip design, such as inflexibility, low yield, and high manufacturing cost.   In this paper, we present the first-of-its-kind multi-chiplet-based FHE accelerator `REED' for overcoming the limitations of prior monolithic designs. To utilize the advantages of multi-chiplet structures while matching the performance of larger monolithic systems, we propose and implement several novel strategies in the context of FHE. These include a scalable chiplet design approach, an effective framework for workload distribution, a custom inter-chiplet communication strategy, and advanced pipelined Number Theoretic Transform and automorphism design to enhance performance.   Experimental results demonstrate that REED 2.5D microprocessor consumes 96.7 mm$^2$ chip area, 49.4 W average power in 7nm technology. It could achieve a remarkable speedup of up to 2,991x compared to a CPU (24-core 2xIntel X5690) and offer 1.9x better performance, along with a 50% reduction in development costs when compared to state-of-the-art ASIC FHE accelerators. Furthermore, our work presents the first instance of benchmarking an encrypted deep neural network (DNN) training. Overall, the REED architecture design offers a highly effective solution for accelerating FHE, thereby significantly advancing the practicality and deployability of FHE in real-world applications.

**Link**: [arxiv](http://arxiv.org/abs/2308.02885v3),  [pdf](http://arxiv.org/pdf/2308.02885v3)

**Tags**: cs.CR 



### Reveal the Mystery of DPO: The Connection between DPO and RL Algorithms
**Authors**: Xuerui Su, Yue Wang, Jinhua Zhu, Mingyang Yi, Feng Xu, Zhiming Ma, Yuting Liu

**Updated**: 2025-02-05T11:41:43Z

**Summary**: With the rapid development of Large Language Models (LLMs), numerous Reinforcement Learning from Human Feedback (RLHF) algorithms have been introduced to improve model safety and alignment with human preferences. These algorithms can be divided into two main frameworks based on whether they require an explicit reward (or value) function for training: actor-critic-based Proximal Policy Optimization (PPO) and alignment-based Direct Preference Optimization (DPO). The mismatch between DPO and PPO, such as DPO's use of a classification loss driven by human-preferred data, has raised confusion about whether DPO should be classified as a Reinforcement Learning (RL) algorithm. To address these ambiguities, we focus on three key aspects related to DPO, RL, and other RLHF algorithms: (1) the construction of the loss function; (2) the target distribution at which the algorithm converges; (3) the impact of key components within the loss function. Specifically, we first establish a unified framework named UDRRA connecting these algorithms based on the construction of their loss functions. Next, we uncover their target policy distributions within this framework. Finally, we investigate the critical components of DPO to understand their impact on the convergence rate. Our work provides a deeper understanding of the relationship between DPO, RL, and other RLHF algorithms, offering new insights for improving existing algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2502.03095v1),  [pdf](http://arxiv.org/pdf/2502.03095v1)

**Tags**: cs.LG 



### UGPhysics: A Comprehensive Benchmark for Undergraduate Physics Reasoning   with Large Language Models
**Authors**: Xin Xu, Qiyun Xu, Tong Xiao, Tianhao Chen, Yuchen Yan, Jiaxin Zhang, Shizhe Diao, Can Yang, Yang Wang

**Updated**: 2025-02-05T11:36:53Z

**Summary**: Large language models (LLMs) have demonstrated remarkable capabilities in solving complex reasoning tasks, particularly in mathematics. However, the domain of physics reasoning presents unique challenges that have received significantly less attention. Existing benchmarks often fall short in evaluating LLMs' abilities on the breadth and depth of undergraduate-level physics, underscoring the need for a comprehensive evaluation. To fill this gap, we introduce UGPhysics, a large-scale and comprehensive benchmark specifically designed to evaluate UnderGraduate-level Physics (UGPhysics) reasoning with LLMs. UGPhysics includes 5,520 undergraduate-level physics problems in both English and Chinese, covering 13 subjects with seven different answer types and four distinct physics reasoning skills, all rigorously screened for data leakage. Additionally, we develop a Model-Assistant Rule-based Judgment (MARJ) pipeline specifically tailored for assessing answer correctness of physics problems, ensuring accurate evaluation. Our evaluation of 31 leading LLMs shows that the highest overall accuracy, 49.8% (achieved by OpenAI-o1-mini), emphasizes the necessity for models with stronger physics reasoning skills, beyond math abilities. We hope UGPhysics, along with MARJ, will drive future advancements in AI for physics reasoning. Codes and data are available at https://github.com/YangLabHKUST/UGPhysics .

**Link**: [arxiv](http://arxiv.org/abs/2502.00334v2),  [pdf](http://arxiv.org/pdf/2502.00334v2)

**Tags**: cs.CL cs.AI 



### IAO Prompting: Making Knowledge Flow Explicit in LLMs through Structured   Reasoning Templates
**Authors**: Aissatou Diallo, Antonis Bikakis, Luke Dickens, Anthony Hunter, Rob Miller

**Updated**: 2025-02-05T11:14:20Z

**Summary**: While Large Language Models (LLMs) demonstrate impressive reasoning capabilities, understanding and validating their knowledge utilization remains challenging. Chain-of-thought (CoT) prompting partially addresses this by revealing intermediate reasoning steps, but the knowledge flow and application remain implicit. We introduce IAO (Input-Action-Output) prompting, a structured template-based method that explicitly models how LLMs access and apply their knowledge during complex reasoning tasks. IAO decomposes problems into sequential steps, each clearly identifying the input knowledge being used, the action being performed, and the resulting output. This structured decomposition enables us to trace knowledge flow, verify factual consistency, and identify potential knowledge gaps or misapplications. Through experiments across diverse reasoning tasks, we demonstrate that IAO not only improves zero-shot performance but also provides transparency in how LLMs leverage their stored knowledge. Human evaluation confirms that this structured approach enhances our ability to verify knowledge utilization and detect potential hallucinations or reasoning errors. Our findings provide insights into both knowledge representation within LLMs and methods for more reliable knowledge application.

**Link**: [arxiv](http://arxiv.org/abs/2502.03080v1),  [pdf](http://arxiv.org/pdf/2502.03080v1)

**Tags**: cs.CL 



### Optimizing Electric Vehicles Charging using Large Language Models and   Graph Neural Networks
**Authors**: Stavros Orfanoudakis, Peter Palensky, Pedro P. Vergara

**Updated**: 2025-02-05T11:00:51Z

**Summary**: Maintaining grid stability amid widespread electric vehicle (EV) adoption is vital for sustainable transportation. Traditional optimization methods and Reinforcement Learning (RL) approaches often struggle with the high dimensionality and dynamic nature of real-time EV charging, leading to sub-optimal solutions. To address these challenges, this study demonstrates that combining Large Language Models (LLMs), for sequence modeling, with Graph Neural Networks (GNNs), for relational information extraction, not only outperforms conventional EV smart charging methods, but also paves the way for entirely new research directions and innovative solutions.

**Link**: [arxiv](http://arxiv.org/abs/2502.03067v1),  [pdf](http://arxiv.org/pdf/2502.03067v1)

**Tags**: eess.SY cs.LG cs.SY 



### Almost Surely Safe Alignment of Large Language Models at Inference-Time
**Authors**: Xiaotong Ji, Shyam Sundhar Ramesh, Matthieu Zimmer, Ilija Bogunovic, Jun Wang, Haitham Bou Ammar

**Updated**: 2025-02-05T10:47:19Z

**Summary**: Even highly capable large language models (LLMs) can produce biased or unsafe responses, and alignment techniques, such as RLHF, aimed at mitigating this issue, are expensive and prone to overfitting as they retrain the LLM. This paper introduces a novel inference-time alignment approach that ensures LLMs generate safe responses almost surely, i.e., with a probability approaching one. We achieve this by framing the safe generation of inference-time responses as a constrained Markov decision process within the LLM's latent space. Crucially, we augment a safety state that tracks the evolution of safety constraints and enables us to demonstrate formal safety guarantees upon solving the MDP in the latent space. Building on this foundation, we propose InferenceGuard, a practical implementation that safely aligns LLMs without modifying the model weights. Empirically, we demonstrate InferenceGuard effectively balances safety and task performance, outperforming existing inference-time alignment methods in generating safe and aligned responses.

**Link**: [arxiv](http://arxiv.org/abs/2502.01208v2),  [pdf](http://arxiv.org/pdf/2502.01208v2)

**Tags**: cs.LG cs.CL 



### Text-to-CAD Generation Through Infusing Visual Feedback in Large   Language Models
**Authors**: Ruiyu Wang, Yu Yuan, Shizhao Sun, Jiang Bian

**Updated**: 2025-02-05T10:43:26Z

**Summary**: Creating Computer-Aided Design (CAD) models requires significant expertise and effort. Text-to-CAD, which converts textual descriptions into CAD parametric sequences, is crucial in streamlining this process. Recent studies have utilized ground-truth parametric sequences, known as sequential signals, as supervision to achieve this goal. However, CAD models are inherently multimodal, comprising parametric sequences and corresponding rendered visual objects. Besides,the rendering process from parametric sequences to visual objects is many-to-one. Therefore, both sequential and visual signals are critical for effective training. In this work, we introduce CADFusion, a framework that uses Large Language Models (LLMs) as the backbone and alternates between two training stages: the sequential learning (SL) stage and the visual feedback (VF) stage. In the SL stage, we train LLMs using ground-truth parametric sequences, enabling the generation of logically coherent parametric sequences. In the VF stage, we reward parametric sequences that render into visually preferred objects and penalize those that do not, allowing LLMs to learn how rendered visual objects are perceived and evaluated. These two stages alternate throughout the training, ensuring balanced learning and preserving benefits of both signals. Experiments demonstrate that CADFusion significantly improves performance, both qualitatively and quantitatively.

**Link**: [arxiv](http://arxiv.org/abs/2501.19054v2),  [pdf](http://arxiv.org/pdf/2501.19054v2)

**Tags**: cs.CV cs.LG 



### Understanding and Enhancing the Transferability of Jailbreaking Attacks
**Authors**: Runqi Lin, Bo Han, Fengwang Li, Tongling Liu

**Updated**: 2025-02-05T10:29:54Z

**Summary**: Jailbreaking attacks can effectively manipulate open-source large language models (LLMs) to produce harmful responses. However, these attacks exhibit limited transferability, failing to disrupt proprietary LLMs consistently. To reliably identify vulnerabilities in proprietary LLMs, this work investigates the transferability of jailbreaking attacks by analysing their impact on the model's intent perception. By incorporating adversarial sequences, these attacks can redirect the source LLM's focus away from malicious-intent tokens in the original input, thereby obstructing the model's intent recognition and eliciting harmful responses. Nevertheless, these adversarial sequences fail to mislead the target LLM's intent perception, allowing the target LLM to refocus on malicious-intent tokens and abstain from responding. Our analysis further reveals the inherent distributional dependency within the generated adversarial sequences, whose effectiveness stems from overfitting the source LLM's parameters, resulting in limited transferability to target LLMs. To this end, we propose the Perceived-importance Flatten (PiF) method, which uniformly disperses the model's focus across neutral-intent tokens in the original input, thus obscuring malicious-intent tokens without relying on overfitted adversarial sequences. Extensive experiments demonstrate that PiF provides an effective and efficient red-teaming evaluation for proprietary LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2502.03052v1),  [pdf](http://arxiv.org/pdf/2502.03052v1)

**Tags**: cs.LG cs.CR 



### SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a   Practical Manner
**Authors**: Xunguang Wang, Daoyuan Wu, Zhenlan Ji, Zongjie Li, Pingchuan Ma, Shuai Wang, Yingjiu Li, Yang Liu, Ning Liu, Juergen Rahmel

**Updated**: 2025-02-05T10:29:07Z

**Summary**: Jailbreaking is an emerging adversarial attack that bypasses the safety alignment deployed in off-the-shelf large language models (LLMs) and has evolved into multiple categories: human-based, optimization-based, generation-based, and the recent indirect and multilingual jailbreaks. However, delivering a practical jailbreak defense is challenging because it needs to not only handle all the above jailbreak attacks but also incur negligible delays to user prompts, as well as be compatible with both open-source and closed-source LLMs. Inspired by how the traditional security concept of shadow stacks defends against memory overflow attacks, this paper introduces a generic LLM jailbreak defense framework called SelfDefend, which establishes a shadow LLM as a defense instance (in detection state) to concurrently protect the target LLM instance (in normal answering state) in the normal stack and collaborate with it for checkpoint-based access control. The effectiveness of SelfDefend builds upon our observation that existing LLMs can identify harmful prompts or intentions in user queries, which we empirically validate using mainstream GPT-3.5/4 models against major jailbreak attacks. To further improve the defense's robustness and minimize costs, we employ a data distillation approach to tune dedicated open-source defense models. When deployed to protect GPT-3.5/4, Claude, Llama-2-7b/13b, and Mistral, these models outperform seven state-of-the-art defenses and match the performance of GPT-4-based SelfDefend, with significantly lower extra delays. Further experiments show that the tuned models are robust to adaptive jailbreaks and prompt injections.

**Link**: [arxiv](http://arxiv.org/abs/2406.05498v3),  [pdf](http://arxiv.org/pdf/2406.05498v3)

**Tags**: cs.CR cs.AI 



### Large Language Models Are Universal Recommendation Learners
**Authors**: Junguang Jiang, Yanwen Huang, Bin Liu, Xiaoyu Kong, Ziru Xu, Han Zhu, Jian Xu, Bo Zheng

**Updated**: 2025-02-05T09:56:52Z

**Summary**: In real-world recommender systems, different tasks are typically addressed using supervised learning on task-specific datasets with carefully designed model architectures. We demonstrate that large language models (LLMs) can function as universal recommendation learners, capable of handling multiple tasks within a unified input-output framework, eliminating the need for specialized model designs. To improve the recommendation performance of LLMs, we introduce a multimodal fusion module for item representation and a sequence-in-set-out approach for efficient candidate generation. When applied to industrial-scale data, our LLM achieves competitive results with expert models elaborately designed for different recommendation tasks. Furthermore, our analysis reveals that recommendation outcomes are highly sensitive to text input, highlighting the potential of prompt engineering in optimizing industrial-scale recommender systems.

**Link**: [arxiv](http://arxiv.org/abs/2502.03041v1),  [pdf](http://arxiv.org/pdf/2502.03041v1)

**Tags**: cs.IR cs.LG 



### A Framework for IoT-Enabled Smart Manufacturing for Energy and Resource   Optimization
**Authors**: Bazigu Alex, Mwebaze Johnson

**Updated**: 2025-02-05T09:54:13Z

**Summary**: The increasing demands for sustainable and efficient manufacturing systems have driven the integration of Internet of Things (IoT) technologies into smart manufacturing. This study investigates IoT-enabled systems designed to enhance energy efficiency and resource optimization in the manufacturing sector, focusing on a multi-layered architecture integrating sensors, edge computing, and cloud platforms. MATLAB Simulink was utilized for modeling and simulation, replicating typical manufacturing conditions to evaluate energy consumption, machine uptime, and resource usage. The results demonstrate an 18% reduction in energy consumption, a 22% decrease in machine downtime, and a 15% improvement in resource utilization. Comparative analyses highlight the superiority of the proposed framework in addressing operational inefficiencies and aligning with sustainability goals. The study underscores the potential of IoT in transforming traditional manufacturing into interconnected, intelligent systems, offering practical implications for industrial stakeholders aiming to optimize operations while adhering to global sustainability standards. Future work will focus on addressing identified challenges such as high deployment costs and data security concerns, aiming to facilitate the broader adoption of IoT in industrial applications.   Keywords: IoT (Internet of Things), Smart Manufacturing, Energy Efficiency, Resource Optimization, Manufacturing

**Link**: [arxiv](http://arxiv.org/abs/2502.03040v1),  [pdf](http://arxiv.org/pdf/2502.03040v1)

**Tags**: cs.NI 



### Knowledge Distillation from Large Language Models for Household Energy   Modeling
**Authors**: Mohannad Takrouri, Nicolás M. Cuadrado, Martin Takáč

**Updated**: 2025-02-05T09:43:14Z

**Summary**: Machine learning (ML) is increasingly vital for smart-grid research, yet restricted access to realistic, diverse data - often due to privacy concerns - slows progress and fuels doubts within the energy sector about adopting ML-based strategies. We propose integrating Large Language Models (LLMs) in energy modeling to generate realistic, culturally sensitive, and behavior-specific data for household energy usage across diverse geographies. In this study, we employ and compare five different LLMs to systematically produce family structures, weather patterns, and daily consumption profiles for households in six distinct countries. A four-stage methodology synthesizes contextual daily data, including culturally nuanced activities, realistic weather ranges, HVAC operations, and distinct `energy signatures' that capture unique consumption footprints. Additionally, we explore an alternative strategy where external weather datasets can be directly integrated, bypassing intermediate weather modeling stages while ensuring physically consistent data inputs. The resulting dataset provides insights into how cultural, climatic, and behavioral factors converge to shape carbon emissions, offering a cost-effective avenue for scenario-based energy optimization. This approach underscores how prompt engineering, combined with knowledge distillation, can advance sustainable energy research and climate mitigation efforts. Source code is available at https://github.com/Singularity-AI-Lab/LLM-Energy-Knowledge-Distillation .

**Link**: [arxiv](http://arxiv.org/abs/2502.03034v1),  [pdf](http://arxiv.org/pdf/2502.03034v1)

**Tags**: cs.CL cs.LG 



### Efficient Prompt Compression with Evaluator Heads for Long-Context   Transformer Inference
**Authors**: Weizhi Fei, Xueyan Niu, Guoqing Xie, Yingqing Liu, Bo Bai, Wei Han

**Updated**: 2025-02-05T09:35:38Z

**Summary**: Although applications involving long-context inputs are crucial for the effective utilization of large language models (LLMs), they also result in increased computational costs and reduced performance. To address this challenge, we propose an efficient, training-free prompt compression method that retains key information within compressed prompts. We identify specific attention heads in transformer-based LLMs, which we designate as evaluator heads, that are capable of selecting tokens in long inputs that are most significant for inference. Building on this discovery, we develop EHPC, an Evaluator Head-based Prompt Compression method, which enables LLMs to rapidly "skim through" input prompts by leveraging only the first few layers with evaluator heads during the pre-filling stage, subsequently passing only the important tokens to the model for inference. EHPC achieves state-of-the-art results across two mainstream benchmarks: prompt compression and long-context inference acceleration. Consequently, it effectively reduces the complexity and costs associated with commercial API calls. We further demonstrate that EHPC attains competitive results compared to key-value cache-based acceleration methods, thereby highlighting its potential to enhance the efficiency of LLMs for long-context tasks.

**Link**: [arxiv](http://arxiv.org/abs/2501.12959v2),  [pdf](http://arxiv.org/pdf/2501.12959v2)

**Tags**: cs.CL 



### On Zero-Initialized Attention: Optimal Prompt and Gating Factor   Estimation
**Authors**: Nghiem T. Diep, Huy Nguyen, Chau Nguyen, Minh Le, Duy M. H. Nguyen, Daniel Sonntag, Mathias Niepert, Nhat Ho

**Updated**: 2025-02-05T09:31:27Z

**Summary**: The LLaMA-Adapter has recently emerged as an efficient fine-tuning technique for LLaMA models, leveraging zero-initialized attention to stabilize training and enhance performance. However, despite its empirical success, the theoretical foundations of zero-initialized attention remain largely unexplored. In this paper, we provide a rigorous theoretical analysis, establishing a connection between zero-initialized attention and mixture-of-expert models. We prove that both linear and non-linear prompts, along with gating functions, can be optimally estimated, with non-linear prompts offering greater flexibility for future applications. Empirically, we validate our findings on the open LLM benchmarks, demonstrating that non-linear prompts outperform linear ones. Notably, even with limited training data, both prompt types consistently surpass vanilla attention, highlighting the robustness and adaptability of zero-initialized attention.

**Link**: [arxiv](http://arxiv.org/abs/2502.03029v1),  [pdf](http://arxiv.org/pdf/2502.03029v1)

**Tags**: cs.LG 



### Beyond English: Evaluating Automated Measurement of Moral Foundations in   Non-English Discourse with a Chinese Case Study
**Authors**: Calvin Yixiang Cheng, Scott A Hale

**Updated**: 2025-02-05T09:17:17Z

**Summary**: This study explores computational approaches for measuring moral foundations (MFs) in non-English corpora. Since most resources are developed primarily for English, cross-linguistic applications of moral foundation theory remain limited. Using Chinese as a case study, this paper evaluates the effectiveness of applying English resources to machine translated text, local language lexicons, multilingual language models, and large language models (LLMs) in measuring MFs in non-English texts. The results indicate that machine translation and local lexicon approaches are insufficient for complex moral assessments, frequently resulting in a substantial loss of cultural information. In contrast, multilingual models and LLMs demonstrate reliable cross-language performance with transfer learning, with LLMs excelling in terms of data efficiency. Importantly, this study also underscores the need for human-in-the-loop validation of automated MF assessment, as the most advanced models may overlook cultural nuances in cross-language measurements. The findings highlight the potential of LLMs for cross-language MF measurements and other complex multilingual deductive coding tasks.

**Link**: [arxiv](http://arxiv.org/abs/2502.02451v2),  [pdf](http://arxiv.org/pdf/2502.02451v2)

**Tags**: cs.CL cs.SI 



### Virgo: A Preliminary Exploration on Reproducing o1-like MLLM
**Authors**: Yifan Du, Zikang Liu, Yifan Li, Wayne Xin Zhao, Yuqi Huo, Bingning Wang, Weipeng Chen, Zheng Liu, Zhongyuan Wang, Ji-Rong Wen

**Updated**: 2025-02-05T09:17:01Z

**Summary**: Recently, slow-thinking reasoning systems, built upon large language models (LLMs), have garnered widespread attention by scaling the thinking time during inference. There is also growing interest in adapting this capability to multimodal large language models (MLLMs). Given that MLLMs handle more complex data semantics across different modalities, it is intuitively more challenging to implement multimodal slow-thinking systems.   To address this issue, in this paper, we explore a straightforward approach by fine-tuning a capable MLLM with a small amount of textual long-form thought data, resulting in a multimodal slow-thinking system, Virgo (Visual reasoning with long thought). We find that these long-form reasoning processes, expressed in natural language, can be effectively transferred to MLLMs. Moreover, it seems that such textual reasoning data can be even more effective than visual reasoning data in eliciting the slow-thinking capacities of MLLMs. While this work is preliminary, it demonstrates that slow-thinking capacities are fundamentally associated with the language model component, which can be transferred across modalities or domains. This finding can be leveraged to guide the development of more powerful slow-thinking reasoning systems. We release our resources at https://github.com/RUCAIBox/Virgo.

**Link**: [arxiv](http://arxiv.org/abs/2501.01904v2),  [pdf](http://arxiv.org/pdf/2501.01904v2)

**Tags**: cs.CV cs.AI 



### UITrans: Seamless UI Translation from Android to HarmonyOS
**Authors**: Lina Gong, Chen Wang, Yujun Huang, Di Cui, Mingqiang Wei

**Updated**: 2025-02-05T09:11:45Z

**Summary**: Seamless user interface (i.e., UI) translation has emerged as a pivotal technique for modern mobile developers, addressing the challenge of developing separate UI applications for Android and HarmonyOS platforms due to fundamental differences in layout structures and development paradigms. In this paper, we present UITrans, the first automated UI translation tool designed for Android to HarmonyOS. UITrans leverages an LLM-driven multi-agent reflective collaboration framework to convert Android XML layouts into HarmonyOS ArkUI layouts. It not only maps component-level and page-level elements to ArkUI equivalents but also handles project-level challenges, including complex layouts and interaction logic. Our evaluation of six Android applications demonstrates that our UITrans achieves translation success rates of over 90.1%, 89.3%, and 89.2% at the component, page, and project levels, respectively. UITrans is available at https://github.com/OpenSELab/UITrans and the demo video can be viewed at https://www.youtube.com/watch?v=iqKOSmCnJG0.

**Link**: [arxiv](http://arxiv.org/abs/2412.13693v3),  [pdf](http://arxiv.org/pdf/2412.13693v3)

**Tags**: cs.SE 



### Scaling Laws for Upcycling Mixture-of-Experts Language Models
**Authors**: Seng Pei Liew, Takuya Kato, Sho Takase

**Updated**: 2025-02-05T09:11:13Z

**Summary**: Pretraining large language models (LLMs) is resource-intensive, often requiring months of training time even with high-end GPU clusters. There are two approaches of mitigating such computational demands: reusing smaller models to train larger ones (upcycling), and training computationally efficient models like mixture-of-experts (MoE). In this paper, we study the upcycling of LLMs to MoE models, of which the scaling behavior remains underexplored. Through extensive experiments, we identify empirical scaling laws that describe how performance depends on dataset size and model configuration. Particularly, we show that, while scaling these factors improves performance, there is a novel interaction term between the dense and upcycled training dataset that limits the efficiency of upcycling at large computational budgets. Based on these findings, we provide guidance to scale upcycling, and establish conditions under which upcycling outperforms from-scratch trainings within budget constraints.

**Link**: [arxiv](http://arxiv.org/abs/2502.03009v1),  [pdf](http://arxiv.org/pdf/2502.03009v1)

**Tags**: cs.LG cs.CL 



### MedBioLM: Optimizing Medical and Biological QA with Fine-Tuned Large   Language Models and Retrieval-Augmented Generation
**Authors**: Seonok Kim

**Updated**: 2025-02-05T08:58:35Z

**Summary**: Large Language Models (LLMs) have demonstrated impressive capabilities across natural language processing tasks. However, their application to specialized domains such as medicine and biology requires further optimization to ensure factual accuracy, reliability, and contextual depth. We introduce MedBioLM, a domain-adapted biomedical question-answering model designed to enhance both short-form and long-form queries. By integrating fine-tuning and retrieval-augmented generation (RAG), MedBioLM dynamically incorporates domain-specific knowledge, improving reasoning abilities and factual accuracy. To evaluate its effectiveness, we fine-tuned the model on diverse biomedical QA datasets, covering structured multiple-choice assessments and complex clinical reasoning tasks. Fine-tuning significantly improves accuracy on benchmark datasets, while RAG enhances factual consistency. These results highlight the potential of domain-optimized LLMs in advancing biomedical research, medical education, and clinical decision support.

**Link**: [arxiv](http://arxiv.org/abs/2502.03004v1),  [pdf](http://arxiv.org/pdf/2502.03004v1)

**Tags**: cs.CL cs.AI 



### Armadillo: An Efficient Framework for Numerical Linear Algebra
**Authors**: Conrad Sanderson, Ryan Curtin

**Updated**: 2025-02-05T08:52:37Z

**Summary**: A major challenge in the deployment of scientific software solutions is the adaptation of research prototypes to production-grade code. While high-level languages like MATLAB are useful for rapid prototyping, they lack the resource efficiency required for scalable production applications, necessitating translation into lower level languages like C++. Further, for machine learning and signal processing applications, the underlying linear algebra primitives, generally provided by the standard BLAS and LAPACK libraries, are unwieldy and difficult to use, requiring manual memory management and other tedium. To address this challenge, the Armadillo C++ linear algebra library provides an intuitive interface for writing linear algebra expressions that are easily compiled into efficient production-grade implementations. We describe the expression optimisations we have implemented in Armadillo, exploiting template metaprogramming. We demonstrate that these optimisations result in considerable efficiency gains on a variety of benchmark linear algebra expressions.

**Link**: [arxiv](http://arxiv.org/abs/2502.03000v1),  [pdf](http://arxiv.org/pdf/2502.03000v1)

**Tags**: cs.MS 68N99, 65Y04, 65Y15, 65F45 G.4; G.1.3 



### MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution   Real-World Scenarios that are Difficult for Humans?
**Authors**: Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, Liang Wang, Rong Jin, Tieniu Tan

**Updated**: 2025-02-05T08:44:02Z

**Summary**: Comprehensive evaluation of Multimodal Large Language Models (MLLMs) has recently garnered widespread attention in the research community. However, we observe that existing benchmarks present several common barriers that make it difficult to measure the significant challenges that models face in the real world, including: 1) small data scale leads to a large performance variance; 2) reliance on model-based annotations results in restricted data quality; 3) insufficient task difficulty, especially caused by the limited image resolution. To tackle these issues, we introduce MME-RealWorld. Specifically, we collect more than $300$K images from public datasets and the Internet, filtering $13,366$ high-quality images for annotation. This involves the efforts of professional $25$ annotators and $7$ experts in MLLMs, contributing to $29,429$ question-answer pairs that cover $43$ subtasks across $5$ real-world scenarios, extremely challenging even for humans. As far as we know, MME-RealWorld is the largest manually annotated benchmark to date, featuring the highest resolution and a targeted focus on real-world applications. We further conduct a thorough evaluation involving $28$ prominent MLLMs, such as GPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet. Our results show that even the most advanced models struggle with our benchmarks, where none of them reach $60\%$ accuracy. The challenges of perceiving high-resolution images and understanding complex real-world scenarios remain urgent issues to be addressed. The data and evaluation code are released at https://mme-realworld.github.io/ .

**Link**: [arxiv](http://arxiv.org/abs/2408.13257v3),  [pdf](http://arxiv.org/pdf/2408.13257v3)

**Tags**: cs.CV 



### Training an LLM-as-a-Judge Model: Pipeline, Insights, and Practical   Lessons
**Authors**: Renjun Hu, Yi Cheng, Libin Meng, Jiaxin Xia, Yi Zong, Xing Shi, Wei Lin

**Updated**: 2025-02-05T08:35:55Z

**Summary**: The rapid advancement of large language models (LLMs) has opened new possibilities for their adoption as evaluative judges. This paper introduces Themis, a fine-tuned LLM judge that delivers sophisticated context-aware evaluations. We provide a comprehensive overview of the development pipeline for Themis, highlighting its scenario-dependent evaluation prompts and two novel methods for controlled instruction generation. These designs enable Themis to effectively distill evaluative skills from teacher models, while retaining flexibility for continuous development. We introduce two human-labeled benchmarks for meta-evaluation, demonstrating that Themis can achieve high alignment with human preferences in an economical manner. Additionally, we explore insights into the LLM-as-a-judge paradigm, revealing nuances in performance and the varied effects of reference answers. Notably, we observe that pure knowledge distillation from strong LLMs, though common, does not guarantee performance improvement through scaling. We propose a mitigation strategy based on instruction-following difficulty. Furthermore, we provide practical guidelines covering data balancing, prompt customization, multi-objective training, and metric aggregation. We aim for our method and findings, along with the fine-tuning data, benchmarks, and model checkpoints, to support future research and development in this area.

**Link**: [arxiv](http://arxiv.org/abs/2502.02988v1),  [pdf](http://arxiv.org/pdf/2502.02988v1)

**Tags**: cs.CL cs.AI cs.LG 



### A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference
**Authors**: You Wu, Haoyi Wu, Kewei Tu

**Updated**: 2025-02-05T08:22:05Z

**Summary**: Recently, sharing key-value (KV) cache across layers has been found effective in efficient inference of large language models (LLMs). To systematically investigate different techniques of cross-layer KV sharing, we propose a unified framework that covers several recent methods and their novel variants. We conduct comprehensive experiments on all the configurations of the framework, evaluating their generation throughput and performance in language modeling and downstream tasks. We find that when reducing the size of the KV cache by 2$\times$, most configurations can achieve higher throughput than standard transformers while maintaining competitive performance. When further reducing the size of the KV cache, however, pairing queries of all layers with KVs of upper layers performs better, at the expense of additional training cost and prefilling latency. We hope that this work will help users make more informed choices of cross-layer KV sharing approaches and facilitate future research on efficient LLM inference.

**Link**: [arxiv](http://arxiv.org/abs/2410.14442v2),  [pdf](http://arxiv.org/pdf/2410.14442v2)

**Tags**: cs.CL 



### Lost in Overlap: Exploring Logit-based Watermark Collision in LLMs
**Authors**: Yiyang Luo, Ke Lin, Chao Gu, Jiahui Hou, Lijie Wen, Ping Luo

**Updated**: 2025-02-05T08:11:40Z

**Summary**: The proliferation of large language models (LLMs) in generating content raises concerns about text copyright. Watermarking methods, particularly logit-based approaches, embed imperceptible identifiers into text to address these challenges. However, the widespread usage of watermarking across diverse LLMs has led to an inevitable issue known as watermark collision during common tasks, such as paraphrasing or translation. In this paper, we introduce watermark collision as a novel and general philosophy for watermark attacks, aimed at enhancing attack performance on top of any other attacking methods. We also provide a comprehensive demonstration that watermark collision poses a threat to all logit-based watermark algorithms, impacting not only specific attack scenarios but also downstream applications.

**Link**: [arxiv](http://arxiv.org/abs/2403.10020v3),  [pdf](http://arxiv.org/pdf/2403.10020v3)

**Tags**: cs.CL cs.MM 



### Qrazor: Reliable and Effortless 4-bit LLM Quantization by Significant   Data Razoring
**Authors**: Dongyoung Lee, Seungkyu Choi, Ik Joon Chang

**Updated**: 2025-02-05T08:10:45Z

**Summary**: Large-scale language models (LLMs) excel in language processing tasks but face deployment challenges due to high memory and computational demands. While low-bit quantization, such as 4-bit techniques, offers a potential solution, these methods often suffer from significant accuracy loss or require considerable effort for implementation such as reordering, rotation, etc. To address these challenges, we propose QRazor, a simple yet effective quantization scheme that enables 4-bit quantization of weights, activations, and KV cache in transformer-based LLMs. QRazor operates in two stages: first, quantizing data using 8 or 16-bit integers as a basis with absolute max scaling to preserve accuracy close to full-precision models, and second, compressing the quantized data to 4-bit using our significant data razoring (SDR) technique, which retains only the four most salient bits. Without any additional requirment of fine-tuning or additional training, QRazor achieves performance similar or better compared to state-of-the-art in 4-bit quantization method, surpassing Smoothquant and QLLM by over 12 points and Quarot(RTN) by more than 2.9 points in zero-shot reasoning task accuracy on the LLaMA2-7B model. Additionally, we introduce an integer-based arithmetic unit optimized for QRazor, allowing direct low-precision operations on SDR data without decompression.

**Link**: [arxiv](http://arxiv.org/abs/2501.13331v2),  [pdf](http://arxiv.org/pdf/2501.13331v2)

**Tags**: cs.LG 



### Demonstrating a Control Framework for Physical Human-Robot Interaction   Toward Industrial Applications
**Authors**: Bastien Muraccioli, Celerier Mathieu, Benallegue Mehdi, Venture Gentiane

**Updated**: 2025-02-05T08:07:38Z

**Summary**: Human-Robot Interaction (pHRI) is critical for implementing Industry 5.0 which focuses on human-centric approaches. However, few studies explore the practical alignment of pHRI to industrial grade performance. This paper introduces a versatile control framework designed to bridge this gap by incorporating the torque-based control modes: compliance control, null-space compliance, dual compliance, all in static and dynamic scenarios. Thanks to our second-order Quadratic Programming (QP) formulation, strict kinematic and collision constraints are integrated into the system as safety features, and a weighted hierarchy guarantees singularity-robust task tracking performance. The framework is implemented on a Kinova Gen3 collaborative robot (cobot) equipped with a Bota force/torque sensor. A DualShock 4 game controller is attached at the robot's end-effector to demonstrate the framework's capabilities. This setup enables seamless dynamic switching between the modes, and real-time adjustment of parameters, such as transitioning between position and torque control or selecting a more robust custom-developed low-level torque controller over the default one.Built on the open-source robotic control software mc_rtc, to ensure reproducibility for both research and industrial deployment, this framework demonstrates industrial-grade performance and repeatability, showcasing its potential as a robust pHRI control system for industrial environments.

**Link**: [arxiv](http://arxiv.org/abs/2502.02967v1),  [pdf](http://arxiv.org/pdf/2502.02967v1)

**Tags**: cs.RO cs.SY eess.SY 



### FACTER: Fairness-Aware Conformal Thresholding and Prompt Engineering for   Enabling Fair LLM-Based Recommender Systems
**Authors**: Arya Fayyazi, Mehdi Kamal, Massoud Pedram

**Updated**: 2025-02-05T08:07:04Z

**Summary**: We propose FACTER, a fairness-aware framework for LLM-based recommendation systems that integrates conformal prediction with dynamic prompt engineering. By introducing an adaptive semantic variance threshold and a violation-triggered mechanism, FACTER automatically tightens fairness constraints whenever biased patterns emerge. We further develop an adversarial prompt generator that leverages historical violations to reduce repeated demographic biases without retraining the LLM. Empirical results on MovieLens and Amazon show that FACTER substantially reduces fairness violations (up to 95.5%) while maintaining strong recommendation accuracy, revealing semantic variance as a potent proxy of bias.

**Link**: [arxiv](http://arxiv.org/abs/2502.02966v1),  [pdf](http://arxiv.org/pdf/2502.02966v1)

**Tags**: cs.IR cs.AI cs.CY cs.LG 



### Bridging and Modeling Correlations in Pairwise Data for Direct   Preference Optimization
**Authors**: Yuxin Jiang, Bo Huang, Yufei Wang, Xingshan Zeng, Liangyou Li, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, Wei Wang

**Updated**: 2025-02-05T08:05:42Z

**Summary**: Direct preference optimization (DPO), a widely adopted offline preference optimization algorithm, aims to align large language models (LLMs) with human-desired behaviors using pairwise preference data. However, the generation of the winning response and the losing response within pairwise data are typically isolated, leading to weak correlations between them as well as suboptimal alignment performance. To address this issue, we propose an effective framework for Bridging and Modeling Correlations in pairwise data, named BMC. Firstly, we increase the consistency and informativeness of the pairwise preference signals through targeted modifications, synthesizing a pseudo-winning response by improving the losing response with the winning response as a reference. Secondly, we identify that DPO alone is insufficient to model these correlations and capture nuanced variations. Therefore, we propose learning token-level correlations by dynamically leveraging the policy model's confidence during training. Comprehensive experiments on QA, math, and instruction-following tasks demonstrate the effectiveness of our approach, significantly surpassing competitive baselines, including DPO. Additionally, our in-depth quantitative analysis reveals the reasons behind our method's superior performance over DPO and showcases its versatility to other DPO variants. We release our repository at https://github.com/YJiangcm/BMC.

**Link**: [arxiv](http://arxiv.org/abs/2408.07471v3),  [pdf](http://arxiv.org/pdf/2408.07471v3)

**Tags**: cs.CL 



### Large Language Model Adversarial Landscape Through the Lens of Attack   Objectives
**Authors**: Nan Wang, Kane Walter, Yansong Gao, Alsharif Abuadbba

**Updated**: 2025-02-05T07:54:07Z

**Summary**: Large Language Models (LLMs) represent a transformative leap in artificial intelligence, enabling the comprehension, generation, and nuanced interaction with human language on an unparalleled scale. However, LLMs are increasingly vulnerable to a range of adversarial attacks that threaten their privacy, reliability, security, and trustworthiness. These attacks can distort outputs, inject biases, leak sensitive information, or disrupt the normal functioning of LLMs, posing significant challenges across various applications.   In this paper, we provide a novel comprehensive analysis of the adversarial landscape of LLMs, framed through the lens of attack objectives. By concentrating on the core goals of adversarial actors, we offer a fresh perspective that examines threats from the angles of privacy, integrity, availability, and misuse, moving beyond conventional taxonomies that focus solely on attack techniques. This objective-driven adversarial landscape not only highlights the strategic intent behind different adversarial approaches but also sheds light on the evolving nature of these threats and the effectiveness of current defenses. Our analysis aims to guide researchers and practitioners in better understanding, anticipating, and mitigating these attacks, ultimately contributing to the development of more resilient and robust LLM systems.

**Link**: [arxiv](http://arxiv.org/abs/2502.02960v1),  [pdf](http://arxiv.org/pdf/2502.02960v1)

**Tags**: cs.CR 



### Rationale Behind Essay Scores: Enhancing S-LLM's Multi-Trait Essay   Scoring with Rationale Generated by LLMs
**Authors**: SeongYeub Chu, JongWoo Kim, Bryan Wong, MunYong Yi

**Updated**: 2025-02-05T07:52:14Z

**Summary**: Existing automated essay scoring (AES) has solely relied on essay text without using explanatory rationales for the scores, thereby forgoing an opportunity to capture the specific aspects evaluated by rubric indicators in a fine-grained manner. This paper introduces Rationale-based Multiple Trait Scoring (RMTS), a novel approach for multi-trait essay scoring that integrates prompt-engineering-based large language models (LLMs) with a fine-tuning-based essay scoring model using a smaller large language model (S-LLM). RMTS uses an LLM-based trait-wise rationale generation system where a separate LLM agent generates trait-specific rationales based on rubric guidelines, which the scoring model uses to accurately predict multi-trait scores. Extensive experiments on benchmark datasets, including ASAP, ASAP++, and Feedback Prize, show that RMTS significantly outperforms state-of-the-art models and vanilla S-LLMs in trait-specific scoring. By assisting quantitative assessment with fine-grained qualitative rationales, RMTS enhances the trait-wise reliability, providing partial explanations about essays. The code is available at https://github.com/BBeeChu/RMTS.git.

**Link**: [arxiv](http://arxiv.org/abs/2410.14202v3),  [pdf](http://arxiv.org/pdf/2410.14202v3)

**Tags**: cs.CL cs.AI 



### Position: Editing Large Language Models Poses Serious Safety Risks
**Authors**: Paul Youssef, Zhixue Zhao, Daniel Braun, Jörg Schlötterer, Christin Seifert

**Updated**: 2025-02-05T07:51:32Z

**Summary**: Large Language Models (LLMs) contain large amounts of facts about the world. These facts can become outdated over time, which has led to the development of knowledge editing methods (KEs) that can change specific facts in LLMs with limited side effects. This position paper argues that editing LLMs poses serious safety risks that have been largely overlooked. First, we note the fact that KEs are widely available, computationally inexpensive, highly performant, and stealthy makes them an attractive tool for malicious actors. Second, we discuss malicious use cases of KEs, showing how KEs can be easily adapted for a variety of malicious purposes. Third, we highlight vulnerabilities in the AI ecosystem that allow unrestricted uploading and downloading of updated models without verification. Fourth, we argue that a lack of social and institutional awareness exacerbates this risk, and discuss the implications for different stakeholders. We call on the community to (i) research tamper-resistant models and countermeasures against malicious model editing, and (ii) actively engage in securing the AI ecosystem.

**Link**: [arxiv](http://arxiv.org/abs/2502.02958v1),  [pdf](http://arxiv.org/pdf/2502.02958v1)

**Tags**: cs.CL 



### Multilingual Machine Translation with Open Large Language Models at   Practical Scale: An Empirical Study
**Authors**: Menglong Cui, Pengzhi Gao, Wei Liu, Jian Luan, Bin Wang

**Updated**: 2025-02-05T07:48:33Z

**Summary**: Large language models (LLMs) have shown continuously improving multilingual capabilities, and even small-scale open-source models have demonstrated rapid performance enhancement. In this paper, we systematically explore the abilities of open LLMs with less than ten billion parameters to handle multilingual machine translation (MT) tasks. We conduct comprehensive evaluations on six popular LLMs and find that models like Gemma2-9B exhibit impressive multilingual translation capabilities. We then introduce the Parallel-First Monolingual-Second (PFMS) data mixing strategy in the continual pretraining stage to further enhance the MT performance and present GemmaX2-28, a 9B model achieving top-tier multilingual translation performance across 28 languages. Specifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA) models such as TowerInstruct and XALMA and achieves competitive performance with Google Translate and GPT-4-turbo.

**Link**: [arxiv](http://arxiv.org/abs/2502.02481v2),  [pdf](http://arxiv.org/pdf/2502.02481v2)

**Tags**: cs.CL 



### Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse   Representation Adjustment in Large Language Models
**Authors**: Guobin Shen, Dongcheng Zhao, Yiting Dong, Xiang He, Yi Zeng

**Updated**: 2025-02-05T07:23:59Z

**Summary**: As large language models (LLMs) become integral to various applications, ensuring both their safety and utility is paramount. Jailbreak attacks, which manipulate LLMs into generating harmful content, pose significant challenges to this balance. Existing defenses, such as prompt engineering and safety fine-tuning, often introduce computational overhead, increase inference latency, and lack runtime flexibility. Moreover, overly restrictive safety measures can degrade model utility by causing refusals of benign queries. In this paper, we introduce Jailbreak Antidote, a method that enables real-time adjustment of LLM safety preferences by manipulating a sparse subset of the model's internal states during inference. By shifting the model's hidden representations along a safety direction with varying strengths, we achieve flexible control over the safety-utility balance without additional token overhead or inference delays. Our analysis reveals that safety-related information in LLMs is sparsely distributed; adjusting approximately 5% of the internal state is as effective as modifying the entire state. Extensive experiments on nine LLMs (ranging from 2 billion to 72 billion parameters), evaluated against ten jailbreak attack methods and compared with six defense strategies, validate the effectiveness and efficiency of our approach. By directly manipulating internal states during reasoning, Jailbreak Antidote offers a lightweight, scalable solution that enhances LLM safety while preserving utility, opening new possibilities for real-time safety mechanisms in widely-deployed AI systems.

**Link**: [arxiv](http://arxiv.org/abs/2410.02298v3),  [pdf](http://arxiv.org/pdf/2410.02298v3)

**Tags**: cs.CR cs.CL 



### LLM-KT: Aligning Large Language Models with Knowledge Tracing using a   Plug-and-Play Instruction
**Authors**: Ziwei Wang, Jie Zhou, Qin Chen, Min Zhang, Bo Jiang, Aimin Zhou, Qinchun Bai, Liang He

**Updated**: 2025-02-05T07:21:49Z

**Summary**: The knowledge tracing (KT) problem is an extremely important topic in personalized education, which aims to predict whether students can correctly answer the next question based on their past question-answer records. Prior work on this task mainly focused on learning the sequence of behaviors based on the IDs or textual information. However, these studies usually fail to capture students' sufficient behavioral patterns without reasoning with rich world knowledge about questions. In this paper, we propose a large language models (LLMs)-based framework for KT, named \texttt{\textbf{LLM-KT}}, to integrate the strengths of LLMs and traditional sequence interaction models. For task-level alignment, we design Plug-and-Play instruction to align LLMs with KT, leveraging LLMs' rich knowledge and powerful reasoning capacity. For modality-level alignment, we design the plug-in context and sequence to integrate multiple modalities learned by traditional methods. To capture the long context of history records, we present a plug-in context to flexibly insert the compressed context embedding into LLMs using question-specific and concept-specific tokens. Furthermore, we introduce a plug-in sequence to enhance LLMs with sequence interaction behavior representation learned by traditional sequence models using a sequence adapter. Extensive experiments show that \texttt{\textbf{LLM-KT}} obtains state-of-the-art performance on four typical datasets by comparing it with approximately 20 strong baselines.

**Link**: [arxiv](http://arxiv.org/abs/2502.02945v1),  [pdf](http://arxiv.org/pdf/2502.02945v1)

**Tags**: cs.CL cs.AI 



### From Yes-Men to Truth-Tellers: Addressing Sycophancy in Large Language   Models with Pinpoint Tuning
**Authors**: Wei Chen, Zhen Huang, Liang Xie, Binbin Lin, Houqiang Li, Le Lu, Xinmei Tian, Deng Cai, Yonggang Zhang, Wenxiao Wang, Xu Shen, Jieping Ye

**Updated**: 2025-02-05T07:09:32Z

**Summary**: Large Language Models (LLMs) tend to prioritize adherence to user prompts over providing veracious responses, leading to the sycophancy issue. When challenged by users, LLMs tend to admit mistakes and provide inaccurate responses even if they initially provided the correct answer. Recent works propose to employ supervised fine-tuning (SFT) to mitigate the sycophancy issue, while it typically leads to the degeneration of LLMs' general capability. To address the challenge, we propose a novel supervised pinpoint tuning (SPT), where the region-of-interest modules are tuned for a given objective. Specifically, SPT first reveals and verifies a small percentage (<5%) of the basic modules, which significantly affect a particular behavior of LLMs. i.e., sycophancy. Subsequently, SPT merely fine-tunes these identified modules while freezing the rest. To verify the effectiveness of the proposed SPT, we conduct comprehensive experiments, demonstrating that SPT significantly mitigates the sycophancy issue of LLMs (even better than SFT). Moreover, SPT introduces limited or even no side effects on the general capability of LLMs. Our results shed light on how to precisely, effectively, and efficiently explain and improve the targeted ability of LLMs. Code and data are available at https://github.com/yellowtownhz/sycophancy-interpretability.

**Link**: [arxiv](http://arxiv.org/abs/2409.01658v3),  [pdf](http://arxiv.org/pdf/2409.01658v3)

**Tags**: cs.CL 



### Weak-to-Strong Diffusion with Reflection
**Authors**: Lichen Bai, Masashi Sugiyama, Zeke Xie

**Updated**: 2025-02-05T07:01:26Z

**Summary**: The goal of diffusion generative models is to align the learned distribution with the real data distribution through gradient score matching. However, inherent limitations in training data quality, modeling strategies, and architectural design lead to inevitable gap between generated outputs and real data. To reduce this gap, we propose Weak-to-Strong Diffusion (W2SD), a novel framework that utilizes the estimated difference between existing weak and strong models (i.e., weak-to-strong difference) to approximate the gap between an ideal model and a strong model. By employing a reflective operation that alternates between denoising and inversion with weak-to-strong difference, we theoretically understand that W2SD steers latent variables along sampling trajectories toward regions of the real data distribution. W2SD is highly flexible and broadly applicable, enabling diverse improvements through the strategic selection of weak-to-strong model pairs (e.g., DreamShaper vs. SD1.5, good experts vs. bad experts in MoE). Extensive experiments demonstrate that W2SD significantly improves human preference, aesthetic quality, and prompt adherence, achieving SOTA performance across various modalities (e.g., image, video), architectures (e.g., UNet-based, DiT-based, MoE), and benchmarks. For example, Juggernaut-XL with W2SD can improve with the HPSv2 winning rate up to 90% over the original results. Moreover, the performance gains achieved by W2SD markedly outweigh its additional computational overhead, while the cumulative improvements from different weak-to-strong difference further solidify its practical utility and deployability.

**Link**: [arxiv](http://arxiv.org/abs/2502.00473v2),  [pdf](http://arxiv.org/pdf/2502.00473v2)

**Tags**: cs.LG cs.CV 



### Mixture-of-Instructions: Aligning Large Language Models via Mixture   Prompting
**Authors**: Bowen Xu, Shaoyu Wu, Kai Liu, Lulu Hu

**Updated**: 2025-02-05T06:56:47Z

**Summary**: With the proliferation of large language models (LLMs), the comprehensive alignment of such models across multiple tasks has emerged as a critical area of research. Existing alignment methodologies primarily address single task, such as multi-turn dialogue, coding, mathematical problem-solving, and tool usage. Although there is a large amount of high-quality data available for those tasks, most of them provide only questions and answers without including the system prompt. Though a detailed analysis of the Qwen language model, we found that the system prompt has a significant impact on both training and inference processes of LLM. We attributes this phenomenon to overfitting to the system prompt. In address this issue, we introduce a novel technique termed Mixture-of-Instructions (MoI), which employs a strategy of instruction packing combined with diverse system prompts to boost the alignment efficiency of language models. We have also compiled a diverse set of seven benchmark datasets to rigorously evaluate the alignment efficacy of the MoI-enhanced language model. Our methodology was applied to the open-source Qwen-7B-chat model, culminating in the development of Qwen-SFT-MoI. This enhanced model demonstrates significant advancements in generative capabilities across coding, mathematics, and tool use tasks.

**Link**: [arxiv](http://arxiv.org/abs/2404.18410v2),  [pdf](http://arxiv.org/pdf/2404.18410v2)

**Tags**: cs.CL 



### TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text   Classification with Minimal Supervision
**Authors**: Yunyi Zhang, Ruozhen Yang, Xueqiang Xu, Rui Li, Jinfeng Xiao, Jiaming Shen, Jiawei Han

**Updated**: 2025-02-05T06:45:46Z

**Summary**: Hierarchical text classification aims to categorize each document into a set of classes in a label taxonomy, which is a fundamental web text mining task with broad applications such as web content analysis and semantic indexing. Most earlier works focus on fully or semi-supervised methods that require a large amount of human annotated data which is costly and time-consuming to acquire. To alleviate human efforts, in this paper, we work on hierarchical text classification with a minimal amount of supervision: using the sole class name of each node as the only supervision. Recently, large language models (LLM) have shown competitive performance on various tasks through zero-shot prompting, but this method performs poorly in the hierarchical setting because it is ineffective to include the large and structured label space in a prompt. On the other hand, previous weakly-supervised hierarchical text classification methods only utilize the raw taxonomy skeleton and ignore the rich information hidden in the text corpus that can serve as additional class-indicative features. To tackle the above challenges, we propose TELEClass, Taxonomy Enrichment and LLM-Enhanced weakly-supervised hierarchical text Classification, which combines the general knowledge of LLMs and task-specific features mined from an unlabeled corpus. TELEClass automatically enriches the raw taxonomy with class-indicative features for better label space understanding and utilizes novel LLM-based data annotation and generation methods specifically tailored for the hierarchical setting. Experiments show that TELEClass can significantly outperform previous baselines while achieving comparable performance to zero-shot prompting of LLMs with drastically less inference cost.

**Link**: [arxiv](http://arxiv.org/abs/2403.00165v3),  [pdf](http://arxiv.org/pdf/2403.00165v3)

**Tags**: cs.CL cs.LG 



### Large Language Model Guided Self-Debugging Code Generation
**Authors**: Muntasir Adnan, Zhiwei Xu, Carlos C. N. Kuhn

**Updated**: 2025-02-05T06:43:40Z

**Summary**: Automated code generation is gaining significant importance in intelligent computer programming and system deployment. However, current approaches often face challenges in computational efficiency and lack robust mechanisms for code parsing and error correction. In this work, we propose a novel framework, PyCapsule, with a simple yet effective two-agent pipeline and efficient self-debugging modules for Python code generation. PyCapsule features sophisticated prompt inference, iterative error handling, and case testing, ensuring high generation stability, safety, and correctness. Empirically, PyCapsule achieves up to 5.7% improvement of success rate on HumanEval, 10.3% on HumanEval-ET, and 24.4% on BigCodeBench compared to the state-of-art methods. We also observe a decrease in normalized success rate given more self-debugging attempts, potentially affected by limited and noisy error feedback in retention. PyCapsule demonstrates broader impacts on advancing lightweight and efficient code generation for artificial intelligence systems.

**Link**: [arxiv](http://arxiv.org/abs/2502.02928v1),  [pdf](http://arxiv.org/pdf/2502.02928v1)

**Tags**: cs.SE cs.AI 



### Can Many-Shot In-Context Learning Help LLMs as Evaluators? A Preliminary   Empirical Study
**Authors**: Mingyang Song, Mao Zheng, Xuan Luo, Yue Pan

**Updated**: 2025-02-05T06:42:43Z

**Summary**: Utilizing Large Language Models (LLMs) as evaluators to assess the performance of LLMs has garnered attention. However, this kind of evaluation approach is affected by potential biases within LLMs, raising concerns about the accuracy and reliability of the evaluation results of LLMs. To address this problem, we propose and study two many-shot In-Context Learning (ICL) prompt templates to help LLM evaluators mitigate potential biases: Many-Shot with Reference (MSwR) and Many-Shot without Reference (MSoR). Specifically, the former utilizes in-context examples with model-generated evaluation rationales as references, while the latter does not include these references. Using these prompt designs, we investigate the impact of increasing the number of in-context examples on the consistency and quality of the evaluation results. Experimental results show that advanced LLMs, such as GPT-4o, perform better in the many-shot regime than in the zero-shot and few-shot regimes. Furthermore, when using GPT-4o as an evaluator in the many-shot regime, adopting MSwR as the prompt template performs better than MSoR.

**Link**: [arxiv](http://arxiv.org/abs/2406.11629v6),  [pdf](http://arxiv.org/pdf/2406.11629v6)

**Tags**: cs.CL 



### Privacy Token: Surprised to Find Out What You Accidentally Revealed
**Authors**: Jiayang Meng, Tao Huang, Xin Shi, Qingyu Huang, Chen Hou, Hong Chen

**Updated**: 2025-02-05T06:20:20Z

**Summary**: The widespread deployment of deep learning models in privacy-sensitive domains has amplified concerns regarding privacy risks, particularly those stemming from gradient leakage during training. Current privacy assessments primarily rely on post-training attack simulations. However, these methods are inherently reactive, unable to encompass all potential attack scenarios, and often based on idealized adversarial assumptions. These limitations underscore the need for proactive approaches to privacy risk assessment during the training process. To address this gap, we propose the concept of privacy tokens, which are derived directly from private gradients during training. Privacy tokens encapsulate gradient features and, when combined with data features, offer valuable insights into the extent of private information leakage from training data, enabling real-time measurement of privacy risks without relying on adversarial attack simulations. Additionally, we employ Mutual Information (MI) as a robust metric to quantify the relationship between training data and gradients, providing precise and continuous assessments of privacy leakage throughout the training process. Extensive experiments validate our framework, demonstrating the effectiveness of privacy tokens and MI in identifying and quantifying privacy risks. This proactive approach marks a significant advancement in privacy monitoring, promoting the safer deployment of deep learning models in sensitive applications.

**Link**: [arxiv](http://arxiv.org/abs/2502.02913v1),  [pdf](http://arxiv.org/pdf/2502.02913v1)

**Tags**: cs.LG cs.CR 



### Train-Attention: Meta-Learning Where to Focus in Continual Knowledge   Learning
**Authors**: Yeongbin Seo, Dongha Lee, Jinyoung Yeo

**Updated**: 2025-02-05T06:12:13Z

**Summary**: Previous studies on continual knowledge learning (CKL) in large language models (LLMs) have predominantly focused on approaches such as regularization, architectural modifications, and rehearsal techniques to mitigate catastrophic forgetting. However, these methods naively inherit the inefficiencies of standard training procedures, indiscriminately applying uniform weight across all tokens, which can lead to unnecessary parameter updates and increased forgetting. To address these shortcomings, we propose a novel CKL approach termed Train-Attention-Augmented Language Model (TAALM), which enhances learning efficiency by dynamically predicting and applying weights to tokens based on their usefulness. This method employs a meta-learning framework that optimizes token importance predictions, facilitating targeted knowledge updates and minimizing forgetting. Also, we observe that existing benchmarks do not clearly exhibit the trade-off between learning and retaining, therefore we propose a new benchmark, \textsc{LAMA-ckl}, to address this issue. Through experiments conducted on both newly introduced and established CKL benchmarks, TAALM proves the state-of-the-art performance upon the baselines, and also shows synergistic compatibility when integrated with previous CKL approaches.

**Link**: [arxiv](http://arxiv.org/abs/2407.16920v2),  [pdf](http://arxiv.org/pdf/2407.16920v2)

**Tags**: cs.CL 68T50 I.2.7 



### SPARC: Subspace-Aware Prompt Adaptation for Robust Continual Learning in   LLMs
**Authors**: Dinithi Jayasuriya, Sina Tayebati, Davide Ettori, Ranganath Krishnan, Amit Ranjan Trivedi

**Updated**: 2025-02-05T06:11:55Z

**Summary**: We propose SPARC, a lightweight continual learning framework for large language models (LLMs) that enables efficient task adaptation through prompt tuning in a lower-dimensional space. By leveraging principal component analysis (PCA), we identify a compact subspace of the training data. Optimizing prompts in this lower-dimensional space enhances training efficiency, as it focuses updates on the most relevant features while reducing computational overhead. Furthermore, since the model's internal structure remains unaltered, the extensive knowledge gained from pretraining is fully preserved, ensuring that previously learned information is not compromised during adaptation. Our method achieves high knowledge retention in both task-incremental and domain-incremental continual learning setups while fine-tuning only 0.04% of the model's parameters. Additionally, by integrating LoRA, we enhance adaptability to computational constraints, allowing for a tradeoff between accuracy and training cost. Experiments on the SuperGLUE benchmark demonstrate that our PCA-based prompt tuning combined with LoRA maintains full knowledge retention while improving accuracy, utilizing only 1% of the model's parameters. These results establish our approach as a scalable and resource-efficient solution for continual learning in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2502.02909v1),  [pdf](http://arxiv.org/pdf/2502.02909v1)

**Tags**: cs.LG cs.AI cs.CL 



### COSMosFL: Ensemble of Small Language Models for Fault Localisation
**Authors**: Hyunjoon Cho, Sungmin Kang, Gabin An, Shin Yoo

**Updated**: 2025-02-05T06:09:26Z

**Summary**: LLMs are rapidly being adopted to build powerful tools and agents for software engineering, but most of them rely heavily on extremely large closed-source models. This, in turn, can hinder wider adoption due to security issues as well as financial cost and environmental impact. Recently, a number of open source Small Language Models (SLMs) are being released and gaining traction. While SLMs are smaller, more energy-efficient, and therefore easier to locally deploy, they tend to show worse performance when compared to larger closed LLMs. We present COSMos, a task-level LLM ensemble technique that uses voting mechanism, to provide a broader range of choice between SLMs and LLMs. We instantiate COSMos with an LLM-based Fault Localisation technique, AutoFL, and report the cost-benefit trade-off between LLM accuracy and various costs such as energy consumption, inference time, and the number of tokens used. An empirical evaluation using Defects4J shows that COSMos can build effective ensembles that can achieve Pareto-optimality in terms of FL accuracy and inference cost, when compared to individual models.

**Link**: [arxiv](http://arxiv.org/abs/2502.02908v1),  [pdf](http://arxiv.org/pdf/2502.02908v1)

**Tags**: cs.SE cs.LG 



### CoS: Enhancing Personalization and Mitigating Bias with Context Steering
**Authors**: Jerry Zhi-Yang He, Sashrika Pandey, Mariah L. Schrum, Anca Dragan

**Updated**: 2025-02-05T05:57:44Z

**Summary**: When querying a large language model (LLM), the context, i.e. personal, demographic, and cultural information specific to an end-user, can significantly shape the response of the LLM. For example, asking the model to explain Newton's second law with the context "I am a toddler" yields a different answer compared to the context "I am a physics professor." Proper usage of the context enables the LLM to generate personalized responses, whereas inappropriate contextual influence can lead to stereotypical and potentially harmful generations (e.g. associating "female" with "housekeeper"). In practice, striking the right balance when leveraging context is a nuanced and challenging problem that is often situation-dependent. One common approach to address this challenge is to fine-tune LLMs on contextually appropriate responses. However, this approach is expensive, time-consuming, and not controllable for end-users in different situations. In this work, we propose Context Steering (CoS) - a simple training-free method that can be easily applied to autoregressive LLMs at inference time. By measuring the contextual influence in terms of token prediction likelihood and modulating it, our method enables practitioners to determine the appropriate level of contextual influence based on their specific use case and end-user base. We showcase a variety of applications of CoS including amplifying the contextual influence to achieve better personalization and mitigating unwanted influence for reducing model bias. In addition, we show that we can combine CoS with Bayesian Inference to quantify the extent of hate speech on the internet. We demonstrate the effectiveness of CoS on state-of-the-art LLMs and benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2405.01768v2),  [pdf](http://arxiv.org/pdf/2405.01768v2)

**Tags**: cs.CL cs.AI 



