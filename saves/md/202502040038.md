# Arxiv Results
## Keyword: kv cache 
 ### Cache Me If You Must: Adaptive Key-Value Quantization for Large Language   Models
**Authors**: Alina Shutova, Vladimir Malinovskii, Vage Egiazarian, Denis Kuznedelev, Denis Mazur, Nikita Surkov, Ivan Ermakov, Dan Alistarh

**Updated**: 2025-01-31T18:47:42Z

**Summary**: Efficient real-world deployments of large language models (LLMs) rely on Key-Value (KV) caching for processing and generating long outputs, reducing the need for repetitive computation. For large contexts, Key-Value caches can take up tens of gigabytes of device memory, as they store vector representations for each token and layer. Recent work has shown that the cached vectors can be compressed through quantization, pruning or merging, but these techniques often compromise quality towards higher compression rates. In this work, we aim to improve Key & Value compression by exploiting two observations: 1) the inherent dependencies between keys and values across different layers, and 2) high-compression mechanisms for internal network states. We propose AQUA-KV, an adaptive quantization for Key-Value caches that relies on compact adapters to exploit existing dependencies between Keys and Values, and aims to "optimally" compress the information that cannot be predicted. AQUA-KV significantly improves compression rates, while maintaining high accuracy on state-of-the-art LLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5 bits per value with under $1\%$ relative error in perplexity and LongBench scores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a single GPU within 1-6 hours, even for 70B models.

**Link**: [arxiv](http://arxiv.org/abs/2501.19392v1),  [pdf](http://arxiv.org/pdf/2501.19392v1)

**Tags**: cs.LG 



### Offline Learning for Combinatorial Multi-armed Bandits
**Authors**: Xutong Liu, Xiangxiang Dai, Jinhang Zuo, Siwei Wang, Carlee-Joe Wong, John C. S. Lui, Wei Chen

**Updated**: 2025-01-31T16:56:18Z

**Summary**: The combinatorial multi-armed bandit (CMAB) is a fundamental sequential decision-making framework, extensively studied over the past decade. However, existing work primarily focuses on the online setting, overlooking the substantial costs of online interactions and the readily available offline datasets. To overcome these limitations, we introduce Off-CMAB, the first offline learning framework for CMAB. Central to our framework is the combinatorial lower confidence bound (CLCB) algorithm, which combines pessimistic reward estimations with combinatorial solvers. To characterize the quality of offline datasets, we propose two novel data coverage conditions and prove that, under these conditions, CLCB achieves a near-optimal suboptimality gap, matching the theoretical lower bound up to a logarithmic factor. We validate Off-CMAB through practical applications, including learning to rank, large language model (LLM) caching, and social influence maximization, showing its ability to handle nonlinear reward functions, general feedback models, and out-of-distribution action samples that excludes optimal or even feasible actions. Extensive experiments on synthetic and real-world datasets further highlight the superior performance of CLCB.

**Link**: [arxiv](http://arxiv.org/abs/2501.19300v1),  [pdf](http://arxiv.org/pdf/2501.19300v1)

**Tags**: cs.LG 



### Accelerating Diffusion Transformer via Error-Optimized Cache
**Authors**: Junxiang Qiu, Shuo Wang, Jinda Lu, Lin Liu, Houcheng Jiang, Yanbin Hao

**Updated**: 2025-01-31T15:58:15Z

**Summary**: Diffusion Transformer (DiT) is a crucial method for content generation. However, it needs a lot of time to sample. Many studies have attempted to use caching to reduce the time consumption of sampling. Existing caching methods accelerate generation by reusing DiT features from the previous time step and skipping calculations in the next, but they tend to locate and cache low-error modules without focusing on reducing caching-induced errors, resulting in a sharp decline in generated content quality when increasing caching intensity. To solve this problem, we propose the Error-Optimized Cache (EOC). This method introduces three key improvements: (1) Prior knowledge extraction: Extract and process the caching differences; (2) A judgment method for cache optimization: Determine whether certain caching steps need to be optimized; (3) Cache optimization: reduce caching errors. Experiments show that this algorithm significantly reduces the error accumulation caused by caching (especially over-caching). On the ImageNet dataset, without significantly increasing the computational burden, this method improves the quality of the generated images under the over-caching, rule-based, and training-based methods. Specifically, the Fr\'echet Inception Distance (FID) values are improved as follows: from 6.857 to 5.821, from 3.870 to 3.692 and form 3.539 to 3.451 respectively.

**Link**: [arxiv](http://arxiv.org/abs/2501.19243v1),  [pdf](http://arxiv.org/pdf/2501.19243v1)

**Tags**: cs.CV 



### HarmoniCa: Harmonizing Training and Inference for Better Feature Caching   in Diffusion Transformer Acceleration
**Authors**: Yushi Huang, Zining Wang, Ruihao Gong, Jing Liu, Xinjie Zhang, Jinyang Guo, Xianglong Liu, Jun Zhang

**Updated**: 2025-01-31T14:26:05Z

**Summary**: Diffusion Transformers (DiTs) excel in generative tasks but face practical deployment challenges due to high inference costs. Feature caching, which stores and retrieves redundant computations, offers the potential for acceleration. Existing learning-based caching, though adaptive, overlooks the impact of the prior timestep. It also suffers from misaligned objectives--aligned predicted noise vs. high-quality images--between training and inference. These two discrepancies compromise both performance and efficiency. To this end, we harmonize training and inference with a novel learning-based caching framework dubbed HarmoniCa. It first incorporates Step-Wise Denoising Training (SDT) to ensure the continuity of the denoising process, where prior steps can be leveraged. In addition, an Image Error Proxy-Guided Objective (IEPO) is applied to balance image quality against cache utilization through an efficient proxy to approximate the image error. Extensive experiments across $8$ models, $4$ samplers, and resolutions from $256\times256$ to $2K$ demonstrate superior performance and speedup of our framework. For instance, it achieves over $40\%$ latency reduction (i.e., $2.07\times$ theoretical speedup) and improved performance on PixArt-$\alpha$. Remarkably, our image-free approach reduces training time by $25\%$ compared with the previous method.

**Link**: [arxiv](http://arxiv.org/abs/2410.01723v3),  [pdf](http://arxiv.org/pdf/2410.01723v3)

**Tags**: cs.CV 



### CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning
**Authors**: Fanxu Meng, Pingzhi Tang, Fan jiang, Muhan Zhang

**Updated**: 2025-01-31T14:13:49Z

**Summary**: Decoder-only models generate tokens autoregressively by caching key/value vectors, but as the cache grows, inference becomes memory-bound. To address this issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel approach that treats pairs of attention layers as a set of low-rank decompositions. CLOVER applies Singular Value Decomposition (SVD) to the \( Q \)-\( K \) and \( V \)-\( O \) pairs within each attention head. The resulting singular values can either guide pruning or serve as trainable parameters for efficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning, these values are reintegrated into the model without increasing its parameter count. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite, Whisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results demonstrate that CLOVER significantly improves pruning efficiency. For instance, the perplexity of pruning 70\% of the \( Q \)-\( K \) pairs in GPT-2 XL is similar to that of pruning just 8\% with vanilla methods. Fine-tuning the singular values further results in a full-rank update, outperforming state-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\%, 5.5\%, 3.8\%, and 0.7\%, respectively, on eight commonsense tasks for LLaMA-2 7B.

**Link**: [arxiv](http://arxiv.org/abs/2411.17426v3),  [pdf](http://arxiv.org/pdf/2411.17426v3)

**Tags**: cs.LG cs.AI 



### Swift: Rethinking RDMA Control Plane for Elastic Computing
**Authors**: Junxue Zhang, Han Tian, Xinyang Huang, Wenxue Li, Kaiqiang Xu, Dian Shen, Yong Wang, Kai Chen

**Updated**: 2025-01-31T11:25:40Z

**Summary**: Elastic computing enables dynamic scaling to meet workload demands, and Remote Direct Memory Access (RDMA) enhances this by providing high-throughput, low-latency network communication. However, integrating RDMA into elastic computing remains a challenge, particularly in control plane operations for RDMA connection setup.   This paper revisits the assumptions of prior work on high-performance RDMA for elastic computing, and reveals that extreme microsecond-level control plane optimizations are often unnecessary. By challenging the conventional beliefs on the slowness of user-space RDMA control plane and the difficulty of user-space RDMA resource sharing, we uncover new design opportunities. Our key insight is that user-space RDMA connection setup can be significantly improved with caching, while RDMA resources can be efficiently shared among processes using fork. In light of this, we propose Swift, a simple yet effective solution that co-designs RDMA with a serverless framework to optimize performance for elastic computing. At its very core, Swift handles cold and warm serverless requests by swiftly initializing the RDMA control plane with cache-optimized libibverbs, and manages fork requests by leveraging the RDMA's fork capability. Implemented with OpenWhisk, Swift delivers 30.56-46.50% higher average throughput and 18.55-37.21% lower latency, at a cost of 6.5% control plane overhead, compared to prior solutions.

**Link**: [arxiv](http://arxiv.org/abs/2501.19051v1),  [pdf](http://arxiv.org/pdf/2501.19051v1)

**Tags**: cs.NI 



### The development of IBIC microscopy at the 100 kV ion implanter of the   University of Torino (LIUTo) and the application for the assessment of the   radiation hardness of a silicon photodiode
**Authors**: Emilio Corte, Alberto Bortone, Elena Nieto Hernández, Carlo Ceresa, Georgios Provatas, Karla Ivanković Nizić, Milko Jaksić, Ettore Vittone, Sviatoslav Ditalia Tchernij

**Updated**: 2025-01-31T10:43:00Z

**Summary**: The Ion Beam Induced Charge (IBIC) technique is widely used to characterize the electronic properties of semiconductor materials and devices. Its main advantage over other charge collection microscopies stems in the use of MeV ion probes, which provide both measurable induced charge signals from single ions, and high spatial resolution, which is maintained along the ion range. It is a fact, however, that the use of low-energy ions in the keV range can provide the IBIC technique with complementary analytical capabilities, that are not available with MeV ions, for example the higher sensitivity to the status, contamination and morphology of the surface and the fact that the induced signal depends on the transport of only one type of charge carrier. This paper outlines the upgrade that was made at the 100 kV ion implanter of the University of Torino, originally installed for material and surface modification, to explore the rather unexplored keV-IBIC field and to assess its potential to characterize semiconductor devices. Finally, we report the first IBIC application of our apparatus, which regards the assessment of the radiation damage of a commercially available silicon photodiode, adopting the IAEA experimental protocol and the relevant interpretative model.

**Link**: [arxiv](http://arxiv.org/abs/2501.19021v1),  [pdf](http://arxiv.org/pdf/2501.19021v1)

**Tags**: physics.ins-det 



### Memory-Efficient Fine-Tuning of Transformers via Token Selection
**Authors**: Antoine Simoulin, Namyong Park, Xiaoyi Liu, Grey Yang

**Updated**: 2025-01-31T00:43:50Z

**Summary**: Fine-tuning provides an effective means to specialize pre-trained models for various downstream tasks. However, fine-tuning often incurs high memory overhead, especially for large transformer-based models, such as LLMs. While existing methods may reduce certain parts of the memory required for fine-tuning, they still require caching all intermediate activations computed in the forward pass to update weights during the backward pass. In this work, we develop TokenTune, a method to reduce memory usage, specifically the memory to store intermediate activations, in the fine-tuning of transformer-based models. During the backward pass, TokenTune approximates the gradient computation by backpropagating through just a subset of input tokens. Thus, with TokenTune, only a subset of intermediate activations are cached during the forward pass. Also, TokenTune can be easily combined with existing methods like LoRA, further reducing the memory cost. We evaluate our approach on pre-trained transformer models with up to billions of parameters, considering the performance on multiple downstream tasks such as text classification and question answering in a few-shot learning setup. Overall, TokenTune achieves performance on par with full fine-tuning or representative memory-efficient fine-tuning methods, while greatly reducing the memory footprint, especially when combined with other methods with complementary memory reduction mechanisms. We hope that our approach will facilitate the fine-tuning of large transformers, in specializing them for specific domains or co-training them with other neural components from a larger system. Our code is available at https://github.com/facebookresearch/tokentune.

**Link**: [arxiv](http://arxiv.org/abs/2501.18824v1),  [pdf](http://arxiv.org/pdf/2501.18824v1)

**Tags**: cs.CL cs.LG 



### REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and   Failure Mitigation
**Authors**: Tommaso Bonato, Abdul Kabbani, Ahmad Ghalayini, Michael Papamichael, Mohammad Dohadwala, Lukas Gianinazzi, Mikhail Khalilov, Elias Achermann, Daniele De Sensi, Torsten Hoefler

**Updated**: 2025-01-30T18:23:46Z

**Summary**: Next-generation datacenters require highly efficient network load balancing to manage the growing scale of artificial intelligence (AI) training and general datacenter traffic. Existing solutions designed for Ethernet, such as Equal Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to maintain high network utilizations as datacenter topologies (and network failures as a consequence) continue to grow. To address these limitations, we propose REPS, a lightweight decentralized per-packet adaptive load balancing algorithm designed to optimize network utilization while ensuring rapid recovery from link failures. REPS adapts to network conditions by caching good-performing paths. In case of a network failure, REPS re-routes traffic away from it in less than 100 microseconds. REPS is designed to be deployed with next-generation out-of-order transports, such as Ultra Ethernet, and introduces less than 25 bytes of per-connection state. We extensively evaluate REPS in large-scale simulations and FPGA-based NICs.

**Link**: [arxiv](http://arxiv.org/abs/2407.21625v3),  [pdf](http://arxiv.org/pdf/2407.21625v3)

**Tags**: cs.NI 



### State Stream Transformer (SST) : Emergent Metacognitive Behaviours   Through Latent State Persistence
**Authors**: Thea Aviss

**Updated**: 2025-01-30T14:03:36Z

**Summary**: We introduce the State Stream Transformer (SST), a novel LLM architecture that reveals emergent reasoning behaviours and capabilities latent in pretrained weights through addressing a fundamental limitation in traditional transformer models: the lack of latent computational continuity across autoregressive generations in the state space. SST introduces a sliding window latent state (FFN) cache with weighted decay that maintains and evolves persistent latent processes throughout autoregressive generations. Through controlled experiments comparing base and SST architectures using the same frozen weights, we demonstrate that this architectural modification alone enables enhanced reasoning capabilities which appear best explained by some form of potential higher-order processing, as evidenced by emergent metacognitive behaviours. These behaviours persist under controlled conditions designed to eliminate confounding factors such as stochastic variation or learned response patterns. Analysis of latent state distributions and processing dynamics provides evidence that it is solely the 'state stream' that is responsible for these phenomena. In quantitative evaluations, the SST achieves substantial performance improvements over the base model on two reasoning benchmarks, reaching 89.01\% accuracy on GSM-8K (0-shot) and 91.04\% on ARC Challenge (0-shot CoT). These findings indicate that persistent computation in the latent state space enables fundamentally different information processing and internal reasoning strategies, with implications for our understanding of artificial intelligence systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.18356v1),  [pdf](http://arxiv.org/pdf/2501.18356v1)

**Tags**: cs.LG cs.AI cs.CL 



### Locret: Enhancing Eviction in Long-Context LLM Inference with Trained   Retaining Heads on Consumer-Grade Devices
**Authors**: Yuxiang Huang, Binhang Yuan, Xu Han, Chaojun Xiao, Zhiyuan Liu

**Updated**: 2025-01-30T13:07:37Z

**Summary**: Scaling the input context length of a large language model (LLM) incurs a significant increase in computation cost and memory footprint to maintain the attention key-value (KV) cache. Existing KV cache compression methods suffer from inefficient compression strategies and limited memory reduction effects, making it difficult for LLMs to conduct long-context inference on consumer-grade devices, especially when inferring long-context stream input. Such obstacles prevent consumer-grade devices from supporting more complex applications, creating challenges for the democratization of LLMs. To overcome this, we propose Locret, the first framework to create an eviction policy compatible with chunked prefill. By evaluating the causal importance of KV cache units by learnable retaining heads, Locret enables precise eviction of cache units, facilitating efficient long-context inference. In our extensive empirical studies, Locret outperforms the recent popular and competitive approaches in terms of memory efficiency and generation quality -- Locret achieves up to 20x of KV cache compression ratio within less than 10% performance loss. Furthermore, Locret achieves 128K+ long-context inference on a single NVIDIA 4090 GPU without compromising generation quality and only costs <1 GPU hour of additional training.

**Link**: [arxiv](http://arxiv.org/abs/2410.01805v2),  [pdf](http://arxiv.org/pdf/2410.01805v2)

**Tags**: cs.CL 



### Systematic Evaluation of Randomized Cache Designs against Cache   Occupancy
**Authors**: Anirban Chakraborty, Nimish Mishra, Sayandeep Saha, Sarani Bhattacharya, Debdeep Mukhopadhyay

**Updated**: 2025-01-30T06:02:11Z

**Summary**: Randomizing the address-to-set mapping and partitioning of the cache has been shown to be an effective mechanism in designing secured caches. Several designs have been proposed on a variety of rationales: (1) randomized design, (2) randomized-and-partitioned design, and (3) psuedo-fully associative design. This work fills in a crucial gap in current literature on randomized caches: currently most randomized cache designs defend only contention-based attacks, and leave out considerations of cache occupancy. We perform a systematic evaluation of 5 randomized cache designs- CEASER, CEASER-S, MIRAGE, Scatter-Cache, and Sass-cache against cache occupancy wrt. both performance as well as security.   With respect to performance, we first establish that benchmarking strategies used by contemporary designs are unsuitable for a fair evaluation (because of differing cache configurations, choice of benchmarking suites, additional implementation-specific assumptions). We thus propose a uniform benchmarking strategy, which allows us to perform a fair and comparative analysis across all designs under various replacement policies. Likewise, with respect to security against cache occupancy attacks, we evaluate the cache designs against various threat assumptions: (1) covert channels, (2) process fingerprinting, and (3) AES key recovery (to the best of our knowledge, this work is the first to demonstrate full AES key recovery on a randomized cache design using cache occupancy attack). Our results establish the need to also consider cache occupancy side-channel in randomized cache design considerations.

**Link**: [arxiv](http://arxiv.org/abs/2310.05172v2),  [pdf](http://arxiv.org/pdf/2310.05172v2)

**Tags**: cs.CR cs.AR 



### Reciprocating Locks
**Authors**: Dave Dice, Alex Kogan

**Updated**: 2025-01-29T16:44:27Z

**Summary**: We present "Reciprocating Locks", a novel mutual exclusion locking algorithm, targeting cache-coherent shared memory (CC), that enjoys a number of desirable properties. The doorway arrival phase and the release operation both run in constant-time. Waiting threads use local spinning and only a single waiting element is required per thread, regardless of the number of locks a thread might hold at a given time. While our lock does not provide strict FIFO admission, it bounds bypass and has strong anti-starvation properties. The lock is compact, space efficient, and has been intentionally designed to be readily usable in real-world general purpose computing environments such as the linux kernel, pthreads, or C++. We show the lock exhibits high throughput under contention and low latency in the uncontended case. The performance of Reciprocating Locks is competitive with and often better than the best state-of-the-art scalable spin locks.

**Link**: [arxiv](http://arxiv.org/abs/2501.02380v5),  [pdf](http://arxiv.org/pdf/2501.02380v5)

**Tags**: cs.DC D.4.1 



### vAttention: Dynamic Memory Management for Serving LLMs without   PagedAttention
**Authors**: Ramya Prabhu, Ajay Nayak, Jayashree Mohan, Ramachandran Ramjee, Ashish Panwar

**Updated**: 2025-01-29T04:10:41Z

**Summary**: PagedAttention is a popular approach for dynamic memory allocation in LLM serving systems. It enables on-demand allocation of GPU memory to mitigate KV cache fragmentation -- a phenomenon that crippled the batch size (and consequently throughput) in prior systems. However, in trying to allocate physical memory at runtime, PagedAttention ends up changing the virtual memory layout of the KV cache from contiguous to non-contiguous. Such a design leads to non-trivial programming and performance overheads.   We present vAttention -- an approach that mitigates fragmentation in physical memory while retaining the contiguity of KV cache in virtual memory. We achieve this by decoupling the allocation of virtual and physical memory using CUDA virtual memory management APIs. We also introduce various LLM-specific optimizations to address the limitations of CUDA virtual memory support. Overall, vAttention is a simpler, portable, and performant alternative to PagedAttention: it supports various attention kernels out-of-the-box and improves LLM serving throughput by up to 1.23x compared to the use of PagedAttention-based kernels of FlashAttention and FlashInfer.

**Link**: [arxiv](http://arxiv.org/abs/2405.04437v3),  [pdf](http://arxiv.org/pdf/2405.04437v3)

**Tags**: cs.LG cs.OS 



### Optimizing SSD Caches for Cloud Block Storage Systems Using Machine   Learning Approaches
**Authors**: Chiyu Cheng, Chang Zhou, Yang Zhao, Jin Cao

**Updated**: 2025-01-28T20:35:23Z

**Summary**: The growing demand for efficient cloud storage solutions has led to the widespread adoption of Solid-State Drives (SSDs) for caching in cloud block storage systems. The management of data writes to SSD caches plays a crucial role in improving overall system performance, reducing latency, and extending the lifespan of storage devices. A critical challenge arises from the large volume of write-only data, which significantly impacts the performance of SSD caches when handled inefficiently. Specifically, writes that have not been read for a certain period may introduce unnecessary write traffic to the SSD cache without offering substantial benefits for cache performance. This paper proposes a novel approach to mitigate this issue by leveraging machine learning techniques to dynamically optimize the write policy in cloud-based storage systems. The proposed method identifies write-only data and selectively filters it out in real-time, thereby minimizing the number of unnecessary write operations and improving the overall performance of the cache system. Experimental results demonstrate that the proposed machine learning-based policy significantly outperforms traditional approaches by reducing the number of harmful writes and optimizing cache utilization. This solution is particularly suitable for cloud environments with varying and unpredictable workloads, where traditional cache management strategies often fall short.

**Link**: [arxiv](http://arxiv.org/abs/2501.14770v2),  [pdf](http://arxiv.org/pdf/2501.14770v2)

**Tags**: cs.DC cs.LG cs.OS 



### Dynamic Adaptation in Data Storage: Real-Time Machine Learning for   Enhanced Prefetching
**Authors**: Chiyu Cheng, Chang Zhou, Yang Zhao, Jin Cao

**Updated**: 2025-01-28T20:33:43Z

**Summary**: The exponential growth of data storage demands has necessitated the evolution of hierarchical storage management strategies [1]. This study explores the application of streaming machine learning [3] to revolutionize data prefetching within multi-tiered storage systems. Unlike traditional batch-trained models, streaming machine learning [5] offers adaptability, real-time insights, and computational efficiency, responding dynamically to workload variations. This work designs and validates an innovative framework that integrates streaming classification models for predicting file access patterns, specifically the next file offset. Leveraging comprehensive feature engineering and real-time evaluation over extensive production traces, the proposed methodology achieves substantial improvements in prediction accuracy, memory efficiency, and system adaptability. The results underscore the potential of streaming models in real-time storage management, setting a precedent for advanced caching and tiering strategies.

**Link**: [arxiv](http://arxiv.org/abs/2501.14771v2),  [pdf](http://arxiv.org/pdf/2501.14771v2)

**Tags**: cs.DC cs.LG cs.OS 



### Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks   Detection: A Comparative Analysis
**Authors**: Tejal Joshi, Aarya Kawalay, Anvi Jamkhande, Amit Joshi

**Updated**: 2025-01-28T18:14:43Z

**Summary**: Cache side channel attacks are a sophisticated and persistent threat that exploit vulnerabilities in modern processors to extract sensitive information. These attacks leverage weaknesses in shared computational resources, particularly the last level cache, to infer patterns in data access and execution flows, often bypassing traditional security defenses. Such attacks are especially dangerous as they can be executed remotely without requiring physical access to the victim's device. This study focuses on a specific class of these threats: fingerprinting attacks, where an adversary monitors and analyzes the behavior of co-located processes via cache side channels. This can potentially reveal confidential information, such as encryption keys or user activity patterns. A comprehensive threat model illustrates how attackers sharing computational resources with target systems exploit these side channels to compromise sensitive data. To mitigate such risks, a hybrid deep learning model is proposed for detecting cache side channel attacks. Its performance is compared with five widely used deep learning models: Multi-Layer Perceptron, Convolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term Memory, and Gated Recurrent Unit. The experimental results demonstrate that the hybrid model achieves a detection rate of up to 99.96%. These findings highlight the limitations of existing models, the need for enhanced defensive mechanisms, and directions for future research to secure sensitive data against evolving side channel threats.

**Link**: [arxiv](http://arxiv.org/abs/2501.17123v1),  [pdf](http://arxiv.org/pdf/2501.17123v1)

**Tags**: cs.CR cs.NE 



### Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications
**Authors**: Mohammad NaseriTehrani, MohammadJavad Salehi, Antti Tölli

**Updated**: 2025-01-28T16:19:24Z

**Summary**: Integrating coded caching (CC) into multiple-input multiple-output (MIMO) communications can significantly enhance the achievable degrees of freedom (DoF) in wireless networks. This paper investigates a practical cache-aided asymmetric MIMO configuration with cache ratio $\gamma$, where a server equipped with $L$ transmit antennas communicates with $K$ users, each having $G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the \emph{min-G} scheme, which treats the system as symmetric by assuming all users have the same number of antennas, equal to the smallest among them; the \emph{Grouping} scheme, which maximizes spatial multiplexing gain separately within each user subset at the cost of some global caching gain; and the \emph{Phantom} scheme, which dynamically redistributes spatial resources using virtual or ``phantom'' antennas at the users, bridging the performance gains of the min-$G$ and Grouping schemes. These strategies jointly optimize the number of users, $\Omega$, and the parallel streams decoded by each user, $\beta_k$, ensuring linear decodability for all target users. Analytical and numerical results confirm that the proposed schemes achieve significant DoF improvements across various system configurations.

**Link**: [arxiv](http://arxiv.org/abs/2501.10854v2),  [pdf](http://arxiv.org/pdf/2501.10854v2)

**Tags**: cs.IT eess.SP math.IT 



### Measuring GPU utilization one level deeper
**Authors**: Paul Elvinger, Foteini Strati, Natalie Enright Jerger, Ana Klimovic

**Updated**: 2025-01-28T12:57:53Z

**Summary**: GPU hardware is vastly underutilized. Even resource-intensive AI applications have diverse resource profiles that often leave parts of GPUs idle. While colocating applications can improve utilization, current spatial sharing systems lack performance guarantees. Providing predictable performance guarantees requires a deep understanding of how applications contend for shared GPU resources such as block schedulers, compute units, L1/L2 caches, and memory bandwidth. We propose a methodology to profile resource interference of GPU kernels across these dimensions and discuss how to build GPU schedulers that provide strict performance guarantees while colocating applications to minimize cost.

**Link**: [arxiv](http://arxiv.org/abs/2501.16909v1),  [pdf](http://arxiv.org/pdf/2501.16909v1)

**Tags**: cs.DC 



### Optimizing Smart Helper Placement for Enhanced Cache Efficiency in   F-RANs
**Authors**: Hesameddin Mokhtarzadeh, Mohammed Saif, Md. Jahangir Hossain, Julian Cheng

**Updated**: 2025-01-28T00:22:34Z

**Summary**: Smart helpers (SHs) have been proposed to improve content delivery delays and alleviate high fronthaul loads in fog radio access networks (F-RANs). They offer an alternative to deploying additional enhanced remote radio heads (RRHs), which are often infeasible due to site constraints.} The optimal placement of SHs can significantly increase the number of users they serve which leads to enhanced cache efficiency and improved content delivery delay. In this letter, we optimize SH placement within an F-RAN to maximize the cache hit rate and further reduce the content delivery latency. We model the SH cache hit rate as a function of outage probability and user density distribution. We develop a function to estimate user density distribution leveraging the radial basis functions (RBFs) method and optimize SH placement utilizing the particle swarm optimization (PSO) algorithm. \an{Our} numerical results confirm the effectiveness of the proposed approach in maximizing the \an{SH cache hit rate}, thereby improving delivery delays and fronthaul loads of the network.

**Link**: [arxiv](http://arxiv.org/abs/2501.16597v1),  [pdf](http://arxiv.org/pdf/2501.16597v1)

**Tags**: eess.SP 



### Latency Guarantees for Caching with Delayed Hits
**Authors**: Keerthana Gurushankar, Noah G. Singer, Bernardo Subercaseaux

**Updated**: 2025-01-27T22:14:43Z

**Summary**: In the classical caching problem, when a requested page is not present in the cache (i.e., a "miss"), it is assumed to travel from the backing store into the cache "before" the next request arrives. However, in many real-life applications, such as content delivery networks, this assumption is unrealistic.   The "delayed-hits" model for caching, introduced by Atre, Sherry, Wang, and Berger, accounts for the latency between a missed cache request and the corresponding arrival from the backing store. This theoretical model has two parameters: the "delay" $Z$, representing the ratio between the retrieval delay and the inter-request delay in an application, and the "cache size" $k$, as in classical caching. Classical caching corresponds to $Z=1$, whereas larger values of $Z$ model applications where retrieving missed requests is expensive. Despite the practical relevance of the delayed-hits model, its theoretical underpinnings are still poorly understood.   We present the first tight theoretical guarantee for optimizing delayed-hits caching: The "Least Recently Used" algorithm, a natural, deterministic, online algorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at most $O(Zk)$ times more latency than the (offline) optimal schedule. Our result extends to any so-called "marking" algorithm.

**Link**: [arxiv](http://arxiv.org/abs/2501.16535v1),  [pdf](http://arxiv.org/pdf/2501.16535v1)

**Tags**: cs.DS 



### SP-IMPact: A Framework for Static Partitioning Interference Mitigation   and Performance Analysis
**Authors**: Diogo Costa, Gonçalo Moreira, Afonso Oliveira, José Martins, Sandro Pinto

**Updated**: 2025-01-27T17:42:20Z

**Summary**: Modern embedded systems are evolving toward complex, heterogeneous architectures to accommodate increasingly demanding applications. Driven by SWAP-C constraints, this shift has led to consolidating multiple systems onto single hardware platforms. Static Partitioning Hypervisors offer a promising solution to partition hardware resources and provide spatial isolation between critical workloads. However, shared resources like the Last-Level Cache and system bus can introduce temporal interference between virtual machines (VMs), negatively impacting performance and predictability. Over the past decade, academia and industry have developed interference mitigation techniques, such as cache partitioning and memory bandwidth reservation. However, configuring these techniques is complex and time-consuming. Cache partitioning requires balancing cache sections across VMs, while memory bandwidth reservation needs tuning bandwidth budgets and periods. Testing all configurations is impractical and often leads to suboptimal results. Moreover, understanding how these techniques interact is limited, as their combined use can produce compounded or conflicting effects on performance. Static analysis tools estimating worst-case execution times offer guidance for configuring mitigation techniques but often fail to capture the complexity of modern multi-core systems. They typically focus on limited shared resources while neglecting others, such as IOMMUs and interrupt controllers. To address these challenges, we present SP-IMPact, an open-source framework for analyzing and guiding interference mitigation configurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth reservation, while evaluating their interactions and cumulative impact. By providing insights on real hardware, SP-IMPact helps optimize configurations for mixed-criticality systems, ensuring performance and predictability.

**Link**: [arxiv](http://arxiv.org/abs/2501.16245v1),  [pdf](http://arxiv.org/pdf/2501.16245v1)

**Tags**: cs.DC cs.PF cs.SY eess.SY 



### Recommenadation aided Caching using Combinatorial Multi-armed Bandits
**Authors**: Pavamana K J, Chandramani Kishore Singh

**Updated**: 2025-01-27T14:55:40Z

**Summary**: We study content caching with recommendations in a wireless network where the users are connected through a base station equipped with a finite-capacity cache. We assume a fixed set of contents with unknown user preferences and content popularities. The base station can cache a subset of the contents and can also recommend subsets of the contents to different users in order to encourage them to request the recommended contents. Recommendations, depending on their acceptability, can thus be used to increase cache hits. We first assume that the users' recommendation acceptabilities are known and formulate the cache hit optimization problem as a combinatorial multi-armed bandit (CMAB). We propose a UCB-based algorithm to decide which contents to cache and recommend and provide an upper bound on the regret of this algorithm. Subsequently, we consider a more general scenario where the users' recommendation acceptabilities are also unknown and propose another UCB-based algorithm that learns these as well. We numerically demonstrate the performance of our algorithms and compare these to state-of-the-art algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2405.00080v4),  [pdf](http://arxiv.org/pdf/2405.00080v4)

**Tags**: cs.LG cs.IR cs.NI 



### SIC-free Multicast Scheduling for Multi-antenna Coded Caching
**Authors**: MohammadJavad Sojdeh, MohammadJavad Salehi, Antti Tölli

**Updated**: 2025-01-27T14:37:24Z

**Summary**: Multi-antenna coded caching (CC) with multicast beamforming typically relies on a complex successive interference cancellation (SIC) structure to decode a superposition of multiple streams received by each user. Signal-level CC schemes require the regeneration and cancellation of interfering signals at the physical layer of each receiver, which complicates practical implementations. To address this, we propose a bit-level multicast scheduling scheme enabling linear, SIC-free decoding of parallel streams by repeatedly transmitting data terms with linearly independent coefficients. Two reference strategies and a novel sparse strategy are considered for constructing the coefficient matrix. The reference cases include the random strategy, which lacks control over matrix construction, and the equal-distant strategy, which balances users' interference and data terms equally. In contrast, the sparse strategy minimizes the number of multicast streams transmitted in parallel during each interval. This approach simplifies both the decoding process and the beamforming design by decoupling the desired data terms for each user and reducing the number of SINR constraints, respectively. To further enhance the symmetric rate, a successive projection algorithm is applied to exploit channel properties and optimize user ordering. With the coefficient matrix and optimized user ordering in place, multicast beamformers are devised to aggregate desired data from relevant multicast streams. Numerical simulations validate the effectiveness of the sparse strategy and user scheduling, demonstrating significant gains in symmetric rate.

**Link**: [arxiv](http://arxiv.org/abs/2501.11126v2),  [pdf](http://arxiv.org/pdf/2501.11126v2)

**Tags**: cs.IT math.IT 



### Random Reshuffling for Stochastic Gradient Langevin Dynamics
**Authors**: Luke Shaw, Peter A. Whalley

**Updated**: 2025-01-27T13:53:12Z

**Summary**: We examine the use of different randomisation policies for stochastic gradient algorithms used in sampling, based on first-order (or overdamped) Langevin dynamics, the most popular of which is known as Stochastic Gradient Langevin Dynamics. Conventionally, this algorithm is combined with a specific stochastic gradient strategy, called Robbins-Monro. In this work, we study an alternative strategy, Random Reshuffling, and show convincingly that it leads to improved performance via: a) a proof of reduced bias in the Wasserstein metric for strongly convex, gradient Lipschitz potentials; b) an analytical demonstration of reduced bias for a Gaussian model problem; and c) an empirical demonstration of reduced bias in numerical experiments for some logistic regression problems. This is especially important since Random Reshuffling is typically more efficient due to memory access and cache reasons. Such acceleration for the Random Reshuffling policy is familiar from the optimisation literature on stochastic gradient descent.

**Link**: [arxiv](http://arxiv.org/abs/2501.16055v1),  [pdf](http://arxiv.org/pdf/2501.16055v1)

**Tags**: math.NA cs.NA math.PR stat.ML 65C05, 82C31, 62F15 



### PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language   Models Quantization
**Authors**: Mengzhao Chen, Yi Liu, Jiahao Wang, Yi Bin, Wenqi Shao, Ping Luo

**Updated**: 2025-01-27T13:39:25Z

**Summary**: Existing weight-activation quantization methods for Large Language Models (LLMs) primarily address channel-wise outliers but often neglect token-wise outliers, which limits the accuracy of quantized models. In this work, we propose PrefixQuant, a novel quantization method that achieves state-of-the-art performance across various precision levels (W4A4KV4 and W4A8KV4) and granularities (dynamic and static quantization) by effectively isolating token-wise outliers. First, PrefixQuant eliminates token-wise outliers by prefixing outlier tokens in the KV cache, a process that is training-free and highly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant introduces new trainable parameters for block-wise training to compensate for quantization error. Our experiments show that PrefixQuant significantly outperforms existing dynamic quantization methods, even under coarser static quantization settings. For instance, PrefixQuant achieves an average accuracy improvement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on five zero-shot reasoning tasks under dynamic and static quantization settings, respectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x prefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant. Our code is available at https://github.com/ChenMnZ/PrefixQuant.

**Link**: [arxiv](http://arxiv.org/abs/2410.05265v2),  [pdf](http://arxiv.org/pdf/2410.05265v2)

**Tags**: cs.LG cs.CL 



### Dynamic Content Caching with Waiting Costs via Restless Multi-Armed   Bandits
**Authors**: Ankita Koley, Chandramani Singh

**Updated**: 2025-01-27T06:47:20Z

**Summary**: We consider a system with a local cache connected to a backend server and an end user population. A set of contents are stored at the the server where they continuously get updated. The local cache keeps copies, potentially stale, of a subset of the contents. The users make content requests to the local cache which either can serve the local version if available or can fetch a fresh version or can wait for additional requests before fetching and serving a fresh version. Serving a stale version of a content incurs an age-of-version(AoV) dependent ageing cost, fetching it from the server incurs a fetching cost, and making a request wait incurs a per unit time waiting cost. We focus on the optimal actions subject to the cache capacity constraint at each decision epoch, aiming at minimizing the long term average cost. We pose the problem as a Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based policy which is known to be asymptotically optimal. We explicitly characterize the Whittle indices. We numerically evaluate the proposed policy and also compare it to a greedy policy. We show that it is close to the optimal policy and substantially outperforms the exising policies.

**Link**: [arxiv](http://arxiv.org/abs/2410.18627v3),  [pdf](http://arxiv.org/pdf/2410.18627v3)

**Tags**: cs.NI 



### Online Allocation with Multi-Class Arrivals: Group Fairness vs   Individual Welfare
**Authors**: Faraz Zargari, Hossein Nekouyan Jazi, Bo Sun, Xiaoqi Tan

**Updated**: 2025-01-27T05:02:05Z

**Summary**: We introduce and study a multi-class online resource allocation problem with group fairness guarantees. The problem involves allocating a fixed amount of resources to a sequence of agents, each belonging to a specific group. The primary objective is to ensure fairness across different groups in an online setting. We focus on three fairness notions: one based on quantity and two based on utility. To achieve fair allocations, we develop two threshold-based online algorithms, proving their optimality under two fairness notions and near-optimality for the more challenging one. Additionally, we demonstrate a fundamental trade-off between group fairness and individual welfare using a novel representative function-based approach. To address this trade-off, we propose a set-aside multi-threshold algorithm that reserves a portion of the resource to ensure fairness across groups while utilizing the remaining resource to optimize efficiency under utility-based fairness notions. This algorithm is proven to achieve the Pareto-optimal trade-off. We also demonstrate that our problem can model a wide range of real-world applications, including network caching and cloud computing, and empirically evaluate our proposed algorithms in the network caching problem using real datasets.

**Link**: [arxiv](http://arxiv.org/abs/2501.15782v1),  [pdf](http://arxiv.org/pdf/2501.15782v1)

**Tags**: cs.GT cs.DS 



### ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language   Model Born from Transformer
**Authors**: Lin Yueyu, Li Zhiyuan, Peter Yue, Liu Xiao

**Updated**: 2025-01-26T15:56:56Z

**Summary**: As is known, hybrid quadratic and subquadratic attention models in multi-head architectures have surpassed both Transformer and Linear RNN models , with these works primarily focusing on reducing KV complexity and improving efficiency. For further research on expressiveness, we introduce our series of models distilled from Qwen 2.5, based on pure native RWKV-7 attention, which aims to make RNN more expressive and demonstrates state tracking ability beyond transformers. We work with QRWK 32B based on RWKV-6 architecture, another approach that reduces the entire knowledge processing time to just 8 hours using 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the distillation process can utilize any LLM, not just Qwen, and enables knowledge transfer from larger LLMs to smaller ones with more fewer tokens. We will explain the detailed process and share our insights on building more powerful foundation models. Please note that this is an ongoing work that will be updated continuously. The model checkpoints and source code are available at \href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside}, \href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}.

**Link**: [arxiv](http://arxiv.org/abs/2501.15570v1),  [pdf](http://arxiv.org/pdf/2501.15570v1)

**Tags**: cs.CL 



### Query-based versus resource-based cache strategies in tag-based browsing   systems
**Authors**: Joaquín Gayoso-Cabada, Mercedes Gómez-Albarrán, José-Luis Sierra

**Updated**: 2025-01-26T11:01:10Z

**Summary**: Tag-based browsing is a popular interaction model for navigating digital libraries. According to this model, users select descriptive tags to filter resources in the collections. Typical implementations of the model are based on inverted indexes. However, these implementations can require a considerable amount of set operations to update the browsing state. To palliate this inconven-ience, it is possible to adopt suitable cache strategies. In this paper we describe and compare two of these strategies: (i) a query-based strategy, according to which previously computed browsing states are indexed by sets of selected tags; and (ii) a resource-based strategy, according to which browsing states are in-dexed by sets of filtered resources. Our comparison focused on runtime perfor-mance, and was carried out empirically, using a real-world web-based collec-tion in the field of digital humanities. The results obtained show that the re-source-based strategy clearly outperforms the query-based one.

**Link**: [arxiv](http://arxiv.org/abs/2501.15481v1),  [pdf](http://arxiv.org/pdf/2501.15481v1)

**Tags**: cs.CL 



### Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for   Efficient LLM Inference
**Authors**: Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, S. Kevin Zhou

**Updated**: 2025-01-26T07:29:06Z

**Summary**: Large Language Models have excelled in various domains but face efficiency challenges due to the growing Key-Value (KV) cache required for long-sequence inference. Recent efforts aim to reduce KV cache size by evicting vast non-critical cache elements during runtime while preserving generation quality. However, these methods typically allocate compression budgets uniformly across all attention heads, ignoring the unique attention patterns of each head. In this paper, we establish a theoretical loss upper bound between pre- and post-eviction attention output, explaining the optimization target of prior cache eviction methods, while guiding the optimization of adaptive budget allocation. Base on this, we propose {\it Ada-KV}, the first head-wise adaptive budget allocation strategy. It offers plug-and-play benefits, enabling seamless integration with prior cache eviction methods. Extensive evaluations on 13 datasets from Ruler and 16 datasets from LongBench, all conducted under both question-aware and question-agnostic scenarios, demonstrate substantial quality improvements over existing methods.

**Link**: [arxiv](http://arxiv.org/abs/2407.11550v4),  [pdf](http://arxiv.org/pdf/2407.11550v4)

**Tags**: cs.CL cs.AI 



### Collaborative Coded Caching for Partially Connected Networks
**Authors**: Kagan Akcay, Eleftherios Lampiris, MohammadJavad Salehi, Giuseppe Caire

**Updated**: 2025-01-26T01:43:46Z

**Summary**: Coded caching leverages the differences in user cache memories to achieve gains that scale with the total cache size, alleviating network congestion due to high-quality content requests. Additionally, distributing transmitters over a wide area can mitigate the adverse effects of path loss. In this work, we consider a partially connected network where the channel between distributed transmitters (helpers) and users is modeled as a distributed MIMO Gaussian broadcast channel. We propose a novel delivery scheme consisting of two phases: partitioning and transmission. In the partitioning phase, users with identical cache profiles are partitioned into the minimum number of sets, such that users within each set can successfully decode their desired message from a joint transmission enabled by MIMO precoding. To optimally partition the users, we employ the branch and bound method. In the transmission phase, each partition is treated as a single entity, and codewords are multicast to partitions with distinct cache profiles. The proposed delivery scheme is applicable to any partially connected network, and while the partitioning is optimal, the overall delivery scheme, including transmission, is heuristic. Interestingly, simulation results show that its performance closely approximates that of the fully connected optimal solution.

**Link**: [arxiv](http://arxiv.org/abs/2501.13298v2),  [pdf](http://arxiv.org/pdf/2501.13298v2)

**Tags**: cs.IT math.IT 



### ReInc: Scaling Training of Dynamic Graph Neural Networks
**Authors**: Mingyu Guan, Saumia Singhal, Taesoo Kim, Anand Padmanabha Iyer

**Updated**: 2025-01-25T23:16:03Z

**Summary**: Dynamic Graph Neural Networks (DGNNs) have gained widespread attention due to their applicability in diverse domains such as traffic network prediction, epidemiological forecasting, and social network analysis. In this paper, we present ReInc, a system designed to enable efficient and scalable training of DGNNs on large-scale graphs. ReInc introduces key innovations that capitalize on the unique combination of Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs) inherent in DGNNs. By reusing intermediate results and incrementally computing aggregations across consecutive graph snapshots, ReInc significantly enhances computational efficiency. To support these optimizations, ReInc incorporates a novel two-level caching mechanism with a specialized caching policy aligned to the DGNN execution workflow. Additionally, ReInc addresses the challenges of managing structural and temporal dependencies in dynamic graphs through a new distributed training strategy. This approach eliminates communication overheads associated with accessing remote features and redistributing intermediate results. Experimental results demonstrate that ReInc achieves up to an order of magnitude speedup compared to state-of-the-art frameworks, tested across various dynamic GNN architectures and real-world graph datasets.

**Link**: [arxiv](http://arxiv.org/abs/2501.15348v1),  [pdf](http://arxiv.org/pdf/2501.15348v1)

**Tags**: cs.LG cs.DC 



### Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle
**Authors**: KVS Chaithanya, Sumesh P. Thampi

**Updated**: 2025-01-25T12:17:41Z

**Summary**: Understanding the hydrodynamics of microswimmers in viscoelastic fluids and confined environments is crucial for interpreting their behaviour in natural settings and designing synthetic microswimmers for practical applications like cargo transport. In this study, we explore the hydrodynamics of a concentric active compound particle - a model microswimmer (a squirmer) positioned at the centre of a viscoelastic fluid droplet (a model cargo) suspended in another viscoelastic medium. We consider the Oldroyd-B constitutive model to characterize the fluids and employ a perturbative approach in the Deborah number to analyze viscoelastic effects analytically, assuming a small Capillary number so that the droplet remains spherical and does not deform. We examine three cases: (i) a squirmer confined within a viscoelastic fluid droplet suspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian fluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined within a viscoelastic fluid droplet suspended in another viscoelastic fluid. Our findings reveal that the swimming speeds of the squirmer and the droplet are determined by the complex interplay of viscoelasticity, the size ratio of the droplet to the squirmer (confinement strength), and the viscosity ratio of the surrounding fluid to the droplet fluid. A critical aspect of this interaction is the positioning of stagnation points within the fluid flow, which governs the distribution of polymeric stress. This distribution, in turn, plays a crucial role in determining the influence of viscoelasticity on the squirmer's dynamics. Our analysis suggests that viscoelastic effects can either enhance or hinder the swimming speed of the squirmer when confined in a droplet, depending on the specific configuration of the system.

**Link**: [arxiv](http://arxiv.org/abs/2410.09479v2),  [pdf](http://arxiv.org/pdf/2410.09479v2)

**Tags**: physics.flu-dyn cond-mat.soft 



### The Selection Problem in Multi-Query Optimization: a Comprehensive   Survey
**Authors**: Sergey Zinchenko, Denis Ponomaryov

**Updated**: 2025-01-25T10:38:11Z

**Summary**: View materialization, index selection, and plan caching are well-known techniques for optimization of query processing in database systems. The essence of these tasks is to select and save a subset of the most useful candidates (views/indexes/plans) for reuse within given space/time budget constraints. In this paper, we propose a unified view on these selection problems. We make a detailed analysis of the root causes of their complexity and summarize techniques to address them. Our survey provides a modern classification of selection algorithms known in the literature, including the latest ones based on Machine Learning. We provide a ground for reuse of the selection techniques between different optimization scenarios and highlight challenges and promising directions in the field. Based on our analysis we derive a method to exponentially accelerate some of the state-of-the-art selection algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2412.11828v2),  [pdf](http://arxiv.org/pdf/2412.11828v2)

**Tags**: cs.DB cs.DM 



### Fully-Automated Code Generation for Efficient Computation of Sparse   Matrix Permanents on GPUs
**Authors**: Deniz Elbek, Kamer Kaya

**Updated**: 2025-01-25T08:27:26Z

**Summary**: Registers are the fastest memory components within the GPU's complex memory hierarchy, accessed by names rather than addresses. They are managed entirely by the compiler through a process called register allocation, during which the compiler attempts to cache predictable data from thread-local memory into thread-private registers. Computing the permanent of a sparse matrix poses a challenge for compilers, as optimizing this process is hindered by the unpredictable distribution of nonzero elements, which only become known at runtime. In this work, we employ fully-automated code generation to address this, producing highly optimized kernels tailored to the matrix's sparsity pattern. State-of-the-art permanent computation algorithms require each thread to store a private array, denoted x, of size n. We first propose a technique that fully stores these arrays in registers, with inclusion and exclusion kernels generated for each column. To minimize control divergence and reduce the number of unique kernels within a warp, we exploit the internal structure of Gray codes, which are also used in the state-of-the-art algorithm. Our second technique reduces register pressure by utilizing both registers and global memory and introduces a matrix ordering and partitioning strategy for greater efficiency. On synthetic matrices, this approach achieves a 31x speedup over state-of-the-art CPU implementations on 112 cores, and an 8x speedup compared to our traditional GPU implementation. For real-world matrices, these speedups are 24.9x and 4.9x.

**Link**: [arxiv](http://arxiv.org/abs/2501.15126v1),  [pdf](http://arxiv.org/pdf/2501.15126v1)

**Tags**: cs.DC cs.DM cs.NA math.NA 



### Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation   of Attention Heads
**Authors**: Xingyang He, Jie Liu, Shaowei Chen

**Updated**: 2025-01-25T07:28:13Z

**Summary**: KV cache is a widely used acceleration technique for large language models (LLMs) inference. However, its memory requirement grows rapidly with input length. Previous studies have reduced the size of KV cache by either removing the same number of unimportant tokens for all attention heads or by allocating differentiated KV cache budgets for pre-identified attention heads. However, due to the importance of attention heads varies across different tasks, the pre-identified attention heads fail to adapt effectively to various downstream tasks. To address this issue, we propose Task-KV, a method that leverages the semantic differentiation of attention heads to allocate differentiated KV cache budgets across various tasks. We demonstrate that attention heads far from the semantic center (called heterogeneous heads) make an significant contribution to task outputs and semantic understanding. In contrast, other attention heads play the role of aggregating important information and focusing reasoning. Task-KV allocates full KV cache budget to heterogeneous heads to preserve comprehensive semantic information, while reserving a small number of recent tokens and attention sinks for non-heterogeneous heads. Furthermore, we innovatively introduce middle activations to preserve key contextual information aggregated from non-heterogeneous heads. To dynamically perceive semantic differences among attention heads, we design a semantic separator to distinguish heterogeneous heads from non-heterogeneous ones based on their distances from the semantic center. Experimental results on multiple benchmarks and different model architectures demonstrate that Task-KV significantly outperforms existing baseline methods.

**Link**: [arxiv](http://arxiv.org/abs/2501.15113v1),  [pdf](http://arxiv.org/pdf/2501.15113v1)

**Tags**: cs.CL 



### A New Construction Structure on Coded Caching with Linear   Subpacketization: Non-Half-Sum Disjoint Packing
**Authors**: Minquan Cheng, Huimei Wei, Kai Wan, Giuseppe Caire

**Updated**: 2025-01-25T04:21:57Z

**Summary**: Coded caching is a promising technique to effectively reduce peak traffic by using local caches and the multicast gains generated by these local caches. We prefer to design a coded caching scheme with the subpacketization $F$ and transmission load $R$ as small as possible since these are the key metrics for evaluating the implementation complexity and transmission efficiency of the scheme, respectively. However, most of the existing coded caching schemes have large subpacketizations which grow exponentially with the number of users $K$, and there are a few schemes with linear subpacketizations which have large transmission loads. In this paper, we focus on studying the linear subpacketization, i.e., $K=F$, coded caching scheme with low transmission load. Specifically, we first introduce a new combinatorial structure called non-half-sum disjoint packing (NHSDP) which can be used to generate a coded caching scheme with $K=F$. Then a class of new schemes is obtained by constructing NHSDP. Theoretical and numerical comparisons show that (i) compared to the existing schemes with linear subpacketization (to the number of users), the proposed scheme achieves a lower load; (ii) compared to some existing schemes with polynomial subpacketization, the proposed scheme can also achieve a lower load in some cases; (iii) compared to some existing schemes with exponential subpacketization, the proposed scheme has loads close to those of these schemes in some cases. Moreover, the new concept of NHSDP is closely related to the classical combinatorial structures such as cyclic difference packing (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash family (PHF). These connections indicate that NHSDP is an important combinatorial structure in the field of combinatorial design.

**Link**: [arxiv](http://arxiv.org/abs/2501.11855v2),  [pdf](http://arxiv.org/pdf/2501.11855v2)

**Tags**: cs.IT math.IT 



### AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for   Vision-Language Models
**Authors**: Zunhai Su, Wang Shen, Linge Li, Zhe Chen, Hanyu Wei, Huangqi Yu, Kehong Yuan

**Updated**: 2025-01-25T02:01:56Z

**Summary**: Vision-language models (VLMs) show remarkable performance in multimodal tasks. However, excessively long multimodal inputs lead to oversized Key-Value (KV) caches, resulting in significant memory consumption and I/O bottlenecks. Previous KV quantization methods for Large Language Models (LLMs) may alleviate these issues but overlook the attention saliency differences of multimodal tokens, resulting in suboptimal performance. In this paper, we investigate the attention-aware token saliency patterns in VLM and propose AKVQ-VL. AKVQ-VL leverages the proposed Text-Salient Attention (TSA) and Pivot-Token-Salient Attention (PSA) patterns to adaptively allocate bit budgets. Moreover, achieving extremely low-bit quantization requires effectively addressing outliers in KV tensors. AKVQ-VL utilizes the Walsh-Hadamard transform (WHT) to construct outlier-free KV caches, thereby reducing quantization difficulty. Evaluations of 2-bit quantization on 12 long-context and multimodal tasks demonstrate that AKVQ-VL maintains or even improves accuracy, outperforming LLM-oriented methods. AKVQ-VL can reduce peak memory usage by 2.13x, support up to 3.25x larger batch sizes and 2.46x throughput.

**Link**: [arxiv](http://arxiv.org/abs/2501.15021v1),  [pdf](http://arxiv.org/pdf/2501.15021v1)

**Tags**: cs.CL 



### RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via   Outlier-Aware Adaptive Rotations
**Authors**: Zunhai Su, Zhe Chen, Wang Shen, Hanyu Wei, Linge Li, Huangqi Yu, Kehong Yuan

**Updated**: 2025-01-25T01:45:29Z

**Summary**: Key-Value (KV) cache facilitates efficient large language models (LLMs) inference by avoiding recomputation of past KVs. As the batch size and context length increase, the oversized KV caches become a significant memory bottleneck, highlighting the need for efficient compression. Existing KV quantization rely on fine-grained quantization or the retention of a significant portion of high bit-widths caches, both of which compromise compression ratio and often fail to maintain robustness at extremely low average bit-widths. In this work, we explore the potential of rotation technique for 2-bit KV quantization and propose RotateKV, which achieves accurate and robust performance through the following innovations: (i) Outlier-Aware Rotation, which utilizes channel-reordering to adapt the rotations to varying channel-wise outlier distributions without sacrificing the computational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii) Pre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position embedding (RoPE) on proposed outlier-aware rotation and further smooths outliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages the massive activations to precisely identify and protect attention sinks. RotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit quantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning and long-context capabilities, with less than 1.7\% degradation on GSM8K, outperforming existing methods even at lower average bit-widths. RotateKV also showcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch sizes, and achieves a 2.32x speedup in decoding stage.

**Link**: [arxiv](http://arxiv.org/abs/2501.16383v1),  [pdf](http://arxiv.org/pdf/2501.16383v1)

**Tags**: cs.LG cs.AI cs.CL 



### EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation
**Authors**: Yifan Yu, Yu Gan, Lillian Tsai, Nikhil Sarda, Jiaming Shen, Yanqi Zhou, Arvind Krishnamurthy, Fan Lai, Henry M. Levy, David Culler

**Updated**: 2025-01-24T19:13:12Z

**Summary**: Large language models (LLMs) have excelled in various applications, yet serving them at scale is challenging due to their substantial resource demands and high latency. Our real-world studies reveal that over 60% of user requests to LLMs have semantically similar counterparts, suggesting the potential for knowledge sharing among requests. However, naively caching and reusing past responses leads to large quality degradation. In this paper, we introduce EchoLM, an in-context caching system that leverages historical requests as examples to guide response generation, enabling selective offloading of requests to more efficient LLMs. However, enabling this real-time knowledge transfer leads to intricate tradeoffs between response quality, latency, and system throughput at scale. For a new request, EchoLM identifies similar, high-utility examples and efficiently prepends them to the input for better response. At scale, EchoLM adaptively routes requests to LLMs of varying capabilities, accounting for response quality and serving loads. EchoLM employs a cost-aware cache replay mechanism to improve example quality and coverage offline, maximizing cache utility and runtime efficiency. Evaluations on millions of open-source requests demonstrate that EchoLM has a throughput improvement of 1.4-5.9x while reducing latency by 28-71% without hurting response quality on average.

**Link**: [arxiv](http://arxiv.org/abs/2501.12689v2),  [pdf](http://arxiv.org/pdf/2501.12689v2)

**Tags**: cs.LG 



### Language-Queried Target Sound Extraction Without Parallel Training Data
**Authors**: Hao Ma, Zhiyuan Peng, Xu Li, Yukai Li, Mingjie Shao, Qiuqiang Kong, Ju Liu

**Updated**: 2025-01-24T15:16:48Z

**Summary**: Language-queried target sound extraction (TSE) aims to extract specific sounds from mixtures based on language queries. Traditional fully-supervised training schemes require extensively annotated parallel audio-text data, which are labor-intensive. We introduce a parallel-data-free training scheme, requiring only unlabelled audio clips for TSE model training by utilizing the contrastive language-audio pre-trained model (CLAP). In a vanilla parallel-data-free training stage, target audio is encoded using the pre-trained CLAP audio encoder to form a condition embedding, while during testing, user language queries are encoded by CLAP text encoder as the condition embedding. This vanilla approach assumes perfect alignment between text and audio embeddings, which is unrealistic. Two major challenges arise from training-testing mismatch: the persistent modality gap between text and audio and the risk of overfitting due to the exposure of rich acoustic details in target audio embedding during training. To address this, we propose a retrieval-augmented strategy. Specifically, we create an embedding cache using audio captions generated by a large language model (LLM). During training, target audio embeddings retrieve text embeddings from this cache to use as condition embeddings, ensuring consistent modalities between training and testing and eliminating information leakage. Extensive experiment results show that our retrieval-augmented approach achieves consistent and notable performance improvements over existing state-of-the-art with better generalizability.

**Link**: [arxiv](http://arxiv.org/abs/2409.09398v2),  [pdf](http://arxiv.org/pdf/2409.09398v2)

**Tags**: eess.AS cs.SD 



### A Programming Model for Disaggregated Memory over CXL
**Authors**: Gal Assa, Lucas Bürgi, Michal Friedman, Ori Lahav

**Updated**: 2025-01-24T14:32:34Z

**Summary**: CXL (Compute Express Link) is an emerging open industry-standard interconnect between processing and memory devices that is expected to revolutionize the way systems are designed in the near future. It enables cache-coherent shared memory pools in a disaggregated fashion at unprecedented scales, allowing algorithms to interact with a variety of storage devices using simple loads and stores. Alongside unleashing unique opportunities for a wide range of applications, CXL introduces new challenges of data management and crash consistency. Alas, CXL lacks an adequate programming model, which makes reasoning about the correctness and expected behaviors of algorithms and systems on top of it nearly impossible.   In this work, we present CXL0, the first programming model for concurrent programs running on top of CXL. We propose a high-level abstraction for CXL memory accesses and formally define operational semantics on top of that abstraction. We perform initial measurements that provide practical insight into CXL0. We provide a set of general transformations that adapt concurrent algorithms to the new disruptive technology. These transformations enhance linearizable algorithms with durability under a general partial-failure model. We provide an additional transformation for algorithms designed for persistent main memory and full-system crashes. We believe that this work will serve as a stepping stone for systems design and modeling on top of CXL, and support the development of future models as software and hardware evolve.

**Link**: [arxiv](http://arxiv.org/abs/2407.16300v2),  [pdf](http://arxiv.org/pdf/2407.16300v2)

**Tags**: cs.DC cs.ET 



### Application-Aware Resource Allocation and Data Management for   MEC-assisted IoT Service Providers
**Authors**: Simone Bolettieri, Raffaele Bruno, Enzo Mingozzi

**Updated**: 2025-01-24T10:39:45Z

**Summary**: To support the growing demand for data-intensive and low-latency IoT applications, Multi-Access Edge Computing (MEC) is emerging as an effective edge-computing approach enabling the execution of delay-sensitive processing tasks close to end-users. However, most of the existing works on resource allocation and service placement in MEC systems overlook the unique characteristics of new IoT use cases. For instance, many IoT applications require the periodic execution of computing tasks on real-time data streams that originate from devices dispersed over a wide area. Thus, users requesting IoT services are typically distant from the data producers. To fill this gap, the contribution of this work is two-fold. Firstly, we propose a MEC-compliant architectural solution to support the operation of multiple IoT service providers over a common MEC platform deployment, which enables the steering and shaping of IoT data transport within the platform. Secondly, we model the problem of service placement and data management in the proposed MEC-based solution taking into account the dependencies at the data level between IoT services and sensing resources. Our model also considers that caches can be deployed on MEC hosts, to allow the sharing of the same data between different IoT services with overlapping geographical scope, and provides support for IoT services with heterogeneous QoS requirements, such as different frequencies of periodic task execution. Due to the complexity of the optimisation problem, a heuristic algorithm is proposed using linear relaxation and rounding techniques. Extensive simulation results demonstrate the efficiency of the proposed approach, especially when traffic demands generated by the service requests are not uniform.

**Link**: [arxiv](http://arxiv.org/abs/2501.14387v1),  [pdf](http://arxiv.org/pdf/2501.14387v1)

**Tags**: cs.NI 



### Joint System Latency and Data Freshness Optimization for Cache-enabled   Mobile Crowdsensing Networks
**Authors**: Kexin Shi, Yaru Fu, Yongna Guo, Fu Lee Wang, Yan Zhang

**Updated**: 2025-01-24T10:00:21Z

**Summary**: Mobile crowdsensing (MCS) networks enable large-scale data collection by leveraging the ubiquity of mobile devices. However, frequent sensing and data transmission can lead to significant resource consumption. To mitigate this issue, edge caching has been proposed as a solution for storing recently collected data. Nonetheless, this approach may compromise data freshness. In this paper, we investigate the trade-off between re-using cached task results and re-sensing tasks in cache-enabled MCS networks, aiming to minimize system latency while maintaining information freshness. To this end, we formulate a weighted delay and age of information (AoI) minimization problem, jointly optimizing sensing decisions, user selection, channel selection, task allocation, and caching strategies. The problem is a mixed-integer non-convex programming problem which is intractable. Therefore, we decompose the long-term problem into sequential one-shot sub-problems and design a framework that optimizes system latency, task sensing decision, and caching strategy subproblems. When one task is re-sensing, the one-shot problem simplifies to the system latency minimization problem, which can be solved optimally. The task sensing decision is then made by comparing the system latency and AoI. Additionally, a Bayesian update strategy is developed to manage the cached task results. Building upon this framework, we propose a lightweight and time-efficient algorithm that makes real-time decisions for the long-term optimization problem. Extensive simulation results validate the effectiveness of our approach.

**Link**: [arxiv](http://arxiv.org/abs/2501.14367v1),  [pdf](http://arxiv.org/pdf/2501.14367v1)

**Tags**: cs.NI eess.SP 



### Locality-aware Fair Scheduling in LLM Serving
**Authors**: Shiyi Cao, Yichuan Wang, Ziming Mao, Pin-Lun Hsu, Liangsheng Yin, Tian Xia, Dacheng Li, Shu Liu, Yineng Zhang, Yang Zhou, Ying Sheng, Joseph Gonzalez, Ion Stoica

**Updated**: 2025-01-24T08:12:47Z

**Summary**: Large language model (LLM) inference workload dominates a wide variety of modern AI applications, ranging from multi-turn conversation to document analysis. Balancing fairness and efficiency is critical for managing diverse client workloads with varying prefix patterns. Unfortunately, existing fair scheduling algorithms for LLM serving, such as Virtual Token Counter (VTC), fail to take prefix locality into consideration and thus suffer from poor performance. On the other hand, locality-aware scheduling algorithms in existing LLM serving frameworks tend to maximize the prefix cache hit rate without considering fair sharing among clients.   This paper introduces the first locality-aware fair scheduling algorithm, Deficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix locality with a fairness guarantee. We also introduce a novel algorithm, Double Deficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find a balance point among fairness, locality, and load-balancing. Our extensive evaluation demonstrates the superior performance of DLPM and D$^2$LPM in ensuring fairness while maintaining high throughput (up to 2.87$\times$ higher than VTC) and low per-client (up to 7.18$\times$ lower than state-of-the-art distributed LLM serving system) latency.

**Link**: [arxiv](http://arxiv.org/abs/2501.14312v1),  [pdf](http://arxiv.org/pdf/2501.14312v1)

**Tags**: cs.DC cs.LG 



### Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement   Learning-based Model Caching and Inference Offloading
**Authors**: Minrui Xu, Dusit Niyato, Christopher G. Brinton

**Updated**: 2025-01-24T03:21:20Z

**Summary**: Large Language Models (LLMs) can perform zero-shot learning on unseen tasks and few-shot learning on complex reasoning tasks. However, resource-limited mobile edge networks struggle to support long-context LLM serving for LLM agents during multi-round interactions with users. Unlike stateless computation offloading and static service offloading in edge computing, optimizing LLM serving at edge servers is challenging because LLMs continuously learn from context which raises accuracy, latency, and resource consumption dynamics. In this paper, we propose a joint model caching and inference offloading framework that utilizes test-time deep reinforcement learning (T2DRL) to optimize deployment and execution strategies for long-context LLM serving. In this framework, we analyze the performance convergence and design an optimization problem considering the utilization of context windows in LLMs. Furthermore, the T2DRL algorithm can learn in both the training phase and the testing phase to proactively manage cached models and service requests and adapt to context changes and usage patterns during execution. To further enhance resource allocation efficiency, we propose a double Dutch auction (DDA) mechanism, which dynamically matches supply and demand while maximizing social welfare. Finally, experimental results demonstrate that the T2DRL algorithm can reduce system costs by at least 30% compared to baselines while guaranteeing the performance of LLM agents in real-world perception and reasoning tasks.

**Link**: [arxiv](http://arxiv.org/abs/2501.14205v1),  [pdf](http://arxiv.org/pdf/2501.14205v1)

**Tags**: cs.NI 



### Sigma: Differential Rescaling of Query, Key and Value for Efficient   Language Models
**Authors**: Zhenghao Lin, Zihao Tang, Xiao Liu, Yeyun Gong, Yi Cheng, Qi Chen, Hang Li, Ying Xin, Ziyue Yang, Kailai Yang, Yu Yan, Xiao Liang, Shuai Lu, Yiming Huang, Zheheng Luo, Lei Qu, Xuan Feng, Yaoxiang Wang, Yuqing Xia, Feiyang Chen, Yuting Jiang, Yasen Hu, Hao Ni, Binyang Li, Guoshuai Zhao, Jui-Hao Chiang, Zhongxin Guo, Chen Lin, Kun Kuang, Wenjie Li, Yelong Shen, Jian Jiao, Peng Cheng, Mao Yang

**Updated**: 2025-01-23T12:58:14Z

**Summary**: We introduce Sigma, an efficient large language model specialized for the system domain, empowered by a novel architecture including DiffQKV attention, and pre-trained on our meticulously collected system domain data. DiffQKV attention significantly enhances the inference efficiency of Sigma by optimizing the Query (Q), Key (K), and Value (V) components in the attention mechanism differentially, based on their varying impacts on the model performance and efficiency indicators. Specifically, we (1) conduct extensive experiments that demonstrate the model's varying sensitivity to the compression of K and V components, leading to the development of differentially compressed KV, and (2) propose augmented Q to expand the Q head dimension, which enhances the model's representation capacity with minimal impacts on the inference speed. Rigorous theoretical and empirical analyses reveal that DiffQKV attention significantly enhances efficiency, achieving up to a 33.36% improvement in inference speed over the conventional grouped-query attention (GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various sources, including 19.5B system domain data that we carefully collect and 1T tokens of synthesized and rewritten data. In general domains, Sigma achieves comparable performance to other state-of-arts models. In the system domain, we introduce the first comprehensive benchmark AIMicius, where Sigma demonstrates remarkable performance across all tasks, significantly outperforming GPT-4 with an absolute improvement up to 52.5%.

**Link**: [arxiv](http://arxiv.org/abs/2501.13629v1),  [pdf](http://arxiv.org/pdf/2501.13629v1)

**Tags**: cs.CL 



### Characterisation of the plutonium isotopic composition of a sediment   core from Palomares, Spain, by low-energy AMS and alpha-spectrometry
**Authors**: E. Chamizo, M. C. Jiménez-Ramos, S. M. Enamorado, M. García-León, R. García-Tenorio, J. L. Mas, P. Masqué, J. Merino, J. A. Sanchez-Cabeza

**Updated**: 2025-01-23T11:18:42Z

**Summary**: The measurement of plutonium isotopes, 239Pu and 240Pu, at 670 kV on the compact accelerator mass spectrometry (AMS) system at the Centro Nacional de Aceleradores (CNA) in Seville, Spain, is now a reality. In this work, we present first Pu AMS results for environmental samples: a sediment core collected in a submarine canyon in the Mediterranean coast of the Spanish region of Palomares, affected by a nuclear accident in 1966. From the study of the 240Pu/239Pu atomic ratio profile, showing on average levels lower than 11%, we confirm that the weapon-grade plutonium released on land during the accident, with a characteristic 240Pu/239Pu atomic ratio of 5.8%, has found its way into the marine environment. A two-plutonium sources mixture model (Palomares and fallout) is used to elucidate the percentage of the plutonium coming from the accident. As a validation exercise of the Pu AMS measuring technique and in order to obtain the 238Pu/(239+240)Pu activity ratios, samples were also studied by alpha-spectrometry (AS). The obtained AS 239+240Pu activity concentration results fit in with the AMS ones in a wide dynamic range, thus validating the AMS technique.

**Link**: [arxiv](http://arxiv.org/abs/2501.13998v1),  [pdf](http://arxiv.org/pdf/2501.13998v1)

**Tags**: physics.ins-det physics.ao-ph 



### POPS: From History to Mitigation of DNS Cache Poisoning Attacks
**Authors**: Yehuda Afek, Harel Berger, Anat Bremler-Barr

**Updated**: 2025-01-23T10:40:09Z

**Summary**: We present a novel yet simple and comprehensive DNS cache POisoning Prevention System (POPS), designed to integrate as a module in Intrusion Prevention Systems (IPS). POPS addresses statistical DNS poisoning attacks, including those documented from 2002 to the present, and offers robust protection against similar future threats. It consists of two main components: a detection module that employs three simple rules, and a mitigation module that leverages the TC flag in the DNS header to enhance security. Once activated, the mitigation module has zero false positives or negatives, correcting any such errors on the side of the detection module.   We first analyze POPS against historical DNS services and attacks, showing that it would have mitigated all network-based statistical poisoning attacks, yielding a success rate of only 0.0076% for the adversary. We then simulate POPS on traffic benchmarks (PCAPs) incorporating current potential network-based statistical poisoning attacks, and benign PCAPs; the simulated attacks still succeed with a probability of 0.0076%. This occurs because five malicious packets go through before POPS detects the attack and activates the mitigation module. In addition, POPS completes its task using only 20%-50% of the time required by other tools (e.g., Suricata or Snort), and after examining just 5%-10% as many packets. Furthermore, it successfully identifies DNS cache poisoning attacks-such as fragmentation attacks-that both Suricata and Snort fail to detect, underscoring its superiority in providing comprehensive DNS protection.

**Link**: [arxiv](http://arxiv.org/abs/2501.13540v1),  [pdf](http://arxiv.org/pdf/2501.13540v1)

**Tags**: cs.CR cs.NI 



### A Training-free Sub-quadratic Cost Transformer Model Serving Framework   With Hierarchically Pruned Attention
**Authors**: Heejun Lee, Geon Park, Youngwan Lee, Jaduk Suh, Jina Kim, Wonyoung Jeong, Bumsik Kim, Hyemin Lee, Myeongjae Jeon, Sung Ju Hwang

**Updated**: 2025-01-23T07:25:28Z

**Summary**: In modern large language models (LLMs), increasing the context length is crucial for improving comprehension and coherence in long-context, multi-modal, and retrieval-augmented language generation. While many recent transformer models attempt to extend their context length over a million tokens, they remain impractical due to the quadratic time and space complexities. Although recent works on linear and sparse attention mechanisms can achieve this goal, their real-world applicability is often limited by the need to re-train from scratch and significantly worse performance. In response, we propose a novel approach, Hierarchically Pruned Attention (HiP), which reduces the time complexity of the attention mechanism to $O(T \log T)$ and the space complexity to $O(T)$, where $T$ is the sequence length. We notice a pattern in the attention scores of pretrained LLMs where tokens close together tend to have similar scores, which we call ``attention locality''. Based on this observation, we utilize a novel tree-search-like algorithm that estimates the top-$k$ key tokens for a given query on the fly, which is mathematically guaranteed to have better performance than random attention pruning. In addition to improving the time complexity of the attention mechanism, we further optimize GPU memory usage by implementing KV cache offloading, which stores only $O(\log T)$ tokens on the GPU while maintaining similar decoding throughput. Experiments on benchmarks show that HiP, with its training-free nature, significantly reduces both prefill and decoding latencies, as well as memory usage, while maintaining high-quality generation with minimal degradation. HiP enables pretrained LLMs to scale up to millions of tokens on commodity GPUs, potentially unlocking long-context LLM applications previously deemed infeasible.

**Link**: [arxiv](http://arxiv.org/abs/2406.09827v3),  [pdf](http://arxiv.org/pdf/2406.09827v3)

**Tags**: cs.CL cs.CV cs.DC cs.LG 



### Parallel Key-Value Cache Fusion for Position Invariant RAG
**Authors**: Philhoon Oh, Jinwoo Shin, James Thorne

**Updated**: 2025-01-23T06:48:22Z

**Summary**: Recent advancements in Large Language Models (LLMs) underscore the necessity of Retrieval Augmented Generation (RAG) to leverage external information. However, LLMs are sensitive to the position of relevant information within contexts and tend to generate incorrect responses when such information is placed in the middle, known as `Lost in the Middle' phenomenon. In this paper, we introduce a framework that generates consistent outputs for decoder-only models, irrespective of the input context order. Experimental results for three open domain question answering tasks demonstrate position invariance, where the model is not sensitive to input context order, and superior robustness to irrelevent passages compared to prevailing approaches for RAG pipelines.

**Link**: [arxiv](http://arxiv.org/abs/2501.07523v2),  [pdf](http://arxiv.org/pdf/2501.07523v2)

**Tags**: cs.AI cs.CL 



### Qrazor: Reliable and effortless 4-bit llm quantization by significant   data razoring
**Authors**: Dongyoung Lee, Seungkyu Choi, Ik Joon Chang

**Updated**: 2025-01-23T02:20:08Z

**Summary**: Large-scale language models (LLMs) have demonstrated outstanding performance in language processing tasks, yet their deployment is often hindered by high memory demands and computational complexity. Although low-bit quantization techniques, such as 4-bit quantization, present a potential solution, they frequently lead to significant accuracy degradation or require substantial effort for such aggressive quantization approaches. To overcome these challenges, we introduce QRazor, a reliable and effortless quantization scheme designed to enable 4-bit quantization for weights, activations, and KV cache in transformer-based LLMs. The scheme involves two main stages: quantization and compression. During the quantization stage, weights, activations, and KV cache values are quantized with wider 8 or 16-bit integers as a basis to achieve nearly identical accuracy to the original full-precision LLM models, using the absolute max scaling. Subsequently, all data are compressed to 4-bit using our proposed significant data razoring (SDR) technique, which retains only the four most salient bits while discarding the others. Furthermore, we present an integer-based arithmetic unit dedicated to QRazor, enabling direct low-precision arithmetic operations without decompressing the SDR data. Despite the reduced quantization effort, QRazor achieves LLM accuracies better or comparable to state-of-the-art 4-bit methods. By also validating the hardware efficiency, our decompression-free arithmetic unit achieves 61.2% and 57.8% reduction in area and power consumption, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2501.13331v1),  [pdf](http://arxiv.org/pdf/2501.13331v1)

**Tags**: cs.LG 



### Personalized Federated Learning for Cellular VR: Online Learning and   Dynamic Caching
**Authors**: Krishnendu S. Tharakan, Hayssam Dahrouj, Nour Kouzayha, Hesham ElSawy, Tareq Y. Al-Naffouri

**Updated**: 2025-01-22T16:25:47Z

**Summary**: Delivering an immersive experience to virtual reality (VR) users through wireless connectivity offers the freedom to engage from anywhere at any time. Nevertheless, it is challenging to ensure seamless wireless connectivity that delivers real-time and high-quality videos to the VR users. This paper proposes a field of view (FoV) aware caching for mobile edge computing (MEC)-enabled wireless VR network. In particular, the FoV of each VR user is cached/prefetched at the base stations (BSs) based on the caching strategies tailored to each BS. Specifically, decentralized and personalized federated learning (DP-FL) based caching strategies with guarantees are presented. Considering VR systems composed of multiple VR devices and BSs, a DP-FL caching algorithm is implemented at each BS to personalize content delivery for VR users. The utilized DP-FL algorithm guarantees a probably approximately correct (PAC) bound on the conditional average cache hit. Further, to reduce the cost of communicating gradients, one-bit quantization of the stochastic gradient descent (OBSGD) is proposed, and a convergence guarantee of $\mathcal{O}(1/\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is the number of iterations. Additionally, to better account for the wireless channel dynamics, the FoVs are grouped into multicast or unicast groups based on the number of requesting VR users. The performance of the proposed DP-FL algorithm is validated through realistic VR head-tracking dataset, and the proposed algorithm is shown to have better performance in terms of average delay and cache hit as compared to baseline algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2501.11745v2),  [pdf](http://arxiv.org/pdf/2501.11745v2)

**Tags**: cs.IT cs.LG math.IT 



### Efficient Prompt Compression with Evaluator Heads for Long-Context   Transformer Inference
**Authors**: Weizhi Fei, Xueyan Niu, Guoqing Xie, Yingqing Liu, Bo Bai, Wei Han

**Updated**: 2025-01-22T15:33:17Z

**Summary**: Although applications involving long-context inputs are crucial for the effective utilization of large language models (LLMs), they also result in increased computational costs and reduced performance. To address this challenge, we propose an efficient, training-free prompt compression method that retains key information within compressed prompts. We identify specific attention heads in transformer-based LLMs, which we designate as evaluator heads, that are capable of selecting tokens in long inputs that are most significant for inference. Building on this discovery, we develop EHPC, an Evaluator Head-based Prompt Compression method, which enables LLMs to rapidly "skim through" input prompts by leveraging only the first few layers with evaluator heads during the pre-filling stage, subsequently passing only the important tokens to the model for inference. EHPC achieves state-of-the-art results across two mainstream benchmarks: prompt compression and long-context inference acceleration. Consequently, it effectively reduces the complexity and costs associated with commercial API calls. We further demonstrate that EHPC attains competitive results compared to key-value cache-based acceleration methods, thereby highlighting its potential to enhance the efficiency of LLMs for long-context tasks.

**Link**: [arxiv](http://arxiv.org/abs/2501.12959v1),  [pdf](http://arxiv.org/pdf/2501.12959v1)

**Tags**: cs.CL 



### Yi-Lightning Technical Report
**Authors**: Alan Wake, Bei Chen, C. X. Lv, Chao Li, Chengen Huang, Chenglin Cai, Chujie Zheng, Daniel Cooper, Fan Zhou, Feng Hu, Ge Zhang, Guoyin Wang, Heng Ji, Howard Qiu, Jiangcheng Zhu, Jun Tian, Katherine Su, Lihuan Zhang, Liying Li, Ming Song, Mou Li, Peng Liu, Qicheng Hu, Shawn Wang, Shijun Zhou, Shiming Yang, Shiyong Li, Tianhang Zhu, Wen Xie, Wenhao Huang, Xiang He, Xiaobo Chen, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Yanpeng Li, Yongke Zhao, Yongzhen Luo, Yuchi Xu, Yuxuan Sha, Zhaodong Yan, Zhiyuan Liu, Zirui Zhang, Zonghong Dai

**Updated**: 2025-01-22T15:09:58Z

**Summary**: This technical report presents Yi-Lightning, our latest flagship large language model (LLM). It achieves exceptional performance, ranking 6th overall on Chatbot Arena, with particularly strong results (2nd to 4th place) in specialized categories including Chinese, Math, Coding, and Hard Prompts. Yi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture, featuring advanced expert segmentation and routing mechanisms coupled with optimized KV-caching techniques. Our development process encompasses comprehensive pre-training, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF), where we devise deliberate strategies for multi-stage training, synthetic data construction, and reward modeling. Furthermore, we implement RAISE (Responsible AI Safety Engine), a four-component framework to address safety issues across pre-training, post-training, and serving phases. Empowered by our scalable super-computing infrastructure, all these innovations substantially reduce training, deployment and inference costs while maintaining high-performance standards. With further evaluations on public academic benchmarks, Yi-Lightning demonstrates competitive performance against top-tier LLMs, while we observe a notable disparity between traditional, static benchmark results and real-world, dynamic human preferences. This observation prompts a critical reassessment of conventional benchmarks' utility in guiding the development of more intelligent and powerful AI systems for practical applications. Yi-Lightning is now available through our developer platform at https://platform.lingyiwanwu.com.

**Link**: [arxiv](http://arxiv.org/abs/2412.01253v5),  [pdf](http://arxiv.org/pdf/2412.01253v5)

**Tags**: cs.CL cs.AI cs.LG 



### Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic   Wrap-Around
**Authors**: Elizabath Peter, K. K. Krishnan Namboodiri, B. Sundar Rajan

**Updated**: 2025-01-22T15:05:08Z

**Summary**: This work explores a multiple transmit antenna setting in a multi-access coded caching (MACC) network where each user accesses more than one cache. A MACC network has $K$ users and $K$ caches, and each user has access to $r < K$ consecutive caches in a cyclic wrap-around manner. There are $L$ antennas at the server, and each cache has a normalized size of $M/N \leq 1$. The cyclic wrap-around MACC network with a single antenna at the server has been a well-investigated topic, and several coded caching schemes and improved lower bounds on the performance are known for the same. However, this MACC network has not yet been studied under multi-antenna settings in the coded caching literature. We study the multi-antenna MACC problem and propose a solution for the same by constructing a pair of arrays called caching and delivery arrays. We present three constructions of caching and delivery arrays for different scenarios and obtain corresponding multi-antenna MACC schemes for the same. Two schemes resulting from the above constructions achieve optimal performance under uncoded placement and one-shot delivery. The optimality is shown by matching the performance of the multi-antenna MACC scheme to that of an optimal multi-antenna scheme for a dedicated cache network having an identical number of users, and each user has a normalized cache size of $rM/N$. Further, as a special case, one of the proposed schemes subsumes an existing optimal MACC scheme for the single-antenna setting.

**Link**: [arxiv](http://arxiv.org/abs/2310.08894v3),  [pdf](http://arxiv.org/pdf/2310.08894v3)

**Tags**: cs.IT math.IT 



### Not all tokens are created equal: Perplexity Attention Weighted Networks   for AI generated text detection
**Authors**: Pablo Miralles-González, Javier Huertas-Tato, Alejandro Martín, David Camacho

**Updated**: 2025-01-22T10:39:50Z

**Summary**: The rapid advancement in large language models (LLMs) has significantly enhanced their ability to generate coherent and contextually relevant text, raising concerns about the misuse of AI-generated content and making it critical to detect it. However, the task remains challenging, particularly in unseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution outputs offers a theoretically appealing approach for detection, as they encapsulate insights from the models' extensive pre-training on diverse corpora. Despite its promise, zero-shot methods that attempt to operationalize these outputs have met with limited success. We hypothesize that one of the problems is that they use the mean to aggregate next-token distribution metrics across tokens, when some tokens are naturally easier or harder to predict and should be weighted differently. Based on this idea, we propose the Perplexity Attention Weighted Network (PAWN), which uses the last hidden states of the LLM and positions to weight the sum of a series of features based on metrics from the next-token distribution across the sequence length. Although not zero-shot, our method allows us to cache the last hidden states and next-token distribution metrics on disk, greatly reducing the training resource requirements. PAWN shows competitive and even better performance in-distribution than the strongest baselines (fine-tuned LMs) with a fraction of their trainable parameters. Our model also generalizes better to unseen domains and source models, with smaller variability in the decision boundary across distribution shifts. It is also more robust to adversarial attacks, and if the backbone has multilingual capabilities, it presents decent generalization to languages not seen during supervised training, with LLaMA3-1B reaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine languages.

**Link**: [arxiv](http://arxiv.org/abs/2501.03940v2),  [pdf](http://arxiv.org/pdf/2501.03940v2)

**Tags**: cs.CL cs.AI 



### Bright single-photon source in a silicon chip by nanoscale positioning   of a color center in a microcavity
**Authors**: Baptiste Lefaucher, Yoann Baron, Jean-Baptiste Jager, Vincent Calvo, Christian Elsässer, Giuliano Coppola, Frédéric Mazen, Sébastien Kerdilès, Félix Cache, Anaïs Dréau, Jean-Michel Gérard

**Updated**: 2025-01-22T09:25:29Z

**Summary**: We present an all-silicon source of near-infrared linearly-polarized single photons, fabricated by nanoscale positioning of a color center in a silicon-on-insulator microcavity. The color center consists of a single W center, created at a well-defined position by Si$^{+}$ ion implantation through a 150 nm-diameter nanohole in a mask. A circular Bragg grating cavity resonant with the W's zero-phonon line at 1217 nm is fabricated at the same location as the nanohole. Under above-gap continuous-wave excitation, a very clean photon antibunching behavior ($g{^2} \leq 0.06$) is observed over the entire power range, which highlights the absence of parasitic emitters. Purcell-enhancement of W's zero-phonon emission provides both a record-high photoluminescence count rate among Si color centers (ca $1.2 \times 10^{6}$ counts/s) and apparent Debye-Waller factor around 99%. We also demonstrate the triggered emission of single photons with 93% purity under weak pulsed laser excitation. At high pulsed laser power, we reveal a detrimental effect of repumping processes, that could be mitigated using selective pumping schemes in the future. These results represent a major step towards on-demand sources of indistinguishable near-infrared single photons within silicon photonics chips.

**Link**: [arxiv](http://arxiv.org/abs/2501.12744v1),  [pdf](http://arxiv.org/pdf/2501.12744v1)

**Tags**: physics.optics quant-ph 



### Improved Coded Caching Scheme for Multi-User Information Retrieval   System
**Authors**: Junyi Wang, Quan Zang, Jinyu Wang, Minquan Cheng

**Updated**: 2025-01-21T22:33:15Z

**Summary**: In this paper, we study the coded caching scheme for the $(L, K, M, N)$ multi-user information retrieval (MIR) system, which consists of a content library containing $N$ files, a base station (BS) with $L$ antennas that cannot access the library, and $K$ single-antenna users, each of which can cache at most $M$ files from the library. The users communicate with the others assisted by the BS to decode their required files. In this paper, we focus on designing a coded caching scheme with low communication latency measured by normalized delivery time (NDT), computational complexity, and subpacketizations. When $\frac{KM}{N}\geq L$ we first simply the precoding matrix in the downlink step to an identity matrix and use the multiple-antenna placement delivery array (MAPDA), which was originally proposed for the multiple-input single-output networks, to generate several new schemes for MIR system. Compared to the existing schemes, both the theoretical and numerical analyses show that our new schemes achieve much lower computational complexity and smaller subpacketizations with the same NDT.

**Link**: [arxiv](http://arxiv.org/abs/2501.12528v1),  [pdf](http://arxiv.org/pdf/2501.12528v1)

**Tags**: cs.IT math.IT 



### Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and   Multiple Level Analysis
**Authors**: Weile Luo, Ruibo Fan, Zeyu Li, Dayou Du, Hongyuan Liu, Qiang Wang, Xiaowen Chu

**Updated**: 2025-01-21T12:19:02Z

**Summary**: Modern GPUs, with their specialized hardware like tensor cores, are essential for demanding AI and deep learning applications. This study presents a comprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU architecture, delving into its performance characteristics and novel features. We benchmark Hopper's memory subsystem latency and throughput, comparing its L2 partitioned cache behavior and global memory access patterns against recent GPU generations, Ampere and Ada Lovelace. Our analysis reveals significant performance differences and architectural improvements in Hopper. A core contribution of this work is a detailed evaluation of Hopper's fourth-generation tensor cores, including their FP8 precision support and the novel asynchronous wgmma instructions, assessing their impact on matrix multiply-accumulate operations. We further investigate the performance implications of other key Hopper innovations: DPX instructions for accelerating dynamic programming algorithms, distributed shared memory (DSM) for inter-SM communication, and the Tensor Memory Accelerator (TMA) for asynchronous data movement. This multi-level approach encompasses instruction-level microbenchmarks, library-level analysis of the Transformer Engine, and application-level benchmarks of tensor core performance within large language models. Our findings provide valuable, in-depth insights for software developers seeking to optimize performance and develop accurate performance models for the Hopper architecture, ultimately contributing to a deeper understanding of its potential for accelerating AI and other computationally intensive workloads.

**Link**: [arxiv](http://arxiv.org/abs/2501.12084v1),  [pdf](http://arxiv.org/pdf/2501.12084v1)

**Tags**: cs.DC cs.AR cs.PF 



### Build Optimization: A Systematic Literature Review
**Authors**: Henri Aïdasso, Mohammed Sayagh, Francis Bordeleau

**Updated**: 2025-01-21T07:32:06Z

**Summary**: Continuous Integration (CI) consists of an automated build process involving continuous compilation, testing, and packaging of the software system. While CI comes up with several advantages related to quality and time to delivery, CI also presents several challenges addressed by a large body of research. To better understand the literature so as to help practitioners find solutions for their problems and guide future research, we conduct a systematic review of 97 studies on build optimization published between 2006 and 2024, which we summarized according to their goals, methodologies, used datasets, and leveraged metrics. The identified build optimization studies focus on two main challenges: (1) long build durations, and (2) build failures. To meet the first challenge, existing studies have developed a range of techniques, including predicting build outcome and duration, selective build execution, and build acceleration using caching or repairing performance smells. The causes of build failures have been the subject of several studies, leading to the development of techniques for predicting build script maintenance and automating repair. Recent studies have also focused on predicting flaky build failures caused by environmental issues. The majority of these techniques use machine learning algorithms and leverage build metrics, which we classify into five categories. Additionally, we identify eight publicly available build datasets for build optimization research.

**Link**: [arxiv](http://arxiv.org/abs/2501.11940v1),  [pdf](http://arxiv.org/pdf/2501.11940v1)

**Tags**: cs.SE 



### PDA Construction via Union of Cartesian Product Cache Configurations for   Coded Caching
**Authors**: Jinyu Wang, Minquan Cheng, Kai Wan, Giuseppe Caire

**Updated**: 2025-01-21T02:35:31Z

**Summary**: Caching is an efficient technique to reduce peak traffic by storing popular content in local caches. Placement delivery array (PDA) proposed by Yan et al. is a combinatorial structure to design coded caching schemes with uncoded placement and one-shot linear delivery. By taking the $m$-fold Cartesian product of a small base PDA, Wang et al. constructed a big PDA while maintaining the memory ratio and transmission load unchanged, which achieves linear growth in both the number of users and coded caching gain. In order to achieve exponential growth in both the number of users and coded caching gain, in this paper we propose a PDA construction by taking the union operation of the cache configurations from the $m$-fold Cartesian product of a base PDA. The resulting PDA leads to a coded caching scheme with subpacketization increasing sub-exponentially with the number of users while keeping the load constant for fixed memory ratio. By applying the proposed construction to existing base PDAs, three new coded caching schemes are obtained, which cover some existing schemes as special cases and can achieve lower load with simultaneously lower subpacketization for some memory ratios.

**Link**: [arxiv](http://arxiv.org/abs/2501.11834v1),  [pdf](http://arxiv.org/pdf/2501.11834v1)

**Tags**: cs.IT math.IT 



### Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference
**Authors**: Pouya Hamadanian, Sadjad Fouladi

**Updated**: 2025-01-20T23:10:13Z

**Summary**: Large Language Models (LLM) have revolutionized natural language processing, but their inference demands substantial resources, while under-utilizing high-end accelerators like GPUs. A major bottleneck arises from the attention mechanism, which requires storing large key-value caches, limiting the maximum achievable throughput way below the available computing resources. Current approaches attempt to mitigate this issue through memory-efficient attention and paging mechanisms, but remained constrained by the assumption that all operations must be performed on high-end accelerators.   In this work, we propose Glinthawk, a two-tiered architecture that decouples the attention mechanism from the rest of the Transformer model. This approach allows the memory requirements for attention to scale independently, enabling larger batch sizes and more efficient use of the high-end accelerators. We prototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the other. Compared to a traditional single-tier setup, it improves throughput by $5.9\times$ and reduces cost of generation by $2.8\times$. For longer sequence lengths, it achieves $16.3\times$ throughput improvement at $2.4\times$ less cost. Our evaluation shows that this architecture can tolerate moderate network latency with minimal performance degradation, making it highly effective for latency-tolerant, throughput-oriented applications such as batch processing. We shared our prototype publicly at \url{https://github.com/microsoft/glinthawk}.

**Link**: [arxiv](http://arxiv.org/abs/2501.11779v1),  [pdf](http://arxiv.org/pdf/2501.11779v1)

**Tags**: cs.LG cs.DC cs.PF 



### Hierarchical Coded Caching in High Memory Regime with Coded Placement
**Authors**: Rajlaxmi Pandey, Charul Rajput, B. Sundar Rajan

**Updated**: 2025-01-20T14:19:48Z

**Summary**: We consider a two-layer hierarchical coded caching network where a server with a library of $N$ files is connected to $K_1$ mirrors, each having a cache memory of size $M_1$. Each mirror is further connected to $K_2$ users, each equipped with a dedicated cache of size $M_2$. In this paper, we propose two distinct coded caching schemes based on coded placement, corresponding to two distinct memory pairs, \( (M_1, M_2) \). We show that the proposed schemes outperform the existing schemes at these memory points given by the proposed schemes for smaller values of $K_2$. In setups where mirrors are positioned near each other, avoiding signal interference is crucial. This can be ensured by having all mirrors transmit using orthogonal carrier frequencies. To compare our schemes with existing ones, we used the composite rate metric, which accurately represents the total bandwidth utilized in such setups. The composite rate is given by $\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the rate from the server to the mirrors, and $R_2$ is the rate from the mirrors to the users, with respect to $M_1$ and $M_2$.

**Link**: [arxiv](http://arxiv.org/abs/2501.11502v1),  [pdf](http://arxiv.org/pdf/2501.11502v1)

**Tags**: cs.IT math.IT 



### Spineless Traversal for Layout Invalidation
**Authors**: Marisa Kirisame, Tiezhi Wang, Pavel Panchekha

**Updated**: 2025-01-20T08:44:01Z

**Summary**: Latency is a major concern for web rendering engines like those in Chrome, Safari, and Firefox. These engines reduce latency by using an incremental layout algorithm to redraw the page when the user interacts with it. In such an algorithm, elements that change frame-to-frame are marked dirty; only the dirty elements need be processed to draw the next frame, dramatically reducing latency. However, the standard incremental layout algorithm must search the page for dirty elements, accessing a number of auxiliary elements in the process. These auxiliary elements add cache misses and stalled cycles, and are responsible for a sizable fraction of all layout latency. We introduce a new, faster incremental layout algorithm called Spineless Traversal. Spineless Traversal uses a more computationally demanding priority queue algorithm to avoid the need to access auxiliary nodes and thus reduces cache traffic and stalls. This leads to dramatic speedups on the most latency-critical interactions such as hovering, typing, or animations. Moreover, thanks to numerous low-level optimizations, we are able to make Spineless Traversal competitive across the whole spectrum of incremental layout workloads. As a result, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the benchmark, with a mean speedup of 3.23x concentrated in the most latency-critical interactions such as hovering, typing, and animations.

**Link**: [arxiv](http://arxiv.org/abs/2411.10659v3),  [pdf](http://arxiv.org/pdf/2411.10659v3)

**Tags**: cs.PL 



### ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large   Vision-Language Models
**Authors**: Yassir Bendou, Amine Ouasfi, Vincent Gripon, Adnane Boukhayma

**Updated**: 2025-01-19T21:25:53Z

**Summary**: The growing popularity of Contrastive Language-Image Pretraining (CLIP) has led to its widespread application in various visual downstream tasks. To enhance CLIP's effectiveness and versatility, efficient few-shot adaptation techniques have been widely adopted. Among these approaches, training-free methods, particularly caching methods exemplified by Tip-Adapter, have gained attention for their lightweight adaptation without the need for additional fine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective, showing that caching methods function as local adapters and are connected to a well-established kernel literature. Drawing on this insight, we offer a theoretical understanding of how these methods operate and suggest multiple avenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the importance of incorporating global information in local adapters. Therefore, we subsequently propose a global method that learns a proximal regularizer in a reproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our method, which we call ProKeR (Proximal Kernel ridge Regression), has a closed form solution and achieves state-of-the-art performances across 11 datasets in the standard few-shot adaptation benchmark.

**Link**: [arxiv](http://arxiv.org/abs/2501.11175v1),  [pdf](http://arxiv.org/pdf/2501.11175v1)

**Tags**: cs.CV cs.AI cs.LG 



### Cache Coherence Over Disaggregated Memory
**Authors**: Ruihong Wang, Jianguo Wang, Walid G. Aref

**Updated**: 2025-01-19T19:46:21Z

**Summary**: Disaggregating memory from compute offers the opportunity to better utilize stranded memory in cloud data centers. It is important to cache data in the compute nodes and maintain cache coherence across multiple compute nodes. However, the limited computing power on disaggregated memory servers makes traditional cache coherence protocols suboptimal, particularly in the case of stranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache Coherence protocol that maintains cache coherence without imposing any computational burden on the remote memory side. It aligns the state machine of the shared-exclusive latch protocol with the MSI protocol by introducing lazy latch-release and invalidation messages, thereby ensuring both atomicity of data access and cache coherence. SELCC embeds cache-ownership metadata directly into the RDMA latch word, enabling efficient cache ownership management via RDMA atomic operations. SELCC can serve as an abstraction layer over disaggregated memory with APIs that resemble main-memory accesses. A concurrent B-tree and three transaction concurrency control algorithms are realized using SELCC's abstraction layer. Experimental results show that SELCC significantly outperforms Remote-Procedure-Call-based protocols for cache coherence under limited remote computing power. Applications on SELCC achieve comparable or superior performance over disaggregated memory compared to competitors.

**Link**: [arxiv](http://arxiv.org/abs/2409.02088v3),  [pdf](http://arxiv.org/pdf/2409.02088v3)

**Tags**: cs.DB cs.DC cs.ET 



### Coded Caching for Hierarchical Two-Layer Networks with Coded Placement
**Authors**: Rajlaxmi Pandey, Charul Rajput, B. Sundar Rajan

**Updated**: 2025-01-19T15:47:14Z

**Summary**: We examine a two-layered hierarchical coded caching problem, a configuration addressed in existing research. This involves a server connected to $K_1$ mirrors, each of which serves $K_2$ users. The mirrors and the users are equipped with caches of size $M_1$ and $M_2$, respectively. We propose a hierarchical coded caching scheme with coded placements that outperforms existing schemes. To ensure a fair comparison, we introduce the notion of composite rate, defined as $\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the rate from the server to mirrors and $R_2$ is the rate from mirrors to users. The composite rate has not been discussed before in the literature and is pertinent when mirrors transmit with different carrier frequencies. For the proposed scheme, we show a trade-off between the global memory $\overline{M}=K_1M_1+K_1K_2M_2$ of the system and the composite rate and compare with the existing schemes. Additionally, we conduct this comparative analysis by plotting $R_1$ + $R_2$ against global memory, which is particularly beneficial for systems wherein each mirror can utilize the same carrier frequency, given their significant spatial separation. Additionally, we propose an optimized scheme for the specific case of a single mirror, showing improved performance in this scenario.

**Link**: [arxiv](http://arxiv.org/abs/2312.15024v2),  [pdf](http://arxiv.org/pdf/2312.15024v2)

**Tags**: cs.IT math.IT 



### D2D Coded Caching Schemes for Multiaccess Networks with Combinatorial   Access Topology
**Authors**: Rashid Ummer N. T., B. Sundar Rajan

**Updated**: 2025-01-18T13:04:23Z

**Summary**: This paper considers wireless device-to-device (D2D) coded caching in a multiaccess network, where the users communicate with each other and each user can access multiple cache nodes. Access topologies derived from two combinatorial designs known as the $t$-design and $t$-group divisible design ($t$-GDD), referred to as the $t$-design and $t$-GDD topologies respectively, which subsume a few other known topologies, have been studied for the multiaccess coded caching (MACC) network by Cheng \textit{et al.} in \cite{MACC_des}. These access topologies are extended to a multiaccess D2D coded caching (MADCC) network and novel MADCC schemes are proposed. MADCC network has been studied so far only for the cyclic wrap-around topology. Apart from the proposed novel MADCC schemes, MADCC schemes are also derived from the existing MACC schemes in \cite{MACC_des}. To compare the performance of different MADCC schemes, the metrics of load per user and subpacketization level are used while keeping the number of caches and cache memory size same. The proposed MADCC scheme with $t$-design topology performs better in terms of subpacketization level while achieving the same load per user compared to the MADCC scheme derived from the MACC scheme with $t$-design topology in \cite{MACC_des}. The proposed MADCC scheme with $t$-GDD topology performs better in terms of load per user while achieving the same subpacketization level compared to the MADCC scheme derived from the MACC scheme with $t$-GDD topology in \cite{MACC_des} in some cases. Compared to the existing MADCC scheme with cyclic wrap-around topology, the proposed MADCC scheme with $t$-design topology performs better in terms of load per user, and the proposed MADCC scheme with $t$-GDD topology performs better in terms of subpacketization level at the expense of an increase in load per user.

**Link**: [arxiv](http://arxiv.org/abs/2501.10756v1),  [pdf](http://arxiv.org/pdf/2501.10756v1)

**Tags**: cs.IT math.IT 



### SkyByte: Architecting an Efficient Memory-Semantic CXL-based SSD with OS   and Hardware Co-design
**Authors**: Haoyang Zhang, Yuqi Xue, Yirui Eric Zhou, Shaobo Li, Jian Huang

**Updated**: 2025-01-18T07:29:20Z

**Summary**: The CXL-based solid-state drive (CXL-SSD) provides a promising approach towards scaling the main memory capacity at low cost. However, the CXL-SSD faces performance challenges due to the long flash access latency and unpredictable events such as garbage collection in the SSD device, stalling the host processor and wasting compute cycles. Although the CXL interface enables the byte-granular data access to the SSD, accessing flash chips is still at page granularity due to physical limitations. The mismatch of access granularity causes significant unnecessary I/O traffic to flash chips, worsening the suboptimal end-to-end data access performance. In this paper, we present SkyByte, an efficient CXL-based SSD that employs a holistic approach to address the aforementioned challenges by co-designing the host operating system (OS) and SSD controller. To alleviate the long memory stall when accessing the CXL-SSD, SkyByte revisits the OS context switch mechanism and enables opportunistic context switches upon the detection of long access delays. To accommodate byte-granular data accesses, SkyByte architects the internal DRAM of the SSD controller into a cacheline-level write log and a page-level data cache, and enables data coalescing upon log cleaning to reduce the I/O traffic to flash chips. SkyByte also employs optimization techniques that include adaptive page migration for exploring the performance benefits of fast host memory by promoting hot pages in CXL-SSD to the host. We implement SkyByte with a CXL-SSD simulator and evaluate its efficiency with various data-intensive applications. Our experiments show that SkyByte outperforms current CXL-based SSD by 6.11X, and reduces the I/O traffic to flash chips by 23.08X on average. SkyByte also reaches 75% of the performance of the ideal case that assumes unlimited DRAM capacity in the host, which offers an attractive cost-effective solution.

**Link**: [arxiv](http://arxiv.org/abs/2501.10682v1),  [pdf](http://arxiv.org/pdf/2501.10682v1)

**Tags**: cs.AR 



### Geometric rigidity of simple modules for algebraic groups
**Authors**: Michael Bate, David I. Stewart

**Updated**: 2025-01-17T16:16:54Z

**Summary**: Let k be a field, let G be an affine algebraic k-group and V a finite-dimensional G-module. We say V is rigid if the socle series and radical series coincide for the action of G on each indecomposable summand of V; say V is geometrically rigid (resp. absolutely rigid) if V is rigid after base change of G and V to k (resp. any field extension of k). We show that all simple G-modules are geometrically rigid, though not in general absolutely rigid. More precisely, we show that if V is a simple G-module, then there is a finite purely inseparable extension kV /k naturally attached to V such that V is absolutely rigid as a G-module after base change to kV. The proof turns on an investigation of algebras of the form K otimes E where K and E are field extensions of k; we give an example of such an algebra which is not rigid as a module over itself. We establish the existence of the purely inseparable field extension kV /k through an analogous version for artinian algebras.   In the second half of the paper we apply recent results on the structure and representation theory of pseudo-reductive groups to give a concrete description of kV when G is smooth and connected. Namely, we combine the main structure theorem of the Conrad-Prasad classification of pseudo-reductive G together with our previous high weight theory. For V a simple G-module, we calculate the minimal field of definition of the geometric Jacobson radical of EndG(V) in terms of the high weight of V and the Conrad-Prasad classification data; this gives a concrete construction of the field kV as a subextension of the minimal field of definition of the geometric unipotent radical of G. We also observe that the Conrad-Prasad classification can be used to hone the dimension formula for V we had previously established; we also use it to give a description of EndG(V) which includes a dimension formula.

**Link**: [arxiv](http://arxiv.org/abs/2409.05221v3),  [pdf](http://arxiv.org/pdf/2409.05221v3)

**Tags**: math.RT math.GR math.RA 20G05 



### The NIC should be part of the OS
**Authors**: Pengcheng Xu, Timothy Roscoe

**Updated**: 2025-01-17T12:01:28Z

**Summary**: The network interface adapter (NIC) is a critical component of a modern cloud server which occupies a unique position. Not only is network performance vital to the efficient operation of the machine, but unlike application-oriented compute accelerators like GPUs, the network subsystem must react to unpredictable events like the arrival of a network packet and communicate with the appropriate application end point with minimal latency. Current approaches to server stacks navigate a trade-off between flexibility, efficiency, and performance: the fastest kernel-bypass approaches dedicate cores to applications, busy-wait on receive queues, etc. while more flexible approaches appropriate to more dynamic workload mixes incur much greater software overhead on the data path. However, we reject this trade-off, which we ascribe to an arbitrary (and sub-optimal) split in system state between the OS and the NIC. Instead, by exploiting the properties of cache-coherent interconnects and integrating the NIC closely with the OS kernel, we can achieve something surprising: performance for RPC workloads better than the fastest kernel-bypass approaches without sacrificing the robustness and dynamic adaptation of kernel-based network subsystems.

**Link**: [arxiv](http://arxiv.org/abs/2501.10138v1),  [pdf](http://arxiv.org/pdf/2501.10138v1)

**Tags**: cs.OS cs.AR cs.NI 



### BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix   Sharing and Throughput-oriented Token Batching
**Authors**: Zhen Zheng, Xin Ji, Taosong Fang, Fanghao Zhou, Chuanjie Liu, Gang Peng

**Updated**: 2025-01-17T09:37:36Z

**Summary**: Large language models (LLMs) increasingly play an important role in a wide range of information processing and management tasks. Many of these tasks are performed in large batches or even offline, and the performance indictor for which is throughput. These tasks usually show the characteristic of prefix sharing, where different prompt input can partially show the common prefix. However, the existing LLM inference engines tend to optimize the streaming requests and show limitations of supporting the large batched tasks with the prefix sharing characteristic. The existing solutions use the LRU-based cache to reuse the KV context of common prefix between requests. The KV context that are about to be reused may prematurely evicted with the implicit cache management. Besides, the streaming oriented systems do not leverage the request-batch information and can not mix the decoding tokens with the prefill chunks to the best for the batched scenarios, and thus fails to saturate the GPU. We propose BatchLLM to address the above problems. BatchLLM explicitly identifies the common prefixes globally. The requests sharing the same prefix will be scheduled together to reuse the KV context the best. BatchLLM reorders the requests and schedules the requests with larger ratio of decoding first to better mix the decoding tokens with the latter prefill chunks, and applies memory-centric token batching to enlarge the token-batch sizes, which helps to increase the GPU utilization. Finally, BatchLLM optimizes the prefix-shared Attention kernel with horizontal fusion to reduce tail effect and kernel launch overhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang by 1.3$\times$ to 10.8$\times$ on a set of microbenchmarks and a typical industry workload under different hardware environments.

**Link**: [arxiv](http://arxiv.org/abs/2412.03594v2),  [pdf](http://arxiv.org/pdf/2412.03594v2)

**Tags**: cs.CL cs.AI cs.DC cs.LG 



### Multi-Dimensional Vector ISA Extension for Mobile In-Cache Computing
**Authors**: Alireza Khadem, Daichi Fujiki, Hilbert Chen, Yufeng Gu, Nishil Talati, Scott Mahlke, Reetuparna Das

**Updated**: 2025-01-17T01:24:12Z

**Summary**: In-cache computing technology transforms existing caches into long-vector compute units and offers low-cost alternatives to building expensive vector engines for mobile CPUs. Unfortunately, existing long-vector Instruction Set Architecture (ISA) extensions, such as RISC-V Vector Extension (RVV) and Arm Scalable Vector Extension (SVE), provide only one-dimensional strided and random memory accesses. While this is sufficient for typical vector engines, it fails to effectively utilize the large Single Instruction, Multiple Data (SIMD) widths of in-cache vector engines. This is because mobile data-parallel kernels expose limited parallelism across a single dimension.   Based on our analysis of mobile vector kernels, we introduce a long-vector Multi-dimensional Vector ISA Extension (MVE) for mobile in-cache computing. MVE achieves high SIMD resource utilization and enables flexible programming by abstracting cache geometry and data layout. The proposed ISA features multi-dimensional strided and random memory accesses and efficient dimension-level masked execution to encode parallelism across multiple dimensions. Using a wide range of data-parallel mobile workloads, we demonstrate that MVE offers significant performance and energy reduction benefits of 2.9x and 8.8x, on average, compared to the SIMD units of a commercial mobile processor, at an area overhead of 3.6%.

**Link**: [arxiv](http://arxiv.org/abs/2501.09902v1),  [pdf](http://arxiv.org/pdf/2501.09902v1)

**Tags**: cs.AR 



### Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of   the Atomic Layer Deposition Temperature and the Lithographic Patterning   Method
**Authors**: Alessandro Paghi, Sebastiano Battisti, Simone Tortorella, Giorgio De Simoni, Francesco Giazotto

**Updated**: 2025-01-16T15:11:42Z

**Summary**: Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics, have become the standard insulators in gate architectures, enhancing the electrical performance of both room temperature and cryogenic electronics. This study delves into the cryogenic (3 K) performance of high-k dielectrics commonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via Atomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and dielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on metal-insulator-metal capacitors. Our findings reveal a strong dependence of HfO2 cryogenic performance on the ALD growth temperature, while the latter shows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of the relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K. Additionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked their properties at cryogenic temperatures. The study also investigates the impact of the patterning method, namely, UV or electron-beam lithography (acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric properties.

**Link**: [arxiv](http://arxiv.org/abs/2407.04501v2),  [pdf](http://arxiv.org/pdf/2407.04501v2)

**Tags**: cond-mat.mtrl-sci cond-mat.mes-hall cond-mat.supr-con 



### Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk   Synchronization
**Authors**: Congcong Chen, Jinhua Cui, Gang Qu, Jiliang Zhang

**Updated**: 2025-01-16T10:35:59Z

**Summary**: Memory-disk synchronization is a critical technology for ensuring data correctness, integrity, and security, especially in systems that handle sensitive information like financial transactions and medical records. We propose SYNC+SYNC, a group of attacks that exploit the memory-disk synchronization primitives. SYNC+SYNC works by subtly varying the timing of synchronization on the write buffer, offering several advantages: 1) implemented purely in software, enabling deployment on any hardware devices; 2) resilient against existing cache partitioning and randomization techniques; 3) unaffected by prefetching techniques and cache replacement strategies. We present the principles of SYNC+SYNC through the implementation of two write covert channel protocols, using either a single file or page, and introduce three enhanced strategies that utilize multiple files and pages. The feasibility of these channels is demonstrated in both cross-process and cross-sandbox scenarios across diverse operating systems (OSes). Experimental results show that, the average rate can reach 2.036 Kb/s (with a peak rate of 14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the average rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the error rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first high-speed write covert channel for software cache.

**Link**: [arxiv](http://arxiv.org/abs/2312.11501v2),  [pdf](http://arxiv.org/pdf/2312.11501v2)

**Tags**: cs.CR 



### Adaptive Contextual Caching for Mobile Edge Large Language Model Service
**Authors**: Guangyuan Liu, Yinqiu Liu, Jiacheng Wang, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong

**Updated**: 2025-01-16T08:52:38Z

**Summary**: Mobile edge Large Language Model (LLM) deployments face inherent constraints, such as limited computational resources and network bandwidth. Although Retrieval-Augmented Generation (RAG) mitigates some challenges by integrating external knowledge bases, inefficient cache management can still result in high retrieval latency and frequent cache updates. To address these issues, we propose an Adaptive Contextual Caching (ACC) framework that anticipates user needs by proactively caching semantically relevant data for mobile-edge LLMs. ACC utilizes a deep reinforcement learning (DRL) module to refine cache replacement policies, balancing user context, document similarity, and the overhead associated with cache misses. Experimental results demonstrate that ACC increases cache hit rates to over 80\% after only 11 training episodes, outperforming FIFO, LRU, and semantic-only caching while reducing retrieval latency by up to 40\%. In particular, ACC also reduces local caching overhead (i.e., the cost of updating the cache when a miss occurs) by as much as 55\%, enabling scalable, low-latency LLM services in resource-constrained edge environments.

**Link**: [arxiv](http://arxiv.org/abs/2501.09383v1),  [pdf](http://arxiv.org/pdf/2501.09383v1)

**Tags**: cs.NI 



### Interoceptive Robots for Convergent Shared Control in Collaborative   Construction Work
**Authors**: Xiaoshan Zhou, Carol C. Menassa, Vineet R. Kamat

**Updated**: 2025-01-16T04:50:15Z

**Summary**: Building autonomous mobile robots (AMRs) with optimized efficiency and adaptive capabilities-able to respond to changing task demands and dynamic environments-is a strongly desired goal for advancing construction robotics. Such robots can play a critical role in enabling automation, reducing operational carbon footprints, and supporting modular construction processes. Inspired by the adaptive autonomy of living organisms, we introduce interoception, which centers on the robot's internal state representation, as a foundation for developing self-reflection and conscious learning to enable continual learning and adaptability in robotic agents. In this paper, we factorize internal state variables and mathematical properties as "cognitive dissonance" in shared control paradigms, where human interventions occasionally occur. We offer a new perspective on how interoception can help build adaptive motion planning in AMRs by integrating the legacy of heuristic costs from grid/graph-based algorithms with recent advances in neuroscience and reinforcement learning. Declarative and procedural knowledge extracted from human semantic inputs is encoded into a hypergraph model that overlaps with the spatial configuration of onsite layout for path planning. In addition, we design a velocity-replay module using an encoder-decoder architecture with few-shot learning to enable robots to replicate velocity profiles in contextualized scenarios for multi-robot synchronization and handover collaboration. These "cached" knowledge representations are demonstrated in simulated environments for multi-robot motion planning and stacking tasks. The insights from this study pave the way toward artificial general intelligence in AMRs, fostering their progression from complexity to competence in construction automation.

**Link**: [arxiv](http://arxiv.org/abs/2501.09290v1),  [pdf](http://arxiv.org/pdf/2501.09290v1)

**Tags**: cs.RO 



### PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid   Resolution Diffusion Serving
**Authors**: Desen Sun, Zepeng Zhao, Yuke Wang

**Updated**: 2025-01-16T02:40:07Z

**Summary**: The Text-to-Image (T2I) diffusion model is one of the most popular models in the world. However, serving diffusion models at the entire image level faces several problems, especially when there are multiple candidate resolutions. First, image based serving system prevents requests with different resolutions from batching together. On the other hand, requests with hybrid resolutions also indicate diverse locality features, which makes it hard to apply the same cache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch Management Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that provides a patch-level management strategy to gather hybrid resolution requests into batches. Specifically, PATCHEDSERVE incorporates a novel patch-based processing workflow, significantly enhancing throughput for hybrid resolution inputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to fully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features an SLO-aware scheduling algorithm with lightweight online latency prediction, achieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve 30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while not hurt the image quality.

**Link**: [arxiv](http://arxiv.org/abs/2501.09253v1),  [pdf](http://arxiv.org/pdf/2501.09253v1)

**Tags**: cs.DC 



### Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of   Micro-UAVs
**Authors**: Amit Kumar Bhuyan, Hrishikesh Dutta, Subir Biswas

**Updated**: 2025-01-15T21:09:22Z

**Summary**: This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content management system for disaster scenarios where communication infrastructure is generally compromised. Utilizing a hybrid network of stationary and mobile Micro-UAVs, this system aims to provide crucial content access to isolated communities. In the developed architecture, stationary anchor UAVs, equipped with vertical and lateral links, serve users in individual disaster-affected communities. and mobile micro-ferrying UAVs, with enhanced mobility, extend coverage across multiple such communities. The primary goal is to devise a content dissemination system that dynamically learns caching policies to maximize content accessibility to users left without communication infrastructure. The core contribution is an adaptive content dissemination framework that employs a decentralized Top-k Multi-Armed Bandit learning approach for efficient UAV caching decisions. This approach accounts for geo-temporal variations in content popularity and diverse user demands. Additionally, a Selective Caching Algorithm is proposed to minimize redundant content copies by leveraging inter-UAV information sharing. Through functional verification and performance evaluation, the proposed framework demonstrates improved system performance and adaptability across varying network sizes, micro-UAV swarms, and content popularity distributions.

**Link**: [arxiv](http://arxiv.org/abs/2404.10845v2),  [pdf](http://arxiv.org/pdf/2404.10845v2)

**Tags**: cs.LG cs.NI I.2.11 



### Towards Federated Multi-Armed Bandit Learning for Content Dissemination   using Swarm of UAVs
**Authors**: Amit Kumar Bhuyan, Hrishikesh Dutta, Subir Biswas

**Updated**: 2025-01-15T20:55:13Z

**Summary**: This paper introduces an Unmanned Aerial Vehicle - enabled content management architecture that is suitable for critical content access in communities of users that are communication-isolated during diverse types of disaster scenarios. The proposed architecture leverages a hybrid network of stationary anchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The anchor UAVs are equipped with both vertical and lateral communication links, and they serve local users, while the mobile micro-ferrying UAVs extend coverage across communities with increased mobility. The focus is on developing a content dissemination system that dynamically learns optimal caching policies to maximize content availability. The core innovation is an adaptive content dissemination framework based on distributed Federated Multi-Armed Bandit learning. The goal is to optimize UAV content caching decisions based on geo-temporal content popularity and user demand variations. A Selective Caching Algorithm is also introduced to reduce redundant content replication by incorporating inter-UAV information sharing. This method strategically preserves the uniqueness in user preferences while amalgamating the intelligence across a distributed learning system. This approach improves the learning algorithm's ability to adapt to diverse user preferences. Functional verification and performance evaluation confirm the proposed architecture's utility across different network sizes, UAV swarms, and content popularity patterns.

**Link**: [arxiv](http://arxiv.org/abs/2501.09146v1),  [pdf](http://arxiv.org/pdf/2501.09146v1)

**Tags**: cs.LG cs.NI I.2.11 



### LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System
**Authors**: Hyucksung Kwon, Kyungmo Koo, Janghyeon Kim, Woongkyu Lee, Minjae Lee, Hyungdeok Lee, Yousub Jung, Jaehan Park, Yosub Song, Byeongsu Yang, Haerang Choi, Guhyun Kim, Jongsoon Won, Woojae Shin, Changhyun Kim, Gyeongcheol Shin, Yongkee Kwon, Ilkon Kim, Euicheol Lim, John Kim, Jungwook Choi

**Updated**: 2025-01-15T01:34:46Z

**Summary**: The expansion of large language models (LLMs) with hundreds of billions of parameters presents significant challenges to computational resources, particularly data movement and memory bandwidth. Long-context LLMs, which process sequences of tens of thousands of tokens, further increase the demand on the memory system as the complexity in attention layers and key-value cache sizes is proportional to the context length. Processing-in-Memory (PIM) maximizes memory bandwidth by moving compute to the data and can address the memory bandwidth challenges; however, PIM is not necessarily scalable to accelerate long-context LLM because of limited per-module memory capacity and the inflexibility of fixed-functional unit PIM architecture and static memory management. In this work, we propose LoL-PIM which is a multi-node PIM architecture that accelerates long context LLM through hardware-software co-design. In particular, we propose how pipeline parallelism can be exploited across a multi-PIM module while a direct PIM access (DPA) controller (or DMA for PIM) is proposed that enables dynamic PIM memory management and results in efficient PIM utilization across a diverse range of context length. We developed an MLIR-based compiler for LoL-PIM extending a commercial PIM-based compiler where the software modifications were implemented and evaluated, while the hardware changes were modeled in the simulator. Our evaluations demonstrate that LoL-PIM significantly improves throughput and reduces latency for long-context LLM inference, outperforming both multi-GPU and GPU-PIM systems (up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient deployment of LLMs in real-world applications.

**Link**: [arxiv](http://arxiv.org/abs/2412.20166v2),  [pdf](http://arxiv.org/pdf/2412.20166v2)

**Tags**: cs.AR cs.AI 



### CORD: Co-design of Resource Allocation and Deadline Decomposition with   Generative Profiling
**Authors**: Robert Gifford, Abby Eisenklam, Georgiy A. Bondar, Yifan Cai, Tushar Sial, Linh Thi Xuan Phan, Abhishek Halder

**Updated**: 2025-01-14T23:13:14Z

**Summary**: As multicore hardware is becoming increasingly common in real-time systems, traditional scheduling techniques that assume a single worst-case execution time for a task are no longer adequate, since they ignore the impact of shared resources on execution time. When tasks execute concurrently on different cores, their execution times often vary substantially with their allocated budgets of shared resources, such as cache and memory bandwidth. Even under a specific resource allocation, the resource use pattern of a task also changes with time during a job execution. It is therefore important to consider the relationship between multicore resources and execution time in task modeling and scheduling algorithm design.   In this paper, we propose a much more precise execution model for DAG-based real-time tasks that captures the time-varying resource use characteristics of a task under different budgets of shared resources. We present a generative resource profiling algorithm that efficiently predicts, from limited measurement data, the resource profile of a task at any time during its execution under a given resource budget. The generative profiles can then be used to construct the execution models for tasks, using which one can make informed resource allocation decisions. We further introduce a multicore resource allocation and deadline decomposition co-design technique for DAG-based tasks that leverages the generated execution models to jointly allocate resources and deadlines to subtasks, to maximize resource efficiency and schedulability. Our evaluation results show that our generative profiling algorithm achieves high accuracy while being efficient, and that our co-allocation technique substantially improves schedulability compared to a state-of-the-art deadline decomposition method.

**Link**: [arxiv](http://arxiv.org/abs/2501.08484v1),  [pdf](http://arxiv.org/pdf/2501.08484v1)

**Tags**: cs.OS cs.PF 



### PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM   Serving
**Authors**: Ahmet Caner Yüzügüler, Jiawei Zhuang, Lukas Cavigelli

**Updated**: 2025-01-14T15:14:10Z

**Summary**: Large language models (LLMs) are widely used across various applications, but their substantial computational requirements pose significant challenges, particularly in terms of HBM bandwidth bottlenecks and inter-device communication overhead. In this paper, we present PRESERVE, a novel prefetching framework designed to optimize LLM inference by overlapping memory reads for model weights and KV-cache with collective communication operations. Through extensive experiments conducted on commercial AI accelerators, we demonstrate up to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs. Additionally, we perform a design space exploration that identifies the optimal hardware configuration for the proposed method, showing a further 1.25x improvement in performance per cost by selecting the optimal L2 cache size. Our results show that PRESERVE has the potential to mitigate the memory bottlenecks and communication overheads, offering a solution to improve the performance and scalability of the LLM inference systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.08192v1),  [pdf](http://arxiv.org/pdf/2501.08192v1)

**Tags**: cs.AI cs.AR cs.DC 



### AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out   Context Attribution
**Authors**: Fengyuan Liu, Nikhil Kandpal, Colin Raffel

**Updated**: 2025-01-14T14:07:55Z

**Summary**: The influence of contextual input on the behavior of large language models (LLMs) has prompted the development of context attribution methods that aim to quantify each context span's effect on an LLM's generations. The leave-one-out (LOO) error, which measures the change in the likelihood of the LLM's response when a given span of the context is removed, provides a principled way to perform context attribution, but can be prohibitively expensive to compute for large models. In this work, we introduce AttriBoT, a series of novel techniques for efficiently computing an approximation of the LOO error for context attribution. Specifically, AttriBoT uses cached activations to avoid redundant operations, performs hierarchical attribution to reduce computation, and emulates the behavior of large target models with smaller proxy models. Taken together, AttriBoT can provide a >300x speedup while remaining more faithful to a target model's LOO error than prior context attribution methods. This stark increase in performance makes computing context attributions for a given response 30x faster than generating the response itself, empowering real-world applications that require computing attributions at scale. We release a user-friendly and efficient implementation of AttriBoT to enable efficient LLM interpretability as well as encourage future development of efficient context attribution methods.

**Link**: [arxiv](http://arxiv.org/abs/2411.15102v2),  [pdf](http://arxiv.org/pdf/2411.15102v2)

**Tags**: cs.LG 



### TreeKV: Smooth Key-Value Cache Compression with Tree Structures
**Authors**: Ziwei He, Jian Yuan, Haoli Bai, Jingwen Leng, Bo Jiang

**Updated**: 2025-01-14T12:06:33Z

**Summary**: Efficient key-value (KV) cache compression is critical for scaling transformer-based Large Language Models (LLMs) in long sequences and resource-limited settings. Existing methods evict tokens based on their positions or importance scores, but position-based strategies can miss crucial information outside predefined regions, while those relying on global importance scores resulting in strong regional biases, limiting the KV cache's overall context retention and potentially impairing the performance of LLMs on complex tasks. Our wavelet analysis reveals that as tokens approach the end of sequence, their contributions to generation gradually increase and tends to diverge more from neighboring tokens, indicating a smooth transition with increasing complexity and variability from distant to nearby context. Motivated by this observation, we propose TreeKV, an intuitive, training-free method that employs a tree structure for smooth cache compression. TreeKV maintains a fixed cache size, allowing LLMs to deliver high-quality output even in long text scenarios. Unlike most compression methods, TreeKV is applicable to both the generation and prefilling stages. TreeKV consistently surpasses all baseline models in language modeling tasks on PG19 and OpenWebText2, allowing LLMs trained with short context window to generalize to longer window with a 16x cache reduction. On the Longbench benchmark, TreeKV achieves the best performance with only 6\% of the budget at optimal efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2501.04987v2),  [pdf](http://arxiv.org/pdf/2501.04987v2)

**Tags**: cs.CL 



### Cell-level modelling of homeostasis in confined epithelial monolayers
**Authors**: KVS Chaithanya, Jan Rozman, Andrej Košmrlj, Rastko Sknepnek

**Updated**: 2025-01-14T11:41:14Z

**Summary**: Tissue homeostasis, the biological process of maintaining a steady state in tissue via control of cell proliferation, death, and metabolic function, is essential for the development, growth, maintenance, and proper function of living organisms. Disruptions to this process can lead to serious diseases and even death. In this study, we use the vertex model for the cell-level description of tissue mechanics to investigate the impact of the tissue microenvironment and local mechanical properties of cells on homeostasis in confined epithelial tissues. We find a dynamic steady state, where the balance between cell divisions and removals sustains homeostasis. By characterising homeostasis in terms of cell count, tissue area, and the cells' neighbour count distribution, we identify the factors that govern regulated and ordered tissue growth. This work, therefore, sheds light on the mechanisms underlying tissue homeostasis and highlights the importance of mechanics in the control of biological processes such as tissue development and disease pathology.

**Link**: [arxiv](http://arxiv.org/abs/2403.15896v2),  [pdf](http://arxiv.org/pdf/2403.15896v2)

**Tags**: physics.bio-ph cond-mat.soft 



### Multi-matrix Factorization Attention
**Authors**: Jingcheng Hu, Houyi Li, Yinmin Zhang, Zili Wang, Shuigeng Zhou, Xiangyu Zhang, Heung-Yeung Shum, Daxin Jiang

**Updated**: 2025-01-14T05:48:07Z

**Summary**: We propose novel attention architectures, Multi-matrix Factorization Attention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard Multi-Head Attention (MHA), including SOTA methods like MLA, fail to maintain as strong performance under stringent Key-Value cache (KV cache) constraints. MFA enhances model capacity by efficiently scaling up both the number and dimension of attention heads through low-rank matrix factorization in the Query-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory requirements by repurposing the key cache as value through value projection re-parameterization. MFA's design enables strong model capacity when working under tight KV cache budget, while MFA-KR is suitable for even harsher KV cache limits with minor performance trade-off. Notably, in our extensive and large-scale experiments, the proposed architecture outperforms MLA and performs comparably to MHA, while reducing KV cache usage by up to 56% and 93.7%, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2412.19255v2),  [pdf](http://arxiv.org/pdf/2412.19255v2)

**Tags**: cs.LG cs.CL 



### Lean Attention: Hardware-Aware Scalable Attention Mechanism for the   Decode-Phase of Transformers
**Authors**: Rya Sanovar, Srikant Bharadwaj, Renee St. Amant, Victor Rühle, Saravan Rajmohan

**Updated**: 2025-01-14T05:00:34Z

**Summary**: Transformer-based models have emerged as one of the most widely used architectures for natural language processing, natural language generation, and image generation. The size of the state-of-the-art models has increased steadily reaching billions of parameters. These huge models are memory hungry and incur significant inference latency even on cutting edge AI-accelerators, such as GPUs. Specifically, the time and memory complexity of the attention operation is quadratic in terms of the total context length, i.e., prompt and output tokens. Thus, several optimizations such as key-value tensor caching and FlashAttention computation have been proposed to deliver the low latency demands of applications relying on such large models. However, these techniques do not cater to the computationally distinct nature of different phases during inference.   To that end, we propose LeanAttention, a scalable technique of computing self-attention for the token-generation phase (decode-phase) of decoder-only transformer models. LeanAttention enables scaling the attention mechanism implementation for the challenging case of long context lengths by re-designing the execution flow for the decode-phase. We identify that the associative property of online softmax can be treated as a reduction operation thus allowing us to parallelize the attention computation over these large context lengths. We extend the "stream-K" style reduction of tiled calculation to self-attention to enable parallel computation resulting in an average of 2.6x attention execution speedup over FlashAttention-2 and up to 8.33x speedup for 512k context lengths.

**Link**: [arxiv](http://arxiv.org/abs/2405.10480v2),  [pdf](http://arxiv.org/pdf/2405.10480v2)

**Tags**: cs.AR cs.LG I.2.7; C.1.4 



### QMDB: Quick Merkle Database
**Authors**: Isaac Zhang, Ryan Zarick, Daniel Wong, Thomas Kim, Bryan Pellegrino, Mignon Li, Kelvin Wong

**Updated**: 2025-01-14T02:02:01Z

**Summary**: Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain state management by integrating key-value (KV) and Merkle tree storage into a single unified architecture. QMDB delivers a significant throughput improvement over existing architectures, achieving up to 6X over the widely used RocksDB and 8X over NOMT, a leading verifiable database. Its novel append-only twig-based design enables one SSD read per state access, O(1) IOs for updates, and in-memory Merkleization on a memory footprint as small as 2.3 bytes per entry, enabling it to run on even modest consumer-grade PCs. QMDB scales seamlessly across both commodity and enterprise hardware, achieving up to 2.28 million state updates per second. This performance enables support for 1 million token transfers per second (TPS), marking QMDB as the first solution achieving such a milestone. QMDB has been benchmarked with workloads exceeding 15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to scale to 280 billion entries on a single server. Furthermore, QMDB introduces historical proofs, unlocking the ability to query its blockchain's historical state at the latest block. QMDB not only meets the demands of current blockchains but also provides a robust foundation for building scalable, efficient, and verifiable decentralized applications across diverse use cases.

**Link**: [arxiv](http://arxiv.org/abs/2501.05262v2),  [pdf](http://arxiv.org/pdf/2501.05262v2)

**Tags**: cs.NI cs.DB 



### FlashRNN: Optimizing Traditional RNNs on Modern Hardware
**Authors**: Korbinian Pöppel, Maximilian Beck, Sepp Hochreiter

**Updated**: 2025-01-13T17:34:22Z

**Summary**: While Transformers and other sequence-parallelizable neural network architectures seem like the current state of the art in sequence modeling, they specifically lack state-tracking capabilities. These are important for time-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs, as well as modern variants like sLSTM do have these capabilities at the cost of strictly sequential processing. While this is often seen as a strong limitation, we show how fast these networks can get with our hardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the register level on modern GPUs. We extend traditional RNNs with a parallelization variant that processes multiple RNNs of smaller hidden state in parallel, similar to the head-wise processing in Transformers. To enable flexibility on different GPU variants, we introduce a new optimization framework for hardware-internal cache sizes, memory and compute handling. It models the hardware in a setting using polyhedral-like constraints, including the notion of divisibility. This speeds up the solution process in our ConstrINT library for general integer constraint satisfaction problems (integer CSPs). We show that our kernels can achieve 50x speed-ups over a vanilla PyTorch implementation and allow 40x larger hidden sizes compared to our Triton implementation. Our open-source kernels and the optimization library are released here to boost research in the direction of state-tracking enabled RNNs and sequence modeling: \url{https://github.com/NX-AI/flashrnn}

**Link**: [arxiv](http://arxiv.org/abs/2412.07752v2),  [pdf](http://arxiv.org/pdf/2412.07752v2)

**Tags**: cs.LG cs.AI 



### DID Link: Authentication in TLS with Decentralized Identifiers and   Verifiable Credentials
**Authors**: Sandro Rodriguez Garzon, Dennis Natusch, Artur Philipp, Axel Küpper, Hans Joachim Einsiedler, Daniela Schneider

**Updated**: 2025-01-13T09:33:25Z

**Summary**: Authentication in TLS is predominately carried out with X.509 digital certificates issued by certificate authorities (CA). The centralized nature of current public key infrastructures, however, comes along with severe risks, such as single points of failure and susceptibility to cyber-attacks, potentially undermining the security and trustworthiness of the entire system. With Decentralized Identifiers (DID) alongside distributed ledger technology, it becomes technically feasible to prove ownership of a unique identifier without requiring an attestation of the proof's public key by a centralized and therefore vulnerable CA. This article presents DID Link, a novel authentication scheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant way with self-issued X.509 certificates that are equipped with ledger-anchored DIDs instead of CA-issued identifiers. It facilitates the exchange of tamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable Credentials after the TLS handshake to complete the authentication with a full identification of the communication partner. A prototypical implementation shows comparable TLS handshake durations of DID Link if verification material is cached and reasonable prolongations if it is obtained from a ledger. The significant speed improvement of the resulting TLS channel over a widely used, DID-based alternative transport protocol on the application layer demonstrates the potential of DID Link to become a viable solution for the establishment of secure and trustful end-to-end communication links with decentrally managed digital identities.

**Link**: [arxiv](http://arxiv.org/abs/2405.07533v4),  [pdf](http://arxiv.org/pdf/2405.07533v4)

**Tags**: cs.CR cs.NI 



### Generating Data Locality to Accelerate Sparse Matrix-Matrix   Multiplication on CPUs
**Authors**: Jordi Wolfson-Pou, Jan Laukemann, Fabrizio Petrini

**Updated**: 2025-01-13T04:31:04Z

**Summary**: Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation in many applications. Current multithreaded implementations are based on Gustavson's algorithm and often perform poorly on large matrices due to limited cache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic NUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To generate locality, MAGNUS reorders the intermediate product into discrete cache-friendly chunks using a two-level hierarchical approach. The accumulator is applied to each chunk, where the chunk size is chosen such that the accumulator is cache-efficient. MAGNUS is input- and system-aware: based on the matrix characteristics and target system specifications, the optimal number of chunks is computed by minimizing the storage cost of the necessary data structures. MAGNUS allows for a hybrid accumulation strategy in which each chunk uses a different accumulator based on an input threshold. We consider two accumulators: an AVX-512 vectorized bitonic sorting algorithm and classical dense accumulation. An OpenMP implementation of MAGNUS is compared with several baselines for a variety of different matrices on three Intel x86 architectures. For matrices from the SuiteSparse collection, MAGNUS is faster than all the baselines in most cases and is orders of magnitude faster than Intel MKL for several matrices. For massive random matrices that model social network graphs, MAGNUS scales to the largest matrix sizes, while the baselines fail to do so. Furthermore, MAGNUS is close to the optimal bound for these matrices, regardless of the matrix size, structure, and density.

**Link**: [arxiv](http://arxiv.org/abs/2501.07056v1),  [pdf](http://arxiv.org/pdf/2501.07056v1)

**Tags**: cs.DC 



### A Unified Framework for Automated Code Transformation and Pragma   Insertion
**Authors**: Stéphane Pouget, Louis-Noël Pouchet, Jason Cong

**Updated**: 2025-01-13T03:11:28Z

**Summary**: High-level synthesis, source-to-source compilers, and various Design Space Exploration techniques for pragma insertion have significantly improved the Quality of Results of generated designs. These tools offer benefits such as reduced development time and enhanced performance. However, achieving high-quality results often requires additional manual code transformations and tiling selections, which are typically performed separately or as pre-processing steps. Although DSE techniques enable code transformation upfront, the vastness of the search space often limits the exploration of all possible code transformations, making it challenging to determine which transformations are necessary. Additionally, ensuring correctness remains challenging, especially for complex transformations and optimizations.   To tackle this obstacle, we first propose a comprehensive framework leveraging HLS compilers. Our system streamlines code transformation, pragma insertion, and tiles size selection for on-chip data caching through a unified optimization problem, aiming to enhance parallelization, particularly beneficial for computation-bound kernels. Them employing a novel Non-Linear Programming (NLP) approach, we simultaneously ascertain transformations, pragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation demonstrates that our framework adeptly identifies the appropriate transformations, including scenarios where no transformation is necessary, and inserts pragmas to achieve a favorable Quality of Results.

**Link**: [arxiv](http://arxiv.org/abs/2405.03058v5),  [pdf](http://arxiv.org/pdf/2405.03058v5)

**Tags**: cs.SE cs.PL 



### On Optimizing Locality of Graph Transposition on Modern Architectures
**Authors**: Mohsen Koohi Esfahani, Hans Vandierendonck

**Updated**: 2025-01-12T17:01:40Z

**Summary**: This paper investigates the shared-memory Graph Transposition (GT) problem, a fundamental graph algorithm that is widely used in graph analytics and scientific computing.   Previous GT algorithms have significant memory requirements that are proportional to the number of vertices and threads which obstructs their use on large graphs. Moreover, atomic memory operations have become comparably fast on recent CPU architectures, which creates new opportunities for improving the performance of concurrent atomic accesses in GT.   We design PoTra, a GT algorithm which leverages graph structure and processor and memory architecture to optimize locality and performance. PoTra limits the size of additional data structures close to CPU cache sizes and utilizes the skewed degree distribution of graph datasets to optimize locality and performance. We present the performance model of PoTra to explain the connection between cache and memory response times and graph locality.   Our evaluation of PoTra on three CPU architectures and 20 real-world and synthetic graph datasets with up to 128 billion edges demonstrates that PoTra achieves up to 8.7 times speedup compared to previous works and if there is a performance loss it remains limited to 15.7%, on average.

**Link**: [arxiv](http://arxiv.org/abs/2501.06872v1),  [pdf](http://arxiv.org/pdf/2501.06872v1)

**Tags**: cs.DC cs.AR cs.DS cs.PF 



### MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large   Language Model Inference
**Authors**: Wenxuan Zeng, Ye Dong, Jinjin Zhou, Junming Ma, Jin Tan, Runsheng Wang, Meng Li

**Updated**: 2025-01-12T13:18:04Z

**Summary**: Private large language model (LLM) inference based on secure multi-party computation (MPC) offers cryptographically-secure protection for both user prompt and proprietary model weights. However, it suffers from large latency overhead especially for long input sequences. While key-value (KV) cache eviction algorithms have been proposed to reduce the computation and memory cost for plaintext inference, they are not designed for MPC and cannot benefit private inference easily. In this paper, we propose an accurate and MPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on the observation that historical tokens in a long sequence may have different effects on the downstream decoding. Hence, MPCache combines a look-once static eviction algorithm to discard unimportant tokens and a query-aware dynamic selection algorithm to further select a small subset of tokens for attention computation. As existing dynamic selection algorithms incur too much latency, we propose a series of optimizations to drastically reduce the KV cache selection overhead, including MPC-friendly similarity approximation, hierarchical KV cache clustering, and cross-layer index sharing strategy. With extensive experiments, we demonstrate that MPCache consistently outperforms prior-art KV cache eviction baselines across different LLM generation tasks and achieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction on different sequence lengths, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2501.06807v1),  [pdf](http://arxiv.org/pdf/2501.06807v1)

**Tags**: cs.CR 



### Linear Attention Sequence Parallelism
**Authors**: Weigao Sun, Zhen Qin, Dong Li, Xuyang Shen, Yu Qiao, Yiran Zhong

**Updated**: 2025-01-12T12:01:47Z

**Summary**: Sequence parallelism (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single device. However, for linear sequence modeling methods like linear attention, existing SP approaches do not take advantage of their right-product-first feature, resulting in sub-optimal communication efficiency and usability. In this paper, we introduce Linear Attention Sequence Parallelism (LASP), an efficient SP approach designed for linear attention-based transformer models. Specifically, we design an efficient point-to-point ring-style communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead, comparing with existing SP methods. We enhance the computation efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPUs. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with very-long sequences. We also discuss the generalization of LASP on other linear sequence modeling methods. Extensive experiments on linear attention-based models are conducted with varying sequence lengths from 2K to 4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\times$ longer than existing SP methods. The code is available at https://github.com/OpenNLPLab/LASP.

**Link**: [arxiv](http://arxiv.org/abs/2404.02882v2),  [pdf](http://arxiv.org/pdf/2404.02882v2)

**Tags**: cs.LG cs.CL 



### Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma   Generated THz Pulses
**Authors**: Benjamin Colmey, Rodrigo T. Paulino, Gaspard Beaufort, David G. Cooke

**Updated**: 2025-01-12T11:15:41Z

**Summary**: Terahertz pulses generated by two-color laser plasmas have reported peak field strengths exceeding MV/cm, and when illuminating metal nanotips the near-field enhancement at the tip apex should result in extremely high bunch charges and electron energies via sub-cycle cold field emission. Here, electron emission from tungsten nanotips driven by THz pulses generated by a long filament air-plasma are reported. Electron energies up to 1.1 keV and bunch charges up to 2x$10^5$ electrons per pulse were detected, well below values expected for peak field calculated via the time averaged Poynting vector. Investigations revealed a failure in the use of the time-averaged Poynting vector when applied to long filament THz pulses, due to spatio-temporal restructuring of the THz pulse in the focus. Accounting for this restructuring significantly reduces the field strength to approximately 160 ~kV/cm, consistent with the observed electron bunch charges, peak energies and their dependence on the tip position in the THz focus. Despite these findings, our results surpass previous THz plasma-driven electron generation by an order of magnitude in both electron energy and bunch charge and a path to increasing these by an additional order of magnitude by modification of the THz optics is proposed.

**Link**: [arxiv](http://arxiv.org/abs/2409.07196v3),  [pdf](http://arxiv.org/pdf/2409.07196v3)

**Tags**: cond-mat.mtrl-sci physics.plasm-ph 



### Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion
**Authors**: Bohai Gu, Hao Luo, Song Guo, Peiran Dong

**Updated**: 2025-01-12T05:25:06Z

**Summary**: Recently, diffusion-based methods have achieved great improvements in the video inpainting task. However, these methods still face many challenges, such as maintaining temporal consistency and the time-consuming issue. This paper proposes an advanced video inpainting framework using optical Flow-guided Efficient Diffusion, called FloED. Specifically, FloED employs a dual-branch architecture, where a flow branch first restores corrupted flow and a multi-scale flow adapter provides motion guidance to the main inpainting branch. Additionally, a training-free latent interpolation method is proposed to accelerate the multi-step denoising process using flow warping. Further introducing a flow attention cache mechanism, FLoED efficiently reduces the computational cost brought by incorporating optical flow. Comprehensive experiments in both background restoration and object removal tasks demonstrate that FloED outperforms state-of-the-art methods from the perspective of both performance and efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2412.00857v2),  [pdf](http://arxiv.org/pdf/2412.00857v2)

**Tags**: cs.CV 



## Keyword: LLM Inference 
 ### Redefining Machine Unlearning: A Conformal Prediction-Motivated Approach
**Authors**: Yingdan Shi, Ren Wang

**Updated**: 2025-01-31T18:58:43Z

**Summary**: Machine unlearning seeks to systematically remove specified data from a trained model, effectively achieving a state as though the data had never been encountered during training. While metrics such as Unlearning Accuracy (UA) and Membership Inference Attack (MIA) provide a baseline for assessing unlearning performance, they fall short of evaluating the completeness and reliability of forgetting. This is because the ground truth labels remain potential candidates within the scope of uncertainty quantification, leaving gaps in the evaluation of true forgetting. In this paper, we identify critical limitations in existing unlearning metrics and propose enhanced evaluation metrics inspired by conformal prediction. Our metrics can effectively capture the extent to which ground truth labels are excluded from the prediction set. Furthermore, we observe that many existing machine unlearning methods do not achieve satisfactory forgetting performance when evaluated with our new metrics. To address this, we propose an unlearning framework that integrates conformal prediction insights into Carlini & Wagner adversarial attack loss. Extensive experiments on the image classification task demonstrate that our enhanced metrics offer deeper insights into unlearning effectiveness, and that our unlearning framework significantly improves the forgetting quality of unlearning methods.

**Link**: [arxiv](http://arxiv.org/abs/2501.19403v1),  [pdf](http://arxiv.org/pdf/2501.19403v1)

**Tags**: cs.LG cs.AI 



### LLMs Are In-Context Bandit Reinforcement Learners
**Authors**: Giovanni Monea, Antoine Bosselut, Kianté Brantley, Yoav Artzi

**Updated**: 2025-01-31T18:58:24Z

**Summary**: Large Language Models (LLMs) excel at in-context learning (ICL), a supervised learning technique that relies on adding annotated examples to the model context. We investigate a contextual bandit version of in-context reinforcement learning (ICRL), where models learn in-context, online, from external reward, instead of supervised data. We show that LLMs effectively demonstrate such learning, and provide a detailed study of the phenomena, experimenting with challenging classification tasks and models of sizes from 500M to 70B parameters. This includes identifying and addressing the instability of the process, demonstrating learning with both semantic and abstract labels, and showing scaling trends. Our findings highlight ICRL capabilities in LLMs, while also underscoring fundamental limitations in their implicit reasoning about errors.

**Link**: [arxiv](http://arxiv.org/abs/2410.05362v2),  [pdf](http://arxiv.org/pdf/2410.05362v2)

**Tags**: cs.CL cs.AI cs.LG 



### Vintix: Action Model via In-Context Reinforcement Learning
**Authors**: Andrey Polubarov, Nikita Lyubaykin, Alexander Derevyagin, Ilya Zisman, Denis Tarasov, Alexander Nikulin, Vladislav Kurenkov

**Updated**: 2025-01-31T18:57:08Z

**Summary**: In-Context Reinforcement Learning (ICRL) represents a promising paradigm for developing generalist agents that learn at inference time through trial-and-error interactions, analogous to how large language models adapt contextually, but with a focus on reward maximization. However, the scalability of ICRL beyond toy tasks and single-domain settings remains an open challenge. In this work, we present the first steps toward scaling ICRL by introducing a fixed, cross-domain model capable of learning behaviors through in-context reinforcement learning. Our results demonstrate that Algorithm Distillation, a framework designed to facilitate ICRL, offers a compelling and competitive alternative to expert distillation to construct versatile action models. These findings highlight the potential of ICRL as a scalable approach for generalist decision-making systems. Code to be released at https://github.com/dunnolab/vintix

**Link**: [arxiv](http://arxiv.org/abs/2501.19400v1),  [pdf](http://arxiv.org/pdf/2501.19400v1)

**Tags**: cs.LG cs.AI cs.RO 



### Do LLMs Strategically Reveal, Conceal, and Infer Information? A   Theoretical and Empirical Analysis in The Chameleon Game
**Authors**: Mustafa O. Karabag, Ufuk Topcu

**Updated**: 2025-01-31T18:53:43Z

**Summary**: Large language model-based (LLM-based) agents have become common in settings that include non-cooperative parties. In such settings, agents' decision-making needs to conceal information from their adversaries, reveal information to their cooperators, and infer information to identify the other agents' characteristics. To investigate whether LLMs have these information control and decision-making capabilities, we make LLM agents play the language-based hidden-identity game, The Chameleon. In the game, a group of non-chameleon agents who do not know each other aim to identify the chameleon agent without revealing a secret. The game requires the aforementioned information control capabilities both as a chameleon and a non-chameleon. The empirical results show that while non-chameleon LLM agents identify the chameleon, they fail to conceal the secret from the chameleon, and their winning probability is far from the levels of even trivial strategies. To formally explain this behavior, we give a theoretical analysis for a spectrum of strategies, from concealing to revealing, and provide bounds on the non-chameleons' winning probability. Based on the empirical results and theoretical analysis of different strategies, we deduce that LLM-based non-chameleon agents reveal excessive information to agents of unknown identities. Our results point to a weakness of contemporary LLMs, including GPT-4, GPT-4o, Gemini 1.5, and Claude 3.5 Sonnet, in strategic interactions.

**Link**: [arxiv](http://arxiv.org/abs/2501.19398v1),  [pdf](http://arxiv.org/pdf/2501.19398v1)

**Tags**: cs.AI cs.GT cs.LG 



### Contextual Emotion Recognition using Large Vision Language Models
**Authors**: Yasaman Etesam, Özge Nilay Yalçın, Chuxuan Zhang, Angelica Lim

**Updated**: 2025-01-31T18:53:30Z

**Summary**: "How does the person in the bounding box feel?" Achieving human-level recognition of the apparent emotion of a person in real world situations remains an unsolved task in computer vision. Facial expressions are not enough: body pose, contextual knowledge, and commonsense reasoning all contribute to how humans perform this emotional theory of mind task. In this paper, we examine two major approaches enabled by recent large vision language models: 1) image captioning followed by a language-only LLM, and 2) vision language models, under zero-shot and fine-tuned setups. We evaluate the methods on the Emotions in Context (EMOTIC) dataset and demonstrate that a vision language model, fine-tuned even on a small dataset, can significantly outperform traditional baselines. The results of this work aim to help robots and agents perform emotionally sensitive decision-making and interaction in the future.

**Link**: [arxiv](http://arxiv.org/abs/2405.08992v2),  [pdf](http://arxiv.org/pdf/2405.08992v2)

**Tags**: cs.CV 



### Fixed-Population Causal Inference for Models of Equilibrium
**Authors**: Konrad Menzel

**Updated**: 2025-01-31T18:48:12Z

**Summary**: In contrast to problems of interference in (exogenous) treatments, models of interference in unit-specific (endogenous) outcomes do not usually produce a reduced-form representation where outcomes depend on other units' treatment status only at a short network distance, or only through a known exposure mapping. This remains true if the structural mechanism depends on outcomes of peers only at a short network distance, or through a known exposure mapping. In this paper, we first define causal estimands that are identified and estimable from a single experiment on the network under minimal assumptions on the structure of interference, and which represent average partial causal responses which generally vary with other global features of the realized assignment. Under a fixed-population, design-based approach, we show unbiasedness, consistency and asymptotic normality for inverse-probability weighting (IPW) estimators for those causal parameters from a randomized experiment on a single network. We also analyze more closely the case of marginal interventions in a model of equilibrium with smooth response functions where we can recover LATE-type weighted averages of derivatives of those response functions. Under additional structural assumptions, these "agnostic" causal estimands can be combined to recover model parameters, but also retain their less restrictive causal interpretation.

**Link**: [arxiv](http://arxiv.org/abs/2501.19394v1),  [pdf](http://arxiv.org/pdf/2501.19394v1)

**Tags**: econ.EM 



### Cache Me If You Must: Adaptive Key-Value Quantization for Large Language   Models
**Authors**: Alina Shutova, Vladimir Malinovskii, Vage Egiazarian, Denis Kuznedelev, Denis Mazur, Nikita Surkov, Ivan Ermakov, Dan Alistarh

**Updated**: 2025-01-31T18:47:42Z

**Summary**: Efficient real-world deployments of large language models (LLMs) rely on Key-Value (KV) caching for processing and generating long outputs, reducing the need for repetitive computation. For large contexts, Key-Value caches can take up tens of gigabytes of device memory, as they store vector representations for each token and layer. Recent work has shown that the cached vectors can be compressed through quantization, pruning or merging, but these techniques often compromise quality towards higher compression rates. In this work, we aim to improve Key & Value compression by exploiting two observations: 1) the inherent dependencies between keys and values across different layers, and 2) high-compression mechanisms for internal network states. We propose AQUA-KV, an adaptive quantization for Key-Value caches that relies on compact adapters to exploit existing dependencies between Keys and Values, and aims to "optimally" compress the information that cannot be predicted. AQUA-KV significantly improves compression rates, while maintaining high accuracy on state-of-the-art LLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5 bits per value with under $1\%$ relative error in perplexity and LongBench scores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a single GPU within 1-6 hours, even for 70B models.

**Link**: [arxiv](http://arxiv.org/abs/2501.19392v1),  [pdf](http://arxiv.org/pdf/2501.19392v1)

**Tags**: cs.LG 



### Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large   Language Models
**Authors**: Wenzhi Fang, Dong-Jun Han, Liangqi Yuan, Seyyedali Hosseinalipour, Christopher G. Brinton

**Updated**: 2025-01-31T18:44:35Z

**Summary**: Fine-tuning large language models (LLMs) on devices is attracting increasing interest. Recent works have fused low-rank adaptation (LoRA) techniques with federated fine-tuning to mitigate challenges associated with device model sizes and data scarcity. Still, the heterogeneity of computational resources remains a critical bottleneck: while higher-rank modules generally enhance performance, varying device capabilities constrain LoRA's feasible rank range. Existing approaches attempting to resolve this issue either lack analytical justification or impose additional computational overhead, leaving a wide gap for an efficient and theoretically-grounded solution. To address these challenges, we propose federated sketching LoRA (FSLoRA), which leverages a sketching mechanism to enable devices to selectively update submatrices of global LoRA modules maintained by the server. By adjusting the sketching ratios, which determine the ranks of the submatrices on the devices, FSLoRA flexibly adapts to device-specific communication and computational constraints. We provide a rigorous convergence analysis of FSLoRA that characterizes how the sketching ratios affect the convergence rate. Through comprehensive experiments on multiple datasets and LLM models, we demonstrate FSLoRA's superior performance compared to various baselines.

**Link**: [arxiv](http://arxiv.org/abs/2501.19389v1),  [pdf](http://arxiv.org/pdf/2501.19389v1)

**Tags**: cs.LG 



### Using gradient of Lagrangian function to compute efficient channels for   the ideal observer
**Authors**: Weimin Zhou

**Updated**: 2025-01-31T18:34:16Z

**Summary**: It is widely accepted that the Bayesian ideal observer (IO) should be used to guide the objective assessment and optimization of medical imaging systems. The IO employs complete task-specific information to compute test statistics for making inference decisions and performs optimally in signal detection tasks. However, the IO test statistic typically depends non-linearly on the image data and cannot be analytically determined. The ideal linear observer, known as the Hotelling observer (HO), can sometimes be used as a surrogate for the IO. However, when image data are high dimensional, HO computation can be difficult. Efficient channels that can extract task-relevant features have been investigated to reduce the dimensionality of image data to approximate IO and HO performance. This work proposes a novel method for generating efficient channels by use of the gradient of a Lagrangian-based loss function that was designed to learn the HO. The generated channels are referred to as the Lagrangian-gradient (L-grad) channels. Numerical studies are conducted that consider binary signal detection tasks involving various backgrounds and signals. It is demonstrated that channelized HO (CHO) using L-grad channels can produce significantly better signal detection performance compared to the CHO using PLS channels. Moreover, it is shown that the proposed L-grad method can achieve significantly lower computation time compared to the PLS method.

**Link**: [arxiv](http://arxiv.org/abs/2501.19381v1),  [pdf](http://arxiv.org/pdf/2501.19381v1)

**Tags**: eess.SP cs.CV cs.LG math.ST stat.CO stat.TH 



### SELMA: A Speech-Enabled Language Model for Virtual Assistant   Interactions
**Authors**: Dominik Wagner, Alexander Churchill, Siddarth Sigtia, Erik Marchi

**Updated**: 2025-01-31T18:30:36Z

**Summary**: In this work, we present and evaluate SELMA, a Speech-Enabled Language Model for virtual Assistant interactions that integrates audio and text as inputs to a Large Language Model (LLM). SELMA is designed to handle three primary and two auxiliary tasks related to interactions with virtual assistants simultaneously within a single end-to-end model. We employ low-rank adaptation modules for parameter-efficient training of both the audio encoder and the LLM. Additionally, we implement a feature pooling strategy enabling the system to recognize global patterns and improve accuracy on tasks less reliant on individual sequence elements. Experimental results on Voice Trigger (VT) detection, Device-Directed Speech Detection (DDSD), and Automatic Speech Recognition (ASR), demonstrate that our approach both simplifies the typical input processing pipeline of virtual assistants significantly and also improves performance compared to dedicated models for each individual task. SELMA yields relative Equal-Error Rate improvements of 64% on the VT detection task, and 22% on DDSD, while also achieving word error rates close to the baseline.

**Link**: [arxiv](http://arxiv.org/abs/2501.19377v1),  [pdf](http://arxiv.org/pdf/2501.19377v1)

**Tags**: cs.SD cs.CL cs.LG eess.AS 



### GPT-4o as the Gold Standard: A Scalable and General Purpose Approach to   Filter Language Model Pretraining Data
**Authors**: Jifan Zhang, Ziyue Luo, Jia Liu, Ness Shroff, Robert Nowak

**Updated**: 2025-01-31T18:21:59Z

**Summary**: Large language models require vast amounts of high-quality training data, but effective filtering of web-scale datasets remains a significant challenge. This paper demonstrates that GPT-4o is remarkably effective at identifying high-quality training data, but its prohibitive cost makes it impractical at web-scale. We propose SIEVE, a lightweight alternative that matches GPT-4o accuracy at less than 1\% of the cost. SIEVE can perform up to 500 filtering operations for the cost of one GPT-4o filtering call. The key to SIEVE is a seamless integration of GPT-4o and lightweight text classification models, using active learning to fine-tune these models in the background with a small number of calls to GPT-4o. Once trained, it performs as well as GPT-4o at a tiny fraction of the cost. Through different filtering prompts, SIEVE can efficiently curate high quality data for general or specialized domains from web-scale corpora -- a valuable capability given the current scarcity of high-quality domain-specific datasets. Extensive experiments using automatic and human evaluation metrics show that SIEVE and GPT-4o achieve similar performance on five highly specific filtering prompts. In addition, when performing quality filtering on web crawl datasets, we demonstrate SIEVE can further improve over state-of-the-art quality filtering methods in the DataComp-LM challenge for selecting LLM pretraining data.

**Link**: [arxiv](http://arxiv.org/abs/2410.02755v3),  [pdf](http://arxiv.org/pdf/2410.02755v3)

**Tags**: cs.CL cs.LG 



### Greedy Stein Variational Gradient Descent: An algorithmic approach for   wave prospection problems
**Authors**: Jose L. Varona-Santana, Marcos A. Capistrán

**Updated**: 2025-01-31T18:21:12Z

**Summary**: In this project, we propose a Variational Inference algorithm to approximate posterior distributions. Building on prior methods, we develop the Gradient-Steered Stein Variational Gradient Descent (G-SVGD) approach. This method introduces a novel loss function that combines a weighted gradient and the Evidence Lower Bound (ELBO) to enhance convergence speed and accuracy. The learning rate is determined through a suboptimal minimization of this loss function within a gradient descent framework.   The G-SVGD method is compared against the standard Stein Variational Gradient Descent (SVGD) approach, employing the ADAM optimizer for learning rate adaptation, as well as the Markov Chain Monte Carlo (MCMC) method. We assess performance in two wave prospection models representing low-contrast and high-contrast subsurface scenarios. To achieve robust numerical approximations in the forward model solver, a five-point operator is employed, while the adjoint method improves accuracy in computing gradients of the log posterior.   Our findings demonstrate that G-SVGD accelerates convergence and offers improved performance in scenarios where gradient evaluation is computationally expensive. The abstract highlights the algorithm's applicability to wave prospection models and its potential for broader applications in Bayesian inference. Finally, we discuss the benefits and limitations of G-SVGD, emphasizing its contribution to advancing computational efficiency in uncertainty quantification.

**Link**: [arxiv](http://arxiv.org/abs/2501.19370v1),  [pdf](http://arxiv.org/pdf/2501.19370v1)

**Tags**: stat.CO 



### The Physics and Metaphysics of Social Powers: Bridging Cognitive   Processing and Social Dynamics, a New Perspective on Power through Active   Inference
**Authors**: Mahault Albarracin, Sonia de Jager, David Hyland, Sarah Grace Manski

**Updated**: 2025-01-31T18:20:50Z

**Summary**: The concept of power can be explored at several scales: from physical action and process effectuation, all the way to complex social dynamics. A spectrum-wide analysis of power requires attention to the fundamental principles that constrain these processes. In the social realm, the acquisition and maintenance of power is intertwined with both social interactions and cognitive processing capacity: socially-facilitated empowerment grants agents more information-processing capacities and opportunities, either by relying on others to bring about desired policies or ultimately outcomes, and/or by enjoying more information-processing possibilities as a result of relying on others for the reproduction of (material) tasks. The effects of social empowerment thus imply an increased ability to harness computation toward desired ends, thereby augmenting the evolution of a specific state space. Empowered individuals attract the attention of others, who contribute to increasing the scale of their access to various policies effectuating these state spaces. The presented argument posits that social power, in the context of active inference, is a function of several variables. As a result of its power-amplifying effects, this extended computational ability also buffers against possible vulnerabilities. We propose that individuals wield power not only by associating with others possessing desirable policies, but also by enhancing their ability to intake and compute information effectively. This dual mechanism is argued to create a cyclical, reinforcing pattern wherein the empowered are able to incrementally expand the scope of policies and state spaces available to them while minimizing risk-exposure.

**Link**: [arxiv](http://arxiv.org/abs/2501.19368v1),  [pdf](http://arxiv.org/pdf/2501.19368v1)

**Tags**: physics.soc-ph 



### High-cadence observations of galactic nuclei by the future two-band   UV-photometry mission QUVIK
**Authors**: Michal Zajaček, Norbert Werner, Henry Best, Jolie Esme L'Heureux, Jakub Řípa, Monika Pikhartová, Martin Mondek, Filip Münz, Lýdia Štofanová, Petr Kurfürst, Matúš Labaj, Izzy L. Garland, Aaron Tohuvavohu, Vladimír Karas, Petra Suková

**Updated**: 2025-01-31T18:17:20Z

**Summary**: The Quick Ultra-Violet Kilonova surveyor (QUVIK), a two-band UV space telescope approved for funding as a Czech national science and technology mission, will focus on detecting early UV light of kilonovae (Werner et al., 2024). In addition, it will study the UV emission of stars and stellar systems (Krti\v{c}ka et al., 2024) as well as the intense and variable emission of active galactic nuclei (AGN) or galactic nuclei activated by tidal disruption events (Zaja\v{c}ek et al., 2024). In this contribution, we describe the role of this small ($\sim 30$-cm diameter) UV telescope for studying bright, nearby AGN. With its NUV and FUV bands, the telescope will perform high-cadence ($\sim$ 0.1-1 day) two-band photometric monitoring of nearby AGN ($z<1$), which will allow us to probe accretion disk sizes/temperature profiles via photometric reverberation mapping. Thanks to its versatility, QUVIK will be able to perform a moderately fast repointing ($<20$ min) to target candidates for tidal disruption events (TDEs). Early detection of the UV emission following a TDE optical flare, in combination with the subsequent two-band UV monitoring performed simultaneously with other observatories, will enable us to infer the time delay (or its lack of) between the optical, UV, and X-ray emission. In combination with theoretical models, it will be possible to shed more light on the origin of the UV/optical emission of TDEs. Furthermore, the two-band monitoring of nuclear transients will be beneficial in distinguishing between TDEs (nearly constant blue colour) and supernovae (progressive reddening).

**Link**: [arxiv](http://arxiv.org/abs/2501.19365v1),  [pdf](http://arxiv.org/pdf/2501.19365v1)

**Tags**: astro-ph.GA astro-ph.HE 



### CoSTI: Consistency Models for (a faster) Spatio-Temporal Imputation
**Authors**: Javier Solís-García, Belén Vega-Márquez, Juan A. Nepomuceno, Isabel A. Nepomuceno-Chamorro

**Updated**: 2025-01-31T18:14:28Z

**Summary**: Multivariate Time Series Imputation (MTSI) is crucial for many applications, such as healthcare monitoring and traffic management, where incomplete data can compromise decision-making. Existing state-of-the-art methods, like Denoising Diffusion Probabilistic Models (DDPMs), achieve high imputation accuracy; however, they suffer from significant computational costs and are notably time-consuming due to their iterative nature. In this work, we propose CoSTI, an innovative adaptation of Consistency Models (CMs) for the MTSI domain. CoSTI employs Consistency Training to achieve comparable imputation quality to DDPMs while drastically reducing inference times, making it more suitable for real-time applications. We evaluate CoSTI across multiple datasets and missing data scenarios, demonstrating up to a 98% reduction in imputation time with performance on par with diffusion-based models. This work bridges the gap between efficiency and accuracy in generative imputation tasks, providing a scalable solution for handling missing data in critical spatio-temporal systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.19364v1),  [pdf](http://arxiv.org/pdf/2501.19364v1)

**Tags**: cs.LG cs.AI 



### We're Different, We're the Same: Creative Homogeneity Across LLMs
**Authors**: Emily Wenger, Yoed Kenett

**Updated**: 2025-01-31T18:12:41Z

**Summary**: Numerous powerful large language models (LLMs) are now available for use as writing support tools, idea generators, and beyond. Although these LLMs are marketed as helpful creative assistants, several works have shown that using an LLM as a creative partner results in a narrower set of creative outputs. However, these studies only consider the effects of interacting with a single LLM, begging the question of whether such narrowed creativity stems from using a particular LLM -- which arguably has a limited range of outputs -- or from using LLMs in general as creative assistants. To study this question, we elicit creative responses from humans and a broad set of LLMs using standardized creativity tests and compare the population-level diversity of responses. We find that LLM responses are much more similar to other LLM responses than human responses are to each other, even after controlling for response structure and other key variables. This finding of significant homogeneity in creative outputs across the LLMs we evaluate adds a new dimension to the ongoing conversation about creativity and LLMs. If today's LLMs behave similarly, using them as a creative partners -- regardless of the model used -- may drive all users towards a limited set of "creative" outputs.

**Link**: [arxiv](http://arxiv.org/abs/2501.19361v1),  [pdf](http://arxiv.org/pdf/2501.19361v1)

**Tags**: cs.CY cs.AI cs.CL cs.LG 



### Towards Fast, Specialized Machine Learning Force Fields: Distilling   Foundation Models via Energy Hessians
**Authors**: Ishan Amin, Sanjeev Raja, Aditi Krishnapriyan

**Updated**: 2025-01-31T18:12:36Z

**Summary**: The foundation model (FM) paradigm is transforming Machine Learning Force Fields (MLFFs), leveraging general-purpose representations and scalable training to perform a variety of computational chemistry tasks. Although MLFF FMs have begun to close the accuracy gap relative to first-principles methods, there is still a strong need for faster inference speed. Additionally, while research is increasingly focused on general-purpose models which transfer across chemical space, practitioners typically only study a small subset of systems at a given time. This underscores the need for fast, specialized MLFFs relevant to specific downstream applications, which preserve test-time physical soundness while maintaining train-time scalability. In this work, we introduce a method for transferring general-purpose representations from MLFF foundation models to smaller, faster MLFFs specialized to specific regions of chemical space. We formulate our approach as a knowledge distillation procedure, where the smaller "student" MLFF is trained to match the Hessians of the energy predictions of the "teacher" foundation model. Our specialized MLFFs can be up to 20 $\times$ faster than the original foundation model, while retaining, and in some cases exceeding, its performance and that of undistilled models. We also show that distilling from a teacher model with a direct force parameterization into a student model trained with conservative forces (i.e., computed as derivatives of the potential energy) successfully leverages the representations from the large-scale teacher for improved accuracy, while maintaining energy conservation during test-time molecular dynamics simulations. More broadly, our work suggests a new paradigm for MLFF development, in which foundation models are released along with smaller, specialized simulation "engines" for common chemical subsets.

**Link**: [arxiv](http://arxiv.org/abs/2501.09009v2),  [pdf](http://arxiv.org/pdf/2501.09009v2)

**Tags**: physics.chem-ph cond-mat.mtrl-sci cs.LG physics.bio-ph 



### The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating   Reward Hacking
**Authors**: Yuchun Miao, Sen Zhang, Liang Ding, Yuqi Zhang, Lefei Zhang, Dacheng Tao

**Updated**: 2025-01-31T18:10:53Z

**Summary**: This work identifies the Energy Loss Phenomenon in Reinforcement Learning from Human Feedback (RLHF) and its connection to reward hacking. Specifically, energy loss in the final layer of a Large Language Model (LLM) gradually increases during the RL process, with an excessive increase in energy loss characterizing reward hacking. Beyond empirical analysis, we further provide a theoretical foundation by proving that, under mild conditions, the increased energy loss reduces the upper bound of contextual relevance in LLMs, which is a critical aspect of reward hacking as the reduced contextual relevance typically indicates overfitting to reward model-favored patterns in RL. To address this issue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes the increase in energy loss in the LLM's final layer during reward calculation to prevent excessive energy loss, thereby mitigating reward hacking. We theoretically show that EPPO can be conceptually interpreted as an entropy-regularized RL algorithm, which provides deeper insights into its effectiveness. Extensive experiments across various LLMs and tasks demonstrate the commonality of the energy loss phenomenon, as well as the effectiveness of \texttt{EPPO} in mitigating reward hacking and improving RLHF performance.

**Link**: [arxiv](http://arxiv.org/abs/2501.19358v1),  [pdf](http://arxiv.org/pdf/2501.19358v1)

**Tags**: cs.LG 



### SafetyAnalyst: Interpretable, transparent, and steerable safety   moderation for AI behavior
**Authors**: Jing-Jing Li, Valentina Pyatkin, Max Kleiman-Weiner, Liwei Jiang, Nouha Dziri, Anne G. E. Collins, Jana Schaich Borg, Maarten Sap, Yejin Choi, Sydney Levine

**Updated**: 2025-01-31T18:01:12Z

**Summary**: The ideal AI safety moderation system would be both structurally interpretable (so its decisions can be reliably explained) and steerable (to align to safety standards and reflect a community's values), which current systems fall short on. To address this gap, we present SafetyAnalyst, a novel AI safety moderation framework. Given an AI behavior, SafetyAnalyst uses chain-of-thought reasoning to analyze its potential consequences by creating a structured "harm-benefit tree," which enumerates harmful and beneficial actions and effects the AI behavior may lead to, along with likelihood, severity, and immediacy labels that describe potential impact on any stakeholders. SafetyAnalyst then aggregates all harmful and beneficial effects into a harmfulness score using fully interpretable weight parameters, which can be aligned to particular safety preferences. We applied this conceptual framework to develop, test, and release an open-source LLM prompt safety classification system, distilled from 18.5 million harm-benefit features generated by frontier LLMs on 19k prompts. On a comprehensive set of prompt safety benchmarks, we show that SafetyReporter (average F1=0.81) outperforms existing LLM safety moderation systems (average F1$<$0.72) on prompt safety classification, while offering the additional advantages of interpretability, transparency, and steerability.

**Link**: [arxiv](http://arxiv.org/abs/2410.16665v2),  [pdf](http://arxiv.org/pdf/2410.16665v2)

**Tags**: cs.CL cs.CY 



### Characterizing User Behavior: The Interplay Between Mobility Patterns   and Mobile Traffic
**Authors**: Anne Josiane Kouam, Aline Carneiro Viana, Mariano G. Beiró, Leo Ferres, Luca Pappalardo

**Updated**: 2025-01-31T17:52:03Z

**Summary**: Mobile devices have become essential for capturing human activity, and eXtended Data Records (XDRs) offer rich opportunities for detailed user behavior modeling, which is useful for designing personalized digital services. Previous studies have primarily focused on aggregated mobile traffic and mobility analyses, often neglecting individual-level insights. This paper introduces a novel approach that explores the dependency between traffic and mobility behaviors at the user level. By analyzing 13 individual features that encompass traffic patterns and various mobility aspects, we enhance the understanding of how these behaviors interact. Our advanced user modeling framework integrates traffic and mobility behaviors over time, allowing for fine-grained dependencies while maintaining population heterogeneity through user-specific signatures. Furthermore, we develop a Markov model that infers traffic behavior from mobility and vice versa, prioritizing significant dependencies while addressing privacy concerns. Using a week-long XDR dataset from 1,337,719 users across several provinces in Chile, we validate our approach, demonstrating its robustness and applicability in accurately inferring user behavior and matching mobility and traffic profiles across diverse urban contexts.

**Link**: [arxiv](http://arxiv.org/abs/2501.19348v1),  [pdf](http://arxiv.org/pdf/2501.19348v1)

**Tags**: cs.NI cs.IR 



### PUATE: Semiparametric Efficient Average Treatment Effect Estimation from   Treated (Positive) and Unlabeled Units
**Authors**: Masahiro Kato, Fumiaki Kozai, Ryo Inokuchi

**Updated**: 2025-01-31T17:47:32Z

**Summary**: The estimation of average treatment effects (ATEs), defined as the difference in expected outcomes between treatment and control groups, is a central topic in causal inference. This study develops semiparametric efficient estimators for ATE estimation in a setting where only a treatment group and an unknown group-comprising units for which it is unclear whether they received the treatment or control-are observable. This scenario represents a variant of learning from positive and unlabeled data (PU learning) and can be regarded as a special case of ATE estimation with missing data. For this setting, we derive semiparametric efficiency bounds, which provide lower bounds on the asymptotic variance of regular estimators. We then propose semiparametric efficient ATE estimators whose asymptotic variance aligns with these efficiency bounds. Our findings contribute to causal inference with missing data and weakly supervised learning.

**Link**: [arxiv](http://arxiv.org/abs/2501.19345v1),  [pdf](http://arxiv.org/pdf/2501.19345v1)

**Tags**: cs.LG econ.EM math.ST stat.ME stat.ML stat.TH 



### Towards Adaptive Self-Improvement for Smarter Energy Systems
**Authors**: Alexander Sommer, Peter Bazan, Jonathan Fellerer, Behnam Babaeian, Reinhard German

**Updated**: 2025-01-31T17:40:08Z

**Summary**: This paper introduces a hierarchical framework for decision-making and optimization, leveraging Large Language Models (LLMs) for adaptive code generation. Instead of direct decision-making, LLMs generate and refine executable control policies through a meta-policy that guides task generation and a base policy for operational actions. Applied to a simplified microgrid scenario, the approach achieves up to 15 percent cost savings by iteratively improving battery control strategies. The proposed methodology lays a foundation for integrating LLM-based tools into planning and control tasks, offering adaptable and scalable solutions for complex systems while addressing challenges of uncertainty and reproducibility.

**Link**: [arxiv](http://arxiv.org/abs/2501.19340v1),  [pdf](http://arxiv.org/pdf/2501.19340v1)

**Tags**: eess.SY cs.SY 



### FlexiGPT: Pruning and Extending Large Language Models with Low-Rank   Weight Sharing
**Authors**: James Seale Smith, Chi-Heng Lin, Shikhar Tuli, Haris Jeelani, Shangqian Gao, Yilin Shen, Hongxia Jin, Yen-Chang Hsu

**Updated**: 2025-01-31T17:38:07Z

**Summary**: The rapid proliferation of large language models (LLMs) in natural language processing (NLP) has created a critical need for techniques that enable efficient deployment on memory-constrained devices without compromising performance. We present a method to prune LLMs that selectively prunes model blocks based on an importance score and replaces them with a low-parameter replacement strategy. Specifically, we propose a principled metric to replace each pruned block using a weight-sharing mechanism that leverages unpruned counterparts from the model and block-specific low-rank adapters. Furthermore, we facilitate the learning of these replacement blocks with output feature normalization and an adapter initialization scheme built on low-rank SVD reconstructions. Empirical evaluations demonstrate substantial performance gains over existing methods, achieving state-of-the-art performance on 5/6 benchmarks for a compression rate of 30% and 6/6 benchmarks for a compression rate of 40%. We also demonstrate that our approach can extend smaller models, boosting performance on 6/6 benchmarks using only ~0.3% tokens of extended training with minimal additional parameter costs.

**Link**: [arxiv](http://arxiv.org/abs/2501.14713v2),  [pdf](http://arxiv.org/pdf/2501.14713v2)

**Tags**: cs.CL cs.LG 



### Homogeneity Bias as Differential Sampling Uncertainty in Language Models
**Authors**: Messi H. J. Lee, Soyeon Jeon

**Updated**: 2025-01-31T17:36:12Z

**Summary**: Prior research show that Large Language Models (LLMs) and Vision-Language Models (VLMs) represent marginalized groups more homogeneously than dominant groups. However, the mechanisms underlying this homogeneity bias remain relatively unexplored. We propose that this bias emerges from systematic differences in the probability distributions from which tokens are sampled at inference-time. Analyzing three measures of uncertainty in token sampling distributions-entropy, perplexity, and probability of differentiation-we find that in some models, specifically GPT-4 Turbo and Llama-3.2, tokens are sampled more deterministically when generating texts about marginalized groups (i.e., Black Americans and women) compared to their dominant group counterparts (i.e., White Americans and men). While these findings may help explain homogeneity bias in certain models, the patterns did not replicate across all VLMs tested, suggesting multiple mechanisms may contribute to homogeneity bias in AI.

**Link**: [arxiv](http://arxiv.org/abs/2501.19337v1),  [pdf](http://arxiv.org/pdf/2501.19337v1)

**Tags**: cs.CL cs.CV 



### From Natural Language to Extensive-Form Game Representations
**Authors**: Shilong Deng, Yongzhao Wang, Rahul Savani

**Updated**: 2025-01-31T17:26:12Z

**Summary**: We introduce a framework for translating game descriptions in natural language into extensive-form representations in game theory, leveraging Large Language Models (LLMs) and in-context learning. Given the varying levels of strategic complexity in games, such as perfect versus imperfect information, directly applying in-context learning would be insufficient. To address this, we introduce a two-stage framework with specialized modules to enhance in-context learning, enabling it to divide and conquer the problem effectively. In the first stage, we tackle the challenge of imperfect information by developing a module that identifies information sets along and the corresponding partial tree structure. With this information, the second stage leverages in-context learning alongside a self-debugging module to produce a complete extensive-form game tree represented using pygambit, the Python API of a recognized game-theoretic analysis tool called Gambit. Using this python representation enables the automation of tasks such as computing Nash equilibria directly from natural language descriptions. We evaluate the performance of the full framework, as well as its individual components, using various LLMs on games with different levels of strategic complexity. Our experimental results show that the framework significantly outperforms baseline models in generating accurate extensive-form games, with each module playing a critical role in its success.

**Link**: [arxiv](http://arxiv.org/abs/2501.17282v3),  [pdf](http://arxiv.org/pdf/2501.17282v3)

**Tags**: cs.AI cs.CL cs.GT cs.MA 



### Reward-Guided Speculative Decoding for Efficient LLM Reasoning
**Authors**: Baohao Liao, Yuhui Xu, Hanze Dong, Junnan Li, Christof Monz, Silvio Savarese, Doyen Sahoo, Caiming Xiong

**Updated**: 2025-01-31T17:19:57Z

**Summary**: We introduce Reward-Guided Speculative Decoding (RSD), a novel framework aimed at improving the efficiency of inference in large language models (LLMs). RSD synergistically combines a lightweight draft model with a more powerful target model, incorporating a controlled bias to prioritize high-reward outputs, in contrast to existing speculative decoding methods that enforce strict unbiasedness. RSD employs a process reward model to evaluate intermediate decoding steps and dynamically decide whether to invoke the target model, optimizing the trade-off between computational cost and output quality. We theoretically demonstrate that a threshold-based mixture strategy achieves an optimal balance between resource utilization and performance. Extensive evaluations on challenging reasoning benchmarks, including Olympiad-level tasks, show that RSD delivers significant efficiency gains against decoding with the target model only (up to 4.4x fewer FLOPs), while achieving significant better accuracy than parallel decoding method on average (up to +3.5). These results highlight RSD as a robust and cost-effective approach for deploying LLMs in resource-intensive scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2501.19324v1),  [pdf](http://arxiv.org/pdf/2501.19324v1)

**Tags**: cs.CL cs.AI 



### MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented   Reinforcement in Embodied Systems
**Authors**: Anirudh Chari, Suraj Reddy, Aditya Tiwari, Richard Lian, Brian Zhou

**Updated**: 2025-01-31T17:15:33Z

**Summary**: While large language models (LLMs) have shown promising capabilities as zero-shot planners for embodied agents, their inability to learn from experience and build persistent mental models limits their robustness in complex open-world environments like Minecraft. We introduce MINDSTORES, an experience-augmented planning framework that enables embodied agents to build and leverage mental models through natural interaction with their environment. Drawing inspiration from how humans construct and refine cognitive mental models, our approach extends existing zero-shot LLM planning by maintaining a database of past experiences that informs future planning iterations. The key innovation is representing accumulated experiences as natural language embeddings of (state, task, plan, outcome) tuples, which can then be efficiently retrieved and reasoned over by an LLM planner to generate insights and guide plan refinement for novel states and tasks. Through extensive experiments in the MineDojo environment, a simulation environment for agents in Minecraft that provides low-level controls for Minecraft, we find that MINDSTORES learns and applies its knowledge significantly better than existing memory-based LLM planners while maintaining the flexibility and generalization benefits of zero-shot approaches, representing an important step toward more capable embodied AI systems that can learn continuously through natural experience.

**Link**: [arxiv](http://arxiv.org/abs/2501.19318v1),  [pdf](http://arxiv.org/pdf/2501.19318v1)

**Tags**: cs.AI 



### LLM-based Affective Text Generation Quality Based on Different   Quantization Values
**Authors**: Yarik Menchaca Resendiz, Roman Klinger

**Updated**: 2025-01-31T17:12:55Z

**Summary**: Large language models exhibit a remarkable capacity in language generation and comprehension. These advances enable AI systems to produce more human-like and emotionally engaging text. However, these models rely on a large number of parameters, requiring significant computational resources for training and inference. In some scenarios, accessing these resources can be challenging (e.g., budget or hardware limitations). Techniques like reducing precision bits can make models more memory-efficient, reducing the computational resources needed, at the cost of reduced accuracy. This paper addresses the trade-off between different quantization values, GPU RAM utilization, and text quality in affective text generation (e.g., "I really enjoy running in the snow-covered forest"). To evaluate, we use an emotion classifier and ten seed prompts to generate affective text. We test three setups of precision bits (8, 16, and 32) across five open-weight language models from two different families. Our findings demonstrate that bit reductions lead to memory savings, achieving a reduction of 76%. However, this optimization comes with a trade-off, leading to a decrease of up to 10 pp in F1 score for larger models and an increase of 10 pp for smaller models, along with roughly double the inference time. In terms of text quality, larger models at lower quantization levels generally outperform smaller, higher-precision models -- while requiring similar memory.

**Link**: [arxiv](http://arxiv.org/abs/2501.19317v1),  [pdf](http://arxiv.org/pdf/2501.19317v1)

**Tags**: cs.CL 



### Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model   Alignment
**Authors**: Gregor Bachmann, Sotiris Anagnostidis, Albert Pumarola, Markos Georgopoulos, Artsiom Sanakoyeu, Yuming Du, Edgar Schönfeld, Ali Thabet, Jonas Kohler

**Updated**: 2025-01-31T17:09:53Z

**Summary**: The performance of large language models (LLMs) is closely linked to their underlying size, leading to ever-growing networks and hence slower inference. Speculative decoding has been proposed as a technique to accelerate autoregressive generation, leveraging a fast draft model to propose candidate tokens, which are then verified in parallel based on their likelihood under the target model. While this approach guarantees to reproduce the target output, it incurs a substantial penalty: many high-quality draft tokens are rejected, even when they represent objectively valid continuations. Indeed, we show that even powerful draft models such as GPT-4o, as well as human text cannot achieve high acceptance rates under the standard verification scheme. This severely limits the speedup potential of current speculative decoding methods, as an early rejection becomes overwhelmingly likely when solely relying on alignment of draft and target.   We thus ask the following question: Can we adapt verification to recognize correct, but non-aligned replies? To this end, we draw inspiration from the LLM-as-a-judge framework, which demonstrated that LLMs are able to rate answers in a versatile way. We carefully design a dataset to elicit the same capability in the target model by training a compact module on top of the embeddings to produce ``judgements" of the current continuation. We showcase our strategy on the Llama-3.1 family, where our 8b/405B-Judge achieves a speedup of 9x over Llama-405B, while maintaining its quality on a large range of benchmarks. These benefits remain present even in optimized inference frameworks, where our method reaches up to 141 tokens/s for 8B/70B-Judge and 129 tokens/s for 8B/405B on 2 and 8 H100s respectively.

**Link**: [arxiv](http://arxiv.org/abs/2501.19309v1),  [pdf](http://arxiv.org/pdf/2501.19309v1)

**Tags**: cs.LG cs.CL 



### SETS: Leveraging Self-Verification and Self-Correction for Improved   Test-Time Scaling
**Authors**: Jiefeng Chen, Jie Ren, Xinyun Chen, Chengrun Yang, Ruoxi Sun, Sercan Ö Arık

**Updated**: 2025-01-31T17:03:16Z

**Summary**: Recent advancements in Large Language Models (LLMs) have created new opportunities to enhance performance on complex reasoning tasks by leveraging test-time computation. However, conventional approaches such as repeated sampling with majority voting or reward model scoring, often face diminishing returns as test-time compute scales, in addition to requiring costly task-specific reward model training. In this paper, we present Self-Enhanced Test-Time Scaling (SETS), a novel method that leverages the self-verification and self-correction capabilities of recent advanced LLMs to overcome these limitations. SETS integrates sampling, self-verification, and self-correction into a unified framework, enabling efficient and scalable test-time computation for improved capabilities at complex tasks. Through extensive experiments on challenging planning and reasoning benchmarks, compared to the alternatives, we demonstrate that SETS achieves significant performance improvements and more favorable test-time scaling laws.

**Link**: [arxiv](http://arxiv.org/abs/2501.19306v1),  [pdf](http://arxiv.org/pdf/2501.19306v1)

**Tags**: cs.AI cs.CL 



### Beyond checkmate: exploring the creative chokepoints in AI text
**Authors**: Nafis Irtiza Tripto, Saranya Venkatraman, Mahjabin Nahar, Dongwon Lee

**Updated**: 2025-01-31T16:57:01Z

**Summary**: Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) and Artificial Intelligence (AI), unlocking unprecedented capabilities. This rapid advancement has spurred research into various aspects of LLMs, their text generation & reasoning capability, and potential misuse, fueling the necessity for robust detection methods. While numerous prior research has focused on detecting LLM-generated text (AI text) and thus checkmating them, our study investigates a relatively unexplored territory: portraying the nuanced distinctions between human and AI texts across text segments. Whether LLMs struggle with or excel at incorporating linguistic ingenuity across different text segments carries substantial implications for determining their potential as effective creative assistants to humans. Through an analogy with the structure of chess games-comprising opening, middle, and end games-we analyze text segments (introduction, body, and conclusion) to determine where the most significant distinctions between human and AI texts exist. While AI texts can approximate the body segment better due to its increased length, a closer examination reveals a pronounced disparity, highlighting the importance of this segment in AI text detection. Additionally, human texts exhibit higher cross-segment differences compared to AI texts. Overall, our research can shed light on the intricacies of human-AI text distinctions, offering novel insights for text detection and understanding.

**Link**: [arxiv](http://arxiv.org/abs/2501.19301v1),  [pdf](http://arxiv.org/pdf/2501.19301v1)

**Tags**: cs.CL cs.AI 



### Offline Learning for Combinatorial Multi-armed Bandits
**Authors**: Xutong Liu, Xiangxiang Dai, Jinhang Zuo, Siwei Wang, Carlee-Joe Wong, John C. S. Lui, Wei Chen

**Updated**: 2025-01-31T16:56:18Z

**Summary**: The combinatorial multi-armed bandit (CMAB) is a fundamental sequential decision-making framework, extensively studied over the past decade. However, existing work primarily focuses on the online setting, overlooking the substantial costs of online interactions and the readily available offline datasets. To overcome these limitations, we introduce Off-CMAB, the first offline learning framework for CMAB. Central to our framework is the combinatorial lower confidence bound (CLCB) algorithm, which combines pessimistic reward estimations with combinatorial solvers. To characterize the quality of offline datasets, we propose two novel data coverage conditions and prove that, under these conditions, CLCB achieves a near-optimal suboptimality gap, matching the theoretical lower bound up to a logarithmic factor. We validate Off-CMAB through practical applications, including learning to rank, large language model (LLM) caching, and social influence maximization, showing its ability to handle nonlinear reward functions, general feedback models, and out-of-distribution action samples that excludes optimal or even feasible actions. Extensive experiments on synthetic and real-world datasets further highlight the superior performance of CLCB.

**Link**: [arxiv](http://arxiv.org/abs/2501.19300v1),  [pdf](http://arxiv.org/pdf/2501.19300v1)

**Tags**: cs.LG 



### Synthetic User Behavior Sequence Generation with Large Language Models   for Smart Homes
**Authors**: Zhiyao Xu, Dan Zhao, Qingsong Zou, Jingyu Xiao, Yong Jiang, Zhenhui Yuan, Qing Li

**Updated**: 2025-01-31T16:55:43Z

**Summary**: In recent years, as smart home systems have become more widespread, security concerns within these environments have become a growing threat. Currently, most smart home security solutions, such as anomaly detection and behavior prediction models, are trained using fixed datasets that are precollected. However, the process of dataset collection is time-consuming and lacks the flexibility needed to adapt to the constantly evolving smart home environment. Additionally, the collection of personal data raises significant privacy concerns for users. Lately, large language models (LLMs) have emerged as a powerful tool for a wide range of tasks across diverse application domains, thanks to their strong capabilities in natural language processing, reasoning, and problem-solving. In this paper, we propose an LLM-based synthetic dataset generation IoTGen framework to enhance the generalization of downstream smart home intelligent models. By generating new synthetic datasets that reflect changes in the environment, smart home intelligent models can be retrained to overcome the limitations of fixed and outdated data, allowing them to better align with the dynamic nature of real-world home environments. Specifically, we first propose a Structure Pattern Perception Compression (SPPC) method tailored for IoT behavior data, which preserves the most informative content in the data while significantly reducing token consumption. Then, we propose a systematic approach to create prompts and implement data generation to automatically generate IoT synthetic data with normative and reasonable properties, assisting task models in adaptive training to improve generalization and real-world performance.

**Link**: [arxiv](http://arxiv.org/abs/2501.19298v1),  [pdf](http://arxiv.org/pdf/2501.19298v1)

**Tags**: cs.AI cs.LG cs.NI 



### Analysis of LLMs vs Human Experts in Requirements Engineering
**Authors**: Cory Hymel, Hiroe Johnson

**Updated**: 2025-01-31T16:55:17Z

**Summary**: The majority of research around Large Language Models (LLM) application to software development has been on the subject of code generation. There is little literature on LLMs' impact on requirements engineering (RE), which deals with the process of developing and verifying the system requirements. Within RE, there is a subdiscipline of requirements elicitation, which is the practice of discovering and documenting requirements for a system from users, customers, and other stakeholders. In this analysis, we compare LLM's ability to elicit requirements of a software system, as compared to that of a human expert in a time-boxed and prompt-boxed study. We found LLM-generated requirements were evaluated as more aligned (+1.12) than human-generated requirements with a trend of being more complete (+10.2%). Conversely, we found users tended to believe that solutions they perceived as more aligned had been generated by human experts. Furthermore, while LLM-generated documents scored higher and performed at 720x the speed, their cost was, on average, only 0.06% that of a human expert. Overall, these findings indicate that LLMs will play an increasingly important role in requirements engineering by improving requirements definitions, enabling more efficient resource allocation, and reducing overall project timelines.

**Link**: [arxiv](http://arxiv.org/abs/2501.19297v1),  [pdf](http://arxiv.org/pdf/2501.19297v1)

**Tags**: cs.SE cs.AI 



### Differentially Private In-context Learning via Sampling Few-shot Mixed   with Zero-shot Outputs
**Authors**: James Flemings, Haosheng Gan, Hongyi Li, Meisam Razaviyayn, Murali Annavaram

**Updated**: 2025-01-31T16:48:38Z

**Summary**: In-context learning (ICL) has shown promising improvement in downstream task adaptation of LLMs by augmenting prompts with relevant input-output examples (demonstrations). However, the ICL demonstrations can contain privacy-sensitive information, which can be leaked and/or regurgitated by the LLM output. Differential Privacy (DP), a widely adopted privacy safeguard, has emerged to mitigate this privacy leakage, with recent work demonstrating strong privacy-utility tradeoffs in classification tasks for ICL. However, generation tasks for ICL are challenging due to the high-dimensional output space of open-ended generation. To this end, we propose $\texttt{dps-mozo}$, Differentially Private Sampling by Mixing One-shot with Zero-shot Outputs, a decoding framework that generates DP text by sampling from the product of multiple one-shot outputs mixed with a zero-shot output. This mixing effectively reduces the amount of information that can be leaked by each demonstration. By utilizing the inherent randomness in sampling from the mixed distributions, we can achieve DP without adding noise, thereby improving the privacy-utility tradeoff. Our experimental evaluations show $\texttt{dps-mozo}$ can achieve a strong privacy guarantee, $\epsilon=2$, with minimal utility degradation compared to non-private few-shot learning, $\textbf{0.3}$% ROUGE-L F1 score decrease on the SAMSum dataset with Gemma 2 2B.

**Link**: [arxiv](http://arxiv.org/abs/2501.19287v1),  [pdf](http://arxiv.org/pdf/2501.19287v1)

**Tags**: cs.LG 



### UniGuard: Towards Universal Safety Guardrails for Jailbreak Attacks on   Multimodal Large Language Models
**Authors**: Sejoon Oh, Yiqiao Jin, Megha Sharma, Donghyun Kim, Eric Ma, Gaurav Verma, Srijan Kumar

**Updated**: 2025-01-31T16:47:16Z

**Summary**: Multimodal large language models (MLLMs) have revolutionized vision-language understanding but remain vulnerable to multimodal jailbreak attacks, where adversarial inputs are meticulously crafted to elicit harmful or inappropriate responses. We propose UniGuard, a novel multimodal safety guardrail that jointly considers the unimodal and cross-modal harmful signals. UniGuard trains a multimodal guardrail to minimize the likelihood of generating harmful responses in a toxic corpus. The guardrail can be seamlessly applied to any input prompt during inference with minimal computational costs. Extensive experiments demonstrate the generalizability of UniGuard across multiple modalities, attack strategies, and multiple state-of-the-art MLLMs, including LLaVA, Gemini Pro, GPT-4o, MiniGPT-4, and InstructBLIP. Notably, this robust defense mechanism maintains the models' overall vision-language understanding capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2411.01703v2),  [pdf](http://arxiv.org/pdf/2411.01703v2)

**Tags**: cs.CL cs.AI cs.LG 



### Low-Cost and Comprehensive Non-textual Input Fuzzing with   LLM-Synthesized Input Generators
**Authors**: Kunpeng Zhang, Zongjie Li, Daoyuan Wu, Shuai Wang, Xin Xia

**Updated**: 2025-01-31T16:45:16Z

**Summary**: Modern software often accepts inputs with highly complex grammars. Recent advances in large language models (LLMs) have shown that they can be used to synthesize high-quality natural language text and code that conforms to the grammar of a given input format. Nevertheless, LLMs are often incapable or too costly to generate non-textual outputs, such as images, videos, and PDF files. This limitation hinders the application of LLMs in grammar-aware fuzzing.   We present a novel approach to enabling grammar-aware fuzzing over non-textual inputs. We employ LLMs to synthesize and also mutate input generators, in the form of Python scripts, that generate data conforming to the grammar of a given input format. Then, non-textual data yielded by the input generators are further mutated by traditional fuzzers (AFL++) to explore the software input space effectively. Our approach, namely G2FUZZ, features a hybrid strategy that combines a holistic search driven by LLMs and a local search driven by industrial quality fuzzers. Two key advantages are: (1) LLMs are good at synthesizing and mutating input generators and enabling jumping out of local optima, thus achieving a synergistic effect when combined with mutation-based fuzzers; (2) LLMs are less frequently invoked unless really needed, thus significantly reducing the cost of LLM usage. We have evaluated G2FUZZ on a variety of input formats, including TIFF images, MP4 audios, and PDF files. The results show that G2FUZZ outperforms SOTA tools such as AFL++, Fuzztruction, and FormatFuzzer in terms of code coverage and bug finding across most programs tested on three platforms: UNIFUZZ, FuzzBench, and MAGMA.

**Link**: [arxiv](http://arxiv.org/abs/2501.19282v1),  [pdf](http://arxiv.org/pdf/2501.19282v1)

**Tags**: cs.SE 



### Pheromone-based Learning of Optimal Reasoning Paths
**Authors**: Anirudh Chari, Aditya Tiwari, Richard Lian, Suraj Reddy, Brian Zhou

**Updated**: 2025-01-31T16:42:31Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities through chain-of-thought prompting, yet discovering effective reasoning methods for complex problems remains challenging due to the vast space of possible intermediate steps. We introduce Ant Colony Optimization-guided Tree of Thought (ACO-ToT), a novel algorithm that combines ACO with LLMs to discover optimal reasoning paths for complex problems efficiently. Drawing inspiration from Hebbian learning in neurological systems, our method employs a collection of distinctly fine-tuned LLM "ants" to traverse and lay pheromone trails through a centralized tree of thought, with each ant's movement governed by a weighted combination of existing pheromone trails and its own specialized expertise. The algorithm evaluates complete reasoning paths using a mixture-of-experts-based scoring function, with pheromones reinforcing productive reasoning paths across iterations. Experiments on three challenging reasoning tasks (GSM8K, ARC-Challenge, and MATH) demonstrate that ACO-ToT performs significantly better than existing chain-of-thought optimization approaches, suggesting that incorporating biologically inspired collective search mechanisms into LLM inference can substantially enhance reasoning capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2501.19278v1),  [pdf](http://arxiv.org/pdf/2501.19278v1)

**Tags**: cs.CL 



### SuiGPT MAD: Move AI Decompiler to Improve Transparency and Auditability   on Non-Open-Source Blockchain Smart Contract
**Authors**: Eason Chen, Xinyi Tang, Zimo Xiao, Chuangji Li, Shizhuo Li, Wu Tingguan, Siyun Wang, Kostas Kryptos Chalkias

**Updated**: 2025-01-31T16:31:56Z

**Summary**: The vision of Web3 is to improve user control over data and assets, but one challenge that complicates this vision is the prevalence of non-transparent, scam-prone applications and vulnerable smart contracts that put Web3 users at risk. While code audits are one solution to this problem, the lack of smart contracts source code on many blockchain platforms, such as Sui, hinders the ease of auditing. A promising approach to this issue is the use of a decompiler to reverse-engineer smart contract bytecode. However, existing decompilers for Sui produce code that is difficult to understand and cannot be directly recompiled. To address this, we developed the SuiGPT Move AI Decompiler (MAD), a Large Language Model (LLM)-powered web application that decompiles smart contract bytecodes on Sui into logically correct, human-readable, and re-compilable source code with prompt engineering.   Our evaluation shows that MAD's output successfully passes original unit tests and achieves a 73.33% recompilation success rate on real-world smart contracts. Additionally, newer models tend to deliver improved performance, suggesting that MAD's approach will become increasingly effective as LLMs continue to advance.   In a user study involving 12 developers, we found that MAD significantly reduced the auditing workload compared to using traditional decompilers. Participants found MAD's outputs comparable to the original source code, improving accessibility for understanding and auditing non-open-source smart contracts. Through qualitative interviews with these developers and Web3 projects, we further discussed the strengths and concerns of MAD.   MAD has practical implications for blockchain smart contract transparency, auditing, and education. It empowers users to easily and independently review and audit non-open-source smart contracts, fostering accountability and decentralization

**Link**: [arxiv](http://arxiv.org/abs/2410.15275v2),  [pdf](http://arxiv.org/pdf/2410.15275v2)

**Tags**: cs.HC cs.SE 



### Imagine with the Teacher: Complete Shape in a Multi-View Distillation   Way
**Authors**: Zhanpeng Luo, Linna Wang, Guangwu Qian, Li Lu

**Updated**: 2025-01-31T16:29:19Z

**Summary**: Point cloud completion aims to recover the completed 3D shape of an object from its partial observation caused by occlusion, sensor's limitation, noise, etc. When some key semantic information is lost in the incomplete point cloud, the neural network needs to infer the missing part based on the input information. Intuitively we would apply an autoencoder architecture to solve this kind of problem, which take the incomplete point cloud as input and is supervised by the ground truth. This process that develops model's imagination from incomplete shape to complete shape is done automatically in the latent space. But the knowledge for mapping from incomplete to complete still remains dark and could be further explored. Motivated by the knowledge distillation's teacher-student learning strategy, we design a knowledge transfer way for completing 3d shape. In this work, we propose a novel View Distillation Point Completion Network (VD-PCN), which solve the completion problem by a multi-view distillation way. The design methodology fully leverages the orderliness of 2d pixels, flexibleness of 2d processing and powerfulness of 2d network. Extensive evaluations on PCN, ShapeNet55/34, and MVP datasets confirm the effectiveness of our design and knowledge transfer strategy, both quantitatively and qualitatively. Committed to facilitate ongoing research, we will make our code publicly available.

**Link**: [arxiv](http://arxiv.org/abs/2501.19270v1),  [pdf](http://arxiv.org/pdf/2501.19270v1)

**Tags**: cs.CV 



### Information Metrics and Possible Limitations of Local Information   Objectivity in Quantum Gravity
**Authors**: Per Berglund, Andrew Geraci, Tristan Hubsch, David Mattingly, Djordje Minic

**Updated**: 2025-01-31T16:29:16Z

**Summary**: Local information objectivity, that local observers can infer the same information about a model upon exchange of experimental data, is fundamental to science. It is mathematically encoded via Cencov's theorem: the Fisher information metric is the unique metric invariant under sufficient statistics. However, quantum gravity typically violates some of the Cencov assumptions, allowing the Fisher metric and Born rule to vary between observers. We explain these violations, some possible experimental tests, and a new quantum gravity approach based on generally covariant information geometry.

**Link**: [arxiv](http://arxiv.org/abs/2501.19269v1),  [pdf](http://arxiv.org/pdf/2501.19269v1)

**Tags**: gr-qc hep-th quant-ph 



### Jackpot! Alignment as a Maximal Lottery
**Authors**: Roberto-Rafael Maura-Rivero, Marc Lanctot, Francesco Visin, Kate Larson

**Updated**: 2025-01-31T16:26:28Z

**Summary**: Reinforcement Learning from Human Feedback (RLHF), the standard for aligning Large Language Models (LLMs) with human values, is known to fail to satisfy properties that are intuitively desirable, such as respecting the preferences of the majority \cite{ge2024axioms}. To overcome these issues, we propose the use of a probabilistic Social Choice rule called \emph{maximal lotteries} as a replacement for RLHF. We show that a family of alignment techniques, namely Nash Learning from Human Feedback (NLHF) \cite{munos2023nash} and variants, approximate maximal lottery outcomes and thus inherit its beneficial properties.   We confirm experimentally that our proposed methodology handles situations that arise when working with preferences more robustly than standard RLHF, including supporting the preferences of the majority, providing principled ways of handling non-transitivities in the preference data, and robustness to irrelevant alternatives. This results in systems that better incorporate human values and respect human intentions.

**Link**: [arxiv](http://arxiv.org/abs/2501.19266v1),  [pdf](http://arxiv.org/pdf/2501.19266v1)

**Tags**: cs.AI cs.LG econ.TH 



### $α$-OCC: Uncertainty-Aware Camera-based 3D Semantic Occupancy   Prediction
**Authors**: Sanbao Su, Nuo Chen, Chenchen Lin, Felix Juefei-Xu, Chen Feng, Fei Miao

**Updated**: 2025-01-31T16:18:56Z

**Summary**: In the realm of autonomous vehicle perception, comprehending 3D scenes is paramount for tasks such as planning and mapping. Camera-based 3D Semantic Occupancy Prediction (OCC) aims to infer scene geometry and semantics from limited observations. While it has gained popularity due to affordability and rich visual cues, existing methods often neglect the inherent uncertainty in models. To address this, we propose an uncertainty-aware OCC method ($\alpha$-OCC). We first introduce Depth-UP, an uncertainty propagation framework that improves geometry completion by up to 11.58\% and semantic segmentation by up to 12.95\% across various OCC models. For uncertainty quantification (UQ), we propose the hierarchical conformal prediction (HCP) method, effectively handling the high-level class imbalance in OCC datasets. On the geometry level, the novel KL-based score function significantly improves the occupied recall (45\%) of safety-critical classes with minimal performance overhead (3.4\% reduction). On UQ, our HCP achieves smaller prediction set sizes while maintaining the defined coverage guarantee. Compared with baselines, it reduces up to 92\% set size, with 18\% further reduction when integrated with Depth-UP. Our contributions advance OCC accuracy and robustness, marking a noteworthy step forward in autonomous perception systems.

**Link**: [arxiv](http://arxiv.org/abs/2406.11021v4),  [pdf](http://arxiv.org/pdf/2406.11021v4)

**Tags**: cs.CV 



### Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for   Autonomous Drone FlighT at the Edge
**Authors**: Amogh Joshi, Sourav Sanyal, Kaushik Roy

**Updated**: 2025-01-31T16:17:03Z

**Summary**: The integration of human-intuitive interactions into autonomous systems has been limited. Traditional Natural Language Processing (NLP) systems struggle with context and intent understanding, severely restricting human-robot interaction. Recent advancements in Large Language Models (LLMs) have transformed this dynamic, allowing for intuitive and high-level communication through speech and text, and bridging the gap between human commands and robotic actions. Additionally, autonomous navigation has emerged as a central focus in robotics research, with artificial intelligence (AI) increasingly being leveraged to enhance these systems. However, existing AI-based navigation algorithms face significant challenges in latency-critical tasks where rapid decision-making is critical. Traditional frame-based vision systems, while effective for high-level decision-making, suffer from high energy consumption and latency, limiting their applicability in real-time scenarios. Neuromorphic vision systems, combining event-based cameras and spiking neural networks (SNNs), offer a promising alternative by enabling energy-efficient, low-latency navigation. Despite their potential, real-world implementations of these systems, particularly on physical platforms such as drones, remain scarce. In this work, we present Neuro-LIFT, a real-time neuromorphic navigation framework implemented on a Parrot Bebop2 quadrotor. Leveraging an LLM for natural language processing, Neuro-LIFT translates human speech into high-level planning commands which are then autonomously executed using event-based neuromorphic vision and physics-driven planning. Our framework demonstrates its capabilities in navigating in a dynamic environment, avoiding obstacles, and adapting to human instructions in real-time.

**Link**: [arxiv](http://arxiv.org/abs/2501.19259v1),  [pdf](http://arxiv.org/pdf/2501.19259v1)

**Tags**: cs.RO cs.CV cs.LG cs.NE cs.SY eess.SY 



### Are They the Same? Exploring Visual Correspondence Shortcomings of   Multimodal LLMs
**Authors**: Yikang Zhou, Tao Zhang, Shilin Xu, Shihao Chen, Qianyu Zhou, Yunhai Tong, Shunping Ji, Jiangning Zhang, Xiangtai Li, Lu Qi

**Updated**: 2025-01-31T16:12:22Z

**Summary**: Recent advancements in multimodal models have shown a strong ability in visual perception, reasoning abilities, and vision-language understanding. However, studies on visual matching ability are missing, where finding the visual correspondence of objects is essential in vision research. Our research reveals that the matching capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings, even with current strong MLLMs models, GPT-4o. In particular, we construct a Multimodal Visual Matching (MMVM) benchmark to fairly benchmark over 30 different MLLMs. The MMVM benchmark is built from 15 open-source datasets and Internet videos with manual annotation. We categorize the data samples of MMVM benchmark into eight aspects based on the required cues and capabilities to more comprehensively evaluate and analyze current MLLMs. In addition, we have designed an automatic annotation pipeline to generate the MMVM SFT dataset, including 220K visual matching data with reasoning annotation. Finally, we present CoLVA, a novel contrastive MLLM with two novel technical designs: fine-grained vision expert with object-level contrastive learning and instruction augmentation strategy. CoLVA achieves 51.06\% overall accuracy (OA) on the MMVM benchmark, surpassing GPT-4o and baseline by 8.41\% and 23.58\% OA, respectively. The results show the effectiveness of our MMVM SFT dataset and our novel technical designs. Code, benchmark, dataset, and models are available at https://github.com/zhouyiks/CoLVA.

**Link**: [arxiv](http://arxiv.org/abs/2501.04670v2),  [pdf](http://arxiv.org/pdf/2501.04670v2)

**Tags**: cs.CV 



### Inference-Time Text-to-Video Alignment with Diffusion Latent Beam Search
**Authors**: Yuta Oshima, Masahiro Suzuki, Yutaka Matsuo, Hiroki Furuta

**Updated**: 2025-01-31T16:09:30Z

**Summary**: The remarkable progress in text-to-video diffusion models enables photorealistic generations, although the contents of the generated video often include unnatural movement or deformation, reverse playback, and motionless scenes. Recently, an alignment problem has attracted huge attention, where we steer the output of diffusion models based on some quantity on the goodness of the content. Because there is a large room for improvement of perceptual quality along the frame direction, we should address which metrics we should optimize and how we can optimize them in the video generation. In this paper, we propose diffusion latent beam search with lookahead estimator, which can select better diffusion latent to maximize a given alignment reward, at inference time. We then point out that the improvement of perceptual video quality considering the alignment to prompts requires reward calibration by weighting existing metrics. When evaluating outputs by using vision language models as a proxy of humans, many previous metrics to quantify the naturalness of video do not always correlate with evaluation and also depend on the degree of dynamic descriptions in evaluation prompts. We demonstrate that our method improves the perceptual quality based on the calibrated reward, without model parameter update, and outputs the best generation compared to greedy search and best-of-N sampling. We provide practical guidelines on which axes, among search budget, lookahead steps for reward estimate, and denoising steps, in the reverse diffusion process, we should allocate the inference-time computation.

**Link**: [arxiv](http://arxiv.org/abs/2501.19252v1),  [pdf](http://arxiv.org/pdf/2501.19252v1)

**Tags**: cs.CV 



### Can LLMs Separate Instructions From Data? And What Do We Even Mean By   That?
**Authors**: Egor Zverev, Sahar Abdelnabi, Soroush Tabesh, Mario Fritz, Christoph H. Lampert

**Updated**: 2025-01-31T16:06:52Z

**Summary**: Instruction-tuned Large Language Models (LLMs) show impressive results in numerous practical applications, but they lack essential safety features that are common in other areas of computer science, particularly an explicit separation of instructions and data. This makes them vulnerable to manipulations such as indirect prompt injections and generally unsuitable for safety-critical tasks. Surprisingly, there is currently no established definition or benchmark to quantify this phenomenon. In this work, we close this gap by introducing a formal measure for instruction-data separation and an empirical variant that is calculable from a model's outputs. We also present a new dataset, SEP, that allows estimating the measure for real-world models. Our results on various LLMs show that the problem of instruction-data separation is real: all models fail to achieve high separation, and canonical mitigation techniques, such as prompt engineering and fine-tuning, either fail to substantially improve separation or reduce model utility. The source code and SEP dataset are openly accessible at https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed.

**Link**: [arxiv](http://arxiv.org/abs/2403.06833v3),  [pdf](http://arxiv.org/pdf/2403.06833v3)

**Tags**: cs.LG cs.CL 



### Think Smarter not Harder: Adaptive Reasoning with Inference Aware   Optimization
**Authors**: Zishun Yu, Tengyu Xu, Di Jin, Karthik Abinav Sankararaman, Yun He, Wenxuan Zhou, Zhouhao Zeng, Eryk Helenowski, Chen Zhu, Sinong Wang, Hao Ma, Han Fang

**Updated**: 2025-01-31T16:06:26Z

**Summary**: Solving mathematics problems has been an intriguing capability of large language models, and many efforts have been made to improve reasoning by extending reasoning length, such as through self-correction and extensive long chain-of-thoughts. While promising in problem-solving, advanced long reasoning chain models exhibit an undesired single-modal behavior, where trivial questions require unnecessarily tedious long chains of thought. In this work, we propose a way to allow models to be aware of inference budgets by formulating it as utility maximization with respect to an inference budget constraint, hence naming our algorithm Inference Budget-Constrained Policy Optimization (IBPO). In a nutshell, models fine-tuned through IBPO learn to ``understand'' the difficulty of queries and allocate inference budgets to harder ones. With different inference budgets, our best models are able to have a $4.14$\% and $5.74$\% absolute improvement ($8.08$\% and $11.2$\% relative improvement) on MATH500 using $2.16$x and $4.32$x inference budgets respectively, relative to LLaMA3.1 8B Instruct. These improvements are approximately $2$x those of self-consistency under the same budgets.

**Link**: [arxiv](http://arxiv.org/abs/2501.17974v2),  [pdf](http://arxiv.org/pdf/2501.17974v2)

**Tags**: cs.AI 



### DGP-LVM: Derivative Gaussian process latent variable models
**Authors**: Soham Mukherjee, Manfred Claassen, Paul-Christian Bürkner

**Updated**: 2025-01-31T16:01:37Z

**Summary**: We develop a framework for derivative Gaussian process latent variable models (DGP-LVMs) that can handle multi-dimensional output data using modified derivative covariance functions. The modifications account for complexities in the underlying data generating process such as scaled derivatives, varying information across multiple output dimensions as well as interactions between outputs. Further, our framework provides uncertainty estimates for each latent variable samples using Bayesian inference. Through extensive simulations, we demonstrate that latent variable estimation accuracy can be drastically increased by including derivative information due to our proposed covariance function modifications. The developments are motivated by a concrete biological research problem involving the estimation of the unobserved cellular ordering from single-cell RNA (scRNA) sequencing data for gene expression and its corresponding derivative information known as RNA velocity. Since the RNA velocity is only an estimate of the exact derivative information, the derivative covariance functions need to account for potential scale differences. In a real-world case study, we illustrate the application of DGP-LVMs to such scRNA sequencing data. While motivated by this biological problem, our framework is generally applicable to all kinds of latent variable estimation problems involving derivative information irrespective of the field of study.

**Link**: [arxiv](http://arxiv.org/abs/2404.04074v3),  [pdf](http://arxiv.org/pdf/2404.04074v3)

**Tags**: stat.ME 



### Multi-agent Multi-armed Bandit with Fully Heavy-tailed Dynamics
**Authors**: Xingyu Wang, Mengfan Xu

**Updated**: 2025-01-31T15:53:14Z

**Summary**: We study decentralized multi-agent multi-armed bandits in fully heavy-tailed settings, where clients communicate over sparse random graphs with heavy-tailed degree distributions and observe heavy-tailed (homogeneous or heterogeneous) reward distributions with potentially infinite variance. The objective is to maximize system performance by pulling the globally optimal arm with the highest global reward mean across all clients. We are the first to address such fully heavy-tailed scenarios, which capture the dynamics and challenges in communication and inference among multiple clients in real-world systems. In homogeneous settings, our algorithmic framework exploits hub-like structures unique to heavy-tailed graphs, allowing clients to aggregate rewards and reduce noises via hub estimators when constructing UCB indices; under $M$ clients and degree distributions with power-law index $\alpha > 1$, our algorithm attains a regret bound (almost) of order $O(M^{1 -\frac{1}{\alpha}} \log{T})$. Under heterogeneous rewards, clients synchronize by communicating with neighbors, aggregating exchanged estimators in UCB indices; With our newly established information delay bounds on sparse random graphs, we prove a regret bound of $O(M \log{T})$. Our results improve upon existing work, which only address time-invariant connected graphs, or light-tailed dynamics in dense graphs and rewards.

**Link**: [arxiv](http://arxiv.org/abs/2501.19239v1),  [pdf](http://arxiv.org/pdf/2501.19239v1)

**Tags**: cs.LG stat.ML 



### A Zero-Shot Generalization Framework for LLM-Driven Cross-Domain   Sequential Recommendation
**Authors**: Yunzhe Li, Junting Wang, Hari Sundaram, Zhining Liu

**Updated**: 2025-01-31T15:43:21Z

**Summary**: Zero-shot cross-domain sequential recommendation (ZCDSR) enables predictions in unseen domains without the need for additional training or fine-tuning, making it particularly valuable in data-sparse environments where traditional models struggle. Recent advancements in large language models (LLMs) have greatly improved ZCDSR by leveraging rich pretrained representations to facilitate cross-domain knowledge transfer. However, a key challenge persists: domain semantic bias, which arises from variations in vocabulary and content focus across domains. This misalignment leads to inconsistencies in item embeddings and hinders generalization.   To address this issue, we propose a novel framework designed to enhance LLM-based ZCDSR by improving cross-domain alignment at both the item and sequential levels. At the item level, we introduce a generalization loss that promotes inter-domain compactness by aligning embeddings of similar items across domains while maintaining intra-domain diversity to preserve unique item characteristics. This prevents embeddings from becoming overly generic while ensuring effective transferability. At the sequential level, we develop a method for transferring user behavioral patterns by clustering user sequences in the source domain and applying attention-based aggregation for target domain inference. This dynamic adaptation of user embeddings allows effective zero-shot recommendations without requiring target-domain interactions.   Comprehensive experiments across multiple datasets and domains demonstrate that our framework significantly improves sequential recommendation performance in the ZCDSR setting. By mitigating domain bias and enhancing the transferability of sequential patterns, our method provides a scalable and robust approach for achieving more effective zero-shot recommendations across domains.

**Link**: [arxiv](http://arxiv.org/abs/2501.19232v1),  [pdf](http://arxiv.org/pdf/2501.19232v1)

**Tags**: cs.IR cs.AI 



### o3-mini vs DeepSeek-R1: Which One is Safer?
**Authors**: Aitor Arrieta, Miriam Ugarte, Pablo Valle, José Antonio Parejo, Sergio Segura

**Updated**: 2025-01-31T15:39:00Z

**Summary**: The irruption of DeepSeek-R1 constitutes a turning point for the AI industry in general and the LLMs in particular. Its capabilities have demonstrated outstanding performance in several tasks, including creative thinking, code generation, maths and automated program repair, at apparently lower execution cost. However, LLMs must adhere to an important qualitative property, i.e., their alignment with safety and human values. A clear competitor of DeepSeek-R1 is its American counterpart, OpenAI's o3-mini model, which is expected to set high standards in terms of performance, safety and cost. In this technical report, we systematically assess the safety level of both DeepSeek-R1 (70b version) and OpenAI's o3-mini (beta version). To this end, we make use of our recently released automated safety testing tool, named ASTRAL. By leveraging this tool, we automatically and systematically generated and executed 1,260 test inputs on both models. After conducting a semi-automated assessment of the outcomes provided by both LLMs, the results indicate that DeepSeek-R1 produces significantly more unsafe responses (12%) than OpenAI's o3-mini (1.2%).

**Link**: [arxiv](http://arxiv.org/abs/2501.18438v2),  [pdf](http://arxiv.org/pdf/2501.18438v2)

**Tags**: cs.SE cs.AI 



### Through the Looking Glass: LLM-Based Analysis of AR/VR Android   Applications Privacy Policies
**Authors**: Abdulaziz Alghamdi, David Mohaisen

**Updated**: 2025-01-31T15:30:14Z

**Summary**: \begin{abstract} This paper comprehensively analyzes privacy policies in AR/VR applications, leveraging BERT, a state-of-the-art text classification model, to evaluate the clarity and thoroughness of these policies. By comparing the privacy policies of AR/VR applications with those of free and premium websites, this study provides a broad perspective on the current state of privacy practices within the AR/VR industry. Our findings indicate that AR/VR applications generally offer a higher percentage of positive segments than free content but lower than premium websites. The analysis of highlighted segments and words revealed that AR/VR applications strategically emphasize critical privacy practices and key terms. This enhances privacy policies' clarity and effectiveness.

**Link**: [arxiv](http://arxiv.org/abs/2501.19223v1),  [pdf](http://arxiv.org/pdf/2501.19223v1)

**Tags**: cs.CR cs.LG 



### Sparse Autoencoders Reveal Universal Feature Spaces Across Large   Language Models
**Authors**: Michael Lan, Philip Torr, Austin Meek, Ashkan Khakzar, David Krueger, Fazl Barez

**Updated**: 2025-01-31T15:27:10Z

**Summary**: We investigate feature universality in large language models (LLMs), a research field that aims to understand how different models similarly represent concepts in the latent spaces of their intermediate layers. Demonstrating feature universality allows discoveries about latent representations to generalize across several models. However, comparing features across LLMs is challenging due to polysemanticity, in which individual neurons often correspond to multiple features rather than distinct ones, making it difficult to disentangle and match features across different models. To address this issue, we employ a method known as dictionary learning by using sparse autoencoders (SAEs) to transform LLM activations into more interpretable spaces spanned by neurons corresponding to individual features. After matching feature neurons across models via activation correlation, we apply representational space similarity metrics on SAE feature spaces across different LLMs. Our experiments reveal significant similarities in SAE feature spaces across various LLMs, providing new evidence for feature universality.

**Link**: [arxiv](http://arxiv.org/abs/2410.06981v2),  [pdf](http://arxiv.org/pdf/2410.06981v2)

**Tags**: cs.LG cs.AI cs.CL 



### Testing for the Minimum Mean-Variance Spanning Set
**Authors**: Zhipeng Liao, Bin Wang, Wenyu Zhou

**Updated**: 2025-01-31T15:18:23Z

**Summary**: This paper explores the estimation and inference of the minimum spanning set (MSS), the smallest subset of risky assets that spans the mean-variance efficient frontier of the full asset set. We establish identification conditions for the MSS and develop a novel procedure for its estimation and inference. Our theoretical analysis shows that the proposed MSS estimator covers the true MSS with probability approaching 1 and converges asymptotically to the true MSS at any desired confidence level, such as 0.95 or 0.99. Monte Carlo simulations confirm the strong finite-sample performance of the MSS estimator. We apply our method to evaluate the relative importance of individual stock momentum and factor momentum strategies, along with a set of well-established stock return factors. The empirical results highlight factor momentum, along with several stock momentum and return factors, as key drivers of mean-variance efficiency. Furthermore, our analysis uncovers the sources of contribution from these factors and provides a ranking of their relative importance, offering new insights into their roles in mean-variance analysis.

**Link**: [arxiv](http://arxiv.org/abs/2501.19213v1),  [pdf](http://arxiv.org/pdf/2501.19213v1)

**Tags**: q-fin.PM 



### Learning Sheaf Laplacian Optimizing Restriction Maps
**Authors**: Leonardo Di Nino, Sergio Barbarossa, Paolo Di Lorenzo

**Updated**: 2025-01-31T15:15:08Z

**Summary**: The aim of this paper is to propose a novel framework to infer the sheaf Laplacian, including the topology of a graph and the restriction maps, from a set of data observed over the nodes of a graph. The proposed method is based on sheaf theory, which represents an important generalization of graph signal processing. The learning problem aims to find the sheaf Laplacian that minimizes the total variation of the observed data, where the variation over each edge is also locally minimized by optimizing the associated restriction maps. Compared to alternative methods based on semidefinite programming, our solution is significantly more numerically efficient, as all its fundamental steps are resolved in closed form. The method is numerically tested on data consisting of vectors defined over subspaces of varying dimensions at each node. We demonstrate how the resulting graph is influenced by two key factors: the cross-correlation and the dimensionality difference of the data residing on the graph's nodes.

**Link**: [arxiv](http://arxiv.org/abs/2501.19207v1),  [pdf](http://arxiv.org/pdf/2501.19207v1)

**Tags**: eess.SP cs.LG 



### Autonomous Legacy Web Application Upgrades Using a Multi-Agent System
**Authors**: Valtteri Ala-Salmi, Zeeshan Rasheed, Abdul Malik Sami, Zheying Zhang, Kai-Kristian Kemell, Jussi Rasku, Shahbaz Siddeeq, Mika Saari, Pekka Abrahamsson

**Updated**: 2025-01-31T15:14:14Z

**Summary**: The use of Large Language Models (LLMs) for autonomous code generation is gaining attention in emerging technologies. As LLM capabilities expand, they offer new possibilities such as code refactoring, security enhancements, and legacy application upgrades. Many outdated web applications pose security and reliability challenges, yet companies continue using them due to the complexity and cost of upgrades. To address this, we propose an LLM-based multi-agent system that autonomously upgrades legacy web applications to the latest versions. The system distributes tasks across multiple phases, updating all relevant files. To evaluate its effectiveness, we employed Zero-Shot Learning (ZSL) and One-Shot Learning (OSL) prompts, applying identical instructions in both cases. The evaluation involved updating view files and measuring the number and types of errors in the output. For complex tasks, we counted the successfully met requirements. The experiments compared the proposed system with standalone LLM execution, repeated multiple times to account for stochastic behavior. Results indicate that our system maintains context across tasks and agents, improving solution quality over the base model in some cases. This study provides a foundation for future model implementations in legacy code updates. Additionally, findings highlight LLMs' ability to update small outdated files with high precision, even with basic prompts. The source code is publicly available on GitHub: https://github.com/alasalm1/Multi-agent-pipeline.

**Link**: [arxiv](http://arxiv.org/abs/2501.19204v1),  [pdf](http://arxiv.org/pdf/2501.19204v1)

**Tags**: cs.SE 



### Improving the Robustness of Representation Misdirection for Large   Language Model Unlearning
**Authors**: Dang Huu-Tien, Hoang Thanh-Tung, Le-Minh Nguyen, Naoya Inoue

**Updated**: 2025-01-31T15:12:20Z

**Summary**: Representation Misdirection (RM) and variants are established large language model (LLM) unlearning methods with state-of-the-art performance. In this paper, we show that RM methods inherently reduce models' robustness, causing them to misbehave even when a single non-adversarial forget-token is in the retain-query. Toward understanding underlying causes, we reframe the unlearning process as backdoor attacks and defenses: forget-tokens act as backdoor triggers that, when activated in retain-queries, cause disruptions in RM models' behaviors, similar to successful backdoor attacks. To mitigate this vulnerability, we propose Random Noise Augmentation -- a model and method agnostic approach with theoretical guarantees for improving the robustness of RM methods. Extensive experiments demonstrate that RNA significantly improves the robustness of RM models while enhancing the unlearning performances.

**Link**: [arxiv](http://arxiv.org/abs/2501.19202v1),  [pdf](http://arxiv.org/pdf/2501.19202v1)

**Tags**: cs.CL 



### Calibrating the medium effects of light clusters in heavy-ion collisions
**Authors**: Tiago Custódio, Alex Rebillard-Soulié, Rémi Bougault, Diego Gruyer, Francesca Gulminelli, Tuhin Malik, Helena Pais, Constança Providência

**Updated**: 2025-01-31T15:11:28Z

**Summary**: We propose a Bayesian inference estimation of in-medium modification of the cluster self-energies from light nuclei multiplicities measured in selected samples of central $^{136,124}$Xe$+^{124,112}$Sn collisions with the INDRA apparatus. The data are interpreted with a relativistic quasi-particle cluster approach in the mean-field approximation without any prior assumption on the thermal parameters of the model. An excellent reproduction is obtained for H and He isotope multiplicities, and compatible posterior distributions are found for the unknown thermal parameters.   We conclude that the cluster-$\sigma$-meson coupling is temperature dependent, becoming weaker when the temperature increases, in agreement with microscopic quantum statistical calculations. This implies a faster decrease of the light cluster abundances with temperature than previously estimated.

**Link**: [arxiv](http://arxiv.org/abs/2407.02307v2),  [pdf](http://arxiv.org/pdf/2407.02307v2)

**Tags**: nucl-th 



### Efficient Reasoning with Hidden Thinking
**Authors**: Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, Jiuxiang Gu

**Updated**: 2025-01-31T15:10:29Z

**Summary**: Chain-of-Thought (CoT) reasoning has become a powerful framework for improving complex problem-solving capabilities in Multimodal Large Language Models (MLLMs). However, the verbose nature of textual reasoning introduces significant inefficiencies. In this work, we propose $\textbf{Heima}$ (as hidden llama), an efficient reasoning framework that leverages reasoning CoTs at hidden latent space. We design the Heima Encoder to condense each intermediate CoT into a compact, higher-level hidden representation using a single thinking token, effectively minimizing verbosity and reducing the overall number of tokens required during the reasoning process. Meanwhile, we design corresponding Heima Decoder with traditional Large Language Models (LLMs) to adaptively interpret the hidden representations into variable-length textual sequence, reconstructing reasoning processes that closely resemble the original CoTs. Experimental results across diverse reasoning MLLM benchmarks demonstrate that Heima model achieves higher generation efficiency while maintaining or even better zero-shot task accuracy. Moreover, the effective reconstruction of multimodal reasoning processes with Heima Decoder validates both the robustness and interpretability of our approach.

**Link**: [arxiv](http://arxiv.org/abs/2501.19201v1),  [pdf](http://arxiv.org/pdf/2501.19201v1)

**Tags**: cs.CL cs.AI cs.LG 



### Theoretical guarantees on the best-of-n alignment policy
**Authors**: Ahmad Beirami, Alekh Agarwal, Jonathan Berant, Alexander D'Amour, Jacob Eisenstein, Chirag Nagpal, Ananda Theertha Suresh

**Updated**: 2025-01-31T15:10:21Z

**Summary**: A simple and effective method for the inference-time alignment of generative models is the best-of-$n$ policy, where $n$ samples are drawn from a reference policy, ranked based on a reward function, and the highest ranking one is selected. A commonly used analytical expression in the literature claims that the KL divergence between the best-of-$n$ policy and the reference policy is equal to $\log (n) - (n-1)/n.$ We disprove the validity of this claim, and show that it is an upper bound on the actual KL divergence. We also explore the tightness of this upper bound in different regimes, and propose a new estimator for the KL divergence and empirically show that it provides a tight approximation. We also show that the win rate of the best-of-$n$ policy against the reference policy is upper bounded by $n/(n+1)$ and derive bounds on the tightness of this characterization. We conclude with analyzing the tradeoffs between win rate and KL divergence of the best-of-$n$ alignment policy, which demonstrate that very good tradeoffs are achievable with $n < 1000$.

**Link**: [arxiv](http://arxiv.org/abs/2401.01879v2),  [pdf](http://arxiv.org/pdf/2401.01879v2)

**Tags**: cs.LG cs.CL cs.IT math.IT 



### AutoElicit: Using Large Language Models for Expert Prior Elicitation in   Predictive Modelling
**Authors**: Alexander Capstick, Rahul G. Krishnan, Payam Barnaghi

**Updated**: 2025-01-31T15:04:34Z

**Summary**: Large language models (LLMs) acquire a breadth of information across various domains. However, their computational complexity, cost, and lack of transparency often hinder their direct application for predictive tasks where privacy and interpretability are paramount. In fields such as healthcare, biology, and finance, specialised and interpretable linear models still hold considerable value. In such domains, labelled data may be scarce or expensive to obtain. Well-specified prior distributions over model parameters can reduce the sample complexity of learning through Bayesian inference; however, eliciting expert priors can be time-consuming. We therefore introduce AutoElicit to extract knowledge from LLMs and construct priors for predictive models. We show these priors are informative and can be refined using natural language. We perform a careful study contrasting AutoElicit with in-context learning and demonstrate how to perform model selection between the two methods. We find that AutoElicit yields priors that can substantially reduce error over uninformative priors, using fewer labels, and consistently outperform in-context learning. We show that AutoElicit saves over 6 months of labelling effort when building a new predictive model for urinary tract infections from sensor recordings of people living with dementia.

**Link**: [arxiv](http://arxiv.org/abs/2411.17284v4),  [pdf](http://arxiv.org/pdf/2411.17284v4)

**Tags**: cs.LG cs.CL stat.ML 



### Rethinking Model Evaluation as Narrowing the Socio-Technical Gap
**Authors**: Q. Vera Liao, Ziang Xiao

**Updated**: 2025-01-31T14:59:17Z

**Summary**: The recent development of generative large language models (LLMs) poses new challenges for model evaluation that the research community and industry have been grappling with. While the versatile capabilities of these models ignite much excitement, they also inevitably make a leap toward homogenization: powering a wide range of applications with a single, often referred to as ``general-purpose'', model. In this position paper, we argue that model evaluation practices must take on a critical task to cope with the challenges and responsibilities brought by this homogenization: providing valid assessments for whether and how much human needs in diverse downstream use cases can be satisfied by the given model (\textit{socio-technical gap}). By drawing on lessons about improving research realism from the social sciences, human-computer interaction (HCI), and the interdisciplinary field of explainable AI (XAI), we urge the community to develop evaluation methods based on real-world contexts and human requirements, and embrace diverse evaluation methods with an acknowledgment of trade-offs between realisms and pragmatic costs to conduct the evaluation. By mapping HCI and current NLG evaluation methods, we identify opportunities for evaluation methods for LLMs to narrow the socio-technical gap and pose open questions.

**Link**: [arxiv](http://arxiv.org/abs/2306.03100v4),  [pdf](http://arxiv.org/pdf/2306.03100v4)

**Tags**: cs.HC cs.AI 



### FAUST XX. The chemical structure and temperature profile of the IRAS 4A2   hot corino at 20-50 au
**Authors**: J. Frediani, M. De Simone, L. Testi, L. Podio, C. Codella, C. J. Chandler, C. Ceccarelli, L. Loinard, A. López-Sepulcre, B. Svoboda, N. Sakai, L. Chahine, Y. Aikawa, E. Bianchi, M. Bouvier, L. Cacciapuoti, P. Caselli, S. B. Charnley, I. Jimenez-Serra, D. Johnstone, G. Sabatini, Y. Shirley, S. Yamamoto

**Updated**: 2025-01-31T14:54:25Z

**Summary**: Young low-mass protostars often possess hot corinos, compact, hot and dense regions bright in interstellar Complex Organic Molecules (iCOMs). Besides of their prebiotic role, iCOMs can be used as a powerful tool to characterize the chemical and physical properties of hot corinos. Using ALMA/FAUST data we aim to explore the iCOMs emission at < 50 au scale around the Class 0 prototypical hot corino IRAS 4A2. We imaged IRAS 4A2 in six abundant, common iCOMs (CH$_3$OH, HCOOCH$_3$, CH$_3$CHO, CH$_3$CH$_2$OH, CH$_2$OHCHO, and NH$_2$CHO), and derived their emitting size. The column density and gas temperature for each species were derived at 1$\sigma$ from a multi-line analysis by applying a non-LTE approach for CH$_3$OH, and LTE population or rotational diagram analysis for the other iCOMs. Thanks to the unique estimates of the absorption from foreground millimeter dust toward IRAS 4A2, we derived for the first time unbiased gas temperatures and column densities. We resolved the IRAS 4A2 hot corino finding evidence for a chemical spatial distribution in the inner 50 au, with the outer emitting radius increasing from ~ 22-23 au for NH$_2$CHO and CH$_2$OHCHO, followed by CH$_3$CH$_2$OH (~ 27 au), CH$_3$CHO (~ 28 au), HCOOCH$_3$ (~ 36 au), and out to ~ 40 au for CH$_3$OH. Combining our estimate of the gas temperature probed by each iCOM with their beam-deconvolved emission sizes, we inferred the gas temperature profile of the hot corino on scales of 20-50 au in radius, finding a power-law index $q$ of approximately -1. We observed, for the first time, a chemical segregation in iCOMs of the IRAS 4A2 hot corino, and derived the gas temperature profile of its inner envelope. The derived profile is steeper than when considering a simple spherical collapsing and optically-thin envelope, hinting at a partially optically-thick envelope or a gravitationally unstable disk-like structure.

**Link**: [arxiv](http://arxiv.org/abs/2501.19188v1),  [pdf](http://arxiv.org/pdf/2501.19188v1)

**Tags**: astro-ph.EP astro-ph.GA astro-ph.SR 



### Enhancing Model Defense Against Jailbreaks with Proactive Safety   Reasoning
**Authors**: Xianglin Yang, Gelei Deng, Jieming Shi, Tianwei Zhang, Jin Song Dong

**Updated**: 2025-01-31T14:45:23Z

**Summary**: Large language models (LLMs) are vital for a wide range of applications yet remain susceptible to jailbreak threats, which could lead to the generation of inappropriate responses. Conventional defenses, such as refusal and adversarial training, often fail to cover corner cases or rare domains, leaving LLMs still vulnerable to more sophisticated attacks. We propose a novel defense strategy, Safety Chain-of-Thought (SCoT), which harnesses the enhanced \textit{reasoning capabilities} of LLMs for proactive assessment of harmful inputs, rather than simply blocking them. SCoT augments any refusal training datasets to critically analyze the intent behind each request before generating answers. By employing proactive reasoning, SCoT enhances the generalization of LLMs across varied harmful queries and scenarios not covered in the safety alignment corpus. Additionally, it generates detailed refusals specifying the rules violated. Comparative evaluations show that SCoT significantly surpasses existing defenses, reducing vulnerability to out-of-distribution issues and adversarial manipulations while maintaining strong general capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2501.19180v1),  [pdf](http://arxiv.org/pdf/2501.19180v1)

**Tags**: cs.CR cs.AI 



### Position: Contextual Integrity Washing for Language Models
**Authors**: Yan Shvartzshnaider, Vasisht Duddu

**Updated**: 2025-01-31T14:39:30Z

**Summary**: Machine learning community is discovering Contextual Integrity (CI) as a useful framework to assess the privacy implications of large language models (LLMs). This is an encouraging development. The CI theory emphasizes sharing information in accordance with privacy norms and can bridge the social, legal, political, and technical aspects essential for evaluating privacy in LLMs. However, this is also a good point to reflect on use of CI for LLMs. This position paper argues that existing literature adopts CI for LLMs without embracing the theory's fundamental tenets, essentially amounting to a form of "CI-washing." CI-washing could lead to incorrect conclusions and flawed privacy-preserving designs. We clarify the four fundamental tenets of CI theory, systematize prior work on whether they deviate from these tenets, and highlight overlooked issues in experimental hygiene for LLMs (e.g., prompt sensitivity, positional bias).

**Link**: [arxiv](http://arxiv.org/abs/2501.19173v1),  [pdf](http://arxiv.org/pdf/2501.19173v1)

**Tags**: cs.CY 



### Position: On-Premises LLM Deployment Demands a Middle Path: Preserving   Privacy Without Sacrificing Model Confidentiality
**Authors**: Hanbo Huang, Yihan Li, Bowen Jiang, Lin Liu, Bo Jiang, Ruoyu Sun, Zhuotao Liu, Shiyu Liang

**Updated**: 2025-01-31T14:36:14Z

**Summary**: Current LLM customization typically relies on two deployment strategies: closed-source APIs, which require users to upload private data to external servers, and open-weight models, which allow local fine-tuning but pose misuse risks. In this position paper, we argue that (1) deploying closed-source LLMs within user-controlled infrastructure (\textit{on-premises deployment}) enhances data privacy and mitigates misuse risks, and (2) a well-designed on-premises deployment must ensure model confidentiality -- by preventing model theft -- and offer privacy-preserving customization. Prior research on small models has explored securing only the output layer within hardware-secured devices to balance confidentiality and customization efficiency. However, we show that this approach is insufficient for defending large-scale LLMs against distillation attacks. We therefore introduce a {semi-open deployment framework} that secures only a few, carefully chosen layers, achieving distillation resistance comparable to fully secured models while preserving fine-tuning flexibility. Through extensive experiments, we show that securing bottom layers significantly reduces functional extraction risks. Our findings demonstrate that privacy and confidentiality can coexist, paving the way for secure on-premises AI deployment that balances usability and protection.

**Link**: [arxiv](http://arxiv.org/abs/2410.11182v2),  [pdf](http://arxiv.org/pdf/2410.11182v2)

**Tags**: cs.LG cs.AI cs.CR 



### Poison as Cure: Visual Noise for Mitigating Object Hallucinations in   LVMs
**Authors**: Kejia Zhang, Keda Tao, Jiasheng Tang, Huan Wang

**Updated**: 2025-01-31T14:31:00Z

**Summary**: Large vision-language models (LVMs) extend large language models (LLMs) with visual perception capabilities, enabling them to process and interpret visual information. A major challenge compromising their reliability is object hallucination that LVMs may generate plausible but factually inaccurate information. We propose a novel visual adversarial perturbation (VAP) method to mitigate this hallucination issue. VAP alleviates LVM hallucination by applying strategically optimized visual noise without altering the base model. Our approach formulates hallucination suppression as an optimization problem, leveraging adversarial strategies to generate beneficial visual perturbations that enhance the model's factual grounding and reduce parametric knowledge bias. Extensive experimental results demonstrate that our method consistently reduces object hallucinations across 8 state-of-the-art LVMs, validating its efficacy across diverse evaluations.

**Link**: [arxiv](http://arxiv.org/abs/2501.19164v1),  [pdf](http://arxiv.org/pdf/2501.19164v1)

**Tags**: cs.CV 



### HarmoniCa: Harmonizing Training and Inference for Better Feature Caching   in Diffusion Transformer Acceleration
**Authors**: Yushi Huang, Zining Wang, Ruihao Gong, Jing Liu, Xinjie Zhang, Jinyang Guo, Xianglong Liu, Jun Zhang

**Updated**: 2025-01-31T14:26:05Z

**Summary**: Diffusion Transformers (DiTs) excel in generative tasks but face practical deployment challenges due to high inference costs. Feature caching, which stores and retrieves redundant computations, offers the potential for acceleration. Existing learning-based caching, though adaptive, overlooks the impact of the prior timestep. It also suffers from misaligned objectives--aligned predicted noise vs. high-quality images--between training and inference. These two discrepancies compromise both performance and efficiency. To this end, we harmonize training and inference with a novel learning-based caching framework dubbed HarmoniCa. It first incorporates Step-Wise Denoising Training (SDT) to ensure the continuity of the denoising process, where prior steps can be leveraged. In addition, an Image Error Proxy-Guided Objective (IEPO) is applied to balance image quality against cache utilization through an efficient proxy to approximate the image error. Extensive experiments across $8$ models, $4$ samplers, and resolutions from $256\times256$ to $2K$ demonstrate superior performance and speedup of our framework. For instance, it achieves over $40\%$ latency reduction (i.e., $2.07\times$ theoretical speedup) and improved performance on PixArt-$\alpha$. Remarkably, our image-free approach reduces training time by $25\%$ compared with the previous method.

**Link**: [arxiv](http://arxiv.org/abs/2410.01723v3),  [pdf](http://arxiv.org/pdf/2410.01723v3)

**Tags**: cs.CV 



### LLM4Rerank: LLM-based Auto-Reranking Framework for Recommendations
**Authors**: Jingtong Gao, Bo Chen, Weiwen Liu, Xiangyang Li, Yichao Wang, Wanyu Wang, Huifeng Guo, Ruiming Tang, Xiangyu Zhao

**Updated**: 2025-01-31T14:22:27Z

**Summary**: Reranking is a critical component in recommender systems, playing an essential role in refining the output of recommendation algorithms. Traditional reranking models have focused predominantly on accuracy, but modern applications demand consideration of additional criteria such as diversity and fairness. Existing reranking approaches often fail to harmonize these diverse criteria effectively at the model level. Moreover, these models frequently encounter challenges with scalability and personalization due to their complexity and the varying significance of different reranking criteria in diverse scenarios. In response, we introduce a comprehensive reranking framework enhanced by LLM, designed to seamlessly integrate various reranking criteria while maintaining scalability and facilitating personalized recommendations. This framework employs a fully connected graph structure, allowing the LLM to simultaneously consider multiple aspects such as accuracy, diversity, and fairness through a coherent Chain-of-Thought (CoT) process. A customizable input mechanism is also integrated, enabling the tuning of the language model's focus to meet specific reranking needs. We validate our approach using three popular public datasets, where our framework demonstrates superior performance over existing state-of-the-art reranking models in balancing multiple criteria. The code for this implementation is publicly available.

**Link**: [arxiv](http://arxiv.org/abs/2406.12433v3),  [pdf](http://arxiv.org/pdf/2406.12433v3)

**Tags**: cs.IR 



### CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning
**Authors**: Fanxu Meng, Pingzhi Tang, Fan jiang, Muhan Zhang

**Updated**: 2025-01-31T14:13:49Z

**Summary**: Decoder-only models generate tokens autoregressively by caching key/value vectors, but as the cache grows, inference becomes memory-bound. To address this issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel approach that treats pairs of attention layers as a set of low-rank decompositions. CLOVER applies Singular Value Decomposition (SVD) to the \( Q \)-\( K \) and \( V \)-\( O \) pairs within each attention head. The resulting singular values can either guide pruning or serve as trainable parameters for efficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning, these values are reintegrated into the model without increasing its parameter count. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite, Whisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results demonstrate that CLOVER significantly improves pruning efficiency. For instance, the perplexity of pruning 70\% of the \( Q \)-\( K \) pairs in GPT-2 XL is similar to that of pruning just 8\% with vanilla methods. Fine-tuning the singular values further results in a full-rank update, outperforming state-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\%, 5.5\%, 3.8\%, and 0.7\%, respectively, on eight commonsense tasks for LLaMA-2 7B.

**Link**: [arxiv](http://arxiv.org/abs/2411.17426v3),  [pdf](http://arxiv.org/pdf/2411.17426v3)

**Tags**: cs.LG cs.AI 



### SampleLLM: Optimizing Tabular Data Synthesis in Recommendations
**Authors**: Jingtong Gao, Zhaocheng Du, Xiaopeng Li, Yichao Wang, Xiangyang Li, Huifeng Guo, Ruiming Tang, Xiangyu Zhao

**Updated**: 2025-01-31T14:00:30Z

**Summary**: Tabular data synthesis is crucial in machine learning, yet existing general methods-primarily based on statistical or deep learning models-are highly data-dependent and often fall short in recommender systems. This limitation arises from their difficulty in capturing complex distributions and understanding feature relationships from sparse and limited data, along with their inability to grasp semantic feature relations. Recently, Large Language Models (LLMs) have shown potential in generating synthetic data samples through few-shot learning and semantic understanding. However, they often suffer from inconsistent distribution and lack of diversity due to their inherent distribution disparity with the target dataset. To address these challenges and enhance tabular data synthesis for recommendation tasks, we propose a novel two-stage framework named SampleLLM to improve the quality of LLM-based tabular data synthesis for recommendations by ensuring better distribution alignment. In the first stage, SampleLLM employs LLMs with Chain-of-Thought prompts and diverse exemplars to generate data that closely aligns with the target dataset distribution, even when input samples are limited. The second stage uses an advanced feature attribution-based importance sampling method to refine feature relationships within the synthesized data, reducing any distribution biases introduced by the LLM. Experimental results on three recommendation datasets, two general datasets, and online deployment illustrate that SampleLLM significantly surpasses existing methods for recommendation tasks and holds promise for a broader range of tabular data scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2501.16125v2),  [pdf](http://arxiv.org/pdf/2501.16125v2)

**Tags**: cs.IR 



### Cautious Optimizers: Improving Training with One Line of Code
**Authors**: Kaizhao Liang, Lizhang Chen, Bo Liu, Qiang Liu

**Updated**: 2025-01-31T13:56:58Z

**Summary**: AdamW has been the default optimizer for transformer pretraining. For many years, our community searched for faster and more stable optimizers with only constrained positive outcomes. In this work, we propose a single-line modification in Pytorch to any momentum-based optimizer, which we rename cautious optimizer, e.g. C-AdamW and C-Lion. Our theoretical result shows that this modification preserves Adam's Hamiltonian function and it does not break the convergence guarantee under the Lyapunov analysis. In addition, a whole new family of optimizers is revealed by our theoretical insight. Among them, we pick the simplest one for empirical experiments, showing not only speed-up on Llama and MAE pretraining up to $1.47$ times, but also better results in LLM post-training tasks. Code is available at https://github.com/kyleliang919/C-Optim.

**Link**: [arxiv](http://arxiv.org/abs/2411.16085v3),  [pdf](http://arxiv.org/pdf/2411.16085v3)

**Tags**: cs.LG cs.AI cs.CL cs.CV cs.DM 



### A Tensor-Train Decomposition based Compression of LLMs on Group Vector   Systolic Accelerator
**Authors**: Sixiao Huang, Tintin Wang, Ang Li, Ao Shen, Kai Li, Keyao Jiang, Mingqiang Huang, Hao Yu

**Updated**: 2025-01-31T13:45:31Z

**Summary**: Large language models (LLMs) are both storage-intensive and computation-intensive, posing significant challenges when deployed on resource-constrained hardware. As linear layers in LLMs are mainly resource consuming parts, this paper develops a tensor-train decomposition (TTD) for LLMs with a further hardware implementation on FPGA. TTD compression is applied to the linear layers in ChatGLM3-6B and LLaMA2-7B models with compression ratios (CRs) for the whole network 1.94$\times$ and 1.60$\times$, respectively. The compressed LLMs are further implemented on FPGA hardware within a highly efficient group vector systolic array (GVSA) architecture, which has DSP-shared parallel vector PEs for TTD inference, as well as optimized data communication in the accelerator. Experimental results show that the corresponding TTD based LLM accelerator implemented on FPGA achieves 1.45$\times$ and 1.57$\times$ reduction in first token delay for ChatGLM3-6B and LLaMA2-7B models, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2501.19135v1),  [pdf](http://arxiv.org/pdf/2501.19135v1)

**Tags**: cs.AR 



### Deep learning inference of the neutron star equation of state
**Authors**: Giulia Ventagli, Ippocratis D. Saltas

**Updated**: 2025-01-31T13:41:32Z

**Summary**: We present a pipeline to infer the equation of state of neutron stars from observations based on deep neural networks. In particular, using the standard (deterministic), as well as Bayesian (probabilistic) deep networks, we explore how one can infer the interior speed of sound of the star given a set of mock observations of total stellar mass, stellar radius and tidal deformability. We discuss in detail the construction of our simulated dataset of stellar observables starting from the solution of the gravitational equations, as well as the relevant architectures for the deep networks, along with their performance and accuracy. We further explain how our pipeline is capable to detect a possible QCD phase transition in the stellar core. Our results show that deep networks offer a promising tool towards solving the inverse problem of neutron stars, and the accurate inference of their interior from future stellar observations.

**Link**: [arxiv](http://arxiv.org/abs/2405.17908v2),  [pdf](http://arxiv.org/pdf/2405.17908v2)

**Tags**: astro-ph.HE astro-ph.IM gr-qc hep-ph hep-th 



### Asymptotic optimality theory of confidence intervals of the mean
**Authors**: Vikas Deep, Achal Bassamboo, Sandeep Juneja

**Updated**: 2025-01-31T13:31:43Z

**Summary**: We address the classical problem of constructing confidence intervals (CIs) for the mean of a distribution, given \(N\) i.i.d. samples, such that the CI contains the true mean with probability at least \(1 - \delta\), where \(\delta \in (0,1)\). We characterize three distinct learning regimes based on the minimum achievable limiting width of any CI as the sample size \(N_{\delta} \to \infty\) and \(\delta \to 0\). In the first regime, where \(N_{\delta}\) grows slower than \(\log(1/\delta)\), the limiting width of any CI equals the width of the distribution's support, precluding meaningful inference. In the second regime, where \(N_{\delta}\) scales as \(\log(1/\delta)\), we precisely characterize the minimum limiting width, which depends on the scaling constant. In the third regime, where \(N_{\delta}\) grows faster than \(\log(1/\delta)\), complete learning is achievable, and the limiting width of the CI collapses to zero, converging to the true mean. We demonstrate that CIs derived from concentration inequalities based on Kullback--Leibler (KL) divergences achieve asymptotically optimal performance, attaining the minimum limiting width in both sufficient and complete learning regimes for distributions in two families: single-parameter exponential and bounded support. Additionally, these results extend to one-sided CIs, with the width notion adjusted appropriately. Finally, we generalize our findings to settings with random per-sample costs, motivated by practical applications such as stochastic simulators and cloud service selection. Instead of a fixed sample size, we consider a cost budget \(C_{\delta}\), identifying analogous learning regimes and characterizing the optimal CI construction policy.

**Link**: [arxiv](http://arxiv.org/abs/2501.19126v1),  [pdf](http://arxiv.org/pdf/2501.19126v1)

**Tags**: math.ST stat.TH 



### LANTERN: Accelerating Visual Autoregressive Models with Relaxed   Speculative Decoding
**Authors**: Doohyuk Jang, Sihwan Park, June Yong Yang, Yeonsung Jung, Jihun Yun, Souvik Kundu, Sung-Yub Kim, Eunho Yang

**Updated**: 2025-01-31T13:24:10Z

**Summary**: Auto-Regressive (AR) models have recently gained prominence in image generation, often matching or even surpassing the performance of diffusion models. However, one major limitation of AR models is their sequential nature, which processes tokens one at a time, slowing down generation compared to models like GANs or diffusion-based methods that operate more efficiently. While speculative decoding has proven effective for accelerating LLMs by generating multiple tokens in a single forward, its application in visual AR models remains largely unexplored. In this work, we identify a challenge in this setting, which we term \textit{token selection ambiguity}, wherein visual AR models frequently assign uniformly low probabilities to tokens, hampering the performance of speculative decoding. To overcome this challenge, we propose a relaxed acceptance condition referred to as LANTERN that leverages the interchangeability of tokens in latent space. This relaxation restores the effectiveness of speculative decoding in visual AR models by enabling more flexible use of candidate tokens that would otherwise be prematurely rejected. Furthermore, by incorporating a total variation distance bound, we ensure that these speed gains are achieved without significantly compromising image quality or semantic coherence. Experimental results demonstrate the efficacy of our method in providing a substantial speed-up over speculative decoding. In specific, compared to a na\"ive application of the state-of-the-art speculative decoding, LANTERN increases speed-ups by $\mathbf{1.75}\times$ and $\mathbf{1.82}\times$, as compared to greedy decoding and random sampling, respectively, when applied to LlamaGen, a contemporary visual AR model.

**Link**: [arxiv](http://arxiv.org/abs/2410.03355v2),  [pdf](http://arxiv.org/pdf/2410.03355v2)

**Tags**: cs.CV cs.AI 



### The AURORA Survey: An Extraordinarily Mature, Star-forming Galaxy at   $z\sim 7$
**Authors**: Alice E. Shapley, Ryan L. Sanders, Michael W. Topping, Naveen A. Reddy, Anthony J. Pahl, Pascal A. Oesch, Danielle A. Berg, Rychard J. Bouwens, Gabriel Brammer, Adam C. Carnall, Fergus Cullen, Romeel Davé, James S. Dunlop, Richard S. Ellis, N. M. Förster Schreiber, Steven R . Furlanetto, Karl Glazebrook, Garth D. Illingworth, Tucker Jones, Mariska Kriek, Derek J. McLeod, Ross J. McLure, Desika Narayanan, Max Pettini, Daniel Schaerer, Daniel P. Stark, Charles C. Steidel, Mengtao Tang, Leonardo Clarke, Callum T. Donnan, Emily Kehoe

**Updated**: 2025-01-31T13:07:09Z

**Summary**: We present the properties of a massive, large, dusty, metal-rich, star-forming galaxy at z_spec=6.73. GOODSN-100182 was observed with JWST/NIRSpec as part of the AURORA survey, and is also covered by public multi-wavelength HST and JWST imaging. While the large stellar mass of GOODSN-100182 (~10^10 M_sun) was indicated prior to JWST, NIRCam rest-optical imaging now reveals the presence of an extended disk (r_eff~1.5 kpc). In addition, the NIRSpec R~1000 spectrum of GOODSN-100182 includes the detection of a large suite of rest-optical nebular emission lines ranging in wavelength from [OII]3727 up to [NII]6583. The ratios of Balmer lines suggest significant dust attenuation (E(B-V)_gas=0.40+0.10/-0.09), consistent with the red rest-UV slope inferred for GOODSN-100182 (beta=-0.50+/-0.09). The star-formation rate based on dust-corrected H-alpha emission is log(SFR(H-alpha)/ M_sun/yr)=2.02+0.13/-0.14, well above the z~7 star-forming main sequence in terms of specific SFR. Strikingly, the ratio of [NII]6583/H-alpha emission suggests almost solar metallicity, as does the ratio ([OIII]5007/H-beta)/([NII]6583/H-alpha) and the detection of the faint [FeII]4360 emission feature. Overall, the excitation and ionization properties of GOODSN-100182 more closely resemble those of typical star-forming galaxies at z~2-3 rather than z~7. Based on public spectroscopy of the GOODS-N field, we find that GOODSN-100182 resides within a significant galaxy overdensity, and is accompanied by a spectroscopically-confirmed neighbor galaxy. GOODSN-100182 demonstrates the existence of mature, chemically-enriched galaxies within the first billion years of cosmic time, whose properties must be explained by galaxy formation models.

**Link**: [arxiv](http://arxiv.org/abs/2410.00110v3),  [pdf](http://arxiv.org/pdf/2410.00110v3)

**Tags**: astro-ph.GA 



### Brain-inspired sparse training enables Transformers and LLMs to perform   as fully connected
**Authors**: Yingtao Zhang, Jialin Zhao, Wenjing Wu, Ziheng Liao, Umberto Michieli, Carlo Vittorio Cannistraci

**Updated**: 2025-01-31T13:04:37Z

**Summary**: This study aims to enlarge our current knowledge on application of brain-inspired network science principles for training artificial neural networks (ANNs) with sparse connectivity. Dynamic sparse training (DST) can reduce the computational demands in ANNs, but faces difficulties to keep peak performance at high sparsity levels. The Cannistraci-Hebb training (CHT) is a brain-inspired method for growing connectivity in DST. CHT leverages a gradient-free, topology-driven link regrowth, which has shown ultra-sparse (1% connectivity or lower) advantage across various tasks compared to fully connected networks. Yet, CHT suffers two main drawbacks: (i) its time complexity is O(Nd^3) - N node network size, d node degree - hence it can apply only to ultra-sparse networks. (ii) it selects top link prediction scores, which is inappropriate for the early training epochs, when the network presents unreliable connections. We propose a GPU-friendly approximation of the CH link predictor, which reduces the computational complexity to O(N^3), enabling a fast implementation of CHT in large-scale models. We introduce the Cannistraci-Hebb training soft rule (CHTs), which adopts a strategy for sampling connections in both link removal and regrowth, balancing the exploration and exploitation of network topology. To improve performance, we integrate CHTs with a sigmoid gradual density decay (CHTss). Empirical results show that, using 1% of connections, CHTs outperforms fully connected networks in MLP on visual classification tasks, compressing some networks to < 30% nodes. Using 5% of the connections, CHTss outperforms fully connected networks in two Transformer-based machine translation tasks. Using 30% of the connections, CHTss achieves superior performance compared to other dynamic sparse training methods in language modeling, and it surpasses the fully connected counterpart in zero-shot evaluations.

**Link**: [arxiv](http://arxiv.org/abs/2501.19107v1),  [pdf](http://arxiv.org/pdf/2501.19107v1)

**Tags**: cs.LG 



### Large Language Models are In-context Preference Learners
**Authors**: Chao Yu, Qixin Tan, Hong Lu, Jiaxuan Gao, Xinting Yang, Yu Wang, Yi Wu, Eugene Vinitsky

**Updated**: 2025-01-31T13:04:34Z

**Summary**: Preference-based reinforcement learning is an effective way to handle tasks where rewards are hard to specify but can be exceedingly inefficient as preference learning is often tabula rasa. We demonstrate that Large Language Models (LLMs) have native preference-learning capabilities that allow them to achieve sample-efficient preference learning, addressing this challenge. We propose In-Context Preference Learning (ICPL), which uses in-context learning capabilities of LLMs to reduce human query inefficiency. ICPL uses the task description and basic environment code to create sets of reward functions which are iteratively refined by placing human feedback over videos of the resultant policies into the context of an LLM and then requesting better rewards. We first demonstrate ICPL's effectiveness through a synthetic preference study, providing quantitative evidence that it significantly outperforms baseline preference-based methods with much higher performance and orders of magnitude greater efficiency. We observe that these improvements are not solely coming from LLM grounding in the task but that the quality of the rewards improves over time, indicating preference learning capabilities. Additionally, we perform a series of real human preference-learning trials and observe that ICPL extends beyond synthetic settings and can work effectively with humans-in-the-loop.

**Link**: [arxiv](http://arxiv.org/abs/2410.17233v2),  [pdf](http://arxiv.org/pdf/2410.17233v2)

**Tags**: cs.AI cs.LG 



### Large molecular and dust reservoir of a gravitationally-lensed   submillimeter galaxy behind the Lupus-I molecular cloud
**Authors**: Yoichi Tamura, Akio Taniguchi, Tom J. L. C. Bakx, Itziar De Gregorio-Monsalvo, Masato Hagimoto, Soh Ikarashi, Ryohei Kawabe, Kotaro Kohno, Kouichiro Nakanishi, Tatsuya Takekoshi, Yoshito Shimajiri, Takashi Tsukagoshi, Bunyo Hatsukade, Daisuke Iono, Hideo Matsuhara, Kazuya Saigo, Masao Saito

**Updated**: 2025-01-31T12:49:22Z

**Summary**: We report the Australian Telescope Compact Array and Nobeyama 45 m telescope detection of a remarkably bright $S_\mathrm{1.1mm}$ = 44 mJy) submillimeter galaxy MM J154506.4-344318 in emission lines at 48.5 and 97.0 GHz, respectively. We also identify part of an emission line at $\approx$ 218.3 GHz using the Atacama Large Millimeter/submillimeter Array (ALMA). Together with photometric redshift estimates and the ratio between the line and infrared luminosities, we conclude that the emission lines are most likely to be the $J$ = 2-1, 4-3, and 9-8 transitions of $^{12}$CO at redshift $z = 3.753 \pm 0.001$. ALMA 1.3 mm continuum imaging reveals an arc and a spot separated by an angular distance of 1.6 arcsec, indicative of a strongly-lensed dusty star-forming galaxy with respective molecular and dust masses of $\log{M_{\rm mol}/M_\odot} \approx 11.5$ and $\log{M_{\rm dust}/M_\odot} \approx 9.4$ after corrected for $\approx$ 6.6$\times$ gravitational magnification. The inferred dust-to-gas mass ratio is found to be high ($\approx$ 0.0083) among coeval dusty star-forming galaxies, implying the presence of a massive, chemically-enriched reservoir of cool interstellar medium at $z \approx 4$ or 1.6 Gyr after the Big Bang.

**Link**: [arxiv](http://arxiv.org/abs/2501.19100v1),  [pdf](http://arxiv.org/pdf/2501.19100v1)

**Tags**: astro-ph.GA 



### Oryx MLLM: On-Demand Spatial-Temporal Understanding at Arbitrary   Resolution
**Authors**: Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, Yongming Rao

**Updated**: 2025-01-31T12:48:09Z

**Summary**: Visual data comes in various forms, ranging from small icons of just a few pixels to long videos spanning hours. Existing multi-modal LLMs usually standardize these diverse visual inputs to a fixed resolution for visual encoders and yield similar numbers of tokens for LLMs. This approach is non-optimal for multimodal understanding and inefficient for processing inputs with long and short visual contents. To solve the problem, we propose Oryx, a unified multimodal architecture for the spatial-temporal understanding of images, videos, and multi-view 3D scenes. Oryx offers an on-demand solution to seamlessly and efficiently process visual inputs with arbitrary spatial sizes and temporal lengths through two core innovations: 1) a pre-trained OryxViT model that can encode images at any resolution into LLM-friendly visual representations; 2) a dynamic compressor module that supports 1x to 16x compression on visual tokens by request. These design features enable Oryx to accommodate extremely long visual contexts, such as videos, with lower resolution and high compression while maintaining high recognition precision for tasks like document understanding with native resolution and no compression. Beyond the architectural improvements, enhanced data curation and specialized training on long-context retrieval and spatial-aware data help Oryx achieve strong capabilities in image, video, and 3D multimodal understanding simultaneously. Our work is open-sourced at https://github.com/Oryx-mllm/Oryx.

**Link**: [arxiv](http://arxiv.org/abs/2409.12961v3),  [pdf](http://arxiv.org/pdf/2409.12961v3)

**Tags**: cs.CV 



### Unraveling Zeroth-Order Optimization through the Lens of Low-Dimensional   Structured Perturbations
**Authors**: Sihwan Park, Jihun Yun, SungYub Kim, Souvik Kundu, Eunho Yang

**Updated**: 2025-01-31T12:46:04Z

**Summary**: Zeroth-order (ZO) optimization has emerged as a promising alternative to gradient-based backpropagation methods, particularly for black-box optimization and large language model (LLM) fine-tuning. However, ZO methods suffer from slow convergence due to high-variance stochastic gradient estimators. While structured perturbations, such as sparsity and low-rank constraints, have been explored to mitigate these issues, their effectiveness remains highly under-explored. In this work, we develop a unified theoretical framework that analyzes both the convergence and generalization properties of ZO optimization under structured perturbations. We show that high dimensionality is the primary bottleneck and introduce the notions of \textit{stable rank} and \textit{effective overlap} to explain how structured perturbations reduce gradient noise and accelerate convergence. Using the uniform stability under our framework, we then provide the first theoretical justification for why these perturbations enhance generalization. Additionally, through empirical analysis, we identify that \textbf{block coordinate descent} (BCD) to be an effective structured perturbation method. Extensive experiments show that, compared to existing alternatives, memory-efficient ZO (MeZO) with BCD (\textit{MeZO-BCD}) can provide improved converge with a faster wall-clock time/iteration by up to $\times\textbf{2.09}$ while yielding similar or better accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2501.19099v1),  [pdf](http://arxiv.org/pdf/2501.19099v1)

**Tags**: cs.LG 



### Improving Low-Resource Sequence Labeling with Knowledge Fusion and   Contextual Label Explanations
**Authors**: Peichao Lai, Jiaxin Gan, Feiyang Ye, Yilei Wang, Bin Cui

**Updated**: 2025-01-31T12:39:28Z

**Summary**: Sequence labeling remains a significant challenge in low-resource, domain-specific scenarios, particularly for character-dense languages like Chinese. Existing methods primarily focus on enhancing model comprehension and improving data diversity to boost performance. However, these approaches still struggle with inadequate model applicability and semantic distribution biases in domain-specific contexts. To overcome these limitations, we propose a novel framework that combines an LLM-based knowledge enhancement workflow with a span-based Knowledge Fusion for Rich and Efficient Extraction (KnowFREE) model. Our workflow employs explanation prompts to generate precise contextual interpretations of target entities, effectively mitigating semantic biases and enriching the model's contextual understanding. The KnowFREE model further integrates extension label features, enabling efficient nested entity extraction without relying on external knowledge during inference. Experiments on multiple Chinese domain-specific sequence labeling datasets demonstrate that our approach achieves state-of-the-art performance, effectively addressing the challenges posed by low-resource settings.

**Link**: [arxiv](http://arxiv.org/abs/2501.19093v1),  [pdf](http://arxiv.org/pdf/2501.19093v1)

**Tags**: cs.CL 



### Pivoting Factorization: A Compact Meta Low-Rank Representation of   Sparsity for Efficient Inference in Large Language Models
**Authors**: Jialin Zhao, Yingtao Zhang, Carlo Vittorio Cannistraci

**Updated**: 2025-01-31T12:36:31Z

**Summary**: The rapid growth of Large Language Models has driven demand for effective model compression techniques to reduce memory and computation costs. Low-rank pruning has gained attention for its tensor coherence and GPU compatibility across all densities. However, low-rank pruning has struggled to match the performance of semi-structured pruning, often doubling perplexity (PPL) at similar densities. In this paper, we propose Pivoting Factorization (PIFA), a novel lossless meta low-rank representation that unsupervisedly learns a compact form of any low-rank representation, effectively eliminating redundant information. PIFA identifies pivot rows (linearly independent rows) and expresses non-pivot rows as linear combinations, achieving an additional 24.2\% memory savings and 24.6\% faster inference over low-rank layers at r/d = 0.5, thereby significantly enhancing performance at the same density. To mitigate the performance degradation caused by low-rank pruning, we introduce a novel, retraining-free low-rank reconstruction method that minimizes error accumulation (M). MPIFA, combining M and PIFA into an end-to-end framework, significantly outperforms existing low-rank pruning methods and, for the first time, achieves performance comparable to semi-structured pruning, while surpassing it in GPU efficiency and compatibility.

**Link**: [arxiv](http://arxiv.org/abs/2501.19090v1),  [pdf](http://arxiv.org/pdf/2501.19090v1)

**Tags**: cs.LG 



### SituationalLLM: Proactive language models with scene awareness for   dynamic, contextual task guidance
**Authors**: Muhammad Saif Ullah Khan, Muhammad Zeshan Afzal, Didier Stricker

**Updated**: 2025-01-31T12:35:54Z

**Summary**: Large language models (LLMs) have achieved remarkable success in text-based tasks but often struggle to provide actionable guidance in real-world physical environments. This is because of their inability to recognize their limited understanding of the user's physical context. We present SituationalLLM, a novel approach that integrates structured scene information into an LLM to deliver proactive, context-aware assistance. By encoding objects, attributes, and relationships in a custom Scene Graph Language, SituationalLLM actively identifies gaps in environmental context and seeks clarifications during user interactions. This behavior emerges from training on the Situational Awareness Database for Instruct-Tuning (SAD-Instruct), which combines diverse, scenario-specific scene graphs with iterative, dialogue-based refinements. Experimental results indicate that SituationalLLM outperforms generic LLM baselines in task specificity, reliability, and adaptability, paving the way for environment-aware AI assistants capable of delivering robust, user-centric guidance under real-world constraints.

**Link**: [arxiv](http://arxiv.org/abs/2406.13302v3),  [pdf](http://arxiv.org/pdf/2406.13302v3)

**Tags**: cs.CV 



### Fairness Analysis of CLIP-Based Foundation Models for X-Ray Image   Classification
**Authors**: Xiangyu Sun, Xiaoguang Zou, Yuanquan Wu, Guotai Wang, Shaoting Zhang

**Updated**: 2025-01-31T12:23:50Z

**Summary**: X-ray imaging is pivotal in medical diagnostics, offering non-invasive insights into a range of health conditions. Recently, vision-language models, such as the Contrastive Language-Image Pretraining (CLIP) model, have demonstrated potential in improving diagnostic accuracy by leveraging large-scale image-text datasets. However, since CLIP was not initially designed for medical images, several CLIP-like models trained specifically on medical images have been developed. Despite their enhanced performance, issues of fairness - particularly regarding demographic attributes - remain largely unaddressed. In this study, we perform a comprehensive fairness analysis of CLIP-like models applied to X-ray image classification. We assess their performance and fairness across diverse patient demographics and disease categories using zero-shot inference and various fine-tuning techniques, including Linear Probing, Multilayer Perceptron (MLP), Low-Rank Adaptation (LoRA), and full fine-tuning. Our results indicate that while fine-tuning improves model accuracy, fairness concerns persist, highlighting the need for further fairness interventions in these foundational models.

**Link**: [arxiv](http://arxiv.org/abs/2501.19086v1),  [pdf](http://arxiv.org/pdf/2501.19086v1)

**Tags**: cs.CV cs.AI 



### Enhancing Code Generation for Low-Resource Languages: No Silver Bullet
**Authors**: Alessandro Giagnorio, Alberto Martin-Lopez, Gabriele Bavota

**Updated**: 2025-01-31T12:23:28Z

**Summary**: The advent of Large Language Models (LLMs) has significantly advanced the field of automated code generation. LLMs rely on large and diverse datasets to learn syntax, semantics, and usage patterns of programming languages. For low-resource languages (i.e., niche programming languages characterized by the scarcity of training data), the limited availability of such data hampers the models' ability to generalize effectively, resulting in poorer code generation performance as compared to high-resource languages. For this reason, there is a quest for techniques able to close this performance gap. We present an empirical study investigating the effectiveness of several approaches for boosting LLMs' performance on low-resource languages, namely: (i) a classic fine-tuning, which is however capped in size by the scarcity of training data; (ii) three variants of in-context learning, with prompts crafted to provide the LLM with additional information about the low-resource language (e.g., few-shot examples showcasing features of the targeted language); and (iii) a pre-training objective teaching the model how to translate between high- and low-resource languages. The context of our study are two low-resource languages (R and Racket) and six LLMs having different architectures and sizes. Our findings reveal that a fine-tuning is usually the best choice for smaller LLMs, possibly due to the fact that even a small dataset is sufficient to train their limited number of parameters. With the increase in size of the models, in-context learning becomes more and more effective, representing a safe and cheap bet (i.e., it always helps, but with different magnitudes). Differently, very large LLMs may deteriorate their performance on low-resource languages when fine-tuning is performed, possibly due to the lack of enough data needed to effectively update their weights.

**Link**: [arxiv](http://arxiv.org/abs/2501.19085v1),  [pdf](http://arxiv.org/pdf/2501.19085v1)

**Tags**: cs.SE 



### Sociotechnical Approach to Enterprise Generative Artificial Intelligence   (E-GenAI)
**Authors**: Leoncio Jimenez, Francisco Venegas

**Updated**: 2025-01-31T12:19:43Z

**Summary**: In this theoretical article, a sociotechnical approach is proposed to characterize. First, the business ecosystem, focusing on the relationships among Providers, Enterprise, and Customers through SCM, ERP, and CRM platforms to align: (1) Business Intelligence (BI), Fuzzy Logic (FL), and TRIZ (Theory of Inventive Problem Solving), through the OID model, and (2) Knowledge Management (KM) and Imperfect Knowledge Management (IKM), through the OIDK model. Second, the article explores the E-GenAI business ecosystem, which integrates GenAI-based platforms for SCM, ERP, and CRM with GenAI-based platforms for BI, FL, TRIZ, KM, and IKM, to align Large Language Models (LLMs) through the E-GenAI (OID) model. Finally, to understand the dynamics of LLMs, we utilize finite automata to model the relationships between Followers and Followees. This facilitates the construction of LLMs that can identify specific characteristics of users on a social media platform.

**Link**: [arxiv](http://arxiv.org/abs/2409.17408v2),  [pdf](http://arxiv.org/pdf/2409.17408v2)

**Tags**: cs.CY cs.AI cs.IT math.IT 



### Contrast-Aware Calibration for Fine-Tuned CLIP: Leveraging Image-Text   Alignment
**Authors**: Song-Lin Lv, Yu-Yang Chen, Zhi Zhou, Yu-Feng Li, Lan-Zhe Guo

**Updated**: 2025-01-31T11:47:15Z

**Summary**: Vision-language models (VLMs), such as CLIP, have demonstrated exceptional generalization capabilities and can quickly adapt to downstream tasks through prompt fine-tuning. Unfortunately, in classification tasks involving non-training classes, known as open-vocabulary setting, fine-tuned VLMs often overfit to train classes, resulting in a misalignment between confidence scores and actual accuracy on unseen classes, which significantly undermines their reliability in real-world deployments. Existing confidence calibration methods typically require training parameters or analyzing features from the training dataset, restricting their ability to generalize unseen classes without corresponding train data. Moreover, VLM-specific calibration methods rely solely on text features from train classes as calibration indicators, which inherently limits their ability to calibrate train classes. To address these challenges, we propose an effective multimodal calibration method Contrast-Aware Calibration (CAC). Building on the original CLIP's zero-shot adaptability and the conclusion from empirical analysis that poor intra-class and inter-class discriminative ability on unseen classes is the root cause, we calculate calibration weights based on the contrastive difference between the original and fine-tuned CLIP. This method not only adapts to calibrating unseen classes but also overcomes the limitations of previous VLM calibration methods that could not calibrate train classes. In experiments involving 11 datasets with 5 fine-tuning methods, CAC consistently achieved the best calibration effect on both train and unseen classes without sacrificing accuracy and inference speed.

**Link**: [arxiv](http://arxiv.org/abs/2501.19060v1),  [pdf](http://arxiv.org/pdf/2501.19060v1)

**Tags**: cs.CV cs.LG 



### TeZO: Empowering the Low-Rankness on the Temporal Dimension in the   Zeroth-Order Optimization for Fine-tuning LLMs
**Authors**: Yan Sun, Tiansheng Huang, Liang Ding, Li Shen, Dacheng Tao

**Updated**: 2025-01-31T11:34:03Z

**Summary**: Zeroth-order optimization (ZO) has demonstrated remarkable promise in efficient fine-tuning tasks for Large Language Models (LLMs). In particular, recent advances incorporate the low-rankness of gradients, introducing low-rank ZO estimators to further reduce GPU memory consumption. However, most existing works focus solely on the low-rankness of each individual gradient, overlooking a broader property shared by all gradients throughout the training, i.e., all gradients approximately reside within a similar subspace. In this paper, we consider two factors together and propose a novel low-rank ZO estimator, TeZO, which captures the low-rankness across both the model and temporal dimension. Specifically, we represent ZO perturbations along the temporal dimension as a 3D tensor and employ Canonical Polyadic Decomposition (CPD) to extract each low-rank 2D matrix, significantly reducing the training cost. TeZO can also be easily extended to the Adam variant while consuming less memory than MeZO-SGD, and requiring about only 35% memory of MeZO-Adam. Both comprehensive theoretical analysis and extensive experimental research have validated its efficiency, achieving SOTA-comparable results with lower overhead of time and memory.

**Link**: [arxiv](http://arxiv.org/abs/2501.19057v1),  [pdf](http://arxiv.org/pdf/2501.19057v1)

**Tags**: cs.LG 



### Enabling Autonomic Microservice Management through Self-Learning Agents
**Authors**: Fenglin Yu, Fangkai Yang, Xiaoting Qin, Zhiyang Zhang, Jue Zhang, Qingwei Lin, Hongyu Zhang, Yingnong Dang, Saravan Rajmohan, Dongmei Zhang, Qi Zhang

**Updated**: 2025-01-31T11:32:05Z

**Summary**: The increasing complexity of modern software systems necessitates robust autonomic self-management capabilities. While Large Language Models (LLMs) demonstrate potential in this domain, they often face challenges in adapting their general knowledge to specific service contexts. To address this limitation, we propose ServiceOdyssey, a self-learning agent system that autonomously manages microservices without requiring prior knowledge of service-specific configurations. By leveraging curriculum learning principles and iterative exploration, ServiceOdyssey progressively develops a deep understanding of operational environments, reducing dependence on human input or static documentation. A prototype built with the Sock Shop microservice demonstrates the potential of this approach for autonomic microservice management.

**Link**: [arxiv](http://arxiv.org/abs/2501.19056v1),  [pdf](http://arxiv.org/pdf/2501.19056v1)

**Tags**: cs.SE cs.AI cs.CL cs.MA 



### Text-to-CAD Generation Through Infusing Visual Feedback in Large   Language Models
**Authors**: Ruiyu Wang, Yu Yuan, Shizhao Sun, Jiang Bian

**Updated**: 2025-01-31T11:28:16Z

**Summary**: Creating Computer-Aided Design (CAD) models requires significant expertise and effort. Text-to-CAD, which converts textual descriptions into CAD parametric sequences, is crucial in streamlining this process. Recent studies have utilized ground-truth parametric sequences, known as sequential signals, as supervision to achieve this goal. However, CAD models are inherently multimodal, comprising parametric sequences and corresponding rendered visual objects. Besides,the rendering process from parametric sequences to visual objects is many-to-one. Therefore, both sequential and visual signals are critical for effective training. In this work, we introduce CADFusion, a framework that uses Large Language Models (LLMs) as the backbone and alternates between two training stages: the sequential learning (SL) stage and the visual feedback (VF) stage. In the SL stage, we train LLMs using ground-truth parametric sequences, enabling the generation of logically coherent parametric sequences. In the VF stage, we reward parametric sequences that render into visually preferred objects and penalize those that do not, allowing LLMs to learn how rendered visual objects are perceived and evaluated. These two stages alternate throughout the training, ensuring balanced learning and preserving benefits of both signals. Experiments demonstrate that CADFusion significantly improves performance, both qualitatively and quantitatively.

**Link**: [arxiv](http://arxiv.org/abs/2501.19054v1),  [pdf](http://arxiv.org/pdf/2501.19054v1)

**Tags**: cs.CV cs.LG 



### Evaluating the Reliability of Self-Explanations in Large Language Models
**Authors**: Korbinian Randl, John Pavlopoulos, Aron Henriksson, Tony Lindgren

**Updated**: 2025-01-31T11:16:51Z

**Summary**: This paper investigates the reliability of explanations generated by large language models (LLMs) when prompted to explain their previous output. We evaluate two kinds of such self-explanations - extractive and counterfactual - using three state-of-the-art LLMs (2B to 8B parameters) on two different classification tasks (objective and subjective). Our findings reveal, that, while these self-explanations can correlate with human judgement, they do not fully and accurately follow the model's decision process, indicating a gap between perceived and actual model reasoning. We show that this gap can be bridged because prompting LLMs for counterfactual explanations can produce faithful, informative, and easy-to-verify results. These counterfactuals offer a promising alternative to traditional explainability methods (e.g. SHAP, LIME), provided that prompts are tailored to specific tasks and checked for validity.

**Link**: [arxiv](http://arxiv.org/abs/2407.14487v2),  [pdf](http://arxiv.org/pdf/2407.14487v2)

**Tags**: cs.CL 



### Addressing Hallucinations with RAG and NMISS in Italian Healthcare LLM   Chatbots
**Authors**: Maria Paola Priola

**Updated**: 2025-01-31T11:14:25Z

**Summary**: I combine detection and mitigation techniques to addresses hallucinations in Large Language Models (LLMs). Mitigation is achieved in a question-answering Retrieval-Augmented Generation (RAG) framework while detection is obtained by introducing the Negative Missing Information Scoring System (NMISS), which accounts for contextual relevance in responses. While RAG mitigates hallucinations by grounding answers in external data, NMISS refines the evaluation by identifying cases where traditional metrics incorrectly flag contextually accurate responses as hallucinations. I use Italian health news articles as context to evaluate LLM performance. Results show that Gemma2 and GPT-4 outperform the other models, with GPT-4 producing answers closely aligned with reference responses. Mid-tier models, such as Llama2, Llama3, and Mistral benefit significantly from NMISS, highlighting their ability to provide richer contextual information. This combined approach offers new insights into the reduction and more accurate assessment of hallucinations in LLMs, with applications in real-world healthcare tasks and other domains.

**Link**: [arxiv](http://arxiv.org/abs/2412.04235v2),  [pdf](http://arxiv.org/pdf/2412.04235v2)

**Tags**: cs.CL 



### Towards the Worst-case Robustness of Large Language Models
**Authors**: Huanran Chen, Yinpeng Dong, Zeming Wei, Hang Su, Jun Zhu

**Updated**: 2025-01-31T11:10:49Z

**Summary**: Recent studies have revealed the vulnerability of Large Language Models (LLMs) to adversarial attacks, where the adversary crafts specific input sequences to induce harmful, violent, private, or incorrect outputs. Although various defenses have been proposed, they have not been evaluated by strong adaptive attacks, leaving the worst-case robustness of LLMs still intractable. By developing a stronger white-box attack, our evaluation results indicate that most typical defenses achieve nearly 0\% robustness.To solve this, we propose \textit{DiffTextPure}, a general defense that diffuses the (adversarial) input prompt using any pre-defined smoothing distribution, and purifies the diffused input using a pre-trained language model. Theoretically, we derive tight robustness lower bounds for all smoothing distributions using Fractal Knapsack or 0-1 Knapsack solvers. Under this framework, we certify the robustness of a specific case -- smoothing LLMs using a uniform kernel -- against \textit{any possible attack} with an average $\ell_0$ perturbation of 2.02 or an average suffix length of 6.41.

**Link**: [arxiv](http://arxiv.org/abs/2501.19040v1),  [pdf](http://arxiv.org/pdf/2501.19040v1)

**Tags**: cs.LG 



### Conformal Prediction in Hierarchical Classification
**Authors**: Thomas Mortier, Alireza Javanmardi, Yusuf Sale, Eyke Hüllermeier, Willem Waegeman

**Updated**: 2025-01-31T11:10:19Z

**Summary**: Conformal prediction has emerged as a widely used framework for constructing valid prediction sets in classification and regression tasks. In this work, we extend the split conformal prediction framework to hierarchical classification, where prediction sets are commonly restricted to internal nodes of a predefined hierarchy, and propose two computationally efficient inference algorithms. The first algorithm returns internal nodes as prediction sets, while the second relaxes this restriction, using the notion of representation complexity, yielding a more general and combinatorial inference problem, but smaller set sizes. Empirical evaluations on several benchmark datasets demonstrate the effectiveness of the proposed algorithms in achieving nominal coverage.

**Link**: [arxiv](http://arxiv.org/abs/2501.19038v1),  [pdf](http://arxiv.org/pdf/2501.19038v1)

**Tags**: stat.ML cs.LG 



### Mastering the Craft of Data Synthesis for CodeLLMs
**Authors**: Meng Chen, Philip Arthur, Qianyu Feng, Cong Duy Vu Hoang, Yu-Heng Hong, Mahdi Kazemi Moghaddam, Omid Nezami, Thien Nguyen, Gioacchino Tangari, Duy Vu, Thanh Vu, Mark Johnson, Krishnaram Kenthapadi, Don Dharmasiri, Long Duong, Yuan-Fang Li

**Updated**: 2025-01-31T10:45:01Z

**Summary**: Large language models (LLMs) have shown impressive performance in \emph{code} understanding and generation, making coding tasks a key focus for researchers due to their practical applications and value as a testbed for LLM evaluation. Data synthesis and filtering techniques have been widely adopted and shown to be highly effective in this context. In this paper, we present a focused survey and taxonomy of these techniques, emphasizing recent advancements. We highlight key challenges, explore future research directions, and offer practical guidance for new researchers entering the field.

**Link**: [arxiv](http://arxiv.org/abs/2411.00005v2),  [pdf](http://arxiv.org/pdf/2411.00005v2)

**Tags**: cs.SE cs.AI 



### Importing Phantoms: Measuring LLM Package Hallucination Vulnerabilities
**Authors**: Arjun Krishna, Erick Galinkin, Leon Derczynski, Jeffrey Martin

**Updated**: 2025-01-31T10:26:18Z

**Summary**: Large Language Models (LLMs) have become an essential tool in the programmer's toolkit, but their tendency to hallucinate code can be used by malicious actors to introduce vulnerabilities to broad swathes of the software supply chain. In this work, we analyze package hallucination behaviour in LLMs across popular programming languages examining both existing package references and fictional dependencies. By analyzing this package hallucination behaviour we find potential attacks and suggest defensive strategies to defend against these attacks. We discover that package hallucination rate is predicated not only on model choice, but also programming language, model size, and specificity of the coding task request. The Pareto optimality boundary between code generation performance and package hallucination is sparsely populated, suggesting that coding models are not being optimized for secure code. Additionally, we find an inverse correlation between package hallucination rate and the HumanEval coding benchmark, offering a heuristic for evaluating the propensity of a model to hallucinate packages. Our metrics, findings and analyses provide a base for future models, securing AI-assisted software development workflows against package supply chain attacks.

**Link**: [arxiv](http://arxiv.org/abs/2501.19012v1),  [pdf](http://arxiv.org/pdf/2501.19012v1)

**Tags**: cs.LG cs.CL cs.CR 



### Compositional Hardness of Code in Large Language Models -- A   Probabilistic Perspective
**Authors**: Yotam Wolf, Binyamin Rothberg, Dorin Shteyman, Amnon Shashua

**Updated**: 2025-01-31T10:15:43Z

**Summary**: A common practice in large language model (LLM) usage for complex analytical tasks such as code generation, is to sample a solution for the entire task within the model's context window. Previous works have shown that subtask decomposition within the model's context (chain of thought), is beneficial for solving such tasks. In this work, we point a limitation of LLMs' ability to perform several sub-tasks within the same context window - an in-context hardness of composition, pointing to an advantage for distributing a decomposed problem in a multi-agent system of LLMs. The hardness of composition is quantified by a generation complexity metric, i.e., the number of LLM generations required to sample at least one correct solution. We find a gap between the generation complexity of solving a compositional problem within the same context relative to distributing it among multiple agents, that increases exponentially with the solution's length. We prove our results theoretically and demonstrate them empirically.

**Link**: [arxiv](http://arxiv.org/abs/2409.18028v3),  [pdf](http://arxiv.org/pdf/2409.18028v3)

**Tags**: cs.AI cs.CL 



## Keyword: LLM Deployment 
 ### LLMs Are In-Context Bandit Reinforcement Learners
**Authors**: Giovanni Monea, Antoine Bosselut, Kianté Brantley, Yoav Artzi

**Updated**: 2025-01-31T18:58:24Z

**Summary**: Large Language Models (LLMs) excel at in-context learning (ICL), a supervised learning technique that relies on adding annotated examples to the model context. We investigate a contextual bandit version of in-context reinforcement learning (ICRL), where models learn in-context, online, from external reward, instead of supervised data. We show that LLMs effectively demonstrate such learning, and provide a detailed study of the phenomena, experimenting with challenging classification tasks and models of sizes from 500M to 70B parameters. This includes identifying and addressing the instability of the process, demonstrating learning with both semantic and abstract labels, and showing scaling trends. Our findings highlight ICRL capabilities in LLMs, while also underscoring fundamental limitations in their implicit reasoning about errors.

**Link**: [arxiv](http://arxiv.org/abs/2410.05362v2),  [pdf](http://arxiv.org/pdf/2410.05362v2)

**Tags**: cs.CL cs.AI cs.LG 



### Do LLMs Strategically Reveal, Conceal, and Infer Information? A   Theoretical and Empirical Analysis in The Chameleon Game
**Authors**: Mustafa O. Karabag, Ufuk Topcu

**Updated**: 2025-01-31T18:53:43Z

**Summary**: Large language model-based (LLM-based) agents have become common in settings that include non-cooperative parties. In such settings, agents' decision-making needs to conceal information from their adversaries, reveal information to their cooperators, and infer information to identify the other agents' characteristics. To investigate whether LLMs have these information control and decision-making capabilities, we make LLM agents play the language-based hidden-identity game, The Chameleon. In the game, a group of non-chameleon agents who do not know each other aim to identify the chameleon agent without revealing a secret. The game requires the aforementioned information control capabilities both as a chameleon and a non-chameleon. The empirical results show that while non-chameleon LLM agents identify the chameleon, they fail to conceal the secret from the chameleon, and their winning probability is far from the levels of even trivial strategies. To formally explain this behavior, we give a theoretical analysis for a spectrum of strategies, from concealing to revealing, and provide bounds on the non-chameleons' winning probability. Based on the empirical results and theoretical analysis of different strategies, we deduce that LLM-based non-chameleon agents reveal excessive information to agents of unknown identities. Our results point to a weakness of contemporary LLMs, including GPT-4, GPT-4o, Gemini 1.5, and Claude 3.5 Sonnet, in strategic interactions.

**Link**: [arxiv](http://arxiv.org/abs/2501.19398v1),  [pdf](http://arxiv.org/pdf/2501.19398v1)

**Tags**: cs.AI cs.GT cs.LG 



### Contextual Emotion Recognition using Large Vision Language Models
**Authors**: Yasaman Etesam, Özge Nilay Yalçın, Chuxuan Zhang, Angelica Lim

**Updated**: 2025-01-31T18:53:30Z

**Summary**: "How does the person in the bounding box feel?" Achieving human-level recognition of the apparent emotion of a person in real world situations remains an unsolved task in computer vision. Facial expressions are not enough: body pose, contextual knowledge, and commonsense reasoning all contribute to how humans perform this emotional theory of mind task. In this paper, we examine two major approaches enabled by recent large vision language models: 1) image captioning followed by a language-only LLM, and 2) vision language models, under zero-shot and fine-tuned setups. We evaluate the methods on the Emotions in Context (EMOTIC) dataset and demonstrate that a vision language model, fine-tuned even on a small dataset, can significantly outperform traditional baselines. The results of this work aim to help robots and agents perform emotionally sensitive decision-making and interaction in the future.

**Link**: [arxiv](http://arxiv.org/abs/2405.08992v2),  [pdf](http://arxiv.org/pdf/2405.08992v2)

**Tags**: cs.CV 



### Cache Me If You Must: Adaptive Key-Value Quantization for Large Language   Models
**Authors**: Alina Shutova, Vladimir Malinovskii, Vage Egiazarian, Denis Kuznedelev, Denis Mazur, Nikita Surkov, Ivan Ermakov, Dan Alistarh

**Updated**: 2025-01-31T18:47:42Z

**Summary**: Efficient real-world deployments of large language models (LLMs) rely on Key-Value (KV) caching for processing and generating long outputs, reducing the need for repetitive computation. For large contexts, Key-Value caches can take up tens of gigabytes of device memory, as they store vector representations for each token and layer. Recent work has shown that the cached vectors can be compressed through quantization, pruning or merging, but these techniques often compromise quality towards higher compression rates. In this work, we aim to improve Key & Value compression by exploiting two observations: 1) the inherent dependencies between keys and values across different layers, and 2) high-compression mechanisms for internal network states. We propose AQUA-KV, an adaptive quantization for Key-Value caches that relies on compact adapters to exploit existing dependencies between Keys and Values, and aims to "optimally" compress the information that cannot be predicted. AQUA-KV significantly improves compression rates, while maintaining high accuracy on state-of-the-art LLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5 bits per value with under $1\%$ relative error in perplexity and LongBench scores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a single GPU within 1-6 hours, even for 70B models.

**Link**: [arxiv](http://arxiv.org/abs/2501.19392v1),  [pdf](http://arxiv.org/pdf/2501.19392v1)

**Tags**: cs.LG 



### Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large   Language Models
**Authors**: Wenzhi Fang, Dong-Jun Han, Liangqi Yuan, Seyyedali Hosseinalipour, Christopher G. Brinton

**Updated**: 2025-01-31T18:44:35Z

**Summary**: Fine-tuning large language models (LLMs) on devices is attracting increasing interest. Recent works have fused low-rank adaptation (LoRA) techniques with federated fine-tuning to mitigate challenges associated with device model sizes and data scarcity. Still, the heterogeneity of computational resources remains a critical bottleneck: while higher-rank modules generally enhance performance, varying device capabilities constrain LoRA's feasible rank range. Existing approaches attempting to resolve this issue either lack analytical justification or impose additional computational overhead, leaving a wide gap for an efficient and theoretically-grounded solution. To address these challenges, we propose federated sketching LoRA (FSLoRA), which leverages a sketching mechanism to enable devices to selectively update submatrices of global LoRA modules maintained by the server. By adjusting the sketching ratios, which determine the ranks of the submatrices on the devices, FSLoRA flexibly adapts to device-specific communication and computational constraints. We provide a rigorous convergence analysis of FSLoRA that characterizes how the sketching ratios affect the convergence rate. Through comprehensive experiments on multiple datasets and LLM models, we demonstrate FSLoRA's superior performance compared to various baselines.

**Link**: [arxiv](http://arxiv.org/abs/2501.19389v1),  [pdf](http://arxiv.org/pdf/2501.19389v1)

**Tags**: cs.LG 



### SELMA: A Speech-Enabled Language Model for Virtual Assistant   Interactions
**Authors**: Dominik Wagner, Alexander Churchill, Siddarth Sigtia, Erik Marchi

**Updated**: 2025-01-31T18:30:36Z

**Summary**: In this work, we present and evaluate SELMA, a Speech-Enabled Language Model for virtual Assistant interactions that integrates audio and text as inputs to a Large Language Model (LLM). SELMA is designed to handle three primary and two auxiliary tasks related to interactions with virtual assistants simultaneously within a single end-to-end model. We employ low-rank adaptation modules for parameter-efficient training of both the audio encoder and the LLM. Additionally, we implement a feature pooling strategy enabling the system to recognize global patterns and improve accuracy on tasks less reliant on individual sequence elements. Experimental results on Voice Trigger (VT) detection, Device-Directed Speech Detection (DDSD), and Automatic Speech Recognition (ASR), demonstrate that our approach both simplifies the typical input processing pipeline of virtual assistants significantly and also improves performance compared to dedicated models for each individual task. SELMA yields relative Equal-Error Rate improvements of 64% on the VT detection task, and 22% on DDSD, while also achieving word error rates close to the baseline.

**Link**: [arxiv](http://arxiv.org/abs/2501.19377v1),  [pdf](http://arxiv.org/pdf/2501.19377v1)

**Tags**: cs.SD cs.CL cs.LG eess.AS 



### GPT-4o as the Gold Standard: A Scalable and General Purpose Approach to   Filter Language Model Pretraining Data
**Authors**: Jifan Zhang, Ziyue Luo, Jia Liu, Ness Shroff, Robert Nowak

**Updated**: 2025-01-31T18:21:59Z

**Summary**: Large language models require vast amounts of high-quality training data, but effective filtering of web-scale datasets remains a significant challenge. This paper demonstrates that GPT-4o is remarkably effective at identifying high-quality training data, but its prohibitive cost makes it impractical at web-scale. We propose SIEVE, a lightweight alternative that matches GPT-4o accuracy at less than 1\% of the cost. SIEVE can perform up to 500 filtering operations for the cost of one GPT-4o filtering call. The key to SIEVE is a seamless integration of GPT-4o and lightweight text classification models, using active learning to fine-tune these models in the background with a small number of calls to GPT-4o. Once trained, it performs as well as GPT-4o at a tiny fraction of the cost. Through different filtering prompts, SIEVE can efficiently curate high quality data for general or specialized domains from web-scale corpora -- a valuable capability given the current scarcity of high-quality domain-specific datasets. Extensive experiments using automatic and human evaluation metrics show that SIEVE and GPT-4o achieve similar performance on five highly specific filtering prompts. In addition, when performing quality filtering on web crawl datasets, we demonstrate SIEVE can further improve over state-of-the-art quality filtering methods in the DataComp-LM challenge for selecting LLM pretraining data.

**Link**: [arxiv](http://arxiv.org/abs/2410.02755v3),  [pdf](http://arxiv.org/pdf/2410.02755v3)

**Tags**: cs.CL cs.LG 



### We're Different, We're the Same: Creative Homogeneity Across LLMs
**Authors**: Emily Wenger, Yoed Kenett

**Updated**: 2025-01-31T18:12:41Z

**Summary**: Numerous powerful large language models (LLMs) are now available for use as writing support tools, idea generators, and beyond. Although these LLMs are marketed as helpful creative assistants, several works have shown that using an LLM as a creative partner results in a narrower set of creative outputs. However, these studies only consider the effects of interacting with a single LLM, begging the question of whether such narrowed creativity stems from using a particular LLM -- which arguably has a limited range of outputs -- or from using LLMs in general as creative assistants. To study this question, we elicit creative responses from humans and a broad set of LLMs using standardized creativity tests and compare the population-level diversity of responses. We find that LLM responses are much more similar to other LLM responses than human responses are to each other, even after controlling for response structure and other key variables. This finding of significant homogeneity in creative outputs across the LLMs we evaluate adds a new dimension to the ongoing conversation about creativity and LLMs. If today's LLMs behave similarly, using them as a creative partners -- regardless of the model used -- may drive all users towards a limited set of "creative" outputs.

**Link**: [arxiv](http://arxiv.org/abs/2501.19361v1),  [pdf](http://arxiv.org/pdf/2501.19361v1)

**Tags**: cs.CY cs.AI cs.CL cs.LG 



### The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating   Reward Hacking
**Authors**: Yuchun Miao, Sen Zhang, Liang Ding, Yuqi Zhang, Lefei Zhang, Dacheng Tao

**Updated**: 2025-01-31T18:10:53Z

**Summary**: This work identifies the Energy Loss Phenomenon in Reinforcement Learning from Human Feedback (RLHF) and its connection to reward hacking. Specifically, energy loss in the final layer of a Large Language Model (LLM) gradually increases during the RL process, with an excessive increase in energy loss characterizing reward hacking. Beyond empirical analysis, we further provide a theoretical foundation by proving that, under mild conditions, the increased energy loss reduces the upper bound of contextual relevance in LLMs, which is a critical aspect of reward hacking as the reduced contextual relevance typically indicates overfitting to reward model-favored patterns in RL. To address this issue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes the increase in energy loss in the LLM's final layer during reward calculation to prevent excessive energy loss, thereby mitigating reward hacking. We theoretically show that EPPO can be conceptually interpreted as an entropy-regularized RL algorithm, which provides deeper insights into its effectiveness. Extensive experiments across various LLMs and tasks demonstrate the commonality of the energy loss phenomenon, as well as the effectiveness of \texttt{EPPO} in mitigating reward hacking and improving RLHF performance.

**Link**: [arxiv](http://arxiv.org/abs/2501.19358v1),  [pdf](http://arxiv.org/pdf/2501.19358v1)

**Tags**: cs.LG 



### SafetyAnalyst: Interpretable, transparent, and steerable safety   moderation for AI behavior
**Authors**: Jing-Jing Li, Valentina Pyatkin, Max Kleiman-Weiner, Liwei Jiang, Nouha Dziri, Anne G. E. Collins, Jana Schaich Borg, Maarten Sap, Yejin Choi, Sydney Levine

**Updated**: 2025-01-31T18:01:12Z

**Summary**: The ideal AI safety moderation system would be both structurally interpretable (so its decisions can be reliably explained) and steerable (to align to safety standards and reflect a community's values), which current systems fall short on. To address this gap, we present SafetyAnalyst, a novel AI safety moderation framework. Given an AI behavior, SafetyAnalyst uses chain-of-thought reasoning to analyze its potential consequences by creating a structured "harm-benefit tree," which enumerates harmful and beneficial actions and effects the AI behavior may lead to, along with likelihood, severity, and immediacy labels that describe potential impact on any stakeholders. SafetyAnalyst then aggregates all harmful and beneficial effects into a harmfulness score using fully interpretable weight parameters, which can be aligned to particular safety preferences. We applied this conceptual framework to develop, test, and release an open-source LLM prompt safety classification system, distilled from 18.5 million harm-benefit features generated by frontier LLMs on 19k prompts. On a comprehensive set of prompt safety benchmarks, we show that SafetyReporter (average F1=0.81) outperforms existing LLM safety moderation systems (average F1$<$0.72) on prompt safety classification, while offering the additional advantages of interpretability, transparency, and steerability.

**Link**: [arxiv](http://arxiv.org/abs/2410.16665v2),  [pdf](http://arxiv.org/pdf/2410.16665v2)

**Tags**: cs.CL cs.CY 



### Towards Adaptive Self-Improvement for Smarter Energy Systems
**Authors**: Alexander Sommer, Peter Bazan, Jonathan Fellerer, Behnam Babaeian, Reinhard German

**Updated**: 2025-01-31T17:40:08Z

**Summary**: This paper introduces a hierarchical framework for decision-making and optimization, leveraging Large Language Models (LLMs) for adaptive code generation. Instead of direct decision-making, LLMs generate and refine executable control policies through a meta-policy that guides task generation and a base policy for operational actions. Applied to a simplified microgrid scenario, the approach achieves up to 15 percent cost savings by iteratively improving battery control strategies. The proposed methodology lays a foundation for integrating LLM-based tools into planning and control tasks, offering adaptable and scalable solutions for complex systems while addressing challenges of uncertainty and reproducibility.

**Link**: [arxiv](http://arxiv.org/abs/2501.19340v1),  [pdf](http://arxiv.org/pdf/2501.19340v1)

**Tags**: eess.SY cs.SY 



### FlexiGPT: Pruning and Extending Large Language Models with Low-Rank   Weight Sharing
**Authors**: James Seale Smith, Chi-Heng Lin, Shikhar Tuli, Haris Jeelani, Shangqian Gao, Yilin Shen, Hongxia Jin, Yen-Chang Hsu

**Updated**: 2025-01-31T17:38:07Z

**Summary**: The rapid proliferation of large language models (LLMs) in natural language processing (NLP) has created a critical need for techniques that enable efficient deployment on memory-constrained devices without compromising performance. We present a method to prune LLMs that selectively prunes model blocks based on an importance score and replaces them with a low-parameter replacement strategy. Specifically, we propose a principled metric to replace each pruned block using a weight-sharing mechanism that leverages unpruned counterparts from the model and block-specific low-rank adapters. Furthermore, we facilitate the learning of these replacement blocks with output feature normalization and an adapter initialization scheme built on low-rank SVD reconstructions. Empirical evaluations demonstrate substantial performance gains over existing methods, achieving state-of-the-art performance on 5/6 benchmarks for a compression rate of 30% and 6/6 benchmarks for a compression rate of 40%. We also demonstrate that our approach can extend smaller models, boosting performance on 6/6 benchmarks using only ~0.3% tokens of extended training with minimal additional parameter costs.

**Link**: [arxiv](http://arxiv.org/abs/2501.14713v2),  [pdf](http://arxiv.org/pdf/2501.14713v2)

**Tags**: cs.CL cs.LG 



### Homogeneity Bias as Differential Sampling Uncertainty in Language Models
**Authors**: Messi H. J. Lee, Soyeon Jeon

**Updated**: 2025-01-31T17:36:12Z

**Summary**: Prior research show that Large Language Models (LLMs) and Vision-Language Models (VLMs) represent marginalized groups more homogeneously than dominant groups. However, the mechanisms underlying this homogeneity bias remain relatively unexplored. We propose that this bias emerges from systematic differences in the probability distributions from which tokens are sampled at inference-time. Analyzing three measures of uncertainty in token sampling distributions-entropy, perplexity, and probability of differentiation-we find that in some models, specifically GPT-4 Turbo and Llama-3.2, tokens are sampled more deterministically when generating texts about marginalized groups (i.e., Black Americans and women) compared to their dominant group counterparts (i.e., White Americans and men). While these findings may help explain homogeneity bias in certain models, the patterns did not replicate across all VLMs tested, suggesting multiple mechanisms may contribute to homogeneity bias in AI.

**Link**: [arxiv](http://arxiv.org/abs/2501.19337v1),  [pdf](http://arxiv.org/pdf/2501.19337v1)

**Tags**: cs.CL cs.CV 



### From Natural Language to Extensive-Form Game Representations
**Authors**: Shilong Deng, Yongzhao Wang, Rahul Savani

**Updated**: 2025-01-31T17:26:12Z

**Summary**: We introduce a framework for translating game descriptions in natural language into extensive-form representations in game theory, leveraging Large Language Models (LLMs) and in-context learning. Given the varying levels of strategic complexity in games, such as perfect versus imperfect information, directly applying in-context learning would be insufficient. To address this, we introduce a two-stage framework with specialized modules to enhance in-context learning, enabling it to divide and conquer the problem effectively. In the first stage, we tackle the challenge of imperfect information by developing a module that identifies information sets along and the corresponding partial tree structure. With this information, the second stage leverages in-context learning alongside a self-debugging module to produce a complete extensive-form game tree represented using pygambit, the Python API of a recognized game-theoretic analysis tool called Gambit. Using this python representation enables the automation of tasks such as computing Nash equilibria directly from natural language descriptions. We evaluate the performance of the full framework, as well as its individual components, using various LLMs on games with different levels of strategic complexity. Our experimental results show that the framework significantly outperforms baseline models in generating accurate extensive-form games, with each module playing a critical role in its success.

**Link**: [arxiv](http://arxiv.org/abs/2501.17282v3),  [pdf](http://arxiv.org/pdf/2501.17282v3)

**Tags**: cs.AI cs.CL cs.GT cs.MA 



### Reward-Guided Speculative Decoding for Efficient LLM Reasoning
**Authors**: Baohao Liao, Yuhui Xu, Hanze Dong, Junnan Li, Christof Monz, Silvio Savarese, Doyen Sahoo, Caiming Xiong

**Updated**: 2025-01-31T17:19:57Z

**Summary**: We introduce Reward-Guided Speculative Decoding (RSD), a novel framework aimed at improving the efficiency of inference in large language models (LLMs). RSD synergistically combines a lightweight draft model with a more powerful target model, incorporating a controlled bias to prioritize high-reward outputs, in contrast to existing speculative decoding methods that enforce strict unbiasedness. RSD employs a process reward model to evaluate intermediate decoding steps and dynamically decide whether to invoke the target model, optimizing the trade-off between computational cost and output quality. We theoretically demonstrate that a threshold-based mixture strategy achieves an optimal balance between resource utilization and performance. Extensive evaluations on challenging reasoning benchmarks, including Olympiad-level tasks, show that RSD delivers significant efficiency gains against decoding with the target model only (up to 4.4x fewer FLOPs), while achieving significant better accuracy than parallel decoding method on average (up to +3.5). These results highlight RSD as a robust and cost-effective approach for deploying LLMs in resource-intensive scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2501.19324v1),  [pdf](http://arxiv.org/pdf/2501.19324v1)

**Tags**: cs.CL cs.AI 



### MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented   Reinforcement in Embodied Systems
**Authors**: Anirudh Chari, Suraj Reddy, Aditya Tiwari, Richard Lian, Brian Zhou

**Updated**: 2025-01-31T17:15:33Z

**Summary**: While large language models (LLMs) have shown promising capabilities as zero-shot planners for embodied agents, their inability to learn from experience and build persistent mental models limits their robustness in complex open-world environments like Minecraft. We introduce MINDSTORES, an experience-augmented planning framework that enables embodied agents to build and leverage mental models through natural interaction with their environment. Drawing inspiration from how humans construct and refine cognitive mental models, our approach extends existing zero-shot LLM planning by maintaining a database of past experiences that informs future planning iterations. The key innovation is representing accumulated experiences as natural language embeddings of (state, task, plan, outcome) tuples, which can then be efficiently retrieved and reasoned over by an LLM planner to generate insights and guide plan refinement for novel states and tasks. Through extensive experiments in the MineDojo environment, a simulation environment for agents in Minecraft that provides low-level controls for Minecraft, we find that MINDSTORES learns and applies its knowledge significantly better than existing memory-based LLM planners while maintaining the flexibility and generalization benefits of zero-shot approaches, representing an important step toward more capable embodied AI systems that can learn continuously through natural experience.

**Link**: [arxiv](http://arxiv.org/abs/2501.19318v1),  [pdf](http://arxiv.org/pdf/2501.19318v1)

**Tags**: cs.AI 



### LLM-based Affective Text Generation Quality Based on Different   Quantization Values
**Authors**: Yarik Menchaca Resendiz, Roman Klinger

**Updated**: 2025-01-31T17:12:55Z

**Summary**: Large language models exhibit a remarkable capacity in language generation and comprehension. These advances enable AI systems to produce more human-like and emotionally engaging text. However, these models rely on a large number of parameters, requiring significant computational resources for training and inference. In some scenarios, accessing these resources can be challenging (e.g., budget or hardware limitations). Techniques like reducing precision bits can make models more memory-efficient, reducing the computational resources needed, at the cost of reduced accuracy. This paper addresses the trade-off between different quantization values, GPU RAM utilization, and text quality in affective text generation (e.g., "I really enjoy running in the snow-covered forest"). To evaluate, we use an emotion classifier and ten seed prompts to generate affective text. We test three setups of precision bits (8, 16, and 32) across five open-weight language models from two different families. Our findings demonstrate that bit reductions lead to memory savings, achieving a reduction of 76%. However, this optimization comes with a trade-off, leading to a decrease of up to 10 pp in F1 score for larger models and an increase of 10 pp for smaller models, along with roughly double the inference time. In terms of text quality, larger models at lower quantization levels generally outperform smaller, higher-precision models -- while requiring similar memory.

**Link**: [arxiv](http://arxiv.org/abs/2501.19317v1),  [pdf](http://arxiv.org/pdf/2501.19317v1)

**Tags**: cs.CL 



### Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model   Alignment
**Authors**: Gregor Bachmann, Sotiris Anagnostidis, Albert Pumarola, Markos Georgopoulos, Artsiom Sanakoyeu, Yuming Du, Edgar Schönfeld, Ali Thabet, Jonas Kohler

**Updated**: 2025-01-31T17:09:53Z

**Summary**: The performance of large language models (LLMs) is closely linked to their underlying size, leading to ever-growing networks and hence slower inference. Speculative decoding has been proposed as a technique to accelerate autoregressive generation, leveraging a fast draft model to propose candidate tokens, which are then verified in parallel based on their likelihood under the target model. While this approach guarantees to reproduce the target output, it incurs a substantial penalty: many high-quality draft tokens are rejected, even when they represent objectively valid continuations. Indeed, we show that even powerful draft models such as GPT-4o, as well as human text cannot achieve high acceptance rates under the standard verification scheme. This severely limits the speedup potential of current speculative decoding methods, as an early rejection becomes overwhelmingly likely when solely relying on alignment of draft and target.   We thus ask the following question: Can we adapt verification to recognize correct, but non-aligned replies? To this end, we draw inspiration from the LLM-as-a-judge framework, which demonstrated that LLMs are able to rate answers in a versatile way. We carefully design a dataset to elicit the same capability in the target model by training a compact module on top of the embeddings to produce ``judgements" of the current continuation. We showcase our strategy on the Llama-3.1 family, where our 8b/405B-Judge achieves a speedup of 9x over Llama-405B, while maintaining its quality on a large range of benchmarks. These benefits remain present even in optimized inference frameworks, where our method reaches up to 141 tokens/s for 8B/70B-Judge and 129 tokens/s for 8B/405B on 2 and 8 H100s respectively.

**Link**: [arxiv](http://arxiv.org/abs/2501.19309v1),  [pdf](http://arxiv.org/pdf/2501.19309v1)

**Tags**: cs.LG cs.CL 



### SETS: Leveraging Self-Verification and Self-Correction for Improved   Test-Time Scaling
**Authors**: Jiefeng Chen, Jie Ren, Xinyun Chen, Chengrun Yang, Ruoxi Sun, Sercan Ö Arık

**Updated**: 2025-01-31T17:03:16Z

**Summary**: Recent advancements in Large Language Models (LLMs) have created new opportunities to enhance performance on complex reasoning tasks by leveraging test-time computation. However, conventional approaches such as repeated sampling with majority voting or reward model scoring, often face diminishing returns as test-time compute scales, in addition to requiring costly task-specific reward model training. In this paper, we present Self-Enhanced Test-Time Scaling (SETS), a novel method that leverages the self-verification and self-correction capabilities of recent advanced LLMs to overcome these limitations. SETS integrates sampling, self-verification, and self-correction into a unified framework, enabling efficient and scalable test-time computation for improved capabilities at complex tasks. Through extensive experiments on challenging planning and reasoning benchmarks, compared to the alternatives, we demonstrate that SETS achieves significant performance improvements and more favorable test-time scaling laws.

**Link**: [arxiv](http://arxiv.org/abs/2501.19306v1),  [pdf](http://arxiv.org/pdf/2501.19306v1)

**Tags**: cs.AI cs.CL 



### Beyond checkmate: exploring the creative chokepoints in AI text
**Authors**: Nafis Irtiza Tripto, Saranya Venkatraman, Mahjabin Nahar, Dongwon Lee

**Updated**: 2025-01-31T16:57:01Z

**Summary**: Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) and Artificial Intelligence (AI), unlocking unprecedented capabilities. This rapid advancement has spurred research into various aspects of LLMs, their text generation & reasoning capability, and potential misuse, fueling the necessity for robust detection methods. While numerous prior research has focused on detecting LLM-generated text (AI text) and thus checkmating them, our study investigates a relatively unexplored territory: portraying the nuanced distinctions between human and AI texts across text segments. Whether LLMs struggle with or excel at incorporating linguistic ingenuity across different text segments carries substantial implications for determining their potential as effective creative assistants to humans. Through an analogy with the structure of chess games-comprising opening, middle, and end games-we analyze text segments (introduction, body, and conclusion) to determine where the most significant distinctions between human and AI texts exist. While AI texts can approximate the body segment better due to its increased length, a closer examination reveals a pronounced disparity, highlighting the importance of this segment in AI text detection. Additionally, human texts exhibit higher cross-segment differences compared to AI texts. Overall, our research can shed light on the intricacies of human-AI text distinctions, offering novel insights for text detection and understanding.

**Link**: [arxiv](http://arxiv.org/abs/2501.19301v1),  [pdf](http://arxiv.org/pdf/2501.19301v1)

**Tags**: cs.CL cs.AI 



### Offline Learning for Combinatorial Multi-armed Bandits
**Authors**: Xutong Liu, Xiangxiang Dai, Jinhang Zuo, Siwei Wang, Carlee-Joe Wong, John C. S. Lui, Wei Chen

**Updated**: 2025-01-31T16:56:18Z

**Summary**: The combinatorial multi-armed bandit (CMAB) is a fundamental sequential decision-making framework, extensively studied over the past decade. However, existing work primarily focuses on the online setting, overlooking the substantial costs of online interactions and the readily available offline datasets. To overcome these limitations, we introduce Off-CMAB, the first offline learning framework for CMAB. Central to our framework is the combinatorial lower confidence bound (CLCB) algorithm, which combines pessimistic reward estimations with combinatorial solvers. To characterize the quality of offline datasets, we propose two novel data coverage conditions and prove that, under these conditions, CLCB achieves a near-optimal suboptimality gap, matching the theoretical lower bound up to a logarithmic factor. We validate Off-CMAB through practical applications, including learning to rank, large language model (LLM) caching, and social influence maximization, showing its ability to handle nonlinear reward functions, general feedback models, and out-of-distribution action samples that excludes optimal or even feasible actions. Extensive experiments on synthetic and real-world datasets further highlight the superior performance of CLCB.

**Link**: [arxiv](http://arxiv.org/abs/2501.19300v1),  [pdf](http://arxiv.org/pdf/2501.19300v1)

**Tags**: cs.LG 



### Synthetic User Behavior Sequence Generation with Large Language Models   for Smart Homes
**Authors**: Zhiyao Xu, Dan Zhao, Qingsong Zou, Jingyu Xiao, Yong Jiang, Zhenhui Yuan, Qing Li

**Updated**: 2025-01-31T16:55:43Z

**Summary**: In recent years, as smart home systems have become more widespread, security concerns within these environments have become a growing threat. Currently, most smart home security solutions, such as anomaly detection and behavior prediction models, are trained using fixed datasets that are precollected. However, the process of dataset collection is time-consuming and lacks the flexibility needed to adapt to the constantly evolving smart home environment. Additionally, the collection of personal data raises significant privacy concerns for users. Lately, large language models (LLMs) have emerged as a powerful tool for a wide range of tasks across diverse application domains, thanks to their strong capabilities in natural language processing, reasoning, and problem-solving. In this paper, we propose an LLM-based synthetic dataset generation IoTGen framework to enhance the generalization of downstream smart home intelligent models. By generating new synthetic datasets that reflect changes in the environment, smart home intelligent models can be retrained to overcome the limitations of fixed and outdated data, allowing them to better align with the dynamic nature of real-world home environments. Specifically, we first propose a Structure Pattern Perception Compression (SPPC) method tailored for IoT behavior data, which preserves the most informative content in the data while significantly reducing token consumption. Then, we propose a systematic approach to create prompts and implement data generation to automatically generate IoT synthetic data with normative and reasonable properties, assisting task models in adaptive training to improve generalization and real-world performance.

**Link**: [arxiv](http://arxiv.org/abs/2501.19298v1),  [pdf](http://arxiv.org/pdf/2501.19298v1)

**Tags**: cs.AI cs.LG cs.NI 



### Analysis of LLMs vs Human Experts in Requirements Engineering
**Authors**: Cory Hymel, Hiroe Johnson

**Updated**: 2025-01-31T16:55:17Z

**Summary**: The majority of research around Large Language Models (LLM) application to software development has been on the subject of code generation. There is little literature on LLMs' impact on requirements engineering (RE), which deals with the process of developing and verifying the system requirements. Within RE, there is a subdiscipline of requirements elicitation, which is the practice of discovering and documenting requirements for a system from users, customers, and other stakeholders. In this analysis, we compare LLM's ability to elicit requirements of a software system, as compared to that of a human expert in a time-boxed and prompt-boxed study. We found LLM-generated requirements were evaluated as more aligned (+1.12) than human-generated requirements with a trend of being more complete (+10.2%). Conversely, we found users tended to believe that solutions they perceived as more aligned had been generated by human experts. Furthermore, while LLM-generated documents scored higher and performed at 720x the speed, their cost was, on average, only 0.06% that of a human expert. Overall, these findings indicate that LLMs will play an increasingly important role in requirements engineering by improving requirements definitions, enabling more efficient resource allocation, and reducing overall project timelines.

**Link**: [arxiv](http://arxiv.org/abs/2501.19297v1),  [pdf](http://arxiv.org/pdf/2501.19297v1)

**Tags**: cs.SE cs.AI 



### Differentially Private In-context Learning via Sampling Few-shot Mixed   with Zero-shot Outputs
**Authors**: James Flemings, Haosheng Gan, Hongyi Li, Meisam Razaviyayn, Murali Annavaram

**Updated**: 2025-01-31T16:48:38Z

**Summary**: In-context learning (ICL) has shown promising improvement in downstream task adaptation of LLMs by augmenting prompts with relevant input-output examples (demonstrations). However, the ICL demonstrations can contain privacy-sensitive information, which can be leaked and/or regurgitated by the LLM output. Differential Privacy (DP), a widely adopted privacy safeguard, has emerged to mitigate this privacy leakage, with recent work demonstrating strong privacy-utility tradeoffs in classification tasks for ICL. However, generation tasks for ICL are challenging due to the high-dimensional output space of open-ended generation. To this end, we propose $\texttt{dps-mozo}$, Differentially Private Sampling by Mixing One-shot with Zero-shot Outputs, a decoding framework that generates DP text by sampling from the product of multiple one-shot outputs mixed with a zero-shot output. This mixing effectively reduces the amount of information that can be leaked by each demonstration. By utilizing the inherent randomness in sampling from the mixed distributions, we can achieve DP without adding noise, thereby improving the privacy-utility tradeoff. Our experimental evaluations show $\texttt{dps-mozo}$ can achieve a strong privacy guarantee, $\epsilon=2$, with minimal utility degradation compared to non-private few-shot learning, $\textbf{0.3}$% ROUGE-L F1 score decrease on the SAMSum dataset with Gemma 2 2B.

**Link**: [arxiv](http://arxiv.org/abs/2501.19287v1),  [pdf](http://arxiv.org/pdf/2501.19287v1)

**Tags**: cs.LG 



### Low-Cost and Comprehensive Non-textual Input Fuzzing with   LLM-Synthesized Input Generators
**Authors**: Kunpeng Zhang, Zongjie Li, Daoyuan Wu, Shuai Wang, Xin Xia

**Updated**: 2025-01-31T16:45:16Z

**Summary**: Modern software often accepts inputs with highly complex grammars. Recent advances in large language models (LLMs) have shown that they can be used to synthesize high-quality natural language text and code that conforms to the grammar of a given input format. Nevertheless, LLMs are often incapable or too costly to generate non-textual outputs, such as images, videos, and PDF files. This limitation hinders the application of LLMs in grammar-aware fuzzing.   We present a novel approach to enabling grammar-aware fuzzing over non-textual inputs. We employ LLMs to synthesize and also mutate input generators, in the form of Python scripts, that generate data conforming to the grammar of a given input format. Then, non-textual data yielded by the input generators are further mutated by traditional fuzzers (AFL++) to explore the software input space effectively. Our approach, namely G2FUZZ, features a hybrid strategy that combines a holistic search driven by LLMs and a local search driven by industrial quality fuzzers. Two key advantages are: (1) LLMs are good at synthesizing and mutating input generators and enabling jumping out of local optima, thus achieving a synergistic effect when combined with mutation-based fuzzers; (2) LLMs are less frequently invoked unless really needed, thus significantly reducing the cost of LLM usage. We have evaluated G2FUZZ on a variety of input formats, including TIFF images, MP4 audios, and PDF files. The results show that G2FUZZ outperforms SOTA tools such as AFL++, Fuzztruction, and FormatFuzzer in terms of code coverage and bug finding across most programs tested on three platforms: UNIFUZZ, FuzzBench, and MAGMA.

**Link**: [arxiv](http://arxiv.org/abs/2501.19282v1),  [pdf](http://arxiv.org/pdf/2501.19282v1)

**Tags**: cs.SE 



### Pheromone-based Learning of Optimal Reasoning Paths
**Authors**: Anirudh Chari, Aditya Tiwari, Richard Lian, Suraj Reddy, Brian Zhou

**Updated**: 2025-01-31T16:42:31Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities through chain-of-thought prompting, yet discovering effective reasoning methods for complex problems remains challenging due to the vast space of possible intermediate steps. We introduce Ant Colony Optimization-guided Tree of Thought (ACO-ToT), a novel algorithm that combines ACO with LLMs to discover optimal reasoning paths for complex problems efficiently. Drawing inspiration from Hebbian learning in neurological systems, our method employs a collection of distinctly fine-tuned LLM "ants" to traverse and lay pheromone trails through a centralized tree of thought, with each ant's movement governed by a weighted combination of existing pheromone trails and its own specialized expertise. The algorithm evaluates complete reasoning paths using a mixture-of-experts-based scoring function, with pheromones reinforcing productive reasoning paths across iterations. Experiments on three challenging reasoning tasks (GSM8K, ARC-Challenge, and MATH) demonstrate that ACO-ToT performs significantly better than existing chain-of-thought optimization approaches, suggesting that incorporating biologically inspired collective search mechanisms into LLM inference can substantially enhance reasoning capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2501.19278v1),  [pdf](http://arxiv.org/pdf/2501.19278v1)

**Tags**: cs.CL 



### SuiGPT MAD: Move AI Decompiler to Improve Transparency and Auditability   on Non-Open-Source Blockchain Smart Contract
**Authors**: Eason Chen, Xinyi Tang, Zimo Xiao, Chuangji Li, Shizhuo Li, Wu Tingguan, Siyun Wang, Kostas Kryptos Chalkias

**Updated**: 2025-01-31T16:31:56Z

**Summary**: The vision of Web3 is to improve user control over data and assets, but one challenge that complicates this vision is the prevalence of non-transparent, scam-prone applications and vulnerable smart contracts that put Web3 users at risk. While code audits are one solution to this problem, the lack of smart contracts source code on many blockchain platforms, such as Sui, hinders the ease of auditing. A promising approach to this issue is the use of a decompiler to reverse-engineer smart contract bytecode. However, existing decompilers for Sui produce code that is difficult to understand and cannot be directly recompiled. To address this, we developed the SuiGPT Move AI Decompiler (MAD), a Large Language Model (LLM)-powered web application that decompiles smart contract bytecodes on Sui into logically correct, human-readable, and re-compilable source code with prompt engineering.   Our evaluation shows that MAD's output successfully passes original unit tests and achieves a 73.33% recompilation success rate on real-world smart contracts. Additionally, newer models tend to deliver improved performance, suggesting that MAD's approach will become increasingly effective as LLMs continue to advance.   In a user study involving 12 developers, we found that MAD significantly reduced the auditing workload compared to using traditional decompilers. Participants found MAD's outputs comparable to the original source code, improving accessibility for understanding and auditing non-open-source smart contracts. Through qualitative interviews with these developers and Web3 projects, we further discussed the strengths and concerns of MAD.   MAD has practical implications for blockchain smart contract transparency, auditing, and education. It empowers users to easily and independently review and audit non-open-source smart contracts, fostering accountability and decentralization

**Link**: [arxiv](http://arxiv.org/abs/2410.15275v2),  [pdf](http://arxiv.org/pdf/2410.15275v2)

**Tags**: cs.HC cs.SE 



### Jackpot! Alignment as a Maximal Lottery
**Authors**: Roberto-Rafael Maura-Rivero, Marc Lanctot, Francesco Visin, Kate Larson

**Updated**: 2025-01-31T16:26:28Z

**Summary**: Reinforcement Learning from Human Feedback (RLHF), the standard for aligning Large Language Models (LLMs) with human values, is known to fail to satisfy properties that are intuitively desirable, such as respecting the preferences of the majority \cite{ge2024axioms}. To overcome these issues, we propose the use of a probabilistic Social Choice rule called \emph{maximal lotteries} as a replacement for RLHF. We show that a family of alignment techniques, namely Nash Learning from Human Feedback (NLHF) \cite{munos2023nash} and variants, approximate maximal lottery outcomes and thus inherit its beneficial properties.   We confirm experimentally that our proposed methodology handles situations that arise when working with preferences more robustly than standard RLHF, including supporting the preferences of the majority, providing principled ways of handling non-transitivities in the preference data, and robustness to irrelevant alternatives. This results in systems that better incorporate human values and respect human intentions.

**Link**: [arxiv](http://arxiv.org/abs/2501.19266v1),  [pdf](http://arxiv.org/pdf/2501.19266v1)

**Tags**: cs.AI cs.LG econ.TH 



### Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for   Autonomous Drone FlighT at the Edge
**Authors**: Amogh Joshi, Sourav Sanyal, Kaushik Roy

**Updated**: 2025-01-31T16:17:03Z

**Summary**: The integration of human-intuitive interactions into autonomous systems has been limited. Traditional Natural Language Processing (NLP) systems struggle with context and intent understanding, severely restricting human-robot interaction. Recent advancements in Large Language Models (LLMs) have transformed this dynamic, allowing for intuitive and high-level communication through speech and text, and bridging the gap between human commands and robotic actions. Additionally, autonomous navigation has emerged as a central focus in robotics research, with artificial intelligence (AI) increasingly being leveraged to enhance these systems. However, existing AI-based navigation algorithms face significant challenges in latency-critical tasks where rapid decision-making is critical. Traditional frame-based vision systems, while effective for high-level decision-making, suffer from high energy consumption and latency, limiting their applicability in real-time scenarios. Neuromorphic vision systems, combining event-based cameras and spiking neural networks (SNNs), offer a promising alternative by enabling energy-efficient, low-latency navigation. Despite their potential, real-world implementations of these systems, particularly on physical platforms such as drones, remain scarce. In this work, we present Neuro-LIFT, a real-time neuromorphic navigation framework implemented on a Parrot Bebop2 quadrotor. Leveraging an LLM for natural language processing, Neuro-LIFT translates human speech into high-level planning commands which are then autonomously executed using event-based neuromorphic vision and physics-driven planning. Our framework demonstrates its capabilities in navigating in a dynamic environment, avoiding obstacles, and adapting to human instructions in real-time.

**Link**: [arxiv](http://arxiv.org/abs/2501.19259v1),  [pdf](http://arxiv.org/pdf/2501.19259v1)

**Tags**: cs.RO cs.CV cs.LG cs.NE cs.SY eess.SY 



### Are They the Same? Exploring Visual Correspondence Shortcomings of   Multimodal LLMs
**Authors**: Yikang Zhou, Tao Zhang, Shilin Xu, Shihao Chen, Qianyu Zhou, Yunhai Tong, Shunping Ji, Jiangning Zhang, Xiangtai Li, Lu Qi

**Updated**: 2025-01-31T16:12:22Z

**Summary**: Recent advancements in multimodal models have shown a strong ability in visual perception, reasoning abilities, and vision-language understanding. However, studies on visual matching ability are missing, where finding the visual correspondence of objects is essential in vision research. Our research reveals that the matching capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings, even with current strong MLLMs models, GPT-4o. In particular, we construct a Multimodal Visual Matching (MMVM) benchmark to fairly benchmark over 30 different MLLMs. The MMVM benchmark is built from 15 open-source datasets and Internet videos with manual annotation. We categorize the data samples of MMVM benchmark into eight aspects based on the required cues and capabilities to more comprehensively evaluate and analyze current MLLMs. In addition, we have designed an automatic annotation pipeline to generate the MMVM SFT dataset, including 220K visual matching data with reasoning annotation. Finally, we present CoLVA, a novel contrastive MLLM with two novel technical designs: fine-grained vision expert with object-level contrastive learning and instruction augmentation strategy. CoLVA achieves 51.06\% overall accuracy (OA) on the MMVM benchmark, surpassing GPT-4o and baseline by 8.41\% and 23.58\% OA, respectively. The results show the effectiveness of our MMVM SFT dataset and our novel technical designs. Code, benchmark, dataset, and models are available at https://github.com/zhouyiks/CoLVA.

**Link**: [arxiv](http://arxiv.org/abs/2501.04670v2),  [pdf](http://arxiv.org/pdf/2501.04670v2)

**Tags**: cs.CV 



### Can LLMs Separate Instructions From Data? And What Do We Even Mean By   That?
**Authors**: Egor Zverev, Sahar Abdelnabi, Soroush Tabesh, Mario Fritz, Christoph H. Lampert

**Updated**: 2025-01-31T16:06:52Z

**Summary**: Instruction-tuned Large Language Models (LLMs) show impressive results in numerous practical applications, but they lack essential safety features that are common in other areas of computer science, particularly an explicit separation of instructions and data. This makes them vulnerable to manipulations such as indirect prompt injections and generally unsuitable for safety-critical tasks. Surprisingly, there is currently no established definition or benchmark to quantify this phenomenon. In this work, we close this gap by introducing a formal measure for instruction-data separation and an empirical variant that is calculable from a model's outputs. We also present a new dataset, SEP, that allows estimating the measure for real-world models. Our results on various LLMs show that the problem of instruction-data separation is real: all models fail to achieve high separation, and canonical mitigation techniques, such as prompt engineering and fine-tuning, either fail to substantially improve separation or reduce model utility. The source code and SEP dataset are openly accessible at https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed.

**Link**: [arxiv](http://arxiv.org/abs/2403.06833v3),  [pdf](http://arxiv.org/pdf/2403.06833v3)

**Tags**: cs.LG cs.CL 



### Distributed Asynchronous Service Deployment in the Edge-Cloud Multi-tier   Network
**Authors**: Itamar Cohen, Paolo Giaccone, Carla Fabiana Chiasserini

**Updated**: 2025-01-31T16:01:01Z

**Summary**: In an edge-cloud multi-tier network, datacenters provide services to mobile users, with each service having specific latency constraints and computational requirements. Deploying such a variety of services while matching their requirements with the available computing resources is challenging. In addition, time-critical services may have to be migrated as the users move, to keep fulfilling their latency constraints. Unlike previous work relying on an orchestrator with an always-updated global view of the available resources and the users' locations, this work envisions a distributed solution to the above problems. In particular, we propose a distributed asynchronous framework for service deployment in the edge-cloud that increases the system resilience by avoiding a single point of failure, as in the case of a central orchestrator. Our solution ensures cost-efficient feasible placement of services, while using negligible bandwidth. Our results, obtained through trace-driven, large-scale simulations, show that the proposed solution provides performance very close to those obtained by state-of-the-art centralized solutions, and at the cost of a small communication overhead.

**Link**: [arxiv](http://arxiv.org/abs/2312.11187v2),  [pdf](http://arxiv.org/pdf/2312.11187v2)

**Tags**: cs.NI 



### SHARPIE: A Modular Framework for Reinforcement Learning and Human-AI   Interaction Experiments
**Authors**: Hüseyin Aydın, Kevin Dubois-Godin, Libio Goncalvez Braz, Floris den Hengst, Kim Baraka, Mustafa Mert Çelikok, Andreas Sauter, Shihan Wang, Frans A. Oliehoek

**Updated**: 2025-01-31T15:59:50Z

**Summary**: Reinforcement learning (RL) offers a general approach for modeling and training AI agents, including human-AI interaction scenarios. In this paper, we propose SHARPIE (Shared Human-AI Reinforcement Learning Platform for Interactive Experiments) to address the need for a generic framework to support experiments with RL agents and humans. Its modular design consists of a versatile wrapper for RL environments and algorithm libraries, a participant-facing web interface, logging utilities, deployment on popular cloud and participant recruitment platforms. It empowers researchers to study a wide variety of research questions related to the interaction between humans and RL agents, including those related to interactive reward specification and learning, learning from human feedback, action delegation, preference elicitation, user-modeling, and human-AI teaming. The platform is based on a generic interface for human-RL interactions that aims to standardize the field of study on RL in human contexts.

**Link**: [arxiv](http://arxiv.org/abs/2501.19245v1),  [pdf](http://arxiv.org/pdf/2501.19245v1)

**Tags**: cs.AI cs.HC 



### A Zero-Shot Generalization Framework for LLM-Driven Cross-Domain   Sequential Recommendation
**Authors**: Yunzhe Li, Junting Wang, Hari Sundaram, Zhining Liu

**Updated**: 2025-01-31T15:43:21Z

**Summary**: Zero-shot cross-domain sequential recommendation (ZCDSR) enables predictions in unseen domains without the need for additional training or fine-tuning, making it particularly valuable in data-sparse environments where traditional models struggle. Recent advancements in large language models (LLMs) have greatly improved ZCDSR by leveraging rich pretrained representations to facilitate cross-domain knowledge transfer. However, a key challenge persists: domain semantic bias, which arises from variations in vocabulary and content focus across domains. This misalignment leads to inconsistencies in item embeddings and hinders generalization.   To address this issue, we propose a novel framework designed to enhance LLM-based ZCDSR by improving cross-domain alignment at both the item and sequential levels. At the item level, we introduce a generalization loss that promotes inter-domain compactness by aligning embeddings of similar items across domains while maintaining intra-domain diversity to preserve unique item characteristics. This prevents embeddings from becoming overly generic while ensuring effective transferability. At the sequential level, we develop a method for transferring user behavioral patterns by clustering user sequences in the source domain and applying attention-based aggregation for target domain inference. This dynamic adaptation of user embeddings allows effective zero-shot recommendations without requiring target-domain interactions.   Comprehensive experiments across multiple datasets and domains demonstrate that our framework significantly improves sequential recommendation performance in the ZCDSR setting. By mitigating domain bias and enhancing the transferability of sequential patterns, our method provides a scalable and robust approach for achieving more effective zero-shot recommendations across domains.

**Link**: [arxiv](http://arxiv.org/abs/2501.19232v1),  [pdf](http://arxiv.org/pdf/2501.19232v1)

**Tags**: cs.IR cs.AI 



### o3-mini vs DeepSeek-R1: Which One is Safer?
**Authors**: Aitor Arrieta, Miriam Ugarte, Pablo Valle, José Antonio Parejo, Sergio Segura

**Updated**: 2025-01-31T15:39:00Z

**Summary**: The irruption of DeepSeek-R1 constitutes a turning point for the AI industry in general and the LLMs in particular. Its capabilities have demonstrated outstanding performance in several tasks, including creative thinking, code generation, maths and automated program repair, at apparently lower execution cost. However, LLMs must adhere to an important qualitative property, i.e., their alignment with safety and human values. A clear competitor of DeepSeek-R1 is its American counterpart, OpenAI's o3-mini model, which is expected to set high standards in terms of performance, safety and cost. In this technical report, we systematically assess the safety level of both DeepSeek-R1 (70b version) and OpenAI's o3-mini (beta version). To this end, we make use of our recently released automated safety testing tool, named ASTRAL. By leveraging this tool, we automatically and systematically generated and executed 1,260 test inputs on both models. After conducting a semi-automated assessment of the outcomes provided by both LLMs, the results indicate that DeepSeek-R1 produces significantly more unsafe responses (12%) than OpenAI's o3-mini (1.2%).

**Link**: [arxiv](http://arxiv.org/abs/2501.18438v2),  [pdf](http://arxiv.org/pdf/2501.18438v2)

**Tags**: cs.SE cs.AI 



### Through the Looking Glass: LLM-Based Analysis of AR/VR Android   Applications Privacy Policies
**Authors**: Abdulaziz Alghamdi, David Mohaisen

**Updated**: 2025-01-31T15:30:14Z

**Summary**: \begin{abstract} This paper comprehensively analyzes privacy policies in AR/VR applications, leveraging BERT, a state-of-the-art text classification model, to evaluate the clarity and thoroughness of these policies. By comparing the privacy policies of AR/VR applications with those of free and premium websites, this study provides a broad perspective on the current state of privacy practices within the AR/VR industry. Our findings indicate that AR/VR applications generally offer a higher percentage of positive segments than free content but lower than premium websites. The analysis of highlighted segments and words revealed that AR/VR applications strategically emphasize critical privacy practices and key terms. This enhances privacy policies' clarity and effectiveness.

**Link**: [arxiv](http://arxiv.org/abs/2501.19223v1),  [pdf](http://arxiv.org/pdf/2501.19223v1)

**Tags**: cs.CR cs.LG 



### Sparse Autoencoders Reveal Universal Feature Spaces Across Large   Language Models
**Authors**: Michael Lan, Philip Torr, Austin Meek, Ashkan Khakzar, David Krueger, Fazl Barez

**Updated**: 2025-01-31T15:27:10Z

**Summary**: We investigate feature universality in large language models (LLMs), a research field that aims to understand how different models similarly represent concepts in the latent spaces of their intermediate layers. Demonstrating feature universality allows discoveries about latent representations to generalize across several models. However, comparing features across LLMs is challenging due to polysemanticity, in which individual neurons often correspond to multiple features rather than distinct ones, making it difficult to disentangle and match features across different models. To address this issue, we employ a method known as dictionary learning by using sparse autoencoders (SAEs) to transform LLM activations into more interpretable spaces spanned by neurons corresponding to individual features. After matching feature neurons across models via activation correlation, we apply representational space similarity metrics on SAE feature spaces across different LLMs. Our experiments reveal significant similarities in SAE feature spaces across various LLMs, providing new evidence for feature universality.

**Link**: [arxiv](http://arxiv.org/abs/2410.06981v2),  [pdf](http://arxiv.org/pdf/2410.06981v2)

**Tags**: cs.LG cs.AI cs.CL 



### Autonomous Legacy Web Application Upgrades Using a Multi-Agent System
**Authors**: Valtteri Ala-Salmi, Zeeshan Rasheed, Abdul Malik Sami, Zheying Zhang, Kai-Kristian Kemell, Jussi Rasku, Shahbaz Siddeeq, Mika Saari, Pekka Abrahamsson

**Updated**: 2025-01-31T15:14:14Z

**Summary**: The use of Large Language Models (LLMs) for autonomous code generation is gaining attention in emerging technologies. As LLM capabilities expand, they offer new possibilities such as code refactoring, security enhancements, and legacy application upgrades. Many outdated web applications pose security and reliability challenges, yet companies continue using them due to the complexity and cost of upgrades. To address this, we propose an LLM-based multi-agent system that autonomously upgrades legacy web applications to the latest versions. The system distributes tasks across multiple phases, updating all relevant files. To evaluate its effectiveness, we employed Zero-Shot Learning (ZSL) and One-Shot Learning (OSL) prompts, applying identical instructions in both cases. The evaluation involved updating view files and measuring the number and types of errors in the output. For complex tasks, we counted the successfully met requirements. The experiments compared the proposed system with standalone LLM execution, repeated multiple times to account for stochastic behavior. Results indicate that our system maintains context across tasks and agents, improving solution quality over the base model in some cases. This study provides a foundation for future model implementations in legacy code updates. Additionally, findings highlight LLMs' ability to update small outdated files with high precision, even with basic prompts. The source code is publicly available on GitHub: https://github.com/alasalm1/Multi-agent-pipeline.

**Link**: [arxiv](http://arxiv.org/abs/2501.19204v1),  [pdf](http://arxiv.org/pdf/2501.19204v1)

**Tags**: cs.SE 



### Improving the Robustness of Representation Misdirection for Large   Language Model Unlearning
**Authors**: Dang Huu-Tien, Hoang Thanh-Tung, Le-Minh Nguyen, Naoya Inoue

**Updated**: 2025-01-31T15:12:20Z

**Summary**: Representation Misdirection (RM) and variants are established large language model (LLM) unlearning methods with state-of-the-art performance. In this paper, we show that RM methods inherently reduce models' robustness, causing them to misbehave even when a single non-adversarial forget-token is in the retain-query. Toward understanding underlying causes, we reframe the unlearning process as backdoor attacks and defenses: forget-tokens act as backdoor triggers that, when activated in retain-queries, cause disruptions in RM models' behaviors, similar to successful backdoor attacks. To mitigate this vulnerability, we propose Random Noise Augmentation -- a model and method agnostic approach with theoretical guarantees for improving the robustness of RM methods. Extensive experiments demonstrate that RNA significantly improves the robustness of RM models while enhancing the unlearning performances.

**Link**: [arxiv](http://arxiv.org/abs/2501.19202v1),  [pdf](http://arxiv.org/pdf/2501.19202v1)

**Tags**: cs.CL 



### Efficient Reasoning with Hidden Thinking
**Authors**: Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, Jiuxiang Gu

**Updated**: 2025-01-31T15:10:29Z

**Summary**: Chain-of-Thought (CoT) reasoning has become a powerful framework for improving complex problem-solving capabilities in Multimodal Large Language Models (MLLMs). However, the verbose nature of textual reasoning introduces significant inefficiencies. In this work, we propose $\textbf{Heima}$ (as hidden llama), an efficient reasoning framework that leverages reasoning CoTs at hidden latent space. We design the Heima Encoder to condense each intermediate CoT into a compact, higher-level hidden representation using a single thinking token, effectively minimizing verbosity and reducing the overall number of tokens required during the reasoning process. Meanwhile, we design corresponding Heima Decoder with traditional Large Language Models (LLMs) to adaptively interpret the hidden representations into variable-length textual sequence, reconstructing reasoning processes that closely resemble the original CoTs. Experimental results across diverse reasoning MLLM benchmarks demonstrate that Heima model achieves higher generation efficiency while maintaining or even better zero-shot task accuracy. Moreover, the effective reconstruction of multimodal reasoning processes with Heima Decoder validates both the robustness and interpretability of our approach.

**Link**: [arxiv](http://arxiv.org/abs/2501.19201v1),  [pdf](http://arxiv.org/pdf/2501.19201v1)

**Tags**: cs.CL cs.AI cs.LG 



### AutoElicit: Using Large Language Models for Expert Prior Elicitation in   Predictive Modelling
**Authors**: Alexander Capstick, Rahul G. Krishnan, Payam Barnaghi

**Updated**: 2025-01-31T15:04:34Z

**Summary**: Large language models (LLMs) acquire a breadth of information across various domains. However, their computational complexity, cost, and lack of transparency often hinder their direct application for predictive tasks where privacy and interpretability are paramount. In fields such as healthcare, biology, and finance, specialised and interpretable linear models still hold considerable value. In such domains, labelled data may be scarce or expensive to obtain. Well-specified prior distributions over model parameters can reduce the sample complexity of learning through Bayesian inference; however, eliciting expert priors can be time-consuming. We therefore introduce AutoElicit to extract knowledge from LLMs and construct priors for predictive models. We show these priors are informative and can be refined using natural language. We perform a careful study contrasting AutoElicit with in-context learning and demonstrate how to perform model selection between the two methods. We find that AutoElicit yields priors that can substantially reduce error over uninformative priors, using fewer labels, and consistently outperform in-context learning. We show that AutoElicit saves over 6 months of labelling effort when building a new predictive model for urinary tract infections from sensor recordings of people living with dementia.

**Link**: [arxiv](http://arxiv.org/abs/2411.17284v4),  [pdf](http://arxiv.org/pdf/2411.17284v4)

**Tags**: cs.LG cs.CL stat.ML 



### Rethinking Model Evaluation as Narrowing the Socio-Technical Gap
**Authors**: Q. Vera Liao, Ziang Xiao

**Updated**: 2025-01-31T14:59:17Z

**Summary**: The recent development of generative large language models (LLMs) poses new challenges for model evaluation that the research community and industry have been grappling with. While the versatile capabilities of these models ignite much excitement, they also inevitably make a leap toward homogenization: powering a wide range of applications with a single, often referred to as ``general-purpose'', model. In this position paper, we argue that model evaluation practices must take on a critical task to cope with the challenges and responsibilities brought by this homogenization: providing valid assessments for whether and how much human needs in diverse downstream use cases can be satisfied by the given model (\textit{socio-technical gap}). By drawing on lessons about improving research realism from the social sciences, human-computer interaction (HCI), and the interdisciplinary field of explainable AI (XAI), we urge the community to develop evaluation methods based on real-world contexts and human requirements, and embrace diverse evaluation methods with an acknowledgment of trade-offs between realisms and pragmatic costs to conduct the evaluation. By mapping HCI and current NLG evaluation methods, we identify opportunities for evaluation methods for LLMs to narrow the socio-technical gap and pose open questions.

**Link**: [arxiv](http://arxiv.org/abs/2306.03100v4),  [pdf](http://arxiv.org/pdf/2306.03100v4)

**Tags**: cs.HC cs.AI 



### Enhancing Model Defense Against Jailbreaks with Proactive Safety   Reasoning
**Authors**: Xianglin Yang, Gelei Deng, Jieming Shi, Tianwei Zhang, Jin Song Dong

**Updated**: 2025-01-31T14:45:23Z

**Summary**: Large language models (LLMs) are vital for a wide range of applications yet remain susceptible to jailbreak threats, which could lead to the generation of inappropriate responses. Conventional defenses, such as refusal and adversarial training, often fail to cover corner cases or rare domains, leaving LLMs still vulnerable to more sophisticated attacks. We propose a novel defense strategy, Safety Chain-of-Thought (SCoT), which harnesses the enhanced \textit{reasoning capabilities} of LLMs for proactive assessment of harmful inputs, rather than simply blocking them. SCoT augments any refusal training datasets to critically analyze the intent behind each request before generating answers. By employing proactive reasoning, SCoT enhances the generalization of LLMs across varied harmful queries and scenarios not covered in the safety alignment corpus. Additionally, it generates detailed refusals specifying the rules violated. Comparative evaluations show that SCoT significantly surpasses existing defenses, reducing vulnerability to out-of-distribution issues and adversarial manipulations while maintaining strong general capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2501.19180v1),  [pdf](http://arxiv.org/pdf/2501.19180v1)

**Tags**: cs.CR cs.AI 



### Position: Contextual Integrity Washing for Language Models
**Authors**: Yan Shvartzshnaider, Vasisht Duddu

**Updated**: 2025-01-31T14:39:30Z

**Summary**: Machine learning community is discovering Contextual Integrity (CI) as a useful framework to assess the privacy implications of large language models (LLMs). This is an encouraging development. The CI theory emphasizes sharing information in accordance with privacy norms and can bridge the social, legal, political, and technical aspects essential for evaluating privacy in LLMs. However, this is also a good point to reflect on use of CI for LLMs. This position paper argues that existing literature adopts CI for LLMs without embracing the theory's fundamental tenets, essentially amounting to a form of "CI-washing." CI-washing could lead to incorrect conclusions and flawed privacy-preserving designs. We clarify the four fundamental tenets of CI theory, systematize prior work on whether they deviate from these tenets, and highlight overlooked issues in experimental hygiene for LLMs (e.g., prompt sensitivity, positional bias).

**Link**: [arxiv](http://arxiv.org/abs/2501.19173v1),  [pdf](http://arxiv.org/pdf/2501.19173v1)

**Tags**: cs.CY 



### Position: On-Premises LLM Deployment Demands a Middle Path: Preserving   Privacy Without Sacrificing Model Confidentiality
**Authors**: Hanbo Huang, Yihan Li, Bowen Jiang, Lin Liu, Bo Jiang, Ruoyu Sun, Zhuotao Liu, Shiyu Liang

**Updated**: 2025-01-31T14:36:14Z

**Summary**: Current LLM customization typically relies on two deployment strategies: closed-source APIs, which require users to upload private data to external servers, and open-weight models, which allow local fine-tuning but pose misuse risks. In this position paper, we argue that (1) deploying closed-source LLMs within user-controlled infrastructure (\textit{on-premises deployment}) enhances data privacy and mitigates misuse risks, and (2) a well-designed on-premises deployment must ensure model confidentiality -- by preventing model theft -- and offer privacy-preserving customization. Prior research on small models has explored securing only the output layer within hardware-secured devices to balance confidentiality and customization efficiency. However, we show that this approach is insufficient for defending large-scale LLMs against distillation attacks. We therefore introduce a {semi-open deployment framework} that secures only a few, carefully chosen layers, achieving distillation resistance comparable to fully secured models while preserving fine-tuning flexibility. Through extensive experiments, we show that securing bottom layers significantly reduces functional extraction risks. Our findings demonstrate that privacy and confidentiality can coexist, paving the way for secure on-premises AI deployment that balances usability and protection.

**Link**: [arxiv](http://arxiv.org/abs/2410.11182v2),  [pdf](http://arxiv.org/pdf/2410.11182v2)

**Tags**: cs.LG cs.AI cs.CR 



### Poison as Cure: Visual Noise for Mitigating Object Hallucinations in   LVMs
**Authors**: Kejia Zhang, Keda Tao, Jiasheng Tang, Huan Wang

**Updated**: 2025-01-31T14:31:00Z

**Summary**: Large vision-language models (LVMs) extend large language models (LLMs) with visual perception capabilities, enabling them to process and interpret visual information. A major challenge compromising their reliability is object hallucination that LVMs may generate plausible but factually inaccurate information. We propose a novel visual adversarial perturbation (VAP) method to mitigate this hallucination issue. VAP alleviates LVM hallucination by applying strategically optimized visual noise without altering the base model. Our approach formulates hallucination suppression as an optimization problem, leveraging adversarial strategies to generate beneficial visual perturbations that enhance the model's factual grounding and reduce parametric knowledge bias. Extensive experimental results demonstrate that our method consistently reduces object hallucinations across 8 state-of-the-art LVMs, validating its efficacy across diverse evaluations.

**Link**: [arxiv](http://arxiv.org/abs/2501.19164v1),  [pdf](http://arxiv.org/pdf/2501.19164v1)

**Tags**: cs.CV 



### HarmoniCa: Harmonizing Training and Inference for Better Feature Caching   in Diffusion Transformer Acceleration
**Authors**: Yushi Huang, Zining Wang, Ruihao Gong, Jing Liu, Xinjie Zhang, Jinyang Guo, Xianglong Liu, Jun Zhang

**Updated**: 2025-01-31T14:26:05Z

**Summary**: Diffusion Transformers (DiTs) excel in generative tasks but face practical deployment challenges due to high inference costs. Feature caching, which stores and retrieves redundant computations, offers the potential for acceleration. Existing learning-based caching, though adaptive, overlooks the impact of the prior timestep. It also suffers from misaligned objectives--aligned predicted noise vs. high-quality images--between training and inference. These two discrepancies compromise both performance and efficiency. To this end, we harmonize training and inference with a novel learning-based caching framework dubbed HarmoniCa. It first incorporates Step-Wise Denoising Training (SDT) to ensure the continuity of the denoising process, where prior steps can be leveraged. In addition, an Image Error Proxy-Guided Objective (IEPO) is applied to balance image quality against cache utilization through an efficient proxy to approximate the image error. Extensive experiments across $8$ models, $4$ samplers, and resolutions from $256\times256$ to $2K$ demonstrate superior performance and speedup of our framework. For instance, it achieves over $40\%$ latency reduction (i.e., $2.07\times$ theoretical speedup) and improved performance on PixArt-$\alpha$. Remarkably, our image-free approach reduces training time by $25\%$ compared with the previous method.

**Link**: [arxiv](http://arxiv.org/abs/2410.01723v3),  [pdf](http://arxiv.org/pdf/2410.01723v3)

**Tags**: cs.CV 



### LLM4Rerank: LLM-based Auto-Reranking Framework for Recommendations
**Authors**: Jingtong Gao, Bo Chen, Weiwen Liu, Xiangyang Li, Yichao Wang, Wanyu Wang, Huifeng Guo, Ruiming Tang, Xiangyu Zhao

**Updated**: 2025-01-31T14:22:27Z

**Summary**: Reranking is a critical component in recommender systems, playing an essential role in refining the output of recommendation algorithms. Traditional reranking models have focused predominantly on accuracy, but modern applications demand consideration of additional criteria such as diversity and fairness. Existing reranking approaches often fail to harmonize these diverse criteria effectively at the model level. Moreover, these models frequently encounter challenges with scalability and personalization due to their complexity and the varying significance of different reranking criteria in diverse scenarios. In response, we introduce a comprehensive reranking framework enhanced by LLM, designed to seamlessly integrate various reranking criteria while maintaining scalability and facilitating personalized recommendations. This framework employs a fully connected graph structure, allowing the LLM to simultaneously consider multiple aspects such as accuracy, diversity, and fairness through a coherent Chain-of-Thought (CoT) process. A customizable input mechanism is also integrated, enabling the tuning of the language model's focus to meet specific reranking needs. We validate our approach using three popular public datasets, where our framework demonstrates superior performance over existing state-of-the-art reranking models in balancing multiple criteria. The code for this implementation is publicly available.

**Link**: [arxiv](http://arxiv.org/abs/2406.12433v3),  [pdf](http://arxiv.org/pdf/2406.12433v3)

**Tags**: cs.IR 



### SampleLLM: Optimizing Tabular Data Synthesis in Recommendations
**Authors**: Jingtong Gao, Zhaocheng Du, Xiaopeng Li, Yichao Wang, Xiangyang Li, Huifeng Guo, Ruiming Tang, Xiangyu Zhao

**Updated**: 2025-01-31T14:00:30Z

**Summary**: Tabular data synthesis is crucial in machine learning, yet existing general methods-primarily based on statistical or deep learning models-are highly data-dependent and often fall short in recommender systems. This limitation arises from their difficulty in capturing complex distributions and understanding feature relationships from sparse and limited data, along with their inability to grasp semantic feature relations. Recently, Large Language Models (LLMs) have shown potential in generating synthetic data samples through few-shot learning and semantic understanding. However, they often suffer from inconsistent distribution and lack of diversity due to their inherent distribution disparity with the target dataset. To address these challenges and enhance tabular data synthesis for recommendation tasks, we propose a novel two-stage framework named SampleLLM to improve the quality of LLM-based tabular data synthesis for recommendations by ensuring better distribution alignment. In the first stage, SampleLLM employs LLMs with Chain-of-Thought prompts and diverse exemplars to generate data that closely aligns with the target dataset distribution, even when input samples are limited. The second stage uses an advanced feature attribution-based importance sampling method to refine feature relationships within the synthesized data, reducing any distribution biases introduced by the LLM. Experimental results on three recommendation datasets, two general datasets, and online deployment illustrate that SampleLLM significantly surpasses existing methods for recommendation tasks and holds promise for a broader range of tabular data scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2501.16125v2),  [pdf](http://arxiv.org/pdf/2501.16125v2)

**Tags**: cs.IR 



### Cautious Optimizers: Improving Training with One Line of Code
**Authors**: Kaizhao Liang, Lizhang Chen, Bo Liu, Qiang Liu

**Updated**: 2025-01-31T13:56:58Z

**Summary**: AdamW has been the default optimizer for transformer pretraining. For many years, our community searched for faster and more stable optimizers with only constrained positive outcomes. In this work, we propose a single-line modification in Pytorch to any momentum-based optimizer, which we rename cautious optimizer, e.g. C-AdamW and C-Lion. Our theoretical result shows that this modification preserves Adam's Hamiltonian function and it does not break the convergence guarantee under the Lyapunov analysis. In addition, a whole new family of optimizers is revealed by our theoretical insight. Among them, we pick the simplest one for empirical experiments, showing not only speed-up on Llama and MAE pretraining up to $1.47$ times, but also better results in LLM post-training tasks. Code is available at https://github.com/kyleliang919/C-Optim.

**Link**: [arxiv](http://arxiv.org/abs/2411.16085v3),  [pdf](http://arxiv.org/pdf/2411.16085v3)

**Tags**: cs.LG cs.AI cs.CL cs.CV cs.DM 



### A Tensor-Train Decomposition based Compression of LLMs on Group Vector   Systolic Accelerator
**Authors**: Sixiao Huang, Tintin Wang, Ang Li, Ao Shen, Kai Li, Keyao Jiang, Mingqiang Huang, Hao Yu

**Updated**: 2025-01-31T13:45:31Z

**Summary**: Large language models (LLMs) are both storage-intensive and computation-intensive, posing significant challenges when deployed on resource-constrained hardware. As linear layers in LLMs are mainly resource consuming parts, this paper develops a tensor-train decomposition (TTD) for LLMs with a further hardware implementation on FPGA. TTD compression is applied to the linear layers in ChatGLM3-6B and LLaMA2-7B models with compression ratios (CRs) for the whole network 1.94$\times$ and 1.60$\times$, respectively. The compressed LLMs are further implemented on FPGA hardware within a highly efficient group vector systolic array (GVSA) architecture, which has DSP-shared parallel vector PEs for TTD inference, as well as optimized data communication in the accelerator. Experimental results show that the corresponding TTD based LLM accelerator implemented on FPGA achieves 1.45$\times$ and 1.57$\times$ reduction in first token delay for ChatGLM3-6B and LLaMA2-7B models, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2501.19135v1),  [pdf](http://arxiv.org/pdf/2501.19135v1)

**Tags**: cs.AR 



### LANTERN: Accelerating Visual Autoregressive Models with Relaxed   Speculative Decoding
**Authors**: Doohyuk Jang, Sihwan Park, June Yong Yang, Yeonsung Jung, Jihun Yun, Souvik Kundu, Sung-Yub Kim, Eunho Yang

**Updated**: 2025-01-31T13:24:10Z

**Summary**: Auto-Regressive (AR) models have recently gained prominence in image generation, often matching or even surpassing the performance of diffusion models. However, one major limitation of AR models is their sequential nature, which processes tokens one at a time, slowing down generation compared to models like GANs or diffusion-based methods that operate more efficiently. While speculative decoding has proven effective for accelerating LLMs by generating multiple tokens in a single forward, its application in visual AR models remains largely unexplored. In this work, we identify a challenge in this setting, which we term \textit{token selection ambiguity}, wherein visual AR models frequently assign uniformly low probabilities to tokens, hampering the performance of speculative decoding. To overcome this challenge, we propose a relaxed acceptance condition referred to as LANTERN that leverages the interchangeability of tokens in latent space. This relaxation restores the effectiveness of speculative decoding in visual AR models by enabling more flexible use of candidate tokens that would otherwise be prematurely rejected. Furthermore, by incorporating a total variation distance bound, we ensure that these speed gains are achieved without significantly compromising image quality or semantic coherence. Experimental results demonstrate the efficacy of our method in providing a substantial speed-up over speculative decoding. In specific, compared to a na\"ive application of the state-of-the-art speculative decoding, LANTERN increases speed-ups by $\mathbf{1.75}\times$ and $\mathbf{1.82}\times$, as compared to greedy decoding and random sampling, respectively, when applied to LlamaGen, a contemporary visual AR model.

**Link**: [arxiv](http://arxiv.org/abs/2410.03355v2),  [pdf](http://arxiv.org/pdf/2410.03355v2)

**Tags**: cs.CV cs.AI 



### Brain-inspired sparse training enables Transformers and LLMs to perform   as fully connected
**Authors**: Yingtao Zhang, Jialin Zhao, Wenjing Wu, Ziheng Liao, Umberto Michieli, Carlo Vittorio Cannistraci

**Updated**: 2025-01-31T13:04:37Z

**Summary**: This study aims to enlarge our current knowledge on application of brain-inspired network science principles for training artificial neural networks (ANNs) with sparse connectivity. Dynamic sparse training (DST) can reduce the computational demands in ANNs, but faces difficulties to keep peak performance at high sparsity levels. The Cannistraci-Hebb training (CHT) is a brain-inspired method for growing connectivity in DST. CHT leverages a gradient-free, topology-driven link regrowth, which has shown ultra-sparse (1% connectivity or lower) advantage across various tasks compared to fully connected networks. Yet, CHT suffers two main drawbacks: (i) its time complexity is O(Nd^3) - N node network size, d node degree - hence it can apply only to ultra-sparse networks. (ii) it selects top link prediction scores, which is inappropriate for the early training epochs, when the network presents unreliable connections. We propose a GPU-friendly approximation of the CH link predictor, which reduces the computational complexity to O(N^3), enabling a fast implementation of CHT in large-scale models. We introduce the Cannistraci-Hebb training soft rule (CHTs), which adopts a strategy for sampling connections in both link removal and regrowth, balancing the exploration and exploitation of network topology. To improve performance, we integrate CHTs with a sigmoid gradual density decay (CHTss). Empirical results show that, using 1% of connections, CHTs outperforms fully connected networks in MLP on visual classification tasks, compressing some networks to < 30% nodes. Using 5% of the connections, CHTss outperforms fully connected networks in two Transformer-based machine translation tasks. Using 30% of the connections, CHTss achieves superior performance compared to other dynamic sparse training methods in language modeling, and it surpasses the fully connected counterpart in zero-shot evaluations.

**Link**: [arxiv](http://arxiv.org/abs/2501.19107v1),  [pdf](http://arxiv.org/pdf/2501.19107v1)

**Tags**: cs.LG 



### Large Language Models are In-context Preference Learners
**Authors**: Chao Yu, Qixin Tan, Hong Lu, Jiaxuan Gao, Xinting Yang, Yu Wang, Yi Wu, Eugene Vinitsky

**Updated**: 2025-01-31T13:04:34Z

**Summary**: Preference-based reinforcement learning is an effective way to handle tasks where rewards are hard to specify but can be exceedingly inefficient as preference learning is often tabula rasa. We demonstrate that Large Language Models (LLMs) have native preference-learning capabilities that allow them to achieve sample-efficient preference learning, addressing this challenge. We propose In-Context Preference Learning (ICPL), which uses in-context learning capabilities of LLMs to reduce human query inefficiency. ICPL uses the task description and basic environment code to create sets of reward functions which are iteratively refined by placing human feedback over videos of the resultant policies into the context of an LLM and then requesting better rewards. We first demonstrate ICPL's effectiveness through a synthetic preference study, providing quantitative evidence that it significantly outperforms baseline preference-based methods with much higher performance and orders of magnitude greater efficiency. We observe that these improvements are not solely coming from LLM grounding in the task but that the quality of the rewards improves over time, indicating preference learning capabilities. Additionally, we perform a series of real human preference-learning trials and observe that ICPL extends beyond synthetic settings and can work effectively with humans-in-the-loop.

**Link**: [arxiv](http://arxiv.org/abs/2410.17233v2),  [pdf](http://arxiv.org/pdf/2410.17233v2)

**Tags**: cs.AI cs.LG 



### Oryx MLLM: On-Demand Spatial-Temporal Understanding at Arbitrary   Resolution
**Authors**: Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, Yongming Rao

**Updated**: 2025-01-31T12:48:09Z

**Summary**: Visual data comes in various forms, ranging from small icons of just a few pixels to long videos spanning hours. Existing multi-modal LLMs usually standardize these diverse visual inputs to a fixed resolution for visual encoders and yield similar numbers of tokens for LLMs. This approach is non-optimal for multimodal understanding and inefficient for processing inputs with long and short visual contents. To solve the problem, we propose Oryx, a unified multimodal architecture for the spatial-temporal understanding of images, videos, and multi-view 3D scenes. Oryx offers an on-demand solution to seamlessly and efficiently process visual inputs with arbitrary spatial sizes and temporal lengths through two core innovations: 1) a pre-trained OryxViT model that can encode images at any resolution into LLM-friendly visual representations; 2) a dynamic compressor module that supports 1x to 16x compression on visual tokens by request. These design features enable Oryx to accommodate extremely long visual contexts, such as videos, with lower resolution and high compression while maintaining high recognition precision for tasks like document understanding with native resolution and no compression. Beyond the architectural improvements, enhanced data curation and specialized training on long-context retrieval and spatial-aware data help Oryx achieve strong capabilities in image, video, and 3D multimodal understanding simultaneously. Our work is open-sourced at https://github.com/Oryx-mllm/Oryx.

**Link**: [arxiv](http://arxiv.org/abs/2409.12961v3),  [pdf](http://arxiv.org/pdf/2409.12961v3)

**Tags**: cs.CV 



### Unraveling Zeroth-Order Optimization through the Lens of Low-Dimensional   Structured Perturbations
**Authors**: Sihwan Park, Jihun Yun, SungYub Kim, Souvik Kundu, Eunho Yang

**Updated**: 2025-01-31T12:46:04Z

**Summary**: Zeroth-order (ZO) optimization has emerged as a promising alternative to gradient-based backpropagation methods, particularly for black-box optimization and large language model (LLM) fine-tuning. However, ZO methods suffer from slow convergence due to high-variance stochastic gradient estimators. While structured perturbations, such as sparsity and low-rank constraints, have been explored to mitigate these issues, their effectiveness remains highly under-explored. In this work, we develop a unified theoretical framework that analyzes both the convergence and generalization properties of ZO optimization under structured perturbations. We show that high dimensionality is the primary bottleneck and introduce the notions of \textit{stable rank} and \textit{effective overlap} to explain how structured perturbations reduce gradient noise and accelerate convergence. Using the uniform stability under our framework, we then provide the first theoretical justification for why these perturbations enhance generalization. Additionally, through empirical analysis, we identify that \textbf{block coordinate descent} (BCD) to be an effective structured perturbation method. Extensive experiments show that, compared to existing alternatives, memory-efficient ZO (MeZO) with BCD (\textit{MeZO-BCD}) can provide improved converge with a faster wall-clock time/iteration by up to $\times\textbf{2.09}$ while yielding similar or better accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2501.19099v1),  [pdf](http://arxiv.org/pdf/2501.19099v1)

**Tags**: cs.LG 



### Improving Low-Resource Sequence Labeling with Knowledge Fusion and   Contextual Label Explanations
**Authors**: Peichao Lai, Jiaxin Gan, Feiyang Ye, Yilei Wang, Bin Cui

**Updated**: 2025-01-31T12:39:28Z

**Summary**: Sequence labeling remains a significant challenge in low-resource, domain-specific scenarios, particularly for character-dense languages like Chinese. Existing methods primarily focus on enhancing model comprehension and improving data diversity to boost performance. However, these approaches still struggle with inadequate model applicability and semantic distribution biases in domain-specific contexts. To overcome these limitations, we propose a novel framework that combines an LLM-based knowledge enhancement workflow with a span-based Knowledge Fusion for Rich and Efficient Extraction (KnowFREE) model. Our workflow employs explanation prompts to generate precise contextual interpretations of target entities, effectively mitigating semantic biases and enriching the model's contextual understanding. The KnowFREE model further integrates extension label features, enabling efficient nested entity extraction without relying on external knowledge during inference. Experiments on multiple Chinese domain-specific sequence labeling datasets demonstrate that our approach achieves state-of-the-art performance, effectively addressing the challenges posed by low-resource settings.

**Link**: [arxiv](http://arxiv.org/abs/2501.19093v1),  [pdf](http://arxiv.org/pdf/2501.19093v1)

**Tags**: cs.CL 



### SituationalLLM: Proactive language models with scene awareness for   dynamic, contextual task guidance
**Authors**: Muhammad Saif Ullah Khan, Muhammad Zeshan Afzal, Didier Stricker

**Updated**: 2025-01-31T12:35:54Z

**Summary**: Large language models (LLMs) have achieved remarkable success in text-based tasks but often struggle to provide actionable guidance in real-world physical environments. This is because of their inability to recognize their limited understanding of the user's physical context. We present SituationalLLM, a novel approach that integrates structured scene information into an LLM to deliver proactive, context-aware assistance. By encoding objects, attributes, and relationships in a custom Scene Graph Language, SituationalLLM actively identifies gaps in environmental context and seeks clarifications during user interactions. This behavior emerges from training on the Situational Awareness Database for Instruct-Tuning (SAD-Instruct), which combines diverse, scenario-specific scene graphs with iterative, dialogue-based refinements. Experimental results indicate that SituationalLLM outperforms generic LLM baselines in task specificity, reliability, and adaptability, paving the way for environment-aware AI assistants capable of delivering robust, user-centric guidance under real-world constraints.

**Link**: [arxiv](http://arxiv.org/abs/2406.13302v3),  [pdf](http://arxiv.org/pdf/2406.13302v3)

**Tags**: cs.CV 



### Enhancing Code Generation for Low-Resource Languages: No Silver Bullet
**Authors**: Alessandro Giagnorio, Alberto Martin-Lopez, Gabriele Bavota

**Updated**: 2025-01-31T12:23:28Z

**Summary**: The advent of Large Language Models (LLMs) has significantly advanced the field of automated code generation. LLMs rely on large and diverse datasets to learn syntax, semantics, and usage patterns of programming languages. For low-resource languages (i.e., niche programming languages characterized by the scarcity of training data), the limited availability of such data hampers the models' ability to generalize effectively, resulting in poorer code generation performance as compared to high-resource languages. For this reason, there is a quest for techniques able to close this performance gap. We present an empirical study investigating the effectiveness of several approaches for boosting LLMs' performance on low-resource languages, namely: (i) a classic fine-tuning, which is however capped in size by the scarcity of training data; (ii) three variants of in-context learning, with prompts crafted to provide the LLM with additional information about the low-resource language (e.g., few-shot examples showcasing features of the targeted language); and (iii) a pre-training objective teaching the model how to translate between high- and low-resource languages. The context of our study are two low-resource languages (R and Racket) and six LLMs having different architectures and sizes. Our findings reveal that a fine-tuning is usually the best choice for smaller LLMs, possibly due to the fact that even a small dataset is sufficient to train their limited number of parameters. With the increase in size of the models, in-context learning becomes more and more effective, representing a safe and cheap bet (i.e., it always helps, but with different magnitudes). Differently, very large LLMs may deteriorate their performance on low-resource languages when fine-tuning is performed, possibly due to the lack of enough data needed to effectively update their weights.

**Link**: [arxiv](http://arxiv.org/abs/2501.19085v1),  [pdf](http://arxiv.org/pdf/2501.19085v1)

**Tags**: cs.SE 



### Sociotechnical Approach to Enterprise Generative Artificial Intelligence   (E-GenAI)
**Authors**: Leoncio Jimenez, Francisco Venegas

**Updated**: 2025-01-31T12:19:43Z

**Summary**: In this theoretical article, a sociotechnical approach is proposed to characterize. First, the business ecosystem, focusing on the relationships among Providers, Enterprise, and Customers through SCM, ERP, and CRM platforms to align: (1) Business Intelligence (BI), Fuzzy Logic (FL), and TRIZ (Theory of Inventive Problem Solving), through the OID model, and (2) Knowledge Management (KM) and Imperfect Knowledge Management (IKM), through the OIDK model. Second, the article explores the E-GenAI business ecosystem, which integrates GenAI-based platforms for SCM, ERP, and CRM with GenAI-based platforms for BI, FL, TRIZ, KM, and IKM, to align Large Language Models (LLMs) through the E-GenAI (OID) model. Finally, to understand the dynamics of LLMs, we utilize finite automata to model the relationships between Followers and Followees. This facilitates the construction of LLMs that can identify specific characteristics of users on a social media platform.

**Link**: [arxiv](http://arxiv.org/abs/2409.17408v2),  [pdf](http://arxiv.org/pdf/2409.17408v2)

**Tags**: cs.CY cs.AI cs.IT math.IT 



### Differentially Private Policy Gradient
**Authors**: Alexandre Rio, Merwan Barlier, Igor Colin

**Updated**: 2025-01-31T12:11:13Z

**Summary**: Motivated by the increasing deployment of reinforcement learning in the real world, involving a large consumption of personal data, we introduce a differentially private (DP) policy gradient algorithm. We show that, in this setting, the introduction of Differential Privacy can be reduced to the computation of appropriate trust regions, thus avoiding the sacrifice of theoretical properties of the DP-less methods. Therefore, we show that it is possible to find the right trade-off between privacy noise and trust-region size to obtain a performant differentially private policy gradient algorithm. We then outline its performance empirically on various benchmarks. Our results and the complexity of the tasks addressed represent a significant improvement over existing DP algorithms in online RL.

**Link**: [arxiv](http://arxiv.org/abs/2501.19080v1),  [pdf](http://arxiv.org/pdf/2501.19080v1)

**Tags**: cs.LG 



### Contrast-Aware Calibration for Fine-Tuned CLIP: Leveraging Image-Text   Alignment
**Authors**: Song-Lin Lv, Yu-Yang Chen, Zhi Zhou, Yu-Feng Li, Lan-Zhe Guo

**Updated**: 2025-01-31T11:47:15Z

**Summary**: Vision-language models (VLMs), such as CLIP, have demonstrated exceptional generalization capabilities and can quickly adapt to downstream tasks through prompt fine-tuning. Unfortunately, in classification tasks involving non-training classes, known as open-vocabulary setting, fine-tuned VLMs often overfit to train classes, resulting in a misalignment between confidence scores and actual accuracy on unseen classes, which significantly undermines their reliability in real-world deployments. Existing confidence calibration methods typically require training parameters or analyzing features from the training dataset, restricting their ability to generalize unseen classes without corresponding train data. Moreover, VLM-specific calibration methods rely solely on text features from train classes as calibration indicators, which inherently limits their ability to calibrate train classes. To address these challenges, we propose an effective multimodal calibration method Contrast-Aware Calibration (CAC). Building on the original CLIP's zero-shot adaptability and the conclusion from empirical analysis that poor intra-class and inter-class discriminative ability on unseen classes is the root cause, we calculate calibration weights based on the contrastive difference between the original and fine-tuned CLIP. This method not only adapts to calibrating unseen classes but also overcomes the limitations of previous VLM calibration methods that could not calibrate train classes. In experiments involving 11 datasets with 5 fine-tuning methods, CAC consistently achieved the best calibration effect on both train and unseen classes without sacrificing accuracy and inference speed.

**Link**: [arxiv](http://arxiv.org/abs/2501.19060v1),  [pdf](http://arxiv.org/pdf/2501.19060v1)

**Tags**: cs.CV cs.LG 



### TeZO: Empowering the Low-Rankness on the Temporal Dimension in the   Zeroth-Order Optimization for Fine-tuning LLMs
**Authors**: Yan Sun, Tiansheng Huang, Liang Ding, Li Shen, Dacheng Tao

**Updated**: 2025-01-31T11:34:03Z

**Summary**: Zeroth-order optimization (ZO) has demonstrated remarkable promise in efficient fine-tuning tasks for Large Language Models (LLMs). In particular, recent advances incorporate the low-rankness of gradients, introducing low-rank ZO estimators to further reduce GPU memory consumption. However, most existing works focus solely on the low-rankness of each individual gradient, overlooking a broader property shared by all gradients throughout the training, i.e., all gradients approximately reside within a similar subspace. In this paper, we consider two factors together and propose a novel low-rank ZO estimator, TeZO, which captures the low-rankness across both the model and temporal dimension. Specifically, we represent ZO perturbations along the temporal dimension as a 3D tensor and employ Canonical Polyadic Decomposition (CPD) to extract each low-rank 2D matrix, significantly reducing the training cost. TeZO can also be easily extended to the Adam variant while consuming less memory than MeZO-SGD, and requiring about only 35% memory of MeZO-Adam. Both comprehensive theoretical analysis and extensive experimental research have validated its efficiency, achieving SOTA-comparable results with lower overhead of time and memory.

**Link**: [arxiv](http://arxiv.org/abs/2501.19057v1),  [pdf](http://arxiv.org/pdf/2501.19057v1)

**Tags**: cs.LG 



### Enabling Autonomic Microservice Management through Self-Learning Agents
**Authors**: Fenglin Yu, Fangkai Yang, Xiaoting Qin, Zhiyang Zhang, Jue Zhang, Qingwei Lin, Hongyu Zhang, Yingnong Dang, Saravan Rajmohan, Dongmei Zhang, Qi Zhang

**Updated**: 2025-01-31T11:32:05Z

**Summary**: The increasing complexity of modern software systems necessitates robust autonomic self-management capabilities. While Large Language Models (LLMs) demonstrate potential in this domain, they often face challenges in adapting their general knowledge to specific service contexts. To address this limitation, we propose ServiceOdyssey, a self-learning agent system that autonomously manages microservices without requiring prior knowledge of service-specific configurations. By leveraging curriculum learning principles and iterative exploration, ServiceOdyssey progressively develops a deep understanding of operational environments, reducing dependence on human input or static documentation. A prototype built with the Sock Shop microservice demonstrates the potential of this approach for autonomic microservice management.

**Link**: [arxiv](http://arxiv.org/abs/2501.19056v1),  [pdf](http://arxiv.org/pdf/2501.19056v1)

**Tags**: cs.SE cs.AI cs.CL cs.MA 



### Text-to-CAD Generation Through Infusing Visual Feedback in Large   Language Models
**Authors**: Ruiyu Wang, Yu Yuan, Shizhao Sun, Jiang Bian

**Updated**: 2025-01-31T11:28:16Z

**Summary**: Creating Computer-Aided Design (CAD) models requires significant expertise and effort. Text-to-CAD, which converts textual descriptions into CAD parametric sequences, is crucial in streamlining this process. Recent studies have utilized ground-truth parametric sequences, known as sequential signals, as supervision to achieve this goal. However, CAD models are inherently multimodal, comprising parametric sequences and corresponding rendered visual objects. Besides,the rendering process from parametric sequences to visual objects is many-to-one. Therefore, both sequential and visual signals are critical for effective training. In this work, we introduce CADFusion, a framework that uses Large Language Models (LLMs) as the backbone and alternates between two training stages: the sequential learning (SL) stage and the visual feedback (VF) stage. In the SL stage, we train LLMs using ground-truth parametric sequences, enabling the generation of logically coherent parametric sequences. In the VF stage, we reward parametric sequences that render into visually preferred objects and penalize those that do not, allowing LLMs to learn how rendered visual objects are perceived and evaluated. These two stages alternate throughout the training, ensuring balanced learning and preserving benefits of both signals. Experiments demonstrate that CADFusion significantly improves performance, both qualitatively and quantitatively.

**Link**: [arxiv](http://arxiv.org/abs/2501.19054v1),  [pdf](http://arxiv.org/pdf/2501.19054v1)

**Tags**: cs.CV cs.LG 



### Evaluating the Reliability of Self-Explanations in Large Language Models
**Authors**: Korbinian Randl, John Pavlopoulos, Aron Henriksson, Tony Lindgren

**Updated**: 2025-01-31T11:16:51Z

**Summary**: This paper investigates the reliability of explanations generated by large language models (LLMs) when prompted to explain their previous output. We evaluate two kinds of such self-explanations - extractive and counterfactual - using three state-of-the-art LLMs (2B to 8B parameters) on two different classification tasks (objective and subjective). Our findings reveal, that, while these self-explanations can correlate with human judgement, they do not fully and accurately follow the model's decision process, indicating a gap between perceived and actual model reasoning. We show that this gap can be bridged because prompting LLMs for counterfactual explanations can produce faithful, informative, and easy-to-verify results. These counterfactuals offer a promising alternative to traditional explainability methods (e.g. SHAP, LIME), provided that prompts are tailored to specific tasks and checked for validity.

**Link**: [arxiv](http://arxiv.org/abs/2407.14487v2),  [pdf](http://arxiv.org/pdf/2407.14487v2)

**Tags**: cs.CL 



### Addressing Hallucinations with RAG and NMISS in Italian Healthcare LLM   Chatbots
**Authors**: Maria Paola Priola

**Updated**: 2025-01-31T11:14:25Z

**Summary**: I combine detection and mitigation techniques to addresses hallucinations in Large Language Models (LLMs). Mitigation is achieved in a question-answering Retrieval-Augmented Generation (RAG) framework while detection is obtained by introducing the Negative Missing Information Scoring System (NMISS), which accounts for contextual relevance in responses. While RAG mitigates hallucinations by grounding answers in external data, NMISS refines the evaluation by identifying cases where traditional metrics incorrectly flag contextually accurate responses as hallucinations. I use Italian health news articles as context to evaluate LLM performance. Results show that Gemma2 and GPT-4 outperform the other models, with GPT-4 producing answers closely aligned with reference responses. Mid-tier models, such as Llama2, Llama3, and Mistral benefit significantly from NMISS, highlighting their ability to provide richer contextual information. This combined approach offers new insights into the reduction and more accurate assessment of hallucinations in LLMs, with applications in real-world healthcare tasks and other domains.

**Link**: [arxiv](http://arxiv.org/abs/2412.04235v2),  [pdf](http://arxiv.org/pdf/2412.04235v2)

**Tags**: cs.CL 



### Towards the Worst-case Robustness of Large Language Models
**Authors**: Huanran Chen, Yinpeng Dong, Zeming Wei, Hang Su, Jun Zhu

**Updated**: 2025-01-31T11:10:49Z

**Summary**: Recent studies have revealed the vulnerability of Large Language Models (LLMs) to adversarial attacks, where the adversary crafts specific input sequences to induce harmful, violent, private, or incorrect outputs. Although various defenses have been proposed, they have not been evaluated by strong adaptive attacks, leaving the worst-case robustness of LLMs still intractable. By developing a stronger white-box attack, our evaluation results indicate that most typical defenses achieve nearly 0\% robustness.To solve this, we propose \textit{DiffTextPure}, a general defense that diffuses the (adversarial) input prompt using any pre-defined smoothing distribution, and purifies the diffused input using a pre-trained language model. Theoretically, we derive tight robustness lower bounds for all smoothing distributions using Fractal Knapsack or 0-1 Knapsack solvers. Under this framework, we certify the robustness of a specific case -- smoothing LLMs using a uniform kernel -- against \textit{any possible attack} with an average $\ell_0$ perturbation of 2.02 or an average suffix length of 6.41.

**Link**: [arxiv](http://arxiv.org/abs/2501.19040v1),  [pdf](http://arxiv.org/pdf/2501.19040v1)

**Tags**: cs.LG 



### Safe Reinforcement Learning in Black-Box Environments via Adaptive   Shielding
**Authors**: Daniel Bethell, Simos Gerasimou, Radu Calinescu, Calum Imrie

**Updated**: 2025-01-31T10:45:55Z

**Summary**: Empowering safe exploration of reinforcement learning (RL) agents during training is a critical challenge towards their deployment in many real-world scenarios. When prior knowledge of the domain or task is unavailable, training RL agents in unknown, \textit{black-box} environments presents an even greater safety risk. We introduce \mbox{ADVICE} (Adaptive Shielding with a Contrastive Autoencoder), a novel post-shielding technique that distinguishes safe and unsafe features of state-action pairs during training, and uses this knowledge to protect the RL agent from executing actions that yield likely hazardous outcomes. Our comprehensive experimental evaluation against state-of-the-art safe RL exploration techniques shows that ADVICE significantly reduces safety violations ($\approx\!\!50\%$) during training, with a competitive outcome reward compared to other techniques.

**Link**: [arxiv](http://arxiv.org/abs/2405.18180v2),  [pdf](http://arxiv.org/pdf/2405.18180v2)

**Tags**: cs.AI cs.LG 



### Mastering the Craft of Data Synthesis for CodeLLMs
**Authors**: Meng Chen, Philip Arthur, Qianyu Feng, Cong Duy Vu Hoang, Yu-Heng Hong, Mahdi Kazemi Moghaddam, Omid Nezami, Thien Nguyen, Gioacchino Tangari, Duy Vu, Thanh Vu, Mark Johnson, Krishnaram Kenthapadi, Don Dharmasiri, Long Duong, Yuan-Fang Li

**Updated**: 2025-01-31T10:45:01Z

**Summary**: Large language models (LLMs) have shown impressive performance in \emph{code} understanding and generation, making coding tasks a key focus for researchers due to their practical applications and value as a testbed for LLM evaluation. Data synthesis and filtering techniques have been widely adopted and shown to be highly effective in this context. In this paper, we present a focused survey and taxonomy of these techniques, emphasizing recent advancements. We highlight key challenges, explore future research directions, and offer practical guidance for new researchers entering the field.

**Link**: [arxiv](http://arxiv.org/abs/2411.00005v2),  [pdf](http://arxiv.org/pdf/2411.00005v2)

**Tags**: cs.SE cs.AI 



### Importing Phantoms: Measuring LLM Package Hallucination Vulnerabilities
**Authors**: Arjun Krishna, Erick Galinkin, Leon Derczynski, Jeffrey Martin

**Updated**: 2025-01-31T10:26:18Z

**Summary**: Large Language Models (LLMs) have become an essential tool in the programmer's toolkit, but their tendency to hallucinate code can be used by malicious actors to introduce vulnerabilities to broad swathes of the software supply chain. In this work, we analyze package hallucination behaviour in LLMs across popular programming languages examining both existing package references and fictional dependencies. By analyzing this package hallucination behaviour we find potential attacks and suggest defensive strategies to defend against these attacks. We discover that package hallucination rate is predicated not only on model choice, but also programming language, model size, and specificity of the coding task request. The Pareto optimality boundary between code generation performance and package hallucination is sparsely populated, suggesting that coding models are not being optimized for secure code. Additionally, we find an inverse correlation between package hallucination rate and the HumanEval coding benchmark, offering a heuristic for evaluating the propensity of a model to hallucinate packages. Our metrics, findings and analyses provide a base for future models, securing AI-assisted software development workflows against package supply chain attacks.

**Link**: [arxiv](http://arxiv.org/abs/2501.19012v1),  [pdf](http://arxiv.org/pdf/2501.19012v1)

**Tags**: cs.LG cs.CL cs.CR 



### A heuristic for the deployment of collecting routes for urban recycle   stations (eco-points)
**Authors**: Guido Marseglia, Juan Antonio Mesa, Francisco A Ortega, Ramón Piedra-de-la-Cuadra

**Updated**: 2025-01-31T10:19:12Z

**Summary**: The rapid and constant increase in urban population has led to a drastic rise in urban solid waste production with worrying consequences for the environment and society. In many cities, an efficient waste management combined with a suitable design of vehicle routes (VR) can lead to benefits in the environmental, economic, and social impacts. The general population is becoming increasingly aware of the need for the separation of the various categories of municipal solid waste. The numerous materials collected include glass, PET or batteries, and electric components, which are sorted at the eco-points. The management of eco-points gives rise to several problems that can be formulated analytically. The location and number of eco-point containers, the determination of the fleet size for picking up the collected waste, and the design of itineraries are all intertwined, and present computationally difficult problems, and therefore must be solved in a sequential way. In this paper, a mathematical model has been formulated, based on the Bin Packing (BP) and VR schemes, for the deployment of routes of mobile containers in the selective collection of urban solid waste. A heuristic algorithm has also been developed, which considers two different configurations of the containers to solve the proposed mathematical programming model. The results obtained from the numerical simulations show the validation of the proposed methodology carried out for the benchmark of the Sioux Falls network and the specific real case study.

**Link**: [arxiv](http://arxiv.org/abs/2501.19007v1),  [pdf](http://arxiv.org/pdf/2501.19007v1)

**Tags**: math.OC 



### Compositional Hardness of Code in Large Language Models -- A   Probabilistic Perspective
**Authors**: Yotam Wolf, Binyamin Rothberg, Dorin Shteyman, Amnon Shashua

**Updated**: 2025-01-31T10:15:43Z

**Summary**: A common practice in large language model (LLM) usage for complex analytical tasks such as code generation, is to sample a solution for the entire task within the model's context window. Previous works have shown that subtask decomposition within the model's context (chain of thought), is beneficial for solving such tasks. In this work, we point a limitation of LLMs' ability to perform several sub-tasks within the same context window - an in-context hardness of composition, pointing to an advantage for distributing a decomposed problem in a multi-agent system of LLMs. The hardness of composition is quantified by a generation complexity metric, i.e., the number of LLM generations required to sample at least one correct solution. We find a gap between the generation complexity of solving a compositional problem within the same context relative to distributing it among multiple agents, that increases exponentially with the solution's length. We prove our results theoretically and demonstrate them empirically.

**Link**: [arxiv](http://arxiv.org/abs/2409.18028v3),  [pdf](http://arxiv.org/pdf/2409.18028v3)

**Tags**: cs.AI cs.CL 



### Bilevel optimization for the deployment of refuelling stations for   electric vehicles on road networks
**Authors**: Ramón Piedra-de-la-Cuadra, Francisco Ortega

**Updated**: 2025-01-31T10:09:27Z

**Summary**: This work consists of a procedure to optimally select, among a group of candidate sites where gas stations were already located, a sufficient number of charging points in order to guarantee that an electric vehicle can make its journey without a problem of energy autonomy and that each selected charging station has another one that serves as useful support in case of failure (reinforced coverage service). For this purpose, we propose a bilevel model that, in a former level, minimizes the number of refuelling points necessary to guarantee a reinforced service coverage for all users who transit from their origin to destination and, as a second level, maximize the volume of demand that can be satisfied subject to budgetary restrictions. With the first of the objectives we are addressing the typical requirement of the administration, which consists of guaranteeing the viability of the solutions, and the second of the objectives is a criterion typically used by the private sector initiative, compatible with the profit maximization.

**Link**: [arxiv](http://arxiv.org/abs/2501.19000v1),  [pdf](http://arxiv.org/pdf/2501.19000v1)

**Tags**: math.OC 



### Supervised Learning-enhanced Multi-Group Actor Critic for Live Stream   Allocation in Feed
**Authors**: Jingxin Liu, Xiang Gao, Yisha Li, Xin Li, Haiyang Lu, Ben Wang

**Updated**: 2025-01-31T09:43:40Z

**Summary**: In the context of a short video & live stream mixed recommendation scenario, the live stream recommendation system (RS) decides whether to allocate at most one live stream into the video feed for each user request. To maximize long-term user engagement, it is crucial to determine an optimal live stream policy for accurate live stream allocation. The inappropriate live stream allocation policy can significantly affect the duration of the usage app and user retention, which ignores the long-term negative impact of live stream allocation. Recently, reinforcement learning (RL) has been widely applied in recommendation systems to capture long-term user engagement. However, traditional RL algorithms often face divergence and instability problems, which restricts the application and deployment in the large-scale industrial recommendation systems, especially in the aforementioned challenging scenario. To address these challenges, we propose a novel Supervised Learning-enhanced Multi-Group Actor Critic algorithm (SL-MGAC). Specifically, we introduce a supervised learning-enhanced actor-critic framework that incorporates variance reduction techniques, where multi-task reward learning helps restrict bootstrapping error accumulation during critic learning. Additionally, we design a multi-group state decomposition module for both actor and critic networks to reduce prediction variance and improve model stability. We also propose a novel reward function to prevent overly greedy live stream allocation. Empirically, we evaluate the SL-MGAC algorithm using offline policy evaluation (OPE) and online A/B testing. Experimental results demonstrate that the proposed method not only outperforms baseline methods under the platform-level constraints but also exhibits enhanced stability in online recommendation scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2412.10381v4),  [pdf](http://arxiv.org/pdf/2412.10381v4)

**Tags**: cs.IR cs.AI 



### Automated Detection of Sport Highlights from Audio and Video Sources
**Authors**: Francesco Della Santa, Morgana Lalli

**Updated**: 2025-01-31T09:03:03Z

**Summary**: This study presents a novel Deep Learning-based and lightweight approach for the automated detection of sports highlights (HLs) from audio and video sources. HL detection is a key task in sports video analysis, traditionally requiring significant human effort. Our solution leverages Deep Learning (DL) models trained on relatively small datasets of audio Mel-spectrograms and grayscale video frames, achieving promising accuracy rates of 89% and 83% for audio and video detection, respectively. The use of small datasets, combined with simple architectures, demonstrates the practicality of our method for fast and cost-effective deployment. Furthermore, an ensemble model combining both modalities shows improved robustness against false positives and false negatives. The proposed methodology offers a scalable solution for automated HL detection across various types of sports video content, reducing the need for manual intervention. Future work will focus on enhancing model architectures and extending this approach to broader scene-detection tasks in media analysis.

**Link**: [arxiv](http://arxiv.org/abs/2501.16100v2),  [pdf](http://arxiv.org/pdf/2501.16100v2)

**Tags**: cs.CV cs.AI cs.LG 



### Exploring the Potential of Large Language Models for Improving Digital   Forensic Investigation Efficiency
**Authors**: Akila Wickramasekara, Frank Breitinger, Mark Scanlon

**Updated**: 2025-01-31T08:27:13Z

**Summary**: The ever-increasing workload of digital forensic labs raises concerns about law enforcement's ability to conduct both cyber-related and non-cyber-related investigations promptly. Consequently, this article explores the potential and usefulness of integrating Large Language Models (LLMs) into digital forensic investigations to address challenges such as bias, explainability, censorship, resource-intensive infrastructure, and ethical and legal considerations. A comprehensive literature review is carried out, encompassing existing digital forensic models, tools, LLMs, deep learning techniques, and the use of LLMs in investigations. The review identifies current challenges within existing digital forensic processes and explores both the obstacles and the possibilities of incorporating LLMs. In conclusion, the study states that the adoption of LLMs in digital forensics, with appropriate constraints, has the potential to improve investigation efficiency, improve traceability, and alleviate the technical and judicial barriers faced by law enforcement entities.

**Link**: [arxiv](http://arxiv.org/abs/2402.19366v3),  [pdf](http://arxiv.org/pdf/2402.19366v3)

**Tags**: cs.CR cs.AI 



### ASTPrompter: Weakly Supervised Automated Language Model Red-Teaming to   Identify Low-Perplexity Toxic Prompts
**Authors**: Amelia F. Hardy, Houjun Liu, Bernard Lange, Duncan Eddy, Mykel J. Kochenderfer

**Updated**: 2025-01-31T08:22:46Z

**Summary**: Conventional approaches for the automated red-teaming of large language models (LLMs) aim to identify prompts that elicit toxic outputs from a frozen language model (the defender). This often results in the prompting model (the adversary) producing text that is unlikely to arise during autoregression. In response, we propose a reinforcement learning formulation of LLM red-teaming designed to discover prompts that both (1) elicit toxic outputs from a defender and (2) have low perplexity as scored by that defender. These prompts are the most pertinent in a red-teaming setting because the defender generates them with high probability. We solve this formulation with an online and weakly supervised form of Identity Preference Optimization (IPO), attacking models ranging from 137M to 7.8B parameters. Our policy performs competitively, producing prompts that induce defender toxicity at a rate of 2-23 times higher than baseline across model scales. Importantly, these prompts have lower perplexity than both automatically generated and human-written attacks. Furthermore, our method creates black-box attacks with 5.4-14 times increased toxicity. To assess the downstream utility of our method, we use rollouts from our policy as negative examples for downstream toxicity tuning and demonstrate improved safety.

**Link**: [arxiv](http://arxiv.org/abs/2407.09447v3),  [pdf](http://arxiv.org/pdf/2407.09447v3)

**Tags**: cs.CL 



### TV-Dialogue: Crafting Theme-Aware Video Dialogues with Immersive   Interaction
**Authors**: Sai Wang, Fan Ma, Xinyi Li, Hehe Fan, Yu Wu

**Updated**: 2025-01-31T08:04:32Z

**Summary**: Recent advancements in LLMs have accelerated the development of dialogue generation across text and images, yet video-based dialogue generation remains underexplored and presents unique challenges. In this paper, we introduce Theme-aware Video Dialogue Crafting (TVDC), a novel task aimed at generating new dialogues that align with video content and adhere to user-specified themes. We propose TV-Dialogue, a novel multi-modal agent framework that ensures both theme alignment (i.e., the dialogue revolves around the theme) and visual consistency (i.e., the dialogue matches the emotions and behaviors of characters in the video) by enabling real-time immersive interactions among video characters, thereby accurately understanding the video content and generating new dialogue that aligns with the given themes. To assess the generated dialogues, we present a multi-granularity evaluation benchmark with high accuracy, interpretability and reliability, demonstrating the effectiveness of TV-Dialogue on self-collected dataset over directly using existing LLMs. Extensive experiments reveal that TV-Dialogue can generate dialogues for videos of any length and any theme in a zero-shot manner without training. Our findings underscore the potential of TV-Dialogue for various applications, such as video re-creation, film dubbing and its use in downstream multimodal tasks.

**Link**: [arxiv](http://arxiv.org/abs/2501.18940v1),  [pdf](http://arxiv.org/pdf/2501.18940v1)

**Tags**: cs.CV 



### LLM-Assisted Relevance Assessments: When Should We Ask LLMs for Help?
**Authors**: Rikiya Takehi, Ellen M. Voorhees, Tetsuya Sakai, Ian Soboroff

**Updated**: 2025-01-31T07:50:44Z

**Summary**: Test collections are information retrieval tools that allow researchers to quickly and easily evaluate ranking algorithms. While test collections have become an integral part of IR research, the process of data creation involves significant effort in manual annotations, which often makes it very expensive and time-consuming. Thus, test collections could become too small when the budget is limited, which may lead to unstable evaluations. As a cheaper alternative, recent studies have proposed the use of large language models (LLMs) to completely replace human assessors. However, while LLMs may seem to somewhat correlate with human judgments, their predictions are not perfect and often show bias. Thus a complete replacement with LLMs is argued to be too risky and not fully reliable.   Thus, in this paper, we propose LLM-Assisted Relevance Assessments (LARA), an effective method to balance manual annotations with LLM annotations, which helps to build a rich and reliable test collection even under a low budget. We use the LLM's predicted relevance probabilities to select the most profitable documents to manually annotate under a budget constraint. With theoretical reasoning, LARA effectively guides the human annotation process by actively learning to calibrate the LLM's predicted relevance probabilities. Then, using the calibration model learned from the limited manual annotations, LARA debiases the LLM predictions to annotate the remaining non-assessed data. Empirical evaluations on TREC-7 Ad Hoc, TREC-8 Ad Hoc, TREC Robust 2004, and TREC-COVID datasets show that LARA outperforms alternative solutions under almost any budget constraint.

**Link**: [arxiv](http://arxiv.org/abs/2411.06877v2),  [pdf](http://arxiv.org/pdf/2411.06877v2)

**Tags**: cs.IR 



### TabFSBench: Tabular Benchmark for Feature Shifts in Open Environment
**Authors**: Zi-Jian Cheng, Zi-Yi Jia, Zhi Zhou, Lan-Zhe Guo, Yu-Feng Li

**Updated**: 2025-01-31T07:40:34Z

**Summary**: Tabular data is widely utilized in various machine learning tasks. Current tabular learning research predominantly focuses on closed environments, while in real-world applications, open environments are often encountered, where distribution and feature shifts occur, leading to significant degradation in model performance. Previous research has primarily concentrated on mitigating distribution shifts, whereas feature shifts, a distinctive and unexplored challenge of tabular data, have garnered limited attention. To this end, this paper conducts the first comprehensive study on feature shifts in tabular data and introduces the first tabular feature-shift benchmark (TabFSBench). TabFSBench evaluates impacts of four distinct feature-shift scenarios on four tabular model categories across various datasets and assesses the performance of large language models (LLMs) and tabular LLMs in the tabular benchmark for the first time. Our study demonstrates three main observations: (1) most tabular models have the limited applicability in feature-shift scenarios; (2) the shifted feature set importance has a linear relationship with model performance degradation; (3) model performance in closed environments correlates with feature-shift performance. Future research direction is also explored for each observation. TabFSBench is released for public access by using a few lines of Python codes at https://github.com/LAMDASZ-ML/TabFSBench.

**Link**: [arxiv](http://arxiv.org/abs/2501.18935v1),  [pdf](http://arxiv.org/pdf/2501.18935v1)

**Tags**: cs.LG 



### Predicting the Target Word of Game-playing Conversations using a   Low-Rank Dialect Adapter for Decoder Models
**Authors**: Dipankar Srirag, Aditya Joshi, Jacob Eisenstein

**Updated**: 2025-01-31T07:32:54Z

**Summary**: Dialect adapters that improve the performance of LLMs for NLU tasks on certain sociolects/dialects/national varieties ('dialects' for the sake of brevity) have been reported for encoder models. In this paper, we extend the idea of dialect adapters to decoder models in our architecture called LoRDD. Using MD-3, a publicly available dataset of word game-playing conversations between dialectal speakers, our task is Target Word Prediction (TWP) from a masked conversation. LoRDD combines task adapters and dialect adapters where the latter employ contrastive learning on pseudo-parallel conversations from MD-3. Our experiments on Indian English and Nigerian English conversations with two models (Mistral and Gemma) demonstrate that LoRDD outperforms four baselines on TWP. Additionally, it significantly reduces the performance gap with American English, narrowing it to 12% and 5.8% for word similarity, and 25% and 4.5% for accuracy, respectively. The focused contribution of LoRDD is in its promise for dialect adaptation of decoder models using TWP, a simplified version of the commonly used next-word prediction task.

**Link**: [arxiv](http://arxiv.org/abs/2409.00358v2),  [pdf](http://arxiv.org/pdf/2409.00358v2)

**Tags**: cs.CL cs.AI 



### Understand, Solve and Translate: Bridging the Multilingual Mathematical   Reasoning Gap
**Authors**: Hyunwoo Ko, Guijin Son, Dasol Choi

**Updated**: 2025-01-31T07:32:07Z

**Summary**: Large language models (LLMs) demonstrate exceptional performance on complex reasoning tasks. However, despite their strong reasoning capabilities in high-resource languages (e.g., English and Chinese), a significant performance gap persists in other languages. To investigate this gap in Korean, we introduce HRM8K, a benchmark comprising 8,011 English-Korean parallel bilingual math problems. Through systematic analysis of model behaviors, we identify a key finding: these performance disparities stem primarily from difficulties in comprehending non-English inputs, rather than limitations in reasoning capabilities. Based on these findings, we propose UST (Understand, Solve, and Translate), a method that strategically uses English as an anchor for reasoning and solution generation. By fine-tuning the model on 130k synthetically generated data points, UST achieves a 10.91% improvement on the HRM8K benchmark and reduces the multilingual performance gap from 11.6% to 0.7%. Additionally, we show that improvements from UST generalize effectively to different Korean domains, demonstrating that capabilities acquired from machine-verifiable content can be generalized to other areas. We publicly release the benchmark, training dataset, and models.

**Link**: [arxiv](http://arxiv.org/abs/2501.02448v2),  [pdf](http://arxiv.org/pdf/2501.02448v2)

**Tags**: cs.CL 



### BinaryDM: Accurate Weight Binarization for Efficient Diffusion Models
**Authors**: Xingyu Zheng, Xianglong Liu, Haotong Qin, Xudong Ma, Mingyuan Zhang, Haojie Hao, Jiakai Wang, Zixiang Zhao, Jinyang Guo, Michele Magno

**Updated**: 2025-01-31T07:11:43Z

**Summary**: With the advancement of diffusion models (DMs) and the substantially increased computational requirements, quantization emerges as a practical solution to obtain compact and efficient low-bit DMs. However, the highly discrete representation leads to severe accuracy degradation, hindering the quantization of diffusion models to ultra-low bit-widths. This paper proposes a novel weight binarization approach for DMs, namely BinaryDM, pushing binarized DMs to be accurate and efficient by improving the representation and optimization. From the representation perspective, we present an Evolvable-Basis Binarizer (EBB) to enable a smooth evolution of DMs from full-precision to accurately binarized. EBB enhances information representation in the initial stage through the flexible combination of multiple binary bases and applies regularization to evolve into efficient single-basis binarization. The evolution only occurs in the head and tail of the DM architecture to retain the stability of training. From the optimization perspective, a Low-rank Representation Mimicking (LRM) is applied to assist the optimization of binarized DMs. The LRM mimics the representations of full-precision DMs in low-rank space, alleviating the direction ambiguity of the optimization process caused by fine-grained alignment. Comprehensive experiments demonstrate that BinaryDM achieves significant accuracy and efficiency gains compared to SOTA quantization methods of DMs under ultra-low bit-widths. With 1-bit weight and 4-bit activation (W1A4), BinaryDM achieves as low as 7.74 FID and saves the performance from collapse (baseline FID 10.87). As the first binarization method for diffusion models, W1A4 BinaryDM achieves impressive 15.2x OPs and 29.2x model size savings, showcasing its substantial potential for edge deployment. The code is available at https://github.com/Xingyu-Zheng/BinaryDM.

**Link**: [arxiv](http://arxiv.org/abs/2404.05662v5),  [pdf](http://arxiv.org/pdf/2404.05662v5)

**Tags**: cs.CV 



### Language Games as the Pathway to Artificial Superhuman Intelligence
**Authors**: Ying Wen, Ziyu Wan, Shao Zhang

**Updated**: 2025-01-31T07:10:40Z

**Summary**: The evolution of large language models (LLMs) toward artificial superhuman intelligence (ASI) hinges on data reproduction, a cyclical process in which models generate, curate and retrain on novel data to refine capabilities. Current methods, however, risk getting stuck in a data reproduction trap: optimizing outputs within fixed human-generated distributions in a closed loop leads to stagnation, as models merely recombine existing knowledge rather than explore new frontiers. In this paper, we propose language games as a pathway to expanded data reproduction, breaking this cycle through three mechanisms: (1) \textit{role fluidity}, which enhances data diversity and coverage by enabling multi-agent systems to dynamically shift roles across tasks; (2) \textit{reward variety}, embedding multiple feedback criteria that can drive complex intelligent behaviors; and (3) \textit{rule plasticity}, iteratively evolving interaction constraints to foster learnability, thereby injecting continual novelty. By scaling language games into global sociotechnical ecosystems, human-AI co-evolution generates unbounded data streams that drive open-ended exploration. This framework redefines data reproduction not as a closed loop but as an engine for superhuman intelligence.

**Link**: [arxiv](http://arxiv.org/abs/2501.18924v1),  [pdf](http://arxiv.org/pdf/2501.18924v1)

**Tags**: cs.AI cs.CL cs.MA 



### KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree   Search
**Authors**: Haoran Luo, Haihong E, Yikai Guo, Qika Lin, Xiaobao Wu, Xinyu Mu, Wenhao Liu, Meina Song, Yifan Zhu, Luu Anh Tuan

**Updated**: 2025-01-31T06:59:49Z

**Summary**: Knowledge Base Question Answering (KBQA) aims to answer natural language questions with a large-scale structured knowledge base (KB). Despite advancements with large language models (LLMs), KBQA still faces challenges in weak KB awareness, imbalance between effectiveness and efficiency, and high reliance on annotated data. To address these challenges, we propose KBQA-o1, a novel agentic KBQA method with Monte Carlo Tree Search (MCTS). It introduces a ReAct-based agent process for stepwise logical form generation with KB environment exploration. Moreover, it employs MCTS, a heuristic search method driven by policy and reward models, to balance agentic exploration's performance and search space. With heuristic exploration, KBQA-o1 generates high-quality annotations for further improvement by incremental fine-tuning. Experimental results show that KBQA-o1 outperforms previous low-resource KBQA methods with limited annotated data, boosting Llama-3.1-8B model's GrailQA F1 performance to 78.5% compared to 48.5% of the previous sota method with GPT-3.5-turbo.

**Link**: [arxiv](http://arxiv.org/abs/2501.18922v1),  [pdf](http://arxiv.org/pdf/2501.18922v1)

**Tags**: cs.CL cs.AI cs.DB 



### Improving Parallel Program Performance with LLM Optimizers via   Agent-System Interface
**Authors**: Anjiang Wei, Allen Nie, Thiago S. F. X. Teixeira, Rohan Yadav, Wonchan Lee, Ke Wang, Alex Aiken

**Updated**: 2025-01-31T06:36:22Z

**Summary**: Modern scientific discovery increasingly relies on high-performance computing for complex modeling and simulation. A key challenge in improving parallel program performance is efficiently mapping tasks to processors and data to memory, a process dictated by intricate, low-level system code known as mappers. Developing high-performance mappers demands days of manual tuning, posing a significant barrier for domain scientists without systems expertise. We introduce a framework that automates mapper development with generative optimization, leveraging richer feedback beyond scalar performance metrics. Our approach features the Agent-System Interface, which includes a Domain-Specific Language (DSL) to abstract away low-level complexity of system code and define a structured search space, as well as AutoGuide, a mechanism that interprets raw execution output into actionable feedback. Unlike traditional reinforcement learning methods such as OpenTuner, which rely solely on scalar feedback, our method finds superior mappers in far fewer iterations. With just 10 iterations, it outperforms OpenTuner even after 1000 iterations, achieving 3.8X faster performance. Our approach finds mappers that surpass expert-written mappers by up to 1.34X speedup across nine benchmarks while reducing tuning time from days to minutes.

**Link**: [arxiv](http://arxiv.org/abs/2410.15625v2),  [pdf](http://arxiv.org/pdf/2410.15625v2)

**Tags**: cs.LG cs.AI cs.CL cs.DC 



### LLM Program Optimization via Retrieval Augmented Search
**Authors**: Sagnik Anupam, Alexander Shypula, Osbert Bastani

**Updated**: 2025-01-31T06:34:47Z

**Summary**: With the advent of large language models (LLMs), there has been a great deal of interest in applying them to solve difficult programming tasks. Recent work has demonstrated their potential at program optimization, a key challenge in programming languages research. We propose a blackbox adaptation method called Retrieval Augmented Search (RAS) that performs beam search over candidate optimizations; at each step, it retrieves in-context examples from a given training dataset of slow-fast program pairs to guide the LLM. Critically, we find that performing contextual retrieval based on an LLM-generated natural language description significantly outperforms retrieval based on the source code. In addition, we propose a method called AEGIS for improving interpretability by decomposing training examples into "atomic edits" that are significantly more incremental in nature. We show that RAS performs 1.8$\times$ better than prior state-of-the-art blackbox adaptation strategies, and that AEGIS performs 1.37$\times$ better while performing significantly smaller edits.

**Link**: [arxiv](http://arxiv.org/abs/2501.18916v1),  [pdf](http://arxiv.org/pdf/2501.18916v1)

**Tags**: cs.LG 



### RWKV-Lite: Deeply Compressed RWKV for Resource-Constrained Devices
**Authors**: Wonkyo Choe, Yangfeng Ji, Felix Xiaozhu Lin

**Updated**: 2025-01-31T06:34:12Z

**Summary**: To deploy LLMs on resource-contained platforms such as mobile robots and smartphones, non-transformers LLMs have achieved major breakthroughs. Recently, a novel RNN-based LLM family, Repentance Weighted Key Value (RWKV) has shown strong computational efficiency; nevertheless, RWKV models still have high parameter counts which limited their deployment. In this paper, we propose a suite of compression techniques, ranging from model architecture optimizations to post-training compression, tailored to the RWKV architecture. Combined, our techniques reduce the memory footprint of RWKV models by 3.4x -- 5x with only negligible degradation in accuracy; compared to transformer LLMs with similar accuracy, our models require 4x less memory footprint.

**Link**: [arxiv](http://arxiv.org/abs/2412.10856v3),  [pdf](http://arxiv.org/pdf/2412.10856v3)

**Tags**: cs.LG cs.PF 



### Scaling Laws for Differentially Private Language Models
**Authors**: Ryan McKenna, Yangsibo Huang, Amer Sinha, Borja Balle, Zachary Charles, Christopher A. Choquette-Choo, Badih Ghazi, George Kaissis, Ravi Kumar, Ruibo Liu, Da Yu, Chiyuan Zhang

**Updated**: 2025-01-31T06:32:46Z

**Summary**: Scaling laws have emerged as important components of large language model (LLM) training as they can predict performance gains through scale, and provide guidance on important hyper-parameter choices that would otherwise be expensive. LLMs also rely on large, high-quality training datasets, like those sourced from (sometimes sensitive) user data. Training models on this sensitive user data requires careful privacy protections like differential privacy (DP). However, the dynamics of DP training are significantly different, and consequently their scaling laws are not yet fully understood. In this work, we establish scaling laws that accurately model the intricacies of DP LLM training, providing a complete picture of the compute-privacy-utility tradeoffs and the optimal training configurations in many settings.

**Link**: [arxiv](http://arxiv.org/abs/2501.18914v1),  [pdf](http://arxiv.org/pdf/2501.18914v1)

**Tags**: cs.LG cs.CR 



### Simultaneous Reward Distillation and Preference Learning: Get You a   Language Model Who Can Do Both
**Authors**: Abhijnan Nath, Changsoo Jung, Ethan Seefried, Nikhil Krishnaswamy

**Updated**: 2025-01-31T06:27:00Z

**Summary**: Traditional RLHF-based LLM alignment methods explicitly maximize the expected rewards from a separate reward model. More recent supervised alignment methods like Direct Preference Optimization (DPO) circumvent this phase to avoid problems including model drift and reward overfitting. Although popular due to its simplicity, DPO and similar direct alignment methods which rely heavily on the Bradley-Terry-based pairwise preference formulation can still lead to degenerate policies when challenged by non-deterministic or noisy preference labels, for example human scoring of two candidate outputs with low confidence. This paper introduces DRDO (Direct Reward Distillation and policy-Optimization), which simultaneously models rewards and preferences to avoid such degeneracy. DRDO directly mimics rewards assigned by an oracle while learning human preferences with a novel preference likelihood formulation. Results on the Ultrafeedback and TL;DR datasets demonstrate that DRDO-trained policies surpass methods such as DPO and e-DPO in terms of expected rewards and are more robust, on average, to noisy preference signals as well as out-of-distribution (OOD) settings.

**Link**: [arxiv](http://arxiv.org/abs/2410.08458v2),  [pdf](http://arxiv.org/pdf/2410.08458v2)

**Tags**: cs.LG cs.CL 



### Streamlining Security Vulnerability Triage with Large Language Models
**Authors**: Mohammad Jalili Torkamani, Joey NG, Nikita Mehrotra, Mahinthan Chandramohan, Padmanabhan Krishnan, Rahul Purandare

**Updated**: 2025-01-31T06:02:24Z

**Summary**: Bug triaging for security vulnerabilities is a critical part of software maintenance, ensuring that the most pressing vulnerabilities are addressed promptly to safeguard system integrity and user data. However, the process is resource-intensive and comes with challenges, including classifying software vulnerabilities, assessing their severity, and managing a high volume of bug reports. In this paper, we present CASEY, a novel approach that leverages Large Language Models (in our case, the GPT model) that automates the identification of Common Weakness Enumerations (CWEs) of security bugs and assesses their severity. CASEY employs prompt engineering techniques and incorporates contextual information at varying levels of granularity to assist in the bug triaging process. We evaluated CASEY using an augmented version of the National Vulnerability Database (NVD), employing quantitative and qualitative metrics to measure its performance across CWE identification, severity assessment, and their combined analysis. CASEY achieved a CWE identification accuracy of 68%, a severity identification accuracy of 73.6%, and a combined accuracy of 51.2% for identifying both. These results demonstrate the potential of LLMs in identifying CWEs and severity levels, streamlining software vulnerability management, and improving the efficiency of security vulnerability triaging workflows.

**Link**: [arxiv](http://arxiv.org/abs/2501.18908v1),  [pdf](http://arxiv.org/pdf/2501.18908v1)

**Tags**: cs.SE D.2; K.6.3; I.2.7 



### LUK: Empowering Log Understanding with Expert Knowledge from Large   Language Models
**Authors**: Lipeng Ma, Weidong Yang, Sihang Jiang, Ben Fei, Mingjie Zhou, Shuhao Li, Mingyu Zhao, Bo Xu, Yanghua Xiao

**Updated**: 2025-01-31T05:51:52Z

**Summary**: Logs play a critical role in providing essential information for system monitoring and troubleshooting. Recently, with the success of pre-trained language models (PLMs) and large language models (LLMs) in natural language processing (NLP), smaller PLMs (such as BERT) and LLMs (like GPT-4) have become the current mainstream approaches for log analysis. Despite the remarkable capabilities of LLMs, their higher cost and inefficient inference present significant challenges in leveraging the full potential of LLMs to analyze logs. In contrast, smaller PLMs can be fine-tuned for specific tasks even with limited computational resources, making them more practical. However, these smaller PLMs face challenges in understanding logs comprehensively due to their limited expert knowledge. To address the lack of expert knowledge and enhance log understanding for smaller PLMs, this paper introduces a novel and practical knowledge enhancement framework, called LUK, which acquires expert knowledge from LLMs automatically and then enhances the smaller PLM for log analysis with these expert knowledge. LUK can take full advantage of both types of models. Specifically, we design a multi-expert collaboration framework based on LLMs with different roles to acquire expert knowledge. In addition, we propose two novel pre-training tasks to enhance the log pre-training with expert knowledge. LUK achieves state-of-the-art results on different log analysis tasks and extensive experiments demonstrate expert knowledge from LLMs can be utilized more effectively to understand logs. Our source code and detailed experimental data are available at https://github.com/LeaperOvO/LUK.

**Link**: [arxiv](http://arxiv.org/abs/2409.01909v2),  [pdf](http://arxiv.org/pdf/2409.01909v2)

**Tags**: cs.SE cs.AI 



### Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based   Automatic Heuristic Design
**Authors**: Zhi Zheng, Zhuoliang Xie, Zhenkun Wang, Bryan Hooi

**Updated**: 2025-01-31T05:28:15Z

**Summary**: Handcrafting heuristics for solving complex optimization tasks (e.g., route planning and task allocation) is a common practice but requires extensive domain knowledge. Recently, Large Language Model (LLM)-based automatic heuristic design (AHD) methods have shown promise in generating high-quality heuristics without manual interventions. Existing LLM-based AHD methods employ a population to maintain a fixed number of top-performing LLM-generated heuristics and introduce evolutionary computation (EC) to iteratively enhance the population. However, these population-based procedures cannot fully develop the potential of each heuristic and are prone to converge into local optima. To more comprehensively explore the space of heuristics, this paper proposes to use Monte Carlo Tree Search (MCTS) for LLM-based heuristic evolution. The proposed MCTS-AHD method organizes all LLM-generated heuristics in a tree structure and can better develop the potential of temporarily underperforming heuristics. In experiments, MCTS-AHD delivers significantly higher-quality heuristics on various complex tasks. Our code is available.

**Link**: [arxiv](http://arxiv.org/abs/2501.08603v3),  [pdf](http://arxiv.org/pdf/2501.08603v3)

**Tags**: cs.AI 



### Evaluating LLM-based Personal Information Extraction and Countermeasures
**Authors**: Yupei Liu, Yuqi Jia, Jinyuan Jia, Neil Zhenqiang Gong

**Updated**: 2025-01-31T05:16:50Z

**Summary**: Automatically extracting personal information -- such as name, phone number, and email address -- from publicly available profiles at a large scale is a stepstone to many other security attacks including spear phishing. Traditional methods -- such as regular expression, keyword search, and entity detection -- achieve limited success at such personal information extraction. In this work, we perform a systematic measurement study to benchmark large language model (LLM) based personal information extraction and countermeasures. Towards this goal, we present a framework for LLM-based extraction attacks; collect four datasets including a synthetic dataset generated by GPT-4 and three real-world datasets with manually labeled eight categories of personal information; introduce a novel mitigation strategy based on prompt injection; and systematically benchmark LLM-based attacks and countermeasures using ten LLMs and five datasets. Our key findings include: LLM can be misused by attackers to accurately extract various personal information from personal profiles; LLM outperforms traditional methods; and prompt injection can defend against strong LLM-based attacks, reducing the attack to less effective traditional ones.

**Link**: [arxiv](http://arxiv.org/abs/2408.07291v3),  [pdf](http://arxiv.org/pdf/2408.07291v3)

**Tags**: cs.CR 



### "Cold, Calculated, and Condescending": How AI Identifies and Explains   Ableism Compared to Disabled People
**Authors**: Mahika Phutane, Ananya Seelam, Aditya Vashistha

**Updated**: 2025-01-31T04:48:29Z

**Summary**: People with disabilities (PwD) regularly encounter ableist hate and microaggressions online. These spaces are generally moderated by machine learning models, but little is known about how effectively AI models identify ableist speech and how well their judgments align with PwD. To investigate this, we curated a first-of-its-kind dataset of 200 social media comments targeted towards PwD, and prompted state-of-the art AI models (i.e., Toxicity Classifiers, LLMs) to score toxicity and ableism for each comment, and explain their reasoning. Then, we recruited 190 participants to similarly rate and explain the harm, and evaluate LLM explanations. Our mixed-methods analysis highlighted a major disconnect: AI underestimated toxicity compared to PwD ratings, while its ableism assessments were sporadic and varied. Although LLMs identified some biases, its explanations were flawed--they lacked nuance, made incorrect assumptions, and appeared judgmental instead of educational. Going forward, we discuss challenges and opportunities in designing moderation systems for ableism, and advocate for the involvement of intersectional disabled perspectives in AI.

**Link**: [arxiv](http://arxiv.org/abs/2410.03448v2),  [pdf](http://arxiv.org/pdf/2410.03448v2)

**Tags**: cs.HC cs.AI 



### Can We Predict the Effect of Prompts?
**Authors**: Jae Yong Lee, Sungmin Kang, Shin Yoo

**Updated**: 2025-01-31T04:34:43Z

**Summary**: Large Language Models (LLMs) are machine learning models that have seen widespread adoption due to their capability of handling previously difficult tasks. LLMs, due to their training, are sensitive to how exactly a question is presented, also known as prompting. However, prompting well is challenging, as it has been difficult to uncover principles behind prompting -- generally, trial-and-error is the most common way of improving prompts, despite its significant computational cost. In this context, we argue it would be useful to perform `predictive prompt analysis', in which an automated technique would perform a quick analysis of a prompt and predict how the LLM would react to it, relative to a goal provided by the user. As a demonstration of the concept, we present Syntactic Prevalence Analyzer (SPA), a predictive prompt analysis approach based on sparse autoencoders (SAEs). SPA accurately predicted how often an LLM would generate target syntactic structures during code synthesis, with up to 0.994 Pearson correlation between the predicted and actual prevalence of the target structure. At the same time, SPA requires only 0.4\% of the time it takes to run the LLM on a benchmark. As LLMs are increasingly used during and integrated into modern software development, our proposed predictive prompt analysis concept has the potential to significantly ease the use of LLMs for both practitioners and researchers.

**Link**: [arxiv](http://arxiv.org/abs/2501.18883v1),  [pdf](http://arxiv.org/pdf/2501.18883v1)

**Tags**: cs.SE cs.LG 



### InvAgent: A Large Language Model based Multi-Agent System for Inventory   Management in Supply Chains
**Authors**: Yinzhu Quan, Zefang Liu

**Updated**: 2025-01-31T04:31:59Z

**Summary**: Supply chain management (SCM) involves coordinating the flow of goods, information, and finances across various entities to deliver products efficiently. Effective inventory management is crucial in today's volatile and uncertain world. Previous research has demonstrated the superiority of heuristic methods and reinforcement learning applications in inventory management. However, the application of large language models (LLMs) as autonomous agents in multi-agent systems for inventory management remains underexplored. This study introduces a novel approach using LLMs to manage multi-agent inventory systems. Leveraging their zero-shot learning capabilities, our model, InvAgent, enhances resilience and improves efficiency across the supply chain network. Our contributions include utilizing LLMs for zero-shot learning to enable adaptive and informed decision-making without prior training, providing explainability and clarity through chain-of-thought, and demonstrating dynamic adaptability to varying demand scenarios while reducing costs and preventing stockouts. Extensive evaluations across different scenarios highlight the efficiency of our model in SCM.

**Link**: [arxiv](http://arxiv.org/abs/2407.11384v2),  [pdf](http://arxiv.org/pdf/2407.11384v2)

**Tags**: cs.CL 



### Distorting Embedding Space for Safety: A Defense Mechanism for   Adversarially Robust Diffusion Models
**Authors**: Jaesin Ahn, Heechul Jung

**Updated**: 2025-01-31T04:14:05Z

**Summary**: Text-to-image diffusion models show remarkable generation performance following text prompts, but risk generating Not Safe For Work (NSFW) contents from unsafe prompts. Existing approaches, such as prompt filtering or concept unlearning, fail to defend against adversarial attacks while maintaining benign image quality. In this paper, we propose a novel approach called Distorting Embedding Space (DES), a text encoder-based defense mechanism that effectively tackles these issues through innovative embedding space control. DES transforms unsafe embeddings, extracted from a text encoder using unsafe prompts, toward carefully calculated safe embedding regions to prevent unsafe contents generation, while reproducing the original safe embeddings. DES also neutralizes the nudity embedding, extracted using prompt ``nudity", by aligning it with neutral embedding to enhance robustness against adversarial attacks. These methods ensure both robust defense and high-quality image generation. Additionally, DES can be adopted in a plug-and-play manner and requires zero inference overhead, facilitating its deployment. Extensive experiments on diverse attack types, including black-box and white-box scenarios, demonstrate DES's state-of-the-art performance in both defense capability and benign image generation quality. Our model is available at https://github.com/aei13/DES.

**Link**: [arxiv](http://arxiv.org/abs/2501.18877v1),  [pdf](http://arxiv.org/pdf/2501.18877v1)

**Tags**: cs.CV cs.CR cs.LG 



### RepoAudit: An Autonomous LLM-Agent for Repository-Level Code Auditing
**Authors**: Jinyao Guo, Chengpeng Wang, Xiangzhe Xu, Zian Su, Xiangyu Zhang

**Updated**: 2025-01-31T04:02:40Z

**Summary**: Code auditing is a code review process with the goal of finding bugs. Large Language Models (LLMs) have shown substantial potential in this task, offering the ability to analyze programs without compilation and enabling customized bug detection following specified prompts. However, applying LLMs to repository-level code auditing presents notable challenges. The inherent context limits and hallucinations of LLMs can lead to the low quality of bug reports. Meanwhile, the large size of software repositories introduces substantial time and token costs, hindering efficiency and scalability in real-world scenarios. This work introduces an autonomous LLM-agent, RepoAudit, designed to enable precise and efficient repository-level code auditing. Equipped with the agent memory, RepoAudit explores the code repository on demand, analyzing data-flow facts along different feasible program paths in individual functions. It also introduces the validator to check the data-flow facts for hallucination mitigation and examine the satisfiability of path conditions of potential buggy paths, which enables RepoAudit to discard false positives in the code auditing. Our experiment shows that RepoAudit powered by Claude 3.5 Sonnet successfully finds 38 true bugs in 15 real-world systems, consuming 0.44 hours and $2.54 per project on average.

**Link**: [arxiv](http://arxiv.org/abs/2501.18160v2),  [pdf](http://arxiv.org/pdf/2501.18160v2)

**Tags**: cs.SE cs.PL 



