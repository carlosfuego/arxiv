# Arxiv Results
## Keyword: kv cache 
 ### SQuat: Subspace-orthogonal KV Cache Quantization
**Authors**: Hao Wang, Ligong Han, Kai Xu, Akash Srivastava

**Updated**: 2025-03-31T17:37:32Z

**Summary**: The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from previously generated tokens. It reduces redundant computation at the cost of increased memory usage. To mitigate this overhead, existing approaches compress KV tensors into lower-bit representations; however, quantization errors can accumulate as more tokens are generated, potentially resulting in undesired outputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache quantization). It first constructs a subspace spanned by query tensors to capture the most critical task-related information. During key tensor quantization, it enforces that the difference between the (de)quantized and original keys remains orthogonal to this subspace, minimizing the impact of quantization errors on the attention mechanism's outputs. SQuat requires no model fine-tuning, no additional calibration dataset for offline learning, and is grounded in a theoretical framework we develop. Through numerical experiments, we show that our method reduces peak memory by 2.17 to 2.82, improves throughput by 2.45 to 3.60, and achieves more favorable benchmark scores than existing KV cache quantization algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2503.24358v1),  [pdf](http://arxiv.org/pdf/2503.24358v1)

**Tags**: cs.LG cs.AI cs.CL cs.IT math.IT 



### CITRAS: Covariate-Informed Transformer for Time Series Forecasting
**Authors**: Yosuke Yamaguchi, Issei Suemitsu, Wenpeng Wei

**Updated**: 2025-03-31T12:32:23Z

**Summary**: Covariates play an indispensable role in practical time series forecasting, offering rich context from the past and sometimes extending into the future. However, their availability varies depending on the scenario, and situations often involve multiple target variables simultaneously. Moreover, the cross-variate dependencies between them are multi-granular, with some covariates having a short-term impact on target variables and others showing long-term correlations. This heterogeneity and the intricate dependencies arising in covariate-informed forecasting present significant challenges to existing deep models. To address these issues, we propose CITRAS, a patch-based Transformer that flexibly leverages multiple targets and covariates covering both the past and the future forecasting horizon. While preserving the strong autoregressive capabilities of the canonical Transformer, CITRAS introduces two novel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and Attention Score Smoothing. KV Shift seamlessly incorporates future known covariates into the forecasting of target variables based on their concurrent dependencies. Additionally, Attention Score Smoothing transforms locally accurate patch-wise cross-variate dependencies into global variate-level dependencies by smoothing the past series of attention scores. Experimentally, CITRAS achieves state-of-the-art performance in both covariate-informed and multivariate forecasting, demonstrating its versatile ability to leverage cross-variate dependency for improved forecasting accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2503.24007v1),  [pdf](http://arxiv.org/pdf/2503.24007v1)

**Tags**: cs.LG cs.AI 



### Rethinking Key-Value Cache Compression Techniques for Large Language   Model Serving
**Authors**: Wei Gao, Xinyu Zhou, Peng Sun, Tianwei Zhang, Yonggang Wen

**Updated**: 2025-03-31T12:23:31Z

**Summary**: Key-Value cache (\texttt{KV} \texttt{cache}) compression has emerged as a promising technique to optimize Large Language Model (LLM) serving. It primarily decreases the memory consumption of \texttt{KV} \texttt{cache} to reduce the computation cost. Despite the development of many compression algorithms, their applications in production environments are still not prevalent. In this paper, we revisit mainstream \texttt{KV} \texttt{cache} compression solutions from a practical perspective. Our contributions are three-fold. First, we comprehensively review existing algorithmic designs and benchmark studies for \texttt{KV} \texttt{cache} compression and identify missing pieces in their performance measurement, which could hinder their adoption in practice. Second, we empirically evaluate representative \texttt{KV} \texttt{cache} compression methods to uncover two key issues that affect the computational efficiency: (1) while compressing \texttt{KV} \texttt{cache} can reduce memory consumption, current implementations (e.g., FlashAttention, PagedAttention) do not optimize for production-level LLM serving, resulting in suboptimal throughput performance; (2) compressing \texttt{KV} \texttt{cache} may lead to longer outputs, resulting in increased end-to-end latency. We further investigate the accuracy performance of individual samples rather than the overall performance, revealing the intrinsic limitations in \texttt{KV} \texttt{cache} compression when handling specific LLM tasks. Third, we provide tools to shed light on future \texttt{KV} \texttt{cache} compression studies and facilitate their practical deployment in production. They are open-sourced in \href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.

**Link**: [arxiv](http://arxiv.org/abs/2503.24000v1),  [pdf](http://arxiv.org/pdf/2503.24000v1)

**Tags**: cs.LG cs.AI 



### Deep Learning Model Deployment in Multiple Cloud Providers: an   Exploratory Study Using Low Computing Power Environments
**Authors**: Elayne Lemos, Rodrigo Oliveira, Jairson Rodrigues, Rosalvo F. Oliveira Neto

**Updated**: 2025-03-31T11:58:37Z

**Summary**: The deployment of Machine Learning models at cloud have grown by tech companies. Hardware requirements are higher when these models involve Deep Learning (DL) techniques and the cloud providers' costs may be a barrier. We explore deploying DL models using for experiments the GECToR model, a DL solution for Grammatical Error Correction, across three of the major cloud platforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware usage and cost at each cloud provider by 7 execution environments with 10 experiments reproduced. We found that while GPUs excel in performance, they had an average cost 300% higher than solutions without GPU. Our analysis also identifies that processor cache size is crucial for cost-effective CPU deployments, enabling over 50% of cost reduction compared to GPUs. This study demonstrates the feasibility and affordability of cloud-based DL inference solutions without GPUs, benefiting resource-constrained users like startups.

**Link**: [arxiv](http://arxiv.org/abs/2503.23988v1),  [pdf](http://arxiv.org/pdf/2503.23988v1)

**Tags**: cs.DC cs.AI cs.PF 68T07, 68U01 C.4; I.2.0; B.8.2 



### AirCache: Activating Inter-modal Relevancy KV Cache Compression for   Efficient Large Vision-Language Model Inference
**Authors**: Kai Huang, Hao Zou, Bochen Wang, Ye Xi, Zhen Xie, Hao Wang

**Updated**: 2025-03-31T11:13:18Z

**Summary**: Recent advancements in Large Visual Language Models (LVLMs) have gained significant attention due to their remarkable reasoning capabilities and proficiency in generalization. However, processing a large number of visual tokens and generating long-context outputs impose substantial computational overhead, leading to excessive demands for key-value (KV) cache. To address this critical bottleneck, we propose AirCache, a novel KV cache compression method aimed at accelerating LVLMs inference. This work systematically investigates the correlations between visual and textual tokens within the attention mechanisms of LVLMs. Our empirical analysis reveals considerable redundancy in cached visual tokens, wherein strategically eliminating these tokens preserves model performance while significantly accelerating context generation. Inspired by these findings, we introduce an elite observation window for assessing the importance of visual components in the KV cache, focusing on stable inter-modal relevancy modeling with enhanced multi-perspective consistency. Additionally, we develop an adaptive layer-wise budget allocation strategy that capitalizes on the strength and skewness of token importance distribution, showcasing superior efficiency compared to uniform allocation. Comprehensive evaluations across multiple LVLMs and benchmarks demonstrate that our method achieves comparable performance to the full cache while retaining only 10% of visual KV cache, thereby reducing decoding latency by 29% to 66% across various batch size and prompt length of inputs. Notably, as cache retention rates decrease, our method exhibits increasing performance advantages over existing approaches.

**Link**: [arxiv](http://arxiv.org/abs/2503.23956v1),  [pdf](http://arxiv.org/pdf/2503.23956v1)

**Tags**: cs.CV cs.AI 



### Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language   Models
**Authors**: Haotian Zhai, Xinyu Chen, Can Zhang, Tianming Sha, Ruirui Li

**Updated**: 2025-03-31T10:28:04Z

**Summary**: Test-time adaptation (TTA) of visual language models has recently attracted significant attention as a solution to the performance degradation caused by distribution shifts in downstream tasks. However, existing cache-based TTA methods have certain limitations. They mainly rely on the accuracy of cached feature labels, and the presence of noisy pseudo-labels can cause these features to deviate from their true distribution. This makes cache retrieval methods based on similarity matching highly sensitive to outliers or extreme samples. Moreover, current methods lack effective mechanisms to model class distributions, which limits their ability to fully exploit the potential of cached information. To address these challenges, we introduce a comprehensive and reliable caching mechanism and propose a novel zero-shot TTA method called "Cache, Residual, Gaussian" (CRG). This method not only employs learnable residual parameters to better align positive and negative visual prototypes with text prototypes, thereby optimizing the quality of cached features, but also incorporates Gaussian Discriminant Analysis (GDA) to dynamically model intra-class feature distributions, further mitigating the impact of noisy features. Experimental results on 13 benchmarks demonstrate that CRG outperforms state-of-the-art TTA methods, showcasing exceptional robustness and adaptability.

**Link**: [arxiv](http://arxiv.org/abs/2503.18334v2),  [pdf](http://arxiv.org/pdf/2503.18334v2)

**Tags**: cs.CV 



### Training-Free Text-Guided Image Editing with Visual Autoregressive Model
**Authors**: Yufei Wang, Lanqing Guo, Zhihao Li, Jiaxing Huang, Pichao Wang, Bihan Wen, Jian Wang

**Updated**: 2025-03-31T09:46:56Z

**Summary**: Text-guided image editing is an essential task that enables users to modify images through natural language descriptions. Recent advances in diffusion models and rectified flows have significantly improved editing quality, primarily relying on inversion techniques to extract structured noise from input images. However, inaccuracies in inversion can propagate errors, leading to unintended modifications and compromising fidelity. Moreover, even with perfect inversion, the entanglement between textual prompts and image features often results in global changes when only local edits are intended. To address these challenges, we propose a novel text-guided image editing framework based on VAR (Visual AutoRegressive modeling), which eliminates the need for explicit inversion while ensuring precise and controlled modifications. Our method introduces a caching mechanism that stores token indices and probability distributions from the original image, capturing the relationship between the source prompt and the image. Using this cache, we design an adaptive fine-grained masking strategy that dynamically identifies and constrains modifications to relevant regions, preventing unintended changes. A token reassembling approach further refines the editing process, enhancing diversity, fidelity, and control. Our framework operates in a training-free manner and achieves high-fidelity editing with faster inference speeds, processing a 1K resolution image in as fast as 1.2 seconds. Extensive experiments demonstrate that our method achieves performance comparable to, or even surpassing, existing diffusion- and rectified flow-based approaches in both quantitative metrics and visual quality. The code will be released.

**Link**: [arxiv](http://arxiv.org/abs/2503.23897v1),  [pdf](http://arxiv.org/pdf/2503.23897v1)

**Tags**: cs.CV cs.AI 



### Training-Free Exponential Context Extension via Cascading KV Cache
**Authors**: Jeffrey Willette, Heejun Lee, Youngwan Lee, Myeongjae Jeon, Sung Ju Hwang

**Updated**: 2025-03-31T03:28:44Z

**Summary**: The transformer's context window is vital for tasks such as few-shot learning and conditional generation as it preserves previous tokens for active memory. However, as the context lengths increase, the computational costs grow quadratically, hindering the deployment of large language models (LLMs) in real-world, long sequence scenarios. Although some recent key-value caching (KV Cache) methods offer linear inference complexity, they naively manage the stored context, prematurely evicting tokens and losing valuable information. Moreover, they lack an optimized prefill/prompt stage strategy, resulting in higher latency than even quadratic attention for realistic context sizes. In response, we introduce a novel mechanism that leverages cascading sub-cache buffers to selectively retain the most relevant tokens, enabling the model to maintain longer context histories without increasing the cache size. Our approach outperforms linear caching baselines across key benchmarks, including streaming perplexity, question answering, book summarization, and passkey retrieval, where it retains better retrieval accuracy at 1M tokens after four doublings of the cache size of 65K. Additionally, our method reduces prefill stage latency by a factor of 6.8 when compared to flash attention on 1M tokens. These innovations not only enhance the computational efficiency of LLMs but also pave the way for their effective deployment in resource-constrained environments, enabling large-scale, real-time applications with significantly reduced latency.

**Link**: [arxiv](http://arxiv.org/abs/2406.17808v4),  [pdf](http://arxiv.org/pdf/2406.17808v4)

**Tags**: cs.CL cs.AI cs.LG 



### Skip-Vision: Efficient and Scalable Acceleration of Vision-Language   Models via Adaptive Token Skipping
**Authors**: Weili Zeng, Ziyuan Huang, Kaixiang Ji, Yichao Yan

**Updated**: 2025-03-31T02:19:29Z

**Summary**: Transformer-based models have driven significant advancements in Multimodal Large Language Models (MLLMs), yet their computational costs surge drastically when scaling resolution, training data, and model parameters. A key bottleneck stems from the proliferation of visual tokens required for fine-grained image understanding. We propose Skip-Vision, a unified framework addressing both training and inference inefficiencies in vision-language models. On top of conventional token compression approaches, our method introduces two complementary acceleration strategies. For training acceleration, we observe that Feed-Forward Network (FFN) computations on visual tokens induce marginal feature updates. This motivates our Skip-FFN strategy, which bypasses FFN layers for redundant visual tokens. For inference acceleration, we design a selective KV-cache removal mechanism that prunes the skipped key-value pairs during decoding while preserving model performance. Experimental results demonstrate that Skip-Vision reduces training time by up to 35\%, inference FLOPs by 75\%, and latency by 45\%, while achieving comparable or superior performance to existing methods. Our work provides a practical solution for scaling high-performance MLLMs with enhanced efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2503.21817v2),  [pdf](http://arxiv.org/pdf/2503.21817v2)

**Tags**: cs.CV 



### EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient   Image Editing
**Authors**: Zexuan Yan, Yue Ma, Chang Zou, Wenteng Chen, Qifeng Chen, Linfeng Zhang

**Updated**: 2025-03-30T11:14:17Z

**Summary**: Inversion-based image editing is rapidly gaining momentum while suffering from significant computation overhead, hindering its application in real-time interactive scenarios. In this paper, we rethink that the redundancy in inversion-based image editing exists in both the spatial and temporal dimensions, such as the unnecessary computation in unedited regions and the redundancy in the inversion progress. To tackle these challenges, we propose a practical framework, named EEdit, to achieve efficient image editing. Specifically, we introduce three techniques to solve them one by one. For spatial redundancy, spatial locality caching is introduced to compute the edited region and its neighboring regions while skipping the unedited regions, and token indexing preprocessing is designed to further accelerate the caching. For temporal redundancy, inversion step skipping is proposed to reuse the latent for efficient editing. Our experiments demonstrate an average of 2.46 $\times$ acceleration without performance drop in a wide range of editing tasks including prompt-guided image editing, dragging and image composition. Our codes are available at https://github.com/yuriYanZeXuan/EEdit

**Link**: [arxiv](http://arxiv.org/abs/2503.10270v2),  [pdf](http://arxiv.org/pdf/2503.10270v2)

**Tags**: cs.CV 



### FB$^+$-tree: A Memory-Optimized B$^+$-tree with Latch-Free Update
**Authors**: Yuan Chen, Ao Li, Wenhai Li, Lingfeng Deng

**Updated**: 2025-03-30T11:09:06Z

**Summary**: B$^+$-trees are prevalent in traditional database systems due to their versatility and balanced structure. While binary search is typically utilized for branch operations, it may lead to inefficient cache utilization in main-memory scenarios. In contrast, trie-based index structures drive branch operations through prefix matching. While these structures generally produce fewer cache misses and are thus increasingly popular, they may underperform in range scans because of frequent pointer chasing. This paper proposes a new high-performance B$^+$-tree variant called \textbf{Feature B$^+$-tree (FB$^+$-tree)}. Similar to employing bit or byte for branch operation in tries, FB$^+$-tree progressively considers several bytes following the common prefix on each level of its inner nodes\textemdash referred to as features, which allows FB$^+$-tree to benefit from prefix skewness. FB$^+$-tree blurs the lines between B$^+$-trees and tries, while still retaining balance. In the best case, FB$^+$-tree almost becomes a trie, whereas in the worst case, it continues to function as a B$^+$-tree. Meanwhile, a crafted synchronization protocol that combines the link technique and optimistic lock is designed to support efficient concurrent index access. Distinctively, FB$^+$-tree leverages subtle atomic operations seamlessly coordinated with optimistic lock to facilitate latch-free updates, which can be easily extended to other structures. Intensive experiments on multiple workload-dataset combinations demonstrate that FB$^+$-tree shows comparable lookup performance to state-of-the-art trie-based indexes and outperforms popular B$^+$-trees by 2.3x$\ \sim\ $3.7x under 96 threads. FB$^+$-tree also exhibits significant potential on other workloads, especially update workloads under contention and scan workloads.

**Link**: [arxiv](http://arxiv.org/abs/2503.23397v1),  [pdf](http://arxiv.org/pdf/2503.23397v1)

**Tags**: cs.DB 



### COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP   Test-Time Adaptation
**Authors**: Fanding Huang, Jingyan Jiang, Qinting Jiang, Hebei Li, Faisal Nadeem Khan, Zhi Wang

**Updated**: 2025-03-30T10:34:45Z

**Summary**: Recent vision-language models (VLMs) face significant challenges in test-time adaptation to novel domains. While cache-based methods show promise by leveraging historical information, they struggle with both caching unreliable feature-label pairs and indiscriminately using single-class information during querying, significantly compromising adaptation accuracy. To address these limitations, we propose COSMIC (Clique-Oriented Semantic Multi-space Integration for CLIP), a robust test-time adaptation framework that enhances adaptability through multi-granular, cross-modal semantic caching and graph-based querying mechanisms. Our framework introduces two key innovations: Dual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual Semantics Graph constructs complementary semantic spaces by incorporating textual features, coarse-grained CLIP features, and fine-grained DINOv2 features to capture rich semantic relationships. Building upon these dual graphs, the Clique Guided Hyper-class component leverages structured class relationships to enhance prediction robustness through correlated class selection. Extensive experiments demonstrate COSMIC's superior performance across multiple benchmarks, achieving significant improvements over state-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on cross-domain generation with CLIP RN-50. Code is available at github.com/hf618/COSMIC.

**Link**: [arxiv](http://arxiv.org/abs/2503.23388v1),  [pdf](http://arxiv.org/pdf/2503.23388v1)

**Tags**: cs.CV cs.AI cs.LG cs.MM 



### A Unified Framework for Quantitative Cache Analysis
**Authors**: Sophie Kahlen, Jan Reineke

**Updated**: 2025-03-30T09:46:34Z

**Summary**: In this work we unify two existing lines of work towards cache analysis for non-LRU policies. To this end, we extend the notion of competitiveness to block competitiveness and systematically analyze the competitiveness and block competitiveness of FIFO and MRU relative to LRU for arbitrary associativities. We show how competitiveness and block competitiveness can be exploited in state-of-the-art WCET analysis based on the results of existing persistence analyses for LRU. Unlike prior work, our approach is applicable to microarchitectures that exhibit timing anomalies. We experimentally evaluate the precision and cost of our approach on benchmarks from TACLeBench. The experiments demonstrate that quantitative cache analysis for FIFO and MRU comes close to the precision of LRU.

**Link**: [arxiv](http://arxiv.org/abs/2503.16588v2),  [pdf](http://arxiv.org/pdf/2503.16588v2)

**Tags**: cs.PL 68 D.3.4 



### MVREC: A General Few-shot Defect Classification Model Using Multi-View   Region-Context
**Authors**: Shuai Lyu, Rongchen Zhang, Zeqi Ma, Fangjian Liao, Dongmei Mo, Waikeung Wong

**Updated**: 2025-03-30T09:19:53Z

**Summary**: Few-shot defect multi-classification (FSDMC) is an emerging trend in quality control within industrial manufacturing. However, current FSDMC research often lacks generalizability due to its focus on specific datasets. Additionally, defect classification heavily relies on contextual information within images, and existing methods fall short of effectively extracting this information. To address these challenges, we propose a general FSDMC framework called MVREC, which offers two primary advantages: (1) MVREC extracts general features for defect instances by incorporating the pre-trained AlphaCLIP model. (2) It utilizes a region-context framework to enhance defect features by leveraging mask region input and multi-view context augmentation. Furthermore, Few-shot Zip-Adapter(-F) classifiers within the model are introduced to cache the visual features of the support set and perform few-shot classification. We also introduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes 1228 defect images with instance-level mask annotations and 46 defect types. Extensive experiments conducted on MVTec-FS and four additional datasets demonstrate its effectiveness in general defect classification and its ability to incorporate contextual information to improve classification performance. Code: https://github.com/ShuaiLYU/MVREC

**Link**: [arxiv](http://arxiv.org/abs/2412.16897v2),  [pdf](http://arxiv.org/pdf/2412.16897v2)

**Tags**: cs.CV cs.AI 



### FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning
**Authors**: Hang Guo, Yawei Li, Taolin Zhang, Jiangshan Wang, Tao Dai, Shu-Tao Xia, Luca Benini

**Updated**: 2025-03-30T08:51:19Z

**Summary**: Visual Autoregressive (VAR) modeling has gained popularity for its shift towards next-scale prediction. However, existing VAR paradigms process the entire token map at each scale step, leading to the complexity and runtime scaling dramatically with image resolution. To address this challenge, we propose FastVAR, a post-training acceleration method for efficient resolution scaling with VARs. Our key finding is that the majority of latency arises from the large-scale step where most tokens have already converged. Leveraging this observation, we develop the cached token pruning strategy that only forwards pivotal tokens for scale-specific modeling while using cached tokens from previous scale steps to restore the pruned slots. This significantly reduces the number of forwarded tokens and improves the efficiency at larger resolutions. Experiments show the proposed FastVAR can further speedup FlashAttention-accelerated VAR by 2.7$\times$ with negligible performance drop of <1%. We further extend FastVAR to zero-shot generation of higher resolution images. In particular, FastVAR can generate one 2K image with 15GB memory footprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at https://github.com/csguoh/FastVAR.

**Link**: [arxiv](http://arxiv.org/abs/2503.23367v1),  [pdf](http://arxiv.org/pdf/2503.23367v1)

**Tags**: cs.CV 



### PQCache: Product Quantization-based KVCache for Long Context LLM   Inference
**Authors**: Hailin Zhang, Xiaodong Ji, Yilin Chen, Fangcheng Fu, Xupeng Miao, Xiaonan Nie, Weipeng Chen, Bin Cui

**Updated**: 2025-03-30T08:13:50Z

**Summary**: As the field of Large Language Models (LLMs) continues to evolve, the context length in inference is steadily growing. Key-Value Cache (KVCache), the intermediate representations of tokens within LLM inference, has now become the primary memory bottleneck due to limited GPU memory. Current methods selectively determine suitable keys and values for self-attention computation in LLMs to address the issue. However, they either fall short in maintaining model quality or result in high serving latency. Drawing inspiration from advanced embedding retrieval techniques prevalent in the data management community, we consider the storage and retrieval of KVCache as a typical embedding retrieval problem. We propose PQCache, which employs Product Quantization (PQ) to manage KVCache, maintaining model quality while ensuring low serving latency. During the prefilling phase, we apply PQ to tokens' keys for each LLM layer and head. During the autoregressive decoding phase, we use PQ codes and centroids to approximately identify important preceding tokens, then fetch the corresponding key-value pairs for self-attention computation. Through meticulous design of overlapping and caching, we minimize any additional computation and communication overhead during both phases. Extensive experiments demonstrate that PQCache achieves both effectiveness and efficiency, with 4.60% score improvement over existing methods on InfiniteBench and low system latency in both prefilling and decoding.

**Link**: [arxiv](http://arxiv.org/abs/2407.12820v2),  [pdf](http://arxiv.org/pdf/2407.12820v2)

**Tags**: cs.CL cs.AI cs.LG 



### Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context   LLM Inference
**Authors**: Wei Tao, Bin Zhang, Xiaoyang Qu, Jiguang Wan, Jianzong Wang

**Updated**: 2025-03-30T03:20:34Z

**Summary**: Recently, large language models (LLMs) have been able to handle longer and longer contexts. However, a context that is too long may cause intolerant inference latency and GPU memory usage. Existing methods propose mixed-precision quantization to the key-value (KV) cache in LLMs based on token granularity, which is time-consuming in the search process and hardware inefficient during computation. This paper introduces a novel approach called Cocktail, which employs chunk-adaptive mixed-precision quantization to optimize the KV cache. Cocktail consists of two modules: chunk-level quantization search and chunk-level KV cache computation. Chunk-level quantization search determines the optimal bitwidth configuration of the KV cache chunks quickly based on the similarity scores between the corresponding context chunks and the query, maintaining the model accuracy. Furthermore, chunk-level KV cache computation reorders the KV cache chunks before quantization, avoiding the hardware inefficiency caused by mixed-precision quantization in inference computation. Extensive experiments demonstrate that Cocktail outperforms state-of-the-art KV cache quantization methods on various models and datasets.

**Link**: [arxiv](http://arxiv.org/abs/2503.23294v1),  [pdf](http://arxiv.org/pdf/2503.23294v1)

**Tags**: cs.CL 



### Key, Value, Compress: A Systematic Exploration of KV Cache Compression   Techniques
**Authors**: Neusha Javidnia, Bita Darvish Rouhani, Farinaz Koushanfar

**Updated**: 2025-03-30T02:45:00Z

**Summary**: Large language models (LLMs) have demonstrated exceptional capabilities in generating text, images, and video content. However, as context length grows, the computational cost of attention increases quadratically with the number of tokens, presenting significant efficiency challenges. This paper presents an analysis of various Key-Value (KV) cache compression strategies, offering a comprehensive taxonomy that categorizes these methods by their underlying principles and implementation techniques. Furthermore, we evaluate their impact on performance and inference latency, providing critical insights into their effectiveness. Our findings highlight the trade-offs involved in KV cache compression and its influence on handling long-context scenarios, paving the way for more efficient LLM implementations.

**Link**: [arxiv](http://arxiv.org/abs/2503.11816v2),  [pdf](http://arxiv.org/pdf/2503.11816v2)

**Tags**: cs.CL 



### TopV: Compatible Token Pruning with Inference Time Optimization for Fast   and Low-Memory Multimodal Vision Language Model
**Authors**: Cheng Yang, Yang Sui, Jinqi Xiao, Lingyi Huang, Yu Gong, Chendi Li, Jinghua Yan, Yu Bai, Ponnuswamy Sadayappan, Xia Hu, Bo Yuan

**Updated**: 2025-03-29T23:00:27Z

**Summary**: Vision-Language Models (VLMs) demand substantial computational resources during inference, largely due to the extensive visual input tokens for representing visual information. Previous studies have noted that visual tokens tend to receive less attention than text tokens, suggesting their lower importance during inference and potential for pruning. However, their methods encounter several challenges: reliance on greedy heuristic criteria for token importance and incompatibility with FlashAttention and KV cache. To address these issues, we introduce \textbf{TopV}, a compatible \textbf{TO}ken \textbf{P}runing with inference Time Optimization for fast and low-memory \textbf{V}LM, achieving efficient pruning without additional training or fine-tuning. Instead of relying on attention scores, we formulate token pruning as an optimization problem, accurately identifying important visual tokens while remaining compatible with FlashAttention. Additionally, since we only perform this pruning once during the prefilling stage, it effectively reduces KV cache size. Our optimization framework incorporates a visual-aware cost function considering factors such as Feature Similarity, Relative Spatial Distance, and Absolute Central Distance, to measure the importance of each source visual token, enabling effective pruning of low-importance tokens. Extensive experiments demonstrate that our method outperforms previous token pruning methods, validating the effectiveness and efficiency of our approach.

**Link**: [arxiv](http://arxiv.org/abs/2503.18278v2),  [pdf](http://arxiv.org/pdf/2503.18278v2)

**Tags**: cs.CV cs.AI 



### X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and   Extreme KV Compression
**Authors**: Guihong Li, Mehdi Rezagholizadeh, Mingyu Yang, Vikram Appia, Emad Barsoum

**Updated**: 2025-03-29T04:43:11Z

**Summary**: Multi-head latent attention (MLA) is designed to optimize KV cache memory through low-rank key-value joint compression. Rather than caching keys and values separately, MLA stores their compressed latent representations, reducing memory overhead while maintaining the performance. While MLA improves memory efficiency without compromising language model accuracy, its major limitation lies in its integration during the pre-training phase, requiring models to be trained from scratch. This raises a key question: can we use MLA's benefits fully or partially in models that have already been pre-trained with different attention mechanisms? In this paper, we propose X-EcoMLA to deploy post training distillation to enable the upcycling of Transformer-based attention into an efficient hybrid MLA variant through lightweight post-training adaptation, bypassing the need for extensive pre-training. We demonstrate that leveraging the dark knowledge of a well-trained model can enhance training accuracy and enable extreme KV cache compression in MLA without compromising model performance. The experimental results show that our proposed method can effectively compress the KV cache while preserving the performance on the benchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression achieves the same average score by using only 3.6B training tokens and 70 GPU hours on AMD MI300, whereas a 10.6x compression have less than 0.1\% average score drop with 7B training tokens and 140 GPU hours.

**Link**: [arxiv](http://arxiv.org/abs/2503.11132v2),  [pdf](http://arxiv.org/pdf/2503.11132v2)

**Tags**: cs.CL 



### SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with   Sweep Reconstruction
**Authors**: Zikang Yuan, Ruiye Ming, Chengwei Zhao, Yonghao Tan, Pingcheng Dong, Hongcheng Luo, Yuzhong Jiao, Xin Yang, Kwang-Ting Cheng

**Updated**: 2025-03-29T01:06:54Z

**Summary**: Addressing the inherent low acquisition frequency limitation of 3D LiDAR to achieve high-frequency output has become a critical research focus in the LiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance, frequency-enhanced LIO systems must process each sweep within significantly reduced timeframe, which presents substantial challenges for deployment on low-computational-power platforms. To address these limitations, we introduce SR-LIO++, an innovative LIO system capable of achieving doubled output frequency relative to input frequency on resource-constrained hardware platforms, including the Raspberry Pi 4B. Our system employs a sweep reconstruction methodology to enhance LiDAR sweep frequency, generating high-frequency reconstructed sweeps. Building upon this foundation, we propose a caching mechanism for intermediate results (i.e., surface parameters) of the most recent segments, effectively minimizing redundant processing of common segments in adjacent reconstructed sweeps. This method decouples processing time from the traditionally linear dependence on reconstructed sweep frequency. Furthermore, we present a quantized map point management based on index table mapping, significantly reducing memory usage by converting global 3D point storage from 64-bit double precision to 8-bit char representation. This method also converts the computationally intensive Euclidean distance calculations in nearest neighbor searches from 64-bit double precision to 16-bit short and 32-bit integer formats, significantly reducing both memory and computational cost. Extensive experimental evaluations across three distinct computing platforms and four public datasets demonstrate that SR-LIO++ maintains state-of-the-art accuracy while substantially enhancing efficiency. Notably, our system successfully achieves 20Hz state output on Raspberry Pi 4B hardware.

**Link**: [arxiv](http://arxiv.org/abs/2503.22926v1),  [pdf](http://arxiv.org/pdf/2503.22926v1)

**Tags**: cs.RO 



### A Pilot Study on Tunable Precision Emulation via Automatic BLAS   Offloading
**Authors**: Hang Liu, Junjie Li, Yinzhi Wang

**Updated**: 2025-03-28T21:02:32Z

**Summary**: This study explores the use of automatic BLAS offloading and INT8-based emulation for accelerating traditional HPC workloads on modern GPU architectures. Through the use of low-bitwidth integer units and cache-coherent Unified Memory Architecture, we emulate double-precision matrix multiplications in the MuST application without code changes. We find that accuracy depends on both arithmetic precision and the properties of the operator, which can be dealt with through tunable precision emulation. Unlike traditional mixed-precision approaches, this method preserves original algorithms while optimizing hardware utilization. We showcases the potential of improving accuracy and performance at the same time. This work highlights the potential of AI-driven hardware to transform HPC, advocating for adaptive precision strategies in future scientific computing.

**Link**: [arxiv](http://arxiv.org/abs/2503.22875v1),  [pdf](http://arxiv.org/pdf/2503.22875v1)

**Tags**: cs.DC cs.PF 



### DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality   Diffusion Transformers
**Authors**: Hanling Zhang, Rundong Su, Zhihang Yuan, Pengtao Chen, Mingzhu Shen Yibo Fan, Shengen Yan, Guohao Dai, Yu Wang

**Updated**: 2025-03-28T18:00:12Z

**Summary**: Text-to-image generation models, especially Multimodal Diffusion Transformers (MMDiT), have shown remarkable progress in generating high-quality images. However, these models often face significant computational bottlenecks, particularly in attention mechanisms, which hinder their scalability and efficiency. In this paper, we introduce DiTFastAttnV2, a post-training compression method designed to accelerate attention in MMDiT. Through an in-depth analysis of MMDiT's attention patterns, we identify key differences from prior DiT-based methods and propose head-wise arrow attention and caching mechanisms to dynamically adjust attention heads, effectively bridging this gap. We also design an Efficient Fused Kernel for further acceleration. By leveraging local metric methods and optimization techniques, our approach significantly reduces the search time for optimal compression schemes to just minutes while maintaining generation quality. Furthermore, with the customized kernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x end-to-end speedup on 2K image generation without compromising visual fidelity.

**Link**: [arxiv](http://arxiv.org/abs/2503.22796v1),  [pdf](http://arxiv.org/pdf/2503.22796v1)

**Tags**: cs.CV cs.AI 



### Towards Stabilized and Efficient Diffusion Transformers through   Long-Skip-Connections with Spectral Constraints
**Authors**: Guanjie Chen, Xinyu Zhao, Yucheng Zhou, Xiaoye Qu, Tianlong Chen, Yu Cheng

**Updated**: 2025-03-28T16:15:19Z

**Summary**: Diffusion Transformers (DiT) have emerged as a powerful architecture for image and video generation, offering superior quality and scalability. However, their practical application suffers from inherent dynamic feature instability, leading to error amplification during cached inference. Through systematic analysis, we identify the absence of long-range feature preservation mechanisms as the root cause of unstable feature propagation and perturbation sensitivity. To this end, we propose Skip-DiT, a novel DiT variant enhanced with Long-Skip-Connections (LSCs) - the key efficiency component in U-Nets. Theoretical spectral norm and visualization analysis demonstrate how LSCs stabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic feature enable an efficient statical caching mechanism that reuses deep features across timesteps while updating shallow components. Extensive experiments across image and video generation tasks demonstrate that Skip-DiT achieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2 times inference acceleration without quality loss and high fidelity to original output, outperforming existing DiT caching methods across various quantitative metrics. Our findings establish long-skip connections as critical architectural components for training stable and efficient diffusion transformers.

**Link**: [arxiv](http://arxiv.org/abs/2411.17616v3),  [pdf](http://arxiv.org/pdf/2411.17616v3)

**Tags**: cs.CV 



### DyCoke: Dynamic Compression of Tokens for Fast Video Large Language   Models
**Authors**: Keda Tao, Can Qin, Haoxuan You, Yang Sui, Huan Wang

**Updated**: 2025-03-28T14:11:37Z

**Summary**: Video large language models (VLLMs) have significantly advanced recently in processing complex video content, yet their inference efficiency remains constrained because of the high computational cost stemming from the thousands of visual tokens generated from the video inputs. We empirically observe that, unlike single image inputs, VLLMs typically attend visual tokens from different frames at different decoding iterations, making a one-shot pruning strategy prone to removing important tokens by mistake. Motivated by this, we present DyCoke, a training-free token compression method to optimize token representation and accelerate VLLMs. DyCoke incorporates a plug-and-play temporal compression module to minimize temporal redundancy by merging redundant tokens across frames, and applies dynamic KV cache reduction to prune spatially redundant tokens selectively. It ensures high-quality inference by dynamically retaining the critical tokens at each decoding step. Extensive experimental results demonstrate that DyCoke can outperform the prior SoTA counterparts, achieving 1.5X inference speedup, 1.4X memory reduction against the baseline VLLM, while still improving the performance, with no training.

**Link**: [arxiv](http://arxiv.org/abs/2411.15024v3),  [pdf](http://arxiv.org/pdf/2411.15024v3)

**Tags**: cs.CV cs.LG 



### A Refined Analysis of Massive Activations in LLMs
**Authors**: Louis Owen, Nilabhra Roy Chowdhury, Abhay Kumar, Fabian GÃ¼ra

**Updated**: 2025-03-28T11:08:34Z

**Summary**: Motivated in part by their relevance for low-precision training and quantization, massive activations in large language models (LLMs) have recently emerged as a topic of interest. However, existing analyses are limited in scope, and generalizability across architectures is unclear. This paper helps address some of these gaps by conducting an analysis of massive activations across a broad range of LLMs, including both GLU-based and non-GLU-based architectures. Our findings challenge several prior assumptions, most importantly: (1) not all massive activations are detrimental, i.e. suppressing them does not lead to an explosion of perplexity or a collapse in downstream task performance; (2) proposed mitigation strategies such as Attention KV bias are model-specific and ineffective in certain cases. We consequently investigate novel hybrid mitigation strategies; in particular pairing Target Variance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT) successfully balances the mitigation of massive activations with preserved downstream model performance in the scenarios we investigated. Our code is available at: https://github.com/bluorion-com/refine_massive_activations.

**Link**: [arxiv](http://arxiv.org/abs/2503.22329v1),  [pdf](http://arxiv.org/pdf/2503.22329v1)

**Tags**: cs.CL 



### EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge   Devices
**Authors**: Jiyu Chen, Shuang Peng, Daxiong Luo, Fan Yang, Renshou Wu, Fangyuan Li, Xiaoxin Chen

**Updated**: 2025-03-28T07:26:37Z

**Summary**: Transformer-based large language models (LLMs) encounter challenges in processing long sequences on edge devices due to the quadratic complexity of attention mechanisms and growing memory demands from Key-Value (KV) cache. Existing KV cache optimizations struggle with irreversible token eviction in long-output tasks, while alternative sequence modeling architectures prove costly to adopt within established Transformer infrastructure. We present EdgeInfinite, a memory-efficient solution for infinite contexts that integrates compressed memory into Transformer-based LLMs through a trainable memory-gating module. This approach maintains full compatibility with standard Transformer architectures, requiring fine-tuning only a small part of parameters, and enables selective activation of the memory-gating module for long and short context task routing. The experimental result shows that EdgeInfinite achieves comparable performance to baseline Transformer-based LLM on long context benchmarks while optimizing memory consumption and time to first token.

**Link**: [arxiv](http://arxiv.org/abs/2503.22196v1),  [pdf](http://arxiv.org/pdf/2503.22196v1)

**Tags**: cs.CL 



### Performance Characterizations and Usage Guidelines of Samsung CXL Memory   Module Hybrid Prototype
**Authors**: Jianping Zeng, Shuyi Pei, Da Zhang, Yuchen Zhou, Amir Beygi, Xuebin Yao, Ramdas Kachare, Tong Zhang, Zongwang Li, Marie Nguyen, Rekha Pitchumani, Yang Soek Ki, Changhee Jung

**Updated**: 2025-03-27T22:16:57Z

**Summary**: The growing prevalence of data-intensive workloads, such as artificial intelligence (AI), machine learning (ML), high-performance computing (HPC), in-memory databases, and real-time analytics, has exposed limitations in conventional memory technologies like DRAM. While DRAM offers low latency and high throughput, it is constrained by high costs, scalability challenges, and volatility, making it less viable for capacity-bound and persistent applications in modern datacenters.   Recently, Compute Express Link (CXL) has emerged as a promising alternative, enabling high-speed, cacheline-granular communication between CPUs and external devices. By leveraging CXL technology, NAND flash can now be used as memory expansion, offering three-fold benefits: byte-addressability, scalable capacity, and persistence at a low cost. Samsung's CXL Memory Module Hybrid (CMM-H) is the first product to deliver these benefits through a hardware-only solution, i.e., it does not incur any OS and IO overheads like conventional block devices. In particular, CMM-H integrates a DRAM cache with NAND flash in a single device to deliver near-DRAM latency. This paper presents the first publicly available study for comprehensive characterizations of an FPGA-based CMM-H prototype. Through this study, we address users' concerns about whether a wide variety of applications can successfully run on a memory device backed by NAND flash medium. Additionally, based on these characterizations, we provide key insights into how to best take advantage of the CMM-H device.

**Link**: [arxiv](http://arxiv.org/abs/2503.22017v1),  [pdf](http://arxiv.org/pdf/2503.22017v1)

**Tags**: cs.AR 



### Reimagining Memory Access for LLM Inference: Compression-Aware Memory   Controller Design
**Authors**: Rui Xie, Asad Ul Haq, Linsen Ma, Yunhua Fang, Zirak Burzin Engineer, Liu Liu, Tong Zhang

**Updated**: 2025-03-27T17:48:14Z

**Summary**: The efficiency of Large Language Model~(LLM) inference is often constrained by substantial memory bandwidth and capacity demands. Existing techniques, such as pruning, quantization, and mixture of experts/depth, reduce memory capacity and/or bandwidth consumption at the cost of slight degradation in inference quality. This paper introduces a design solution that further alleviates memory bottlenecks by enhancing the on-chip memory controller in AI accelerators to achieve two main objectives: (1) significantly reducing memory capacity and bandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of model weights and key-value (KV) cache without compromising inference quality, and (2) enabling memory bandwidth and energy consumption to scale proportionally with context-dependent dynamic quantization. These goals are accomplished by equipping the on-chip memory controller with mechanisms to improve fine-grained bit-level accessibility and compressibility of weights and KV cache through LLM-aware configuration of in-memory placement and representation. Experimental results on publicly available LLMs demonstrate the effectiveness of this approach, showing memory footprint reductions of 25.2\% for model weights and 46.9\% for KV cache. In addition, our hardware prototype at 4\,GHz and 32 lanes (7\,nm) achieves 8\,TB/s throughput with a modest area overhead (under 3.8\,mm\(^2\)), which underscores the viability of LLM-aware memory control as a key to efficient large-scale inference.

**Link**: [arxiv](http://arxiv.org/abs/2503.18869v2),  [pdf](http://arxiv.org/pdf/2503.18869v2)

**Tags**: cs.AR 



### Low-noise environment for probing fundamental symmetries
**Authors**: F. J. Collings, N. J. Fitch, J. M. Dyne, R. A. Jenkins, E. Wursten, M. T. Ziemba, X. S. Zheng, F. Castellini, J. Lim, B. E. Sauer, M. R. Tarbutt

**Updated**: 2025-03-27T17:37:12Z

**Summary**: We present the design and characterization of a low-noise environment for measuring the electron's electric dipole moment (EDM) with a beam of molecules. To minimize magnetic Johnson noise from metals, the design features ceramic electric field plates housed in a glass vacuum chamber. To suppress external magnetic noise the apparatus is enclosed within a cylindrical four-layer mu-metal shield with a shielding factor exceeding $10^6$ in one radial direction and $10^5$ in the other. Finite element modelling shows that the difference between these shielding factors is due to imperfect joints between sections of mu-metal. Using atomic magnetometers to monitor the magnetic field inside the shield, we measure noise below 40 fT/$\sqrt{{\rm Hz}}$ at 1 Hz and above, rising to 500 fT/$\sqrt{{\rm Hz}}$ at 0.1 Hz. Analytical and numerical studies show that residual magnetic Johnson noise contributes approximately 13 fT/$\sqrt{{\rm Hz}}$. The background magnetic field averaged along the beamline is maintained below 3 pT, with typical gradients of a few nT/m. An electric field of 20 kV/cm is applied without discharges and with leakage currents below 1 nA. Each magnetometer measures the magnetic field correlated with the direction of the applied electric field with a precision of 0.11 fT in 104 hours of data. These results demonstrate that the apparatus is suitable for measuring the electron EDM with precision at the $10^{-31}$ e cm level. The design principles and characterization techniques presented here are broadly applicable to precision measurements probing fundamental symmetries in molecules, atoms, and neutrons.

**Link**: [arxiv](http://arxiv.org/abs/2503.21725v1),  [pdf](http://arxiv.org/pdf/2503.21725v1)

**Tags**: physics.atom-ph 



### Spineless Traversal for Layout Invalidation
**Authors**: Marisa Kirisame, Tiezhi Wang, Pavel Panchekha

**Updated**: 2025-03-27T15:21:19Z

**Summary**: Latency is a major concern for web rendering engines like those in Chrome, Safari, and Firefox. These engines reduce latency by using an incremental layout algorithm to redraw the page when the user interacts with it. In such an algorithm, elements that change frame-to-frame are marked dirty; only the dirty elements need be processed to draw the next frame, dramatically reducing latency. However, the standard incremental layout algorithm must search the page for dirty elements, accessing a number of auxiliary elements in the process. These auxiliary elements add cache misses and stalled cycles, and are responsible for a sizable fraction of all layout latency. We introduce a new, faster incremental layout algorithm called Spineless Traversal. Spineless Traversal uses a more computationally demanding priority queue algorithm to avoid the need to access auxiliary nodes and thus reduces cache traffic and stalls. This leads to dramatic speedups on the most latency-critical interactions such as hovering, typing, or animations. Moreover, thanks to numerous low-level optimizations, we are able to make Spineless Traversal competitive across the whole spectrum of incremental layout workloads. As a result, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the benchmark, with a mean speedup of 3.23x concentrated in the most latency-critical interactions such as hovering, typing, and animations.

**Link**: [arxiv](http://arxiv.org/abs/2411.10659v4),  [pdf](http://arxiv.org/pdf/2411.10659v4)

**Tags**: cs.PL 



### WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for   Efficient LLM Inference
**Authors**: Youhui Zuo, Sibo Wei, Chen Zhang, Zhuorui Liu, Wenpeng Lu, Dawei Song

**Updated**: 2025-03-27T14:11:37Z

**Summary**: With the advancements in long-context inference capabilities of large language models (LLMs), the KV cache has become one of the foundational components. However, its substantial GPU memory consumption makes KV cache compression a key technique for enabling efficient LLM inference in industrial scenarios. While recent studies have focused on optimizing the memory occupied by the KV cache, they overlook two critical factors: preserving semantic coherence and considering task-specific characteristic during compression. To address these limitations, we propose a novel task-adaptive KV cache window selection method, WindowKV. WindowKV dynamically selects local semantic windows consisting of consecutive tokens, according to task-specific characteristics, ensuring the retained KV cache captures continuous, essential context. Additionally, we introduce an intra-group layer KV cache indices sharing strategy to reduce computational overhead, achieving a balance between performance and efficiency. We rigorously evaluate WindowKV on the LongBench benchmark, and the results demonstrate that it maintains a performance comparable to full KV cache retention while using only 12% of the original KV cache, significantly reducing memory requirements. Furthermore, our method also achieves state-of-the-art results in the Needle-in-a-Haystack evaluation, highlighting its effectiveness and robustness.

**Link**: [arxiv](http://arxiv.org/abs/2503.17922v2),  [pdf](http://arxiv.org/pdf/2503.17922v2)

**Tags**: cs.CL 



### Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation
**Authors**: Ashutosh Pradhan, Daniele Ottaviano, Yi Jiang, Haozheng Huang, Alexander Zuepke, Andrea Bastoni, Marco Caccamo

**Updated**: 2025-03-27T12:14:56Z

**Summary**: The increasing complexity of embedded hardware platforms poses significant challenges for real-time workloads. Architectural features such as Intel RDT, Arm QoS, and Arm MPAM are either unavailable on commercial embedded platforms or designed primarily for server environments optimized for average-case performance and might fail to deliver the expected real-time guarantees. Arm DynamIQ Shared Unit (DSU) includes isolation features-among others, hardware per-way cache partitioning-that can improve the real-time guarantees of complex embedded multicore systems and facilitate real-time analysis. However, the DSU also targets average cases, and its real-time capabilities have not yet been evaluated. This paper presents the first comprehensive analysis of three real-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and NVIDIA Orin platforms. We integrate support for the DSU at the operating system and hypervisor level and conduct a large-scale evaluation using both synthetic and real-world benchmarks with varying types and intensities of interference. Our results make extensive use of performance counters and indicate that, although effective, the quality of partitioning and isolation provided by the DSU depends on the type and the intensity of the interfering workloads. In addition, we uncover and analyze in detail the correlation between benchmarks and different types and intensities of interference.

**Link**: [arxiv](http://arxiv.org/abs/2503.17038v3),  [pdf](http://arxiv.org/pdf/2503.17038v3)

**Tags**: cs.PF cs.AR 68M20 C.3; C.4; D.4.7 



### Rethinking Video Tokenization: A Conditioned Diffusion-based Approach
**Authors**: Nianzu Yang, Pandeng Li, Liming Zhao, Yang Li, Chen-Wei Xie, Yehui Tang, Xudong Lu, Zhihang Liu, Yun Zheng, Yu Liu, Junchi Yan

**Updated**: 2025-03-27T11:46:22Z

**Summary**: Existing video tokenizers typically use the traditional Variational Autoencoder (VAE) architecture for video compression and reconstruction. However, to achieve good performance, its training process often relies on complex multi-stage training tricks that go beyond basic reconstruction loss and KL regularization. Among these tricks, the most challenging is the precise tuning of adversarial training with additional Generative Adversarial Networks (GANs) in the final stage, which can hinder stable convergence. In contrast to GANs, diffusion models offer more stable training processes and can generate higher-quality results. Inspired by these advantages, we propose CDT, a novel Conditioned Diffusion-based video Tokenizer, that replaces the GAN-based decoder with a conditional causal diffusion model. The encoder compresses spatio-temporal information into compact latents, while the decoder reconstructs videos through a reverse diffusion process conditioned on these latents. During inference, we incorporate a feature cache mechanism to generate videos of arbitrary length while maintaining temporal continuity and adopt sampling acceleration technique to enhance efficiency. Trained using only a basic MSE diffusion loss for reconstruction, along with KL term and LPIPS perceptual loss from scratch, extensive experiments demonstrate that CDT achieves state-of-the-art performance in video reconstruction tasks with just a single-step sampling. Even a scaled-down version of CDT (3$\times$ inference speedup) still performs comparably with top baselines. Moreover, the latent video generation model trained with CDT also exhibits superior performance. The source code and pretrained weights are available at https://github.com/ali-vilab/CDT.

**Link**: [arxiv](http://arxiv.org/abs/2503.03708v3),  [pdf](http://arxiv.org/pdf/2503.03708v3)

**Tags**: cs.CV cs.AI 



### FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide   Physical Links and End-to-End AXI4 Parallel Multi-Stream Support
**Authors**: Tim Fischer, Michael Rogenmoser, Thomas Benz, Frank K. GÃ¼rkaynak, Luca Benini

**Updated**: 2025-03-27T09:53:15Z

**Summary**: The new generation of domain-specific AI accelerators is characterized by rapidly increasing demands for bulk data transfers, as opposed to small, latency-critical cache line transfers typical of traditional cache-coherent systems. In this paper, we address this critical need by introducing the FlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible Interface (AXI4) compliant links designed to meet the massive bandwidth needs at high energy efficiency. At the transport level, non-blocking transactions are supported for latency tolerance. Additionally, a novel end-to-end ordering approach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA) engine simplifies network interfaces and eliminates inter-stream dependencies. Furthermore, dedicated physical links are instantiated for short, latency-critical messages. A complete end-to-end reference implementation in 12nm FinFET technology demonstrates the physical feasibility and power performance area (PPA) benefits of our approach. Utilizing wide links on high levels of metal, we achieve a bandwidth of 645 Gbps per link and a total aggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles, with a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of only 3.5% per compute tile and achieves a leading-edge energy efficiency of 0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers three times the energy efficiency and more than double the link bandwidth. Furthermore, compared to a traditional AXI4-based multi-layer interconnect, our NoC achieves a 30% reduction in area, corresponding to a 47% increase in GFLOPSDP within the same floorplan.

**Link**: [arxiv](http://arxiv.org/abs/2409.17606v2),  [pdf](http://arxiv.org/pdf/2409.17606v2)

**Tags**: cs.AR 



### Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer   Decoding
**Authors**: Yifang Chen, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Yu Tian

**Updated**: 2025-03-27T07:02:19Z

**Summary**: The key-value (KV) cache in the tensor version of transformers presents a significant bottleneck during inference. While previous work analyzes the fundamental space complexity barriers in standard attention mechanisms [Haris and Onak, 2025], our work generalizes the space complexity barriers result to tensor attention version. Our theoretical contributions rely on a reduction from communication complexity and deduce the memory lower bound for tensor-structured attention mechanisms when $d = \Omega(\log n)$. Furthermore, we introduce two types of tensor attention cache and present a trade-off between time and memory for two scenarios. Overall, our work provides a theoretical foundation for us to understand the time-memory tradeoff of KV-Cache compression in tensor attention decoding and offers more perspectives in developing more memory-efficient tensor attention Transformer architectures.

**Link**: [arxiv](http://arxiv.org/abs/2503.11108v2),  [pdf](http://arxiv.org/pdf/2503.11108v2)

**Tags**: cs.LG cs.AI cs.CC cs.CL 



### MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context   Generation with Speculative Decoding
**Authors**: Ranajoy Sadhukhan, Jian Chen, Zhuoming Chen, Vashisth Tiwari, Ruihang Lai, Jinyuan Shi, Ian En-Hsu Yen, Avner May, Tianqi Chen, Beidi Chen

**Updated**: 2025-03-26T17:42:17Z

**Summary**: Large Language Models (LLMs) have become more prevalent in long-context applications such as interactive chatbots, document analysis, and agent workflows, but it is challenging to serve long-context requests with low latency and high throughput. Speculative decoding (SD) is a widely used technique to reduce latency losslessly, but the conventional wisdom suggests that its efficacy is limited to small batch sizes. In MagicDec, we show that surprisingly SD can achieve speedup even for a high throughput inference regime for moderate to long sequences. More interestingly, an intelligent drafting strategy can achieve better speedup with increasing batch size based on our rigorous analysis. MagicDec first identifies the bottleneck shifts with increasing batch size and sequence length, and uses these insights to deploy SD more effectively for high throughput inference. We leverage draft model with sparse KV cache to address the KV bottleneck, which scales with both sequence length and batch size. Additionally, we propose a theoretical model to select the optimal drafting strategy for maximum speedup. Our work highlights the broad applicability of speculative decoding in long-context serving, as it can enhance throughput and reduce latency without compromising accuracy. For moderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B when serving batch sizes ranging from 32 to 256 on various types of hardware and tasks.

**Link**: [arxiv](http://arxiv.org/abs/2408.11049v4),  [pdf](http://arxiv.org/pdf/2408.11049v4)

**Tags**: cs.CL 



### Unleashing Vecset Diffusion Model for Fast Shape Generation
**Authors**: Zeqiang Lai, Yunfei Zhao, Zibo Zhao, Haolin Liu, Fuyun Wang, Huiwen Shi, Xianghui Yang, Qingxiang Lin, Jingwei Huang, Yuhong Liu, Jie Jiang, Chunchao Guo, Xiangyu Yue

**Updated**: 2025-03-26T15:08:12Z

**Summary**: 3D shape generation has greatly flourished through the development of so-called "native" 3D diffusion, particularly through the Vecset Diffusion Model (VDM). While recent advancements have shown promising results in generating high-resolution 3D shapes, VDM still struggles with high-speed generation. Challenges exist because of difficulties not only in accelerating diffusion sampling but also VAE decoding in VDM, areas under-explored in previous works. To address these challenges, we present FlashVDM, a systematic framework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables flexible diffusion sampling with as few as 5 inference steps and comparable quality, which is made possible by stabilizing consistency distillation with our newly introduced Progressive Flow Distillation. For VAE, we introduce a lightning vecset decoder equipped with Adaptive KV Selection, Hierarchical Volume Decoding, and Efficient Network Design. By exploiting the locality of the vecset and the sparsity of shape surface in the volume, our decoder drastically lowers FLOPs, minimizing the overall decoding overhead. We apply FlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic evaluation, we show that our model significantly outperforms existing fast 3D generation methods, achieving comparable performance to the state-of-the-art while reducing inference time by over 45x for reconstruction and 32x for generation. Code and models are available at https://github.com/Tencent/FlashVDM.

**Link**: [arxiv](http://arxiv.org/abs/2503.16302v2),  [pdf](http://arxiv.org/pdf/2503.16302v2)

**Tags**: cs.CV cs.AI eess.IV 



### SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN   Inference on NVIDIA GPUs
**Authors**: Yongkang Zhang, Haoxuan Yu, Chenxia Han, Cheng Wang, Baotong Lu, Yunzhe Li, Zhifeng Jiang, Yang Li, Xiaowen Chu, Huaicheng Li

**Updated**: 2025-03-26T13:59:53Z

**Summary**: Cloud service providers heavily colocate high-priority, latency-sensitive (LS), and low-priority, best-effort (BE) DNN inference services on the same GPU to improve resource utilization in data centers. Among the critical shared GPU resources, there has been very limited analysis on the dynamic allocation of compute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU resource management solutions are either hardware-specific, or unable to dynamically allocate resources to different tenants, or both; (2) NVIDIA doesn't expose interfaces for VRAM bandwidth allocation, and the software stack and VRAM channel architectures are black-box, both of which limit the software-level resource management. These drive prior work to design either conservative sharing policies detrimental to throughput, or static resource partitioning only applicable to a few GPU models.   To bridge this gap, this paper proposes SGDRC, a fully software-defined dynamic VRAM bandwidth and compute unit management solution for concurrent DNN inference services. SGDRC aims at guaranteeing service quality, maximizing the overall throughput, and providing general applicability to NVIDIA GPUs. SGDRC first reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs through comprehensive reverse engineering and eliminates VRAM channel conflicts using software-level cache coloring. SGDRC applies bimodal tensors and tidal SM masking to dynamically allocate VRAM bandwidth and compute units, and guides the allocation of resources based on offline profiling. We evaluate 11 mainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show that compared with the state-of-the-art GPU sharing solutions, SGDRC achieves the highest SLO attainment rates (99.0% on average), and improves overall throughput by up to 1.47x and BE job throughput by up to 2.36x.

**Link**: [arxiv](http://arxiv.org/abs/2407.13996v3),  [pdf](http://arxiv.org/pdf/2407.13996v3)

**Tags**: cs.DC cs.AR cs.PF D.4.9; I.2.5 



### Analyzing Modern NVIDIA GPU cores
**Authors**: Rodrigo Huerta, Mojtaba Abaie Shoushtary, JosÃ©-Lorenzo Cruz, Antonio GonzÃ¡lez

**Updated**: 2025-03-26T12:10:53Z

**Summary**: GPUs are the most popular platform for accelerating HPC workloads, such as artificial intelligence and science simulations. However, most microarchitectural research in academia relies on GPU core pipeline designs based on architectures that are more than 15 years old.   This paper reverse engineers modern NVIDIA GPU cores, unveiling many key aspects of its design and explaining how GPUs leverage hardware-compiler techniques where the compiler guides hardware during execution. In particular, it reveals how the issue logic works including the policy of the issue scheduler, the structure of the register file and its associated cache, and multiple features of the memory pipeline. Moreover, it analyses how a simple instruction prefetcher based on a stream buffer fits well with modern NVIDIA GPUs and is likely to be used. Furthermore, we investigate the impact of the register file cache and the number of register file read ports on both simulation accuracy and performance.   By modeling all these new discovered microarchitectural details, we achieve 18.24% lower mean absolute percentage error (MAPE) in execution cycles than previous state-of-the-art simulators, resulting in an average of 13.98% MAPE with respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that this new model stands for other NVIDIA architectures, such as Turing. Finally, we show that the software-based dependence management mechanism included in modern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in terms of performance and area.

**Link**: [arxiv](http://arxiv.org/abs/2503.20481v1),  [pdf](http://arxiv.org/pdf/2503.20481v1)

**Tags**: cs.AR 



### Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and   Generalizable Point Cloud Analysis
**Authors**: Hongyu Sun, Qiuhong Ke, Ming Cheng, Yongcai Wang, Deying Li, Chenhui Gou, Jianfei Cai

**Updated**: 2025-03-26T11:08:20Z

**Summary**: This paper proposes a general solution to enable point cloud recognition models to handle distribution shifts at test time. Unlike prior methods, which rely heavily on training data (often inaccessible during online inference) and are limited to recognizing a fixed set of point cloud classes predefined during training, we explore a more practical and challenging scenario: adapting the model solely based on online test data to recognize both previously seen classes and novel, unseen classes at test time. To this end, we develop \textbf{Point-Cache}, a hierarchical cache model that captures essential clues of online test samples, particularly focusing on the global structure of point clouds and their local-part details. Point-Cache, which serves as a rich 3D knowledge base, is dynamically managed to prioritize the inclusion of high-quality samples. Designed as a plug-and-play module, our method can be flexibly integrated into large multimodal 3D models to support open-vocabulary point cloud recognition. Notably, our solution operates with efficiency comparable to zero-shot inference, as it is entirely training-free. Point-Cache demonstrates substantial gains across 8 challenging benchmarks and 4 representative large 3D models, highlighting its effectiveness. Code is available at https://github.com/auniquesun/Point-Cache.

**Link**: [arxiv](http://arxiv.org/abs/2503.12150v2),  [pdf](http://arxiv.org/pdf/2503.12150v2)

**Tags**: cs.CV 



### Devil is in the Uniformity: Exploring Diverse Learners within   Transformer for Image Restoration
**Authors**: Shihao Zhou, Dayu Li, Jinshan Pan, Juncheng Zhou, Jinglei Shi, Jufeng Yang

**Updated**: 2025-03-26T02:58:41Z

**Summary**: Transformer-based approaches have gained significant attention in image restoration, where the core component, i.e, Multi-Head Attention (MHA), plays a crucial role in capturing diverse features and recovering high-quality results. In MHA, heads perform attention calculation independently from uniform split subspaces, and a redundancy issue is triggered to hinder the model from achieving satisfactory outputs. In this paper, we propose to improve MHA by exploring diverse learners and introducing various interactions between heads, which results in a Hierarchical multI-head atteNtion driven Transformer model, termed HINT, for image restoration. HINT contains two modules, i.e., the Hierarchical Multi-Head Attention (HMHA) and the Query-Key Cache Updating (QKCU) module, to address the redundancy problem that is rooted in vanilla MHA. Specifically, HMHA extracts diverse contextual features by employing heads to learn from subspaces of varying sizes and containing different information. Moreover, QKCU, comprising intra- and inter-layer schemes, further reduces the redundancy problem by facilitating enhanced interactions between attention heads within and across layers. Extensive experiments are conducted on 12 benchmarks across 5 image restoration tasks, including low-light enhancement, dehazing, desnowing, denoising, and deraining, to demonstrate the superiority of HINT. The source code is available in the supplementary materials.

**Link**: [arxiv](http://arxiv.org/abs/2503.20174v1),  [pdf](http://arxiv.org/pdf/2503.20174v1)

**Tags**: cs.CV 



### Medha: Efficiently Serving Multi-Million Context Length LLM Inference   Requests Without Approximations
**Authors**: Amey Agrawal, Haoran Qiu, Junda Chen, ÃÃ±igo Goiri, Ramachandran Ramjee, Chaojie Zhang, Alexey Tumanov, Esha Choukse

**Updated**: 2025-03-26T01:58:40Z

**Summary**: As large language models (LLMs) handle increasingly longer contexts, serving inference requests for context lengths in the range of millions of tokens presents unique challenges. While existing techniques are effective for training, they fail to address the unique challenges of inference, such as varying prefill and decode phases and their associated latency constraints -- like Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore, no long-context inference solutions address head-of-line blocking today.   We present Medha, a system for efficient long-context LLM inference that introduces three key innovations: adaptive chunking with slack-aware scheduling to prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce TTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into a novel 3D parallelism serving engine, Medha achieves unprecedented scale -- supporting contexts up to 10M tokens with production-grade latency. Our evaluation shows Medha reduces median latency by up to 30x compared to state-of-the-art systems when serving a mix of short and long requests, while improving throughput by upwards of 5x. This enables, for the first time, efficient long-context LLM inference at scale without compromising on shorter request latencies or system efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2409.17264v2),  [pdf](http://arxiv.org/pdf/2409.17264v2)

**Tags**: cs.LG cs.DC 



### Visualizing the Invisible: A Generative AR System for Intuitive   Multi-Modal Sensor Data Presentation
**Authors**: Yunqi Guo, Kaiyuan Hou, Heming Fu, Hongkai Chen, Zhenyu Yan, Guoliang Xing, Xiaofan Jiang

**Updated**: 2025-03-25T17:56:01Z

**Summary**: Understanding sensor data can be difficult for non-experts because of the complexity and different semantic meanings of sensor modalities. This leads to a need for intuitive and effective methods to present sensor information. However, creating intuitive sensor data visualizations presents three key challenges: the variability of sensor readings, gaps in domain comprehension, and the dynamic nature of sensor data. To address these issues, we propose Vivar, a novel system that integrates multi-modal sensor data and presents 3D volumetric content for AR visualization. In particular, we introduce a cross-modal embedding approach that maps sensor data into a pre-trained visual embedding space through barycentric interpolation. This approach accurately reflects value changes in multi-modal sensor information, ensuring that sensor variations are properly shown in visualization outcomes. Vivar also incorporates sensor-aware AR scene generation using foundation models and 3D Gaussian Splatting (3DGS) without requiring domain expertise. In addition, Vivar leverages latent reuse and caching strategies to accelerate 2D and AR content generation, demonstrating 11x latency reduction without compromising quality. A user study involving over 503 participants, including domain experts, demonstrates Vivar's effectiveness in accuracy, consistency, and real-world applicability, paving the way for more intuitive sensor data visualization.

**Link**: [arxiv](http://arxiv.org/abs/2412.13509v2),  [pdf](http://arxiv.org/pdf/2412.13509v2)

**Tags**: cs.HC 



### LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior   Accuracy Preservation
**Authors**: Han Chen, Zicong Jiang, Zining Zhang, Bingsheng He, Pingyi Luo, Mian Lu, Yuqiang Chen

**Updated**: 2025-03-25T16:24:45Z

**Summary**: We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV Cache in large language model (LLM) inference, delivering substantial memory savings while preserving superior performance. Previous methods either assume that later tokens are more important or attempt to predict important tokens based on earlier attention patterns. Both approaches, however, can result in performance bottlenecks or frequent mispredictions.   LogQuant takes a different approach. By applying a log-based filtering mechanism, it selectively compresses the KV Cache across the entire context, achieving better performance with the same or even reduced memory footprint compared to existing methods. In benchmark tests, it enhances throughput by 25% and boosts batch size by 60% without increasing memory consumption. For challenging tasks such as Math and Code Completion, LogQuant improves accuracy by 40% to 200% at the same compression ratio, outperforming comparable techniques.LogQuant integrates effortlessly with popular inference frameworks like Python's transformers library. Implementation can be available in https://github.com/Concyclics/LogQuantKV.

**Link**: [arxiv](http://arxiv.org/abs/2503.19950v1),  [pdf](http://arxiv.org/pdf/2503.19950v1)

**Tags**: cs.LG cs.AI cs.CL 



### Gemma 3 Technical Report
**Authors**: Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre RamÃ©, Morgane RiviÃ¨re, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, GaÃ«l Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alex Feng, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, AndrÃ¡s GyÃ¶rgy, AndrÃ© Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-PluciÅska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John Wieting, Jonathan Lai, Jordi Orbay, Joseph Fernandez, Josh Newlan, Ju-yeong Ji, Jyotinder Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus Greff, Linhai Qiu, Marcella Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi, Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Momchev, Nilay Chauhan, Noveen Sachdeva, Oskar Bunyan, Pankil Botarda, Paul Caron, Paul Kishan Rubenstein, Phil Culliton, Philipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Tafti, Rakesh Shivanna, Renjie Wu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim PÃµder, Sijal Bhatnagar, Sindhu Raghuram Panyam, Sivan Eiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, Utku Evci, Vedant Misra, Vincent Roseberry, Vlad Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow, Yuvein Zhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Kat Black, Nabila Babar, Jessica Lo, Erica Moreira, Luiz Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher, Tris Warkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, Yossi Matias, D. Sculley, Slav Petrov, Noah Fiedel, Noam Shazeer, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Jean-Baptiste Alayrac, Rohan Anil, Dmitry, Lepikhin, Sebastian Borgeaud, Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, LÃ©onard Hussenot

**Updated**: 2025-03-25T15:52:34Z

**Summary**: We introduce Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters. This version introduces vision understanding abilities, a wider coverage of languages and longer context - at least 128K tokens. We also change the architecture of the model to reduce the KV-cache memory that tends to explode with long context. This is achieved by increasing the ratio of local to global attention layers, and keeping the span on local attention short. The Gemma 3 models are trained with distillation and achieve superior performance to Gemma 2 for both pre-trained and instruction finetuned versions. In particular, our novel post-training recipe significantly improves the math, chat, instruction-following and multilingual abilities, making Gemma3-4B-IT competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro across benchmarks. We release all our models to the community.

**Link**: [arxiv](http://arxiv.org/abs/2503.19786v1),  [pdf](http://arxiv.org/pdf/2503.19786v1)

**Tags**: cs.CL cs.AI 



### Integrating Prefetcher Selection with Dynamic Request Allocation   Improves Prefetching Efficiency
**Authors**: Mengming Li, Qijun Zhang, Yongqing Ren, Zhiyao Xie

**Updated**: 2025-03-25T06:45:13Z

**Summary**: Hardware prefetching plays a critical role in hiding the off-chip DRAM latency. The complexity of applications results in a wide variety of memory access patterns, prompting the development of numerous cache-prefetching algorithms. Consequently, commercial processors often employ a hybrid of these algorithms to enhance the overall prefetching performance. Nonetheless, since these prefetchers share hardware resources, conflicts arising from competing prefetching requests can negate the benefits of hardware prefetching. Under such circumstances, several prefetcher selection algorithms have been proposed to mitigate conflicts between prefetchers. However, these prior solutions suffer from two limitations. First, the input demand request allocation is inaccurate. Second, the prefetcher selection criteria are coarse-grained.   In this paper, we address both limitations by introducing an efficient and widely applicable prefetcher selection algorithm--Alecto, which tailors the demand requests for each prefetcher. Every demand request is first sent to Alecto to identify suitable prefetchers before being routed to prefetchers for training and prefetching. Our analysis shows that Alecto is adept at not only harmonizing prefetching accuracy, coverage, and timeliness but also significantly enhancing the utilization of the prefetcher table, which is vital for temporal prefetching. Alecto outperforms the state-of-the-art RL-based prefetcher selection algorithm--Bandit by 2.76% in single-core, and 7.56% in eight-core. For memory-intensive benchmarks, Alecto outperforms Bandit by 5.25%. Alecto consistently delivers state-of-the-art performance in scheduling various types of cache prefetchers. In addition to the performance improvement, Alecto can reduce the energy consumption associated with accessing the prefetchers' table by 48%, while only adding less than 1 KB of storage overhead.

**Link**: [arxiv](http://arxiv.org/abs/2503.19390v1),  [pdf](http://arxiv.org/pdf/2503.19390v1)

**Tags**: cs.AR 



### CalibQuant: 1-Bit KV Cache Quantization for Multimodal LLMs
**Authors**: Insu Han, Zeliang Zhang, Zhiyuan Wang, Yifan Zhu, Susan Liang, Jiani Liu, Haiting Lin, Mingjie Zhao, Chenliang Xu, Kun Wan, Wentian Zhao

**Updated**: 2025-03-24T23:47:51Z

**Summary**: Multimodal Large Language Models (MLLMs) have demonstrated remarkable performance across diverse applications. However, their computational overhead during deployment remains a critical bottleneck. While Key-Value (KV) caching effectively trades memory for computation to enhance inference efficiency, the growing memory footprint from extensive KV caches significantly reduces throughput and restricts prolonged deployment on memory-constrained GPU devices. To address this challenge, we propose CalibQuant, a simple yet highly effective visual quantization strategy that drastically reduces both memory and computational overhead. Specifically, CalibQuant introduces an extreme 1-bit quantization scheme, complemented by novel post-scaling and calibration techniques tailored to the intrinsic patterns of KV caches, thereby ensuring high efficiency without compromising model performance. Leveraging Triton for runtime optimization, we achieve a 10x throughput increase on InternVL models. Our method is designed to be plug-and-play, seamlessly integrating with various existing MLLMs without requiring architectural changes. Extensive experiments confirm that our approach significantly reduces memory usage while maintaining computational efficiency and preserving multimodal capabilities. Codes are available at https://github.com/insuhan/calibquant.

**Link**: [arxiv](http://arxiv.org/abs/2502.14882v2),  [pdf](http://arxiv.org/pdf/2502.14882v2)

**Tags**: cs.CV 



### Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures
**Authors**: Ishna Satyarth, Chao Yin, Devin A. Matthews, Maggie Myers, Robert van de Geijn, RuQing G. Xu

**Updated**: 2025-03-24T21:27:53Z

**Summary**: The factorization of skew-symmetric matrices is a critically understudied area of dense linear algebra, particularly in comparison to that of general and symmetric matrices. While some algorithms can be adapted from the symmetric case, the cost of algorithms can be reduced by exploiting skew-symmetry. This work examines the factorization of a skew-symmetric matrix $X$ into its $LTL^\mathrm{T}$ decomposition, where $L$ is unit lower triangular and $T$ is tridiagonal. This is also known as a triangular tridiagonalization. This operation is a means for computing the determinant of $X$ as the square of the (cheaply-computed) Pfaffian of the skew-symmetric tridiagonal matrix $T$ as well as for solving systems of equations, across fields such as quantum electronic structure and machine learning. Its application also often requires pivoting in order to improve numerical stability. We compare and contrast previously-published algorithms with those systematically derived using the FLAME methodology. Performant parallel CPU implementations are achieved by fusing operations at multiple levels in order to reduce memory traffic overhead. A key factor is the employment of new capabilities of the BLAS-like Library Instantiation Software (BLIS) framework, which now supports casting level-2 and level-3 BLAS-like operations by leveraging its gemm and other kernels, hierarchical parallelism, and cache blocking. A prototype, concise C++ API facilitates the translation of correct-by-construction algorithms into correct code. Experiments verify that the resulting implementations greatly exceed the performance of previous work.

**Link**: [arxiv](http://arxiv.org/abs/2411.09859v2),  [pdf](http://arxiv.org/pdf/2411.09859v2)

**Tags**: cs.MS 



### Compositional Caching for Training-free Open-vocabulary Attribute   Detection
**Authors**: Marco Garosi, Alessandro Conti, Gaowen Liu, Elisa Ricci, Massimiliano Mancini

**Updated**: 2025-03-24T21:00:37Z

**Summary**: Attribute detection is crucial for many computer vision tasks, as it enables systems to describe properties such as color, texture, and material. Current approaches often rely on labor-intensive annotation processes which are inherently limited: objects can be described at an arbitrary level of detail (e.g., color vs. color shades), leading to ambiguities when the annotators are not instructed carefully. Furthermore, they operate within a predefined set of attributes, reducing scalability and adaptability to unforeseen downstream applications. We present Compositional Caching (ComCa), a training-free method for open-vocabulary attribute detection that overcomes these constraints. ComCa requires only the list of target attributes and objects as input, using them to populate an auxiliary cache of images by leveraging web-scale databases and Large Language Models to determine attribute-object compatibility. To account for the compositional nature of attributes, cache images receive soft attribute labels. Those are aggregated at inference time based on the similarity between the input and cache images, refining the predictions of underlying Vision-Language Models (VLMs). Importantly, our approach is model-agnostic, compatible with various VLMs. Experiments on public datasets demonstrate that ComCa significantly outperforms zero-shot and cache-based baselines, competing with recent training-based methods, proving that a carefully designed training-free approach can successfully address open-vocabulary attribute detection.

**Link**: [arxiv](http://arxiv.org/abs/2503.19145v1),  [pdf](http://arxiv.org/pdf/2503.19145v1)

**Tags**: cs.CV 



### Mitigating KV Cache Competition to Enhance User Experience in LLM   Inference
**Authors**: Haiying Shen, Tanmoy Sen, Masahiro Tanaka

**Updated**: 2025-03-24T18:50:09Z

**Summary**: In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes high tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing user experience, particularly in time-sensitive applications. However, satisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To address this, we propose a system, named CacheOPT for mitigating KV Cache competition, based on key insights from our measurements, incorporating novel components. First, it estimates a request's output length, bounding the deviation with a high specified probability, adjusted based on the request arrival rate. Second, it allocates the estimated KVC demand to a request, and reuses other requests' allocated KVC to avoid preemptions while reducing waiting time. Third, it proactively allocates KVC before instead of at the time a request exhausts its allocation and reserves KVC globally to prevent preemptions. Fourth, it chooses a request that has long TBT SLO, long job remaining time and short preemption time to preempt. Fifth, it selects the shortest-latency strategy between swapping and recomputation for preemptions. Experiments show that CacheOPT achieves up to 3.29$\times$ and 2.83$\times$ lower tail TBT and tail TTFT, 47\% and 53\% higher TTFT and TBT SLO attainments, and supports up to 1.58$\times$ higher request arrival rate than the state-of-the-art methods.

**Link**: [arxiv](http://arxiv.org/abs/2503.13773v2),  [pdf](http://arxiv.org/pdf/2503.13773v2)

**Tags**: cs.CL 



### EconoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in   LLM Serving
**Authors**: Haiying Shen, Tanmoy Sen

**Updated**: 2025-03-24T18:16:58Z

**Summary**: As Large Language Models (LLMs) continue to grow, reducing costs and alleviating GPU demands has become increasingly critical. However, existing schedulers primarily target either GPU compute or Key-Value Cache (KVC) utilization, failing to fully optimize both GPU compute and KVC usage during each iteration or guarantee timely KVC allocations when needed. To address these challenges, we conducted a trace-based experimental analysis and made insightful observations, leading to the design of a system called EconoServe. EconoServe maximizes multi-resource utilization while ensuring service-level objective (SLO) guarantees in LLM serving. To enable adding prompts to a batch to maximize GPU utilization in each iteration, EconoServe maintains separate waiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It batches GTs with the same predicted response lengths (RL) to save scheduling time and allocates KVC space for the predicted RL to avoid KVC allocation failures. It further has a novel KVC pipelining method, allowing sharing allocated but unused KVC space to enhance KVC utilization. In addition, it prioritizes queued requests that occupy more KVC to release KVC earlier and satisfy request service-level-objective (SLO). Experimental results demonstrate that EconoServe increases throughput by up to 4$\times$ with the same level of latency, generates up to 91\% lower job completion time and up to 91\% higher SLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs used in DistServe by up to 78\% while maintaining the same level of goodput.

**Link**: [arxiv](http://arxiv.org/abs/2411.06364v2),  [pdf](http://arxiv.org/pdf/2411.06364v2)

**Tags**: cs.DC 



### xKV: Cross-Layer SVD for KV-Cache Compression
**Authors**: Chi-Chih Chang, Chien-Yu Lin, Yash Akhauri, Wei-Cheng Lin, Kai-Chiang Wu, Luis Ceze, Mohamed S. Abdelfattah

**Updated**: 2025-03-24T17:06:37Z

**Summary**: Large Language Models (LLMs) with long context windows enable powerful applications but come at the cost of high memory consumption to store the Key and Value states (KV-Cache). Recent studies attempted to merge KV-cache from multiple layers into shared representations, yet these approaches either require expensive pretraining or rely on assumptions of high per-token cosine similarity across layers which generally does not hold in practice. We find that the dominant singular vectors are remarkably well-aligned across multiple layers of the KV-Cache. Exploiting this insight, we propose xKV, a simple post-training method that applies Singular Value Decomposition (SVD) on the KV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers into a shared low-rank subspace, significantly reducing KV-Cache sizes. Through extensive evaluations on the RULER long-context benchmark with widely-used LLMs (e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates than state-of-the-art inter-layer technique while improving accuracy by 2.7%. Moreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA) (e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding tasks without performance degradation. These results highlight xKV's strong capability and versatility in addressing memory bottlenecks for long-context LLM inference. Our code is publicly available at: https://github.com/abdelfattah-lab/xKV.

**Link**: [arxiv](http://arxiv.org/abs/2503.18893v1),  [pdf](http://arxiv.org/pdf/2503.18893v1)

**Tags**: cs.CL cs.LG 



### HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads
**Authors**: Pranav Suryadevara

**Updated**: 2025-03-24T16:47:48Z

**Summary**: The growth of machine learning (ML) workloads has underscored the importance of efficient memory hierarchies to address bandwidth, latency, and scalability challenges. HERMES focuses on optimizing memory subsystems for RISC-V architectures to meet the computational needs of ML models such as CNNs, RNNs, and Transformers. This project explores state-of-the-art techniques such as advanced prefetching, tensor-aware caching, and hybrid memory models. The cornerstone of HERMES is the integration of shared L3 caches with fine-grained coherence protocols equipped with specialized pathways to deep-learning accelerators such as Gemmini. Simulation tools like Gem5 and DRAMSim2 were used to evaluate baseline performance and scalability under representative ML workloads. The findings of this study highlight the design choices, and the anticipated challenges, paving the way for low-latency scalable memory operations for ML applications.

**Link**: [arxiv](http://arxiv.org/abs/2503.13064v2),  [pdf](http://arxiv.org/pdf/2503.13064v2)

**Tags**: cs.AR cs.PF B.3.2; C.1.3; C.3 



### Exploring the Integration of Key-Value Attention Into Pure and Hybrid   Transformers for Semantic Segmentation
**Authors**: DeShin Hwa, Tobias Holmes, Klaus Drechsler

**Updated**: 2025-03-24T16:38:31Z

**Summary**: While CNNs were long considered state of the art for image processing, the introduction of Transformer architectures has challenged this position. While achieving excellent results in image classification and segmentation, Transformers remain inherently reliant on large training datasets and remain computationally expensive. A newly introduced Transformer derivative named KV Transformer shows promising results in synthetic, NLP, and image classification tasks, while reducing complexity and memory usage. This is especially conducive to use cases where local inference is required, such as medical screening applications. We endeavoured to further evaluate the merit of KV Transformers on semantic segmentation tasks, specifically in the domain of medical imaging. By directly comparing traditional and KV variants of the same base architectures, we provide further insight into the practical tradeoffs of reduced model complexity. We observe a notable reduction in parameter count and multiply accumulate operations, while achieving similar performance from most of the KV variant models when directly compared to their QKV implementation.

**Link**: [arxiv](http://arxiv.org/abs/2503.18862v1),  [pdf](http://arxiv.org/pdf/2503.18862v1)

**Tags**: cs.CV cs.AI cs.LG 



### BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with   Low-Bit KV Cache
**Authors**: Dayou Du, Shijie Cao, Jianyi Cheng, Ting Cao, Mao Yang

**Updated**: 2025-03-24T15:22:41Z

**Summary**: The growing adoption of long-context Large Language Models (LLMs) has introduced significant memory and computational challenges in autoregressive decoding due to the expanding Key-Value (KV) cache. KV cache quantization has emerged as a promising solution, with prior work showing that 4-bit or even 2-bit quantization can maintain model accuracy while reducing memory costs. However, despite these benefits, preliminary implementations for the low-bit KV cache struggle to deliver the expected speedup due to quantization and dequantization overheads and the lack of Tensor Cores utilization. In this work, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor Cores for efficient decoding with low-bit KV cache. Efficiently leveraging Tensor Cores for low-bit KV cache is challenging due to the dynamic nature of KV cache generation at each decoding step. BitDecoding addresses these challenges with a Tensor Cores-Centric BitFusion Scheme that ensures data layout compatibility to enable high utilization of Tensor Cores. Additionally, BitDecoding incorporates a warp-efficient parallel decoding kernel and a fine-grained asynchronous pipeline, minimizing dequantization overhead and improving computational efficiency. Experiments show that BitDecoding achieves up to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to FP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV cache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K sequence length, BitDecoding reduces single-batch decoding latency by 3x, demonstrating its effectiveness in long-context generation scenarios. The code is available at https://github.com/DD-DuDa/BitDecoding.

**Link**: [arxiv](http://arxiv.org/abs/2503.18773v1),  [pdf](http://arxiv.org/pdf/2503.18773v1)

**Tags**: cs.AR cs.AI cs.CL cs.PF 



### Choosing Augmentation Parameters in OSQP- A New Approach based on   Conjugate Directions
**Authors**: Avinash Kumar

**Updated**: 2025-03-24T13:09:03Z

**Summary**: This work proposes a new method to select the augmentation parameters in the operator splitting quadratic program (OSQP) algorithm so as to reduce the computation time of overall algorithm. The selection is based upon the information of conjugate directions of the coefficient matrix of a linear system of equations present in the algorithm. This selection makes it possible to cache these conjugate directions, instead of computing them at each iteration, resulting in faster computation of the solution of the linear system thus reducing the overall computation time. This reduction is demonstrated by a numerical example.

**Link**: [arxiv](http://arxiv.org/abs/2503.05941v2),  [pdf](http://arxiv.org/pdf/2503.05941v2)

**Tags**: math.OC 



### Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV   Cache Quantization
**Authors**: Minsu Kim, Seongmin Hong, RyeoWook Ko, Soongyu Choi, Hunjong Lee, Junsoo Kim, Joo-Young Kim, Jongse Park

**Updated**: 2025-03-24T11:56:50Z

**Summary**: Modern Large Language Model serving system batches multiple requests to achieve high throughput, while batching attention operations is challenging, rendering memory bandwidth a critical bottleneck. The community relies on high-end GPUs with multiple high-bandwidth memory channels. Unfortunately, HBM's high bandwidth often comes at the expense of limited memory capacity, which reduces core utilization and increases costs. Recent advancements enabling longer contexts for LLMs have substantially increased the key-value cache size, further intensifying the pressures on memory capacity. The literature has explored KV cache quantization techniques, which commonly use low bitwidth for most values, selectively using higher bitwidth for outlier values. While this approach helps achieve high accuracy and low bitwidth simultaneously, it comes with the limitation that cost for online outlier detection is excessively high, negating the advantages. We propose Oaken, an acceleration solution that achieves high accuracy and high performance simultaneously through co-designing algorithm and hardware. To effectively find a sweet spot in the accuracy-performance trade-off space of KV cache quantization, Oaken employs an online-offline hybrid approach, setting outlier thresholds offline, which are then used to determine the quantization scale online. To translate the proposed algorithmic technique into tangible performance gains, Oaken also comes with custom quantization engines and memory management units that can be integrated with any LLM accelerators. We built an Oaken accelerator on top of an LLM accelerator, LPU, and conducted a comprehensive evaluation. Our experiments show that for a batch size of 256, Oaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU, incurring a minimal accuracy loss of only 0.54\% on average, compared to state-of-the-art KV cache quantization techniques.

**Link**: [arxiv](http://arxiv.org/abs/2503.18599v1),  [pdf](http://arxiv.org/pdf/2503.18599v1)

**Tags**: cs.AR cs.LG 



### Register Dispersion: Reducing the Footprint of the Vector Register File   in Vector Engines of Low-Cost RISC-V CPUs
**Authors**: Vasileios Titopoulos, George Alexakis, Kosmas Alexandridis, Chrysostomos Nicopoulos, Giorgos Dimitrakopoulos

**Updated**: 2025-03-24T11:00:35Z

**Summary**: The deployment of Machine Learning (ML) applications at the edge on resource-constrained devices has accentuated the need for efficient ML processing on low-cost processors. While traditional CPUs provide programming flexibility, their general-purpose architecture often lacks the throughput required for complex ML models. The augmentation of a RISC-V processor with a vector unit can provide substantial data-level parallelism. However, increasing the data-level parallelism supported by vector processing would make the Vector Register File (VRF) a major area consumer in ultra low-cost processors, since 32 vector registers are required for RISC-V Vector ISA compliance. This work leverages the insight that many ML vectorized kernels require a small number of active vector registers, and proposes the use of a physically smaller VRF that dynamically caches only the vector registers currently accessed by the application. This approach, called Register Dispersion, maps the architectural vector registers to a smaller set of physical registers. The proposed ISA-compliant VRF is significantly smaller than a full-size VRF and operates like a conventional cache, i.e., it only stores the most recently accessed vector registers. Essential registers remain readily accessible within the compact VRF, while the others are offloaded to the cache/memory sub-system. The compact VRF design is demonstrated to yield substantial area and power savings, as compared to using a full VRF, with no or minimal impact on performance. This effective trade-off renders the inclusion of vector units in low-cost processors feasible and practical.

**Link**: [arxiv](http://arxiv.org/abs/2503.17333v2),  [pdf](http://arxiv.org/pdf/2503.17333v2)

**Tags**: cs.AR 



### iFlame: Interleaving Full and Linear Attention for Efficient Mesh   Generation
**Authors**: Hanxiao Wang, Biao Zhang, Weize Quan, Dong-Ming Yan, Peter Wonka

**Updated**: 2025-03-24T03:18:49Z

**Summary**: This paper propose iFlame, a novel transformer-based network architecture for mesh generation. While attention-based models have demonstrated remarkable performance in mesh generation, their quadratic computational complexity limits scalability, particularly for high-resolution 3D data. Conversely, linear attention mechanisms offer lower computational costs but often struggle to capture long-range dependencies, resulting in suboptimal outcomes. To address this trade-off, we propose an interleaving autoregressive mesh generation framework that combines the efficiency of linear attention with the expressive power of full attention mechanisms. To further enhance efficiency and leverage the inherent structure of mesh representations, we integrate this interleaving approach into an hourglass architecture, which significantly boosts efficiency. Our approach reduces training time while achieving performance comparable to pure attention-based models. To improve inference efficiency, we implemented a caching algorithm that almost doubles the speed and reduces the KV cache size by seven-eighths compared to the original Transformer. We evaluate our framework on ShapeNet and Objaverse, demonstrating its ability to generate high-quality 3D meshes efficiently. Our results indicate that the proposed interleaving framework effectively balances computational efficiency and generative performance, making it a practical solution for mesh generation. The training takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces on Objaverse.

**Link**: [arxiv](http://arxiv.org/abs/2503.16653v2),  [pdf](http://arxiv.org/pdf/2503.16653v2)

**Tags**: cs.CV 



### Jenga: Effective Memory Management for Serving LLM with Heterogeneity
**Authors**: Chen Zhang, Kuntai Du, Shu Liu, Woosuk Kwon, Xiangxi Mo, Yufeng Wang, Xiaoxuan Liu, Kaichao You, Zhuohan Li, Mingsheng Long, Jidong Zhai, Joseph Gonzalez, Ion Stoica

**Updated**: 2025-03-24T02:28:04Z

**Summary**: Large language models (LLMs) are widely used but expensive to run, especially as inference workloads grow. To lower costs, maximizing the request batch size by managing GPU memory efficiently is crucial. While PagedAttention has recently been proposed to improve the efficiency of memory management, we find that the growing heterogeneity in the embeddings dimensions, attention, and access patterns of modern LLM architectures introduces new challenges for memory allocation.   In this paper, we present Jenga, a novel memory allocation framework for heterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1) minimizing memory fragmentation when managing embeddings of different sizes, and (2) enabling flexible caching and eviction policies tailored to the specific token-dependency patterns of various layers. Jenga employs a two-level memory allocator, leveraging the least common multiple (LCM) of embedding sizes to optimize memory usage and providing APIs to express layer-specific caching logic to enhance memory reuse.   We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and evaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations show that Jenga improves GPU memory utilization by up to 79.6%, and increases serving throughput by up to 4.92x (1.80x on average).

**Link**: [arxiv](http://arxiv.org/abs/2503.18292v1),  [pdf](http://arxiv.org/pdf/2503.18292v1)

**Tags**: cs.DC 



### ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video   Understanding
**Authors**: Xiao Wang, Qingyi Si, Jianlong Wu, Shiyu Zhu, Li Cao, Liqiang Nie

**Updated**: 2025-03-24T02:17:34Z

**Summary**: Video Large Language Models (VideoLLMs) have made significant strides in video understanding but struggle with long videos due to the limitations of their backbone LLMs. Existing solutions rely on length extrapolation, which is memory-constrained, or visual token compression, which primarily leverages low-level temporal redundancy while overlooking the more effective high-level knowledge redundancy. To address this, we propose $\textbf{ReTaKe}$, a training-free method with two novel modules DPSelect and PivotKV, to jointly reduce both temporal visual redundancy and knowledge redundancy for video compression. To align with the way of human temporal perception, DPSelect identifies keyframes based on inter-frame distance peaks. To leverage LLMs' learned prior knowledge, PivotKV marks the keyframes as pivots and compress non-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe enables VideoLLMs to process 8 times longer frames (up to 2048), outperforming similar-sized models by 3-5% and even rivaling much larger ones on VideoMME, MLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression operations with prefilling, ReTaKe introduces only ~10% prefilling latency overhead while reducing decoding latency by ~20%. Our code is available at https://github.com/SCZwangxiao/video-ReTaKe.

**Link**: [arxiv](http://arxiv.org/abs/2412.20504v5),  [pdf](http://arxiv.org/pdf/2412.20504v5)

**Tags**: cs.CV cs.CL cs.MM 



### Risk Management for Distributed Arbitrage Systems: Integrating   Artificial Intelligence
**Authors**: Akaash Vishal Hazarika, Mahak Shah, Swapnil Patil, Pradyumna Shukla

**Updated**: 2025-03-24T01:15:43Z

**Summary**: Effective risk management solutions become absolutely crucial when financial markets embrace distributed technology and decentralized financing (DeFi). This study offers a thorough survey and comparative analysis of the integration of artificial intelligence (AI) in risk management for distributed arbitrage systems. We examine several modern caching techniques namely in memory caching, distributed caching, and proxy caching and their functions in enhancing performance in decentralized settings. Through literature review we examine the utilization of AI techniques for alleviating risks related to market volatility, liquidity challenges, operational failures, regulatory compliance, and security threats. This comparison research evaluates various case studies from prominent DeFi technologies, emphasizing critical performance metrics like latency reduction, load balancing, and system resilience. Additionally, we examine the problems and trade offs associated with these technologies, emphasizing their effects on consistency, scalability, and fault tolerance. By meticulously analyzing real world applications, specifically centering on the Aave platform as our principal case study, we illustrate how the purposeful amalgamation of AI with contemporary caching methodologies has revolutionized risk management in distributed arbitrage systems.

**Link**: [arxiv](http://arxiv.org/abs/2503.18265v1),  [pdf](http://arxiv.org/pdf/2503.18265v1)

**Tags**: cs.DC cs.AI cs.LG I.2.11; G.3 



### Enabling the Write-Back Page Cache with Strong Consistency in   Distributed Userspace File Systems
**Authors**: Haoyu Li, Jingkai Fu, Qing Li, Windsor Hsu, Asaf Cidon

**Updated**: 2025-03-23T20:18:16Z

**Summary**: The large-scale, multi-tenant nature of cloud computing requires distributed file systems that offer stability, adaptability, and compatibility. FUSE-based distributed file systems have emerged as a popular solution for the cloud, offering fast deployment, fault isolation, and POSIX compliance. However, FUSE's performance limitations, particularly its inability to reconcile page caching with strong consistency in distributed environments, remain a persistent problem. Existing approaches either sacrifice consistency for performance or rely on inefficient caching, limiting their practicality.   To this end, we present DistFUSE, the first FUSE-based distributed file system that relies on a write-back kernel-based page cache for performance and provides strong consistency. DistFUSE achieves this by offloading userspace lock management to the kernel driver, allowing coordinated access to the kernel's page cache across nodes. This design eliminates blind local cache updates and ensures cluster-wide consistency without compromising performance. Our evaluation shows DistFUSE improves throughput by up to 75% compared to baseline approaches.

**Link**: [arxiv](http://arxiv.org/abs/2503.18191v1),  [pdf](http://arxiv.org/pdf/2503.18191v1)

**Tags**: cs.OS 



### Formal Verification of Parameterized Systems based on Induction
**Authors**: Jiaqi Xiu, Yongjian Li

**Updated**: 2025-03-23T11:07:24Z

**Summary**: Parameterized systems play a crucial role in the computer field, and their security is of great significance. Formal verification of parameterized protocols is especially challenging due to its "parameterized" feature, which brings complexity and undecidability. Existing automated parameterized verification methods have limitations, such as facing difficulties in automatically deriving parameterized invariants constrained by mixed Forall and Exists quantifiers, or having challenges in completing the parameterized verification of large and complex protocols. This paper proposes a formal verification framework for parameterized systems based on induction, named wiseParaverifier. It starts from small concretizations of protocols, analyzes inductive counterexamples, and constructs counterexample formulas to guide the entire process of parameterized verification. It also presents a heuristic Generalize method to quickly find auxiliary invariants, a method for promoting complex mixed quantifiers and merging parameterized invariants, and uses symmetric reduction ideas to accelerate the verification process. Experimental results show that wiseParaverifier can successfully complete automatic inductive verification on 7 cache coherence protocols and 10 distributed protocols. It has strong verification capabilities and migration capabilities, and can provide concise and readable verification results, which is helpful for learners to understand protocol behaviors.

**Link**: [arxiv](http://arxiv.org/abs/2503.18030v1),  [pdf](http://arxiv.org/pdf/2503.18030v1)

**Tags**: cs.LO cs.SC 



### Knowledge Rumination for Client Utility Evaluation in Heterogeneous   Federated Learning
**Authors**: Xiaorui Jiang, Yu Gao, Hengwei Xu, Qi Zhang, Yong Liao, Pengyuan Zhou

**Updated**: 2025-03-23T06:14:35Z

**Summary**: Federated Learning (FL) allows several clients to cooperatively train machine learning models without disclosing the raw data. In practical applications, asynchronous FL (AFL) can address the straggler effect compared to synchronous FL. However, Non-IID data and stale models pose significant challenges to AFL, as they can diminish the practicality of the global model and even lead to training failures. In this work, we propose a novel AFL framework called Federated Historical Learning (FedHist), which effectively addresses the challenges posed by both Non-IID data and gradient staleness based on the concept of knowledge rumination. FedHist enhances the stability of local gradients by performing weighted fusion with historical global gradients cached on the server. Relying on hindsight, it assigns aggregation weights to each participant in a multi-dimensional manner during each communication round. To further enhance the efficiency and stability of the training process, we introduce an intelligent $\ell_2$-norm amplification scheme, which dynamically regulates the learning progress based on the $\ell_2$-norms of the submitted gradients. Extensive experiments indicate FedHist outperforms state-of-the-art methods in terms of convergence performance and test accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2312.10425v2),  [pdf](http://arxiv.org/pdf/2312.10425v2)

**Tags**: cs.LG 



### Cache-Aware Cooperative Multicast Beamforming in Dynamic   Satellite-Terrestrial Networks
**Authors**: Shuo Yuan, Yaohua Sun, Mugen Peng

**Updated**: 2025-03-23T03:20:25Z

**Summary**: With the burgeoning demand for data-intensive services, satellite-terrestrial networks (STNs) face increasing backhaul link congestion, deteriorating user quality of service (QoS), and escalating power consumption. Cache-aided STNs are acknowledged as a promising paradigm for accelerating content delivery to users and alleviating the load of backhaul links. However, the dynamic nature of low earth orbit (LEO) satellites and the complex interference among satellite beams and terrestrial base stations pose challenges in effectively managing limited edge resources. To address these issues, this paper proposes a method for dynamically scheduling caching and communication resources, aiming to reduce network costs in terms of transmission power consumption and backhaul traffic, while meeting user QoS demands and resource constraints. We formulate a mixed timescale problem to jointly optimize cache placement, LEO satellite beam direction, and cooperative multicast beamforming among satellite beams and base stations. To tackle this intricate problem, we propose a two-stage solution framework, where the primary problem is decoupled into a short-term content delivery subproblem and a long-term cache placement subproblem. The former subproblem is solved by designing an alternating optimization approach with whale optimization and successive convex approximation methods according to the cache placement state, while cache content in STNs is updated using an iterative algorithm that utilizes historical information. Simulation results demonstrate the effectiveness of our proposed algorithms, showcasing their convergence and significantly reducing transmission power consumption and backhaul traffic by up to 52%.

**Link**: [arxiv](http://arxiv.org/abs/2503.17913v1),  [pdf](http://arxiv.org/pdf/2503.17913v1)

**Tags**: cs.NI eess.SP 



### VSAG: An Optimized Search Framework for Graph-based Approximate Nearest   Neighbor Search
**Authors**: Xiaoyao Zhong, Haotian Li, Jiabao Jin, Mingyu Yang, Deming Chu, Xiangyu Wang, Zhitao Shen, Wei Jia, George Gu, Yi Xie, Xuemin Lin, Heng Tao Shen, Jingkuan Song, Peng Cheng

**Updated**: 2025-03-23T03:16:50Z

**Summary**: Approximate nearest neighbor search (ANNS) is a fundamental problem in vector databases and AI infrastructures. Recent graph-based ANNS algorithms have achieved high search accuracy with practical efficiency. Despite the advancements, these algorithms still face performance bottlenecks in production, due to the random memory access patterns of graph-based search and the high computational overheads of vector distance. In addition, the performance of a graph-based ANNS algorithm is highly sensitive to parameters, while selecting the optimal parameters is cost-prohibitive, e.g., manual tuning requires repeatedly re-building the index.   This paper introduces VSAG, an open-source framework that aims to enhance the in production performance of graph-based ANNS algorithms. VSAG has been deployed at scale in the services of Ant Group, and it incorporates three key optimizations: (i) efficient memory access: it reduces L3 cache misses with pre-fetching and cache-friendly vector organization; (ii) automated parameter tuning: it automatically selects performance-optimal parameters without requiring index rebuilding; (iii) efficient distance computation: it leverages modern hardware, scalar quantization, and smartly switches to low-precision representation to dramatically reduce the distance computation costs. We evaluate VSAG on real-world datasets. The experimental results show that VSAG achieves the state-of-the-art performance and provides up to 4x speedup over HNSWlib (an industry-standard library) while ensuring the same accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2503.17911v1),  [pdf](http://arxiv.org/pdf/2503.17911v1)

**Tags**: cs.DB 



### Orientation-Dependent \b{eta}-Ga2O3 Heterojunction Diode with Atomic   Layer Deposition (ALD) Grown NiO
**Authors**: Yizheng Liu, Shane M. W. Witsell, John F. Conley, Sriram Krishnamoorthy

**Updated**: 2025-03-23T01:17:08Z

**Summary**: This work reports the demonstration of ALD-deposited NiO/\b{eta}-Ga2O3 heterojunction diodes (HJDs) on low doped drift layer and highly doped (001) & (100) n+ substrates with experimental observation of a parallel-plane junction electric field as high as 7.5 MV/cm, revealing a crystal orientation dependence in \b{eta}-Ga2O3. We use a novel metalorganic precursor bis(1,4-di-tert-butyl-1,3-diazadienyl) (nickel Ni(tBu2DAD)2) with ozone (O3) to deposit NiO. The NiO/\b{eta}-Ga2O3 HJD on 7.7 {\mu}m-thick HVPE-grown drift region exhibited an on-state current density of ~20 A/cm2 at 5 V, ~10-8 A/cm2 reverse leakage at low reverse bias(-5 V), and a rectifying ratio(Jon/Joff) of ~109. The HJD broke down at ~2.2 kV reverse bias, corresponding to a ~3.4 MV/cm parallel-plane junction electric field, with a noise floor reverse leakage (10-8~10-6 A/cm2, nA) at 80% of the device catastrophic breakdown voltage. The NiO/\b{eta}-Ga2O3 HJDs on n+ (001) & (100) highly-doped substrates exhibited breakdown voltages at 12.5-16.0 V and 28.5-70.5 V, respectively, with extracted critical electric field (EC) at 2.30-2.76 MV/cm, and 4.33-7.50 MV/cm, revealing a substrate crystal orientation dependence on breakdown electric field for \b{eta}-Ga2O3. The 7.5 MV/cm EC reported here is one of the highest parallel-plane junction electric fields reported in literature.

**Link**: [arxiv](http://arxiv.org/abs/2503.17895v1),  [pdf](http://arxiv.org/pdf/2503.17895v1)

**Tags**: cond-mat.mtrl-sci physics.app-ph 



### A Generative Caching System for Large Language Models
**Authors**: Arun Iyengar, Ashish Kundu, Ramana Kompella, Sai Nandan Mamidi

**Updated**: 2025-03-22T01:17:56Z

**Summary**: Caching has the potential to be of significant benefit for accessing large language models (LLMs) due to their high latencies which typically range from a small number of seconds to well over a minute. Furthermore, many LLMs charge money for queries; caching thus has a clear monetary benefit. This paper presents a new caching system for improving user experiences with LLMs. In addition to reducing both latencies and monetary costs for accessing LLMs, our system also provides important features that go beyond the performance benefits typically associated with caches. A key feature we provide is generative caching, wherein multiple cached responses can be synthesized to provide answers to queries which have never been seen before. Our generative caches function as repositories of valuable information which can be mined and analyzed. We also improve upon past semantic caching techniques by tailoring the caching algorithms to optimally balance cost and latency reduction with the quality of responses provided. Performance tests indicate that our caches are considerably faster than GPTcache.

**Link**: [arxiv](http://arxiv.org/abs/2503.17603v1),  [pdf](http://arxiv.org/pdf/2503.17603v1)

**Tags**: cs.DB cs.AI cs.DC cs.NI 



### Multiport Support for Vortex OpenGPU Memory Hierarchy
**Authors**: Injae Shin, Blaise Tine

**Updated**: 2025-03-22T01:16:24Z

**Summary**: Modern day applications have grown in size and require more computational power. The rise of machine learning and AI increased the need for parallel computation, which has increased the need for GPGPUs. With the increasing demand for computational power, GPGPUs' SIMT architecture has solved this with an increase in the number of threads and the number of cores in a GPU, increasing the throughput of these processors to match the demand of the applications. However, this created a larger demand for the memory, making the memory bandwidth a bottleneck. The introduction of High-Bandwidth Memory (HBM) with its increased number of memory ports offers a potential solution for the GPU to exploit its memory parallelism to increase the memory bandwidth. However, effectively leveraging HBM's memory parallelism to maximize bandwidth presents a unique and complex challenge for GPU architectures on how to distribute those ports among the streaming multiprocessors in the GPGPU. In this work, we extend the Vortex OpenGPU microarchitecture to incorporate a multiport memory hierarchy, spanning from the L1 cache to the last-level cache (LLC). In addition, we propose various arbitration strategies to optimize memory transfers across the cache hierarchy. The results have shown that an increase in memory ports increases IPC, achieving an average speedup of 2.34x with 8 memory ports in the tested configuration while showing relatively small area overhead.

**Link**: [arxiv](http://arxiv.org/abs/2503.17602v1),  [pdf](http://arxiv.org/pdf/2503.17602v1)

**Tags**: cs.AR 



### PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language   Model Inference
**Authors**: Yufeng Gu, Alireza Khadem, Sumanth Umesh, Ning Liang, Xavier Servot, Onur Mutlu, Ravi Iyer, Reetuparna Das

**Updated**: 2025-03-21T21:10:02Z

**Summary**: Large Language Model (LLM) inference uses an autoregressive manner to generate one token at a time, which exhibits notably lower operational intensity compared to earlier Machine Learning (ML) models such as encoder-only transformers and Convolutional Neural Networks. At the same time, LLMs possess large parameter sizes and use key-value caches to store context information. Modern LLMs support context windows with up to 1 million tokens to generate versatile text, audio, and video content. A large key-value cache unique to each prompt requires a large memory capacity, limiting the inference batch size. Both low operational intensity and limited batch size necessitate a high memory bandwidth. However, contemporary hardware systems for ML model deployment, such as GPUs and TPUs, are primarily optimized for compute throughput. This mismatch challenges the efficient deployment of advanced LLMs and makes users pay for expensive compute resources that are poorly utilized for the memory-bound LLM inference tasks.   We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which harnesses CXL memory expansion capabilities to accommodate substantial LLM sizes, and utilizes near-bank processing units to deliver high memory bandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable CXL network to support peer-to-peer and collective communication primitives across CXL devices. We implement various parallelism strategies to distribute LLMs across these devices. Compared to GPU baselines with maximum supported batch sizes and similar average power, CENT achieves 2.3$\times$ higher throughput and consumes 2.3$\times$ less energy. CENT enhances the Total Cost of Ownership (TCO), generating 5.2$\times$ more tokens per dollar than GPUs.

**Link**: [arxiv](http://arxiv.org/abs/2502.07578v2),  [pdf](http://arxiv.org/pdf/2502.07578v2)

**Tags**: cs.AR 



### RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach   for Large Language Models
**Authors**: Quan Wei, Chung-Yiu Yau, Hoi-To Wai, Yang Katie Zhao, Dongyeop Kang, Youngsuk Park, Mingyi Hong

**Updated**: 2025-03-21T19:26:12Z

**Summary**: Supervised fine-tuning is a standard method for adapting pre-trained large language models (LLMs) to downstream tasks. Quantization has been recently studied as a post-training technique for efficient LLM deployment. To obtain quantized fine-tuned LLMs, conventional pipelines would first fine-tune the pre-trained models, followed by post-training quantization. This often yields suboptimal performance as it fails to leverage the synergy between fine-tuning and quantization. To effectively realize low-bit quantization of weights, activations, and KV caches in LLMs, we propose an algorithm named Rotated Straight-Through-Estimator (RoSTE), which combines quantization-aware supervised fine-tuning (QA-SFT) with an adaptive rotation strategy that identifies an effective rotation configuration to reduce activation outliers. We provide theoretical insights on RoSTE by analyzing its prediction error when applied to an overparameterized least square quantized training problem. Our findings reveal that the prediction error is directly proportional to the quantization error of the converged weights, which can be effectively managed through an optimized rotation configuration. Experiments on Pythia, Qwen and Llama models of different sizes demonstrate the effectiveness of RoSTE. Compared to existing post-SFT quantization baselines, our method consistently achieves superior performances across various tasks and different LLM architectures.

**Link**: [arxiv](http://arxiv.org/abs/2502.09003v2),  [pdf](http://arxiv.org/pdf/2502.09003v2)

**Tags**: cs.LG cs.AI 



### LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers
**Authors**: Xuan Shen, Zhao Song, Yufa Zhou, Bo Chen, Yanyu Li, Yifan Gong, Kai Zhang, Hao Tan, Jason Kuen, Henghui Ding, Zhihao Shu, Wei Niu, Pu Zhao, Yanzhi Wang, Jiuxiang Gu

**Updated**: 2025-03-21T15:52:39Z

**Summary**: Diffusion Transformers have emerged as the preeminent models for a wide array of generative tasks, demonstrating superior performance and efficacy across various applications. The promising results come at the cost of slow inference, as each denoising step requires running the whole transformer model with a large amount of parameters. In this paper, we show that performing the full computation of the model at each diffusion step is unnecessary, as some computations can be skipped by lazily reusing the results of previous steps. Furthermore, we show that the lower bound of similarity between outputs at consecutive steps is notably high, and this similarity can be linearly approximated using the inputs. To verify our demonstrations, we propose the \textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached results from earlier steps to skip redundant computations. Specifically, we incorporate lazy learning layers into the model, effectively trained to maximize laziness, enabling dynamic skipping of redundant computations. Experimental results show that LazyDiT outperforms the DDIM sampler across multiple diffusion transformer models at various resolutions. Furthermore, we implement our method on mobile devices, achieving better performance than DDIM with similar latency. Code: https://github.com/shawnricecake/lazydit

**Link**: [arxiv](http://arxiv.org/abs/2412.12444v3),  [pdf](http://arxiv.org/pdf/2412.12444v3)

**Tags**: cs.LG cs.AI 



### AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out   Context Attribution
**Authors**: Fengyuan Liu, Nikhil Kandpal, Colin Raffel

**Updated**: 2025-03-21T15:47:53Z

**Summary**: The influence of contextual input on the behavior of large language models (LLMs) has prompted the development of context attribution methods that aim to quantify each context span's effect on an LLM's generations. The leave-one-out (LOO) error, which measures the change in the likelihood of the LLM's response when a given span of the context is removed, provides a principled way to perform context attribution, but can be prohibitively expensive to compute for large models. In this work, we introduce AttriBoT, a series of novel techniques for efficiently computing an approximation of the LOO error for context attribution. Specifically, AttriBoT uses cached activations to avoid redundant operations, performs hierarchical attribution to reduce computation, and emulates the behavior of large target models with smaller proxy models. Taken together, AttriBoT can provide a >300x speedup while remaining more faithful to a target model's LOO error than prior context attribution methods. This stark increase in performance makes computing context attributions for a given response 30x faster than generating the response itself, empowering real-world applications that require computing attributions at scale. We release a user-friendly and efficient implementation of AttriBoT to enable efficient LLM interpretability as well as encourage future development of efficient context attribution methods.

**Link**: [arxiv](http://arxiv.org/abs/2411.15102v3),  [pdf](http://arxiv.org/pdf/2411.15102v3)

**Tags**: cs.LG 



### Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic   Vision-language Context Sparsification
**Authors**: Wenxuan Huang, Zijie Zhai, Yunhang Shen, Shaosheng Cao, Fei Zhao, Xiangfeng Xu, Zheyu Ye, Yao Hu, Shaohui Lin

**Updated**: 2025-03-21T13:30:33Z

**Summary**: Multimodal Large Language Models (MLLMs) have achieved remarkable success in vision understanding, reasoning, and interaction. However, the inference computation and memory increase progressively with the generation of output tokens during decoding, directly affecting the efficacy of MLLMs. Existing methods attempt to reduce the vision context redundancy to achieve efficient MLLMs. Unfortunately, the efficiency benefits of the vision context reduction in the prefill stage gradually diminish during the decoding stage. To address this problem, we proposed a dynamic vision-language context sparsification framework Dynamic-LLaVA, which dynamically reduces the redundancy of vision context in the prefill stage and decreases the memory and computation overhead of the generated language context during decoding. Dynamic-LLaVA designs a tailored sparsification inference scheme for different inference modes, i.e., prefill, decoding with and without KV cache, to achieve efficient inference of MLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by $\sim$75\% in the prefill stage. Meanwhile, throughout the entire generation process of MLLMs, Dynamic-LLaVA reduces the $\sim$50\% computation consumption under decoding without KV cache, while saving $\sim$50\% GPU memory overhead when decoding with KV cache, due to the vision-language context sparsification. Extensive experiments also demonstrate that Dynamic-LLaVA achieves efficient inference for MLLMs with negligible understanding and generation ability degradation or even performance gains compared to the full-context inference baselines. Code is available at https://github.com/Osilly/dynamic_llava .

**Link**: [arxiv](http://arxiv.org/abs/2412.00876v4),  [pdf](http://arxiv.org/pdf/2412.00876v4)

**Tags**: cs.CV cs.AI cs.CL cs.LG 



### Language-Queried Target Sound Extraction Without Parallel Training Data
**Authors**: Hao Ma, Zhiyuan Peng, Xu Li, Yukai Li, Mingjie Shao, Qiuqiang Kong, Ju Liu

**Updated**: 2025-03-21T12:51:15Z

**Summary**: Language-queried target sound extraction (TSE) aims to extract specific sounds from mixtures based on language queries. Traditional fully-supervised training schemes require extensively annotated parallel audio-text data, which are labor-intensive. We introduce a parallel-data-free training scheme, requiring only unlabelled audio clips for TSE model training by utilizing the contrastive language-audio pre-trained model (CLAP). In a vanilla parallel-data-free training stage, target audio is encoded using the pre-trained CLAP audio encoder to form a condition embedding, while during testing, user language queries are encoded by CLAP text encoder as the condition embedding. This vanilla approach assumes perfect alignment between text and audio embeddings, which is unrealistic. Two major challenges arise from training-testing mismatch: the persistent modality gap between text and audio and the risk of overfitting due to the exposure of rich acoustic details in target audio embedding during training. To address this, we propose a retrieval-augmented strategy. Specifically, we create an embedding cache using audio captions generated by a large language model (LLM). During training, target audio embeddings retrieve text embeddings from this cache to use as condition embeddings, ensuring consistent modalities between training and testing and eliminating information leakage. Extensive experiment results show that our retrieval-augmented approach achieves consistent and notable performance improvements over existing state-of-the-art with better generalizability.

**Link**: [arxiv](http://arxiv.org/abs/2409.09398v3),  [pdf](http://arxiv.org/pdf/2409.09398v3)

**Tags**: eess.AS cs.SD 



### Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs
**Authors**: Anshumann, Mohd Abbas Zaidi, Akhil Kedia, Jinwoo Ahn, Taehwak Kwon, Kangwook Lee, Haejun Lee, Joohyung Lee

**Updated**: 2025-03-21T05:58:18Z

**Summary**: Knowledge distillation can be a cost-effective technique to distill knowledge in Large Language Models, if the teacher output logits can be pre-computed and cached. However, successfully applying this to pre-training remains largely unexplored. In this work, we prove that naive approaches for sparse knowledge distillation such as caching Top-K probabilities, while intuitive, provide biased estimates of teacher probability distribution to the student, resulting in suboptimal performance and calibration. We propose an importance-sampling-based method `Random Sampling Knowledge Distillation', which provides unbiased estimates, preserves the gradient in expectation, and requires storing significantly sparser logits. Our method enables faster training of student models with marginal overhead (<10%) compared to cross-entropy based training, while maintaining competitive performance compared to full distillation, across a range of model sizes from 300M to 3B.

**Link**: [arxiv](http://arxiv.org/abs/2503.16870v1),  [pdf](http://arxiv.org/pdf/2503.16870v1)

**Tags**: cs.LG cs.AI cs.CL 68T50 I.2.7 



### MKG-Rank: Enhancing Large Language Models with Knowledge Graph for   Multilingual Medical Question Answering
**Authors**: Feiyang Li, Yingjian Chen, Haoran Liu, Rui Yang, Han Yuan, Yuang Jiang, Tianxiao Li, Edison Marrese Taylor, Hossein Rouhizadeh, Yusuke Iwasawa, Douglas Teodoro, Yutaka Matsuo, Irene Li

**Updated**: 2025-03-21T01:59:12Z

**Summary**: Large Language Models (LLMs) have shown remarkable progress in medical question answering (QA), yet their effectiveness remains predominantly limited to English due to imbalanced multilingual training data and scarce medical resources for low-resource languages. To address this critical language gap in medical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking (MKG-Rank), a knowledge graph-enhanced framework that enables English-centric LLMs to perform multilingual medical QA. Through a word-level translation mechanism, our framework efficiently integrates comprehensive English-centric medical knowledge graphs into LLM reasoning at a low cost, mitigating cross-lingual semantic distortion and achieving precise medical QA across language barriers. To enhance efficiency, we introduce caching and multi-angle ranking strategies to optimize the retrieval process, significantly reducing response times and prioritizing relevant medical knowledge. Extensive evaluations on multilingual medical QA benchmarks across Chinese, Japanese, Korean, and Swahili demonstrate that MKG-Rank consistently outperforms zero-shot LLMs, achieving maximum 35.03% increase in accuracy, while maintaining an average retrieval time of only 0.0009 seconds.

**Link**: [arxiv](http://arxiv.org/abs/2503.16131v2),  [pdf](http://arxiv.org/pdf/2503.16131v2)

**Tags**: cs.CL 



### A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals
**Authors**: RÃ³bert Busa-Fekete, Julian Zimmert, AndrÃ¡s GyÃ¶rgy, Linhai Qiu, Tzu-Wei Sung, Hao Shen, Hyomin Choi, Sharmila Subramaniam, Li Xiao

**Updated**: 2025-03-20T21:49:15Z

**Summary**: Web refresh crawling is the problem of keeping a cache of web pages fresh, that is, having the most recent copy available when a page is requested, given a limited bandwidth available to the crawler. Under the assumption that the change and request events, resp., to each web page follow independent Poisson processes, the optimal scheduling policy was derived by Azar et al. 2018. In this paper, we study an extension of this problem where side information indicating content changes, such as various types of web pings, for example, signals from sitemaps, content delivery networks, etc., is available. Incorporating such side information into the crawling policy is challenging, because (i) the signals can be noisy with false positive events and with missing change events; and (ii) the crawler should achieve a fair performance over web pages regardless of the quality of the side information, which might differ from web page to web page. We propose a scalable crawling algorithm which (i) uses the noisy side information in an optimal way under mild assumptions; (ii) can be deployed without heavy centralized computation; (iii) is able to crawl web pages at a constant total rate without spikes in the total bandwidth usage over any time interval, and automatically adapt to the new optimal solution when the total bandwidth changes without centralized computation. Experiments clearly demonstrate the versatility of our approach.

**Link**: [arxiv](http://arxiv.org/abs/2502.02430v3),  [pdf](http://arxiv.org/pdf/2502.02430v3)

**Tags**: stat.ML cs.IR cs.LG 



### Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language   Models
**Authors**: Keda Tao, Haoxuan You, Yang Sui, Can Qin, Huan Wang

**Updated**: 2025-03-20T15:52:43Z

**Summary**: Video large language models (VideoLLMs) have demonstrated the capability to process longer video inputs and enable complex reasoning and analysis. However, due to the thousands of visual tokens from the video frames, key-value (KV) cache can significantly increase memory requirements, becoming a bottleneck for inference speed and memory usage. KV cache quantization is a widely used approach to address this problem. In this paper, we find that 2-bit KV quantization of VideoLLMs can hardly hurt the model performance, while the limit of KV cache quantization in even lower bits has not been investigated. To bridge this gap, we introduce VidKV, a plug-and-play KV cache quantization method to compress the KV cache to lower than 2 bits. Specifically, (1) for key, we propose a mixed-precision quantization strategy in the channel dimension, where we perform 2-bit quantization for anomalous channels and 1-bit quantization combined with FFT for normal channels; (2) for value, we implement 1.58-bit quantization while selectively filtering semantically salient visual tokens for targeted preservation, for a better trade-off between precision and model performance. Importantly, our findings suggest that the value cache of VideoLLMs should be quantized in a per-channel fashion instead of the per-token fashion proposed by prior KV cache quantization works for LLMs. Empirically, extensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show that VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit precision with almost no performance drop compared to the FP16 counterparts.

**Link**: [arxiv](http://arxiv.org/abs/2503.16257v1),  [pdf](http://arxiv.org/pdf/2503.16257v1)

**Tags**: cs.CV 



### SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs
**Authors**: Shibo Jie, Yehui Tang, Kai Han, Zhi-Hong Deng, Jing Han

**Updated**: 2025-03-20T14:01:56Z

**Summary**: Transformer-based large language models (LLMs) have already achieved remarkable results on long-text tasks, but the limited GPU memory (VRAM) resources struggle to accommodate the linearly growing demand for key-value (KV) cache as the sequence length increases, which has become a bottleneck for the application of LLMs on long sequences. Existing KV cache compression methods include eviction, merging, or quantization of the KV cache to reduce its size. However, compression results in irreversible information forgetting, potentially affecting the accuracy of subsequent decoding. In this paper, we propose SpeCache, which takes full advantage of the large and easily expandable CPU memory to offload the complete KV cache, and dynamically fetches KV pairs back in each decoding step based on their importance measured by low-bit KV cache copy in VRAM. To avoid inference latency caused by CPU-GPU communication, SpeCache speculatively predicts the KV pairs that the next token might attend to, allowing us to prefetch them before the next decoding step which enables parallelization of prefetching and computation. Experiments on LongBench and Needle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM usage while avoiding information forgetting for long sequences without re-training, even with a 10x high KV cache compression ratio.

**Link**: [arxiv](http://arxiv.org/abs/2503.16163v1),  [pdf](http://arxiv.org/pdf/2503.16163v1)

**Tags**: cs.CL 



### PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video   Streaming
**Authors**: Liming Liu, Jiangkai Wu, Haoyang Wang, Peiheng Wang, Xinggong Zhang, Zongming Guo

**Updated**: 2025-03-20T13:00:36Z

**Summary**: Traditional video compression algorithms exhibit significant quality degradation at extremely low bitrates. Promptus emerges as a new paradigm for video streaming, substantially cutting down the bandwidth essential for video streaming. However, Promptus is computationally intensive and can not run in real-time on mobile devices. This paper presents PromptMobile, an efficient acceleration framework tailored for on-device Promptus. Specifically, we propose (1) a two-stage efficient generation framework to reduce computational cost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant computations by 16.6\%, (3) system-level optimizations to further enhance efficiency. The evaluations demonstrate that compared with the original Promptus, PromptMobile achieves a 13.6x increase in image generation speed. Compared with other streaming methods, PromptMobile achives an average LPIPS improvement of 0.016 (compared with H.265), reducing 60\% of severely distorted frames (compared to VQGAN).

**Link**: [arxiv](http://arxiv.org/abs/2503.16112v1),  [pdf](http://arxiv.org/pdf/2503.16112v1)

**Tags**: cs.NI cs.AI cs.MM 



### BlockDance: Reuse Structurally Similar Spatio-Temporal Features to   Accelerate Diffusion Transformers
**Authors**: Hui Zhang, Tingwei Gao, Jie Shao, Zuxuan Wu

**Updated**: 2025-03-20T08:07:31Z

**Summary**: Diffusion models have demonstrated impressive generation capabilities, particularly with recent advancements leveraging transformer architectures to improve both visual and artistic quality. However, Diffusion Transformers (DiTs) continue to encounter challenges related to low inference speed, primarily due to the iterative denoising process. To address this issue, we propose BlockDance, a training-free approach that explores feature similarities at adjacent time steps to accelerate DiTs. Unlike previous feature-reuse methods that lack tailored reuse strategies for features at different scales, BlockDance prioritizes the identification of the most structurally similar features, referred to as Structurally Similar Spatio-Temporal (STSS) features. These features are primarily located within the structure-focused blocks of the transformer during the later stages of denoising. BlockDance caches and reuses these highly similar features to mitigate redundant computation, thereby accelerating DiTs while maximizing consistency with the generated results of the original model. Furthermore, considering the diversity of generated content and the varying distributions of redundant features, we introduce BlockDance-Ada, a lightweight decision-making network tailored for instance-specific acceleration. BlockDance-Ada dynamically allocates resources and provides superior content quality. Both BlockDance and BlockDance-Ada have proven effective across various generation tasks and models, achieving accelerations between 25% and 50% while maintaining generation quality.

**Link**: [arxiv](http://arxiv.org/abs/2503.15927v1),  [pdf](http://arxiv.org/pdf/2503.15927v1)

**Tags**: cs.CV 



### Mobile Edge Intelligence for Large Language Models: A Contemporary   Survey
**Authors**: Guanqiao Qu, Qiyuan Chen, Wei Wei, Zheng Lin, Xianhao Chen, Kaibin Huang

**Updated**: 2025-03-20T05:23:42Z

**Summary**: On-device large language models (LLMs), referring to running LLMs on edge devices, have raised considerable interest since they are more cost-effective, latency-efficient, and privacy-preserving compared with the cloud paradigm. Nonetheless, the performance of on-device LLMs is intrinsically constrained by resource limitations on edge devices. Sitting between cloud and on-device AI, mobile edge intelligence (MEI) presents a viable solution by provisioning AI capabilities at the edge of mobile networks, enabling end users to offload heavy AI computation to capable edge servers nearby. This article provides a contemporary survey on harnessing MEI for LLMs. We begin by illustrating several killer applications to demonstrate the urgent need for deploying LLMs at the network edge. Next, we present the preliminaries of LLMs and MEI, followed by resource-efficient LLM techniques. We then present an architectural overview of MEI for LLMs (MEI4LLM), outlining its core components and how it supports the deployment of LLMs. Subsequently, we delve into various aspects of MEI4LLM, extensively covering edge LLM caching and delivery, edge LLM training, and edge LLM inference. Finally, we identify future research opportunities. We hope this article inspires researchers in the field to leverage mobile edge computing to facilitate LLM deployment, thereby unleashing the potential of LLMs across various privacy- and delay-sensitive applications.

**Link**: [arxiv](http://arxiv.org/abs/2407.18921v2),  [pdf](http://arxiv.org/pdf/2407.18921v2)

**Tags**: cs.NI cs.AI cs.LG 



### Formalising CXL Cache Coherence
**Authors**: Chengsong Tan, Alastair F. Donaldson, John Wickerson

**Updated**: 2025-03-19T10:19:30Z

**Summary**: We report our experience formally modelling and verifying CXL.cache, the inter-device cache coherence protocol of the Compute Express Link standard. We have used the Isabelle proof assistant to create a formal model for CXL.cache based on the prose English specification. This led to us identifying and proposing fixes to several problems we identified as unclear, ambiguous or inaccurate, some of which could lead to incoherence if left unfixed. Nearly all our issues and proposed fixes have been confirmed and tentatively accepted by the CXL consortium for adoption, save for one which is still under discussion. To validate the faithfulness of our model we performed scenario verification of essential restrictions such as "Snoop-pushes-GO", and produced a fully mechanised proof of a coherence property of the model. The considerable size of this proof, comprising tens of thousands of lemmas, prompted us to develop new proof automation tools, which we have made available for other Isabelle users working with similarly cumbersome proofs.

**Link**: [arxiv](http://arxiv.org/abs/2410.15908v2),  [pdf](http://arxiv.org/pdf/2410.15908v2)

**Tags**: cs.AR cs.PL 68 (Primary) C.1; F.3 



### Exploring the Limits of KV Cache Compression in Visual Autoregressive   Transformers
**Authors**: Bo Chen, Xiaoyu Li, Yekun Ke, Yingyu Liang, Zhenmei Shi, Zhao Song

**Updated**: 2025-03-19T04:18:57Z

**Summary**: A fundamental challenge in Visual Autoregressive models is the substantial memory overhead required during inference to store previously generated representations. Despite various attempts to mitigate this issue through compression techniques, prior works have not explicitly formalized the problem of KV-cache compression in this context. In this work, we take the first step in formally defining the KV-cache compression problem for Visual Autoregressive transformers. We then establish a fundamental negative result, proving that any mechanism for sequential visual token generation under attention-based architectures must use at least $\Omega(n^2 d)$ memory, when $d = \Omega(\log n)$, where $n$ is the number of tokens generated and $d$ is the embedding dimensionality. This result demonstrates that achieving truly sub-quadratic memory usage is impossible without additional structural constraints. Our proof is constructed via a reduction from a computational lower bound problem, leveraging randomized embedding techniques inspired by dimensionality reduction principles. Finally, we discuss how sparsity priors on visual representations can influence memory efficiency, presenting both impossibility results and potential directions for mitigating memory overhead.

**Link**: [arxiv](http://arxiv.org/abs/2503.14881v1),  [pdf](http://arxiv.org/pdf/2503.14881v1)

**Tags**: cs.LG cs.AI cs.CV 



### Degradation of 2.4-kV $Ga_{2}O_{3}$ Schottky Barrier Diode at High   Temperatures up to 500 Â°C
**Authors**: Hunter Ellis, Wei Jia, Imteaz Rahaman, Apostoli Hillas, Botong Li, Michael A. Scarpulla, Berardi Sensale Rodriguez, Kai Fu

**Updated**: 2025-03-19T00:30:43Z

**Summary**: Ga2O3 Schottky barrier diodes featuring a field plate and a composite SiO2/SiNx dielectric layer beneath the field plate were fabricated, achieving a breakdown voltage of 2.4 kV at room temperature. Electrical performance and degradation were analyzed via I-V and C-V measurements from 25 {\deg}C to 500 {\deg}C, revealing temperature-dependent transport, interface stability, and device stability. Upon returning to room temperature, the diodes exhibited nearly unchanged forward characteristics, while the breakdown voltage declined significantly from 2.4 kV to 700 V. This behavior indicates a temperature-induced reduction in the barrier height. Detailed analysis revealed that variable range hopping (VRH) dominated the leakage mechanism at moderate temperatures, while thermal emission (TE) became increasingly significant at temperatures exceeding 400 {\deg}C.

**Link**: [arxiv](http://arxiv.org/abs/2503.14805v1),  [pdf](http://arxiv.org/pdf/2503.14805v1)

**Tags**: cond-mat.mtrl-sci 



### NeCTAr: A Heterogeneous RISC-V SoC for Language Model Inference in Intel   16
**Authors**: Viansa Schmulbach, Jason Kim, Ethan Gao, Lucy Revina, Nikhil Jha, Ethan Wu, Borivoje Nikolic

**Updated**: 2025-03-18T20:16:50Z

**Summary**: This paper introduces NeCTAr (Near-Cache Transformer Accelerator), a 16nm heterogeneous multicore RISC-V SoC for sparse and dense machine learning kernels with both near-core and near-memory accelerators. A prototype chip runs at 400MHz at 0.85V and performs matrix-vector multiplications with 109 GOPs/W. The effectiveness of the design is demonstrated by running inference on a sparse language model, ReLU-Llama.

**Link**: [arxiv](http://arxiv.org/abs/2503.14708v1),  [pdf](http://arxiv.org/pdf/2503.14708v1)

**Tags**: cs.AR 



### Towards More Economical Context-Augmented LLM Generation by Reusing   Stored KV Cache
**Authors**: Hanchen Li, Yuhan Liu, Yihua Cheng, Kuntai Du, Junchen Jiang

**Updated**: 2025-03-18T18:52:03Z

**Summary**: Across large language model (LLM) applications, we observe an emerging trend for reusing KV caches to save the prefill delays of processing repeated input texts in different LLM inputs. This has led to a broad design space, including colocating stored KV caches with (or close to) GPUs to various KV cache compression. However, a key question remains unanswered: can these delay reductions also be economically favorable? Specifically, we ask whether a developer can use public cloud services to store precomputed KV caches and reuse them to save delay without incurring more costs in terms of compute, storage, and network. To answer this question, we propose an validated analytical model for the cloud cost (in compute, storage, and network) of storing and reusing KV caches based on various workload parameters, such as reuse frequency, generated text lengths, model sizes, etc. Preliminary results show that KV cache reusing is able to save both delay and cloud cost across a range of workloads with long context. And we call more efforts on building more economical context augmented LLM by KV cache reusing.

**Link**: [arxiv](http://arxiv.org/abs/2503.14647v1),  [pdf](http://arxiv.org/pdf/2503.14647v1)

**Tags**: cs.NI 



### Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse   Attention
**Authors**: Emily Xiao, Chin-Jou Li, Yilin Zhang, Graham Neubig, Amanda Bertsch

**Updated**: 2025-03-18T17:13:42Z

**Summary**: Many-shot in-context learning has recently shown promise as an alternative to finetuning, with the major advantage that the same model can be served for multiple tasks. However, this shifts the computational burden from training-time to inference-time, making deployment of many-shot ICL challenging to justify in-practice. This cost is further increased if a custom demonstration set is retrieved for each inference example. We present Dynamic Block-Sparse Attention, a training-free framework for retrieval-based many-shot in-context learning. By combining carefully designed block-sparse attention and retrieval of cached groups of demonstrations, we achieve comparable per-example latency to finetuning while maintaining on average >95% of the best method's accuracy across strong ICL and finetuning baselines. We hope that this will further enable the deployment of many-shot ICL at scale.

**Link**: [arxiv](http://arxiv.org/abs/2503.08640v2),  [pdf](http://arxiv.org/pdf/2503.08640v2)

**Tags**: cs.CL 



### Block Diffusion: Interpolating Between Autoregressive and Diffusion   Language Models
**Authors**: Marianne Arriola, Aaron Gokaslan, Justin T Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, Volodymyr Kuleshov

**Updated**: 2025-03-18T15:58:18Z

**Summary**: Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page: https://m-arriola.com/bd3lms/

**Link**: [arxiv](http://arxiv.org/abs/2503.09573v2),  [pdf](http://arxiv.org/pdf/2503.09573v2)

**Tags**: cs.LG cs.AI 



### Suffixient Arrays: a New Efficient Suffix Array Compression Technique
**Authors**: Davide Cenzato, Lore Depuydt, Travis Gagie, Sung-Hwan Kim, Giovanni Manzini, Francisco Olivares, Nicola Prezza

**Updated**: 2025-03-18T09:43:33Z

**Summary**: The Suffix Array is a classic text index enabling on-line pattern matching queries via simple binary search. The main drawback of the Suffix Array is that it takes linear space in the text's length, even if the text itself is extremely compressible. Several works in the literature showed that the Suffix Array can be compressed, but they all rely on complex succinct data structures which in practice tend to exhibit poor cache locality and thus significantly slow down queries. In this paper, we propose a new simple and very efficient solution to this problem by presenting the \emph{Suffixient Array}: a tiny subset of the Suffix Array \emph{sufficient} to locate on-line one pattern occurrence (in general, all its Maximal Exact Matches) via binary search, provided that random access to the text is available. We prove that: (i) the Suffixient Array length $\chi$ is a strong repetitiveness measure, (ii) unlike most existing repetition-aware indexes such as the $r$-index, our new index is efficient in the I/O model, and (iii) Suffixient Arrays can be computed in linear time and compressed working space. We show experimentally that, when using well-established compressed random access data structures on repetitive collections, the Suffixient Array $\SuA$ is \emph{simultaneously} (i) faster and orders of magnitude smaller than the Suffix Array $\SA$ and (ii) smaller and \emph{one to two orders of magnitude faster} than the $r$-index. With an average pattern matching query time as low as 3.5 ns per character, our new index gets very close to the ultimate lower bound: the RAM throughput of our workstation (1.18 ns per character).

**Link**: [arxiv](http://arxiv.org/abs/2407.18753v2),  [pdf](http://arxiv.org/pdf/2407.18753v2)

**Tags**: cs.DS 



### Multimodal Mamba: Decoder-only Multimodal State Space Model via   Quadratic to Linear Distillation
**Authors**: Bencheng Liao, Hongyuan Tao, Qian Zhang, Tianheng Cheng, Yingyue Li, Haoran Yin, Wenyu Liu, Xinggang Wang

**Updated**: 2025-03-18T07:02:33Z

**Summary**: Recent Multimodal Large Language Models (MLLMs) have achieved remarkable performance but face deployment challenges due to their quadratic computational complexity, growing Key-Value cache requirements, and reliance on separate vision encoders. We propose mmMamba, a framework for developing linear-complexity native multimodal state space models through progressive distillation from existing MLLMs using moderate academic computational resources. Our approach enables the direct conversion of trained decoder-only MLLMs to linear-complexity architectures without requiring pre-trained RNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba from trained Transformer and a three-stage distillation recipe, which can effectively transfer the knowledge from Transformer to Mamba while preserving multimodal capabilities. Our method also supports flexible hybrid architectures that combine Transformer and Mamba layers for customizable efficiency-performance trade-offs. Distilled from the Transformer-based decoder-only HoVLE, mmMamba-linear achieves competitive performance against existing linear and quadratic-complexity VLMs, while mmMamba-hybrid further improves performance significantly, approaching HoVLE's capabilities. At 103K tokens, mmMamba-linear demonstrates 20.6$\times$ speedup and 75.8% GPU memory reduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\times$ speedup and 60.2% memory savings. Code and models are released at https://github.com/hustvl/mmMamba

**Link**: [arxiv](http://arxiv.org/abs/2502.13145v2),  [pdf](http://arxiv.org/pdf/2502.13145v2)

**Tags**: cs.CV 



### Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model
**Authors**: Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, Fang Wan

**Updated**: 2025-03-18T04:49:23Z

**Summary**: As a fundamental backbone for video generation, diffusion models are challenged by low inference speed due to the sequential nature of denoising. Previous methods speed up the models by caching and reusing model outputs at uniformly selected timesteps. However, such a strategy neglects the fact that differences among model outputs are not uniform across timesteps, which hinders selecting the appropriate model outputs to cache, leading to a poor balance between inference efficiency and visual quality. In this study, we introduce Timestep Embedding Aware Cache (TeaCache), a training-free caching approach that estimates and leverages the fluctuating differences among model outputs across timesteps. Rather than directly using the time-consuming model outputs, TeaCache focuses on model inputs, which have a strong correlation with the modeloutputs while incurring negligible computational cost. TeaCache first modulates the noisy inputs using the timestep embeddings to ensure their differences better approximating those of model outputs. TeaCache then introduces a rescaling strategy to refine the estimated differences and utilizes them to indicate output caching. Experiments show that TeaCache achieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07% Vbench score) degradation of visual quality.

**Link**: [arxiv](http://arxiv.org/abs/2411.19108v2),  [pdf](http://arxiv.org/pdf/2411.19108v2)

**Tags**: cs.CV 



### Efficient Hardware Accelerator Based on Medium Granularity Dataflow for   SpTRSV
**Authors**: Qian Chen, Xiaofeng Yang, Shengli Lu

**Updated**: 2025-03-18T01:58:36Z

**Summary**: Sparse triangular solve (SpTRSV) is widely used in various domains. Numerous studies have been conducted using CPUs, GPUs, and specific hardware accelerators, where dataflows can be categorized into coarse and fine granularity. Coarse dataflows offer good spatial locality but suffer from low parallelism, while fine dataflows provide high parallelism but disrupt the spatial structure, leading to increased nodes and poor data reuse. This paper proposes a novel hardware accelerator for SpTRSV or SpTRSV-like DAGs. The accelerator implements a medium granularity dataflow through hardware-software codesign and achieves both excellent spatial locality and high parallelism. Additionally, a partial sum caching mechanism is introduced to reduce the blocking frequency of processing elements (PEs), and a reordering algorithm of intra-node edges computation is developed to enhance data reuse. Experimental results on 245 benchmarks with node counts reaching up to 85,392 demonstrate that this work achieves average performance improvements of 7.0$\times$ (up to 27.8$\times$) over CPUs and 5.8$\times$ (up to 98.8$\times$) over GPUs. Compared to the state-of-the-art technique (DPU-v2), this work shows a 2.5$\times$ (up to 5.9$\times$) average performance improvement and 1.7$\times$ (up to 4.1$\times$) average energy efficiency enhancement.

**Link**: [arxiv](http://arxiv.org/abs/2406.10511v3),  [pdf](http://arxiv.org/pdf/2406.10511v3)

**Tags**: cs.DC cs.AR cs.NA cs.PF math.NA 



### AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference   Serving for Diverse Applications
**Authors**: Haiying Shen, Tanmoy Sen

**Updated**: 2025-03-17T21:47:43Z

**Summary**: In this paper, we consider a mixed-prompt scenario for a large language model (LLM) inference serving system that supports diverse applications with both short prompts and long prompts and heterogeneous SLOs for iteration time. To improve throughput when handling long prompts, previous research introduces a chunking method, but has not addressed heterogeneous SLOs. To address the limitation, we propose AccelGen, a high-throughput LLM inference serving system with heterogeneous SLO guarantees for diverse applications. AccelGen introduces four core components: (1) SLO-guaranteed dynamic chunking, which dynamically adjusts chunk sizes to maximize GPU compute utilization while meeting iteration-level SLOs; (2) Iteration-level SLO-based task prioritization, which prioritizes tight-SLO requests and batches requests with similar SLOs; (3) Multi-resource-aware batching, which selects queued requests to maximize the utilizations of both GPU compute resource and key-value cache (KVC). Trace-driven real experiments demonstrate that AccelGen achieves 1.42-11.21X higher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment, and 1.61-12.22X lower response latency compared to the state-of-the-art approaches. It achieves performance near the Oracle, which optimally maximizes goodput.

**Link**: [arxiv](http://arxiv.org/abs/2503.13737v1),  [pdf](http://arxiv.org/pdf/2503.13737v1)

**Tags**: cs.CL 



### Fast Maximum Likelihood Positioning for a Staggered Layer Scintillation   PET Detector
**Authors**: Christoph W. Lerche, Wenwei Bi, Mirjam Schoeneck, Debora Niekaemper, Qi Liu, Elisabeth Pfaehler, Lutz Tellmann, Juergen J. Scheins, N. Jon Shah

**Updated**: 2025-03-17T21:11:30Z

**Summary**: In this study, we propose a fast implementation of a Maximum Likelihood Positioning (MLP) algorithm to estimate the energy and identify the active scintillator pixel in staggered layer scintillation detectors for PET. The staggered layer design with pixelated scintillators enables the determination of the gamma's depth of interaction and facilitates an iteration-free formulation of the MLP algorithm. The efficacy of the algorithm optimization was tested on a scintillation detector block designed for an ultra-high field BrainPET 7T, comprising three scintillator pixel layers. The three layers contain 24 x 24, 24 x 23 and 23 x 22 scintillator pixels, respectively, with a pixel pitch of 2 mm in both directions and layer thicknesses of 9, 8 and 7 mm. Calibration measurements, in combination with an automated calibration script, were used to obtain the expected counts of scintillation photons required in the MLP algorithm. Using Single-Instruction-Multiple-Data parallelization, multi-threading and optimized cache lines, a maximum processing speed of approximately 22.5 million singles per second was achieved on a platform with four Intel Xeon Platinum 8168 CPUs and 60 threads, encompassing all required processing steps. The automatic calibration failed for 1 to 15 individual scintillator pixels in approximately 10 per cent of the 120 scintillation detector blocks, necessitating manual correction. After applying the energy correction to the positioned single events, an energy resolution of of 12 +/- 2 per cent FWHM was obtained for the entire scintillation block. This value is very close to the energy resolutions measured for the individual scintillator pixels, proving that the MLP accurately identifies the scintillating pixel and that the energy correction method effectively compensates for the light collection variations of the SiPM array.

**Link**: [arxiv](http://arxiv.org/abs/2503.13723v1),  [pdf](http://arxiv.org/pdf/2503.13723v1)

**Tags**: physics.ins-det physics.med-ph 92C55 (Primary) 94A08 (Secondary) 



### NVR: Vector Runahead on NPUs for Sparse Memory Access
**Authors**: Hui Wang, Zhengpeng Zhao, Jing Wang, Yushu Du, Yuan Cheng, Bing Guo, He Xiao, Chenhao Ma, Xiaomeng Han, Dean You, Jiapeng Guan, Ran Wei, Dawei Yang, Zhe Jiang

**Updated**: 2025-03-17T20:31:46Z

**Summary**: Deep Neural Networks are increasingly leveraging sparsity to reduce the scaling up of model parameter size. However, reducing wall-clock time through sparsity and pruning remains challenging due to irregular memory access patterns, leading to frequent cache misses. In this paper, we present NPU Vector Runahead (NVR), a prefetching mechanism tailored for NPUs to address cache miss problems in sparse DNN workloads. Rather than optimising memory patterns with high overhead and poor portability, NVR adapts runahead execution to the unique architecture of NPUs. NVR provides a general micro-architectural solution for sparse DNN workloads without requiring compiler or algorithmic support, operating as a decoupled, speculative, lightweight hardware sub-thread alongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an average 90% reduction in cache misses compared to SOTA prefetching in general-purpose processors, delivering 4x average speedup on sparse workloads versus NPUs without prefetching. Moreover, we investigate the advantages of incorporating a small cache (16KB) into the NPU combined with NVR. Our evaluation shows that expanding this modest cache delivers 5x higher performance benefits than increasing the L2 cache size by the same amount.

**Link**: [arxiv](http://arxiv.org/abs/2502.13873v2),  [pdf](http://arxiv.org/pdf/2502.13873v2)

**Tags**: cs.AR cs.AI 



### PrETi: Predicting Execution Time in Early Stage with LLVM and Machine   Learning
**Authors**: Risheng Xu, Philipp Sieweck, Hermann von Hasseln, Dirk Nowotka

**Updated**: 2025-03-17T19:32:26Z

**Summary**: We introduce preti, a novel framework for predicting software execution time during the early stages of development. preti leverages an LLVM-based simulation environment to extract timing-related runtime information, such as the count of executed LLVM IR instructions. This information, combined with historical execution time data, is utilized to train machine learning models for accurate time prediction. To further enhance prediction accuracy, our approach incorporates simulations of cache accesses and branch prediction. The evaluations on public benchmarks demonstrate that preti achieves an average Absolute Percentage Error (APE) of 11.98\%, surpassing state-of-the-art methods. These results underscore the effectiveness and efficiency of preti as a robust solution for early-stage timing analysis.

**Link**: [arxiv](http://arxiv.org/abs/2503.13679v1),  [pdf](http://arxiv.org/pdf/2503.13679v1)

**Tags**: cs.PF cs.LG 



## Keyword: LLM Inference 
 ### Easi3R: Estimating Disentangled Motion from DUSt3R Without Training
**Authors**: Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, Anpei Chen

**Updated**: 2025-03-31T17:59:58Z

**Summary**: Recent advances in DUSt3R have enabled robust estimation of dense point clouds and camera parameters of static scenes, leveraging Transformer network architectures and direct supervision on large-scale 3D datasets. In contrast, the limited scale and diversity of available 4D datasets present a major bottleneck for training a highly generalizable 4D model. This constraint has driven conventional 4D methods to fine-tune 3D models on scalable dynamic video data with additional geometric priors such as optical flow and depths. In this work, we take an opposite path and introduce Easi3R, a simple yet efficient training-free method for 4D reconstruction. Our approach applies attention adaptation during inference, eliminating the need for from-scratch pre-training or network fine-tuning. We find that the attention layers in DUSt3R inherently encode rich information about camera and object motion. By carefully disentangling these attention maps, we achieve accurate dynamic region segmentation, camera pose estimation, and 4D dense point map reconstruction. Extensive experiments on real-world dynamic videos demonstrate that our lightweight attention adaptation significantly outperforms previous state-of-the-art methods that are trained or finetuned on extensive dynamic datasets. Our code is publicly available for research purpose at https://easi3r.github.io/

**Link**: [arxiv](http://arxiv.org/abs/2503.24391v1),  [pdf](http://arxiv.org/pdf/2503.24391v1)

**Tags**: cs.CV 



### RIG: Synergizing Reasoning and Imagination in End-to-End Generalist   Policy
**Authors**: Zhonghan Zhao, Wenwei Zhang, Haian Huang, Kuikun Liu, Jianfei Gao, Gaoang Wang, Kai Chen

**Updated**: 2025-03-31T17:59:52Z

**Summary**: Reasoning before action and imagining potential outcomes (i.e., world models) are essential for embodied agents operating in complex open-world environments. Yet, prior work either incorporates only one of these abilities in an end-to-end agent or integrates multiple specialized models into an agent system, limiting the learning efficiency and generalization of the policy. Thus, this paper makes the first attempt to synergize Reasoning and Imagination in an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end manner, we construct a data pipeline that progressively integrates and enriches the content of imagination and reasoning in the trajectories collected from existing agents. The joint learning of reasoning and next image generation explicitly models the inherent correlation between reasoning, action, and dynamics of environments, and thus exhibits more than $17\times$ sample efficiency improvements and generalization in comparison with previous works. During inference, RIG first reasons about the next action, produces potential action, and then predicts the action outcomes, which offers the agent a chance to review and self-correct based on the imagination before taking real actions. Experimental results show that the synergy of reasoning and imagination not only improves the robustness, generalization, and interoperability of generalist policy but also enables test-time scaling to enhance overall performance.

**Link**: [arxiv](http://arxiv.org/abs/2503.24388v1),  [pdf](http://arxiv.org/pdf/2503.24388v1)

**Tags**: cs.AI cs.CL cs.CV cs.LG 



### Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for   Large Language Models
**Authors**: Rui Wang, Hongru Wang, Boyang Xue, Jianhui Pang, Shudong Liu, Yi Chen, Jiahao Qiu, Derek Fai Wong, Heng Ji, Kam-Fai Wong

**Updated**: 2025-03-31T17:58:07Z

**Summary**: Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to perform complex reasoning tasks, transitioning from fast and intuitive thinking (System 1) to slow and deep reasoning (System 2). While System 2 reasoning improves task accuracy, it often incurs substantial computational costs due to its slow thinking nature and inefficient or unnecessary reasoning behaviors. In contrast, System 1 reasoning is computationally efficient but leads to suboptimal performance. Consequently, it is critical to balance the trade-off between performance (benefits) and computational costs (budgets), giving rise to the concept of reasoning economy. In this survey, we provide a comprehensive analysis of reasoning economy in both the post-training and test-time inference stages of LLMs, encompassing i) the cause of reasoning inefficiency, ii) behavior analysis of different reasoning patterns, and iii) potential solutions to achieve reasoning economy. By offering actionable insights and highlighting open challenges, we aim to shed light on strategies for improving the reasoning economy of LLMs, thereby serving as a valuable resource for advancing research in this evolving area. We also provide a public repository to continually track developments in this fast-evolving field.

**Link**: [arxiv](http://arxiv.org/abs/2503.24377v1),  [pdf](http://arxiv.org/pdf/2503.24377v1)

**Tags**: cs.CL cs.AI 



### EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues
**Authors**: Yuhan Liu, Yunbo Long

**Updated**: 2025-03-31T17:55:35Z

**Summary**: While large language model (LLM)-based chatbots have been applied for effective engagement in credit dialogues, their capacity for dynamic emotional expression remains limited. Current agents primarily rely on passive empathy rather than affective reasoning. For instance, when faced with persistent client negativity, the agent should employ strategic emotional adaptation by expressing measured anger to discourage counterproductive behavior and guide the conversation toward resolution. This context-aware emotional modulation is essential for imitating the nuanced decision-making of human negotiators. This paper introduces an EQ-negotiator that combines emotion sensing from pre-trained language models (PLMs) with emotional reasoning based on Game Theory and Hidden Markov Models. It takes into account both the current and historical emotions of the client to better manage and address negative emotions during interactions. By fine-tuning pre-trained language models (PLMs) on public emotion datasets and validating them on the credit dialogue datasets, our approach enables LLM-based agents to effectively capture shifts in client emotions and dynamically adjust their response tone based on our emotion decision policies in real-world financial negotiations. This EQ-negotiator can also help credit agencies foster positive client relationships, enhancing satisfaction in credit services.

**Link**: [arxiv](http://arxiv.org/abs/2503.21080v3),  [pdf](http://arxiv.org/pdf/2503.21080v3)

**Tags**: cs.CL 



### Exploring the Effect of Reinforcement Learning on Video Understanding:   Insights from SEED-Bench-R1
**Authors**: Yi Chen, Yuying Ge, Rui Wang, Yixiao Ge, Lu Qiu, Ying Shan, Xihui Liu

**Updated**: 2025-03-31T17:55:23Z

**Summary**: Recent advancements in Chain of Thought (COT) generation have significantly improved the reasoning capabilities of Large Language Models (LLMs), with reinforcement learning (RL) emerging as an effective post-training approach. Multimodal Large Language Models (MLLMs) inherit this reasoning potential but remain underexplored in tasks requiring both perception and logical reasoning. To address this, we introduce SEED-Bench-R1, a benchmark designed to systematically evaluate post-training methods for MLLMs in video understanding. It includes intricate real-world videos and complex everyday planning tasks in the format of multiple-choice questions, requiring sophisticated perception and reasoning. SEED-Bench-R1 assesses generalization through a three-level hierarchy: in-distribution, cross-environment, and cross-environment-task scenarios, equipped with a large-scale training dataset with easily verifiable ground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL with supervised fine-tuning (SFT), demonstrating RL's data efficiency and superior performance on both in-distribution and out-of-distribution tasks, even outperforming SFT on general video understanding benchmarks like LongVideoBench. Our detailed analysis reveals that RL enhances visual perception but often produces less logically coherent reasoning chains. We identify key limitations such as inconsistent reasoning and overlooked visual cues, and suggest future improvements in base model reasoning, reward modeling, and RL robustness against noisy signals.

**Link**: [arxiv](http://arxiv.org/abs/2503.24376v1),  [pdf](http://arxiv.org/pdf/2503.24376v1)

**Tags**: cs.CV cs.AI cs.CL cs.LG 



### ERUPT: Efficient Rendering with Unposed Patch Transformer
**Authors**: Maxim V. Shugaev, Vincent Chen, Maxim Karrenbach, Kyle Ashley, Bridget Kennedy, Naresh P. Cuntoor

**Updated**: 2025-03-31T17:53:05Z

**Summary**: This work addresses the problem of novel view synthesis in diverse scenes from small collections of RGB images. We propose ERUPT (Efficient Rendering with Unposed Patch Transformer) a state-of-the-art scene reconstruction model capable of efficient scene rendering using unposed imagery. We introduce patch-based querying, in contrast to existing pixel-based queries, to reduce the compute required to render a target view. This makes our model highly efficient both during training and at inference, capable of rendering at 600 fps on commercial hardware. Notably, our model is designed to use a learned latent camera pose which allows for training using unposed targets in datasets with sparse or inaccurate ground truth camera pose. We show that our approach can generalize on large real-world data and introduce a new benchmark dataset (MSVS-1M) for latent view synthesis using street-view imagery collected from Mapillary. In contrast to NeRF and Gaussian Splatting, which require dense imagery and precise metadata, ERUPT can render novel views of arbitrary scenes with as few as five unposed input images. ERUPT achieves better rendered image quality than current state-of-the-art methods for unposed image synthesis tasks, reduces labeled data requirements by ~95\% and decreases computational requirements by an order of magnitude, providing efficient novel view synthesis for diverse real-world scenes.

**Link**: [arxiv](http://arxiv.org/abs/2503.24374v1),  [pdf](http://arxiv.org/pdf/2503.24374v1)

**Tags**: cs.CV 



### Effectively Controlling Reasoning Models through Thinking Intervention
**Authors**: Tong Wu, Chong Xiang, Jiachen T. Wang, Prateek Mittal

**Updated**: 2025-03-31T17:50:13Z

**Summary**: Reasoning-enhanced large language models (LLMs) explicitly generate intermediate reasoning steps prior to generating final answers, helping the model excel in complex problem-solving. In this paper, we demonstrate that this emerging generation framework offers a unique opportunity for more fine-grained control over model behavior. We propose Thinking Intervention, a novel paradigm designed to explicitly guide the internal reasoning processes of LLMs by strategically inserting or revising specific thinking tokens. We conduct comprehensive evaluations across multiple tasks, including instruction following on IFEval, instruction hierarchy on SEP, and safety alignment on XSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention significantly outperforms baseline prompting approaches, achieving up to 6.7% accuracy gains in instruction-following scenarios, 15.4% improvements in reasoning about instruction hierarchies, and a 40.0% increase in refusal rates for unsafe prompts using open-source DeepSeek R1 models. Overall, our work opens a promising new research avenue for controlling reasoning LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2503.24370v1),  [pdf](http://arxiv.org/pdf/2503.24370v1)

**Tags**: cs.LG cs.AI cs.CL 



### Adapting Vision Foundation Models for Real-time Ultrasound Image   Segmentation
**Authors**: Xiaoran Zhang, Eric Z. Chen, Lin Zhao, Xiao Chen, Yikang Liu, Boris Maihe, James S. Duncan, Terrence Chen, Shanhui Sun

**Updated**: 2025-03-31T17:47:42Z

**Summary**: We propose a novel approach that adapts hierarchical vision foundation models for real-time ultrasound image segmentation. Existing ultrasound segmentation methods often struggle with adaptability to new tasks, relying on costly manual annotations, while real-time approaches generally fail to match state-of-the-art performance. To overcome these limitations, we introduce an adaptive framework that leverages the vision foundation model Hiera to extract multi-scale features, interleaved with DINOv2 representations to enhance visual expressiveness. These enriched features are then decoded to produce precise and robust segmentation. We conduct extensive evaluations on six public datasets and one in-house dataset, covering both cardiac and thyroid ultrasound segmentation. Experiments show that our approach outperforms state-of-the-art methods across multiple datasets and excels with limited supervision, surpassing nnUNet by over 20\% on average in the 1\% and 10\% data settings. Our method achieves $\sim$77 FPS inference speed with TensorRT on a single GPU, enabling real-time clinical applications.

**Link**: [arxiv](http://arxiv.org/abs/2503.24368v1),  [pdf](http://arxiv.org/pdf/2503.24368v1)

**Tags**: cs.CV 



### The structure and topology of an amorphous metal-organic framework
**Authors**: Thomas C. Nicholas, Daniel F. Thomas du Toit, Louise A. M. Rosset, Davide M. Proserpio, Andrew L. Goodwin, Volker L. Deringer

**Updated**: 2025-03-31T17:46:37Z

**Summary**: Amorphous metal-organic frameworks are an important emerging materials class that combine the attractive physical properties of the amorphous state with the versatility of metal-organic framework (MOF) chemistry. The structures of amorphous MOFs have largely been inferred by drawing analogies to crystalline polymorphs and inorganic glasses, but ultimately the validity of such structural models has been challenging to establish either experimentally or computationally. Here we use a unified data-driven approach, combining experimental scattering data and active machine learning for interatomic potentials, to determine the structure of an amorphous zeolitic imidazolate framework (a-ZIF) -- the canonical amorphous MOF. Our results reveal clear differences between the structure of a-ZIF and that of other amorphous tetrahedral networks, allowing us to invalidate the long-standing assumption that these inorganic and hybrid glasses are topologically equivalent. To this end, we introduce a systematic notation for the network topology of amorphous solids, building a bridge to the successful use of topology analysis in crystalline MOFs and to materials informatics. Our work provides insights into the structure and topology of the archetypal amorphous MOF and opens up new avenues for modelling and understanding amorphous framework materials more generally.

**Link**: [arxiv](http://arxiv.org/abs/2503.24367v1),  [pdf](http://arxiv.org/pdf/2503.24367v1)

**Tags**: cond-mat.mtrl-sci physics.chem-ph 



### Query and Conquer: Execution-Guided SQL Generation
**Authors**: Åukasz Borchmann, Marek Wydmuch

**Updated**: 2025-03-31T17:43:36Z

**Summary**: We propose a novel approach for generating complex outputs that significantly improves accuracy in text-to-SQL tasks. Our method leverages execution results to select the most semantically consistent query from multiple candidates, enabling smaller, cost-effective models to surpass computationally intensive reasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference cost by as much as 30 times. It integrates effortlessly with existing models, offering a practical and scalable pathway to state-of-the-art SQL generation.

**Link**: [arxiv](http://arxiv.org/abs/2503.24364v1),  [pdf](http://arxiv.org/pdf/2503.24364v1)

**Tags**: cs.CL 



### SQuat: Subspace-orthogonal KV Cache Quantization
**Authors**: Hao Wang, Ligong Han, Kai Xu, Akash Srivastava

**Updated**: 2025-03-31T17:37:32Z

**Summary**: The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from previously generated tokens. It reduces redundant computation at the cost of increased memory usage. To mitigate this overhead, existing approaches compress KV tensors into lower-bit representations; however, quantization errors can accumulate as more tokens are generated, potentially resulting in undesired outputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache quantization). It first constructs a subspace spanned by query tensors to capture the most critical task-related information. During key tensor quantization, it enforces that the difference between the (de)quantized and original keys remains orthogonal to this subspace, minimizing the impact of quantization errors on the attention mechanism's outputs. SQuat requires no model fine-tuning, no additional calibration dataset for offline learning, and is grounded in a theoretical framework we develop. Through numerical experiments, we show that our method reduces peak memory by 2.17 to 2.82, improves throughput by 2.45 to 3.60, and achieves more favorable benchmark scores than existing KV cache quantization algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2503.24358v1),  [pdf](http://arxiv.org/pdf/2503.24358v1)

**Tags**: cs.LG cs.AI cs.CL cs.IT math.IT 



### ORAL: Prompting Your Large-Scale LoRAs via Conditional Recurrent   Diffusion
**Authors**: Rana Muhammad Shahroz Khan, Dongwen Tang, Pingzhi Li, Kai Wang, Tianlong Chen

**Updated**: 2025-03-31T17:34:59Z

**Summary**: Parameter generation has emerged as a novel paradigm for neural network development, offering an alternative to traditional neural network training by synthesizing high-quality model weights directly. In the context of Low-Rank Adaptation (LoRA) for evolving ($\textit{i.e.}$, constantly updated) large language models (LLMs), this approach promises efficient adaptation without costly retraining. However, existing methods face critical limitations in simultaneously achieving scalability and controllability. In this paper, we introduce $\texttt{ORAL}$, a novel $\textbf{conditional recurrent diffusion}$ framework that addresses these challenges. $\texttt{ORAL}$ incorporates a novel conditioning mechanism that integrates model architecture and textual task specifications, enabling the generation of task-specific LoRA parameters that can seamlessly transfer across evolving foundation models. Our approach successfully scales to billions-of-parameter LLMs and maintains controllability. Through extensive experiments across seven language tasks, four vision tasks, and three multimodal tasks using five pre-trained LLMs, we demonstrate that $\texttt{ORAL}$ generates high-quality LoRA parameters that achieve comparable or superior performance to vanilla trained counterparts.

**Link**: [arxiv](http://arxiv.org/abs/2503.24354v1),  [pdf](http://arxiv.org/pdf/2503.24354v1)

**Tags**: cs.LG cs.AI cs.CL cs.CV 



### NoProp: Training Neural Networks without Back-propagation or   Forward-propagation
**Authors**: Qinyu Li, Yee Whye Teh, Razvan Pascanu

**Updated**: 2025-03-31T17:08:57Z

**Summary**: The canonical deep learning approach for learning requires computing a gradient term at each layer by back-propagating the error signal from the output towards each learnable parameter. Given the stacked structure of neural networks, where each layer builds on the representation of the layer below, this approach leads to hierarchical representations. More abstract features live on the top layers of the model, while features on lower layers are expected to be less abstract. In contrast to this, we introduce a new learning method named NoProp, which does not rely on either forward or backwards propagation. Instead, NoProp takes inspiration from diffusion and flow matching methods, where each layer independently learns to denoise a noisy target. We believe this work takes a first step towards introducing a new family of gradient-free learning methods, that does not learn hierarchical representations -- at least not in the usual sense. NoProp needs to fix the representation at each layer beforehand to a noised version of the target, learning a local denoising process that can then be exploited at inference. We demonstrate the effectiveness of our method on MNIST, CIFAR-10, and CIFAR-100 image classification benchmarks. Our results show that NoProp is a viable learning algorithm which achieves superior accuracy, is easier to use and computationally more efficient compared to other existing back-propagation-free methods. By departing from the traditional gradient based learning paradigm, NoProp alters how credit assignment is done within the network, enabling more efficient distributed learning as well as potentially impacting other characteristics of the learning process.

**Link**: [arxiv](http://arxiv.org/abs/2503.24322v1),  [pdf](http://arxiv.org/pdf/2503.24322v1)

**Tags**: cs.LG stat.ML 



### Can Test-Time Scaling Improve World Foundation Model?
**Authors**: Wenyan Cong, Hanqing Zhu, Peihao Wang, Bangya Liu, Dejia Xu, Kevin Wang, David Z. Pan, Yan Wang, Zhiwen Fan, Zhangyang Wang

**Updated**: 2025-03-31T17:07:37Z

**Summary**: World foundation models, which simulate the physical world by predicting future states from current observations and inputs, have become central to many applications in physical intelligence, including autonomous driving and robotics. However, these models require substantial computational resources for pretraining and are further constrained by available data during post-training. As such, scaling computation at test time emerges as both a critical and practical alternative to traditional model enlargement or re-training. In this work, we introduce SWIFT, a test-time scaling framework tailored for WFMs. SWIFT integrates our extensible WFM evaluation toolkit with process-level inference strategies, including fast tokenization, probability-based Top-K pruning, and efficient beam search. Empirical results on the COSMOS model demonstrate that test-time scaling exists even in a compute-optimal way. Our findings reveal that test-time scaling laws hold for WFMs and that SWIFT provides a scalable and effective pathway for improving WFM inference without retraining or increasing model size. The code is available at https://github.com/Mia-Cong/SWIFT.git.

**Link**: [arxiv](http://arxiv.org/abs/2503.24320v1),  [pdf](http://arxiv.org/pdf/2503.24320v1)

**Tags**: cs.CV 



### Selective Inference in Graphical Models via Maximum Likelihood
**Authors**: Sofia Guglielmini, Gerda Claeskens, Snigdha Panigrahi

**Updated**: 2025-03-31T16:57:04Z

**Summary**: The graphical lasso is a widely used algorithm for fitting undirected Gaussian graphical models. However, for inference on functionals of edge values in the learned graph, standard tools lack formal statistical guarantees, such as control of the type I error rate. In this paper, we introduce a selective inference method for asymptotically valid inference after graphical lasso selection with added randomization. We obtain a selective likelihood, conditional on the event of selection, through a change of variable on the known density of the randomization variables. Our method enables interval estimation and hypothesis testing for a wide range of functionals of edge values in the learned graph using the conditional maximum likelihood estimate. Our numerical studies show that introducing a small amount of randomization: (i) greatly increases power and yields substantially shorter intervals compared to other conditional inference methods, including data splitting; (ii) ensures intervals of bounded length in high-dimensional settings where data splitting is infeasible due to insufficient samples for inference; (iii) enables inference for a wide range of inferential targets in the learned graph, including measures of node influence and connectivity between nodes.

**Link**: [arxiv](http://arxiv.org/abs/2503.24311v1),  [pdf](http://arxiv.org/pdf/2503.24311v1)

**Tags**: stat.ME stat.AP stat.CO 



### BEATS: Bias Evaluation and Assessment Test Suite for Large Language   Models
**Authors**: Alok Abhishek, Lisa Erickson, Tushar Bandopadhyay

**Updated**: 2025-03-31T16:56:52Z

**Summary**: In this research, we introduce BEATS, a novel framework for evaluating Bias, Ethics, Fairness, and Factuality in Large Language Models (LLMs). Building upon the BEATS framework, we present a bias benchmark for LLMs that measure performance across 29 distinct metrics. These metrics span a broad range of characteristics, including demographic, cognitive, and social biases, as well as measures of ethical reasoning, group fairness, and factuality related misinformation risk. These metrics enable a quantitative assessment of the extent to which LLM generated responses may perpetuate societal prejudices that reinforce or expand systemic inequities. To achieve a high score on this benchmark a LLM must show very equitable behavior in their responses, making it a rigorous standard for responsible AI evaluation. Empirical results based on data from our experiment show that, 37.65\% of outputs generated by industry leading models contained some form of bias, highlighting a substantial risk of using these models in critical decision making systems. BEATS framework and benchmark offer a scalable and statistically rigorous methodology to benchmark LLMs, diagnose factors driving biases, and develop mitigation strategies. With the BEATS framework, our goal is to help the development of more socially responsible and ethically aligned AI models.

**Link**: [arxiv](http://arxiv.org/abs/2503.24310v1),  [pdf](http://arxiv.org/pdf/2503.24310v1)

**Tags**: cs.CL cs.AI 68T01 (Primary), 68T50 (Secondary) I.2.0; I.2.7 



### A Systematic Evaluation of LLM Strategies for Mental Health Text   Analysis: Fine-tuning vs. Prompt Engineering vs. RAG
**Authors**: Arshia Kermani, Veronica Perez-Rosas, Vangelis Metsis

**Updated**: 2025-03-31T16:54:04Z

**Summary**: This study presents a systematic comparison of three approaches for the analysis of mental health text using large language models (LLMs): prompt engineering, retrieval augmented generation (RAG), and fine-tuning. Using LLaMA 3, we evaluate these approaches on emotion classification and mental health condition detection tasks across two datasets. Fine-tuning achieves the highest accuracy (91% for emotion classification, 80% for mental health conditions) but requires substantial computational resources and large training sets, while prompt engineering and RAG offer more flexible deployment with moderate performance (40-68% accuracy). Our findings provide practical insights for implementing LLM-based solutions in mental health applications, highlighting the trade-offs between accuracy, computational requirements, and deployment flexibility.

**Link**: [arxiv](http://arxiv.org/abs/2503.24307v1),  [pdf](http://arxiv.org/pdf/2503.24307v1)

**Tags**: cs.CL cs.AI cs.IR cs.LG 



### Point Tracking in Surgery--The 2024 Surgical Tattoos in Infrared (STIR)   Challenge
**Authors**: Adam Schmidt, Mert Asim Karaoglu, Soham Sinha, Mingang Jang, Ho-Gun Ha, Kyungmin Jung, Kyeongmo Gu, Ihsan Ullah, Hyunki Lee, JonÃ¡Å¡ Å erÃ½ch, Michal Neoral, JiÅÃ­ Matas, Rulin Zhou, Wenlong He, An Wang, Hongliang Ren, Bruno Silva, Sandro QueirÃ³s, EstÃªvÃ£o Lima, JoÃ£o L. VilaÃ§a, Shunsuke Kikuchi, Atsushi Kouno, Hiroki Matsuzaki, Tongtong Li, Yulu Chen, Ling Li, Xiang Ma, Xiaojian Li, Mona Sheikh Zeinoddin, Xu Wang, Zafer Tandogdu, Greg Shaw, Evangelos Mazomenos, Danail Stoyanov, Yuxin Chen, Zijian Wu, Alexander Ladikos, Simon DiMaio, Septimiu E. Salcudean, Omid Mohareri

**Updated**: 2025-03-31T16:53:09Z

**Summary**: Understanding tissue motion in surgery is crucial to enable applications in downstream tasks such as segmentation, 3D reconstruction, virtual tissue landmarking, autonomous probe-based scanning, and subtask autonomy. Labeled data are essential to enabling algorithms in these downstream tasks since they allow us to quantify and train algorithms. This paper introduces a point tracking challenge to address this, wherein participants can submit their algorithms for quantification. The submitted algorithms are evaluated using a dataset named surgical tattoos in infrared (STIR), with the challenge aptly named the STIR Challenge 2024. The STIR Challenge 2024 comprises two quantitative components: accuracy and efficiency. The accuracy component tests the accuracy of algorithms on in vivo and ex vivo sequences. The efficiency component tests the latency of algorithm inference. The challenge was conducted as a part of MICCAI EndoVis 2024. In this challenge, we had 8 total teams, with 4 teams submitting before and 4 submitting after challenge day. This paper details the STIR Challenge 2024, which serves to move the field towards more accurate and efficient algorithms for spatial understanding in surgery. In this paper we summarize the design, submissions, and results from the challenge. The challenge dataset is available here: https://zenodo.org/records/14803158 , and the code for baseline models and metric calculation is available here: https://github.com/athaddius/STIRMetrics

**Link**: [arxiv](http://arxiv.org/abs/2503.24306v1),  [pdf](http://arxiv.org/pdf/2503.24306v1)

**Tags**: cs.CV 



### Is analogy enough to draw novel adjective-noun inferences?
**Authors**: Hayley Ross, Kathryn Davidson, Najoung Kim

**Updated**: 2025-03-31T16:41:16Z

**Summary**: Recent work (Ross et al., 2025, 2024) has argued that the ability of humans and LLMs respectively to generalize to novel adjective-noun combinations shows that they each have access to a compositional mechanism to determine the phrase's meaning and derive inferences. We study whether these inferences can instead be derived by analogy to known inferences, without need for composition. We investigate this by (1) building a model of analogical reasoning using similarity over lexical items, and (2) asking human participants to reason by analogy. While we find that this strategy works well for a large proportion of the dataset of Ross et al. (2025), there are novel combinations for which both humans and LLMs derive convergent inferences but which are not well handled by analogy. We thus conclude that the mechanism humans and LLMs use to generalize in these cases cannot be fully reduced to analogy, and likely involves composition.

**Link**: [arxiv](http://arxiv.org/abs/2503.24293v1),  [pdf](http://arxiv.org/pdf/2503.24293v1)

**Tags**: cs.CL 



### Rec-R1: Bridging Generative Large Language Models and User-Centric   Recommendation Systems via Reinforcement Learning
**Authors**: Jiacheng Lin, Tian Wang, Kun Qian

**Updated**: 2025-03-31T16:36:00Z

**Summary**: We propose Rec-R1, a general reinforcement learning framework that bridges large language models (LLMs) with recommendation systems through closed-loop optimization. Unlike prompting and supervised fine-tuning (SFT), Rec-R1 directly optimizes LLM generation using feedback from a fixed black-box recommendation model, without relying on synthetic SFT data from proprietary models such as GPT-4o. This avoids the substantial cost and effort required for data distillation. To verify the effectiveness of Rec-R1, we evaluate it on two representative tasks: product search and sequential recommendation. Experimental results demonstrate that Rec-R1 not only consistently outperforms prompting- and SFT-based methods, but also achieves significant gains over strong discriminative baselines, even when used with simple retrievers such as BM25. Moreover, Rec-R1 preserves the general-purpose capabilities of the LLM, unlike SFT, which often impairs instruction-following and reasoning. These findings suggest Rec-R1 as a promising foundation for continual task-specific adaptation without catastrophic forgetting.

**Link**: [arxiv](http://arxiv.org/abs/2503.24289v1),  [pdf](http://arxiv.org/pdf/2503.24289v1)

**Tags**: cs.IR cs.CL 



### How Does A Text Preprocessing Pipeline Affect Ontology Syntactic   Matching?
**Authors**: Zhangcheng Qiang, Kerry Taylor, Weiqing Wang

**Updated**: 2025-03-31T16:35:00Z

**Summary**: The classic text preprocessing pipeline, comprising Tokenisation, Normalisation, Stop Words Removal, and Stemming/Lemmatisation, has been implemented in many systems for syntactic ontology matching (OM). However, the lack of standardisation in text preprocessing creates diversity in mapping results. In this paper we investigate the effect of the text preprocessing pipeline on syntactic OM in 8 Ontology Alignment Evaluation Initiative (OAEI) tracks with 49 distinct alignments. We find that Phase 1 text preprocessing (Tokenisation and Normalisation) is more effective than Phase 2 text preprocessing (Stop Words Removal and Stemming/Lemmatisation). To repair the unwanted false mappings caused by Phase 2 text preprocessing, we propose a novel context-based pipeline repair approach that employs a post hoc check to find common words that cause false mappings. These words are stored in a reserved word set and applied in text preprocessing. The experimental results show that our approach improves the matching correctness and the overall matching performance. We then consider the broader integration of the classic text preprocessing pipeline with modern large language models (LLMs) for OM. We recommend that (1) the text preprocessing pipeline be injected via function calling into LLMs to avoid the tendency towards unstable true mappings produced by LLM prompting; or (2) LLMs be used to repair non-existent and counter-intuitive false mappings generated by the text preprocessing pipeline.

**Link**: [arxiv](http://arxiv.org/abs/2411.03962v5),  [pdf](http://arxiv.org/pdf/2411.03962v5)

**Tags**: cs.CL 



### PharmAgents: Building a Virtual Pharma with Large Language Model Agents
**Authors**: Bowen Gao, Yanwen Huang, Yiqiao Liu, Wenxuan Xie, Wei-Ying Ma, Ya-Qin Zhang, Yanyan Lan

**Updated**: 2025-03-31T16:26:42Z

**Summary**: The discovery of novel small molecule drugs remains a critical scientific challenge with far-reaching implications for treating diseases and advancing human health. Traditional drug development--especially for small molecule therapeutics--is a highly complex, resource-intensive, and time-consuming process that requires multidisciplinary collaboration. Recent breakthroughs in artificial intelligence (AI), particularly the rise of large language models (LLMs), present a transformative opportunity to streamline and accelerate this process. In this paper, we introduce PharmAgents, a virtual pharmaceutical ecosystem driven by LLM-based multi-agent collaboration. PharmAgents simulates the full drug discovery workflow--from target discovery to preclinical evaluation--by integrating explainable, LLM-driven agents equipped with specialized machine learning models and computational tools. Through structured knowledge exchange and automated optimization, PharmAgents identifies potential therapeutic targets, discovers promising lead compounds, enhances binding affinity and key molecular properties, and performs in silico analyses of toxicity and synthetic feasibility. Additionally, the system supports interpretability, agent interaction, and self-evolvement, enabling it to refine future drug designs based on prior experience. By showcasing the potential of LLM-powered multi-agent systems in drug discovery, this work establishes a new paradigm for autonomous, explainable, and scalable pharmaceutical research, with future extensions toward comprehensive drug lifecycle management.

**Link**: [arxiv](http://arxiv.org/abs/2503.22164v2),  [pdf](http://arxiv.org/pdf/2503.22164v2)

**Tags**: q-bio.BM cs.AI 



### BOWIE-ALIGN: Sub-stellar metallicity and carbon depletion in the aligned   TrES-4b with JWST NIRSpec transmission spectroscopy
**Authors**: Annabella Meech, Alastair B. Claringbold, Eva-Maria Ahrer, James Kirk, Mercedes LÃ³pez-Morales, Jake Taylor, Richard A. Booth, Anna B. T. Penzlin, Lili Alderson, Duncan A. Christie, Emma Esparza-Borges, Charlotte Fairman, Nathan J. Mayne, Mason McCormack, James E. Owen, Vatsal Panwar, Diana Powell, Denis E. Sergeev, Daniel Valentine, Hannah R. Wakeford, Peter J. Wheatley, Maria Zamyatina

**Updated**: 2025-03-31T16:25:50Z

**Summary**: The formation and migration history of a planet is expected to be imprinted in its atmosphere, in particular its carbon-to-oxygen (C/O) ratio and metallicity. The BOWIE-ALIGN programme is performing a comparative study of JWST spectra of four aligned and four misaligned hot Jupiters, with the aim of characterising their atmospheres and corroborating the link between the observables and the formation history. In this work, we present the $2.8-5.2$ micron transmission spectrum of TrES-4b, a hot Jupiter with an orbit aligned with the rotation axis of its F-type host star. Using free chemistry atmospheric retrievals, we report a confident detection of H$_2$O at an abundance of $\log X_\mathrm{H_2O}=-2.98^{+0.68}_{-0.73}$ at a significance of $8.4\sigma$. We also find evidence for CO and small amounts of CO$_2$, retrieving abundances $\log X_\mathrm{CO}= -3.76^{+0.89}_{-1.01}$ and $\log X_\mathrm{CO_2}= -6.86^{+0.62}_{-0.65}$ ($3.1\sigma$ and $4.0\sigma$ respectively). The observations are consistent with the the atmosphere being in chemical equilibrium; our retrievals yield $\mathrm{C/O}$ between $0.30-0.42$ and constrain the atmospheric metallicity to the range $0.4-0.7\times$ solar. The inferred sub-stellar properties (C/O and metallicity) challenge traditional models, and could have arisen from an oxygen-rich gas accretion scenario, or a combination of low-metallicity gas and carbon-poor solid accretion.

**Link**: [arxiv](http://arxiv.org/abs/2503.24280v1),  [pdf](http://arxiv.org/pdf/2503.24280v1)

**Tags**: astro-ph.EP 



### Evaluating and Designing Sparse Autoencoders by Approximating   Quasi-Orthogonality
**Authors**: Sewoong Lee, Adam Davies, Marc E. Canby, Julia Hockenmaier

**Updated**: 2025-03-31T16:22:11Z

**Summary**: Sparse autoencoders (SAEs) have emerged as a workhorse of modern mechanistic interpretability, but leading SAE approaches with top-$k$ style activation functions lack theoretical grounding for selecting the hyperparameter $k$. SAEs are based on the linear representation hypothesis (LRH), which assumes that the representations of large language models (LLMs) are linearly encoded, and the superposition hypothesis (SH), which states that there can be more features in the model than its dimensionality. We show that, based on the formal definitions of the LRH and SH, the magnitude of sparse feature vectors (the latent representations learned by SAEs of the dense embeddings of LLMs) can be approximated using their corresponding dense vector with a closed-form error bound. To visualize this, we propose the ZF plot, which reveals a previously unknown relationship between LLM hidden embeddings and SAE feature vectors, allowing us to make the first empirical measurement of the extent to which feature vectors of pre-trained SAEs are over- or under-activated for a given input. Correspondingly, we introduce Approximate Feature Activation (AFA), which approximates the magnitude of the ground-truth sparse feature vector, and propose a new evaluation metric derived from AFA to assess the alignment between inputs and activations. We also leverage AFA to introduce a novel SAE architecture, the top-AFA SAE, leading to SAEs that: (a) are more in line with theoretical justifications; and (b) obviate the need to tune SAE sparsity hyperparameters. Finally, we empirically demonstrate that top-AFA SAEs achieve reconstruction loss comparable to that of state-of-the-art top-k SAEs, without requiring the hyperparameter $k$ to be tuned. Our code is available at: https://github.com/SewoongLee/top-afa-sae.

**Link**: [arxiv](http://arxiv.org/abs/2503.24277v1),  [pdf](http://arxiv.org/pdf/2503.24277v1)

**Tags**: cs.LG cs.AI 



### Visual Acoustic Fields
**Authors**: Yuelei Li, Hyunjin Kim, Fangneng Zhan, Ri-Zhao Qiu, Mazeyu Ji, Xiaojun Shan, Xueyan Zou, Paul Liang, Hanspeter Pfister, Xiaolong Wang

**Updated**: 2025-04-01T03:16:38Z

**Summary**: Objects produce different sounds when hit, and humans can intuitively infer how an object might sound based on its appearance and material properties. Inspired by this intuition, we propose Visual Acoustic Fields, a framework that bridges hitting sounds and visual signals within a 3D space using 3D Gaussian Splatting (3DGS). Our approach features two key modules: sound generation and sound localization. The sound generation module leverages a conditional diffusion model, which takes multiscale features rendered from a feature-augmented 3DGS to generate realistic hitting sounds. Meanwhile, the sound localization module enables querying the 3D scene, represented by the feature-augmented 3DGS, to localize hitting positions based on the sound sources. To support this framework, we introduce a novel pipeline for collecting scene-level visual-sound sample pairs, achieving alignment between captured images, impact locations, and corresponding sounds. To the best of our knowledge, this is the first dataset to connect visual and acoustic signals in a 3D context. Extensive experiments on our dataset demonstrate the effectiveness of Visual Acoustic Fields in generating plausible impact sounds and accurately localizing impact sources. Our project page is at https://yuelei0428.github.io/projects/Visual-Acoustic-Fields/.

**Link**: [arxiv](http://arxiv.org/abs/2503.24270v2),  [pdf](http://arxiv.org/pdf/2503.24270v2)

**Tags**: cs.CV cs.AI 



### EP240414a: Off-axis View of a Jet-Cocoon System from an Expanded   Progenitor Star
**Authors**: Jian-He Zheng, Jin-Ping Zhu, Wenbin Lu, Bing Zhang

**Updated**: 2025-03-31T16:12:46Z

**Summary**: When a relativistic jet is launched following the core-collapse of a star, its interaction with the stellar envelope leads to the formation of a hot cocoon, which produces various viewing-angle-dependent observational phenomena following the breakout from the surface. We study the observational signatures of fast X-ray transient (FXT) EP240414a, which may originate from a jet-cocoon system viewed slightly off-axis. In our model, (1) the prompt X-ray emission lasting $\sim\! 100\,{\rm{s}}$ is attributed to the cooling emission from the inner cocoon (shocked jet material); (2) the $\sim\! 0.1\,{\rm{d}}$ X-ray emission comes from the inner cocoon's afterglow; (3) the $\sim\! 0.4\,{\rm{d}}$ thermal-dominated optical emission arises from the cooling of the outer cocoon (shocked stellar material); (4) the $\sim\! 3\,{\rm{d}}$ non-thermal optical component and subsequent radio emission can be explained by the afterglow from a jet with a viewing angle of $10^{\circ}\lesssim \theta_{\rm{v}}\lesssim15^\circ$; and (5) the associated broad-lined Type Ic supernova only dominates the optical emission after $\sim\! 7\rm\, d$. Both the jet inferred from the off-axis afterglow and the inner cocoon constrained by the cooling emission are found to have similar kinetic energies, on the order of $10^{51}\,{\rm{erg}}$. We find that the progenitor's radius is $\sim3\,R_\odot$ as constrained by the { inner cocoon's} cooling emissions, indicating that the pre-explosion star may be a massive helium star that is slightly inflated. More FXTs associated with off-axis jets and supernovae will be further examined by the Einstein Probe, leading to a deeper understanding of jet-cocoon systems.

**Link**: [arxiv](http://arxiv.org/abs/2503.24266v1),  [pdf](http://arxiv.org/pdf/2503.24266v1)

**Tags**: astro-ph.HE 



### CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards   CWE Detection
**Authors**: Richard A. Dubniczky, Krisztofer ZoltÃ¡n HorvÃ¡t, TamÃ¡s Bisztray, Mohamed Amine Ferrag, Lucas C. Cordeiro, Norbert Tihanyi

**Updated**: 2025-03-31T16:07:10Z

**Summary**: Identifying vulnerabilities in source code is crucial, especially in critical software components. Existing methods such as static analysis, dynamic analysis, formal verification, and recently Large Language Models are widely used to detect security flaws. This paper introduces CASTLE (CWE Automated Security Testing and Low-Level Evaluation), a benchmarking framework for evaluating the vulnerability detection capabilities of different methods. We assess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using a hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs. We propose the CASTLE Score, a novel evaluation metric to ensure fair comparison. Our results reveal key differences: ESBMC (a formal verification tool) minimizes false positives but struggles with vulnerabilities beyond model checking, such as weak cryptography or SQL injection. Static analyzers suffer from high false positives, increasing manual validation efforts for developers. LLMs perform exceptionally well in the CASTLE dataset when identifying vulnerabilities in small code snippets. However, their accuracy declines, and hallucinations increase as the code size grows. These results suggest that LLMs could play a pivotal role in future security solutions, particularly within code completion frameworks, where they can provide real-time guidance to prevent vulnerabilities. The dataset is accessible at https://github.com/CASTLE-Benchmark.

**Link**: [arxiv](http://arxiv.org/abs/2503.09433v2),  [pdf](http://arxiv.org/pdf/2503.09433v2)

**Tags**: cs.CR cs.AI cs.SE 



### Inductive Moment Matching
**Authors**: Linqi Zhou, Stefano Ermon, Jiaming Song

**Updated**: 2025-03-31T16:02:38Z

**Summary**: Diffusion models and Flow Matching generate high-quality samples but are slow at inference, and distilling them into few-step models often leads to instability and extensive tuning. To resolve these trade-offs, we propose Inductive Moment Matching (IMM), a new class of generative models for one- or few-step sampling with a single-stage training procedure. Unlike distillation, IMM does not require pre-training initialization and optimization of two networks; and unlike Consistency Models, IMM guarantees distribution-level convergence and remains stable under various hyperparameters and standard model architectures. IMM surpasses diffusion models on ImageNet-256x256 with 1.99 FID using only 8 inference steps and achieves state-of-the-art 2-step FID of 1.98 on CIFAR-10 for a model trained from scratch.

**Link**: [arxiv](http://arxiv.org/abs/2503.07565v6),  [pdf](http://arxiv.org/pdf/2503.07565v6)

**Tags**: cs.LG cs.AI stat.ML 



### Enhancing Large Language Models (LLMs) for Telecommunications using   Knowledge Graphs and Retrieval-Augmented Generation
**Authors**: Dun Yuan, Hao Zhou, Di Wu, Xue Liu, Hao Chen, Yan Xin, Jianzhong, Zhang

**Updated**: 2025-03-31T15:58:08Z

**Summary**: Large language models (LLMs) have made significant progress in general-purpose natural language processing tasks. However, LLMs are still facing challenges when applied to domain-specific areas like telecommunications, which demands specialized expertise and adaptability to evolving standards. This paper presents a novel framework that combines knowledge graph (KG) and retrieval-augmented generation (RAG) techniques to enhance LLM performance in the telecom domain. The framework leverages a KG to capture structured, domain-specific information about network protocols, standards, and other telecom-related entities, comprehensively representing their relationships. By integrating KG with RAG, LLMs can dynamically access and utilize the most relevant and up-to-date knowledge during response generation. This hybrid approach bridges the gap between structured knowledge representation and the generative capabilities of LLMs, significantly enhancing accuracy, adaptability, and domain-specific comprehension. Our results demonstrate the effectiveness of the KG-RAG framework in addressing complex technical queries with precision. The proposed KG-RAG model attained an accuracy of 88% for question answering tasks on a frequently used telecom-specific dataset, compared to 82% for the RAG-only and 48% for the LLM-only approaches.

**Link**: [arxiv](http://arxiv.org/abs/2503.24245v1),  [pdf](http://arxiv.org/pdf/2503.24245v1)

**Tags**: cs.CL 



### Orlando's flask: detection of a lost-and-found valley on the Moon
**Authors**: Vito Squicciarini, Irina Mirova, Francis D. Anderson, Zhiyuan He, Wahman al-Khwarizmi

**Updated**: 2025-03-31T15:54:30Z

**Summary**: High angular resolution holds the key to extending our knowledge in several domains of astronomical research. In addition to the development of new instruments, advancements in post-processing algorithms can enhance the performances attainable in an observation, turning archival observations into a treasure. We developed a machine-learning tool, named zoom-in, that is able to improve the angular resolution of an astronomical image by a factor of $\sim 100$ by optimally recombining short-cadence sequences of images. After training our model on real-life photographs, we tested our method on archival images of the Moon taken through ESO instruments. We were able to achieve a remarkable spatial resolution of $\sim 1$ m of the lunar surface. While analyzing one of the fields from the sample, we discovered structures of clear anthropic origin inside the Aristarchus crater. The features appear to be consistent with ancient ruins of cities and castles. A thorough analysis of the relevant literature allowed us to conclude that this valley corresponds to the one described in Ludovico Ariosto's "Orlando Furioso": a place where all the items lost by humans gather and pile up. Analyses of the surface brightness from our images, indicating an abnormally high albedo of $\sim 0.25$, further corroborate this idea suggesting a conspicuous presence of glass. We infer the presence of >1 billion flasks of human wits on the lunar surface, whose origin we investigate in detail. We urge for a dedicated mission, astolfo, to be carried out by Artemis astronauts in order to recover human wits and bring them back to the Earth.

**Link**: [arxiv](http://arxiv.org/abs/2503.24242v1),  [pdf](http://arxiv.org/pdf/2503.24242v1)

**Tags**: astro-ph.EP 



### What, How, Where, and How Well? A Survey on Test-Time Scaling in Large   Language Models
**Authors**: Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Yufei Wang, Irwin King, Xue Liu, Chen Ma

**Updated**: 2025-03-31T15:46:15Z

**Summary**: As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q&A. However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering a systemic understanding. To fill this gap, we propose a unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions.

**Link**: [arxiv](http://arxiv.org/abs/2503.24235v1),  [pdf](http://arxiv.org/pdf/2503.24235v1)

**Tags**: cs.CL cs.AI 



### PAARS: Persona Aligned Agentic Retail Shoppers
**Authors**: Saab Mansour, Leonardo Perelli, Lorenzo Mainetti, George Davidson, Stefano D'Amato

**Updated**: 2025-03-31T15:41:51Z

**Summary**: In e-commerce, behavioral data is collected for decision making which can be costly and slow. Simulation with LLM powered agents is emerging as a promising alternative for representing human population behavior. However, LLMs are known to exhibit certain biases, such as brand bias, review rating bias and limited representation of certain groups in the population, hence they need to be carefully benchmarked and aligned to user behavior. Ultimately, our goal is to synthesise an agent population and verify that it collectively approximates a real sample of humans. To this end, we propose a framework that: (i) creates synthetic shopping agents by automatically mining personas from anonymised historical shopping data, (ii) equips agents with retail-specific tools to synthesise shopping sessions and (iii) introduces a novel alignment suite measuring distributional differences between humans and shopping agents at the group (i.e. population) level rather than the traditional "individual" level. Experimental results demonstrate that using personas improves performance on the alignment suite, though a gap remains to human behaviour. We showcase an initial application of our framework for automated agentic A/B testing and compare the findings to human results. Finally, we discuss applications, limitations and challenges setting the stage for impactful future work.

**Link**: [arxiv](http://arxiv.org/abs/2503.24228v1),  [pdf](http://arxiv.org/pdf/2503.24228v1)

**Tags**: cs.AI cs.CL cs.MA 



### CASA: Class-Agnostic Shared Attributes in Vision-Language Models for   Efficient Incremental Object Detection
**Authors**: Mingyi Guo, Yuyang Liu, Zhiyuan Yan, Zongying Lin, Peixi Peng, Yonghong Tian

**Updated**: 2025-03-31T15:30:45Z

**Summary**: Incremental object detection is fundamentally challenged by catastrophic forgetting. A major factor contributing to this issue is background shift, where background categories in sequential tasks may overlap with either previously learned or future unseen classes. To address this, we propose a novel method called Class-Agnostic Shared Attribute Base (CASA) that encourages the model to learn category-agnostic attributes shared across incremental classes. Our approach leverages an LLM to generate candidate textual attributes, selects the most relevant ones based on the current training data, and records their importance in an assignment matrix. For subsequent tasks, the retained attributes are frozen, and new attributes are selected from the remaining candidates, ensuring both knowledge retention and adaptability. Extensive experiments on the COCO dataset demonstrate the state-of-the-art performance of our method.

**Link**: [arxiv](http://arxiv.org/abs/2410.05804v3),  [pdf](http://arxiv.org/pdf/2410.05804v3)

**Tags**: cs.CV 



### Surgical Action Planning with Large Language Models
**Authors**: Mengya Xu, Zhongzhen Huang, Jie Zhang, Xiaofan Zhang, Qi Dou

**Updated**: 2025-03-31T15:29:24Z

**Summary**: In robot-assisted minimally invasive surgery, we introduce the Surgical Action Planning (SAP) task, which generates future action plans from visual inputs to address the absence of intraoperative predictive planning in current intelligent applications. SAP shows great potential for enhancing intraoperative guidance and automating procedures. However, it faces challenges such as understanding instrument-action relationships and tracking surgical progress. Large Language Models (LLMs) show promise in understanding surgical video content but remain underexplored for predictive decision-making in SAP, as they focus mainly on retrospective analysis. Challenges like data privacy, computational demands, and modality-specific constraints further highlight significant research gaps. To tackle these challenges, we introduce LLM-SAP, a Large Language Models-based Surgical Action Planning framework that predicts future actions and generates text responses by interpreting natural language prompts of surgical goals. The text responses potentially support surgical education, intraoperative decision-making, procedure documentation, and skill analysis. LLM-SAP integrates two novel modules: the Near-History Focus Memory Module (NHF-MM) for modeling historical states and the prompts factory for action planning. We evaluate LLM-SAP on our constructed CholecT50-SAP dataset using models like Qwen2.5 and Qwen2-VL, demonstrating its effectiveness in next-action prediction. Pre-trained LLMs are tested in a zero-shot setting, and supervised fine-tuning (SFT) with LoRA is implemented. Our experiments show that Qwen2.5-72B-SFT surpasses Qwen2.5-72B with a 19.3% higher accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2503.18296v2),  [pdf](http://arxiv.org/pdf/2503.18296v2)

**Tags**: cs.CL 



### Synthetic News Generation for Fake News Classification
**Authors**: Abdul Sittar, Luka Golob, Mateja Smiljanic

**Updated**: 2025-03-31T15:24:05Z

**Summary**: This study explores the generation and evaluation of synthetic fake news through fact based manipulations using large language models (LLMs). We introduce a novel methodology that extracts key facts from real articles, modifies them, and regenerates content to simulate fake news while maintaining coherence. To assess the quality of the generated content, we propose a set of evaluation metrics coherence, dissimilarity, and correctness. The research also investigates the application of synthetic data in fake news classification, comparing traditional machine learning models with transformer based models such as BERT. Our experiments demonstrate that transformer models, especially BERT, effectively leverage synthetic data for fake news detection, showing improvements with smaller proportions of synthetic data. Additionally, we find that fact verification features, which focus on identifying factual inconsistencies, provide the most promising results in distinguishing synthetic fake news. The study highlights the potential of synthetic data to enhance fake news detection systems, offering valuable insights for future research and suggesting that targeted improvements in synthetic data generation can further strengthen detection models.

**Link**: [arxiv](http://arxiv.org/abs/2503.24206v1),  [pdf](http://arxiv.org/pdf/2503.24206v1)

**Tags**: cs.CL 



### TwT: Thinking without Tokens by Habitual Reasoning Distillation with   Multi-Teachers' Guidance
**Authors**: Jingxian Xu, Mengyu Zhou, Weichang Liu, Hanbing Liu, Shi Han, Dongmei Zhang

**Updated**: 2025-03-31T15:16:31Z

**Summary**: Large Language Models (LLMs) have made significant strides in problem-solving by incorporating reasoning processes. However, this enhanced reasoning capability results in an increased number of output tokens during inference, leading to higher computational costs. To address this challenge, we propose TwT (Thinking without Tokens), a method that reduces inference-time costs through habitual reasoning distillation with multi-teachers' guidance, while maintaining high performance. Our approach introduces a Habitual Reasoning Distillation method, which internalizes explicit reasoning into the model's habitual behavior through a Teacher-Guided compression strategy inspired by human cognition. Additionally, we propose Dual-Criteria Rejection Sampling (DCRS), a technique that generates a high-quality and diverse distillation dataset using multiple teacher models, making our method suitable for unsupervised scenarios. Experimental results demonstrate that TwT effectively reduces inference costs while preserving superior performance, achieving up to a 13.6% improvement in accuracy with fewer output tokens compared to other distillation methods, offering a highly practical solution for efficient LLM deployment.

**Link**: [arxiv](http://arxiv.org/abs/2503.24198v1),  [pdf](http://arxiv.org/pdf/2503.24198v1)

**Tags**: cs.CL 



### Text2Tracks: Prompt-based Music Recommendation via Generative Retrieval
**Authors**: Enrico Palumbo, Gustavo Penha, Andreas Damianou, JosÃ© Luis Redondo GarcÃ­a, Timothy Christopher Heath, Alice Wang, Hugues Bouchard, Mounia Lalmas

**Updated**: 2025-03-31T15:09:19Z

**Summary**: In recent years, Large Language Models (LLMs) have enabled users to provide highly specific music recommendation requests using natural language prompts (e.g. "Can you recommend some old classics for slow dancing?"). In this setup, the recommended tracks are predicted by the LLM in an autoregressive way, i.e. the LLM generates the track titles one token at a time. While intuitive, this approach has several limitation. First, it is based on a general purpose tokenization that is optimized for words rather than for track titles. Second, it necessitates an additional entity resolution layer that matches the track title to the actual track identifier. Third, the number of decoding steps scales linearly with the length of the track title, slowing down inference. In this paper, we propose to address the task of prompt-based music recommendation as a generative retrieval task. Within this setting, we introduce novel, effective, and efficient representations of track identifiers that significantly outperform commonly used strategies. We introduce Text2Tracks, a generative retrieval model that learns a mapping from a user's music recommendation prompt to the relevant track IDs directly. Through an offline evaluation on a dataset of playlists with language inputs, we find that (1) the strategy to create IDs for music tracks is the most important factor for the effectiveness of Text2Tracks and semantic IDs significantly outperform commonly used strategies that rely on song titles as identifiers (2) provided with the right choice of track identifiers, Text2Tracks outperforms sparse and dense retrieval solutions trained to retrieve tracks from language prompts.

**Link**: [arxiv](http://arxiv.org/abs/2503.24193v1),  [pdf](http://arxiv.org/pdf/2503.24193v1)

**Tags**: cs.IR 



### Output Constraints as Attack Surface: Exploiting Structured Generation   to Bypass LLM Safety Mechanisms
**Authors**: Shuoming Zhang, Jiacheng Zhao, Ruiyuan Xu, Xiaobing Feng, Huimin Cui

**Updated**: 2025-03-31T15:08:06Z

**Summary**: Content Warning: This paper may contain unsafe or harmful content generated by LLMs that may be offensive to readers. Large Language Models (LLMs) are extensively used as tooling platforms through structured output APIs to ensure syntax compliance so that robust integration with existing softwares like agent systems, could be achieved. However, the feature enabling functionality of grammar-guided structured output presents significant security vulnerabilities. In this work, we reveal a critical control-plane attack surface orthogonal to traditional data-plane vulnerabilities. We introduce Constrained Decoding Attack (CDA), a novel jailbreak class that weaponizes structured output constraints to bypass safety mechanisms. Unlike prior attacks focused on input prompts, CDA operates by embedding malicious intent in schema-level grammar rules (control-plane) while maintaining benign surface prompts (data-plane). We instantiate this with a proof-of-concept Chain Enum Attack, achieves 96.2% attack success rates across proprietary and open-weight LLMs on five safety benchmarks with a single query, including GPT-4o and Gemini-2.0-flash. Our findings identify a critical security blind spot in current LLM architectures and urge a paradigm shift in LLM safety to address control-plane vulnerabilities, as current mechanisms focused solely on data-plane threats leave critical systems exposed.

**Link**: [arxiv](http://arxiv.org/abs/2503.24191v1),  [pdf](http://arxiv.org/pdf/2503.24191v1)

**Tags**: cs.CR cs.AI 



### Cascade Reward Sampling for Efficient Decoding-Time Alignment
**Authors**: Bolian Li, Yifan Wang, Anamika Lochab, Ananth Grama, Ruqi Zhang

**Updated**: 2025-03-31T15:07:35Z

**Summary**: Aligning large language models (LLMs) with human preferences is essential for their applications. Recently, decoding-time alignment has emerged as an effective plug-and-play technique that avoids fine-tuning model parameters. This approach retains the general utility of pretrained LLMs but often suffers from significant inefficiencies during decoding, primarily due to wasted token generation and excessive reward evaluations. To address these challenges, we introduce Cascade Reward Sampling (CARDS) to resolve both efficiency bottlenecks in decoding-time alignment. Specifically, we develop a segment-level rejection sampling algorithm that minimizes redundant computations of both LLMs and reward models (RMs). Central to CARDS is an uncertainty-based segmentation mechanism, which ensures the accuracy of RMs evaluations on incomplete segments. Furthermore, we provide a detailed analysis of reward scores on segments to elucidate the improved alignment performance. Experimental results demonstrate that CARDS significantly improves decoding efficiency, alignment quality, and general utility compared to existing decoding-time alignment methods, achieving approximately a 70% reduction in decoding time and over 90% win-ties in utility and safety benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2406.16306v2),  [pdf](http://arxiv.org/pdf/2406.16306v2)

**Tags**: cs.CL cs.LG stat.ML 



### Implicit In-Context Learning: Evidence from Artificial Language   Experiments
**Authors**: Xiaomeng Ma, Qihui Xu

**Updated**: 2025-03-31T15:07:08Z

**Summary**: Humans acquire language through implicit learning, absorbing complex patterns without explicit awareness. While LLMs demonstrate impressive linguistic capabilities, it remains unclear whether they exhibit human-like pattern recognition during in-context learning at inferencing level. We adapted three classic artificial language learning experiments spanning morphology, morphosyntax, and syntax to systematically evaluate implicit learning at inferencing level in two state-of-the-art OpenAI models: gpt-4o and o3-mini. Our results reveal linguistic domain-specific alignment between models and human behaviors, o3-mini aligns better in morphology while both models align in syntax.

**Link**: [arxiv](http://arxiv.org/abs/2503.24190v1),  [pdf](http://arxiv.org/pdf/2503.24190v1)

**Tags**: cs.CL 



### Theoretical Foundations of Conformal Prediction
**Authors**: Anastasios N. Angelopoulos, Rina Foygel Barber, Stephen Bates

**Updated**: 2025-03-31T14:50:37Z

**Summary**: This book is about conformal prediction and related inferential techniques that build on permutation tests and exchangeability. These techniques are useful in a diverse array of tasks, including hypothesis testing and providing uncertainty quantification guarantees for machine learning systems. Much of the current interest in conformal prediction is due to its ability to integrate into complex machine learning workflows, solving the problem of forming prediction sets without any assumptions on the form of the data generating distribution. Since contemporary machine learning algorithms have generally proven difficult to analyze directly, conformal prediction's main appeal is its ability to provide formal, finite-sample guarantees when paired with such methods.   The goal of this book is to teach the reader about the fundamental technical arguments that arise when researching conformal prediction and related questions in distribution-free inference. Many of these proof strategies, especially the more recent ones, are scattered among research papers, making it difficult for researchers to understand where to look, which results are important, and how exactly the proofs work. We hope to bridge this gap by curating what we believe to be some of the most important results in the literature and presenting their proofs in a unified language, with illustrations, and with an eye towards pedagogy.

**Link**: [arxiv](http://arxiv.org/abs/2411.11824v2),  [pdf](http://arxiv.org/pdf/2411.11824v2)

**Tags**: math.ST stat.ME stat.ML stat.TH 



### LLM4FS: Leveraging Large Language Models for Feature Selection and How   to Improve It
**Authors**: Jianhao Li, Xianchao Xiu

**Updated**: 2025-03-31T14:40:31Z

**Summary**: Recent advances in large language models (LLMs) have provided new opportunities for decision-making, particularly in the task of automated feature selection. In this paper, we first comprehensively evaluate LLM-based feature selection methods, covering the state-of-the-art DeepSeek-R1, GPT-o3-mini, and GPT-4.5. Then, we propose a novel hybrid strategy called LLM4FS that integrates LLMs with traditional data-driven methods. Specifically, input data samples into LLMs, and directly call traditional data-driven techniques such as random forest and forward sequential selection. Notably, our analysis reveals that the hybrid strategy leverages the contextual understanding of LLMs and the high statistical reliability of traditional data-driven methods to achieve excellent feature selection performance, even surpassing LLMs and traditional data-driven methods. Finally, we point out the limitations of its application in decision-making.

**Link**: [arxiv](http://arxiv.org/abs/2503.24157v1),  [pdf](http://arxiv.org/pdf/2503.24157v1)

**Tags**: cs.LG 



### ScienceAgentBench: Toward Rigorous Assessment of Language Agents for   Data-Driven Scientific Discovery
**Authors**: Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N. Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun

**Updated**: 2025-03-31T14:39:44Z

**Summary**: The advancements of large language models (LLMs) have piqued growing interest in developing LLM-based language agents to automate scientific discovery end-to-end, which has sparked both excitement and skepticism about their true capabilities. In this work, we call for rigorous assessment of agents on individual tasks in a scientific workflow before making bold claims on end-to-end automation. To this end, we present ScienceAgentBench, a new benchmark for evaluating language agents for data-driven scientific discovery. To ensure the scientific authenticity and real-world relevance of our benchmark, we extract 102 tasks from 44 peer-reviewed publications in four disciplines and engage nine subject matter experts to validate them. We unify the target output for every task to a self-contained Python program file and employ an array of evaluation metrics to examine the generated programs, execution results, and costs. Each task goes through multiple rounds of manual validation by annotators and subject matter experts to ensure its annotation quality and scientific plausibility. We also propose two effective strategies to mitigate data contamination concerns. Using ScienceAgentBench, we evaluate five open-weight and proprietary LLMs, each with three frameworks: direct prompting, OpenHands CodeAct, and self-debug. Given three attempts for each task, the best-performing agent can only solve 32.4% of the tasks independently and 34.3% with expert-provided knowledge. In addition, we evaluate OpenAI o1-preview with direct prompting and self-debug, which can boost the performance to 42.2%, demonstrating the effectiveness of increasing inference-time compute but with more than 10 times the cost of other LLMs. Still, our results underscore the limitations of current language agents in generating code for data-driven discovery, let alone end-to-end automation for scientific research.

**Link**: [arxiv](http://arxiv.org/abs/2410.05080v3),  [pdf](http://arxiv.org/pdf/2410.05080v3)

**Tags**: cs.CL cs.AI cs.LG 



### Concept Navigation and Classification via Open-Source Large Language   Model Processing
**Authors**: MaÃ«l Kubli

**Updated**: 2025-03-31T14:37:40Z

**Summary**: This paper presents a novel methodological framework for detecting and classifying latent constructs, including frames, narratives, and topics, from textual data using Open-Source Large Language Models (LLMs). The proposed hybrid approach combines automated summarization with human-in-the-loop validation to enhance the accuracy and interpretability of construct identification. By employing iterative sampling coupled with expert refinement, the framework guarantees methodological robustness and ensures conceptual precision. Applied to diverse data sets, including AI policy debates, newspaper articles on encryption, and the 20 Newsgroups data set, this approach demonstrates its versatility in systematically analyzing complex political discourses, media framing, and topic classification tasks.

**Link**: [arxiv](http://arxiv.org/abs/2502.04756v2),  [pdf](http://arxiv.org/pdf/2502.04756v2)

**Tags**: cs.CL cs.AI cs.LG I.2.7 



### PhD Knowledge Not Required: A Reasoning Challenge for Large Language   Models
**Authors**: Zixuan Wu, Francesca Lucchetti, Aleksander Boruch-Gruszecki, Jingmiao Zhao, Carolyn Jane Anderson, Joydeep Biswas, Federico Cassano, Molly Q Feldman, Arjun Guha

**Updated**: 2025-03-31T14:21:49Z

**Summary**: Existing benchmarks for frontier models often test specialized, "PhD-level" knowledge that is difficult for non-experts to grasp. In contrast, we present a benchmark with 594 problems based on the NPR Sunday Puzzle Challenge that requires only general knowledge. Our benchmark is challenging for both humans and models; however correct solutions are easy to verify, and models' mistakes are easy to spot. As LLMs are more widely deployed in society, we believe it is useful to develop benchmarks for frontier models that humans can understand without the need for deep domain expertise.   Our work reveals capability gaps that are not evident in existing benchmarks: OpenAI o1 significantly outperforms other reasoning models on our benchmark, despite being on par with other models when tested on benchmarks that test specialized knowledge. Furthermore, our analysis of reasoning outputs uncovers new kinds of failures. DeepSeek R1, for instance, often concedes with "I give up" before providing an answer that it knows is wrong. R1 can also be remarkably "uncertain" in its output and in rare cases, it does not "finish thinking," which suggests the need for techniques to "wrap up" before the context window limit is reached. We also quantify the effectiveness of reasoning longer to identify the point beyond which more reasoning is unlikely to improve accuracy on our benchmark.

**Link**: [arxiv](http://arxiv.org/abs/2502.01584v3),  [pdf](http://arxiv.org/pdf/2502.01584v3)

**Tags**: cs.AI cs.LG 



### Compression Metadata-assisted RoI Extraction and Adaptive Inference for   Efficient Video Analytics
**Authors**: Chengzhi Wang, Peng Yang

**Updated**: 2025-03-31T14:12:02Z

**Summary**: Video analytics demand substantial computing resources, posing significant challenges in computing resource-constrained environment. In this paper, to achieve high accuracy with acceptable computational workload, we propose a cost-effective regions of interest (RoIs) extraction and adaptive inference scheme based on the informative encoding metadata. Specifically, to achieve efficient RoI-based analytics, we explore motion vectors from encoding metadata to identify RoIs in non-reference frames through morphological opening operation. Furthermore, considering the content variation of RoIs, which calls for inference by models with distinct size, we measure RoI complexity based on the bitrate allocation information from encoding metadata. Finally, we design an algorithm that prioritizes scheduling RoIs to models of the appropriate complexity, balancing accuracy and latency. Extensive experimental results show that our proposed scheme reduces latency by nearly 40% and improves 2.2% on average in accuracy, outperforming the latest benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2503.24127v1),  [pdf](http://arxiv.org/pdf/2503.24127v1)

**Tags**: cs.MM 



### CTSketch: Compositional Tensor Sketching for Scalable Neurosymbolic   Learning
**Authors**: Seewon Choi, Alaia Solko-Breslin, Rajeev Alur, Eric Wong

**Updated**: 2025-03-31T14:08:58Z

**Summary**: Many computational tasks benefit from being formulated as the composition of neural networks followed by a discrete symbolic program. The goal of neurosymbolic learning is to train the neural networks using only end-to-end input-output labels of the composite. We introduce CTSketch, a novel, scalable neurosymbolic learning algorithm. CTSketch uses two techniques to improve the scalability of neurosymbolic inference: decompose the symbolic program into sub-programs and summarize each sub-program with a sketched tensor. This strategy allows us to approximate the output distribution of the program with simple tensor operations over the input distributions and summaries. We provide theoretical insight into the maximum error of the approximation. Furthermore, we evaluate CTSketch on many benchmarks from the neurosymbolic literature, including some designed for evaluating scalability. Our results show that CTSketch pushes neurosymbolic learning to new scales that have previously been unattainable by obtaining high accuracy on tasks involving over one thousand inputs.

**Link**: [arxiv](http://arxiv.org/abs/2503.24123v1),  [pdf](http://arxiv.org/pdf/2503.24123v1)

**Tags**: cs.LG 



### TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud   Detection
**Authors**: Zhiming Ma, Peidong Wang, Minhua Huang, Jingpeng Wang, Kai Wu, Xiangzhao Lv, Yachun Pang, Yin Yang, Wenjie Tang, Yuchen Kang

**Updated**: 2025-03-31T14:06:17Z

**Summary**: The detection of telecom fraud faces significant challenges due to the lack of high-quality multimodal training data that integrates audio signals with reasoning-oriented textual analysis. To address this gap, we present TeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset specifically designed for automated telecom fraud analysis. Our dataset is constructed through three strategies: (1) Privacy-preserved text-truth sample generation using automatically speech recognition (ASR)-transcribed call recordings (with anonymized original audio), ensuring real-world consistency through text-to-speech (TTS) model regeneration; (2) Semantic enhancement via large language model (LLM)-based self-instruction sampling on authentic ASR outputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that simulates emerging fraud tactics through predefined communication scenarios and fraud typologies. The generated dataset contains 28,511 rigorously processed speech-text pairs, complete with detailed annotations for fraud reasoning. The dataset is divided into three tasks: scenario classification, fraud detection, fraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a standardized evaluation benchmark comprising proportionally sampled instances from the dataset, to facilitate systematic testing of model performance on telecom fraud detection tasks. We also contribute a production-optimized supervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while open-sourcing the data processing framework to enable community-driven dataset expansion. This work establishes a foundational framework for multimodal anti-fraud research while addressing critical challenges in data privacy and scenario diversity. The project will be released at https://github.com/JimmyMa99/TeleAntiFraud.

**Link**: [arxiv](http://arxiv.org/abs/2503.24115v1),  [pdf](http://arxiv.org/pdf/2503.24115v1)

**Tags**: cs.CL cs.MM 



### The Quantum Technology Job Market: Data Driven Analysis of 3641 Job   Posts
**Authors**: Simon Goorney, Eleni Karydi, Borja MuÃ±oz, Otto Santesson, Zeki Can Seskir, Ana Alina Tudoran, Jacob Sherson

**Updated**: 2025-03-31T14:03:13Z

**Summary**: The rapid advancement of Quantum Technology (QT) has created a growing demand for a specialized workforce, spanning across academia and industry. This study presents a quantitative analysis of the QT job market by systematically extracting and classifying thousands of job postings worldwide. The classification pipeline leverages large language models (LLMs) whilst incorporating a "human-in-the-loop" validation process to ensure reliability, achieving an F1-score of 89%: a high level of accuracy. The research identifies key trends in regional job distribution, degree and skill requirements, and the evolving demand for QT-related roles. Findings reveal a strong presence of the QT job market in the United States and Europe, with increasing corporate demand for engineers, software developers, and PhD-level researchers. Despite growing industry applications, the sector remains in its early stages, dominated by large technology firms and requiring significant investment in education and workforce development. The study highlights the need for targeted educational programs, interdisciplinary collaboration, and industry-academic partnerships to bridge the QT workforce gap.

**Link**: [arxiv](http://arxiv.org/abs/2503.19004v2),  [pdf](http://arxiv.org/pdf/2503.19004v2)

**Tags**: physics.ed-ph cs.CY quant-ph 



### Grounding Agent Reasoning in Image Schemas: A Neurosymbolic Approach to   Embodied Cognition
**Authors**: FranÃ§ois Olivier, Zied Bouraoui

**Updated**: 2025-03-31T14:01:39Z

**Summary**: Despite advances in embodied AI, agent reasoning systems still struggle to capture the fundamental conceptual structures that humans naturally use to understand and interact with their environment. To address this, we propose a novel framework that bridges embodied cognition theory and agent systems by leveraging a formal characterization of image schemas, which are defined as recurring patterns of sensorimotor experience that structure human cognition. By customizing LLMs to translate natural language descriptions into formal representations based on these sensorimotor patterns, we will be able to create a neurosymbolic system that grounds the agent's understanding in fundamental conceptual structures. We argue that such an approach enhances both efficiency and interpretability while enabling more intuitive human-agent interactions through shared embodied understanding.

**Link**: [arxiv](http://arxiv.org/abs/2503.24110v1),  [pdf](http://arxiv.org/pdf/2503.24110v1)

**Tags**: cs.AI cs.CL 



### Is LLM the Silver Bullet to Low-Resource Languages Machine Translation?
**Authors**: Yewei Song, Lujun Li, Cedric Lothritz, Saad Ezzini, Lama Sleem, Niccolo Gentile, Radu State, TegawendÃ© F. BissyandÃ©, Jacques Klein

**Updated**: 2025-03-31T13:56:03Z

**Summary**: Low-Resource Languages (LRLs) present significant challenges in natural language processing due to their limited linguistic resources and underrepresentation in standard datasets. While recent advancements in Large Language Models (LLMs) and Neural Machine Translation (NMT) have substantially improved translation capabilities for high-resource languages, performance disparities persist for LRLs, particularly impacting privacy-sensitive and resource-constrained scenarios. This paper systematically evaluates the limitations of current LLMs across 200 languages using benchmarks such as FLORES-200. We also explore alternative data sources, including news articles and bilingual dictionaries, and demonstrate how knowledge distillation from large pre-trained models can significantly improve smaller LRL translations. Additionally, we investigate various fine-tuning strategies, revealing that incremental enhancements markedly reduce performance gaps on smaller LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2503.24102v1),  [pdf](http://arxiv.org/pdf/2503.24102v1)

**Tags**: cs.CL 



### Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-Oasis
**Authors**: Alessandro ScirÃ¨, Andrei Stefan Bejgu, Simone Tedeschi, Karim Ghonim, Federico Martelli, Roberto Navigli

**Updated**: 2025-03-31T13:55:07Z

**Summary**: After the introduction of Large Language Models (LLMs), there have been substantial improvements in the performance of Natural Language Generation (NLG) tasks, including Text Summarization and Machine Translation. However, LLMs still produce outputs containing hallucinations, that is, content not grounded in factual information. Therefore, developing methods to assess the factuality of LLMs has become urgent.   Indeed, resources for factuality evaluation have recently emerged. Although challenging, these resources face one or more of the following limitations: (i) they are tailored to a specific task or domain; (ii) they are limited in size, thereby preventing the training of new factuality evaluators; (iii) they are designed for simpler verification tasks, such as claim verification.   To address these issues, we introduce LLM-Oasis, to the best of our knowledge the largest resource for training end-to-end factuality evaluators. LLM-Oasis is constructed by extracting claims from Wikipedia, falsifying a subset of these claims, and generating pairs of factual and unfactual texts. We then rely on human annotators to both validate the quality of our dataset and to create a gold standard test set for benchmarking factuality evaluation systems.   Our experiments demonstrate that LLM-Oasis presents a significant challenge for state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in our proposed end-to-end factuality evaluation task, highlighting its potential to drive future research in the field.

**Link**: [arxiv](http://arxiv.org/abs/2411.19655v3),  [pdf](http://arxiv.org/pdf/2411.19655v3)

**Tags**: cs.CL 



### DANTE-AD: Dual-Vision Attention Network for Long-Term Audio Description
**Authors**: Adrienne Deganutti, Simon Hadfield, Andrew Gilbert

**Updated**: 2025-03-31T13:49:43Z

**Summary**: Audio Description is a narrated commentary designed to aid vision-impaired audiences in perceiving key visual elements in a video. While short-form video understanding has advanced rapidly, a solution for maintaining coherent long-term visual storytelling remains unresolved. Existing methods rely solely on frame-level embeddings, effectively describing object-based content but lacking contextual information across scenes. We introduce DANTE-AD, an enhanced video description model leveraging a dual-vision Transformer-based architecture to address this gap. DANTE-AD sequentially fuses both frame and scene level embeddings to improve long-term contextual understanding. We propose a novel, state-of-the-art method for sequential cross-attention to achieve contextual grounding for fine-grained audio description generation. Evaluated on a broad range of key scenes from well-known movie clips, DANTE-AD outperforms existing methods across traditional NLP metrics and LLM-based evaluations.

**Link**: [arxiv](http://arxiv.org/abs/2503.24096v1),  [pdf](http://arxiv.org/pdf/2503.24096v1)

**Tags**: cs.CV 



### Teola: Towards End-to-End Optimization of LLM-based Applications
**Authors**: Xin Tan, Yimin Jiang, Yitao Yang, Hong Xu

**Updated**: 2025-03-31T13:33:54Z

**Summary**: Large language model (LLM)-based applications consist of both LLM and non-LLM components, each contributing to the end-to-end latency. Despite great efforts to optimize LLM inference, end-to-end workflow optimization has been overlooked. Existing frameworks employ coarse-grained orchestration with task modules, which confines optimizations to within each module and yields suboptimal scheduling decisions. We propose fine-grained end-to-end orchestration, which utilizes task primitives as the basic units and represents each query's workflow as a primitive-level dataflow graph. This explicitly exposes a much larger design space, enables optimizations in parallelization and pipelining across primitives of different modules, and enhances scheduling to improve application-level performance. We build Teola, a novel orchestration framework for LLM-based applications that implements this scheme. Comprehensive experiments show that Teola can achieve up to 2.09x speedup over existing systems across various popular LLM applications. The code is available at https://github.com/NetX-lab/Ayo.

**Link**: [arxiv](http://arxiv.org/abs/2407.00326v3),  [pdf](http://arxiv.org/pdf/2407.00326v3)

**Tags**: cs.DC cs.AI cs.NI 



### A Complete Epistemic Temporal Logic for Intelligent Agent
**Authors**: Zining Cao

**Updated**: 2025-03-31T13:33:30Z

**Summary**: In this paper, we present a complete epistemic temporal logic, called BPICTL, which generalizes CTL by introducing epistemic modalities. A sound and complete inference system of BPICTL is given. We prove the finite model property of BPICTL. Furthermore, we present a model checking algorithm for BPICTL.

**Link**: [arxiv](http://arxiv.org/abs/2503.24078v1),  [pdf](http://arxiv.org/pdf/2503.24078v1)

**Tags**: cs.LO 



### MoMuSE: Momentum Multi-modal Target Speaker Extraction for Real-time   Scenarios with Impaired Visual Cues
**Authors**: Junjie Li, Ke Zhang, Shuai Wang, Kong Aik Lee, Man-Wai Mak, Haizhou Li

**Updated**: 2025-03-31T13:31:19Z

**Summary**: Audio-visual Target Speaker Extraction (AV-TSE) aims to isolate the speech of a specific target speaker from an audio mixture using time-synchronized visual cues. In real-world scenarios, visual cues are not always available due to various impairments, which undermines the stability of AV-TSE. Despite this challenge, humans can maintain attentional momentum over time, even when the target speaker is not visible. In this paper, we introduce the Momentum Multi-modal target Speaker Extraction (MoMuSE), which retains a speaker identity momentum in memory, enabling the model to continuously track the target speaker. Designed for real-time inference, MoMuSE extracts the current speech window with guidance from both visual cues and dynamically updated speaker momentum. Experimental results demonstrate that MoMuSE exhibits significant improvement, particularly in scenarios with severe impairment of visual cues.

**Link**: [arxiv](http://arxiv.org/abs/2412.08247v2),  [pdf](http://arxiv.org/pdf/2412.08247v2)

**Tags**: cs.SD cs.CV cs.MM eess.AS 



### Physics-informed neural networks for hidden boundary detection and flow   field reconstruction
**Authors**: Yongzheng Zhu, Weizheng Chen, Jian Deng, Xin Bian

**Updated**: 2025-03-31T13:30:46Z

**Summary**: Simultaneously detecting hidden solid boundaries and reconstructing flow fields from sparse observations poses a significant inverse challenge in fluid mechanics. This study presents a physics-informed neural network (PINN) framework designed to infer the presence, shape, and motion of static or moving solid boundaries within a flow field. By integrating a body fraction parameter into the governing equations, the model enforces no-slip/no-penetration boundary conditions in solid regions while preserving conservation laws of fluid dynamics. Using partial flow field data, the method simultaneously reconstructs the unknown flow field and infers the body fraction distribution, thereby revealing solid boundaries. The framework is validated across diverse scenarios, including incompressible Navier-Stokes and compressible Euler flows, such as steady flow past a fixed cylinder, an inline oscillating cylinder, and subsonic flow over an airfoil. The results demonstrate accurate detection of hidden boundaries, reconstruction of missing flow data, and estimation of trajectories and velocities of a moving body. Further analysis examines the effects of data sparsity, velocity-only measurements, and noise on inference accuracy. The proposed method exhibits robustness and versatility, highlighting its potential for applications when only limited experimental or numerical data are available.

**Link**: [arxiv](http://arxiv.org/abs/2503.24074v1),  [pdf](http://arxiv.org/pdf/2503.24074v1)

**Tags**: physics.flu-dyn cs.LG physics.comp-ph 



### Artificial Conversations, Real Results: Fostering Language Detection   with Synthetic Data
**Authors**: Fatemeh Mohammadi, Tommaso Romano, Samira Maghool, Paolo Ceravolo

**Updated**: 2025-03-31T13:22:34Z

**Summary**: Collecting high-quality training data is essential for fine-tuning Large Language Models (LLMs). However, acquiring such data is often costly and time-consuming, especially for non-English languages such as Italian. Recently, researchers have begun to explore the use of LLMs to generate synthetic datasets as a viable alternative. This study proposes a pipeline for generating synthetic data and a comprehensive approach for investigating the factors that influence the validity of synthetic data generated by LLMs by examining how model performance is affected by metrics such as prompt strategy, text length and target position in a specific task, i.e. inclusive language detection in Italian job advertisements. Our results show that, in most cases and across different metrics, the fine-tuned models trained on synthetic data consistently outperformed other models on both real and synthetic test datasets. The study discusses the practical implications and limitations of using synthetic data for language detection tasks with LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2503.24062v1),  [pdf](http://arxiv.org/pdf/2503.24062v1)

**Tags**: cs.CL cs.AI cs.LG 



### ReaLM: Reliable and Efficient Large Language Model Inference with   Statistical Algorithm-Based Fault Tolerance
**Authors**: Tong Xie, Jiawang Zhao, Zishen Wan, Zuodong Zhang, Yuan Wang, Runsheng Wang, Ru Huang, Meng Li

**Updated**: 2025-03-31T13:15:03Z

**Summary**: The demand for efficient large language model (LLM) inference has propelled the development of dedicated accelerators. As accelerators are vulnerable to hardware faults due to aging, variation, etc, existing accelerator designs often reserve a large voltage margin or leverage algorithm-based fault tolerance (ABFT) techniques to ensure LLM inference correctness. However, previous methods often overlook the inherent fault tolerance of LLMs, leading to high computation and energy overhead. To enable reliable yet efficient LLM inference, in this paper, we propose a novel algorithm/circuit co-design framework, dubbed ReaLM. For the first time, we systematically characterize the fault tolerance of LLMs by performing a large-scale error injection study of representative LLMs and natural language understanding tasks. Then, we propose a statistical ABFT algorithm that fully leverages the error robustness to minimize error recovery as much as possible. We also customize the error detection circuits to enable a low-cost online collection of error statistics. Extensive experiments show that with only 1.42% circuit area and 1.79% power overhead, our ReaLM can reduce perplexity degradation from 18.54 to 0.29. Compared to existing methods, ReaLM consistently reduces recovery costs across different operating voltages and improves energy efficiency by up to 35.83% without compromising LLM performance. Our error injection code is available at https://github.com/2000012835xt/ReaLM-DAC.

**Link**: [arxiv](http://arxiv.org/abs/2503.24053v1),  [pdf](http://arxiv.org/pdf/2503.24053v1)

**Tags**: cs.AR 



### TestART: Improving LLM-based Unit Testing via Co-evolution of Automated   Generation and Repair Iteration
**Authors**: Siqi Gu, Quanjun Zhang, Kecheng Li, Chunrong Fang, Fangyuan Tian, Liuchuan Zhu, Jianyi Zhou, Zhenyu Chen

**Updated**: 2025-03-31T13:13:27Z

**Summary**: Unit testing is crucial for detecting bugs in individual program units but consumes time and effort. Recently, large language models (LLMs) have demonstrated remarkable capabilities in generating unit test cases. However, several problems limit their ability to generate high-quality unit test cases: (1) compilation and runtime errors caused by the hallucination of LLMs; (2) lack of testing and coverage feedback information restricting the increase of code coverage;(3) the repetitive suppression problem causing invalid LLM-based repair and generation attempts. To address these limitations, we propose TestART, a novel unit test generation method. TestART improves LLM-based unit testing via co-evolution of automated generation and repair iteration, representing a significant advancement in automated unit test generation. TestART leverages the template-based repair strategy to effectively fix bugs in LLM-generated test cases for the first time. Meanwhile, TestART extracts coverage information from successful test cases and uses it as coverage-guided testing feedback. It also incorporates positive prompt injection to prevent repetition suppression, thereby enhancing the sufficiency of the final test case. This synergy between generation and repair elevates the correctness and sufficiency of the produced test cases significantly beyond previous methods. In comparative experiments, TestART demonstrates an 18% improvement in pass rate and a 20% enhancement in coverage across three types of datasets compared to baseline models. Additionally, it achieves better coverage rates than EvoSuite with only half the number of test cases. These results demonstrate TestART's superior ability to produce high-quality unit test cases by harnessing the power of LLMs while overcoming their inherent flaws.

**Link**: [arxiv](http://arxiv.org/abs/2408.03095v6),  [pdf](http://arxiv.org/pdf/2408.03095v6)

**Tags**: cs.SE 



### Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents
**Authors**: Shuo Ren, Pu Jian, Zhenjiang Ren, Chunlin Leng, Can Xie, Jiajun Zhang

**Updated**: 2025-03-31T13:11:28Z

**Summary**: As scientific research becomes increasingly complex, innovative tools are needed to manage vast data, facilitate interdisciplinary collaboration, and accelerate discovery. Large language models (LLMs) are now evolving into LLM-based scientific agents that automate critical tasks, ranging from hypothesis generation and experiment design to data analysis and simulation. Unlike general-purpose LLMs, these specialized agents integrate domain-specific knowledge, advanced tool sets, and robust validation mechanisms, enabling them to handle complex data types, ensure reproducibility, and drive scientific breakthroughs. This survey provides a focused review of the architectures, design, benchmarks, applications, and ethical considerations surrounding LLM-based scientific agents. We highlight why they differ from general agents and the ways in which they advance research across various scientific fields. By examining their development and challenges, this survey offers a comprehensive roadmap for researchers and practitioners to harness these agents for more efficient, reliable, and ethically sound scientific discovery.

**Link**: [arxiv](http://arxiv.org/abs/2503.24047v1),  [pdf](http://arxiv.org/pdf/2503.24047v1)

**Tags**: cs.AI cs.MA 



### Bi-Level Route Optimization and Path Planning with Hazard Exploration
**Authors**: Jimin Choi, Grant Stagg, Cameron K. Peterson, Max Z. Li

**Updated**: 2025-03-31T13:08:51Z

**Summary**: Effective risk monitoring in dynamic environments such as disaster zones requires an adaptive exploration strategy to detect hidden threats. We propose a bi-level unmanned aerial vehicle (UAV) monitoring strategy that efficiently integrates high-level route optimization with low-level path planning for known and unknown hazards. At the high level, we formulate the route optimization as a vehicle routing problem (VRP) to determine the optimal sequence for visiting known hazard locations. To strategically incorporate exploration efficiency, we introduce an edge-based centroidal Voronoi tessellation (CVT), which refines baseline routes using pseudo-nodes and allocates path budgets based on the UAV's battery capacity using a line segment Voronoi diagram. At the low level, path planning maximizes information gain within the allocated path budget by generating kinematically feasible B-spline trajectories. Bayesian inference is applied to dynamically update hazard probabilities, enabling the UAVs to prioritize unexplored regions. Simulation results demonstrate that edge-based CVT improves spatial coverage and route uniformity compared to the node-based method. Additionally, our optimized path planning consistently outperforms baselines in hazard discovery rates across a diverse set of scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2503.24044v1),  [pdf](http://arxiv.org/pdf/2503.24044v1)

**Tags**: math.OC 



### MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data   Uncertainty
**Authors**: Yongjin Yang, Haneul Yoo, Hwaran Lee

**Updated**: 2025-03-31T13:03:14Z

**Summary**: Despite the massive advancements in large language models (LLMs), they still suffer from producing plausible but incorrect responses. To improve the reliability of LLMs, recent research has focused on uncertainty quantification to predict whether a response is correct or not. However, most uncertainty quantification methods have been evaluated on single-labeled questions, which removes data uncertainty: the irreducible randomness often present in user queries, which can arise from factors like multiple possible answers. This limitation may cause uncertainty quantification results to be unreliable in practical settings. In this paper, we investigate previous uncertainty quantification methods under the presence of data uncertainty. Our contributions are two-fold: 1) proposing a new Multi-Answer Question Answering dataset, MAQA, consisting of world knowledge, mathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty quantification regarding data uncertainty, and 2) assessing 5 uncertainty quantification methods of diverse white- and black-box LLMs. Our findings show that previous methods relatively struggle compared to single-answer settings, though this varies depending on the task. Moreover, we observe that entropy- and consistency-based methods effectively estimate model uncertainty, even in the presence of data uncertainty. We believe these observations will guide future work on uncertainty quantification in more realistic settings.

**Link**: [arxiv](http://arxiv.org/abs/2408.06816v2),  [pdf](http://arxiv.org/pdf/2408.06816v2)

**Tags**: cs.AI cs.CL 



### Are Large Language Models Memorizing Bug Benchmarks?
**Authors**: Daniel Ramos, Claudia Mamede, Kush Jain, Paulo Canelas, Catarina Gamboa, Claire Le Goues

**Updated**: 2025-03-31T13:02:51Z

**Summary**: Large Language Models (LLMs) have become integral to various software engineering tasks, including code generation, bug detection, and repair. To evaluate model performance in these domains, numerous bug benchmarks containing real-world bugs from software projects have been developed. However, a growing concern within the software engineering community is that these benchmarks may not reliably reflect true LLM performance due to the risk of data leakage. Despite this concern, limited research has been conducted to quantify the impact of potential leakage. In this paper, we systematically evaluate popular LLMs to assess their susceptibility to data leakage from widely used bug benchmarks. To identify potential leakage, we use multiple metrics, including a study of benchmark membership within commonly used training datasets, as well as analyses of negative log-likelihood and n-gram accuracy. Our findings show that certain models, in particular codegen-multi, exhibit significant evidence of memorization in widely used benchmarks like Defects4J, while newer models trained on larger datasets like LLaMa 3.1 exhibit limited signs of leakage. These results highlight the need for careful benchmark selection and the adoption of robust metrics to adequately assess models capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2411.13323v3),  [pdf](http://arxiv.org/pdf/2411.13323v3)

**Tags**: cs.SE cs.AI cs.LG 



### Pay More Attention to the Robustness of Prompt for Instruction Data   Mining
**Authors**: Qiang Wang, Dawei Feng, Xu Zhang, Ao Shen, Yang Xu, Bo Ding, Huaimin Wang

**Updated**: 2025-03-31T12:53:08Z

**Summary**: Instruction tuning has emerged as a paramount method for tailoring the behaviors of LLMs. Recent work has unveiled the potential for LLMs to achieve high performance through fine-tuning with a limited quantity of high-quality instruction data. Building upon this approach, we further explore the impact of prompt's robustness on the selection of high-quality instruction data. This paper proposes a pioneering framework of high-quality online instruction data mining for instruction tuning, focusing on the impact of prompt's robustness on the data mining process. Our notable innovation, is to generate the adversarial instruction data by conducting the attack for the prompt of online instruction data. Then, we introduce an Adversarial Instruction-Following Difficulty metric to measure how much help the adversarial instruction data can provide to the generation of the corresponding response. Apart from it, we propose a novel Adversarial Instruction Output Embedding Consistency approach to select high-quality online instruction data. We conduct extensive experiments on two benchmark datasets to assess the performance. The experimental results serve to underscore the effectiveness of our proposed two methods. Moreover, the results underscore the critical practical significance of considering prompt's robustness.

**Link**: [arxiv](http://arxiv.org/abs/2503.24028v1),  [pdf](http://arxiv.org/pdf/2503.24028v1)

**Tags**: cs.AI 



### IntelliCircos: A Data-driven and AI-powered Authoring Tool for Circos   Plots
**Authors**: Mingyang Gu, Jiamin Zhu, Qipeng Wang, Fengjie Wang, Xiaolin Wen, Yong Wang, Min Zhu

**Updated**: 2025-03-31T12:48:39Z

**Summary**: Genomics data is essential in biological and medical domains, and bioinformatics analysts often manually create circos plots to analyze the data and extract valuable insights. However, creating circos plots is complex, as it requires careful design for multiple track attributes and positional relationships between them. Typically, analysts often seek inspiration from existing circos plots, and they have to iteratively adjust and refine the plot to achieve a satisfactory final design, making the process both tedious and time-intensive. To address these challenges, we propose IntelliCircos, an AI-powered interactive authoring tool that streamlines the process from initial visual design to the final implementation of circos plots. Specifically, we build a new dataset containing 4396 circos plots with corresponding annotations and configurations, which are extracted and labeled from published papers. With the dataset, we further identify track combination patterns, and utilize Large Language Model (LLM) to provide domain-specific design recommendations and configuration references to navigate the design of circos plots. We conduct a user study with 8 bioinformatics analysts to evaluate IntelliCircos, and the results demonstrate its usability and effectiveness in authoring circos plots.

**Link**: [arxiv](http://arxiv.org/abs/2503.24021v1),  [pdf](http://arxiv.org/pdf/2503.24021v1)

**Tags**: cs.HC 



### Bayesian Predictive Coding
**Authors**: Alexander Tschantz, Magnus Koudahl, Hampus Linander, Lancelot Da Costa, Conor Heins, Jeff Beck, Christopher Buckley

**Updated**: 2025-03-31T12:40:50Z

**Summary**: Predictive coding (PC) is an influential theory of information processing in the brain, providing a biologically plausible alternative to backpropagation. It is motivated in terms of Bayesian inference, as hidden states and parameters are optimised via gradient descent on variational free energy. However, implementations of PC rely on maximum \textit{a posteriori} (MAP) estimates of hidden states and maximum likelihood (ML) estimates of parameters, limiting their ability to quantify epistemic uncertainty. In this work, we investigate a Bayesian extension to PC that estimates a posterior distribution over network parameters. This approach, termed Bayesian Predictive coding (BPC), preserves the locality of PC and results in closed-form Hebbian weight updates. Compared to PC, our BPC algorithm converges in fewer epochs in the full-batch setting and remains competitive in the mini-batch setting. Additionally, we demonstrate that BPC offers uncertainty quantification comparable to existing methods in Bayesian deep learning, while also improving convergence properties. Together, these results suggest that BPC provides a biologically plausible method for Bayesian learning in the brain, as well as an attractive approach to uncertainty quantification in deep learning.

**Link**: [arxiv](http://arxiv.org/abs/2503.24016v1),  [pdf](http://arxiv.org/pdf/2503.24016v1)

**Tags**: cs.LG cs.AI stat.ML 



### Exploiting Dynamic Sparsity for Near-Field Spatial Non-Stationary   XL-MIMO Channel Tracking
**Authors**: Wenkang Xu, An Liu, Min-jian Zhao, Giuseppe Caire, Yik-Chung Wu

**Updated**: 2025-03-31T12:39:04Z

**Summary**: This work considers a spatial non-stationary channel tracking problem in broadband extremely large-scale multiple-input-multiple-output (XL-MIMO) systems. In the case of spatial non-stationary, each scatterer has a certain visibility region (VR) over antennas and power change may occur among visible antennas. Concentrating on the temporal correlation of XL-MIMO channels, we design a three-layer Markov prior model and hierarchical two-dimensional (2D) Markov model to exploit the dynamic sparsity of sparse channel vectors and VRs, respectively. Then, we formulate the channel tracking problem as a bilinear measurement process, and a novel dynamic alternating maximum a posteriori (DA-MAP) framework is developed to solve the problem. The DA-MAP contains four basic modules: channel estimation module, VR detection module, grid update module, and temporal correlated module. Specifically, the first module is an inverse-free variational Bayesian inference (IF-VBI) estimator that avoids computational intensive matrix inverse each iteration; the second module is a turbo compressive sensing (Turbo-CS) algorithm that only needs small-scale matrix operations in a parallel fashion; the third module refines the polar-delay domain grid; and the fourth module can process the temporal prior information to ensure high-efficiency channel tracking. Simulations show that the proposed method can achieve a significant channel tracking performance while achieving low computational overhead.

**Link**: [arxiv](http://arxiv.org/abs/2412.19475v2),  [pdf](http://arxiv.org/pdf/2412.19475v2)

**Tags**: eess.SP 



### Simulations in Statistical Workflows
**Authors**: Paul-Christian BÃ¼rkner, Marvin Schmitt, Stefan T. Radev

**Updated**: 2025-03-31T12:38:21Z

**Summary**: Simulations play important and diverse roles in statistical workflows, for example, in model specification, checking, validation, and even directly in model inference. Over the past decades, the application areas and overall potential of simulations in statistical workflows have expanded significantly, driven by the development of new simulation-based algorithms and exponentially increasing computational resources. In this paper, we examine past and current trends in the field and offer perspectives on how simulations may shape the future of statistical practice.

**Link**: [arxiv](http://arxiv.org/abs/2503.24011v1),  [pdf](http://arxiv.org/pdf/2503.24011v1)

**Tags**: stat.CO 



### Fast and Accurate Task Planning using Neuro-Symbolic Language Models and   Multi-level Goal Decomposition
**Authors**: Minseo Kwon, Yaesol Kim, Young J. Kim

**Updated**: 2025-03-31T12:24:30Z

**Summary**: In robotic task planning, symbolic planners using rule-based representations like PDDL are effective but struggle with long-sequential tasks in complicated environments due to exponentially increasing search space. Meanwhile, LLM-based approaches, which are grounded in artificial neural networks, offer faster inference and commonsense reasoning but suffer from lower success rates. To address the limitations of the current symbolic (slow speed) or LLM-based approaches (low accuracy), we propose a novel neuro-symbolic task planner that decomposes complex tasks into subgoals using LLM and carries out task planning for each subgoal using either symbolic or MCTS-based LLM planners, depending on the subgoal complexity. This decomposition reduces planning time and improves success rates by narrowing the search space and enabling LLMs to focus on more manageable tasks. Our method significantly reduces planning time while maintaining high success rates across task planning domains, as well as real-world and simulated robotics environments. More details are available at http://graphics.ewha.ac.kr/LLMTAMP/.

**Link**: [arxiv](http://arxiv.org/abs/2409.19250v2),  [pdf](http://arxiv.org/pdf/2409.19250v2)

**Tags**: cs.RO 



### Rethinking Key-Value Cache Compression Techniques for Large Language   Model Serving
**Authors**: Wei Gao, Xinyu Zhou, Peng Sun, Tianwei Zhang, Yonggang Wen

**Updated**: 2025-03-31T12:23:31Z

**Summary**: Key-Value cache (\texttt{KV} \texttt{cache}) compression has emerged as a promising technique to optimize Large Language Model (LLM) serving. It primarily decreases the memory consumption of \texttt{KV} \texttt{cache} to reduce the computation cost. Despite the development of many compression algorithms, their applications in production environments are still not prevalent. In this paper, we revisit mainstream \texttt{KV} \texttt{cache} compression solutions from a practical perspective. Our contributions are three-fold. First, we comprehensively review existing algorithmic designs and benchmark studies for \texttt{KV} \texttt{cache} compression and identify missing pieces in their performance measurement, which could hinder their adoption in practice. Second, we empirically evaluate representative \texttt{KV} \texttt{cache} compression methods to uncover two key issues that affect the computational efficiency: (1) while compressing \texttt{KV} \texttt{cache} can reduce memory consumption, current implementations (e.g., FlashAttention, PagedAttention) do not optimize for production-level LLM serving, resulting in suboptimal throughput performance; (2) compressing \texttt{KV} \texttt{cache} may lead to longer outputs, resulting in increased end-to-end latency. We further investigate the accuracy performance of individual samples rather than the overall performance, revealing the intrinsic limitations in \texttt{KV} \texttt{cache} compression when handling specific LLM tasks. Third, we provide tools to shed light on future \texttt{KV} \texttt{cache} compression studies and facilitate their practical deployment in production. They are open-sourced in \href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.

**Link**: [arxiv](http://arxiv.org/abs/2503.24000v1),  [pdf](http://arxiv.org/pdf/2503.24000v1)

**Tags**: cs.LG cs.AI 



### On the origin of Jupiter's fuzzy core: constraints from N-body, impact   and evolution simulations
**Authors**: Thomas Meier, Christian Reinhardt, Sho Shibata, Simon MÃ¼ller, Joachim Stadel, Ravit Helled

**Updated**: 2025-03-31T12:20:59Z

**Summary**: It has been suggested that Jupiter's fuzzy core could be a result of a giant impact. Here, we investigate the expected impact conditions from N-body simulations. We then use state-of-the-art SPH simulations to investigate the results of impacts with different conditions including various impactor masses and composition, different formation stages in Jupiter's growth, and different resolutions. We next simulate the long-term thermal evolution of Jupiter post-impact. We find that N-body simulations predict rather oblique impacts, and that head-on collisions are rare. Moreover, our results show that even under a head-on collision, Jupiter's fuzzy core cannot be formed. We next simulated Jupiter's thermal evolution and showed that unless post-impact temperatures are extremely low, a giant impact would not lead to an extended dilute core as inferred by interior models. We conclude that Jupiter's fuzzy core is not caused by an impact and is likely to be an outcome of its formation process.

**Link**: [arxiv](http://arxiv.org/abs/2503.23997v1),  [pdf](http://arxiv.org/pdf/2503.23997v1)

**Tags**: astro-ph.EP 



### A cost of capital approach to determining the LGD discount rate
**Authors**: Janette Larney, Arno Botha, Gerrit Lodewicus Grobler, Helgard Raubenheimer

**Updated**: 2025-03-31T12:09:21Z

**Summary**: Loss Given Default (LGD) is a key risk parameter in determining a bank's regulatory capital. During LGD-estimation, realised recovery cash flows are to be discounted at an appropriate rate. Regulatory guidance mandates that this rate should allow for the time value of money, as well as include a risk premium that reflects the "undiversifiable risk" within these recoveries. Having extensively reviewed earlier methods of determining this rate, we propose a new approach that is inspired by the cost of capital approach from the Solvency II regulatory regime. Our method involves estimating a market-consistent price for a portfolio of defaulted loans, from which an associated discount rate may be inferred. We apply this method to mortgage and personal loans data from a large South African bank. The results reveal the main drivers of the discount rate to be the mean and variance of these recoveries, as well as the bank's cost of capital in excess of the risk-free rate. Our method therefore produces a discount rate that reflects both the undiversifiable risk of recovery recoveries and the time value of money, thereby satisfying regulatory requirements. This work can subsequently enhance the LGD-component within the modelling of both regulatory and economic capital.

**Link**: [arxiv](http://arxiv.org/abs/2503.23992v1),  [pdf](http://arxiv.org/pdf/2503.23992v1)

**Tags**: q-fin.RM q-fin.ST 



### Modelling variability power spectra of active galaxies from irregular   time series
**Authors**: Mehdy Lefkir, Simon Vaughan, Daniela Huppenkothen, Phil Uttley, Vysakh Anilkumar

**Updated**: 2025-03-31T12:01:55Z

**Summary**: A common feature of Active Galactic Nuclei (AGN) is their random variations in brightness across the whole emission spectrum, from radio to $\gamma$-rays. Studying the nature and origin of these fluctuations is critical to characterising the underlying variability process of the accretion flow that powers AGN. Random timing fluctuations are often studied with the power spectrum; this quantifies how the amplitude of variations is distributed over temporal frequencies. Red noise variability -- when the power spectrum increases smoothly towards low frequencies -- is ubiquitous in AGN. The commonly used Fourier analysis methods, have significant challenges when applied to arbitrarily sampled light curves of red noise variability. Several time-domain methods exist to infer the power spectral shape in the case of irregular sampling but they suffer from biases which can be difficult to mitigate, or are computationally expensive. In this paper, we demonstrate a method infer the shape of broad-band power spectra for irregular time series, using a Gaussian process regression method scalable to large datasets. The power spectrum is modelled as a power-law model with one or two bends with flexible slopes. The method is fully Bayesian and we demonstrate its utility using simulated light curves. Finally, Ark 564, a well-known variable Seyfert 1 galaxy, is used as a test case and we find consistent results with the literature using independent X-ray data from XMM-Newton and Swift. We provide publicly available, documented and tested implementations in Python and Julia.

**Link**: [arxiv](http://arxiv.org/abs/2501.05886v2),  [pdf](http://arxiv.org/pdf/2501.05886v2)

**Tags**: astro-ph.GA astro-ph.IM 



### SoftCVI: Contrastive variational inference with self-generated soft   labels
**Authors**: Daniel Ward, Mark Beaumont, Matteo Fasiolo

**Updated**: 2025-03-31T12:00:43Z

**Summary**: Estimating a distribution given access to its unnormalized density is pivotal in Bayesian inference, where the posterior is generally known only up to an unknown normalizing constant. Variational inference and Markov chain Monte Carlo methods are the predominant tools for this task; however, both are often challenging to apply reliably, particularly when the posterior has complex geometry. Here, we introduce Soft Contrastive Variational Inference (SoftCVI), which allows a family of variational objectives to be derived through a contrastive estimation framework. The approach parameterizes a classifier in terms of a variational distribution, reframing the inference task as a contrastive estimation problem aiming to identify a single true posterior sample among a set of samples. Despite this framing, we do not require positive or negative samples, but rather learn by sampling the variational distribution and computing ground truth soft classification labels from the unnormalized posterior itself. The objectives have zero variance gradient when the variational approximation is exact, without the need for specialized gradient estimators. We empirically investigate the performance on a variety of Bayesian inference tasks, using both simple (e.g. normal) and expressive (normalizing flow) variational distributions. We find that SoftCVI can be used to form objectives which are stable to train and mass-covering, frequently outperforming inference with other variational approaches.

**Link**: [arxiv](http://arxiv.org/abs/2407.15687v3),  [pdf](http://arxiv.org/pdf/2407.15687v3)

**Tags**: stat.ML cs.LG 



### Rubric Is All You Need: Enhancing LLM-based Code Evaluation With   Question-Specific Rubrics
**Authors**: Aditya Pathak, Rachit Gandhi, Vaibhav Uttam, Devansh, Yashwanth Nakka, Aaryan Raj Jindal, Pratyush Ghosh, Arnav Ramamoorthy, Shreyash Verma, Aditya Mittal, Aashna Ased, Chirag Khatri, Jagat Sesh Challa, Dhruv Kumar

**Updated**: 2025-03-31T11:59:43Z

**Summary**: Since the disruption in LLM technology brought about by the release of GPT-3 and ChatGPT, LLMs have shown remarkable promise in programming-related tasks. While code generation remains a popular field of research, code evaluation using LLMs remains a problem with no conclusive solution. In this paper, we focus on LLM-based code evaluation and attempt to fill in the existing gaps. We propose multi-agentic novel approaches using question-specific rubrics tailored to the problem statement, arguing that these perform better for logical assessment than the existing approaches that use question-agnostic rubrics. To address the lack of suitable evaluation datasets, we introduce two datasets: a Data Structures and Algorithms dataset containing 150 student submissions from a popular Data Structures and Algorithms practice website, and an Object Oriented Programming dataset comprising 80 student submissions from undergraduate computer science courses. In addition to using standard metrics (Spearman Correlation, Cohen's Kappa), we additionally propose a new metric called as Leniency, which quantifies evaluation strictness relative to expert assessment. Our comprehensive analysis demonstrates that question-specific rubrics significantly enhance logical assessment of code in educational settings, providing better feedback aligned with instructional goals beyond mere syntactic correctness.

**Link**: [arxiv](http://arxiv.org/abs/2503.23989v1),  [pdf](http://arxiv.org/pdf/2503.23989v1)

**Tags**: cs.SE cs.AI 



### Deep Learning Model Deployment in Multiple Cloud Providers: an   Exploratory Study Using Low Computing Power Environments
**Authors**: Elayne Lemos, Rodrigo Oliveira, Jairson Rodrigues, Rosalvo F. Oliveira Neto

**Updated**: 2025-03-31T11:58:37Z

**Summary**: The deployment of Machine Learning models at cloud have grown by tech companies. Hardware requirements are higher when these models involve Deep Learning (DL) techniques and the cloud providers' costs may be a barrier. We explore deploying DL models using for experiments the GECToR model, a DL solution for Grammatical Error Correction, across three of the major cloud platforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware usage and cost at each cloud provider by 7 execution environments with 10 experiments reproduced. We found that while GPUs excel in performance, they had an average cost 300% higher than solutions without GPU. Our analysis also identifies that processor cache size is crucial for cost-effective CPU deployments, enabling over 50% of cost reduction compared to GPUs. This study demonstrates the feasibility and affordability of cloud-based DL inference solutions without GPUs, benefiting resource-constrained users like startups.

**Link**: [arxiv](http://arxiv.org/abs/2503.23988v1),  [pdf](http://arxiv.org/pdf/2503.23988v1)

**Tags**: cs.DC cs.AI cs.PF 68T07, 68U01 C.4; I.2.0; B.8.2 



### InPK: Infusing Prior Knowledge into Prompt for Vision-Language Models
**Authors**: Shuchang Zhou, Jiwei Wei, Shiyuan He, Yuyang Zhou, Chaoning Zhang, Jie Zou, Ning Xie, Yang Yang

**Updated**: 2025-03-31T11:44:28Z

**Summary**: Prompt tuning has become a popular strategy for adapting Vision-Language Models (VLMs) to zero/few-shot visual recognition tasks. Some prompting techniques introduce prior knowledge due to its richness, but when learnable tokens are randomly initialized and disconnected from prior knowledge, they tend to overfit on seen classes and struggle with domain shifts for unseen ones. To address this issue, we propose the InPK model, which infuses class-specific prior knowledge into the learnable tokens during initialization, thus enabling the model to explicitly focus on class-relevant information. Furthermore, to mitigate the weakening of class information by multi-layer encoders, we continuously reinforce the interaction between learnable tokens and prior knowledge across multiple feature levels. This progressive interaction allows the learnable tokens to better capture the fine-grained differences and universal visual concepts within prior knowledge, enabling the model to extract more discriminative and generalized text features. Even for unseen classes, the learned interaction allows the model to capture their common representations and infer their appropriate positions within the existing semantic structure. Moreover, we introduce a learnable text-to-vision projection layer to accommodate the text adjustments, ensuring better alignment of visual-text semantics. Extensive experiments on 11 recognition datasets show that InPK significantly outperforms state-of-the-art methods in multiple zero/few-shot image classification tasks.

**Link**: [arxiv](http://arxiv.org/abs/2502.19777v2),  [pdf](http://arxiv.org/pdf/2502.19777v2)

**Tags**: cs.CV 



### Estimation of nuclear polarization via discrete measurement of NV center   spin evolution
**Authors**: Mateusz Kuniej, Katarzyna Roszak

**Updated**: 2025-03-31T11:38:20Z

**Summary**: We propose a method for the estimation of the initial polarization of spinful nuclei of the 13 C isotope in diamond via a measurement of the evolution of the coherence of an NV center spin qubit. Existing polarization measurement methods are difficult to implement experimentally, because they require direct interference in the environment of the qubit. Here, in order to obtain the information, it is necessary to measure the qubit coherence at certain points of time, which are unambiguously determined by the applied magnetic field. For sufficiently high magnetic fields, the minimum value of the measured coherence constitutes an upper bound on the product of the initial polarizations of each environmental spin. The most significant advantage of the method, which allows to infer initial values of nuclear polarizations without any direct access to the environment, lies in its simplicity and the small amount of experimental resources that it requires. We exemplify the operation of the scheme on a realistic, randomly generated environment of eight nuclear spins, obtaining a reasonably accurate estimation of the initial polarization.

**Link**: [arxiv](http://arxiv.org/abs/2401.03916v2),  [pdf](http://arxiv.org/pdf/2401.03916v2)

**Tags**: quant-ph 



### Electronic structure of UGe$_2$ at ambient pressure: comparison with   X-ray photoemission spectra
**Authors**: M. Samsel-CzekaÅa, M. WerwiÅski, A. Szajek, G. CheÅkowska, R. TroÄ

**Updated**: 2025-03-31T11:34:09Z

**Summary**: Based on experimental crystallographic data, electronic structure of UGe$_2$ have been calculated and compared with our results of X-ray photoelectron spectroscopy (XPS) measurements. We employed two different advanced full potential (FP) methods: FP-local-orbital (FPLO) and FP-linear augmented plane waves (Wien2k) codes for non-magnetic and ferromagnetic states. Starting from the local spin-density approximation (LSDA) or generalised gradient approximation (GGA), we verified either the orbital polarisation (OP) correction or the GGA+U approach for the U 5f-electrons, changing Coulomb-repulsion energies U in the range 0-4 eV. Satisfying agreement was achieved between experimental and our calculated magnetic moments using ab-initio LSDA+OP and non-ab-initio GGA+U approaches, the latter for realistic U values of 2-3 eV. We proved by the LSDA+OP approach an existence of the Fermi surface nesting vector along the a axis, possibly responsible for the triplet superconducting pairing. The calculated data reveal predominantly an itinerant U 5f-electron character of bands near the Fermi level, EF, with only small contributions from the U 6d and Ge 4p states. The experimental XPS spectrum of valence bands (VB) also contains the sharp main 5f-electron peak at EF, a wide hump (around -2 eV), and broad small peaks at higher energies. In the calculated XPS spectrum, the width of the main 5f-electron peak varies between 0.8 and 1.4 eV, depending on a method used in computations, but the hump remains unresolved. A newly observed asymmetric 1-eV satellite in the experimental 4f-core XPS spectrum together with known 3-eV and 7-eV satellites suggest dual behaviour of U-5f-electrons in UGe$_2$, the feature is inferred also from the VB studies.

**Link**: [arxiv](http://arxiv.org/abs/2503.23969v1),  [pdf](http://arxiv.org/pdf/2503.23969v1)

**Tags**: cond-mat.mtrl-sci cond-mat.str-el physics.comp-ph 



### A Benchmark for Vision-Centric HD Mapping by V2I Systems
**Authors**: Miao Fan, Shanshan Yu, Shengtong Xu, Kun Jiang, Haoyi Xiong, Xiangzeng Liu

**Updated**: 2025-03-31T11:24:53Z

**Summary**: Autonomous driving faces safety challenges due to a lack of global perspective and the semantic information of vectorized high-definition (HD) maps. Information from roadside cameras can greatly expand the map perception range through vehicle-to-infrastructure (V2I) communications. However, there is still no dataset from the real world available for the study on map vectorization onboard under the scenario of vehicle-infrastructure cooperation. To prosper the research on online HD mapping for Vehicle-Infrastructure Cooperative Autonomous Driving (VICAD), we release a real-world dataset, which contains collaborative camera frames from both vehicles and roadside infrastructures, and provides human annotations of HD map elements. We also present an end-to-end neural framework (i.e., V2I-HD) leveraging vision-centric V2I systems to construct vectorized maps. To reduce computation costs and further deploy V2I-HD on autonomous vehicles, we introduce a directionally decoupled self-attention mechanism to V2I-HD. Extensive experiments show that V2I-HD has superior performance in real-time inference speed, as tested by our real-world dataset. Abundant qualitative results also demonstrate stable and robust map construction quality with low cost in complex and various driving scenes. As a benchmark, both source codes and the dataset have been released at OneDrive for the purpose of further study.

**Link**: [arxiv](http://arxiv.org/abs/2503.23963v1),  [pdf](http://arxiv.org/pdf/2503.23963v1)

**Tags**: cs.CV cs.RO 



### Local Information Matters: Inference Acceleration For Grounded   Conversation Generation Models Through Adaptive Local-Aware Token Pruning
**Authors**: Bizhe Bai, Jianjian Cao, Yadan Luo, Tao Chen

**Updated**: 2025-04-01T08:34:57Z

**Summary**: Grounded Conversation Generation (GCG) is an emerging vision-language task that requires models to generate natural language responses seamlessly intertwined with corresponding object segmentation masks. Recent models, such as GLaMM and OMG-LLaVA, achieve pixel-level grounding but incur significant computational costs due to processing a large number of visual tokens. Existing token pruning methods, like FastV and PyramidDrop, fail to preserve the local visual features critical for accurate grounding, leading to substantial performance drops in GCG tasks. To address this, we propose Adaptive Local-Aware Token Pruning (ALTP), a simple yet effective framework that accelerates GCG models by prioritizing local object information. ALTP introduces two key components: (1) Detail Density Capture (DDC), which uses superpixel segmentation to retain tokens in object-centric regions, preserving fine-grained details, and (2) Dynamic Density Formation (DDF), which dynamically allocates tokens based on information density, ensuring higher retention in semantically rich areas. Extensive experiments on the GranDf dataset demonstrate that ALTP significantly outperforms existing token pruning methods, such as FastV and PyramidDrop, on both GLaMM and OMG-LLaVA models. Notably, when applied to GLaMM, ALTP achieves a 90% reduction in visual tokens with a 4.9% improvement in AP50 and a 5.0% improvement in Recall compared to PyramidDrop. Similarly, on OMG-LLaVA, ALTP improves AP by 2.1% and mIOU by 3.0% at a 90% token reduction compared with PDrop.

**Link**: [arxiv](http://arxiv.org/abs/2503.23959v2),  [pdf](http://arxiv.org/pdf/2503.23959v2)

**Tags**: cs.CV 



### Written in the Stars: How your (pens and) papers decide the fate of the   arXiverse
**Authors**: Joanne Tan

**Updated**: 2025-03-31T11:15:30Z

**Summary**: We all love the ecstasy that comes with submitting papers to journals or arXiv. Some have described it as yeeting their back-breaking products of labor into the void, wishing they could never deal with them ever again. The very act of yeeting papers onto arXiv contributes to the expansion of the arXiverse; however, we have yet to quantify our contribution to the cause. In this work, I investigate the expansion of the arXiverse using the arXiv astro-ph submission data from 1992 to date. I coin the term "the arXiverse constant", $a_0$, to quantify the rate of expansion of the arXiverse. I find that astro-ph as a whole has a positive $a_0$, but this does not always hold true for the six subcategories of astro-ph. I then investigate the temporal changes in $a_0$ for the astro-ph subcategories and astro-ph as a whole, from which I infer the fate of the arXiverse.

**Link**: [arxiv](http://arxiv.org/abs/2503.23957v1),  [pdf](http://arxiv.org/pdf/2503.23957v1)

**Tags**: physics.pop-ph astro-ph.CO astro-ph.EP astro-ph.GA astro-ph.HE astro-ph.IM astro-ph.SR 



### AirCache: Activating Inter-modal Relevancy KV Cache Compression for   Efficient Large Vision-Language Model Inference
**Authors**: Kai Huang, Hao Zou, Bochen Wang, Ye Xi, Zhen Xie, Hao Wang

**Updated**: 2025-03-31T11:13:18Z

**Summary**: Recent advancements in Large Visual Language Models (LVLMs) have gained significant attention due to their remarkable reasoning capabilities and proficiency in generalization. However, processing a large number of visual tokens and generating long-context outputs impose substantial computational overhead, leading to excessive demands for key-value (KV) cache. To address this critical bottleneck, we propose AirCache, a novel KV cache compression method aimed at accelerating LVLMs inference. This work systematically investigates the correlations between visual and textual tokens within the attention mechanisms of LVLMs. Our empirical analysis reveals considerable redundancy in cached visual tokens, wherein strategically eliminating these tokens preserves model performance while significantly accelerating context generation. Inspired by these findings, we introduce an elite observation window for assessing the importance of visual components in the KV cache, focusing on stable inter-modal relevancy modeling with enhanced multi-perspective consistency. Additionally, we develop an adaptive layer-wise budget allocation strategy that capitalizes on the strength and skewness of token importance distribution, showcasing superior efficiency compared to uniform allocation. Comprehensive evaluations across multiple LVLMs and benchmarks demonstrate that our method achieves comparable performance to the full cache while retaining only 10% of visual KV cache, thereby reducing decoding latency by 29% to 66% across various batch size and prompt length of inputs. Notably, as cache retention rates decrease, our method exhibits increasing performance advantages over existing approaches.

**Link**: [arxiv](http://arxiv.org/abs/2503.23956v1),  [pdf](http://arxiv.org/pdf/2503.23956v1)

**Tags**: cs.CV cs.AI 



### PharMolixFM: All-Atom Foundation Models for Molecular Modeling and   Generation
**Authors**: Yizhen Luo, Jiashuo Wang, Siqi Fan, Zaiqing Nie

**Updated**: 2025-04-01T02:12:44Z

**Summary**: Structural biology relies on accurate three-dimensional biomolecular structures to advance our understanding of biological functions, disease mechanisms, and therapeutics. While recent advances in deep learning have enabled the development of all-atom foundation models for molecular modeling and generation, existing approaches face challenges in generalization due to the multi-modal nature of atomic data and the lack of comprehensive analysis of training and sampling strategies. To address these limitations, we propose PharMolixFM, a unified framework for constructing all-atom foundation models based on multi-modal generative techniques. Our framework includes three variants using state-of-the-art multi-modal generative models. By formulating molecular tasks as a generalized denoising process with task-specific priors, PharMolixFM achieves robust performance across various structural biology applications. Experimental results demonstrate that PharMolixFM-Diff achieves competitive prediction accuracy in protein-small-molecule docking (83.9% vs. 90.2% RMSD < 2{\AA}, given pocket) with significantly improved inference speed. Moreover, we explore the empirical inference scaling law by introducing more sampling repeats or steps. Our code and model are available at https://github.com/PharMolix/OpenBioMed.

**Link**: [arxiv](http://arxiv.org/abs/2503.21788v3),  [pdf](http://arxiv.org/pdf/2503.21788v3)

**Tags**: q-bio.BM cs.LG 



### MagicDistillation: Weak-to-Strong Video Distillation for Large-Scale   Few-Step Synthesis
**Authors**: Shitong Shao, Hongwei Yi, Hanzhong Guo, Tian Ye, Daquan Zhou, Michael Lingelbach, Zhiqiang Xu, Zeke Xie

**Updated**: 2025-03-31T11:03:18Z

**Summary**: Recently, open-source video diffusion models (VDMs), such as WanX, Magic141 and HunyuanVideo, have been scaled to over 10 billion parameters. These large-scale VDMs have demonstrated significant improvements over smaller-scale VDMs across multiple dimensions, including enhanced visual quality and more natural motion dynamics. However, these models face two major limitations: (1) High inference overhead: Large-scale VDMs require approximately 10 minutes to synthesize a 28-step video on a single H100 GPU. (2) Limited in portrait video synthesis: Models like WanX-I2V and HunyuanVideo-I2V often produce unnatural facial expressions and movements in portrait videos. To address these challenges, we propose MagicDistillation, a novel framework designed to reduce inference overhead while ensuring the generalization of VDMs for portrait video synthesis. Specifically, we primarily use sufficiently high-quality talking video to fine-tune Magic141, which is dedicated to portrait video synthesis. We then employ LoRA to effectively and efficiently fine-tune the fake DiT within the step distillation framework known as distribution matching distillation (DMD). Following this, we apply weak-to-strong (W2S) distribution matching and minimize the discrepancy between the fake data distribution and the ground truth distribution, thereby improving the visual fidelity and motion dynamics of the synthesized videos. Experimental results on portrait video synthesis demonstrate the effectiveness of MagicDistillation, as our method surpasses Euler, LCM, and DMD baselines in both FID/FVD metrics and VBench. Moreover, MagicDistillation, requiring only 4 steps, also outperforms WanX-I2V (14B) and HunyuanVideo-I2V (13B) on visualization and VBench. Our project page is https://magicdistillation.github.io/MagicDistillation/.

**Link**: [arxiv](http://arxiv.org/abs/2503.13319v2),  [pdf](http://arxiv.org/pdf/2503.13319v2)

**Tags**: cs.CV 



### Green MLOps to Green GenOps: An Empirical Study of Energy Consumption in   Discriminative and Generative AI Operations
**Authors**: AdriÃ¡n SÃ¡nchez-MompÃ³, Ioannis Mavromatis, Peizheng Li, Konstantinos Katsaros, Aftab Khan

**Updated**: 2025-03-31T10:28:04Z

**Summary**: This study presents an empirical investigation into the energy consumption of Discriminative and Generative AI models within real-world MLOps pipelines. For Discriminative models, we examine various architectures and hyperparameters during training and inference and identify energy-efficient practices. For Generative AI, Large Language Models (LLMs) are assessed, focusing primarily on energy consumption across different model sizes and varying service requests. Our study employs software-based power measurements, ensuring ease of replication across diverse configurations, models, and datasets. We analyse multiple models and hardware setups to uncover correlations among various metrics, identifying key contributors to energy consumption. The results indicate that for Discriminative models, optimising architectures, hyperparameters, and hardware can significantly reduce energy consumption without sacrificing performance. For LLMs, energy efficiency depends on balancing model size, reasoning complexity, and request-handling capacity, as larger models do not necessarily consume more energy when utilisation remains low. This analysis provides practical guidelines for designing green and sustainable ML operations, emphasising energy consumption and carbon footprint reductions while maintaining performance. This paper can serve as a benchmark for accurately estimating total energy use across different types of AI models.

**Link**: [arxiv](http://arxiv.org/abs/2503.23934v1),  [pdf](http://arxiv.org/pdf/2503.23934v1)

**Tags**: cs.LG cs.AI 



### Asymptotic theory for Bayesian inference and prediction: from the   ordinary to a conditional Peaks-Over-Threshold method
**Authors**: ClÃ©ment Dombry, Simone A. Padoan, Stefano Rizzelli

**Updated**: 2025-03-31T10:21:54Z

**Summary**: The Peaks Over Threshold (POT) method is the most popular statistical method for the analysis of univariate extremes. Even though there is a rich applied literature on Bayesian inference for the POT, the asymptotic theory for such proposals is missing. Even more importantly, the ambitious and challenging problem of predicting future extreme events according to a proper predictive statistical approach has received no attention to date. In this paper we fill this gap by developing the asymptotic theory of posterior distributions (consistency, contraction rates, asymptotic normality and asymptotic coverage of credible intervals) and prediction within the Bayesian framework in the POT context. We extend this asymptotic theory to account for cases where the focus is on the tail properties of the conditional distribution of a response variable given a vector of random covariates. To enable accurate predictions of extreme events more severe than those previously observed, we derive the posterior predictive distribution as an estimator of the conditional distribution of an out-of-sample random variable, given that it exceeds a sufficiently high threshold. We establish Wasserstein consistency of the posterior predictive distribution under both the unconditional and covariate-conditional approaches and derive its contraction rates. Simulations show the good performances of the proposed Bayesian inferential methods. The analysis of the change in the frequency of financial crises over time shows the utility of our methodology.

**Link**: [arxiv](http://arxiv.org/abs/2310.06720v2),  [pdf](http://arxiv.org/pdf/2310.06720v2)

**Tags**: math.ST stat.ME stat.TH 62G32, 62F15, 62E20 



### Model Hemorrhage and the Robustness Limits of Large Language Models
**Authors**: Ziyang Ma, Zuchao Li, Lefei Zhang, Gui-Song Xia, Bo Du, Liangpei Zhang, Dacheng Tao

**Updated**: 2025-03-31T10:16:03Z

**Summary**: Large language models (LLMs) demonstrate strong performance across natural language processing tasks, yet undergo significant performance degradation when modified for deployment through quantization, pruning, or decoding strategy adjustments. We define this phenomenon as model hemorrhage - performance decline caused by parameter alterations and architectural changes. Through systematic analysis of various LLM frameworks, we identify key vulnerability patterns: layer expansion frequently disrupts attention mechanisms, compression techniques induce information loss cascades, and decoding adjustments amplify prediction divergences. Our investigation reveals transformer architectures exhibit inherent robustness thresholds that determine hemorrhage severity across modification types. We propose three mitigation strategies: gradient-aware pruning preserves critical weight pathways, dynamic quantization scaling maintains activation integrity, and decoding calibration aligns generation trajectories with original model distributions. This work establishes foundational metrics for evaluating model stability during adaptation, providing practical guidelines for maintaining performance while enabling efficient LLM deployment. Our findings advance understanding of neural network resilience under architectural transformations, particularly for large-scale language models.

**Link**: [arxiv](http://arxiv.org/abs/2503.23924v1),  [pdf](http://arxiv.org/pdf/2503.23924v1)

**Tags**: cs.CL cs.LG 



### Entropy-guided sequence weighting for efficient exploration in RL-based   LLM fine-tuning
**Authors**: Abdullah Vanlioglu

**Updated**: 2025-03-31T10:13:48Z

**Summary**: We introduce Entropy-Guided Sequence Weighting (EGSW), a novel approach that enhances the exploration-exploitation tradeoff by dynamically assigning weights to generated outputs based on their advantage and entropy for Reinforcement Learning-based Large Language Model fine-tuning. EGSW integrates entropy regularization with advantage-based weighting to balance policy updates, enabling efficient exploration in high-dimensional state spaces. By employing temperature-scaled softmax weighting over sequences, EGSW prioritizing high-reward, high-uncertainty steps while maintaining training stability. Although originally developed to improve Group Relative Policy Optimization (GRPO) during large language model (LLM) fine-tuning, EGSW is generalizable to other reinforcement learning (RL) algorithms and can be implemented in both step-wise and trajectory-wise settings. Empirical evaluations demonstrate that EGSW enhances GRPO reasoning ability, yielding improvements in sample efficiency. Future work will explore the application of EGSW to advanced RL methodologies.

**Link**: [arxiv](http://arxiv.org/abs/2503.22456v2),  [pdf](http://arxiv.org/pdf/2503.22456v2)

**Tags**: cs.LG cs.AI 



### Hypothesis testing of symmetry in quantum dynamics
**Authors**: Yu-Ao Chen, Chenghong Zhu, Keming He, Yingjian Liu, Xin Wang

**Updated**: 2025-03-31T10:11:20Z

**Summary**: Symmetry plays a crucial role in quantum physics, dictating the behavior and dynamics of physical systems. In this paper, we develop a hypothesis-testing framework for quantum dynamics symmetry using a limited number of queries to the unknown unitary operation and establish the quantum max-relative entropy lower bound for the type-II error. We construct optimal ancilla-free protocols that achieve optimal type-II error probability for testing time-reversal symmetry (T-symmetry) and diagonal symmetry (Z-symmetry) with limited queries. Contrasting with the advantages of indefinite causal order strategies in various quantum information processing tasks, we show that parallel, adaptive, and indefinite causal order strategies have equal power for our tasks. We establish optimal protocols for T-symmetry testing and Z-symmetry testing for 6 and 5 queries, respectively, from which we infer that the type-II error exhibits a decay rate of $\mathcal{O}(m^{-2})$ with respect to the number of queries $m$. This represents a significant improvement over the basic repetition protocols without using global entanglement, where the error decays at a slower rate of $\mathcal{O}(m^{-1})$.

**Link**: [arxiv](http://arxiv.org/abs/2411.14292v2),  [pdf](http://arxiv.org/pdf/2411.14292v2)

**Tags**: quant-ph cs.IT math.IT 



### Rubrik's Cube: Testing a New Rubric for Evaluating Explanations on the   CUBE dataset
**Authors**: Diana Galvan-Sosa, Gabrielle Gaudeau, Pride Kavumba, Yunmeng Li, Hongyi gu, Zheng Yuan, Keisuke Sakaguchi, Paula Buttery

**Updated**: 2025-03-31T09:48:59Z

**Summary**: The performance and usability of Large-Language Models (LLMs) are driving their use in explanation generation tasks. However, despite their widespread adoption, LLM explanations have been found to be unreliable, making it difficult for users to distinguish good from bad explanations. To address this issue, we present Rubrik's CUBE, an education-inspired rubric and a dataset of 26k explanations, written and later quality-annotated using the rubric by both humans and six open- and closed-source LLMs. The CUBE dataset focuses on two reasoning and two language tasks, providing the necessary diversity for us to effectively test our proposed rubric. Using Rubrik, we find that explanations are influenced by both task and perceived difficulty. Low quality stems primarily from a lack of conciseness in LLM-generated explanations, rather than cohesion and word choice. The full dataset, rubric, and code will be made available upon acceptance.

**Link**: [arxiv](http://arxiv.org/abs/2503.23899v1),  [pdf](http://arxiv.org/pdf/2503.23899v1)

**Tags**: cs.CL I.2.7 



### Training-Free Text-Guided Image Editing with Visual Autoregressive Model
**Authors**: Yufei Wang, Lanqing Guo, Zhihao Li, Jiaxing Huang, Pichao Wang, Bihan Wen, Jian Wang

**Updated**: 2025-03-31T09:46:56Z

**Summary**: Text-guided image editing is an essential task that enables users to modify images through natural language descriptions. Recent advances in diffusion models and rectified flows have significantly improved editing quality, primarily relying on inversion techniques to extract structured noise from input images. However, inaccuracies in inversion can propagate errors, leading to unintended modifications and compromising fidelity. Moreover, even with perfect inversion, the entanglement between textual prompts and image features often results in global changes when only local edits are intended. To address these challenges, we propose a novel text-guided image editing framework based on VAR (Visual AutoRegressive modeling), which eliminates the need for explicit inversion while ensuring precise and controlled modifications. Our method introduces a caching mechanism that stores token indices and probability distributions from the original image, capturing the relationship between the source prompt and the image. Using this cache, we design an adaptive fine-grained masking strategy that dynamically identifies and constrains modifications to relevant regions, preventing unintended changes. A token reassembling approach further refines the editing process, enhancing diversity, fidelity, and control. Our framework operates in a training-free manner and achieves high-fidelity editing with faster inference speeds, processing a 1K resolution image in as fast as 1.2 seconds. Extensive experiments demonstrate that our method achieves performance comparable to, or even surpassing, existing diffusion- and rectified flow-based approaches in both quantitative metrics and visual quality. The code will be released.

**Link**: [arxiv](http://arxiv.org/abs/2503.23897v1),  [pdf](http://arxiv.org/pdf/2503.23897v1)

**Tags**: cs.CV cs.AI 



### Better wit than wealth: Dynamic Parametric Retrieval Augmented   Generation for Test-time Knowledge Enhancement
**Authors**: Yuqiao Tan, Shizhu He, Huanxuan Liao, Jun Zhao, Kang Liu

**Updated**: 2025-03-31T09:46:35Z

**Summary**: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving relevant documents from external sources and incorporating them into the context. While it improves reliability by providing factual texts, it significantly increases inference costs as context length grows and introduces challenging issue of RAG hallucination, primarily caused by the lack of corresponding parametric knowledge in LLMs. An efficient solution is to enhance the knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by embedding document into LLMs parameters to perform test-time knowledge enhancement, effectively reducing inference costs through offline training. However, its high training and storage costs, along with limited generalization ability, significantly restrict its practical adoption. To address these challenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that leverages a lightweight parameter translator model to efficiently convert documents into parametric knowledge. DyPRAG not only reduces inference, training, and storage costs but also dynamically generates parametric knowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge conflicts in a plug-and-play manner at test-time. Extensive experiments on multiple datasets demonstrate the effectiveness and generalization capabilities of DyPRAG, offering a powerful and practical RAG paradigm which enables superior knowledge fusion and mitigates RAG hallucination in real-world applications. Our code is available at https://github.com/Trae1ounG/DyPRAG.

**Link**: [arxiv](http://arxiv.org/abs/2503.23895v1),  [pdf](http://arxiv.org/pdf/2503.23895v1)

**Tags**: cs.CL cs.AI 



### Robust Predictive Routing for Internet of Vehicles Leveraging Both V2I   and V2V Links
**Authors**: Yawen Chang, Xudong Wang

**Updated**: 2025-03-31T09:41:16Z

**Summary**: With the developments of the Internet of Vehicles (IoV) from 4G to 5G, vehicle-to-infrastructure (V2I) communications are becoming attractive for vehicle users (VUEs) to obtain diverse cloud service through base stations (BSs). To tackle V2I link deterioration caused by blockage and out-of-coverage cases, multi-hop V2X routing with both vehicle-to-vehicle (V2V) and V2I links needs to be investigated. However, traditional routing reacts to statistical or real-time information, which may suffer link degradation during path switchover in fast-changing vehicular networks. Predictive routing protocols take timely actions by forecasting link connectivity, but they fail to satisfy specific QoS requirements. Low robustness to link failures is also incurred without considering imperfect prediction. To build continual paths between VUEs and BSs for QoS provision of cloud service, a robust predictive routing framework (ROPE) is proposed with three major components: 1) an early warning scheme detects V2I link deterioration in advance via predicting vehicle mobility and link signal strength to facilitate seamless path switchover; 2) a virtual routing mechanism finds top3 paths that have the highest path strength and satisfy the connectivity and hop count constraints based on the prediction results to fulfill QoS requirements of cloud service; 3) a path verification protocol checks availability and quality of the top3 paths shortly before switchover and activates one qualified path for switchover to ensure routing robustness. We implement ROPE in a simulation framework incorporating real-world urban maps, microscopic traffic generation, geometry-based channel modeling, and offline data analysis as well as online inference. Extensive simulations demonstrate the superiority of ROPE over direct V2I communications and a connectivity-based predictive routing protocol under various scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2503.23889v1),  [pdf](http://arxiv.org/pdf/2503.23889v1)

**Tags**: cs.NI 



### SchemaAgent: A Multi-Agents Framework for Generating Relational Database   Schema
**Authors**: Qin Wang, Youhuan Li, Yansong Feng, Si Chen, Ziming Li, Pan Zhang, Zhichao Shi, Yuequn Dou, chuchu Gao, Zebin Huang, Zihui Si, Yixuan Chen, Zhaohai Sun, Ke Tang, Wenqiang Jin

**Updated**: 2025-03-31T09:39:19Z

**Summary**: The relational database design would output a schema based on user's requirements, which defines table structures and their interrelated relations. Translating requirements into accurate schema involves several non-trivial subtasks demanding both database expertise and domain-specific knowledge. This poses unique challenges for automated design of relational databases. Existing efforts are mostly based on customized rules or conventional deep learning models, often producing suboptimal schema. Recently, large language models (LLMs) have significantly advanced intelligent application development across various domains. In this paper, we propose SchemaAgent, a unified LLM-based multi-agent framework for the automated generation of high-quality database schema. SchemaAgent is the first to apply LLMs for schema generation, which emulates the workflow of manual schema design by assigning specialized roles to agents and enabling effective collaboration to refine their respective subtasks. Schema generation is a streamlined workflow, where directly applying the multi-agent framework may cause compounding impact of errors. To address this, we incorporate dedicated roles for reflection and inspection, alongside an innovative error detection and correction mechanism to identify and rectify issues across various phases. For evaluation, we present a benchmark named \textit{RSchema}, which contains more than 500 pairs of requirement description and schema. Experimental results on this benchmark demonstrate the superiority of our approach over mainstream LLMs for relational database schema generation.

**Link**: [arxiv](http://arxiv.org/abs/2503.23886v1),  [pdf](http://arxiv.org/pdf/2503.23886v1)

**Tags**: cs.DB cs.AI 



### Exploring In-Context Learning Capabilities of ChatGPT for Pathological   Speech Detection
**Authors**: Mahdi Amiri, Hatef Otroshi Shahreza, Ina Kodrasi

**Updated**: 2025-03-31T09:23:52Z

**Summary**: Automatic pathological speech detection approaches have shown promising results, gaining attention as potential diagnostic tools alongside costly traditional methods. While these approaches can achieve high accuracy, their lack of interpretability limits their applicability in clinical practice. In this paper, we investigate the use of multimodal Large Language Models (LLMs), specifically ChatGPT-4o, for automatic pathological speech detection in a few-shot in-context learning setting. Experimental results show that this approach not only delivers promising performance but also provides explanations for its decisions, enhancing model interpretability. To further understand its effectiveness, we conduct an ablation study to analyze the impact of different factors, such as input type and system prompts, on the final results. Our findings highlight the potential of multimodal LLMs for further exploration and advancement in automatic pathological speech detection.

**Link**: [arxiv](http://arxiv.org/abs/2503.23873v1),  [pdf](http://arxiv.org/pdf/2503.23873v1)

**Tags**: eess.AS cs.SD 



### Communication-Efficient and Personalized Federated Foundation Model   Fine-Tuning via Tri-Matrix Adaptation
**Authors**: Yongle Li, Bo Liu, Sheng Huang, ZHeng ZHang, Xiaotong Yuan, Richang Hong

**Updated**: 2025-03-31T09:18:42Z

**Summary**: In federated learning, fine-tuning pre-trained foundation models poses significant challenges, particularly regarding high communication cost and suboptimal model performance due to data heterogeneity between the clients. To address these issues, this paper introduces communication-efficient federated LoRA adaption (CE-LoRA), a method that employs a tri-factorization low-rank adaptation approach with personalized model parameter aggregation. We first presents a novel LoRA parameter factorization by introducing a small-size dense matrix, which can significantly reduce the communication cost and achieve comparable empirical performance than transferring the low-rank parameter matrix used by existing methods. Without violating data privacy, the server considers the client similarity in both training dataset and model parameter space, and learns personalized weights for model aggregation. Our experiments on various LLM and VLM fine-tuning tasks demonstrate that CE-LoRA not only significantly reduces communication overhead but also improves performance under not independently and identically distributed data conditions. In addition, CE-LoRA improves data privacy protection, effectively mitigating gradient-based data reconstruction attacks.

**Link**: [arxiv](http://arxiv.org/abs/2503.23869v1),  [pdf](http://arxiv.org/pdf/2503.23869v1)

**Tags**: cs.LG 



### FeedSign: Robust Full-parameter Federated Fine-tuning of Large Models   with Extremely Low Communication Overhead of One Bit
**Authors**: Zhijie Cai, Haolong Chen, Guangxu Zhu

**Updated**: 2025-03-31T09:12:59Z

**Summary**: Federated fine-tuning (FFT) attempts to fine-tune a pre-trained model with private data from distributed clients by exchanging models rather than data under the orchestration of a parameter server (PS). To overcome the bottleneck forged by the growing communication and memory overhead for clients in such systems due to the growing model sizes, we propose \textit{FeedSign}, an FFT algorithm in which the upload and download payload for an aggregation step is exactly $1$ bit per step, while the memory overhead is squeezed to the amount needed for inference. This is realized by utilizing zeroth-order (ZO) optimizers on large models and shared pseudo-random number generators (PRNG) across devices to represent the gradient estimates as seed-sign pairs. We conduct theoretical analysis on FeedSign and show that it converges at an exponential rate $\mathcal{O}(e^{-t})$, where $t$ is the number of elapsed steps under widely used assumptions. Moreover, FeedSign is found to be robust against data heterogeneity and Byzantine attacks. We conducted extensive experiments on models across different structures and sizes (11M to 13B) and found that the proposed method performs better or closely, depending on scenarios, compared to its ZO and FO counterparts, albeit with an orders-of-magnitude lower communication overhead. We also discuss some interesting advantages as byproducts guaranteed by the minimalistic design of \textit{FeedSign}.

**Link**: [arxiv](http://arxiv.org/abs/2501.17610v2),  [pdf](http://arxiv.org/pdf/2501.17610v2)

**Tags**: cs.DC 



### Evaluating small vision-language models as AI assistants for radio   astronomical source analysis tasks
**Authors**: S. Riggi, T. Cecconello, A. Pilzer, S. Palazzo, N. Gupta, A. M. Hopkins, C. Trigilio, G. Umana

**Updated**: 2025-03-31T09:06:23Z

**Summary**: The advent of next-generation radio telescopes is set to transform radio astronomy by producing massive data volumes that challenge traditional processing methods. Deep learning techniques have shown strong potential in automating radio analysis tasks, yet are often constrained by the limited availability of large annotated datasets. Recent progress in self-supervised learning has led to foundational radio vision models, but adapting them for new tasks typically requires coding expertise, limiting their accessibility to a broader astronomical community. Text-based AI interfaces offer a promising alternative by enabling task-specific queries and example-driven learning. In this context, Large Language Models (LLMs), with their remarkable zero-shot capabilities, are increasingly used in scientific domains. However, deploying large-scale models remains resource-intensive, and there is a growing demand for AI systems that can reason over both visual and textual data in astronomical analysis. This study explores small-scale Vision-Language Models (VLMs) as AI assistants for radio astronomy, combining LLM capabilities with vision transformers. We fine-tuned the LLaVA VLM on a dataset of 59k radio images from multiple surveys, enriched with 38k image-caption pairs from the literature. The fine-tuned models show clear improvements over base models in radio-specific tasks, achieving ~30% F1-score gains in extended source detection, but they underperform pure vision models and exhibit ~20% drop on general multimodal tasks. Inclusion of caption data and LoRA fine-tuning enhances instruction-following and helps recover ~10% accuracy on standard benchmarks. This work lays the foundation for future advancements in radio VLMs, highlighting their potential and limitations, such as the need for better multimodal alignment, higher-quality datasets, and mitigation of catastrophic forgetting.

**Link**: [arxiv](http://arxiv.org/abs/2503.23859v1),  [pdf](http://arxiv.org/pdf/2503.23859v1)

**Tags**: astro-ph.IM 



## Keyword: LLM Deployment 
 ### Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for   Large Language Models
**Authors**: Rui Wang, Hongru Wang, Boyang Xue, Jianhui Pang, Shudong Liu, Yi Chen, Jiahao Qiu, Derek Fai Wong, Heng Ji, Kam-Fai Wong

**Updated**: 2025-03-31T17:58:07Z

**Summary**: Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to perform complex reasoning tasks, transitioning from fast and intuitive thinking (System 1) to slow and deep reasoning (System 2). While System 2 reasoning improves task accuracy, it often incurs substantial computational costs due to its slow thinking nature and inefficient or unnecessary reasoning behaviors. In contrast, System 1 reasoning is computationally efficient but leads to suboptimal performance. Consequently, it is critical to balance the trade-off between performance (benefits) and computational costs (budgets), giving rise to the concept of reasoning economy. In this survey, we provide a comprehensive analysis of reasoning economy in both the post-training and test-time inference stages of LLMs, encompassing i) the cause of reasoning inefficiency, ii) behavior analysis of different reasoning patterns, and iii) potential solutions to achieve reasoning economy. By offering actionable insights and highlighting open challenges, we aim to shed light on strategies for improving the reasoning economy of LLMs, thereby serving as a valuable resource for advancing research in this evolving area. We also provide a public repository to continually track developments in this fast-evolving field.

**Link**: [arxiv](http://arxiv.org/abs/2503.24377v1),  [pdf](http://arxiv.org/pdf/2503.24377v1)

**Tags**: cs.CL cs.AI 



### Privacy-Preserving Secure Neighbor Discovery for Wireless Networks
**Authors**: Ahmed Mohamed Hussain, Panos Papadimitratos

**Updated**: 2025-03-31T17:56:29Z

**Summary**: Traditional Neighbor Discovery (ND) and Secure Neighbor Discovery (SND) are key elements for network functionality. SND is a hard problem, satisfying not only typical security properties (authentication, integrity) but also verification of direct communication, which involves distance estimation based on time measurements and device coordinates. Defeating relay attacks, also known as "wormholes", leading to stealthy Byzantine links and significant degradation of communication and adversarial control, is key in many wireless networked systems. However, SND is not concerned with privacy; it necessitates revealing the identity and location of the device(s) participating in the protocol execution. This can be a deterrent for deployment, especially involving user-held devices in the emerging Internet of Things (IoT) enabled smart environments. To address this challenge, we present a novel Privacy-Preserving Secure Neighbor Discovery (PP-SND) protocol, enabling devices to perform SND without revealing their actual identities and locations, effectively decoupling discovery from the exposure of sensitive information. We use Homomorphic Encryption (HE) for computing device distances without revealing their actual coordinates, as well as employing a pseudonymous device authentication to hide identities while preserving communication integrity. PP-SND provides SND [1] along with pseudonymity, confidentiality, and unlinkability. Our presentation here is not specific to one wireless technology, and we assess the performance of the protocols (cryptographic overhead) on a Raspberry Pi 4 and provide a security and privacy analysis.

**Link**: [arxiv](http://arxiv.org/abs/2503.22232v2),  [pdf](http://arxiv.org/pdf/2503.22232v2)

**Tags**: cs.CR cs.NI 



### EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues
**Authors**: Yuhan Liu, Yunbo Long

**Updated**: 2025-03-31T17:55:35Z

**Summary**: While large language model (LLM)-based chatbots have been applied for effective engagement in credit dialogues, their capacity for dynamic emotional expression remains limited. Current agents primarily rely on passive empathy rather than affective reasoning. For instance, when faced with persistent client negativity, the agent should employ strategic emotional adaptation by expressing measured anger to discourage counterproductive behavior and guide the conversation toward resolution. This context-aware emotional modulation is essential for imitating the nuanced decision-making of human negotiators. This paper introduces an EQ-negotiator that combines emotion sensing from pre-trained language models (PLMs) with emotional reasoning based on Game Theory and Hidden Markov Models. It takes into account both the current and historical emotions of the client to better manage and address negative emotions during interactions. By fine-tuning pre-trained language models (PLMs) on public emotion datasets and validating them on the credit dialogue datasets, our approach enables LLM-based agents to effectively capture shifts in client emotions and dynamically adjust their response tone based on our emotion decision policies in real-world financial negotiations. This EQ-negotiator can also help credit agencies foster positive client relationships, enhancing satisfaction in credit services.

**Link**: [arxiv](http://arxiv.org/abs/2503.21080v3),  [pdf](http://arxiv.org/pdf/2503.21080v3)

**Tags**: cs.CL 



### Exploring the Effect of Reinforcement Learning on Video Understanding:   Insights from SEED-Bench-R1
**Authors**: Yi Chen, Yuying Ge, Rui Wang, Yixiao Ge, Lu Qiu, Ying Shan, Xihui Liu

**Updated**: 2025-03-31T17:55:23Z

**Summary**: Recent advancements in Chain of Thought (COT) generation have significantly improved the reasoning capabilities of Large Language Models (LLMs), with reinforcement learning (RL) emerging as an effective post-training approach. Multimodal Large Language Models (MLLMs) inherit this reasoning potential but remain underexplored in tasks requiring both perception and logical reasoning. To address this, we introduce SEED-Bench-R1, a benchmark designed to systematically evaluate post-training methods for MLLMs in video understanding. It includes intricate real-world videos and complex everyday planning tasks in the format of multiple-choice questions, requiring sophisticated perception and reasoning. SEED-Bench-R1 assesses generalization through a three-level hierarchy: in-distribution, cross-environment, and cross-environment-task scenarios, equipped with a large-scale training dataset with easily verifiable ground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL with supervised fine-tuning (SFT), demonstrating RL's data efficiency and superior performance on both in-distribution and out-of-distribution tasks, even outperforming SFT on general video understanding benchmarks like LongVideoBench. Our detailed analysis reveals that RL enhances visual perception but often produces less logically coherent reasoning chains. We identify key limitations such as inconsistent reasoning and overlooked visual cues, and suggest future improvements in base model reasoning, reward modeling, and RL robustness against noisy signals.

**Link**: [arxiv](http://arxiv.org/abs/2503.24376v1),  [pdf](http://arxiv.org/pdf/2503.24376v1)

**Tags**: cs.CV cs.AI cs.CL cs.LG 



### Effectively Controlling Reasoning Models through Thinking Intervention
**Authors**: Tong Wu, Chong Xiang, Jiachen T. Wang, Prateek Mittal

**Updated**: 2025-03-31T17:50:13Z

**Summary**: Reasoning-enhanced large language models (LLMs) explicitly generate intermediate reasoning steps prior to generating final answers, helping the model excel in complex problem-solving. In this paper, we demonstrate that this emerging generation framework offers a unique opportunity for more fine-grained control over model behavior. We propose Thinking Intervention, a novel paradigm designed to explicitly guide the internal reasoning processes of LLMs by strategically inserting or revising specific thinking tokens. We conduct comprehensive evaluations across multiple tasks, including instruction following on IFEval, instruction hierarchy on SEP, and safety alignment on XSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention significantly outperforms baseline prompting approaches, achieving up to 6.7% accuracy gains in instruction-following scenarios, 15.4% improvements in reasoning about instruction hierarchies, and a 40.0% increase in refusal rates for unsafe prompts using open-source DeepSeek R1 models. Overall, our work opens a promising new research avenue for controlling reasoning LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2503.24370v1),  [pdf](http://arxiv.org/pdf/2503.24370v1)

**Tags**: cs.LG cs.AI cs.CL 



### SQuat: Subspace-orthogonal KV Cache Quantization
**Authors**: Hao Wang, Ligong Han, Kai Xu, Akash Srivastava

**Updated**: 2025-03-31T17:37:32Z

**Summary**: The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from previously generated tokens. It reduces redundant computation at the cost of increased memory usage. To mitigate this overhead, existing approaches compress KV tensors into lower-bit representations; however, quantization errors can accumulate as more tokens are generated, potentially resulting in undesired outputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache quantization). It first constructs a subspace spanned by query tensors to capture the most critical task-related information. During key tensor quantization, it enforces that the difference between the (de)quantized and original keys remains orthogonal to this subspace, minimizing the impact of quantization errors on the attention mechanism's outputs. SQuat requires no model fine-tuning, no additional calibration dataset for offline learning, and is grounded in a theoretical framework we develop. Through numerical experiments, we show that our method reduces peak memory by 2.17 to 2.82, improves throughput by 2.45 to 3.60, and achieves more favorable benchmark scores than existing KV cache quantization algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2503.24358v1),  [pdf](http://arxiv.org/pdf/2503.24358v1)

**Tags**: cs.LG cs.AI cs.CL cs.IT math.IT 



### Performance Analysis and Industry Deployment of Post-Quantum   Cryptography Algorithms
**Authors**: Elif Dicle Demir, Buse Bilgin, Mehmet Cengiz Onbasli

**Updated**: 2025-03-31T17:36:36Z

**Summary**: As quantum computing advances, modern cryptographic standards face an existential threat, necessitating a transition to post-quantum cryptography (PQC). The National Institute of Standards and Technology (NIST) has selected CRYSTALS-Kyber and CRYSTALS-Dilithium as standardized PQC algorithms for secure key exchange and digital signatures, respectively. This study conducts a comprehensive performance analysis of these algorithms by benchmarking execution times across cryptographic operations such as key generation, encapsulation, decapsulation, signing, and verification. Additionally, the impact of AVX2 optimizations is evaluated to assess hardware acceleration benefits. Our findings demonstrate that Kyber and Dilithium achieve efficient execution times, outperforming classical cryptographic schemes such as RSA and ECDSA at equivalent security levels. Beyond technical performance, the real-world deployment of PQC introduces challenges in telecommunications networks, where large-scale infrastructure upgrades, interoperability with legacy systems, and regulatory constraints must be addressed. This paper examines the feasibility of PQC adoption in telecom environments, highlighting key transition challenges, security risks, and implementation strategies. Through industry case studies, we illustrate how telecom operators are integrating PQC into 5G authentication, subscriber identity protection, and secure communications. Our analysis provides insights into the computational trade-offs, deployment considerations, and standardization efforts shaping the future of quantum-safe cryptographic infrastructure.

**Link**: [arxiv](http://arxiv.org/abs/2503.12952v2),  [pdf](http://arxiv.org/pdf/2503.12952v2)

**Tags**: cs.CR 68 E.3 



### ORAL: Prompting Your Large-Scale LoRAs via Conditional Recurrent   Diffusion
**Authors**: Rana Muhammad Shahroz Khan, Dongwen Tang, Pingzhi Li, Kai Wang, Tianlong Chen

**Updated**: 2025-03-31T17:34:59Z

**Summary**: Parameter generation has emerged as a novel paradigm for neural network development, offering an alternative to traditional neural network training by synthesizing high-quality model weights directly. In the context of Low-Rank Adaptation (LoRA) for evolving ($\textit{i.e.}$, constantly updated) large language models (LLMs), this approach promises efficient adaptation without costly retraining. However, existing methods face critical limitations in simultaneously achieving scalability and controllability. In this paper, we introduce $\texttt{ORAL}$, a novel $\textbf{conditional recurrent diffusion}$ framework that addresses these challenges. $\texttt{ORAL}$ incorporates a novel conditioning mechanism that integrates model architecture and textual task specifications, enabling the generation of task-specific LoRA parameters that can seamlessly transfer across evolving foundation models. Our approach successfully scales to billions-of-parameter LLMs and maintains controllability. Through extensive experiments across seven language tasks, four vision tasks, and three multimodal tasks using five pre-trained LLMs, we demonstrate that $\texttt{ORAL}$ generates high-quality LoRA parameters that achieve comparable or superior performance to vanilla trained counterparts.

**Link**: [arxiv](http://arxiv.org/abs/2503.24354v1),  [pdf](http://arxiv.org/pdf/2503.24354v1)

**Tags**: cs.LG cs.AI cs.CL cs.CV 



### Entanglement Distribution in Lossy Quantum Networks
**Authors**: Leonardo Oleynik, Junaid ur Rehman, Seid Koudia, Symeon Chatzinotas

**Updated**: 2025-03-31T17:32:18Z

**Summary**: Entanglement distribution is essential for unlocking the potential of distributed quantum information processing. We consider an $N$-partite network where entanglement is distributed via a central source over lossy channels, and network participants cooperate to establish entanglement between any two chosen parties under local operations and classical communication (LOCC) constraints. We develop a general mathematical framework to assess the optimal average bipartite entanglement shared in a lossy distribution, and introduce a tractable lower bound by optimizing over a subset of single-parameter LOCC transformations. Our results show that probabilistically extracting Bell pairs from W states is more advantageous than deterministically extracting them from GHZ-like states in lossy networks, with this advantage increasing with network size. We further extend our analysis analytically, proving that W states remain more effective in large-scale networks. These findings offer valuable insights into the practical deployment of near-term networks, revealing a fundamental trade-off between deterministic entanglement distribution protocols and loss-sensitive resources.

**Link**: [arxiv](http://arxiv.org/abs/2503.24347v1),  [pdf](http://arxiv.org/pdf/2503.24347v1)

**Tags**: quant-ph 



### BEATS: Bias Evaluation and Assessment Test Suite for Large Language   Models
**Authors**: Alok Abhishek, Lisa Erickson, Tushar Bandopadhyay

**Updated**: 2025-03-31T16:56:52Z

**Summary**: In this research, we introduce BEATS, a novel framework for evaluating Bias, Ethics, Fairness, and Factuality in Large Language Models (LLMs). Building upon the BEATS framework, we present a bias benchmark for LLMs that measure performance across 29 distinct metrics. These metrics span a broad range of characteristics, including demographic, cognitive, and social biases, as well as measures of ethical reasoning, group fairness, and factuality related misinformation risk. These metrics enable a quantitative assessment of the extent to which LLM generated responses may perpetuate societal prejudices that reinforce or expand systemic inequities. To achieve a high score on this benchmark a LLM must show very equitable behavior in their responses, making it a rigorous standard for responsible AI evaluation. Empirical results based on data from our experiment show that, 37.65\% of outputs generated by industry leading models contained some form of bias, highlighting a substantial risk of using these models in critical decision making systems. BEATS framework and benchmark offer a scalable and statistically rigorous methodology to benchmark LLMs, diagnose factors driving biases, and develop mitigation strategies. With the BEATS framework, our goal is to help the development of more socially responsible and ethically aligned AI models.

**Link**: [arxiv](http://arxiv.org/abs/2503.24310v1),  [pdf](http://arxiv.org/pdf/2503.24310v1)

**Tags**: cs.CL cs.AI 68T01 (Primary), 68T50 (Secondary) I.2.0; I.2.7 



### A Systematic Evaluation of LLM Strategies for Mental Health Text   Analysis: Fine-tuning vs. Prompt Engineering vs. RAG
**Authors**: Arshia Kermani, Veronica Perez-Rosas, Vangelis Metsis

**Updated**: 2025-03-31T16:54:04Z

**Summary**: This study presents a systematic comparison of three approaches for the analysis of mental health text using large language models (LLMs): prompt engineering, retrieval augmented generation (RAG), and fine-tuning. Using LLaMA 3, we evaluate these approaches on emotion classification and mental health condition detection tasks across two datasets. Fine-tuning achieves the highest accuracy (91% for emotion classification, 80% for mental health conditions) but requires substantial computational resources and large training sets, while prompt engineering and RAG offer more flexible deployment with moderate performance (40-68% accuracy). Our findings provide practical insights for implementing LLM-based solutions in mental health applications, highlighting the trade-offs between accuracy, computational requirements, and deployment flexibility.

**Link**: [arxiv](http://arxiv.org/abs/2503.24307v1),  [pdf](http://arxiv.org/pdf/2503.24307v1)

**Tags**: cs.CL cs.AI cs.IR cs.LG 



### Is analogy enough to draw novel adjective-noun inferences?
**Authors**: Hayley Ross, Kathryn Davidson, Najoung Kim

**Updated**: 2025-03-31T16:41:16Z

**Summary**: Recent work (Ross et al., 2025, 2024) has argued that the ability of humans and LLMs respectively to generalize to novel adjective-noun combinations shows that they each have access to a compositional mechanism to determine the phrase's meaning and derive inferences. We study whether these inferences can instead be derived by analogy to known inferences, without need for composition. We investigate this by (1) building a model of analogical reasoning using similarity over lexical items, and (2) asking human participants to reason by analogy. While we find that this strategy works well for a large proportion of the dataset of Ross et al. (2025), there are novel combinations for which both humans and LLMs derive convergent inferences but which are not well handled by analogy. We thus conclude that the mechanism humans and LLMs use to generalize in these cases cannot be fully reduced to analogy, and likely involves composition.

**Link**: [arxiv](http://arxiv.org/abs/2503.24293v1),  [pdf](http://arxiv.org/pdf/2503.24293v1)

**Tags**: cs.CL 



### Rec-R1: Bridging Generative Large Language Models and User-Centric   Recommendation Systems via Reinforcement Learning
**Authors**: Jiacheng Lin, Tian Wang, Kun Qian

**Updated**: 2025-03-31T16:36:00Z

**Summary**: We propose Rec-R1, a general reinforcement learning framework that bridges large language models (LLMs) with recommendation systems through closed-loop optimization. Unlike prompting and supervised fine-tuning (SFT), Rec-R1 directly optimizes LLM generation using feedback from a fixed black-box recommendation model, without relying on synthetic SFT data from proprietary models such as GPT-4o. This avoids the substantial cost and effort required for data distillation. To verify the effectiveness of Rec-R1, we evaluate it on two representative tasks: product search and sequential recommendation. Experimental results demonstrate that Rec-R1 not only consistently outperforms prompting- and SFT-based methods, but also achieves significant gains over strong discriminative baselines, even when used with simple retrievers such as BM25. Moreover, Rec-R1 preserves the general-purpose capabilities of the LLM, unlike SFT, which often impairs instruction-following and reasoning. These findings suggest Rec-R1 as a promising foundation for continual task-specific adaptation without catastrophic forgetting.

**Link**: [arxiv](http://arxiv.org/abs/2503.24289v1),  [pdf](http://arxiv.org/pdf/2503.24289v1)

**Tags**: cs.IR cs.CL 



### How Does A Text Preprocessing Pipeline Affect Ontology Syntactic   Matching?
**Authors**: Zhangcheng Qiang, Kerry Taylor, Weiqing Wang

**Updated**: 2025-03-31T16:35:00Z

**Summary**: The classic text preprocessing pipeline, comprising Tokenisation, Normalisation, Stop Words Removal, and Stemming/Lemmatisation, has been implemented in many systems for syntactic ontology matching (OM). However, the lack of standardisation in text preprocessing creates diversity in mapping results. In this paper we investigate the effect of the text preprocessing pipeline on syntactic OM in 8 Ontology Alignment Evaluation Initiative (OAEI) tracks with 49 distinct alignments. We find that Phase 1 text preprocessing (Tokenisation and Normalisation) is more effective than Phase 2 text preprocessing (Stop Words Removal and Stemming/Lemmatisation). To repair the unwanted false mappings caused by Phase 2 text preprocessing, we propose a novel context-based pipeline repair approach that employs a post hoc check to find common words that cause false mappings. These words are stored in a reserved word set and applied in text preprocessing. The experimental results show that our approach improves the matching correctness and the overall matching performance. We then consider the broader integration of the classic text preprocessing pipeline with modern large language models (LLMs) for OM. We recommend that (1) the text preprocessing pipeline be injected via function calling into LLMs to avoid the tendency towards unstable true mappings produced by LLM prompting; or (2) LLMs be used to repair non-existent and counter-intuitive false mappings generated by the text preprocessing pipeline.

**Link**: [arxiv](http://arxiv.org/abs/2411.03962v5),  [pdf](http://arxiv.org/pdf/2411.03962v5)

**Tags**: cs.CL 



### PharmAgents: Building a Virtual Pharma with Large Language Model Agents
**Authors**: Bowen Gao, Yanwen Huang, Yiqiao Liu, Wenxuan Xie, Wei-Ying Ma, Ya-Qin Zhang, Yanyan Lan

**Updated**: 2025-03-31T16:26:42Z

**Summary**: The discovery of novel small molecule drugs remains a critical scientific challenge with far-reaching implications for treating diseases and advancing human health. Traditional drug development--especially for small molecule therapeutics--is a highly complex, resource-intensive, and time-consuming process that requires multidisciplinary collaboration. Recent breakthroughs in artificial intelligence (AI), particularly the rise of large language models (LLMs), present a transformative opportunity to streamline and accelerate this process. In this paper, we introduce PharmAgents, a virtual pharmaceutical ecosystem driven by LLM-based multi-agent collaboration. PharmAgents simulates the full drug discovery workflow--from target discovery to preclinical evaluation--by integrating explainable, LLM-driven agents equipped with specialized machine learning models and computational tools. Through structured knowledge exchange and automated optimization, PharmAgents identifies potential therapeutic targets, discovers promising lead compounds, enhances binding affinity and key molecular properties, and performs in silico analyses of toxicity and synthetic feasibility. Additionally, the system supports interpretability, agent interaction, and self-evolvement, enabling it to refine future drug designs based on prior experience. By showcasing the potential of LLM-powered multi-agent systems in drug discovery, this work establishes a new paradigm for autonomous, explainable, and scalable pharmaceutical research, with future extensions toward comprehensive drug lifecycle management.

**Link**: [arxiv](http://arxiv.org/abs/2503.22164v2),  [pdf](http://arxiv.org/pdf/2503.22164v2)

**Tags**: q-bio.BM cs.AI 



### Evaluating and Designing Sparse Autoencoders by Approximating   Quasi-Orthogonality
**Authors**: Sewoong Lee, Adam Davies, Marc E. Canby, Julia Hockenmaier

**Updated**: 2025-03-31T16:22:11Z

**Summary**: Sparse autoencoders (SAEs) have emerged as a workhorse of modern mechanistic interpretability, but leading SAE approaches with top-$k$ style activation functions lack theoretical grounding for selecting the hyperparameter $k$. SAEs are based on the linear representation hypothesis (LRH), which assumes that the representations of large language models (LLMs) are linearly encoded, and the superposition hypothesis (SH), which states that there can be more features in the model than its dimensionality. We show that, based on the formal definitions of the LRH and SH, the magnitude of sparse feature vectors (the latent representations learned by SAEs of the dense embeddings of LLMs) can be approximated using their corresponding dense vector with a closed-form error bound. To visualize this, we propose the ZF plot, which reveals a previously unknown relationship between LLM hidden embeddings and SAE feature vectors, allowing us to make the first empirical measurement of the extent to which feature vectors of pre-trained SAEs are over- or under-activated for a given input. Correspondingly, we introduce Approximate Feature Activation (AFA), which approximates the magnitude of the ground-truth sparse feature vector, and propose a new evaluation metric derived from AFA to assess the alignment between inputs and activations. We also leverage AFA to introduce a novel SAE architecture, the top-AFA SAE, leading to SAEs that: (a) are more in line with theoretical justifications; and (b) obviate the need to tune SAE sparsity hyperparameters. Finally, we empirically demonstrate that top-AFA SAEs achieve reconstruction loss comparable to that of state-of-the-art top-k SAEs, without requiring the hyperparameter $k$ to be tuned. Our code is available at: https://github.com/SewoongLee/top-afa-sae.

**Link**: [arxiv](http://arxiv.org/abs/2503.24277v1),  [pdf](http://arxiv.org/pdf/2503.24277v1)

**Tags**: cs.LG cs.AI 



### New Statistical Framework for Extreme Error Probability in High-Stakes   Domains for Reliable Machine Learning
**Authors**: Umberto Michelucci, Francesca Venturini

**Updated**: 2025-03-31T16:08:11Z

**Summary**: Machine learning is vital in high-stakes domains, yet conventional validation methods rely on averaging metrics like mean squared error (MSE) or mean absolute error (MAE), which fail to quantify extreme errors. Worst-case prediction failures can have substantial consequences, but current frameworks lack statistical foundations for assessing their probability. In this work a new statistical framework, based on Extreme Value Theory (EVT), is presented that provides a rigorous approach to estimating worst-case failures. Applying EVT to synthetic and real-world datasets, this method is shown to enable robust estimation of catastrophic failure probabilities, overcoming the fundamental limitations of standard cross-validation. This work establishes EVT as a fundamental tool for assessing model reliability, ensuring safer AI deployment in new technologies where uncertainty quantification is central to decision-making or scientific analysis.

**Link**: [arxiv](http://arxiv.org/abs/2503.24262v1),  [pdf](http://arxiv.org/pdf/2503.24262v1)

**Tags**: cs.LG cs.AI stat.ME stat.ML 



### CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards   CWE Detection
**Authors**: Richard A. Dubniczky, Krisztofer ZoltÃ¡n HorvÃ¡t, TamÃ¡s Bisztray, Mohamed Amine Ferrag, Lucas C. Cordeiro, Norbert Tihanyi

**Updated**: 2025-03-31T16:07:10Z

**Summary**: Identifying vulnerabilities in source code is crucial, especially in critical software components. Existing methods such as static analysis, dynamic analysis, formal verification, and recently Large Language Models are widely used to detect security flaws. This paper introduces CASTLE (CWE Automated Security Testing and Low-Level Evaluation), a benchmarking framework for evaluating the vulnerability detection capabilities of different methods. We assess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using a hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs. We propose the CASTLE Score, a novel evaluation metric to ensure fair comparison. Our results reveal key differences: ESBMC (a formal verification tool) minimizes false positives but struggles with vulnerabilities beyond model checking, such as weak cryptography or SQL injection. Static analyzers suffer from high false positives, increasing manual validation efforts for developers. LLMs perform exceptionally well in the CASTLE dataset when identifying vulnerabilities in small code snippets. However, their accuracy declines, and hallucinations increase as the code size grows. These results suggest that LLMs could play a pivotal role in future security solutions, particularly within code completion frameworks, where they can provide real-time guidance to prevent vulnerabilities. The dataset is accessible at https://github.com/CASTLE-Benchmark.

**Link**: [arxiv](http://arxiv.org/abs/2503.09433v2),  [pdf](http://arxiv.org/pdf/2503.09433v2)

**Tags**: cs.CR cs.AI cs.SE 



### Enhancing Large Language Models (LLMs) for Telecommunications using   Knowledge Graphs and Retrieval-Augmented Generation
**Authors**: Dun Yuan, Hao Zhou, Di Wu, Xue Liu, Hao Chen, Yan Xin, Jianzhong, Zhang

**Updated**: 2025-03-31T15:58:08Z

**Summary**: Large language models (LLMs) have made significant progress in general-purpose natural language processing tasks. However, LLMs are still facing challenges when applied to domain-specific areas like telecommunications, which demands specialized expertise and adaptability to evolving standards. This paper presents a novel framework that combines knowledge graph (KG) and retrieval-augmented generation (RAG) techniques to enhance LLM performance in the telecom domain. The framework leverages a KG to capture structured, domain-specific information about network protocols, standards, and other telecom-related entities, comprehensively representing their relationships. By integrating KG with RAG, LLMs can dynamically access and utilize the most relevant and up-to-date knowledge during response generation. This hybrid approach bridges the gap between structured knowledge representation and the generative capabilities of LLMs, significantly enhancing accuracy, adaptability, and domain-specific comprehension. Our results demonstrate the effectiveness of the KG-RAG framework in addressing complex technical queries with precision. The proposed KG-RAG model attained an accuracy of 88% for question answering tasks on a frequently used telecom-specific dataset, compared to 82% for the RAG-only and 48% for the LLM-only approaches.

**Link**: [arxiv](http://arxiv.org/abs/2503.24245v1),  [pdf](http://arxiv.org/pdf/2503.24245v1)

**Tags**: cs.CL 



### What, How, Where, and How Well? A Survey on Test-Time Scaling in Large   Language Models
**Authors**: Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Yufei Wang, Irwin King, Xue Liu, Chen Ma

**Updated**: 2025-03-31T15:46:15Z

**Summary**: As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q&A. However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering a systemic understanding. To fill this gap, we propose a unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions.

**Link**: [arxiv](http://arxiv.org/abs/2503.24235v1),  [pdf](http://arxiv.org/pdf/2503.24235v1)

**Tags**: cs.CL cs.AI 



### PAARS: Persona Aligned Agentic Retail Shoppers
**Authors**: Saab Mansour, Leonardo Perelli, Lorenzo Mainetti, George Davidson, Stefano D'Amato

**Updated**: 2025-03-31T15:41:51Z

**Summary**: In e-commerce, behavioral data is collected for decision making which can be costly and slow. Simulation with LLM powered agents is emerging as a promising alternative for representing human population behavior. However, LLMs are known to exhibit certain biases, such as brand bias, review rating bias and limited representation of certain groups in the population, hence they need to be carefully benchmarked and aligned to user behavior. Ultimately, our goal is to synthesise an agent population and verify that it collectively approximates a real sample of humans. To this end, we propose a framework that: (i) creates synthetic shopping agents by automatically mining personas from anonymised historical shopping data, (ii) equips agents with retail-specific tools to synthesise shopping sessions and (iii) introduces a novel alignment suite measuring distributional differences between humans and shopping agents at the group (i.e. population) level rather than the traditional "individual" level. Experimental results demonstrate that using personas improves performance on the alignment suite, though a gap remains to human behaviour. We showcase an initial application of our framework for automated agentic A/B testing and compare the findings to human results. Finally, we discuss applications, limitations and challenges setting the stage for impactful future work.

**Link**: [arxiv](http://arxiv.org/abs/2503.24228v1),  [pdf](http://arxiv.org/pdf/2503.24228v1)

**Tags**: cs.AI cs.CL cs.MA 



### CASA: Class-Agnostic Shared Attributes in Vision-Language Models for   Efficient Incremental Object Detection
**Authors**: Mingyi Guo, Yuyang Liu, Zhiyuan Yan, Zongying Lin, Peixi Peng, Yonghong Tian

**Updated**: 2025-03-31T15:30:45Z

**Summary**: Incremental object detection is fundamentally challenged by catastrophic forgetting. A major factor contributing to this issue is background shift, where background categories in sequential tasks may overlap with either previously learned or future unseen classes. To address this, we propose a novel method called Class-Agnostic Shared Attribute Base (CASA) that encourages the model to learn category-agnostic attributes shared across incremental classes. Our approach leverages an LLM to generate candidate textual attributes, selects the most relevant ones based on the current training data, and records their importance in an assignment matrix. For subsequent tasks, the retained attributes are frozen, and new attributes are selected from the remaining candidates, ensuring both knowledge retention and adaptability. Extensive experiments on the COCO dataset demonstrate the state-of-the-art performance of our method.

**Link**: [arxiv](http://arxiv.org/abs/2410.05804v3),  [pdf](http://arxiv.org/pdf/2410.05804v3)

**Tags**: cs.CV 



### Surgical Action Planning with Large Language Models
**Authors**: Mengya Xu, Zhongzhen Huang, Jie Zhang, Xiaofan Zhang, Qi Dou

**Updated**: 2025-03-31T15:29:24Z

**Summary**: In robot-assisted minimally invasive surgery, we introduce the Surgical Action Planning (SAP) task, which generates future action plans from visual inputs to address the absence of intraoperative predictive planning in current intelligent applications. SAP shows great potential for enhancing intraoperative guidance and automating procedures. However, it faces challenges such as understanding instrument-action relationships and tracking surgical progress. Large Language Models (LLMs) show promise in understanding surgical video content but remain underexplored for predictive decision-making in SAP, as they focus mainly on retrospective analysis. Challenges like data privacy, computational demands, and modality-specific constraints further highlight significant research gaps. To tackle these challenges, we introduce LLM-SAP, a Large Language Models-based Surgical Action Planning framework that predicts future actions and generates text responses by interpreting natural language prompts of surgical goals. The text responses potentially support surgical education, intraoperative decision-making, procedure documentation, and skill analysis. LLM-SAP integrates two novel modules: the Near-History Focus Memory Module (NHF-MM) for modeling historical states and the prompts factory for action planning. We evaluate LLM-SAP on our constructed CholecT50-SAP dataset using models like Qwen2.5 and Qwen2-VL, demonstrating its effectiveness in next-action prediction. Pre-trained LLMs are tested in a zero-shot setting, and supervised fine-tuning (SFT) with LoRA is implemented. Our experiments show that Qwen2.5-72B-SFT surpasses Qwen2.5-72B with a 19.3% higher accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2503.18296v2),  [pdf](http://arxiv.org/pdf/2503.18296v2)

**Tags**: cs.CL 



### Synthetic News Generation for Fake News Classification
**Authors**: Abdul Sittar, Luka Golob, Mateja Smiljanic

**Updated**: 2025-03-31T15:24:05Z

**Summary**: This study explores the generation and evaluation of synthetic fake news through fact based manipulations using large language models (LLMs). We introduce a novel methodology that extracts key facts from real articles, modifies them, and regenerates content to simulate fake news while maintaining coherence. To assess the quality of the generated content, we propose a set of evaluation metrics coherence, dissimilarity, and correctness. The research also investigates the application of synthetic data in fake news classification, comparing traditional machine learning models with transformer based models such as BERT. Our experiments demonstrate that transformer models, especially BERT, effectively leverage synthetic data for fake news detection, showing improvements with smaller proportions of synthetic data. Additionally, we find that fact verification features, which focus on identifying factual inconsistencies, provide the most promising results in distinguishing synthetic fake news. The study highlights the potential of synthetic data to enhance fake news detection systems, offering valuable insights for future research and suggesting that targeted improvements in synthetic data generation can further strengthen detection models.

**Link**: [arxiv](http://arxiv.org/abs/2503.24206v1),  [pdf](http://arxiv.org/pdf/2503.24206v1)

**Tags**: cs.CL 



### TwT: Thinking without Tokens by Habitual Reasoning Distillation with   Multi-Teachers' Guidance
**Authors**: Jingxian Xu, Mengyu Zhou, Weichang Liu, Hanbing Liu, Shi Han, Dongmei Zhang

**Updated**: 2025-03-31T15:16:31Z

**Summary**: Large Language Models (LLMs) have made significant strides in problem-solving by incorporating reasoning processes. However, this enhanced reasoning capability results in an increased number of output tokens during inference, leading to higher computational costs. To address this challenge, we propose TwT (Thinking without Tokens), a method that reduces inference-time costs through habitual reasoning distillation with multi-teachers' guidance, while maintaining high performance. Our approach introduces a Habitual Reasoning Distillation method, which internalizes explicit reasoning into the model's habitual behavior through a Teacher-Guided compression strategy inspired by human cognition. Additionally, we propose Dual-Criteria Rejection Sampling (DCRS), a technique that generates a high-quality and diverse distillation dataset using multiple teacher models, making our method suitable for unsupervised scenarios. Experimental results demonstrate that TwT effectively reduces inference costs while preserving superior performance, achieving up to a 13.6% improvement in accuracy with fewer output tokens compared to other distillation methods, offering a highly practical solution for efficient LLM deployment.

**Link**: [arxiv](http://arxiv.org/abs/2503.24198v1),  [pdf](http://arxiv.org/pdf/2503.24198v1)

**Tags**: cs.CL 



### Text2Tracks: Prompt-based Music Recommendation via Generative Retrieval
**Authors**: Enrico Palumbo, Gustavo Penha, Andreas Damianou, JosÃ© Luis Redondo GarcÃ­a, Timothy Christopher Heath, Alice Wang, Hugues Bouchard, Mounia Lalmas

**Updated**: 2025-03-31T15:09:19Z

**Summary**: In recent years, Large Language Models (LLMs) have enabled users to provide highly specific music recommendation requests using natural language prompts (e.g. "Can you recommend some old classics for slow dancing?"). In this setup, the recommended tracks are predicted by the LLM in an autoregressive way, i.e. the LLM generates the track titles one token at a time. While intuitive, this approach has several limitation. First, it is based on a general purpose tokenization that is optimized for words rather than for track titles. Second, it necessitates an additional entity resolution layer that matches the track title to the actual track identifier. Third, the number of decoding steps scales linearly with the length of the track title, slowing down inference. In this paper, we propose to address the task of prompt-based music recommendation as a generative retrieval task. Within this setting, we introduce novel, effective, and efficient representations of track identifiers that significantly outperform commonly used strategies. We introduce Text2Tracks, a generative retrieval model that learns a mapping from a user's music recommendation prompt to the relevant track IDs directly. Through an offline evaluation on a dataset of playlists with language inputs, we find that (1) the strategy to create IDs for music tracks is the most important factor for the effectiveness of Text2Tracks and semantic IDs significantly outperform commonly used strategies that rely on song titles as identifiers (2) provided with the right choice of track identifiers, Text2Tracks outperforms sparse and dense retrieval solutions trained to retrieve tracks from language prompts.

**Link**: [arxiv](http://arxiv.org/abs/2503.24193v1),  [pdf](http://arxiv.org/pdf/2503.24193v1)

**Tags**: cs.IR 



### Output Constraints as Attack Surface: Exploiting Structured Generation   to Bypass LLM Safety Mechanisms
**Authors**: Shuoming Zhang, Jiacheng Zhao, Ruiyuan Xu, Xiaobing Feng, Huimin Cui

**Updated**: 2025-03-31T15:08:06Z

**Summary**: Content Warning: This paper may contain unsafe or harmful content generated by LLMs that may be offensive to readers. Large Language Models (LLMs) are extensively used as tooling platforms through structured output APIs to ensure syntax compliance so that robust integration with existing softwares like agent systems, could be achieved. However, the feature enabling functionality of grammar-guided structured output presents significant security vulnerabilities. In this work, we reveal a critical control-plane attack surface orthogonal to traditional data-plane vulnerabilities. We introduce Constrained Decoding Attack (CDA), a novel jailbreak class that weaponizes structured output constraints to bypass safety mechanisms. Unlike prior attacks focused on input prompts, CDA operates by embedding malicious intent in schema-level grammar rules (control-plane) while maintaining benign surface prompts (data-plane). We instantiate this with a proof-of-concept Chain Enum Attack, achieves 96.2% attack success rates across proprietary and open-weight LLMs on five safety benchmarks with a single query, including GPT-4o and Gemini-2.0-flash. Our findings identify a critical security blind spot in current LLM architectures and urge a paradigm shift in LLM safety to address control-plane vulnerabilities, as current mechanisms focused solely on data-plane threats leave critical systems exposed.

**Link**: [arxiv](http://arxiv.org/abs/2503.24191v1),  [pdf](http://arxiv.org/pdf/2503.24191v1)

**Tags**: cs.CR cs.AI 



### Cascade Reward Sampling for Efficient Decoding-Time Alignment
**Authors**: Bolian Li, Yifan Wang, Anamika Lochab, Ananth Grama, Ruqi Zhang

**Updated**: 2025-03-31T15:07:35Z

**Summary**: Aligning large language models (LLMs) with human preferences is essential for their applications. Recently, decoding-time alignment has emerged as an effective plug-and-play technique that avoids fine-tuning model parameters. This approach retains the general utility of pretrained LLMs but often suffers from significant inefficiencies during decoding, primarily due to wasted token generation and excessive reward evaluations. To address these challenges, we introduce Cascade Reward Sampling (CARDS) to resolve both efficiency bottlenecks in decoding-time alignment. Specifically, we develop a segment-level rejection sampling algorithm that minimizes redundant computations of both LLMs and reward models (RMs). Central to CARDS is an uncertainty-based segmentation mechanism, which ensures the accuracy of RMs evaluations on incomplete segments. Furthermore, we provide a detailed analysis of reward scores on segments to elucidate the improved alignment performance. Experimental results demonstrate that CARDS significantly improves decoding efficiency, alignment quality, and general utility compared to existing decoding-time alignment methods, achieving approximately a 70% reduction in decoding time and over 90% win-ties in utility and safety benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2406.16306v2),  [pdf](http://arxiv.org/pdf/2406.16306v2)

**Tags**: cs.CL cs.LG stat.ML 



### Implicit In-Context Learning: Evidence from Artificial Language   Experiments
**Authors**: Xiaomeng Ma, Qihui Xu

**Updated**: 2025-03-31T15:07:08Z

**Summary**: Humans acquire language through implicit learning, absorbing complex patterns without explicit awareness. While LLMs demonstrate impressive linguistic capabilities, it remains unclear whether they exhibit human-like pattern recognition during in-context learning at inferencing level. We adapted three classic artificial language learning experiments spanning morphology, morphosyntax, and syntax to systematically evaluate implicit learning at inferencing level in two state-of-the-art OpenAI models: gpt-4o and o3-mini. Our results reveal linguistic domain-specific alignment between models and human behaviors, o3-mini aligns better in morphology while both models align in syntax.

**Link**: [arxiv](http://arxiv.org/abs/2503.24190v1),  [pdf](http://arxiv.org/pdf/2503.24190v1)

**Tags**: cs.CL 



### LLM4FS: Leveraging Large Language Models for Feature Selection and How   to Improve It
**Authors**: Jianhao Li, Xianchao Xiu

**Updated**: 2025-03-31T14:40:31Z

**Summary**: Recent advances in large language models (LLMs) have provided new opportunities for decision-making, particularly in the task of automated feature selection. In this paper, we first comprehensively evaluate LLM-based feature selection methods, covering the state-of-the-art DeepSeek-R1, GPT-o3-mini, and GPT-4.5. Then, we propose a novel hybrid strategy called LLM4FS that integrates LLMs with traditional data-driven methods. Specifically, input data samples into LLMs, and directly call traditional data-driven techniques such as random forest and forward sequential selection. Notably, our analysis reveals that the hybrid strategy leverages the contextual understanding of LLMs and the high statistical reliability of traditional data-driven methods to achieve excellent feature selection performance, even surpassing LLMs and traditional data-driven methods. Finally, we point out the limitations of its application in decision-making.

**Link**: [arxiv](http://arxiv.org/abs/2503.24157v1),  [pdf](http://arxiv.org/pdf/2503.24157v1)

**Tags**: cs.LG 



### ScienceAgentBench: Toward Rigorous Assessment of Language Agents for   Data-Driven Scientific Discovery
**Authors**: Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N. Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun

**Updated**: 2025-03-31T14:39:44Z

**Summary**: The advancements of large language models (LLMs) have piqued growing interest in developing LLM-based language agents to automate scientific discovery end-to-end, which has sparked both excitement and skepticism about their true capabilities. In this work, we call for rigorous assessment of agents on individual tasks in a scientific workflow before making bold claims on end-to-end automation. To this end, we present ScienceAgentBench, a new benchmark for evaluating language agents for data-driven scientific discovery. To ensure the scientific authenticity and real-world relevance of our benchmark, we extract 102 tasks from 44 peer-reviewed publications in four disciplines and engage nine subject matter experts to validate them. We unify the target output for every task to a self-contained Python program file and employ an array of evaluation metrics to examine the generated programs, execution results, and costs. Each task goes through multiple rounds of manual validation by annotators and subject matter experts to ensure its annotation quality and scientific plausibility. We also propose two effective strategies to mitigate data contamination concerns. Using ScienceAgentBench, we evaluate five open-weight and proprietary LLMs, each with three frameworks: direct prompting, OpenHands CodeAct, and self-debug. Given three attempts for each task, the best-performing agent can only solve 32.4% of the tasks independently and 34.3% with expert-provided knowledge. In addition, we evaluate OpenAI o1-preview with direct prompting and self-debug, which can boost the performance to 42.2%, demonstrating the effectiveness of increasing inference-time compute but with more than 10 times the cost of other LLMs. Still, our results underscore the limitations of current language agents in generating code for data-driven discovery, let alone end-to-end automation for scientific research.

**Link**: [arxiv](http://arxiv.org/abs/2410.05080v3),  [pdf](http://arxiv.org/pdf/2410.05080v3)

**Tags**: cs.CL cs.AI cs.LG 



### Concept Navigation and Classification via Open-Source Large Language   Model Processing
**Authors**: MaÃ«l Kubli

**Updated**: 2025-03-31T14:37:40Z

**Summary**: This paper presents a novel methodological framework for detecting and classifying latent constructs, including frames, narratives, and topics, from textual data using Open-Source Large Language Models (LLMs). The proposed hybrid approach combines automated summarization with human-in-the-loop validation to enhance the accuracy and interpretability of construct identification. By employing iterative sampling coupled with expert refinement, the framework guarantees methodological robustness and ensures conceptual precision. Applied to diverse data sets, including AI policy debates, newspaper articles on encryption, and the 20 Newsgroups data set, this approach demonstrates its versatility in systematically analyzing complex political discourses, media framing, and topic classification tasks.

**Link**: [arxiv](http://arxiv.org/abs/2502.04756v2),  [pdf](http://arxiv.org/pdf/2502.04756v2)

**Tags**: cs.CL cs.AI cs.LG I.2.7 



### Trident: Interference Avoidance in Multi-reader Backscatter Network via   Frequency-space Division
**Authors**: Yang Zou, Xin Na, Yimiao Sun, Yuan He

**Updated**: 2025-03-31T14:33:18Z

**Summary**: Backscatter is a key technology for battery-free sensing in industrial IoT applications. To fully cover numerous tags in the deployment area, one often needs to deploy multiple readers, each of which communicates with tags within its communication range. However, the actual backscattered signals from a tag are likely to reach a reader outside its communication range and cause interference. Conventional TDMA or CSMA based approaches for interference avoidance separate readers' media access in time, leading to limited network throughput. In this paper, we propose TRIDENT, a novel backscatter design that enables interference avoidance via frequency-space division. By incorporating a tunable bandpass filter and multiple terminal loads, a TRIDENT tag can detect its channel condition and adaptively adjust the frequency and the power of its backscattered signals. We further propose a frequency assignment algorithm for the readers. With these designs, all the readers in the network can operate concurrently without being interfered. We implement TRIDENT and evaluate its performance under various settings. The results demonstrate that TRIDENT enhances the network throughput by 3.18x, compared to the TDMA-based scheme.

**Link**: [arxiv](http://arxiv.org/abs/2503.24148v1),  [pdf](http://arxiv.org/pdf/2503.24148v1)

**Tags**: cs.NI 



### Channel Estimation for Pinching-Antenna Systems (PASS)
**Authors**: Jian Xiao, Ji Wang, Yuanwei Liu

**Updated**: 2025-03-31T14:30:41Z

**Summary**: Pinching Antennas (PAs) represent a revolutionary flexible antenna technology that leverages dielectric waveguides and electromagnetic coupling to mitigate large-scale path loss. This letter is the first to explore channel estimation for Pinching-Antenna SyStems (PASS), addressing their uniquely ill-conditioned and underdetermined channel characteristics. In particular, two efficient deep learning-based channel estimators are proposed. 1) PAMoE: This estimator incorporates dynamic padding, feature embedding, fusion, and mixture of experts (MoE) modules, which effectively leverage the positional information of PAs and exploit expert diversity. 2) PAformer: This Transformer-style estimator employs the self-attention mechanism to predict channel coefficients in a per-antenna manner, which offers more flexibility to adaptively deal with dynamic numbers of PAs in practical deployment. Numerical results demonstrate that 1) the proposed deep learning-based channel estimators outperform conventional methods and exhibit excellent zero-shot learning capabilities, and 2) PAMoE delivers higher channel estimation accuracy via MoE specialization, while PAformer natively handles an arbitrary number of PAs, trading self-attention complexity for superior scalability.

**Link**: [arxiv](http://arxiv.org/abs/2503.13268v2),  [pdf](http://arxiv.org/pdf/2503.13268v2)

**Tags**: cs.IT eess.SP math.IT 



### PhD Knowledge Not Required: A Reasoning Challenge for Large Language   Models
**Authors**: Zixuan Wu, Francesca Lucchetti, Aleksander Boruch-Gruszecki, Jingmiao Zhao, Carolyn Jane Anderson, Joydeep Biswas, Federico Cassano, Molly Q Feldman, Arjun Guha

**Updated**: 2025-03-31T14:21:49Z

**Summary**: Existing benchmarks for frontier models often test specialized, "PhD-level" knowledge that is difficult for non-experts to grasp. In contrast, we present a benchmark with 594 problems based on the NPR Sunday Puzzle Challenge that requires only general knowledge. Our benchmark is challenging for both humans and models; however correct solutions are easy to verify, and models' mistakes are easy to spot. As LLMs are more widely deployed in society, we believe it is useful to develop benchmarks for frontier models that humans can understand without the need for deep domain expertise.   Our work reveals capability gaps that are not evident in existing benchmarks: OpenAI o1 significantly outperforms other reasoning models on our benchmark, despite being on par with other models when tested on benchmarks that test specialized knowledge. Furthermore, our analysis of reasoning outputs uncovers new kinds of failures. DeepSeek R1, for instance, often concedes with "I give up" before providing an answer that it knows is wrong. R1 can also be remarkably "uncertain" in its output and in rare cases, it does not "finish thinking," which suggests the need for techniques to "wrap up" before the context window limit is reached. We also quantify the effectiveness of reasoning longer to identify the point beyond which more reasoning is unlikely to improve accuracy on our benchmark.

**Link**: [arxiv](http://arxiv.org/abs/2502.01584v3),  [pdf](http://arxiv.org/pdf/2502.01584v3)

**Tags**: cs.AI cs.LG 



### TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud   Detection
**Authors**: Zhiming Ma, Peidong Wang, Minhua Huang, Jingpeng Wang, Kai Wu, Xiangzhao Lv, Yachun Pang, Yin Yang, Wenjie Tang, Yuchen Kang

**Updated**: 2025-03-31T14:06:17Z

**Summary**: The detection of telecom fraud faces significant challenges due to the lack of high-quality multimodal training data that integrates audio signals with reasoning-oriented textual analysis. To address this gap, we present TeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset specifically designed for automated telecom fraud analysis. Our dataset is constructed through three strategies: (1) Privacy-preserved text-truth sample generation using automatically speech recognition (ASR)-transcribed call recordings (with anonymized original audio), ensuring real-world consistency through text-to-speech (TTS) model regeneration; (2) Semantic enhancement via large language model (LLM)-based self-instruction sampling on authentic ASR outputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that simulates emerging fraud tactics through predefined communication scenarios and fraud typologies. The generated dataset contains 28,511 rigorously processed speech-text pairs, complete with detailed annotations for fraud reasoning. The dataset is divided into three tasks: scenario classification, fraud detection, fraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a standardized evaluation benchmark comprising proportionally sampled instances from the dataset, to facilitate systematic testing of model performance on telecom fraud detection tasks. We also contribute a production-optimized supervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while open-sourcing the data processing framework to enable community-driven dataset expansion. This work establishes a foundational framework for multimodal anti-fraud research while addressing critical challenges in data privacy and scenario diversity. The project will be released at https://github.com/JimmyMa99/TeleAntiFraud.

**Link**: [arxiv](http://arxiv.org/abs/2503.24115v1),  [pdf](http://arxiv.org/pdf/2503.24115v1)

**Tags**: cs.CL cs.MM 



### The Quantum Technology Job Market: Data Driven Analysis of 3641 Job   Posts
**Authors**: Simon Goorney, Eleni Karydi, Borja MuÃ±oz, Otto Santesson, Zeki Can Seskir, Ana Alina Tudoran, Jacob Sherson

**Updated**: 2025-03-31T14:03:13Z

**Summary**: The rapid advancement of Quantum Technology (QT) has created a growing demand for a specialized workforce, spanning across academia and industry. This study presents a quantitative analysis of the QT job market by systematically extracting and classifying thousands of job postings worldwide. The classification pipeline leverages large language models (LLMs) whilst incorporating a "human-in-the-loop" validation process to ensure reliability, achieving an F1-score of 89%: a high level of accuracy. The research identifies key trends in regional job distribution, degree and skill requirements, and the evolving demand for QT-related roles. Findings reveal a strong presence of the QT job market in the United States and Europe, with increasing corporate demand for engineers, software developers, and PhD-level researchers. Despite growing industry applications, the sector remains in its early stages, dominated by large technology firms and requiring significant investment in education and workforce development. The study highlights the need for targeted educational programs, interdisciplinary collaboration, and industry-academic partnerships to bridge the QT workforce gap.

**Link**: [arxiv](http://arxiv.org/abs/2503.19004v2),  [pdf](http://arxiv.org/pdf/2503.19004v2)

**Tags**: physics.ed-ph cs.CY quant-ph 



### Grounding Agent Reasoning in Image Schemas: A Neurosymbolic Approach to   Embodied Cognition
**Authors**: FranÃ§ois Olivier, Zied Bouraoui

**Updated**: 2025-03-31T14:01:39Z

**Summary**: Despite advances in embodied AI, agent reasoning systems still struggle to capture the fundamental conceptual structures that humans naturally use to understand and interact with their environment. To address this, we propose a novel framework that bridges embodied cognition theory and agent systems by leveraging a formal characterization of image schemas, which are defined as recurring patterns of sensorimotor experience that structure human cognition. By customizing LLMs to translate natural language descriptions into formal representations based on these sensorimotor patterns, we will be able to create a neurosymbolic system that grounds the agent's understanding in fundamental conceptual structures. We argue that such an approach enhances both efficiency and interpretability while enabling more intuitive human-agent interactions through shared embodied understanding.

**Link**: [arxiv](http://arxiv.org/abs/2503.24110v1),  [pdf](http://arxiv.org/pdf/2503.24110v1)

**Tags**: cs.AI cs.CL 



### AI in radiological imaging of soft-tissue and bone tumours: a systematic   review evaluating against CLAIM and FUTURE-AI guidelines
**Authors**: Douwe J. Spaanderman, Matthew Marzetti, Xinyi Wan, Andrew F. Scarsbrook, Philip Robinson, Edwin H. G. Oei, Jacob J. Visser, Robert Hemke, Kirsten van Langevelde, David F. Hanff, Geert J. L. H. van Leenders, Cornelis Verhoef, Dirk J. GruÃ¼hagen, Wiro J. Niessen, Stefan Klein, Martijn P. A. Starmans

**Updated**: 2025-03-31T13:58:36Z

**Summary**: Soft-tissue and bone tumours (STBT) are rare, diagnostically challenging lesions with variable clinical behaviours and treatment approaches. This systematic review provides an overview of Artificial Intelligence (AI) methods using radiological imaging for diagnosis and prognosis of these tumours, highlighting challenges in clinical translation, and evaluating study alignment with the Checklist for AI in Medical Imaging (CLAIM) and the FUTURE-AI international consensus guidelines for trustworthy and deployable AI to promote the clinical translation of AI methods. The review covered literature from several bibliographic databases, including papers published before 17/07/2024. Original research in peer-reviewed journals focused on radiology-based AI for diagnosing or prognosing primary STBT was included. Exclusion criteria were animal, cadaveric, or laboratory studies, and non-English papers. Abstracts were screened by two of three independent reviewers for eligibility. Eligible papers were assessed against guidelines by one of three independent reviewers. The search identified 15,015 abstracts, from which 325 articles were included for evaluation. Most studies performed moderately on CLAIM, averaging a score of 28.9$\pm$7.5 out of 53, but poorly on FUTURE-AI, averaging 5.1$\pm$2.1 out of 30. Imaging-AI tools for STBT remain at the proof-of-concept stage, indicating significant room for improvement. Future efforts by AI developers should focus on design (e.g. define unmet clinical need, intended clinical setting and how AI would be integrated in clinical workflow), development (e.g. build on previous work, explainability), evaluation (e.g. evaluating and addressing biases, evaluating AI against best practices), and data reproducibility and availability (making documented code and data publicly available). Following these recommendations could improve clinical translation of AI methods.

**Link**: [arxiv](http://arxiv.org/abs/2408.12491v2),  [pdf](http://arxiv.org/pdf/2408.12491v2)

**Tags**: cs.AI cs.LG 



### Is LLM the Silver Bullet to Low-Resource Languages Machine Translation?
**Authors**: Yewei Song, Lujun Li, Cedric Lothritz, Saad Ezzini, Lama Sleem, Niccolo Gentile, Radu State, TegawendÃ© F. BissyandÃ©, Jacques Klein

**Updated**: 2025-03-31T13:56:03Z

**Summary**: Low-Resource Languages (LRLs) present significant challenges in natural language processing due to their limited linguistic resources and underrepresentation in standard datasets. While recent advancements in Large Language Models (LLMs) and Neural Machine Translation (NMT) have substantially improved translation capabilities for high-resource languages, performance disparities persist for LRLs, particularly impacting privacy-sensitive and resource-constrained scenarios. This paper systematically evaluates the limitations of current LLMs across 200 languages using benchmarks such as FLORES-200. We also explore alternative data sources, including news articles and bilingual dictionaries, and demonstrate how knowledge distillation from large pre-trained models can significantly improve smaller LRL translations. Additionally, we investigate various fine-tuning strategies, revealing that incremental enhancements markedly reduce performance gaps on smaller LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2503.24102v1),  [pdf](http://arxiv.org/pdf/2503.24102v1)

**Tags**: cs.CL 



### Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-Oasis
**Authors**: Alessandro ScirÃ¨, Andrei Stefan Bejgu, Simone Tedeschi, Karim Ghonim, Federico Martelli, Roberto Navigli

**Updated**: 2025-03-31T13:55:07Z

**Summary**: After the introduction of Large Language Models (LLMs), there have been substantial improvements in the performance of Natural Language Generation (NLG) tasks, including Text Summarization and Machine Translation. However, LLMs still produce outputs containing hallucinations, that is, content not grounded in factual information. Therefore, developing methods to assess the factuality of LLMs has become urgent.   Indeed, resources for factuality evaluation have recently emerged. Although challenging, these resources face one or more of the following limitations: (i) they are tailored to a specific task or domain; (ii) they are limited in size, thereby preventing the training of new factuality evaluators; (iii) they are designed for simpler verification tasks, such as claim verification.   To address these issues, we introduce LLM-Oasis, to the best of our knowledge the largest resource for training end-to-end factuality evaluators. LLM-Oasis is constructed by extracting claims from Wikipedia, falsifying a subset of these claims, and generating pairs of factual and unfactual texts. We then rely on human annotators to both validate the quality of our dataset and to create a gold standard test set for benchmarking factuality evaluation systems.   Our experiments demonstrate that LLM-Oasis presents a significant challenge for state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in our proposed end-to-end factuality evaluation task, highlighting its potential to drive future research in the field.

**Link**: [arxiv](http://arxiv.org/abs/2411.19655v3),  [pdf](http://arxiv.org/pdf/2411.19655v3)

**Tags**: cs.CL 



### DANTE-AD: Dual-Vision Attention Network for Long-Term Audio Description
**Authors**: Adrienne Deganutti, Simon Hadfield, Andrew Gilbert

**Updated**: 2025-03-31T13:49:43Z

**Summary**: Audio Description is a narrated commentary designed to aid vision-impaired audiences in perceiving key visual elements in a video. While short-form video understanding has advanced rapidly, a solution for maintaining coherent long-term visual storytelling remains unresolved. Existing methods rely solely on frame-level embeddings, effectively describing object-based content but lacking contextual information across scenes. We introduce DANTE-AD, an enhanced video description model leveraging a dual-vision Transformer-based architecture to address this gap. DANTE-AD sequentially fuses both frame and scene level embeddings to improve long-term contextual understanding. We propose a novel, state-of-the-art method for sequential cross-attention to achieve contextual grounding for fine-grained audio description generation. Evaluated on a broad range of key scenes from well-known movie clips, DANTE-AD outperforms existing methods across traditional NLP metrics and LLM-based evaluations.

**Link**: [arxiv](http://arxiv.org/abs/2503.24096v1),  [pdf](http://arxiv.org/pdf/2503.24096v1)

**Tags**: cs.CV 



### Teola: Towards End-to-End Optimization of LLM-based Applications
**Authors**: Xin Tan, Yimin Jiang, Yitao Yang, Hong Xu

**Updated**: 2025-03-31T13:33:54Z

**Summary**: Large language model (LLM)-based applications consist of both LLM and non-LLM components, each contributing to the end-to-end latency. Despite great efforts to optimize LLM inference, end-to-end workflow optimization has been overlooked. Existing frameworks employ coarse-grained orchestration with task modules, which confines optimizations to within each module and yields suboptimal scheduling decisions. We propose fine-grained end-to-end orchestration, which utilizes task primitives as the basic units and represents each query's workflow as a primitive-level dataflow graph. This explicitly exposes a much larger design space, enables optimizations in parallelization and pipelining across primitives of different modules, and enhances scheduling to improve application-level performance. We build Teola, a novel orchestration framework for LLM-based applications that implements this scheme. Comprehensive experiments show that Teola can achieve up to 2.09x speedup over existing systems across various popular LLM applications. The code is available at https://github.com/NetX-lab/Ayo.

**Link**: [arxiv](http://arxiv.org/abs/2407.00326v3),  [pdf](http://arxiv.org/pdf/2407.00326v3)

**Tags**: cs.DC cs.AI cs.NI 



### Artificial Conversations, Real Results: Fostering Language Detection   with Synthetic Data
**Authors**: Fatemeh Mohammadi, Tommaso Romano, Samira Maghool, Paolo Ceravolo

**Updated**: 2025-03-31T13:22:34Z

**Summary**: Collecting high-quality training data is essential for fine-tuning Large Language Models (LLMs). However, acquiring such data is often costly and time-consuming, especially for non-English languages such as Italian. Recently, researchers have begun to explore the use of LLMs to generate synthetic datasets as a viable alternative. This study proposes a pipeline for generating synthetic data and a comprehensive approach for investigating the factors that influence the validity of synthetic data generated by LLMs by examining how model performance is affected by metrics such as prompt strategy, text length and target position in a specific task, i.e. inclusive language detection in Italian job advertisements. Our results show that, in most cases and across different metrics, the fine-tuned models trained on synthetic data consistently outperformed other models on both real and synthetic test datasets. The study discusses the practical implications and limitations of using synthetic data for language detection tasks with LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2503.24062v1),  [pdf](http://arxiv.org/pdf/2503.24062v1)

**Tags**: cs.CL cs.AI cs.LG 



### ReaLM: Reliable and Efficient Large Language Model Inference with   Statistical Algorithm-Based Fault Tolerance
**Authors**: Tong Xie, Jiawang Zhao, Zishen Wan, Zuodong Zhang, Yuan Wang, Runsheng Wang, Ru Huang, Meng Li

**Updated**: 2025-03-31T13:15:03Z

**Summary**: The demand for efficient large language model (LLM) inference has propelled the development of dedicated accelerators. As accelerators are vulnerable to hardware faults due to aging, variation, etc, existing accelerator designs often reserve a large voltage margin or leverage algorithm-based fault tolerance (ABFT) techniques to ensure LLM inference correctness. However, previous methods often overlook the inherent fault tolerance of LLMs, leading to high computation and energy overhead. To enable reliable yet efficient LLM inference, in this paper, we propose a novel algorithm/circuit co-design framework, dubbed ReaLM. For the first time, we systematically characterize the fault tolerance of LLMs by performing a large-scale error injection study of representative LLMs and natural language understanding tasks. Then, we propose a statistical ABFT algorithm that fully leverages the error robustness to minimize error recovery as much as possible. We also customize the error detection circuits to enable a low-cost online collection of error statistics. Extensive experiments show that with only 1.42% circuit area and 1.79% power overhead, our ReaLM can reduce perplexity degradation from 18.54 to 0.29. Compared to existing methods, ReaLM consistently reduces recovery costs across different operating voltages and improves energy efficiency by up to 35.83% without compromising LLM performance. Our error injection code is available at https://github.com/2000012835xt/ReaLM-DAC.

**Link**: [arxiv](http://arxiv.org/abs/2503.24053v1),  [pdf](http://arxiv.org/pdf/2503.24053v1)

**Tags**: cs.AR 



### TestART: Improving LLM-based Unit Testing via Co-evolution of Automated   Generation and Repair Iteration
**Authors**: Siqi Gu, Quanjun Zhang, Kecheng Li, Chunrong Fang, Fangyuan Tian, Liuchuan Zhu, Jianyi Zhou, Zhenyu Chen

**Updated**: 2025-03-31T13:13:27Z

**Summary**: Unit testing is crucial for detecting bugs in individual program units but consumes time and effort. Recently, large language models (LLMs) have demonstrated remarkable capabilities in generating unit test cases. However, several problems limit their ability to generate high-quality unit test cases: (1) compilation and runtime errors caused by the hallucination of LLMs; (2) lack of testing and coverage feedback information restricting the increase of code coverage;(3) the repetitive suppression problem causing invalid LLM-based repair and generation attempts. To address these limitations, we propose TestART, a novel unit test generation method. TestART improves LLM-based unit testing via co-evolution of automated generation and repair iteration, representing a significant advancement in automated unit test generation. TestART leverages the template-based repair strategy to effectively fix bugs in LLM-generated test cases for the first time. Meanwhile, TestART extracts coverage information from successful test cases and uses it as coverage-guided testing feedback. It also incorporates positive prompt injection to prevent repetition suppression, thereby enhancing the sufficiency of the final test case. This synergy between generation and repair elevates the correctness and sufficiency of the produced test cases significantly beyond previous methods. In comparative experiments, TestART demonstrates an 18% improvement in pass rate and a 20% enhancement in coverage across three types of datasets compared to baseline models. Additionally, it achieves better coverage rates than EvoSuite with only half the number of test cases. These results demonstrate TestART's superior ability to produce high-quality unit test cases by harnessing the power of LLMs while overcoming their inherent flaws.

**Link**: [arxiv](http://arxiv.org/abs/2408.03095v6),  [pdf](http://arxiv.org/pdf/2408.03095v6)

**Tags**: cs.SE 



### Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents
**Authors**: Shuo Ren, Pu Jian, Zhenjiang Ren, Chunlin Leng, Can Xie, Jiajun Zhang

**Updated**: 2025-03-31T13:11:28Z

**Summary**: As scientific research becomes increasingly complex, innovative tools are needed to manage vast data, facilitate interdisciplinary collaboration, and accelerate discovery. Large language models (LLMs) are now evolving into LLM-based scientific agents that automate critical tasks, ranging from hypothesis generation and experiment design to data analysis and simulation. Unlike general-purpose LLMs, these specialized agents integrate domain-specific knowledge, advanced tool sets, and robust validation mechanisms, enabling them to handle complex data types, ensure reproducibility, and drive scientific breakthroughs. This survey provides a focused review of the architectures, design, benchmarks, applications, and ethical considerations surrounding LLM-based scientific agents. We highlight why they differ from general agents and the ways in which they advance research across various scientific fields. By examining their development and challenges, this survey offers a comprehensive roadmap for researchers and practitioners to harness these agents for more efficient, reliable, and ethically sound scientific discovery.

**Link**: [arxiv](http://arxiv.org/abs/2503.24047v1),  [pdf](http://arxiv.org/pdf/2503.24047v1)

**Tags**: cs.AI cs.MA 



### MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data   Uncertainty
**Authors**: Yongjin Yang, Haneul Yoo, Hwaran Lee

**Updated**: 2025-03-31T13:03:14Z

**Summary**: Despite the massive advancements in large language models (LLMs), they still suffer from producing plausible but incorrect responses. To improve the reliability of LLMs, recent research has focused on uncertainty quantification to predict whether a response is correct or not. However, most uncertainty quantification methods have been evaluated on single-labeled questions, which removes data uncertainty: the irreducible randomness often present in user queries, which can arise from factors like multiple possible answers. This limitation may cause uncertainty quantification results to be unreliable in practical settings. In this paper, we investigate previous uncertainty quantification methods under the presence of data uncertainty. Our contributions are two-fold: 1) proposing a new Multi-Answer Question Answering dataset, MAQA, consisting of world knowledge, mathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty quantification regarding data uncertainty, and 2) assessing 5 uncertainty quantification methods of diverse white- and black-box LLMs. Our findings show that previous methods relatively struggle compared to single-answer settings, though this varies depending on the task. Moreover, we observe that entropy- and consistency-based methods effectively estimate model uncertainty, even in the presence of data uncertainty. We believe these observations will guide future work on uncertainty quantification in more realistic settings.

**Link**: [arxiv](http://arxiv.org/abs/2408.06816v2),  [pdf](http://arxiv.org/pdf/2408.06816v2)

**Tags**: cs.AI cs.CL 



### Are Large Language Models Memorizing Bug Benchmarks?
**Authors**: Daniel Ramos, Claudia Mamede, Kush Jain, Paulo Canelas, Catarina Gamboa, Claire Le Goues

**Updated**: 2025-03-31T13:02:51Z

**Summary**: Large Language Models (LLMs) have become integral to various software engineering tasks, including code generation, bug detection, and repair. To evaluate model performance in these domains, numerous bug benchmarks containing real-world bugs from software projects have been developed. However, a growing concern within the software engineering community is that these benchmarks may not reliably reflect true LLM performance due to the risk of data leakage. Despite this concern, limited research has been conducted to quantify the impact of potential leakage. In this paper, we systematically evaluate popular LLMs to assess their susceptibility to data leakage from widely used bug benchmarks. To identify potential leakage, we use multiple metrics, including a study of benchmark membership within commonly used training datasets, as well as analyses of negative log-likelihood and n-gram accuracy. Our findings show that certain models, in particular codegen-multi, exhibit significant evidence of memorization in widely used benchmarks like Defects4J, while newer models trained on larger datasets like LLaMa 3.1 exhibit limited signs of leakage. These results highlight the need for careful benchmark selection and the adoption of robust metrics to adequately assess models capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2411.13323v3),  [pdf](http://arxiv.org/pdf/2411.13323v3)

**Tags**: cs.SE cs.AI cs.LG 



### Pay More Attention to the Robustness of Prompt for Instruction Data   Mining
**Authors**: Qiang Wang, Dawei Feng, Xu Zhang, Ao Shen, Yang Xu, Bo Ding, Huaimin Wang

**Updated**: 2025-03-31T12:53:08Z

**Summary**: Instruction tuning has emerged as a paramount method for tailoring the behaviors of LLMs. Recent work has unveiled the potential for LLMs to achieve high performance through fine-tuning with a limited quantity of high-quality instruction data. Building upon this approach, we further explore the impact of prompt's robustness on the selection of high-quality instruction data. This paper proposes a pioneering framework of high-quality online instruction data mining for instruction tuning, focusing on the impact of prompt's robustness on the data mining process. Our notable innovation, is to generate the adversarial instruction data by conducting the attack for the prompt of online instruction data. Then, we introduce an Adversarial Instruction-Following Difficulty metric to measure how much help the adversarial instruction data can provide to the generation of the corresponding response. Apart from it, we propose a novel Adversarial Instruction Output Embedding Consistency approach to select high-quality online instruction data. We conduct extensive experiments on two benchmark datasets to assess the performance. The experimental results serve to underscore the effectiveness of our proposed two methods. Moreover, the results underscore the critical practical significance of considering prompt's robustness.

**Link**: [arxiv](http://arxiv.org/abs/2503.24028v1),  [pdf](http://arxiv.org/pdf/2503.24028v1)

**Tags**: cs.AI 



### IntelliCircos: A Data-driven and AI-powered Authoring Tool for Circos   Plots
**Authors**: Mingyang Gu, Jiamin Zhu, Qipeng Wang, Fengjie Wang, Xiaolin Wen, Yong Wang, Min Zhu

**Updated**: 2025-03-31T12:48:39Z

**Summary**: Genomics data is essential in biological and medical domains, and bioinformatics analysts often manually create circos plots to analyze the data and extract valuable insights. However, creating circos plots is complex, as it requires careful design for multiple track attributes and positional relationships between them. Typically, analysts often seek inspiration from existing circos plots, and they have to iteratively adjust and refine the plot to achieve a satisfactory final design, making the process both tedious and time-intensive. To address these challenges, we propose IntelliCircos, an AI-powered interactive authoring tool that streamlines the process from initial visual design to the final implementation of circos plots. Specifically, we build a new dataset containing 4396 circos plots with corresponding annotations and configurations, which are extracted and labeled from published papers. With the dataset, we further identify track combination patterns, and utilize Large Language Model (LLM) to provide domain-specific design recommendations and configuration references to navigate the design of circos plots. We conduct a user study with 8 bioinformatics analysts to evaluate IntelliCircos, and the results demonstrate its usability and effectiveness in authoring circos plots.

**Link**: [arxiv](http://arxiv.org/abs/2503.24021v1),  [pdf](http://arxiv.org/pdf/2503.24021v1)

**Tags**: cs.HC 



### Fast and Accurate Task Planning using Neuro-Symbolic Language Models and   Multi-level Goal Decomposition
**Authors**: Minseo Kwon, Yaesol Kim, Young J. Kim

**Updated**: 2025-03-31T12:24:30Z

**Summary**: In robotic task planning, symbolic planners using rule-based representations like PDDL are effective but struggle with long-sequential tasks in complicated environments due to exponentially increasing search space. Meanwhile, LLM-based approaches, which are grounded in artificial neural networks, offer faster inference and commonsense reasoning but suffer from lower success rates. To address the limitations of the current symbolic (slow speed) or LLM-based approaches (low accuracy), we propose a novel neuro-symbolic task planner that decomposes complex tasks into subgoals using LLM and carries out task planning for each subgoal using either symbolic or MCTS-based LLM planners, depending on the subgoal complexity. This decomposition reduces planning time and improves success rates by narrowing the search space and enabling LLMs to focus on more manageable tasks. Our method significantly reduces planning time while maintaining high success rates across task planning domains, as well as real-world and simulated robotics environments. More details are available at http://graphics.ewha.ac.kr/LLMTAMP/.

**Link**: [arxiv](http://arxiv.org/abs/2409.19250v2),  [pdf](http://arxiv.org/pdf/2409.19250v2)

**Tags**: cs.RO 



### Rethinking Key-Value Cache Compression Techniques for Large Language   Model Serving
**Authors**: Wei Gao, Xinyu Zhou, Peng Sun, Tianwei Zhang, Yonggang Wen

**Updated**: 2025-03-31T12:23:31Z

**Summary**: Key-Value cache (\texttt{KV} \texttt{cache}) compression has emerged as a promising technique to optimize Large Language Model (LLM) serving. It primarily decreases the memory consumption of \texttt{KV} \texttt{cache} to reduce the computation cost. Despite the development of many compression algorithms, their applications in production environments are still not prevalent. In this paper, we revisit mainstream \texttt{KV} \texttt{cache} compression solutions from a practical perspective. Our contributions are three-fold. First, we comprehensively review existing algorithmic designs and benchmark studies for \texttt{KV} \texttt{cache} compression and identify missing pieces in their performance measurement, which could hinder their adoption in practice. Second, we empirically evaluate representative \texttt{KV} \texttt{cache} compression methods to uncover two key issues that affect the computational efficiency: (1) while compressing \texttt{KV} \texttt{cache} can reduce memory consumption, current implementations (e.g., FlashAttention, PagedAttention) do not optimize for production-level LLM serving, resulting in suboptimal throughput performance; (2) compressing \texttt{KV} \texttt{cache} may lead to longer outputs, resulting in increased end-to-end latency. We further investigate the accuracy performance of individual samples rather than the overall performance, revealing the intrinsic limitations in \texttt{KV} \texttt{cache} compression when handling specific LLM tasks. Third, we provide tools to shed light on future \texttt{KV} \texttt{cache} compression studies and facilitate their practical deployment in production. They are open-sourced in \href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.

**Link**: [arxiv](http://arxiv.org/abs/2503.24000v1),  [pdf](http://arxiv.org/pdf/2503.24000v1)

**Tags**: cs.LG cs.AI 



### Rubric Is All You Need: Enhancing LLM-based Code Evaluation With   Question-Specific Rubrics
**Authors**: Aditya Pathak, Rachit Gandhi, Vaibhav Uttam, Devansh, Yashwanth Nakka, Aaryan Raj Jindal, Pratyush Ghosh, Arnav Ramamoorthy, Shreyash Verma, Aditya Mittal, Aashna Ased, Chirag Khatri, Jagat Sesh Challa, Dhruv Kumar

**Updated**: 2025-03-31T11:59:43Z

**Summary**: Since the disruption in LLM technology brought about by the release of GPT-3 and ChatGPT, LLMs have shown remarkable promise in programming-related tasks. While code generation remains a popular field of research, code evaluation using LLMs remains a problem with no conclusive solution. In this paper, we focus on LLM-based code evaluation and attempt to fill in the existing gaps. We propose multi-agentic novel approaches using question-specific rubrics tailored to the problem statement, arguing that these perform better for logical assessment than the existing approaches that use question-agnostic rubrics. To address the lack of suitable evaluation datasets, we introduce two datasets: a Data Structures and Algorithms dataset containing 150 student submissions from a popular Data Structures and Algorithms practice website, and an Object Oriented Programming dataset comprising 80 student submissions from undergraduate computer science courses. In addition to using standard metrics (Spearman Correlation, Cohen's Kappa), we additionally propose a new metric called as Leniency, which quantifies evaluation strictness relative to expert assessment. Our comprehensive analysis demonstrates that question-specific rubrics significantly enhance logical assessment of code in educational settings, providing better feedback aligned with instructional goals beyond mere syntactic correctness.

**Link**: [arxiv](http://arxiv.org/abs/2503.23989v1),  [pdf](http://arxiv.org/pdf/2503.23989v1)

**Tags**: cs.SE cs.AI 



### Deep Learning Model Deployment in Multiple Cloud Providers: an   Exploratory Study Using Low Computing Power Environments
**Authors**: Elayne Lemos, Rodrigo Oliveira, Jairson Rodrigues, Rosalvo F. Oliveira Neto

**Updated**: 2025-03-31T11:58:37Z

**Summary**: The deployment of Machine Learning models at cloud have grown by tech companies. Hardware requirements are higher when these models involve Deep Learning (DL) techniques and the cloud providers' costs may be a barrier. We explore deploying DL models using for experiments the GECToR model, a DL solution for Grammatical Error Correction, across three of the major cloud platforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware usage and cost at each cloud provider by 7 execution environments with 10 experiments reproduced. We found that while GPUs excel in performance, they had an average cost 300% higher than solutions without GPU. Our analysis also identifies that processor cache size is crucial for cost-effective CPU deployments, enabling over 50% of cost reduction compared to GPUs. This study demonstrates the feasibility and affordability of cloud-based DL inference solutions without GPUs, benefiting resource-constrained users like startups.

**Link**: [arxiv](http://arxiv.org/abs/2503.23988v1),  [pdf](http://arxiv.org/pdf/2503.23988v1)

**Tags**: cs.DC cs.AI cs.PF 68T07, 68U01 C.4; I.2.0; B.8.2 



### Video-based Traffic Light Recognition by Rockchip RV1126 for Autonomous   Driving
**Authors**: Miao Fan, Xuxu Kong, Shengtong Xu, Haoyi Xiong, Xiangzeng Liu

**Updated**: 2025-03-31T11:27:48Z

**Summary**: Real-time traffic light recognition is fundamental for autonomous driving safety and navigation in urban environments. While existing approaches rely on single-frame analysis from onboard cameras, they struggle with complex scenarios involving occlusions and adverse lighting conditions. We present \textit{ViTLR}, a novel video-based end-to-end neural network that processes multiple consecutive frames to achieve robust traffic light detection and state classification. The architecture leverages a transformer-like design with convolutional self-attention modules, which is optimized specifically for deployment on the Rockchip RV1126 embedded platform. Extensive evaluations on two real-world datasets demonstrate that \textit{ViTLR} achieves state-of-the-art performance while maintaining real-time processing capabilities (>25 FPS) on RV1126's NPU. The system shows superior robustness across temporal stability, varying target distances, and challenging environmental conditions compared to existing single-frame approaches. We have successfully integrated \textit{ViTLR} into an ego-lane traffic light recognition system using HD maps for autonomous driving applications. The complete implementation, including source code and datasets, is made publicly available to facilitate further research in this domain.

**Link**: [arxiv](http://arxiv.org/abs/2503.23965v1),  [pdf](http://arxiv.org/pdf/2503.23965v1)

**Tags**: cs.CV cs.RO 



### AI2Agent: An End-to-End Framework for Deploying AI Projects as   Autonomous Agents
**Authors**: Jiaxiang Chen, Jingwei Shi, Lei Gan, Jiale Zhang, Qingyu Zhang, Dongqian Zhang, Xin Pang, Zhucong Li, Yinghui Xu

**Updated**: 2025-03-31T10:58:34Z

**Summary**: As AI technology advances, it is driving innovation across industries, increasing the demand for scalable AI project deployment. However, deployment remains a critical challenge due to complex environment configurations, dependency conflicts, cross-platform adaptation, and debugging difficulties, which hinder automation and adoption. This paper introduces AI2Agent, an end-to-end framework that automates AI project deployment through guideline-driven execution, self-adaptive debugging, and case \& solution accumulation. AI2Agent dynamically analyzes deployment challenges, learns from past cases, and iteratively refines its approach, significantly reducing human intervention. To evaluate its effectiveness, we conducted experiments on 30 AI deployment cases, covering TTS, text-to-image generation, image editing, and other AI applications. Results show that AI2Agent significantly reduces deployment time and improves success rates. The code and demo video are now publicly accessible.

**Link**: [arxiv](http://arxiv.org/abs/2503.23948v1),  [pdf](http://arxiv.org/pdf/2503.23948v1)

**Tags**: cs.AI 



### Choco-Q: Commute Hamiltonian-based QAOA for Constrained Binary   Optimization
**Authors**: Debin Xiang, Qifan Jiang, Liqiang Lu, Siwei Tan, Jianwei Yin

**Updated**: 2025-03-31T10:47:20Z

**Summary**: Constrained binary optimization aims to find an optimal assignment to minimize or maximize the objective meanwhile satisfying the constraints, which is a representative NP problem in various domains, including transportation, scheduling, and economy. Quantum approximate optimization algorithms (QAOA) provide a promising methodology for solving this problem by exploiting the parallelism of quantum entanglement. However, existing QAOA approaches based on penalty-term or Hamiltonian simulation fail to thoroughly encode the constraints, leading to extremely low success rate and long searching latency.   This paper proposes Choco-Q, a formal and universal framework for constrained binary optimization problems, which comprehensively covers all constraints and exhibits high deployability for current quantum devices. The main innovation of Choco-Q is to embed the commute Hamiltonian as the driver Hamiltonian, resulting in a much more general encoding formulation that can deal with arbitrary linear constraints. Leveraging the arithmetic features of commute Hamiltonian, we propose three optimization techniques to squeeze the overall circuit complexity, including Hamiltonian serialization, equivalent decomposition, and variable elimination. The serialization mechanism transforms the original Hamiltonian into smaller ones. Our decomposition methods only take linear time complexity, achieving end-to-end acceleration. Experiments demonstrate that Choco-Q shows more than 235$\times$ algorithmic improvement in successfully finding the optimal solution, and achieves 4.69$\times$ end-to-end acceleration, compared to prior QAOA designs.

**Link**: [arxiv](http://arxiv.org/abs/2503.23941v1),  [pdf](http://arxiv.org/pdf/2503.23941v1)

**Tags**: quant-ph 



### Green MLOps to Green GenOps: An Empirical Study of Energy Consumption in   Discriminative and Generative AI Operations
**Authors**: AdriÃ¡n SÃ¡nchez-MompÃ³, Ioannis Mavromatis, Peizheng Li, Konstantinos Katsaros, Aftab Khan

**Updated**: 2025-03-31T10:28:04Z

**Summary**: This study presents an empirical investigation into the energy consumption of Discriminative and Generative AI models within real-world MLOps pipelines. For Discriminative models, we examine various architectures and hyperparameters during training and inference and identify energy-efficient practices. For Generative AI, Large Language Models (LLMs) are assessed, focusing primarily on energy consumption across different model sizes and varying service requests. Our study employs software-based power measurements, ensuring ease of replication across diverse configurations, models, and datasets. We analyse multiple models and hardware setups to uncover correlations among various metrics, identifying key contributors to energy consumption. The results indicate that for Discriminative models, optimising architectures, hyperparameters, and hardware can significantly reduce energy consumption without sacrificing performance. For LLMs, energy efficiency depends on balancing model size, reasoning complexity, and request-handling capacity, as larger models do not necessarily consume more energy when utilisation remains low. This analysis provides practical guidelines for designing green and sustainable ML operations, emphasising energy consumption and carbon footprint reductions while maintaining performance. This paper can serve as a benchmark for accurately estimating total energy use across different types of AI models.

**Link**: [arxiv](http://arxiv.org/abs/2503.23934v1),  [pdf](http://arxiv.org/pdf/2503.23934v1)

**Tags**: cs.LG cs.AI 



### PupiNet: Seamless OCT-OCTA Interconversion Through Wavelet-Driven and   Multi-Scale Attention Mechanisms
**Authors**: Renzhi Tian, Jinjie Wang, Wei Yang, Weizhen Li, Haoran Chen, Yiran Zhu, Chengchang Pan, Honggang Qi

**Updated**: 2025-03-31T10:27:23Z

**Summary**: Optical Coherence Tomography (OCT) and Optical Coherence Tomography Angiography (OCTA) are key diagnostic tools for clinical evaluation and management of retinal diseases. Compared to traditional OCT, OCTA provides richer microvascular information, but its acquisition requires specialized sensors and high-cost equipment, creating significant challenges for the clinical deployment of hardware-dependent OCTA imaging methods. Given the technical complexity of OCTA image acquisition and potential mechanical artifacts, this study proposes a bidirectional image conversion framework called PupiNet, which accurately achieves bidirectional transformation between 3D OCT and 3D OCTA. The generator module of this framework innovatively integrates wavelet transformation and multi-scale attention mechanisms, significantly enhancing image conversion quality. Meanwhile, an Adaptive Discriminator Augmentation (ADA) module has been incorporated into the discriminator to optimize model training stability and convergence efficiency. To ensure clinical accuracy of vascular structures in the converted images, we designed a Vessel Structure Matcher (VSM) supervision module, achieving precise matching of vascular morphology between generated images and target images. Additionally, the Hierarchical Feature Calibration (HFC) module further guarantees high consistency of texture details between generated images and target images across different depth levels. To rigorously validate the clinical effectiveness of the proposed method, we conducted a comprehensive evaluation on a paired OCT-OCTA image dataset containing 300 eyes with various retinal pathologies. Experimental results demonstrate that PupiNet not only reliably achieves high-quality bidirectional transformation between the two modalities but also shows significant advantages in image fidelity, vessel structure preservation, and clinical usability.

**Link**: [arxiv](http://arxiv.org/abs/2503.23933v1),  [pdf](http://arxiv.org/pdf/2503.23933v1)

**Tags**: eess.IV 



### Reliable Traffic Monitoring Using Low-Cost Doppler Radar Units
**Authors**: Mishay Naidoo, Stephen Paine, Amit Kumar Mishra, Mohammed Yunus Abdul Gaffar

**Updated**: 2025-03-31T10:18:42Z

**Summary**: Road traffic monitoring typically involves the counting and recording of vehicles on public roads over extended periods. The data gathered from such monitoring provides useful information to municipal authorities in urban areas. This paper presents a low-cost, widely deployable sensing subsystem based on Continuous Wave Doppler radar. The proposed system can perform vehicle detection and speed estimation with a total cost of less than 100 USD. The sensing system (including the hardware subsystem and the algorithms) is designed to be placed on the side of the road, allowing for easy deployment and serviceability.

**Link**: [arxiv](http://arxiv.org/abs/2503.23926v1),  [pdf](http://arxiv.org/pdf/2503.23926v1)

**Tags**: eess.SP 



### Model Hemorrhage and the Robustness Limits of Large Language Models
**Authors**: Ziyang Ma, Zuchao Li, Lefei Zhang, Gui-Song Xia, Bo Du, Liangpei Zhang, Dacheng Tao

**Updated**: 2025-03-31T10:16:03Z

**Summary**: Large language models (LLMs) demonstrate strong performance across natural language processing tasks, yet undergo significant performance degradation when modified for deployment through quantization, pruning, or decoding strategy adjustments. We define this phenomenon as model hemorrhage - performance decline caused by parameter alterations and architectural changes. Through systematic analysis of various LLM frameworks, we identify key vulnerability patterns: layer expansion frequently disrupts attention mechanisms, compression techniques induce information loss cascades, and decoding adjustments amplify prediction divergences. Our investigation reveals transformer architectures exhibit inherent robustness thresholds that determine hemorrhage severity across modification types. We propose three mitigation strategies: gradient-aware pruning preserves critical weight pathways, dynamic quantization scaling maintains activation integrity, and decoding calibration aligns generation trajectories with original model distributions. This work establishes foundational metrics for evaluating model stability during adaptation, providing practical guidelines for maintaining performance while enabling efficient LLM deployment. Our findings advance understanding of neural network resilience under architectural transformations, particularly for large-scale language models.

**Link**: [arxiv](http://arxiv.org/abs/2503.23924v1),  [pdf](http://arxiv.org/pdf/2503.23924v1)

**Tags**: cs.CL cs.LG 



### Entropy-guided sequence weighting for efficient exploration in RL-based   LLM fine-tuning
**Authors**: Abdullah Vanlioglu

**Updated**: 2025-03-31T10:13:48Z

**Summary**: We introduce Entropy-Guided Sequence Weighting (EGSW), a novel approach that enhances the exploration-exploitation tradeoff by dynamically assigning weights to generated outputs based on their advantage and entropy for Reinforcement Learning-based Large Language Model fine-tuning. EGSW integrates entropy regularization with advantage-based weighting to balance policy updates, enabling efficient exploration in high-dimensional state spaces. By employing temperature-scaled softmax weighting over sequences, EGSW prioritizing high-reward, high-uncertainty steps while maintaining training stability. Although originally developed to improve Group Relative Policy Optimization (GRPO) during large language model (LLM) fine-tuning, EGSW is generalizable to other reinforcement learning (RL) algorithms and can be implemented in both step-wise and trajectory-wise settings. Empirical evaluations demonstrate that EGSW enhances GRPO reasoning ability, yielding improvements in sample efficiency. Future work will explore the application of EGSW to advanced RL methodologies.

**Link**: [arxiv](http://arxiv.org/abs/2503.22456v2),  [pdf](http://arxiv.org/pdf/2503.22456v2)

**Tags**: cs.LG cs.AI 



### Scenarios for the Deployment of Automated Vehicles in Europe
**Authors**: Louison Duboz, Ioan Cristinel Raileanu, Jette Krause, Ana Norman-LÃ³pez, Matthias Weitzel, Biagio Ciuffo

**Updated**: 2025-03-31T10:06:27Z

**Summary**: The deployment of Automated Vehicles (AVs) is expected to address road transport externalities (e.g., safety, traffic, environmental impact, etc.). For this reason, a legal framework for their large-scale market introduction and deployment is currently being developed in the European Union. Despite the first steps towards road transport automation, the timeline for full automation and its potential economic benefits remains uncertain. The aim of this paper is twofold. First, it presents a methodological framework to determine deployment pathways of the five different levels of automation in EU27+UK to 2050 under three scenarios (i.e., slow, medium baseline and fast) focusing on passenger vehicles. Second, it proposes an assessment of the economic impact of AVs through the calculation of the value-added. The method to define assumptions and uptake trajectories involves a comprehensive literature review, expert interviews, and a model to forecast the new registrations of different levels of automation. In this way, the interviews provided insights that complemented the literature and informed the design of assumptions and deployment trajectories. The added-value assessment shows additional economic activity due to the introduction of automated technologies in all uptake scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2503.23914v1),  [pdf](http://arxiv.org/pdf/2503.23914v1)

**Tags**: econ.GN q-fin.EC 



### Rubrik's Cube: Testing a New Rubric for Evaluating Explanations on the   CUBE dataset
**Authors**: Diana Galvan-Sosa, Gabrielle Gaudeau, Pride Kavumba, Yunmeng Li, Hongyi gu, Zheng Yuan, Keisuke Sakaguchi, Paula Buttery

**Updated**: 2025-03-31T09:48:59Z

**Summary**: The performance and usability of Large-Language Models (LLMs) are driving their use in explanation generation tasks. However, despite their widespread adoption, LLM explanations have been found to be unreliable, making it difficult for users to distinguish good from bad explanations. To address this issue, we present Rubrik's CUBE, an education-inspired rubric and a dataset of 26k explanations, written and later quality-annotated using the rubric by both humans and six open- and closed-source LLMs. The CUBE dataset focuses on two reasoning and two language tasks, providing the necessary diversity for us to effectively test our proposed rubric. Using Rubrik, we find that explanations are influenced by both task and perceived difficulty. Low quality stems primarily from a lack of conciseness in LLM-generated explanations, rather than cohesion and word choice. The full dataset, rubric, and code will be made available upon acceptance.

**Link**: [arxiv](http://arxiv.org/abs/2503.23899v1),  [pdf](http://arxiv.org/pdf/2503.23899v1)

**Tags**: cs.CL I.2.7 



### Better wit than wealth: Dynamic Parametric Retrieval Augmented   Generation for Test-time Knowledge Enhancement
**Authors**: Yuqiao Tan, Shizhu He, Huanxuan Liao, Jun Zhao, Kang Liu

**Updated**: 2025-03-31T09:46:35Z

**Summary**: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving relevant documents from external sources and incorporating them into the context. While it improves reliability by providing factual texts, it significantly increases inference costs as context length grows and introduces challenging issue of RAG hallucination, primarily caused by the lack of corresponding parametric knowledge in LLMs. An efficient solution is to enhance the knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by embedding document into LLMs parameters to perform test-time knowledge enhancement, effectively reducing inference costs through offline training. However, its high training and storage costs, along with limited generalization ability, significantly restrict its practical adoption. To address these challenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that leverages a lightweight parameter translator model to efficiently convert documents into parametric knowledge. DyPRAG not only reduces inference, training, and storage costs but also dynamically generates parametric knowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge conflicts in a plug-and-play manner at test-time. Extensive experiments on multiple datasets demonstrate the effectiveness and generalization capabilities of DyPRAG, offering a powerful and practical RAG paradigm which enables superior knowledge fusion and mitigates RAG hallucination in real-world applications. Our code is available at https://github.com/Trae1ounG/DyPRAG.

**Link**: [arxiv](http://arxiv.org/abs/2503.23895v1),  [pdf](http://arxiv.org/pdf/2503.23895v1)

**Tags**: cs.CL cs.AI 



### SchemaAgent: A Multi-Agents Framework for Generating Relational Database   Schema
**Authors**: Qin Wang, Youhuan Li, Yansong Feng, Si Chen, Ziming Li, Pan Zhang, Zhichao Shi, Yuequn Dou, chuchu Gao, Zebin Huang, Zihui Si, Yixuan Chen, Zhaohai Sun, Ke Tang, Wenqiang Jin

**Updated**: 2025-03-31T09:39:19Z

**Summary**: The relational database design would output a schema based on user's requirements, which defines table structures and their interrelated relations. Translating requirements into accurate schema involves several non-trivial subtasks demanding both database expertise and domain-specific knowledge. This poses unique challenges for automated design of relational databases. Existing efforts are mostly based on customized rules or conventional deep learning models, often producing suboptimal schema. Recently, large language models (LLMs) have significantly advanced intelligent application development across various domains. In this paper, we propose SchemaAgent, a unified LLM-based multi-agent framework for the automated generation of high-quality database schema. SchemaAgent is the first to apply LLMs for schema generation, which emulates the workflow of manual schema design by assigning specialized roles to agents and enabling effective collaboration to refine their respective subtasks. Schema generation is a streamlined workflow, where directly applying the multi-agent framework may cause compounding impact of errors. To address this, we incorporate dedicated roles for reflection and inspection, alongside an innovative error detection and correction mechanism to identify and rectify issues across various phases. For evaluation, we present a benchmark named \textit{RSchema}, which contains more than 500 pairs of requirement description and schema. Experimental results on this benchmark demonstrate the superiority of our approach over mainstream LLMs for relational database schema generation.

**Link**: [arxiv](http://arxiv.org/abs/2503.23886v1),  [pdf](http://arxiv.org/pdf/2503.23886v1)

**Tags**: cs.DB cs.AI 



### ZeroMimic: Distilling Robotic Manipulation Skills from Web Videos
**Authors**: Junyao Shi, Zhuolun Zhao, Tianyou Wang, Ian Pedroza, Amy Luo, Jie Wang, Jason Ma, Dinesh Jayaraman

**Updated**: 2025-03-31T09:27:00Z

**Summary**: Many recent advances in robotic manipulation have come through imitation learning, yet these rely largely on mimicking a particularly hard-to-acquire form of demonstrations: those collected on the same robot in the same room with the same objects as the trained policy must handle at test time. In contrast, large pre-recorded human video datasets demonstrating manipulation skills in-the-wild already exist, which contain valuable information for robots. Is it possible to distill a repository of useful robotic skill policies out of such data without any additional requirements on robot-specific demonstrations or exploration? We present the first such system ZeroMimic, that generates immediately deployable image goal-conditioned skill policies for several common categories of manipulation tasks (opening, closing, pouring, pick&place, cutting, and stirring) each capable of acting upon diverse objects and across diverse unseen task setups. ZeroMimic is carefully designed to exploit recent advances in semantic and geometric visual understanding of human videos, together with modern grasp affordance detectors and imitation policy classes. After training ZeroMimic on the popular EpicKitchens dataset of ego-centric human videos, we evaluate its out-of-the-box performance in varied real-world and simulated kitchen settings with two different robot embodiments, demonstrating its impressive abilities to handle these varied tasks. To enable plug-and-play reuse of ZeroMimic policies on other task setups and robots, we release software and policy checkpoints of our skill policies.

**Link**: [arxiv](http://arxiv.org/abs/2503.23877v1),  [pdf](http://arxiv.org/pdf/2503.23877v1)

**Tags**: cs.RO cs.CV cs.LG 



### GenSwarm: Scalable Multi-Robot Code-Policy Generation and Deployment via   Language Models
**Authors**: Wenkang Ji, Huaben Chen, Mingyang Chen, Guobin Zhu, Lufeng Xu, Roderich GroÃ, Rui Zhou, Ming Cao, Shiyu Zhao

**Updated**: 2025-03-31T09:26:34Z

**Summary**: The development of control policies for multi-robot systems traditionally follows a complex and labor-intensive process, often lacking the flexibility to adapt to dynamic tasks. This has motivated research on methods to automatically create control policies. However, these methods require iterative processes of manually crafting and refining objective functions, thereby prolonging the development cycle. This work introduces \textit{GenSwarm}, an end-to-end system that leverages large language models to automatically generate and deploy control policies for multi-robot tasks based on simple user instructions in natural language. As a multi-language-agent system, GenSwarm achieves zero-shot learning, enabling rapid adaptation to altered or unseen tasks. The white-box nature of the code policies ensures strong reproducibility and interpretability. With its scalable software and hardware architectures, GenSwarm supports efficient policy deployment on both simulated and real-world multi-robot systems, realizing an instruction-to-execution end-to-end functionality that could prove valuable for robotics specialists and non-specialists alike.The code of the proposed GenSwarm system is available online: https://github.com/WindyLab/GenSwarm.

**Link**: [arxiv](http://arxiv.org/abs/2503.23875v1),  [pdf](http://arxiv.org/pdf/2503.23875v1)

**Tags**: cs.RO cs.AI cs.MA 



### Exploring In-Context Learning Capabilities of ChatGPT for Pathological   Speech Detection
**Authors**: Mahdi Amiri, Hatef Otroshi Shahreza, Ina Kodrasi

**Updated**: 2025-03-31T09:23:52Z

**Summary**: Automatic pathological speech detection approaches have shown promising results, gaining attention as potential diagnostic tools alongside costly traditional methods. While these approaches can achieve high accuracy, their lack of interpretability limits their applicability in clinical practice. In this paper, we investigate the use of multimodal Large Language Models (LLMs), specifically ChatGPT-4o, for automatic pathological speech detection in a few-shot in-context learning setting. Experimental results show that this approach not only delivers promising performance but also provides explanations for its decisions, enhancing model interpretability. To further understand its effectiveness, we conduct an ablation study to analyze the impact of different factors, such as input type and system prompts, on the final results. Our findings highlight the potential of multimodal LLMs for further exploration and advancement in automatic pathological speech detection.

**Link**: [arxiv](http://arxiv.org/abs/2503.23873v1),  [pdf](http://arxiv.org/pdf/2503.23873v1)

**Tags**: eess.AS cs.SD 



### Communication-Efficient and Personalized Federated Foundation Model   Fine-Tuning via Tri-Matrix Adaptation
**Authors**: Yongle Li, Bo Liu, Sheng Huang, ZHeng ZHang, Xiaotong Yuan, Richang Hong

**Updated**: 2025-03-31T09:18:42Z

**Summary**: In federated learning, fine-tuning pre-trained foundation models poses significant challenges, particularly regarding high communication cost and suboptimal model performance due to data heterogeneity between the clients. To address these issues, this paper introduces communication-efficient federated LoRA adaption (CE-LoRA), a method that employs a tri-factorization low-rank adaptation approach with personalized model parameter aggregation. We first presents a novel LoRA parameter factorization by introducing a small-size dense matrix, which can significantly reduce the communication cost and achieve comparable empirical performance than transferring the low-rank parameter matrix used by existing methods. Without violating data privacy, the server considers the client similarity in both training dataset and model parameter space, and learns personalized weights for model aggregation. Our experiments on various LLM and VLM fine-tuning tasks demonstrate that CE-LoRA not only significantly reduces communication overhead but also improves performance under not independently and identically distributed data conditions. In addition, CE-LoRA improves data privacy protection, effectively mitigating gradient-based data reconstruction attacks.

**Link**: [arxiv](http://arxiv.org/abs/2503.23869v1),  [pdf](http://arxiv.org/pdf/2503.23869v1)

**Tags**: cs.LG 



### Evaluating small vision-language models as AI assistants for radio   astronomical source analysis tasks
**Authors**: S. Riggi, T. Cecconello, A. Pilzer, S. Palazzo, N. Gupta, A. M. Hopkins, C. Trigilio, G. Umana

**Updated**: 2025-03-31T09:06:23Z

**Summary**: The advent of next-generation radio telescopes is set to transform radio astronomy by producing massive data volumes that challenge traditional processing methods. Deep learning techniques have shown strong potential in automating radio analysis tasks, yet are often constrained by the limited availability of large annotated datasets. Recent progress in self-supervised learning has led to foundational radio vision models, but adapting them for new tasks typically requires coding expertise, limiting their accessibility to a broader astronomical community. Text-based AI interfaces offer a promising alternative by enabling task-specific queries and example-driven learning. In this context, Large Language Models (LLMs), with their remarkable zero-shot capabilities, are increasingly used in scientific domains. However, deploying large-scale models remains resource-intensive, and there is a growing demand for AI systems that can reason over both visual and textual data in astronomical analysis. This study explores small-scale Vision-Language Models (VLMs) as AI assistants for radio astronomy, combining LLM capabilities with vision transformers. We fine-tuned the LLaVA VLM on a dataset of 59k radio images from multiple surveys, enriched with 38k image-caption pairs from the literature. The fine-tuned models show clear improvements over base models in radio-specific tasks, achieving ~30% F1-score gains in extended source detection, but they underperform pure vision models and exhibit ~20% drop on general multimodal tasks. Inclusion of caption data and LoRA fine-tuning enhances instruction-following and helps recover ~10% accuracy on standard benchmarks. This work lays the foundation for future advancements in radio VLMs, highlighting their potential and limitations, such as the need for better multimodal alignment, higher-quality datasets, and mitigation of catastrophic forgetting.

**Link**: [arxiv](http://arxiv.org/abs/2503.23859v1),  [pdf](http://arxiv.org/pdf/2503.23859v1)

**Tags**: astro-ph.IM 



### SpeechDialogueFactory: Generating High-Quality Speech Dialogue Data to   Accelerate Your Speech-LLM Development
**Authors**: Minghan Wang, Ye Bai, Yuxia Wang, Thuy-Trang Vu, Ehsan Shareghi, Gholamreza Haffari

**Updated**: 2025-03-31T08:52:21Z

**Summary**: High-quality speech dialogue datasets are crucial for Speech-LLM development, yet existing acquisition methods face significant limitations. Human recordings incur high costs and privacy concerns, while synthetic approaches often lack conversational authenticity. To address these challenges, we introduce \textsc{SpeechDialogueFactory}, a production-ready framework for generating natural speech dialogues efficiently. Our solution employs a comprehensive pipeline including metadata generation, dialogue scripting, paralinguistic-enriched utterance simulation, and natural speech synthesis with voice cloning. Additionally, the system provides an interactive UI for detailed sample inspection and a high-throughput batch synthesis mode. Evaluations show that dialogues generated by our system achieve a quality comparable to human recordings while significantly reducing production costs. We release our work as an open-source toolkit, alongside example datasets available in English and Chinese, empowering researchers and developers in Speech-LLM research and development.

**Link**: [arxiv](http://arxiv.org/abs/2503.23848v1),  [pdf](http://arxiv.org/pdf/2503.23848v1)

**Tags**: cs.CL 



### Expanding RL with Verifiable Rewards Across Diverse Domains
**Authors**: Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, Min Zhang, Dong Yu

**Updated**: 2025-03-31T08:22:49Z

**Summary**: Reinforcement learning (RL) with verifiable rewards (RLVR) has shown promising results in mathematical reasoning and coding tasks where well-structured reference answers are available. However, its applicability to broader domains remains underexplored. In this work, we study the extension of RLVR to more diverse domains such as medicine, chemistry, psychology, and economics. We observe high agreement in binary judgments across different large language models (LLMs) when objective reference answers exist, which challenges the necessity of large-scale annotation for training domain-specific reward models. To address the limitations of binary rewards when handling unstructured reference answers, we further incorporate model-based soft scoring into RLVR to improve its flexibility. Our experiments show that a distilled generative reward model can serve as an effective cross-domain verifier, providing reliable reward signals for RL without requiring domain-specific annotations. By fine-tuning a base 7B model using various RL algorithms against our reward model, we obtain policies that outperform state-of-the-art open-source aligned LLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large margin, across domains in free-form answer settings. This also strengthens RLVR's robustness and scalability, highlighting its potential for real-world applications with noisy or weak labels.

**Link**: [arxiv](http://arxiv.org/abs/2503.23829v1),  [pdf](http://arxiv.org/pdf/2503.23829v1)

**Tags**: cs.CL 



### Aud-Sur: An Audio Analyzer Assistant for Audio Surveillance Applications
**Authors**: Phat Lam, Lam Pham, Dat Tran, Alexander Schindler, Silvia Poletti, Marcel Hasenbalg, David Fischinger, Martin Boyer

**Updated**: 2025-03-31T08:21:11Z

**Summary**: In this paper, we present an audio analyzer assistant tool designed for a wide range of audio-based surveillance applications (This work is a part of our DEFAME FAKES and EUCINF projects). The proposed tool, refered to as Aud-Sur, comprises two main phases Audio Analysis and Audio Retrieval, respectively. In the first phase, multiple open-source audio models are leveraged to extract information from input audio recording uploaded by a user. In the second phase, users interact with the Aud-Sur tool via a natural question-and-answer manner, powered by a large language model (LLM), to retrieve the information extracted from the processed audio file. The Aud-Sur tool was deployed using Docker on a microservices-based architecture design. By leveraging open-source audio models for information extraction, LLM for audio information retrieval, and a microservices-based deployment approach, the proposed Aud-Sur tool offers a highly extensible and adaptable framework that can integrate more audio tasks, and be widely shared within the audio community for further development.

**Link**: [arxiv](http://arxiv.org/abs/2503.23827v1),  [pdf](http://arxiv.org/pdf/2503.23827v1)

**Tags**: eess.AS 



### Blockchain for Federated Learning in the Internet of Things: Trustworthy   Adaptation, Standards, and the Road Ahead
**Authors**: Farhana Javed, Engin Zeydan, Josep Mangues-Bafalluy, Kapal Dev, Luis Blanco

**Updated**: 2025-03-31T08:19:18Z

**Summary**: As edge computing gains prominence in Internet of Things (IoTs), smart cities, and autonomous systems, the demand for real-time machine intelligence with low latency and model reliability continues to grow. Federated Learning (FL) addresses these needs by enabling distributed model training without centralizing user data, yet it remains reliant on centralized servers and lacks built-in mechanisms for transparency and trust. Blockchain and Distributed Ledger Technologies (DLTs) can fill this gap by introducing immutability, decentralized coordination, and verifiability into FL workflows. This article presents current standardization efforts from 3GPP, ETSI, ITU-T, IEEE, and O-RAN that steer the integration of FL and blockchain in IoT ecosystems. We then propose a blockchain-based FL framework that replaces the centralized aggregator, incorporates reputation monitoring of IoT devices, and minimizes overhead via selective on-chain storage of model updates. We validate our approach with IOTA Tangle, demonstrating stable throughput and block confirmations, even under increasing FL workloads. Finally, we discuss architectural considerations and future directions for embedding trustworthy and resource-efficient FL in emerging 6G networks and vertical IoT applications. Our results underscore the potential of DLT-enhanced FL to meet stringent trust and energy requirements of next-generation IoT deployments.

**Link**: [arxiv](http://arxiv.org/abs/2503.23823v1),  [pdf](http://arxiv.org/pdf/2503.23823v1)

**Tags**: cs.NI 



### HyperGLM: HyperGraph for Video Scene Graph Generation and Anticipation
**Authors**: Trong-Thuan Nguyen, Pha Nguyen, Jackson Cothren, Alper Yilmaz, Khoa Luu

**Updated**: 2025-03-31T08:16:49Z

**Summary**: Multimodal LLMs have advanced vision-language tasks but still struggle with understanding video scenes. To bridge this gap, Video Scene Graph Generation (VidSGG) has emerged to capture multi-object relationships across video frames. However, prior methods rely on pairwise connections, limiting their ability to handle complex multi-object interactions and reasoning. To this end, we propose Multimodal LLMs on a Scene HyperGraph (HyperGLM), promoting reasoning about multi-way interactions and higher-order relationships. Our approach uniquely integrates entity scene graphs, which capture spatial relationships between objects, with a procedural graph that models their causal transitions, forming a unified HyperGraph. Significantly, HyperGLM enables reasoning by injecting this unified HyperGraph into LLMs. Additionally, we introduce a new Video Scene Graph Reasoning (VSGR) dataset featuring 1.9M frames from third-person, egocentric, and drone views and supports five tasks: Scene Graph Generation, Scene Graph Anticipation, Video Question Answering, Video Captioning, and Relation Reasoning. Empirically, HyperGLM consistently outperforms state-of-the-art methods across five tasks, effectively modeling and reasoning complex relationships in diverse video scenes.

**Link**: [arxiv](http://arxiv.org/abs/2411.18042v2),  [pdf](http://arxiv.org/pdf/2411.18042v2)

**Tags**: cs.CV 



### MVDRAM: Enabling GeMV Execution in Unmodified DRAM for Low-Bit LLM   Acceleration
**Authors**: Tatsuya Kubo, Daichi Tokuda, Tomoya Nagatani, Masayuki Usui, Lei Qu, Ting Cao, Shinya Takamaeda-Yamazaki

**Updated**: 2025-03-31T07:54:59Z

**Summary**: General matrix-vector multiplication (GeMV) remains a critical latency bottleneck in large language model (LLM) inference, even with quantized low-bit models. Processing-Using-DRAM (PUD), an analog in-DRAM computing technique, has the potential to repurpose on-device DRAM as a GeMV engine, offering additional high-throughput processing capabilities to widespread consumer devices without DRAM modifications. However, applying PUD to GeMV operations in the LLM inference pipeline incurs significant overheads $\textit{before}$ and $\textit{after}$ in-DRAM computation, diminishing the benefits of its high-throughput processing capabilities.   This paper presents MVDRAM, the first practical system to accelerate GeMV operations for low-bit LLM inference using unmodified DRAM. By leveraging the data sharing patterns and mathematical linearity in GeMV operations, MVDRAM orchestrates the processor and DRAM to eliminate the costs associated with pre-arranging inputs and bit-transposition of outputs required in conventional PUD approaches. Our experimental evaluation with four DDR4 DRAM modules shows that MVDRAM achieves comparable or even better inference speed than the processor-based implementation for GeMV operations in low-bit (under 4-bit) LLM. In particular, MVDRAM achieves up to 7.29$\times$ speedup and 30.5$\times$ energy efficiency for low-bit GeMV operations. For end-to-end LLM inference, MVDRAM achieves 2.18$\times$ and 1.31$\times$ throughput improvements, along with 3.04$\times$ and 2.35$\times$ energy efficiency, for 2-bit and 4-bit quantized low-bit models, respectively. MVDRAM has the potential to redefine the AI hardware landscape by demonstrating the feasibility of standard DRAM as an LLM accelerator.

**Link**: [arxiv](http://arxiv.org/abs/2503.23817v1),  [pdf](http://arxiv.org/pdf/2503.23817v1)

**Tags**: cs.AR cs.DC 



### Did ChatGPT or Copilot use alter the style of internet news headlines? A   time series regression analysis
**Authors**: Chris Brogly, Connor McElroy

**Updated**: 2025-04-01T06:56:57Z

**Summary**: The release of advanced Large Language Models (LLMs) such as ChatGPT and Copilot is changing the way text is created and may influence the content that we find on the web. This study investigated whether the release of these two popular LLMs coincided with a change in writing style in headlines and links on worldwide news websites. 175 NLP features were obtained for each text in a dataset of 451 million headlines/links. An interrupted time series analysis was applied for each of the 175 NLP features to evaluate whether there were any statistically significant sustained changes after the release dates of ChatGPT and/or Copilot. There were a total of 44 features that did not appear to have any significant sustained change after the release of ChatGPT/Copilot. A total of 91 other features did show significant change with ChatGPT and/or Copilot although significance with earlier control LLM release dates (GPT-1/2/3, Gopher) removed them from consideration. This initial analysis suggests these language models may have had a limited impact on the style of individual news headlines/links, with respect to only some NLP measures.

**Link**: [arxiv](http://arxiv.org/abs/2503.23811v2),  [pdf](http://arxiv.org/pdf/2503.23811v2)

**Tags**: cs.CL cs.SI 



### Adaptive Attention-Based Model for 5G Radio-based Outdoor Localization
**Authors**: Ilayda Yaman, Guoda Tian, Fredrik Tufvesson, Ove Edfors, Zhengya Zhang, Liang Liu

**Updated**: 2025-03-31T07:44:14Z

**Summary**: Radio-based localization in dynamic environments, such as urban and vehicular settings, requires systems that can efficiently adapt to varying signal conditions and environmental changes. Factors such as multipath interference and obstructions introduce different levels of complexity that affect the accuracy of the localization. Although generalized models offer broad applicability, they often struggle to capture the nuances of specific environments, leading to suboptimal performance in real-world deployments. In contrast, specialized models can be tailored to particular conditions, enabling more precise localization by effectively handling domain-specific variations and noise patterns. However, deploying multiple specialized models requires an efficient mechanism to select the most appropriate one for a given scenario. In this work, we develop an adaptive localization framework that combines shallow attention-based models with a router/switching mechanism based on a single-layer perceptron (SLP). This enables seamless transitions between specialized localization models optimized for different conditions, balancing accuracy, computational efficiency, and robustness to environmental variations. We design three low-complex localization models tailored for distinct scenarios, optimized for reduced computational complexity, test time, and model size. The router dynamically selects the most suitable model based on real-time input characteristics. The proposed framework is validated using real-world vehicle localization data collected from a massive MIMO base station (BS), demonstrating its ability to seamlessly adapt to diverse deployment conditions while maintaining high localization accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2503.23810v1),  [pdf](http://arxiv.org/pdf/2503.23810v1)

**Tags**: eess.SP cs.LG 



### DSU-Net:An Improved U-Net Model Based on DINOv2 and SAM2 with   Multi-scale Cross-model Feature Enhancement
**Authors**: Yimin Xu, Fan Yang, Bin Xu

**Updated**: 2025-03-31T07:41:23Z

**Summary**: Despite the significant advancements in general image segmentation achieved by large-scale pre-trained foundation models (such as Meta's Segment Any-thing Model (SAM) series and DINOv2), their performance in specialized fields remains limited by two critical issues: the excessive training costs due to large model parameters, and the insufficient ability to represent specific domain characteristics. This paper proposes a multi-scale feature collabora-tion framework guided by DINOv2 for SAM2, with core innovations in three aspects: (1) Establishing a feature collaboration mechanism between DINOv2 and SAM2 backbones, where high-dimensional semantic features extracted by the self-supervised model guide multi-scale feature fusion; (2) Designing lightweight adapter modules and cross-modal, cross-layer feature fusion units to inject cross-domain knowledge while freezing the base model parameters; (3) Constructing a U-shaped network structure based on U-net, which utilizes attention mechanisms to achieve adaptive aggregation decoding of multi-granularity features. This framework surpasses existing state-of-the-art meth-ods in downstream tasks such as camouflage target detection and salient ob-ject detection, without requiring costly training processes. It provides a tech-nical pathway for efficient deployment of visual image segmentation, demon-strating significant application value in a wide range of downstream tasks and specialized fields within image segmentation.Project page: https://github.com/CheneyXuYiMin/SAM2DINO-Seg

**Link**: [arxiv](http://arxiv.org/abs/2503.21187v2),  [pdf](http://arxiv.org/pdf/2503.21187v2)

**Tags**: cs.CV 



### Thinking Longer, Not Larger: Enhancing Software Engineering Agents via   Scaling Test-Time Compute
**Authors**: Yingwei Ma, Binhua Li, Yihong Dong, Xue Jiang, Rongyu Cao, Jue Chen, Fei Huang, Yongbin Li

**Updated**: 2025-03-31T07:31:32Z

**Summary**: Recent advancements in software engineering agents have demonstrated promising capabilities in automating program improvements. However, their reliance on closed-source or resource-intensive models introduces significant deployment challenges in private environments, prompting a critical question: \textit{How can personally deployable open-source LLMs achieve comparable code reasoning performance?}   To this end, we propose a unified Test-Time Compute scaling framework that leverages increased inference-time computation instead of larger models. Our framework incorporates two complementary strategies: internal TTC and external TTC. Internally, we introduce a \textit{development-contextualized trajectory synthesis} method leveraging real-world software repositories to bootstrap multi-stage reasoning processes, such as fault localization and patch generation. We further enhance trajectory quality through rejection sampling, rigorously evaluating trajectories along accuracy and complexity. Externally, we propose a novel \textit{development-process-based search} strategy guided by reward models and execution verification. This approach enables targeted computational allocation at critical development decision points, overcoming limitations of existing "end-point only" verification methods.   Evaluations on SWE-bench Verified demonstrate our \textbf{32B model achieves a 46\% issue resolution rate}, surpassing significantly larger models such as DeepSeek R1 671B and OpenAI o1. Additionally, we provide the empirical validation of the test-time scaling phenomenon within SWE agents, revealing that \textbf{models dynamically allocate more tokens to increasingly challenging problems}, effectively enhancing reasoning capabilities. We publicly release all training data, models, and code to facilitate future research. https://github.com/yingweima2022/SWE-Reasoner

**Link**: [arxiv](http://arxiv.org/abs/2503.23803v1),  [pdf](http://arxiv.org/pdf/2503.23803v1)

**Tags**: cs.SE cs.AI 



### A PINN Methodology for Temperature Field Reconstruction in the PIV   Measurement Plane: Case of Rayleigh-BÃ©nard Convection
**Authors**: Marie-Christine Volk, Anne Sergent, Didier Lucor, Michael Mommert, Christian Bauer, Claus Wagner

**Updated**: 2025-03-31T07:24:06Z

**Summary**: We present a method to infer temperature fields from stereo particle-image velocimetry (PIV) data in turbulent Rayleigh-B\'enard convection (RBC) using Physics-informed neural networks (PINNs). The physical setup is a cubic RBC cell with Rayleigh number $\text{Ra}=10^7$ and Prandtl number $\text{Pr}=0.7$. With data only available in a vertical plane $A:x=x_0$, the residuals of the governing partial differential equations are minimised in an enclosing 3D domain around $A$ with thickness $\delta_x$. Dynamic collocation point sampling strategies are used to overcome the lack of 3D labelled information and to optimize the overall convergence of the PINN. In particular, in the out-of-plane direction $x$, the collocation points are distributed according to a normal distribution, in order to emphasize the region where data is provided. Along the vertical direction, we leverage meshing information and sample points from a distribution designed based on the grid of a direct numerical simulation (DNS). This approach points greater attention to critical regions, particularly the areas with high temperature gradients within the thermal boundary layers. Using planar three-component velocity data from a DNS, we successfully validate the reconstruction of the temperature fields in the PIV plane. We evaluate the robustness of our method with respect to characteristics of the labelled data used for training: the data time span, the sampling frequency, some noisy data and boundary data omission, aiming to better accommodate the challenges associated with experimental data. Developing PINNs on controlled simulation data is a crucial step toward their effective deployment on experimental data. The key is to systematically introduce noise, gaps, and uncertainties in simulated data to mimic real-world conditions and ensure robust generalization.

**Link**: [arxiv](http://arxiv.org/abs/2503.23801v1),  [pdf](http://arxiv.org/pdf/2503.23801v1)

**Tags**: physics.flu-dyn 



### Adaptive Layer-skipping in Pre-trained LLMs
**Authors**: Xuan Luo, Weizhi Wang, Xifeng Yan

**Updated**: 2025-03-31T07:20:58Z

**Summary**: Various layer-skipping methods have been proposed to accelerate token generation in large language models (LLMs). However, they have overlooked a fundamental question: How do computational demands vary across the generation of different tokens? In this work, we introduce FlexiDepth, a method that dynamically adjusts the number of Transformer layers used in text generation. By incorporating a plug-in router and adapter, FlexiDepth enables adaptive layer-skipping in LLMs without modifying their original parameters. Introducing FlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32, and meanwhile maintains the full 100\% benchmark performance. Experimental results with FlexiDepth demonstrate that computational demands in LLMs significantly vary based on token type. Specifically, generating repetitive tokens or fixed phrases requires fewer layers, whereas producing tokens involving computation or high uncertainty requires more layers. Interestingly, this adaptive allocation pattern aligns with human intuition. To advance research in this area, we open sourced FlexiDepth and a dataset documenting FlexiDepth's layer allocation patterns for future exploration.

**Link**: [arxiv](http://arxiv.org/abs/2503.23798v1),  [pdf](http://arxiv.org/pdf/2503.23798v1)

**Tags**: cs.CL cs.AI 



### LLMigrate: Transforming "Lazy" Large Language Models into Efficient   Source Code Migrators
**Authors**: Yuchen Liu, Junhao Hu, Yingdi Shan, Ge Li, Yanzhen Zou, Yihong Dong, Tao Xie

**Updated**: 2025-03-31T07:09:07Z

**Summary**: Rewriting C code in Rust provides stronger memory safety, yet migrating large codebases such as the 32-million-line Linux kernel remains challenging. While rule-based translators (e.g., C2Rust) provide accurate yet largely unsafe Rust programs, recent Large Language Model (LLM) approaches produce more idiomatic, safe Rust programs but frequently exhibit "laziness", omitting significant portions of the target code. To address the issue, in this paper, we present LLMigrate, an LLM-based C-to-Rust translation tool that splits modules into discrete functions, translating them individually, and then reintegrating them. LLMigrate uses static analysis to retain necessary context, pairs GPT-4o (a state-of-the-art LLM) with compiler-driven translation and program-repair techniques for complex core functions, and leverages call-graph-guided translation to ensure consistent interfaces. Evaluations on three representative Linux kernel modules (math, sort, and ramfs) show that LLMigrate requires modifying less than 15\% of the target code, significantly outperforming a pure GPT-4o-based migration.

**Link**: [arxiv](http://arxiv.org/abs/2503.23791v1),  [pdf](http://arxiv.org/pdf/2503.23791v1)

**Tags**: cs.PL cs.SE 



### SwiftCoder: Enhancing Code Generation in Large Language Models through   Efficiency-Aware Fine-tuning
**Authors**: Dong Huang, Guangtao Zeng, Jianbo Dai, Meng Luo, Han Weng, Yuhao Qing, Heming Cui, Zhijiang Guo, Jie M. Zhang

**Updated**: 2025-03-31T07:00:08Z

**Summary**: As large language models (LLMs) play an increasingly important role in code generation, enhancing both correctness and efficiency has become crucial. Current methods primarily focus on correctness, often overlooking efficiency. To address this gap, we introduce \dataset to improve both aspects by fine-tuning LLMs on a high-quality dataset comprising correct and efficient code samples. Our methodology involves leveraging multiple LLMs to generate diverse candidate code solutions for various tasks across different programming languages. We then evaluate these solutions by directly measuring their execution time and memory usage through local execution. The code solution with the lowest execution time and memory consumption is selected as the final output for each task. Experimental results demonstrate significant improvements when fine-tuning with \dataset. For instance, Qwen2.5-Coder-7B-Instruct's pass@1 score increases from 44.8\% to 57.7\%, while the average execution time for correct tasks decreases by 48.4\%. \dataset offers a scalable and effective solution for advancing AI-driven code generation, benefiting both software development and computational problem-solving. The source code of Effi-Code was released in https://github.com/huangd1999/Effi-Code.

**Link**: [arxiv](http://arxiv.org/abs/2410.10209v3),  [pdf](http://arxiv.org/pdf/2410.10209v3)

**Tags**: cs.CL cs.SE 



### DebFlow: Automating Agent Creation via Agent Debate
**Authors**: Jinwei Su, Yinghui Xia, Ronghua Shi, Jianhui Wang, Jianuo Huang, Yijin Wang, Tianyu Shi, Yang Jingsong, Lewei He

**Updated**: 2025-03-31T06:56:13Z

**Summary**: Large language models (LLMs) have demonstrated strong potential and impressive performance in automating the generation and optimization of workflows. However, existing approaches are marked by limited reasoning capabilities, high computational demands, and significant resource requirements. To address these issues, we propose DebFlow, a framework that employs a debate mechanism to optimize workflows and integrates reflexion to improve based on previous experiences. We evaluated our method across six benchmark datasets, including HotpotQA, MATH, and ALFWorld. Our approach achieved a 3\% average performance improvement over the latest baselines, demonstrating its effectiveness in diverse problem domains. In particular, during training, our framework reduces resource consumption by 37\% compared to the state-of-the-art baselines. Additionally, we performed ablation studies. Removing the Debate component resulted in a 4\% performance drop across two benchmark datasets, significantly greater than the 2\% drop observed when the Reflection component was removed. These findings strongly demonstrate the critical role of Debate in enhancing framework performance, while also highlighting the auxiliary contribution of reflexion to overall optimization.

**Link**: [arxiv](http://arxiv.org/abs/2503.23781v1),  [pdf](http://arxiv.org/pdf/2503.23781v1)

**Tags**: cs.AI 



### WinoWhat: A Parallel Corpus of Paraphrased WinoGrande Sentences with   Common Sense Categorization
**Authors**: Ine Gevers, Victor De Marez, Luna De Bruyne, Walter Daelemans

**Updated**: 2025-03-31T06:53:53Z

**Summary**: In this study, we take a closer look at how Winograd schema challenges can be used to evaluate common sense reasoning in LLMs. Specifically, we evaluate generative models of different sizes on the popular WinoGrande benchmark. We release WinoWhat, a new corpus, in which each instance of the WinoGrande validation set is paraphrased. Additionally, we evaluate the performance on the challenge across five common sense knowledge categories, giving more fine-grained insights on what types of knowledge are more challenging for LLMs. Surprisingly, all models perform significantly worse on WinoWhat, implying that LLM reasoning capabilities are overestimated on WinoGrande. To verify whether this is an effect of benchmark memorization, we match benchmark instances to LLM trainingdata and create two test-suites. We observe that memorization has a minimal effect on model performance on WinoGrande.

**Link**: [arxiv](http://arxiv.org/abs/2503.23779v1),  [pdf](http://arxiv.org/pdf/2503.23779v1)

**Tags**: cs.CL cs.AI 



### CONGRAD:Conflicting Gradient Filtering for Multilingual Preference   Alignment
**Authors**: Jiangnan Li, Thuy-Trang Vu, Christian Herold, Amirhossein Tebbifakhr, Shahram Khadivi, Gholamreza Haffari

**Updated**: 2025-03-31T06:52:56Z

**Summary**: Naive joint training of large language models (LLMs) for multilingual preference alignment can suffer from negative interference. This is a known issue in multilingual training, where conflicting objectives degrade overall performance. However, the impact of this phenomenon in the context of multilingual preference alignment remains largely underexplored. To address this issue, we propose CONGRAD, a scalable and effective filtering method that selects high-quality preference samples with minimal gradient conflicts across languages. Our method leverages gradient surgery to retain samples aligned with an aggregated multilingual update direction. Additionally, we incorporate a sublinear gradient compression strategy that reduces memory overhead during gradient accumulation. We integrate CONGRAD into self-rewarding framework and evaluate on LLaMA3-8B and Gemma2-2B across 10 languages. Results show that CONGRAD consistently outperforms strong baselines in both seen and unseen languages, with minimal alignment tax.

**Link**: [arxiv](http://arxiv.org/abs/2503.23777v1),  [pdf](http://arxiv.org/pdf/2503.23777v1)

**Tags**: cs.CL 



### XLRS-Bench: Could Your Multimodal LLMs Understand Extremely Large   Ultra-High-Resolution Remote Sensing Imagery?
**Authors**: Fengxiang Wang, Hongzhen Wang, Mingshuo Chen, Di Wang, Yulin Wang, Zonghao Guo, Qiang Ma, Long Lan, Wenjing Yang, Jing Zhang, Zhiyuan Liu, Maosong Sun

**Updated**: 2025-03-31T06:41:18Z

**Summary**: The astonishing breakthrough of multimodal large language models (MLLMs) has necessitated new benchmarks to quantitatively assess their capabilities, reveal their limitations, and indicate future research directions. However, this is challenging in the context of remote sensing (RS), since the imagery features ultra-high resolution that incorporates extremely complex semantic relationships. Existing benchmarks usually adopt notably smaller image sizes than real-world RS scenarios, suffer from limited annotation quality, and consider insufficient dimensions of evaluation. To address these issues, we present XLRS-Bench: a comprehensive benchmark for evaluating the perception and reasoning capabilities of MLLMs in ultra-high-resolution RS scenarios. XLRS-Bench boasts the largest average image size (8500$\times$8500) observed thus far, with all evaluation samples meticulously annotated manually, assisted by a novel semi-automatic captioner on ultra-high-resolution RS images. On top of the XLRS-Bench, 16 sub-tasks are defined to evaluate MLLMs' 10 kinds of perceptual capabilities and 6 kinds of reasoning capabilities, with a primary emphasis on advanced cognitive processes that facilitate real-world decision-making and the capture of spatiotemporal changes. The results of both general and RS-focused MLLMs on XLRS-Bench indicate that further efforts are needed for real-world RS applications. We have open-sourced XLRS-Bench to support further research in developing more powerful MLLMs for remote sensing.

**Link**: [arxiv](http://arxiv.org/abs/2503.23771v1),  [pdf](http://arxiv.org/pdf/2503.23771v1)

**Tags**: cs.CV 



### Know "No'' Better: A Data-Driven Approach for Enhancing Negation   Awareness in CLIP
**Authors**: Junsung Park, Jungbeom Lee, Jongyoon Song, Sangwon Yu, Dahuin Jung, Sungroh Yoon

**Updated**: 2025-03-31T06:38:48Z

**Summary**: While CLIP has significantly advanced multimodal understanding by bridging vision and language, the inability to grasp negation - such as failing to differentiate concepts like "parking" from "no parking" - poses substantial challenges. By analyzing the data used in the public CLIP model's pre-training, we posit this limitation stems from a lack of negation-inclusive data. To address this, we introduce data generation pipelines that employ a large language model (LLM) and a multimodal LLM to produce negation-inclusive captions. Fine-tuning CLIP with data generated from our pipelines, we develop NegationCLIP, which enhances negation awareness while preserving the generality. Moreover, to enable a comprehensive evaluation of negation understanding, we propose NegRefCOCOg-a benchmark tailored to test VLMs' ability to interpret negation across diverse expressions and positions within a sentence. Experiments on various CLIP architectures validate the effectiveness of our data generation pipelines in enhancing CLIP's ability to perceive negation accurately. Additionally, NegationCLIP's enhanced negation awareness has practical applications across various multimodal tasks, demonstrated by performance gains in text-to-image generation and referring image segmentation.

**Link**: [arxiv](http://arxiv.org/abs/2501.10913v2),  [pdf](http://arxiv.org/pdf/2501.10913v2)

**Tags**: cs.CV cs.CL 



### WaveFormer: A 3D Transformer with Wavelet-Driven Feature Representation   for Efficient Medical Image Segmentation
**Authors**: Md Mahfuz Al Hasan, Mahdi Zaman, Abdul Jawad, Alberto Santamaria-Pang, Ho Hin Lee, Ivan Tarapov, Kyle See, Md Shah Imran, Antika Roy, Yaser Pourmohammadi Fallah, Navid Asadizanjani, Reza Forghani

**Updated**: 2025-04-01T02:13:23Z

**Summary**: Transformer-based architectures have advanced medical image analysis by effectively modeling long-range dependencies, yet they often struggle in 3D settings due to substantial memory overhead and insufficient capture of fine-grained local features. We address these limitations with WaveFormer, a novel 3D-transformer that: i) leverages the fundamental frequency-domain properties of features for contextual representation, and ii) is inspired by the top-down mechanism of the human visual recognition system, making it a biologically motivated architecture. By employing discrete wavelet transformations (DWT) at multiple scales, WaveFormer preserves both global context and high-frequency details while replacing heavy upsampling layers with efficient wavelet-based summarization and reconstruction. This significantly reduces the number of parameters, which is critical for real-world deployment where computational resources and training times are constrained. Furthermore, the model is generic and easily adaptable to diverse applications. Evaluations on BraTS2023, FLARE2021, and KiTS2023 demonstrate performance on par with state-of-the-art methods while offering substantially lower computational complexity.

**Link**: [arxiv](http://arxiv.org/abs/2503.23764v2),  [pdf](http://arxiv.org/pdf/2503.23764v2)

**Tags**: cs.CV cs.AI 



### UniSep: Universal Target Audio Separation with Language Models at Scale
**Authors**: Yuanyuan Wang, Hangting Chen, Dongchao Yang, Weiqin Li, Dan Luo, Guangzhi Li, Shan Yang, Zhiyong Wu, Helen Meng, Xixin Wu

**Updated**: 2025-03-31T06:27:37Z

**Summary**: We propose Universal target audio Separation (UniSep), addressing the separation task on arbitrary mixtures of different types of audio. Distinguished from previous studies, UniSep is performed on unlimited source domains and unlimited source numbers. We formulate the separation task as a sequence-to-sequence problem, and a large language model (LLM) is used to model the audio sequence in the discrete latent space, leveraging the power of LLM in handling complex mixture audios with large-scale data. Moreover, a novel pre-training strategy is proposed to utilize audio-only data, which reduces the efforts of large-scale data simulation and enhances the ability of LLMs to understand the consistency and correlation of information within audio sequences. We also demonstrate the effectiveness of scaling datasets in an audio separation task: we use large-scale data (36.5k hours), including speech, music, and sound, to train a universal target audio separation model that is not limited to a specific domain. Experiments show that UniSep achieves competitive subjective and objective evaluation results compared with single-task models.

**Link**: [arxiv](http://arxiv.org/abs/2503.23762v1),  [pdf](http://arxiv.org/pdf/2503.23762v1)

**Tags**: cs.SD eess.AS 



### THEMIS: Towards Practical Intellectual Property Protection for   Post-Deployment On-Device Deep Learning Models
**Authors**: Yujin Huang, Zhi Zhang, Qingchuan Zhao, Xingliang Yuan, Chunyang Chen

**Updated**: 2025-03-31T05:58:57Z

**Summary**: On-device deep learning (DL) has rapidly gained adoption in mobile apps, offering the benefits of offline model inference and user privacy preservation over cloud-based approaches. However, it inevitably stores models on user devices, introducing new vulnerabilities, particularly model-stealing attacks and intellectual property infringement. While system-level protections like Trusted Execution Environments (TEEs) provide a robust solution, practical challenges remain in achieving scalable on-device DL model protection, including complexities in supporting third-party models and limited adoption in current mobile solutions. Advancements in TEE-enabled hardware, such as NVIDIA's GPU-based TEEs, may address these obstacles in the future. Currently, watermarking serves as a common defense against model theft but also faces challenges here as many mobile app developers lack corresponding machine learning expertise and the inherent read-only and inference-only nature of on-device DL models prevents third parties like app stores from implementing existing watermarking techniques in post-deployment models.   To protect the intellectual property of on-device DL models, in this paper, we propose THEMIS, an automatic tool that lifts the read-only restriction of on-device DL models by reconstructing their writable counterparts and leverages the untrainable nature of on-device DL models to solve watermark parameters and protect the model owner's intellectual property. Extensive experimental results across various datasets and model structures show the superiority of THEMIS in terms of different metrics. Further, an empirical investigation of 403 real-world DL mobile apps from Google Play is performed with a success rate of 81.14%, showing the practicality of THEMIS.

**Link**: [arxiv](http://arxiv.org/abs/2503.23748v1),  [pdf](http://arxiv.org/pdf/2503.23748v1)

**Tags**: cs.CR cs.LG cs.SE 



### Short-video Propagation Influence Rating: A New Real-world Dataset and A   New Large Graph Model
**Authors**: Dizhan Xue, Jing Cui, Shengsheng Qian, Chuanrui Hu, Changsheng Xu

**Updated**: 2025-03-31T05:53:15Z

**Summary**: Short-video platforms have gained immense popularity, captivating the interest of millions, if not billions, of users globally. Recently, researchers have highlighted the significance of analyzing the propagation of short-videos, which typically involves discovering commercial values, public opinions, user behaviors, etc. This paper proposes a new Short-video Propagation Influence Rating (SPIR) task and aims to promote SPIR from both the dataset and method perspectives. First, we propose a new Cross-platform Short-Video (XS-Video) dataset, which aims to provide a large-scale and real-world short-video propagation network across various platforms to facilitate the research on short-video propagation. Our XS-Video dataset includes 117,720 videos, 381,926 samples, and 535 topics across 5 biggest Chinese platforms, annotated with the propagation influence from level 0 to 9. To the best of our knowledge, this is the first large-scale short-video dataset that contains cross-platform data or provides all of the views, likes, shares, collects, fans, comments, and comment content. Second, we propose a Large Graph Model (LGM) named NetGPT, based on a novel three-stage training mechanism, to bridge heterogeneous graph-structured data with the powerful reasoning ability and knowledge of Large Language Models (LLMs). Our NetGPT can comprehend and analyze the short-video propagation graph, enabling it to predict the long-term propagation influence of short-videos. Comprehensive experimental results evaluated by both classification and regression metrics on our XS-Video dataset indicate the superiority of our method for SPIR.

**Link**: [arxiv](http://arxiv.org/abs/2503.23746v1),  [pdf](http://arxiv.org/pdf/2503.23746v1)

**Tags**: cs.CV cs.CL cs.LG cs.MM cs.SI 



### DUNE Software and Computing Research and Development
**Authors**: DUNE Collaboration, A. Abed Abud, R. Acciarri, M. A. Acero, M. R. Adames, G. Adamov, M. Adamowski, D. Adams, M. Adinolfi, C. Adriano, A. Aduszkiewicz, J. Aguilar, F. Akbar, F. Alemanno, N. S. Alex, K. Allison, M. Alrashed, A. Alton, R. Alvarez, T. Alves, A. Aman, H. Amar, P. Amedo, J. Anderson, D. A. Andrade, C. Andreopoulos, M. Andreotti, M. P. Andrews, F. Andrianala, S. Andringa, F. Anjarazafy, D. Antic, M. Antoniassi, M. Antonova, A. Aranda-Fernandez, L. Arellano, E. Arrieta Diaz, M. A. Arroyave, J. Asaadi, M. Ascencio, A. Ashkenazi, D. Asner, L. Asquith, E. Atkin, D. Auguste, A. Aurisano, V. Aushev, D. Autiero, D. Ãvila GÃ³mez, M. B. Azam, F. Azfar, A. Back, H. Back, J. J. Back, I. Bagaturia, L. Bagby, D. Baigarashev, S. Balasubramanian, A. Balboni, P. Baldi, W. Baldini, J. Baldonedo, B. Baller, B. Bambah, R. Banerjee, F. Barao, D. Barbu, G. Barenboim, P. \ Barham AlzÃ¡s, G. J. Barker, W. Barkhouse, G. Barr, J. Barranco Monarca, A. Barros, N. Barros, D. Barrow, J. L. Barrow, A. Basharina-Freshville, A. Bashyal, V. Basque, D. Basu, C. Batchelor, L. Bathe-Peters, J. B. R. Battat, F. Battisti, F. Bay, M. C. Q. Bazetto, J. L. L. Bazo Alba, J. F. Beacom, E. Bechetoille, B. Behera, E. Belchior, B. Bell, G. Bell, L. Bellantoni, G. Bellettini, V. Bellini, O. Beltramello, C. Benitez Montiel, D. Benjamin, F. Bento Neves, J. Berger, S. Berkman, J. Bernal, P. Bernardini, A. Bersani, E. Bertolini, S. Bertolucci, M. Betancourt, A. Betancur RodrÃ­guez, Y. Bezawada, A. T. Bezerra, A. Bhat, V. Bhatnagar, J. Bhatt, M. Bhattacharjee, M. Bhattacharya, S. Bhuller, B. Bhuyan, S. Biagi, J. Bian, K. Biery, B. Bilki, M. Bishai, A. Blake, F. D. Blaszczyk, G. C. Blazey, E. Blucher, B. Bogart, J. Bogenschuetz, J. Boissevain, S. Bolognesi, T. Bolton, L. Bomben, M. Bonesini, C. Bonilla-Diaz, A. Booth, F. Boran, R. Borges Merlo, N. Bostan, G. Botogoske, B. Bottino, R. Bouet, J. Boza, J. Bracinik, B. Brahma, D. Brailsford, F. Bramati, A. Branca, A. Brandt, J. Bremer, S. J. Brice, V. Brio, C. Brizzolari, C. Bromberg, J. Brooke, A. Bross, G. Brunetti, M. B. Brunetti, N. Buchanan, H. Budd, J. Buergi, A. Bundock, D. Burgardt, S. Butchart, G. Caceres V., T. Cai, R. Calabrese, R. Calabrese, J. Calcutt, L. Calivers, E. Calvo, A. Caminata, A. F. Camino, W. Campanelli, A. Campani, A. Campos Benitez, N. Canci, J. CapÃ³, I. Caracas, D. Caratelli, D. Carber, J. M. Carceller, G. Carini, B. Carlus, M. F. Carneiro, P. Carniti, I. Caro Terrazas, H. Carranza, N. Carrara, L. Carroll, T. Carroll, A. Carter, E. Casarejos, D. Casazza, J. F. CastaÃ±o Forero, F. A. CastaÃ±o, A. Castillo, C. Castromonte, E. Catano-Mur, C. Cattadori, F. Cavalier, F. Cavanna, S. Centro, G. Cerati, C. Cerna, A. Cervelli, A. Cervera Villanueva, J. Chakrani, M. Chalifour, A. Chappell, A. Chatterjee, B. Chauhan, C. Chavez Barajas, H. Chen, M. Chen, W. C. Chen, Y. Chen, Z. Chen, D. Cherdack, S. S. Chhibra, C. Chi, F. Chiapponi, R. Chirco, N. Chitirasreemadam, K. Cho, S. Choate, G. Choi, D. Chokheli, P. S. Chong, B. Chowdhury, D. Christian, M. Chung, E. Church, M. F. Cicala, M. Cicerchia, V. Cicero, R. Ciolini, P. Clarke, G. Cline, A. G. Cocco, J. A. B. Coelho, A. Cohen, J. Collazo, J. Collot, J. M. Conrad, M. Convery, K. Conway, S. Copello, P. Cova, C. Cox, L. Cremonesi, J. I. Crespo-AnadÃ³n, M. Crisler, E. Cristaldo, J. Crnkovic, G. Crone, R. Cross, A. Cudd, C. Cuesta, Y. Cui, F. Curciarello, D. Cussans, J. Dai, O. Dalager, W. Dallaway, R. D'Amico, H. da Motta, Z. A. Dar, R. Darby, L. Da Silva Peres, Q. David, G. S. Davies, S. Davini, J. Dawson, R. De Aguiar, P. De Almeida, P. Debbins, M. P. Decowski, A. de GouvÃªa, P. C. De Holanda, P. De Jong, P. Del Amo Sanchez, G. De Lauretis, A. Delbart, D. Delepine, M. Delgado, A. Dell'Acqua, G. Delle Monache, N. Delmonte, P. De Lurgio, R. Demario, G. De Matteis, J. R. T. de Mello Neto, A. P. A. De Mendonca, D. M. DeMuth, S. Dennis, C. Densham, P. Denton, G. W. Deptuch, A. De Roeck, V. De Romeri, J. P. Detje, J. Devine, R. Dharmapalan, M. Dias, A. Diaz, J. S. DÃ­az, F. DÃ­az, F. Di Capua, A. Di Domenico, S. Di Domizio, S. Di Falco, L. Di Giulio, P. Ding, L. Di Noto, E. Diociaiuti, V. Di Silvestre, C. Distefano, R. Diurba, M. Diwan, Z. Djurcic, S. Dolan, M. Dolce, F. Dolek, M. J. Dolinski, D. Domenici, S. Donati, Y. Donon, S. Doran, D. Douglas, T. A. Doyle, F. Drielsma, L. Duarte, D. Duchesneau, K. Duffy, K. Dugas, P. Dunne, B. Dutta, H. Duyang, D. A. Dwyer, A. S. Dyshkant, S. Dytman, M. Eads, A. Earle, S. Edayath, D. Edmunds, J. Eisch, W. Emark, P. Englezos, A. Ereditato, T. Erjavec, C. O. Escobar, J. J. Evans, E. Ewart, A. C. Ezeribe, K. Fahey, A. Falcone, M. Fani', C. Farnese, S. Farrell, Y. Farzan, J. Felix, Y. Feng, M. Ferreira da Silva, G. Ferry, E. Fialova, L. Fields, P. Filip, A. Filkins, F. Filthaut, G. Fiorillo, M. Fiorini, S. Fogarty, W. Foreman, J. Fowler, J. Franc, K. Francis, D. Franco, J. Freeman, J. Fried, A. Friedland, M. Fucci, S. Fuess, I. K. Furic, K. Furman, A. P. Furmanski, R. Gaba, A. Gabrielli, A. M. Gago, F. Galizzi, H. Gallagher, M. Galli, N. Gallice, V. Galymov, E. Gamberini, T. Gamble, R. Gandhi, S. Ganguly, F. Gao, S. Gao, D. Garcia-Gamez, M. Ã. GarcÃ­a-Peris, F. Gardim, S. Gardiner, D. Gastler, A. Gauch, P. Gauzzi, S. Gazzana, G. Ge, N. Geffroy, B. Gelli, S. Gent, L. Gerlach, A. Ghosh, T. Giammaria, D. Gibin, I. Gil-Botella, S. Gilligan, A. Gioiosa, S. Giovannella, A. K. Giri, C. Giugliano, V. Giusti, D. Gnani, O. Gogota, S. Gollapinni, K. Gollwitzer, R. A. Gomes, L. V. Gomez Bermeo, L. S. Gomez Fajardo, D. Gonzalez-Diaz, M. C. Goodman, S. Goswami, C. Gotti, J. Goudeau, E. Goudzovski, C. Grace, E. Gramellini, R. Gran, E. Granados, P. Granger, C. Grant, D. R. Gratieri, G. Grauso, P. Green, S. Greenberg, J. Greer, W. C. Griffith, K. Grzelak, L. Gu, W. Gu, V. Guarino, M. Guarise, R. Guenette, M. Guerzoni, D. Guffanti, A. Guglielmi, B. Guo, F. Y. Guo, V. Gupta, G. Gurung, D. Gutierrez, P. Guzowski, M. M. Guzzo, S. Gwon, A. Habig, L. Haegel, R. Hafeji, L. Hagaman, A. Hahn, J. HakenmÃ¼ller, T. Hamernik, P. Hamilton, J. Hancock, M. Handley, F. Happacher, D. A. Harris, A. L. Hart, J. Hartnell, T. Hartnett, J. Harton, T. Hasegawa, C. M. Hasnip, R. Hatcher, S. Hawkins, J. Hays, M. He, A. Heavey, K. M. Heeger, A. Heindel, J. Heise, P. Hellmuth, L. Henderson, K. Herner, V. Hewes, A. Higuera, C. Hilgenberg, A. Himmel, E. Hinkle, L. R. Hirsch, J. Ho, J. Hoefken Zink, J. Hoff, A. Holin, T. Holvey, C. Hong, E. Hoppe, S. Horiuchi, G. A. Horton-Smith, R. Hosokawa, T. Houdy, B. Howard, R. Howell, I. Hristova, M. S. Hronek, H. Hua, J. Huang, R. G. Huang, X. Huang, Z. Hulcher, G. Iles, N. Ilic, A. M. Iliescu, R. Illingworth, G. Ingratta, A. Ioannisian, B. Irwin, M. Ismerio Oliveira, C. M. Jackson, V. Jain, E. James, W. Jang, B. Jargowsky, D. Jena, I. Jentz, X. Ji, C. Jiang, J. Jiang, A. Jipa, J. H. Jo, F. R. Joaquim, W. Johnson, C. Jollet, R. Jones, N. Jovancevic, M. Judah, C. K. Jung, K. Y. Jung, T. Junk, Y. Jwa, M. Kabirnezhad, A. C. Kaboth, I. Kadenko, O. Kalikulov, D. Kalra, M. Kandemir, D. M. Kaplan, G. Karagiorgi, G. Karaman, A. Karcher, Y. Karyotakis, S. P. Kasetti, L. Kashur, A. Kauther, N. Kazaryan, L. Ke, E. Kearns, P. T. Keener, K. J. Kelly, R. Keloth, E. Kemp, O. Kemularia, Y. Kermaidic, W. Ketchum, S. H. Kettell, N. Khan, A. Khvedelidze, D. Kim, J. Kim, M. J. Kim, S. Kim, B. King, M. King, M. Kirby, A. Kish, J. Klein, J. Kleykamp, A. Klustova, T. Kobilarcik, L. Koch, K. Koehler, L. W. Koerner, D. H. Koh, M. Kordosky, T. Kosc, V. A. KosteleckÃ½, K. Kothekar, I. Kotler, M. Kovalcuk, R. Kralik, M. Kramer, L. Kreczko, F. Krennrich, T. Kroupova, S. Kubota, M. Kubu, V. A. Kudryavtsev, G. Kufatty, S. Kuhlmann, J. Kumar, M. Kumar, P. Kumar, P. Kumar, S. Kumaran, J. Kunzmann, R. Kuravi, V. Kus, T. Kutter, J. Kvasnicka, T. Labree, T. Lackey, I. LalÄu, A. Lambert, B. J. Land, C. E. Lane, N. Lane, K. Lang, T. Langford, M. Langstaff, F. Lanni, J. Larkin, P. Lasorak, D. Last, A. Laundrie, G. Laurenti, E. Lavaut, P. Laycock, I. Lazanu, R. LaZur, M. Lazzaroni, T. Le, S. Leardini, J. Learned, T. LeCompte, G. Lehmann Miotto, R. Lehnert, M. Leitner, H. Lemoine, D. Leon Silverio, L. M. Lepin, J. -Y Li, S. W. Li, Y. Li, H. Liao, R. Lima, C. S. Lin, D. Lindebaum, S. Linden, R. A. Lineros, A. Lister, B. R. Littlejohn, H. Liu, J. Liu, Y. Liu, S. Lockwitz, I. Lomidze, K. Long, T. V. Lopes, J. Lopez, I. LÃ³pez de Rego, N. LÃ³pez-March, J. M. LoSecco, W. C. Louis, A. Lozano Sanchez, X. -G. Lu, K. B. Luk, X. Luo, E. Luppi, A. A. Machado, P. Machado, C. T. Macias, J. R. Macier, M. MacMahon, S. Magill, C. Magueur, K. Mahn, A. Maio, A. Major, K. Majumdar, A. Malige, S. Mameli, M. Man, R. C. Mandujano, J. Maneira, S. Manly, A. Mann, K. Manolopoulos, M. Manrique Plata, S. Manthey Corchado, V. N. Manyam, L. Manzanillas-Velez, M. Marchan, A. Marchionni, W. Marciano, D. Marfatia, C. Mariani, J. Maricic, F. Marinho, A. D. Marino, T. Markiewicz, F. Das Chagas Marques, C. Marquet, M. Marshak, C. M. Marshall, J. Marshall, L. Martina, J. MartÃ­n-Albo, N. Martinez, D. A. Martinez Caicedo, M. Martinez-Casales, F. MartÃ­nez LÃ³pez, P. MartÃ­nez MiravÃ©, S. Martynenko, V. Mascagna, A. Mastbaum, M. Masud, F. Matichard, G. Matteucci, J. Matthews, C. Mauger, N. Mauri, K. Mavrokoridis, I. Mawby, F. Mayhew, R. Mazza, T. McAskill, N. McConkey, K. S. McFarland, C. McGrew, A. McNab, C. McNulty, J. Mead, L. Meazza, V. C. N. Meddage, M. Mehmood, B. Mehta, P. Mehta, F. Mei, P. Melas, L. Mellet, O. Mena, H. Mendez, D. P. MÃ©ndez, A. Menegolli, G. Meng, A. C. E. A. Mercuri, A. Meregaglia, M. D. Messier, S. Metallo, W. Metcalf, M. Mewes, H. Meyer, T. Miao, J. Micallef, A. Miccoli, G. Michna, R. Milincic, F. Miller, G. Miller, W. Miller, A. Minotti, L. Miralles Verge, C. Mironov, S. Miryala, S. Miscetti, C. S. Mishra, P. Mishra, S. R. Mishra, A. Mislivec, D. Mladenov, I. Mocioiu, A. Mogan, R. Mohanta, T. A. Mohayai, N. Mokhov, J. Molina, L. Molina Bueno, E. Montagna, A. Montanari, C. Montanari, D. Montanari, D. Montanino, L. M. MontaÃ±o Zetina, M. Mooney, A. F. Moor, M. Moore, Z. Moore, D. Moreno, G. Moreno-Granados, O. Moreno-Palacios, L. Morescalchi, R. Moretti, C. Morris, C. Mossey, E. Motuk, C. A. Moura, G. Mouster, W. Mu, L. Mualem, J. Mueller, M. Muether, F. Muheim, A. Muir, Y. Mukhamejanov, A. Mukhamejanova, M. Mulhearn, D. Munford, L. J. Munteanu, H. Muramatsu, J. Muraz, M. Murphy, T. Murphy, J. Muse, A. Mytilinaki, J. Nachtman, Y. Nagai, S. Nagu, D. Naples, S. Narita, J. Nava, A. Navrer-Agasson, N. Nayak, M. Nebot-Guinot, A. Nehm, J. K. Nelson, O. Neogi, J. Nesbit, M. Nessi, D. Newbold, M. Newcomer, R. Nichol, F. Nicolas-Arnaldos, A. Nielsen, A. Nikolica, J. Nikolov, E. Niner, X. Ning, K. Nishimura, A. Norman, A. Norrick, P. Novella, A. Nowak, J. A. Nowak, M. Oberling, J. P. Ochoa-Ricoux, S. Oh, S. B. Oh, A. Olivier, T. Olson, Y. Onel, Y. Onishchuk, A. Oranday, M. Osbiston, J. A. Osorio VÃ©lez, L. O'Sullivan, L. Otiniano Ormachea, L. Pagani, G. Palacio, O. Palamara, S. Palestini, J. M. Paley, M. Pallavicini, C. Palomares, S. Pan, M. Panareo, P. Panda, V. Pandey, W. Panduro Vazquez, E. Pantic, V. Paolone, A. Papadopoulou, R. Papaleo, D. Papoulias, S. Paramesvaran, S. Parke, S. Parsa, Z. Parsa, S. Parveen, M. Parvu, D. Pasciuto, S. Pascoli, L. Pasqualini, J. Pasternak, J. L. Paton, C. Patrick, L. Patrizii, R. B. Patterson, T. Patzak, A. Paudel, L. Paulucci, Z. Pavlovic, G. Pawloski, D. Payne, A. Peake, V. Pec, E. Pedreschi, S. J. M. Peeters, W. Pellico, E. Pennacchio, A. Penzo, O. L. G. Peres, L. PÃ©rez-Molina, C. Pernas, J. Perry, D. Pershey, G. Pessina, G. Petrillo, C. Petta, R. Petti, M. Pfaff, V. Pia, L. Pickering, L. Pierini, F. Pietropaolo, V. L. Pimentel, G. Pinaroli, S. Pincha, J. Pinchault, K. Pitts, K. Pletcher, K. Plows, C. Pollack, T. Pollmann, F. Pompa, X. Pons, N. Poonthottathil, V. Popov, F. Poppi, J. Porter, L. G. Porto PaixÃ£o, M. Potekhin, M. Pozzato, R. Pradhan, T. Prakash, M. Prest, F. Psihas, D. Pugnere, D. Pullia, X. Qian, J. Queen, J. L. Raaf, M. Rabelhofer, V. Radeka, J. Rademacker, B. Radics, F. Raffaelli, A. Rafique, E. Raguzin, A. Rahe, S. Rajagopalan, M. Rajaoalisoa, I. Rakhno, L. Rakotondravohitra, M. A. Ralaikoto, L. Ralte, M. A. Ramirez Delgado, B. Ramson, S. S. Randriamanampisoa, A. Rappoldi, G. Raselli, T. Rath, P. Ratoff, R. Ray, H. Razafinime, R. F. Razakamiandra, E. M. Rea, J. S. Real, B. Rebel, R. Rechenmacher, J. Reichenbacher, S. D. Reitzner, E. Renner, S. Repetto, S. Rescia, F. Resnati, C. Reynolds, M. Ribas, S. Riboldi, C. Riccio, G. Riccobene, J. S. Ricol, M. Rigan, A. Rikalo, E. V. RincÃ³n, A. Ritchie-Yates, S. Ritter, D. Rivera, A. Robert, A. Roberts, E. Robles, J. L. Rocabado Rocha, M. Roda, D. Rodas RodrÃ­guez, M. J. O. Rodrigues, J. Rodriguez Rondon, S. Rosauro-Alcaraz, P. Rosier, D. Ross, M. Rossella, M. Rossi, M. Ross-Lonergan, N. Roy, P. Roy, P. Roy, C. Rubbia, D. Rudik, A. Ruggeri, G. Ruiz Ferreira, K. Rushiya, B. Russell, S. Sacerdoti, N. Saduyev, S. K. Sahoo, N. Sahu, S. Sakhiyev, P. Sala, G. Salmoria, S. Samanta, N. Samios, M. C. Sanchez, A. SÃ¡nchez Bravo, A. SÃ¡nchez-Castillo, P. Sanchez-Lucas, D. A. Sanders, S. Sanfilippo, D. Santoro, N. Saoulidou, P. Sapienza, I. Sarcevic, I. Sarra, G. Savage, V. Savinov, G. Scanavini, A. Scaramelli, A. Scarff, T. Schefke, H. Schellman, S. Schifano, P. Schlabach, D. Schmitz, A. W. Schneider, K. Scholberg, A. Schukraft, B. Schuld, S. Schwartz, A. Segade, E. Segreto, C. R. Senise, J. Sensenig, S. H. Seo, D. Seppela, M. H. Shaevitz, P. Shanahan, P. Sharma, R. Kumar, S. Sharma Poudel, K. Shaw, T. Shaw, K. Shchablo, J. Shen, C. Shepherd-Themistocleous, J. Shi, W. Shi, S. Shin, S. Shivakoti, A. Shmakov, I. Shoemaker, D. Shooltz, R. Shrock, M. Siden, J. Silber, L. Simard, J. Sinclair, G. Sinev, Jaydip Singh, J. Singh, L. Singh, P. Singh, V. Singh, S. Singh Chauhan, R. Sipos, C. Sironneau, G. Sirri, K. Siyeon, K. Skarpaas, J. Smedley, J. Smith, P. Smith, J. Smolik, M. Smy, M. Snape, E. L. Snider, P. Snopok, M. Soares Nunes, H. Sobel, M. Soderberg, C. J. Solano Salinas, S. SÃ¶ldner-Rembold, N. Solomey, V. Solovov, W. E. Sondheim, M. Sorbara, M. Sorel, J. Soto-Oton, A. Sousa, K. Soustruznik, D. Souza Correia, F. Spinella, J. Spitz, N. J. C. Spooner, D. Stalder, M. Stancari, L. Stanco, J. Steenis, R. Stein, H. M. Steiner, A. F. Steklain LisbÃ´a, J. Stewart, B. Stillwell, J. Stock, T. Stokes, M. Strait, T. Strauss, L. Strigari, A. Stuart, J. G. Suarez, J. Subash, A. Surdo, L. Suter, K. Sutton, Y. Suvorov, R. Svoboda, S. K. Swain, C. Sweeney, B. Szczerbinska, A. M. Szelc, A. Sztuc, A. Taffara, N. Talukdar, J. Tamara, H. A. Tanaka, S. Tang, N. Taniuchi, A. M. Tapia Casanova, A. Tapper, S. Tariq, E. Tarpara, E. Tatar, R. Tayloe, D. Tedeschi, A. M. Teklu, K. Tellez Giron Flores, J. Tena Vidal, P. Tennessen, M. Tenti, K. Terao, F. Terranova, G. Testera, T. Thakore, A. Thea, S. Thomas, A. Thompson, C. Thorn, C. Thorpe, S. C. Timm, E. Tiras, V. Tishchenko, S. Tiwari, N. TodoroviÄ, L. Tomassetti, A. Tonazzo, D. Torbunov, D. Torres MuÃ±oz, M. Torti, M. Tortola, Y. Torun, N. Tosi, D. Totani, M. Toups, C. Touramanis, V. Trabattoni, D. Tran, R. Travaglini, J. Trevor, E. Triller, S. Trilov, D. Trotta, J. Truchon, D. Truncali, W. H. Trzaska, Y. Tsai, Y. -T. Tsai, Z. Tsamalaidze, K. V. Tsang, N. Tsverava, S. Z. Tu, S. Tufanli, C. Tunnell, M. Tuzi, J. Tyler, E. Tyley, M. Tzanov, M. A. Uchida, J. UreÃ±a GonzÃ¡lez, J. Urheim, T. Usher, H. Utaegbulam, S. Uzunyan, M. R. Vagins, P. Vahle, G. A. Valdiviesso, V. Vale, E. Valencia, R. Valentim, Z. Vallari, E. Vallazza, J. W. F. Valle, R. Van Berg, D. V. Forero, A. Vannozzi, M. Van Nuland-Troost, F. Varanini, D. Vargas Oliva, N. Vaughan, K. Vaziri, A. VÃ¡zquez-Ramos, J. Vega, J. Vences, S. Ventura, A. Verdugo, S. Vergani, M. Verzocchi, K. Vetter, M. Vicenzi, H. Vieira de Souza, C. Vignoli, C. Vilela, E. Villa, S. Viola, B. Viren, G. V. Stenico, R. Vizarreta, A. P. Vizcaya Hernandez, S. Vlachos, G. Vorobyev, Q. Vuong, A. V. Waldron, M. Wallach, J. Walsh, T. Walton, L. Wan, B. Wang, H. Wang, J. Wang, L. Wang, M. H. L. S. Wang, X. Wang, Y. Wang, K. Warburton, D. Warner, L. Warsame, M. O. Wascko, D. Waters, A. Watson, K. Wawrowska, A. Weber, C. M. Weber, M. Weber, H. Wei, A. Weinstein, S. Westerdale, M. Wetstein, K. Whalen, A. White, L. H. Whitehead, D. Whittington, F. Wieler, J. Wilhlemi, M. J. Wilking, A. Wilkinson, C. Wilkinson, F. Wilson, R. J. Wilson, P. Winter, J. Wolcott, J. Wolfs, T. Wongjirad, A. Wood, K. Wood, E. Worcester, M. Worcester, K. Wresilo, M. Wright, M. Wrobel, S. Wu, W. Wu, W. Wu, M. Wurm, J. Wyenberg, B. M. Wynne, Y. Xiao, I. Xiotidis, B. Yaeggy, N. Yahlali, E. Yandel, G. Yang, J. Yang, T. Yang, A. Yankelevich, L. Yates, K. Yonehara, T. Young, B. Yu, H. Yu, J. Yu, Y. Yu, W. Yuan, R. Zaki, J. Zalesak, L. Zambelli, B. Zamorano, A. Zani, O. Zapata, L. Zazueta, G. P. Zeller, J. Zennamo, J. Zettlemoyer, K. Zeug, C. Zhang, S. Zhang, M. Zhao, E. Zhivun, E. D. Zimmerman, S. Zucchelli, V. Zutshi, R. Zwaska

**Updated**: 2025-03-31T05:47:08Z

**Summary**: The international collaboration designing and constructing the Deep Underground Neutrino Experiment (DUNE) at the Long-Baseline Neutrino Facility (LBNF) has developed a two-phase strategy toward the implementation of this leading-edge, large-scale science project. The ambitious physics program of Phase I and Phase II of DUNE is dependent upon deployment and utilization of significant computing resources, and successful research and development of software (both infrastructure and algorithmic) in order to achieve these scientific goals. This submission discusses the computing resources projections, infrastructure support, and software development needed for DUNE during the coming decades as an input to the European Strategy for Particle Physics Update for 2026. The DUNE collaboration is submitting four main contributions to the 2026 Update of the European Strategy for Particle Physics process. This submission to the 'Computing' stream focuses on DUNE software and computing. Additional inputs related to the DUNE science program, DUNE detector technologies and R&D, and European contributions to Fermilab accelerator upgrades and facilities for the DUNE experiment, are also being submitted to other streams.

**Link**: [arxiv](http://arxiv.org/abs/2503.23743v1),  [pdf](http://arxiv.org/pdf/2503.23743v1)

**Tags**: physics.data-an hep-ex physics.ins-det 



### LANID: LLM-assisted New Intent Discovery
**Authors**: Lu Fan, Jiashu Pu, Rongsheng Zhang, Xiao-Ming Wu

**Updated**: 2025-03-31T05:34:32Z

**Summary**: Task-oriented Dialogue Systems (TODS) often face the challenge of encountering new intents. New Intent Discovery (NID) is a crucial task that aims to identify these novel intents while maintaining the capability to recognize existing ones. Previous efforts to adapt TODS to new intents have struggled with inadequate semantic representation or have depended on external knowledge, which is often not scalable or flexible. Recently, Large Language Models (LLMs) have demonstrated strong zero-shot capabilities; however, their scale can be impractical for real-world applications that involve extensive queries. To address the limitations of existing NID methods by leveraging LLMs, we propose LANID, a framework that enhances the semantic representation of lightweight NID encoders with the guidance of LLMs. Specifically, LANID employs the $K$-nearest neighbors and Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithms to sample selective utterance pairs from the training set. It then queries an LLM to ascertain the relationships between these pairs. The data produced from this process is utilized to design a contrastive fine-tuning task, which is then used to train a small encoder with a contrastive triplet loss. Our experimental results demonstrate the efficacy of the proposed method across three distinct NID datasets, surpassing strong baselines in both unsupervised and semi-supervised settings. Our code is available at https://github.com/floatSDSDS/LANID.

**Link**: [arxiv](http://arxiv.org/abs/2503.23740v1),  [pdf](http://arxiv.org/pdf/2503.23740v1)

**Tags**: cs.CL cs.AI 



### DCAD-2000: A Multilingual Dataset across 2000+ Languages with Data   Cleaning as Anomaly Detection
**Authors**: Yingli Shen, Wen Lai, Shuo Wang, Xueren Zhang, Kangyang Luo, Alexander Fraser, Maosong Sun

**Updated**: 2025-03-31T05:25:57Z

**Summary**: The rapid development of multilingual large language models (LLMs) highlights the need for high-quality, diverse, and clean multilingual datasets. In this paper, we introduce DCAD-2000 (Data Cleaning as Anomaly Detection), a large-scale multilingual corpus built using newly extracted Common Crawl data and existing multilingual datasets. DCAD-2000 includes over 2,282 languages, 46.72TB of data, and 8.63 billion documents, spanning 155 high- and medium-resource languages and 159 writing scripts. To overcome the limitations of current data cleaning methods, which rely on manual heuristic thresholds, we propose reframing data cleaning as an anomaly detection task. This dynamic filtering approach significantly enhances data quality by identifying and removing noisy or anomalous content. We evaluate the quality of DCAD-2000 on the FineTask benchmark, demonstrating substantial improvements in multilingual dataset quality and task performance.

**Link**: [arxiv](http://arxiv.org/abs/2502.11546v2),  [pdf](http://arxiv.org/pdf/2502.11546v2)

**Tags**: cs.CL 



### AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models   with Unsupervised Coefficient Optimization
**Authors**: Yiyang Du, Xiaochen Wang, Chi Chen, Jiabo Ye, Yiru Wang, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Zhifang Sui, Maosong Sun, Yang Liu

**Updated**: 2025-03-31T05:13:02Z

**Summary**: Recently, model merging methods have demonstrated powerful strengths in combining abilities on various tasks from multiple Large Language Models (LLMs). While previous model merging methods mainly focus on merging homogeneous models with identical architecture, they meet challenges when dealing with Multimodal Large Language Models (MLLMs) with inherent heterogeneous property, including differences in model architecture and the asymmetry in the parameter space. In this work, we propose AdaMMS, a novel model merging method tailored for heterogeneous MLLMs. Our method tackles the challenges in three steps: mapping, merging and searching. Specifically, we first design mapping function between models to apply model merging on MLLMs with different architecture. Then we apply linear interpolation on model weights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally in the hyper-parameter searching step, we propose an unsupervised hyper-parameter selection method for model merging. As the first model merging method capable of merging heterogeneous MLLMs without labeled data, extensive experiments on various model combinations demonstrated that AdaMMS outperforms previous model merging methods on various vision-language benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2503.23733v1),  [pdf](http://arxiv.org/pdf/2503.23733v1)

**Tags**: cs.CL cs.CV 



### CL-Attack: Textual Backdoor Attacks via Cross-Lingual Triggers
**Authors**: Jingyi Zheng, Tianyi Hu, Tianshuo Cong, Xinlei He

**Updated**: 2025-03-31T04:48:28Z

**Summary**: Backdoor attacks significantly compromise the security of large language models by triggering them to output specific and controlled content. Currently, triggers for textual backdoor attacks fall into two categories: fixed-token triggers and sentence-pattern triggers. However, the former are typically easy to identify and filter, while the latter, such as syntax and style, do not apply to all original samples and may lead to semantic shifts. In this paper, inspired by cross-lingual (CL) prompts of LLMs in real-world scenarios, we propose a higher-dimensional trigger method at the paragraph level, namely CL-attack. CL-attack injects the backdoor by using texts with specific structures that incorporate multiple languages, thereby offering greater stealthiness and universality compared to existing backdoor attack techniques. Extensive experiments on different tasks and model architectures demonstrate that CL-attack can achieve nearly 100% attack success rate with a low poisoning rate in both classification and generation tasks. We also empirically show that the CL-attack is more robust against current major defense methods compared to baseline backdoor attacks. Additionally, to mitigate CL-attack, we further develop a new defense called TranslateDefense, which can partially mitigate the impact of CL-attack.

**Link**: [arxiv](http://arxiv.org/abs/2412.19037v2),  [pdf](http://arxiv.org/pdf/2412.19037v2)

**Tags**: cs.CR cs.AI 



