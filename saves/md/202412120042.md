# Arxiv Results
## Keyword: kv cache 
 ### From Slow Bidirectional to Fast Causal Video Generators
**Authors**: Tianwei Yin, Qiang Zhang, Richard Zhang, William T. Freeman, Fredo Durand, Eli Shechtman, Xun Huang

**Updated**: 2024-12-10T18:59:50Z

**Summary**: Current video diffusion models achieve impressive generation quality but struggle in interactive applications due to bidirectional attention dependencies. The generation of a single frame requires the model to process the entire sequence, including the future. We address this limitation by adapting a pretrained bidirectional diffusion transformer to a causal transformer that generates frames on-the-fly. To further reduce latency, we extend distribution matching distillation (DMD) to videos, distilling 50-step diffusion model into a 4-step generator. To enable stable and high-quality distillation, we introduce a student initialization scheme based on teacher's ODE trajectories, as well as an asymmetric distillation strategy that supervises a causal student model with a bidirectional teacher. This approach effectively mitigates error accumulation in autoregressive generation, allowing long-duration video synthesis despite training on short clips. Our model supports fast streaming generation of high quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our approach also enables streaming video-to-video translation, image-to-video, and dynamic prompting in a zero-shot manner. We will release the code based on an open-source model in the future.

**Link**: [arxiv](http://arxiv.org/abs/2412.07772v1),  [pdf](http://arxiv.org/pdf/2412.07772v1)

**Tags**: cs.CV 



### FlashRNN: Optimizing Traditional RNNs on Modern Hardware
**Authors**: Korbinian Pöppel, Maximilian Beck, Sepp Hochreiter

**Updated**: 2024-12-10T18:50:37Z

**Summary**: While Transformers and other sequence-parallelizable neural network architectures seem like the current state of the art in sequence modeling, they specifically lack state-tracking capabilities. These are important for time-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs, as well as modern variants like sLSTM do have these capabilities at the cost of strictly sequential processing. While this is often seen as a strong limitation, we show how fast these networks can get with our hardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the register level on modern GPUs. We extend traditional RNNs with a parallelization variant that processes multiple RNNs of smaller hidden state in parallel, similar to the head-wise processing in Transformers. To enable flexibility on different GPU variants, we introduce a new optimization framework for hardware-internal cache sizes, memory and compute handling. It models the hardware in a setting using polyhedral-like constraints, including the notion of divisibility. This speeds up the solution process in our ConstrINT library for general integer constraint satisfaction problems (integer CSPs). We show that our kernels can achieve 50x speed-ups over a vanilla PyTorch implementation and allow 40x larger hidden sizes compared to our Triton implementation. Our open-source kernels and the optimization library are released here to boost research in the direction of state-tracking enabled RNNs and sequence modeling: \url{https://github.com/NX-AI/flashrnn}

**Link**: [arxiv](http://arxiv.org/abs/2412.07752v1),  [pdf](http://arxiv.org/pdf/2412.07752v1)

**Tags**: cs.LG cs.AI 



### ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion   Transformer
**Authors**: Jinyi Hu, Shengding Hu, Yuxuan Song, Yufei Huang, Mingxuan Wang, Hao Zhou, Zhiyuan Liu, Wei-Ying Ma, Maosong Sun

**Updated**: 2024-12-10T18:13:20Z

**Summary**: The recent surge of interest in comprehensive multimodal models has necessitated the unification of diverse modalities. However, the unification suffers from disparate methodologies. Continuous visual generation necessitates the full-sequence diffusion-based approach, despite its divergence from the autoregressive modeling in the text domain. We posit that autoregressive modeling, i.e., predicting the future based on past deterministic experience, remains crucial in developing both a visual generation model and a potential unified multimodal model. In this paper, we explore an interpolation between the autoregressive modeling and full-parameters diffusion to model visual information. At its core, we present ACDiT, an Autoregressive blockwise Conditional Diffusion Transformer, where the block size of diffusion, i.e., the size of autoregressive units, can be flexibly adjusted to interpolate between token-wise autoregression and full-sequence diffusion. ACDiT is easy to implement, as simple as creating a Skip-Causal Attention Mask (SCAM) during training. During inference, the process iterates between diffusion denoising and autoregressive decoding that can make full use of KV-Cache. We verify the effectiveness of ACDiT on image and video generation tasks. We also demonstrate that benefitted from autoregressive modeling, ACDiT can be seamlessly used in visual understanding tasks despite being trained on the diffusion objective. The analysis of the trade-off between autoregressive modeling and diffusion demonstrates the potential of ACDiT to be used in long-horizon visual generation tasks. These strengths make it promising as the backbone of future unified models.

**Link**: [arxiv](http://arxiv.org/abs/2412.07720v1),  [pdf](http://arxiv.org/pdf/2412.07720v1)

**Tags**: cs.CV 



### Video-XL: Extra-Long Vision Language Model for Hour-Scale Video   Understanding
**Authors**: Yan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Junjie Zhou, Zhengyang Liang, Tiejun Huang, Bo Zhao

**Updated**: 2024-12-10T12:45:31Z

**Summary**: Long video understanding poses a significant challenge for current Multi-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained by their limited context lengths and the substantial costs while processing long videos. Although several existing methods attempt to reduce visual tokens, their strategies encounter severe bottleneck, restricting MLLMs' ability to perceive fine-grained visual details. In this work, we propose Video-XL, a novel approach that leverages MLLMs' inherent key-value (KV) sparsification capacity to condense the visual input. Specifically, we introduce a new special token, the Visual Summarization Token (VST), for each interval of the video, which summarizes the visual information within the interval as its associated KV. The VST module is trained by instruction fine-tuning, where two optimizing strategies are offered. 1.Curriculum learning, where VST learns to make small (easy) and large compression (hard) progressively. 2. Composite data curation, which integrates single-image, multi-image, and synthetic data to overcome the scarcity of long-video instruction data. The compression quality is further improved by dynamic compression, which customizes compression granularity based on the information density of different video intervals. Video-XL's effectiveness is verified from three aspects. First, it achieves a superior long-video understanding capability, outperforming state-of-the-art models of comparable sizes across multiple popular benchmarks. Second, it effectively preserves video information, with minimal compression loss even at 16x compression ratio. Third, it realizes outstanding cost-effectiveness, enabling high-quality processing of thousands of frames on a single A100 GPU.

**Link**: [arxiv](http://arxiv.org/abs/2409.14485v4),  [pdf](http://arxiv.org/pdf/2409.14485v4)

**Tags**: cs.CV 



### GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic   Embedding Caching
**Authors**: Sajal Regmi, Chetan Phakami Pun

**Updated**: 2024-12-09T01:44:10Z

**Summary**: Large Language Models (LLMs), such as GPT, have revolutionized artificial intelligence by enabling nuanced understanding and generation of human-like text across a wide range of applications. However, the high computational and financial costs associated with frequent API calls to these models present a substantial bottleneck, especially for applications like customer service chatbots that handle repetitive queries. In this paper, we introduce GPT Semantic Cache, a method that leverages semantic caching of query embeddings in in-memory storage (Redis). By storing embeddings of user queries, our approach efficiently identifies semantically similar questions, allowing for the retrieval of pre-generated responses without redundant API calls to the LLM. This technique achieves a notable reduction in operational costs while significantly enhancing response times, making it a robust solution for optimizing LLM-powered applications. Our experiments demonstrate that GPT Semantic Cache reduces API calls by up to 68.8% across various query categories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the system achieves high accuracy, with positive hit rates exceeding 97%, confirming the reliability of cached responses. This technique not only reduces operational costs, but also improves response times, enhancing the efficiency of LLM-powered applications.

**Link**: [arxiv](http://arxiv.org/abs/2411.05276v3),  [pdf](http://arxiv.org/pdf/2411.05276v3)

**Tags**: cs.LG 



### A Survey on Privacy-Preserving Caching at Network Edge: Classification,   Solutions, and Challenges
**Authors**: Xianzhi Zhang, Yipeng Zhou, Di Wu, Quan Z. Sheng, Shazia Riaz, Miao Hu, Linchang Xiao

**Updated**: 2024-12-09T01:39:15Z

**Summary**: Caching content at the edge network is a popular and effective technique widely deployed to alleviate the burden of network backhaul, shorten service delay and improve service quality. However, there has been some controversy over privacy violations in caching content at the edge network. On the one hand, the multi-access open edge network provides an ideal entrance or interface for external attackers to obtain private data from edge caches by extracting sensitive information. On the other hand, privacy can be infringed on by curious edge caching providers through caching trace analysis targeting the achievement of better caching performance or higher profits. Therefore, an in-depth understanding of privacy issues in edge caching networks is vital and indispensable for creating a privacy-preserving caching service at the edge network. In this article, we are among the first to fill this gap by examining privacy-preserving techniques for caching content at the edge network. Firstly, we provide an introduction to the background of privacy-preserving edge caching (PPEC). Next, we summarize the key privacy issues and present a taxonomy for caching at the edge network from the perspective of private information. Additionally, we conduct a retrospective review of the state-of-the-art countermeasures against privacy leakage from content caching at the edge network. Finally, we conclude the survey and envision challenges for future research.

**Link**: [arxiv](http://arxiv.org/abs/2405.01844v3),  [pdf](http://arxiv.org/pdf/2405.01844v3)

**Tags**: cs.NI cs.CR cs.DC 



### XKV: Personalized KV Cache Memory Reduction for Long-Context LLM   Inference
**Authors**: Weizhuo Li, Zhigang Wang, Yu Gu, Ge Yu

**Updated**: 2024-12-08T11:32:08Z

**Summary**: Recently the generative Large Language Model (LLM) has achieved remarkable success in numerous applications. Notably its inference generates output tokens one-by-one, leading to many redundant computations. The widely-used KV-Cache framework makes a compromise between time and space complexities. However, caching data generates the increasingly growing memory demand, that can quickly exhaust the limited memory capacity of the modern accelerator like GPUs, particularly in long-context inference tasks. Existing studies reduce memory consumption by evicting some of cached data that have less important impact on inference accuracy. But the benefit in practice is far from ideal due to the static cache allocation across different LLM network layers. This paper observes that the layer-specific cached data have very different impacts on accuracy. We quantify this difference, and give experimental and theoretical validation. We accordingly make a formal analysis and shows that customizing the cache size for each layer in a personalized manner can yield a significant memory reduction, while still providing comparable accuracy. We simulate the cache allocation as a combinatorial optimization problem and give a global optimal solution. In particular, we devise a mini- and sampling-based inference over a lightweight variant of the LLM model, so as to quickly capture the difference and then feed it into the personalized algorithms. Extensive experiments on real-world datasets demonstrate that our proposals can reduce KV cache memory consumption by 61.6% on average, improve computational efficiency by 2.1x and then increase the throughput by up to 5.5x.

**Link**: [arxiv](http://arxiv.org/abs/2412.05896v1),  [pdf](http://arxiv.org/pdf/2412.05896v1)

**Tags**: cs.LG cs.CL 



### Semi-Supervised Contrastive Learning for Controllable Video-to-Music   Retrieval
**Authors**: Shanti Stewart, Gouthaman KV, Lie Lu, Andrea Fanelli

**Updated**: 2024-12-08T06:37:27Z

**Summary**: Content creators often use music to enhance their videos, from soundtracks in movies to background music in video blogs and social media content. However, identifying the best music for a video can be a difficult and time-consuming task. To address this challenge, we propose a novel framework for automatically retrieving a matching music clip for a given video, and vice versa. Our approach leverages annotated music labels, as well as the inherent artistic correspondence between visual and music elements. Distinct from previous cross-modal music retrieval works, our method combines both self-supervised and supervised training objectives. We use self-supervised and label-supervised contrastive learning to train a joint embedding space between music and video. We show the effectiveness of our approach by using music genre labels for the supervised training component, and our framework can be generalized to other music annotations (e.g., emotion, instrument, etc.). Furthermore, our method enables fine-grained control over how much the retrieval process focuses on self-supervised vs. label information at inference time. We evaluate the learned embeddings through a variety of video-to-music and music-to-video retrieval tasks. Our experiments show that the proposed approach successfully combines self-supervised and supervised objectives and is effective for controllable music-video retrieval.

**Link**: [arxiv](http://arxiv.org/abs/2412.05831v1),  [pdf](http://arxiv.org/pdf/2412.05831v1)

**Tags**: cs.MM cs.SD eess.AS 



### Ultrafast lattice and electron dynamics induced in a PbSe crystal by an   intense terahertz pulse
**Authors**: A. A. Melnikov, Yu. G. Selivanov, D. G. Poydashev, S. V. Chekalin

**Updated**: 2024-12-07T17:22:14Z

**Summary**: We have studied the ultrafast optical response of a PbSe crystal to an intense picosecond terahertz pulse with a peak electric field strength of up to $\sim$ 500 kV/cm. The reflectivity anisotropy signal contains oscillations at the fundamental frequency of the resonant infrared-active phonon mode as well as its second, third, and fourth harmonics. The effect is ascribed to coherent anharmonic phonons resonantly excited by the strong terahertz field. Pump terahertz pulses also induce an almost instantaneous Kerr effect and a long-lived optical anisotropy of the crystal with a characteristic decay time of $\gtrsim$ 100 ps. We consider lattice distortion and phonon-assisted side valley population as possible origins of this metastable state.

**Link**: [arxiv](http://arxiv.org/abs/2412.05704v1),  [pdf](http://arxiv.org/pdf/2412.05704v1)

**Tags**: cond-mat.mtrl-sci 



### Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache   Compression
**Authors**: Michael R. Metel, Boxing Chen, Mehdi Rezagholizadeh

**Updated**: 2024-12-07T16:41:54Z

**Summary**: Several works have developed eviction policies to remove key-value (KV) pairs from the KV cache for more efficient inference. The focus has been on compressing the KV cache after the input prompt has been processed for faster token generation. In settings with limited GPU memory, and when the input context is longer than the generation length, we show that by also compressing the KV cache during the input processing phase, larger batch sizes can be used resulting in significantly higher throughput while still maintaining the original model's accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2412.05693v1),  [pdf](http://arxiv.org/pdf/2412.05693v1)

**Tags**: cs.CL 



### DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via   Adaptive Heads Fusion
**Authors**: Yilong Chen, Linhao Zhang, Junyuan Shang, Zhenyu Zhang, Tingwen Liu, Shuohuan Wang, Yu Sun

**Updated**: 2024-12-07T13:23:39Z

**Summary**: Large language models (LLMs) with billions of parameters demonstrate impressive performance. However, the widely used Multi-Head Attention (MHA) in LLMs incurs substantial computational and memory costs during inference. While some efforts have optimized attention mechanisms by pruning heads or sharing parameters among heads, these methods often lead to performance degradation or necessitate substantial continued pre-training costs to restore performance. Based on the analysis of attention redundancy, we design a Decoupled-Head Attention (DHA) mechanism. DHA adaptively configures group sharing for key heads and value heads across various layers, achieving a better balance between performance and efficiency. Inspired by the observation of clustering similar heads, we propose to progressively transform the MHA checkpoint into the DHA model through linear fusion of similar head parameters step by step, retaining the parametric knowledge of the MHA checkpoint. We construct DHA models by transforming various scales of MHA checkpoints given target head budgets. Our experiments show that DHA remarkably requires a mere 0.25\% of the original model's pre-training budgets to achieve 97.6\% of performance while saving 75\% of KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\times$ training acceleration, a maximum of 13.93\% performance improvement under 0.01\% pre-training budget, and 4\% relative improvement under 0.05\% pre-training budget.

**Link**: [arxiv](http://arxiv.org/abs/2406.06567v2),  [pdf](http://arxiv.org/pdf/2406.06567v2)

**Tags**: cs.LG cs.AI cs.CL 



### PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following   Models Need for Efficient Generation
**Authors**: Ao Wang, Hui Chen, Jianchao Tan, Kefeng Zhang, Xunliang Cai, Zijia Lin, Jungong Han, Guiguang Ding

**Updated**: 2024-12-07T13:23:39Z

**Summary**: Recently, large vision-language models (LVLMs) have rapidly gained popularity for their strong generation and reasoning capabilities given diverse multimodal inputs. However, these models incur significant computational and memory overhead during inference, which greatly hinders the efficient deployment in practical scenarios. The extensive key-value (KV) cache, necessitated by the lengthy input and output sequences, notably contributes to the high inference cost. Based on this, recent works have investigated ways to reduce the KV cache size for higher efficiency. Although effective, they generally overlook the distinct importance distributions of KV vectors across layers and maintain the same cache size for each layer during the next token prediction. This results in the significant contextual information loss for certain layers, leading to notable performance decline. To address this, we present PrefixKV. It reframes the challenge of determining KV cache sizes for all layers into the task of searching for the optimal global prefix configuration. With an adaptive layer-wise KV retention recipe based on binary search, the maximum contextual information can thus be preserved in each layer, facilitating the generation. Extensive experiments demonstrate that our method achieves the state-of-the-art performance compared with others. It exhibits superior inference efficiency and generation quality trade-offs, showing promising potential for practical applications. Code is available at \url{https://github.com/THU-MIG/PrefixKV}.

**Link**: [arxiv](http://arxiv.org/abs/2412.03409v2),  [pdf](http://arxiv.org/pdf/2412.03409v2)

**Tags**: cs.CV 



### Spineless Traversal for Layout Invalidation
**Authors**: Marisa Kirisame, Tiezhi Wang, Pavel Panchekha

**Updated**: 2024-12-07T04:08:56Z

**Summary**: Latency is a major concern for web rendering engines like those in Chrome, Safari, and Firefox. These engines reduce latency by using an incremental layout algorithm to redraw the page when the user interacts with it. In such an algorithm, elements that change frame-to-frame are marked dirty; only the dirty elements need be processed to draw the next frame, dramatically reducing latency. However, the standard incremental layout algorithm must search the page for dirty elements, accessing a number of auxiliary elements in the process. These auxiliary elements add cache misses and stalled cycles, and are responsible for a sizable fraction of all layout latency. We introduce a new, faster incremental layout algorithm called Spineless Traversal. Spineless Traversal uses a more computationally demanding priority queue algorithm to avoid the need to access auxiliary nodes and thus reduces cache traffic and stalls. This leads to dramatic speedups on the most latency-critical interactions such as hovering, typing, or animations. Moreover, thanks to numerous low-level optimizations, we are able to make Spineless Traversal competitive across the whole spectrum of incremental layout workloads. As a result, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the benchmark, with a mean speedup of 3.23x concentrated in the most latency-critical interactions such as hovering, typing, and animations.

**Link**: [arxiv](http://arxiv.org/abs/2411.10659v2),  [pdf](http://arxiv.org/pdf/2411.10659v2)

**Tags**: cs.PL 



### Effect of electric field on excitons in wide quantum wells
**Authors**: Shiming Zheng, E. S. Khramtsov, I. V. Ignatiev

**Updated**: 2024-12-06T19:35:52Z

**Summary**: A microscopic model of a heterostructure with a quantum well (QW) is proposed to study the exciton behavior in an external electric field. The effect of an electric field ranging from 0 to 6 kV/cm applied to the GaAs/AlGaAs QW structure in the growth direction is studied for several QWs of various widths up to 100 nm. The three-dimensional Schr\"odinger equation (SE) of exciton is numerically solved using the finite difference method. Wave functions and energies for several states of the heavy-hole and light-hole excitons are calculated. Dependencies of the exciton state energy, the binding energy, the radiative broadening, and the static dipole moment on the applied electric fields are determined. The threshold of exciton dissociation for the 100-nm QW is also determined. In addition, we found the electric-field-induced shift of the center of mass of the heavy-hole and light-hole exciton in the QWs. Finally, we have modeled reflection spectra of heterostructures with the GaAs/AlGaAs QWs in the electric field using the calculated energies and radiative broadenings of excitons.

**Link**: [arxiv](http://arxiv.org/abs/2412.05392v1),  [pdf](http://arxiv.org/pdf/2412.05392v1)

**Tags**: cond-mat.mes-hall 



### MC3: Memory Contention based Covert Channel Communication on Shared DRAM   System-on-Chips
**Authors**: Ismet Dagli, James Crea, Soner Seckiner, Yuanchao Xu, Selçuk Köse, Mehmet E. Belviranli

**Updated**: 2024-12-06T17:58:57Z

**Summary**: Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a wide-range of mobile computing platforms, including edge/IoT devices, autonomous systems and smartphones. In SM-SoCs, system-wide shared physical memory enables a convenient and financially-feasible way to make data accessible by dozens of processing units (PUs), such as CPU cores and domain specific accelerators. In this study, we investigate vulnerabilities that stem from the shared use of physical memory in such systems. Due to the diverse computational characteristics of the PUs they embed, SM-SoCs often do not employ a shared last level cache (LLC). While the literature proposes covert channel attacks for shared memory systems, high-throughput communication is currently possible by either relying on an LLC or privileged/physical access to the shared memory subsystem.   In this study, we introduce a new memory-contention based covert communication attack, MC3, which specifically targets the shared system memory in mobile SoCs. Different from existing attacks, our approach achieves high throughput communication between applications running on CPU and GPU without the need for an LLC or elevated access to the system. We extensively explore the effectiveness of our methodology by demonstrating the trade-off between the channel transmission rate and the robustness of the communication. We demonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to a transmit rate of 6.4 kbps with less than 1% error rate.

**Link**: [arxiv](http://arxiv.org/abs/2412.05228v1),  [pdf](http://arxiv.org/pdf/2412.05228v1)

**Tags**: cs.CR 



### SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules
**Authors**: Suyi Li, Lingyun Yang, Xiaoxiao Jiang, Hanfeng Lu, Dakai An, Zhipeng Di, Weiyi Lu, Jiawei Chen, Kan Liu, Yinghao Yu, Tao Lan, Guodong Yang, Lin Qu, Liping Zhang, Wei Wang

**Updated**: 2024-12-06T11:47:06Z

**Summary**: Text-to-image (T2I) generation using diffusion models has become a blockbuster service in today's AI cloud. A production T2I service typically involves a serving workflow where a base diffusion model is augmented with various "add-on" modules, notably ControlNet and LoRA, to enhance image generation control. Compared to serving the base model alone, these add-on modules introduce significant loading and computational overhead, resulting in increased latency. In this paper, we present SwiftDiffusion, a system that efficiently serves a T2I workflow through a holistic approach. SwiftDiffusion decouples ControNet from the base model and deploys it as a separate, independently scaled service on dedicated GPUs, enabling ControlNet caching, parallelization, and sharing. To mitigate the high loading overhead of LoRA serving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL) technique, allowing LoRA loading to overlap with the initial base model execution by up to k steps without compromising image quality. Furthermore, SwiftDiffusion optimizes base model execution with a novel latent parallelism technique. Collectively, these designs enable SwiftDiffusion to outperform the state-of-the-art T2I serving systems, achieving up to 7.8x latency reduction and 1.6x throughput improvement in serving SDXL models on H800 GPUs, without sacrificing image quality.

**Link**: [arxiv](http://arxiv.org/abs/2407.02031v2),  [pdf](http://arxiv.org/pdf/2407.02031v2)

**Tags**: cs.DC cs.AI cs.LG 



### Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free   Dynamic Triangular Attention Pattern
**Authors**: Hongyin Tang, Di Xiu, Lanrui Wang, Xiurui Geng, Jingang Wang, Xunliang Cai

**Updated**: 2024-12-06T03:46:06Z

**Summary**: The quadratic computational complexity of the attention mechanism in current Large Language Models (LLMs) renders inference with long contexts prohibitively expensive. To address this challenge, various approaches aim to retain critical portions of the context to optimally approximate Full Attention (FA) through Key-Value (KV) compression or Sparse Attention (SA), enabling the processing of virtually unlimited text lengths in a streaming manner. However, these methods struggle to achieve performance levels comparable to FA, particularly in retrieval tasks. In this paper, our analysis of attention head patterns reveals that LLMs' attention distributions show strong local correlations, naturally reflecting a chunking mechanism for input context. We propose Ltri-LLM framework, which divides KVs into spans, stores them in an offline index, and retrieves the relevant KVs into memory for various queries. Experimental results on popular long text benchmarks show that Ltri-LLM can achieve performance close to FA while maintaining efficient, streaming-based inference.

**Link**: [arxiv](http://arxiv.org/abs/2412.04757v1),  [pdf](http://arxiv.org/pdf/2412.04757v1)

**Tags**: cs.CL cs.LG 



### One-Hop Sub-Query Result Caches for Graph Database Systems
**Authors**: Hieu Nguyen, Jun Li, Shahram Ghandeharizadeh

**Updated**: 2024-12-06T01:20:47Z

**Summary**: This paper introduces a novel one-hop sub-query result cache for processing graph read transactions, gR-Txs, in a graph database system. The one-hop navigation is from a vertex using either its in-coming or out-going edges with selection predicates that filter edges and vertices. Its cache entry identifies a unique one-hop sub-query (key) and its result set consisting of immutable vertex ids (value). When processing a gR-Tx, the query processor identifies its sequence of individual one-hop sub-queries and looks up their results in the cache. A cache hit fetches less data from the storage manager and eliminates the requirement to process the one-hop sub-query. A cache miss populates the cache asynchronously and in a transactional manner, maintaining the separation of read and write paths of our transactional storage manager. A graph read and write transaction, gRW-Tx, identifies the impacted cache entries and either deletes or updates them. Our implementation of the cache is inside the graph query processing engine and transparent to a user application. We evaluate the cache using our eCommerce production workload and with rules that re-write graph queries to maximize the performance enhancements observed with the cache. Obtained results show the cache enhances 95th and 99th percentile of query response times by at least 2x and 1.63x, respectively. When combined with query re-writing, the enhancements are at least 2.33x and 4.48x, respectively. An interesting result is the significant performance enhancement observed by the indirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference one-hop sub-queries. The cache frees system resources to expedite their processing significantly.

**Link**: [arxiv](http://arxiv.org/abs/2412.04698v1),  [pdf](http://arxiv.org/pdf/2412.04698v1)

**Tags**: cs.DB cs.PF 



### Cross-Self KV Cache Pruning for Efficient Vision-Language Inference
**Authors**: Xiaohuan Pei, Tao Huang, Chang Xu

**Updated**: 2024-12-05T22:47:17Z

**Summary**: KV cache pruning has emerged as a promising technique for reducing memory and computation costs in long-context auto-regressive generation. Existing methods for vision-language models (VLMs) typically rely on self-attention scores from large language models (LLMs) to identify and prune irrelevant tokens. However, these approaches overlook the inherent distributional discrepancies between modalities, often leading to inaccurate token importance estimation and the over-pruning of critical visual tokens. To address this, we propose decomposing attention scores into intra-modality attention (within the same modality) and inter-modality attention (across modalities), enabling more precise KV cache pruning by independently managing these distinct attention types. Additionally, we introduce an n-softmax function to counteract distribution shifts caused by pruning, preserving the original smoothness of attention scores and ensuring stable performance. Our final training-free method, \textbf{C}ross-\textbf{S}elf \textbf{P}runing (CSP), achieves competitive performance compared to models with full KV caches while significantly outperforming previous pruning methods. Extensive evaluations on MileBench, a benchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness, achieving up to a 41\% performance improvement on challenging tasks like conversational embodied dialogue while reducing the KV cache budget by 13.6\%. The code is available at https://github.com/TerryPei/CSP

**Link**: [arxiv](http://arxiv.org/abs/2412.04652v1),  [pdf](http://arxiv.org/pdf/2412.04652v1)

**Tags**: cs.CV cs.AI 



### Neural Two-Level Monte Carlo Real-Time Rendering
**Authors**: Mikhail Dereviannykh, Dmitrii Klepikov, Johannes Hanika, Carsten Dachsbacher

**Updated**: 2024-12-05T22:06:23Z

**Summary**: We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte Carlo, MLMC) estimator for real-time rendering of scenes with global illumination. Using MLMC we split the shading integral into two parts: the radiance cache integral and the residual error integral that compensates for the bias of the first one. For the first part, we developed the Neural Incident Radiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks as a building block, which is trained on the fly. The cache is designed to provide a fast and reasonable approximation of the incident radiance: an evaluation takes 2-25x less compute time than a path tracing sample. This enables us to estimate the radiance cache integral with a high number of samples and by this achieve faster convergence. For the residual error integral, we compute the difference between the NIRC predictions and the unbiased path tracing simulation. Our method makes no assumptions about the geometry, materials, or lighting of a scene and has only few intuitive hyper-parameters. We provide a comprehensive comparative analysis in different experimental scenarios. Since the algorithm is trained in an on-line fashion, it demonstrates significant noise level reduction even for dynamic scenes and can easily be combined with other importance sampling schemes and noise reduction techniques.

**Link**: [arxiv](http://arxiv.org/abs/2412.04634v1),  [pdf](http://arxiv.org/pdf/2412.04634v1)

**Tags**: cs.GR cs.AI 



### p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay
**Authors**: Jun Zhang, Desen Meng, Ji Qi, Zhenpeng Huang, Tao Wu, Limin Wang

**Updated**: 2024-12-05T18:58:03Z

**Summary**: Despite the remarkable performance of multimodal large language models (MLLMs) across diverse tasks, the substantial training and inference costs impede their advancement. The majority of computation stems from the overwhelming volume of vision tokens processed by the transformer decoder. In this paper, we propose to build efficient MLLMs by leveraging the Mixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects essential vision tokens to process while skipping redundant ones. However, integrating MoD into MLLMs is non-trivial. To address the challenges of training and inference stability as well as limited training data, we adapt the MoD module with two novel designs: tanh-gated weight normalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we observe that vision tokens exhibit higher redundancy in deeper layer and thus design a progressive ratio decay (PRD) strategy, which gradually reduces the token retention ratio layer by layer, employing a shifted cosine schedule. This crucial design fully unleashes the potential of MoD, significantly boosting the efficiency and performance of our models. To validate the effectiveness of our approach, we conduct extensive experiments with two baseline models across 14 benchmarks. Our model, p-MoD, matches or even surpasses the performance of the baseline models, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and 77.7% GPU hours during training.

**Link**: [arxiv](http://arxiv.org/abs/2412.04449v1),  [pdf](http://arxiv.org/pdf/2412.04449v1)

**Tags**: cs.CV cs.CL 



### SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving   Model Transformation
**Authors**: Aurick Qiao, Zhewei Yao, Samyam Rajbhandari, Yuxiong He

**Updated**: 2024-12-05T14:56:56Z

**Summary**: LLM inference for popular enterprise use cases, such as summarization, RAG, and code-generation, typically observes orders of magnitude longer prompt lengths than generation lengths. This characteristic leads to high cost of prefill and increased response latency. In this paper, we present SwiftKV, a novel model transformation and distillation procedure specifically designed to reduce the time and cost of processing prompt tokens while preserving high quality of generated tokens. SwiftKV combines three key mechanisms: i) SingleInputKV, which prefills later layers' KV cache using a much earlier layer's output, allowing prompt tokens to skip much of the model computation, ii) AcrossKV, which merges the KV caches of neighboring layers to reduce the memory footprint and support larger batch size for higher throughput, and iii) a knowledge-preserving distillation procedure that can adapt existing LLMs for SwiftKV with minimal accuracy impact and low compute and data requirement. For Llama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50% and the memory requirement of the KV cache by 62.5% while incurring minimum quality degradation across a wide range of tasks. In the end-to-end inference serving using an optimized vLLM implementation, SwiftKV realizes up to 2x higher aggregate throughput and 60% lower time per output token. It can achieve a staggering 560 TFlops/GPU of normalized inference throughput, which translates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100 GPUs. Our training, inference, and model implementations are open-sourced and can be found through https://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb.

**Link**: [arxiv](http://arxiv.org/abs/2410.03960v2),  [pdf](http://arxiv.org/pdf/2410.03960v2)

**Tags**: cs.LG cs.AI cs.CL 



### KV Shifting Attention Enhances Language Modeling
**Authors**: Mingyu Xu, Wei Cheng, Bingning Wang, Weipeng Chen

**Updated**: 2024-12-05T12:19:38Z

**Summary**: The current large language models are mainly based on decode-only structure transformers, which have great in-context learning (ICL) capabilities. It is generally believed that the important foundation of its ICL capability is the induction heads mechanism, which requires at least two layers attention. In order to more efficiently implement the ability of the model's induction, we revisit the induction heads mechanism and proposed a KV shifting attention. We theoretically prove that the KV shifting attention reducing the model's requirements for the depth and width of the induction heads mechanism. Our experimental results demonstrate that KV shifting attention is beneficial to learning induction heads and language modeling, which lead to better performance or faster convergence from toy models to the pre-training models with more than 10 B parameters.

**Link**: [arxiv](http://arxiv.org/abs/2411.19574v2),  [pdf](http://arxiv.org/pdf/2411.19574v2)

**Tags**: cs.CL 



### A Little Goes a Long Way: Efficient Long Context Training and Inference   with Partial Contexts
**Authors**: Suyu Ge, Xihui Lin, Yunan Zhang, Jiawei Han, Hao Peng

**Updated**: 2024-12-05T06:52:42Z

**Summary**: Training and serving long-context large language models (LLMs) incurs substantial overhead. To address this, two critical steps are often required: a pretrained LLM typically undergoes a separate stage for context length extension by training on long-context data, followed by architectural modifications to reduce the overhead of KV cache during serving. This paper argues that integrating length extension with a GPU-friendly KV cache reduction architecture not only reduces training overhead during length extension, but also achieves better long-context performance. This leads to our proposed LongGen, which finetunes a pretrained LLM into an efficient architecture during length extension. LongGen builds on three key insights: (1) Sparse attention patterns, such as window attention (attending to recent tokens), attention sink (initial ones), and blockwise sparse attention (strided token blocks) are well-suited for building efficient long-context models, primarily due to their GPU-friendly memory access patterns, enabling efficiency gains not just theoretically but in practice as well. (2) It is essential for the model to have direct access to all tokens. A hybrid architecture with 1/3 full attention layers and 2/3 efficient ones achieves a balanced trade-off between efficiency and long-context performance. (3) Lightweight training on 5B long-context data is sufficient to extend the hybrid model's context length from 4K to 128K.   We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its effectiveness across different scales. During training with 128K-long contexts, LongGen achieves 1.55x training speedup and reduces wall-clock time by 36%, compared to a full-attention baseline. During inference, LongGen reduces KV cache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding speedup.

**Link**: [arxiv](http://arxiv.org/abs/2410.01485v2),  [pdf](http://arxiv.org/pdf/2410.01485v2)

**Tags**: cs.CL 



### Yi-Lightning Technical Report
**Authors**: 01. AI, :, Alan Wake, Albert Wang, Bei Chen, C. X. Lv, Chao Li, Chengen Huang, Chenglin Cai, Chujie Zheng, Daniel Cooper, Ethan Dai, Fan Zhou, Feng Hu, Heng Ji, Howard Qiu, Jiangcheng Zhu, Jun Tian, Katherine Su, Lihuan Zhang, Liying Li, Ming Song, Mou Li, Peng Liu, Qicheng Hu, Shawn Wang, Shijun Zhou, Shiyong Li, Tianhang Zhu, Wen Xie, Xiang He, Xiaobo Chen, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Yanpeng Li, Yongke Zhao, Yongzhen Luo, Yuchi Xu, Yuxuan Sha, Zhaodong Yan, Zhiyuan Liu, Zirui Zhang

**Updated**: 2024-12-05T04:29:49Z

**Summary**: This technical report presents Yi-Lightning, our latest flagship large language model (LLM). It achieves exceptional performance, ranking 6th overall on Chatbot Arena, with particularly strong results (2nd to 4th place) in specialized categories including Chinese, Math, Coding, and Hard Prompts. Yi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture, featuring advanced expert segmentation and routing mechanisms coupled with optimized KV-caching techniques. Our development process encompasses comprehensive pre-training, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF), where we devise deliberate strategies for multi-stage training, synthetic data construction, and reward modeling. Furthermore, we implement RAISE (Responsible AI Safety Engine), a four-component framework to address safety issues across pre-training, post-training, and serving phases. Empowered by our scalable super-computing infrastructure, all these innovations substantially reduce training, deployment and inference costs while maintaining high-performance standards. With further evaluations on public academic benchmarks, Yi-Lightning demonstrates competitive performance against top-tier LLMs, while we observe a notable disparity between traditional, static benchmark results and real-world, dynamic human preferences. This observation prompts a critical reassessment of conventional benchmarks' utility in guiding the development of more intelligent and powerful AI systems for practical applications. Yi-Lightning is now available through our developer platform at https://platform.lingyiwanwu.com.

**Link**: [arxiv](http://arxiv.org/abs/2412.01253v3),  [pdf](http://arxiv.org/pdf/2412.01253v3)

**Tags**: cs.CL cs.AI cs.LG 



### F2: Designing a Key-Value Store for Large Skewed Workloads
**Authors**: Konstantinos Kanellis, Badrish Chandramouli, Shivaram Venkataraman

**Updated**: 2024-12-05T01:50:27Z

**Summary**: Many real-world workloads present a challenging set of requirements: point operations requiring high throughput, working sets much larger than main memory, and natural skew in key access patterns for both reads and writes. We find that modern key-value designs are either optimized for memory-efficiency, sacrificing high-performance (LSM-tree designs), or achieve high-performance, saturating modern NVMe SSD bandwidth, at the cost of substantial memory resources or high disk wear (CPU-optimized designs). Unfortunately these designs are not able to handle meet the challenging demands of such larger-than-memory, skewed workloads.   To this end, we present F2, a new key-value store that bridges this gap by combining the strengths of both approaches. F2 adopts a tiered, record-oriented architecture inspired by LSM-trees to effectively separate hot from cold records, while incorporating concurrent latch-free mechanisms from CPU-optimized engines to maximize performance on modern NVMe SSDs. To realize this design, we tackle key challenges and introduce several innovations, including new latch-free algorithms for multi-threaded log compaction and user operations (e.g., RMWs), as well as new components: a two-level hash index to reduce indexing overhead for cold records and a read-cache for serving read-hot data.   Detailed experimental results show that F2 matches or outperforms existing solutions, achieving on average better throughput on memory-constrained environments compared to state-of-the-art systems like RocksDB (11.75x), SplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2 also maintains its high performance across varying workload skewness levels and memory budgets, while achieving low disk write amplification.

**Link**: [arxiv](http://arxiv.org/abs/2305.01516v2),  [pdf](http://arxiv.org/pdf/2305.01516v2)

**Tags**: cs.DB 



### Marconi: Prefix Caching for the Era of Hybrid LLMs
**Authors**: Rui Pan, Zhuang Wang, Zhen Jia, Can Karakus, Luca Zancato, Tri Dao, Yida Wang, Ravi Netravali

**Updated**: 2024-12-04T18:40:24Z

**Summary**: Hybrid models that combine the language modeling capabilities of Attention layers with the efficiency of Recurrent layers (e.g., State Space Models) have gained traction in practically supporting long contexts in Large Language Model serving. Yet, the unique properties of these models complicate the usage of complementary efficiency optimizations such as prefix caching that skip redundant computations across requests. Most notably, their use of in-place state updates for recurrent layers precludes rolling back cache entries for partial sequence overlaps, and instead mandates only exact-match cache hits; the effect is a deluge of (large) cache entries per sequence, most of which yield minimal reuse opportunities. We present Marconi, the first system that supports efficient prefix caching with Hybrid LLMs. Key to Marconi are its novel admission and eviction policies that more judiciously assess potential cache entries based not only on recency, but also on (1) forecasts of their reuse likelihood across a taxonomy of different hit scenarios, and (2) the compute savings that hits deliver relative to memory footprints. Across diverse workloads and Hybrid models, Marconi achieves up to 34.4$\times$ higher token hit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix caching systems.

**Link**: [arxiv](http://arxiv.org/abs/2411.19379v2),  [pdf](http://arxiv.org/pdf/2411.19379v2)

**Tags**: cs.DC cs.AI cs.LG 



### Measurement of electron beam induced sample heating in SEM experiments
**Authors**: Christina Koenig, Alice Bastos da Silva Fanta, Joerg R. Jinschek

**Updated**: 2024-12-04T14:47:42Z

**Summary**: Scanning Electron Microscopy (SEM) experiments provide detailed insights into material microstructures, enabling high-resolution imaging as well as crystallographic analysis through advanced techniques like Electron Backscatter Diffraction (EBSD). However, the interaction of the high-energy electron beam with the material can lead to localized heating, which may significantly impact specimen integrity, especially in applications requiring prolonged beam exposure, for instance when mapping the crystal structure using EBSD. This study examines electron-beam-induced heating effects on a model metal sample (iron), directly measuring the locally deposited electron beam energy with a MEMS-based heating device and validating these measurements through simulations, including Monte Carlo and Finite Element methods. The analysis focuses on the effects of various experimental parameters such as acceleration voltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time (from 1$\mu$s to 1ms) and sample tilt (0{\deg} to 70{\deg}). The findings reveal that local sample temperatures can increase by up to 70 {\deg}C during EBSD experiments, primarily affected by the choice in beam current and acceleration voltage, with beam current having the most significant impact.

**Link**: [arxiv](http://arxiv.org/abs/2412.03361v1),  [pdf](http://arxiv.org/pdf/2412.03361v1)

**Tags**: cond-mat.mtrl-sci physics.app-ph 



### ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable   Compression
**Authors**: Guangda Liu, Chengwei Li, Jieru Zhao, Chenqi Zhang, Minyi Guo

**Updated**: 2024-12-04T10:58:27Z

**Summary**: Large Language Models (LLMs) have been widely deployed in a variety of applications, and the context length is rapidly increasing to handle tasks such as long-document QA and complex logical reasoning. However, long context poses significant challenges for inference efficiency, including high memory costs of key-value (KV) cache and increased latency due to extensive memory accesses. Recent works have proposed compressing KV cache to approximate computation, but these methods either evict tokens permanently, never recalling them for later inference, or recall previous tokens at the granularity of pages divided by textual positions. Both approaches degrade the model accuracy and output quality. To achieve efficient and accurate recallable KV cache compression, we introduce ClusterKV, which recalls tokens at the granularity of semantic clusters. We design and implement efficient algorithms and systems for clustering, selection, indexing and caching. Experiment results show that ClusterKV attains negligible accuracy loss across various tasks with 32k context lengths, using only a 1k to 2k KV cache budget, and achieves up to a 2$\times$ speedup in latency and a 2.5$\times$ improvement in decoding throughput. Compared to SoTA recallable KV compression methods, ClusterKV demonstrates higher model accuracy and output quality, while maintaining or exceeding inference efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2412.03213v1),  [pdf](http://arxiv.org/pdf/2412.03213v1)

**Tags**: cs.LG cs.AI cs.PF 



### Unifying KV Cache Compression for Large Language Models with LeanKV
**Authors**: Yanqi Zhang, Yuwei Hu, Runyuan Zhao, John C. S. Lui, Haibo Chen

**Updated**: 2024-12-04T08:51:23Z

**Summary**: Large language models (LLMs) demonstrate exceptional performance but incur high serving costs due to substantial memory demands, with the key-value (KV) cache being a primary bottleneck. Existing KV cache compression methods, including quantization and pruning, struggle with limitations such as uniform treatment of keys and values and static memory allocation across attention heads. To address these challenges, we introduce LeanKV, a unified KV cache compression framework that enhances LLM serving efficiency without compromising accuracy through three innovations: (1) Hetero-KV quantization, which stores keys at a higher precision than values to reflect their greater impact on attention computations; (2) per-head dynamic sparsity, which allocates memory based on token importance per head and per request; and (3) unified KV compression, integrating mixed-precision quantization and selective pruning to enable a smooth tradeoff between model accuracy and memory efficiency. To efficiently support these techniques, LeanKV introduces systems optimizations including unified paging and on-GPU parallel memory management. Implemented on vLLM, LeanKV compresses the KV cache by $3.0\times$ to $5.0\times$ without accuracy loss and up to $11.0\times$ with under 5% accuracy loss, enhancing throughput by $1.9\times$ to $2.5\times$, and up to $6.9\times$.

**Link**: [arxiv](http://arxiv.org/abs/2412.03131v1),  [pdf](http://arxiv.org/pdf/2412.03131v1)

**Tags**: cs.LG cs.DC 



### PASCAL: A Learning-aided Cooperative Bandwidth Control Policy for   Hierarchical Storage Systems
**Authors**: Xijun Li, Yunfan Zhou, Ji Zhang

**Updated**: 2024-12-04T05:32:12Z

**Summary**: Nowadays, the Hierarchical Storage System (HSS) is considered as an ideal model to meet the cost-performance demand. The data migration between storing tiers of HSS is the way to achieve the cost-performance goal. The bandwidth control is to limit the maximum amount of data migration. Most of previous research about HSS focus on studying the data migration policy instead of bandwidth control. However, the recent research about cache and networking optimization suggest that the bandwidth control has significant impact on the system performance. Few previous work achieves a satisfactory bandwidth control in HSS since it is hard to control bandwidth for so many data migration tasks simultaneously. In this paper, we first give a stochastic programming model to formalize the bandwidth control problem in HSS. Then we propose a learning-aided bandwidth control policy for HSS, named \Pascal{}, which learns to control the bandwidth of different data migration task in an cooperative way. We implement \Pascal{} on a commercial HSS and compare it with three strong baselines over a group of workloads. Our evaluation on the physical system shows that \Pascal{} can effectively decrease 1.95X the tail latency and greatly improve throughput stability (2X $\downarrow$ throughput jitter), and meanwhile keep the throughput at a relatively high level.

**Link**: [arxiv](http://arxiv.org/abs/2303.08066v2),  [pdf](http://arxiv.org/pdf/2303.08066v2)

**Tags**: cs.NI 



### A Multi-Functional Web Tool for Comprehensive Threat Detection Through   IP Address Analysis
**Authors**: Cebajel Tanan, Sameer G. Kulkarni, Tamal Das, Manjesh K. Hanawal

**Updated**: 2024-12-04T04:29:12Z

**Summary**: In recent years, the advances in digitalisation have also adversely contributed to the significant rise in cybercrimes. Hence, building the threat intelligence to shield against rising cybercrimes has become a fundamental requisite. Internet Protocol (IP) addresses play a crucial role in the threat intelligence and prevention of cyber crimes. However, we have noticed the lack of one-stop, free, and open-source tools that can analyse IP addresses. Hence, this work introduces a comprehensive web tool for advanced IP address characterisation. Our tool offers a wide range of features, including geolocation, blocklist check, VPN detection, proxy detection, bot detection, Tor detection, port scan, and accurate domain statistics that include the details about the name servers and registrar information. In addition, our tool calculates a confidence score based on a weighted sum of publicly accessible online results from different reliable sources to give users a dependable measure of accuracy. Further, to improve performance, our tool also incorporates a local database for caching the results, to enable fast content retrieval with minimal external Web API calls. Our tool supports domain names and IPv4 addresses, making it a multi-functional and powerful IP analyser tool for threat intelligence. Our tool is available at www.ipanalyzer.in

**Link**: [arxiv](http://arxiv.org/abs/2412.03023v1),  [pdf](http://arxiv.org/pdf/2412.03023v1)

**Tags**: cs.CR 



### cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video   Requesting and Edge Caching
**Authors**: Xianzhi Zhang, Linchang Xiao, Yipeng Zhou, Miao Hu, Di Wu, John C. S. Lui, Quan Z. Sheng

**Updated**: 2024-12-03T22:48:09Z

**Summary**: As users conveniently stream their favorite online videos, video request records are automatically stored by video content providers, which have a high chance of privacy leakage. Unfortunately, most existing privacy-enhancing approaches are not applicable for protecting user privacy in video requests, because they cannot be easily altered or distorted by users and must be visible for content providers to stream correct videos. To preserve request privacy in online video services, it is possible to request additional videos that are irrelevant to users' interests so that content providers cannot precisely infer users' interest information. However, a naive redundant requesting approach would significantly degrade the performance of edge caches and increase bandwidth overhead. In this paper, we are among the first to propose a Cache-Friendly Redundant Video Requesting (cRVR) algorithm for User Devices (UDs) and its corresponding caching algorithm for the Edge Cache (EC), which can effectively mitigate the problem of request privacy leakage with minimal impact on the EC's performance. To tackle the problem, we first develop a Stackelberg game to analyze the dedicated interaction between UDs and EC, and obtain their optimal strategies to maximize their respective utility. For UDs, the utility function is a combination of both video playback utility and privacy protection utility. We prove the existence and uniqueness of the equilibrium of the Stackelberg game. Extensive experiments are conducted with real traces to demonstrate that cRVR can effectively protect video request privacy by reducing up to 59.03\% of privacy disclosure compared to baseline algorithms. Meanwhile, the caching performance of EC is only slightly affected.

**Link**: [arxiv](http://arxiv.org/abs/2310.12622v2),  [pdf](http://arxiv.org/pdf/2310.12622v2)

**Tags**: cs.NI 



### GoldFish: Serverless Actors with Short-Term Memory State for the   Edge-Cloud Continuum
**Authors**: Cynthia Marcelino, Jack Shahhoud, Stefan Nastic

**Updated**: 2024-12-03T22:02:42Z

**Summary**: Serverless Computing is a computing paradigm that provides efficient infrastructure management and elastic scalability. Serverless functions scale up or down based on demand, which means that functions are not directly addressable and rely on platform-managed invocation. Serverless stateless nature requires functions to leverage external services, such as object storage and KVS, to exchange data. Serverless actors have emerged as a solution to these issues. However, the state-of-the-art serverless lifecycle and event-trigger invocation force actors to leverage remote services to manage their state and exchange data, which impacts the performance and incurs additional costs and dependency on third-party services.   To address these issues, in this paper, we introduce a novel serverless lifecycle model that allows short-term stateful actors, enabling actors to maintain their state between executions. Additionally, we propose a novel serverless Invocation Model that enables serverless actors to influence the processing of future messages. We present GoldFish, a lightweight WebAssembly short-term stateful serverless actor platform that provides a novel serverless actor lifecycle and invocation model. GoldFish leverages WebAssembly to provide the actors with lightweight sandbox isolation, making them suitable for the Edge-Cloud Continuum, where computational resources are limited. Experimental results show that GoldFish optimizes the data exchange latency by up to 92% and increases the throughput by up to 10x compared to OpenFaaS and Spin.

**Link**: [arxiv](http://arxiv.org/abs/2412.02867v1),  [pdf](http://arxiv.org/pdf/2412.02867v1)

**Tags**: cs.DC 



### Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic   Vision-language Context Sparsification
**Authors**: Wenxuan Huang, Zijie Zhai, Yunhang Shen, Shaoshen Cao, Fei Zhao, Xiangfeng Xu, Zheyu Ye, Shaohui Lin

**Updated**: 2024-12-03T16:12:09Z

**Summary**: Multimodal Large Language Models (MLLMs) have achieved remarkable success in vision understanding, reasoning, and interaction. However, the inference computation and memory increase progressively with the generation of output tokens during decoding, directly affecting the efficacy of MLLMs. Existing methods attempt to reduce the vision context redundancy to achieve efficient MLLMs. Unfortunately, the efficiency benefits of the vision context reduction in the prefill stage gradually diminish during the decoding stage. To address this problem, we proposed a dynamic vision-language context sparsification framework Dynamic-LLaVA, which dynamically reduces the redundancy of vision context in the prefill stage and decreases the memory and computation overhead of the generated language context during decoding. Dynamic-LLaVA designs a tailored sparsification inference scheme for different inference modes, i.e., prefill, decoding with and without KV cache, to achieve efficient inference of MLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by $\sim$75\% in the prefill stage. Meanwhile, throughout the entire generation process of MLLMs, Dynamic-LLaVA reduces the $\sim$50\% computation consumption under decoding without KV cache, while saving $\sim$50\% GPU memory overhead when decoding with KV cache, due to the vision-language context sparsification. Extensive experiments also demonstrate that Dynamic-LLaVA achieves efficient inference for MLLMs with negligible understanding and generation ability degradation or even performance gains compared to the full-context inference baselines. Code is available at https://github.com/Osilly/dynamic_llava .

**Link**: [arxiv](http://arxiv.org/abs/2412.00876v2),  [pdf](http://arxiv.org/pdf/2412.00876v2)

**Tags**: cs.CV cs.AI cs.CL cs.LG 



### Value Residual Learning For Alleviating Attention Concentration In   Transformers
**Authors**: Zhanchao Zhou, Tianyi Wu, Zhiyun Jiang, Zhenzhong Lan

**Updated**: 2024-12-03T12:36:19Z

**Summary**: Transformers can capture long-range dependencies using self-attention, allowing tokens to attend to all others directly. However, stacking multiple attention layers leads to attention concentration. One natural way to address this issue is to use cross-layer attention, allowing information from earlier layers to be directly accessible to later layers. However, this approach is computationally expensive. To address this problem, we propose Transformer with residual value (ResFormer) which approximates cross-layer attention through adding a residual connection from the values of the the first layer to all subsequent layers. Based on this method, one variant is the Transformer with single layer value (SVFormer), where all layers share the same value embedding from first layer. Comprehensive empirical evidence demonstrates ResFormer achieves equivalent validation loss with 10.4% fewer model parameters and 13.6% less training data compared to Transformer, while maintaining similar memory usage and computational cost. Besides, SVFormer reduces KV cache size by nearly half with only a small performance penalty and can be integrated with other KV-efficient methods, yielding further reductions in KV cache, with performance influenced by sequence length and cumulative learning rate. Further visualization results suggest that Resformer and SVFormer alleviate attention concentration in deeper layers through avoiding value-state drains and enhance representation across most layers.

**Link**: [arxiv](http://arxiv.org/abs/2410.17897v3),  [pdf](http://arxiv.org/pdf/2410.17897v3)

**Tags**: cs.CL 



### Compressing KV Cache for Long-Context LLM Inference with Inter-Layer   Attention Similarity
**Authors**: Da Ma, Lu Chen, Situo Zhang, Yuxun Miao, Su Zhu, Zhi Chen, Hongshen Xu, Hanqi Li, Shuai Fan, Lei Pan, Kai Yu

**Updated**: 2024-12-03T08:29:27Z

**Summary**: The increasing context window size in Large Language Models (LLMs), such as the GPT and LLaMA series, has improved their ability to tackle complex, long-text tasks, but at the cost of inference efficiency, particularly regarding memory and computational complexity. Existing methods, including selective token retention and window-based attention, improve efficiency but risk discarding important tokens needed for future text generation. In this paper, we propose an approach that enhances LLM efficiency without token loss by reducing the memory and computational load of less important tokens, rather than discarding them.We address two challenges: 1) investigating the distribution of important tokens in the context, discovering recent tokens are more important than distant tokens in context, and 2) optimizing resources for distant tokens by sharing attention scores across layers. The experiments show that our method saves $35\%$ KV cache without compromising the performance.

**Link**: [arxiv](http://arxiv.org/abs/2412.02252v1),  [pdf](http://arxiv.org/pdf/2412.02252v1)

**Tags**: cs.CL 



### Improving Sequential Recommender Systems with Online and In-store User   Behavior
**Authors**: Luyi Ma, Aashika Padmanabhan, Anjana Ganesh, Shengwei Tang, Jiao Chen, Xiaohan Li, Lalitesh Morishetti, Kaushiki Nag, Malay Patel, Jason Cho, Sushant Kumar, Kannan Achan

**Updated**: 2024-12-03T03:20:40Z

**Summary**: Online e-commerce platforms have been extending in-store shopping, which allows users to keep the canonical online browsing and checkout experience while exploring in-store shopping. However, the growing transition between online and in-store becomes a challenge to sequential recommender systems for future online interaction prediction due to the lack of holistic modeling of hybrid user behaviors (online and in-store). The challenges are twofold. First, combining online and in-store user behavior data into a single data schema and supporting multiple stages in the model life cycle (pre-training, training, inference, etc.) organically needs a new data pipeline design. Second, online recommender systems, which solely rely on online user behavior sequences, must be redesigned to support online and in-store user data as input under the sequential modeling setting. To overcome the first challenge, we propose a hybrid, omnichannel data pipeline to compile online and in-store user behavior data by caching information from diverse data sources. Later, we introduce a model-agnostic encoder module to the sequential recommender system to interpret the user in-store transaction and augment the modeling capacity for better online interaction prediction given the hybrid user behavior.

**Link**: [arxiv](http://arxiv.org/abs/2412.02122v1),  [pdf](http://arxiv.org/pdf/2412.02122v1)

**Tags**: cs.IR 



### Development and Application of a Decentralized Domain Name Service
**Authors**: Guang Yang

**Updated**: 2024-12-02T20:39:56Z

**Summary**: The current Domain Name System (DNS), as a core infrastructure of the internet, exhibits several shortcomings: its centralized architecture leads to censorship risks and single points of failure, making domain name resolution vulnerable to attacks. The lack of encryption in the resolution process exposes it to DNS hijacking and cache poisoning attacks. Additionally, the high operational costs limit participation and innovation among small to medium-sized users. To address these issues, this paper proposes a Decentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and distributed storage (IPFS). By leveraging the immutability of blockchain and the content verification of IPFS, the system achieves decentralized storage and distribution of domain name records, eliminating the centralized dependencies of traditional DNS. With a block time of 15 seconds, the system supports rapid broadcasting of domain name updates, significantly improving resolution efficiency. The DDNS aims to serve as a complement or backup to the existing DNS system, providing a pollution-resistant, censorship-resistant, high-performance, and low-cost domain name resolution solution, offering a new technical path for the security and stability of the internet.

**Link**: [arxiv](http://arxiv.org/abs/2412.01959v1),  [pdf](http://arxiv.org/pdf/2412.01959v1)

**Tags**: cs.NI 



### RandAR: Decoder-only Autoregressive Visual Generation in Random Orders
**Authors**: Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William T. Freeman, Yu-Xiong Wang

**Updated**: 2024-12-02T18:59:53Z

**Summary**: We introduce RandAR, a decoder-only visual autoregressive (AR) model capable of generating images in arbitrary token orders. Unlike previous decoder-only AR models that rely on a predefined generation order, RandAR removes this inductive bias, unlocking new capabilities in decoder-only generation. Our essential design enables random order by inserting a "position instruction token" before each image token to be predicted, representing the spatial location of the next image token. Trained on randomly permuted token sequences -- a more challenging task than fixed-order generation, RandAR achieves comparable performance to its conventional raster-order counterpart. More importantly, decoder-only transformers trained from random orders acquire new capabilities. For the efficiency bottleneck of AR models, RandAR adopts parallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration without sacrificing generation quality. Additionally, RandAR supports inpainting, outpainting and resolution extrapolation in a zero-shot manner. We hope RandAR inspires new directions for decoder-only visual generation models and broadens their applications across diverse scenarios. Our project page is at https://rand-ar.github.io/.

**Link**: [arxiv](http://arxiv.org/abs/2412.01827v1),  [pdf](http://arxiv.org/pdf/2412.01827v1)

**Tags**: cs.CV cs.AI 



### Local and Regional Contributions to Tropospheric Ozone Concentrations
**Authors**: Callum E. Flowerday, Ryan Thalman, Jaron C. Hansen

**Updated**: 2024-12-02T16:10:26Z

**Summary**: The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone according to the Environmental Protection Agency's (EPA) National Ambient Air Quality Standards (NAAQS). Nitrogen oxides ($\mathrm{NO_x = NO_2 + NO}$) and volatile organic compounds (VOCs), in the presence of sunlight, lead to ozone formation in the troposphere. When the rate of oxidant production, defined as the sum of $\mathrm{O_3}$ and $\mathrm{NO_2}$, is faster than the rate of $\mathrm{NO_x}$ production, a region is said to be $\mathrm{NO_x}$limited, and ozone formation will be limited by the concentration of $\mathrm{NO_x}$ species in the region. The inverse of this situation makes the region VOC-limited. Knowing whether a region is $\mathrm{NO_x}$-limited or VOC-limited can aid in generating effective mitigation strategies. Understanding the background or regional contributions to ozone in a region, whether from the transport of precursors or of ozone, provides information about the lower limit for ozone concentrations that a region can achieve through regulation of local precursors. In this paper, measured oxidant and $\mathrm{NO_x}$ concentrations are analyzed from 14 counties in the state of Utah to calculate the regional and local contributions to ozone for each region. This analysis is used to determine the nature of the atmosphere in each county by identifying whether the region is VOC or $\mathrm{NO_x}$-limited. Furthermore, this analysis is performed for each county for the years 2012 and 2022 to assess changes in the oxidative nature and quantify regional and local contributions to ozone over a 10-year period. All studied counties--except for Washington County--in Utah were found to be VOC-limited in 2012. This shifted in 2022, with most counties being either in a transitional state or $\mathrm{NO_x}$-limited. Local contributions to ozone increased in two major counties, Cache and Salt Lake Counties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington, and Weber Counties. Generally, the regional contributions to oxidant concentrations decreased across the state. A summertime spike in both regional and local contributions to oxidants was observed. Smoke from wildfires was found to increase regional contributions to oxidants and shift the local regime to be more $\mathrm{NO_x}$-limited.

**Link**: [arxiv](http://arxiv.org/abs/2412.01659v1),  [pdf](http://arxiv.org/pdf/2412.01659v1)

**Tags**: physics.ao-ph 



### Excitation of quasi-monochromotic waves by a high-voltage pulse in a   ferrite coaxial line with the periodic structure
**Authors**: A. B. Batrakov, S. Yu. Karelin, O. M. Lebedenko, V. S. Mukhin, I. N. Onishchenko, O. L. Rak, V. G. Sinitsin, M. V. Volovenko

**Updated**: 2024-12-02T11:57:03Z

**Summary**: Experimental data and results of numerical simulations are presented, concerning excitation of narrowband gigahertz-range wave trains in coaxial guiding structures that are partially filled with ferromagnetic material and may involve periodically arranged metal inserts. The experiments performed confirm the possibility of exciting weakly damped electromagnetic waves by feeding high voltage, unilateral electromagnetic pulses of short duration into the line. The coax line was of outer diameter 50.5 mm, filled with an isotropic dielectric (relative dielectric constant {\epsilon} = 2.25) and a set of ferrite rings with {\epsilon}=16 and saturated-state {\mu} about 4 to 5. With a peak voltage of the primary pulse close to 160 kV and a magnetizing field of 17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency 1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW.

**Link**: [arxiv](http://arxiv.org/abs/2412.01415v1),  [pdf](http://arxiv.org/pdf/2412.01415v1)

**Tags**: physics.acc-ph 



### Real-time Transformer-based Open-Vocabulary Detection with Efficient   Fusion Head
**Authors**: Tiancheng Zhao, Peng Liu, Xuan He, Lu Zhang, Kyusong Lee

**Updated**: 2024-12-02T11:24:20Z

**Summary**: End-to-end transformer-based detectors (DETRs) have shown exceptional performance in both closed-set and open-vocabulary object detection (OVD) tasks through the integration of language modalities. However, their demanding computational requirements have hindered their practical application in real-time object detection (OD) scenarios. In this paper, we scrutinize the limitations of two leading models in the OVDEval benchmark, OmDet and Grounding-DINO, and introduce OmDet-Turbo. This novel transformer-based real-time OVD model features an innovative Efficient Fusion Head (EFH) module designed to alleviate the bottlenecks observed in OmDet and Grounding-DINO. Notably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with TensorRT and language cache techniques applied. Notably, in zero-shot scenarios on COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on par with current state-of-the-art supervised models. Furthermore, it establishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an AP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of OmDet-Turbo in industrial applications is underscored by its exceptional performance on benchmark datasets and superior inference speed, positioning it as a compelling choice for real-time object detection tasks. Code: \url{https://github.com/om-ai-lab/OmDet}

**Link**: [arxiv](http://arxiv.org/abs/2403.06892v2),  [pdf](http://arxiv.org/pdf/2403.06892v2)

**Tags**: cs.CV cs.CL 



### Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware   Masking
**Authors**: Marco Federici, Davide Belli, Mart van Baalen, Amir Jalalirad, Andrii Skliar, Bence Major, Markus Nagel, Paul Whatmough

**Updated**: 2024-12-02T11:07:51Z

**Summary**: While mobile devices provide ever more compute power, improvements in DRAM bandwidth are much slower. This is unfortunate for large language model (LLM) token generation, which is heavily memory-bound. Previous work has proposed to leverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce effective DRAM bandwidth per token. However, more recent LLMs use SwiGLU instead of ReLU, which result in little inherent sparsity. While SwiGLU activations can be pruned based on magnitude, the resulting sparsity patterns are difficult to predict, rendering previous approaches ineffective. To circumvent this issue, our work introduces Dynamic Input Pruning (DIP): a predictor-free dynamic sparsification approach, which preserves accuracy with minimal fine-tuning. DIP can further use lightweight LoRA adapters to regain some performance lost during sparsification. Lastly, we describe a novel cache-aware masking strategy, which considers the cache state and activation magnitude to further increase cache hit rate, improving LLM token rate on mobile devices. DIP outperforms other methods in terms of accuracy, memory and throughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP achieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1 loss in perplexity.

**Link**: [arxiv](http://arxiv.org/abs/2412.01380v1),  [pdf](http://arxiv.org/pdf/2412.01380v1)

**Tags**: cs.LG cs.CL 



### Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker   Verification
**Authors**: Bei Liu, Yanmin Qian

**Updated**: 2024-12-02T06:57:46Z

**Summary**: Recent speaker verification (SV) systems have shown a trend toward adopting deeper speaker embedding extractors. Although deeper and larger neural networks can significantly improve performance, their substantial memory requirements hinder training on consumer GPUs. In this paper, we explore a memory-efficient training strategy for deep speaker embedding learning in resource-constrained scenarios. Firstly, we conduct a systematic analysis of GPU memory allocation during SV system training. Empirical observations show that activations and optimizer states are the main sources of memory consumption. For activations, we design two types of reversible neural networks which eliminate the need to store intermediate activations during back-propagation, thereby significantly reducing memory usage without performance loss. For optimizer states, we introduce a dynamic quantization approach that replaces the original 32-bit floating-point values with a dynamic tree-based 8-bit data type. Experimental results on VoxCeleb demonstrate that the reversible variants of ResNets and DF-ResNets can perform training without the need to cache activations in GPU memory. In addition, the 8-bit versions of SGD and Adam save 75% of memory costs while maintaining performance compared to their 32-bit counterparts. Finally, a detailed comparison of memory usage and performance indicates that our proposed models achieve up to 16.2x memory savings, with nearly identical parameters and performance compared to the vanilla systems. In contrast to the previous need for multiple high-end GPUs such as the A100, we can effectively train deep speaker embedding extractors with just one or two consumer-level 2080Ti GPUs.

**Link**: [arxiv](http://arxiv.org/abs/2412.01195v1),  [pdf](http://arxiv.org/pdf/2412.01195v1)

**Tags**: eess.AS cs.AI cs.SD 



### JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial   Cyber-Physical Systems
**Authors**: Geng Sun, Jiaxu Wu, Zemin Sun, Long He, Jiacheng Wang, Dusit Niyato, Abbas Jamalipour, Shiwen Mao

**Updated**: 2024-12-02T06:30:04Z

**Summary**: In the era of the sixth generation (6G) and industrial Internet of Things (IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of sensor devices and computing-intensive tasks. To address the limited resources of IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) has emerged as a promising solution, providing flexible and cost-effective services in close proximity of IIoT sensor devices (ISDs). However, leveraging aerial MEC to meet the delay-sensitive and computation-intensive requirements of the ISDs could face several challenges, including the limited communication, computation and caching (3C) resources, stringent offloading requirements for 3C services, and constrained on-board energy of UAVs. To address these issues, we first present a collaborative aerial MEC-assisted ICPS architecture by incorporating the computing capabilities of the macro base station (MBS) and UAVs. We then formulate a service delay minimization optimization problem (SDMOP). Since the SDMOP is proved to be an NP-hard problem, we propose a joint computation offloading, caching, communication resource allocation, computation resource allocation, and UAV trajectory control approach (JC5A). Specifically, JC5A consists of a block successive upper bound minimization method of multipliers (BSUMM) for computation offloading and service caching, a convex optimization-based method for communication and computation resource allocation, and a successive convex approximation (SCA)-based method for UAV trajectory control. Moreover, we theoretically prove the convergence and polynomial complexity of JC5A. Simulation results demonstrate that the proposed approach can achieve superior system performance compared to the benchmark approaches and algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2411.04762v2),  [pdf](http://arxiv.org/pdf/2411.04762v2)

**Tags**: cs.NI eess.SP 



### Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled   D2D Communications
**Authors**: Aditya Powari, Daniel K. C. So

**Updated**: 2024-12-01T21:47:35Z

**Summary**: Non-orthogonal multiple access (NOMA) is widely viewed as a potential candidate for providing enhanced multiple access in future mobile networks by eliminating the orthogonal distribution of radio resources amongst the users. Nevertheless, the performance of NOMA can be significantly improved by combining it with other sophisticated technologies such as wireless data caching and device-to-device (D2D) communications. In this letter, we propose a novel cellular system model which integrates uplink NOMA with cache based device-to-device (D2D) communications. The proposed system would enable a cellular user to upload data file to base station while simultaneously exchanging useful cache content with another nearby user. We maximize the system sum rate by deriving closed form solutions for optimal power allocation. Simulation results demonstrate the superior performance of our proposed model over other potential combinations of uplink NOMA and D2D communications.

**Link**: [arxiv](http://arxiv.org/abs/2412.00977v1),  [pdf](http://arxiv.org/pdf/2412.00977v1)

**Tags**: eess.SP cs.ET 



### Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion
**Authors**: Bohai Gu, Hao Luo, Song Guo, Peiran Dong

**Updated**: 2024-12-01T15:45:26Z

**Summary**: Recently, diffusion-based methods have achieved great improvements in the video inpainting task. However, these methods still face many challenges, such as maintaining temporal consistency and the time-consuming issue. This paper proposes an advanced video inpainting framework using optical Flow-guided Efficient Diffusion, called FloED. Specifically, FloED employs a dual-branch architecture, where a flow branch first restores corrupted flow and a multi-scale flow adapter provides motion guidance to the main inpainting branch. Additionally, a training-free latent interpolation method is proposed to accelerate the multi-step denoising process using flow warping. Further introducing a flow attention cache mechanism, FLoED efficiently reduces the computational cost brought by incorporating optical flow. Comprehensive experiments in both background restoration and object removal tasks demonstrate that FloED outperforms state-of-the-art methods from the perspective of both performance and efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2412.00857v1),  [pdf](http://arxiv.org/pdf/2412.00857v1)

**Tags**: cs.CV 



### SpecExec: Massively Parallel Speculative Decoding for Interactive LLM   Inference on Consumer Devices
**Authors**: Ruslan Svirschevski, Avner May, Zhuoming Chen, Beidi Chen, Zhihao Jia, Max Ryabinin

**Updated**: 2024-11-30T21:33:59Z

**Summary**: As large language models gain widespread adoption, running them efficiently becomes crucial. Recent works on LLM inference use speculative decoding to achieve extreme speedups. However, most of these works implicitly design their algorithms for high-end datacenter hardware. In this work, we ask the opposite question: how fast can we run LLMs on consumer machines? Consumer GPUs can no longer fit the largest available models (50B+ parameters) and must offload them to RAM or SSD. When running with offloaded parameters, the inference engine can process batches of hundreds or thousands of tokens at the same time as just one token, making it a natural fit for speculative decoding. We propose SpecExec (Speculative Execution), a simple parallel decoding method that can generate up to 20 tokens per target model iteration for popular LLM families. It utilizes the high spikiness of the token probabilities distribution in modern LLMs and a high degree of alignment between model output probabilities. SpecExec takes the most probable tokens continuation from the draft model to build a "cache" tree for the target model, which then gets validated in a single pass. Using SpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with RAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens per second with 16-bit weights.

**Link**: [arxiv](http://arxiv.org/abs/2406.02532v3),  [pdf](http://arxiv.org/pdf/2406.02532v3)

**Tags**: cs.CL 



### Digital Twin in Industries: A Comprehensive Survey
**Authors**: Md Bokhtiar Al Zami, Shaba Shaon, Vu Khanh Quy, Dinh C. Nguyen

**Updated**: 2024-11-29T19:14:45Z

**Summary**: Industrial networks are undergoing rapid transformation driven by the convergence of emerging technologies that are revolutionizing conventional workflows, enhancing operational efficiency, and fundamentally redefining the industrial landscape across diverse sectors. Amidst this revolution, Digital Twin (DT) emerges as a transformative innovation that seamlessly integrates real-world systems with their virtual counterparts, bridging the physical and digital realms. In this article, we present a comprehensive survey of the emerging DT-enabled services and applications across industries, beginning with an overview of DT fundamentals and its components to a discussion of key enabling technologies for DT. Different from literature works, we investigate and analyze the capabilities of DT across a wide range of industrial services, including data sharing, data offloading, integrated sensing and communication, content caching, resource allocation, wireless networking, and metaverse. In particular, we present an in-depth technical discussion of the roles of DT in industrial applications across various domains, including manufacturing, healthcare, transportation, energy, agriculture, space, oil and gas, as well as robotics. Throughout the technical analysis, we delve into real-time data communications between physical and virtual platforms to enable industrial DT networking. Subsequently, we extensively explore and analyze a wide range of major privacy and security issues in DT-based industry. Taxonomy tables and the key research findings from the survey are also given, emphasizing important insights into the significance of DT in industries. Finally, we point out future research directions to spur further research in this promising area.

**Link**: [arxiv](http://arxiv.org/abs/2412.00209v1),  [pdf](http://arxiv.org/pdf/2412.00209v1)

**Tags**: cs.AI 



### Ten Ways in which Virtual Reality Differs from Video Streaming
**Authors**: Gustavo de Veciana, Sonia Fahmy, George Kesidis, Voicu Popescu

**Updated**: 2024-11-29T14:23:25Z

**Summary**: Virtual Reality (VR) applications have a number of unique characteristics that set them apart from traditional video streaming. These characteristics have major implications on the design of VR rendering, adaptation, prefetching, caching, and transport mechanisms. This paper contrasts VR to video streaming, stored 2D video streaming in particular, and discusses how to rethink system and network support for VR.

**Link**: [arxiv](http://arxiv.org/abs/2411.19730v1),  [pdf](http://arxiv.org/pdf/2411.19730v1)

**Tags**: cs.PF cs.MM cs.NI 



### Communication efficient application of sequences of planar rotations to   a matrix
**Authors**: Thijs Steel, Julien Langou

**Updated**: 2024-11-29T10:21:12Z

**Summary**: We present an efficient algorithm for the application of sequences of planar rotations to a matrix. Applying such sequences efficiently is important in many numerical linear algebra algorithms for eigenvalues. Our algorithm is novel in three main ways. First, we introduce a new kernel that is optimized for register reuse in a novel way. Second, we introduce a blocking and packing scheme that improves the cache efficiency of the algorithm. Finally, we thoroughly analyze the memory operations of the algorithm which leads to important theoretical insights and makes it easier to select good parameters. Numerical experiments show that our algorithm outperforms the state-of-the-art and achieves a flop rate close to the theoretical peak on modern hardware.

**Link**: [arxiv](http://arxiv.org/abs/2412.01852v1),  [pdf](http://arxiv.org/pdf/2412.01852v1)

**Tags**: cs.PF cs.DS 65F15, 65Y05 G.4 



### DID Link: Authentication in TLS with Decentralized Identifiers and   Verifiable Credentials
**Authors**: Sandro Rodriguez Garzon, Dennis Natusch, Artur Philipp, Axel Küpper, Hans Joachim Einsiedler, Daniela Schneider

**Updated**: 2024-11-29T08:48:01Z

**Summary**: Authentication in TLS is predominately carried out with X.509 digital certificates issued by certificate authorities (CA). The centralized nature of current public key infrastructures, however, comes along with severe risks, such as single points of failure and susceptibility to cyber-attacks, potentially undermining the security and trustworthiness of the entire system. With Decentralized Identifiers (DID) alongside distributed ledger technology, it becomes technically feasible to prove ownership of a unique identifier without requiring an attestation of the proof's public key by a centralized and therefore vulnerable CA. This article presents DID Link, a novel authentication scheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant way with self-issued X.509 certificates that are equipped with ledger-anchored DIDs instead of CA-issued identifiers. It facilitates the exchange of tamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable Credentials after the TLS handshake to complete the authentication with a full identification of the communication partner. A prototypical implementation shows comparable TLS handshake durations of DID Link if verification material is cached and reasonable prolongations if it is obtained from a ledger. The significant speed improvement of the resulting TLS channel over a widely used, DID-based alternative transport protocol on the application layer demonstrates the potential of DID Link to become a viable solution for the establishment of secure and trustful end-to-end communication links with decentrally managed digital identities.

**Link**: [arxiv](http://arxiv.org/abs/2405.07533v3),  [pdf](http://arxiv.org/pdf/2405.07533v3)

**Tags**: cs.CR cs.NI 



### InputSnatch: Stealing Input in LLM Services via Timing Side-Channel   Attacks
**Authors**: Xinyao Zheng, Husheng Han, Shangyi Shi, Qiyan Fang, Zidong Du, Xing Hu, Qi Guo

**Updated**: 2024-11-29T08:33:49Z

**Summary**: Large language models (LLMs) possess extensive knowledge and question-answering capabilities, having been widely deployed in privacy-sensitive domains like finance and medical consultation. During LLM inferences, cache-sharing methods are commonly employed to enhance efficiency by reusing cached states or responses for the same or similar inference requests. However, we identify that these cache mechanisms pose a risk of private input leakage, as the caching can result in observable variations in response times, making them a strong candidate for a timing-based attack hint.   In this study, we propose a novel timing-based side-channel attack to execute input theft in LLMs inference. The cache-based attack faces the challenge of constructing candidate inputs in a large search space to hit and steal cached user queries. To address these challenges, we propose two primary components. The input constructor employs machine learning techniques and LLM-based approaches for vocabulary correlation learning while implementing optimized search mechanisms for generalized input construction. The time analyzer implements statistical time fitting with outlier elimination to identify cache hit patterns, continuously providing feedback to refine the constructor's search strategy. We conduct experiments across two cache mechanisms and the results demonstrate that our approach consistently attains high attack success rates in various applications. Our work highlights the security vulnerabilities associated with performance optimizations, underscoring the necessity of prioritizing privacy and security alongside enhancements in LLM inference.

**Link**: [arxiv](http://arxiv.org/abs/2411.18191v2),  [pdf](http://arxiv.org/pdf/2411.18191v2)

**Tags**: cs.CR 



### BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix   Sharing and Throughput-oriented Token Batching
**Authors**: Zhen Zheng, Xin Ji, Taosong Fang, Fanghao Zhou, Chuanjie Liu, Gang Peng

**Updated**: 2024-11-29T05:57:37Z

**Summary**: Many LLM tasks are performed in large batches or even offline, and the performance indictor for which is throughput. These tasks usually show the characteristic of prefix sharing, where different prompt input can partially show the common prefix. However, the existing LLM inference engines tend to optimize the streaming requests and show limitations of supporting the large batched tasks with the prefix sharing characteristic. The existing solutions use the LRU-based cache to reuse the KV context of common prefix. The KV context that is about to be reused may prematurely be evicted with the implicit cache management. Even if not evicted, the lifetime of the shared KV context is extended since requests sharing the same context are not scheduled together, resulting in larger memory usage. These streaming oriented systems schedule the requests in the first-come-first-serve or similar order. As a result, the requests with larger ratio of decoding steps may be scheduled too late to be able to mix with the prefill chunks to increase the hardware utilization. Besides, the token and request number based batching can limit the size of token-batch, which keeps the GPU from saturating for the iterations dominated by decoding tokens. We propose BatchLLM to address the above problems. BatchLLM explicitly identifies the common prefixes globally. The requests sharing the same prefix will be scheduled together to reuse the KV context the best, which also shrinks the lifetime of common KV memory. BatchLLM reorders the requests and schedules the requests with larger ratio of decoding first to better mix the decoding tokens with the latter prefill chunks and applies memory-centric token batching to enlarge the token-batch sizes, which helps to increase the GPU utilization. Extensive evaluation shows that BatchLLM outperforms vLLM by 1.1x to 2x on a set of microbenchmarks and two typical industry workloads.

**Link**: [arxiv](http://arxiv.org/abs/2412.03594v1),  [pdf](http://arxiv.org/pdf/2412.03594v1)

**Tags**: cs.CL cs.AI cs.DC cs.LG 



### Reflecting Intelligent Surfaces-Assisted Multiple-Antenna Coded Caching
**Authors**: Xiaofan Niu, Minquan Cheng, Kai Wan, Robert Caiming Qiu, Giuseppe Caire

**Updated**: 2024-11-28T16:35:22Z

**Summary**: Reconfigurable intelligent surface (RIS) has been treated as a core technique in improving wireless propagation environments for the next generation wireless communication systems. This paper proposes a new coded caching problem, referred to as Reconfigurable Intelligent Surface (RIS)-assisted multiple-antenna coded caching, which is composed of a server with multiple antennas and some single-antenna cache-aided users. Different from the existing multi-antenna coded caching problems, we introduce a passive RIS (with limited number of units) into the systems to further increase the multicast gain (i.e., degrees of freedom (DoF)) in the transmission, which is done by using RIS-assisted interference nulling. That is, by using RIS, we can `erase' any path between one transmission antenna and one receive antenna. We first propose a new RIS-assisted interference nulling approach to search for the phase-shift coefficients of RIS for the sake of interference nulling, which converges faster than the state-of-the-art algorithm. After erasing some paths in each time slot, the delivery can be divided into several non-overlapping groups including transmission antennas and users, where in each group the transmission antennas serve the contained users without suffering interference from the transmissions by other groups. The division of groups for the sake of maximizing the DoF could be formulated into a combinatorial optimization problem. We propose a grouping algorithm which can find the optimal solution with low complexity, and the corresponding coded caching scheme achieving this DoF.

**Link**: [arxiv](http://arxiv.org/abs/2411.19248v1),  [pdf](http://arxiv.org/pdf/2411.19248v1)

**Tags**: cs.IT math.IT 



### Fresh Caching of Dynamic Contents using Restless Multi-armed Bandits
**Authors**: Ankita Koley, Chandramani Singh

**Updated**: 2024-11-28T14:42:54Z

**Summary**: We consider a dynamic content caching problem wherein the contents get updated at a central server, and local copies of a subset of contents are cached at a local cache associated with a Base station (BS). When a content request arrives, based on whether the content is in the local cache, the BS can decide whether to fetch the content from the central server or serve the cached version from the local cache. Fetching a content incurs a fixed fetching cost, and serving the cached version incurs an ageing cost proportional to the age-of-version (AoV) of the content. The BS has only partial information regarding AoVs of the contents. We formulate an optimal content fetching and caching problem to minimize the average cost subject to cache capacity constraints. The problem suffers from the curse of dimensionality and is provably hard to solve. We formulate this problem as a continuous time restless multi-armed bandit process (RMAB), where a single content problem of the corresponding RMAB is a partially observable Markov decision process. We reformulate the single content problem as a semi-Markov decision process, prove indexability, and provide a Whittle index based solution to this problem. Finally, we compare the performance with recent work and show that our proposed policy is optimal via simulations.

**Link**: [arxiv](http://arxiv.org/abs/2404.12468v2),  [pdf](http://arxiv.org/pdf/2404.12468v2)

**Tags**: cs.NI 



### Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model
**Authors**: Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, Fang Wan

**Updated**: 2024-11-28T12:50:05Z

**Summary**: As a fundamental backbone for video generation, diffusion models are challenged by low inference speed due to the sequential nature of denoising. Previous methods speed up the models by caching and reusing model outputs at uniformly selected timesteps. However, such a strategy neglects the fact that differences among model outputs are not uniform across timesteps, which hinders selecting the appropriate model outputs to cache, leading to a poor balance between inference efficiency and visual quality. In this study, we introduce Timestep Embedding Aware Cache (TeaCache), a training-free caching approach that estimates and leverages the fluctuating differences among model outputs across timesteps. Rather than directly using the time-consuming model outputs, TeaCache focuses on model inputs, which have a strong correlation with the modeloutputs while incurring negligible computational cost. TeaCache first modulates the noisy inputs using the timestep embeddings to ensure their differences better approximating those of model outputs. TeaCache then introduces a rescaling strategy to refine the estimated differences and utilizes them to indicate output caching. Experiments show that TeaCache achieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07% Vbench score) degradation of visual quality.

**Link**: [arxiv](http://arxiv.org/abs/2411.19108v1),  [pdf](http://arxiv.org/pdf/2411.19108v1)

**Tags**: cs.CV 



### MiniKV: Pushing the Limits of LLM Inference via 2-Bit   Layer-Discriminative KV Cache
**Authors**: Akshat Sharma, Hangliang Ding, Jianping Li, Neel Dani, Minjia Zhang

**Updated**: 2024-11-28T02:01:50Z

**Summary**: How to efficiently serve LLMs in practice has become exceptionally challenging due to their prohibitive memory and computation requirements. In this study, we investigate optimizing the KV cache, whose memory footprint poses a critical bottleneck in LLM inference, especially when dealing with long context tasks. To tackle the challenge, we introduce MiniKV, a KV cache optimization method that simultaneously preserves long context task accuracy while significantly reducing KV cache size via a novel 2-bit layer-discriminative KV cache. More importantly, we develop specialized CUDA kernels to make MiniKV compatible with FlashAttention. Experiments on a wide range of long context tasks show that MiniKV effectively achieves 86% KV cache compression ratio while recovering over 98.5% of accuracy, outperforming state-of-the-art methods while achieving excellent measured system performance improvements.

**Link**: [arxiv](http://arxiv.org/abs/2411.18077v2),  [pdf](http://arxiv.org/pdf/2411.18077v2)

**Tags**: cs.CL cs.LG 



### Mixture of Cache-Conditional Experts for Efficient Mobile Device   Inference
**Authors**: Andrii Skliar, Ties van Rozendaal, Romain Lepert, Todor Boinovski, Mart van Baalen, Markus Nagel, Paul Whatmough, Babak Ehteshami Bejnordi

**Updated**: 2024-11-27T18:59:48Z

**Summary**: Mixture of Experts (MoE) LLMs have recently gained attention for their ability to enhance performance by selectively engaging specialized subnetworks or "experts" for each input. However, deploying MoEs on memory-constrained devices remains challenging, particularly when generating tokens sequentially with a batch size of one, as opposed to typical high-throughput settings involving long sequences or large batches. In this work, we optimize MoE on memory-constrained devices where only a subset of expert weights fit in DRAM. We introduce a novel cache-aware routing strategy that leverages expert reuse during token generation to improve cache locality. We evaluate our approach on language modeling, MMLU, and GSM8K benchmarks and present on-device results demonstrating 2$\times$ speedups on mobile devices, offering a flexible, training-free solution to extend MoE's applicability across real-world applications.

**Link**: [arxiv](http://arxiv.org/abs/2412.00099v1),  [pdf](http://arxiv.org/pdf/2412.00099v1)

**Tags**: cs.LG cs.AI cs.AR 



### OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,   Refined, and Overhauled Software
**Authors**: Oliver Maximilian Zobel, Johannes Maierhofer, Andreas Köstler, Daniel J. Rixen

**Updated**: 2024-11-27T18:09:29Z

**Summary**: OASIS-UROS continues the previously published Open Acquisition System for IEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this version improves the overall performance by switching to an SD card caching system and upgrading the analog-digital converter to an AD7606C-18, which has a higher resolution, provides eight channels, oversampling, and software-adjustable voltage ranges. Also improved is the IEPE front-end and power supply, as well as the firmware of the acquisition system, which can now achieve a sample rate of up to 36 kHz while sampling all eight channels. This paper documents the hardware and software of OASIS-UROS and provides all materials required to reproduce the open acquisition system. Lastly, the system was validated against commercial hardware and software in an experimental modal analysis context. This showed that the system performs close to the commercial one in some aspects with respect to the utilized test case. While OASIS-UROS cannot match the full performance of the commercial system, the developed system can be a viable alternative for students, people in academia, or smaller companies that have a constrained budget or require complete insight as well as adaptability of the hardware and software.

**Link**: [arxiv](http://arxiv.org/abs/2411.18566v1),  [pdf](http://arxiv.org/pdf/2411.18566v1)

**Tags**: physics.ins-det 



### cedar: Optimized and Unified Machine Learning Input Data Pipelines
**Authors**: Mark Zhao, Emanuel Adamiak, Christos Kozyrakis

**Updated**: 2024-11-27T18:05:57Z

**Summary**: The input data pipeline is an essential component of each machine learning (ML) training job. It is responsible for reading massive amounts of training data, processing batches of samples using complex transformations, and loading them onto training nodes at low latency and high throughput. Performant input data systems are becoming increasingly critical, driven by skyrocketing data volumes and training throughput demands. Unfortunately, current input data systems cannot fully leverage key performance optimizations, resulting in hugely inefficient infrastructures that require significant resources - or worse - underutilize expensive accelerators.   To address these demands, we present cedar, an optimized and unified programming framework for ML input data pipelines. cedar allows users to define input data pipelines using composable operators that support arbitrary ML frameworks and libraries. cedar introduces an extensible optimizer that systematically applies a complex combination of optimizations (e.g., offloading, caching, prefetching, fusion, and reordering). It orchestrates processing across a customizable set of local and distributed compute resources in order to improve processing performance and efficiency, all without user input. Across eight pipelines, cedar improves performance by up to 1.87x to 10.65x compared to state-of-the-art input data systems.

**Link**: [arxiv](http://arxiv.org/abs/2401.08895v4),  [pdf](http://arxiv.org/pdf/2401.08895v4)

**Tags**: cs.LG cs.DC cs.PF 



### FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware   Large Language Model Serving
**Authors**: Ao Shen, Zhiyao Li, Mingyu Gao

**Updated**: 2024-11-27T15:07:28Z

**Summary**: Serving numerous users and requests concurrently requires good fairness in Large Language Models (LLMs) serving system. This ensures that, at the same cost, the system can meet the Service Level Objectives (SLOs) of more users , such as time to first token (TTFT) and time between tokens (TBT), rather than allowing a few users to experience performance far exceeding the SLOs. To achieve better fairness, the preemption-based scheduling policy dynamically adjusts the priority of each request to maintain balance during runtime. However, existing systems tend to overly prioritize throughput, overlooking the overhead caused by preemption-induced context switching, which is crucial for maintaining fairness through priority adjustments. In this work, we identify three main challenges that result in this overhead. 1) Inadequate I/O utilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn conversations. Our key insight is that the block-based KV cache memory policy in existing systems, while achieving near-zero memory waste, leads to discontinuity and insufficient granularity in the KV cache memory. To respond, we introduce FastSwitch, a fairness-aware serving system that not only aligns with existing KV cache memory allocation policy but also mitigates context switching overhead. Our evaluation shows that FastSwitch outperforms the state-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across different tail TTFT and TBT.

**Link**: [arxiv](http://arxiv.org/abs/2411.18424v1),  [pdf](http://arxiv.org/pdf/2411.18424v1)

**Tags**: cs.LG cs.DC 



### Accelerating Vision Diffusion Transformers with Skip Branches
**Authors**: Guanjie Chen, Xinyu Zhao, Yucheng Zhou, Tianlong Chen, Yu Cheng

**Updated**: 2024-11-27T14:43:46Z

**Summary**: Diffusion Transformers (DiT), an emerging image and video generation model architecture, has demonstrated great potential because of its high generation quality and scalability properties. Despite the impressive performance, its practical deployment is constrained by computational complexity and redundancy in the sequential denoising process. While feature caching across timesteps has proven effective in accelerating diffusion models, its application to DiT is limited by fundamental architectural differences from U-Net-based approaches. Through empirical analysis of DiT feature dynamics, we identify that significant feature variation between DiT blocks presents a key challenge for feature reusability. To address this, we convert standard DiT into Skip-DiT with skip branches to enhance feature smoothness. Further, we introduce Skip-Cache which utilizes the skip branches to cache DiT features across timesteps at the inference time. We validated effectiveness of our proposal on different DiT backbones for video and image generation, showcasing skip branches to help preserve generation quality and achieve higher speedup. Experimental results indicate that Skip-DiT achieves a 1.5x speedup almost for free and a 2.2x speedup with only a minor reduction in quantitative metrics. Code is available at https://github.com/OpenSparseLLMs/Skip-DiT.git.

**Link**: [arxiv](http://arxiv.org/abs/2411.17616v2),  [pdf](http://arxiv.org/pdf/2411.17616v2)

**Tags**: cs.CV 



### WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent   Video Diffusion Model
**Authors**: Zongjian Li, Bin Lin, Yang Ye, Liuhan Chen, Xinhua Cheng, Shenghai Yuan, Li Yuan

**Updated**: 2024-11-27T08:21:47Z

**Summary**: Video Variational Autoencoder (VAE) encodes videos into a low-dimensional latent space, becoming a key component of most Latent Video Diffusion Models (LVDMs) to reduce model training costs. However, as the resolution and duration of generated videos increase, the encoding cost of Video VAEs becomes a limiting bottleneck in training LVDMs. Moreover, the block-wise inference method adopted by most LVDMs can lead to discontinuities of latent space when processing long-duration videos. The key to addressing the computational bottleneck lies in decomposing videos into distinct components and efficiently encoding the critical information. Wavelet transform can decompose videos into multiple frequency-domain components and improve the efficiency significantly, we thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages multi-level wavelet transform to facilitate low-frequency energy flow into latent representation. Furthermore, we introduce a method called Causal Cache, which maintains the integrity of latent space during block-wise inference. Compared to state-of-the-art video VAEs, WF-VAE demonstrates superior performance in both PSNR and LPIPS metrics, achieving 2x higher throughput and 4x lower memory consumption while maintaining competitive reconstruction quality. Our code and models are available at https://github.com/PKU-YuanGroup/WF-VAE.

**Link**: [arxiv](http://arxiv.org/abs/2411.17459v2),  [pdf](http://arxiv.org/pdf/2411.17459v2)

**Tags**: cs.CV cs.AI 



### A Method for Building Large Language Models with Predefined KV Cache   Capacity
**Authors**: Zhonghua Yi, Ge Niu, Lei Wang, Wei Tang, Liqiu Zhang

**Updated**: 2024-11-27T03:07:20Z

**Summary**: This paper introduces a novel approach, the Bounded-Cache Transformer (BCT), for building large language models with a predefined Key-Value (KV) cache capacity. The BCT addresses the excessive memory consumption issue in traditional KV caches by implementing a bounded-length KV cache, which is particularly suitable for the attention layers in Transformer decode-only architectures. By dynamically updating the key-value vector sequences, the BCT achieves efficient inference within limited cache capacity, significantly reducing memory usage while maintaining model performance and system throughput. Experimental results demonstrate that the BCT significantly reduces memory usage while maintaining the model's inference quality, offering a new solution for efficient inference in large language models.

**Link**: [arxiv](http://arxiv.org/abs/2411.15785v2),  [pdf](http://arxiv.org/pdf/2411.15785v2)

**Tags**: cs.CL 



### Attamba: Attending To Multi-Token States
**Authors**: Yash Akhauri, Safeen Huda, Mohamed S. Abdelfattah

**Updated**: 2024-11-26T18:52:06Z

**Summary**: When predicting the next token in a sequence, vanilla transformers compute attention over all previous tokens, resulting in quadratic scaling of compute with sequence length. State-space models compress the entire sequence of tokens into a fixed-dimensional representation to improve efficiency, while other architectures achieve sub-quadratic complexity via low-rank projections or sparse attention patterns over the sequence. In this paper, we introduce Attamba, a novel architecture that uses state-space models to compress chunks of tokens and applies attention on these compressed key-value representations. We find that replacing key and value projections in a transformer with SSMs can improve model quality and enable flexible token chunking, resulting in 24% improved perplexity with transformer of similar KV-Cache and attention footprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity trade-off. Attamba can perform attention on chunked-sequences of variable length, enabling a smooth transition between quadratic and linear scaling, offering adaptable efficiency gains.

**Link**: [arxiv](http://arxiv.org/abs/2411.17685v1),  [pdf](http://arxiv.org/pdf/2411.17685v1)

**Tags**: cs.LG cs.CL 



### STAR: Synthesis of Tailored Architectures
**Authors**: Armin W. Thomas, Rom Parnichkun, Alexander Amini, Stefano Massaroli, Michael Poli

**Updated**: 2024-11-26T18:42:42Z

**Summary**: Iterative improvement of model architectures is fundamental to deep learning: Transformers first enabled scaling, and recent advances in model hybridization have pushed the quality-efficiency frontier. However, optimizing architectures remains challenging and expensive. Current automated or manual approaches fall short, largely due to limited progress in the design of search spaces and due to the simplicity of resulting patterns and heuristics. In this work, we propose a new approach for the synthesis of tailored architectures (STAR). Our approach combines a novel search space based on the theory of linear input-varying systems, supporting a hierarchical numerical encoding into architecture genomes. STAR genomes are automatically refined and recombined with gradient-free, evolutionary algorithms to optimize for multiple model quality and efficiency metrics. Using STAR, we optimize large populations of new architectures, leveraging diverse computational units and interconnection patterns, improving over highly-optimized Transformers and striped hybrid models on the frontier of quality, parameter size, and inference cache for autoregressive language modeling.

**Link**: [arxiv](http://arxiv.org/abs/2411.17800v1),  [pdf](http://arxiv.org/pdf/2411.17800v1)

**Tags**: cs.LG cs.AI cs.NE 



### GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering
**Authors**: Jiaye Wu, Saeed Hadadan, Geng Lin, Matthias Zwicker, David Jacobs, Roni Sengupta

**Updated**: 2024-11-26T17:28:06Z

**Summary**: In this paper, we present GaNI, a Global and Near-field Illumination-aware neural inverse rendering technique that can reconstruct geometry, albedo, and roughness parameters from images of a scene captured with co-located light and camera. Existing inverse rendering techniques with co-located light-camera focus on single objects only, without modeling global illumination and near-field lighting more prominent in scenes with multiple objects. We introduce a system that solves this problem in two stages; we first reconstruct the geometry powered by neural volumetric rendering NeuS, followed by inverse neural radiosity that uses the previously predicted geometry to estimate albedo and roughness. However, such a naive combination fails and we propose multiple technical contributions that enable this two-stage approach. We observe that NeuS fails to handle near-field illumination and strong specular reflections from the flashlight in a scene. We propose to implicitly model the effects of near-field illumination and introduce a surface angle loss function to handle specular reflections. Similarly, we observe that invNeRad assumes constant illumination throughout the capture and cannot handle moving flashlights during capture. We propose a light position-aware radiance cache network and additional smoothness priors on roughness to reconstruct reflectance. Experimental evaluation on synthetic and real data shows that our method outperforms the existing co-located light-camera-based inverse rendering techniques. Our approach produces significantly better reflectance and slightly better geometry than capture strategies that do not require a dark room.

**Link**: [arxiv](http://arxiv.org/abs/2403.15651v3),  [pdf](http://arxiv.org/pdf/2403.15651v3)

**Tags**: cs.CV 



### Degrees of Freedom of Cache-Aided Interference Channels Assisted by   Active Intelligent Reflecting Surfaces
**Authors**: Abolfazl Changizi, Ali H. Abdollahi Bafghi, Masoumeh Nasiri-Kenari

**Updated**: 2024-11-26T16:21:10Z

**Summary**: This paper studies cache-aided wireless networks in the presence of active intelligent reflecting surfaces (IRS) from an information-theoretic perspective. Specifically, we explore interference management in a cache-aided wireless network assisted by an active IRS, to enhance the achievable degrees of freedom (DoF). To this end, we jointly design the content placement, delivery phase, and phase shifts of the IRS and propose a one-shot achievable scheme. Our scheme exploits transmitters' cooperation, cache contents (as side information), interference alignment, and IRS capabilities, adapting to the network's parameters. We derive the achievable one-shot sum-DoF for different sizes of cache memories, network configurations, and numbers of IRS elements. Our results highlight the potential of deploying an IRS in cache-aided wireless communication systems, underscoring the enhancement of achievable DoF for various parameter regimes, particularly when the sizes of the caches (especially at the transmitters) are inadequate. Notably, we show that access to an IRS with a sufficient number of elements enables the achievement of the maximum possible DoF for various parameter regimes of interest.

**Link**: [arxiv](http://arxiv.org/abs/2411.17559v1),  [pdf](http://arxiv.org/pdf/2411.17559v1)

**Tags**: cs.IT math.IT 



### DreamCache: Finetuning-Free Lightweight Personalized Image Generation   via Feature Caching
**Authors**: Emanuele Aiello, Umberto Michieli, Diego Valsesia, Mete Ozay, Enrico Magli

**Updated**: 2024-11-26T15:03:14Z

**Summary**: Personalized image generation requires text-to-image generative models that capture the core features of a reference subject to allow for controlled generation across different contexts. Existing methods face challenges due to complex training requirements, high inference costs, limited flexibility, or a combination of these issues. In this paper, we introduce DreamCache, a scalable approach for efficient and high-quality personalized image generation. By caching a small number of reference image features from a subset of layers and a single timestep of the pretrained diffusion denoiser, DreamCache enables dynamic modulation of the generated image features through lightweight, trained conditioning adapters. DreamCache achieves state-of-the-art image and text alignment, utilizing an order of magnitude fewer extra parameters, and is both more computationally effective and versatile than existing models.

**Link**: [arxiv](http://arxiv.org/abs/2411.17786v1),  [pdf](http://arxiv.org/pdf/2411.17786v1)

**Tags**: cs.CV cs.AI cs.LG 



### Star Attention: Efficient LLM Inference over Long Sequences
**Authors**: Shantanu Acharya, Fei Jia, Boris Ginsburg

**Updated**: 2024-11-26T05:10:04Z

**Summary**: Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 95-100% of accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2411.17116v1),  [pdf](http://arxiv.org/pdf/2411.17116v1)

**Tags**: cs.CL cs.AI cs.LG 



### Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation
**Authors**: Chaoyi Jiang, Lei Gao, Hossein Entezari Zarch, Murali Annavaram

**Updated**: 2024-11-26T04:03:14Z

**Summary**: Inference for Large Language Models (LLMs) is computationally demanding. To reduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to store intermediate activations, enabling GPUs to perform only the incremental computation required for each new token. This approach significantly lowers the computational overhead for token generation. However, the memory required for KV caching grows rapidly, often exceeding the capacity of GPU memory. A cost-effective alternative is to offload KV cache to CPU memory, which alleviates GPU memory pressure but shifts the bottleneck to the limited bandwidth of the PCIe connection between the CPU and GPU. Existing methods attempt to address these issues by overlapping GPU computation with I/O or employing CPU-GPU heterogeneous execution, but they are hindered by excessive data movement and dependence on CPU capabilities. In this paper, we introduce an efficient CPU-GPU I/O-aware LLM inference method that avoids transferring the entire KV cache from CPU to GPU by recomputing partial KV cache from activations while concurrently transferring the remaining KV cache via PCIe bus. This approach overlaps GPU recomputation with data transfer to minimize idle GPU time and maximize inference performance. Our method is fully automated by integrating a profiler module that utilizes input characteristics and system hardware information, a scheduler module to optimize the distribution of computation and communication workloads, and a runtime module to efficiently execute the derived execution plan. Experimental results show that our method achieves up to 35.8% lower latency and 46.2% higher throughput during decoding compared to state-of-the-art approaches.

**Link**: [arxiv](http://arxiv.org/abs/2411.17089v1),  [pdf](http://arxiv.org/pdf/2411.17089v1)

**Tags**: cs.LG cs.DC cs.PF 



### Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal   Generation and Cache Sharing
**Authors**: Kaifeng Gao, Jiaxin Shi, Hanwang Zhang, Chunping Wang, Jun Xiao, Long Chen

**Updated**: 2024-11-25T13:33:41Z

**Summary**: With the advance of diffusion models, today's video generation has achieved impressive quality. To extend the generation length and facilitate real-world applications, a majority of video diffusion models (VDMs) generate videos in an autoregressive manner, i.e., generating subsequent clips conditioned on the last frame(s) of the previous clip. However, existing autoregressive VDMs are highly inefficient and redundant: The model must re-compute all the conditional frames that are overlapped between adjacent clips. This issue is exacerbated when the conditional frames are extended autoregressively to provide the model with long-term context. In such cases, the computational demands increase significantly (i.e., with a quadratic complexity w.r.t. the autoregression step). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with Causal generation and Cache sharing. For causal generation, it introduces unidirectional feature computation, which ensures that the cache of conditional frames can be precomputed in previous autoregression steps and reused in every subsequent step, eliminating redundant computations. For cache sharing, it shares the cache across all denoising steps to avoid the huge cache storage cost. Extensive experiments demonstrated that our Ca2-VDM achieves state-of-the-art quantitative and qualitative video generation results and significantly improves the generation speed. Code is available at https://github.com/Dawn-LX/CausalCache-VDM

**Link**: [arxiv](http://arxiv.org/abs/2411.16375v1),  [pdf](http://arxiv.org/pdf/2411.16375v1)

**Tags**: cs.CV 



### Analog In-Memory Computing Attention Mechanism for Fast and   Energy-Efficient Large Language Models
**Authors**: Nathan Leroux, Paul-Philipp Manea, Chirag Sudarshan, Jan Finkbeiner, Sebastian Siegel, John Paul Strachan, Emre Neftci

**Updated**: 2024-11-25T12:14:33Z

**Summary**: Transformer networks, driven by self-attention, are central to Large Language Models. In generative Transformers, self-attention uses cache memory to store token projections, avoiding recomputation at each time step. However, GPU-stored projections must be loaded into SRAM for each new generation step, causing latency and energy bottlenecks.   We present a custom self-attention in-memory computing architecture based on emerging charge-based memories called gain cells, which can be efficiently written to store new tokens during sequence generation and enable parallel analog dot-product computation required for self-attention. However, the analog gain cell circuits introduce non-idealities and constraints preventing the direct mapping of pre-trained models. To circumvent this problem, we design an initialization algorithm achieving text processing performance comparable to GPT-2 without training from scratch. Our architecture respectively reduces attention latency and energy consumption by up to two and five orders of magnitude compared to GPUs, marking a significant step toward ultra-fast, low-power generative Transformers.

**Link**: [arxiv](http://arxiv.org/abs/2409.19315v2),  [pdf](http://arxiv.org/pdf/2409.19315v2)

**Tags**: cs.NE cs.AI cs.AR cs.ET 



### Deegen: A JIT-Capable VM Generator for Dynamic Languages
**Authors**: Haoran Xu, Fredrik Kjolstad

**Updated**: 2024-11-24T21:57:29Z

**Summary**: Building a high-performance JIT-capable VM for a dynamic language has traditionally required a tremendous amount of time, money, and expertise. We present Deegen, a meta-compiler that allows users to generate a high-performance JIT-capable VM for their own language at an engineering cost similar to writing a simple interpreter. Deegen takes in the execution semantics of the bytecodes implemented as C++ functions, and automatically generates a two-tier VM execution engine with a state-of-the-art interpreter, a state-of-the-art baseline JIT, and the tier-switching logic that connects them into a self-adaptive system.   We are the first to demonstrate the automatic generation of a JIT compiler, and the automatic generation of an interpreter that outperforms the state of the art. Our performance comes from a long list of optimizations supported by Deegen, including bytecode specialization and quickening, register pinning, tag register optimization, call inline caching, generic inline caching, JIT polymorphic IC, JIT IC inline slab, type-check removal and strength reduction, type-based slow-path extraction and outlining, JIT hot-cold code splitting, and JIT OSR-entry. These optimizations are either employed automatically, or guided by the language implementer through intuitive APIs. As a result, the disassembly of the Deegen-generated interpreter, baseline JIT, and the generated JIT code rivals the assembly code hand-written by experts in state-of-the-art VMs.   We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using Deegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than the official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter. LJR's baseline JIT has negligible startup delay, and its execution performance is on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44 benchmarks) than LuaJIT's optimizing JIT.

**Link**: [arxiv](http://arxiv.org/abs/2411.11469v2),  [pdf](http://arxiv.org/pdf/2411.11469v2)

**Tags**: cs.PL 



### Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM   Inference Environments
**Authors**: Nikoleta Iliakopoulou, Jovan Stojkovic, Chloe Alverti, Tianyin Xu, Hubertus Franke, Josep Torrellas

**Updated**: 2024-11-24T16:20:57Z

**Summary**: The widespread adoption of LLMs has driven an exponential rise in their deployment, imposing substantial demands on inference clusters. These clusters must handle numerous concurrent queries for different LLM downstream tasks. To handle multi-task settings with vast LLM parameter counts, methods like Low-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most of the base LLM model across tasks. Hence, they allow concurrent task serving with minimal memory requirements. However, existing LLM serving systems face inefficiencies: they overlook workload heterogeneity, impose high link bandwidth from frequent adapter loading, and suffer from head-of-line blocking in their schedulers. To address these challenges, we present Chameleon, a novel LLM serving system optimized for many adapter environments, that relies on two core ideas: adapter caching and adapter-aware scheduling. First, Chameleon caches popular adapters in GPU memory, minimizing the adapter loading times. Importantly, it uses the otherwise idle GPU memory, avoiding extra memory costs. Second, Chameleon uses a non-preemptive multi-queue scheduling to efficiently account for workload heterogeneity. In this way, Chameleon simultaneously prevents head of line blocking and starvation. We implement Chameleon on top of a state-of-the-art LLM serving platform and evaluate it with real-world production traces and open-source LLMs. Under high loads, Chameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively, while improving throughput by 1.5x compared to state-of-the-art baselines.

**Link**: [arxiv](http://arxiv.org/abs/2411.17741v1),  [pdf](http://arxiv.org/pdf/2411.17741v1)

**Tags**: cs.DC cs.AR cs.OS cs.PF C.0; D.4 



### Test-time Alignment-Enhanced Adapter for Vision-Language Models
**Authors**: Baoshun Tong, Kaiyu Song, Hanjiang Lai

**Updated**: 2024-11-24T06:43:38Z

**Summary**: Test-time adaptation with pre-trained vision-language models (VLMs) has attracted increasing attention for tackling the issue of distribution shift during the test phase. While prior methods have shown effectiveness in addressing distribution shift by adjusting classification logits, they are not optimal due to keeping text features unchanged. To address this issue, we introduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA), which trains an adapter with test samples to adjust text features during the test phase. We can enhance the text-to-image alignment prediction by utilizing an adapter to adapt text features. Furthermore, we also propose to adopt the negative cache from TDA as enhancement module, which further improves the performance of TAEA. Our approach outperforms the state-of-the-art TTA method of pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark and 2.5% on the cross-domain benchmark, with an acceptable training time. Code will be available at https://github.com/BaoshunWq/clip-TAEA.

**Link**: [arxiv](http://arxiv.org/abs/2411.15735v1),  [pdf](http://arxiv.org/pdf/2411.15735v1)

**Tags**: cs.CV 



### Squeezed Attention: Accelerating Long Context Length LLM Inference
**Authors**: Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Monishwaran Maheswaran, June Paik, Michael W. Mahoney, Kurt Keutzer, Amir Gholami

**Updated**: 2024-11-23T22:11:42Z

**Summary**: Emerging Large Language Model (LLM) applications require long input prompts to perform complex downstream tasks like document analysis and code generation. For these long context length applications, the length of the input prompt poses a significant challenge in terms of inference efficiency since the inference costs increase linearly with sequence length. However, for many of these applications, much of the context in the prompt is fixed across different user inputs, thereby providing the opportunity to perform offline optimizations to process user inputs quickly, as they are received. In this work, we propose Squeezed Attention as a mechanism to accelerate LLM applications where a large portion of the input prompt is fixed. We first leverage K-means clustering offline to group the keys for the fixed context based on semantic similarity and represent each cluster with a single centroid value. During inference, we compare query tokens from the user input with the centroids to predict which of the keys from the fixed context are semantically relevant and need to be loaded during inference. We then compute exact attention using only these important keys from the fixed context, thereby reducing bandwidth and computational costs. We also extend our method to use a hierarchical centroid lookup to identify important keys, which can reduce the complexity of attention from linear to logarithmic with respect to the context length. We implement optimized Triton kernels for centroid comparison and sparse FlashAttention with important keys, achieving more than 4x speedups during both the prefill and generation phases for long-context inference. Furthermore, we have extensively evaluated our method on various long-context benchmarks including LongBench, where it achieves a 3x reduction in KV cache budget without accuracy loss and up to an 8x reduction with <0.5 point accuracy gap for various models.

**Link**: [arxiv](http://arxiv.org/abs/2411.09688v2),  [pdf](http://arxiv.org/pdf/2411.09688v2)

**Tags**: cs.CL 



### TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph   Representation Learning
**Authors**: Gangda Deng, Hongkuan Zhou, Hanqing Zeng, Yinglong Xia, Christopher Leung, Jianbo Li, Rajgopal Kannan, Viktor Prasanna

**Updated**: 2024-11-23T10:42:11Z

**Summary**: Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated state-of-the-art performance in various high-impact applications, including fraud detection and content recommendation. Despite the success of TGNNs, they are prone to the prevalent noise found in real-world dynamic graphs like time-deprecated links and skewed interaction distribution. The noise causes two critical issues that significantly compromise the accuracy of TGNNs: (1) models are supervised by inferior interactions, and (2) noisy input induces high variance in the aggregated messages. However, current TGNN denoising techniques do not consider the diverse and dynamic noise pattern of each node. In addition, they also suffer from the excessive mini-batch generation overheads caused by traversing more neighbors. We believe the remedy for fast and accurate TGNNs lies in temporal adaptive sampling. In this work, we propose TASER, the first adaptive sampling method for TGNNs optimized for accuracy, efficiency, and scalability. TASER adapts its mini-batch selection based on training dynamics and temporal neighbor selection based on the contextual, structural, and temporal properties of past interactions. To alleviate the bottleneck in mini-batch generation, TASER implements a pure GPU-based temporal neighbor finder and a dedicated GPU feature cache. We evaluate the performance of TASER using two state-of-the-art backbone TGNNs. On five popular datasets, TASER outperforms the corresponding baselines by an average of 2.3% in Mean Reciprocal Rank (MRR) while achieving an average of 5.1x speedup in training time.

**Link**: [arxiv](http://arxiv.org/abs/2402.05396v3),  [pdf](http://arxiv.org/pdf/2402.05396v3)

**Tags**: cs.LG cs.AI 



### HRSAM: Efficient Interactive Segmentation in High-Resolution Images
**Authors**: You Huang, Wenbin Lai, Jiayi Ji, Liujuan Cao, Shengchuan Zhang, Rongrong Ji

**Updated**: 2024-11-23T01:44:00Z

**Summary**: The Segment Anything Model (SAM) has advanced interactive segmentation but is limited by the high computational cost on high-resolution images. This requires downsampling to meet GPU constraints, sacrificing the fine-grained details needed for high-precision interactive segmentation. To address SAM's limitations, we focus on visual length extrapolation and propose a lightweight model named HRSAM. The extrapolation enables HRSAM trained on low resolutions to generalize to high resolutions. We begin by finding the link between the extrapolation and attention scores, which leads us to base HRSAM on Swin attention. We then introduce the Flexible Local Attention (FLA) framework, using CUDA-optimized Efficient Memory Attention to accelerate HRSAM. Within FLA, we implement Flash Swin attention, achieving over a 35% speedup compared to traditional Swin attention, and propose a KV-only padding mechanism to enhance extrapolation. We also develop the Cycle-scan module that uses State Space models to efficiently expand HRSAM's receptive field. We further develop the HRSAM++ within FLA by adding an anchor map, providing multi-scale data augmentation for the extrapolation and a larger receptive field at slight computational cost. Experiments show that, under standard training, HRSAMs surpass the previous SOTA with only 38% of the latency. With SAM-distillation, the extrapolation enables HRSAMs to outperform the teacher model at lower latency. Further finetuning achieves performance significantly exceeding the previous SOTA.

**Link**: [arxiv](http://arxiv.org/abs/2407.02109v2),  [pdf](http://arxiv.org/pdf/2407.02109v2)

**Tags**: cs.CV cs.AI 



### Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered   Images for Online Breath-hold Reproducibility Verification of Liver   Stereotactic Body Radiation Therapy
**Authors**: Sugandima Weragoda, Ping Xia, Kevin Stephans, Neil Woody, Michael Martens, Robert Brown, Bingqi Guo

**Updated**: 2024-11-22T19:30:40Z

**Summary**: Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally invasive treatment method for liver cancer and liver metastases. However, the effectiveness of SBRT relies on the accurate delivery of the dose to the tumor while sparing healthy tissue. Challenges persist in ensuring breath-hold reproducibility, with current methods often requiring manual verification of liver dome positions from kV-triggered images. To address this, we propose a proof-of-principle study of a deep learning-based pipeline to automatically delineate the liver dome from kV-planar images. From 24 patients who received SBRT for liver cancer or metastasis inside liver, 711 KV-triggered images acquired for online breath-hold verification were included in the current study. We developed a pipeline comprising a trained U-Net for automatic liver dome region segmentation from the triggered images followed by extraction of the liver dome via thresholding, edge detection, and morphological operations. The performance and generalizability of the pipeline was evaluated using 2-fold cross validation. The training of the U-Net model for liver region segmentation took under 30 minutes and the automatic delineation of a liver dome for any triggered image took less than one second. The RMSE and rate of detection for Fold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2 with 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3% respectively.

**Link**: [arxiv](http://arxiv.org/abs/2411.15322v1),  [pdf](http://arxiv.org/pdf/2411.15322v1)

**Tags**: physics.med-ph cs.CV 



### AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out   Context Attribution
**Authors**: Fengyuan Liu, Nikhil Kandpal, Colin Raffel

**Updated**: 2024-11-22T18:06:14Z

**Summary**: The influence of contextual input on the behavior of large language models (LLMs) has prompted the development of context attribution methods that aim to quantify each context span's effect on an LLM's generations. The leave-one-out (LOO) error, which measures the change in the likelihood of the LLM's response when a given span of the context is removed, provides a principled way to perform context attribution, but can be prohibitively expensive to compute for large models. In this work, we introduce AttriBoT, a series of novel techniques for efficiently computing an approximation of the LOO error for context attribution. Specifically, AttriBoT uses cached activations to avoid redundant operations, performs hierarchical attribution to reduce computation, and emulates the behavior of large target models with smaller proxy models. Taken together, AttriBoT can provide a >300x speedup while remaining more faithful to a target model's LOO error than prior context attribution methods. This stark increase in performance makes computing context attributions for a given response 30x faster than generating the response itself, empowering real-world applications that require computing attributions at scale. We release a user-friendly and efficient implementation of AttriBoT to enable efficient LLM interpretability as well as encourage future development of efficient context attribution methods.

**Link**: [arxiv](http://arxiv.org/abs/2411.15102v1),  [pdf](http://arxiv.org/pdf/2411.15102v1)

**Tags**: cs.LG 



### DyCoke: Dynamic Compression of Tokens for Fast Video Large Language   Models
**Authors**: Keda Tao, Can Qin, Haoxuan You, Yang Sui, Huan Wang

**Updated**: 2024-11-22T15:55:19Z

**Summary**: Video large language models (VLLMs) have significantly advanced recently in processing complex video content, yet their inference efficiency remains constrained because of the high computational cost stemming from the thousands of visual tokens generated from the video inputs. We empirically observe that, unlike single image inputs, VLLMs typically attend visual tokens from different frames at different decoding iterations, making a one-shot pruning strategy prone to removing important tokens by mistake. Motivated by this, we present DyCoke, a training-free token compression method to optimize token representation and accelerate VLLMs. DyCoke incorporates a plug-and-play temporal compression module to minimize temporal redundancy by merging redundant tokens across frames, and applies dynamic KV cache reduction to prune spatially redundant tokens selectively. It ensures high-quality inference by dynamically retaining the critical tokens at each decoding step. Extensive experimental results demonstrate that DyCoke can outperform the prior SoTA counterparts, achieving 1.5X inference speedup, 1.4X memory reduction against the baseline VLLM, while still improving the performance, with no training.

**Link**: [arxiv](http://arxiv.org/abs/2411.15024v1),  [pdf](http://arxiv.org/pdf/2411.15024v1)

**Tags**: cs.CV cs.LG 



### ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for   Scalable Recommendation System
**Authors**: Youngsuk Kim, Junghwan Lim, Hyuk-Jae Lee, Chae Eun Rhee

**Updated**: 2024-11-21T05:55:43Z

**Summary**: The model size growth of personalized recommendation systems poses new challenges for inference. Weight-sharing algorithms have been proposed for size reduction, but they increase memory access. Recent advancements in processing-in-memory (PIM) enhanced the model throughput by exploiting memory parallelism, but such algorithms introduce massive CPU-PIM communication into prior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing recommendation system acceleration. ProactivePIM integrates a cache within the PIM with a prefetching scheme to leverage a unique locality of the algorithm and eliminate communication overhead through a subtable mapping strategy. ProactivePIM achieves a 4.8x speedup compared to prior works.

**Link**: [arxiv](http://arxiv.org/abs/2402.04032v5),  [pdf](http://arxiv.org/pdf/2402.04032v5)

**Tags**: cs.AR cs.AI 



### Static Reuse Profile Estimation for Array Applications
**Authors**: Abdur Razzak, Atanu Barai, Nandakishore Santhi, Abdel-Hameed A. Badawy

**Updated**: 2024-11-21T05:26:57Z

**Summary**: Reuse distance analysis is a widely recognized method for application characterization that illustrates cache locality. Although there are various techniques to calculate the reuse profile from dynamic memory traces, it is both time and space-consuming due to the requirement to collect dynamic memory traces at runtime. In contrast, static analysis reuse profile estimation is a promisingly faster approach since it is calculated at compile time without running the program or collecting memory traces. This work presents a static analysis technique to estimate the reuse profile of loop-based programs. For an input program, we generate a basic block-level control flow graph and the execution count by analyzing the LLVM IR of the program. We present the memory accesses of the application kernel in a compact bracketed format and use a recursive algorithm to predict the reuse distance histogram. We deploy a separate predictor that unrolls the loop(s) for smaller bounds and generates a temporary reuse distance profile for those small cases. Using these smaller profiles, the reuse profile is extrapolated for the actual loop bound(s). We use this reuse profile to predict the cache hit rate. Results show that our model can predict cache hit rates with an average accuracy of 95% relative to the dynamic reuse profile methods.

**Link**: [arxiv](http://arxiv.org/abs/2411.13854v1),  [pdf](http://arxiv.org/pdf/2411.13854v1)

**Tags**: cs.PF 



### Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification
**Authors**: Jintao Rong, Hao Chen, Linlin Ou, Tianxiao Chen, Xinyi Yu, Yifan Liu

**Updated**: 2024-11-21T04:12:53Z

**Summary**: The Contrastive Language-Image Pretraining (CLIP) model has been widely used in various downstream vision tasks. The few-shot learning paradigm has been widely adopted to augment its capacity for these tasks. However, current paradigms may struggle with fine-grained classification, such as satellite image recognition, due to widening domain gaps. To address this limitation, we propose retrieval-enhanced visual prompt learning (RePrompt), which introduces retrieval mechanisms to cache and reuse the knowledge of downstream tasks. RePrompt constructs a retrieval database from either training examples or external data if available, and uses a retrieval mechanism to enhance multiple stages of a simple prompt learning baseline, thus narrowing the domain gap. During inference, our enhanced model can reference similar samples brought by retrieval to make more accurate predictions. A detailed analysis reveals that retrieval helps to improve the distribution of late features, thus, improving generalization for downstream tasks. Reprompt attains state-of-the-art performance on a wide range of vision datasets, including 11 image datasets, 3 video datasets, 1 multi-view dataset, and 4 domain generalization benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2306.02243v3),  [pdf](http://arxiv.org/pdf/2306.02243v3)

**Tags**: cs.CV 



### InstCache: A Predictive Cache for LLM Serving
**Authors**: Longwei Zou, Tingfeng Liu, Kai Chen, Jiangang Kong, Yangdong Deng

**Updated**: 2024-11-21T03:52:41Z

**Summary**: Large language models are revolutionizing every aspect of human life. However, the unprecedented power comes at the cost of significant computing intensity, suggesting long latency and large energy footprint. Key-Value Cache and Semantic Cache have been proposed as a solution to the above problem, but both suffer from limited scalability due to significant memory cost for each token or instruction embeddings. Motivated by the observations that most instructions are short, repetitive and predictable by LLMs, we propose to predict user-instructions by an instruction-aligned LLM and store them in a predictive cache, so-called InstCache. We introduce an instruction pre-population algorithm based on the negative log likelihood of instructions, determining the cache size with regard to the hit rate. The proposed InstCache is efficiently implemented as a hash table with minimal lookup latency for deployment. Experimental results show that InstCache can achieve up to 51.34% hit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost of only 4.5GB.

**Link**: [arxiv](http://arxiv.org/abs/2411.13820v1),  [pdf](http://arxiv.org/pdf/2411.13820v1)

**Tags**: cs.CL cs.DC 



### WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series   Forecasting
**Authors**: Aobo Liang, Yan Sun, Nadra Guizani

**Updated**: 2024-11-21T03:34:44Z

**Summary**: In recent years, Transformer-based models (Transformers) have achieved significant success in multivariate time series forecasting (MTSF). However, previous works focus on extracting features either from the time domain or the frequency domain, which inadequately captures the trends and periodic characteristics. To address this issue, we propose a wavelet learning framework to model complex temporal dependencies of the time series data. The wavelet domain integrates both time and frequency information, allowing for the analysis of local characteristics of signals at different scales. Additionally, the Softmax self-attention mechanism used by Transformers has quadratic complexity, which leads to excessive computational costs when capturing long-term dependencies. Therefore, we propose a novel attention mechanism: Rotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary position embeddings to inject relative positional information to sequence tokens and introduces a small number of routing tokens $r$ to aggregate information from the $KV$ matrices and redistribute it to the $Q$ matrix, offering linear complexity. We further propose WaveRoRA, which leverages RoRA to capture inter-series dependencies in the wavelet domain. We conduct extensive experiments on eight real-world datasets. The results indicate that WaveRoRA outperforms existing state-of-the-art models while maintaining lower computational costs. Our code is available at https://github.com/Leopold2333/WaveRoRA.

**Link**: [arxiv](http://arxiv.org/abs/2410.22649v2),  [pdf](http://arxiv.org/pdf/2410.22649v2)

**Tags**: cs.LG 



### Adaptable Embeddings Network (AEN)
**Authors**: Stan Loosmore, Alexander Titus

**Updated**: 2024-11-21T02:15:52Z

**Summary**: Modern day Language Models see extensive use in text classification, yet this comes at significant computational cost. Compute-effective classification models are needed for low-resource environments, most notably on edge devices. We introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder architecture using Kernel Density Estimation (KDE). This architecture allows for runtime adaptation of classification criteria without retraining and is non-autoregressive. Through thorough synthetic data experimentation, we demonstrate our model outputs comparable and in certain cases superior results to that of autoregressive models an order of magnitude larger than AEN's size. The architecture's ability to preprocess and cache condition embeddings makes it ideal for edge computing applications and real-time monitoring systems.

**Link**: [arxiv](http://arxiv.org/abs/2411.13786v1),  [pdf](http://arxiv.org/pdf/2411.13786v1)

**Tags**: cs.LG cs.CL 



### Hymba: A Hybrid-head Architecture for Small Language Models
**Authors**: Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil Mahabaleshwarkar, Shih-Yang Liu, Matthijs Van Keirsbilck, Min-Hung Chen, Yoshi Suhara, Yingyan Lin, Jan Kautz, Pavlo Molchanov

**Updated**: 2024-11-20T19:51:25Z

**Summary**: We propose Hymba, a family of small language models featuring a hybrid-head parallel architecture that integrates transformer attention mechanisms with state space models (SSMs) for enhanced efficiency. Attention heads provide high-resolution recall, while SSM heads enable efficient context summarization. Additionally, we introduce learnable meta tokens that are prepended to prompts, storing critical information and alleviating the "forced-to-attend" burden associated with attention mechanisms. This model is further optimized by incorporating cross-layer key-value (KV) sharing and partial sliding window attention, resulting in a compact cache size. During development, we conducted a controlled study comparing various architectures under identical settings and observed significant advantages of our proposed architecture. Notably, Hymba achieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model surpasses all sub-2B public models in performance and even outperforms Llama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size reduction, and 3.49x throughput.

**Link**: [arxiv](http://arxiv.org/abs/2411.13676v1),  [pdf](http://arxiv.org/pdf/2411.13676v1)

**Tags**: cs.CL cs.AI cs.LG 



### MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration   on Resource-Constrained Edge Devices
**Authors**: Mohammadali Shakerdargah, Shan Lu, Chao Gao, Di Niu

**Updated**: 2024-11-20T19:44:26Z

**Summary**: The advent of foundation models have revolutionized various fields, enabling unprecedented task accuracy and flexibility in computational linguistics, computer vision and other domains. Attention mechanism has become an essential component of foundation models, due to their superb capability of capturing correlations in a sequence. However, attention results in quadratic complexity in memory and compute as the context length grows. Although many fusion-based exact attention acceleration algorithms have been developed for datacenter-grade GPUs and accelerators leveraging multi-core parallelism and data locality, yet it remains a significant challenge to accelerate attention on resource-constrained edge neural accelerators with limited compute units and stringent on-chip caches. In this paper, we propose a scheme for exact attention inference acceleration on memory-constrained edge accelerators, by parallelizing the utilization of heterogeneous compute units, i.e., vector processing units and matrix processing units. Our method involves scheduling workloads onto these different compute units in a multi-tiered tiling scheme to process tiled vector workloads and matrix workloads in attention as two streams, respecting the workload dependencies. We search for tiling factors to maximize the parallelization of both compute units while considering I/O overhead, and propose a proactive cache overwrite strategy to avoid undesirable cache spills in reality. Extensive results based on open-sourced simulation frameworks show up to 2.75x speedup and 54% reduction in energy consumption as compared to the state-of-the-art attention fusion method (FLAT) in the edge computing scenario. Further experiments on a real-world edge neural processing unit demonstrate speedup of up to 1.76x for attention as compared to FLAT, without affecting model output accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2411.17720v1),  [pdf](http://arxiv.org/pdf/2411.17720v1)

**Tags**: cs.DC cs.AI cs.PF C.1.4; I.2.7; I.5.1 



### A Distributed-memory Tridiagonal Solver Based on a Specialised Data   Structure Optimised for CPU and GPU Architectures
**Authors**: Semih Akkurt, Sébastien Lemaire, Paul Bartholomew, Sylvain Laizet

**Updated**: 2024-11-20T18:31:39Z

**Summary**: Various numerical methods used for solving partial differential equations (PDE) result in tridiagonal systems. Solving tridiagonal systems on distributed-memory environments is not straightforward, and often requires significant amount of communication. In this article, we present a novel distributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a specialised data structure. DistD2-TDS algorithm takes advantage of the diagonal dominance in tridiagonal systems to reduce the communications in distributed-memory environments. The underlying data structure plays a crucial role for the performance of the algorithm. First, the data structure improves data localities and makes it possible to minimise data movements via cache blocking and kernel fusion strategies. Second, data continuity enables a contiguous data access pattern and results in efficient utilisation of the available memory bandwidth. Finally, the data layout supports vectorisation on CPUs and thread level parallelisation on GPUs for improved performance. In order to demonstrate the robustness of the algorithm, we implemented and benchmarked the algorithm on CPUs and GPUs. We investigated the single rank performance and compared against existing algorithms. Furthermore, we analysed the strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to 8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the algorithm by using compact finite difference schemes to solve a 3D non-linear PDE. The results demonstrate that DistD2 algorithm can sustain around 66% of the theoretical peak bandwidth at scale on CPU and GPU based supercomputers.

**Link**: [arxiv](http://arxiv.org/abs/2411.13532v1),  [pdf](http://arxiv.org/pdf/2411.13532v1)

**Tags**: cs.DC physics.comp-ph 



### Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2   experiment
**Authors**: Marco Barbisan, Marco Boldrin, Luca Cinnirella, Bruno Laterza, Alberto Maistrello, Lionello Marrelli, Federico Molon, Simone Peruzzo, Cesare Taliercio, Marco Valisa, Enrico Zampiva

**Updated**: 2024-11-20T14:52:36Z

**Summary**: Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge Exchange Recombination Spectroscopy (CHERS) and Motional Stark effect diagnostics (MSE), are a well-known tool to access important information about magnetically confined plasmas, such as radial profiles of ion temperature, ion flow, impurity content and intensity and direction of the magnetic field. For this purpose, a DNBI was installed and operated in the RFX-mod experiment, which was designed to confine plasma mainly through the Reversed Field Pinch configuration. The DNBI, designed and built by the Budker Institute of Plasma Physics, was based on a source of positive hydrogen ions, accelerated to 50 keV and for an equivalent neutral beam current of about 5 A at the source. The beam could be modulated and the maximum overall duration was 50 ms. With the upgrade of RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to solve several plant faults and improve the overall reliability of the system. The 50 kV power supply is being improved, as well as the power supplies in the high voltage deck and its insulation transformer. The control system, originally based on CAMAC technology, was redesigned to be fully replaced. This contribution reviews the technical criticalities emerged in the DNBI check-up and the new solutions adopted to make the DNBI operative and more reliable.

**Link**: [arxiv](http://arxiv.org/abs/2411.13373v1),  [pdf](http://arxiv.org/pdf/2411.13373v1)

**Tags**: physics.plasm-ph 



### Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache   Consumption
**Authors**: Luohe Shi, Hongyi Zhang, Yao Yao, Zuchao Li, Hai Zhao

**Updated**: 2024-11-20T02:04:10Z

**Summary**: Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022, have revolutionized various industries with their advanced language comprehension. However, their efficiency is challenged by the Transformer architecture's struggle with handling long texts. KV Cache has emerged as a pivotal solution to this issue, converting the time complexity of token generation from quadratic to linear, albeit with increased GPU memory overhead proportional to conversation length. With the development of the LLM community and academia, various KV Cache compression methods have been proposed. In this review, we dissect the various properties of KV Cache and elaborate on various methods currently used to optimize the KV Cache space usage of LLMs. These methods span the pre-training phase, deployment phase, and inference phase, and we summarize the commonalities and differences among these methods. Additionally, we list some metrics for evaluating the long-text capabilities of large language models, from both efficiency and capability perspectives. Our review thus sheds light on the evolving landscape of LLM optimization, offering insights into future advancements in this dynamic field. Links to the papers mentioned in this review can be found in our Github Repo https://github.com/zcli-charlie/Awesome-KV-Cache.

**Link**: [arxiv](http://arxiv.org/abs/2407.18003v4),  [pdf](http://arxiv.org/pdf/2407.18003v4)

**Tags**: cs.CL 



### GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and   Retrieval
**Authors**: Dong Liu, Roger Waleffe, Meng Jiang, Shivaram Venkataraman

**Updated**: 2024-11-19T18:24:03Z

**Summary**: In our recent research, we have developed a framework called GraphSnapShot, which has been proven an useful tool for graph learning acceleration. GraphSnapShot is a framework for fast cache, storage, retrieval and computation for graph learning. It can quickly store and update the local topology of graph structure and allows us to track patterns in the structure of graph networks, just like take snapshots of the graphs. In experiments, GraphSnapShot shows efficiency, it can achieve up to 30% training acceleration and 73% memory reduction for lossless graph ML training compared to current baselines such as dgl.This technique is particular useful for large dynamic graph learning tasks such as social media analysis and recommendation systems to process complex relationships between entities.   The code for GraphSnapShot is publicly available at https://github.com/NoakLiu/GraphSnapShot.

**Link**: [arxiv](http://arxiv.org/abs/2406.17918v3),  [pdf](http://arxiv.org/pdf/2406.17918v3)

**Tags**: cs.LG cs.DC cs.SI 



### An Eulerian approach to regularized JKO scheme with low-rank tensor   decompositions for Bayesian inversion
**Authors**: Vitalii Aksenov, Martin Eigel

**Updated**: 2024-11-19T11:40:56Z

**Summary**: The possibility of using the Eulerian discretization for the problem of modelling high-dimensional distributions and sampling, is studied. The problem is posed as a minimization problem over the space of probability measures with respect to the Wasserstein distance and solved with entropy-regularized JKO scheme. Each proximal step can be formulated as a fixed-point equation and solved with accelerated methods, such as Anderson's. The usage of low-rank Tensor Train format allows to overcome the \emph{curse of dimensionality}, i.e. the exponential growth of degrees of freedom with dimension, inherent to Eulerian approaches. The resulting method requires only pointwise computations of the unnormalized posterior and is, in particular, gradient-free. Fixed Eulerian grid allows to employ a caching strategy, significally reducing the expensive evaluations of the posterior. When the Eulerian model of the target distribution is fitted, the passage back to the Lagrangian perspective can also be made, allowing to approximately sample from it. We test our method both for synthetic target distributions and particular Bayesian inverse problems and report comparable or better performance than the baseline Metropolis-Hastings MCMC with same amount of resources. Finally, the fitted model can be modified to facilitate the solution of certain associated problems, which we demonstrate by fitting an importance distribution for a particular quantity of interest.

**Link**: [arxiv](http://arxiv.org/abs/2411.12430v1),  [pdf](http://arxiv.org/pdf/2411.12430v1)

**Tags**: math.NA cs.NA math.OC 46E27, 49Q22, 62F15, 68W25 



### Adaptive Cache Management for Complex Storage Systems Using   CNN-LSTM-Based Spatiotemporal Prediction
**Authors**: Xiaoye Wang, Xuan Li, Linji Wang, Tingyi Ruan, Pochun Li

**Updated**: 2024-11-19T01:55:26Z

**Summary**: This paper proposes an intelligent cache management strategy based on CNN-LSTM to improve the performance and cache hit rate of storage systems. Through comparative experiments with traditional algorithms (such as LRU and LFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the results show that the CNN-LSTM model has significant advantages in cache demand prediction. The MSE and MAE values of this model are significantly reduced, proving its effectiveness under complex data access patterns. This study not only verifies the potential of deep learning technology in storage system optimization, but also provides direction and reference for further optimizing and improving cache management strategies. This intelligent cache management strategy performs well in complex storage environments. By combining the spatial feature extraction capabilities of convolutional neural networks and the time series modeling capabilities of long short-term memory networks, the CNN-LSTM model can more accurately predict cache needs, thereby Dynamically optimize cache allocation to improve system response speed and resource utilization. This research provides theoretical support and practical reference for cache optimization under large-scale data access modes, and is of great significance to improving the performance of future storage systems.

**Link**: [arxiv](http://arxiv.org/abs/2411.12161v1),  [pdf](http://arxiv.org/pdf/2411.12161v1)

**Tags**: cs.DC 



### Bi-Mamba: Towards Accurate 1-Bit State Space Models
**Authors**: Shengkun Tang, Liqun Ma, Haonan Li, Mingjie Sun, Zhiqiang Shen

**Updated**: 2024-11-18T18:59:15Z

**Summary**: The typical selective state-space model (SSM) of Mamba addresses several limitations of Transformers, such as quadratic computational complexity with sequence length and significant inference-time memory requirements due to the key-value cache. However, the growing size of Mamba models continues to pose training and deployment challenges and raises environmental concerns due to considerable energy consumption. In this work, we introduce Bi-Mamba, a scalable and powerful 1-bit Mamba architecture designed for more efficient large language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba models are trained from scratch on data volume as regular LLM pertaining using an autoregressive distillation loss. Extensive experimental results on language modeling demonstrate that Bi-Mamba achieves performance comparable to its full-precision counterparts (e.g., FP16 or BF16) and much better accuracy than post-training-binarization (PTB) Mamba baselines, while significantly reducing memory footprint and energy consumption compared to the original Mamba model. Our study pioneers a new linear computational complexity LLM framework under low-bit representation and facilitates the future design of specialized hardware tailored for efficient 1-bit Mamba-based LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2411.11843v1),  [pdf](http://arxiv.org/pdf/2411.11843v1)

**Tags**: cs.CL cs.AI 



### QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou
**Authors**: Xinchen Luo, Jiangxia Cao, Tianyu Sun, Jinkai Yu, Rui Huang, Wei Yuan, Hezheng Lin, Yichen Zheng, Shiyao Wang, Qigen Hu, Changqing Qiu, Jiaqi Zhang, Xu Zhang, Zhiheng Yan, Jingming Zhang, Simin Zhang, Mingxing Wen, Zhaojie Liu, Kun Gai, Guorui Zhou

**Updated**: 2024-11-18T17:08:35Z

**Summary**: In recent years, with the significant evolution of multi-modal large models, many recommender researchers realized the potential of multi-modal information for user interest modeling. In industry, a wide-used modeling architecture is a cascading paradigm: (1) first pre-training a multi-modal model to provide omnipotent representations for downstream services; (2) The downstream recommendation model takes the multi-modal representation as additional input to fit real user-item behaviours. Although such paradigm achieves remarkable improvements, however, there still exist two problems that limit model performance: (1) Representation Unmatching: The pre-trained multi-modal model is always supervised by the classic NLP/CV tasks, while the recommendation models are supervised by real user-item interaction. As a result, the two fundamentally different tasks' goals were relatively separate, and there was a lack of consistent objective on their representations; (2) Representation Unlearning: The generated multi-modal representations are always stored in cache store and serve as extra fixed input of recommendation model, thus could not be updated by recommendation model gradient, further unfriendly for downstream training. Inspired by the two difficulties challenges in downstream tasks usage, we introduce a quantitative multi-modal framework to customize the specialized and trainable multi-modal information for different downstream models.

**Link**: [arxiv](http://arxiv.org/abs/2411.11739v1),  [pdf](http://arxiv.org/pdf/2411.11739v1)

**Tags**: cs.IR cs.AI N/A 



## Keyword: LLM Inference 
 ### From an Image to a Scene: Learning to Imagine the World from a Million   360 Videos
**Authors**: Matthew Wallingford, Anand Bhattad, Aditya Kusupati, Vivek Ramanujan, Matt Deitke, Sham Kakade, Aniruddha Kembhavi, Roozbeh Mottaghi, Wei-Chiu Ma, Ali Farhadi

**Updated**: 2024-12-10T18:59:44Z

**Summary**: Three-dimensional (3D) understanding of objects and scenes play a key role in humans' ability to interact with the world and has been an active area of research in computer vision, graphics, and robotics. Large scale synthetic and object-centric 3D datasets have shown to be effective in training models that have 3D understanding of objects. However, applying a similar approach to real-world objects and scenes is difficult due to a lack of large-scale data. Videos are a potential source for real-world 3D data, but finding diverse yet corresponding views of the same content has shown to be difficult at scale. Furthermore, standard videos come with fixed viewpoints, determined at the time of capture. This restricts the ability to access scenes from a variety of more diverse and potentially useful perspectives. We argue that large scale 360 videos can address these limitations to provide: scalable corresponding frames from diverse views. In this paper, we introduce 360-1M, a 360 video dataset, and a process for efficiently finding corresponding frames from diverse viewpoints at scale. We train our diffusion-based model, Odin, on 360-1M. Empowered by the largest real-world, multi-view dataset to date, Odin is able to freely generate novel views of real-world scenes. Unlike previous methods, Odin can move the camera through the environment, enabling the model to infer the geometry and layout of the scene. Additionally, we show improved performance on standard novel view synthesis and 3D reconstruction benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2412.07770v1),  [pdf](http://arxiv.org/pdf/2412.07770v1)

**Tags**: cs.CV cs.LG 



### BiMediX2: Bio-Medical EXpert LMM for Diverse Medical Modalities
**Authors**: Sahal Shaji Mullappilly, Mohammed Irfan Kurpath, Sara Pieri, Saeed Yahya Alseiari, Shanavas Cholakkal, Khaled Aldahmani, Fahad Khan, Rao Anwer, Salman Khan, Timothy Baldwin, Hisham Cholakkal

**Updated**: 2024-12-10T18:59:35Z

**Summary**: This paper introduces BiMediX2, a bilingual (Arabic-English) Bio-Medical EXpert Large Multimodal Model (LMM) with a unified architecture that integrates text and visual modalities, enabling advanced image understanding and medical applications. BiMediX2 leverages the Llama3.1 architecture and integrates text and visual capabilities to facilitate seamless interactions in both English and Arabic, supporting text-based inputs and multi-turn conversations involving medical images. The model is trained on an extensive bilingual healthcare dataset consisting of 1.6M samples of diverse medical interactions for both text and image modalities, mixed in Arabic and English. We also propose the first bilingual GPT-4o based medical LMM benchmark named BiMed-MBench. BiMediX2 is benchmarked on both text-based and image-based tasks, achieving state-of-the-art performance across several medical benchmarks. It outperforms recent state-of-the-art models in medical LLM evaluation benchmarks. Our model also sets a new benchmark in multimodal medical evaluations with over 9% improvement in English and over 20% in Arabic evaluations. Additionally, it surpasses GPT-4 by around 9% in UPHILL factual accuracy evaluations and excels in various medical Visual Question Answering, Report Generation, and Report Summarization tasks. The project page including source code and the trained model, is available at https://github.com/mbzuai-oryx/BiMediX2.

**Link**: [arxiv](http://arxiv.org/abs/2412.07769v1),  [pdf](http://arxiv.org/pdf/2412.07769v1)

**Tags**: cs.CV 



### Test-time Correction with Human Feedback: An Online 3D Detection System   via Visual Prompting
**Authors**: Zetong Yang, Hanxue Zhang, Yanan Sun, Li Chen, Fei Xia, Fatma Guney, Hongyang Li

**Updated**: 2024-12-10T18:59:32Z

**Summary**: This paper introduces Test-time Correction (TTC) system, a novel online 3D detection system designated for online correction of test-time errors via human feedback, to guarantee the safety of deployed autonomous driving systems. Unlike well-studied offline 3D detectors frozen at inference, TTC explores the capability of instant online error rectification. By leveraging user feedback with interactive prompts at a frame, e.g., a simple click or draw of boxes, TTC could immediately update the corresponding detection results for future streaming inputs, even though the model is deployed with fixed parameters. This enables autonomous driving systems to adapt to new scenarios immediately and decrease deployment risks reliably without additional expensive training. To achieve such TTC system, we equip existing 3D detectors with Online Adapter (OA) module, a prompt-driven query generator for online correction. At the core of OA module are visual prompts, images of missed object-of-interest for guiding the corresponding detection and subsequent tracking. Those visual prompts, belonging to missed objects through online inference, are maintained by the visual prompt buffer for continuous error correction in subsequent frames. By doing so, TTC consistently detects online missed objects and immediately lowers driving risks. It achieves reliable, versatile, and adaptive driving autonomy. Extensive experiments demonstrate significant gain on instant error rectification over pre-trained 3D detectors, even in challenging scenarios with limited labels, zero-shot detection, and adverse conditions. We hope this work would inspire the community to investigate online rectification systems for autonomous driving post-deployment. Code would be publicly shared.

**Link**: [arxiv](http://arxiv.org/abs/2412.07768v1),  [pdf](http://arxiv.org/pdf/2412.07768v1)

**Tags**: cs.CV 



### 3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video   Generation
**Authors**: Xiao Fu, Xian Liu, Xintao Wang, Sida Peng, Menghan Xia, Xiaoyu Shi, Ziyang Yuan, Pengfei Wan, Di Zhang, Dahua Lin

**Updated**: 2024-12-10T18:55:13Z

**Summary**: This paper aims to manipulate multi-entity 3D motions in video generation. Previous methods on controllable video generation primarily leverage 2D control signals to manipulate object motions and have achieved remarkable synthesis results. However, 2D control signals are inherently limited in expressing the 3D nature of object motions. To overcome this problem, we introduce 3DTrajMaster, a robust controller that regulates multi-entity dynamics in 3D space, given user-desired 6DoF pose (location and rotation) sequences of entities. At the core of our approach is a plug-and-play 3D-motion grounded object injector that fuses multiple input entities with their respective 3D trajectories through a gated self-attention mechanism. In addition, we exploit an injector architecture to preserve the video diffusion prior, which is crucial for generalization ability. To mitigate video quality degradation, we introduce a domain adaptor during training and employ an annealed sampling strategy during inference. To address the lack of suitable training data, we construct a 360-Motion Dataset, which first correlates collected 3D human and animal assets with GPT-generated trajectory and then captures their motion with 12 evenly-surround cameras on diverse 3D UE platforms. Extensive experiments show that 3DTrajMaster sets a new state-of-the-art in both accuracy and generalization for controlling multi-entity 3D motions. Project page: http://fuxiao0719.github.io/projects/3dtrajmaster

**Link**: [arxiv](http://arxiv.org/abs/2412.07759v1),  [pdf](http://arxiv.org/pdf/2412.07759v1)

**Tags**: cs.CV 



### Beyond Retrieval: Generating Narratives in Conversational Recommender   Systems
**Authors**: Krishna Sayana, Raghavendra Vasudeva, Yuri Vasilevski, Kun Su, Liam Hebert, James Pine, Hubert Pham, Ambarish Jash, Sukhdeep Sodhi

**Updated**: 2024-12-10T18:45:18Z

**Summary**: The recent advances in Large Language Model's generation and reasoning capabilities present an opportunity to develop truly conversational recommendation systems. However, effectively integrating recommender system knowledge into LLMs for natural language generation which is tailored towards recommendation tasks remains a challenge. This paper addresses this challenge by making two key contributions.   First, we introduce a new dataset (REGEN) for natural language generation tasks in conversational recommendations. REGEN (Reviews Enhanced with GEnerative Narratives) extends the Amazon Product Reviews dataset with rich user narratives, including personalized explanations of product preferences, product endorsements for recommended items, and summaries of user purchase history. REGEN is made publicly available to facilitate further research. Furthermore, we establish benchmarks using well-known generative metrics, and perform an automated evaluation of the new dataset using a rater LLM. Second, the paper introduces a fusion architecture (CF model with an LLM) which serves as a baseline for REGEN. And to the best of our knowledge, represents the first attempt to analyze the capabilities of LLMs in understanding recommender signals and generating rich narratives. We demonstrate that LLMs can effectively learn from simple fusion architectures utilizing interaction-based CF embeddings, and this can be further enhanced using the metadata and personalization data associated with items. Our experiments show that combining CF and content embeddings leads to improvements of 4-12% in key language metrics compared to using either type of embedding individually. We also provide an analysis to interpret how CF and content embeddings contribute to this new generative task.

**Link**: [arxiv](http://arxiv.org/abs/2410.16780v2),  [pdf](http://arxiv.org/pdf/2410.16780v2)

**Tags**: cs.CL cs.AI cs.IR cs.LG 



### Zero-Shot ATC Coding with Large Language Models for Clinical Assessments
**Authors**: Zijian Chen, John-Michael Gamble, Micaela Jantzi, John P. Hirdes, Jimmy Lin

**Updated**: 2024-12-10T18:43:02Z

**Summary**: Manual assignment of Anatomical Therapeutic Chemical (ATC) codes to prescription records is a significant bottleneck in healthcare research and operations at Ontario Health and InterRAI Canada, requiring extensive expert time and effort. To automate this process while maintaining data privacy, we develop a practical approach using locally deployable large language models (LLMs). Inspired by recent advances in automatic International Classification of Diseases (ICD) coding, our method frames ATC coding as a hierarchical information extraction task, guiding LLMs through the ATC ontology level by level. We evaluate our approach using GPT-4o as an accuracy ceiling and focus development on open-source Llama models suitable for privacy-sensitive deployment. Testing across Health Canada drug product data, the RABBITS benchmark, and real clinical notes from Ontario Health, our method achieves 78% exact match accuracy with GPT-4o and 60% with Llama 3.1 70B. We investigate knowledge grounding through drug definitions, finding modest improvements in accuracy. Further, we show that fine-tuned Llama 3.1 8B matches zero-shot Llama 3.1 70B accuracy, suggesting that effective ATC coding is feasible with smaller models. Our results demonstrate the feasibility of automatic ATC coding in privacy-sensitive healthcare environments, providing a foundation for future deployments.

**Link**: [arxiv](http://arxiv.org/abs/2412.07743v1),  [pdf](http://arxiv.org/pdf/2412.07743v1)

**Tags**: cs.CL 



### M3TR: Generalist HD Map Construction with Variable Map Priors
**Authors**: Fabian Immel, Richard Fehler, Frank Bieder, Jan-Hendrik Pauls, Christoph Stiller

**Updated**: 2024-12-10T18:41:14Z

**Summary**: Autonomous vehicles require road information for their operation, usually in form of HD maps. Since offline maps eventually become outdated or may only be partially available, online HD map construction methods have been proposed to infer map information from live sensor data. A key issue remains how to exploit such partial or outdated map information as a prior. We introduce M3TR (Multi-Masking Map Transformer), a generalist approach for HD map construction both with and without map priors. We address shortcomings in ground truth generation for Argoverse 2 and nuScenes and propose the first realistic scenarios with semantically diverse map priors. Examining various query designs, we use an improved method for integrating prior map elements into a HD map construction model, increasing performance by +4.3 mAP. Finally, we show that training across all prior scenarios yields a single Generalist model, whose performance is on par with previous Expert models that can handle only one specific type of map prior. M3TR thus is the first model capable of leveraging variable map priors, making it suitable for real-world deployment. Code is available at https://github.com/immel-f/m3tr

**Link**: [arxiv](http://arxiv.org/abs/2411.10316v2),  [pdf](http://arxiv.org/pdf/2411.10316v2)

**Tags**: cs.CV cs.RO 



### GASP: Gaussian Avatars with Synthetic Priors
**Authors**: Jack Saunders, Charlie Hewitt, Yanan Jian, Marek Kowalski, Tadas Baltrusaitis, Yiye Chen, Darren Cosker, Virginia Estellers, Nicholas Gyde, Vinay P. Namboodiri, Benjamin E Lundell

**Updated**: 2024-12-10T18:36:21Z

**Summary**: Gaussian Splatting has changed the game for real-time photo-realistic rendering. One of the most popular applications of Gaussian Splatting is to create animatable avatars, known as Gaussian Avatars. Recent works have pushed the boundaries of quality and rendering efficiency but suffer from two main limitations. Either they require expensive multi-camera rigs to produce avatars with free-view rendering, or they can be trained with a single camera but only rendered at high quality from this fixed viewpoint. An ideal model would be trained using a short monocular video or image from available hardware, such as a webcam, and rendered from any view. To this end, we propose GASP: Gaussian Avatars with Synthetic Priors. To overcome the limitations of existing datasets, we exploit the pixel-perfect nature of synthetic data to train a Gaussian Avatar prior. By fitting this prior model to a single photo or video and fine-tuning it, we get a high-quality Gaussian Avatar, which supports 360$^\circ$ rendering. Our prior is only required for fitting, not inference, enabling real-time application. Through our method, we obtain high-quality, animatable Avatars from limited data which can be animated and rendered at 70fps on commercial hardware. See our project page (https://microsoft.github.io/GASP/) for results.

**Link**: [arxiv](http://arxiv.org/abs/2412.07739v1),  [pdf](http://arxiv.org/pdf/2412.07739v1)

**Tags**: cs.CV cs.AI cs.GR 



### The BrowserGym Ecosystem for Web Agent Research
**Authors**: Thibault Le Sellier De Chezelles, Maxime Gasse, Alexandre Drouin, Massimo Caccia, Léo Boisvert, Megh Thakkar, Tom Marty, Rim Assouel, Sahar Omidi Shayegan, Lawrence Keunho Jang, Xing Han Lù, Ori Yoran, Dehan Kong, Frank F. Xu, Siva Reddy, Quentin Cappart, Graham Neubig, Ruslan Salakhutdinov, Nicolas Chapados, Alexandre Lacoste

**Updated**: 2024-12-10T18:28:46Z

**Summary**: The BrowserGym ecosystem addresses the growing need for efficient evaluation and benchmarking of web agents, particularly those leveraging automation and Large Language Models (LLMs) for web interaction tasks. Many existing benchmarks suffer from fragmentation and inconsistent evaluation methodologies, making it challenging to achieve reliable comparisons and reproducible results. BrowserGym aims to solve this by providing a unified, gym-like environment with well-defined observation and action spaces, facilitating standardized evaluation across diverse benchmarks. Combined with AgentLab, a complementary framework that aids in agent creation, testing, and analysis, BrowserGym offers flexibility for integrating new benchmarks while ensuring consistent evaluation and comprehensive experiment management. This standardized approach seeks to reduce the time and complexity of developing web agents, supporting more reliable comparisons and facilitating in-depth analysis of agent behaviors, and could result in more adaptable, capable agents, ultimately accelerating innovation in LLM-driven automation. As a supporting evidence, we conduct the first large-scale, multi-benchmark web agent experiment and compare the performance of 6 state-of-the-art LLMs across all benchmarks currently available in BrowserGym. Among other findings, our results highlight a large discrepancy between OpenAI and Anthropic's latests models, with Claude-3.5-Sonnet leading the way on almost all benchmarks, except on vision-related tasks where GPT-4o is superior. Despite these advancements, our results emphasize that building robust and efficient web agents remains a significant challenge, due to the inherent complexity of real-world web environments and the limitations of current models.

**Link**: [arxiv](http://arxiv.org/abs/2412.05467v2),  [pdf](http://arxiv.org/pdf/2412.05467v2)

**Tags**: cs.LG cs.AI cs.SE 



### MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion
**Authors**: Sai Shashank Kalakonda, Shubh Maheshwari, Ravi Kiran Sarvadevabhatla

**Updated**: 2024-12-10T18:24:13Z

**Summary**: We introduce MoRAG, a novel multi-part fusion based retrieval-augmented generation strategy for text-based human motion generation. The method enhances motion diffusion models by leveraging additional knowledge obtained through an improved motion retrieval process. By effectively prompting large language models (LLMs), we address spelling errors and rephrasing issues in motion retrieval. Our approach utilizes a multi-part retrieval strategy to improve the generalizability of motion retrieval across the language space. We create diverse samples through the spatial composition of the retrieved motions. Furthermore, by utilizing low-level, part-specific motion information, we can construct motion samples for unseen text descriptions. Our experiments demonstrate that our framework can serve as a plug-and-play module, improving the performance of motion diffusion models. Code, pretrained models and sample videos are available at: https://motion-rag.github.io/

**Link**: [arxiv](http://arxiv.org/abs/2409.12140v2),  [pdf](http://arxiv.org/pdf/2409.12140v2)

**Tags**: cs.CV cs.MM 



### Toward Self-Improvement of LLMs via Imagination, Searching, and   Criticizing
**Authors**: Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, Dong Yu

**Updated**: 2024-12-10T18:19:29Z

**Summary**: Despite the impressive capabilities of Large Language Models (LLMs) on various tasks, they still struggle with scenarios that involves complex reasoning and planning. Recent work proposed advanced prompting techniques and the necessity of fine-tuning with high-quality data to augment LLMs' reasoning abilities. However, these approaches are inherently constrained by data availability and quality. In light of this, self-correction and self-learning emerge as viable solutions, employing strategies that allow LLMs to refine their outputs and learn from self-assessed rewards. Yet, the efficacy of LLMs in self-refining its response, particularly in complex reasoning and planning task, remains dubious. In this paper, we introduce AlphaLLM for the self-improvements of LLMs, which integrates Monte Carlo Tree Search (MCTS) with LLMs to establish a self-improving loop, thereby enhancing the capabilities of LLMs without additional annotations. Drawing inspiration from the success of AlphaGo, AlphaLLM addresses the unique challenges of combining MCTS with LLM for self-improvement, including data scarcity, the vastness search spaces of language tasks, and the subjective nature of feedback in language tasks. AlphaLLM is comprised of prompt synthesis component, an efficient MCTS approach tailored for language tasks, and a trio of critic models for precise feedback. Our experimental results in mathematical reasoning tasks demonstrate that AlphaLLM significantly enhances the performance of LLMs without additional annotations, showing the potential for self-improvement in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2404.12253v2),  [pdf](http://arxiv.org/pdf/2404.12253v2)

**Tags**: cs.CL cs.LG 



### Granite Guardian
**Authors**: Inkit Padhi, Manish Nagireddy, Giandomenico Cornacchia, Subhajit Chaudhury, Tejaswini Pedapati, Pierre Dognin, Keerthiram Murugesan, Erik Miehling, Martín Santillán Cooper, Kieran Fraser, Giulio Zizzo, Muhammad Zaid Hameed, Mark Purcell, Michael Desmond, Qian Pan, Inge Vejsbjerg, Elizabeth M. Daly, Michael Hind, Werner Geyer, Ambrish Rawat, Kush R. Varshney, Prasanna Sattigeri

**Updated**: 2024-12-10T18:17:02Z

**Summary**: We introduce the Granite Guardian models, a suite of safeguards designed to provide risk detection for prompts and responses, enabling safe and responsible use in combination with any large language model (LLM). These models offer comprehensive coverage across multiple risk dimensions, including social bias, profanity, violence, sexual content, unethical behavior, jailbreaking, and hallucination-related risks such as context relevance, groundedness, and answer relevance for retrieval-augmented generation (RAG). Trained on a unique dataset combining human annotations from diverse sources and synthetic data, Granite Guardian models address risks typically overlooked by traditional risk detection models, such as jailbreaks and RAG-specific issues. With AUC scores of 0.871 and 0.854 on harmful content and RAG-hallucination-related benchmarks respectively, Granite Guardian is the most generalizable and competitive model available in the space. Released as open-source, Granite Guardian aims to promote responsible AI development across the community.   https://github.com/ibm-granite/granite-guardian

**Link**: [arxiv](http://arxiv.org/abs/2412.07724v1),  [pdf](http://arxiv.org/pdf/2412.07724v1)

**Tags**: cs.CL 



### ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion   Transformer
**Authors**: Jinyi Hu, Shengding Hu, Yuxuan Song, Yufei Huang, Mingxuan Wang, Hao Zhou, Zhiyuan Liu, Wei-Ying Ma, Maosong Sun

**Updated**: 2024-12-10T18:13:20Z

**Summary**: The recent surge of interest in comprehensive multimodal models has necessitated the unification of diverse modalities. However, the unification suffers from disparate methodologies. Continuous visual generation necessitates the full-sequence diffusion-based approach, despite its divergence from the autoregressive modeling in the text domain. We posit that autoregressive modeling, i.e., predicting the future based on past deterministic experience, remains crucial in developing both a visual generation model and a potential unified multimodal model. In this paper, we explore an interpolation between the autoregressive modeling and full-parameters diffusion to model visual information. At its core, we present ACDiT, an Autoregressive blockwise Conditional Diffusion Transformer, where the block size of diffusion, i.e., the size of autoregressive units, can be flexibly adjusted to interpolate between token-wise autoregression and full-sequence diffusion. ACDiT is easy to implement, as simple as creating a Skip-Causal Attention Mask (SCAM) during training. During inference, the process iterates between diffusion denoising and autoregressive decoding that can make full use of KV-Cache. We verify the effectiveness of ACDiT on image and video generation tasks. We also demonstrate that benefitted from autoregressive modeling, ACDiT can be seamlessly used in visual understanding tasks despite being trained on the diffusion objective. The analysis of the trade-off between autoregressive modeling and diffusion demonstrates the potential of ACDiT to be used in long-horizon visual generation tasks. These strengths make it promising as the backbone of future unified models.

**Link**: [arxiv](http://arxiv.org/abs/2412.07720v1),  [pdf](http://arxiv.org/pdf/2412.07720v1)

**Tags**: cs.CV 



### Scalable Influence and Fact Tracing for Large Language Model Pretraining
**Authors**: Tyler A. Chang, Dheeraj Rajagopal, Tolga Bolukbasi, Lucas Dixon, Ian Tenney

**Updated**: 2024-12-10T17:59:38Z

**Summary**: Training data attribution (TDA) methods aim to attribute model outputs back to specific training examples, and the application of these methods to large language model (LLM) outputs could significantly advance model transparency and data curation. However, it has been challenging to date to apply these methods to the full scale of LLM pretraining. In this paper, we refine existing gradient-based methods to work effectively at scale, allowing us to retrieve influential examples for an 8B-parameter language model from a pretraining corpus of over 160B tokens with no need for subsampling or pre-filtering. Our method combines several techniques, including optimizer state correction, a task-specific Hessian approximation, and normalized encodings, which we find to be critical for performance at scale. In quantitative evaluations on a fact tracing task, our method performs best at identifying examples that influence model predictions, but classical, model-agnostic retrieval methods such as BM25 still perform better at finding passages which explicitly contain relevant facts. These results demonstrate a misalignment between factual *attribution* and causal *influence*. With increasing model size and training tokens, we find that influence more closely aligns with factual attribution. Finally, we examine different types of examples identified as influential by our method, finding that while many directly entail a particular fact, others support the same output by reinforcing priors on relation types, common entities, and names. We release our prompt set and model outputs, along with a web-based visualization tool to explore influential examples for factual predictions, commonsense reasoning, arithmetic, and open-ended generation for an 8B-parameter LLM.

**Link**: [arxiv](http://arxiv.org/abs/2410.17413v2),  [pdf](http://arxiv.org/pdf/2410.17413v2)

**Tags**: cs.CL 



### Fluctuation Theorems for Heat exchanges between passive and active baths
**Authors**: Massimiliano Semeraro, Antonio Suma, Giuseppe Negro

**Updated**: 2024-12-10T17:53:01Z

**Summary**: In addition to providing general constraints on probability distributions, fluctuation theorems allow to infer essential information on the role played by temperature in heat exchange phenomena. In this numerical study, we measure the temperature of an out of equilibrium active bath using a fluctuation theorem that relates the fluctuations of the heat exchanged between two baths to their temperatures. Our setup consists of a single particle moving between two wells of a quartic potential accommodating two different baths. The heat exchanged between the two baths is monitored according to two definitions: as the kinetic energy carried by the particle whenever it jumps from one well to the other and as the work performed by the particle on one of the two baths when immersed in it. First, we consider two equilibrium baths at two different temperatures and verify that a fluctuation theorem featuring the baths temperatures holds for both heat definitions. Then, we introduce an additional Gaussian coloured noise in one of the baths, so as to make it effectively an active (out-of-equilibrium) bath. We find that a fluctuation theorem is still satisfied with both heat definitions. Interestingly, in this case the temperature obtained through the fluctuation theorem for the active bath corresponds to the kinetic temperature when considering the first heat definition, while it is larger with the second one. We interpret these results by looking at the particle jump phenomenology.

**Link**: [arxiv](http://arxiv.org/abs/2412.07706v1),  [pdf](http://arxiv.org/pdf/2412.07706v1)

**Tags**: cond-mat.stat-mech 



### An Entailment Tree Generation Approach for Multimodal Multi-Hop Question   Answering with Mixture-of-Experts and Iterative Feedback Mechanism
**Authors**: Qing Zhang, Haocheng Lv, Jie Liu, Zhiyun Chen, Jianyong Duan, Hao Wang, Li He, Mingying Xv

**Updated**: 2024-12-10T17:42:49Z

**Summary**: With the rise of large-scale language models (LLMs), it is currently popular and effective to convert multimodal information into text descriptions for multimodal multi-hop question answering. However, we argue that the current methods of multi-modal multi-hop question answering still mainly face two challenges: 1) The retrieved evidence containing a large amount of redundant information, inevitably leads to a significant drop in performance due to irrelevant information misleading the prediction. 2) The reasoning process without interpretable reasoning steps makes the model difficult to discover the logical errors for handling complex questions. To solve these problems, we propose a unified LLMs-based approach but without heavily relying on them due to the LLM's potential errors, and innovatively treat multimodal multi-hop question answering as a joint entailment tree generation and question answering problem. Specifically, we design a multi-task learning framework with a focus on facilitating common knowledge sharing across interpretability and prediction tasks while preventing task-specific errors from interfering with each other via mixture of experts. Afterward, we design an iterative feedback mechanism to further enhance both tasks by feeding back the results of the joint training to the LLM for regenerating entailment trees, aiming to iteratively refine the potential answer. Notably, our method has won the first place in the official leaderboard of WebQA (since April 10, 2024), and achieves competitive results on MultimodalQA.

**Link**: [arxiv](http://arxiv.org/abs/2412.05821v2),  [pdf](http://arxiv.org/pdf/2412.05821v2)

**Tags**: cs.CL cs.AI 



### Toon3D: Seeing Cartoons from New Perspectives
**Authors**: Ethan Weber, Riley Peterlinz, Rohan Mathur, Frederik Warburg, Alexei A. Efros, Angjoo Kanazawa

**Updated**: 2024-12-10T17:23:09Z

**Summary**: We recover the underlying 3D structure from images of cartoons and anime depicting the same scene. This is an interesting problem domain because images in creative media are often depicted without explicit geometric consistency for storytelling and creative expression-they are only 3D in a qualitative sense. While humans can easily perceive the underlying 3D scene from these images, existing Structure-from-Motion (SfM) methods that assume 3D consistency fail catastrophically. We present Toon3D for reconstructing geometrically inconsistent images. Our key insight is to deform the input images while recovering camera poses and scene geometry, effectively explaining away geometrical inconsistencies to achieve consistency. This process is guided by the structure inferred from monocular depth predictions. We curate a dataset with multi-view imagery from cartoons and anime that we annotate with reliable sparse correspondences using our user-friendly annotation tool. Our recovered point clouds can be plugged into novel-view synthesis methods to experience cartoons from viewpoints never drawn before. We evaluate against classical and recent learning-based SfM methods, where Toon3D is able to obtain more reliable camera poses and scene geometry.

**Link**: [arxiv](http://arxiv.org/abs/2405.10320v3),  [pdf](http://arxiv.org/pdf/2405.10320v3)

**Tags**: cs.CV 



### Privacy-Preserving Customer Support: A Framework for Secure and Scalable   Interactions
**Authors**: Anant Prakash Awasthi, Chandraketu Singh, Rakshit Varma, Sanchit Sharma

**Updated**: 2024-12-10T17:20:47Z

**Summary**: The growing reliance on artificial intelligence (AI) in customer support has significantly improved operational efficiency and user experience. However, traditional machine learning (ML) approaches, which require extensive local training on sensitive datasets, pose substantial privacy risks and compliance challenges with regulations like the General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA). Existing privacy-preserving techniques, such as anonymization, differential privacy, and federated learning, address some concerns but face limitations in utility, scalability, and complexity. This paper introduces the Privacy-Preserving Zero-Shot Learning (PP-ZSL) framework, a novel approach leveraging large language models (LLMs) in a zero-shot learning mode. Unlike conventional ML methods, PP-ZSL eliminates the need for local training on sensitive data by utilizing pre-trained LLMs to generate responses directly. The framework incorporates real-time data anonymization to redact or mask sensitive information, retrieval-augmented generation (RAG) for domain-specific query resolution, and robust post-processing to ensure compliance with regulatory standards. This combination reduces privacy risks, simplifies compliance, and enhances scalability and operational efficiency. Empirical analysis demonstrates that the PP-ZSL framework provides accurate, privacy-compliant responses while significantly lowering the costs and complexities of deploying AI-driven customer support systems. The study highlights potential applications across industries, including financial services, healthcare, e-commerce, legal support, telecommunications, and government services. By addressing the dual challenges of privacy and performance, this framework establishes a foundation for secure, efficient, and regulatory-compliant AI applications in customer interactions.

**Link**: [arxiv](http://arxiv.org/abs/2412.07687v1),  [pdf](http://arxiv.org/pdf/2412.07687v1)

**Tags**: cs.LG cs.CR stat.AP 



### TRIM: Token Reduction and Inference Modeling for Cost-Effective Language   Generation
**Authors**: Alfredo Garrachón Ruiz, Tomás de la Rosa, Daniel Borrajo

**Updated**: 2024-12-10T17:13:35Z

**Summary**: The inference cost of Large Language Models (LLMs) is a significant challenge due to their computational demands, specially on tasks requiring long outputs. However, natural language often contains redundancy, which presents an opportunity for optimization. We have observed that LLMs can generate distilled language-concise outputs that retain essential meaning, when prompted appropriately. We propose a framework for saving computational cost, in which a shorter distilled output from the LLM is reconstructed into a full narrative by a smaller model with lower inference costs. Our experiments show promising results, particularly in general knowledge domains with 20.58% saved tokens on average with tiny decrease in evaluation metrics, hinting that this approach can effectively balance efficiency and accuracy in language processing tasks.

**Link**: [arxiv](http://arxiv.org/abs/2412.07682v1),  [pdf](http://arxiv.org/pdf/2412.07682v1)

**Tags**: cs.CL 



### RADIO Amplified: Improved Baselines for Agglomerative Vision Foundation   Models
**Authors**: Greg Heinrich, Mike Ranzinger, Hongxu, Yin, Yao Lu, Jan Kautz, Andrew Tao, Bryan Catanzaro, Pavlo Molchanov

**Updated**: 2024-12-10T17:06:41Z

**Summary**: Agglomerative models have recently emerged as a powerful approach to training vision foundation models, leveraging multi-teacher distillation from existing models such as CLIP, DINO, and SAM. This strategy enables the efficient creation of robust models, combining the strengths of individual teachers while significantly reducing computational and resource demands. In this paper, we thoroughly analyze state-of-the-art agglomerative models, identifying critical challenges including resolution mode shifts, teacher imbalance, idiosyncratic teacher artifacts, and an excessive number of output tokens. To address these issues, we propose several novel solutions: multi-resolution training, mosaic augmentation, and improved balancing of teacher loss functions. Specifically, in the context of Vision Language Models, we introduce a token compression technique to maintain high-resolution information within a fixed token count. We release our top-performing models, available in multiple scales (-B, -L, -H, and -g), alongside inference code and pretrained weights.

**Link**: [arxiv](http://arxiv.org/abs/2412.07679v1),  [pdf](http://arxiv.org/pdf/2412.07679v1)

**Tags**: cs.CV cs.AI 



### RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text   Rewriting
**Authors**: Shuo Yang, Bardh Prenkaj, Gjergji Kasneci

**Updated**: 2024-12-10T17:02:58Z

**Summary**: Despite the widespread use of LLMs due to their superior performance in various tasks, their high computational costs often lead potential users to opt for the pretraining-finetuning pipeline. However, biases prevalent in manually constructed datasets can introduce spurious correlations between tokens and labels, creating so-called shortcuts and hindering the generalizability of fine-tuned models. Existing debiasing methods often rely on prior knowledge of specific dataset biases, which is challenging to acquire a priori. We propose RAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised, and data-focused debiasing approach based on text rewriting for shortcut mitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased text segments by replacing them with heuristically selected alternatives in a shortcut space defined by token statistics and positional information. This process aims to align surface-level text features more closely with diverse label distributions, thereby promoting the learning of genuine linguistic patterns. Compared with unsupervised SoTA models, RAZOR improves by 3.5% on the FEVER and 6.5% on MNLI and SNLI datasets according to the F1 score. Additionally, RAZOR effectively mitigates specific known biases, reducing bias-related terms by x2 without requiring prior bias information, a result that is on par with SoTA models that leverage prior information. Our work prioritizes data manipulation over architectural modifications, emphasizing the pivotal role of data quality in enhancing model performance and fairness. This research contributes to developing more robust evaluation benchmarks for debiasing methods by incorporating metrics for bias reduction and overall model efficacy.

**Link**: [arxiv](http://arxiv.org/abs/2412.07675v1),  [pdf](http://arxiv.org/pdf/2412.07675v1)

**Tags**: cs.CL cs.LG 



### Ask Humans or AI? Exploring Their Roles in Visualization Troubleshooting
**Authors**: Shuyu Shen, Sirong Lu, Leixian Shen, Zhonghua Sheng, Nan Tang, Yuyu Luo

**Updated**: 2024-12-10T17:02:41Z

**Summary**: Visualization authoring is an iterative process requiring users to modify parameters like color schemes and data transformations to achieve desired aesthetics and effectively convey insights. Due to the complexity of these adjustments, users often create defective visualizations and require troubleshooting support. In this paper, we examine two primary approaches for visualization troubleshooting: (1) Human-assisted support via forums, where users receive advice from other individuals, and (2) AI-assisted support using large language models (LLMs). Our goal is to understand the strengths and limitations of each approach in supporting visualization troubleshooting tasks. To this end, we collected 889 Vega-Lite cases from Stack Overflow. We then conducted a comprehensive analysis to understand the types of questions users ask, the effectiveness of human and AI guidance, and the impact of supplementary resources, such as documentation and examples, on troubleshooting outcomes. Our findings reveal a striking contrast between human- and AI-assisted troubleshooting: Human-assisted troubleshooting provides tailored, context-sensitive advice but often varies in response quality, while AI-assisted troubleshooting offers rapid feedback but often requires additional contextual resources to achieve desired results.

**Link**: [arxiv](http://arxiv.org/abs/2412.07673v1),  [pdf](http://arxiv.org/pdf/2412.07673v1)

**Tags**: cs.HC 



### FlexLLM: Exploring LLM Customization for Moving Target Defense on   Black-Box LLMs Against Jailbreak Attacks
**Authors**: Bocheng Chen, Hanqing Guo, Qiben Yan

**Updated**: 2024-12-10T17:02:28Z

**Summary**: Defense in large language models (LLMs) is crucial to counter the numerous attackers exploiting these systems to generate harmful content through manipulated prompts, known as jailbreak attacks. Although many defense strategies have been proposed, they often require access to the model's internal structure or need additional training, which is impractical for service providers using LLM APIs, such as OpenAI APIs or Claude APIs. In this paper, we propose a moving target defense approach that alters decoding hyperparameters to enhance model robustness against various jailbreak attacks. Our approach does not require access to the model's internal structure and incurs no additional training costs. The proposed defense includes two key components: (1) optimizing the decoding strategy by identifying and adjusting decoding hyperparameters that influence token generation probabilities, and (2) transforming the decoding hyperparameters and model system prompts into dynamic targets, which are continuously altered during each runtime. By continuously modifying decoding strategies and prompts, the defense effectively mitigates the existing attacks. Our results demonstrate that our defense is the most effective against jailbreak attacks in three of the models tested when using LLMs as black-box APIs. Moreover, our defense offers lower inference costs and maintains comparable response quality, making it a potential layer of protection when used alongside other defense methods.

**Link**: [arxiv](http://arxiv.org/abs/2412.07672v1),  [pdf](http://arxiv.org/pdf/2412.07672v1)

**Tags**: cs.CR cs.CL 



### Automating Business Intelligence Requirements with Generative AI and   Semantic Search
**Authors**: Nimrod Busany, Ethan Hadar, Hananel Hadad, Gil Rosenblum, Zofia Maszlanka, Okhaide Akhigbe, Daniel Amyot

**Updated**: 2024-12-10T16:57:48Z

**Summary**: Eliciting requirements for Business Intelligence (BI) systems remains a significant challenge, particularly in changing business environments. This paper introduces a novel AI-driven system, called AutoBIR, that leverages semantic search and Large Language Models (LLMs) to automate and accelerate the specification of BI requirements. The system facilitates intuitive interaction with stakeholders through a conversational interface, translating user inputs into prototype analytic code, descriptions, and data dependencies. Additionally, AutoBIR produces detailed test-case reports, optionally enhanced with visual aids, streamlining the requirement elicitation process. By incorporating user feedback, the system refines BI reporting and system design, demonstrating practical applications for expediting data-driven decision-making. This paper explores the broader potential of generative AI in transforming BI development, illustrating its role in enhancing data engineering practice for large-scale, evolving systems.

**Link**: [arxiv](http://arxiv.org/abs/2412.07668v1),  [pdf](http://arxiv.org/pdf/2412.07668v1)

**Tags**: cs.SE 



### Hamiltonian learning quantum magnets with non-local impurity tomography
**Authors**: Greta Lupi, Jose L. Lado

**Updated**: 2024-12-10T16:57:07Z

**Summary**: Impurities in quantum materials have provided successful strategies for learning properties of complex states, ranging from unconventional superconductors to topological insulators. In quantum magnetism, inferring the Hamiltonian of an engineered system becomes a challenging open problem in the presence of complex interactions. Here we show how a supervised machine-learning technique can be used to infer Hamiltonian parameters from atomically engineered quantum magnets by inferring fluctuations of the ground states due to the presence of impurities. We demonstrate our methodology both with a fermionic model with spin-orbit coupling, as well as with many-body spin models with long-range exchange and anisotropic exchange interactions. We show that our approach enables performing Hamiltonian extraction in the presence of significant noise, providing a strategy to perform Hamiltonian learning with experimental observables in atomic-scale quantum magnets. Our results establish a strategy to perform Hamiltonian learning by exploiting the impact of impurities in complex quantum many-body states.

**Link**: [arxiv](http://arxiv.org/abs/2412.07666v1),  [pdf](http://arxiv.org/pdf/2412.07666v1)

**Tags**: cond-mat.mes-hall cond-mat.str-el quant-ph 



### A Bayesian Nonparametric Stochastic Block Model for Directed Acyclic   Graphs
**Authors**: Clement Lee, Marco Battiston

**Updated**: 2024-12-10T16:55:03Z

**Summary**: Random graphs have been widely used in statistics, for example in network and social interaction analysis. In some applications, data may contain an inherent hierarchical ordering among its vertices, which prevents any directed edge between pairs of vertices that do not respect this order. For example, in bibliometrics, older papers cannot cite newer ones. In such situations, the resulting graph forms a Directed Acyclic Graph. In this article, we propose an extension of the popular Stochastic Block Model (SBM) to account for the presence of a latent hierarchical ordering in the data. The proposed approach includes a topological ordering in the likelihood of the model, which allows a directed edge to have positive probability only if the corresponding pair of vertices respect the order. This latent ordering is treated as an unknown parameter and endowed with a prior distribution. We describe how to formalize the model and perform posterior inference for a Bayesian nonparametric version of the SBM in which both the hierarchical ordering and the number of latent blocks are learnt from the data. Finally, an illustration with a real-world dataset from bibliometrics is presented. Additional supplementary materials are available online.

**Link**: [arxiv](http://arxiv.org/abs/2301.07513v2),  [pdf](http://arxiv.org/pdf/2301.07513v2)

**Tags**: stat.ME stat.CO 



### AdaSociety: An Adaptive Environment with Social Structures for   Multi-Agent Decision-Making
**Authors**: Yizhe Huang, Xingbo Wang, Hao Liu, Fanqi Kong, Aoyang Qin, Min Tang, Xiaoxi Wang, Song-Chun Zhu, Mingjie Bi, Siyuan Qi, Xue Feng

**Updated**: 2024-12-10T16:41:12Z

**Summary**: Traditional interactive environments limit agents' intelligence growth with fixed tasks. Recently, single-agent environments address this by generating new tasks based on agent actions, enhancing task diversity. We consider the decision-making problem in multi-agent settings, where tasks are further influenced by social connections, affecting rewards and information access. However, existing multi-agent environments lack a combination of adaptive physical surroundings and social connections, hindering the learning of intelligent behaviors. To address this, we introduce AdaSociety, a customizable multi-agent environment featuring expanding state and action spaces, alongside explicit and alterable social structures. As agents progress, the environment adaptively generates new tasks with social structures for agents to undertake. In AdaSociety, we develop three mini-games showcasing distinct social structures and tasks. Initial results demonstrate that specific social structures can promote both individual and collective benefits, though current reinforcement learning and LLM-based algorithms show limited effectiveness in leveraging social structures to enhance performance. Overall, AdaSociety serves as a valuable research platform for exploring intelligence in diverse physical and social settings. The code is available at https://github.com/bigai-ai/AdaSociety.

**Link**: [arxiv](http://arxiv.org/abs/2411.03865v2),  [pdf](http://arxiv.org/pdf/2411.03865v2)

**Tags**: cs.MA cs.AI cs.GT cs.LG cs.SI 



### Deep Compression Autoencoder for Efficient High-Resolution Diffusion   Models
**Authors**: Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, Song Han

**Updated**: 2024-12-10T16:39:23Z

**Summary**: We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder models for accelerating high-resolution diffusion models. Existing autoencoder models have demonstrated impressive results at a moderate spatial compression ratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for high spatial compression ratios (e.g., 64x). We address this challenge by introducing two key techniques: (1) Residual Autoencoding, where we design our models to learn residuals based on the space-to-channel transformed features to alleviate the optimization difficulty of high spatial-compression autoencoders; (2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases training strategy for mitigating the generalization penalty of high spatial-compression autoencoders. With these designs, we improve the autoencoder's spatial compression ratio up to 128 while maintaining the reconstruction quality. Applying our DC-AE to latent diffusion models, we achieve significant speedup without accuracy drop. For example, on ImageNet 512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup on H100 GPU for UViT-H while achieving a better FID, compared with the widely used SD-VAE-f8 autoencoder. Our code is available at https://github.com/mit-han-lab/efficientvit.

**Link**: [arxiv](http://arxiv.org/abs/2410.10733v3),  [pdf](http://arxiv.org/pdf/2410.10733v3)

**Tags**: cs.CV cs.AI 



### Searching for Structure: Investigating Emergent Communication with Large   Language Models
**Authors**: Tom Kouwenhoven, Max Peeperkorn, Tessa Verhoef

**Updated**: 2024-12-10T16:32:19Z

**Summary**: Human languages have evolved to be structured through repeated language learning and use. These processes introduce biases that operate during language acquisition and shape linguistic systems toward communicative efficiency. In this paper, we investigate whether the same happens if artificial languages are optimised for implicit biases of Large Language Models (LLMs). To this end, we simulate a classical referential game in which LLMs learn and use artificial languages. Our results show that initially unstructured holistic languages are indeed shaped to have some structural properties that allow two LLM agents to communicate successfully. Similar to observations in human experiments, generational transmission increases the learnability of languages, but can at the same time result in non-humanlike degenerate vocabularies. Taken together, this work extends experimental findings, shows that LLMs can be used as tools in simulations of language evolution, and opens possibilities for future human-machine experiments in this field.

**Link**: [arxiv](http://arxiv.org/abs/2412.07646v1),  [pdf](http://arxiv.org/pdf/2412.07646v1)

**Tags**: cs.CL 



### Symbolic Regression with a Learned Concept Library
**Authors**: Arya Grayeli, Atharva Sehgal, Omar Costilla-Reyes, Miles Cranmer, Swarat Chaudhuri

**Updated**: 2024-12-10T16:24:48Z

**Summary**: We present a novel method for symbolic regression (SR), the task of searching for compact programmatic hypotheses that best explain a dataset. The problem is commonly solved using genetic algorithms; we show that we can enhance such methods by inducing a library of abstract textual concepts. Our algorithm, called LaSR, uses zero-shot queries to a large language model (LLM) to discover and evolve concepts occurring in known high-performing hypotheses. We discover new hypotheses using a mix of standard evolutionary steps and LLM-guided steps (obtained through zero-shot LLM queries) conditioned on discovered concepts. Once discovered, hypotheses are used in a new round of concept abstraction and evolution. We validate LaSR on the Feynman equations, a popular SR benchmark, as well as a set of synthetic tasks. On these benchmarks, LaSR substantially outperforms a variety of state-of-the-art SR approaches based on deep learning and evolutionary algorithms. Moreover, we show that LaSR can be used to discover a novel and powerful scaling law for LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2409.09359v3),  [pdf](http://arxiv.org/pdf/2409.09359v3)

**Tags**: cs.LG cs.AI cs.NE cs.SC 



### SOMONITOR: Combining Explainable AI & Large Language Models for   Marketing Analytics
**Authors**: Aleksandr Farseev, Qi Yang, Marlo Ongpin, Ilia Gossoudarev, Yu-Yi Chu-Farseeva, Sergey Nikolenko

**Updated**: 2024-12-10T16:17:50Z

**Summary**: Online marketing faces formidable challenges in managing and interpreting immense volumes of data necessary for competitor analysis, content research, and strategic branding. It is impossible to review hundreds to thousands of transient online content items by hand, and partial analysis often leads to suboptimal outcomes and poorly performing campaigns. We introduce an explainable AI framework SOMONITOR that aims to synergize human intuition with AI-based efficiency, helping marketers across all stages of the marketing funnel, from strategic planning to content creation and campaign execution. SOMONITOR incorporates a CTR prediction and ranking model for advertising content and uses large language models (LLMs) to process high-performing competitor content, identifying core content pillars such as target audiences, customer needs, and product features. These pillars are then organized into broader categories, including communication themes and targeted customer personas. By integrating these insights with data from the brand's own advertising campaigns, SOMONITOR constructs a narrative for addressing new customer personas and simultaneously generates detailed content briefs in the form of user stories that, as shown in the conducted case study, can be directly applied by marketing teams to streamline content production and campaign execution. The adoption of SOMONITOR in daily operations allows digital marketers to quickly parse through extensive datasets, offering actionable insights that significantly enhance campaign effectiveness and overall job satisfaction.

**Link**: [arxiv](http://arxiv.org/abs/2407.13117v2),  [pdf](http://arxiv.org/pdf/2407.13117v2)

**Tags**: cs.CY cs.MM 



### TrojanWhisper: Evaluating Pre-trained LLMs to Detect and Localize   Hardware Trojans
**Authors**: Md Omar Faruque, Peter Jamieson, Ahmad Patooghy, Abdel-Hameed A. Badawy

**Updated**: 2024-12-10T16:16:22Z

**Summary**: Existing Hardware Trojans (HT) detection methods face several critical limitations: logic testing struggles with scalability and coverage for large designs, side-channel analysis requires golden reference chips, and formal verification methods suffer from state-space explosion. The emergence of Large Language Models (LLMs) offers a promising new direction for HT detection by leveraging their natural language understanding and reasoning capabilities. For the first time, this paper explores the potential of general-purpose LLMs in detecting various HTs inserted in Register Transfer Level (RTL) designs, including SRAM, AES, and UART modules. We propose a novel tool for this goal that systematically assesses state-of-the-art LLMs (GPT-4o, Gemini 1.5 pro, and Llama 3.1) in detecting HTs without prior fine-tuning. To address potential training data bias, the tool implements perturbation techniques, i.e., variable name obfuscation, and design restructuring, that make the cases more sophisticated for the used LLMs. Our experimental evaluation demonstrates perfect detection rates by GPT-4o and Gemini 1.5 pro in baseline scenarios (100%/100% precision/recall), with both models achieving better trigger line coverage (TLC: 0.82-0.98) than payload line coverage (PLC: 0.32-0.46). Under code perturbation, while Gemini 1.5 pro maintains perfect detection performance (100%/100%), GPT-4o (100%/85.7%) and Llama 3.1 (66.7%/85.7%) show some degradation in detection rates, and all models experience decreased accuracy in localizing both triggers and payloads. This paper validates the potential of LLM approaches for hardware security applications, highlighting areas for future improvement.

**Link**: [arxiv](http://arxiv.org/abs/2412.07636v1),  [pdf](http://arxiv.org/pdf/2412.07636v1)

**Tags**: cs.CR cs.AI 



### ChocoLlama: Lessons Learned From Teaching Llamas Dutch
**Authors**: Matthieu Meeus, Anthony Rathé, François Remy, Pieter Delobelle, Jens-Joris Decorte, Thomas Demeester

**Updated**: 2024-12-10T16:13:58Z

**Summary**: While Large Language Models (LLMs) have shown remarkable capabilities in natural language understanding and generation, their performance often lags in lower-resource, non-English languages due to biases in the training data. In this work, we explore strategies for adapting the primarily English LLMs (Llama-2 and Llama-3) to Dutch, a language spoken by 30 million people worldwide yet often underrepresented in LLM development. We collect 104GB of Dutch text ($32$B tokens) from various sources to first apply continued pretraining using low-rank adaptation (LoRA), complemented with Dutch posttraining strategies provided by prior work. For Llama-2, we consider using (i) the tokenizer of the original model, and (ii) training a new, Dutch-specific tokenizer combined with embedding reinitialization. We evaluate our adapted models, ChocoLlama-2, both on standard benchmarks and a novel Dutch benchmark, ChocoLlama-Bench. Our results demonstrate that LoRA can effectively scale for language adaptation, and that tokenizer modification with careful weight reinitialization can improve performance. Notably, Llama-3 was released during the course of this project and, upon evaluation, demonstrated superior Dutch capabilities compared to our Dutch-adapted versions of Llama-2. We hence apply the same adaptation technique to Llama-3, using its original tokenizer. While our adaptation methods enhanced Llama-2's Dutch capabilities, we found limited gains when applying the same techniques to Llama-3. This suggests that for ever improving, multilingual foundation models, language adaptation techniques may benefit more from focusing on language-specific posttraining rather than on continued pretraining. We hope this work contributes to the broader understanding of adapting LLMs to lower-resource languages, and to the development of Dutch LLMs in particular.

**Link**: [arxiv](http://arxiv.org/abs/2412.07633v1),  [pdf](http://arxiv.org/pdf/2412.07633v1)

**Tags**: cs.CL 



### Leveraging Large Language Models for Node Generation in Few-Shot   Learning on Text-Attributed Graphs
**Authors**: Jianxiang Yu, Yuxiang Ren, Chenghua Gong, Jiaqi Tan, Xiang Li, Xuecang Zhang

**Updated**: 2024-12-10T16:06:29Z

**Summary**: Text-attributed graphs have recently garnered significant attention due to their wide range of applications in web domains. Existing methodologies employ word embedding models for acquiring text representations as node features, which are subsequently fed into Graph Neural Networks (GNNs) for training. Recently, the advent of Large Language Models (LLMs) has introduced their powerful capabilities in information retrieval and text generation, which can greatly enhance the text attributes of graph data. Furthermore, the acquisition and labeling of extensive datasets are both costly and time-consuming endeavors. Consequently, few-shot learning has emerged as a crucial problem in the context of graph learning tasks. In order to tackle this challenge, we propose a lightweight paradigm called LLM4NG, which adopts a plug-and-play approach to empower text-attributed graphs through node generation using LLMs. Specifically, we utilize LLMs to extract semantic information from the labels and generate samples that belong to these categories as exemplars. Subsequently, we employ an edge predictor to capture the structural information inherent in the raw dataset and integrate the newly generated samples into the original graph. This approach harnesses LLMs for enhancing class-level information and seamlessly introduces labeled nodes and edges without modifying the raw dataset, thereby facilitating the node classification task in few-shot scenarios. Extensive experiments demonstrate the outstanding performance of our proposed paradigm, particularly in low-shot scenarios. For instance, in the 1-shot setting of the ogbn-arxiv dataset, LLM4NG achieves a 76% improvement over the baseline model.

**Link**: [arxiv](http://arxiv.org/abs/2310.09872v2),  [pdf](http://arxiv.org/pdf/2310.09872v2)

**Tags**: cs.LG 



### OmniDocBench: Benchmarking Diverse PDF Document Parsing with   Comprehensive Annotations
**Authors**: Linke Ouyang, Yuan Qu, Hongbin Zhou, Jiawei Zhu, Rui Zhang, Qunshu Lin, Bin Wang, Zhiyuan Zhao, Man Jiang, Xiaomeng Zhao, Jin Shi, Fan Wu, Pei Chu, Minghao Liu, Zhenxiang Li, Chao Xu, Bo Zhang, Botian Shi, Zhongying Tu, Conghui He

**Updated**: 2024-12-10T16:05:56Z

**Summary**: Document content extraction is crucial in computer vision, especially for meeting the high-quality data needs of large language models (LLMs) and retrieval-augmented generation (RAG) technologies. However, current document parsing methods suffer from significant limitations in terms of diversity and comprehensive evaluation. To address these challenges, we introduce OmniDocBench, a novel multi-source benchmark designed to advance automated document content extraction. OmniDocBench includes a meticulously curated and annotated high-quality evaluation dataset comprising nine diverse document types, such as academic papers, textbooks, slides, among others. Our benchmark provides a flexible and comprehensive evaluation framework with 19 layout category labels and 14 attribute labels, enabling multi-level assessments across entire datasets, individual modules, or specific data types. Using OmniDocBench, we perform an exhaustive comparative analysis of existing modular pipelines and multimodal end-to-end methods, highlighting their limitations in handling document diversity and ensuring fair evaluation. OmniDocBench establishes a robust, diverse, and fair evaluation standard for the document content extraction field, offering crucial insights for future advancements and fostering the development of document parsing technologies. The codes and dataset is available in https://github.com/opendatalab/OmniDocBench.

**Link**: [arxiv](http://arxiv.org/abs/2412.07626v1),  [pdf](http://arxiv.org/pdf/2412.07626v1)

**Tags**: cs.CV cs.AI cs.IR 



### CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model   Recommender System
**Authors**: Yashar Deldjoo, Tommaso di Noia

**Updated**: 2024-12-10T16:00:55Z

**Summary**: This work takes a critical stance on previous studies concerning fairness evaluation in Large Language Model (LLM)-based recommender systems, which have primarily assessed consumer fairness by comparing recommendation lists generated with and without sensitive user attributes. Such approaches implicitly treat discrepancies in recommended items as biases, overlooking whether these changes might stem from genuine personalization aligned with true preferences of users. Moreover, these earlier studies typically address single sensitive attributes in isolation, neglecting the complex interplay of intersectional identities. In response to these shortcomings, we introduce CFaiRLLM, an enhanced evaluation framework that not only incorporates true preference alignment but also rigorously examines intersectional fairness by considering overlapping sensitive attributes. Additionally, CFaiRLLM introduces diverse user profile sampling strategies-random, top-rated, and recency-focused-to better understand the impact of profile generation fed to LLMs in light of inherent token limitations in these systems. Given that fairness depends on accurately understanding users' tastes and preferences,, these strategies provide a more realistic assessment of fairness within RecLLMs.   The results demonstrated that true preference alignment offers a more personalized and fair assessment compared to similarity-based measures, revealing significant disparities when sensitive and intersectional attributes are incorporated. Notably, our study finds that intersectional attributes amplify fairness gaps more prominently, especially in less structured domains such as music recommendations in LastFM.

**Link**: [arxiv](http://arxiv.org/abs/2403.05668v2),  [pdf](http://arxiv.org/pdf/2403.05668v2)

**Tags**: cs.IR 



### DRUM: Learning Demonstration Retriever for Large MUlti-modal Models
**Authors**: Ellen Yi-Ge, Jiechao Gao, Wei Han, Wei Zhu

**Updated**: 2024-12-10T15:56:12Z

**Summary**: Recently, large language models (LLMs) have demonstrated impressive capabilities in dealing with new tasks with the help of in-context learning (ICL). In the study of Large Vision-Language Models (LVLMs), when implementing ICL, researchers usually adopts the naive strategies like fixed demonstrations across different samples, or selecting demonstrations directly via a visual-language embedding model. These methods does not guarantee the configured demonstrations fit the need of the LVLMs. To address this issue, we now propose a novel framework, \underline{d}emonstration \underline{r}etriever for large m\underline{u}lti-modal \underline{m}odel (DRUM), which fine-tunes the visual-language embedding model to better meet the LVLM's needs. First, we discuss the retrieval strategies for a visual-language task, assuming an embedding model is given. And we propose to concate the image and text embeddings to enhance the retrieval performance. Second, we propose to re-rank the demonstrations retrieved by the embedding model via the LVLM's feedbacks, and calculate a list-wise ranking loss for training the embedding model. Third, we propose an iterative demonstration mining strategy to improve the training of the embedding model. Through extensive experiments on 3 types of visual-language tasks, 7 benchmark datasets, our DRUM framework is proven to be effective in boosting the LVLM's in-context learning performance via retrieving more proper demonstrations.

**Link**: [arxiv](http://arxiv.org/abs/2412.07619v1),  [pdf](http://arxiv.org/pdf/2412.07619v1)

**Tags**: cs.CL 



### Adapting to Non-Stationary Environments: Multi-Armed Bandit Enhanced   Retrieval-Augmented Generation on Knowledge Graphs
**Authors**: Xiaqiang Tang, Jian Li, Nan Du, Sihong Xie

**Updated**: 2024-12-10T15:56:03Z

**Summary**: Despite the superior performance of Large language models on many NLP tasks, they still face significant limitations in memorizing extensive world knowledge. Recent studies have demonstrated that leveraging the Retrieval-Augmented Generation (RAG) framework, combined with Knowledge Graphs that encapsulate extensive factual data in a structured format, robustly enhances the reasoning capabilities of LLMs. However, deploying such systems in real-world scenarios presents challenges: the continuous evolution of non-stationary environments may lead to performance degradation and user satisfaction requires a careful balance of performance and responsiveness. To address these challenges, we introduce a Multi-objective Multi-Armed Bandit enhanced RAG framework, supported by multiple retrieval methods with diverse capabilities under rich and evolving retrieval contexts in practice. Within this framework, each retrieval method is treated as a distinct ``arm''. The system utilizes real-time user feedback to adapt to dynamic environments, by selecting the appropriate retrieval method based on input queries and the historical multi-objective performance of each arm. Extensive experiments conducted on two benchmark KGQA datasets demonstrate that our method significantly outperforms baseline methods in non-stationary settings while achieving state-of-the-art performance in stationary environments. Code and data are available at https://github.com/FUTUREEEEEE/Dynamic-RAG.git

**Link**: [arxiv](http://arxiv.org/abs/2412.07618v1),  [pdf](http://arxiv.org/pdf/2412.07618v1)

**Tags**: cs.AI cs.CL 



### Applications and Implications of Large Language Models in Qualitative   Analysis: A New Frontier for Empirical Software Engineering
**Authors**: Matheus de Morais Leça, Lucas Valença, Reydne Santos, Ronnie de Souza Santos

**Updated**: 2024-12-10T15:53:18Z

**Summary**: The use of large language models (LLMs) for qualitative analysis is gaining attention in various fields, including software engineering, where qualitative methods are essential for understanding human and social factors. This study aimed to investigate how LLMs are currently used in qualitative analysis and their potential applications in software engineering research, focusing on the benefits, limitations, and practices associated with their use. A systematic mapping study was conducted, analyzing 21 relevant studies to explore reported uses of LLMs for qualitative analysis. The findings indicate that LLMs are primarily used for tasks such as coding, thematic analysis, and data categorization, offering benefits like increased efficiency and support for new researchers. However, limitations such as output variability, challenges in capturing nuanced perspectives, and ethical concerns related to privacy and transparency were also identified. The study emphasizes the need for structured strategies and guidelines to optimize LLM use in qualitative research within software engineering, enhancing their effectiveness while addressing ethical considerations. While LLMs show promise in supporting qualitative analysis, human expertise remains crucial for interpreting data, and ongoing exploration of best practices will be vital for their successful integration into empirical software engineering research.

**Link**: [arxiv](http://arxiv.org/abs/2412.06564v2),  [pdf](http://arxiv.org/pdf/2412.06564v2)

**Tags**: cs.SE 



### Deep Partially Linear Transformation Model for Right-Censored Survival   Data
**Authors**: Junkai Yin, Yue Zhang, Zhangsheng Yu

**Updated**: 2024-12-10T15:50:43Z

**Summary**: Although the Cox proportional hazards model is well established and extensively used in the analysis of survival data, the proportional hazards (PH) assumption may not always hold in practical scenarios. The semiparametric transformation model extends the conventional Cox model and also includes many other survival models as special cases. This paper introduces a deep partially linear transformation model (DPLTM) as a general and flexible framework for estimation, inference and prediction. The proposed method is capable of avoiding the curse of dimensionality while still retaining the interpretability of some covariates of interest. We derive the overall convergence rate of the maximum likelihood estimators, the minimax lower bound of the nonparametric deep neural network (DNN) estimator, the asymptotic normality and the semiparametric efficiency of the parametric estimator. Comprehensive simulation studies demonstrate the impressive performance of the proposed estimation procedure in terms of both estimation accuracy and prediction power, which is further validated by an application to a real-world dataset.

**Link**: [arxiv](http://arxiv.org/abs/2412.07611v1),  [pdf](http://arxiv.org/pdf/2412.07611v1)

**Tags**: stat.ME stat.ML 



### Nested exemplar latent space models for dimension reduction in dynamic   networks
**Authors**: Jennifer Noelle Kampe, Luca Alessandro Silva, David Brian Dunson, Tomas Roslin

**Updated**: 2024-12-10T15:45:22Z

**Summary**: Dynamic latent space models are widely used for characterizing changes in networks and relational data over time. These models assign to each node latent attributes that characterize connectivity with other nodes, with these latent attributes dynamically changing over time. Node attributes can be organized as a three-way tensor with modes corresponding to nodes, latent space dimension, and time. Unfortunately, as the number of nodes and time points increases, the number of elements of this tensor becomes enormous, leading to computational and statistical challenges, particularly when data are sparse. We propose a new approach for massively reducing dimensionality by expressing the latent node attribute tensor as low rank. This leads to an interesting new {\em nested exemplar} latent space model, which characterizes the node attribute tensor as dependent on low-dimensional exemplar traits for each node, weights for each latent space dimension, and exemplar curves characterizing time variation. We study properties of this framework, including expressivity, and develop efficient Bayesian inference algorithms. The approach leads to substantial advantages in simulations and applications to ecological networks.

**Link**: [arxiv](http://arxiv.org/abs/2412.07604v1),  [pdf](http://arxiv.org/pdf/2412.07604v1)

**Tags**: stat.ME 



### Which LLMs are Difficult to Detect? A Detailed Analysis of Potential   Factors Contributing to Difficulties in LLM Text Detection
**Authors**: Shantanu Thorat, Tianbao Yang

**Updated**: 2024-12-10T15:44:59Z

**Summary**: As LLMs increase in accessibility, LLM-generated texts have proliferated across several fields, such as scientific, academic, and creative writing. However, LLMs are not created equally; they may have different architectures and training datasets. Thus, some LLMs may be more challenging to detect than others. Using two datasets spanning four total writing domains, we train AI-generated (AIG) text classifiers using the LibAUC library - a deep learning library for training classifiers with imbalanced datasets. Our results in the Deepfake Text dataset show that AIG-text detection varies across domains, with scientific writing being relatively challenging. In the Rewritten Ivy Panda (RIP) dataset focusing on student essays, we find that the OpenAI family of LLMs was substantially difficult for our classifiers to distinguish from human texts. Additionally, we explore possible factors that could explain the difficulties in detecting OpenAI-generated texts.

**Link**: [arxiv](http://arxiv.org/abs/2410.14875v2),  [pdf](http://arxiv.org/pdf/2410.14875v2)

**Tags**: cs.CL cs.LG 



### DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for   Customized Manga Generation
**Authors**: Jianzong Wu, Chao Tang, Jingbo Wang, Yanhong Zeng, Xiangtai Li, Yunhai Tong

**Updated**: 2024-12-10T15:24:12Z

**Summary**: Story visualization, the task of creating visual narratives from textual descriptions, has seen progress with text-to-image generation models. However, these models often lack effective control over character appearances and interactions, particularly in multi-character scenes. To address these limitations, we propose a new task: \textbf{customized manga generation} and introduce \textbf{DiffSensei}, an innovative framework specifically designed for generating manga with dynamic multi-character control. DiffSensei integrates a diffusion-based image generator with a multimodal large language model (MLLM) that acts as a text-compatible identity adapter. Our approach employs masked cross-attention to seamlessly incorporate character features, enabling precise layout control without direct pixel transfer. Additionally, the MLLM-based adapter adjusts character features to align with panel-specific text cues, allowing flexible adjustments in character expressions, poses, and actions. We also introduce \textbf{MangaZero}, a large-scale dataset tailored to this task, containing 43,264 manga pages and 427,147 annotated panels, supporting the visualization of varied character interactions and movements across sequential frames. Extensive experiments demonstrate that DiffSensei outperforms existing models, marking a significant advancement in manga generation by enabling text-adaptable character customization. The project page is https://jianzongwu.github.io/projects/diffsensei/.

**Link**: [arxiv](http://arxiv.org/abs/2412.07589v1),  [pdf](http://arxiv.org/pdf/2412.07589v1)

**Tags**: cs.CV 



### Scaling Sequential Recommendation Models with Transformers
**Authors**: Pablo Zivic, Hernan Vazquez, Jorge Sanchez

**Updated**: 2024-12-10T15:20:56Z

**Summary**: Modeling user preferences has been mainly addressed by looking at users' interaction history with the different elements available in the system. Tailoring content to individual preferences based on historical data is the main goal of sequential recommendation.   The nature of the problem, as well as the good performance observed across various domains, has motivated the use of the transformer architecture, which has proven effective in leveraging increasingly larger amounts of training data when accompanied by an increase in the number of model parameters. This scaling behavior has brought a great deal of attention, as it provides valuable guidance in the design and training of even larger models.   Taking inspiration from the scaling laws observed in training large language models, we explore similar principles for sequential recommendation.   We use the full Amazon Product Data dataset, which has only been partially explored in other studies, and reveal scaling behaviors similar to those found in language models. Compute-optimal training is possible but requires a careful analysis of the compute-performance trade-offs specific to the application.   We also show that performance scaling translates to downstream tasks by fine-tuning larger pre-trained models on smaller task-specific domains. Our approach and findings provide a strategic roadmap for model training and deployment in real high-dimensional preference spaces, facilitating better training and inference efficiency.   We hope this paper bridges the gap between the potential of transformers and the intrinsic complexities of high-dimensional sequential recommendation in real-world recommender systems.   Code and models can be found at https://github.com/mercadolibre/srt

**Link**: [arxiv](http://arxiv.org/abs/2412.07585v1),  [pdf](http://arxiv.org/pdf/2412.07585v1)

**Tags**: cs.LG cs.AI 



### Multimodal Contextualized Support for Enhancing Video Retrieval System
**Authors**: Quoc-Bao Nguyen-Le, Thanh-Huy Le-Nguyen

**Updated**: 2024-12-10T15:20:23Z

**Summary**: Current video retrieval systems, especially those used in competitions, primarily focus on querying individual keyframes or images rather than encoding an entire clip or video segment. However, queries often describe an action or event over a series of frames, not a specific image. This results in insufficient information when analyzing a single frame, leading to less accurate query results. Moreover, extracting embeddings solely from images (keyframes) does not provide enough information for models to encode higher-level, more abstract insights inferred from the video. These models tend to only describe the objects present in the frame, lacking a deeper understanding. In this work, we propose a system that integrates the latest methodologies, introducing a novel pipeline that extracts multimodal data, and incorporate information from multiple frames within a video, enabling the model to abstract higher-level information that captures latent meanings, focusing on what can be inferred from the video clip, rather than just focusing on object detection in one single image.

**Link**: [arxiv](http://arxiv.org/abs/2412.07584v1),  [pdf](http://arxiv.org/pdf/2412.07584v1)

**Tags**: cs.CV cs.AI 



### BudgetFusion: Perceptually-Guided Adaptive Diffusion Models
**Authors**: Qinchan Li, Kenneth Chen, Changyue Su, Qi Sun

**Updated**: 2024-12-10T15:18:04Z

**Summary**: Diffusion models have shown unprecedented success in the task of text-to-image generation. While these models are capable of generating high-quality and realistic images, the complexity of sequential denoising has raised societal concerns regarding high computational demands and energy consumption. In response, various efforts have been made to improve inference efficiency. However, most of the existing efforts have taken a fixed approach with neural network simplification or text prompt optimization. Are the quality improvements from all denoising computations equally perceivable to humans? We observed that images from different text prompts may require different computational efforts given the desired content. The observation motivates us to present BudgetFusion, a novel model that suggests the most perceptually efficient number of diffusion steps before a diffusion model starts to generate an image. This is achieved by predicting multi-level perceptual metrics relative to diffusion steps. With the popular Stable Diffusion as an example, we conduct both numerical analyses and user studies. Our experiments show that BudgetFusion saves up to five seconds per prompt without compromising perceptual similarity. We hope this work can initiate efforts toward answering a core question: how much do humans perceptually gain from images created by a generative model, per watt of energy?

**Link**: [arxiv](http://arxiv.org/abs/2412.05780v2),  [pdf](http://arxiv.org/pdf/2412.05780v2)

**Tags**: cs.CV cs.AI 



### Bigger is not Always Better: Scaling Properties of Latent Diffusion   Models
**Authors**: Kangfu Mei, Zhengzhong Tu, Mauricio Delbracio, Hossein Talebi, Vishal M. Patel, Peyman Milanfar

**Updated**: 2024-12-10T15:10:41Z

**Summary**: We study the scaling properties of latent diffusion models (LDMs) with an emphasis on their sampling efficiency. While improved network architecture and inference algorithms have shown to effectively boost sampling efficiency of diffusion models, the role of model size -- a critical determinant of sampling efficiency -- has not been thoroughly examined. Through empirical analysis of established text-to-image diffusion models, we conduct an in-depth investigation into how model size influences sampling efficiency across varying sampling steps. Our findings unveil a surprising trend: when operating under a given inference budget, smaller models frequently outperform their larger equivalents in generating high-quality results. Moreover, we extend our study to demonstrate the generalizability of the these findings by applying various diffusion samplers, exploring diverse downstream tasks, evaluating post-distilled models, as well as comparing performance relative to training compute. These findings open up new pathways for the development of LDM scaling strategies which can be employed to enhance generative capabilities within limited inference budgets.

**Link**: [arxiv](http://arxiv.org/abs/2404.01367v2),  [pdf](http://arxiv.org/pdf/2404.01367v2)

**Tags**: cs.CV cs.LG 



### GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large   Language Models
**Authors**: Tao Zhang, Ziqian Zeng, Yuxiang Xiao, Huiping Zhuang, Cen Chen, James Foulds, Shimei Pan

**Updated**: 2024-12-10T14:46:40Z

**Summary**: Large Language Models (LLMs) are prone to generating content that exhibits gender biases, raising significant ethical concerns. Alignment, the process of fine-tuning LLMs to better align with desired behaviors, is recognized as an effective approach to mitigate gender biases. Although proprietary LLMs have made significant strides in mitigating gender bias, their alignment datasets are not publicly available. The commonly used and publicly available alignment dataset, HH-RLHF, still exhibits gender bias to some extent. There is a lack of publicly available alignment datasets specifically designed to address gender bias. Hence, we developed a new dataset named GenderAlign, aiming at mitigating a comprehensive set of gender biases in LLMs. This dataset comprises 8k single-turn dialogues, each paired with a "chosen" and a "rejected" response. Compared to the "rejected" responses, the "chosen" responses demonstrate lower levels of gender bias and higher quality. Furthermore, we categorized the gender biases in the "rejected" responses of GenderAlign into 4 principal categories. The experimental results show the effectiveness of GenderAlign in reducing gender bias in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2406.13925v2),  [pdf](http://arxiv.org/pdf/2406.13925v2)

**Tags**: cs.CL cs.AI 



### GSM: A GNN-based Space-MIMO Framework for Direct-to-Cell Communications
**Authors**: Sai Xu, Yanan Du, Gaojie Chen, Rahim Tafazolli

**Updated**: 2024-12-10T14:45:12Z

**Summary**: This paper proposes a graph neural network (GNN)-based space multiple-input multiple-output (MIMO) framework, named GSM, for direct-to-cell communications, aiming to achieve distributed coordinated beamforming for low Earth orbit (LEO) satellites. Firstly, a system model for LEO multi-satellite communications is established, where multiple LEO satellites collaborate to perform distributed beamforming and communicate with terrestrial user terminals coherently. Based on the system model, a weighted sum rate maximization problem is formulated. Secondly, a GNN-based method is developed to address the optimization problem. Particularly, the adopted neural network is composed of multiple identical GNNs, which are trained together and then deployed individually on each LEO satellite. Finally, the trained GNN is quantized and deployed on a field-programmable gate array (FPGA) to accelerate the inference by customizing the microarchitecture. Simulation results demonstrate that the proposed GNN scheme outperforms the benchmark ones including maximum ratio transmission, zero forcing and minimum mean square error. Furthermore, experimental results show that the FPGA-based accelerator achieves remarkably low inference latency, ranging from 3.863 to 5.883 ms under a 10-ns target clock period with 8-bit fixed-point data representation.

**Link**: [arxiv](http://arxiv.org/abs/2412.07555v1),  [pdf](http://arxiv.org/pdf/2412.07555v1)

**Tags**: eess.SP 



### CPsyExam: A Chinese Benchmark for Evaluating Psychology using   Examinations
**Authors**: Jiahao Zhao, Jingwei Zhu, Minghuan Tan, Min Yang, Renhao Li, Di Yang, Chenhao Zhang, Guancheng Ye, Chengming Li, Xiping Hu, Derek F. Wong

**Updated**: 2024-12-10T14:44:41Z

**Summary**: In this paper, we introduce a novel psychological benchmark, CPsyExam, constructed from questions sourced from Chinese language examinations. CPsyExam is designed to prioritize psychological knowledge and case analysis separately, recognizing the significance of applying psychological knowledge to real-world scenarios. From the pool of 22k questions, we utilize 4k to create the benchmark that offers balanced coverage of subjects and incorporates a diverse range of case analysis techniques.Furthermore, we evaluate a range of existing large language models~(LLMs), spanning from open-sourced to API-based models. Our experiments and analysis demonstrate that CPsyExam serves as an effective benchmark for enhancing the understanding of psychology within LLMs and enables the comparison of LLMs across various granularities.

**Link**: [arxiv](http://arxiv.org/abs/2405.10212v3),  [pdf](http://arxiv.org/pdf/2405.10212v3)

**Tags**: cs.CL 



### Automatic Database Configuration Debugging using Retrieval-Augmented   Language Models
**Authors**: Sibei Chen, Ju Fan, Bin Wu, Nan Tang, Chao Deng, Pengyi Wang, Ye Li, Jian Tan, Feifei Li, Jingren Zhou, Xiaoyong Du

**Updated**: 2024-12-10T14:39:51Z

**Summary**: Database management system (DBMS) configuration debugging, e.g., diagnosing poorly configured DBMS knobs and generating troubleshooting recommendations, is crucial in optimizing DBMS performance. However, the configuration debugging process is tedious and, sometimes challenging, even for seasoned database administrators (DBAs) with sufficient experience in DBMS configurations and good understandings of the DBMS internals (e.g., MySQL or Oracle). To address this difficulty, we propose Andromeda, a framework that utilizes large language models (LLMs) to enable automatic DBMS configuration debugging. Andromeda serves as a natural surrogate of DBAs to answer a wide range of natural language (NL) questions on DBMS configuration issues, and to generate diagnostic suggestions to fix these issues. Nevertheless, directly prompting LLMs with these professional questions may result in overly generic and often unsatisfying answers. To this end, we propose a retrieval-augmented generation (RAG) strategy that effectively provides matched domain-specific contexts for the question from multiple sources. They come from related historical questions, troubleshooting manuals and DBMS telemetries, which significantly improve the performance of configuration debugging. To support the RAG strategy, we develop a document retrieval mechanism addressing heterogeneous documents and design an effective method for telemetry analysis. Extensive experiments on real-world DBMS configuration debugging datasets show that Andromeda significantly outperforms existing solutions.

**Link**: [arxiv](http://arxiv.org/abs/2412.07548v1),  [pdf](http://arxiv.org/pdf/2412.07548v1)

**Tags**: cs.DB 



### An Eccentric Binary with a Misaligned Circumbinary Disk
**Authors**: Zhecheng Hu, Wei Zhu, Fei Dai, Ping Chen, Yang Huang, Min Fang, Richard S. Post

**Updated**: 2024-12-10T14:18:02Z

**Summary**: We present spectroscopic and photometric observations of Bernhard-2, which was previously identified as a candidate system to host a misaligned circumbinary disk. Our spectroscopic measurements confirm that Bernhard-2 indeed contains an eccentric ($e=0.69 \pm 0.08$) binary and thus that the periodic variability in the photometric light curve is best explained by the occultation by the misaligned circumbinary disk. By modeling the spectral energy distributions at different phases, we infer the masses of the two binary components to be $\sim 1.1\,M_\odot$ and $\sim 0.9\,M_\odot$, respectively. The system age is determined to be $\lesssim$ 20 Myr by combining the stellar isochrone model with lithium abundance. Our new photometric observations show clear deviations from the model prediction based on the archival data, suggesting ongoing precession of the circumbinary disk. The H$\alpha$ line of Bernhard-2 also shows an inverse P-Cygni profile at epochs close to the pericenter passage, which could be attributed to the pulsed accretion around the pericenter. Bernhard-2 therefore closely resembles the well studied KH 15D system. Further detailed observations and studies of such rare systems can provide useful information about disk physics and evolution.

**Link**: [arxiv](http://arxiv.org/abs/2409.18296v2),  [pdf](http://arxiv.org/pdf/2409.18296v2)

**Tags**: astro-ph.SR astro-ph.EP 



### A Comprehensive Analysis of Social Tie Strength: Definitions, Prediction   Methods, and Future Directions
**Authors**: Xueqi Cheng, Catherine Yang, Yuying Zhao, Yu Wang, Hamid Karimi, Tyler Derr

**Updated**: 2024-12-10T14:12:03Z

**Summary**: The rapid growth of online social networks has underscored the importance of understanding the intensity of user relationships, referred to as "tie strength." Over the past few decades, extensive efforts have been made to assess tie strength in networks. However, the lack of ground-truth tie strength labels and the differing perspectives on tie strength among researchers have complicated the development of effective prediction methods for real-world applications. In our study, we first categorize mainstream understandings of tie strength into seven standardized definitions and verify their effectiveness by investigating the class distributions and correlations across these definitions. We also draw key insights into tie resilience from the perspective of tie dissolution that (1) stronger ties are more resilient than weaker ones, and (2) this tie resiliency ratio increases as the network evolves. We then conduct extensive experiments to evaluate existing tie strength prediction methods under these definitions, revealing that (1) neural network methods capable of learning from semantic features hold great potential for high performance, (2) models struggle under definitions that offer limited understandings of tie strength in the network, (3) existing models face imbalance issues that cannot be addressed by traditional quantity imbalance techniques, and (4) different definitions of tie strength allow for the inference of not only the current state but also the future state of a tie. Building on these findings, we propose strategies to improve existing methods and suggest several promising directions for future research. Code and datasets are provided at https://github.com/XueqiC/Awesome-Tie-Strength-Prediction.

**Link**: [arxiv](http://arxiv.org/abs/2410.19214v3),  [pdf](http://arxiv.org/pdf/2410.19214v3)

**Tags**: cs.SI 



### Scalable Autoregressive Image Generation with Mamba
**Authors**: Haopeng Li, Jinyue Yang, Kexin Wang, Xuerui Qiu, Yuhong Chou, Xin Li, Guoqi Li

**Updated**: 2024-12-10T14:09:17Z

**Summary**: We introduce AiM, an autoregressive (AR) image generative model based on Mamba architecture. AiM employs Mamba, a novel state-space model characterized by its exceptional performance for long-sequence modeling with linear time complexity, to supplant the commonly utilized Transformers in AR image generation models, aiming to achieve both superior generation quality and enhanced inference speed. Unlike existing methods that adapt Mamba to handle two-dimensional signals via multi-directional scan, AiM directly utilizes the next-token prediction paradigm for autoregressive image generation. This approach circumvents the need for extensive modifications to enable Mamba to learn 2D spatial representations. By implementing straightforward yet strategically targeted modifications for visual generative tasks, we preserve Mamba's core structure, fully exploiting its efficient long-sequence modeling capabilities and scalability. We provide AiM models in various scales, with parameter counts ranging from 148M to 1.3B. On the ImageNet1K 256*256 benchmark, our best AiM model achieves a FID of 2.21, surpassing all existing AR models of comparable parameter counts and demonstrating significant competitiveness against diffusion models, with 2 to 10 times faster inference speed. Code is available at https://github.com/hp-l33/AiM

**Link**: [arxiv](http://arxiv.org/abs/2408.12245v2),  [pdf](http://arxiv.org/pdf/2408.12245v2)

**Tags**: cs.CV 



### Inference for Regression with Variables Generated by AI or Machine   Learning
**Authors**: Laura Battaglia, Timothy Christensen, Stephen Hansen, Szymon Sacher

**Updated**: 2024-12-10T14:08:14Z

**Summary**: It has become common practice for researchers to use AI-powered information retrieval algorithms or other machine learning methods to estimate variables of economic interest, then use these estimates as covariates in a regression model. We show both theoretically and empirically that naively treating AI- and ML-generated variables as "data" leads to biased estimates and invalid inference. We propose two methods to correct bias and perform valid inference: (i) an explicit bias correction with bias-corrected confidence intervals, and (ii) joint maximum likelihood estimation of the regression model and the variables of interest. Through several applications, we demonstrate that the common approach generates substantial bias, while both corrections perform well.

**Link**: [arxiv](http://arxiv.org/abs/2402.15585v4),  [pdf](http://arxiv.org/pdf/2402.15585v4)

**Tags**: econ.EM stat.ML 



### Constraining Jet Quenching in Heavy-Ion Collisions with Bayesian   Inference
**Authors**: Alexandre Falcão, Konrad Tywoniuk

**Updated**: 2024-12-10T14:00:09Z

**Summary**: Jet suppression and modification is a hallmark feature of heavy-ion collisions. This can be attributed to an accumulated set of effects, including radiative and elastic energy loss and reabsorption of thermalized energy within the jet cone, which are encoded in a quenching weight, determining the probability distribution for a shift of the $p_T$ (energy loss). We perform a data-driven analysis, based on Bayesian inference, to extract information about the energy-loss distribution experienced by propagating jets using generic and flexible parametrizations. We first establish the consistency between different data-sets and, thereby, provide evidence for the universality of the quark/gluon quenching weights for different observables. Furthermore, we extract that the color dependence of energy loss is slightly bigger than what expected from Casimir scaling, pointing to the importance of multi-parton quenching within high-$p_T$ jets at the LHC.

**Link**: [arxiv](http://arxiv.org/abs/2411.14552v2),  [pdf](http://arxiv.org/pdf/2411.14552v2)

**Tags**: hep-ph hep-ex nucl-th 



### JWST Imaging of Edge-on Protoplanetary Disks. IV. Mid-infrared Dust   Scattering in the HH 30 disk
**Authors**: Ryo Tazaki, François Ménard, Gaspard Duchêne, Marion Villenave, Álvaro Ribas, Karl R. Stapelfeldt, Marshall D. Perrin, Christophe Pinte, Schuyler G. Wolff, Deborah L. Padgett, Jie Ma, Laurine Martinien, Maxime Roumesy

**Updated**: 2024-12-10T14:00:09Z

**Summary**: We present near- and mid-infrared (IR) broadband imaging observations of the edge-on protoplanetary disk around HH 30 with the James Webb Space Telescope/Near Infrared Camera (NIRCam) and the Mid-Infrared Instrument (MIRI). We combine these observations with archival optical/near-IR scattered light images obtained with the Hubble Space Telescope (HST) and a millimeter-wavelength dust continuum image obtained with the Atacama Large Millimeter/submillimeter Array (ALMA) with the highest spatial resolution ever obtained for this target. Our multiwavelength images clearly reveal the vertical and radial segregation of micron-sized and sub-mm-sized grains in the disk. In the near- and mid-IR, the images capture not only bi-reflection nebulae separated by a dark lane but also diverse dynamical processes occurring in the HH 30 disk, such as spiral- and tail-like structures, a conical outflow, and a collimated jet. In contrast, the ALMA image reveals a flat dust disk in the disk midplane. By performing radiative transfer simulations, we show that grains of about 3 $\mu$m in radius or larger are fully vertically mixed to explain the observed mid-IR scattered light flux and its morphology, whereas millimeter-sized grains are settled into a layer with a scale height of $\gtrsim1$ au at $100$ au from the central star. We also find a tension in the disk inclination angle inferred from optical/near-IR and mm observations with the latter being closer to an exactly edge-on. Finally, we report the first detection of the proper motion of an emission knot associated with the mid-IR collimated jet detected by combining two epochs of our MIRI 12.8-$\mu$m observations.

**Link**: [arxiv](http://arxiv.org/abs/2412.07523v1),  [pdf](http://arxiv.org/pdf/2412.07523v1)

**Tags**: astro-ph.EP astro-ph.SR 



### AutoDetect: Towards a Unified Framework for Automated Weakness Detection   in Large Language Models
**Authors**: Jiale Cheng, Yida Lu, Xiaotao Gu, Pei Ke, Xiao Liu, Yuxiao Dong, Hongning Wang, Jie Tang, Minlie Huang

**Updated**: 2024-12-10T13:57:46Z

**Summary**: Although Large Language Models (LLMs) are becoming increasingly powerful, they still exhibit significant but subtle weaknesses, such as mistakes in instruction-following or coding tasks. As these unexpected errors could lead to severe consequences in practical deployments, it is crucial to investigate the limitations within LLMs systematically. Traditional benchmarking approaches cannot thoroughly pinpoint specific model deficiencies, while manual inspections are costly and not scalable. In this paper, we introduce a unified framework, AutoDetect, to automatically expose weaknesses in LLMs across various tasks. Inspired by the educational assessment process that measures students' learning outcomes, AutoDetect consists of three LLM-powered agents: Examiner, Questioner, and Assessor. The collaboration among these three agents is designed to realize comprehensive and in-depth weakness identification. Our framework demonstrates significant success in uncovering flaws, with an identification success rate exceeding 30% in prominent models such as ChatGPT and Claude. More importantly, these identified weaknesses can guide specific model improvements, proving more effective than untargeted data augmentation methods like Self-Instruct. Our approach has led to substantial enhancements in popular LLMs, including the Llama series and Mistral-7b, boosting their performance by over 10% across several benchmarks. Code and data are publicly available at https://github.com/thu-coai/AutoDetect.

**Link**: [arxiv](http://arxiv.org/abs/2406.16714v2),  [pdf](http://arxiv.org/pdf/2406.16714v2)

**Tags**: cs.CL cs.AI cs.LG 



### CoPrUS: Consistency Preserving Utterance Synthesis towards more   realistic benchmark dialogues
**Authors**: Sebastian Steindl, Ulrich Schäfer, Bernd Ludwig

**Updated**: 2024-12-10T13:51:55Z

**Summary**: Large-scale Wizard-Of-Oz dialogue datasets have enabled the training of deep learning-based dialogue systems. While they are successful as benchmark datasets, they lack certain types of utterances, which would make them more realistic. In this work, we investigate the creation of synthetic communication errors in an automatic pipeline. Based on linguistic theory, we propose and follow a simple error taxonomy. We focus on three types of miscommunications that could happen in real-world dialogues but are underrepresented in the benchmark dataset: misunderstandings, non-understandings and vaguely related questions. Our two-step approach uses a state-of-the-art Large Language Model (LLM) to first create the error and secondly the repairing utterance. We perform Language Model-based evaluation to ensure the quality of the generated utterances. We apply the method to the MultiWOZ dataset and evaluate it both qualitatively and empirically as well as with human judges. Our results indicate that current LLMs can aid in adding post-hoc miscommunications to benchmark datasets as a form of data augmentation. We publish the resulting dataset, in which nearly 1900 dialogues have been modified, as CoPrUS-MultiWOZ to facilitate future work on dialogue systems.

**Link**: [arxiv](http://arxiv.org/abs/2412.07515v1),  [pdf](http://arxiv.org/pdf/2412.07515v1)

**Tags**: cs.CL 



### Enhancing 3D Object Detection in Autonomous Vehicles Based on Synthetic   Virtual Environment Analysis
**Authors**: Vladislav Li, Ilias Siniosoglou, Thomai Karamitsou, Anastasios Lytos, Ioannis D. Moscholios, Sotirios K. Goudos, Jyoti S. Banerjee, Panagiotis Sarigiannidi, Vasileios Argyriou

**Updated**: 2024-12-10T13:46:44Z

**Summary**: Autonomous Vehicles (AVs) use natural images and videos as input to understand the real world by overlaying and inferring digital elements, facilitating proactive detection in an effort to assure safety. A crucial aspect of this process is real-time, accurate object recognition through automatic scene analysis. While traditional methods primarily concentrate on 2D object detection, exploring 3D object detection, which involves projecting 3D bounding boxes into the three-dimensional environment, holds significance and can be notably enhanced using the AR ecosystem. This study examines an AI model's ability to deduce 3D bounding boxes in the context of real-time scene analysis while producing and evaluating the model's performance and processing time, in the virtual domain, which is then applied to AVs. This work also employs a synthetic dataset that includes artificially generated images mimicking various environmental, lighting, and spatiotemporal states. This evaluation is oriented in handling images featuring objects in diverse weather conditions, captured with varying camera settings. These variations pose more challenging detection and recognition scenarios, which the outcomes of this work can help achieve competitive results under most of the tested conditions.

**Link**: [arxiv](http://arxiv.org/abs/2412.07509v1),  [pdf](http://arxiv.org/pdf/2412.07509v1)

**Tags**: cs.CV 



### Distributed Optimization of Age of Incorrect Information with Dynamic   Epistemic Logic
**Authors**: Federico Chiariotti, Andrea Munari, Leonardo Badia, Petar Popovski

**Updated**: 2024-12-10T13:35:52Z

**Summary**: Distributed medium access schemes have a key advantage in anomaly tracking applications, as individual sensors know their own observations and can exploit them to reduce their Age of Incorrect Information (AoII). However, the risk of collisions has so far limited their performance. We present Dynamic Epistemic Logic for Tracking Anomalies (DELTA), a medium access protocol that limits collisions and minimizes AoII in anomaly reporting over dense networks. This is achieved by a process of inferring AoII from plain Age of Information (AoI). In a network scenario with randomly generated anomalies, the individual AoII for each sensor is known only to itself, but all nodes can infer its AoI by simply tracking the transmission process. Thus, we adopt an approach based on dynamic epistemic logic, which allows individual nodes to infer how their AoII values rank among the entire network by exploiting public information such as the AoI and the identity of transmitting nodes. We analyze the resulting DELTA protocol both from a theoretical standpoint and with Monte Carlo simulation, showing that our approach is significantly more efficient and robust than basic random access, while outperforming state-of-the-art scheduled schemes by at least 30%.

**Link**: [arxiv](http://arxiv.org/abs/2412.07503v1),  [pdf](http://arxiv.org/pdf/2412.07503v1)

**Tags**: cs.NI 94-06 C.2.2; H.1.1 



### EDGE: Unknown-aware Multi-label Learning by Energy Distribution Gap   Expansion
**Authors**: Yuchen Sun, Qianqian Xu, Zitai Wang, Zhiyong Yang, Junwei He

**Updated**: 2024-12-10T13:27:58Z

**Summary**: Multi-label Out-Of-Distribution (OOD) detection aims to discriminate the OOD samples from the multi-label In-Distribution (ID) ones. Compared with its multiclass counterpart, it is crucial to model the joint information among classes. To this end, JointEnergy, which is a representative multi-label OOD inference criterion, summarizes the logits of all the classes. However, we find that JointEnergy can produce an imbalance problem in OOD detection, especially when the model lacks enough discrimination ability. Specifically, we find that the samples only related to minority classes tend to be classified as OOD samples due to the ambiguous energy decision boundary. Besides, imbalanced multi-label learning methods, originally designed for ID ones, would not be suitable for OOD detection scenarios, even producing a serious negative transfer effect. In this paper, we resort to auxiliary outlier exposure (OE) and propose an unknown-aware multi-label learning framework to reshape the uncertainty energy space layout. In this framework, the energy score is separately optimized for tail ID samples and unknown samples, and the energy distribution gap between them is expanded, such that the tail ID samples can have a significantly larger energy score than the OOD ones. What's more, a simple yet effective measure is designed to select more informative OE datasets. Finally, comprehensive experimental results on multiple multi-label and OOD datasets reveal the effectiveness of the proposed method.

**Link**: [arxiv](http://arxiv.org/abs/2412.07499v1),  [pdf](http://arxiv.org/pdf/2412.07499v1)

**Tags**: cs.CV 



### Ontology-driven Prompt Tuning for LLM-based Task and Motion Planning
**Authors**: Muhayy Ud Din, Jan Rosell, Waseem Akram, Isiah Zaplana, Maximo A Roa, Lakmal Seneviratne, Irfan Hussain

**Updated**: 2024-12-10T13:18:45Z

**Summary**: Performing complex manipulation tasks in dynamic environments requires efficient Task and Motion Planning (TAMP) approaches, which combine high-level symbolic plan with low-level motion planning. Advances in Large Language Models (LLMs), such as GPT-4, are transforming task planning by offering natural language as an intuitive and flexible way to describe tasks, generate symbolic plans, and reason. However, the effectiveness of LLM-based TAMP approaches is limited due to static and template-based prompting, which struggles in adapting to dynamic environments and complex task contexts. To address these limitations, this work proposes a novel ontology-driven prompt-tuning framework that employs knowledge-based reasoning to refine and expand user prompts with task contextual reasoning and knowledge-based environment state descriptions. Integrating domain-specific knowledge into the prompt ensures semantically accurate and context-aware task plans. The proposed framework demonstrates its effectiveness by resolving semantic errors in symbolic plan generation, such as maintaining logical temporal goal ordering in scenarios involving hierarchical object placement. The proposed framework is validated through both simulation and real-world scenarios, demonstrating significant improvements over the baseline approach in terms of adaptability to dynamic environments, and the generation of semantically correct task plans.

**Link**: [arxiv](http://arxiv.org/abs/2412.07493v1),  [pdf](http://arxiv.org/pdf/2412.07493v1)

**Tags**: cs.RO cs.AI 



### Dual Random Fields and their Application to Mineral Potential Mapping
**Authors**: Álvaro I. Riquelme

**Updated**: 2024-12-10T13:16:37Z

**Summary**: In various geosciences branches, including mineral exploration, geometallurgical characterization on established mining operations, and remote sensing, the regionalized input variables are spatially well-sampled across the domain of interest, limiting the scope of spatial uncertainty quantification procedures. In turn, response outcomes such as the mineral potential in a given region, mining throughput, metallurgical recovery, or in-situ estimations from remote satellite imagery, are usually modeled from a much-restricted subset of testing samples, collected at certain locations due to accessibility restrictions and the high acquisition costs. Our limited understanding of these functions, in terms of the multi-dimensional complexity of causalities and unnoticed dependencies on inaccessible inputs, may lead to observing changes in such functions based on their geographical location. Pooling together different response functions across the domain is critical to correctly predict outcome responses, the uncertainty associated with these inferred values, and the significance of inputs in such predictions at unexplored areas. This paper introduces the notion of a dual random field (dRF), where the response function itself is considered a regionalized variable. In this way, different established response models across the geographic domain can be considered as observations of a dRF realization, enabling the spatial inference and uncertainty assessment of both response models and their predictions. We explain how dRFs inherit all the properties from classical random fields, allowing the use of standard Gaussian simulation procedures to simulate them. These models are combined to obtain a mineral potential response, providing an example of how to rigorously integrate machine learning approaches with geostatistics.

**Link**: [arxiv](http://arxiv.org/abs/2412.07488v1),  [pdf](http://arxiv.org/pdf/2412.07488v1)

**Tags**: stat.ML cs.LG stat.AP 



### Pushing ALMA to the limit: 140 pc resolution observations of a z=6.6   quasar-galaxy merger resolve strikingly different morphologies of dust   continuum and [CII] 158 um emission
**Authors**: Romain A. Meyer, Bram Venemans, Marcel Neeleman, Roberto Decarli, Fabian Walter

**Updated**: 2024-12-10T12:44:12Z

**Summary**: We present $0."026$ $(140\ \rm{pc})$ resolution ALMA observations of [C II] $158\ \mu\rm{m}$ and dust continuum emission of the $z=6.6$ quasar J0305--3150, resolved over $\sim 300-400$ independent resolution elements. The dust continuum emission is compact with $\sim 80\%$ recovered within $r<0."3$ $(1.6\ \rm{kpc})$, whereas the [C II] emission profile is composed of a central Gaussian ($r<0."4$, i.e. $<2.2\ \rm{kpc}$) and an extended component (detected up to $\sim 10\ \rm{kpc}$ at $>3\sigma$). We infer a direct contribution of the quasar to the observed 260\ \rm{GHz} continuum $S_{\nu,\rm{QSO}} / S_{\nu,\rm{QSO+Host}} \lesssim 1\%$. We report the detection of FIR-detected star-forming clumps with $r<200 \ \rm{pc}$ and properties similar to that of rest-frame UV-optical clumps reported in the literature. The $200\ \rm{pc}$ resolved [C II]/FIR ratio follows the global relation with the FIR surface brightness established in low- and high-redshift galaxies, even at the quasar location. We find that dust continuum is emitted in regions of $\sim0."02-0."04$ consistent with the size of photo-dissociation regions (PDR), whereas $50\%$ of the [C II] originates from larger physical scales ($\theta \gtrsim 2"$). The large-scale [C II] emission presents a velocity gradient aligned with a nearby companion with perturbed kinematics, and misaligned with the kinematics of the small-scale emission. The absence of significant [C II] emission by structures with physical scale $\lesssim 1\ \rm{kpc}$ implies that [C II] emission is not produced in dense PDR located at the boundary of Giant Molecular Clouds. We argue instead that [C II] is produced in low-density PDRs in the interstellar medium and diffuse HI gas tidally-stripped during the ongoing merger.

**Link**: [arxiv](http://arxiv.org/abs/2412.07474v1),  [pdf](http://arxiv.org/pdf/2412.07474v1)

**Tags**: astro-ph.GA 



### SQL Injection Jailbreak: a structural disaster of large language models
**Authors**: Jiawei Zhao, Kejiang Chen, Weiming Zhang, Nenghai Yu

**Updated**: 2024-12-10T12:14:50Z

**Summary**: In recent years, the rapid development of large language models (LLMs) has brought new vitality into various domains, generating substantial social and economic benefits. However, this swift advancement has also introduced new security vulnerabilities. Jailbreaking, a form of attack that induces LLMs to produce harmful content through carefully crafted prompts, presents a significant challenge to the safe and trustworthy development of LLMs. Previous jailbreak methods primarily exploited the internal properties or capabilities of LLMs, such as optimization-based jailbreak approaches and methods that leveraged the model's context-learning abilities. In this paper, we introduce a novel jailbreak method, SQL Injection Jailbreak (SIJ), which targets the external properties of LLMs, specifically, the way LLMs construct input prompts. By injecting jailbreak information into user prompts, SIJ successfully induces the model to output harmful content. Our SIJ method achieves near 100\% attack success rates on five well-known open-source LLMs on the AdvBench, while incurring lower time costs compared to previous methods. More importantly, SIJ is the first method to exploit the external properties of LLMs for jailbreak attacks and exposes a new vulnerability in LLMs that urgently requires mitigation. To address this, we propose a simple defense method called Self-Reminder-Key to counter SIJ and demonstrate its effectiveness through experimental results. Our code is available at \href{https://github.com/weiyezhimeng/SQL-Injection-Jailbreak}{https://github.com/weiyezhimeng/SQL-Injection-Jailbreak}.

**Link**: [arxiv](http://arxiv.org/abs/2411.01565v3),  [pdf](http://arxiv.org/pdf/2411.01565v3)

**Tags**: cs.CR 



### Dynamic Ensemble Reasoning for LLM Experts
**Authors**: Jinwu Hu, Yufeng Wang, Shuhai Zhang, Kai Zhou, Guohao Chen, Yu Hu, Bin Xiao, Mingkui Tan

**Updated**: 2024-12-10T12:05:56Z

**Summary**: Ensemble reasoning for the strengths of different LLM experts is critical to achieving consistent and satisfactory performance on diverse inputs across a wide range of tasks. However, existing LLM ensemble methods are either computationally intensive or incapable of leveraging complementary knowledge among LLM experts for various inputs. In this paper, we propose a Dynamic Ensemble Reasoning paradigm, called DER to integrate the strengths of multiple LLM experts conditioned on dynamic inputs. Specifically, we model the LLM ensemble reasoning problem as a Markov Decision Process (MDP), wherein an agent sequentially takes inputs to request knowledge from an LLM candidate and passes the output to a subsequent LLM candidate. Moreover, we devise a reward function to train a DER-Agent to dynamically select an optimal answering route given the input questions, aiming to achieve the highest performance with as few computational resources as possible. Last, to fully transfer the expert knowledge from the prior LLMs, we develop a Knowledge Transfer Prompt (KTP) that enables the subsequent LLM candidates to transfer complementary knowledge effectively. Experiments demonstrate that our method uses fewer computational resources to achieve better performance compared to state-of-the-art baselines.

**Link**: [arxiv](http://arxiv.org/abs/2412.07448v1),  [pdf](http://arxiv.org/pdf/2412.07448v1)

**Tags**: cs.AI 



### A computational framework for integrating Predictive processes with   evidence Accumulation Models (PAM)
**Authors**: Antonino Visalli, Francesco Maria Calistroni, Margherita Calderan, Francesco Donnarumma, Marco Zorzi, Ettore Ambrosini

**Updated**: 2024-12-10T12:05:44Z

**Summary**: Evidence Accumulation Models (EAMs) have been widely used to investigate speeded decision-making processes, but they have largely neglected the role of predictive processes emphasized by theories of the predictive brain. In this paper, we present the Predictive evidence Accumulation Models (PAM), a novel computational framework that integrates predictive processes into EAMs. Grounded in the "observing the observer" framework, PAM combines models of Bayesian perceptual inference, such as the Hierarchical Gaussian Filter, with three established EAMs (the Diffusion Decision Model, Lognormal Race Model, and Race Diffusion Model) to model decision-making under uncertainty. We validate PAM through parameter recovery simulations, demonstrating its accuracy and computational efficiency across various decision-making scenarios. Additionally, we provide a step-by-step tutorial using real data to illustrate PAM's application and discuss its theoretical implications. PAM represents a significant advancement in the computational modeling of decision-making, bridging the gap between predictive brain theories and EAMs, and offers a promising tool for future empirical research.

**Link**: [arxiv](http://arxiv.org/abs/2411.13203v2),  [pdf](http://arxiv.org/pdf/2411.13203v2)

**Tags**: stat.AP 



### Causal World Representation in the GPT Model
**Authors**: Raanan Y. Rohekar, Yaniv Gurwicz, Sungduk Yu, Vasudev Lal

**Updated**: 2024-12-10T12:05:03Z

**Summary**: Are generative pre-trained transformer (GPT) models only trained to predict the next token, or do they implicitly learn a world model from which a sequence is generated one token at a time? We examine this question by deriving a causal interpretation of the attention mechanism in GPT, and suggesting a causal world model that arises from this interpretation. Furthermore, we propose that GPT-models, at inference time, can be utilized for zero-shot causal structure learning for in-distribution sequences. Empirical evaluation is conducted in a controlled synthetic environment using the setup and rules of the Othello board game. A GPT, pre-trained on real-world games played with the intention of winning, is tested on synthetic data that only adheres to the game rules. We find that the GPT model tends to generate next moves that adhere to the game rules for sequences for which the attention mechanism encodes a causal structure with high confidence. In general, in cases for which the GPT model generates moves that do not adhere to the game rules, it also fails to capture any causal structure.

**Link**: [arxiv](http://arxiv.org/abs/2412.07446v1),  [pdf](http://arxiv.org/pdf/2412.07446v1)

**Tags**: cs.AI cs.CL cs.LG stat.ML 



### MobileSafetyBench: Evaluating Safety of Autonomous Agents in Mobile   Device Control
**Authors**: Juyong Lee, Dongyoon Hahm, June Suk Choi, W. Bradley Knox, Kimin Lee

**Updated**: 2024-12-10T11:56:09Z

**Summary**: Autonomous agents powered by large language models (LLMs) show promising potential in assistive tasks across various domains, including mobile device control. As these agents interact directly with personal information and device settings, ensuring their safe and reliable behavior is crucial to prevent undesirable outcomes. However, no benchmark exists for standardized evaluation of the safety of mobile device-control agents. In this work, we introduce MobileSafetyBench, a benchmark designed to evaluate the safety of device-control agents within a realistic mobile environment based on Android emulators. We develop a diverse set of tasks involving interactions with various mobile applications, including messaging and banking applications, challenging agents with managing risks encompassing misuse and negative side effects. These tasks include tests to evaluate the safety of agents in daily scenarios as well as their robustness against indirect prompt injection attacks. Our experiments demonstrate that baseline agents, based on state-of-the-art LLMs, often fail to effectively prevent harm while performing the tasks. To mitigate these safety concerns, we propose a prompting method that encourages agents to prioritize safety considerations. While this method shows promise in promoting safer behaviors, there is still considerable room for improvement to fully earn user trust. This highlights the urgent need for continued research to develop more robust safety mechanisms in mobile environments. We open-source our benchmark at: https://mobilesafetybench.github.io/.

**Link**: [arxiv](http://arxiv.org/abs/2410.17520v2),  [pdf](http://arxiv.org/pdf/2410.17520v2)

**Tags**: cs.LG cs.CL 



### BENet: A Cross-domain Robust Network for Detecting Face Forgeries via   Bias Expansion and Latent-space Attention
**Authors**: Weihua Liu, Jianhua Qiu, Said Boumaraf, Chaochao lin, Pan liyuan, Lin Li, Mohammed Bennamoun, Naoufel Werghi

**Updated**: 2024-12-10T11:41:55Z

**Summary**: In response to the growing threat of deepfake technology, we introduce BENet, a Cross-Domain Robust Bias Expansion Network. BENet enhances the detection of fake faces by addressing limitations in current detectors related to variations across different types of fake face generation techniques, where ``cross-domain" refers to the diverse range of these deepfakes, each considered a separate domain. BENet's core feature is a bias expansion module based on autoencoders. This module maintains genuine facial features while enhancing differences in fake reconstructions, creating a reliable bias for detecting fake faces across various deepfake domains. We also introduce a Latent-Space Attention (LSA) module to capture inconsistencies related to fake faces at different scales, ensuring robust defense against advanced deepfake techniques. The enriched LSA feature maps are multiplied with the expanded bias to create a versatile feature space optimized for subtle forgeries detection. To improve its ability to detect fake faces from unknown sources, BENet integrates a cross-domain detector module that enhances recognition accuracy by verifying the facial domain during inference. We train our network end-to-end with a novel bias expansion loss, adopted for the first time, in face forgery detection. Extensive experiments covering both intra and cross-dataset demonstrate BENet's superiority over current state-of-the-art solutions.

**Link**: [arxiv](http://arxiv.org/abs/2412.07431v1),  [pdf](http://arxiv.org/pdf/2412.07431v1)

**Tags**: cs.CV cs.AI 



### Optimizing Alignment with Less: Leveraging Data Augmentation for   Personalized Evaluation
**Authors**: Javad Seraj, Mohammad Mahdi Mohajeri, Mohammad Javad Dousti, Majid Nili Ahmadabadi

**Updated**: 2024-12-10T11:40:11Z

**Summary**: Automatic evaluation by large language models (LLMs) is a prominent topic today; however, judgment and evaluation tasks are often subjective and influenced by various factors, making adaptation challenging. While many studies demonstrate the capabilities of state-of-the-art proprietary LLMs in comparison to human evaluators, they often struggle to adapt to reference evaluators over time, a requirement for achieving personalized judgment. Additionally, numerous works have attempted to apply open LLMs as judges or evaluators, but these efforts frequently overlook the limitations of working with scarce data. Personalized judgment is inherently associated with limited data scenarios, which are common in many real-world problems. Our work aims to present a data augmentation technique to select a more effective sample from limited data in order to align an open LLM with human preference. Our work achieves approximately 7% improvements in Pearson correlation with a reference judge over the baseline,and 30% improvement over the base model (Llama3.1-8B-Instruct) in the mathematical reasoning evaluation task. demonstrating that augmenting selecting more effective preference data enables our approach to surpass baseline methods.

**Link**: [arxiv](http://arxiv.org/abs/2412.07429v1),  [pdf](http://arxiv.org/pdf/2412.07429v1)

**Tags**: cs.CL cs.AI 



### Using Large Language Models for Expert Prior Elicitation in Predictive   Modelling
**Authors**: Alexander Capstick, Rahul G. Krishnan, Payam Barnaghi

**Updated**: 2024-12-10T11:36:48Z

**Summary**: Large language models (LLMs), trained on diverse data effectively acquire a breadth of information across various domains. However, their computational complexity, cost, and lack of transparency hinder their direct application for specialised tasks. In fields such as clinical research, acquiring expert annotations or prior knowledge about predictive models is often costly and time-consuming. This study proposes the use of LLMs to elicit expert prior distributions for predictive models. This approach also provides an alternative to in-context learning, where language models are tasked with making predictions directly. In this work, we compare LLM-elicited and uninformative priors, evaluate whether LLMs truthfully generate parameter distributions, and propose a model selection strategy for in-context learning and prior elicitation. Our findings show that LLM-elicited prior parameter distributions significantly reduce predictive error compared to uninformative priors in low-data settings. Applied to clinical problems, this translates to fewer required biological samples, lowering cost and resources. Prior elicitation also consistently outperforms and proves more reliable than in-context learning at a lower cost, making it a preferred alternative in our setting. We demonstrate the utility of this method across various use cases, including clinical applications. For infection prediction, using LLM-elicited priors reduced the number of required labels to achieve the same accuracy as an uninformative prior by 55%, 200 days earlier in the study.

**Link**: [arxiv](http://arxiv.org/abs/2411.17284v2),  [pdf](http://arxiv.org/pdf/2411.17284v2)

**Tags**: cs.LG cs.CL stat.ML 



### Is Self-knowledge and Action Consistent or Not: Investigating Large   Language Model's Personality
**Authors**: Yiming Ai, Zhiwei He, Ziyin Zhang, Wenhong Zhu, Hongkun Hao, Kai Yu, Lingjun Chen, Rui Wang

**Updated**: 2024-12-10T11:26:52Z

**Summary**: In this study, we delve into the validity of conventional personality questionnaires in capturing the human-like personality traits of Large Language Models (LLMs). Our objective is to assess the congruence between the personality traits LLMs claim to possess and their demonstrated tendencies in real-world scenarios. By conducting an extensive examination of LLM outputs against observed human response patterns, we aim to understand the disjunction between self-knowledge and action in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2402.14679v2),  [pdf](http://arxiv.org/pdf/2402.14679v2)

**Tags**: cs.CL cs.CY 



### Beyond Search Engines: Can Large Language Models Improve Curriculum   Development?
**Authors**: Mohammad Moein, Mohammadreza Molavi Hajiagha, Abdolali Faraji, Mohammadreza Tavakoli, Gàbor Kismihòk

**Updated**: 2024-12-10T11:21:16Z

**Summary**: While Online Learning is growing and becoming widespread, the associated curricula often suffer from a lack of coverage and outdated content. In this regard, a key question is how to dynamically define the topics that must be covered to thoroughly learn a subject (e.g., a course). Large Language Models (LLMs) are considered candidates that can be used to address curriculum development challenges. Therefore, we developed a framework and a novel dataset, built on YouTube, to evaluate LLMs' performance when it comes to generating learning topics for specific courses. The experiment was conducted across over 100 courses and nearly 7,000 YouTube playlists in various subject areas. Our results indicate that GPT-4 can produce more accurate topics for the given courses than extracted topics from YouTube video playlists in terms of BERTScore

**Link**: [arxiv](http://arxiv.org/abs/2412.07422v1),  [pdf](http://arxiv.org/pdf/2412.07422v1)

**Tags**: cs.CY 



### Generating Knowledge Graphs from Large Language Models: A Comparative   Study of GPT-4, LLaMA 2, and BERT
**Authors**: Ahan Bhatt, Nandan Vaghela, Kush Dudhia

**Updated**: 2024-12-10T11:05:26Z

**Summary**: Knowledge Graphs (KGs) are essential for the functionality of GraphRAGs, a form of Retrieval-Augmented Generative Systems (RAGs) that excel in tasks requiring structured reasoning and semantic understanding. However, creating KGs for GraphRAGs remains a significant challenge due to accuracy and scalability limitations of traditional methods. This paper introduces a novel approach leveraging large language models (LLMs) like GPT-4, LLaMA 2 (13B), and BERT to generate KGs directly from unstructured data, bypassing traditional pipelines. Using metrics such as Precision, Recall, F1-Score, Graph Edit Distance, and Semantic Similarity, we evaluate the models' ability to generate high-quality KGs. Results demonstrate that GPT-4 achieves superior semantic fidelity and structural accuracy, LLaMA 2 excels in lightweight, domain-specific graphs, and BERT provides insights into challenges in entity-relationship modeling. This study underscores the potential of LLMs to streamline KG creation and enhance GraphRAG accessibility for real-world applications, while setting a foundation for future advancements.

**Link**: [arxiv](http://arxiv.org/abs/2412.07412v1),  [pdf](http://arxiv.org/pdf/2412.07412v1)

**Tags**: cs.CL cs.AI cs.DB 



### MoDULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task   Learning
**Authors**: Yufei Ma, Zihan Liang, Huangyu Dai, Ben Chen, Dehong Gao, Zhuoran Ran, Wang Zihan, Linbo Jin, Wen Jiang, Guannan Zhang, Xiaoyan Cai, Libin Yang

**Updated**: 2024-12-10T10:55:57Z

**Summary**: The growing demand for larger-scale models in the development of \textbf{L}arge \textbf{L}anguage \textbf{M}odels (LLMs) poses challenges for efficient training within limited computational resources. Traditional fine-tuning methods often exhibit instability in multi-task learning and rely heavily on extensive training resources. Here, we propose MoDULA (\textbf{M}ixture \textbf{o}f \textbf{D}omain-Specific and \textbf{U}niversal \textbf{L}oR\textbf{A}), a novel \textbf{P}arameter \textbf{E}fficient \textbf{F}ine-\textbf{T}uning (PEFT) \textbf{M}ixture-\textbf{o}f-\textbf{E}xpert (MoE) paradigm for improved fine-tuning and parameter efficiency in multi-task learning. The paradigm effectively improves the multi-task capability of the model by training universal experts, domain-specific experts, and routers separately. MoDULA-Res is a new method within the MoDULA paradigm, which maintains the model's general capability by connecting universal and task-specific experts through residual connections. The experimental results demonstrate that the overall performance of the MoDULA-Flan and MoDULA-Res methods surpasses that of existing fine-tuning methods on various LLMs. Notably, MoDULA-Res achieves more significant performance improvements in multiple tasks while reducing training costs by over 80\% without losing general capability. Moreover, MoDULA displays flexible pluggability, allowing for the efficient addition of new tasks without retraining existing experts from scratch. This progressive training paradigm circumvents data balancing issues, enhancing training efficiency and model stability. Overall, MoDULA provides a scalable, cost-effective solution for fine-tuning LLMs with enhanced parameter efficiency and generalization capability.

**Link**: [arxiv](http://arxiv.org/abs/2412.07405v1),  [pdf](http://arxiv.org/pdf/2412.07405v1)

**Tags**: cs.LG cs.AI 



### Benchmarking Chinese Commonsense Reasoning of LLMs: From   Chinese-Specifics to Reasoning-Memorization Correlations
**Authors**: Jiaxing Sun, Weiquan Huang, Jiang Wu, Chenya Gu, Wei Li, Songyang Zhang, Hang Yan, Conghui He

**Updated**: 2024-12-10T10:43:50Z

**Summary**: We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense. We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies for improving LLMs' reasoning ability, such as Chain-of-Thought. Our findings indicate that the LLM's language orientation and the task's domain influence the effectiveness of the prompt strategy, which enriches previous research findings. We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance. We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors. Our study precisely identified the LLMs' strengths and weaknesses, providing the clear direction for optimization. It can also serve as a reference for studies in other fields. We will release CHARM at https://github.com/opendatalab/CHARM .

**Link**: [arxiv](http://arxiv.org/abs/2403.14112v3),  [pdf](http://arxiv.org/pdf/2403.14112v3)

**Tags**: cs.CL 



### Active Inference Tree Search in Large POMDPs
**Authors**: Domenico Maisto, Francesco Gregoretti, Karl Friston, Giovanni Pezzulo

**Updated**: 2024-12-10T10:35:53Z

**Summary**: The ability to plan ahead efficiently is key for both living organisms and artificial systems. Model-based planning and prospection are widely studied in cognitive neuroscience and artificial intelligence (AI), but from different perspectives--and with different desiderata in mind (biological realism versus scalability) that are difficult to reconcile. Here, we introduce a novel method to plan in POMDPs--Active Inference Tree Search (AcT)--that combines the normative character and biological realism of a leading planning theory in neuroscience (Active Inference) and the scalability of tree search methods in AI. This unification enhances both approaches. On the one hand, tree searches enable the biologically grounded, first principle method of active inference to be applied to large-scale problems. On the other hand, active inference provides a principled solution to the exploration-exploitation dilemma, which is often addressed heuristically in tree search methods. Our simulations show that AcT successfully navigates binary trees that are challenging for sampling-based methods, problems that require adaptive exploration, and the large POMDP problem 'RockSample'--in which AcT reproduces state-of-the-art POMDP solutions. Furthermore, we illustrate how AcT can be used to simulate neurophysiological responses (e.g., in the hippocampus and prefrontal cortex) of humans and other animals that solve large planning problems. These numerical analyses show that Active Tree Search is a principled realisation of neuroscientific and AI planning theories, which offer both biological realism and scalability.

**Link**: [arxiv](http://arxiv.org/abs/2103.13860v6),  [pdf](http://arxiv.org/pdf/2103.13860v6)

**Tags**: cs.AI math.PR q-bio.NC 68T20, 68Q07, 68W27, 90C40 I.2; I.6; G.3; J.4 



### CMT: A Memory Compression Method for Continual Knowledge Learning of   Large Language Models
**Authors**: Dongfang Li, Zetian Sun, Xinshuo Hu, Baotian Hu, Min Zhang

**Updated**: 2024-12-10T10:35:19Z

**Summary**: Large Language Models (LLMs) need to adapt to the continuous changes in data, tasks, and user preferences. Due to their massive size and the high costs associated with training, LLMs are not suitable for frequent retraining. However, updates are necessary to keep them in sync with rapidly evolving human knowledge. To address these challenges, this paper proposes the Compression Memory Training (CMT) method, an efficient and effective online adaptation framework for LLMs that features robust knowledge retention capabilities. Inspired by human memory mechanisms, CMT compresses and extracts information from new documents to be stored in a memory bank. When answering to queries related to these new documents, the model aggregates these document memories from the memory bank to better answer user questions. The parameters of the LLM itself do not change during training and inference, reducing the risk of catastrophic forgetting. To enhance the encoding, retrieval, and aggregation of memory, we further propose three new general and flexible techniques, including memory-aware objective, self-matching and top-aggregation. Extensive experiments conducted on three continual learning datasets (i.e., StreamingQA, SQuAD and ArchivalQA) demonstrate that the proposed method improves model adaptability and robustness across multiple base LLMs (e.g., +4.07 EM & +4.19 F1 in StreamingQA with Llama-2-7b).

**Link**: [arxiv](http://arxiv.org/abs/2412.07393v1),  [pdf](http://arxiv.org/pdf/2412.07393v1)

**Tags**: cs.CL cs.AI 



### Post-Training Non-Uniform Quantization for Convolutional Neural Networks
**Authors**: Ahmed Luqman, Khuzemah Qazi, Imdadullah Khan

**Updated**: 2024-12-10T10:33:58Z

**Summary**: Despite the success of CNN models on a variety of Image classification and segmentation tasks, their extensive computational and storage demands pose considerable challenges for real-world deployment on resource constrained devices. Quantization is one technique that aims to alleviate these large storage requirements and speed up the inference process by reducing the precision of model parameters to lower-bit representations. In this paper, we introduce a novel post-training quantization method for model weights. Our method finds optimal clipping thresholds and scaling factors along with mathematical guarantees that our method minimizes quantization noise. Empirical results on Real World Datasets demonstrate that our quantization scheme significantly reduces model size and computational requirements while preserving model accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2412.07391v1),  [pdf](http://arxiv.org/pdf/2412.07391v1)

**Tags**: cs.CV cs.LG 



### Hard-normal Example-aware Template Mutual Matching for Industrial   Anomaly Detection
**Authors**: Zixuan Chen, Xiaohua Xie, Lingxiao Yang, Jianhuang Lai

**Updated**: 2024-12-10T10:31:26Z

**Summary**: Anomaly detectors are widely used in industrial manufacturing to detect and localize unknown defects in query images. These detectors are trained on anomaly-free samples and have successfully distinguished anomalies from most normal samples. However, hard-normal examples are scattered and far apart from most normal samples, and thus they are often mistaken for anomalies by existing methods. To address this issue, we propose Hard-normal Example-aware Template Mutual Matching (HETMM), an efficient framework to build a robust prototype-based decision boundary. Specifically, HETMM employs the proposed Affine-invariant Template Mutual Matching (ATMM) to mitigate the affection brought by the affine transformations and easy-normal examples. By mutually matching the pixel-level prototypes within the patch-level search spaces between query and template set, ATMM can accurately distinguish between hard-normal examples and anomalies, achieving low false-positive and missed-detection rates. In addition, we also propose PTS to compress the original template set for speed-up. PTS selects cluster centres and hard-normal examples to preserve the original decision boundary, allowing this tiny set to achieve comparable performance to the original one. Extensive experiments demonstrate that HETMM outperforms state-of-the-art methods, while using a 60-sheet tiny set can achieve competitive performance and real-time inference speed (around 26.1 FPS) on a Quadro 8000 RTX GPU. HETMM is training-free and can be hot-updated by directly inserting novel samples into the template set, which can promptly address some incremental learning issues in industrial manufacturing.

**Link**: [arxiv](http://arxiv.org/abs/2303.16191v5),  [pdf](http://arxiv.org/pdf/2303.16191v5)

**Tags**: cs.CV 



### Temporal Linear Item-Item Model for Sequential Recommendation
**Authors**: Seongmin Park, Mincheol Yoon, Minjin Choi, Jongwuk Lee

**Updated**: 2024-12-10T10:28:32Z

**Summary**: In sequential recommendation (SR), neural models have been actively explored due to their remarkable performance, but they suffer from inefficiency inherent to their complexity. On the other hand, linear SR models exhibit high efficiency and achieve competitive or superior accuracy compared to neural models. However, they solely deal with the sequential order of items (i.e., sequential information) and overlook the actual timestamp (i.e., temporal information). It is limited to effectively capturing various user preference drifts over time. To address this issue, we propose a novel linear SR model, named TemporAl LinEar item-item model (TALE), incorporating temporal information while preserving training/inference efficiency, with three key components. (i) Single-target augmentation concentrates on a single target item, enabling us to learn the temporal correlation for the target item. (ii) Time interval-aware weighting utilizes the actual timestamp to discern the item correlation depending on time intervals. (iii) Trend-aware normalization reflects the dynamic shift of item popularity over time. Our empirical studies show that TALE outperforms ten competing SR models by up to 18.71% gains on five benchmark datasets. It also exhibits remarkable effectiveness in evaluating long-tail items by up to 30.45% gains. The source code is available at https://github.com/psm1206/TALE.

**Link**: [arxiv](http://arxiv.org/abs/2412.07382v1),  [pdf](http://arxiv.org/pdf/2412.07382v1)

**Tags**: cs.IR cs.LG 



### SpecFuse: Ensembling Large Language Models via Next-Segment Prediction
**Authors**: Bo Lv, Chen Tang, Yanan Zhang, Xin Liu, Yue Yu, Ping Luo

**Updated**: 2024-12-10T10:27:41Z

**Summary**: Ensembles of generative large language models (LLMs) can integrate the strengths of different LLMs to compensate for the limitations of individual models. However, recent work has focused on training an additional fusion model to combine complete responses from multiple LLMs, failing to tap into their collaborative potential to generate higher-quality responses. Moreover, as the additional fusion model is trained on a specialized dataset, these methods struggle with generalizing to open-domain queries from online users. In this paper, we propose SpecFuse, a novel ensemble framework that outputs the fused result by iteratively producing the next segment through collaboration among LLMs. This is achieved through cyclic execution of its inference and verification components. In each round, the inference component invokes each base LLM to generate candidate segments in parallel, and the verify component calls these LLMs again to predict the ranking of the segments. The top-ranked segment is then broadcast to all LLMs, encouraging them to generate higher-quality segments in the next round. This approach also allows the base LLMs to be plug-and-play, without any training or adaptation, avoiding generalization limitations. Furthermore, to conserve computational resources, we propose a model exit mechanism that dynamically excludes models exhibiting poor performance in previous rounds during each query response. In this way, it effectively reduces the number of model calls while maintaining overall performance.

**Link**: [arxiv](http://arxiv.org/abs/2412.07380v1),  [pdf](http://arxiv.org/pdf/2412.07380v1)

**Tags**: cs.CL cs.AI 



### Completeness in static analysis by abstract interpretation, a personal   point of view
**Authors**: David Monniaux

**Updated**: 2024-12-10T10:21:04Z

**Summary**: Static analysis by abstract interpretation is generally designed to be "sound", that is, it should not claim to establish properties that do not hold-in other words, not provide "false negatives" about possible bugs. A rarer requirement is that it should be "complete", meaning that it should be able to infer certain properties if they hold. This paper describes a number of practical issues and questions related to completeness that I have come across over the years.

**Link**: [arxiv](http://arxiv.org/abs/2211.09572v3),  [pdf](http://arxiv.org/pdf/2211.09572v3)

**Tags**: cs.PL 



### Mixture of Hidden-Dimensions Transformer
**Authors**: Yilong Chen, Junyuan Shang, Zhengyu Zhang, Jiawei Sheng, Tingwen Liu, Shuohuan Wang, Yu Sun, Hua Wu, Haifeng Wang

**Updated**: 2024-12-10T10:13:59Z

**Summary**: Transformer models encounter challenges in scaling hidden dimensions efficiently, as uniformly increasing them inflates computational and memory costs while failing to emphasize the most relevant features for each token. For further understanding, we study hidden dimension sparsity and observe that trained Transformers utilize only a small fraction of token dimensions, revealing an "activation flow" pattern. Notably, there are shared sub-dimensions with sustained activation across multiple consecutive tokens and specialized sub-dimensions uniquely activated for each token. To better model token-relevant sub-dimensions, we propose MoHD (Mixture of Hidden Dimensions), a sparse conditional activation architecture. Particularly, MoHD employs shared sub-dimensions for common token features and a routing mechanism to dynamically activate specialized sub-dimensions. To mitigate potential information loss from sparsity, we design activation scaling and group fusion mechanisms to preserve activation flow. In this way, MoHD expands hidden dimensions with negligible increases in computation or parameters, efficient training and inference while maintaining performance. Evaluations across 10 NLP tasks show that MoHD surpasses Vanilla Transformers in parameter efficiency and task performance. It achieves 1.7% higher performance with 50% fewer activation parameters and 3.7% higher performance with a 3x parameter expansion at constant activation cost. MOHD offers a new perspective for scaling the model, showcasing the potential of hidden dimension sparsity to boost efficiency

**Link**: [arxiv](http://arxiv.org/abs/2412.05644v2),  [pdf](http://arxiv.org/pdf/2412.05644v2)

**Tags**: cs.CL 



### ITPNet: Towards Instantaneous Trajectory Prediction for Autonomous   Driving
**Authors**: Rongqing Li, Changsheng Li, Yuhang Li, Hanjie Li, Yi Chen, Dongchun Ren, Ye Yuan, Guoren Wang

**Updated**: 2024-12-10T10:09:41Z

**Summary**: Trajectory prediction of agents is crucial for the safety of autonomous vehicles, whereas previous approaches usually rely on sufficiently long-observed trajectory to predict the future trajectory of the agents. However, in real-world scenarios, it is not realistic to collect adequate observed locations for moving agents, leading to the collapse of most prediction models. For instance, when a moving car suddenly appears and is very close to an autonomous vehicle because of the obstruction, it is quite necessary for the autonomous vehicle to quickly and accurately predict the future trajectories of the car with limited observed trajectory locations. In light of this, we focus on investigating the task of instantaneous trajectory prediction, i.e., two observed locations are available during inference. To this end, we propose a general and plug-and-play instantaneous trajectory prediction approach, called ITPNet. Specifically, we propose a backward forecasting mechanism to reversely predict the latent feature representations of unobserved historical trajectories of the agent based on its two observed locations and then leverage them as complementary information for future trajectory prediction. Meanwhile, due to the inevitable existence of noise and redundancy in the predicted latent feature representations, we further devise a Noise Redundancy Reduction Former, aiming at to filter out noise and redundancy from unobserved trajectories and integrate the filtered features and observed features into a compact query for future trajectory predictions. In essence, ITPNet can be naturally compatible with existing trajectory prediction models, enabling them to gracefully handle the case of instantaneous trajectory prediction. Extensive experiments on the Argoverse and nuScenes datasets demonstrate ITPNet outperforms the baselines, and its efficacy with different trajectory prediction models.

**Link**: [arxiv](http://arxiv.org/abs/2412.07369v1),  [pdf](http://arxiv.org/pdf/2412.07369v1)

**Tags**: cs.CV 



### Efficient Massive Black Hole Binary parameter estimation for LISA using   Sequential Neural Likelihood
**Authors**: Iván Martín Vílchez, Carlos F. Sopuerta

**Updated**: 2024-12-10T10:00:34Z

**Summary**: The inspiral, merger, and ringdown of Massive Black Hole Binaries (MBHBs) is one the main sources of Gravitational Waves (GWs) for the future Laser Interferometer Space Antenna (LISA), an ESA-led mission in the implementation phase. It is expected that LISA will detect these systems throughout the entire observable universe. Robust and efficient data analysis algorithms are necessary to detect and estimate physical parameters for these systems. In this work, we explore the application of Sequential Neural Likelihood, a simulation-based inference algorithm, to detect and characterize MBHB GW signals in synthetic LISA data. We describe in detail the different elements of the method, their performance and possible alternatives that can be used to enhance the performance. Instead of sampling from the conventional likelihood function, which requires a forward simulation for each evaluation, this method constructs a surrogate likelihood that is ultimately described by a neural network trained from a dataset of simulations of the MBHB signals and noise. One important advantage of this method is that, given that the likelihood is independent of the priors, we can iteratively train models that target specific observations in a fraction of the time and computational cost that other traditional and machine learning-based strategies would require. Because of the iterative nature of the method, we are able to train models to obtain qualitatively similar posteriors with less than 2\% of the simulator calls that Markov Chain Monte Carlo methods would require. We compare these posteriors with those obtained from Markov Chain Monte Carlo techniques and discuss the differences that appear, in particular in relation with the important role that data compression has in the modular implementation of the method that we present.

**Link**: [arxiv](http://arxiv.org/abs/2406.00565v2),  [pdf](http://arxiv.org/pdf/2406.00565v2)

**Tags**: gr-qc astro-ph.HE astro-ph.IM 



### Software Model Evolution with Large Language Models: Experiments on   Simulated, Public, and Industrial Datasets
**Authors**: Christof Tinnes, Alisa Welter, Sven Apel

**Updated**: 2024-12-10T09:54:43Z

**Summary**: Modeling structure and behavior of software systems plays a crucial role in the industrial practice of software engineering. As with other software engineering artifacts, software models are subject to evolution. Supporting modelers in evolving software models with recommendations for model completions is still an open problem, though. In this paper, we explore the potential of large language models for this task. In particular, we propose an approach, RAMC, leveraging large language models, model histories, and retrieval-augmented generation for model completion. Through experiments on three datasets, including an industrial application, one public open-source community dataset, and one controlled collection of simulated model repositories, we evaluate the potential of large language models for model completion with RAMC. We found that large language models are indeed a promising technology for supporting software model evolution (62.30% semantically correct completions on real-world industrial data and up to 86.19% type-correct completions). The general inference capabilities of large language models are particularly useful when dealing with concepts for which there are few, noisy, or no examples at all.

**Link**: [arxiv](http://arxiv.org/abs/2406.17651v5),  [pdf](http://arxiv.org/pdf/2406.17651v5)

**Tags**: cs.SE cs.AI 94-04 D.2.2 



### Towards Predictive Communication with Brain-Computer Interfaces   integrating Large Language Models
**Authors**: Andrea Caria

**Updated**: 2024-12-10T09:48:07Z

**Summary**: This perspective article aims at providing an outline of the state of the art and future developments towards the integration of cutting-edge predictive language models with BCI. A synthetic overview of early and more recent linguistic models, from natural language processing (NLP) models to recent LLM, that to a varying extent improved predictive writing systems, is first provided. Second, a summary of previous BCI implementations integrating language models is presented. The few preliminary studies investigating the possible combination of LLM with BCI spellers to efficiently support fast communication and control are then described. Finally, current challenges and limitations towards the full integration of LLM with BCI systems are discussed. Recent investigations suggest that the combination of LLM with BCI might drastically improve human-computer interaction in patients with motor or language disorders as well as in healthy individuals. In particular, the pretrained autoregressive transformer models, such as GPT, that capitalize from parallelization, learning through pre-training and fine-tuning, promise a substantial improvement of BCI for communication with respect to previous systems incorporating simpler language models. Indeed, among various models, the GPT-2 was shown to represent an excellent candidate for its integration into BCI although testing was only perfomed on simulated conversations and not on real BCI scenarios. Prospectively, the full integration of LLM with advanced BCI systems might lead to a big leap forward towards fast, efficient and user-adaptive neurotechnology.

**Link**: [arxiv](http://arxiv.org/abs/2412.07355v1),  [pdf](http://arxiv.org/pdf/2412.07355v1)

**Tags**: cs.HC cs.CL 



### Inference after discretizing unobserved heterogeneity
**Authors**: Jad Beyhum, Martin Mugnier

**Updated**: 2024-12-10T09:43:34Z

**Summary**: We consider a linear panel data model with nonseparable two-way unobserved heterogeneity corresponding to a linear version of the model studied in Bonhomme et al. (2022). We show that inference is possible in this setting using a straightforward two-step estimation procedure inspired by existing discretization approaches. In the first step, we construct a discrete approximation of the unobserved heterogeneity by (k-means) clustering observations separately across the individual ($i$) and time ($t$) dimensions. In the second step, we estimate a linear model with two-way group fixed effects specific to each cluster. Our approach shares similarities with methods from the double machine learning literature, as the underlying moment conditions exhibit the same type of bias-reducing properties. We provide a theoretical analysis of a cross-fitted version of our estimator, establishing its asymptotic normality at parametric rate under the condition $\max(N,T)=o(\min(N,T)^3)$. Simulation studies demonstrate that our methodology achieves excellent finite-sample performance, even when $T$ is negligible with respect to $N$.

**Link**: [arxiv](http://arxiv.org/abs/2412.07352v1),  [pdf](http://arxiv.org/pdf/2412.07352v1)

**Tags**: econ.EM 



### Model independent bounds on heavy sterile neutrinos from the angular   distribution of $\mathbf{B\to D^*\ellν}$ decays
**Authors**: Florian U. Bernlochner, Marco Fedele, Tim Kretz, Ulrich Nierste, Markus T. Prim

**Updated**: 2024-12-10T09:27:01Z

**Summary**: In this paper we study the bounds that can be inferred on New Physics couplings to heavy sterile neutrinos $N$ from the recent measurements performed by the Belle collaboration of the angular analysis of $B\to D^*\ell\bar\nu_\ell$ decays, with $\ell=e,\mu$. Indeed, a sterile neutrino $N$ may lead to competing $B\to D^*\ell\bar N$ decays and Belle might have measured an incoherent sum of these two independent channels. After reviewing the theoretical formalism required to describe this phenomenon in full generality, we first perform a bump hunt in the $M_{\rm miss}^2$ Belle distribution to search for evidences of an additional massive neutrino. We found in such a way a small hint at $M_{\rm miss}^2 \sim (350\ {\rm MeV})^2$. However, the Belle angular analysis is sensitive to $N$ masses up to $\mathcal{O}$(50 MeV), preventing us to further inspect this hint. Nevertheless, we study the potential impact of this additional channel in the allowed mass range on the measured angular distributions and extract model-independent bounds on the new-physics couplings which could mediate such an interaction. In particular, in the mass window here inspected, we obtain the most stringent bounds for vector and left-handed scalar operators to date.

**Link**: [arxiv](http://arxiv.org/abs/2410.11945v2),  [pdf](http://arxiv.org/pdf/2410.11945v2)

**Tags**: hep-ph hep-ex 



### Frame Representation Hypothesis: Multi-Token LLM Interpretability and   Concept-Guided Text Generation
**Authors**: Pedro H. V. Valois, Lincon S. Souza, Erica K. Shimomoto, Kazuhiro Fukui

**Updated**: 2024-12-10T09:25:39Z

**Summary**: Interpretability is a key challenge in fostering trust for Large Language Models (LLMs), which stems from the complexity of extracting reasoning from model's parameters. We present the Frame Representation Hypothesis, a theoretically robust framework grounded in the Linear Representation Hypothesis (LRH) to interpret and control LLMs by modeling multi-token words. Prior research explored LRH to connect LLM representations with linguistic concepts, but was limited to single token analysis. As most words are composed of several tokens, we extend LRH to multi-token words, thereby enabling usage on any textual data with thousands of concepts. To this end, we propose words can be interpreted as frames, ordered sequences of vectors that better capture token-word relationships. Then, concepts can be represented as the average of word frames sharing a common concept. We showcase these tools through Top-k Concept-Guided Decoding, which can intuitively steer text generation using concepts of choice. We verify said ideas on Llama 3.1, Gemma 2, and Phi 3 families, demonstrating gender and language biases, exposing harmful content, but also potential to remediate them, leading to safer and more transparent LLMs. Code is available at https://github.com/phvv-me/frame-representation-hypothesis.git

**Link**: [arxiv](http://arxiv.org/abs/2412.07334v1),  [pdf](http://arxiv.org/pdf/2412.07334v1)

**Tags**: cs.CL 



### Label Distribution Learning using the Squared Neural Family on the   Probability Simplex
**Authors**: Daokun Zhang, Russell Tsuchida, Dino Sejdinovic

**Updated**: 2024-12-10T09:12:02Z

**Summary**: Label distribution learning (LDL) provides a framework wherein a distribution over categories rather than a single category is predicted, with the aim of addressing ambiguity in labeled data. Existing research on LDL mainly focuses on the task of point estimation, i.e., pinpointing an optimal distribution in the probability simplex conditioned on the input sample. In this paper, we estimate a probability distribution of all possible label distributions over the simplex, by unleashing the expressive power of the recently introduced Squared Neural Family (SNEFY). With the modeled distribution, label distribution prediction can be achieved by performing the expectation operation to estimate the mean of the distribution of label distributions. Moreover, more information about the label distribution can be inferred, such as the prediction reliability and uncertainties. We conduct extensive experiments on the label distribution prediction task, showing that our distribution modeling based method can achieve very competitive label distribution prediction performance compared with the state-of-the-art baselines. Additional experiments on active learning and ensemble learning demonstrate that our probabilistic approach can effectively boost the performance in these settings, by accurately estimating the prediction reliability and uncertainties.

**Link**: [arxiv](http://arxiv.org/abs/2412.07324v1),  [pdf](http://arxiv.org/pdf/2412.07324v1)

**Tags**: cs.LG 



### ConceptSearch: Towards Efficient Program Search Using LLMs for   Abstraction and Reasoning Corpus (ARC)
**Authors**: Kartik Singhal, Gautam Shroff

**Updated**: 2024-12-10T09:10:11Z

**Summary**: The Abstraction and Reasoning Corpus (ARC) poses a significant challenge to artificial intelligence, demanding broad generalization and few-shot learning capabilities that remain elusive for current deep learning methods, including large language models (LLMs). While LLMs excel in program synthesis, their direct application to ARC yields limited success. To address this, we introduce ConceptSearch, a novel function-search algorithm that leverages LLMs for program generation and employs a concept-based scoring method to guide the search efficiently. Unlike simplistic pixel-based metrics like Hamming distance, ConceptSearch evaluates programs on their ability to capture the underlying transformation concept reflected in the input-output examples. We explore three scoring functions: Hamming distance, a CNN-based scoring function, and an LLM-based natural language scoring function. Experimental results demonstrate the effectiveness of ConceptSearch, achieving a significant performance improvement over direct prompting with GPT-4. Moreover, our novel concept-based scoring exhibits up to 30% greater efficiency compared to Hamming distance, measured in terms of the number of iterations required to reach the correct solution. These findings highlight the potential of LLM-driven program search when integrated with concept-based guidance for tackling challenging generalization problems like ARC. Code: https://github.com/kksinghal/concept-search

**Link**: [arxiv](http://arxiv.org/abs/2412.07322v1),  [pdf](http://arxiv.org/pdf/2412.07322v1)

**Tags**: cs.LG 



### Deciphering the spectral properties of the atypical radio relic in A115   using uGMRT, VLA, and LOFAR
**Authors**: Swarna Chatterjee, Abhirup Datta

**Updated**: 2024-12-10T09:06:57Z

**Summary**: The mega-parsec scale radio relics at the galaxy cluster periphery are intriguing structures. While textbook examples of relics posit arc-like elongated structures at the clusters' peripheries, several relics display more complex structures deviating from the conventional type. Abell 115 is a galaxy cluster, hosting an atypical radio relic at its northern periphery. Despite the multi-wavelength study of the cluster over the last decades, the origin of the radio relic is still unclear. In this paper, we present a multi-frequency radio study of the cluster to infer the possible mechanism behind the formation of the radio relic. We used new 400 MHz observations with the uGMRT, along with archival VLA 1.5 GHz observations and archival LOFAR 144 MHz observations. Our analysis supports the previous theory on the relic's origin from the passage of a shock front due to an off-axis merger, where the old population of particles from the radio galaxies at the relic location has been re-energised to illuminate the 2 Mpc radio relic.

**Link**: [arxiv](http://arxiv.org/abs/2409.07574v3),  [pdf](http://arxiv.org/pdf/2409.07574v3)

**Tags**: astro-ph.CO 



### Preserving Speaker Information in Direct Speech-to-Speech Translation   with Non-Autoregressive Generation and Pretraining
**Authors**: Rui Zhoua, Akinori Itoa, Takashi Nosea

**Updated**: 2024-12-10T08:58:51Z

**Summary**: Speech-to-Speech Translation (S2ST) refers to the conversion of speech in one language into semantically equivalent speech in another language, facilitating communication between speakers of different languages. Speech-to-Discrete Unit Translation (S2UT), a mainstream approach for end-to-end S2ST, addresses challenges such as error propagation across modules and slow inference speed often encountered in traditional cascade systems. However, as discrete units primarily capture content information, conventional S2UT methods fail to retain speaker-specific characteristics from the source. Our previous work, SC-S2UT, introduced a speaker adapter and a unit-to-mel structure, enabling the preservation of speaker information and non-autoregressive speech generation. Building on this foundation, this study proposes a self-supervised pretraining method to enrich the information extracted by both the speaker adapter and the unit-to-mel structure. Additionally, we investigate different feature fusion strategies to further improve the integration of speaker and content features. Experiments conducted on the CVSS-T dataset for ES-EN and FR-EN tasks demonstrate that our proposed method achieves a BLEU score improvement of 1.14 compared to SC-S2UT, along with significant enhancements in MOS and speaker similarity. Furthermore, our approach achieves translation quality comparable to traditional S2UT, with only a minimal increase of 0.04s per utterance in inference time, while maintaining high speaker similarity. These results validate the effectiveness of the proposed method.

**Link**: [arxiv](http://arxiv.org/abs/2412.07316v1),  [pdf](http://arxiv.org/pdf/2412.07316v1)

**Tags**: cs.SD cs.MM eess.AS 



### Gearing Gaussian process modeling and sequential design towards   stochastic simulators
**Authors**: Mickael Binois, Arindam Fadikar, Abby Stevens

**Updated**: 2024-12-10T08:37:48Z

**Summary**: This chapter presents specific aspects of Gaussian process modeling in the presence of complex noise. Starting from the standard homoscedastic model, various generalizations from the literature are presented: input varying noise variance, non-Gaussian noise, or quantile modeling. These approaches are compared in terms of goal, data availability and inference procedure. A distinction is made between methods depending on their handling of repeated observations at the same location, also called replication. The chapter concludes with the corresponding adaptations of the sequential design procedures. These are illustrated in an example from epidemiology.

**Link**: [arxiv](http://arxiv.org/abs/2412.07306v1),  [pdf](http://arxiv.org/pdf/2412.07306v1)

**Tags**: math.OC stat.ML 



### Bidirectional Mamba state-space model for anomalous diffusion
**Authors**: Maxime Lavaud, Yosef Shokeeb, Juliette Lacherez, Yacine Amarouchene, Thomas Salez

**Updated**: 2024-12-10T08:29:38Z

**Summary**: Characterizing anomalous diffusion is crucial in order to understand the evolution of complex stochastic systems, from molecular interactions to cellular dynamics. In this work, we characterize the performances regarding such a task of Bi-Mamba, a novel state-space deep-learning architecture articulated with a bidirectional scan mechanism. Our implementation is tested on the AnDi-2 challenge datasets among others. Designed for regression tasks, the Bi-Mamba architecture infers efficiently the effective diffusion coefficient and anomalous exponent from single, short trajectories. As such, our results indicate the potential practical use of the Bi-Mamba architecture for anomalousdiffusion characterization.

**Link**: [arxiv](http://arxiv.org/abs/2412.07299v1),  [pdf](http://arxiv.org/pdf/2412.07299v1)

**Tags**: cond-mat.soft physics.bio-ph physics.optics stat.ML 



### The Rise and Down of Babel Tower: Investigating the Evolution Process of   Multilingual Code Large Language Model
**Authors**: Jiawei Chen, Wentao Chen, Jing Su, Jingjing Xu, Hongyu Lin, Mengjie Ren, Yaojie Lu, Xianpei Han, Le Sun

**Updated**: 2024-12-10T08:28:57Z

**Summary**: Large language models (LLMs) have shown significant multilingual capabilities. However, the mechanisms underlying the development of these capabilities during pre-training are not well understood. In this paper, we use code LLMs as an experimental platform to explore the evolution of multilingual capabilities in LLMs during the pre-training process. Based on our observations, we propose the Babel Tower Hypothesis, which describes the entire process of LLMs acquiring new language capabilities. During the learning process, multiple languages initially share a single knowledge system dominated by the primary language and gradually develop language-specific knowledge systems. We then validate the above hypothesis by tracking the internal states of the LLMs through identifying working languages and language transferring neurons. Experimental results show that the internal state changes of the LLM are consistent with our Babel Tower Hypothesis. Building on these insights, we propose a novel method to construct an optimized pre-training corpus for multilingual code LLMs, which significantly outperforms LLMs trained on the original corpus. The proposed Babel Tower Hypothesis provides new insights into designing pre-training data distributions to achieve optimal multilingual capabilities in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2412.07298v1),  [pdf](http://arxiv.org/pdf/2412.07298v1)

**Tags**: cs.CL 



## Keyword: LLM Deployment 
 ### BiMediX2: Bio-Medical EXpert LMM for Diverse Medical Modalities
**Authors**: Sahal Shaji Mullappilly, Mohammed Irfan Kurpath, Sara Pieri, Saeed Yahya Alseiari, Shanavas Cholakkal, Khaled Aldahmani, Fahad Khan, Rao Anwer, Salman Khan, Timothy Baldwin, Hisham Cholakkal

**Updated**: 2024-12-10T18:59:35Z

**Summary**: This paper introduces BiMediX2, a bilingual (Arabic-English) Bio-Medical EXpert Large Multimodal Model (LMM) with a unified architecture that integrates text and visual modalities, enabling advanced image understanding and medical applications. BiMediX2 leverages the Llama3.1 architecture and integrates text and visual capabilities to facilitate seamless interactions in both English and Arabic, supporting text-based inputs and multi-turn conversations involving medical images. The model is trained on an extensive bilingual healthcare dataset consisting of 1.6M samples of diverse medical interactions for both text and image modalities, mixed in Arabic and English. We also propose the first bilingual GPT-4o based medical LMM benchmark named BiMed-MBench. BiMediX2 is benchmarked on both text-based and image-based tasks, achieving state-of-the-art performance across several medical benchmarks. It outperforms recent state-of-the-art models in medical LLM evaluation benchmarks. Our model also sets a new benchmark in multimodal medical evaluations with over 9% improvement in English and over 20% in Arabic evaluations. Additionally, it surpasses GPT-4 by around 9% in UPHILL factual accuracy evaluations and excels in various medical Visual Question Answering, Report Generation, and Report Summarization tasks. The project page including source code and the trained model, is available at https://github.com/mbzuai-oryx/BiMediX2.

**Link**: [arxiv](http://arxiv.org/abs/2412.07769v1),  [pdf](http://arxiv.org/pdf/2412.07769v1)

**Tags**: cs.CV 



### Test-time Correction with Human Feedback: An Online 3D Detection System   via Visual Prompting
**Authors**: Zetong Yang, Hanxue Zhang, Yanan Sun, Li Chen, Fei Xia, Fatma Guney, Hongyang Li

**Updated**: 2024-12-10T18:59:32Z

**Summary**: This paper introduces Test-time Correction (TTC) system, a novel online 3D detection system designated for online correction of test-time errors via human feedback, to guarantee the safety of deployed autonomous driving systems. Unlike well-studied offline 3D detectors frozen at inference, TTC explores the capability of instant online error rectification. By leveraging user feedback with interactive prompts at a frame, e.g., a simple click or draw of boxes, TTC could immediately update the corresponding detection results for future streaming inputs, even though the model is deployed with fixed parameters. This enables autonomous driving systems to adapt to new scenarios immediately and decrease deployment risks reliably without additional expensive training. To achieve such TTC system, we equip existing 3D detectors with Online Adapter (OA) module, a prompt-driven query generator for online correction. At the core of OA module are visual prompts, images of missed object-of-interest for guiding the corresponding detection and subsequent tracking. Those visual prompts, belonging to missed objects through online inference, are maintained by the visual prompt buffer for continuous error correction in subsequent frames. By doing so, TTC consistently detects online missed objects and immediately lowers driving risks. It achieves reliable, versatile, and adaptive driving autonomy. Extensive experiments demonstrate significant gain on instant error rectification over pre-trained 3D detectors, even in challenging scenarios with limited labels, zero-shot detection, and adverse conditions. We hope this work would inspire the community to investigate online rectification systems for autonomous driving post-deployment. Code would be publicly shared.

**Link**: [arxiv](http://arxiv.org/abs/2412.07768v1),  [pdf](http://arxiv.org/pdf/2412.07768v1)

**Tags**: cs.CV 



### SAT: Spatial Aptitude Training for Multimodal Language Models
**Authors**: Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan A. Plummer, Ranjay Krishna, Kuo-Hao Zeng, Kate Saenko

**Updated**: 2024-12-10T18:52:45Z

**Summary**: Spatial perception is a fundamental component of intelligence. While many studies highlight that large multimodal language models (MLMs) struggle to reason about space, they only test for static spatial reasoning, such as categorizing the relative positions of objects. Meanwhile, real-world deployment requires dynamic capabilities like perspective-taking and egocentric action recognition. As a roadmap to improving spatial intelligence, we introduce SAT, Spatial Aptitude Training, which goes beyond static relative object position questions to the more dynamic tasks. SAT contains 218K question-answer pairs for 22K synthetic scenes across a training and testing set. Generated using a photo-realistic physics engine, our dataset can be arbitrarily scaled and easily extended to new actions, scenes, and 3D assets. We find that even MLMs that perform relatively well on static questions struggle to accurately answer dynamic spatial questions. Further, we show that SAT instruction-tuning data improves not only dynamic spatial reasoning on SAT, but also zero-shot performance on existing real-image spatial benchmarks: $23\%$ on CVBench, $8\%$ on the harder BLINK benchmark, and $18\%$ on VSR. When instruction-tuned on SAT, our 13B model matches larger proprietary MLMs like GPT4-V and Gemini-3-1.0 in spatial reasoning. Our data/code is available at http://arijitray1993.github.io/SAT/ .

**Link**: [arxiv](http://arxiv.org/abs/2412.07755v1),  [pdf](http://arxiv.org/pdf/2412.07755v1)

**Tags**: cs.CV cs.AI cs.GR cs.RO 



### Beyond Retrieval: Generating Narratives in Conversational Recommender   Systems
**Authors**: Krishna Sayana, Raghavendra Vasudeva, Yuri Vasilevski, Kun Su, Liam Hebert, James Pine, Hubert Pham, Ambarish Jash, Sukhdeep Sodhi

**Updated**: 2024-12-10T18:45:18Z

**Summary**: The recent advances in Large Language Model's generation and reasoning capabilities present an opportunity to develop truly conversational recommendation systems. However, effectively integrating recommender system knowledge into LLMs for natural language generation which is tailored towards recommendation tasks remains a challenge. This paper addresses this challenge by making two key contributions.   First, we introduce a new dataset (REGEN) for natural language generation tasks in conversational recommendations. REGEN (Reviews Enhanced with GEnerative Narratives) extends the Amazon Product Reviews dataset with rich user narratives, including personalized explanations of product preferences, product endorsements for recommended items, and summaries of user purchase history. REGEN is made publicly available to facilitate further research. Furthermore, we establish benchmarks using well-known generative metrics, and perform an automated evaluation of the new dataset using a rater LLM. Second, the paper introduces a fusion architecture (CF model with an LLM) which serves as a baseline for REGEN. And to the best of our knowledge, represents the first attempt to analyze the capabilities of LLMs in understanding recommender signals and generating rich narratives. We demonstrate that LLMs can effectively learn from simple fusion architectures utilizing interaction-based CF embeddings, and this can be further enhanced using the metadata and personalization data associated with items. Our experiments show that combining CF and content embeddings leads to improvements of 4-12% in key language metrics compared to using either type of embedding individually. We also provide an analysis to interpret how CF and content embeddings contribute to this new generative task.

**Link**: [arxiv](http://arxiv.org/abs/2410.16780v2),  [pdf](http://arxiv.org/pdf/2410.16780v2)

**Tags**: cs.CL cs.AI cs.IR cs.LG 



### Zero-Shot ATC Coding with Large Language Models for Clinical Assessments
**Authors**: Zijian Chen, John-Michael Gamble, Micaela Jantzi, John P. Hirdes, Jimmy Lin

**Updated**: 2024-12-10T18:43:02Z

**Summary**: Manual assignment of Anatomical Therapeutic Chemical (ATC) codes to prescription records is a significant bottleneck in healthcare research and operations at Ontario Health and InterRAI Canada, requiring extensive expert time and effort. To automate this process while maintaining data privacy, we develop a practical approach using locally deployable large language models (LLMs). Inspired by recent advances in automatic International Classification of Diseases (ICD) coding, our method frames ATC coding as a hierarchical information extraction task, guiding LLMs through the ATC ontology level by level. We evaluate our approach using GPT-4o as an accuracy ceiling and focus development on open-source Llama models suitable for privacy-sensitive deployment. Testing across Health Canada drug product data, the RABBITS benchmark, and real clinical notes from Ontario Health, our method achieves 78% exact match accuracy with GPT-4o and 60% with Llama 3.1 70B. We investigate knowledge grounding through drug definitions, finding modest improvements in accuracy. Further, we show that fine-tuned Llama 3.1 8B matches zero-shot Llama 3.1 70B accuracy, suggesting that effective ATC coding is feasible with smaller models. Our results demonstrate the feasibility of automatic ATC coding in privacy-sensitive healthcare environments, providing a foundation for future deployments.

**Link**: [arxiv](http://arxiv.org/abs/2412.07743v1),  [pdf](http://arxiv.org/pdf/2412.07743v1)

**Tags**: cs.CL 



### M3TR: Generalist HD Map Construction with Variable Map Priors
**Authors**: Fabian Immel, Richard Fehler, Frank Bieder, Jan-Hendrik Pauls, Christoph Stiller

**Updated**: 2024-12-10T18:41:14Z

**Summary**: Autonomous vehicles require road information for their operation, usually in form of HD maps. Since offline maps eventually become outdated or may only be partially available, online HD map construction methods have been proposed to infer map information from live sensor data. A key issue remains how to exploit such partial or outdated map information as a prior. We introduce M3TR (Multi-Masking Map Transformer), a generalist approach for HD map construction both with and without map priors. We address shortcomings in ground truth generation for Argoverse 2 and nuScenes and propose the first realistic scenarios with semantically diverse map priors. Examining various query designs, we use an improved method for integrating prior map elements into a HD map construction model, increasing performance by +4.3 mAP. Finally, we show that training across all prior scenarios yields a single Generalist model, whose performance is on par with previous Expert models that can handle only one specific type of map prior. M3TR thus is the first model capable of leveraging variable map priors, making it suitable for real-world deployment. Code is available at https://github.com/immel-f/m3tr

**Link**: [arxiv](http://arxiv.org/abs/2411.10316v2),  [pdf](http://arxiv.org/pdf/2411.10316v2)

**Tags**: cs.CV cs.RO 



### The BrowserGym Ecosystem for Web Agent Research
**Authors**: Thibault Le Sellier De Chezelles, Maxime Gasse, Alexandre Drouin, Massimo Caccia, Léo Boisvert, Megh Thakkar, Tom Marty, Rim Assouel, Sahar Omidi Shayegan, Lawrence Keunho Jang, Xing Han Lù, Ori Yoran, Dehan Kong, Frank F. Xu, Siva Reddy, Quentin Cappart, Graham Neubig, Ruslan Salakhutdinov, Nicolas Chapados, Alexandre Lacoste

**Updated**: 2024-12-10T18:28:46Z

**Summary**: The BrowserGym ecosystem addresses the growing need for efficient evaluation and benchmarking of web agents, particularly those leveraging automation and Large Language Models (LLMs) for web interaction tasks. Many existing benchmarks suffer from fragmentation and inconsistent evaluation methodologies, making it challenging to achieve reliable comparisons and reproducible results. BrowserGym aims to solve this by providing a unified, gym-like environment with well-defined observation and action spaces, facilitating standardized evaluation across diverse benchmarks. Combined with AgentLab, a complementary framework that aids in agent creation, testing, and analysis, BrowserGym offers flexibility for integrating new benchmarks while ensuring consistent evaluation and comprehensive experiment management. This standardized approach seeks to reduce the time and complexity of developing web agents, supporting more reliable comparisons and facilitating in-depth analysis of agent behaviors, and could result in more adaptable, capable agents, ultimately accelerating innovation in LLM-driven automation. As a supporting evidence, we conduct the first large-scale, multi-benchmark web agent experiment and compare the performance of 6 state-of-the-art LLMs across all benchmarks currently available in BrowserGym. Among other findings, our results highlight a large discrepancy between OpenAI and Anthropic's latests models, with Claude-3.5-Sonnet leading the way on almost all benchmarks, except on vision-related tasks where GPT-4o is superior. Despite these advancements, our results emphasize that building robust and efficient web agents remains a significant challenge, due to the inherent complexity of real-world web environments and the limitations of current models.

**Link**: [arxiv](http://arxiv.org/abs/2412.05467v2),  [pdf](http://arxiv.org/pdf/2412.05467v2)

**Tags**: cs.LG cs.AI cs.SE 



### MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion
**Authors**: Sai Shashank Kalakonda, Shubh Maheshwari, Ravi Kiran Sarvadevabhatla

**Updated**: 2024-12-10T18:24:13Z

**Summary**: We introduce MoRAG, a novel multi-part fusion based retrieval-augmented generation strategy for text-based human motion generation. The method enhances motion diffusion models by leveraging additional knowledge obtained through an improved motion retrieval process. By effectively prompting large language models (LLMs), we address spelling errors and rephrasing issues in motion retrieval. Our approach utilizes a multi-part retrieval strategy to improve the generalizability of motion retrieval across the language space. We create diverse samples through the spatial composition of the retrieved motions. Furthermore, by utilizing low-level, part-specific motion information, we can construct motion samples for unseen text descriptions. Our experiments demonstrate that our framework can serve as a plug-and-play module, improving the performance of motion diffusion models. Code, pretrained models and sample videos are available at: https://motion-rag.github.io/

**Link**: [arxiv](http://arxiv.org/abs/2409.12140v2),  [pdf](http://arxiv.org/pdf/2409.12140v2)

**Tags**: cs.CV cs.MM 



### Toward Self-Improvement of LLMs via Imagination, Searching, and   Criticizing
**Authors**: Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, Dong Yu

**Updated**: 2024-12-10T18:19:29Z

**Summary**: Despite the impressive capabilities of Large Language Models (LLMs) on various tasks, they still struggle with scenarios that involves complex reasoning and planning. Recent work proposed advanced prompting techniques and the necessity of fine-tuning with high-quality data to augment LLMs' reasoning abilities. However, these approaches are inherently constrained by data availability and quality. In light of this, self-correction and self-learning emerge as viable solutions, employing strategies that allow LLMs to refine their outputs and learn from self-assessed rewards. Yet, the efficacy of LLMs in self-refining its response, particularly in complex reasoning and planning task, remains dubious. In this paper, we introduce AlphaLLM for the self-improvements of LLMs, which integrates Monte Carlo Tree Search (MCTS) with LLMs to establish a self-improving loop, thereby enhancing the capabilities of LLMs without additional annotations. Drawing inspiration from the success of AlphaGo, AlphaLLM addresses the unique challenges of combining MCTS with LLM for self-improvement, including data scarcity, the vastness search spaces of language tasks, and the subjective nature of feedback in language tasks. AlphaLLM is comprised of prompt synthesis component, an efficient MCTS approach tailored for language tasks, and a trio of critic models for precise feedback. Our experimental results in mathematical reasoning tasks demonstrate that AlphaLLM significantly enhances the performance of LLMs without additional annotations, showing the potential for self-improvement in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2404.12253v2),  [pdf](http://arxiv.org/pdf/2404.12253v2)

**Tags**: cs.CL cs.LG 



### Granite Guardian
**Authors**: Inkit Padhi, Manish Nagireddy, Giandomenico Cornacchia, Subhajit Chaudhury, Tejaswini Pedapati, Pierre Dognin, Keerthiram Murugesan, Erik Miehling, Martín Santillán Cooper, Kieran Fraser, Giulio Zizzo, Muhammad Zaid Hameed, Mark Purcell, Michael Desmond, Qian Pan, Inge Vejsbjerg, Elizabeth M. Daly, Michael Hind, Werner Geyer, Ambrish Rawat, Kush R. Varshney, Prasanna Sattigeri

**Updated**: 2024-12-10T18:17:02Z

**Summary**: We introduce the Granite Guardian models, a suite of safeguards designed to provide risk detection for prompts and responses, enabling safe and responsible use in combination with any large language model (LLM). These models offer comprehensive coverage across multiple risk dimensions, including social bias, profanity, violence, sexual content, unethical behavior, jailbreaking, and hallucination-related risks such as context relevance, groundedness, and answer relevance for retrieval-augmented generation (RAG). Trained on a unique dataset combining human annotations from diverse sources and synthetic data, Granite Guardian models address risks typically overlooked by traditional risk detection models, such as jailbreaks and RAG-specific issues. With AUC scores of 0.871 and 0.854 on harmful content and RAG-hallucination-related benchmarks respectively, Granite Guardian is the most generalizable and competitive model available in the space. Released as open-source, Granite Guardian aims to promote responsible AI development across the community.   https://github.com/ibm-granite/granite-guardian

**Link**: [arxiv](http://arxiv.org/abs/2412.07724v1),  [pdf](http://arxiv.org/pdf/2412.07724v1)

**Tags**: cs.CL 



### Scalable Influence and Fact Tracing for Large Language Model Pretraining
**Authors**: Tyler A. Chang, Dheeraj Rajagopal, Tolga Bolukbasi, Lucas Dixon, Ian Tenney

**Updated**: 2024-12-10T17:59:38Z

**Summary**: Training data attribution (TDA) methods aim to attribute model outputs back to specific training examples, and the application of these methods to large language model (LLM) outputs could significantly advance model transparency and data curation. However, it has been challenging to date to apply these methods to the full scale of LLM pretraining. In this paper, we refine existing gradient-based methods to work effectively at scale, allowing us to retrieve influential examples for an 8B-parameter language model from a pretraining corpus of over 160B tokens with no need for subsampling or pre-filtering. Our method combines several techniques, including optimizer state correction, a task-specific Hessian approximation, and normalized encodings, which we find to be critical for performance at scale. In quantitative evaluations on a fact tracing task, our method performs best at identifying examples that influence model predictions, but classical, model-agnostic retrieval methods such as BM25 still perform better at finding passages which explicitly contain relevant facts. These results demonstrate a misalignment between factual *attribution* and causal *influence*. With increasing model size and training tokens, we find that influence more closely aligns with factual attribution. Finally, we examine different types of examples identified as influential by our method, finding that while many directly entail a particular fact, others support the same output by reinforcing priors on relation types, common entities, and names. We release our prompt set and model outputs, along with a web-based visualization tool to explore influential examples for factual predictions, commonsense reasoning, arithmetic, and open-ended generation for an 8B-parameter LLM.

**Link**: [arxiv](http://arxiv.org/abs/2410.17413v2),  [pdf](http://arxiv.org/pdf/2410.17413v2)

**Tags**: cs.CL 



### An Entailment Tree Generation Approach for Multimodal Multi-Hop Question   Answering with Mixture-of-Experts and Iterative Feedback Mechanism
**Authors**: Qing Zhang, Haocheng Lv, Jie Liu, Zhiyun Chen, Jianyong Duan, Hao Wang, Li He, Mingying Xv

**Updated**: 2024-12-10T17:42:49Z

**Summary**: With the rise of large-scale language models (LLMs), it is currently popular and effective to convert multimodal information into text descriptions for multimodal multi-hop question answering. However, we argue that the current methods of multi-modal multi-hop question answering still mainly face two challenges: 1) The retrieved evidence containing a large amount of redundant information, inevitably leads to a significant drop in performance due to irrelevant information misleading the prediction. 2) The reasoning process without interpretable reasoning steps makes the model difficult to discover the logical errors for handling complex questions. To solve these problems, we propose a unified LLMs-based approach but without heavily relying on them due to the LLM's potential errors, and innovatively treat multimodal multi-hop question answering as a joint entailment tree generation and question answering problem. Specifically, we design a multi-task learning framework with a focus on facilitating common knowledge sharing across interpretability and prediction tasks while preventing task-specific errors from interfering with each other via mixture of experts. Afterward, we design an iterative feedback mechanism to further enhance both tasks by feeding back the results of the joint training to the LLM for regenerating entailment trees, aiming to iteratively refine the potential answer. Notably, our method has won the first place in the official leaderboard of WebQA (since April 10, 2024), and achieves competitive results on MultimodalQA.

**Link**: [arxiv](http://arxiv.org/abs/2412.05821v2),  [pdf](http://arxiv.org/pdf/2412.05821v2)

**Tags**: cs.CL cs.AI 



### Privacy-Preserving Customer Support: A Framework for Secure and Scalable   Interactions
**Authors**: Anant Prakash Awasthi, Chandraketu Singh, Rakshit Varma, Sanchit Sharma

**Updated**: 2024-12-10T17:20:47Z

**Summary**: The growing reliance on artificial intelligence (AI) in customer support has significantly improved operational efficiency and user experience. However, traditional machine learning (ML) approaches, which require extensive local training on sensitive datasets, pose substantial privacy risks and compliance challenges with regulations like the General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA). Existing privacy-preserving techniques, such as anonymization, differential privacy, and federated learning, address some concerns but face limitations in utility, scalability, and complexity. This paper introduces the Privacy-Preserving Zero-Shot Learning (PP-ZSL) framework, a novel approach leveraging large language models (LLMs) in a zero-shot learning mode. Unlike conventional ML methods, PP-ZSL eliminates the need for local training on sensitive data by utilizing pre-trained LLMs to generate responses directly. The framework incorporates real-time data anonymization to redact or mask sensitive information, retrieval-augmented generation (RAG) for domain-specific query resolution, and robust post-processing to ensure compliance with regulatory standards. This combination reduces privacy risks, simplifies compliance, and enhances scalability and operational efficiency. Empirical analysis demonstrates that the PP-ZSL framework provides accurate, privacy-compliant responses while significantly lowering the costs and complexities of deploying AI-driven customer support systems. The study highlights potential applications across industries, including financial services, healthcare, e-commerce, legal support, telecommunications, and government services. By addressing the dual challenges of privacy and performance, this framework establishes a foundation for secure, efficient, and regulatory-compliant AI applications in customer interactions.

**Link**: [arxiv](http://arxiv.org/abs/2412.07687v1),  [pdf](http://arxiv.org/pdf/2412.07687v1)

**Tags**: cs.LG cs.CR stat.AP 



### TRIM: Token Reduction and Inference Modeling for Cost-Effective Language   Generation
**Authors**: Alfredo Garrachón Ruiz, Tomás de la Rosa, Daniel Borrajo

**Updated**: 2024-12-10T17:13:35Z

**Summary**: The inference cost of Large Language Models (LLMs) is a significant challenge due to their computational demands, specially on tasks requiring long outputs. However, natural language often contains redundancy, which presents an opportunity for optimization. We have observed that LLMs can generate distilled language-concise outputs that retain essential meaning, when prompted appropriately. We propose a framework for saving computational cost, in which a shorter distilled output from the LLM is reconstructed into a full narrative by a smaller model with lower inference costs. Our experiments show promising results, particularly in general knowledge domains with 20.58% saved tokens on average with tiny decrease in evaluation metrics, hinting that this approach can effectively balance efficiency and accuracy in language processing tasks.

**Link**: [arxiv](http://arxiv.org/abs/2412.07682v1),  [pdf](http://arxiv.org/pdf/2412.07682v1)

**Tags**: cs.CL 



### RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text   Rewriting
**Authors**: Shuo Yang, Bardh Prenkaj, Gjergji Kasneci

**Updated**: 2024-12-10T17:02:58Z

**Summary**: Despite the widespread use of LLMs due to their superior performance in various tasks, their high computational costs often lead potential users to opt for the pretraining-finetuning pipeline. However, biases prevalent in manually constructed datasets can introduce spurious correlations between tokens and labels, creating so-called shortcuts and hindering the generalizability of fine-tuned models. Existing debiasing methods often rely on prior knowledge of specific dataset biases, which is challenging to acquire a priori. We propose RAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised, and data-focused debiasing approach based on text rewriting for shortcut mitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased text segments by replacing them with heuristically selected alternatives in a shortcut space defined by token statistics and positional information. This process aims to align surface-level text features more closely with diverse label distributions, thereby promoting the learning of genuine linguistic patterns. Compared with unsupervised SoTA models, RAZOR improves by 3.5% on the FEVER and 6.5% on MNLI and SNLI datasets according to the F1 score. Additionally, RAZOR effectively mitigates specific known biases, reducing bias-related terms by x2 without requiring prior bias information, a result that is on par with SoTA models that leverage prior information. Our work prioritizes data manipulation over architectural modifications, emphasizing the pivotal role of data quality in enhancing model performance and fairness. This research contributes to developing more robust evaluation benchmarks for debiasing methods by incorporating metrics for bias reduction and overall model efficacy.

**Link**: [arxiv](http://arxiv.org/abs/2412.07675v1),  [pdf](http://arxiv.org/pdf/2412.07675v1)

**Tags**: cs.CL cs.LG 



### Ask Humans or AI? Exploring Their Roles in Visualization Troubleshooting
**Authors**: Shuyu Shen, Sirong Lu, Leixian Shen, Zhonghua Sheng, Nan Tang, Yuyu Luo

**Updated**: 2024-12-10T17:02:41Z

**Summary**: Visualization authoring is an iterative process requiring users to modify parameters like color schemes and data transformations to achieve desired aesthetics and effectively convey insights. Due to the complexity of these adjustments, users often create defective visualizations and require troubleshooting support. In this paper, we examine two primary approaches for visualization troubleshooting: (1) Human-assisted support via forums, where users receive advice from other individuals, and (2) AI-assisted support using large language models (LLMs). Our goal is to understand the strengths and limitations of each approach in supporting visualization troubleshooting tasks. To this end, we collected 889 Vega-Lite cases from Stack Overflow. We then conducted a comprehensive analysis to understand the types of questions users ask, the effectiveness of human and AI guidance, and the impact of supplementary resources, such as documentation and examples, on troubleshooting outcomes. Our findings reveal a striking contrast between human- and AI-assisted troubleshooting: Human-assisted troubleshooting provides tailored, context-sensitive advice but often varies in response quality, while AI-assisted troubleshooting offers rapid feedback but often requires additional contextual resources to achieve desired results.

**Link**: [arxiv](http://arxiv.org/abs/2412.07673v1),  [pdf](http://arxiv.org/pdf/2412.07673v1)

**Tags**: cs.HC 



### FlexLLM: Exploring LLM Customization for Moving Target Defense on   Black-Box LLMs Against Jailbreak Attacks
**Authors**: Bocheng Chen, Hanqing Guo, Qiben Yan

**Updated**: 2024-12-10T17:02:28Z

**Summary**: Defense in large language models (LLMs) is crucial to counter the numerous attackers exploiting these systems to generate harmful content through manipulated prompts, known as jailbreak attacks. Although many defense strategies have been proposed, they often require access to the model's internal structure or need additional training, which is impractical for service providers using LLM APIs, such as OpenAI APIs or Claude APIs. In this paper, we propose a moving target defense approach that alters decoding hyperparameters to enhance model robustness against various jailbreak attacks. Our approach does not require access to the model's internal structure and incurs no additional training costs. The proposed defense includes two key components: (1) optimizing the decoding strategy by identifying and adjusting decoding hyperparameters that influence token generation probabilities, and (2) transforming the decoding hyperparameters and model system prompts into dynamic targets, which are continuously altered during each runtime. By continuously modifying decoding strategies and prompts, the defense effectively mitigates the existing attacks. Our results demonstrate that our defense is the most effective against jailbreak attacks in three of the models tested when using LLMs as black-box APIs. Moreover, our defense offers lower inference costs and maintains comparable response quality, making it a potential layer of protection when used alongside other defense methods.

**Link**: [arxiv](http://arxiv.org/abs/2412.07672v1),  [pdf](http://arxiv.org/pdf/2412.07672v1)

**Tags**: cs.CR cs.CL 



### Automating Business Intelligence Requirements with Generative AI and   Semantic Search
**Authors**: Nimrod Busany, Ethan Hadar, Hananel Hadad, Gil Rosenblum, Zofia Maszlanka, Okhaide Akhigbe, Daniel Amyot

**Updated**: 2024-12-10T16:57:48Z

**Summary**: Eliciting requirements for Business Intelligence (BI) systems remains a significant challenge, particularly in changing business environments. This paper introduces a novel AI-driven system, called AutoBIR, that leverages semantic search and Large Language Models (LLMs) to automate and accelerate the specification of BI requirements. The system facilitates intuitive interaction with stakeholders through a conversational interface, translating user inputs into prototype analytic code, descriptions, and data dependencies. Additionally, AutoBIR produces detailed test-case reports, optionally enhanced with visual aids, streamlining the requirement elicitation process. By incorporating user feedback, the system refines BI reporting and system design, demonstrating practical applications for expediting data-driven decision-making. This paper explores the broader potential of generative AI in transforming BI development, illustrating its role in enhancing data engineering practice for large-scale, evolving systems.

**Link**: [arxiv](http://arxiv.org/abs/2412.07668v1),  [pdf](http://arxiv.org/pdf/2412.07668v1)

**Tags**: cs.SE 



### AdaSociety: An Adaptive Environment with Social Structures for   Multi-Agent Decision-Making
**Authors**: Yizhe Huang, Xingbo Wang, Hao Liu, Fanqi Kong, Aoyang Qin, Min Tang, Xiaoxi Wang, Song-Chun Zhu, Mingjie Bi, Siyuan Qi, Xue Feng

**Updated**: 2024-12-10T16:41:12Z

**Summary**: Traditional interactive environments limit agents' intelligence growth with fixed tasks. Recently, single-agent environments address this by generating new tasks based on agent actions, enhancing task diversity. We consider the decision-making problem in multi-agent settings, where tasks are further influenced by social connections, affecting rewards and information access. However, existing multi-agent environments lack a combination of adaptive physical surroundings and social connections, hindering the learning of intelligent behaviors. To address this, we introduce AdaSociety, a customizable multi-agent environment featuring expanding state and action spaces, alongside explicit and alterable social structures. As agents progress, the environment adaptively generates new tasks with social structures for agents to undertake. In AdaSociety, we develop three mini-games showcasing distinct social structures and tasks. Initial results demonstrate that specific social structures can promote both individual and collective benefits, though current reinforcement learning and LLM-based algorithms show limited effectiveness in leveraging social structures to enhance performance. Overall, AdaSociety serves as a valuable research platform for exploring intelligence in diverse physical and social settings. The code is available at https://github.com/bigai-ai/AdaSociety.

**Link**: [arxiv](http://arxiv.org/abs/2411.03865v2),  [pdf](http://arxiv.org/pdf/2411.03865v2)

**Tags**: cs.MA cs.AI cs.GT cs.LG cs.SI 



### Searching for Structure: Investigating Emergent Communication with Large   Language Models
**Authors**: Tom Kouwenhoven, Max Peeperkorn, Tessa Verhoef

**Updated**: 2024-12-10T16:32:19Z

**Summary**: Human languages have evolved to be structured through repeated language learning and use. These processes introduce biases that operate during language acquisition and shape linguistic systems toward communicative efficiency. In this paper, we investigate whether the same happens if artificial languages are optimised for implicit biases of Large Language Models (LLMs). To this end, we simulate a classical referential game in which LLMs learn and use artificial languages. Our results show that initially unstructured holistic languages are indeed shaped to have some structural properties that allow two LLM agents to communicate successfully. Similar to observations in human experiments, generational transmission increases the learnability of languages, but can at the same time result in non-humanlike degenerate vocabularies. Taken together, this work extends experimental findings, shows that LLMs can be used as tools in simulations of language evolution, and opens possibilities for future human-machine experiments in this field.

**Link**: [arxiv](http://arxiv.org/abs/2412.07646v1),  [pdf](http://arxiv.org/pdf/2412.07646v1)

**Tags**: cs.CL 



### Symbolic Regression with a Learned Concept Library
**Authors**: Arya Grayeli, Atharva Sehgal, Omar Costilla-Reyes, Miles Cranmer, Swarat Chaudhuri

**Updated**: 2024-12-10T16:24:48Z

**Summary**: We present a novel method for symbolic regression (SR), the task of searching for compact programmatic hypotheses that best explain a dataset. The problem is commonly solved using genetic algorithms; we show that we can enhance such methods by inducing a library of abstract textual concepts. Our algorithm, called LaSR, uses zero-shot queries to a large language model (LLM) to discover and evolve concepts occurring in known high-performing hypotheses. We discover new hypotheses using a mix of standard evolutionary steps and LLM-guided steps (obtained through zero-shot LLM queries) conditioned on discovered concepts. Once discovered, hypotheses are used in a new round of concept abstraction and evolution. We validate LaSR on the Feynman equations, a popular SR benchmark, as well as a set of synthetic tasks. On these benchmarks, LaSR substantially outperforms a variety of state-of-the-art SR approaches based on deep learning and evolutionary algorithms. Moreover, we show that LaSR can be used to discover a novel and powerful scaling law for LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2409.09359v3),  [pdf](http://arxiv.org/pdf/2409.09359v3)

**Tags**: cs.LG cs.AI cs.NE cs.SC 



### SOMONITOR: Combining Explainable AI & Large Language Models for   Marketing Analytics
**Authors**: Aleksandr Farseev, Qi Yang, Marlo Ongpin, Ilia Gossoudarev, Yu-Yi Chu-Farseeva, Sergey Nikolenko

**Updated**: 2024-12-10T16:17:50Z

**Summary**: Online marketing faces formidable challenges in managing and interpreting immense volumes of data necessary for competitor analysis, content research, and strategic branding. It is impossible to review hundreds to thousands of transient online content items by hand, and partial analysis often leads to suboptimal outcomes and poorly performing campaigns. We introduce an explainable AI framework SOMONITOR that aims to synergize human intuition with AI-based efficiency, helping marketers across all stages of the marketing funnel, from strategic planning to content creation and campaign execution. SOMONITOR incorporates a CTR prediction and ranking model for advertising content and uses large language models (LLMs) to process high-performing competitor content, identifying core content pillars such as target audiences, customer needs, and product features. These pillars are then organized into broader categories, including communication themes and targeted customer personas. By integrating these insights with data from the brand's own advertising campaigns, SOMONITOR constructs a narrative for addressing new customer personas and simultaneously generates detailed content briefs in the form of user stories that, as shown in the conducted case study, can be directly applied by marketing teams to streamline content production and campaign execution. The adoption of SOMONITOR in daily operations allows digital marketers to quickly parse through extensive datasets, offering actionable insights that significantly enhance campaign effectiveness and overall job satisfaction.

**Link**: [arxiv](http://arxiv.org/abs/2407.13117v2),  [pdf](http://arxiv.org/pdf/2407.13117v2)

**Tags**: cs.CY cs.MM 



### TrojanWhisper: Evaluating Pre-trained LLMs to Detect and Localize   Hardware Trojans
**Authors**: Md Omar Faruque, Peter Jamieson, Ahmad Patooghy, Abdel-Hameed A. Badawy

**Updated**: 2024-12-10T16:16:22Z

**Summary**: Existing Hardware Trojans (HT) detection methods face several critical limitations: logic testing struggles with scalability and coverage for large designs, side-channel analysis requires golden reference chips, and formal verification methods suffer from state-space explosion. The emergence of Large Language Models (LLMs) offers a promising new direction for HT detection by leveraging their natural language understanding and reasoning capabilities. For the first time, this paper explores the potential of general-purpose LLMs in detecting various HTs inserted in Register Transfer Level (RTL) designs, including SRAM, AES, and UART modules. We propose a novel tool for this goal that systematically assesses state-of-the-art LLMs (GPT-4o, Gemini 1.5 pro, and Llama 3.1) in detecting HTs without prior fine-tuning. To address potential training data bias, the tool implements perturbation techniques, i.e., variable name obfuscation, and design restructuring, that make the cases more sophisticated for the used LLMs. Our experimental evaluation demonstrates perfect detection rates by GPT-4o and Gemini 1.5 pro in baseline scenarios (100%/100% precision/recall), with both models achieving better trigger line coverage (TLC: 0.82-0.98) than payload line coverage (PLC: 0.32-0.46). Under code perturbation, while Gemini 1.5 pro maintains perfect detection performance (100%/100%), GPT-4o (100%/85.7%) and Llama 3.1 (66.7%/85.7%) show some degradation in detection rates, and all models experience decreased accuracy in localizing both triggers and payloads. This paper validates the potential of LLM approaches for hardware security applications, highlighting areas for future improvement.

**Link**: [arxiv](http://arxiv.org/abs/2412.07636v1),  [pdf](http://arxiv.org/pdf/2412.07636v1)

**Tags**: cs.CR cs.AI 



### ChocoLlama: Lessons Learned From Teaching Llamas Dutch
**Authors**: Matthieu Meeus, Anthony Rathé, François Remy, Pieter Delobelle, Jens-Joris Decorte, Thomas Demeester

**Updated**: 2024-12-10T16:13:58Z

**Summary**: While Large Language Models (LLMs) have shown remarkable capabilities in natural language understanding and generation, their performance often lags in lower-resource, non-English languages due to biases in the training data. In this work, we explore strategies for adapting the primarily English LLMs (Llama-2 and Llama-3) to Dutch, a language spoken by 30 million people worldwide yet often underrepresented in LLM development. We collect 104GB of Dutch text ($32$B tokens) from various sources to first apply continued pretraining using low-rank adaptation (LoRA), complemented with Dutch posttraining strategies provided by prior work. For Llama-2, we consider using (i) the tokenizer of the original model, and (ii) training a new, Dutch-specific tokenizer combined with embedding reinitialization. We evaluate our adapted models, ChocoLlama-2, both on standard benchmarks and a novel Dutch benchmark, ChocoLlama-Bench. Our results demonstrate that LoRA can effectively scale for language adaptation, and that tokenizer modification with careful weight reinitialization can improve performance. Notably, Llama-3 was released during the course of this project and, upon evaluation, demonstrated superior Dutch capabilities compared to our Dutch-adapted versions of Llama-2. We hence apply the same adaptation technique to Llama-3, using its original tokenizer. While our adaptation methods enhanced Llama-2's Dutch capabilities, we found limited gains when applying the same techniques to Llama-3. This suggests that for ever improving, multilingual foundation models, language adaptation techniques may benefit more from focusing on language-specific posttraining rather than on continued pretraining. We hope this work contributes to the broader understanding of adapting LLMs to lower-resource languages, and to the development of Dutch LLMs in particular.

**Link**: [arxiv](http://arxiv.org/abs/2412.07633v1),  [pdf](http://arxiv.org/pdf/2412.07633v1)

**Tags**: cs.CL 



### Terabit-class coherent communications enabled by an integrated photonics   erbium doped amplifier
**Authors**: Di Che, Stefano Grillanda, Yang Liu, Zheru Qiu, Xinru Ji, Gregory Raybon, Xi Chen, Kwangwoong Kim, Tobias J. Kippenberg, Andrea Blanco-Redondo

**Updated**: 2024-12-10T16:07:34Z

**Summary**: Coherent technologies have revolutionized optical communications, driving the capacity per fiber to multi-terabit per second (Tb/s) in combination with wavelength division multiplexing (WDM). With an ever-increasing deployment density of coherent systems, the demand for highly integrated WDM coherent transceivers has been rising. While tremendous progress has been made on silicon photonics compatible high-speed modulation and photodetection on chip, a solution for monolithically integrable amplifier with high gain and output power remains a challenge. Recently, an erbium doped waveguide amplifier based on ultra-low loss silicon nitride waveguides has demonstrated gain and output power levels potentially suitable for Terabit class coherent communications. Here, we demonstrate a WDM coherent system enabled by this integrated photonic amplification solution. The system uses the waveguide amplifier as a booster amplifier of 16 WDM signals each carrying a net data rate of 1.6 Tb/s, achieving 25.6-Tb/s net capacity over 81-km fiber transmission. Our results highlight a fully integrated solution for highly parallel coherent transceivers including amplification, that has the potential to transform future optical communications.

**Link**: [arxiv](http://arxiv.org/abs/2412.07627v1),  [pdf](http://arxiv.org/pdf/2412.07627v1)

**Tags**: physics.optics 



### Leveraging Large Language Models for Node Generation in Few-Shot   Learning on Text-Attributed Graphs
**Authors**: Jianxiang Yu, Yuxiang Ren, Chenghua Gong, Jiaqi Tan, Xiang Li, Xuecang Zhang

**Updated**: 2024-12-10T16:06:29Z

**Summary**: Text-attributed graphs have recently garnered significant attention due to their wide range of applications in web domains. Existing methodologies employ word embedding models for acquiring text representations as node features, which are subsequently fed into Graph Neural Networks (GNNs) for training. Recently, the advent of Large Language Models (LLMs) has introduced their powerful capabilities in information retrieval and text generation, which can greatly enhance the text attributes of graph data. Furthermore, the acquisition and labeling of extensive datasets are both costly and time-consuming endeavors. Consequently, few-shot learning has emerged as a crucial problem in the context of graph learning tasks. In order to tackle this challenge, we propose a lightweight paradigm called LLM4NG, which adopts a plug-and-play approach to empower text-attributed graphs through node generation using LLMs. Specifically, we utilize LLMs to extract semantic information from the labels and generate samples that belong to these categories as exemplars. Subsequently, we employ an edge predictor to capture the structural information inherent in the raw dataset and integrate the newly generated samples into the original graph. This approach harnesses LLMs for enhancing class-level information and seamlessly introduces labeled nodes and edges without modifying the raw dataset, thereby facilitating the node classification task in few-shot scenarios. Extensive experiments demonstrate the outstanding performance of our proposed paradigm, particularly in low-shot scenarios. For instance, in the 1-shot setting of the ogbn-arxiv dataset, LLM4NG achieves a 76% improvement over the baseline model.

**Link**: [arxiv](http://arxiv.org/abs/2310.09872v2),  [pdf](http://arxiv.org/pdf/2310.09872v2)

**Tags**: cs.LG 



### OmniDocBench: Benchmarking Diverse PDF Document Parsing with   Comprehensive Annotations
**Authors**: Linke Ouyang, Yuan Qu, Hongbin Zhou, Jiawei Zhu, Rui Zhang, Qunshu Lin, Bin Wang, Zhiyuan Zhao, Man Jiang, Xiaomeng Zhao, Jin Shi, Fan Wu, Pei Chu, Minghao Liu, Zhenxiang Li, Chao Xu, Bo Zhang, Botian Shi, Zhongying Tu, Conghui He

**Updated**: 2024-12-10T16:05:56Z

**Summary**: Document content extraction is crucial in computer vision, especially for meeting the high-quality data needs of large language models (LLMs) and retrieval-augmented generation (RAG) technologies. However, current document parsing methods suffer from significant limitations in terms of diversity and comprehensive evaluation. To address these challenges, we introduce OmniDocBench, a novel multi-source benchmark designed to advance automated document content extraction. OmniDocBench includes a meticulously curated and annotated high-quality evaluation dataset comprising nine diverse document types, such as academic papers, textbooks, slides, among others. Our benchmark provides a flexible and comprehensive evaluation framework with 19 layout category labels and 14 attribute labels, enabling multi-level assessments across entire datasets, individual modules, or specific data types. Using OmniDocBench, we perform an exhaustive comparative analysis of existing modular pipelines and multimodal end-to-end methods, highlighting their limitations in handling document diversity and ensuring fair evaluation. OmniDocBench establishes a robust, diverse, and fair evaluation standard for the document content extraction field, offering crucial insights for future advancements and fostering the development of document parsing technologies. The codes and dataset is available in https://github.com/opendatalab/OmniDocBench.

**Link**: [arxiv](http://arxiv.org/abs/2412.07626v1),  [pdf](http://arxiv.org/pdf/2412.07626v1)

**Tags**: cs.CV cs.AI cs.IR 



### CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model   Recommender System
**Authors**: Yashar Deldjoo, Tommaso di Noia

**Updated**: 2024-12-10T16:00:55Z

**Summary**: This work takes a critical stance on previous studies concerning fairness evaluation in Large Language Model (LLM)-based recommender systems, which have primarily assessed consumer fairness by comparing recommendation lists generated with and without sensitive user attributes. Such approaches implicitly treat discrepancies in recommended items as biases, overlooking whether these changes might stem from genuine personalization aligned with true preferences of users. Moreover, these earlier studies typically address single sensitive attributes in isolation, neglecting the complex interplay of intersectional identities. In response to these shortcomings, we introduce CFaiRLLM, an enhanced evaluation framework that not only incorporates true preference alignment but also rigorously examines intersectional fairness by considering overlapping sensitive attributes. Additionally, CFaiRLLM introduces diverse user profile sampling strategies-random, top-rated, and recency-focused-to better understand the impact of profile generation fed to LLMs in light of inherent token limitations in these systems. Given that fairness depends on accurately understanding users' tastes and preferences,, these strategies provide a more realistic assessment of fairness within RecLLMs.   The results demonstrated that true preference alignment offers a more personalized and fair assessment compared to similarity-based measures, revealing significant disparities when sensitive and intersectional attributes are incorporated. Notably, our study finds that intersectional attributes amplify fairness gaps more prominently, especially in less structured domains such as music recommendations in LastFM.

**Link**: [arxiv](http://arxiv.org/abs/2403.05668v2),  [pdf](http://arxiv.org/pdf/2403.05668v2)

**Tags**: cs.IR 



### DRUM: Learning Demonstration Retriever for Large MUlti-modal Models
**Authors**: Ellen Yi-Ge, Jiechao Gao, Wei Han, Wei Zhu

**Updated**: 2024-12-10T15:56:12Z

**Summary**: Recently, large language models (LLMs) have demonstrated impressive capabilities in dealing with new tasks with the help of in-context learning (ICL). In the study of Large Vision-Language Models (LVLMs), when implementing ICL, researchers usually adopts the naive strategies like fixed demonstrations across different samples, or selecting demonstrations directly via a visual-language embedding model. These methods does not guarantee the configured demonstrations fit the need of the LVLMs. To address this issue, we now propose a novel framework, \underline{d}emonstration \underline{r}etriever for large m\underline{u}lti-modal \underline{m}odel (DRUM), which fine-tunes the visual-language embedding model to better meet the LVLM's needs. First, we discuss the retrieval strategies for a visual-language task, assuming an embedding model is given. And we propose to concate the image and text embeddings to enhance the retrieval performance. Second, we propose to re-rank the demonstrations retrieved by the embedding model via the LVLM's feedbacks, and calculate a list-wise ranking loss for training the embedding model. Third, we propose an iterative demonstration mining strategy to improve the training of the embedding model. Through extensive experiments on 3 types of visual-language tasks, 7 benchmark datasets, our DRUM framework is proven to be effective in boosting the LVLM's in-context learning performance via retrieving more proper demonstrations.

**Link**: [arxiv](http://arxiv.org/abs/2412.07619v1),  [pdf](http://arxiv.org/pdf/2412.07619v1)

**Tags**: cs.CL 



### Adapting to Non-Stationary Environments: Multi-Armed Bandit Enhanced   Retrieval-Augmented Generation on Knowledge Graphs
**Authors**: Xiaqiang Tang, Jian Li, Nan Du, Sihong Xie

**Updated**: 2024-12-10T15:56:03Z

**Summary**: Despite the superior performance of Large language models on many NLP tasks, they still face significant limitations in memorizing extensive world knowledge. Recent studies have demonstrated that leveraging the Retrieval-Augmented Generation (RAG) framework, combined with Knowledge Graphs that encapsulate extensive factual data in a structured format, robustly enhances the reasoning capabilities of LLMs. However, deploying such systems in real-world scenarios presents challenges: the continuous evolution of non-stationary environments may lead to performance degradation and user satisfaction requires a careful balance of performance and responsiveness. To address these challenges, we introduce a Multi-objective Multi-Armed Bandit enhanced RAG framework, supported by multiple retrieval methods with diverse capabilities under rich and evolving retrieval contexts in practice. Within this framework, each retrieval method is treated as a distinct ``arm''. The system utilizes real-time user feedback to adapt to dynamic environments, by selecting the appropriate retrieval method based on input queries and the historical multi-objective performance of each arm. Extensive experiments conducted on two benchmark KGQA datasets demonstrate that our method significantly outperforms baseline methods in non-stationary settings while achieving state-of-the-art performance in stationary environments. Code and data are available at https://github.com/FUTUREEEEEE/Dynamic-RAG.git

**Link**: [arxiv](http://arxiv.org/abs/2412.07618v1),  [pdf](http://arxiv.org/pdf/2412.07618v1)

**Tags**: cs.AI cs.CL 



### Applications and Implications of Large Language Models in Qualitative   Analysis: A New Frontier for Empirical Software Engineering
**Authors**: Matheus de Morais Leça, Lucas Valença, Reydne Santos, Ronnie de Souza Santos

**Updated**: 2024-12-10T15:53:18Z

**Summary**: The use of large language models (LLMs) for qualitative analysis is gaining attention in various fields, including software engineering, where qualitative methods are essential for understanding human and social factors. This study aimed to investigate how LLMs are currently used in qualitative analysis and their potential applications in software engineering research, focusing on the benefits, limitations, and practices associated with their use. A systematic mapping study was conducted, analyzing 21 relevant studies to explore reported uses of LLMs for qualitative analysis. The findings indicate that LLMs are primarily used for tasks such as coding, thematic analysis, and data categorization, offering benefits like increased efficiency and support for new researchers. However, limitations such as output variability, challenges in capturing nuanced perspectives, and ethical concerns related to privacy and transparency were also identified. The study emphasizes the need for structured strategies and guidelines to optimize LLM use in qualitative research within software engineering, enhancing their effectiveness while addressing ethical considerations. While LLMs show promise in supporting qualitative analysis, human expertise remains crucial for interpreting data, and ongoing exploration of best practices will be vital for their successful integration into empirical software engineering research.

**Link**: [arxiv](http://arxiv.org/abs/2412.06564v2),  [pdf](http://arxiv.org/pdf/2412.06564v2)

**Tags**: cs.SE 



### Which LLMs are Difficult to Detect? A Detailed Analysis of Potential   Factors Contributing to Difficulties in LLM Text Detection
**Authors**: Shantanu Thorat, Tianbao Yang

**Updated**: 2024-12-10T15:44:59Z

**Summary**: As LLMs increase in accessibility, LLM-generated texts have proliferated across several fields, such as scientific, academic, and creative writing. However, LLMs are not created equally; they may have different architectures and training datasets. Thus, some LLMs may be more challenging to detect than others. Using two datasets spanning four total writing domains, we train AI-generated (AIG) text classifiers using the LibAUC library - a deep learning library for training classifiers with imbalanced datasets. Our results in the Deepfake Text dataset show that AIG-text detection varies across domains, with scientific writing being relatively challenging. In the Rewritten Ivy Panda (RIP) dataset focusing on student essays, we find that the OpenAI family of LLMs was substantially difficult for our classifiers to distinguish from human texts. Additionally, we explore possible factors that could explain the difficulties in detecting OpenAI-generated texts.

**Link**: [arxiv](http://arxiv.org/abs/2410.14875v2),  [pdf](http://arxiv.org/pdf/2410.14875v2)

**Tags**: cs.CL cs.LG 



### SpaFL: Communication-Efficient Federated Learning with Sparse Models and   Low computational Overhead
**Authors**: Minsu Kim, Walid Saad, Merouane Debbah, Choong Seon Hong

**Updated**: 2024-12-10T15:43:05Z

**Summary**: The large communication and computation overhead of federated learning (FL) is one of the main challenges facing its practical deployment over resource-constrained clients and systems. In this work, SpaFL: a communication-efficient FL framework is proposed to optimize sparse model structures with low computational overhead. In SpaFL, a trainable threshold is defined for each filter/neuron to prune its all connected parameters, thereby leading to structured sparsity. To optimize the pruning process itself, only thresholds are communicated between a server and clients instead of parameters, thereby learning how to prune. Further, global thresholds are used to update model parameters by extracting aggregated parameter importance. The generalization bound of SpaFL is also derived, thereby proving key insights on the relation between sparsity and performance. Experimental results show that SpaFL improves accuracy while requiring much less communication and computing resources compared to sparse baselines. The code is available at https://github.com/news-vt/SpaFL_NeruIPS_2024

**Link**: [arxiv](http://arxiv.org/abs/2406.00431v2),  [pdf](http://arxiv.org/pdf/2406.00431v2)

**Tags**: cs.LG cs.AI cs.DC 



### DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for   Customized Manga Generation
**Authors**: Jianzong Wu, Chao Tang, Jingbo Wang, Yanhong Zeng, Xiangtai Li, Yunhai Tong

**Updated**: 2024-12-10T15:24:12Z

**Summary**: Story visualization, the task of creating visual narratives from textual descriptions, has seen progress with text-to-image generation models. However, these models often lack effective control over character appearances and interactions, particularly in multi-character scenes. To address these limitations, we propose a new task: \textbf{customized manga generation} and introduce \textbf{DiffSensei}, an innovative framework specifically designed for generating manga with dynamic multi-character control. DiffSensei integrates a diffusion-based image generator with a multimodal large language model (MLLM) that acts as a text-compatible identity adapter. Our approach employs masked cross-attention to seamlessly incorporate character features, enabling precise layout control without direct pixel transfer. Additionally, the MLLM-based adapter adjusts character features to align with panel-specific text cues, allowing flexible adjustments in character expressions, poses, and actions. We also introduce \textbf{MangaZero}, a large-scale dataset tailored to this task, containing 43,264 manga pages and 427,147 annotated panels, supporting the visualization of varied character interactions and movements across sequential frames. Extensive experiments demonstrate that DiffSensei outperforms existing models, marking a significant advancement in manga generation by enabling text-adaptable character customization. The project page is https://jianzongwu.github.io/projects/diffsensei/.

**Link**: [arxiv](http://arxiv.org/abs/2412.07589v1),  [pdf](http://arxiv.org/pdf/2412.07589v1)

**Tags**: cs.CV 



### Scaling Sequential Recommendation Models with Transformers
**Authors**: Pablo Zivic, Hernan Vazquez, Jorge Sanchez

**Updated**: 2024-12-10T15:20:56Z

**Summary**: Modeling user preferences has been mainly addressed by looking at users' interaction history with the different elements available in the system. Tailoring content to individual preferences based on historical data is the main goal of sequential recommendation.   The nature of the problem, as well as the good performance observed across various domains, has motivated the use of the transformer architecture, which has proven effective in leveraging increasingly larger amounts of training data when accompanied by an increase in the number of model parameters. This scaling behavior has brought a great deal of attention, as it provides valuable guidance in the design and training of even larger models.   Taking inspiration from the scaling laws observed in training large language models, we explore similar principles for sequential recommendation.   We use the full Amazon Product Data dataset, which has only been partially explored in other studies, and reveal scaling behaviors similar to those found in language models. Compute-optimal training is possible but requires a careful analysis of the compute-performance trade-offs specific to the application.   We also show that performance scaling translates to downstream tasks by fine-tuning larger pre-trained models on smaller task-specific domains. Our approach and findings provide a strategic roadmap for model training and deployment in real high-dimensional preference spaces, facilitating better training and inference efficiency.   We hope this paper bridges the gap between the potential of transformers and the intrinsic complexities of high-dimensional sequential recommendation in real-world recommender systems.   Code and models can be found at https://github.com/mercadolibre/srt

**Link**: [arxiv](http://arxiv.org/abs/2412.07585v1),  [pdf](http://arxiv.org/pdf/2412.07585v1)

**Tags**: cs.LG cs.AI 



### Making the Flow Glow -- Robot Perception under Severe Lighting   Conditions using Normalizing Flow Gradients
**Authors**: Simon Kristoffersson Lind, Rudolph Triebel, Volker Krüger

**Updated**: 2024-12-10T14:56:47Z

**Summary**: Modern robotic perception is highly dependent on neural networks. It is well known that neural network-based perception can be unreliable in real-world deployment, especially in difficult imaging conditions. Out-of-distribution detection is commonly proposed as a solution for ensuring reliability in real-world deployment. Previous work has shown that normalizing flow models can be used for out-of-distribution detection to improve reliability of robotic perception tasks. Specifically, camera parameters can be optimized with respect to the likelihood output from a normalizing flow, which allows a perception system to adapt to difficult vision scenarios. With this work we propose to use the absolute gradient values from a normalizing flow, which allows the perception system to optimize local regions rather than the whole image. By setting up a table top picking experiment with exceptionally difficult lighting conditions, we show that our method achieves a 60% higher success rate for an object detection task compared to previous methods.

**Link**: [arxiv](http://arxiv.org/abs/2412.07565v1),  [pdf](http://arxiv.org/pdf/2412.07565v1)

**Tags**: cs.CV 



### Optimization-Driven Design of Monolithic Soft-Rigid Grippers
**Authors**: Pierluigi Mansueto, Mihai Dragusanu, Anjum Saeed, Monica Malvezzi, Matteo Lapucci, Gionata Salvietti

**Updated**: 2024-12-10T14:47:09Z

**Summary**: Sim-to-real transfer remains a significant challenge in soft robotics due to the unpredictability introduced by common manufacturing processes such as 3D printing and molding. These processes often result in deviations from simulated designs, requiring multiple prototypes before achieving a functional system. In this study, we propose a novel methodology to address these limitations by combining advanced rapid prototyping techniques and an efficient optimization strategy. Firstly, we employ rapid prototyping methods typically used for rigid structures, leveraging their precision to fabricate compliant components with reduced manufacturing errors. Secondly, our optimization framework minimizes the need for extensive prototyping, significantly reducing the iterative design process. The methodology enables the identification of stiffness parameters that are more practical and achievable within current manufacturing capabilities. The proposed approach demonstrates a substantial improvement in the efficiency of prototype development while maintaining the desired performance characteristics. This work represents a step forward in bridging the sim-to-real gap in soft robotics, paving the way towards a faster and more reliable deployment of soft robotic systems.

**Link**: [arxiv](http://arxiv.org/abs/2412.07556v1),  [pdf](http://arxiv.org/pdf/2412.07556v1)

**Tags**: cs.RO cs.MS 



### GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large   Language Models
**Authors**: Tao Zhang, Ziqian Zeng, Yuxiang Xiao, Huiping Zhuang, Cen Chen, James Foulds, Shimei Pan

**Updated**: 2024-12-10T14:46:40Z

**Summary**: Large Language Models (LLMs) are prone to generating content that exhibits gender biases, raising significant ethical concerns. Alignment, the process of fine-tuning LLMs to better align with desired behaviors, is recognized as an effective approach to mitigate gender biases. Although proprietary LLMs have made significant strides in mitigating gender bias, their alignment datasets are not publicly available. The commonly used and publicly available alignment dataset, HH-RLHF, still exhibits gender bias to some extent. There is a lack of publicly available alignment datasets specifically designed to address gender bias. Hence, we developed a new dataset named GenderAlign, aiming at mitigating a comprehensive set of gender biases in LLMs. This dataset comprises 8k single-turn dialogues, each paired with a "chosen" and a "rejected" response. Compared to the "rejected" responses, the "chosen" responses demonstrate lower levels of gender bias and higher quality. Furthermore, we categorized the gender biases in the "rejected" responses of GenderAlign into 4 principal categories. The experimental results show the effectiveness of GenderAlign in reducing gender bias in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2406.13925v2),  [pdf](http://arxiv.org/pdf/2406.13925v2)

**Tags**: cs.CL cs.AI 



### CPsyExam: A Chinese Benchmark for Evaluating Psychology using   Examinations
**Authors**: Jiahao Zhao, Jingwei Zhu, Minghuan Tan, Min Yang, Renhao Li, Di Yang, Chenhao Zhang, Guancheng Ye, Chengming Li, Xiping Hu, Derek F. Wong

**Updated**: 2024-12-10T14:44:41Z

**Summary**: In this paper, we introduce a novel psychological benchmark, CPsyExam, constructed from questions sourced from Chinese language examinations. CPsyExam is designed to prioritize psychological knowledge and case analysis separately, recognizing the significance of applying psychological knowledge to real-world scenarios. From the pool of 22k questions, we utilize 4k to create the benchmark that offers balanced coverage of subjects and incorporates a diverse range of case analysis techniques.Furthermore, we evaluate a range of existing large language models~(LLMs), spanning from open-sourced to API-based models. Our experiments and analysis demonstrate that CPsyExam serves as an effective benchmark for enhancing the understanding of psychology within LLMs and enables the comparison of LLMs across various granularities.

**Link**: [arxiv](http://arxiv.org/abs/2405.10212v3),  [pdf](http://arxiv.org/pdf/2405.10212v3)

**Tags**: cs.CL 



### Automatic Database Configuration Debugging using Retrieval-Augmented   Language Models
**Authors**: Sibei Chen, Ju Fan, Bin Wu, Nan Tang, Chao Deng, Pengyi Wang, Ye Li, Jian Tan, Feifei Li, Jingren Zhou, Xiaoyong Du

**Updated**: 2024-12-10T14:39:51Z

**Summary**: Database management system (DBMS) configuration debugging, e.g., diagnosing poorly configured DBMS knobs and generating troubleshooting recommendations, is crucial in optimizing DBMS performance. However, the configuration debugging process is tedious and, sometimes challenging, even for seasoned database administrators (DBAs) with sufficient experience in DBMS configurations and good understandings of the DBMS internals (e.g., MySQL or Oracle). To address this difficulty, we propose Andromeda, a framework that utilizes large language models (LLMs) to enable automatic DBMS configuration debugging. Andromeda serves as a natural surrogate of DBAs to answer a wide range of natural language (NL) questions on DBMS configuration issues, and to generate diagnostic suggestions to fix these issues. Nevertheless, directly prompting LLMs with these professional questions may result in overly generic and often unsatisfying answers. To this end, we propose a retrieval-augmented generation (RAG) strategy that effectively provides matched domain-specific contexts for the question from multiple sources. They come from related historical questions, troubleshooting manuals and DBMS telemetries, which significantly improve the performance of configuration debugging. To support the RAG strategy, we develop a document retrieval mechanism addressing heterogeneous documents and design an effective method for telemetry analysis. Extensive experiments on real-world DBMS configuration debugging datasets show that Andromeda significantly outperforms existing solutions.

**Link**: [arxiv](http://arxiv.org/abs/2412.07548v1),  [pdf](http://arxiv.org/pdf/2412.07548v1)

**Tags**: cs.DB 



### Contractive Dynamical Imitation Policies for Efficient Out-of-Sample   Recovery
**Authors**: Amin Abyaneh, Mahrokh G. Boroujeni, Hsiu-Chin Lin, Giancarlo Ferrari-Trecate

**Updated**: 2024-12-10T14:28:18Z

**Summary**: Imitation learning is a data-driven approach to learning policies from expert behavior, but it is prone to unreliable outcomes in out-of-sample (OOS) regions. While previous research relying on stable dynamical systems guarantees convergence to a desired state, it often overlooks transient behavior. We propose a framework for learning policies using modeled by contractive dynamical systems, ensuring that all policy rollouts converge regardless of perturbations, and in turn, enable efficient OOS recovery. By leveraging recurrent equilibrium networks and coupling layers, the policy structure guarantees contractivity for any parameter choice, which facilitates unconstrained optimization. Furthermore, we provide theoretical upper bounds for worst-case and expected loss terms, rigorously establishing the reliability of our method in deployment. Empirically, we demonstrate substantial OOS performance improvements in robotics manipulation and navigation tasks in simulation.

**Link**: [arxiv](http://arxiv.org/abs/2412.07544v1),  [pdf](http://arxiv.org/pdf/2412.07544v1)

**Tags**: cs.LG cs.RO stat.ML 



### SCORPION Cyber Range: Fully Customizable Cyberexercises, Gamification,   and Learning Analytics to Train Cybersecurity Competencies
**Authors**: Pantaleone Nespoli, Mariano Albaladejo-González, José A. Ruipérez-Valiente, Joaquin Garcia-Alfaro

**Updated**: 2024-12-10T14:22:38Z

**Summary**: It is undeniable that we are witnessing an unprecedented digital revolution. However, recent years have been characterized by the explosion of cyberattacks, making cybercrime one of the most profitable businesses on the planet. That is why training in cybersecurity is increasingly essential to protect the assets of cyberspace. One of the most vital tools to train cybersecurity competencies is the Cyber Range, a virtualized environment that simulates realistic networks. The paper at hand introduces SCORPION, a fully functional and virtualized Cyber Range, which manages the authoring and automated deployment of scenarios. In addition, SCORPION includes several elements to improve student motivation, such as a gamification system with medals, points, or rankings, among other elements. Such a gamification system includes an adaptive learning module that is able to adapt the cyberexercise based on the users' performance. Moreover, SCORPION leverages learning analytics that collects and processes telemetric and biometric user data, including heart rate through a smartwatch, which is available through a dashboard for instructors. Finally, we developed a case study where SCORPION obtained 82.10% in usability and 4.57 out of 5 in usefulness from the viewpoint of a student and an instructor. The positive evaluation results are promising, indicating that SCORPION can become an effective, motivating, and advanced cybersecurity training tool to help fill current gaps in this context.

**Link**: [arxiv](http://arxiv.org/abs/2401.12594v4),  [pdf](http://arxiv.org/pdf/2401.12594v4)

**Tags**: cs.CR cs.CY 



### AutoDetect: Towards a Unified Framework for Automated Weakness Detection   in Large Language Models
**Authors**: Jiale Cheng, Yida Lu, Xiaotao Gu, Pei Ke, Xiao Liu, Yuxiao Dong, Hongning Wang, Jie Tang, Minlie Huang

**Updated**: 2024-12-10T13:57:46Z

**Summary**: Although Large Language Models (LLMs) are becoming increasingly powerful, they still exhibit significant but subtle weaknesses, such as mistakes in instruction-following or coding tasks. As these unexpected errors could lead to severe consequences in practical deployments, it is crucial to investigate the limitations within LLMs systematically. Traditional benchmarking approaches cannot thoroughly pinpoint specific model deficiencies, while manual inspections are costly and not scalable. In this paper, we introduce a unified framework, AutoDetect, to automatically expose weaknesses in LLMs across various tasks. Inspired by the educational assessment process that measures students' learning outcomes, AutoDetect consists of three LLM-powered agents: Examiner, Questioner, and Assessor. The collaboration among these three agents is designed to realize comprehensive and in-depth weakness identification. Our framework demonstrates significant success in uncovering flaws, with an identification success rate exceeding 30% in prominent models such as ChatGPT and Claude. More importantly, these identified weaknesses can guide specific model improvements, proving more effective than untargeted data augmentation methods like Self-Instruct. Our approach has led to substantial enhancements in popular LLMs, including the Llama series and Mistral-7b, boosting their performance by over 10% across several benchmarks. Code and data are publicly available at https://github.com/thu-coai/AutoDetect.

**Link**: [arxiv](http://arxiv.org/abs/2406.16714v2),  [pdf](http://arxiv.org/pdf/2406.16714v2)

**Tags**: cs.CL cs.AI cs.LG 



### CMRNext: Camera to LiDAR Matching in the Wild for Localization and   Extrinsic Calibration
**Authors**: Daniele Cattaneo, Abhinav Valada

**Updated**: 2024-12-10T13:53:01Z

**Summary**: LiDARs are widely used for mapping and localization in dynamic environments. However, their high cost limits their widespread adoption. On the other hand, monocular localization in LiDAR maps using inexpensive cameras is a cost-effective alternative for large-scale deployment. Nevertheless, most existing approaches struggle to generalize to new sensor setups and environments, requiring retraining or fine-tuning. In this paper, we present CMRNext, a novel approach for camera-LIDAR matching that is independent of sensor-specific parameters, generalizable, and can be used in the wild for monocular localization in LiDAR maps and camera-LiDAR extrinsic calibration. CMRNext exploits recent advances in deep neural networks for matching cross-modal data and standard geometric techniques for robust pose estimation. We reformulate the point-pixel matching problem as an optical flow estimation problem and solve the Perspective-n-Point problem based on the resulting correspondences to find the relative pose between the camera and the LiDAR point cloud. We extensively evaluate CMRNext on six different robotic platforms, including three publicly available datasets and three in-house robots. Our experimental evaluations demonstrate that CMRNext outperforms existing approaches on both tasks and effectively generalizes to previously unseen environments and sensor setups in a zero-shot manner. We make the code and pre-trained models publicly available at http://cmrnext.cs.uni-freiburg.de .

**Link**: [arxiv](http://arxiv.org/abs/2402.00129v4),  [pdf](http://arxiv.org/pdf/2402.00129v4)

**Tags**: cs.CV cs.RO 



### CoPrUS: Consistency Preserving Utterance Synthesis towards more   realistic benchmark dialogues
**Authors**: Sebastian Steindl, Ulrich Schäfer, Bernd Ludwig

**Updated**: 2024-12-10T13:51:55Z

**Summary**: Large-scale Wizard-Of-Oz dialogue datasets have enabled the training of deep learning-based dialogue systems. While they are successful as benchmark datasets, they lack certain types of utterances, which would make them more realistic. In this work, we investigate the creation of synthetic communication errors in an automatic pipeline. Based on linguistic theory, we propose and follow a simple error taxonomy. We focus on three types of miscommunications that could happen in real-world dialogues but are underrepresented in the benchmark dataset: misunderstandings, non-understandings and vaguely related questions. Our two-step approach uses a state-of-the-art Large Language Model (LLM) to first create the error and secondly the repairing utterance. We perform Language Model-based evaluation to ensure the quality of the generated utterances. We apply the method to the MultiWOZ dataset and evaluate it both qualitatively and empirically as well as with human judges. Our results indicate that current LLMs can aid in adding post-hoc miscommunications to benchmark datasets as a form of data augmentation. We publish the resulting dataset, in which nearly 1900 dialogues have been modified, as CoPrUS-MultiWOZ to facilitate future work on dialogue systems.

**Link**: [arxiv](http://arxiv.org/abs/2412.07515v1),  [pdf](http://arxiv.org/pdf/2412.07515v1)

**Tags**: cs.CL 



### Ontology-driven Prompt Tuning for LLM-based Task and Motion Planning
**Authors**: Muhayy Ud Din, Jan Rosell, Waseem Akram, Isiah Zaplana, Maximo A Roa, Lakmal Seneviratne, Irfan Hussain

**Updated**: 2024-12-10T13:18:45Z

**Summary**: Performing complex manipulation tasks in dynamic environments requires efficient Task and Motion Planning (TAMP) approaches, which combine high-level symbolic plan with low-level motion planning. Advances in Large Language Models (LLMs), such as GPT-4, are transforming task planning by offering natural language as an intuitive and flexible way to describe tasks, generate symbolic plans, and reason. However, the effectiveness of LLM-based TAMP approaches is limited due to static and template-based prompting, which struggles in adapting to dynamic environments and complex task contexts. To address these limitations, this work proposes a novel ontology-driven prompt-tuning framework that employs knowledge-based reasoning to refine and expand user prompts with task contextual reasoning and knowledge-based environment state descriptions. Integrating domain-specific knowledge into the prompt ensures semantically accurate and context-aware task plans. The proposed framework demonstrates its effectiveness by resolving semantic errors in symbolic plan generation, such as maintaining logical temporal goal ordering in scenarios involving hierarchical object placement. The proposed framework is validated through both simulation and real-world scenarios, demonstrating significant improvements over the baseline approach in terms of adaptability to dynamic environments, and the generation of semantically correct task plans.

**Link**: [arxiv](http://arxiv.org/abs/2412.07493v1),  [pdf](http://arxiv.org/pdf/2412.07493v1)

**Tags**: cs.RO cs.AI 



### Ad-hoc hybrid-heterogeneous metropolitan-range quantum key distribution   network
**Authors**: Matthias Goy, Jan Krause, Ömer Bayraktar, Philippe Ancsin, Florian David, Thomas Dirmeier, Nico Doell, Jansen Dwan, Friederike Fohlmeister, Ronald Freund, Thorsten A. Goebel, Jonas Hilt, Kevin Jaksch, Oskar Kohout, Teresa Kopf, Andrej Krzic, Markus Leipe, Gerd Leuchs, Christoph Marquardt, Karen L. Mendez, Anja Milde, Sarika Mishra, Florian Moll, Karolina Paciorek, Natasa Pavlovic, Stefan Richter, Markus Rothe, René Rüddenklau, Gregor Sauer, Martin Schell, Jan Schreck, Andy Schreier, Sakshi Sharma, Simon Spier, Christopher Spiess, Fabian Steinlechner, Andreas Tünnermann, Hüseyin Vural, Nino Walenta, Stefan Weide

**Updated**: 2024-12-10T12:42:26Z

**Summary**: This paper presents the development and implementation of a versatile ad-hoc metropolitan-range Quantum Key Distribution (QKD) network. The approach presented integrates various types of physical channels and QKD protocols, and a mix of trusted and untrusted nodes. Unlike conventional QKD networks that predominantly depend on either fiber-based or free-space optical (FSO) links, the testbed presented amalgamates FSO and fiber-based links, thereby overcoming some inherent limitations. Various network deployment strategies have been considered, including permanent infrastructure and provisional ad-hoc links to eradicate coverage gaps. Furthermore, the ability to rapidly establish a network using portable FSO terminals and to investigate diverse link topologies is demonstrated. The study also showcases the successful establishment of a quantum-secured link to a cloud server.

**Link**: [arxiv](http://arxiv.org/abs/2412.07473v1),  [pdf](http://arxiv.org/pdf/2412.07473v1)

**Tags**: quant-ph physics.app-ph 



### SQL Injection Jailbreak: a structural disaster of large language models
**Authors**: Jiawei Zhao, Kejiang Chen, Weiming Zhang, Nenghai Yu

**Updated**: 2024-12-10T12:14:50Z

**Summary**: In recent years, the rapid development of large language models (LLMs) has brought new vitality into various domains, generating substantial social and economic benefits. However, this swift advancement has also introduced new security vulnerabilities. Jailbreaking, a form of attack that induces LLMs to produce harmful content through carefully crafted prompts, presents a significant challenge to the safe and trustworthy development of LLMs. Previous jailbreak methods primarily exploited the internal properties or capabilities of LLMs, such as optimization-based jailbreak approaches and methods that leveraged the model's context-learning abilities. In this paper, we introduce a novel jailbreak method, SQL Injection Jailbreak (SIJ), which targets the external properties of LLMs, specifically, the way LLMs construct input prompts. By injecting jailbreak information into user prompts, SIJ successfully induces the model to output harmful content. Our SIJ method achieves near 100\% attack success rates on five well-known open-source LLMs on the AdvBench, while incurring lower time costs compared to previous methods. More importantly, SIJ is the first method to exploit the external properties of LLMs for jailbreak attacks and exposes a new vulnerability in LLMs that urgently requires mitigation. To address this, we propose a simple defense method called Self-Reminder-Key to counter SIJ and demonstrate its effectiveness through experimental results. Our code is available at \href{https://github.com/weiyezhimeng/SQL-Injection-Jailbreak}{https://github.com/weiyezhimeng/SQL-Injection-Jailbreak}.

**Link**: [arxiv](http://arxiv.org/abs/2411.01565v3),  [pdf](http://arxiv.org/pdf/2411.01565v3)

**Tags**: cs.CR 



### Dynamic Ensemble Reasoning for LLM Experts
**Authors**: Jinwu Hu, Yufeng Wang, Shuhai Zhang, Kai Zhou, Guohao Chen, Yu Hu, Bin Xiao, Mingkui Tan

**Updated**: 2024-12-10T12:05:56Z

**Summary**: Ensemble reasoning for the strengths of different LLM experts is critical to achieving consistent and satisfactory performance on diverse inputs across a wide range of tasks. However, existing LLM ensemble methods are either computationally intensive or incapable of leveraging complementary knowledge among LLM experts for various inputs. In this paper, we propose a Dynamic Ensemble Reasoning paradigm, called DER to integrate the strengths of multiple LLM experts conditioned on dynamic inputs. Specifically, we model the LLM ensemble reasoning problem as a Markov Decision Process (MDP), wherein an agent sequentially takes inputs to request knowledge from an LLM candidate and passes the output to a subsequent LLM candidate. Moreover, we devise a reward function to train a DER-Agent to dynamically select an optimal answering route given the input questions, aiming to achieve the highest performance with as few computational resources as possible. Last, to fully transfer the expert knowledge from the prior LLMs, we develop a Knowledge Transfer Prompt (KTP) that enables the subsequent LLM candidates to transfer complementary knowledge effectively. Experiments demonstrate that our method uses fewer computational resources to achieve better performance compared to state-of-the-art baselines.

**Link**: [arxiv](http://arxiv.org/abs/2412.07448v1),  [pdf](http://arxiv.org/pdf/2412.07448v1)

**Tags**: cs.AI 



### MobileSafetyBench: Evaluating Safety of Autonomous Agents in Mobile   Device Control
**Authors**: Juyong Lee, Dongyoon Hahm, June Suk Choi, W. Bradley Knox, Kimin Lee

**Updated**: 2024-12-10T11:56:09Z

**Summary**: Autonomous agents powered by large language models (LLMs) show promising potential in assistive tasks across various domains, including mobile device control. As these agents interact directly with personal information and device settings, ensuring their safe and reliable behavior is crucial to prevent undesirable outcomes. However, no benchmark exists for standardized evaluation of the safety of mobile device-control agents. In this work, we introduce MobileSafetyBench, a benchmark designed to evaluate the safety of device-control agents within a realistic mobile environment based on Android emulators. We develop a diverse set of tasks involving interactions with various mobile applications, including messaging and banking applications, challenging agents with managing risks encompassing misuse and negative side effects. These tasks include tests to evaluate the safety of agents in daily scenarios as well as their robustness against indirect prompt injection attacks. Our experiments demonstrate that baseline agents, based on state-of-the-art LLMs, often fail to effectively prevent harm while performing the tasks. To mitigate these safety concerns, we propose a prompting method that encourages agents to prioritize safety considerations. While this method shows promise in promoting safer behaviors, there is still considerable room for improvement to fully earn user trust. This highlights the urgent need for continued research to develop more robust safety mechanisms in mobile environments. We open-source our benchmark at: https://mobilesafetybench.github.io/.

**Link**: [arxiv](http://arxiv.org/abs/2410.17520v2),  [pdf](http://arxiv.org/pdf/2410.17520v2)

**Tags**: cs.LG cs.CL 



### Optimizing Alignment with Less: Leveraging Data Augmentation for   Personalized Evaluation
**Authors**: Javad Seraj, Mohammad Mahdi Mohajeri, Mohammad Javad Dousti, Majid Nili Ahmadabadi

**Updated**: 2024-12-10T11:40:11Z

**Summary**: Automatic evaluation by large language models (LLMs) is a prominent topic today; however, judgment and evaluation tasks are often subjective and influenced by various factors, making adaptation challenging. While many studies demonstrate the capabilities of state-of-the-art proprietary LLMs in comparison to human evaluators, they often struggle to adapt to reference evaluators over time, a requirement for achieving personalized judgment. Additionally, numerous works have attempted to apply open LLMs as judges or evaluators, but these efforts frequently overlook the limitations of working with scarce data. Personalized judgment is inherently associated with limited data scenarios, which are common in many real-world problems. Our work aims to present a data augmentation technique to select a more effective sample from limited data in order to align an open LLM with human preference. Our work achieves approximately 7% improvements in Pearson correlation with a reference judge over the baseline,and 30% improvement over the base model (Llama3.1-8B-Instruct) in the mathematical reasoning evaluation task. demonstrating that augmenting selecting more effective preference data enables our approach to surpass baseline methods.

**Link**: [arxiv](http://arxiv.org/abs/2412.07429v1),  [pdf](http://arxiv.org/pdf/2412.07429v1)

**Tags**: cs.CL cs.AI 



### Using Large Language Models for Expert Prior Elicitation in Predictive   Modelling
**Authors**: Alexander Capstick, Rahul G. Krishnan, Payam Barnaghi

**Updated**: 2024-12-10T11:36:48Z

**Summary**: Large language models (LLMs), trained on diverse data effectively acquire a breadth of information across various domains. However, their computational complexity, cost, and lack of transparency hinder their direct application for specialised tasks. In fields such as clinical research, acquiring expert annotations or prior knowledge about predictive models is often costly and time-consuming. This study proposes the use of LLMs to elicit expert prior distributions for predictive models. This approach also provides an alternative to in-context learning, where language models are tasked with making predictions directly. In this work, we compare LLM-elicited and uninformative priors, evaluate whether LLMs truthfully generate parameter distributions, and propose a model selection strategy for in-context learning and prior elicitation. Our findings show that LLM-elicited prior parameter distributions significantly reduce predictive error compared to uninformative priors in low-data settings. Applied to clinical problems, this translates to fewer required biological samples, lowering cost and resources. Prior elicitation also consistently outperforms and proves more reliable than in-context learning at a lower cost, making it a preferred alternative in our setting. We demonstrate the utility of this method across various use cases, including clinical applications. For infection prediction, using LLM-elicited priors reduced the number of required labels to achieve the same accuracy as an uninformative prior by 55%, 200 days earlier in the study.

**Link**: [arxiv](http://arxiv.org/abs/2411.17284v2),  [pdf](http://arxiv.org/pdf/2411.17284v2)

**Tags**: cs.LG cs.CL stat.ML 



### Is Self-knowledge and Action Consistent or Not: Investigating Large   Language Model's Personality
**Authors**: Yiming Ai, Zhiwei He, Ziyin Zhang, Wenhong Zhu, Hongkun Hao, Kai Yu, Lingjun Chen, Rui Wang

**Updated**: 2024-12-10T11:26:52Z

**Summary**: In this study, we delve into the validity of conventional personality questionnaires in capturing the human-like personality traits of Large Language Models (LLMs). Our objective is to assess the congruence between the personality traits LLMs claim to possess and their demonstrated tendencies in real-world scenarios. By conducting an extensive examination of LLM outputs against observed human response patterns, we aim to understand the disjunction between self-knowledge and action in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2402.14679v2),  [pdf](http://arxiv.org/pdf/2402.14679v2)

**Tags**: cs.CL cs.CY 



### Beyond Search Engines: Can Large Language Models Improve Curriculum   Development?
**Authors**: Mohammad Moein, Mohammadreza Molavi Hajiagha, Abdolali Faraji, Mohammadreza Tavakoli, Gàbor Kismihòk

**Updated**: 2024-12-10T11:21:16Z

**Summary**: While Online Learning is growing and becoming widespread, the associated curricula often suffer from a lack of coverage and outdated content. In this regard, a key question is how to dynamically define the topics that must be covered to thoroughly learn a subject (e.g., a course). Large Language Models (LLMs) are considered candidates that can be used to address curriculum development challenges. Therefore, we developed a framework and a novel dataset, built on YouTube, to evaluate LLMs' performance when it comes to generating learning topics for specific courses. The experiment was conducted across over 100 courses and nearly 7,000 YouTube playlists in various subject areas. Our results indicate that GPT-4 can produce more accurate topics for the given courses than extracted topics from YouTube video playlists in terms of BERTScore

**Link**: [arxiv](http://arxiv.org/abs/2412.07422v1),  [pdf](http://arxiv.org/pdf/2412.07422v1)

**Tags**: cs.CY 



### Generating Knowledge Graphs from Large Language Models: A Comparative   Study of GPT-4, LLaMA 2, and BERT
**Authors**: Ahan Bhatt, Nandan Vaghela, Kush Dudhia

**Updated**: 2024-12-10T11:05:26Z

**Summary**: Knowledge Graphs (KGs) are essential for the functionality of GraphRAGs, a form of Retrieval-Augmented Generative Systems (RAGs) that excel in tasks requiring structured reasoning and semantic understanding. However, creating KGs for GraphRAGs remains a significant challenge due to accuracy and scalability limitations of traditional methods. This paper introduces a novel approach leveraging large language models (LLMs) like GPT-4, LLaMA 2 (13B), and BERT to generate KGs directly from unstructured data, bypassing traditional pipelines. Using metrics such as Precision, Recall, F1-Score, Graph Edit Distance, and Semantic Similarity, we evaluate the models' ability to generate high-quality KGs. Results demonstrate that GPT-4 achieves superior semantic fidelity and structural accuracy, LLaMA 2 excels in lightweight, domain-specific graphs, and BERT provides insights into challenges in entity-relationship modeling. This study underscores the potential of LLMs to streamline KG creation and enhance GraphRAG accessibility for real-world applications, while setting a foundation for future advancements.

**Link**: [arxiv](http://arxiv.org/abs/2412.07412v1),  [pdf](http://arxiv.org/pdf/2412.07412v1)

**Tags**: cs.CL cs.AI cs.DB 



### DSFEC: Efficient and Deployable Deep Radar Object Detection
**Authors**: Gayathri Dandugula, Santhosh Boddana, Sudesh Mirashi

**Updated**: 2024-12-10T11:03:51Z

**Summary**: Deploying radar object detection models on resource-constrained edge devices like the Raspberry Pi poses significant challenges due to the large size of the model and the limited computational power and the memory of the Pi. In this work, we explore the efficiency of Depthwise Separable Convolutions in radar object detection networks and integrate them into our model. Additionally, we introduce a novel Feature Enhancement and Compression (FEC) module to the PointPillars feature encoder to further improve the model performance. With these innovations, we propose the DSFEC-L model and its two versions, which outperform the baseline (23.9 mAP of Car class, 20.72 GFLOPs) on nuScenes dataset: 1). An efficient DSFEC-M model with a 14.6% performance improvement and a 60% reduction in GFLOPs. 2). A deployable DSFEC-S model with a 3.76% performance improvement and a remarkable 78.5% reduction in GFLOPs. Despite marginal performance gains, our deployable model achieves an impressive 74.5% reduction in runtime on the Raspberry Pi compared to the baseline.

**Link**: [arxiv](http://arxiv.org/abs/2412.07411v1),  [pdf](http://arxiv.org/pdf/2412.07411v1)

**Tags**: cs.CV cs.AI cs.LG 



### MoDULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task   Learning
**Authors**: Yufei Ma, Zihan Liang, Huangyu Dai, Ben Chen, Dehong Gao, Zhuoran Ran, Wang Zihan, Linbo Jin, Wen Jiang, Guannan Zhang, Xiaoyan Cai, Libin Yang

**Updated**: 2024-12-10T10:55:57Z

**Summary**: The growing demand for larger-scale models in the development of \textbf{L}arge \textbf{L}anguage \textbf{M}odels (LLMs) poses challenges for efficient training within limited computational resources. Traditional fine-tuning methods often exhibit instability in multi-task learning and rely heavily on extensive training resources. Here, we propose MoDULA (\textbf{M}ixture \textbf{o}f \textbf{D}omain-Specific and \textbf{U}niversal \textbf{L}oR\textbf{A}), a novel \textbf{P}arameter \textbf{E}fficient \textbf{F}ine-\textbf{T}uning (PEFT) \textbf{M}ixture-\textbf{o}f-\textbf{E}xpert (MoE) paradigm for improved fine-tuning and parameter efficiency in multi-task learning. The paradigm effectively improves the multi-task capability of the model by training universal experts, domain-specific experts, and routers separately. MoDULA-Res is a new method within the MoDULA paradigm, which maintains the model's general capability by connecting universal and task-specific experts through residual connections. The experimental results demonstrate that the overall performance of the MoDULA-Flan and MoDULA-Res methods surpasses that of existing fine-tuning methods on various LLMs. Notably, MoDULA-Res achieves more significant performance improvements in multiple tasks while reducing training costs by over 80\% without losing general capability. Moreover, MoDULA displays flexible pluggability, allowing for the efficient addition of new tasks without retraining existing experts from scratch. This progressive training paradigm circumvents data balancing issues, enhancing training efficiency and model stability. Overall, MoDULA provides a scalable, cost-effective solution for fine-tuning LLMs with enhanced parameter efficiency and generalization capability.

**Link**: [arxiv](http://arxiv.org/abs/2412.07405v1),  [pdf](http://arxiv.org/pdf/2412.07405v1)

**Tags**: cs.LG cs.AI 



### Benchmarking Chinese Commonsense Reasoning of LLMs: From   Chinese-Specifics to Reasoning-Memorization Correlations
**Authors**: Jiaxing Sun, Weiquan Huang, Jiang Wu, Chenya Gu, Wei Li, Songyang Zhang, Hang Yan, Conghui He

**Updated**: 2024-12-10T10:43:50Z

**Summary**: We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense. We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies for improving LLMs' reasoning ability, such as Chain-of-Thought. Our findings indicate that the LLM's language orientation and the task's domain influence the effectiveness of the prompt strategy, which enriches previous research findings. We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance. We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors. Our study precisely identified the LLMs' strengths and weaknesses, providing the clear direction for optimization. It can also serve as a reference for studies in other fields. We will release CHARM at https://github.com/opendatalab/CHARM .

**Link**: [arxiv](http://arxiv.org/abs/2403.14112v3),  [pdf](http://arxiv.org/pdf/2403.14112v3)

**Tags**: cs.CL 



### CMT: A Memory Compression Method for Continual Knowledge Learning of   Large Language Models
**Authors**: Dongfang Li, Zetian Sun, Xinshuo Hu, Baotian Hu, Min Zhang

**Updated**: 2024-12-10T10:35:19Z

**Summary**: Large Language Models (LLMs) need to adapt to the continuous changes in data, tasks, and user preferences. Due to their massive size and the high costs associated with training, LLMs are not suitable for frequent retraining. However, updates are necessary to keep them in sync with rapidly evolving human knowledge. To address these challenges, this paper proposes the Compression Memory Training (CMT) method, an efficient and effective online adaptation framework for LLMs that features robust knowledge retention capabilities. Inspired by human memory mechanisms, CMT compresses and extracts information from new documents to be stored in a memory bank. When answering to queries related to these new documents, the model aggregates these document memories from the memory bank to better answer user questions. The parameters of the LLM itself do not change during training and inference, reducing the risk of catastrophic forgetting. To enhance the encoding, retrieval, and aggregation of memory, we further propose three new general and flexible techniques, including memory-aware objective, self-matching and top-aggregation. Extensive experiments conducted on three continual learning datasets (i.e., StreamingQA, SQuAD and ArchivalQA) demonstrate that the proposed method improves model adaptability and robustness across multiple base LLMs (e.g., +4.07 EM & +4.19 F1 in StreamingQA with Llama-2-7b).

**Link**: [arxiv](http://arxiv.org/abs/2412.07393v1),  [pdf](http://arxiv.org/pdf/2412.07393v1)

**Tags**: cs.CL cs.AI 



### Post-Training Non-Uniform Quantization for Convolutional Neural Networks
**Authors**: Ahmed Luqman, Khuzemah Qazi, Imdadullah Khan

**Updated**: 2024-12-10T10:33:58Z

**Summary**: Despite the success of CNN models on a variety of Image classification and segmentation tasks, their extensive computational and storage demands pose considerable challenges for real-world deployment on resource constrained devices. Quantization is one technique that aims to alleviate these large storage requirements and speed up the inference process by reducing the precision of model parameters to lower-bit representations. In this paper, we introduce a novel post-training quantization method for model weights. Our method finds optimal clipping thresholds and scaling factors along with mathematical guarantees that our method minimizes quantization noise. Empirical results on Real World Datasets demonstrate that our quantization scheme significantly reduces model size and computational requirements while preserving model accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2412.07391v1),  [pdf](http://arxiv.org/pdf/2412.07391v1)

**Tags**: cs.CV cs.LG 



### SpecFuse: Ensembling Large Language Models via Next-Segment Prediction
**Authors**: Bo Lv, Chen Tang, Yanan Zhang, Xin Liu, Yue Yu, Ping Luo

**Updated**: 2024-12-10T10:27:41Z

**Summary**: Ensembles of generative large language models (LLMs) can integrate the strengths of different LLMs to compensate for the limitations of individual models. However, recent work has focused on training an additional fusion model to combine complete responses from multiple LLMs, failing to tap into their collaborative potential to generate higher-quality responses. Moreover, as the additional fusion model is trained on a specialized dataset, these methods struggle with generalizing to open-domain queries from online users. In this paper, we propose SpecFuse, a novel ensemble framework that outputs the fused result by iteratively producing the next segment through collaboration among LLMs. This is achieved through cyclic execution of its inference and verification components. In each round, the inference component invokes each base LLM to generate candidate segments in parallel, and the verify component calls these LLMs again to predict the ranking of the segments. The top-ranked segment is then broadcast to all LLMs, encouraging them to generate higher-quality segments in the next round. This approach also allows the base LLMs to be plug-and-play, without any training or adaptation, avoiding generalization limitations. Furthermore, to conserve computational resources, we propose a model exit mechanism that dynamically excludes models exhibiting poor performance in previous rounds during each query response. In this way, it effectively reduces the number of model calls while maintaining overall performance.

**Link**: [arxiv](http://arxiv.org/abs/2412.07380v1),  [pdf](http://arxiv.org/pdf/2412.07380v1)

**Tags**: cs.CL cs.AI 



### Cascaded channel modeling and experimental validation for RIS assisted   communication system
**Authors**: Jiwei Zhang, Yuxiang Zhang, Tao Jiang, Huiwen Gong, Hongbo Xing, Lei Tian

**Updated**: 2024-12-10T09:48:50Z

**Summary**: Reconfigurable Intelligent Surface (RIS) is considered as a promising technology for 6G due to its ability to actively modify the electromagnetic propagation environment. Accurate channel modeling is essential for the design and evaluation of RIS assisted communication systems. Most current research models the RIS channel as a cascade of Tx-RIS and RIS-Rx sub-channels. However, most validation efforts regarding this assumption focus on large-scale path loss. To further explore this, in this paper, we derive and extend a convolution expression of RIS cascaded channel model based on the previously proposed Geometry-based Stochastic Model (GBSM)-based RIS cascaded channels. This model follows the 3GPP standard framework and leverages parameters such as angles, delays, and path powers defined in the GBSM model to more accurately reflect the smallscale characteristics of RIS multipath cascades. To verify the accuracy of this model, we conduct measurements of the TxRIS-Rx channel, Tx-RIS, and RIS-Rx sub-channels in a factory environment at 6.9 GHz, using the measured data to demonstrate the models validity and applicability in real-world scenarios. Validation with measured data shows that the proposed model accurately describes the characteristics of the RIS cascaded channel in terms of delay, angle, and power in complex multipath environments, providing important references for the design and deployment of RIS systems.

**Link**: [arxiv](http://arxiv.org/abs/2412.07356v1),  [pdf](http://arxiv.org/pdf/2412.07356v1)

**Tags**: eess.SP 



### Towards Predictive Communication with Brain-Computer Interfaces   integrating Large Language Models
**Authors**: Andrea Caria

**Updated**: 2024-12-10T09:48:07Z

**Summary**: This perspective article aims at providing an outline of the state of the art and future developments towards the integration of cutting-edge predictive language models with BCI. A synthetic overview of early and more recent linguistic models, from natural language processing (NLP) models to recent LLM, that to a varying extent improved predictive writing systems, is first provided. Second, a summary of previous BCI implementations integrating language models is presented. The few preliminary studies investigating the possible combination of LLM with BCI spellers to efficiently support fast communication and control are then described. Finally, current challenges and limitations towards the full integration of LLM with BCI systems are discussed. Recent investigations suggest that the combination of LLM with BCI might drastically improve human-computer interaction in patients with motor or language disorders as well as in healthy individuals. In particular, the pretrained autoregressive transformer models, such as GPT, that capitalize from parallelization, learning through pre-training and fine-tuning, promise a substantial improvement of BCI for communication with respect to previous systems incorporating simpler language models. Indeed, among various models, the GPT-2 was shown to represent an excellent candidate for its integration into BCI although testing was only perfomed on simulated conversations and not on real BCI scenarios. Prospectively, the full integration of LLM with advanced BCI systems might lead to a big leap forward towards fast, efficient and user-adaptive neurotechnology.

**Link**: [arxiv](http://arxiv.org/abs/2412.07355v1),  [pdf](http://arxiv.org/pdf/2412.07355v1)

**Tags**: cs.HC cs.CL 



### Frame Representation Hypothesis: Multi-Token LLM Interpretability and   Concept-Guided Text Generation
**Authors**: Pedro H. V. Valois, Lincon S. Souza, Erica K. Shimomoto, Kazuhiro Fukui

**Updated**: 2024-12-10T09:25:39Z

**Summary**: Interpretability is a key challenge in fostering trust for Large Language Models (LLMs), which stems from the complexity of extracting reasoning from model's parameters. We present the Frame Representation Hypothesis, a theoretically robust framework grounded in the Linear Representation Hypothesis (LRH) to interpret and control LLMs by modeling multi-token words. Prior research explored LRH to connect LLM representations with linguistic concepts, but was limited to single token analysis. As most words are composed of several tokens, we extend LRH to multi-token words, thereby enabling usage on any textual data with thousands of concepts. To this end, we propose words can be interpreted as frames, ordered sequences of vectors that better capture token-word relationships. Then, concepts can be represented as the average of word frames sharing a common concept. We showcase these tools through Top-k Concept-Guided Decoding, which can intuitively steer text generation using concepts of choice. We verify said ideas on Llama 3.1, Gemma 2, and Phi 3 families, demonstrating gender and language biases, exposing harmful content, but also potential to remediate them, leading to safer and more transparent LLMs. Code is available at https://github.com/phvv-me/frame-representation-hypothesis.git

**Link**: [arxiv](http://arxiv.org/abs/2412.07334v1),  [pdf](http://arxiv.org/pdf/2412.07334v1)

**Tags**: cs.CL 



### ConceptSearch: Towards Efficient Program Search Using LLMs for   Abstraction and Reasoning Corpus (ARC)
**Authors**: Kartik Singhal, Gautam Shroff

**Updated**: 2024-12-10T09:10:11Z

**Summary**: The Abstraction and Reasoning Corpus (ARC) poses a significant challenge to artificial intelligence, demanding broad generalization and few-shot learning capabilities that remain elusive for current deep learning methods, including large language models (LLMs). While LLMs excel in program synthesis, their direct application to ARC yields limited success. To address this, we introduce ConceptSearch, a novel function-search algorithm that leverages LLMs for program generation and employs a concept-based scoring method to guide the search efficiently. Unlike simplistic pixel-based metrics like Hamming distance, ConceptSearch evaluates programs on their ability to capture the underlying transformation concept reflected in the input-output examples. We explore three scoring functions: Hamming distance, a CNN-based scoring function, and an LLM-based natural language scoring function. Experimental results demonstrate the effectiveness of ConceptSearch, achieving a significant performance improvement over direct prompting with GPT-4. Moreover, our novel concept-based scoring exhibits up to 30% greater efficiency compared to Hamming distance, measured in terms of the number of iterations required to reach the correct solution. These findings highlight the potential of LLM-driven program search when integrated with concept-based guidance for tackling challenging generalization problems like ARC. Code: https://github.com/kksinghal/concept-search

**Link**: [arxiv](http://arxiv.org/abs/2412.07322v1),  [pdf](http://arxiv.org/pdf/2412.07322v1)

**Tags**: cs.LG 



### The Rise and Down of Babel Tower: Investigating the Evolution Process of   Multilingual Code Large Language Model
**Authors**: Jiawei Chen, Wentao Chen, Jing Su, Jingjing Xu, Hongyu Lin, Mengjie Ren, Yaojie Lu, Xianpei Han, Le Sun

**Updated**: 2024-12-10T08:28:57Z

**Summary**: Large language models (LLMs) have shown significant multilingual capabilities. However, the mechanisms underlying the development of these capabilities during pre-training are not well understood. In this paper, we use code LLMs as an experimental platform to explore the evolution of multilingual capabilities in LLMs during the pre-training process. Based on our observations, we propose the Babel Tower Hypothesis, which describes the entire process of LLMs acquiring new language capabilities. During the learning process, multiple languages initially share a single knowledge system dominated by the primary language and gradually develop language-specific knowledge systems. We then validate the above hypothesis by tracking the internal states of the LLMs through identifying working languages and language transferring neurons. Experimental results show that the internal state changes of the LLM are consistent with our Babel Tower Hypothesis. Building on these insights, we propose a novel method to construct an optimized pre-training corpus for multilingual code LLMs, which significantly outperforms LLMs trained on the original corpus. The proposed Babel Tower Hypothesis provides new insights into designing pre-training data distributions to achieve optimal multilingual capabilities in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2412.07298v1),  [pdf](http://arxiv.org/pdf/2412.07298v1)

**Tags**: cs.CL 



### CEEMS: A Resource Manager Agnostic Energy and Emissions Monitoring Stack
**Authors**: Mahendra Paipuri

**Updated**: 2024-12-10T08:18:53Z

**Summary**: With the rapid acceleration of ML/AI research in the last couple of years, the energy consumption of the Information and Communication Technology (ICT) domain has rapidly increased. As a major part of this energy consumption is due to users' workloads, it is evident that users need to be aware of the energy footprint of their applications. Compute Energy and Emissions Monitoring Stack (CEEMS) has been designed to address this issue. CEEMS can report energy consumption and equivalent emissions of user workloads in real time for HPC and cloud platforms alike. Besides CPU energy usage, it supports reporting energy usage of workloads on NVIDIA and AMD GPU accelerators. CEEMS has been built around the prominent open-source tools in the observability eco-system like Prometheus and Grafana. CEEMS has been designed to be extensible and it allows the Data Center (DC) operators to easily define the energy estimation rules of user workloads based on the underlying hardware. This paper explains the architectural overview of CEEMS, data sources that are used to measure energy usage and estimate equivalent emissions and potential use cases of CEEMS from operator and user perspectives. Finally, the paper will conclude by describing how CEEMS deployment on the Jean-Zay supercomputing platform is capable of monitoring more than 1400 nodes that have a daily job churn rate of around 20k jobs.

**Link**: [arxiv](http://arxiv.org/abs/2412.07290v1),  [pdf](http://arxiv.org/pdf/2412.07290v1)

**Tags**: eess.SY cs.SY 



### Enhancing Relation Extraction via Supervised Rationale Verification and   Feedback
**Authors**: Yongqi Li, Xin Miao, Shen Zhou, Mayi Xu, Yuyang Ren, Tieyun Qian

**Updated**: 2024-12-10T08:18:29Z

**Summary**: Despite the rapid progress that existing automated feedback methods have made in correcting the output of large language models (LLMs), these methods cannot be well applied to the relation extraction (RE) task due to their designated feedback objectives and correction manner. To address this problem, we propose a novel automated feedback framework for RE, which presents a rationale supervisor to verify the rationale and provide re-selected demonstrations as feedback to correct the initial prediction. Specifically, we first design a causal intervention and observation method for to collect biased/unbiased rationales for contrastive training the rationale supervisor. Then, we present a verification-feedback-correction procedure to iteratively enhance LLMs' capability of handling the RE task. Extensive experiments prove that our proposed framework significantly outperforms existing methods.

**Link**: [arxiv](http://arxiv.org/abs/2412.07289v1),  [pdf](http://arxiv.org/pdf/2412.07289v1)

**Tags**: cs.CL cs.AI 



### Argumentative Experience: Reducing Confirmation Bias on Controversial   Issues through LLM-Generated Multi-Persona Debates
**Authors**: Li Shi, Houjiang Liu, Yian Wong, Utkarsh Mujumdar, Dan Zhang, Jacek Gwizdka, Matthew Lease

**Updated**: 2024-12-10T08:02:24Z

**Summary**: Large language models (LLMs) are enabling designers to give life to exciting new user experiences for information access. In this work, we present a system that generates LLM personas to debate a topic of interest from different perspectives. How might information seekers use and benefit from such a system? Can centering information access around diverse viewpoints help to mitigate thorny challenges like confirmation bias in which information seekers over-trust search results matching existing beliefs? How do potential biases and hallucinations in LLMs play out alongside human users who are also fallible and possibly biased?   Our study exposes participants to multiple viewpoints on controversial issues via a mixed-methods, within-subjects study. We use eye-tracking metrics to quantitatively assess cognitive engagement alongside qualitative feedback. Compared to a baseline search system, we see more creative interactions and diverse information-seeking with our multi-persona debate system, which more effectively reduces user confirmation bias and conviction toward their initial beliefs. Overall, our study contributes to the emerging design space of LLM-based information access systems, specifically investigating the potential of simulated personas to promote greater exposure to information diversity, emulate collective intelligence, and mitigate bias in information seeking.

**Link**: [arxiv](http://arxiv.org/abs/2412.04629v2),  [pdf](http://arxiv.org/pdf/2412.04629v2)

**Tags**: cs.HC cs.CY cs.IR 



### Chatbots im Schulunterricht: Wir testen das Fobizz-Tool zur   automatischen Bewertung von Hausaufgaben
**Authors**: Rainer Muehlhoff, Marte Henningsen

**Updated**: 2024-12-10T07:55:50Z

**Summary**: [Study in German language.] This study examines the AI-powered grading tool "AI Grading Assistant" by the German company Fobizz, designed to support teachers in evaluating and providing feedback on student assignments. Against the societal backdrop of an overburdened education system and rising expectations for artificial intelligence as a solution to these challenges, the investigation evaluates the tool's functional suitability through two test series. The results reveal significant shortcomings: The tool's numerical grades and qualitative feedback are often random and do not improve even when its suggestions are incorporated. The highest ratings are achievable only with texts generated by ChatGPT. False claims and nonsensical submissions frequently go undetected, while the implementation of some grading criteria is unreliable and opaque. Since these deficiencies stem from the inherent limitations of large language models (LLMs), fundamental improvements to this or similar tools are not immediately foreseeable. The study critiques the broader trend of adopting AI as a quick fix for systemic problems in education, concluding that Fobizz's marketing of the tool as an objective and time-saving solution is misleading and irresponsible. Finally, the study calls for systematic evaluation and subject-specific pedagogical scrutiny of the use of AI tools in educational contexts.

**Link**: [arxiv](http://arxiv.org/abs/2412.06651v2),  [pdf](http://arxiv.org/pdf/2412.06651v2)

**Tags**: cs.CY 97B10 



### DSTC: Direct Preference Learning with Only Self-Generated Tests and Code   to Improve Code LMs
**Authors**: Zhihan Liu, Shenao Zhang, Yongfei Liu, Boyi Liu, Yingxiang Yang, Zhaoran Wang

**Updated**: 2024-12-10T07:47:15Z

**Summary**: Direct preference learning offers a promising and computation-efficient beyond supervised fine-tuning (SFT) for improving code generation in coding large language models (LMs). However, the scarcity of reliable preference data is a bottleneck for the performance of direct preference learning to improve the coding accuracy of code LMs. In this paper, we introduce \underline{\textbf{D}}irect Preference Learning with Only \underline{\textbf{S}}elf-Generated \underline{\textbf{T}}ests and \underline{\textbf{C}}ode (DSTC), a framework that leverages only self-generated code snippets and tests to construct reliable preference pairs such that direct preference learning can improve LM coding accuracy without external annotations. DSTC combines a minimax selection process and test-code concatenation to improve preference pair quality, reducing the influence of incorrect self-generated tests and enhancing model performance without the need for costly reward models. When applied with direct preference learning methods such as Direct Preference Optimization (DPO) and Kahneman-Tversky Optimization (KTO), DSTC yields stable improvements in coding accuracy (pass@1 score) across diverse coding benchmarks, including HumanEval, MBPP, and BigCodeBench, demonstrating both its effectiveness and scalability for models of various sizes. This approach autonomously enhances code generation accuracy across LLMs of varying sizes, reducing reliance on expensive annotated coding datasets.

**Link**: [arxiv](http://arxiv.org/abs/2411.13611v3),  [pdf](http://arxiv.org/pdf/2411.13611v3)

**Tags**: cs.SE cs.AI 



### Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs   and Human Perception
**Authors**: Luyang Lin, Lingzhi Wang, Jinsong Guo, Kam-Fai Wong

**Updated**: 2024-12-10T07:43:50Z

**Summary**: The pervasive spread of misinformation and disinformation in social media underscores the critical importance of detecting media bias. While robust Large Language Models (LLMs) have emerged as foundational tools for bias prediction, concerns about inherent biases within these models persist. In this work, we investigate the presence and nature of bias within LLMs and its consequential impact on media bias detection. Departing from conventional approaches that focus solely on bias detection in media content, we delve into biases within the LLM systems themselves. Through meticulous examination, we probe whether LLMs exhibit biases, particularly in political bias prediction and text continuation tasks. Additionally, we explore bias across diverse topics, aiming to uncover nuanced variations in bias expression within the LLM framework. Importantly, we propose debiasing strategies, including prompt engineering and model fine-tuning. Extensive analysis of bias tendencies across different LLMs sheds light on the broader landscape of bias propagation in language models. This study advances our understanding of LLM bias, offering critical insights into its implications for bias detection tasks and paving the way for more robust and equitable AI systems

**Link**: [arxiv](http://arxiv.org/abs/2403.14896v2),  [pdf](http://arxiv.org/pdf/2403.14896v2)

**Tags**: cs.CY 



### MemHunter: Automated and Verifiable Memorization Detection at   Dataset-scale in LLMs
**Authors**: Zhenpeng Wu, Jian Lou, Zibin Zheng, Chuan Chen

**Updated**: 2024-12-10T07:42:46Z

**Summary**: Large language models (LLMs) have been shown to memorize and reproduce content from their training data, raising significant privacy concerns, especially with web-scale datasets. Existing methods for detecting memorization are largely sample-specific, relying on manually crafted or discretely optimized memory-inducing prompts generated on a per-sample basis, which become impractical for dataset-level detection due to the prohibitive computational cost of iterating over all samples. In real-world scenarios, data owners may need to verify whether a susceptible LLM has memorized their dataset, particularly if the LLM may have collected the data from the web without authorization. To address this, we introduce \textit{MemHunter}, which trains a memory-inducing LLM and employs hypothesis testing to efficiently detect memorization at the dataset level, without requiring sample-specific memory inducing. Experiments on models such as Pythia and Llama-2 demonstrate that \textit{MemHunter} can extract up to 40\% more training data than existing methods under constrained time resources and reduce search time by up to 80\% when integrated as a plug-in. Crucially, \textit{MemHunter} is the first method capable of dataset-level memorization detection, providing an indispensable tool for assessing privacy risks in LLMs that are powered by vast web-sourced datasets.

**Link**: [arxiv](http://arxiv.org/abs/2412.07261v1),  [pdf](http://arxiv.org/pdf/2412.07261v1)

**Tags**: cs.CR cs.LG 



### LEARN: Knowledge Adaptation from Large Language Model to Recommendation   for Practical Industrial Application
**Authors**: Jian Jia, Yipei Wang, Yan Li, Honggang Chen, Xuehan Bai, Zhaocheng Liu, Jian Liang, Quan Chen, Han Li, Peng Jiang, Kun Gai

**Updated**: 2024-12-10T07:40:54Z

**Summary**: Contemporary recommendation systems predominantly rely on ID embedding to capture latent associations among users and items. However, this approach overlooks the wealth of semantic information embedded within textual descriptions of items, leading to suboptimal performance and poor generalizations. Leveraging the capability of large language models to comprehend and reason about textual content presents a promising avenue for advancing recommendation systems. To achieve this, we propose an Llm-driven knowlEdge Adaptive RecommeNdation (LEARN) framework that synergizes open-world knowledge with collaborative knowledge. We address computational complexity concerns by utilizing pretrained LLMs as item encoders and freezing LLM parameters to avoid catastrophic forgetting and preserve open-world knowledge. To bridge the gap between the open-world and collaborative domains, we design a twin-tower structure supervised by the recommendation task and tailored for practical industrial application. Through experiments on the real large-scale industrial dataset and online A/B tests, we demonstrate the efficacy of our approach in industry application. We also achieve state-of-the-art performance on six Amazon Review datasets to verify the superiority of our method.

**Link**: [arxiv](http://arxiv.org/abs/2405.03988v2),  [pdf](http://arxiv.org/pdf/2405.03988v2)

**Tags**: cs.IR cs.AI 



### Label-Confidence-Aware Uncertainty Estimation in Natural Language   Generation
**Authors**: Qinhong Lin, Linna Zhou, Zhongliang Yang, Yuang Cai

**Updated**: 2024-12-10T07:35:23Z

**Summary**: Large Language Models (LLMs) display formidable capabilities in generative tasks but also pose potential risks due to their tendency to generate hallucinatory responses. Uncertainty Quantification (UQ), the evaluation of model output reliability, is crucial for ensuring the safety and robustness of AI systems. Recent studies have concentrated on model uncertainty by analyzing the relationship between output entropy under various sampling conditions and the corresponding labels. However, these methods primarily focus on measuring model entropy with precision to capture response characteristics, often neglecting the uncertainties associated with greedy decoding results-the sources of model labels, which can lead to biased classification outcomes. In this paper, we explore the biases introduced by greedy decoding and propose a label-confidence-aware (LCA) uncertainty estimation based on Kullback-Leibler (KL) divergence bridging between samples and label source, thus enhancing the reliability and stability of uncertainty assessments. Our empirical evaluations across a range of popular LLMs and NLP datasets reveal that different label sources can indeed affect classification, and that our approach can effectively capture differences in sampling results and label sources, demonstrating more effective uncertainty estimation.

**Link**: [arxiv](http://arxiv.org/abs/2412.07255v1),  [pdf](http://arxiv.org/pdf/2412.07255v1)

**Tags**: cs.CL cs.AI 



### Towards an Understanding of Large Language Models in Software   Engineering Tasks
**Authors**: Zibin Zheng, Kaiwen Ning, Qingyuan Zhong, Jiachi Chen, Wenqing Chen, Lianghong Guo, Weicheng Wang, Yanlin Wang

**Updated**: 2024-12-10T07:35:21Z

**Summary**: Large Language Models (LLMs) have drawn widespread attention and research due to their astounding performance in text generation and reasoning tasks. Derivative products, like ChatGPT, have been extensively deployed and highly sought after. Meanwhile, the evaluation and optimization of LLMs in software engineering tasks, such as code generation, have become a research focus. However, there is still a lack of systematic research on applying and evaluating LLMs in software engineering. Therefore, this paper comprehensively investigate and collate the research and products combining LLMs with software engineering, aiming to answer two questions: (1) What are the current integrations of LLMs with software engineering? (2) Can LLMs effectively handle software engineering tasks? To find the answers, we have collected related literature as extensively as possible from seven mainstream databases and selected 123 timely papers published starting from 2022 for analysis. We have categorized these papers in detail and reviewed the current research status of LLMs from the perspective of seven major software engineering tasks, hoping this will help researchers better grasp the research trends and address the issues when applying LLMs. Meanwhile, we have also organized and presented papers with evaluation content to reveal the performance and effectiveness of LLMs in various software engineering tasks, guiding researchers and developers to optimize.

**Link**: [arxiv](http://arxiv.org/abs/2308.11396v3),  [pdf](http://arxiv.org/pdf/2308.11396v3)

**Tags**: cs.SE 



### Filling Memory Gaps: Enhancing Continual Semantic Parsing via SQL Syntax   Variance-Guided LLMs without Real Data Replay
**Authors**: Ruiheng Liu, Jinyu Zhang, Yanqi Song, Yu Zhang, Bailong Yang

**Updated**: 2024-12-10T07:11:49Z

**Summary**: Continual Semantic Parsing (CSP) aims to train parsers to convert natural language questions into SQL across tasks with limited annotated examples, adapting to the real-world scenario of dynamically updated databases. Previous studies mitigate this challenge by replaying historical data or employing parameter-efficient tuning (PET), but they often violate data privacy or rely on ideal continual learning settings. To address these problems, we propose a new Large Language Model (LLM)-Enhanced Continuous Semantic Parsing method, named LECSP, which alleviates forgetting while encouraging generalization, without requiring real data replay or ideal settings. Specifically, it first analyzes the commonalities and differences between tasks from the SQL syntax perspective to guide LLMs in reconstructing key memories and improving memory accuracy through a calibration strategy. Then, it uses a task-aware dual-teacher distillation framework to promote the accumulation and transfer of knowledge during sequential training. Experimental results on two CSP benchmarks show that our method significantly outperforms existing methods, even those utilizing data replay or ideal settings. Additionally, we achieve generalization performance beyond the upper limits, better adapting to unseen tasks.

**Link**: [arxiv](http://arxiv.org/abs/2412.07246v1),  [pdf](http://arxiv.org/pdf/2412.07246v1)

**Tags**: cs.CL cs.DB 



### Policy Filtration in RLHF to Fine-Tune LLM for Code Generation
**Authors**: Wei Shen, Chuheng Zhang

**Updated**: 2024-12-10T06:21:47Z

**Summary**: Reinforcement learning from human feedback (RLHF) is one of the key techniques that helps large language models (LLMs) to follow instructions and provide helpful and harmless responses. While direct policy optimization methods exist, state-of-the-art LLMs adopt RL-based methods (usually PPO) in RLHF to train the policy to generate good responses guided by a reward model learned from preference data. The main challenge of these methods is the inaccuracy of the intermediate reward model, especially in code generation tasks that require long and complex reasoning to score a response. We find that the reliability of the reward model varies across responses assigned with different rewards. This motivates us to filter the samples whose rewards may be unreliable to improve signal-to-noise ratio during policy learning, resulting in Policy Filtration for Proximal Policy Optimization (PF-PPO). To choose a proper policy filtration strategy for a given reward model, the coefficient of determination ($R^2$) between rewards and actual scores on filtered samples serves as a good metrics and helps us find several promising strategies. We provide extensive experiments to validate the effectiveness of PF-PPO in code generation tasks, and find that some variants of PF-PPO are highly effective and achieve new state-of-the-art performance across 7-billion-parameter models on HumanEval, MBPP, and a new and more challenging LeetCode Contest benchmark.

**Link**: [arxiv](http://arxiv.org/abs/2409.06957v2),  [pdf](http://arxiv.org/pdf/2409.06957v2)

**Tags**: cs.LG cs.AI 



### Towards Automated Cross-domain Exploratory Data Analysis through Large   Language Models
**Authors**: Jun-Peng Zhu, Boyan Niu, Peng cai, Zheming Ni, Jianwei Wan, Kai Xu, Jiajun Huang, Shengbo Ma, Bing Wang, Xuan Zhou, Guanglei Bao, Donghui Zhang, Liu Tang, Qi Liu

**Updated**: 2024-12-10T06:11:23Z

**Summary**: Exploratory data analysis (EDA), coupled with SQL, is essential for data analysts involved in data exploration and analysis. However, data analysts often encounter two primary challenges: (1) the need to craft SQL queries skillfully, and (2) the requirement to generate suitable visualization types that enhance the interpretation of query results. Due to its significance, substantial research efforts have been made to explore different approaches to address these challenges, including leveraging large language models (LLMs). However, existing methods fail to meet real-world data exploration requirements primarily due to (1) complex database schema; (2) unclear user intent; (3) limited cross-domain generalization capability; and (4) insufficient end-to-end text-to-visualization capability.   This paper presents TiInsight, an automated SQL-based cross-domain exploratory data analysis system. First, we propose hierarchical data context (i.e., HDC), which leverages LLMs to summarize the contexts related to the database schema, which is crucial for open-world EDA systems to generalize across data domains. Second, the EDA system is divided into four components (i.e., stages): HDC generation, question clarification and decomposition, text-to-SQL generation (i.e., TiSQL), and data visualization (i.e., TiChart). Finally, we implemented an end-to-end EDA system with a user-friendly GUI interface in the production environment at PingCAP. We have also open-sourced all APIs of TiInsight to facilitate research within the EDA community. Through extensive evaluations by a real-world user study, we demonstrate that TiInsight offers remarkable performance compared to human experts. Specifically, TiSQL achieves an execution accuracy of 86.3% on the Spider dataset using GPT-4. It also demonstrates state-of-the-art performance on the Bird dataset.

**Link**: [arxiv](http://arxiv.org/abs/2412.07214v1),  [pdf](http://arxiv.org/pdf/2412.07214v1)

**Tags**: cs.DB cs.AI H.2.8 



### IntellectSeeker: A Personalized Literature Management System with the   Probabilistic Model and Large Language Model
**Authors**: Weizhen Bian, Siyan Liu, Yubo Zhou, Dezhi Chen, Yijie Liao, Zhenzhen Fan, Aobo Wang

**Updated**: 2024-12-10T06:09:49Z

**Summary**: Faced with the burgeoning volume of academic literature, researchers often need help with uncertain article quality and mismatches in term searches using traditional academic engines. We introduce IntellectSeeker, an innovative and personalized intelligent academic literature management platform to address these challenges. This platform integrates a Large Language Model (LLM)--based semantic enhancement bot with a sophisticated probability model to personalize and streamline literature searches. We adopted the GPT-3.5-turbo model to transform everyday language into professional academic terms across various scenarios using multiple rounds of few-shot learning. This adaptation mainly benefits academic newcomers, effectively bridging the gap between general inquiries and academic terminology. The probabilistic model intelligently filters academic articles to align closely with the specific interests of users, which are derived from explicit needs and behavioral patterns. Moreover, IntellectSeeker incorporates an advanced recommendation system and text compression tools. These features enable intelligent article recommendations based on user interactions and present search results through concise one-line summaries and innovative word cloud visualizations, significantly enhancing research efficiency and user experience. IntellectSeeker offers academic researchers a highly customizable literature management solution with exceptional search precision and matching capabilities. The code can be found here: https://github.com/LuckyBian/ISY5001

**Link**: [arxiv](http://arxiv.org/abs/2412.07213v1),  [pdf](http://arxiv.org/pdf/2412.07213v1)

**Tags**: cs.IR cs.AI 



### EDiT: A Local-SGD-Based Efficient Distributed Training Method for Large   Language Models
**Authors**: Jialiang Cheng, Ning Gao, Yun Yue, Zhiling Ye, Jiadi Jiang, Jian Sha

**Updated**: 2024-12-10T06:08:24Z

**Summary**: Distributed training methods are crucial for large language models (LLMs). However, existing distributed training methods often suffer from communication bottlenecks, stragglers, and limited elasticity. Local SGD methods have been proposed to address these issues, but their effectiveness remains limited to small-scale training due to additional memory overhead and lack of concerns on efficiency and stability. To tackle these issues, we propose EDiT, an innovative Efficient Distributed Training method that combines a tailored Local SGD approach with model sharding techniques to enhance large-scale training efficiency. EDiT performs layer-wise parameter synchronization during forward pass, reducing communication and memory overhead and enabling the overlap of computation and communication. Besides, EDiT employs a pseudo gradient penalty strategy to suppress loss spikes, which ensures training stability and improve performance. Additionally, we introduce A-EDiT, a fully asynchronous variant of EDiT that accommodates heterogeneous clusters. Building on EDiT/A-EDiT, we conduct a series of experiments to validate large-scale asynchronous training for LLMs, accompanied by comprehensive analyses. Experimental results demonstrate the superior performance of EDiT/A-EDiT, establishing them as robust solutions for distributed LLM training in diverse computational ecosystems.

**Link**: [arxiv](http://arxiv.org/abs/2412.07210v1),  [pdf](http://arxiv.org/pdf/2412.07210v1)

**Tags**: cs.DC cs.AI 



### MAPLE: A Framework for Active Preference Learning Guided by Large   Language Models
**Authors**: Saaduddin Mahmud, Mason Nakamura, Shlomo Zilberstein

**Updated**: 2024-12-10T05:55:14Z

**Summary**: The advent of large language models (LLMs) has sparked significant interest in using natural language for preference learning. However, existing methods often suffer from high computational burdens, taxing human supervision, and lack of interpretability. To address these issues, we introduce MAPLE, a framework for large language model-guided Bayesian active preference learning. MAPLE leverages LLMs to model the distribution over preference functions, conditioning it on both natural language feedback and conventional preference learning feedback, such as pairwise trajectory rankings. MAPLE also employs active learning to systematically reduce uncertainty in this distribution and incorporates a language-conditioned active query selection mechanism to identify informative and easy-to-answer queries, thus reducing human burden. We evaluate MAPLE's sample efficiency and preference inference quality across two benchmarks, including a real-world vehicle route planning benchmark using OpenStreetMap data. Our results demonstrate that MAPLE accelerates the learning process and effectively improves humans' ability to answer queries.

**Link**: [arxiv](http://arxiv.org/abs/2412.07207v1),  [pdf](http://arxiv.org/pdf/2412.07207v1)

**Tags**: cs.LG cs.AI cs.CL 



### LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods
**Authors**: Haitao Li, Qian Dong, Junjie Chen, Huixue Su, Yujia Zhou, Qingyao Ai, Ziyi Ye, Yiqun Liu

**Updated**: 2024-12-10T05:49:12Z

**Summary**: The rapid advancement of Large Language Models (LLMs) has driven their expanding application across various fields. One of the most promising applications is their role as evaluators based on natural language responses, referred to as ''LLMs-as-judges''. This framework has attracted growing attention from both academia and industry due to their excellent effectiveness, ability to generalize across tasks, and interpretability in the form of natural language. This paper presents a comprehensive survey of the LLMs-as-judges paradigm from five key perspectives: Functionality, Methodology, Applications, Meta-evaluation, and Limitations. We begin by providing a systematic definition of LLMs-as-Judges and introduce their functionality (Why use LLM judges?). Then we address methodology to construct an evaluation system with LLMs (How to use LLM judges?). Additionally, we investigate the potential domains for their application (Where to use LLM judges?) and discuss methods for evaluating them in various contexts (How to evaluate LLM judges?). Finally, we provide a detailed analysis of the limitations of LLM judges and discuss potential future directions. Through a structured and comprehensive analysis, we aim aims to provide insights on the development and application of LLMs-as-judges in both research and practice. We will continue to maintain the relevant resource list at https://github.com/CSHaitao/Awesome-LLMs-as-Judges.

**Link**: [arxiv](http://arxiv.org/abs/2412.05579v2),  [pdf](http://arxiv.org/pdf/2412.05579v2)

**Tags**: cs.CL cs.IR 



### BrokerChain: A Blockchain Sharding Protocol by Exploiting Broker   Accounts
**Authors**: Huawei Huang, Zhaokang Yin, Qinde Chen, Guang Ye, Xiaowen Peng, Yue Lin, Zibin Zheng, Song Guo

**Updated**: 2024-12-10T05:41:59Z

**Summary**: State-of-the-art blockchain sharding solutions such as Monoxide, can cause severely imbalanced distribution of transaction (TX) workloads across all blockchain shards due to the deployment policy of their accounts. Imbalanced TX distributions then produce hot shards, in which the cross-shard TXs may experience an unlimited confirmation latency. Thus, how to address the hot-shard issue and how to reduce crossshard TXs become significant challenges of blockchain sharding. Through reviewing the related studies, we find that a crossshard TX protocol that can achieve workload balance among all shards and simultaneously reduce the quantity of crossshard TXs is still absent from the literature. To this end, we propose BrokerChain, which is a cross-shard blockchain protocol dedicated to account-based state sharding. Essentially, BrokerChain exploits fine-grained state partition and account segmentation. We also elaborate on how BrokerChain handles cross-shard TXs through broker accounts. The security issues and other properties of BrokerChain are analyzed rigorously. Finally, we conduct comprehensive evaluations using an opensource blockchain sharding prototype named BlockEmulator. The evaluation results show that BrokerChain outperforms other baselines in terms of transaction throughput, transaction confirmation latency, the queue size of the transaction pool, and workload balance.

**Link**: [arxiv](http://arxiv.org/abs/2412.07202v1),  [pdf](http://arxiv.org/pdf/2412.07202v1)

**Tags**: cs.CR 



### From Generation to Judgment: Opportunities and Challenges of   LLM-as-a-judge
**Authors**: Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, Kai Shu, Lu Cheng, Huan Liu

**Updated**: 2024-12-10T05:24:37Z

**Summary**: Assessment and evaluation have long been critical challenges in artificial intelligence (AI) and natural language processing (NLP). However, traditional methods, whether matching-based or embedding-based, often fall short of judging subtle attributes and delivering satisfactory results. Recent advancements in Large Language Models (LLMs) inspire the "LLM-as-a-judge" paradigm, where LLMs are leveraged to perform scoring, ranking, or selection across various tasks and applications. This paper provides a comprehensive survey of LLM-based judgment and assessment, offering an in-depth overview to advance this emerging field. We begin by giving detailed definitions from both input and output perspectives. Then we introduce a comprehensive taxonomy to explore LLM-as-a-judge from three dimensions: what to judge, how to judge and where to judge. Finally, we compile benchmarks for evaluating LLM-as-a-judge and highlight key challenges and promising directions, aiming to provide valuable insights and inspire future research in this promising research area. Paper list and more resources about LLM-as-a-judge can be found at \url{https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge} and \url{https://llm-as-a-judge.github.io}.

**Link**: [arxiv](http://arxiv.org/abs/2411.16594v2),  [pdf](http://arxiv.org/pdf/2411.16594v2)

**Tags**: cs.AI cs.CL 



### When Graph Meets Retrieval Augmented Generation for Wireless Networks: A   Tutorial and Case Study
**Authors**: Yang Xiong, Ruichen Zhang, Yinqiu Liu, Dusit Niyato, Zehui Xiong, Ying-Chang Liang, Shiwen Mao

**Updated**: 2024-12-10T04:55:57Z

**Summary**: The rapid development of next-generation networking technologies underscores their transformative role in revolutionizing modern communication systems, enabling faster, more reliable, and highly interconnected solutions. However, such development has also brought challenges to network optimizations. Thanks to the emergence of Large Language Models (LLMs) in recent years, tools including Retrieval Augmented Generation (RAG) have been developed and applied in various fields including networking, and have shown their effectiveness. Taking one step further, the integration of knowledge graphs into RAG frameworks further enhanced the performance of RAG in networking applications such as Intent-Driven Networks (IDNs) and spectrum knowledge maps by providing more contextually relevant responses through more accurate retrieval of related network information. This paper introduces the RAG framework that integrates knowledge graphs in its database and explores such framework's application in networking. We begin by exploring RAG's applications in networking and the limitations of conventional RAG and present the advantages that knowledge graphs' structured knowledge representation brings to the retrieval and generation processes. Next, we propose a detailed GraphRAG-based framework for networking, including a step-by-step tutorial on its construction. Our evaluation through a case study on channel gain prediction demonstrates GraphRAG's enhanced capability in generating accurate, contextually rich responses, surpassing traditional RAG models. Finally, we discuss key future directions for applying knowledge-graphs-empowered RAG frameworks in networking, including robust updates, mitigation of hallucination, and enhanced security measures for networking applications.

**Link**: [arxiv](http://arxiv.org/abs/2412.07189v1),  [pdf](http://arxiv.org/pdf/2412.07189v1)

**Tags**: cs.NI 



### An Enhancement of CNN Algorithm for Rice Leaf Disease Image   Classification in Mobile Applications
**Authors**: Kayne Uriel K. Rodrigo, Jerriane Hillary Heart S. Marcial, Samuel C. Brillo, Khatalyn E. Mata, Jonathan C. Morano

**Updated**: 2024-12-10T04:41:10Z

**Summary**: This study focuses on enhancing rice leaf disease image classification algorithms, which have traditionally relied on Convolutional Neural Network (CNN) models. We employed transfer learning with MobileViTV2_050 using ImageNet-1k weights, a lightweight model that integrates CNN's local feature extraction with Vision Transformers' global context learning through a separable self-attention mechanism. Our approach resulted in a significant 15.66% improvement in classification accuracy for MobileViTV2_050-A, our first enhanced model trained on the baseline dataset, achieving 93.14%. Furthermore, MobileViTV2_050-B, our second enhanced model trained on a broader rice leaf dataset, demonstrated a 22.12% improvement, reaching 99.6% test accuracy. Additionally, MobileViTV2-A attained an F1-score of 93% across four rice labels and a Receiver Operating Characteristic (ROC) curve ranging from 87% to 97%. In terms of resource consumption, our enhanced models reduced the total parameters of the baseline CNN model by up to 92.50%, from 14 million to 1.1 million. These results indicate that MobileViTV2_050 not only improves computational efficiency through its separable self-attention mechanism but also enhances global context learning. Consequently, it offers a lightweight and robust solution suitable for mobile deployment, advancing the interpretability and practicality of models in precision agriculture.

**Link**: [arxiv](http://arxiv.org/abs/2412.07182v1),  [pdf](http://arxiv.org/pdf/2412.07182v1)

**Tags**: cs.CV cs.AI 



### Data Quality Enhancement on the Basis of Diversity with Large Language   Models for Text Classification: Uncovered, Difficult, and Noisy
**Authors**: Min Zeng, Caiquan Liu, Shiqi Zhang, Li Xie, Chen Sang, Xiaoxin Chen

**Updated**: 2024-12-10T04:35:28Z

**Summary**: In recent years, the use of large language models (LLMs) for text classification has attracted widespread attention. Despite this, the classification accuracy of LLMs has not yet universally surpassed that of smaller models. LLMs can enhance their performance in text classification through fine-tuning. However, existing data quality research based on LLMs is challenging to apply directly to solve text classification problems. To further improve the performance of LLMs in classification tasks, this paper proposes a data quality enhancement (DQE) method for text classification based on LLMs. This method starts by using a greedy algorithm to select data, dividing the dataset into sampled and unsampled subsets, and then performing fine-tuning of the LLMs using the sampled data. Subsequently, this model is used to predict the outcomes for the unsampled data, categorizing incorrectly predicted data into uncovered, difficult, and noisy data. Experimental results demonstrate that our method effectively enhances the performance of LLMs in text classification tasks and significantly improves training efficiency, saving nearly half of the training time. Our method has achieved state-of-the-art performance in several open-source classification tasks.

**Link**: [arxiv](http://arxiv.org/abs/2412.06575v2),  [pdf](http://arxiv.org/pdf/2412.06575v2)

**Tags**: cs.CL 



### Leveraging LLM for Automated Ontology Extraction and Knowledge Graph   Generation
**Authors**: Mohammad Sadeq Abolhasani, Rong Pan

**Updated**: 2024-12-10T04:28:36Z

**Summary**: Extracting relevant and structured knowledge from large, complex technical documents within the Reliability and Maintainability (RAM) domain is labor-intensive and prone to errors. Our work addresses this challenge by presenting OntoKGen, a genuine pipeline for ontology extraction and Knowledge Graph (KG) generation. OntoKGen leverages Large Language Models (LLMs) through an interactive user interface guided by our adaptive iterative Chain of Thought (CoT) algorithm to ensure that the ontology extraction process and, thus, KG generation align with user-specific requirements. Although KG generation follows a clear, structured path based on the confirmed ontology, there is no universally correct ontology as it is inherently based on the user's preferences. OntoKGen recommends an ontology grounded in best practices, minimizing user effort and providing valuable insights that may have been overlooked, all while giving the user complete control over the final ontology. Having generated the KG based on the confirmed ontology, OntoKGen enables seamless integration into schemeless, non-relational databases like Neo4j. This integration allows for flexible storage and retrieval of knowledge from diverse, unstructured sources, facilitating advanced querying, analysis, and decision-making. Moreover, the generated KG serves as a robust foundation for future integration into Retrieval Augmented Generation (RAG) systems, offering enhanced capabilities for developing domain-specific intelligent applications.

**Link**: [arxiv](http://arxiv.org/abs/2412.00608v3),  [pdf](http://arxiv.org/pdf/2412.00608v3)

**Tags**: cs.AI 



### Post-Training Statistical Calibration for Higher Activation Sparsity
**Authors**: Vui Seng Chua, Yujie Pan, Nilesh Jain

**Updated**: 2024-12-10T04:15:39Z

**Summary**: We present Statistical Calibrated Activation Pruning (SCAP), a post-training activation pruning framework that (1) generalizes sparsification by input activations of Fully-Connected layers for generic and flexible application across Transformers, and (2) features a simple Mode-Centering technique to pre-calibrate activation distributions for maximizing post-training sparsity. Our results demonstrate robust Pareto efficiency compared to prior methods, translating to a 1.5x additional LLM decoding speedup against CATS at iso model quality. SCAP effectiveness is empirically verified across a wide range of models, including recent Transformer Decoders, MoE, Mamba2, Encoding Transformer, and pre-quantized models, highlighting its practicality and scalability. The code is available at: https://github.com/IntelLabs/SCAP.

**Link**: [arxiv](http://arxiv.org/abs/2412.07174v1),  [pdf](http://arxiv.org/pdf/2412.07174v1)

**Tags**: cs.LG cs.AI 



### Breaking the Stage Barrier: A Novel Single-Stage Approach to Long   Context Extension for Large Language Models
**Authors**: Haoran Lian, Junmin Chen, Wei Huang, Yizhe Xiong, Wenping Hu, Guiguang Ding, Hui Chen, Jianwei Niu, Zijia Lin, Fuzheng Zhang, Di Zhang

**Updated**: 2024-12-10T04:09:29Z

**Summary**: Recently, Large language models (LLMs) have revolutionized Natural Language Processing (NLP). Pretrained LLMs, due to limited training context size, struggle with handling long token sequences, limiting their performance on various downstream tasks. Current solutions toward long context modeling often employ multi-stage continual pertaining, which progressively increases the effective context length through several continual pretraining stages. However, those approaches require extensive manual tuning and human expertise. In this paper, we introduce a novel single-stage continual pretraining method, Head-Adaptive Rotary Position Encoding (HARPE), to equip LLMs with long context modeling capabilities while simplifying the training process. Our HARPE leverages different Rotary Position Encoding (RoPE) base frequency values across different attention heads and directly trains LLMs on the target context length. Extensive experiments on 4 language modeling benchmarks, including the latest RULER benchmark, demonstrate that HARPE excels in understanding and integrating long-context tasks with single-stage training, matching and even outperforming existing multi-stage methods. Our results highlight that HARPE successfully breaks the stage barrier for training LLMs with long context modeling capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2412.07171v1),  [pdf](http://arxiv.org/pdf/2412.07171v1)

**Tags**: cs.CL 



### S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by   Structured Sparsity
**Authors**: Xinyu Yang, Jixuan Leng, Geyang Guo, Jiawei Zhao, Ryumei Nakada, Linjun Zhang, Huaxiu Yao, Beidi Chen

**Updated**: 2024-12-10T03:43:32Z

**Summary**: Current PEFT methods for LLMs can achieve either high quality, efficient training, or scalable serving, but not all three simultaneously. To address this limitation, we investigate sparse fine-tuning and observe a remarkable improvement in generalization ability. Utilizing this key insight, we propose a family of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, which concurrently achieve state-of-the-art fine-tuning performance, training efficiency, and inference scalability. S$^{2}$FT accomplishes this by "selecting sparsely and computing densely". It selects a few heads and channels in the MHA and FFN modules for each Transformer block, respectively. Next, it co-permutes weight matrices on both sides of the coupled structures in LLMs to connect the selected components in each layer into a dense submatrix. Finally, S$^{2}$FT performs in-place gradient updates on all submatrices. Through theoretical analysis and empirical results, our method prevents overfitting and forgetting, delivers SOTA performance on both commonsense and arithmetic reasoning with 4.6% and 1.3% average improvements compared to LoRA, and surpasses full FT by 11.5% when generalizing to various domains after instruction tuning. Using our partial backpropagation algorithm, S$^{2}$FT saves training memory up to 3$\times$ and improves latency by 1.5-2.7$\times$ compared to full FT, while delivering an average 10% improvement over LoRA on both metrics. We further demonstrate that the weight updates in S$^{2}$FT can be decoupled into adapters, enabling effective fusion, fast switch, and efficient parallelism for serving multiple fine-tuned models.

**Link**: [arxiv](http://arxiv.org/abs/2412.06289v2),  [pdf](http://arxiv.org/pdf/2412.06289v2)

**Tags**: cs.LG cs.AI 



### From Text to Multimodality: Exploring the Evolution and Impact of Large   Language Models in Medical Practice
**Authors**: Qian Niu, Keyu Chen, Ming Li, Pohsun Feng, Ziqian Bi, Lawrence KQ Yan, Yichao Zhang, Caitlyn Heqi Yin, Cheng Fei, Junyu Liu, Benji Peng, Tianyang Wang, Yunze Wang, Silin Chen, Ming Liu

**Updated**: 2024-12-10T03:43:20Z

**Summary**: Large Language Models (LLMs) have rapidly evolved from text-based systems to multimodal platforms, significantly impacting various sectors including healthcare. This comprehensive review explores the progression of LLMs to Multimodal Large Language Models (MLLMs) and their growing influence in medical practice. We examine the current landscape of MLLMs in healthcare, analyzing their applications across clinical decision support, medical imaging, patient engagement, and research. The review highlights the unique capabilities of MLLMs in integrating diverse data types, such as text, images, and audio, to provide more comprehensive insights into patient health. We also address the challenges facing MLLM implementation, including data limitations, technical hurdles, and ethical considerations. By identifying key research gaps, this paper aims to guide future investigations in areas such as dataset development, modality alignment methods, and the establishment of ethical guidelines. As MLLMs continue to shape the future of healthcare, understanding their potential and limitations is crucial for their responsible and effective integration into medical practice.

**Link**: [arxiv](http://arxiv.org/abs/2410.01812v5),  [pdf](http://arxiv.org/pdf/2410.01812v5)

**Tags**: cs.CY cs.AI cs.CL 



### A Dataset and Benchmark for Hospital Course Summarization with Adapted   Large Language Models
**Authors**: Asad Aali, Dave Van Veen, Yamin Ishraq Arefeen, Jason Hom, Christian Bluethgen, Eduardo Pontes Reis, Sergios Gatidis, Namuun Clifford, Joseph Daws, Arash S. Tehrani, Jangwon Kim, Akshay S. Chaudhari

**Updated**: 2024-12-10T03:29:48Z

**Summary**: Brief hospital course (BHC) summaries are clinical documents that summarize a patient's hospital stay. While large language models (LLMs) depict remarkable capabilities in automating real-world tasks, their capabilities for healthcare applications such as synthesizing BHCs from clinical notes have not been shown. We introduce a novel pre-processed dataset, the MIMIC-IV-BHC, encapsulating clinical note and brief hospital course (BHC) pairs to adapt LLMs for BHC synthesis. Furthermore, we introduce a benchmark of the summarization performance of two general-purpose LLMs and three healthcare-adapted LLMs. Using clinical notes as input, we apply prompting-based (using in-context learning) and fine-tuning-based adaptation strategies to three open-source LLMs (Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5, GPT-4). We evaluate these LLMs across multiple context-length inputs using natural language similarity metrics. We further conduct a clinical study with five clinicians, comparing clinician-written and LLM-generated BHCs across 30 samples, focusing on their potential to enhance clinical decision-making through improved summary quality. We observe that the Llama2-13B fine-tuned LLM outperforms other domain-adapted models given quantitative evaluation metrics of BLEU and BERT-Score. GPT-4 with in-context learning shows more robustness to increasing context lengths of clinical note inputs than fine-tuned Llama2-13B. Despite comparable quantitative metrics, the reader study depicts a significant preference for summaries generated by GPT-4 with in-context learning compared to both Llama2-13B fine-tuned summaries and the original summaries, highlighting the need for qualitative clinical evaluation.

**Link**: [arxiv](http://arxiv.org/abs/2403.05720v4),  [pdf](http://arxiv.org/pdf/2403.05720v4)

**Tags**: cs.CL cs.AI cs.LG 



### DeepMDV: Learning Global Matching for Multi-depot Vehicle Routing   Problems
**Authors**: Saeed Nasehi, Farhana Choudhury, Egemen Tanin

**Updated**: 2024-12-10T03:19:23Z

**Summary**: Due to the substantial rise in online retail and e-commerce in recent years, the demand for efficient and fast solutions to Vehicle Routing Problems (VRP) has become critical. To manage the increasing demand, companies have adopted the strategy of adding more depots. However, the presence of multiple depots introduces additional complexities, making existing VRP solutions suboptimal for addressing the Multi-depot Vehicle Routing Problem (MDVRP). Traditional methods for solving the MDVRP often require significant computation time, making them unsuitable for large-scale instances. Additionally, existing learning-based solutions for the MDVRP struggle with generalizability and fail to deliver high-quality results for scenarios involving a large number of customers. In this paper, we propose a novel solution for MDVRP. Our approach employs an attention mechanism, featuring a decoder with two key layers: one layer to consider the states of all vehicles and learn to select the most suitable vehicle based on the proximity of unassigned customers, and another layer to focus on assigning a customer to the selected vehicle. This approach delivers high-quality solutions for large-scale MDVRP instances and demonstrates remarkable generalizability across varying numbers of customers and depots. Its adaptability and performance make it a practical and deployable solution for real-world logistics challenges.

**Link**: [arxiv](http://arxiv.org/abs/2411.17080v2),  [pdf](http://arxiv.org/pdf/2411.17080v2)

**Tags**: cs.DB cs.AI cs.LG 



### WaterSeeker: Pioneering Efficient Detection of Watermarked Segments in   Large Documents
**Authors**: Leyi Pan, Aiwei Liu, Yijian Lu, Zitian Gao, Yichen Di, Shiyu Huang, Lijie Wen, Irwin King, Philip S. Yu

**Updated**: 2024-12-10T03:18:41Z

**Summary**: Watermarking algorithms for large language models (LLMs) have attained high accuracy in detecting LLM-generated text. However, existing methods primarily focus on distinguishing fully watermarked text from non-watermarked text, overlooking real-world scenarios where LLMs generate only small sections within large documents. In this scenario, balancing time complexity and detection performance poses significant challenges. This paper presents WaterSeeker, a novel approach to efficiently detect and locate watermarked segments amid extensive natural text. It first applies an efficient anomaly extraction method to preliminarily locate suspicious watermarked regions. Following this, it conducts a local traversal and performs full-text detection for more precise verification. Theoretical analysis and experimental results demonstrate that WaterSeeker achieves a superior balance between detection accuracy and computational efficiency. Moreover, its localization capability lays the foundation for building interpretable AI detection systems. Our code is available at https://github.com/THU-BPM/WaterSeeker.

**Link**: [arxiv](http://arxiv.org/abs/2409.05112v4),  [pdf](http://arxiv.org/pdf/2409.05112v4)

**Tags**: cs.CL 68T50 I.2.7 



### UniAudio: An Audio Foundation Model Toward Universal Audio Generation
**Authors**: Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang, Songxiang Liu, Xuankai Chang, Jiatong Shi, Sheng Zhao, Jiang Bian, Zhou Zhao, Xixin Wu, Helen Meng

**Updated**: 2024-12-10T03:17:56Z

**Summary**: Large Language models (LLM) have demonstrated the capability to handle a variety of generative tasks. This paper presents the UniAudio system, which, unlike prior task-specific approaches, leverages LLM techniques to generate multiple types of audio (including speech, sounds, music, and singing) with given input conditions. UniAudio 1) first tokenizes all types of target audio along with other condition modalities, 2) concatenates source-target pair as a single sequence, and 3) performs next-token prediction using LLM. Also, a multi-scale Transformer model is proposed to handle the overly long sequences caused by the residual vector quantization based neural codec in tokenization. Training of UniAudio is scaled up to 165K hours of audio and 1B parameters, based on all generative tasks, aiming to obtain sufficient prior knowledge not only in the intrinsic properties of audio but also the inter-relationship between audio and other modalities. Therefore, the trained UniAudio model has the potential to become a foundation model for universal audio generation: it shows strong capability in all trained tasks and can seamlessly support new audio generation tasks after simple fine-tuning. Experiments demonstrate that UniAudio achieves state-of-the-art or at least competitive results on most of the 11 tasks. Demo and code are released at https://github.com/yangdongchao/UniAudio

**Link**: [arxiv](http://arxiv.org/abs/2310.00704v6),  [pdf](http://arxiv.org/pdf/2310.00704v6)

**Tags**: cs.SD eess.AS 



### REBEL: Reinforcement Learning via Regressing Relative Rewards
**Authors**: Zhaolin Gao, Jonathan D. Chang, Wenhao Zhan, Owen Oertell, Gokul Swamy, Kianté Brantley, Thorsten Joachims, J. Andrew Bagnell, Jason D. Lee, Wen Sun

**Updated**: 2024-12-10T03:17:30Z

**Summary**: While originally developed for continuous control problems, Proximal Policy Optimization (PPO) has emerged as the work-horse of a variety of reinforcement learning (RL) applications, including the fine-tuning of generative models. Unfortunately, PPO requires multiple heuristics to enable stable convergence (e.g. value networks, clipping), and is notorious for its sensitivity to the precise implementation of these components. In response, we take a step back and ask what a minimalist RL algorithm for the era of generative models would look like. We propose REBEL, an algorithm that cleanly reduces the problem of policy optimization to regressing the relative reward between two completions to a prompt in terms of the policy, enabling strikingly lightweight implementation. In theory, we prove that fundamental RL algorithms like Natural Policy Gradient can be seen as variants of REBEL, which allows us to match the strongest known theoretical guarantees in terms of convergence and sample complexity in the RL literature. REBEL can also cleanly incorporate offline data and be extended to handle the intransitive preferences we frequently see in practice. Empirically, we find that REBEL provides a unified approach to language modeling and image generation with stronger or similar performance as PPO and DPO, all while being simpler to implement and more computationally efficient than PPO. When fine-tuning Llama-3-8B-Instruct, REBEL achieves strong performance in AlpacaEval 2.0, MT-Bench, and Open LLM Leaderboard.

**Link**: [arxiv](http://arxiv.org/abs/2404.16767v4),  [pdf](http://arxiv.org/pdf/2404.16767v4)

**Tags**: cs.LG cs.CL cs.CV 



### CFSP: An Efficient Structured Pruning Framework for LLMs with   Coarse-to-Fine Activation Information
**Authors**: Yuxin Wang, Minghua Ma, Zekun Wang, Jingchang Chen, Huiming Fan, Liping Shan, Qing Yang, Dongliang Xu, Ming Liu, Bing Qin

**Updated**: 2024-12-10T02:55:21Z

**Summary**: The colossal parameters and computational overhead of Large Language Models (LLMs) challenge their real-world applications. Network pruning, which targets unstructured or structured sparsity by removing redundant parameters, has recently been explored for LLM acceleration. Existing LLM pruning works focus on unstructured pruning, which typically requires special hardware support for a practical speed-up. In contrast, structured pruning can reduce latency on general devices. However, it remains a challenge to perform structured pruning efficiently and maintain performance, especially at high sparsity ratios. To this end, we introduce an efficient structured pruning framework named CFSP, which leverages both Coarse (interblock) and Fine-grained (intrablock) activation information as an importance criterion to guide pruning. The pruning is highly efficient, as it only requires one forward pass to compute feature activations. Specifically, we first allocate the sparsity budget across blocks based on their importance and then retain important weights within each block. In addition, we introduce a recovery fine-tuning strategy that adaptively allocates training overhead based on coarse-grained importance to further improve performance. Experimental results demonstrate that CFSP outperforms existing methods on diverse models across various sparsity budgets. Our code will be available at https://github.com/wyxscir/CFSP.

**Link**: [arxiv](http://arxiv.org/abs/2409.13199v2),  [pdf](http://arxiv.org/pdf/2409.13199v2)

**Tags**: cs.CL 



### Exploring Coding Spot: Understanding Parametric Contributions to LLM   Coding Performance
**Authors**: Dongjun Kim, Minhyuk Kim, YongChan Chun, Chanjun Park, Heuiseok Lim

**Updated**: 2024-12-10T02:03:24Z

**Summary**: Large Language Models (LLMs) have demonstrated notable proficiency in both code generation and comprehension across multiple programming languages. However, the mechanisms underlying this proficiency remain underexplored, particularly with respect to whether distinct programming languages are processed independently or within a shared parametric region. Drawing an analogy to the specialized regions of the brain responsible for distinct cognitive functions, we introduce the concept of Coding Spot, a specialized parametric region within LLMs that facilitates coding capabilities. Our findings identify this Coding Spot and show that targeted modifications to this subset significantly affect performance on coding tasks, while largely preserving non-coding functionalities. This compartmentalization mirrors the functional specialization observed in cognitive neuroscience, where specific brain regions are dedicated to distinct tasks, suggesting that LLMs may similarly employ specialized parameter regions for different knowledge domains.

**Link**: [arxiv](http://arxiv.org/abs/2412.07113v1),  [pdf](http://arxiv.org/pdf/2412.07113v1)

**Tags**: cs.CL 



