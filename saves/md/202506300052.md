# Arxiv Results
## Keyword: kv cache 
 ### From Memories to Maps: Mechanisms of In-Context Reinforcement Learning   in Transformers
**Authors**: Ching Fang, Kanaka Rajan

**Updated**: 2025-06-26T17:18:54Z

**Summary**: Humans and animals show remarkable learning efficiency, adapting to new environments with minimal experience. This capability is not well captured by standard reinforcement learning algorithms that rely on incremental value updates. Rapid adaptation likely depends on episodic memory -- the ability to retrieve specific past experiences to guide decisions in novel contexts. Transformers provide a useful setting for studying these questions because of their ability to learn rapidly in-context and because their key-value architecture resembles episodic memory systems in the brain. We train a transformer to in-context reinforcement learn in a distribution of planning tasks inspired by rodent behavior. We then characterize the learning algorithms that emerge in the model. We first find that representation learning is supported by in-context structure learning and cross-context alignment, where representations are aligned across environments with different sensory stimuli. We next demonstrate that the reinforcement learning strategies developed by the model are not interpretable as standard model-free or model-based planning. Instead, we show that in-context reinforcement learning is supported by caching intermediate computations within the model's memory tokens, which are then accessed at decision time. Overall, we find that memory may serve as a computational resource, storing both raw experience and cached computations to support flexible behavior. Furthermore, the representations developed in the model resemble computations associated with the hippocampal-entorhinal system in the brain, suggesting that our findings may be relevant for natural cognition. Taken together, our work offers a mechanistic hypothesis for the rapid adaptation that underlies in-context learning in artificial and natural settings.

**Link**: [arxiv](http://arxiv.org/abs/2506.19686v2),  [pdf](http://arxiv.org/pdf/2506.19686v2)

**Tags**: cs.AI 



### Measurements, simulations, and models of the point-spread function of   electron-beam lithography
**Authors**: Nikolaj B. Hougs, Kristian S. Knudsen, Marcus Albrechtsen, Taichi Suhara, Christian A. Rosiek, Søren Stobbe

**Updated**: 2025-06-26T13:22:30Z

**Summary**: When a sample is exposed using electron-beam lithography, the electrons scatter deep and far in the substrate, resulting in unwanted deposition of dose at both the nano- and the microscale. This proximity effect can be mitigated by proximity effect correction provided that accurate and validated models of the point-spread function of the electron scattering are available. Most works so far considered a double-Gaussian model of the electron point-spread function, which is very inaccurate for modern electron-beam writers with high acceleration voltages. We present measurements of the process point-spread function for chemically semi-amplified resist on silicon and indium phosphide substrates using a 150 kV electron-beam lithography system. We find that the double-Gaussian model deviates from experiments by up to four orders of magnitude. We propose instead a model comprising the sum of a power-law and a Gaussian, which is in excellent agreement with simulations of the electron scattering obtained by a Monte Carlo method. We apply the power-law plus Gaussian model to quantify the electron scattering and proximity effect correction parameters across material stacks, processing, and voltages from 5 kV to 150 kV. We find that the power-law term remains remarkably constant, whereas the long-range dose contributions and the clearing dose are significantly affected by the substrate and the acceleration voltage.

**Link**: [arxiv](http://arxiv.org/abs/2506.21236v1),  [pdf](http://arxiv.org/pdf/2506.21236v1)

**Tags**: physics.app-ph 



### Task-Aware KV Compression For Cost-Effective Long Video Understanding
**Authors**: Minghao Qin, Yan Shu, Peitian Zhang, Kun Lun, Huaying Yuan, Juenjie Zhou, Shitao Xiao, Bo Zhao, Zheng Liu

**Updated**: 2025-06-26T12:43:43Z

**Summary**: Long-video understanding (LVU) remains a severe challenge for existing multimodal large language models (MLLMs), primarily due to the prohibitive computational cost. Recent approaches have explored KV compression to mitigate this issue, but they often suffer from significant information loss at high compression ratios. In this paper, we introduce Video-X^2L, which flexibly preserves critical video information for each LVU task. Video-X^2L involves two key operations. The first one is called bi-level KV compression. During the MLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs: low-compression KVs (L-KVs) to capture fine-grained video details and high-compression KVs (H-KVs) to offer compact video representations. The second one is called selective KV re-loading. During the MLLM's decoding stage, Video-X^2L selectively re-loads L-KVs for the most critical video chunks while using H-KVs for other less important ones. This allows the MLLM to fully utilize task-specific information while maintaining the overall compactness. Video-X^2L is simple yet effective: it is free from additional training and directly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L with a variety of popular LVU benchmarks, including VideoMME, MLVU, LongVideoBench, and VNBench. Our experiment result shows that Video-X^2L outperforms existing KV-compression methods by a huge advantage while substantially saving the computation cost.

**Link**: [arxiv](http://arxiv.org/abs/2506.21184v1),  [pdf](http://arxiv.org/pdf/2506.21184v1)

**Tags**: cs.CV cs.AI 



### LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas   and Ad-Hoc Networks
**Authors**: Atonu Ghosh, Sudip Misra

**Updated**: 2025-06-26T05:12:22Z

**Summary**: Minimal infrastructure requirements make LoRa suitable for service delivery in remote areas. Additionally, web applications have become a de-facto standard for modern service delivery. However, Long Range (LoRa) fails to enable HTTP access due to its limited bandwidth, payload size limitations, and high collisions in multi-user setups. We propose LoRaConnect to enable HTTP access over LoRa. The LoRaWeb hardware tethers a WiFi hotspot to which client devices connect and access HTTP resources over LoRa backhaul. It implements caching and synchronization mechanisms to address LoRa's aforementioned limitations. It also implements a message-slicing method in the application layer to overcome LoRa's payload limitations. We evaluate the proposed system using actual hardware in three experimental setups to assess the baseline performance, ideal scenario, and practical application scenario with Frequency Hopping Spread Spectrum (FHSS). Additionally, it implements a ping operation to demonstrate Internet capability and extensible nature. LoRaWeb achieves an average throughput of 1.18 KB/S approximately, with an access delay of only 1.3 S approximately for a 1.5KB webpage in the baseline setup. Moreover, it achieves an access delay of approximately 6.7 S for a 10KB webpage in the ideal case and an average end-to-end delay of only 612 ms approximately in the FHSS-based setup. Comparison with benchmark suggests multi-fold improvement.

**Link**: [arxiv](http://arxiv.org/abs/2501.02469v3),  [pdf](http://arxiv.org/pdf/2501.02469v3)

**Tags**: cs.NI cs.CY cs.SY eess.SY 



### The electronic structures, magnetic transition and Fermi surface   instability of room-temperature altermagnet KV$_{2}$Se$_{2}$O
**Authors**: Yuanji Xu, Huiyuan Zhang, Maoyuan Feng, Fuyang Tian

**Updated**: 2025-06-26T03:13:33Z

**Summary**: Altermagnetism has recently emerged as a distinct and fundamental class of magnetic order. Exploring its interplay with quantum phenomena such as unconventional superconductivity, density-wave instabilities, and many-body effects represents a compelling frontier. In this work, we theoretically confirm the presence of high-temperature metallic altermagnetism in KV$_2$Se$_2$O. We demonstrate that the anomalous metal-insulator-metal transition arises from a Lifshitz transition associated with Fermi surface reconstruction. The previously reported spin-density wave gap is found to lie below the Fermi level in our study and is now recognized to be attributed to the V-shaped density of states, originating from orbital-selective and sublattice-resolved half-metal-like behavior on a specific V atom. Furthermore, we identify the instability from the nesting of spin-momentum-locked two-dimensional Fermi surfaces, which induces the SDW state. These findings position KV$_2$Se$_2$O as a promising platform for investigating the interplay among altermagnetism, unconventional superconductivity, and density-wave order.

**Link**: [arxiv](http://arxiv.org/abs/2506.20968v1),  [pdf](http://arxiv.org/pdf/2506.20968v1)

**Tags**: cond-mat.str-el 



### AirCache: Activating Inter-modal Relevancy KV Cache Compression for   Efficient Large Vision-Language Model Inference
**Authors**: Kai Huang, Hao Zou, Bochen Wang, Ye Xi, Zhen Xie, Hao Wang

**Updated**: 2025-06-26T01:30:43Z

**Summary**: Recent advancements in Large Visual Language Models (LVLMs) have gained significant attention due to their remarkable reasoning capabilities and proficiency in generalization. However, processing a large number of visual tokens and generating long-context outputs impose substantial computational overhead, leading to excessive demands for key-value (KV) cache. To address this critical bottleneck, we propose AirCache, a novel KV cache compression method aimed at accelerating LVLMs inference. This work systematically investigates the correlations between visual and textual tokens within the attention mechanisms of LVLMs. Our empirical analysis reveals considerable redundancy in cached visual tokens, wherein strategically eliminating these tokens preserves model performance while significantly accelerating context generation. Inspired by these findings, we introduce an elite observation window for assessing the importance of visual components in the KV cache, focusing on stable inter-modal relevancy modeling with enhanced multi-perspective consistency. Additionally, we develop an adaptive layer-wise budget allocation strategy that capitalizes on the strength and skewness of token importance distribution, showcasing superior efficiency compared to uniform allocation. Comprehensive evaluations across multiple LVLMs and benchmarks demonstrate that our method achieves comparable performance to the full cache while retaining only 10% of visual KV cache, thereby reducing decoding latency by 29% to 66% across various batch size and prompt length of inputs. Notably, as cache retention rates decrease, our method exhibits increasing performance advantages over existing approaches.

**Link**: [arxiv](http://arxiv.org/abs/2503.23956v2),  [pdf](http://arxiv.org/pdf/2503.23956v2)

**Tags**: cs.CV cs.AI 



### Omniwise: Predicting GPU Kernels Performance with LLMs
**Authors**: Zixian Wang, Cole Ramos, Muhammad A. Awad, Keith Lowery

**Updated**: 2025-06-25T23:36:44Z

**Summary**: In recent years, the rapid advancement of deep neural networks (DNNs) has revolutionized artificial intelligence, enabling models with unprecedented capabilities in understanding, generating, and processing complex data. These powerful architectures have transformed a wide range of downstream applications, tackling tasks beyond human reach. In this paper, we introduce Omniwise, the first end-to-end, self-supervised fine-tuning pipeline that applies large language models (LLMs) to GPU kernel performance prediction--a novel use case in performance profiling. Omniwise is model-agnostic and lightweight, achieving strong results even with a small 3B-parameter model. It can predict key performance metrics, including memory bandwidth, cache hit rates, GFLOPs, and arithmetic intensity, directly from kernel code without the need for code execution or profiling tools. Our approach achieves over 90% of predictions within 10% relative error on GPU kernels executed on AMD MI250 and MI300X architectures. In addition to the pipeline, we develop an online inference server and a Visual Studio Code plugin that seamlessly integrate LLM-based performance prediction into developers' workflows.

**Link**: [arxiv](http://arxiv.org/abs/2506.20886v1),  [pdf](http://arxiv.org/pdf/2506.20886v1)

**Tags**: cs.LG cs.AI 



### A3 : an Analytical Low-Rank Approximation Framework for Attention
**Authors**: Jeffrey T. H. Wong, Cheng Zhang, Xinye Cao, Pedro Gimenes, George A. Constantinides, Wayne Luk, Yiren Zhao

**Updated**: 2025-06-25T23:03:54Z

**Summary**: Large language models have demonstrated remarkable performance; however, their massive parameter counts make deployment highly expensive. Low-rank approximation offers a promising compression solution, yet existing approaches have two main limitations: (1) They focus on minimizing the output error of individual linear layers, without considering the architectural characteristics of Transformers, and (2) they decompose a large weight matrix into two small low-rank matrices. Consequently, these methods often fall short compared to other compression techniques like pruning and quantization, and introduce runtime overhead such as the extra GEMM kernel launches for decomposed small matrices. To address these limitations, we propose $\tt A^\tt 3$, a post-training low-rank approximation framework. $\tt A^\tt 3$ splits a Transformer layer into three functional components, namely $\tt QK$, $\tt OV$, and $\tt MLP$. For each component, $\tt A^\tt 3$ provides an analytical solution that reduces the hidden dimension size inside each component while minimizing the component's functional loss ($\it i.e.$, error in attention scores, attention outputs, and MLP outputs). This approach directly reduces model sizes, KV cache sizes, and FLOPs without introducing any runtime overheads. In addition, it provides a new narrative in advancing the optimization problem from singular linear layer loss optimization toward improved end-to-end performance. Through extensive experiments, we show that $\tt A^\tt 3$ maintains superior performance compared to SoTAs. For example, under the same reduction budget in computation and memory, our low-rank approximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2, outperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the versatility of $\tt A^\tt 3$, including KV cache compression, quantization, and mixed-rank assignments for enhanced performance.

**Link**: [arxiv](http://arxiv.org/abs/2505.12942v3),  [pdf](http://arxiv.org/pdf/2505.12942v3)

**Tags**: cs.CL cs.AI cs.LG 



### Generative Blocks World: Moving Things Around in Pictures
**Authors**: Vaibhav Vavilala, Seemandhar Jain, Rahul Vasanth, D. A. Forsyth, Anand Bhattad

**Updated**: 2025-06-25T17:59:55Z

**Summary**: We describe Generative Blocks World to interact with the scene of a generated image by manipulating simple geometric abstractions. Our method represents scenes as assemblies of convex 3D primitives, and the same scene can be represented by different numbers of primitives, allowing an editor to move either whole structures or small details. Once the scene geometry has been edited, the image is generated by a flow-based method which is conditioned on depth and a texture hint. Our texture hint takes into account the modified 3D primitives, exceeding texture-consistency provided by existing key-value caching techniques. These texture hints (a) allow accurate object and camera moves and (b) largely preserve the identity of objects depicted. Quantitative and qualitative experiments demonstrate that our approach outperforms prior works in visual fidelity, editability, and compositional generalization.

**Link**: [arxiv](http://arxiv.org/abs/2506.20703v1),  [pdf](http://arxiv.org/pdf/2506.20703v1)

**Tags**: cs.GR cs.CV 



### Semantic Caching for Improving Web Affordability
**Authors**: Hafsa Akbar, Danish Athar, Muhammad Ayain Fida Rana, Chaudhary Hammad Javed, Zartash Afzal Uzmi, Ihsan Ayyub Qazi, Zafar Ayyub Qazi

**Updated**: 2025-06-25T13:35:25Z

**Summary**: The rapid growth of web content has led to increasingly large webpages, posing significant challenges for Internet affordability, especially in developing countries where data costs remain prohibitively high. We propose semantic caching using Large Language Models (LLMs) to improve web affordability by enabling reuse of semantically similar images within webpages. Analyzing 50 leading news and media websites, encompassing 4,264 images and over 40,000 image pairs, we demonstrate potential for significant data transfer reduction, with some website categories showing up to 37% of images as replaceable. Our proof-of-concept architecture shows users can achieve approximately 10% greater byte savings compared to exact caching. We evaluate both commercial and open-source multi-modal LLMs for assessing semantic replaceability. GPT-4o performs best with a low Normalized Root Mean Square Error of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA 3.1 model shows comparable performance, highlighting its viability for large-scale applications. This approach offers benefits for both users and website operators, substantially reducing data transmission. We discuss ethical concerns and practical challenges, including semantic preservation, user-driven cache configuration, privacy concerns, and potential resistance from website operators

**Link**: [arxiv](http://arxiv.org/abs/2506.20420v1),  [pdf](http://arxiv.org/pdf/2506.20420v1)

**Tags**: cs.NI F.2.2, I.2.7 



### Do cell culturing influence the radiosensitizing effect of gold   nanoparticles part 2: scrutinizing the methodology producing recent evidence
**Authors**: Hans Rabus, Oswald Msosa Mkanda

**Updated**: 2025-06-25T09:44:25Z

**Summary**: When irradiation is performed with gold nanoparticles (AuNPs), a different shape of cells in suspension or adherent to walls may result in different probability of cell survival. In a recent study, differences of up to a factor of 2 were found between the predicted survival of floating and adherent cells. The present work aims to quantify the biases introduced by the simulation setup and the use of voxelized geometry in conjunction with the local effect model for cell survival. The results show that simulated irradiation of a cell near the surface with an incident beam matched to the cell dimensions results in dose values that are by a factor of about 50 lower than the dose to cells deeper in the medium when irradiated with a Co-60 spectrum and lateral beam dimensions in the centimeter range. Furthermore, the number of ionizing photon interactions in gold nanoparticles in a cell near the surface is lower by a factor of about 2 than for cells at 5 mm and 1 cm depth. Using the average dose in voxels of size in the order of 200 nm for assessing cell survival with the local effect model (LEM) leads to an underestimation of the number of lesions from a single ionized AuNP by roughly two orders of magnitude and thus to an overestimation of cell survival. The effect of cell geometry on the survival rate was examined for approximate cell geometries and 100 kV x-ray irradiation, for which the probability of photon interaction in gold nanoparticles is by more than two orders of magnitude higher than for Co-60 irradiation. The results show that the effects are negligible for 5 nm nanoparticles at the concentration of AuNPs considered in preceding work. For 50 nm nanoparticles and thus a thousand times higher mass fraction of gold, significant reduction in cell survival is found, with a clear additional reduction predicted by the LEM as compared to the prediction based on mean dose to the nucleus.

**Link**: [arxiv](http://arxiv.org/abs/2506.20283v1),  [pdf](http://arxiv.org/pdf/2506.20283v1)

**Tags**: physics.med-ph 



### Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV   Management on a Single Commodity GPU
**Authors**: He Sun, Li Li, Mingjun Xiao, Chengzhong Xu

**Updated**: 2025-06-25T07:26:42Z

**Summary**: Advanced Large Language Models (LLMs) have achieved impressive performance across a wide range of complex and long-context natural language tasks. However, performing long-context LLM inference locally on a commodity GPU (a PC) with privacy concerns remains challenging due to the increasing memory demands of the key-value (KV) cache. Existing systems typically identify important tokens and selectively offload their KV data to GPU and CPU memory. The KV data needs to be offloaded to disk due to the limited memory on a commodity GPU, but the process is bottlenecked by token importance evaluation overhead and the disk's low bandwidth. In this paper, we present LeoAM, the first efficient importance-aware long-context LLM inference system for a single commodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system employs an adaptive KV management strategy that partitions KV data into variable-sized chunks based on the skewed distribution of attention weights across different layers to reduce computational and additional transmission overheads. Moreover, we propose a lightweight KV abstract method, which minimizes transmission latency by storing and extracting the KV abstract of each chunk on disk instead of the full KV data. LeoAM also leverages the dynamic compression and pipeline techniques to further accelerate inference. Experimental results demonstrate that LongInfer achieves an average inference latency speedup of 3.46x, while maintaining comparable LLM response quality. In scenarios with larger batch sizes, it achieves up to a 5.47x speedup.

**Link**: [arxiv](http://arxiv.org/abs/2506.20187v1),  [pdf](http://arxiv.org/pdf/2506.20187v1)

**Tags**: cs.OS cs.CR 68M20 C.4 



### MegaFold: System-Level Optimizations for Accelerating Protein Structure   Prediction Models
**Authors**: Hoa La, Ahan Gupta, Alex Morehead, Jianlin Cheng, Minjia Zhang

**Updated**: 2025-06-24T23:30:49Z

**Summary**: Protein structure prediction models such as AlphaFold3 (AF3) push the frontier of biomolecular modeling by incorporating science-informed architectural changes to the transformer architecture. However, these advances come at a steep system cost, introducing: compute- and memory-intensive operators, 2D attention mechanisms, and retrieval-augmented data pipelines, which collectively hinder the scalability of AF3 training. In this work, we present MegaFold, a cross-platform system to accelerate AF3 training. MegaFold tackles key bottlenecks through ahead-of-time caching to eliminate GPU idle time from the retrieval-augmented data pipeline, Triton-based kernels for memory-efficient EvoAttention on heterogeneous devices, and deep fusion for common and critical small operators in AF3. Evaluation on both NVIDIA H200 and AMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by up to 1.23$\times$ and improves per-iteration training time by up-to 1.73$\times$ and 1.62$\times$ respectively. More importantly, MegaFold enables training on 1.35$\times$ longer sequence lengths compared to PyTorch baselines without running out-of-memory, significantly improving the scalability of modern protein folding models. We open source our code at https://github.com/Supercomputing-System-AI-Lab/MegaFold/.

**Link**: [arxiv](http://arxiv.org/abs/2506.20686v1),  [pdf](http://arxiv.org/pdf/2506.20686v1)

**Tags**: q-bio.BM cs.DC cs.LG cs.PF 



### GainSight: Application-Guided Profiling for Composing Heterogeneous   On-Chip Memories in AI Hardware Accelerators
**Authors**: Peijing Li, Matthew Hung, Yiming Tan, Konstantin Hoßfeld, Jake Cheng Jiajun, Shuhan Liu, Lixian Yan, Xinxin Wang, H. -S. Philip Wong, Thierry Tambe

**Updated**: 2025-06-24T19:02:08Z

**Summary**: As AI workloads drive soaring memory requirements, higher-density on-chip memory is needed for domain-specific accelerators beyond what current SRAM technology can provide. We motivate that algorithms and application behavior should guide the composition of heterogeneous on-chip memories. However, little work has incorporated dynamic application profiles into these design decisions, and no existing tools are expressly designed for this purpose. We present GainSight, a profiling framework that analyzes fine-grained memory access patterns and data lifetimes in domain-specific accelerators. By instrumenting retargetable architectural simulator backends with application- and device-agnostic analytical frontends, GainSight aligns workload-specific traffic and lifetime metrics with mockups of emerging memory devices, informing system-level heterogeneous memory design. We also present a set of case studies on MLPerf Inference and PolyBench workloads using simulated GPU and systolic array architectures, highlighting the utility of GainSight and the insights it provides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic array scratchpad accesses across profiled workloads are short-lived and suitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory arrays that augment SRAM with GCRAM can reduce active energy consumption by up to 66.8%. To facilitate further research in this domain, GainSight is open source at https://gainsight.stanford.edu/.

**Link**: [arxiv](http://arxiv.org/abs/2504.14866v4),  [pdf](http://arxiv.org/pdf/2504.14866v4)

**Tags**: cs.AR cs.ET B.7.1; B.3.1; C.3; I.6; I.2.6 



### CronusVLA: Transferring Latent Motion Across Time for Multi-Frame   Prediction in Manipulation
**Authors**: Hao Li, Shuai Yang, Yilun Chen, Yang Tian, Xiaoda Yang, Xinyi Chen, Hanqing Wang, Tai Wang, Feng Zhao, Dahua Lin, Jiangmiao Pang

**Updated**: 2025-06-24T17:30:27Z

**Summary**: Recent vision-language-action (VLA) models built on pretrained vision-language models (VLMs) have demonstrated strong generalization across manipulation tasks. However, they remain constrained by a single-frame observation paradigm and cannot fully benefit from the motion information offered by aggregated multi-frame historical observations, as the large vision-language backbone introduces substantial computational cost and inference latency. We propose CronusVLA, a unified framework that extends single-frame VLA models to the multi-frame paradigm through an efficient post-training stage. CronusVLA comprises three key components: (1) single-frame pretraining on large-scale embodied datasets with autoregressive action tokens prediction, which establishes an embodied vision-language foundation; (2) multi-frame encoding, adapting the prediction of vision-language backbones from discrete action tokens to motion features during post-training, and aggregating motion features from historical frames into a feature chunking; (3) cross-frame decoding, which maps the feature chunking to accurate actions via a shared decoder with cross-attention. By reducing redundant token computation and caching past motion features, CronusVLA achieves efficient inference. As an application of motion features, we further propose an action adaptation mechanism based on feature-action retrieval to improve model performance during finetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with 70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world Franka experiments also show the strong performance and robustness.

**Link**: [arxiv](http://arxiv.org/abs/2506.19816v1),  [pdf](http://arxiv.org/pdf/2506.19816v1)

**Tags**: cs.RO cs.CV 



### RCStat: A Statistical Framework for using Relative Contextualization in   Transformers
**Authors**: Debabrata Mahapatra, Shubham Agarwal, Apoorv Saxena, Subrata Mitra

**Updated**: 2025-06-24T11:55:43Z

**Summary**: Prior work on input-token importance in auto-regressive transformers has relied on Softmax-normalized attention weights, which obscure the richer structure of pre-Softmax query-key logits. We introduce RCStat, a statistical framework that harnesses raw attention logits via Relative Contextualization (RC), a random variable measuring contextual alignment between token segments, and derive an efficient upper bound for RC. We demonstrate two applications: (i) Key-Value compression, where RC-based thresholds drive adaptive key-value eviction for substantial cache reduction with minimal quality loss; and (ii) Attribution, where RC yields higher-fidelity token-, sentence-, and chunk-level explanations than post-Softmax methods. Across question answering, summarization, and attribution benchmarks, RCStat achieves significant empirical gains, delivering state-of-the-art compression and attribution performance without any model retraining.

**Link**: [arxiv](http://arxiv.org/abs/2506.19549v1),  [pdf](http://arxiv.org/pdf/2506.19549v1)

**Tags**: cs.CL cs.AI cs.LG 



### AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in   Large Language Models
**Authors**: Zeyu Li, Chuanfu Xiao, Yang Wang, Xiang Liu, Zhenheng Tang, Baotong Lu, Mao Yang, Xinyu Chen, Xiaowen Chu

**Updated**: 2025-06-24T10:45:48Z

**Summary**: Quantization has emerged as an effective and lightweight solution to reduce the memory footprint of the KV cache in Large Language Models (LLMs). Nevertheless, minimizing the performance degradation caused by ultra-low-bit KV cache quantization remains a significant challenge. We observe that quantizing the KV cache of different tokens has varying impacts on the quality of attention outputs. To systematically investigate this phenomenon, we perform forward error propagation analysis on attention and propose the Anchor Score (AnS) that quantifies the sensitivity of each token's KV cache to quantization-induced error. Our analysis reveals significant disparities in AnS across tokens, suggesting that preserving a small subset with full precision (FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive quantization scenarios. Based on this insight, we introduce AnTKV, a novel framework that leverages Anchor Token-aware Vector Quantization to compress the KV cache. Furthermore, to support efficient deployment, we design and develop a triton kernel that is fully compatible with FlashAttention, enabling fast online Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context lengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x higher decoding throughput compared to the FP16 baseline. Our experiment results demonstrate that AnTKV matches or outperforms prior works such as KIVI, SKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves significantly lower perplexity under ultra-low-bit quantization on Mistral-7B, with only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of 4.73.

**Link**: [arxiv](http://arxiv.org/abs/2506.19505v1),  [pdf](http://arxiv.org/pdf/2506.19505v1)

**Tags**: cs.CL 



### Mixture of Cache-Conditional Experts for Efficient Mobile Device   Inference
**Authors**: Andrii Skliar, Ties van Rozendaal, Romain Lepert, Todor Boinovski, Mart van Baalen, Markus Nagel, Paul Whatmough, Babak Ehteshami Bejnordi

**Updated**: 2025-06-24T09:27:46Z

**Summary**: Mixture of Experts (MoE) LLMs have recently gained attention for their ability to enhance performance by selectively engaging specialized subnetworks or "experts" for each input. However, deploying MoEs on memory-constrained devices remains challenging, particularly when generating tokens sequentially with a batch size of one, as opposed to typical high-throughput settings involving long sequences or large batches. In this work, we optimize MoE on memory-constrained devices where only a subset of expert weights fit in DRAM. We introduce a novel cache-aware routing strategy that leverages expert reuse during token generation to improve cache locality. We evaluate our approach on language modeling, MMLU, and GSM8K benchmarks and present on-device results demonstrating 2$\times$ speedups on mobile devices, offering a flexible, training-free solution to extend MoE's applicability across real-world applications.

**Link**: [arxiv](http://arxiv.org/abs/2412.00099v2),  [pdf](http://arxiv.org/pdf/2412.00099v2)

**Tags**: cs.LG cs.AI cs.AR 



### Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments   with a Hierarchical Spatial-Cognition Long-Short Memory System
**Authors**: Lixuan He, Haoyu Dong, Zhenxing Chen, Yangcheng Yu, Jie Feng, Yong Li

**Updated**: 2025-06-24T09:00:43Z

**Summary**: Vision-and-Language Navigation (VLN) in large-scale urban environments requires embodied agents to ground linguistic instructions in complex scenes and recall relevant experiences over extended time horizons. Prior modular pipelines offer interpretability but lack unified memory, while end-to-end (M)LLM agents excel at fusing vision and language yet remain constrained by fixed context windows and implicit spatial reasoning. We introduce \textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system that can augment any VLN backbone. Mem4Nav fuses a sparse octree for fine-grained voxel indexing with a semantic topology graph for high-level landmark connectivity, storing both in trainable memory tokens embedded via a reversible Transformer. Long-term memory (LTM) compresses and retains historical observations at both octree and graph nodes, while short-term memory (STM) caches recent multimodal entries in relative coordinates for real-time obstacle avoidance and local planning. At each step, STM retrieval sharply prunes dynamic context, and, when deeper history is needed, LTM tokens are decoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and Map2Seq across three backbones (modular, state-of-the-art VLN with prompt-based LLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13 pp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW improvement. Ablations confirm the indispensability of both the hierarchical map and dual memory modules. Our codes are open-sourced via https://github.com/tsinghua-fib-lab/Mem4Nav.

**Link**: [arxiv](http://arxiv.org/abs/2506.19433v1),  [pdf](http://arxiv.org/pdf/2506.19433v1)

**Tags**: cs.CV cs.AI cs.CL 



### PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning
**Authors**: Duong Bach

**Updated**: 2025-06-24T06:44:47Z

**Summary**: The proliferation of multi-agent systems (MAS) in complex, dynamic environments necessitates robust and efficient mechanisms for managing shared knowledge. A critical challenge is ensuring that distributed memories remain synchronized, relevant, and free from the accumulation of outdated or inconsequential data - a process analogous to biological forgetting. This paper introduces the Co-Forgetting Protocol, a novel, comprehensive framework designed to address this challenge by enabling synchronized memory pruning in MAS. The protocol integrates three key components: (1) context-aware semantic voting, where agents utilize a lightweight DistilBERT model to assess the relevance of memory items based on their content and the current operational context; (2) multi-scale temporal decay functions, which assign diminishing importance to memories based on their age and access frequency across different time horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based consensus mechanism, ensuring that decisions to retain or discard memory items are agreed upon by a qualified and fault-tolerant majority of agents, even in the presence of up to f Byzantine (malicious or faulty) agents in a system of N greater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient inter-agent communication and Pinecone for scalable vector embedding storage and similarity search, with SQLite managing metadata. Experimental evaluations in a simulated MAS environment with four agents demonstrate the protocol's efficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88% voting accuracy in forgetting decisions against human-annotated benchmarks, a 92% PBFT consensus success rate under simulated Byzantine conditions, and an 82% cache hit rate for memory access.

**Link**: [arxiv](http://arxiv.org/abs/2506.17338v2),  [pdf](http://arxiv.org/pdf/2506.17338v2)

**Tags**: cs.DC cs.AI cs.MA 



### Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV   Sparsification
**Authors**: Minghao Qin, Xiangrui Liu, Zhengyang Liang, Yan Shu, Huaying Yuan, Juenjie Zhou, Shitao Xiao, Bo Zhao, Zheng Liu

**Updated**: 2025-06-24T01:19:56Z

**Summary**: Multi-modal large language models (MLLMs) models have made significant progress in video understanding over the past few years. However, processing long video inputs remains a major challenge due to high memory and computational costs. This makes it difficult for current models to achieve both strong performance and high efficiency in long video understanding. To address this challenge, we propose Video-XL-2, a novel MLLM that delivers superior cost-effectiveness for long-video understanding based on task-aware KV sparsification. The proposed framework operates with two key steps: chunk-based pre-filling and bi-level key-value decoding. Chunk-based pre-filling divides the visual token sequence into chunks, applying full attention within each chunk and sparse attention across chunks. This significantly reduces computational and memory overhead. During decoding, bi-level key-value decoding selectively reloads either dense or sparse key-values for each chunk based on its relevance to the task. This approach further improves memory efficiency and enhances the model's ability to capture fine-grained information. Video-XL-2 achieves state-of-the-art performance on various long video understanding benchmarks, outperforming existing open-source lightweight models. It also demonstrates exceptional efficiency, capable of processing over 10,000 frames on a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few seconds.

**Link**: [arxiv](http://arxiv.org/abs/2506.19225v1),  [pdf](http://arxiv.org/pdf/2506.19225v1)

**Tags**: cs.CV cs.AI 



### Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices   and Tensors
**Authors**: Benjamin Brock, Willow Ahrens, Hameer Abbasi, Timothy A. Davis, Juni Kim, James Kitchen, Spencer Patty, Isaac Virshup, Erik Welch

**Updated**: 2025-06-23T22:33:58Z

**Summary**: Sparse matrices and tensors are ubiquitous throughout multiple subfields of computing. The widespread usage of sparse data has inspired many in-memory and on-disk storage formats, but the only widely adopted storage specifications are the Matrix Market and FROSTT file formats, which both use ASCII text. Due to the inefficiency of text storage, these files typically have larger file sizes and longer parsing times than binary storage formats, which directly store an in-memory representation to disk. This can be a major bottleneck; since sparse computation is often bandwidth-bound, the cost of loading or storing a matrix to disk often exceeds the cost of performing a sparse computation. While it is common practice for practitioners to develop their own, custom, non-portable binary formats for high-performance sparse matrix storage, there is currently no cross-platform binary sparse matrix storage format. We present Binsparse, a cross-platform binary sparse matrix and tensor format specification. Binsparse is a modular, embeddable format, consisting of a JSON descriptor, which describes the matrix or tensor dimensions, type, and format, and a series of binary arrays, which can be stored in all modern binary containers, such as HDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse spanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our Binsparse format on every matrix in the SuiteSparse Matrix Collection and a selection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format shows file size reductions of 2.4x on average without compression and 7.5x with compression. We evaluate our parser's read/write performance against a state-of-the-art Matrix Market parser, demonstrating warm cache mean read speedups of 26.5x without compression and 2.6x with compression, and write speedups of 31x without compression and 1.4x with compression.

**Link**: [arxiv](http://arxiv.org/abs/2506.19175v1),  [pdf](http://arxiv.org/pdf/2506.19175v1)

**Tags**: cs.MS cs.DC cs.DS 



### CommVQ: Commutative Vector Quantization for KV Cache Compression
**Authors**: Junyan Li, Yang Zhang, Muhammad Yusuf Hassan, Talha Chafekar, Tianle Cai, Zhile Ren, Pengsheng Guo, Foroozan Karimzadeh, Colorado Reed, Chong Wang, Chuang Gan

**Updated**: 2025-06-23T17:50:11Z

**Summary**: Large Language Models (LLMs) are increasingly used in applications requiring long context lengths, but the key-value (KV) cache often becomes a memory bottleneck on GPUs as context grows. To address this, we propose Commutative Vector Quantization (CommVQ) to significantly reduce memory usage for long-context LLM inference. We first introduce additive quantization with a lightweight encoder and codebook to compress the KV cache, which can be decoded via simple matrix multiplication. To further reduce computational costs during decoding, we design the codebook to be commutative with Rotary Position Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm. This enables efficient integration of decoding into the self-attention mechanism. Our approach achieves high accuracy with additive quantization and low overhead via the RoPE-commutative codebook. Experiments on long-context benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5% with 2-bit quantization, while outperforming state-of-the-art KV cache quantization methods. Notably, it enables 1-bit KV cache quantization with minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context length on a single RTX 4090 GPU. The source code is available at: https://github.com/UMass-Embodied-AGI/CommVQ.

**Link**: [arxiv](http://arxiv.org/abs/2506.18879v1),  [pdf](http://arxiv.org/pdf/2506.18879v1)

**Tags**: cs.CL cs.AI 



### Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo
**Authors**: Minas Karamanis, Uroš Seljak

**Updated**: 2025-06-23T07:59:17Z

**Summary**: Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian inference but suffer from high computational costs due to their reliance on large particle ensembles for accurate estimates. We introduce persistent sampling (PS), an extension of SMC that systematically retains and reuses particles from all prior iterations to construct a growing, weighted ensemble. By leveraging multiple importance sampling and resampling from a mixture of historical distributions, PS mitigates the need for excessively large particle counts, directly addressing key limitations of SMC such as particle impoverishment and mode collapse. Crucially, PS achieves this without additional likelihood evaluations-weights for persistent particles are computed using cached likelihood values. This framework not only yields more accurate posterior approximations but also produces marginal likelihood estimates with significantly lower variance, enhancing reliability in model comparison. Furthermore, the persistent ensemble enables efficient adaptation of transition kernels by leveraging a larger, decorrelated particle pool. Experiments on high-dimensional Gaussian mixtures, hierarchical models, and non-convex targets demonstrate that PS consistently outperforms standard SMC and related variants, including recycled and waste-free SMC, achieving substantial reductions in mean squared error for posterior expectations and evidence estimates, all at reduced computational cost. PS thus establishes itself as a robust, scalable, and efficient alternative for complex Bayesian inference tasks.

**Link**: [arxiv](http://arxiv.org/abs/2407.20722v3),  [pdf](http://arxiv.org/pdf/2407.20722v3)

**Tags**: stat.ML cs.LG stat.CO 



### FutureFill: Fast Generation from Convolutional Sequence Models
**Authors**: Naman Agarwal, Xinyi Chen, Evan Dogariu, Devan Shah, Hubert Strauss, Vlad Feinberg, Daniel Suo, Peter Bartlett, Elad Hazan

**Updated**: 2025-06-23T03:20:46Z

**Summary**: We address the challenge of efficient auto-regressive generation in sequence prediction models by introducing FutureFill, a general-purpose fast generation method for any sequence prediction algorithm based on convolutional operators. FutureFill reduces generation time from quadratic to quasilinear in the context length. Moreover, when generating from a prompt, it requires a prefill cache whose size grows only with the number of tokens to be generated, often much smaller than the caches required by standard convolutional or attention based models. We validate our theoretical claims with experiments on synthetic tasks and demonstrate substantial efficiency gains when generating from a deep convolutional sequence prediction model.

**Link**: [arxiv](http://arxiv.org/abs/2410.03766v3),  [pdf](http://arxiv.org/pdf/2410.03766v3)

**Tags**: cs.LG cs.AI cs.CL 



### RAPID: Long-Context Inference with Retrieval-Augmented Speculative   Decoding
**Authors**: Guanzheng Chen, Qilong Feng, Jinjie Ni, Xin Li, Michael Qizhe Shieh

**Updated**: 2025-06-23T03:05:26Z

**Summary**: The emergence of long-context large language models (LLMs) offers a promising alternative to traditional retrieval-augmented generation (RAG) for processing extensive documents. However, the computational overhead of long-context inference presents significant efficiency challenges. While Speculative Decoding (SD) traditionally accelerates inference using smaller draft models, its effectiveness diminishes substantially in long-context scenarios due to memory-bound KV cache operations. We introduce Retrieval-Augmented Speculative Decoding (RAPID), which leverages RAG for both accelerating and enhancing generation quality in long-context inference. RAPID introduces the RAG drafter-a draft LLM operating on shortened retrieval contexts-to speculate on the generation of long-context target LLMs. Our approach enables a new paradigm where same-scale or even larger LLMs can serve as RAG drafters while maintaining computational efficiency. To fully leverage the potentially superior capabilities from stronger RAG drafters, we develop an inference-time knowledge transfer that enriches the target distribution by RAG. Extensive experiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates the strengths of both RAG and long-context LLMs, achieving significant performance improvements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context inference. Our analyses also reveal the robustness of RAPID across various context lengths and retrieval quality.

**Link**: [arxiv](http://arxiv.org/abs/2502.20330v2),  [pdf](http://arxiv.org/pdf/2502.20330v2)

**Tags**: cs.CL 



### Make It Efficient: Dynamic Sparse Attention for Autoregressive Image   Generation
**Authors**: Xunzhi Xiang, Qi Fan

**Updated**: 2025-06-23T01:27:06Z

**Summary**: Autoregressive conditional image generation models have emerged as a dominant paradigm in text-to-image synthesis. These methods typically convert images into one-dimensional token sequences and leverage the self-attention mechanism, which has achieved remarkable success in natural language processing, to capture long-range dependencies, model global context, and ensure semantic coherence. However, excessively long contexts during inference lead to significant memory overhead caused by KV-cache and computational delays. To alleviate these challenges, we systematically analyze how global semantics, spatial layouts, and fine-grained textures are formed during inference, and propose a novel training-free context optimization method called Adaptive Dynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies historical tokens crucial for maintaining local texture consistency and those essential for ensuring global semantic coherence, thereby efficiently streamlining attention computation. Additionally, we introduce a dynamic KV-cache update mechanism tailored for ADSA, reducing GPU memory consumption during inference by approximately $50\%$. Extensive qualitative and quantitative experiments demonstrate the effectiveness and superiority of our approach in terms of both generation quality and resource efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2506.18226v1),  [pdf](http://arxiv.org/pdf/2506.18226v1)

**Tags**: cs.CV cs.AI 



### Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the   Limits of Embedding Space Capacity
**Authors**: Yuri Kuratov, Mikhail Arkhipov, Aydar Bulatov, Mikhail Burtsev

**Updated**: 2025-06-22T15:07:37Z

**Summary**: A range of recent works addresses the problem of compression of sequence of tokens into a shorter sequence of real-valued vectors to be used as inputs instead of token embeddings or key-value cache. These approaches are focused on reduction of the amount of compute in existing language models rather than minimization of number of bits needed to store text. Despite relying on powerful models as encoders, the maximum attainable lossless compression ratio is typically not higher than x10. This fact is highly intriguing because, in theory, the maximum information capacity of large real-valued vectors is far beyond the presented rates even for 16-bit precision and a modest vector size. In this work, we explore the limits of compression by replacing the encoder with a per-sample optimization procedure. We show that vectors with compression ratios up to x1500 exist, which highlights two orders of magnitude gap between existing and practically attainable solutions. Furthermore, we empirically show that the compression limits are determined not by the length of the input but by the amount of uncertainty to be reduced, namely, the cross-entropy loss on this sequence without any conditioning. The obtained limits highlight the substantial gap between the theoretical capacity of input embeddings and their practical utilization, suggesting significant room for optimization in model design.

**Link**: [arxiv](http://arxiv.org/abs/2502.13063v3),  [pdf](http://arxiv.org/pdf/2502.13063v3)

**Tags**: cs.CL cs.LG 



### Secure User-friendly Blockchain Modular Wallet Design Using Android &   OP-TEE
**Authors**: Seongjin Kim, Sanguk Yun, Jungho Jang

**Updated**: 2025-06-22T10:57:57Z

**Summary**: Emerging crypto economies still hemorrhage digital assets because legacy wallets leak private keys at almost every layer of the software stack, from user-space libraries to kernel memory dumps. This paper solves that twin crisis of security and interoperability by re-imagining key management as a platform-level service anchored in ARM TrustZone through OP-TEE. Our architecture fractures the traditional monolithic Trusted Application into per-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's single-binary ceiling. A cryptographically sealed firmware-over-the-air pipeline welds each TA set to an Android system image, enabling hot-swap updates while Verified Boot enforces rollback protection. Every package carries a chained signature developer first, registry second so even a compromised supply chain cannot smuggle malicious code past the Secure World's RSA-PSS gatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and GP-compliant crypto APIs ensure secrets never bleed across trust boundaries or timing domains. The Rich Execution Environment can interact only via hardware-mediated Secure Monitor Calls, collapsing the surface exposed to malware in Android space. End-users enjoy a single polished interface yet can install or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap, shrinking both storage footprint and audit scope. For auditors, the composition model slashes duplicated verification effort by quarantining blockchain logic inside narrowly scoped modules that share formally specified interfaces. Our threat analysis spans six adversary layers and shows how the design neutralizes REE malware sniffing, OTA injection, and cross-module side channels without exotic hardware. A reference implementation on AOSP exports a Wallet Manager HAL, custom SELinux domains, and a CI/CD pipeline that vet community modules before release. The result is not merely another hardware wallet but a programmable substrate that can evolve at the velocity of the blockchain ecosystem. By welding radical extensibility to hardware-anchored assurance, the platform closes the security-usability gap that has long stymied mass-market self-custody. We posit that modular TEEs are the missing OS primitive for Web3, much as virtual memory unlocked multi-tasking in classical computing. Together, these contributions sketch a blueprint for multi-chain asset management that is auditable, resilient, and poised for global deployment.

**Link**: [arxiv](http://arxiv.org/abs/2506.17988v1),  [pdf](http://arxiv.org/pdf/2506.17988v1)

**Tags**: cs.CR 



### ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training
**Authors**: Maryam Dialameh, Rezaul Karim, Hossein Rajabzadeh, Omar Mohamed Awad, Hyock Ju Kwon, Boxing Chen, Walid Ahmed, Yang Liu

**Updated**: 2025-06-22T03:46:11Z

**Summary**: This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to improve both the training speed and inference throughput of LLaMA architectures while maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models into shared KV caching across certain layers, significantly reducing KV computational complexity while maintaining or improving language performance. Experimental results demonstrate that ECHO-LLaMA achieves up to 77\% higher token-per-second throughput during training, up to 16\% higher Model FLOPs Utilization (MFU), and up to 14\% lower loss when trained on an equal number of tokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\% higher test-time throughput compared to the baseline. By introducing a computationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable and cost-effective solution for pretraining and finetuning large language models, enabling faster and more resource-efficient training without compromising performance.

**Link**: [arxiv](http://arxiv.org/abs/2505.17331v2),  [pdf](http://arxiv.org/pdf/2505.17331v2)

**Tags**: cs.LG cs.CL 



### RPLKG: Robust Prompt Learning with Knowledge Graph
**Authors**: YongTaek Lim, Yewon Kim, Suho Kang, Dokyung Yoon, KyungWoo Song

**Updated**: 2025-06-21T08:27:10Z

**Summary**: Large-scale pre-trained models surpass in transferability and robust generalization across diverse datasets. The emergence of multimodal pre-trained models like CLIP has significantly boosted performance in various experiments. However, generalizing to new datasets or domains remains challenging, especially with limited labeled data. Also, existing methods often lack interpretability and impose high computational costs. To address this, we propose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the knowledge graph to curate diverse, interpretable prompt sets automatically. Our method autonomously selects the optimal interpretable prompt based on dataset characteristics, achieving performance improvements over zero-shot learning and competitive performance compared to various prompt learning methods. Also, RPLKG efficiently reuses cached prompt embeddings from a single model pass and optimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast training. Moreover, RPLKG advances few-shot learning effectiveness while enhancing interpretability and efficiency in model adaptation. Our

**Link**: [arxiv](http://arxiv.org/abs/2304.10805v2),  [pdf](http://arxiv.org/pdf/2304.10805v2)

**Tags**: cs.AI cs.LG 



### A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models
**Authors**: Yanting Miao, William Loh, Suraj Kothawade, Pacal Poupart

**Updated**: 2025-06-20T16:59:05Z

**Summary**: Recent work uses reinforcement learning (RL) to fine-tune text-to-image diffusion models, improving text-image alignment and sample quality. However, existing approaches introduce unnecessary complexity: they cache the full sampling trajectory, depend on differentiable reward models or large preference datasets, or require specialized guidance techniques. Motivated by the "golden noise" hypothesis -- that certain initial noise samples can consistently yield superior alignment -- we introduce Noise PPO, a minimalist RL algorithm that leaves the pre-trained diffusion model entirely frozen and learns a prompt-conditioned initial noise generator. Our approach requires no trajectory storage, reward backpropagation, or complex guidance tricks. Extensive experiments show that optimizing the initial noise distribution consistently improves alignment and sample quality over the original model, with the most significant gains at low inference steps. As the number of inference steps increases, the benefit of noise optimization diminishes but remains present. These findings clarify the scope and limitations of the golden noise hypothesis and reinforce the practical value of minimalist RL fine-tuning for diffusion models.

**Link**: [arxiv](http://arxiv.org/abs/2506.12036v2),  [pdf](http://arxiv.org/pdf/2506.12036v2)

**Tags**: cs.LG cs.AI 



### Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context   LMs?
**Authors**: Adithya Bhaskar, Alexander Wettig, Tianyu Gao, Yihe Dong, Danqi Chen

**Updated**: 2025-06-20T16:21:12Z

**Summary**: Language models handle increasingly long contexts for tasks such as book summarization, but this leads to growing memory costs for the key-value (KV) cache. Many prior works have proposed ways of discarding KVs from memory, but their approaches are tailored to favorable settings, obscuring caveats like high peak memory and performance degradation, and a fair comparison between methods is difficult. In this paper, we propose the *KV footprint* as a unified metric, which accounts for both the amount of KV entries stored and their lifespan in memory. We evaluate methods based on the smallest footprint they attain while preserving performance in both long-context understanding and generation, with context lengths of up to 128K tokens. This metric reveals the high peak memory of prior KV eviction methods. One class of methods -- *post-fill eviction* -- has a high footprint due to being incompatible with eviction during pre-filling. We adapt these methods to be able to evict KVs during pre-filling, achieving substantially lower KV footprints. We then turn to *recency eviction* methods, wherein we propose PruLong, an end-to-end optimization method for learning which attention heads need to retain the full KV cache and which do not. PruLong saves memory while preserving long-context performance, achieving 12% smaller KV footprint than prior methods while retaining performance in challenging recall tasks. Our paper clarifies the complex tangle of long-context inference methods and paves the way for future development to minimize the KV footprint.

**Link**: [arxiv](http://arxiv.org/abs/2506.17121v1),  [pdf](http://arxiv.org/pdf/2506.17121v1)

**Tags**: cs.CL 



### PUL: Pre-load in Software for Caches Wouldn't Always Play Along
**Authors**: Arthur Bernhardt, Sajjad Tamimi, Florian Stock, Andreas Koch, Ilia Petrov

**Updated**: 2025-06-20T13:09:26Z

**Summary**: Memory latencies and bandwidth are major factors, limiting system performance and scalability. Modern CPUs aim at hiding latencies by employing large caches, out-of-order execution, or complex hardware prefetchers. However, software-based prefetching exhibits higher efficiency, improving with newer CPU generations.   In this paper we investigate software-based, post-Moore systems that offload operations to intelligent memories. We show that software-based prefetching has even higher potential in near-data processing settings by maximizing compute utilization through compute/IO interleaving.

**Link**: [arxiv](http://arxiv.org/abs/2506.16976v1),  [pdf](http://arxiv.org/pdf/2506.16976v1)

**Tags**: cs.DB 



### PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental   Learning for Document Retrieval
**Authors**: Tuan-Luc Huynh, Thuy-Trang Vu, Weiqing Wang, Yinwei Wei, Trung Le, Dragan Gasevic, Yuan-Fang Li, Thanh-Toan Do

**Updated**: 2025-06-20T12:59:40Z

**Summary**: Differentiable Search Index (DSI) utilizes pre-trained language models to perform indexing and document retrieval via end-to-end learning without relying on external indexes. However, DSI requires full re-training to index new documents, causing significant computational inefficiencies. Continual learning (CL) offers a solution by enabling the model to incrementally update without full re-training. Existing CL solutions in document retrieval rely on memory buffers or generative models for rehearsal, which is infeasible when accessing previous training data is restricted due to privacy concerns. To this end, we introduce PromptDSI, a prompt-based, rehearsal-free continual learning approach for document retrieval. PromptDSI follows the Prompt-based Continual Learning (PCL) framework, using learnable prompts to efficiently index new documents without accessing previous documents or queries. To improve retrieval latency, we remove the initial forward pass of PCL, which otherwise greatly increases training and inference time, with a negligible trade-off in performance. Additionally, we introduce a novel topic-aware prompt pool that employs neural topic embeddings as fixed keys, eliminating the instability of prompt key optimization while maintaining competitive performance with existing PCL prompt pools. In a challenging rehearsal-free continual learning setup, we demonstrate that PromptDSI variants outperform rehearsal-based baselines, match the strong cache-based baseline in mitigating forgetting, and significantly improving retrieval performance on new corpora.

**Link**: [arxiv](http://arxiv.org/abs/2406.12593v3),  [pdf](http://arxiv.org/pdf/2406.12593v3)

**Tags**: cs.IR cs.AI cs.CL cs.LG 



### Serving Large Language Models on Huawei CloudMatrix384
**Authors**: Pengfei Zuo, Huimin Lin, Junbo Deng, Nan Zou, Xingkun Yang, Yingyu Diao, Weifeng Gao, Ke Xu, Zhangyu Chen, Shirui Lu, Zhao Qiu, Peiyang Li, Xianyu Chang, Zhengzhong Yu, Fangzheng Miao, Jia Zheng, Ying Li, Yuan Feng, Bei Wang, Zaijian Zong, Mosong Zhou, Wenli Zhou, Houjiang Chen, Xingyu Liao, Yipeng Li, Wenxiao Zhang, Ping Zhu, Yinggang Wang, Chuanjie Xiao, Depeng Liang, Dong Cao, Juncheng Liu, Yongqiang Yang, Xiaolong Bai, Yi Li, Huaguo Xie, Huatao Wu, Zhibin Yu, Lv Chen, Hu Liu, Yujun Ding, Haipei Zhu, Jing Xia, Yi Xiong, Zhou Yu, Heng Liao

**Updated**: 2025-06-19T12:27:10Z

**Summary**: The rapid evolution of large language models (LLMs), driven by growing parameter scales, adoption of mixture-of-experts (MoE) architectures, and expanding context lengths, imposes unprecedented demands on AI infrastructure. Traditional AI clusters face limitations in compute intensity, memory bandwidth, inter-chip communication, and latency, compounded by variable workloads and strict service-level objectives. Addressing these issues requires fundamentally redesigned hardware-software integration. This paper introduces Huawei CloudMatrix, a next-generation AI datacenter architecture, realized in the production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910 NPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified Bus (UB) network, enabling direct all-to-all communication and dynamic pooling of resources. These features optimize performance for communication-intensive operations, such as large-scale MoE expert parallelism and distributed key-value cache access. To fully leverage CloudMatrix384, we propose CloudMatrix-Infer, an advanced LLM serving solution incorporating three core innovations: a peer-to-peer serving architecture that independently scales prefill, decode, and caching; a large-scale expert parallelism strategy supporting EP320 via efficient UB-based token dispatch; and hardware-aware optimizations including specialized operators, microbatch-based pipelining, and INT8 quantization. Evaluation with the DeepSeek-R1 model shows CloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of 6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms TPOT). It effectively balances throughput and latency, sustaining 538 tokens/s per NPU even under stringent 15 ms latency constraints, while INT8 quantization maintains model accuracy across benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2506.12708v3),  [pdf](http://arxiv.org/pdf/2506.12708v3)

**Tags**: cs.DC cs.AI cs.AR cs.LG 



### A block Recycled GMRES method with investigations into aspects of solver   performance
**Authors**: Michael L. Parks, Kirk M. Soodhalter, Daniel B. Szyld

**Updated**: 2025-06-19T10:23:50Z

**Summary**: We propose a block Krylov subspace version of the GCRO-DR method proposed in [Parks et al.; SISC 2005], which is an iterative method allowing for the efficient minimization of the the residual over an augmented Krylov subspace. We offer a clean derivation of our proposed method and discuss methods of selecting recycling subspaces at restart as well as implementation decisions in the context of high-performance computing. Numerical experiments are split into those demonstrating convergence properties and those demonstrating the data movement and cache efficiencies of the dominant operations of the method, measured using processor monitoring code from Intel.

**Link**: [arxiv](http://arxiv.org/abs/1604.01713v2),  [pdf](http://arxiv.org/pdf/1604.01713v2)

**Tags**: math.NA cs.NA 65F10 



### Characterization of discharge capillaries via benchmarked hydrodynamic   plasma simulations
**Authors**: S. M. Mewes, G. J. Boyle, R. D'Arcy, J. M. Garland, M. Huck, H. Jones, G. Loisch, A. R. Maier, J. Osterhoff, T. Parikh, S. Wesch, J. C. Wood, M. Thévenet

**Updated**: 2025-06-19T10:17:28Z

**Summary**: Plasma accelerators utilize strong electric fields in plasma waves to accelerate charged particles, making them a compact alternative to radiofrequency technologies. Discharge capillaries are plasma sources used in plasma accelerator research to provide acceleration targets, or as plasma lenses to capture or focus accelerated beams. They have applications for beam-driven and laser-driven plasma accelerators and can sustain high repetition rates for extended periods of time. Despite these advantages, high-fidelity simulations of discharge capillaries remain challenging due to the range of mechanisms involved and the difficulty to diagnose them in experiments. In this work, we utilize hydrodynamic plasma simulations to examine the discharge process of a plasma cell and discuss implications for future accelerator systems. The simulation model is validated with experimental measurements in a 50-mm-long, 1-mm-wide plasma capillary operating a 12-27 kV discharge at 2-12mbar hydrogen pressure. For 20 kV at 8.7mbar the discharge is shown to deposit 178mJ of energy in the plasma. Potential difficulties with the common density measurement method using H{\alpha} emission spectroscopy are discussed. This simulation model enables investigations of repeatability, heat flow management and fine tailoring of the plasma profile with discharges.

**Link**: [arxiv](http://arxiv.org/abs/2506.16192v1),  [pdf](http://arxiv.org/pdf/2506.16192v1)

**Tags**: physics.plasm-ph 



### Resource Allocation for Twin Maintenance and Computing Task Processing   in Digital Twin Vehicular Edge Computing Network
**Authors**: Yu Xie, Qiong Wu, Pingyi Fan, Nan Cheng, Wen Chen, Jiangzhou Wang, Khaled B. Letaief

**Updated**: 2025-06-19T07:29:09Z

**Summary**: As a promising technology, vehicular edge computing (VEC) can provide computing and caching services by deploying VEC servers near vehicles. However, VEC networks still face challenges such as high vehicle mobility. Digital twin (DT), an emerging technology, can predict, estimate, and analyze real-time states by digitally modeling objects in the physical world. By integrating DT with VEC, a virtual vehicle DT can be created in the VEC server to monitor the real-time operating status of vehicles. However, maintaining the vehicle DT model requires ongoing attention from the VEC server, which also needs to offer computing services for the vehicles. Therefore, effective allocation and scheduling of VEC server resources are crucial. This study focuses on a general VEC network with a single VEC service and multiple vehicles, examining the two types of delays caused by twin maintenance and computational processing within the network. By transforming the problem using satisfaction functions, we propose an optimization problem aimed at maximizing each vehicle's resource utility to determine the optimal resource allocation strategy. Given the non-convex nature of the issue, we employ multi-agent Markov decision processes to reformulate the problem. Subsequently, we propose the twin maintenance and computing task processing resource collaborative scheduling (MADRL-CSTC) algorithm, which leverages multi-agent deep reinforcement learning. Through experimental comparisons with alternative algorithms, it demonstrates that our proposed approach is effective in terms of resource allocation.

**Link**: [arxiv](http://arxiv.org/abs/2407.07575v2),  [pdf](http://arxiv.org/pdf/2407.07575v2)

**Tags**: cs.LG cs.NI 



### LazyEviction: Lagged KV Eviction with Attention Pattern Observation for   Efficient Long Reasoning
**Authors**: Haoyue Zhang, Hualei Zhang, Xiaosong Ma, Jie Zhang, Song Guo

**Updated**: 2025-06-19T02:25:04Z

**Summary**: Large Language Models (LLMs) exhibit enhanced reasoning capabilities by employing Chain-of-Thought (CoT). However, the extended reasoning sequences introduce significant GPU memory overhead due to increased key-value (KV) cache size, particularly in tasks requiring long reasoning sequences, such as mathematics and programming. Existing KV cache compression methods mitigate memory bottlenecks but struggle in long reasoning tasks. In this paper, we analyze attention patterns in reasoning tasks and reveal a Token Importance Recurrence phenomenon: a large proportion of tokens receive renewed attention after multiple decoding steps, which is failed to capture by existing works and may lead to unpredictable eviction on such periodically critical tokens. To address this, we propose LazyEviction, a lagged KV eviction framework designed to maintain reasoning performance while reducing KV memory. LazyEviction is an Observation Window-based Lagged Eviction Mechanism retaining latent recurring tokens by performing lagged evictions across decoding steps, which contains two key components: (1) Recurrence Interval Tracking for capturing temporal variations in token importance, and (2) an Maximum Recurrence Interval-Centric Eviction Policy that prioritizes eviction based on tokens' recurrence patterns. Extensive experiments demonstrate that LazyEviction reduces KV cache size by 50% while maintaining comparable accuracy on mathematics reasoning datasets, outperforming state-of-the-art methods. Our findings highlight the importance of preserving recurring tokens, which are critical for maintaining knowledge continuity in multi-step reasoning tasks.

**Link**: [arxiv](http://arxiv.org/abs/2506.15969v1),  [pdf](http://arxiv.org/pdf/2506.15969v1)

**Tags**: cs.LG cs.CL 



### KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache   at a Large Cloud Provider
**Authors**: Jiahao Wang, Jinbo Han, Xingda Wei, Sijie Shen, Dingyan Zhang, Chenguang Fang, Rong Chen, Wenyuan Yu, Haibo Chen

**Updated**: 2025-06-19T02:18:16Z

**Summary**: Serving large language models (LLMs) is important for cloud providers, and caching intermediate results (KV\$) after processing each request substantially improves serving throughput and latency. However, there is limited understanding of how LLM serving benefits from KV\$ caching, where system design decisions like cache eviction policies are highly workload-dependent. In this paper, we present the first systematic characterization of the KV\$ workload patterns from one of the leading LLM service providers. We draw observations that were not covered by previous studies focusing on synthetic workloads, including: KV\$ reuses are skewed across requests, where reuses between single-turn requests are equally important as multi-turn requests; the reuse time and probability are diverse considering all requests, but for a specific request category, the pattern tends to be predictable; and the overall cache size required for an ideal cache hit ratio is moderate. Based on the characterization, we further propose a workload-aware cache eviction policy that improves the serving performance under real-world traces, especially with limited cache capacity.

**Link**: [arxiv](http://arxiv.org/abs/2506.02634v3),  [pdf](http://arxiv.org/pdf/2506.02634v3)

**Tags**: cs.DC cs.AI 



### Medha: Efficiently Serving Multi-Million Context Length LLM Inference   Requests Without Approximations
**Authors**: Amey Agrawal, Haoran Qiu, Junda Chen, Íñigo Goiri, Chaojie Zhang, Rayyan Shahid, Ramachandran Ramjee, Alexey Tumanov, Esha Choukse

**Updated**: 2025-06-18T22:51:06Z

**Summary**: As large language models (LLMs) handle increasingly longer contexts, serving long inference requests of millions of tokens presents unique challenges. We show that existing work for long context inference is largely based on techniques from long context training, and does not handle the high variability in input lengths during inference. This leads to inefficient resource utilization, server fragmentation, and head-of-line (HOL) blocking.   We present Medha, an end-to-end system for efficient long-context LLM inference that addresses these challenges through fine-grained time sharing. Medha introduces three key innovations: (1) the mechanism of adaptive prefill chunking to help mitigate HOL blocking with preemption; (2) two new parallelism strategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token by pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower time-peroutput-token by distributing decoding across servers; and (3) a novel input-length aware least remaining slack scheduling to meet Service Level Objectives (SLOs).   Medha enables exact inference scaling beyond 10 million tokens, maintaining high throughput and low latency across mixed-length workloads. Compared to state-of-the-art systems, Medha reduces server fragmentation, cuts median latency by up to 30x, and improves throughput by over 5x, delivering production-scale long-context inference without compromising performance on shorter requests.

**Link**: [arxiv](http://arxiv.org/abs/2409.17264v4),  [pdf](http://arxiv.org/pdf/2409.17264v4)

**Tags**: cs.LG cs.DC 



### Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model
**Authors**: Anirud Aggarwal, Abhinav Shrivastava, Matthew Gwilliam

**Updated**: 2025-06-18T17:59:50Z

**Summary**: Diffusion-based image generation models excel at producing high-quality synthetic content, but suffer from slow and computationally expensive inference. Prior work has attempted to mitigate this by caching and reusing features within diffusion transformers across inference steps. These methods, however, often rely on rigid heuristics that result in limited acceleration or poor generalization across architectures. We propose Evolutionary Caching to Accelerate Diffusion models (ECAD), a genetic algorithm that learns efficient, per-model, caching schedules forming a Pareto frontier, using only a small set of calibration prompts. ECAD requires no modifications to network parameters or reference images. It offers significant inference speedups, enables fine-grained control over the quality-latency trade-off, and adapts seamlessly to different diffusion models. Notably, ECAD's learned schedules can generalize effectively to resolutions and model variants not seen during calibration. We evaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple metrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k, PartiPrompts), demonstrating consistent improvements over previous approaches. On PixArt-alpha, ECAD identifies a schedule that outperforms the previous state-of-the-art method by 4.47 COCO FID while increasing inference speedup from 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable approach for accelerating diffusion inference. Our project website is available at https://aniaggarwal.github.io/ecad and our code is available at https://github.com/aniaggarwal/ecad.

**Link**: [arxiv](http://arxiv.org/abs/2506.15682v1),  [pdf](http://arxiv.org/pdf/2506.15682v1)

**Tags**: cs.CV 



### Demystifying the Visual Quality Paradox in Multimodal Large Language   Models
**Authors**: Shuo Xing, Lanqing Guo, Hongyuan Hua, Seoyoung Lee, Peiran Li, Yufei Wang, Zhangyang Wang, Zhengzhong Tu

**Updated**: 2025-06-18T17:14:07Z

**Summary**: Recent Multimodal Large Language Models (MLLMs) excel on benchmark vision-language tasks, yet little is known about how input visual quality shapes their responses. Does higher perceptual quality of images already translate to better MLLM understanding? We conduct the first systematic study spanning leading MLLMs and a suite of vision-language benchmarks, applying controlled degradations and stylistic shifts to each image. Surprisingly, we uncover a visual-quality paradox: model, task, and even individual-instance performance can improve when images deviate from human-perceived fidelity. Off-the-shelf restoration pipelines fail to reconcile these idiosyncratic preferences. To close the gap, we introduce Visual-Quality Test-Time Tuning (VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable, low-rank kernel before the frozen vision encoder to modulate frequency content; and (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT dynamically adjusts each input image in a single forward pass, aligning it with task-specific model preferences. Across the evaluated MLLMs and all datasets, VQ-TTT lifts significant average accuracy, with no external models, cached features, or extra training data. These findings redefine ``better'' visual inputs for MLLMs and highlight the need for adaptive, rather than universally ``clean'', imagery, in the new era of AI being the main data customer.

**Link**: [arxiv](http://arxiv.org/abs/2506.15645v1),  [pdf](http://arxiv.org/pdf/2506.15645v1)

**Tags**: cs.CV cs.AI 



### From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and   Instruction Annotation
**Authors**: Miryeong Kwon, Donghyun Gouk, Junhyeok Jang, Jinwoo Baek, Hyunwoo You, Sangyoon Ji, Hongjoo Jung, Junseok Moon, Seungkwan Kang, Seungjun Lee, Myoungsoo Jung

**Updated**: 2025-06-18T16:44:04Z

**Summary**: This paper explores how Compute Express Link (CXL) can transform PCIe-based block storage into a scalable, byte-addressable working memory. We address the challenges of adapting block storage to CXL's memory-centric model by emphasizing cacheability as a key enabler and advocating for Type 3 endpoint devices, referred to as CXL-SSDs. To validate our approach, we prototype a CXL-SSD on a custom FPGA platform and propose annotation mechanisms, Determinism and Bufferability, to enhance performance while preserving data persistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves 10.9x better performance than PCIe-based memory expanders and further reduces latency by 5.4x with annotation enhancements. In workloads with high locality, CXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This work highlights the feasibility of integrating block storage into CXL's ecosystem and provides a foundation for future memory-storage convergence.

**Link**: [arxiv](http://arxiv.org/abs/2506.15613v1),  [pdf](http://arxiv.org/pdf/2506.15613v1)

**Tags**: cs.AR 



### LaViDa: A Large Diffusion Language Model for Multimodal Understanding
**Authors**: Shufan Li, Konstantinos Kallidromitis, Hritik Bansal, Akash Gokul, Yusuke Kato, Kazuki Kozuka, Jason Kuen, Zhe Lin, Kai-Wei Chang, Aditya Grover

**Updated**: 2025-06-18T15:17:40Z

**Summary**: Modern Vision-Language Models (VLMs) can solve a wide range of tasks requiring visual reasoning. In real-world scenarios, desirable properties for VLMs include fast inference and controllable generation (e.g., constraining outputs to adhere to a desired format). However, existing autoregressive (AR) VLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs) offer a promising alternative, enabling parallel decoding for faster inference and bidirectional context for controllable generation through text-infilling. While effective in language-only settings, DMs' potential for multimodal tasks is underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build LaViDa by equipping DMs with a vision encoder and jointly fine-tune the combined parts for multimodal instruction following. To address challenges encountered, LaViDa incorporates novel techniques such as complementary masking for effective training, prefix KV cache for efficient inference, and timestep shifting for high-quality sampling. Experiments show that LaViDa achieves competitive or superior performance to AR VLMs on multi-modal benchmarks such as MMMU, while offering unique advantages of DMs, including flexible speed-quality tradeoff, controllability, and bidirectional reasoning. On COCO captioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x speedup. On bidirectional tasks, it achieves +59% improvement on Constrained Poem Completion. These results demonstrate LaViDa as a strong alternative to AR VLMs. Code and models will be released in the camera-ready version.

**Link**: [arxiv](http://arxiv.org/abs/2505.16839v3),  [pdf](http://arxiv.org/pdf/2505.16839v3)

**Tags**: cs.CV 



### VideoMAR: Autoregressive Video Generatio with Continuous Tokens
**Authors**: Hu Yu, Biao Gong, Hangjie Yuan, DanDan Zheng, Weilong Chai, Jingdong Chen, Kecheng Zheng, Feng Zhao

**Updated**: 2025-06-18T09:44:09Z

**Summary**: Masked-based autoregressive models have demonstrated promising image generation capability in continuous space. However, their potential for video generation remains under-explored. In this paper, we propose \textbf{VideoMAR}, a concise and efficient decoder-only autoregressive image-to-video model with continuous tokens, composing temporal frame-by-frame and spatial masked generation. We first identify temporal causality and spatial bi-directionality as the first principle of video AR models, and propose the next-frame diffusion loss for the integration of mask and video generation. Besides, the huge cost and difficulty of long sequence autoregressive modeling is a basic but crucial issue. To this end, we propose the temporal short-to-long curriculum learning and spatial progressive resolution training, and employ progressive temperature strategy at inference time to mitigate the accumulation error. Furthermore, VideoMAR replicates several unique capacities of language models to video generation. It inherently bears high efficiency due to simultaneous temporal-wise KV cache and spatial-wise parallel generation, and presents the capacity of spatial and temporal extrapolation via 3D rotary embeddings. On the VBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos I2V) while requiring significantly fewer parameters ($9.3\%$), training data ($0.5\%$), and GPU resources ($0.2\%$).

**Link**: [arxiv](http://arxiv.org/abs/2506.14168v2),  [pdf](http://arxiv.org/pdf/2506.14168v2)

**Tags**: cs.CV cs.AI 



### A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in   GPUs
**Authors**: Hossein Albakri, Kazem Cheshmi

**Updated**: 2025-06-18T06:41:35Z

**Summary**: Sparse data structures are commonly used in neural networks to reduce the memory footprint. These data structures are compact but cause irregularities such as random memory accesses, which prevent efficient use of the memory hierarchy. GPUs are a common platform for machine learning practitioners, but running compact data structures on these devices often leads to slow-downs due to inefficient use of computing and memory resources. This paper proposes a new compiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse matrix-matrix multiplication (SPMM) on GPU devices. The transformation increases data reuse in registers and caches while creating more balanced workloads for GPU computing resources. The transformation is tested on sparse neural networks in convolutional and transformer models. On an A100 GPU and across a columns of matrix B (bCols) in $ A \times B = C$ from range of 32 to 128, the transformation yields a geometric mean speedup of 1.84$\times$ to 2.27$\times$ compared to cuBLAS and cuSPARSE baselines, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2506.15174v1),  [pdf](http://arxiv.org/pdf/2506.15174v1)

**Tags**: cs.PL 



### eLLM: Elastic Memory Management Framework for Efficient LLM Serving
**Authors**: Jiale Xu, Rui Zhang, Yi Xiong, Cong Guo, Zihan Liu, Yangjie Zhou, Weiming Hu, Hao Wu, Changxu Shao, Ziqing Wang, Yongjie Yuan, Junping Zhao, Minyi Guo, Jingwen Leng

**Updated**: 2025-06-18T05:56:01Z

**Summary**: Large Language Models are increasingly being deployed in datacenters. Serving these models requires careful memory management, as their memory usage includes static weights, dynamic activations, and key-value caches. While static weights are constant and predictable, dynamic components such as activations and KV caches change frequently during runtime, presenting significant challenges for efficient memory management. Modern LLM serving systems typically handle runtime memory and KV caches at distinct abstraction levels: runtime memory management relies on static tensor abstractions, whereas KV caches utilize a page table-based virtualization layer built on top of the tensor abstraction. This virtualization dynamically manages KV caches to mitigate memory fragmentation. However, this dual-level approach fundamentally isolates runtime memory and KV cache management, resulting in suboptimal memory utilization under dynamic workloads, which can lead to a nearly 20% drop in throughput.   To address these limitations, we propose eLLM, an elastic memory management framework inspired by the classical memory ballooning mechanism in operating systems. The core components of eLLM include: (1) Virtual Tensor Abstraction, which decouples the virtual address space of tensors from the physical GPU memory, creating a unified and flexible memory pool; (2) an Elastic Memory Mechanism that dynamically adjusts memory allocation through runtime memory inflation and deflation, leveraging CPU memory as an extensible buffer; and (3) a Lightweight Scheduling Strategy employing SLO-aware policies to optimize memory utilization and effectively balance performance trade-offs under stringent SLO constraints. Comprehensive evaluations demonstrate that eLLM significantly outperforms state-of-the-art systems, 2.32x higher decoding throughput, and supporting 3x larger batch sizes for 128K-token inputs.

**Link**: [arxiv](http://arxiv.org/abs/2506.15155v1),  [pdf](http://arxiv.org/pdf/2506.15155v1)

**Tags**: cs.DC 



### InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video   Understanding
**Authors**: Minsoo Kim, Kyuhong Shim, Jungwook Choi, Simyung Chang

**Updated**: 2025-06-18T02:22:14Z

**Summary**: Modern multimodal large language models (MLLMs) can reason over hour-long video, yet their key-value (KV) cache grows linearly with time--quickly exceeding the fixed memory of phones, AR glasses, and edge robots. Prior compression schemes either assume the whole video and user query are available offline or must first build the full cache, so memory still scales with stream length. InfiniPot-V is the first training-free, query-agnostic framework that enforces a hard, length-independent memory cap for streaming video understanding. During video encoding it monitors the cache and, once a user-set threshold is reached, runs a lightweight compression pass that (i) removes temporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii) keeps semantically significant tokens via Value-Norm (VaN) ranking. Across four open-source MLLMs and four long-video and two streaming-video benchmarks, InfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation, and matches or surpasses full-cache accuracy--even in multi-turn dialogues. By dissolving the KV cache bottleneck without retraining or query knowledge, InfiniPot-V closes the gap for on-device streaming video assistants.

**Link**: [arxiv](http://arxiv.org/abs/2506.15745v1),  [pdf](http://arxiv.org/pdf/2506.15745v1)

**Tags**: eess.IV cs.LG 



### Compatibility of trapped ions and dielectrics at cryogenic temperatures
**Authors**: M. Bruff, L. Sonderhouse, K. N. David, J. Stuart, D. H. Slichter, D. Leibfried

**Updated**: 2025-06-18T01:37:55Z

**Summary**: We study the impact of an unshielded dielectric $\unicode{x2013}$ here, a bare optical fiber $\unicode{x2013}$ on a $^{40}$Ca${^+}$ ion held several hundred $\mu$m away in a cryogenic surface electrode trap. We observe distance-dependent stray electric fields of up to a few kV/m due to the dielectric, which drift on average less than 10% per month and can be fully compensated with reasonable voltages on the trap electrodes. We observe ion motional heating rates attributable to the dielectric of $\approx$30 quanta per second at an ion-fiber distance of 215(4) $\mu$m and $\approx$1.5 MHz motional frequency. These results demonstrate the viability of using unshielded, trap-integrated dielectric objects such as miniature optical cavities or other optical elements in cryogenic surface electrode ion traps.

**Link**: [arxiv](http://arxiv.org/abs/2506.15057v1),  [pdf](http://arxiv.org/pdf/2506.15057v1)

**Tags**: physics.atom-ph quant-ph 



### CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal   Diffusion
**Authors**: Jiahua Ma, Yiran Qin, Yixiong Li, Xuanqi Liao, Yulan Guo, Ruimao Zhang

**Updated**: 2025-06-17T17:59:12Z

**Summary**: Diffusion Policy (DP) enables robots to learn complex behaviors by imitating expert demonstrations through action diffusion. However, in practical applications, hardware limitations often degrade data quality, while real-time constraints restrict model inference to instantaneous state and scene observations. These limitations seriously reduce the efficacy of learning from expert demonstrations, resulting in failures in object localization, grasp planning, and long-horizon task execution. To address these challenges, we propose Causal Diffusion Policy (CDP), a novel transformer-based diffusion model that enhances action prediction by conditioning on historical action sequences, thereby enabling more coherent and context-aware visuomotor policy learning. To further mitigate the computational cost associated with autoregressive inference, a caching mechanism is also introduced to store attention key-value pairs from previous timesteps, substantially reducing redundant computations during execution. Extensive experiments in both simulated and real-world environments, spanning diverse 2D and 3D manipulation tasks, demonstrate that CDP uniquely leverages historical action sequences to achieve significantly higher accuracy than existing methods. Moreover, even when faced with degraded input observation quality, CDP maintains remarkable precision by reasoning through temporal continuity, which highlights its practical robustness for robotic control under realistic, imperfect conditions.

**Link**: [arxiv](http://arxiv.org/abs/2506.14769v1),  [pdf](http://arxiv.org/pdf/2506.14769v1)

**Tags**: cs.CV cs.RO 



### Keigo: Co-designing Log-Structured Merge Key-Value Stores with a   Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)
**Authors**: Rúben Adão, Zhongjie Wu, Changjun Zhou, Oana Balmau, João Paulo, Ricardo Macedo

**Updated**: 2025-06-17T15:25:11Z

**Summary**: We present Keigo, a concurrency- and workload-aware storage middleware that enhances the performance of log-structured merge key-value stores (LSM KVS) when they are deployed on a hierarchy of storage devices. The key observation behind Keigo is that there is no one-size-fits-all placement of data across the storage hierarchy that optimizes for all workloads. Hence, to leverage the benefits of combining different storage devices, Keigo places files across different devices based on their parallelism, I/O bandwidth, and capacity. We introduce three techniques - concurrency-aware data placement, persistent read-only caching, and context-based I/O differentiation. Keigo is portable across different LSMs, is adaptable to dynamic workloads, and does not require extensive profiling. Our system enables established production KVS such as RocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We evaluate Keigo using synthetic and realistic workloads, showing that it improves the throughput of production-grade LSMs up to 4x for write- and 18x for read-heavy workloads when compared to general-purpose storage systems and specialized LSM KVS.

**Link**: [arxiv](http://arxiv.org/abs/2506.14630v1),  [pdf](http://arxiv.org/pdf/2506.14630v1)

**Tags**: cs.DC cs.DB 



### LongSpec: Long-Context Lossless Speculative Decoding with Efficient   Drafting and Verification
**Authors**: Penghui Yang, Cunxiao Du, Fengzhuo Zhang, Haonan Wang, Tianyu Pang, Chao Du, Bo An

**Updated**: 2025-06-17T05:58:01Z

**Summary**: As Large Language Models (LLMs) can now process extremely long contexts, efficient inference over these extended inputs has become increasingly important, especially for emerging applications like LLM agents that highly depend on this capability. Speculative decoding (SD) offers a promising lossless acceleration technique compared to lossy alternatives such as quantization and model cascades. However, most state-of-the-art SD methods are trained on short texts (typically fewer than 4k tokens), making them unsuitable for long-context scenarios. Specifically, adapting these methods to long contexts presents three key challenges: (1) the excessive memory demands posed by draft models due to large Key-Value (KV) cache; (2) performance degradation resulting from the mismatch between short-context training and long-context inference; and (3) inefficiencies in tree attention mechanisms when managing long token sequences. This work introduces LongSpec, a framework that addresses these challenges through three core innovations: a memory-efficient draft model with a constant-sized KV cache; novel position indices that mitigate the training-inference mismatch; and an attention aggregation strategy that combines fast prefix computation with standard tree attention to enable efficient decoding. Experimental results confirm the effectiveness of LongSpec, achieving up to a 3.26x speedup over strong Flash Attention baselines across five long-context understanding datasets, as well as a 2.25x reduction in wall-clock time on the AIME24 long reasoning task with the QwQ model, demonstrating significant latency improvements for long-context applications. The code is available at https://github.com/sail-sg/LongSpec.

**Link**: [arxiv](http://arxiv.org/abs/2502.17421v2),  [pdf](http://arxiv.org/pdf/2502.17421v2)

**Tags**: cs.CL cs.AI cs.LG 



### Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching
**Authors**: Qizheng Zhang, Michael Wornow, Kunle Olukotun

**Updated**: 2025-06-17T04:42:30Z

**Summary**: LLM-based agentic applications have shown increasingly remarkable capabilities in complex workflows but incur substantial costs due to extensive planning and reasoning requirements. Existing LLM caching techniques (like context caching and semantic caching), primarily designed for serving chatbots, are insufficient for agentic applications where outputs depend on external data or environmental contexts. We propose agentic plan caching, a novel approach that extracts, stores, adapts, and reuses structured plan templates from planning stages of agentic applications across semantically similar tasks to reduce the cost of serving. Unlike traditional semantic caching, our system extracts plan templates from completed agent executions at test-time, employs keyword extraction to match new requests against cached plans, and utilizes lightweight models to adapt these templates to task-specific plans with contexts. Evaluation across multiple real-world agentic applications shows that our system can reduce costs by 46.62% on average while maintaining performance, offering a more efficient solution for serving LLM-based agents that complements existing LLM serving infrastructures.

**Link**: [arxiv](http://arxiv.org/abs/2506.14852v1),  [pdf](http://arxiv.org/pdf/2506.14852v1)

**Tags**: cs.DC cs.AI cs.CL cs.LG cs.PF 



### CXLMemSim: A pure software simulated CXL.mem for performance   characterization
**Authors**: Yiwei Yang, Brian Zhao, Yusheng Zheng, Pooneh Safayenikoo, Tanvir Ahmed Khan, Andi Quinn

**Updated**: 2025-06-17T04:00:42Z

**Summary**: CXLMemSim is a fast, lightweight simulation framework that enables performance characterization of memory systems based on Compute Express Link (CXL) .mem technology. CXL.mem allows disaggregation and pooling of memory to mitigate memory stranding (underutilized memory trapped on fully loaded servers) in cloud and datacenter environments. However, CXL-attached memory introduces additional latency and bandwidth constraints compared to local DRAM, and real CXL .mem hardware is not yet widely available for empirical evaluation. CXLMemSim addresses this gap by attaching to unmodified applications and simulating CXL-based memory pools in software. It operates by tracing memory allocations and accesses using efficient kernel probes and hardware performance counters, dividing execution into epochs, and injecting timing delays to emulate various CXL .mem latency/bandwidth characteristics. This approach incurs modest runtime overhead while preserving realistic load/store memory access patterns. We implement CXLMemSim on commodity hardware without special devices, and our evaluation shows that it runs orders of magnitude faster than cycle-accurate simulators (e.g., Gem5) for real-world workloads, while accurately modeling the performance impact of CXL .mem. We demonstrate use cases where CXLMemSim enables experimentation with memory pooling configurations, scheduling policies, data migration strategies, and caching techniques that were previously infeasible to evaluate at scale. Key findings include the viability of software-based CXL .mem emulation with low overhead, insights into latency and congestion effects in memory pools, and guidance for system designers to optimize memory disaggregation. Overall, CXLMemSim provides a practical and extensible platform for researchers and practitioners to explore CXL.mem innovations before real hardware becomes commonplace.

**Link**: [arxiv](http://arxiv.org/abs/2303.06153v2),  [pdf](http://arxiv.org/pdf/2303.06153v2)

**Tags**: cs.PF cs.AR cs.OS 



### Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache   Compression
**Authors**: Michael R. Metel, Boxing Chen, Mehdi Rezagholizadeh

**Updated**: 2025-06-17T02:24:51Z

**Summary**: Several works have developed eviction policies to remove key-value (KV) pairs from the KV cache for more efficient inference. The focus has been on compressing the KV cache after the input prompt has been processed for faster token generation. In settings with limited GPU memory, and when the input context is longer than the generation length, we show that by also compressing the KV cache during the input processing phase, larger batch sizes can be used resulting in significantly higher throughput while still maintaining the original model's accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2412.05693v2),  [pdf](http://arxiv.org/pdf/2412.05693v2)

**Tags**: cs.CL 



### All-optical electric field sensing with nanodiamond-doped polymer thin   films
**Authors**: Roy Styles, Mengke Han, Toon Goris, James Partridge, Brett C. Johnson, Blanca del Rosal, Amanda N. Abraham, Heike Ebendorff-Heidepriem, Brant C. Gibson, Nikolai Dontschuk, Jean-Philippe Tetienne, Philipp Reineck

**Updated**: 2025-06-17T00:26:21Z

**Summary**: The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that exists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the NV's nanoscale environment. Here, we show that photoluminescence (PL) from NV centers in fluorescent nanodiamonds (FNDs) can be employed for all-optical voltage sensing based on electric field-induced NV charge state modulation. More than 95% of FNDs integrated into a capacitor device show a transient increase in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of an external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The change in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V, corresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices. The electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$. We investigate the NV charge state photodynamics on the millisecond timescale and find that the change in NV PL strongly depends on the rate of photoexcitation. We propose a model that qualitatively explains the observed changes in NV PL based on an electric field-induced redistribution of photoexcited electrons from substitutional nitrogen defects to NV centers, leading to a transient conversion of NV$^0$ to NV$^-$ centers upon application of an external voltage. Our results contribute to the development of FNDs as reliable, all-optical, nanoscale electric field sensors in solid-state systems.

**Link**: [arxiv](http://arxiv.org/abs/2505.07350v2),  [pdf](http://arxiv.org/pdf/2505.07350v2)

**Tags**: cond-mat.mes-hall 



### glass: ordered set data structure for client-side order books
**Authors**: Viktor Krapivensky

**Updated**: 2025-06-16T20:46:20Z

**Summary**: The "ordered set" abstract data type with operations "insert", "erase", "find", "min", "max", "next" and "prev" is ubiquitous in computer science. It is usually implemented with red-black trees, $B$-trees, or $B^+$-trees. We present our implementation of ordered set based on a trie. It only supports integer keys (as opposed to keys of any strict weakly ordered type) and is optimized for market data, namely for what we call sequential locality. The following is the list of what we believe to be novelties:   * Cached path to exploit sequential locality, and fast truncation thereof on erase operation;   * A hash table (or, rather, a cache table) with hard O(1) time guarantees on any operation to speed up key lookup (up to a pre-leaf node);   * Hardware-accelerated "find next/previous set bit" operations with BMI2 instruction set extension on x86-64;   * Order book-specific features: the preemption principle and the tree restructure operation that prevent the tree from consuming too much memory.   We achieve the following speedups over C++'s standard std::map container: 6x-20x on modifying operations, 30x on lookup operations, 9x-15x on real market data, and a more modest 2x-3x speedup on iteration. In this paper, we discuss our implementation.

**Link**: [arxiv](http://arxiv.org/abs/2506.13991v1),  [pdf](http://arxiv.org/pdf/2506.13991v1)

**Tags**: cs.DS 



### Cache-Aided Variable-Length Coding with Perfect Privacy
**Authors**: Amirreza Zamani, Mikael Skoglund

**Updated**: 2025-06-16T17:17:38Z

**Summary**: A cache-aided compression problem with perfect privacy is studied, where a server has access to a database of $N$ files, $(Y_1,...,Y_N)$, each of size $F$ bits. The server is connected to $K$ users through a shared link, where each user has access to a local cache of size $MF$ bits. In the placement phase, the server fills the users$'$ caches without prior knowledge of their future demands, while the delivery phase takes place after the users send their demands to the server. We assume that each file $Y_i$ is arbitrarily correlated with a private attribute $X$, and an adversary is assumed to have access to the shared link. The users and the server have access to a shared secret key $W$. The goal is to design the cache contents and the delivered message $\cal C$ such that the average length of $\mathcal{C}$ is minimized, while satisfying: i. The response $\cal C$ does not disclose any information about $X$, i.e., $X$ and $\cal C$ are statistically independent yielding $I(X;\mathcal{C})=0$, which corresponds to the perfect privacy constraint; ii. User $i$ is able to decode its demand, $Y_{d_i}$, by using its local cache $Z_i$, delivered message $\cal C$, and the shared secret key $W$. Due to the correlation of database with the private attribute, existing codes for cache-aided delivery do not fulfill the perfect privacy constraint. Indeed, in this work, we propose a lossless variable-length coding scheme that combines privacy-aware compression with coded caching techniques. In particular, we use two-part code construction and Functional Representation Lemma. Furthermore, we propose an alternative coding scheme based on the minimum entropy coupling concept and a greedy entropy-based algorithm. We show that the proposed scheme improves the previous results obtained by Functional Representation Lemma.

**Link**: [arxiv](http://arxiv.org/abs/2306.13184v2),  [pdf](http://arxiv.org/pdf/2306.13184v2)

**Tags**: cs.IT math.IT 



### Mixture of Weight-shared Heterogeneous Group Attention Experts for   Dynamic Token-wise KV Optimization
**Authors**: Guanghui Song, Dongping Liao, Yiren Zhao, Kejiang Ye, Cheng-zhong Xu, Xitong Gao

**Updated**: 2025-06-16T14:30:17Z

**Summary**: Transformer models face scalability challenges in causal language modeling (CLM) due to inefficient memory allocation for growing key-value (KV) caches, which strains compute and storage resources. Existing methods like Grouped Query Attention (GQA) and token-level KV optimization improve efficiency but rely on rigid resource allocation, often discarding "low-priority" tokens or statically grouping them, failing to address the dynamic spectrum of token importance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that dynamically optimizes token-wise computation and memory allocation. Unlike prior approaches, mixSGA retains all tokens while adaptively routing them to specialized experts with varying KV group sizes, balancing granularity and efficiency. Our key novelties include: (1) a token-wise expert-choice routing mechanism guided by learned importance scores, enabling proportional resource allocation without token discard; (2) weight-sharing across grouped attention projections to minimize parameter overhead; and (3) an auxiliary loss to ensure one-hot routing decisions for training-inference consistency in CLMs. Extensive evaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show mixSGA's superiority over static baselines. On instruction-following and continued pretraining tasks, mixSGA achieves higher ROUGE-L and lower perplexity under the same KV budgets.

**Link**: [arxiv](http://arxiv.org/abs/2506.13541v1),  [pdf](http://arxiv.org/pdf/2506.13541v1)

**Tags**: cs.CL cs.LG 



### Block-wise Adaptive Caching for Accelerating Diffusion Policy
**Authors**: Kangye Ji, Yuan Meng, Hanyun Cui, Ye Li, Shengjia Hua, Lei Chen, Zhi Wang

**Updated**: 2025-06-16T13:14:58Z

**Summary**: Diffusion Policy has demonstrated strong visuomotor modeling capabilities, but its high computational cost renders it impractical for real-time robotic control. Despite huge redundancy across repetitive denoising steps, existing diffusion acceleration techniques fail to generalize to Diffusion Policy due to fundamental architectural and data divergences. In this paper, we propose Block-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by caching intermediate action features. BAC achieves lossless action generation acceleration by adaptively updating and reusing cached features at the block level, based on a key observation that feature similarities vary non-uniformly across timesteps and locks. To operationalize this insight, we first propose the Adaptive Caching Scheduler, designed to identify optimal update timesteps by maximizing the global feature similarities between cached and skipped features. However, applying this scheduler for each block leads to signiffcant error surges due to the inter-block propagation of caching errors, particularly within Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop the Bubbling Union Algorithm, which truncates these errors by updating the upstream blocks with signiffcant caching errors before downstream FFNs. As a training-free plugin, BAC is readily integrable with existing transformer-based Diffusion Policy and vision-language-action models. Extensive experiments on multiple robotic benchmarks demonstrate that BAC achieves up to 3x inference speedup for free.

**Link**: [arxiv](http://arxiv.org/abs/2506.13456v1),  [pdf](http://arxiv.org/pdf/2506.13456v1)

**Tags**: cs.AI cs.RO 



### On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed   Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains
**Authors**: Craig Steven Wright

**Updated**: 2025-06-16T08:43:56Z

**Summary**: This paper presents a formalised architecture for synthetic agents designed to retain immutable memory, verifiable reasoning, and constrained epistemic growth. Traditional AI systems rely on mutable, opaque statistical models prone to epistemic drift and historical revisionism. In contrast, we introduce the concept of the Merkle Automaton, a cryptographically anchored, deterministic computational framework that integrates formal automata theory with blockchain-based commitments. Each agent transition, memory fragment, and reasoning step is committed within a Merkle structure rooted on-chain, rendering it non-repudiable and auditably permanent. To ensure selective access and confidentiality, we derive symmetric encryption keys from ECDH exchanges contextualised by hierarchical privilege lattices. This enforces cryptographic access control over append-only DAG-structured knowledge graphs. Reasoning is constrained by formal logic systems and verified through deterministic traversal of policy-encoded structures. Updates are non-destructive and historied, preserving epistemic lineage without catastrophic forgetting. Zero-knowledge proofs facilitate verifiable, privacy-preserving inclusion attestations. Collectively, this architecture reframes memory not as a cache but as a ledger - one whose contents are enforced by protocol, bound by cryptography, and constrained by formal logic. The result is not an intelligent agent that mimics thought, but an epistemic entity whose outputs are provably derived, temporally anchored, and impervious to post hoc revision. This design lays foundational groundwork for legal, economic, and high-assurance computational systems that require provable memory, unforgeable provenance, and structural truth.

**Link**: [arxiv](http://arxiv.org/abs/2506.13246v1),  [pdf](http://arxiv.org/pdf/2506.13246v1)

**Tags**: cs.CR cs.AI cs.DC 68Q70, 68P25, 68T37 68Q70, 68P25, 68T37 68Q70, 68P25, 68T37 68Q70,
  68P25, 68T37 F.4.3; D.4.6; E.3; I.2.4 



### InfiniSST: Simultaneous Translation of Unbounded Speech with Large   Language Model
**Authors**: Siqi Ouyang, Xi Xu, Lei Li

**Updated**: 2025-06-16T06:38:23Z

**Summary**: Simultaneous translation of unbounded streaming speech remains a challenging problem due to the need for effectively processing the history speech context and past translations so that quality and latency, including computation overhead, can be balanced. Most prior works assume pre-segmented speech, limiting their real-world applicability. In this paper, we propose InfiniSST, a novel approach that formulates SST as a multi-turn dialogue task, enabling seamless translation of unbounded speech. We construct translation trajectories and robust segments from MuST-C with multi-latency augmentation during training and develop a key-value (KV) cache management strategy to facilitate efficient inference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that InfiniSST reduces computation-aware latency by 0.5 to 1 second while maintaining the same translation quality compared to baselines. Ablation studies further validate the contributions of our data construction and cache management strategy. We release the code and demo at https://github.com/LeiLiLab/InfiniSST

**Link**: [arxiv](http://arxiv.org/abs/2503.02969v2),  [pdf](http://arxiv.org/pdf/2503.02969v2)

**Tags**: cs.CL cs.AI 



### Multipole Attention for Efficient Long Context Reasoning
**Authors**: Coleman Hooper, Sebastian Zhao, Luca Manolache, Sehoon Kim, Michael W. Mahoney, Yakun Sophia Shao, Kurt Keutzer, Amir Gholami

**Updated**: 2025-06-16T03:00:40Z

**Summary**: Large Reasoning Models (LRMs) have shown promising accuracy improvements on complex problem-solving tasks. While these models have attained high accuracy by leveraging additional computation at test time, they need to generate long chain-of-thought reasoning in order to think before answering, which requires generating thousands of tokens. While sparse attention methods can help reduce the KV cache pressure induced by this long autoregressive reasoning, these methods can introduce errors which disrupt the reasoning process. Additionally, prior methods often pre-process the input to make it easier to identify the important prompt tokens when computing attention during generation, and this pre-processing is challenging to perform online for newly generated reasoning tokens. Our work addresses these challenges by introducing Multipole Attention, which accelerates autoregressive reasoning by only computing exact attention for the most important tokens, while maintaining approximate representations for the remaining tokens. Our method first performs clustering to group together semantically similar key vectors, and then uses the cluster centroids both to identify important key vectors and to approximate the remaining key vectors in order to retain high accuracy. We design a fast cluster update process to quickly re-cluster the input and previously generated tokens, thereby allowing for accelerating attention to the previous output tokens. We evaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our approach can maintain accuracy on complex reasoning tasks even with aggressive attention sparsity settings. We also provide kernel implementations to demonstrate the practical efficiency gains from our method, achieving up to 4.5$\times$ speedup for attention in long-context reasoning applications. Our code is available at https://github.com/SqueezeAILab/MultipoleAttention.

**Link**: [arxiv](http://arxiv.org/abs/2506.13059v1),  [pdf](http://arxiv.org/pdf/2506.13059v1)

**Tags**: cs.CL cs.LG 



### Latent Multi-Head Attention for Small Language Models
**Authors**: Sushant Mehta, Raj Dandekar, Rajat Dandekar, Sreedath Panat

**Updated**: 2025-06-16T02:57:37Z

**Summary**: We present the first comprehensive study of latent multi-head attention (MLA) for small language models, revealing interesting efficiency-quality trade-offs. Training 30M-parameter GPT models on 100,000 synthetic stories, we benchmark three architectural variants: standard multi-head attention (MHA), MLA, and MLA with rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE with half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory reduction while incurring only a 0.3% increase in validation loss (essentially matching MHA quality)- a Pareto improvement for memory constrained deployment. We further show that RoPE is crucial for MLA in small models: without it, MLA underperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by 2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2 achieves a 1.4 times speedup over full-rank MLA while maintaining the memory savings. GPT-4 evaluations corroborate perplexity results, with ours achieving the highest quality scores (7.4/10) across grammar, creativity, and consistency metrics. Code and models will be released upon acceptance.

**Link**: [arxiv](http://arxiv.org/abs/2506.09342v2),  [pdf](http://arxiv.org/pdf/2506.09342v2)

**Tags**: cs.CL cs.AI 



### BLITZSCALE: Fast and Live Large Model Autoscaling with O(1) Host Caching
**Authors**: Dingyan Zhang, Haotian Wang, Yang Liu, Xingda Wei, Yizhou Shan, Rong Chen, Haibo Chen

**Updated**: 2025-06-15T13:04:14Z

**Summary**: Model autoscaling is the key mechanism to achieve serverless model-as-a-service, but it faces a fundamental trade-off between scaling speed and storage/memory usage to cache parameters, and cannot meet frequent scaling requirements across multiple hosts. The key problem is that data plane performance is slow, and scaled instances remain stopped while parameters are loading. In this paper, we first show that the data plane can be made fast with no or O(1) caching by loading parameters through the compute network between GPUs because: (1) its speed is comparable to host cache and is underutilized, and (2) scaling multiple instances requires no or O(1) caching with network-optimized multicast. Second, autoscaling can be made live by breaking the scaling abstraction for inference from a coarse-grained instance-level to a fine-grained layer-level. This allows us to offload the layer computation from the overloaded serving instances to the scaled ones without waiting for the parameters to be fully loaded. Under real-world workloads, our system BLITZSCALE achieves up to 94 % lower tail latency reductions compared to state-of-the-art autoscaling system (ServerlessLLM), and it reduces the GPU time used for serving by 49 % when compared with serving systems that do not support autoscaling like DistServe and vLLM with the same service-level-agreement.

**Link**: [arxiv](http://arxiv.org/abs/2412.17246v2),  [pdf](http://arxiv.org/pdf/2412.17246v2)

**Tags**: cs.DC cs.OS 



### I Know What You Said: Unveiling Hardware Cache Side-Channels in Local   Large Language Model Inference
**Authors**: Zibo Gao, Junjie Hu, Feng Guo, Yixin Zhang, Yinglong Han, Siyuan Liu, Haiyang Li, Zhiqiang Lv

**Updated**: 2025-06-15T08:41:09Z

**Summary**: Large Language Models (LLMs) that can be deployed locally have recently gained popularity for privacy-sensitive tasks, with companies such as Meta, Google, and Intel playing significant roles in their development. However, the security of local LLMs through the lens of hardware cache side-channels remains unexplored. In this paper, we unveil novel side-channel vulnerabilities in local LLM inference: token value and token position leakage, which can expose both the victim's input and output text, thereby compromising user privacy. Specifically, we found that adversaries can infer the token values from the cache access patterns of the token embedding operation, and deduce the token positions from the timing of autoregressive decoding phases. To demonstrate the potential of these leaks, we design a novel eavesdropping attack framework targeting both open-source and proprietary LLM inference systems. The attack framework does not directly interact with the victim's LLM and can be executed without privilege.   We evaluate the attack on a range of practical local LLM deployments (e.g., Llama, Falcon, and Gemma), and the results show that our attack achieves promising accuracy. The restored output and input text have an average edit distance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the reconstructed texts achieve average cosine similarity scores of 98.7% (input) and 98.0% (output).

**Link**: [arxiv](http://arxiv.org/abs/2505.06738v3),  [pdf](http://arxiv.org/pdf/2505.06738v3)

**Tags**: cs.CR K.6.5 



### GTA: Grouped-head latenT Attention
**Authors**: Luoyang Sun, Jiwen Jiang, Cheng Deng, Xinjian Wu, Haifeng Zhang, Lei Chen, Lionel Ni, Jun Wang

**Updated**: 2025-06-15T07:19:33Z

**Summary**: Attention mechanisms underpin the success of large language models (LLMs), yet their substantial computational and memory overhead poses challenges for optimizing efficiency and performance. A critical bottleneck arises as KV cache and attention computations scale rapidly with text length, challenging deployment on hardware with limited computational and memory resources. We observe that attention mechanisms exhibit substantial redundancy, since the KV cache can be significantly compressed and attention maps across heads display high similarity, revealing that much of the computation and storage is unnecessary. Leveraging these insights, we propose \textbf{G}rouped-Head Laten\textbf{T} \textbf{A}ttention (GTA), a novel attention mechanism that reduces memory usage and computational complexity while maintaining performance. GTA comprises two components: (1) a shared attention map mechanism that reuses attention scores across multiple heads, decreasing the key cache size; and (2) a nonlinear value decoder with learned projections that compresses the value cache into a latent space, further cutting memory needs. GTA cuts attention computation FLOPs by up to \emph{62.5\%} versus Grouped-Query Attention and shrink the KV cache by up to \emph{70\%}, all while avoiding the extra overhead of Multi-Head Latent Attention to improve LLM deployment efficiency. Consequently, GTA models achieve a \emph{2x} increase in end-to-end inference speed, with prefill benefiting from reduced computational cost and decoding benefiting from the smaller cache footprint.

**Link**: [arxiv](http://arxiv.org/abs/2506.17286v1),  [pdf](http://arxiv.org/pdf/2506.17286v1)

**Tags**: cs.CL cs.AI 



### ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering
**Authors**: Lufei Liu, Tor M. Aamodt

**Updated**: 2025-06-14T20:17:43Z

**Summary**: Graphics rendering applications increasingly leverage neural networks in tasks such as denoising, supersampling, and frame extrapolation to improve image quality while maintaining frame rates. The temporal coherence inherent in these tasks presents an opportunity to reuse intermediate results from previous frames and avoid redundant computations. Recent work has shown that caching intermediate features to be reused in subsequent inferences is an effective method to reduce latency in diffusion models. We extend this idea to real-time rendering and present ReFrame, which explores different caching policies to optimize trade-offs between quality and performance in rendering workloads. ReFrame can be applied to a variety of encoder-decoder style networks commonly found in rendering pipelines. Experimental results show that we achieve 1.4x speedup on average with negligible quality loss in three real-time rendering tasks. Code available: https://ubc-aamodt-group.github.io/reframe-layer-caching/

**Link**: [arxiv](http://arxiv.org/abs/2506.13814v1),  [pdf](http://arxiv.org/pdf/2506.13814v1)

**Tags**: cs.GR cs.LG eess.IV 



### Real-Time Agile Software Management for Edge and Fog Computing Based   Smart City Infrastructure
**Authors**: Debasish Jana, Pinakpani Pal, Pawan Kumar

**Updated**: 2025-06-14T20:00:53Z

**Summary**: The evolution of smart cities demands scalable, secure, and energy-efficient architectures for real-time data processing. With the number of IoT devices expected to exceed 40 billion by 2030, traditional cloud-based systems are increasingly constrained by bandwidth, latency, and energy limitations. This paper leverages the ROOF (Real-time Onsite Operations Facilitation) framework with decentralized computing at intermediary fog and peripheral edge network layers to reduce latency by processing data near its point of origin. ROOF features fog caching to avoid redundancy, ultra-low-power wireless transmission for energy savings, and AI-driven resource allocation for efficiency. Security is enhanced through TLS encryption, blockchain-based authentication, and edge-level access control. Case studies from Bhubaneswar, Barcelona and Copenhagen validate the use of ROOF in traffic systems and environmental monitoring. The paper concludes by outlining key challenges and prospects of AI-driven analytics in smart urban infrastructure.

**Link**: [arxiv](http://arxiv.org/abs/2506.12616v1),  [pdf](http://arxiv.org/pdf/2506.12616v1)

**Tags**: cs.SE 



### FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented   Generation
**Authors**: Zhuocheng Zhang, Yang Feng, Min Zhang

**Updated**: 2025-06-14T13:16:31Z

**Summary**: Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large language model applications, with numerous existing frameworks offering a wide range of functionalities to facilitate the development of RAG systems. However, we have identified several persistent challenges in these frameworks, including difficulties in algorithm reproduction and sharing, lack of new techniques, and high system overhead. To address these limitations, we introduce \textbf{FlexRAG}, an open-source framework specifically designed for research and prototyping. FlexRAG supports text-based, multimodal, and network-based RAG, providing comprehensive lifecycle support alongside efficient asynchronous processing and persistent caching capabilities. By offering a robust and flexible solution, FlexRAG enables researchers to rapidly develop, deploy, and share advanced RAG systems. Our toolkit and resources are available at \href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.

**Link**: [arxiv](http://arxiv.org/abs/2506.12494v1),  [pdf](http://arxiv.org/pdf/2506.12494v1)

**Tags**: cs.CL cs.IR 



### Efficient Unified Caching for Accelerating Heterogeneous AI Workloads
**Authors**: Tianze Wang, Yifei Liu, Chen Chen, Pengfei Zuo, Jiawei Zhang, Qizhen Weng, Yin Chen, Zhenhua Han, Jieru Zhao, Quan Chen, Minyi Guo

**Updated**: 2025-06-14T06:36:54Z

**Summary**: Modern AI clusters, which host diverse workloads like data pre-processing, training and inference, often store the large-volume data in cloud storage and employ caching frameworks to facilitate remote data access. To avoid code-intrusion complexity and minimize cache space wastage, it is desirable to maintain a unified cache shared by all the workloads. However, existing cache management strategies, designed for specific workloads, struggle to handle the heterogeneous AI workloads in a cluster -- which usually exhibit heterogeneous access patterns and item storage granularities. In this paper, we propose IGTCache, a unified, high-efficacy cache for modern AI clusters. IGTCache leverages a hierarchical access abstraction, AccessStreamTree, to organize the recent data accesses in a tree structure, facilitating access pattern detection at various granularities. Using this abstraction, IGTCache applies hypothesis testing to categorize data access patterns as sequential, random, or skewed. Based on these detected access patterns and granularities, IGTCache tailors optimal cache management strategies including prefetching, eviction, and space allocation accordingly. Experimental results show that IGTCache increases the cache hit ratio by 55.6% over state-of-the-art caching frameworks, reducing the overall job completion time by 52.2%.

**Link**: [arxiv](http://arxiv.org/abs/2506.12370v1),  [pdf](http://arxiv.org/pdf/2506.12370v1)

**Tags**: cs.DC cs.LG 



### ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable   Compression
**Authors**: Guangda Liu, Chengwei Li, Jieru Zhao, Chenqi Zhang, Minyi Guo

**Updated**: 2025-06-14T06:17:33Z

**Summary**: Large Language Models (LLMs) have been widely deployed in a variety of applications, and the context length is rapidly increasing to handle tasks such as long-document QA and complex logical reasoning. However, long context poses significant challenges for inference efficiency, including high memory costs of key-value (KV) cache and increased latency due to extensive memory accesses. Recent works have proposed compressing KV cache to approximate computation, but these methods either evict tokens permanently, never recalling them for later inference, or recall previous tokens at the granularity of pages divided by textual positions. Both approaches degrade the model accuracy and output quality. To achieve efficient and accurate recallable KV cache compression, we introduce ClusterKV, which recalls tokens at the granularity of semantic clusters. We design and implement efficient algorithms and systems for clustering, selection, indexing and caching. Experiment results show that ClusterKV attains negligible accuracy loss across various tasks with 32k context lengths, using only a 1k to 2k KV cache budget, and achieves up to a 2$\times$ speedup in latency and a 2.5$\times$ improvement in decoding throughput. Compared to SoTA recallable KV compression methods, ClusterKV demonstrates higher model accuracy and output quality, while maintaining or exceeding inference efficiency. Our code is available at https://github.com/sjtu-zhao-lab/ClusterKV.

**Link**: [arxiv](http://arxiv.org/abs/2412.03213v2),  [pdf](http://arxiv.org/pdf/2412.03213v2)

**Tags**: cs.LG cs.AI cs.PF 



### Federated Learning Assisted Edge Caching Scheme Based on Lightweight   Architecture DDPM
**Authors**: Xun Li, Qiong Wu, Pingyi Fan, Kezhi Wang, Nan Cheng, Khaled B. Letaief

**Updated**: 2025-06-14T00:52:10Z

**Summary**: Edge caching is an emerging technology that empowers caching units at edge nodes, allowing users to fetch contents of interest that have been pre-cached at the edge nodes. The key to pre-caching is to maximize the cache hit percentage for cached content without compromising users' privacy. In this letter, we propose a federated learning (FL) assisted edge caching scheme based on lightweight architecture denoising diffusion probabilistic model (LDPM). Our simulation results verify that our proposed scheme achieves a higher cache hit percentage compared to existing FL-based methods and baseline methods.

**Link**: [arxiv](http://arxiv.org/abs/2506.04593v3),  [pdf](http://arxiv.org/pdf/2506.04593v3)

**Tags**: cs.NI eess.SP 



### R-KV: Redundancy-aware KV Cache Compression for Reasoning Models
**Authors**: Zefan Cai, Wen Xiao, Hanshi Sun, Cheng Luo, Yikai Zhang, Ke Wan, Yucheng Li, Yeyang Zhou, Li-Wen Chang, Jiuxiang Gu, Zhen Dong, Anima Anandkumar, Abedelkadir Asi, Junjie Hu

**Updated**: 2025-06-13T21:01:43Z

**Summary**: Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning. However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference. While chain-of-thought inference significantly improves performance on complex reasoning tasks, it can also lead to reasoning failures when deployed with existing KV cache compression approaches. To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models. Our method preserves nearly 100% of the full KV cache performance using only 10% of the KV cache, substantially outperforming existing KV cache baselines, which reach only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV cache performance with 16% of the KV cache. This KV-cache reduction also leads to a 90% memory saving and a 6.6X throughput over standard chain-of-thought reasoning inference. Experimental results show that R-KV consistently outperforms existing KV cache compression baselines across two mathematical reasoning datasets.

**Link**: [arxiv](http://arxiv.org/abs/2505.24133v3),  [pdf](http://arxiv.org/pdf/2505.24133v3)

**Tags**: cs.CL cs.AI 



### Cartridges: Lightweight and general-purpose long context representations   via self-study
**Authors**: Sabri Eyuboglu, Ryan Ehrlich, Simran Arora, Neel Guha, Dylan Zinsley, Emily Liu, Will Tennien, Atri Rudra, James Zou, Azalia Mirhoseini, Christopher Re

**Updated**: 2025-06-13T17:58:55Z

**Summary**: Large language models are often used to answer queries grounded in large text corpora (e.g. codebases, legal documents, or chat histories) by placing the entire corpus in the context window and leveraging in-context learning (ICL). Although current models support contexts of 100K-1M tokens, this setup is costly to serve because the memory consumption of the KV cache scales with input length. We explore an alternative: training a smaller KV cache offline on each corpus. At inference time, we load this trained KV cache, which we call a Cartridge, and decode a response. Critically, the cost of training a Cartridge can be amortized across all the queries referencing the same corpus. However, we find that the naive approach of training the Cartridge with next-token prediction on the corpus is not competitive with ICL. Instead, we propose self-study, a training recipe in which we generate synthetic conversations about the corpus and train the Cartridge with a context-distillation objective. We find that Cartridges trained with self-study replicate the functionality of ICL, while being significantly cheaper to serve. On challenging long-context benchmarks, Cartridges trained with self-study match ICL performance while using 38.6x less memory and enabling 26.4x higher throughput. Self-study also extends the model's effective context length (e.g. from 128k to 484k tokens on MTOB) and surprisingly, leads to Cartridges that can be composed at inference time without retraining.

**Link**: [arxiv](http://arxiv.org/abs/2506.06266v3),  [pdf](http://arxiv.org/pdf/2506.06266v3)

**Tags**: cs.CL cs.AI cs.LG 



### CnC-PRAC: Coalesce, not Cache, Per Row Activation Counts for an   Efficient in-DRAM Rowhammer Mitigation
**Authors**: Chris S. Lin, Jeonghyun Woo, Prashant J. Nair, Gururaj Saileshwar

**Updated**: 2025-06-13T17:28:38Z

**Summary**: JEDEC has introduced the Per Row Activation Counting (PRAC) framework for DDR5 and future DRAMs to enable precise counting of DRAM row activations using per-row activation counts. While recent PRAC implementations enable holistic mitigation of Rowhammer attacks, they impose slowdowns of up to 10% due to the increased DRAM timings for performing a read-modify-write of the counter. Alternatively, recent work, Chronus, addresses these slowdowns, but incurs energy overheads due to the additional DRAM activations for counters. In this paper, we propose CnC-PRAC, a PRAC implementation that addresses both performance and energy overheads. Unlike prior works focusing on caching activation counts to reduce their overheads, our key idea is to reorder and coalesce accesses to activation counts located in the same physical row. Our design achieves this by decoupling counter access from the critical path of data accesses. This enables optimizations such as buffering counter read-modify-write requests and coalescing requests to the same row. Together, these enable a reduction in row activations for counter accesses by almost 75%-83% compared to state-of-the-art solutions like Chronus and enable a PRAC implementation with negligible slowdown and a minimal dynamic energy overhead of 0.84%-1% compared to insecure DDR5 DRAM.

**Link**: [arxiv](http://arxiv.org/abs/2506.11970v1),  [pdf](http://arxiv.org/pdf/2506.11970v1)

**Tags**: cs.CR 



### Beyond Homogeneous Attention: Memory-Efficient LLMs via   Fourier-Approximated KV Cache
**Authors**: Xiaoran Liu, Siyang He, Qiqi Wang, Ruixiao Li, Yuerong Song, Zhigeng Liu, Linlin Li, Qun Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu

**Updated**: 2025-06-13T15:35:54Z

**Summary**: Large Language Models struggle with memory demands from the growing Key-Value (KV) cache as context lengths increase. Existing compression methods homogenize head dimensions or rely on attention-guided token pruning, often sacrificing accuracy or introducing computational overhead. We propose FourierAttention, a training-free framework that exploits the heterogeneous roles of transformer head dimensions: lower dimensions prioritize local context, while upper ones capture long-range dependencies. By projecting the long-context-insensitive dimensions onto orthogonal Fourier bases, FourierAttention approximates their temporal evolution with fixed-length spectral coefficients. Evaluations on LLaMA models show that FourierAttention achieves the best long-context accuracy on LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel, FlashFourierAttention, is designed to optimize memory via streamlined read-write operations, enabling efficient deployment without performance compromise.

**Link**: [arxiv](http://arxiv.org/abs/2506.11886v1),  [pdf](http://arxiv.org/pdf/2506.11886v1)

**Tags**: cs.CL 



### FlashBack:Efficient Retrieval-Augmented Language Modeling for Long   Context Inference
**Authors**: Runheng Liu, Xingchen Xiao, Heyan Huang, Zewen Chi, Zhijing Wu

**Updated**: 2025-06-13T08:32:26Z

**Summary**: Retrieval-Augmented Language Modeling (RALM) by integrating large language models (LLM) with relevant documents from an external corpus is a proven method for enabling the LLM to generate information beyond the scope of its pre-training corpus. Previous work utilizing retrieved content by simply prepending it to the input poses a high runtime issue, which degrades the inference efficiency of the LLMs because they fail to use the Key-Value (KV) cache efficiently. In this paper, we propose FlashBack, a modular RALM designed to improve the inference efficiency of RALM with appending context pattern while maintaining decent performance after fine-tuning by Low-Rank Adaption. FlashBack appends retrieved documents at the end of the context for efficiently utilizing the KV cache instead of prepending them. And we introduce Marking Token as two special prompt tokens for marking the boundary of the appending context during fine-tuning. Our experiments on testing generation quality show that FlashBack can remain decent generation quality in perplexity. And the inference speed of FlashBack is up to $4\times$ faster than the prepending counterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing unnecessary re-computation, it demonstrates an advancement by achieving significantly faster inference speed, and this heightened efficiency will substantially reduce inferential cost.

**Link**: [arxiv](http://arxiv.org/abs/2405.04065v4),  [pdf](http://arxiv.org/pdf/2405.04065v4)

**Tags**: cs.CL 



### MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based   QA Datasets
**Authors**: Mahir Labib Dihan, Mohammed Eunus Ali, Md Rizwan Parvez

**Updated**: 2025-06-13T07:04:46Z

**Summary**: Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap, are essential for accessing various location-based data, yet they often struggle to handle natural language geospatial queries. Recent advancements in Large Language Models (LLMs) show promise in question answering (QA), but creating reliable geospatial QA datasets from map services remains challenging. We introduce MapQaTor, an extensible open-source framework that streamlines the creation of reproducible, traceable map-based QA datasets. MapQaTor enables seamless integration with any maps API, allowing users to gather and visualize data from diverse sources with minimal setup. By caching API responses, the platform ensures consistent ground truth, enhancing the reliability of the data even as real-world information evolves. MapQaTor centralizes data retrieval, annotation, and visualization within a single platform, offering a unique opportunity to evaluate the current state of LLM-based geospatial reasoning while advancing their capabilities for improved geospatial understanding. Evaluation metrics show that, MapQaTor speeds up the annotation process by at least 30 times compared to manual methods, underscoring its potential for developing geospatial resources, such as complex map reasoning datasets. The website is live at: https://mapqator.github.io/ and a demo video is available at: https://youtu.be/bVv7-NYRsTw.

**Link**: [arxiv](http://arxiv.org/abs/2412.21015v2),  [pdf](http://arxiv.org/pdf/2412.21015v2)

**Tags**: cs.CL cs.HC 



### Lag-Relative Sparse Attention In Long Context Training
**Authors**: Manlai Liang, Wanyi Huang, Mandi Liu, Huaijun Li, Jinlong Li

**Updated**: 2025-06-13T06:49:53Z

**Summary**: Large Language Models (LLMs) have made significant strides in natural language processing and generation, yet their ability to handle long-context input remains constrained by the quadratic complexity of attention computation and linear-increasing key-value memory footprint. To reduce computational costs and memory, key-value cache compression techniques are commonly applied at inference time, but this often leads to severe performance degradation, as models are not trained to handle compressed context. Although there are more sophisticated compression methods, they are typically unsuitable for post-training because of their incompatibility with gradient-based optimization or high computation overhead. To fill this gap with no additional parameter and little computation overhead, we propose Lag-Relative Sparse Attention(LRSA) anchored by the LagKV compression method for long context post-training. Our method performs chunk-by-chunk prefilling, which selects the top K most relevant key-value pairs in a fixed-size lagging window, allowing the model to focus on salient historical context while maintaining efficiency. Experimental results show that our approach significantly enhances the robustness of the LLM with key-value compression and achieves better fine-tuned results in the question-answer tuning task.

**Link**: [arxiv](http://arxiv.org/abs/2506.11498v1),  [pdf](http://arxiv.org/pdf/2506.11498v1)

**Tags**: cs.CL 



### Electric field control of third-order nonlinear Hall effect
**Authors**: Jiaju Yang, Lujun Wei, Yanghui Li, Lina Chen, Wei Niu, Jiarui Chen, Jun Du, Yong Pu

**Updated**: 2025-06-13T02:54:42Z

**Summary**: The third-order nonlinear Hall effect (NLHE) serves as a sensitive probe of energy band geometric property, providing a new paradigm for revealing the Berry curvature distribution and topological response of quantum materials. In the Weyl semimetal TaIrTe4, we report for the first time that the sign of the third-order NLHE reverses with decreasing temperature. Through scaling law analysis, we think that the third-order NLHE at high (T > 23 K) and low (T < 23 K) temperatures is dominated by Berry-connection polarizability (BCP) and impurity scattering, respectively. The third-order NLHE response strength can be effectively modulated by an additional applied in-plane constant electric field. At the high temperature region, the BCP reduction induced by the electric field leads to a decrease in the third-order NLHE response strength, while at the low temperature region, the electric field cause both BCP and impurity scattering effects to weaken, resulting in a more significant modulation of the third-order NLHE response strength. At 4 K and an electric field strength of 0.3 kV/cm, the modulated relative response strength could reach up to 65.3%. This work provides a new means to explore the third-order NLHE and a valuable reference for the development of novel electronic devices.

**Link**: [arxiv](http://arxiv.org/abs/2506.10657v2),  [pdf](http://arxiv.org/pdf/2506.10657v2)

**Tags**: cond-mat.mes-hall cond-mat.mtrl-sci 



### Efficient Long-Context LLM Inference via KV Cache Clustering
**Authors**: Jie Hu, Shengnan Wang, Yutong He, Ping Gong, Jiawei Yi, Juncheng Zhang, Youhui Bai, Renhai Chen, Gong Zhang, Cheng Li, Kun Yuan

**Updated**: 2025-06-13T02:36:15Z

**Summary**: Large language models (LLMs) with extended context windows have become increasingly prevalent for tackling complex tasks. However, the substantial Key-Value (KV) cache required for long-context LLMs poses significant deployment challenges. Existing approaches either discard potentially critical information needed for future generations or offer limited efficiency gains due to high computational overhead. In this paper, we introduce Chelsea, a simple yet effective framework for online KV cache clustering. Our approach is based on the observation that key states exhibit high similarity along the sequence dimension. To enable efficient clustering, we divide the sequence into chunks and propose Chunked Soft Matching, which employs an alternating partition strategy within each chunk and identifies clusters based on similarity. Chelsea then merges the KV cache within each cluster into a single centroid. Additionally, we provide a theoretical analysis of the computational complexity and the optimality of the intra-chunk partitioning strategy. Extensive experiments across various models and long-context benchmarks demonstrate that Chelsea achieves up to 80% reduction in KV cache memory usage while maintaining comparable model performance. Moreover, with minimal computational overhead, Chelsea accelerates the decoding stage of inference by up to 3.19$\times$ and reduces end-to-end latency by up to 2.72$\times$.

**Link**: [arxiv](http://arxiv.org/abs/2506.11418v1),  [pdf](http://arxiv.org/pdf/2506.11418v1)

**Tags**: cs.CL 



### Accelerating Diffusion Large Language Models with SlowFast Sampling: The   Three Golden Principles
**Authors**: Qingyan Wei, Yaojie Zhang, Zhiyuan Liu, Dongrui Liu, Linfeng Zhang

**Updated**: 2025-06-13T02:28:47Z

**Summary**: Diffusion-based language models (dLLMs) have emerged as a promising alternative to traditional autoregressive LLMs by enabling parallel token generation and significantly reducing inference latency. However, existing sampling strategies for dLLMs, such as confidence-based or semi-autoregressive decoding, often suffer from static behavior, leading to suboptimal efficiency and limited flexibility. In this paper, we propose SlowFast Sampling, a novel dynamic sampling strategy that adaptively alternates between exploratory and accelerated decoding stages. Our method is guided by three golden principles: certainty principle, convergence principle, and positional principle, which govern when and where tokens can be confidently and efficiently decoded. We further integrate our strategy with dLLM-Cache to reduce redundant computation. Extensive experiments across benchmarks and models show that SlowFast Sampling achieves up to 15.63$\times$ speedup on LLaDA with minimal accuracy drop, and up to 34.22$\times$ when combined with caching. Notably, our approach outperforms strong autoregressive baselines like LLaMA3 8B in throughput, demonstrating that well-designed sampling can unlock the full potential of dLLMs for fast and high-quality generation.

**Link**: [arxiv](http://arxiv.org/abs/2506.10848v2),  [pdf](http://arxiv.org/pdf/2506.10848v2)

**Tags**: cs.CL cs.AI cs.LG 



### A4: Microarchitecture-Aware LLC Management for Datacenter Servers with   Emerging I/O Devices
**Authors**: Haneul Park, Jiaqi Lou, Sangjin Lee, Yifan Yuan, Kyoung Soo Park, Yongseok Son, Ipoom Jeong, Nam Sung Kim

**Updated**: 2025-06-12T21:57:27Z

**Summary**: In modern server CPUs, the Last-Level Cache (LLC) serves not only as a victim cache for higher-level private caches but also as a buffer for low-latency DMA transfers between CPU cores and I/O devices through Direct Cache Access (DCA). However, prior work has shown that high-bandwidth network-I/O devices can rapidly flood the LLC with packets, often causing significant contention with co-running workloads. One step further, this work explores hidden microarchitectural properties of the Intel Xeon CPUs, uncovering two previously unrecognized LLC contentions triggered by emerging high-bandwidth I/O devices. Specifically, (C1) DMA-written cache lines in LLC ways designated for DCA (referred to as DCA ways) are migrated to certain LLC ways (denoted as inclusive ways) when accessed by CPU cores, unexpectedly contending with non-I/O cache lines within the inclusive ways. In addition, (C2) high-bandwidth storage-I/O devices, which are increasingly common in datacenter servers, benefit little from DCA while contending with (latency-sensitive) network-I/O devices within DCA ways. To this end, we present \design, a runtime LLC management framework designed to alleviate both (C1) and (C2) among diverse co-running workloads, using a hidden knob and other hardware features implemented in those CPUs. Additionally, we demonstrate that \design can also alleviate other previously known network-I/O-driven LLC contentions. Overall, it improves the performance of latency-sensitive, high-priority workloads by 51\% without notably compromising that of low-priority workloads.

**Link**: [arxiv](http://arxiv.org/abs/2506.11329v1),  [pdf](http://arxiv.org/pdf/2506.11329v1)

**Tags**: cs.AR 



### SwiftSpec: Ultra-Low Latency LLM Decoding by Scaling Asynchronous   Speculative Decoding
**Authors**: Ziyi Zhang, Ziheng Jiang, Chengquan Jiang, Menghan Yu, Size Zheng, Haibin Lin, Henry Hoffmann, Xin Liu

**Updated**: 2025-06-12T21:15:58Z

**Summary**: Low-latency decoding for large language models (LLMs) is crucial for applications like chatbots and code assistants, yet generating long outputs remains slow in single-query settings. Prior work on speculative decoding (which combines a small draft model with a larger target model) and tensor parallelism has each accelerated decoding. However, conventional approaches fail to apply both simultaneously due to imbalanced compute requirements (between draft and target models), KV-cache inconsistencies, and communication overheads under small-batch tensor-parallelism. This paper introduces SwiftSpec, a system that targets ultra-low latency for LLM decoding. SwiftSpec redesigns the speculative decoding pipeline in an asynchronous and disaggregated manner, so that each component can be scaled flexibly and remove draft overhead from the critical path. To realize this design, SwiftSpec proposes parallel tree generation, tree-aware KV cache management, and fused, latency-optimized kernels to overcome the challenges listed above. Across 5 model families and 6 datasets, SwiftSpec achieves an average of 1.75x speedup over state-of-the-art speculative decoding systems and, as a highlight, serves Llama3-70B at 348 tokens/s on 8 Nvidia Hopper GPUs, making it the fastest known system for low-latency LLM serving at this scale.

**Link**: [arxiv](http://arxiv.org/abs/2506.11309v1),  [pdf](http://arxiv.org/pdf/2506.11309v1)

**Tags**: cs.DC cs.LG 



### Revisiting Main Memory-Based Covert and Side Channel Attacks in the   Context of Processing-in-Memory
**Authors**: F. Nisa Bostanci, Konstantinos Kanellopoulos, Ataberk Olgun, A. Giray Yaglikci, Ismail Emir Yuksel, Nika Mansouri Ghiasi, Zulal Bingol, Mohammad Sadrosadati, Onur Mutlu

**Updated**: 2025-06-12T20:38:42Z

**Summary**: We introduce IMPACT, a set of high-throughput main memory-based timing attacks that leverage characteristics of processing-in-memory (PiM) architectures to establish covert and side channels. IMPACT enables high-throughput communication and private information leakage by exploiting the shared DRAM row buffer. To achieve high throughput, IMPACT (i) eliminates expensive cache bypassing steps required by processor-centric memory-based timing attacks and (ii) leverages the intrinsic parallelism of PiM operations. We showcase two applications of IMPACT. First, we build two covert channels that leverage different PiM approaches (i.e., processing-near-memory and processing-using-memory) to establish high-throughput covert communication channels. Our covert channels achieve 8.2 Mb/s and 14.8 Mb/s communication throughput, respectively, which is 3.6x and 6.5x higher than the state-of-the-art main memory-based covert channel. Second, we showcase a side-channel attack that leaks private information of concurrently-running victim applications with a low error rate. Our source-code is openly and freely available at https://github.com/CMU-SAFARI/IMPACT.

**Link**: [arxiv](http://arxiv.org/abs/2404.11284v4),  [pdf](http://arxiv.org/pdf/2404.11284v4)

**Tags**: cs.CR cs.AR 



### SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language   Models
**Authors**: Kaushal Kumar Maurya, KV Aditya Srivatsa, Ekaterina Kochmar

**Updated**: 2025-06-12T13:33:52Z

**Summary**: Large language models (LLMs) have been widely adopted due to their remarkable performance across various applications, driving the accelerated development of a large number of diverse models. However, these individual LLMs show limitations in generalization and performance on complex tasks due to inherent training biases, model size constraints, and the quality or diversity of pre-training datasets. A promising direction is to efficiently harness the diverse capabilities of LLMs to overcome these individual limitations. To address these limitations, we introduce a novel LLM selection algorithm called SelectLLM, which efficiently directs input queries to the most suitable subset of LLMs from a large pool, ensuring that the selected models collectively provide accurate responses. SelectLLM employs a multi-label classifier and policy based on the classifier's predictions and confidence scores in selecting an optimal, query-aware, and lightweight subset of LLMs. Our findings indicate that the proposed model outperforms existing ensemble-based baselines and achieves competitive performance with similarly sized top-performing LLMs while maintaining efficiency. Specifically, it achieves a huge reduction in inference latency on two challenging reasoning benchmarks: 13\% on GSM8K and 70\% on MMLU, compared to the top-performing baseline. Also, we establish a theoretical upper bound by an Oracle with LLMs and perform an in-depth linguistic analysis to understand the performance gap between the Oracle and SelectLLM.

**Link**: [arxiv](http://arxiv.org/abs/2408.08545v4),  [pdf](http://arxiv.org/pdf/2408.08545v4)

**Tags**: cs.CL 



### TransMLA: Multi-Head Latent Attention Is All You Need
**Authors**: Fanxu Meng, Pingzhi Tang, Xiaojuan Tang, Zengwei Yao, Xing Sun, Muhan Zhang

**Updated**: 2025-06-12T11:45:57Z

**Summary**: In this paper, we present TransMLA, a framework that seamlessly converts any GQA-based pre-trained model into an MLA-based model. Our approach enables direct compatibility with DeepSeek's codebase, allowing these models to fully leverage DeepSeek-specific optimizations such as vLLM and SGlang. By compressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x inference speedup at an 8K context length while preserving meaningful output quality. Additionally, the model requires only 6 billion tokens for fine-tuning to regain performance on par with the original across multiple benchmarks. TransMLA offers a practical solution for migrating GQA-based models to the MLA structure. When combined with DeepSeek's advanced features, such as FP8 quantization and Multi-Token Prediction, even greater inference acceleration can be realized.

**Link**: [arxiv](http://arxiv.org/abs/2502.07864v5),  [pdf](http://arxiv.org/pdf/2502.07864v5)

**Tags**: cs.LG cs.AI 



### VSAG: An Optimized Search Framework for Graph-based Approximate Nearest   Neighbor Search
**Authors**: Xiaoyao Zhong, Haotian Li, Jiabao Jin, Mingyu Yang, Deming Chu, Xiangyu Wang, Zhitao Shen, Wei Jia, George Gu, Yi Xie, Xuemin Lin, Heng Tao Shen, Jingkuan Song, Peng Cheng

**Updated**: 2025-06-12T11:26:10Z

**Summary**: Approximate nearest neighbor search (ANNS) is a fundamental problem in vector databases and AI infrastructures. Recent graph-based ANNS algorithms have achieved high search accuracy with practical efficiency. Despite the advancements, these algorithms still face performance bottlenecks in production, due to the random memory access patterns of graph-based search and the high computational overheads of vector distance. In addition, the performance of a graph-based ANNS algorithm is highly sensitive to parameters, while selecting the optimal parameters is cost-prohibitive, e.g., manual tuning requires repeatedly re-building the index.   This paper introduces VSAG, an open-source framework that aims to enhance the in production performance of graph-based ANNS algorithms. VSAG has been deployed at scale in the services of Ant Group, and it incorporates three key optimizations: (i) efficient memory access: it reduces L3 cache misses with pre-fetching and cache-friendly vector organization; (ii) automated parameter tuning: it automatically selects performance-optimal parameters without requiring index rebuilding; (iii) efficient distance computation: it leverages modern hardware, scalar quantization, and smartly switches to low-precision representation to dramatically reduce the distance computation costs. We evaluate VSAG on real-world datasets. The experimental results show that VSAG achieves the state-of-the-art performance and provides up to 4x speedup over HNSWlib (an industry-standard library) while ensuring the same accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2503.17911v2),  [pdf](http://arxiv.org/pdf/2503.17911v2)

**Tags**: cs.DB 



### Qronos: Correcting the Past by Shaping the Future... in Post-Training   Quantization
**Authors**: Shihao Zhang, Haoyu Zhang, Ian Colbert, Rayan Saab

**Updated**: 2025-06-12T00:25:14Z

**Summary**: We introduce Qronos -- a new state-of-the-art post-training quantization algorithm that sequentially rounds and updates neural network weights. Qronos not only explicitly corrects errors due to both weight and activation quantization, but also errors resulting from quantizing previous layers. Our iterative algorithm is based on an interpretable and disciplined optimization framework that subsumes and surpasses existing data-driven approaches. At each step, Qronos alternates between error correction and diffusion via optimal update rules. Importantly, we prove that Qronos admits an efficient implementation that uses the Cholesky decomposition for solving least-squares problems. We also demonstrate that Qronos is compatible with existing transformation techniques such as Hadamard-based incoherence processing and weight-activation scaling equalization, among others. We evaluate Qronos using recent autoregressive language generation models in the Llama3 family; Qronos consistently outperforms previous state-of-the-art adaptive rounding methods when quantizing the weights, activations, and/or KV caches.

**Link**: [arxiv](http://arxiv.org/abs/2505.11695v2),  [pdf](http://arxiv.org/pdf/2505.11695v2)

**Tags**: cs.LG cs.AI math.OC 



### Squeezed Attention: Accelerating Long Context Length LLM Inference
**Authors**: Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Monishwaran Maheswaran, Sebastian Zhao, June Paik, Michael W. Mahoney, Kurt Keutzer, Amir Gholami

**Updated**: 2025-06-11T22:50:44Z

**Summary**: Emerging Large Language Model (LLM) applications require long input context in order to perform complex tasks like document analysis and code generation. For these long context length applications, the length of the input prompt poses a significant challenge in terms of inference efficiency since the inference costs increase linearly with sequence length. However, for many of these applications, much of the context in the prompt is fixed across different user inputs, thereby providing the opportunity to perform offline optimizations in order to process user inputs quickly, as they are received. We propose Squeezed Attention to accelerate LLM applications where a large portion of the input context is fixed. We first leverage K-means clustering offline to group the keys for the fixed context based on semantic similarity and represent each cluster with a single centroid value. During inference, we compare query tokens from the user input with the centroids to predict which keys from the fixed context are semantically relevant, and then compute exact attention using only the important keys, thereby reducing bandwidth and computational costs. We also present a hierarchical version of our algorithm which can reduce the complexity of attention from linear to logarithmic with respect to the fixed context length. We evaluate our method on long-context benchmarks including LongBench, where it achieves a 3.1$\times$ reduction in KV budget with no noticeable accuracy loss and up to an 8$\times$ reduction with only a 0.5 point accuracy gap for the LLaMA-2-7B-32K, LWM-Text-Chat-1M, and Longchat-7B-v1.5-32K models. Futhermore, we implement kernels for centroid comparison and sparse FlashAttention with important keys, achieving more than 4$\times$ speedups during both the prefill and generation phases for long-context inference. Our code is available at https://github.com/SqueezeAILab/SqueezedAttention.

**Link**: [arxiv](http://arxiv.org/abs/2411.09688v3),  [pdf](http://arxiv.org/pdf/2411.09688v3)

**Tags**: cs.CL 



### ETS: Efficient Tree Search for Inference-Time Scaling
**Authors**: Coleman Hooper, Sehoon Kim, Suhong Moon, Kerem Dilmen, Monishwaran Maheswaran, Nicholas Lee, Michael W. Mahoney, Sophia Shao, Kurt Keutzer, Amir Gholami

**Updated**: 2025-06-11T21:59:20Z

**Summary**: Test-time compute scaling has emerged as a new axis along which to improve model accuracy, where additional computation is used at inference time to allow the model to think longer for more challenging problems. One promising approach for test-time compute scaling is search against a process reward model, where a model generates multiple potential candidates at each step of the search, and these partial trajectories are then scored by a separate reward model in order to guide the search process. The diversity of trajectories in the tree search process affects the accuracy of the search, since increasing diversity promotes more exploration. However, this diversity comes at a cost, as divergent trajectories have less KV sharing, which means they consume more memory and slow down the search process. Previous search methods either do not perform sufficient exploration, or else explore diverse trajectories but have high latency. We address this challenge by proposing Efficient Tree Search (ETS), which promotes KV sharing by pruning redundant trajectories while maintaining necessary diverse trajectories. ETS incorporates a linear programming cost model to promote KV cache sharing by penalizing the number of nodes retained, while incorporating a semantic coverage term into the cost model to ensure that we retain trajectories which are semantically different. We demonstrate how ETS can achieve 1.8$\times$ reduction in average KV cache size during the search process, leading to 1.4$\times$ increased throughput relative to prior state-of-the-art methods, with minimal accuracy degradation and without requiring any custom kernel implementation. Code is available at: https://github.com/SqueezeAILab/ETS.

**Link**: [arxiv](http://arxiv.org/abs/2502.13575v2),  [pdf](http://arxiv.org/pdf/2502.13575v2)

**Tags**: cs.LG 



### EfficientVLA: Training-Free Acceleration and Compression for   Vision-Language-Action Models
**Authors**: Yantai Yang, Yuhao Wang, Zichen Wen, Luo Zhongwei, Chang Zou, Zhipeng Zhang, Chuan Wen, Linfeng Zhang

**Updated**: 2025-06-11T18:34:57Z

**Summary**: Vision-Language-Action (VLA) models, particularly diffusion-based architectures, demonstrate transformative potential for embodied intelligence but are severely hampered by high computational and memory demands stemming from extensive inherent and inference-time redundancies. While existing acceleration efforts often target isolated inefficiencies, such piecemeal solutions typically fail to holistically address the varied computational and memory bottlenecks across the entire VLA pipeline, thereby limiting practical deployability. We introduce EfficientVLA, a structured and training-free inference acceleration framework that systematically eliminates these barriers by cohesively exploiting multifaceted redundancies. EfficientVLA synergistically integrates three targeted strategies: (1) pruning of functionally inconsequential layers from the language module, guided by an analysis of inter-layer redundancies; (2) optimizing the visual processing pathway through a task-aware strategy that selects a compact, diverse set of visual tokens, balancing task-criticality with informational coverage; and (3) alleviating temporal computational redundancy within the iterative diffusion-based action head by strategically caching and reusing key intermediate features. We apply our method to a standard VLA model CogACT, yielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6% success rate drop in the SIMPLER benchmark.

**Link**: [arxiv](http://arxiv.org/abs/2506.10100v1),  [pdf](http://arxiv.org/pdf/2506.10100v1)

**Tags**: cs.CV 



### Mainframe-style channel controllers for modern disaggregated memory   systems
**Authors**: Zikai Liu, Jasmin Schult, Pengcheng Xu, Timothy Roscoe

**Updated**: 2025-06-11T14:03:13Z

**Summary**: Despite the promise of alleviating the main memory bottleneck, and the existence of commercial hardware implementations, techniques for Near-Data Processing have seen relatively little real-world deployment. The idea has received renewed interest with the appearance of disaggregated or "far" memory, for example in the use of CXL memory pools.   However, we argue that the lack of a clear OS-centric abstraction of Near-Data Processing is a major barrier to adoption of the technology. Inspired by the channel controllers which interface the CPU to disk drives in mainframe systems, we propose memory channel controllers as a convenient, portable, and virtualizable abstraction of Near-Data Processing for modern disaggregated memory systems.   In addition to providing a clean abstraction that enables OS integration while requiring no changes to CPU architecture, memory channel controllers incorporate another key innovation: they exploit the cache coherence provided by emerging interconnects to provide a much richer programming model, with more fine-grained interaction, than has been possible with existing designs.

**Link**: [arxiv](http://arxiv.org/abs/2506.09758v1),  [pdf](http://arxiv.org/pdf/2506.09758v1)

**Tags**: cs.OS cs.AR cs.ET 



### Commissioning, characterization and first high dose rate irradiations at   a compact X-ray tube for microbeam and minibeam radiation therapy
**Authors**: Christian Petrich, Johanna Winter, Anton Dimroth, Thomas Beiser, Monika Dehn, Jessica Stolz, Jacopo Frignani, Stephanie E. Combs, Franz Schilling, Ghaleb Natour, Kurt Aulenbacher, Thomas E. Schmid, Jan J. Wilkens, Stefan Bartzsch

**Updated**: 2025-06-11T09:08:59Z

**Summary**: Minibeam and microbeam radiation therapy promise improved treatment outcomes through reduced normal tissue toxicity at better tumor control rates. The lack of suitable compact radiation sources limits the clinical application of minibeams to superficial tumors and renders it impossible for microbeams. We developed and constructed the first prototype of a compact line-focus X-ray tube (LFXT) with technology potentially suitable for clinical translation of minibeams and microbeams. We give an overview of the commissioning process preceding the first operation, present optical and radiological focal spot characterization methods, and dosimetric measurements. Additionally, we report on first preclinical in vitro cell and in vivo mouse brain irradiations conducted with the LFXT prototype. The focal spot characterization resulted in a strongly eccentric electron distribution with a width of 72.3 $\mu$m. Dosimetry showed sharp microbeam dose profiles with steep lateral penumbras and a peak-to-valley dose ratio above 10 throughout a 70 mm thick PMMA phantom. An open-field dose rate of 4.3 Gy/s was measured at an acceleration voltage of 150 kV and a beam current of 17.4 mA at 150 mm distance from the focal spot. In vitro and in vivo experiments demonstrated the feasibility of the LFXT for minibeam and microbeam applications with field sizes of 1.5-2 cm. The mice displayed no observable side effects throughout the follow-up period after whole-brain 260 $\mu$m-minibeam irradiation. We successfully constructed and commissioned the first proof-of-concept LFXT prototype. Dosimetric characterizations of the achieved microbeam field showed the superiority of the LFXT compared to conventional X-ray tubes in terms of beam quality. In future developments, the remaining limitations of the prototype will be addressed for improved minibeam and first ever microbeam radiation therapy in a clinical setting.

**Link**: [arxiv](http://arxiv.org/abs/2506.09536v1),  [pdf](http://arxiv.org/pdf/2506.09536v1)

**Tags**: physics.med-ph 



### NestQuant: Nested Lattice Quantization for Matrix Products and LLMs
**Authors**: Semyon Savkin, Eitan Porat, Or Ordentlich, Yury Polyanskiy

**Updated**: 2025-06-11T06:01:15Z

**Summary**: Post-training quantization (PTQ) has emerged as a critical technique for efficient deployment of large language models (LLMs). This work proposes NestQuant, a novel PTQ scheme for weights and activations that is based on self-similar nested lattices. Recent works have mathematically shown such quantizers to be information-theoretically optimal for low-precision matrix multiplication. We implement a practical low-complexity version of NestQuant based on Gosset lattice, making it a drop-in quantizer for any matrix multiplication step (e.g., in self-attention, MLP etc). For example, NestQuant quantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving perplexity of 6.6 on wikitext2. This represents more than 55% reduction in perplexity gap with respect to unquantized model (perplexity of 6.14) compared to state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot (8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation benchmarks confirm uniform superiority of NestQuant.

**Link**: [arxiv](http://arxiv.org/abs/2502.09720v2),  [pdf](http://arxiv.org/pdf/2502.09720v2)

**Tags**: cs.LG cs.AI cs.IT math.IT 



### SAFEFLOW: A Principled Protocol for Trustworthy and Transactional   Autonomous Agent Systems
**Authors**: Peiran Li, Xinkai Zou, Zhuohang Wu, Ruifeng Li, Shuo Xing, Hanwen Zheng, Zhikai Hu, Yuping Wang, Haoxi Li, Qin Yuan, Yingmo Zhang, Zhengzhong Tu

**Updated**: 2025-06-11T03:14:10Z

**Summary**: Recent advances in large language models (LLMs) and vision-language models (VLMs) have enabled powerful autonomous agents capable of complex reasoning and multi-modal tool use. Despite their growing capabilities, today's agent frameworks remain fragile, lacking principled mechanisms for secure information flow, reliability, and multi-agent coordination. In this work, we introduce SAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based agents. SAFEFLOW enforces fine-grained information flow control (IFC), precisely tracking provenance, integrity, and confidentiality of all the data exchanged between agents, tools, users, and environments. By constraining LLM reasoning to respect these security labels, SAFEFLOW prevents untrusted or adversarial inputs from contaminating high-integrity decisions. To ensure robustness in concurrent multi-agent settings, SAFEFLOW introduces transactional execution, conflict resolution, and secure scheduling over shared state, preserving global consistency across agents. We further introduce mechanisms, including write-ahead logging, rollback, and secure caches, that further enhance resilience against runtime errors and policy violations. To validate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark suite designed to evaluate agent reliability under adversarial, noisy, and concurrent operational conditions. Extensive experiments demonstrate that agents built with SAFEFLOW maintain impressive task performance and security guarantees even in hostile environments, substantially outperforming state-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for principled, robust, and secure agent ecosystems, advancing the frontier of reliable autonomy.

**Link**: [arxiv](http://arxiv.org/abs/2506.07564v3),  [pdf](http://arxiv.org/pdf/2506.07564v3)

**Tags**: cs.AI cs.CL 



### Autoregressive Adversarial Post-Training for Real-Time Interactive Video   Generation
**Authors**: Shanchuan Lin, Ceyuan Yang, Hao He, Jianwen Jiang, Yuxi Ren, Xin Xia, Yang Zhao, Xuefeng Xiao, Lu Jiang

**Updated**: 2025-06-11T03:04:23Z

**Summary**: Existing large-scale video generation models are computationally intensive, preventing adoption in real-time and interactive applications. In this work, we propose autoregressive adversarial post-training (AAPT) to transform a pre-trained latent video diffusion model into a real-time, interactive video generator. Our model autoregressively generates a latent frame at a time using a single neural function evaluation (1NFE). The model can stream the result to the user in real time and receive interactive responses as controls to generate the next latent frame. Unlike existing approaches, our method explores adversarial training as an effective paradigm for autoregressive generation. This not only allows us to design an architecture that is more efficient for one-step generation while fully utilizing the KV cache, but also enables training the model in a student-forcing manner that proves to be effective in reducing error accumulation during long video generation. Our experiments demonstrate that our 8B model achieves real-time, 24fps, streaming video generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to a minute long (1440 frames). Visit our research website at https://seaweed-apt.com/2

**Link**: [arxiv](http://arxiv.org/abs/2506.09350v1),  [pdf](http://arxiv.org/pdf/2506.09350v1)

**Tags**: cs.CV cs.AI cs.LG 



## Keyword: LLM Inference 
 ### mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and   Model Selection at Scale
**Authors**: Xiaona Zhou, Constantin Brif, Ismini Lourentzou

**Updated**: 2025-06-26T17:59:58Z

**Summary**: Multivariate time series anomaly detection (MTS-AD) is critical in domains like healthcare, cybersecurity, and industrial monitoring, yet remains challenging due to complex inter-variable dependencies, temporal dynamics, and sparse anomaly labels. We introduce mTSBench, the largest benchmark to date for MTS-AD and unsupervised model selection, spanning 344 labeled time series across 19 datasets and 12 diverse application domains. mTSBench evaluates 24 anomaly detection methods, including large language model (LLM)-based detectors for multivariate time series, and systematically benchmarks unsupervised model selection techniques under standardized conditions. Consistent with prior findings, our results confirm that no single detector excels across datasets, underscoring the importance of model selection. However, even state-of-the-art selection methods remain far from optimal, revealing critical gaps. mTSBench provides a unified evaluation suite to enable rigorous, reproducible comparisons and catalyze future advances in adaptive anomaly detection and robust model selection.

**Link**: [arxiv](http://arxiv.org/abs/2506.21550v1),  [pdf](http://arxiv.org/pdf/2506.21550v1)

**Tags**: cs.LG cs.AI 



### Where to find Grokking in LLM Pretraining? Monitor   Memorization-to-Generalization without Test
**Authors**: Ziyue Li, Chenrui Fan, Tianyi Zhou

**Updated**: 2025-06-26T17:59:58Z

**Summary**: Grokking, i.e., test performance keeps improving long after training loss converged, has been recently witnessed in neural network training, making the mechanism of generalization and other emerging capabilities such as reasoning mysterious. While prior studies usually train small models on a few toy or highly-specific tasks for thousands of epochs, we conduct the first study of grokking on checkpoints during one-pass pretraining of a 7B large language model (LLM), i.e., OLMoE. We compute the training loss and evaluate generalization on diverse benchmark tasks, including math reasoning, code generation, and commonsense/domain-specific knowledge retrieval tasks.   Our study, for the first time, verifies that grokking still happens in the pretraining of large-scale foundation models, though different data may enter grokking stages asynchronously. We further demystify grokking's "emergence of generalization" by investigating LLM internal dynamics. Specifically, we find that training samples' pathways (i.e., expert choices across layers) evolve from random, instance-specific to more structured and shareable between samples during grokking. Also, the complexity of a sample's pathway reduces despite the converged loss. These indicate a memorization-to-generalization conversion, providing a mechanistic explanation of delayed generalization. In the study, we develop two novel metrics to quantify pathway distance and the complexity of a single pathway. We show their ability to predict the generalization improvement on diverse downstream tasks. They are efficient, simple to compute and solely dependent on training data. Hence, they have practical value for pretraining, enabling us to monitor the generalization performance without finetuning and test. Theoretically, we show that more structured pathways reduce model complexity and improve the generalization bound.

**Link**: [arxiv](http://arxiv.org/abs/2506.21551v1),  [pdf](http://arxiv.org/pdf/2506.21551v1)

**Tags**: cs.LG 



### Exploring the Design Space of 3D MLLMs for CT Report Generation
**Authors**: Mohammed Baharoon, Jun Ma, Congyu Fang, Augustin Toma, Bo Wang

**Updated**: 2025-06-26T17:54:20Z

**Summary**: Multimodal Large Language Models (MLLMs) have emerged as a promising way to automate Radiology Report Generation (RRG). In this work, we systematically investigate the design space of 3D MLLMs, including visual input representation, projectors, Large Language Models (LLMs), and fine-tuning techniques for 3D CT report generation. We also introduce two knowledge-based report augmentation methods that improve performance on the GREEN score by up to 10\%, achieving the 2nd place on the MICCAI 2024 AMOS-MM challenge. Our results on the 1,687 cases from the AMOS-MM dataset show that RRG is largely independent of the size of LLM under the same training protocol. We also show that larger volume size does not always improve performance if the original ViT was pre-trained on a smaller volume size. Lastly, we show that using a segmentation mask along with the CT volume improves performance. The code is publicly available at https://github.com/bowang-lab/AMOS-MM-Solution

**Link**: [arxiv](http://arxiv.org/abs/2506.21535v1),  [pdf](http://arxiv.org/pdf/2506.21535v1)

**Tags**: eess.IV cs.CV cs.LG 



### "What's Up, Doc?": Analyzing How Users Seek Health Information in   Large-Scale Conversational AI Datasets
**Authors**: Akshay Paruchuri, Maryam Aziz, Rohit Vartak, Ayman Ali, Best Uchehara, Xin Liu, Ishan Chatterjee, Monica Agrawal

**Updated**: 2025-06-26T17:52:18Z

**Summary**: People are increasingly seeking healthcare information from large language models (LLMs) via interactive chatbots, yet the nature and inherent risks of these conversations remain largely unexplored. In this paper, we filter large-scale conversational AI datasets to achieve HealthChat-11K, a curated dataset of 11K real-world conversations composed of 25K user messages. We use HealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs when seeking healthcare information in order to systematically study user interactions across 21 distinct health specialties. Our analysis reveals insights into the nature of how and why users seek health information, such as common interactions, instances of incomplete context, affective behaviors, and interactions (e.g., leading questions) that can induce sycophancy, underscoring the need for improvements in the healthcare support capabilities of LLMs deployed as conversational AI. Code and artifacts to retrieve our analyses and combine them into a curated dataset can be found here: https://github.com/yahskapar/HealthChat

**Link**: [arxiv](http://arxiv.org/abs/2506.21532v1),  [pdf](http://arxiv.org/pdf/2506.21532v1)

**Tags**: cs.CL cs.AI cs.CY 



### OpenNER 1.0: Standardized Open-Access Named Entity Recognition Datasets   in 50+ Languages
**Authors**: Chester Palen-Michel, Maxwell Pickering, Maya Kruse, Jonne Sälevä, Constantine Lignos

**Updated**: 2025-06-26T17:51:40Z

**Summary**: We present OpenNER 1.0, a standardized collection of openly-available named entity recognition (NER) datasets. OpenNER contains 36 NER corpora that span 52 languages, human-annotated in varying named entity ontologies. We correct annotation format issues, standardize the original datasets into a uniform representation with consistent entity type names across corpora, and provide the collection in a structure that enables research in multilingual and multi-ontology NER. We provide baseline results using three pretrained multilingual language models and two large language models to compare the performance of recent models and facilitate future research in NER. We find that no single model is best in all languages and that significant work remains to obtain high performance from LLMs on the NER task.

**Link**: [arxiv](http://arxiv.org/abs/2412.09587v2),  [pdf](http://arxiv.org/pdf/2412.09587v2)

**Tags**: cs.CL 



### Chain-of-Sketch: Enabling Global Visual Reasoning
**Authors**: Aryo Lotfi, Enrico Fini, Samy Bengio, Moin Nabi, Emmanuel Abbe

**Updated**: 2025-06-26T17:48:33Z

**Summary**: Modern vision models have achieved remarkable success in benchmarks where local features provide critical information about the target. There is now a growing interest in tackling tasks requiring more global reasoning, where local features do not provide significant information. Minsky and Papert put forward such tasks in 1969 with their connectivity study, exposing the limitations of the perceptron model. In this paper, we introduce an expanded set of global visual datasets involving graphs, strings, mazes, and image grids. We show that large vision models still struggle to learn these tasks efficiently. Similarly, state-of-the-art multi-modal LLMs perform poorly on these datasets. We explain this learning inefficiency by means of the 'globality degree' measure. To mitigate this, we propose a method called chain-of-sketch (CoS). Similar to the chain-of-thought and scratchpad techniques used in language models, CoS breaks the original task into intermediate visual steps to help learn a complex task. In addition, we show that not all CoS strategies perform equally well. Our key insight is to impose a Markovian structure on the CoS frames. This leads to the introduction of 'inductive CoS' which achieves better out-of-distribution generalization and performs well even with smaller models compared to non-inductive variants.

**Link**: [arxiv](http://arxiv.org/abs/2410.08165v2),  [pdf](http://arxiv.org/pdf/2410.08165v2)

**Tags**: cs.LG cs.CV 



### Asymptotic Inference for Exchangeable Gibbs Partition
**Authors**: Takuya Koriyama

**Updated**: 2025-06-26T17:48:18Z

**Summary**: We study the asymptotic properties of parameter estimation and predictive inference under the exchangeable Gibbs partition, characterized by a discount parameter $\alpha\in(0,1)$ and a triangular array $v_{n,k}$ satisfying a backward recursion. Assuming that $v_{n,k}$ admits a mixture representation over the Ewens--Pitman family $(\alpha, \theta)$, with $\theta$ integrated by an unknown mixing distribution, we show that the (quasi) maximum likelihood estimator $\hat\alpha_n$ (QMLE) for $\alpha$ is asymptotically mixed normal. This generalizes earlier results for the Ewens--Pitman model to a more general class. We further study the predictive task of estimating the probability simplex $\mathsf{p}_n$, which governs the allocation of the $(n+1)$-th item, conditional on the current partition of $[n]$. Based on the asymptotics of the QMLE $\hat{\alpha}_n$, we construct an estimator $\hat{\mathsf{p}}_n$ and derive the limit distributions of the $f$-divergence $\mathsf{D}_f(\hat{\mathsf{p}}_n||\mathsf{p}_n)$ for general convex functions $f$, including explicit results for the TV distance and KL divergence. These results lead to asymptotically valid confidence intervals for both parameter estimation and prediction.

**Link**: [arxiv](http://arxiv.org/abs/2506.21527v1),  [pdf](http://arxiv.org/pdf/2506.21527v1)

**Tags**: math.ST math.PR stat.TH 



### Potemkin Understanding in Large Language Models
**Authors**: Marina Mancoridis, Bec Weeks, Keyon Vafa, Sendhil Mullainathan

**Updated**: 2025-06-26T17:41:35Z

**Summary**: Large language models (LLMs) are regularly evaluated using benchmark datasets. But what justifies making inferences about an LLM's capabilities based on its answers to a curated set of questions? This paper first introduces a formal framework to address this question. The key is to note that the benchmarks used to test LLMs -- such as AP exams -- are also those used to test people. However, this raises an implication: these benchmarks are only valid tests if LLMs misunderstand concepts in ways that mirror human misunderstandings. Otherwise, success on benchmarks only demonstrates potemkin understanding: the illusion of understanding driven by answers irreconcilable with how any human would interpret a concept. We present two procedures for quantifying the existence of potemkins: one using a specially designed benchmark in three domains, the other using a general procedure that provides a lower-bound on their prevalence. We find that potemkins are ubiquitous across models, tasks, and domains. We also find that these failures reflect not just incorrect understanding, but deeper internal incoherence in concept representations.

**Link**: [arxiv](http://arxiv.org/abs/2506.21521v1),  [pdf](http://arxiv.org/pdf/2506.21521v1)

**Tags**: cs.CL cs.AI 



### Mitigating Hallucination of Large Vision-Language Models via Dynamic   Logits Calibration
**Authors**: Jiahe Chen, Jiaying He, Qian Shao, Qiyuan Chen, Jiahe Ying, Hongxia Xu, Jintai Chen, Jianwei Zheng, Jian Wu

**Updated**: 2025-06-26T17:35:40Z

**Summary**: Large Vision-Language Models (LVLMs) have demonstrated significant advancements in multimodal understanding, yet they are frequently hampered by hallucination-the generation of text that contradicts visual input. Existing training-free decoding strategies exhibit critical limitations, including the use of static constraints that do not adapt to semantic drift during generation, inefficiency stemming from the need for multiple forward passes, and degradation of detail due to overly rigid intervention rules. To overcome these challenges, this paper introduces Dynamic Logits Calibration (DLC), a novel training-free decoding framework designed to dynamically align text generation with visual evidence at inference time. At the decoding phase, DLC step-wise employs CLIP to assess the semantic alignment between the input image and the generated text sequence. Then, the Relative Visual Advantage (RVA) of candidate tokens is evaluated against a dynamically updated contextual baseline, adaptively adjusting output logits to favor tokens that are visually grounded. Furthermore, an adaptive weighting mechanism, informed by a real-time context alignment score, carefully balances the visual guidance while ensuring the overall quality of the textual output. Extensive experiments conducted across diverse benchmarks and various LVLM architectures (such as LLaVA, InstructBLIP, and MiniGPT-4) demonstrate that DLC significantly reduces hallucinations, outperforming current methods while maintaining high inference efficiency by avoiding multiple forward passes. Overall, we present an effective and efficient decoding-time solution to mitigate hallucinations, thereby enhancing the reliability of LVLMs for more practices. Code will be released on Github.

**Link**: [arxiv](http://arxiv.org/abs/2506.21509v1),  [pdf](http://arxiv.org/pdf/2506.21509v1)

**Tags**: cs.CV 



### Causal inference via implied interventions
**Authors**: Carlos García Meixide, Mark J. van der Laan

**Updated**: 2025-06-26T17:29:36Z

**Summary**: In the context of having an instrumental variable, the standard practice in causal inference begins by targeting an effect of interest and proceeds by formulating assumptions enabling identification of this effect. We turn this around by simply not making assumptions anymore and just adhere to the interventions we can identify, rather than starting with a desired causal estimand and imposing untestable hypotheses. The randomization of an instrument and its exclusion restriction define a class of auxiliary stochastic interventions on the treatment that are implied by stochastic interventions on the instrument. This mapping effectively characterizes the identifiable causal effects of the treatment on the outcome given the observable probability distribution, leading to an explicit transparent G-computation formula under hidden confounding. Alternatively, searching for an intervention on the instrument whose implied one best approximates a desired target -- whose causal effect the user aims to estimate -- naturally leads to a projection on a function space representing the closest identifiable treatment effect. The generality of this projection allows to select different norms and indexing sets for the function class that turn optimization into different estimation procedures with the Highly Adaptive Lasso. This shift from identification under assumptions to identification under observation redefines how the problem of causal inference is approached.

**Link**: [arxiv](http://arxiv.org/abs/2506.21501v1),  [pdf](http://arxiv.org/pdf/2506.21501v1)

**Tags**: math.ST stat.TH 



### Enhancing User Engagement in Socially-Driven Dialogue through   Interactive LLM Alignments
**Authors**: Jiashuo Wang, Kaitao Song, Chunpu Xu, Changhe Song, Yang Xiao, Dongsheng Li, Lili Qiu, Wenjie Li

**Updated**: 2025-06-26T17:26:17Z

**Summary**: Enhancing user engagement through interactions plays an essential role in socially-driven dialogues. While prior works have optimized models to reason over relevant knowledge or plan a dialogue act flow, the relationship between user engagement and knowledge or dialogue acts is subtle and does not guarantee user engagement in socially-driven dialogues. To this end, we enable interactive LLMs to learn user engagement by leveraging signals from the future development of conversations. Specifically, we adopt a more direct and relevant indicator of user engagement, i.e., the user's reaction related to dialogue intention after the interaction, as a reward to align interactive LLMs. To achieve this, we develop a user simulator to interact with target interactive LLMs and explore interactions between the user and the interactive LLM system via \textit{i$\times$MCTS} (\textit{M}onte \textit{C}arlo \textit{T}ree \textit{S}earch for \textit{i}nteraction). In this way, we collect a dataset containing pairs of higher and lower-quality experiences using \textit{i$\times$MCTS}, and align interactive LLMs for high-level user engagement by direct preference optimization (DPO) accordingly. Experiments conducted on two socially-driven dialogue scenarios (emotional support conversations and persuasion for good) demonstrate that our method effectively enhances user engagement in interactive LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2506.21497v1),  [pdf](http://arxiv.org/pdf/2506.21497v1)

**Tags**: cs.CL 



### Bridging Offline and Online Reinforcement Learning for LLMs
**Authors**: Jack Lanchantin, Angelica Chen, Janice Lan, Xian Li, Swarnadeep Saha, Tianlu Wang, Jing Xu, Ping Yu, Weizhe Yuan, Jason E Weston, Sainbayar Sukhbaatar, Ilia Kulikov

**Updated**: 2025-06-26T17:25:49Z

**Summary**: We investigate the effectiveness of reinforcement learning methods for finetuning large language models when transitioning from offline to semi-online to fully online regimes for both verifiable and non-verifiable tasks. Our experiments cover training on verifiable math as well as non-verifiable instruction following with a set of benchmark evaluations for both. Across these settings, we extensively compare online and semi-online Direct Preference Optimization and Group Reward Policy Optimization objectives, and surprisingly find similar performance and convergence between these variants, which all strongly outperform offline methods. We provide a detailed analysis of the training dynamics and hyperparameter selection strategies to achieve optimal results. Finally, we show that multi-tasking with verifiable and non-verifiable rewards jointly yields improved performance across both task types.

**Link**: [arxiv](http://arxiv.org/abs/2506.21495v1),  [pdf](http://arxiv.org/pdf/2506.21495v1)

**Tags**: cs.CL 



### Prompting with Phonemes: Enhancing LLMs' Multilinguality for Non-Latin   Script Languages
**Authors**: Hoang H Nguyen, Khyati Mahajan, Vikas Yadav, Julian Salazar, Philip S. Yu, Masoud Hashemi, Rishabh Maheshwary

**Updated**: 2025-06-26T17:22:53Z

**Summary**: Although multilingual LLMs have achieved remarkable performance across benchmarks, we find they continue to underperform on non-Latin script languages across contemporary LLM families. This discrepancy arises from the fact that LLMs are pretrained with orthographic scripts, which are dominated by Latin characters that obscure their shared phonology with non-Latin scripts. We propose leveraging phonemic transcriptions as complementary signals to induce script-invariant representations. Our study demonstrates that integrating phonemic signals improves performance across both non-Latin and Latin script languages, with a particularly significant impact on closing the performance gap between the two. Through detailed experiments, we show that phonemic and orthographic scripts retrieve distinct examples for in-context learning (ICL). This motivates our proposed Mixed-ICL retrieval strategy, where further aggregation from both leads to our significant performance improvements for both Latin script languages (up to 12.6%) and non-Latin script languages (up to 15.1%) compared to randomized ICL retrieval.

**Link**: [arxiv](http://arxiv.org/abs/2411.02398v3),  [pdf](http://arxiv.org/pdf/2411.02398v3)

**Tags**: cs.CL cs.AI cs.LG 



### From Web Search towards Agentic Deep Research: Incentivizing Search with   Reasoning Agents
**Authors**: Weizhi Zhang, Yangning Li, Yuanchen Bei, Junyu Luo, Guancheng Wan, Liangwei Yang, Chenxuan Xie, Yuyao Yang, Wei-Chieh Huang, Chunyu Miao, Henry Peng Zou, Xiao Luo, Yusheng Zhao, Yankai Chen, Chunkit Chan, Peilin Zhou, Xinyang Zhang, Chenwei Zhang, Jingbo Shang, Ming Zhang, Yangqiu Song, Irwin King, Philip S. Yu

**Updated**: 2025-06-26T17:18:00Z

**Summary**: Information retrieval is a cornerstone of modern knowledge acquisition, enabling billions of queries each day across diverse domains. However, traditional keyword-based search engines are increasingly inadequate for handling complex, multi-step information needs. Our position is that Large Language Models (LLMs), endowed with reasoning and agentic capabilities, are ushering in a new paradigm termed Agentic Deep Research. These systems transcend conventional information search techniques by tightly integrating autonomous reasoning, iterative retrieval, and information synthesis into a dynamic feedback loop. We trace the evolution from static web search to interactive, agent-based systems that plan, explore, and learn. We also introduce a test-time scaling law to formalize the impact of computational depth on reasoning and search. Supported by benchmark results and the rise of open-source implementations, we demonstrate that Agentic Deep Research not only significantly outperforms existing approaches, but is also poised to become the dominant paradigm for future information seeking. All the related resources, including industry products, research papers, benchmark datasets, and open-source implementations, are collected for the community in https://github.com/DavidZWZ/Awesome-Deep-Research.

**Link**: [arxiv](http://arxiv.org/abs/2506.18959v2),  [pdf](http://arxiv.org/pdf/2506.18959v2)

**Tags**: cs.IR cs.CL cs.LG 



### Implications of feedback solutions to the $S_8$ tension for the baryon   fractions of galaxy groups and clusters
**Authors**: Jaime Salcido, Ian G. McCarthy

**Updated**: 2025-06-26T17:02:47Z

**Summary**: Recent large-scale structure (LSS) surveys have revealed a persistent tension in the value of $S_8$ compared to predictions from the standard cosmological model. This tension may suggest the need for new physics beyond the standard model, but an accurate characterisation of baryonic effects is essential to avoid biases. Although some studies indicate that baryonic effects are too small to resolve this tension, others propose that more aggressive feedback mechanisms could reconcile differences between cosmic microwave background (CMB) measurements and low-redshift LSS observations. In this paper, we investigate the role of baryonic effects in alleviating the $S_8$ tension. We extend the SP(k) model (Salcido et al. 2023), which was trained on hundreds of cosmological hydrodynamical simulations to map the suppression of the matter power spectrum to the baryon fraction in groups and clusters, to predict the required baryon fraction for a given $P(k)$ suppression. We then compare predictions from recent cosmic shear (weak lensing) analyses with the latest baryon budget measurements from X-ray and weak gravitational lensing studies. Our findings show that studies marginalising over baryonic effects while fixing cosmological parameters to a Planck-like cosmology predict strong $P(k)$ suppression and baryon fractions that are much lower than existing low-redshift baryon budget estimates of galaxy groups and clusters. Conversely, most studies that marginalise over both cosmological parameters and baryonic effects imply baryon fractions that are consistent with observations but lower values of $S_8$ than inferred from the CMB. Unless the observed baryon fractions are biased high by a factor of several, these results suggest that a mechanism beyond baryonic physics alone is required to modify or slow down the growth of structure in the universe in order to resolve the $S_8$ tension.

**Link**: [arxiv](http://arxiv.org/abs/2409.05716v3),  [pdf](http://arxiv.org/pdf/2409.05716v3)

**Tags**: astro-ph.CO 



### Kilonova Light Curve Parameter Estimation Using Likelihood-Free   Inference
**Authors**: Malina Desai, Deep Chatterjee, Sahil Jhawar, Philip Harris, Erik Katsavounidis, Michael Coughlin

**Updated**: 2025-06-26T16:55:48Z

**Summary**: Rapid parameter estimation is critical when dealing with short lived signals such as kilonovae. We present a parameter estimation algorithm that combines likelihood-free inference with a pre-trained embedding network, optimized to efficiently process kilonova light curves. Our method is capable of retrieving the mass, velocity, and lanthanide fraction of the neutron star ejecta with an accuracy and precision on par with nested sampling methods while taking significantly less computational time. Our inference uniquely utilizes a pre-trained embedding network that marginalizes the time of arrival and the luminosity distance of the signal, allowing inference of signals at distances up to 200 Mpc. We find that including a pre-trained embedding outperforms the use of likelihood-free inference alone, reducing training time, model size, and offering the capability to marginalize over certain nuisance parameters. This framework has been integrated into the publicly available Nuclear Multi-Messenger Astronomy codebase, enabling the broader scientific community to deploy the model for their inference purposes. Our algorithm is broadly applicable to parameterized or simulated light curves of other transient objects, and can be adapted for quick sky localization.

**Link**: [arxiv](http://arxiv.org/abs/2408.06947v3),  [pdf](http://arxiv.org/pdf/2408.06947v3)

**Tags**: astro-ph.IM astro-ph.HE 



### In-Context Learning Strategies Emerge Rationally
**Authors**: Daniel Wurgaft, Ekdeep Singh Lubana, Core Francisco Park, Hidenori Tanaka, Gautam Reddy, Noah D. Goodman

**Updated**: 2025-06-26T16:54:57Z

**Summary**: Recent work analyzing in-context learning (ICL) has identified a broad set of strategies that describe model behavior in different experimental conditions. We aim to unify these findings by asking why a model learns these disparate strategies in the first place. Specifically, we start with the observation that when trained to learn a mixture of tasks, as is popular in the literature, the strategies learned by a model for performing ICL can be captured by a family of Bayesian predictors: a memorizing predictor, which assumes a discrete prior on the set of seen tasks, and a generalizing predictor, where the prior matches the underlying task distribution. Adopting the normative lens of rational analysis, where a learner's behavior is explained as an optimal adaptation to data given computational constraints, we develop a hierarchical Bayesian framework that almost perfectly predicts Transformer next-token predictions throughout training -- without assuming access to its weights. Under this framework, pretraining is viewed as a process of updating the posterior probability of different strategies, and inference-time behavior as a posterior-weighted average over these strategies' predictions. Our framework draws on common assumptions about neural network learning dynamics, which make explicit a tradeoff between loss and complexity among candidate strategies: beyond how well it explains the data, a model's preference towards implementing a strategy is dictated by its complexity. This helps explain well-known ICL phenomena, while offering novel predictions: e.g., we show a superlinear trend in the timescale for transitioning from generalization to memorization as task diversity increases. Overall, our work advances an explanatory and predictive account of ICL grounded in tradeoffs between strategy loss and complexity.

**Link**: [arxiv](http://arxiv.org/abs/2506.17859v2),  [pdf](http://arxiv.org/pdf/2506.17859v2)

**Tags**: cs.LG cs.AI 



### Efficient and Reuseable Cloud Configuration Search Using Discovery   Spaces
**Authors**: Michael Johnston, Burkhard Ringlein, Christoph Hagleitner, Alessandro Pomponio, Vassilis Vassiliadis, Christian Pinto, Srikumar Venugopal

**Updated**: 2025-06-26T16:54:39Z

**Summary**: Finding the optimal set of cloud resources to deploy a given workload at minimal cost while meeting a defined service level agreement is an active area of research. Combining tens of parameters applicable across a large selection of compute, storage, and services offered by cloud providers with similar numbers of application-specific parameters leads to configuration spaces with millions of deployment options.   In this paper, we propose Discovery Space, an abstraction that formalizes the description of workload configuration problems, and exhibits a set of characteristics required for structured, robust and distributed investigations of large search spaces. We describe a concrete implementation of the Discovery Space abstraction and show that it is generalizable across a diverse set of workloads such as Large Language Model inference and Big Data Analytics.   We demonstrate that our approach enables safe, transparent sharing of data between executions of best-of-breed optimizers increasing the efficiency of optimal configuration detection in large search spaces. We also demonstrate how Discovery Spaces enable transfer and reuse of knowledge across similar search spaces, enabling configuration search speed-ups of over 90%.

**Link**: [arxiv](http://arxiv.org/abs/2506.21467v1),  [pdf](http://arxiv.org/pdf/2506.21467v1)

**Tags**: cs.DC C.4 



### Graphs Meet AI Agents: Taxonomy, Progress, and Future Opportunities
**Authors**: Yuanchen Bei, Weizhi Zhang, Siwen Wang, Weizhi Chen, Sheng Zhou, Hao Chen, Yong Li, Jiajun Bu, Shirui Pan, Yizhou Yu, Irwin King, Fakhri Karray, Philip S. Yu

**Updated**: 2025-06-26T16:54:14Z

**Summary**: AI agents have experienced a paradigm shift, from early dominance by reinforcement learning (RL) to the rise of agents powered by large language models (LLMs), and now further advancing towards a synergistic fusion of RL and LLM capabilities. This progression has endowed AI agents with increasingly strong abilities. Despite these advances, to accomplish complex real-world tasks, agents are required to plan and execute effectively, maintain reliable memory, and coordinate smoothly with other agents. Achieving these capabilities involves contending with ever-present intricate information, operations, and interactions. In light of this challenge, data structurization can play a promising role by transforming intricate and disorganized data into well-structured forms that agents can more effectively understand and process. In this context, graphs, with their natural advantage in organizing, managing, and harnessing intricate data relationships, present a powerful data paradigm for structurization to support the capabilities demanded by advanced AI agents. To this end, this survey presents a first systematic review of how graphs can empower AI agents. Specifically, we explore the integration of graph techniques with core agent functionalities, highlight notable applications, and identify prospective avenues for future research. By comprehensively surveying this burgeoning intersection, we hope to inspire the development of next-generation AI agents equipped to tackle increasingly sophisticated challenges with graphs. Related resources are collected and continuously updated for the community in the Github link.

**Link**: [arxiv](http://arxiv.org/abs/2506.18019v2),  [pdf](http://arxiv.org/pdf/2506.18019v2)

**Tags**: cs.AI 



### PARALLELPROMPT: Extracting Parallelism from Large Language Model Queries
**Authors**: Steven Kolawole, Keshav Santhanam, Virginia Smith, Pratiksha Thaker

**Updated**: 2025-06-26T16:35:54Z

**Summary**: LLM serving systems typically treat user prompts as monolithic inputs, optimizing inference through decoding tricks or inter-query batching. However, many real-world prompts contain latent semantic parallelism--decomposable structures where subtasks can be executed independently to reduce latency while preserving meaning. We introduce PARALLELPROMPT, the first benchmark for measuring intra-query parallelism in natural user prompts. Our dataset comprises over 37,000 real-world prompts from public LLM chat logs, each annotated with a structured schema capturing task templates, shared context, and iteration inputs. These schemas are extracted using LLM-assisted prompting with rule-based multilingual validation. To evaluate the benefits of decomposition, we provide an execution suite that benchmarks serial vs. parallel strategies, measuring latency, structural adherence, and semantic fidelity. Our results show that intra-query parallelism can be successfully parsed in over 75% of curated datasets, unlocking up to 5x speedups on tasks like translation, comprehension, and comparative analysis, with minimal quality degradation. By releasing this benchmark, curation pipeline, and evaluation suite, we provide the first standardized testbed for studying structure-aware execution in LLM serving pipelines.

**Link**: [arxiv](http://arxiv.org/abs/2506.18728v2),  [pdf](http://arxiv.org/pdf/2506.18728v2)

**Tags**: cs.LG 



### Text2Cypher Across Languages: Evaluating Foundational Models Beyond   English
**Authors**: Makbule Gulcin Ozsoy, William Tai

**Updated**: 2025-06-26T16:31:10Z

**Summary**: Recent advances in large language models have enabled natural language interfaces that translate user questions into database queries, such as Text2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database accessibility, most research today focuses solely on English, with limited evaluation in other languages. This paper investigates the performance of foundational LLMs on the Text2Cypher task across multiple languages. We create and release a multilingual test set by translating English questions into Spanish and Turkish while preserving the original Cypher queries, enabling fair cross-lingual comparison. We evaluate multiple foundational models using standardized prompts and metrics. Our results show a consistent performance pattern: highest on English, then Spanish, and lowest on Turkish. We attribute this to differences in training data availability and linguistic characteristics. Additionally, we explore the impact of translating task prompts into Spanish and Turkish. Results show little to no change in evaluation metrics, suggesting prompt translation has minor impact. Our findings highlight the need for more inclusive evaluation and development in multilingual query generation. Future work includes schema localization and fine-tuning across diverse languages.

**Link**: [arxiv](http://arxiv.org/abs/2506.21445v1),  [pdf](http://arxiv.org/pdf/2506.21445v1)

**Tags**: cs.CL cs.IR 



### Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection
**Authors**: Ali Şenol, Garima Agrawal, Huan Liu

**Updated**: 2025-06-26T16:29:45Z

**Summary**: Detecting deceptive conversations on dynamic platforms is increasingly difficult due to evolving language patterns and Concept Drift (CD)-i.e., semantic or topical shifts that alter the context or intent of interactions over time. These shifts can obscure malicious intent or mimic normal dialogue, making accurate classification challenging. While Large Language Models (LLMs) show strong performance in natural language tasks, they often struggle with contextual ambiguity and hallucinations in risk-sensitive scenarios. To address these challenges, we present a Domain Knowledge (DK)-Enhanced LLM framework that integrates pretrained LLMs with structured, task-specific insights to perform fraud and concept drift detection. The proposed architecture consists of three main components: (1) a DK-LLM module to detect fake or deceptive conversations; (2) a drift detection unit (OCDD) to determine whether a semantic shift has occurred; and (3) a second DK-LLM module to classify the drift as either benign or fraudulent. We first validate the value of domain knowledge using a fake review dataset and then apply our full framework to SEConvo, a multiturn dialogue dataset that includes various types of fraud and spam attacks. Results show that our system detects fake conversations with high accuracy and effectively classifies the nature of drift. Guided by structured prompts, the LLaMA-based implementation achieves 98% classification accuracy. Comparative studies against zero-shot baselines demonstrate that incorporating domain knowledge and drift awareness significantly improves performance, interpretability, and robustness in high-stakes NLP applications.

**Link**: [arxiv](http://arxiv.org/abs/2506.21443v1),  [pdf](http://arxiv.org/pdf/2506.21443v1)

**Tags**: cs.CL cs.AI 



### Explainability of Large Language Models using SMILE: Statistical   Model-agnostic Interpretability with Local Explanations
**Authors**: Zeinab Dehghani, Mohammed Naveed Akram, Koorosh Aslansefat, Adil Khan

**Updated**: 2025-06-26T16:16:59Z

**Summary**: Large language models like GPT, LLAMA, and Claude have become incredibly powerful at generating text, but they are still black boxes, so it is hard to understand how they decide what to say. That lack of transparency can be problematic, especially in fields where trust and accountability matter. To help with this, we introduce SMILE, a new method that explains how these models respond to different parts of a prompt. SMILE is model-agnostic and works by slightly changing the input, measuring how the output changes, and then highlighting which words had the most impact. Create simple visual heat maps showing which parts of a prompt matter the most. We tested SMILE on several leading LLMs and used metrics such as accuracy, consistency, stability, and fidelity to show that it gives clear and reliable explanations. By making these models easier to understand, SMILE brings us one step closer to making AI more transparent and trustworthy.

**Link**: [arxiv](http://arxiv.org/abs/2505.21657v3),  [pdf](http://arxiv.org/pdf/2505.21657v3)

**Tags**: cs.CL cs.AI cs.LG 



### Graph Neural Network for Neutrino Physics Event Reconstruction
**Authors**: V Hewes, Adam Aurisano, Giuseppe Cerati, Jim Kowalkowski, Claire Lee, Wei-keng Liao, Daniel Grzenda, Kaushal Gumpula, Xiaohe Zhang

**Updated**: 2025-06-26T16:15:31Z

**Summary**: Liquid Argon Time Projection Chamber (LArTPC) detector technology offers a wealth of high-resolution information on particle interactions, and leveraging that information to its full potential requires sophisticated automated reconstruction techniques. This article describes NuGraph2, a Graph Neural Network (GNN) for low-level reconstruction of simulated neutrino interactions in a LArTPC detector. Simulated neutrino interactions in the MicroBooNE detector geometry are described as heterogeneous graphs, with energy depositions on each detector plane forming nodes on planar subgraphs. The network utilizes a multi-head attention message-passing mechanism to perform background filtering and semantic labelling on these graph nodes, identifying those associated with the primary physics interaction with 98.0\% efficiency and labelling them according to particle type with 94.9\% efficiency. The network operates directly on detector observables across multiple 2D representations, but utilizes a 3D-context-aware mechanism to encourage consistency between these representations. Model inference takes 0.12~s/event on a CPU, and 0.005s/event batched on a GPU. This architecture is designed to be a general-purpose solution for particle reconstruction in neutrino physics, with the potential for deployment across a broad range of detector technologies, and offers a core convolution engine that can be leveraged for a variety of tasks beyond the two described in this article.

**Link**: [arxiv](http://arxiv.org/abs/2403.11872v2),  [pdf](http://arxiv.org/pdf/2403.11872v2)

**Tags**: physics.data-an cs.LG hep-ex 



### Rethinking LLM Training through Information Geometry and Quantum Metrics
**Authors**: Riccardo Di Sipio

**Updated**: 2025-06-26T16:14:42Z

**Summary**: Optimization in large language models (LLMs) unfolds over high-dimensional parameter spaces with non-Euclidean structure. Information geometry frames this landscape using the Fisher information metric, enabling more principled learning via natural gradient descent. Though often impractical, this geometric lens clarifies phenomena such as sharp minima, generalization, and observed scaling laws. We argue that curvature-aware approaches deepen our understanding of LLM training. Finally, we speculate on quantum analogies based on the Fubini-Study metric and Quantum Fisher Information, hinting at efficient optimization in quantum-enhanced systems.

**Link**: [arxiv](http://arxiv.org/abs/2506.15830v2),  [pdf](http://arxiv.org/pdf/2506.15830v2)

**Tags**: cs.CL quant-ph I.2; I.7 



### DisCoPatch: Taming Adversarially-driven Batch Statistics for Improved   Out-of-Distribution Detection
**Authors**: Francisco Caetano, Christiaan Viviers, Luis A. Zavala-Mondragón, Peter H. N. de With, Fons van der Sommen

**Updated**: 2025-06-26T16:11:14Z

**Summary**: Out-of-distribution (OOD) detection holds significant importance across many applications. While semantic and domain-shift OOD problems are well-studied, this work focuses on covariate shifts - subtle variations in the data distribution that can degrade machine learning performance. We hypothesize that detecting these subtle shifts can improve our understanding of in-distribution boundaries, ultimately improving OOD detection. In adversarial discriminators trained with Batch Normalization (BN), real and adversarial samples form distinct domains with unique batch statistics - a property we exploit for OOD detection. We introduce DisCoPatch, an unsupervised Adversarial Variational Autoencoder (VAE) framework that harnesses this mechanism. During inference, batches consist of patches from the same image, ensuring a consistent data distribution that allows the model to rely on batch statistics. DisCoPatch uses the VAE's suboptimal outputs (generated and reconstructed) as negative samples to train the discriminator, thereby improving its ability to delineate the boundary between in-distribution samples and covariate shifts. By tightening this boundary, DisCoPatch achieves state-of-the-art results in public OOD detection benchmarks. The proposed model not only excels in detecting covariate shifts, achieving 95.5% AUROC on ImageNet-1K(-C) but also outperforms all prior methods on public Near-OOD (95.0%) benchmarks. With a compact model size of 25MB, it achieves high OOD detection performance at notably lower latency than existing methods, making it an efficient and practical solution for real-world OOD detection applications. The code is publicly available.

**Link**: [arxiv](http://arxiv.org/abs/2501.08005v3),  [pdf](http://arxiv.org/pdf/2501.08005v3)

**Tags**: cs.CV cs.AI eess.IV 



### Flow-Based Single-Step Completion for Efficient and Expressive Policy   Learning
**Authors**: Prajwal Koirala, Cody Fleming

**Updated**: 2025-06-26T16:09:53Z

**Summary**: Generative models such as diffusion and flow-matching offer expressive policies for offline reinforcement learning (RL) by capturing rich, multimodal action distributions, but their iterative sampling introduces high inference costs and training instability due to gradient propagation across sampling steps. We propose the \textit{Single-Step Completion Policy} (SSCP), a generative policy trained with an augmented flow-matching objective to predict direct completion vectors from intermediate flow samples, enabling accurate, one-shot action generation. In an off-policy actor-critic framework, SSCP combines the expressiveness of generative models with the training and inference efficiency of unimodal policies, without requiring long backpropagation chains. Our method scales effectively to offline, offline-to-online, and online RL settings, offering substantial gains in speed and adaptability over diffusion-based baselines. We further extend SSCP to goal-conditioned RL, enabling flat policies to exploit subgoal structures without explicit hierarchical inference. SSCP achieves strong results across standard offline RL and behavior cloning benchmarks, positioning it as a versatile, expressive, and efficient framework for deep RL and sequential decision-making.

**Link**: [arxiv](http://arxiv.org/abs/2506.21427v1),  [pdf](http://arxiv.org/pdf/2506.21427v1)

**Tags**: cs.LG cs.RO 



### TracLLM: A Generic Framework for Attributing Long Context LLMs
**Authors**: Yanting Wang, Wei Zou, Runpeng Geng, Jinyuan Jia

**Updated**: 2025-06-26T16:09:36Z

**Summary**: Long context large language models (LLMs) are deployed in many real-world applications such as RAG, agent, and broad LLM-integrated applications. Given an instruction and a long context (e.g., documents, PDF files, webpages), a long context LLM can generate an output grounded in the provided context, aiming to provide more accurate, up-to-date, and verifiable outputs while reducing hallucinations and unsupported claims. This raises a research question: how to pinpoint the texts (e.g., sentences, passages, or paragraphs) in the context that contribute most to or are responsible for the generated output by an LLM? This process, which we call context traceback, has various real-world applications, such as 1) debugging LLM-based systems, 2) conducting post-attack forensic analysis for attacks (e.g., prompt injection attack, knowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources to enhance the trust of users towards outputs generated by LLMs. When applied to context traceback for long context LLMs, existing feature attribution methods such as Shapley have sub-optimal performance and/or incur a large computational cost. In this work, we develop TracLLM, the first generic context traceback framework tailored to long context LLMs. Our framework can improve the effectiveness and efficiency of existing feature attribution methods. To improve the efficiency, we develop an informed search based algorithm in TracLLM. We also develop contribution score ensemble/denoising techniques to improve the accuracy of TracLLM. Our evaluation results show TracLLM can effectively identify texts in a long context that lead to the output of an LLM. Our code and data are at: https://github.com/Wang-Yanting/TracLLM.

**Link**: [arxiv](http://arxiv.org/abs/2506.04202v3),  [pdf](http://arxiv.org/pdf/2506.04202v3)

**Tags**: cs.CR cs.AI cs.LG 



### Scalable Bayesian Low-Rank Adaptation of Large Language Models via   Stochastic Variational Subspace Inference
**Authors**: Colin Samplawski, Adam D. Cobb, Manoj Acharya, Ramneet Kaur, Susmit Jha

**Updated**: 2025-06-26T15:54:45Z

**Summary**: Despite their widespread use, large language models (LLMs) are known to hallucinate incorrect information and be poorly calibrated. This makes the uncertainty quantification of these models of critical importance, especially in high-stakes domains, such as autonomy and healthcare. Prior work has made Bayesian deep learning-based approaches to this problem more tractable by performing inference over the low-rank adaptation (LoRA) parameters of a fine-tuned model. While effective, these approaches struggle to scale to larger LLMs due to requiring further additional parameters compared to LoRA. In this work we present $\textbf{Scala}$ble $\textbf{B}$ayesian $\textbf{L}$ow-Rank Adaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform Bayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By repurposing the LoRA parameters as projection matrices, we are able to map samples from this subspace into the full weight space of the LLM. This allows us to learn all the parameters of our approach using stochastic variational inference. Despite the low dimensionality of our subspace, we are able to achieve competitive performance with state-of-the-art approaches while only requiring ${\sim}1000$ additional parameters. Furthermore, it allows us to scale up to the largest Bayesian LLM to date, with four times as a many base parameters as prior work.

**Link**: [arxiv](http://arxiv.org/abs/2506.21408v1),  [pdf](http://arxiv.org/pdf/2506.21408v1)

**Tags**: cs.LG cs.AI cs.CL 



### DiffuCoder: Understanding and Improving Masked Diffusion Models for Code   Generation
**Authors**: Shansan Gong, Ruixiang Zhang, Huangjie Zheng, Jiatao Gu, Navdeep Jaitly, Lingpeng Kong, Yizhe Zhang

**Updated**: 2025-06-26T15:46:40Z

**Summary**: Diffusion large language models (dLLMs) are compelling alternatives to autoregressive (AR) models because their denoising models operate over the entire sequence. The global planning and iterative refinement features of dLLMs are particularly useful for code generation. However, current training and inference mechanisms for dLLMs in coding are still under-explored. To demystify the decoding behavior of dLLMs and unlock their potential for coding, we systematically investigate their denoising processes and reinforcement learning (RL) methods. We train a 7B dLLM, \textbf{DiffuCoder}, on 130B tokens of code. Using this model as a testbed, we analyze its decoding behavior, revealing how it differs from that of AR models: (1) dLLMs can decide how causal their generation should be without relying on semi-AR decoding, and (2) increasing the sampling temperature diversifies not only token choices but also their generation order. This diversity creates a rich search space for RL rollouts. For RL training, to reduce the variance of token log-likelihood estimates and maintain training efficiency, we propose \textbf{coupled-GRPO}, a novel sampling scheme that constructs complementary mask noise for completions used in training. In our experiments, coupled-GRPO significantly improves DiffuCoder's performance on code generation benchmarks (+4.4\% on EvalPlus) and reduces reliance on AR bias during decoding. Our work provides deeper insight into the machinery of dLLM generation and offers an effective, diffusion-native RL training framework. https://github.com/apple/ml-diffucoder.

**Link**: [arxiv](http://arxiv.org/abs/2506.20639v2),  [pdf](http://arxiv.org/pdf/2506.20639v2)

**Tags**: cs.CL 



### Early Stopping Tabular In-Context Learning
**Authors**: Jaris Küken, Lennart Purucker, Frank Hutter

**Updated**: 2025-06-26T15:36:37Z

**Summary**: Tabular foundation models have shown strong performance across various tabular learning tasks via in-context learning, offering robust generalization without any downstream finetuning. However, their inference-time costs remain high, particularly for larger datasets. To address this, we propose early-stopping the in-context learning process. We achieve this by dynamically evaluating whether to stop in-context learning after each Transformer encoder layer. Once stopped, we decode the embedding using a pre-trained layer-wise decoder. Experiments across 34 small classification tasks size show that early stopping in-context learning accelerates inference by up to x1.3 with negligible degradation in predictive performance. To assess scalability, we further evaluate our method on five larger classification tasks, achieving speedups of up to x2.2. Our results demonstrate the potential of early exiting as an effective and practical strategy for improving the efficiency of tabular in-context learning.

**Link**: [arxiv](http://arxiv.org/abs/2506.21387v1),  [pdf](http://arxiv.org/pdf/2506.21387v1)

**Tags**: cs.LG 



### Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented   Generation
**Authors**: Guanting Dong, Xiaoxi Li, Yuyao Zhang, Mengjie Deng

**Updated**: 2025-06-26T15:35:12Z

**Summary**: Real-world live retrieval-augmented generation (RAG) systems face significant challenges when processing user queries that are often noisy, ambiguous, and contain multiple intents. While RAG enhances large language models (LLMs) with external knowledge, current systems typically struggle with such complex inputs, as they are often trained or evaluated on cleaner data. This paper introduces Omni-RAG, a novel framework designed to improve the robustness and effectiveness of RAG systems in live, open-domain settings. Omni-RAG employs LLM-assisted query understanding to preprocess user inputs through three key modules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs with tailored prompts to denoise queries (e.g., correcting spelling errors) and decompose multi-intent queries into structured sub-queries; (2) Intent-Aware Knowledge Retrieval, which performs retrieval for each sub-query from a corpus (i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking and Generation, where a reranker (i.e., BGE) refines document selection before a final response is generated by an LLM (i.e., Falcon-10B) using a chain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG capabilities and the demands of real-world applications, such as those highlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex and noisy queries.

**Link**: [arxiv](http://arxiv.org/abs/2506.21384v1),  [pdf](http://arxiv.org/pdf/2506.21384v1)

**Tags**: cs.CL cs.AI cs.IR 



### Aligned Novel View Image and Geometry Synthesis via Cross-modal   Attention Instillation
**Authors**: Min-Seop Kwak, Junho Kim, Sangdoo Yun, Dongyoon Han, Taekyoung Kim, Seungryong Kim, Jin-Hwa Kim

**Updated**: 2025-06-26T15:26:54Z

**Summary**: We introduce a diffusion-based framework that performs aligned novel view image and geometry generation via a warping-and-inpainting methodology. Unlike prior methods that require dense posed images or pose-embedded generative models limited to in-domain views, our method leverages off-the-shelf geometry predictors to predict partial geometries viewed from reference images, and formulates novel-view synthesis as an inpainting task for both image and geometry. To ensure accurate alignment between generated images and geometry, we propose cross-modal attention distillation, where attention maps from the image diffusion branch are injected into a parallel geometry diffusion branch during both training and inference. This multi-task approach achieves synergistic effects, facilitating geometrically robust image synthesis as well as well-defined geometry prediction. We further introduce proximity-based mesh conditioning to integrate depth and normal cues, interpolating between point cloud and filtering erroneously predicted geometry from influencing the generation process. Empirically, our method achieves high-fidelity extrapolative view synthesis on both image and geometry across a range of unseen scenes, delivers competitive reconstruction quality under interpolation settings, and produces geometrically aligned colored point clouds for comprehensive 3D completion. Project page is available at https://cvlab-kaist.github.io/MoAI.

**Link**: [arxiv](http://arxiv.org/abs/2506.11924v2),  [pdf](http://arxiv.org/pdf/2506.11924v2)

**Tags**: cs.CV 



### Large Language Model-Powered Agent for C to Rust Code Translation
**Authors**: HoHyun Sim, Hyeonjoong Cho, Yeonghyeon Go, Zhoulai Fu, Ali Shokri, Binoy Ravindran

**Updated**: 2025-06-26T15:16:53Z

**Summary**: The C programming language has been foundational in building system-level software. However, its manual memory management model frequently leads to memory safety issues. In response, a modern system programming language, Rust, has emerged as a memory-safe alternative. Moreover, automating the C-to-Rust translation empowered by the rapid advancements of the generative capabilities of LLMs is gaining growing interest for large volumes of legacy C code. Despite some success, existing LLM-based approaches have constrained the role of LLMs to static prompt-response behavior and have not explored their agentic problem-solving capability. Applying the LLM agentic capability for the C-to-Rust translation introduces distinct challenges, as this task differs from the traditional LLM agent applications, such as math or commonsense QA domains. First, the scarcity of parallel C-to-Rust datasets hinders the retrieval of suitable code translation exemplars for in-context learning. Second, unlike math or commonsense QA, the intermediate steps required for C-to-Rust are not well-defined. Third, it remains unclear how to organize and cascade these intermediate steps to construct a correct translation trajectory. To address these challenges in the C-to-Rust translation, we propose a novel intermediate step, the Virtual Fuzzing-based equivalence Test (VFT), and an agentic planning framework, the LLM-powered Agent for C-to-Rust code translation (LAC2R). The VFT guides LLMs to identify input arguments that induce divergent behaviors between an original C function and its Rust counterpart and to generate informative diagnoses to refine the unsafe Rust code. LAC2R uses the MCTS to systematically organize the LLM-induced intermediate steps for correct translation. We experimentally demonstrated that LAC2R effectively conducts C-to-Rust translation on large-scale, real-world benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2505.15858v2),  [pdf](http://arxiv.org/pdf/2505.15858v2)

**Tags**: cs.PL cs.SE 



### Counterfactual Voting Adjustment for Quality Assessment and Fairer   Voting in Online Platforms with Helpfulness Evaluation
**Authors**: Chang Liu, Yixin Wang, Moontae Lee

**Updated**: 2025-06-26T15:13:35Z

**Summary**: Efficient access to high-quality information is vital for online platforms. To promote more useful information, users not only create new content but also evaluate existing content, often through helpfulness voting. Although aggregated votes help service providers rank their user content, these votes are often biased by disparate accessibility per position and the cascaded influence of prior votes. For a fairer assessment of information quality, we propose the Counterfactual Voting Adjustment (CVA), a causal framework that accounts for the context in which individual votes are cast. Through preliminary and semi-synthetic experiments, we show that CVA effectively models the position and herding biases, accurately recovering the predefined content quality. In a real experiment, we demonstrate that reranking content based on the learned quality by CVA exhibits stronger alignment with both user sentiment and quality evaluation assessed by GPT-4o, outperforming system rankings based on aggregated votes and model-based rerankings without causal inference. Beyond the individual quality inference, our embeddings offer comparative insights into the behavioral dynamics of expert user groups across 120 major StackExchange communities.

**Link**: [arxiv](http://arxiv.org/abs/2506.21362v1),  [pdf](http://arxiv.org/pdf/2506.21362v1)

**Tags**: cs.CE 



### Structuralist Approach to AI Literary Criticism: Leveraging Greimas   Semiotic Square for Large Language Models
**Authors**: Fangzhou Dong, Yifan Zeng, Yingpeng Sang, Hong Shen

**Updated**: 2025-06-26T15:10:24Z

**Summary**: Large Language Models (LLMs) excel in understanding and generating text but struggle with providing professional literary criticism for works with profound thoughts and complex narratives. This paper proposes GLASS (Greimas Literary Analysis via Semiotic Square), a structured analytical framework based on Greimas Semiotic Square (GSS), to enhance LLMs' ability to conduct in-depth literary analysis. GLASS facilitates the rapid dissection of narrative structures and deep meanings in narrative works. We propose the first dataset for GSS-based literary criticism, featuring detailed analyses of 48 works. Then we propose quantitative metrics for GSS-based literary criticism using the LLM-as-a-judge paradigm. Our framework's results, compared with expert criticism across multiple works and LLMs, show high performance. Finally, we applied GLASS to 39 classic works, producing original and high-quality analyses that address existing research gaps. This research provides an AI-based tool for literary research and education, offering insights into the cognitive mechanisms underlying literary engagement.

**Link**: [arxiv](http://arxiv.org/abs/2506.21360v1),  [pdf](http://arxiv.org/pdf/2506.21360v1)

**Tags**: cs.CL 



### Semantic Preprocessing for LLM-based Malware Analysis
**Authors**: Benjamin Marais, Tony Quertier, Grégoire Barrue

**Updated**: 2025-06-26T15:09:42Z

**Summary**: In a context of malware analysis, numerous approaches rely on Artificial Intelligence to handle a large volume of data. However, these techniques focus on data view (images, sequences) and not on an expert's view. Noticing this issue, we propose a preprocessing that focuses on expert knowledge to improve malware semantic analysis and result interpretability. We propose a new preprocessing method which creates JSON reports for Portable Executable files. These reports gather features from both static and behavioral analysis, and incorporate packer signature detection, MITRE ATT\&CK and Malware Behavior Catalog (MBC) knowledge. The purpose of this preprocessing is to gather a semantic representation of binary files, understandable by malware analysts, and that can enhance AI models' explainability for malicious files analysis. Using this preprocessing to train a Large Language Model for Malware classification, we achieve a weighted-average F1-score of 0.94 on a complex dataset, representative of market reality.

**Link**: [arxiv](http://arxiv.org/abs/2506.12113v3),  [pdf](http://arxiv.org/pdf/2506.12113v3)

**Tags**: cs.CR cs.AI 



### Mr. DETR++: Instructive Multi-Route Training for Detection Transformers   with Mixture-of-Experts
**Authors**: Chang-Bin Zhang, Yujie Zhong, Kai Han

**Updated**: 2025-06-26T15:06:14Z

**Summary**: Existing methods enhance the training of detection transformers by incorporating an auxiliary one-to-many assignment. In this work, we treat the model as a multi-task framework, simultaneously performing one-to-one and one-to-many predictions. We investigate the roles of each component in the transformer decoder across these two training targets, including self-attention, cross-attention, and feed-forward network. Our empirical results demonstrate that any independent component in the decoder can effectively learn both targets simultaneously, even when other components are shared. This finding leads us to propose a multi-route training mechanism, featuring a primary route for one-to-one prediction and two auxiliary training routes for one-to-many prediction. We propose a novel instructive self-attention mechanism, integrated into the first auxiliary route, which dynamically and flexibly guides object queries for one-to-many prediction. For the second auxiliary route, we introduce a route-aware Mixture-of-Experts (MoE) to facilitate knowledge sharing while mitigating potential conflicts between routes. Additionally, we apply an MoE to low-scale features in the encoder, optimizing the balance between efficiency and effectiveness. The auxiliary routes are discarded during inference. We conduct extensive experiments across various object detection baselines, achieving consistent improvements as demonstrated in Fig. 1. Our method is highly flexible and can be readily adapted to other tasks. To demonstrate its versatility, we conduct experiments on both instance segmentation and panoptic segmentation, further validating its effectiveness. Project page: https://visual-ai.github.io/mrdetr/

**Link**: [arxiv](http://arxiv.org/abs/2412.10028v4),  [pdf](http://arxiv.org/pdf/2412.10028v4)

**Tags**: cs.CV 



### Regression for Astronomical Data with Realistic Distributions, Errors   and Non-linearity
**Authors**: Tao Jing, Cheng Li

**Updated**: 2025-06-26T15:04:59Z

**Summary**: We have developed a new regression technique, the maximum likelihood (ML)-based method and its variant, the KS-test based method, designed to obtain unbiased regression results from typical astronomical data. A normalizing flow model is employed to automatically estimate the unobservable intrinsic distribution of the independent variable as well as the unobservable correlation between uncertainty level and intrinsic value of both independent and dependent variables from the observed data points in a variational inference based empirical Bayes approach. By incorporating these estimated distributions, our method comprehensively accounts for the uncertainties associated with both independent and dependent variables. Our test on both mock data and real astronomical data from PHANGS-ALMA and PHANGS-JWST demonstrates that, given a sufficiently large sample size (> 1000), both the ML-based method and the KS-test based method significantly outperform the existing widely-used methods, particularly in cases of low signal-to-noise ratios. The KS-test based method exhibits remarkable robustness against deviations from underlying assumptions, complex intrinsic distributions, varying correlations between uncertainty levels and intrinsic values, inaccuracies in uncertainty estimations, outliers, and saturation effects. For sample sizes between 300 and 1000, the ML-based method yields the best performance. In the low data regime (< 300), the ML-based method maintains comparable performance to other state-of-the-art methods. A GPU-compatible Python implementation of our methods, nicknamed ``raddest'', will be made publicly available upon acceptance of this paper.

**Link**: [arxiv](http://arxiv.org/abs/2411.08747v2),  [pdf](http://arxiv.org/pdf/2411.08747v2)

**Tags**: astro-ph.IM astro-ph.GA 



### Amplifying microwave pulses with a single qubit engine fueled by quantum   measurements
**Authors**: Rémy Dassonneville, Cyril Elouard, Romain Cazali, Réouven Assouly, Audrey Bienfait, Alexia Auffèves, Benjamin Huard

**Updated**: 2025-06-26T15:04:03Z

**Summary**: Recent progress in manipulating individual quantum systems enables the exploration of engines exploiting non-classical resources. One of the most appealing is the energy provided by the inherent backaction of quantum measurements. While a handful of experiments have investigated the inner dynamics of engines fueled by measurement backaction, powering a useful task by such an engine is missing. Here we demonstrate the amplification of microwave signals by an engine fueled by repeated quantum measurements of a superconducting transmon qubit. Using feedback, the engine acts as a quantum Maxwell demon operating without a hot thermal source. Measuring the gain of this amplification constitutes a direct probing of the work output of the engine, in contrast with inferring the work by measuring the qubit state along its evolution. Observing a good agreement between both work estimation methods, our experiment validates the accuracy of the indirect method. We characterize the long-term stability of the engine as well as its robustness to transmon decoherence, loss and drifts. Our experiment exemplifies a practical usage of the energy brought by quantum measurement backaction.

**Link**: [arxiv](http://arxiv.org/abs/2501.17069v2),  [pdf](http://arxiv.org/pdf/2501.17069v2)

**Tags**: quant-ph cond-mat.mes-hall 



### Nonlinear coupling between magnetar QPOs
**Authors**: Pantelis Pnigouras, Samuel K. Lander

**Updated**: 2025-06-26T15:00:54Z

**Summary**: The quasi-periodic oscillations (QPOs) observed in the tails of magnetar giant $\gamma$-ray flares have long been interpreted as normal oscillation modes of these stars. However, most studies modelling QPOs have neglected some key features in the analyses of the signals, namely that QPOs appear to be detectable only intermittently and exhibit drifts in their frequencies. These are typical characteristics of nonlinear mode coupling, where, at leading order, the modes couple and evolve collectively as triplets. Using a representative triplet of modes, we solve the system's nonlinear equations of motion analytically and argue that the coupling is likely axial-axial-polar in nature, with the observed intermittence and frequency drifts providing a way to infer details of the magnetar's internal magnetic field geometry.

**Link**: [arxiv](http://arxiv.org/abs/2501.04556v2),  [pdf](http://arxiv.org/pdf/2501.04556v2)

**Tags**: astro-ph.HE astro-ph.SR gr-qc 



### PuriDefense: Randomized Local Implicit Adversarial Purification for   Defending Black-box Query-based Attacks
**Authors**: Ping Guo, Xiang Li, Zhiyuan Yang, Xi Lin, Qingchuan Zhao, Qingfu Zhang

**Updated**: 2025-06-26T15:00:42Z

**Summary**: Black-box query-based attacks constitute significant threats to Machine Learning as a Service (MLaaS) systems since they can generate adversarial examples without accessing the target model's architecture and parameters. Traditional defense mechanisms, such as adversarial training, gradient masking, and input transformations, either impose substantial computational costs or compromise the test accuracy of non-adversarial inputs. To address these challenges, we propose an efficient defense mechanism, PuriDefense, that employs random patch-wise purifications with an ensemble of lightweight purification models at a low level of inference cost. These models leverage the local implicit function and rebuild the natural image manifold. Our theoretical analysis suggests that this approach slows down the convergence of query-based attacks by incorporating randomness into purifications. Extensive experiments on CIFAR-10 and ImageNet validate the effectiveness of our proposed purifier-based defense mechanism, demonstrating significant improvements in robustness against query-based attacks.

**Link**: [arxiv](http://arxiv.org/abs/2401.10586v2),  [pdf](http://arxiv.org/pdf/2401.10586v2)

**Tags**: cs.CR cs.AI cs.CV cs.LG 



### DynamicBench: Evaluating Real-Time Report Generation in Large Language   Models
**Authors**: Jingyao Li, Hao Sun, Zile Qiao, Yong Jiang, Pengjun Xie, Fei Huang, Hong Xu, Jiaya Jia

**Updated**: 2025-06-26T14:53:44Z

**Summary**: Traditional benchmarks for large language models (LLMs) typically rely on static evaluations through storytelling or opinion expression, which fail to capture the dynamic requirements of real-time information processing in contemporary applications. To address this limitation, we present DynamicBench, a benchmark designed to evaluate the proficiency of LLMs in storing and processing up-to-the-minute data. DynamicBench utilizes a dual-path retrieval pipeline, integrating web searches with local report databases. It necessitates domain-specific knowledge, ensuring accurate responses report generation within specialized fields. By evaluating models in scenarios that either provide or withhold external documents, DynamicBench effectively measures their capability to independently process recent information or leverage contextual enhancements. Additionally, we introduce an advanced report generation system adept at managing dynamic information synthesis. Our experimental results confirm the efficacy of our approach, with our method achieving state-of-the-art performance, surpassing GPT4o in document-free and document-assisted scenarios by 7.0% and 5.8%, respectively. The code and data will be made publicly available.

**Link**: [arxiv](http://arxiv.org/abs/2506.21343v1),  [pdf](http://arxiv.org/pdf/2506.21343v1)

**Tags**: cs.LG 



### AGTCNet: A Graph-Temporal Approach for Principled Motor Imagery EEG   Classification
**Authors**: Galvin Brice S. Lim, Brian Godwin S. Lim, Argel A. Bandala, John Anthony C. Jose, Timothy Scott C. Chu, Edwin Sybingco

**Updated**: 2025-06-26T14:49:10Z

**Summary**: Brain-computer interface (BCI) technology utilizing electroencephalography (EEG) marks a transformative innovation, empowering motor-impaired individuals to engage with their environment on equal footing. Despite its promising potential, developing subject-invariant and session-invariant BCI systems remains a significant challenge due to the inherent complexity and variability of neural activity across individuals and over time, compounded by EEG hardware constraints. While prior studies have sought to develop robust BCI systems, existing approaches remain ineffective in capturing the intricate spatiotemporal dependencies within multichannel EEG signals. This study addresses this gap by introducing the attentive graph-temporal convolutional network (AGTCNet), a novel graph-temporal model for motor imagery EEG (MI-EEG) classification. Specifically, AGTCNet leverages the topographic configuration of EEG electrodes as an inductive bias and integrates graph convolutional attention network (GCAT) to jointly learn expressive spatiotemporal EEG representations. The proposed model significantly outperformed existing MI-EEG classifiers, achieving state-of-the-art performance while utilizing a compact architecture, underscoring its effectiveness and practicality for BCI deployment. With a 49.87% reduction in model size, 64.65% faster inference time, and shorter input EEG signal, AGTCNet achieved a moving average accuracy of 66.82% for subject-independent classification on the BCI Competition IV Dataset 2a, which further improved to 82.88% when fine-tuned for subject-specific classification. On the EEG Motor Movement/Imagery Dataset, AGTCNet achieved moving average accuracies of 64.14% and 85.22% for 4-class and 2-class subject-independent classifications, respectively, with further improvements to 72.13% and 90.54% for subject-specific classifications.

**Link**: [arxiv](http://arxiv.org/abs/2506.21338v1),  [pdf](http://arxiv.org/pdf/2506.21338v1)

**Tags**: cs.LG cs.HC 



### Active Inference AI Systems for Scientific Discovery
**Authors**: Karthik Duraisamy

**Updated**: 2025-06-26T14:43:04Z

**Summary**: The rapid evolution of artificial intelligence has led to expectations of transformative scientific discovery, yet current systems remain fundamentally limited by their operational architectures, brittle reasoning mechanisms, and their separation from experimental reality. Building on earlier work, we contend that progress in AI-driven science now depends on closing three fundamental gaps -- the abstraction gap, the reasoning gap, and the reality gap -- rather than on model size/data/test time compute. Scientific reasoning demands internal representations that support simulation of actions and response, causal structures that distinguish correlation from mechanism, and continuous calibration. We define active inference AI systems for scientific discovery as those that (i) maintain long-lived research memories grounded in causal self-supervised foundation models, (ii) symbolic or neuro-symbolic planners equipped with Bayesian guardrails, (iii) grow persistent knowledge graphs where thinking generates novel conceptual nodes, reasoning establishes causal edges, and real-world interaction prunes false connections while strengthening verified pathways, and (iv) refine their internal representations through closed-loop interaction with both high-fidelity simulators and automated laboratories - an operational loop where mental simulation guides action and empirical surprise reshapes understanding. In essence, we outline an architecture where discovery arises from the interplay between internal models that enable counterfactual reasoning and external validation that grounds hypotheses in reality. It is also argued that the inherent ambiguity in feedback from simulations and experiments, and underlying uncertainties makes human judgment indispensable, not as a temporary scaffold but as a permanent architectural component.

**Link**: [arxiv](http://arxiv.org/abs/2506.21329v1),  [pdf](http://arxiv.org/pdf/2506.21329v1)

**Tags**: cs.AI physics.soc-ph 68 I.2 



### Is checking for sequential positivity violations getting you down? Try   sPoRT!
**Authors**: Arthur Chatton, Michael Schomaker, Miguel-Angel Luque-Fernandez, Robert W. Platt, Mireille E. Schnitzer

**Updated**: 2025-06-26T14:41:55Z

**Summary**: Background: Sequential positivity is often a necessary assumption for drawing causal inferences, such as through marginal structural modeling. Unfortunately, verification of this assumption can be challenging because it usually relies on multiple parametric propensity score models, unlikely all correctly specified. Therefore, we propose a new algorithm, called sequential Positivity Regression Tree (sPoRT), to overcome this issue and identify the subgroups found to be violating this assumption, allowing for insights about the nature of the violations and potential solutions.   Methods: We present different versions of sPoRT based on either stratifying or pooling over time under static or dynamic treatment strategies. This methodological development was motivated by a real-life application of the impact of the timing of initiation of HIV treatment with and without smoothing over time, which we also use to demonstrate the method.   Results: The illustration of sPoRT demonstrates its easy use and the interpretability of the results for applied epidemiologists. Furthermore, an R notebook showing how to use sPoRT in practice is available at github.com/ArthurChatton/sPoRT-notebook.   Conclusions: The sPoRT algorithm provides interpretable subgroups violating the sequential positivity violation, allowing patterns and trends in the confounders to be easily identified. We finally provided practical implications and recommendations when positivity violations are identified.

**Link**: [arxiv](http://arxiv.org/abs/2412.10245v3),  [pdf](http://arxiv.org/pdf/2412.10245v3)

**Tags**: stat.ME 



### Latent Prototype Routing: Achieving Near-Perfect Load Balancing in   Mixture-of-Experts
**Authors**: Jiajie Yang

**Updated**: 2025-06-26T14:41:18Z

**Summary**: Mixture-of-Experts (MoE) architectures have emerged as a key strategy for scaling large language models (LLMs) efficiently. However, current MoE systems suffer from severe load imbalance, where only a small subset of experts is consistently activated during training and inference, leading to significant underutilization of model capacity and computational resources. In this work, we revisit expert routing through a clustering perspective and propose Latent Prototype Routing (LPR), a novel routing framework that generalizes existing approaches while promoting balanced expert utilization without compromising downstream performance. Extensive experiments across multiple open-source MoE models -- including DeepSeek-V3, Qwen3-MoE, and Mixtral -- demonstrate that LPR reduces the Gini coefficient of expert load from 0.70 to 0.035 on average, improves the min-max expert load ratio from 1e-6 to 0.70, achieving near-perfect load balancing.

**Link**: [arxiv](http://arxiv.org/abs/2506.21328v1),  [pdf](http://arxiv.org/pdf/2506.21328v1)

**Tags**: cs.LG cs.CL 



### Multimodal LLMs for Visualization Reconstruction and Understanding
**Authors**: Can Liu, Chunlin Da, Xiaoxiao Long, Yuxiao Yang, Yu Zhang, Yong Wang

**Updated**: 2025-06-26T14:35:59Z

**Summary**: Visualizations are crucial for data communication, yet understanding them requires comprehension of both visual elements and their underlying data relationships. Current multimodal large models, while effective in natural image understanding, struggle with visualization due to their inability to decode the data-to-visual mapping rules and extract structured information. To address these challenges, we present a novel dataset and train multimodal visualization LLMs specifically designed for understanding. Our approach combines chart images with their corresponding vectorized representations, encoding schemes, and data features. The proposed vector format enables compact and accurate reconstruction of visualization content. Experimental results demonstrate significant improvements in both data extraction accuracy and chart reconstruction quality.

**Link**: [arxiv](http://arxiv.org/abs/2506.21319v1),  [pdf](http://arxiv.org/pdf/2506.21319v1)

**Tags**: cs.HC cs.CV 



### Data-Aware Hybrid Tableaux
**Authors**: Carlos Areces, Valentin Cassano, Raul Fervari

**Updated**: 2025-06-26T14:29:22Z

**Summary**: Labelled tableaux have been a traditional approach to define satisfiability checking procedures for Modal Logics. In many cases, they can also be used to obtain tight complexity bounds and lead to efficient implementations of reasoning tools. More recently, it has been shown that the expressive power provided by the operators characterizing Hybrid Logics (nominals and satisfiability modalities) can be used to \emph{internalize} labels, leading to well-behaved inference procedures for fairly expressive logics. The resulting procedures are attractive because they do not use external mechanisms outside the language of the logic at hand, and have good logical and computational properties.   Many tableau systems based on Hybrid Logic have been investigated, with more recent efforts concentrating on Modal Logics that support data comparison operators. Here, we introduce an internalized tableau calculus for XPath, arguably one of the most prominent approaches for querying semistructured data. More precisely, we define data-aware tableaux for XPath featuring data comparison operators and enriched with nominals and the satisfiability modalities from Hybrid Logic. We prove that the calculus is sound, complete and terminating. Moreover, we show that tableaux can be explored in polynomial space, therefore establishing that the satisfiability problem for the logic is PSPACE-complete. Finally, we explore different extensions of the calculus, in particular how to handle data trees and other frame classes.

**Link**: [arxiv](http://arxiv.org/abs/2406.12090v2),  [pdf](http://arxiv.org/pdf/2406.12090v2)

**Tags**: cs.LO F.4.1; I.2.4 



### Nonparametric Bayesian analysis for the Galton-Watson process
**Authors**: Massimo Cannas, Michele Guindani, Nicola Piras

**Updated**: 2025-06-26T14:23:29Z

**Summary**: The Galton-Watson process is a model for population growth which assumes that individuals reproduce independently according to the same offspring distribution. Inference usually focuses on the offspring average as it allows to classify the process with respect to extinction. We propose a fully non-parametric approach for Bayesian inference on the GW model using a Dirichlet Process prior. The prior naturally generalizes the Dirichlet conjugate prior distribution, and it allows learning the support of the offspring distribution from the data as well as taking into account possible overdispersion of the data. The performance of the proposed approach is compared with both frequentist and Bayesian procedures via simulation. In particular, we show that the use of a DP prior yields good classification performance with both complete and incomplete data. A real-world data example concerning COVID-19 data from Sardinia illustrates the use of the approach in practice.

**Link**: [arxiv](http://arxiv.org/abs/2506.21304v1),  [pdf](http://arxiv.org/pdf/2506.21304v1)

**Tags**: stat.ME stat.CO 62G05, 62F15 



### Semantic Scene Graph for Ultrasound Image Explanation and Scanning   Guidance
**Authors**: Xuesong Li, Dianye Huang, Yameng Zhang, Nassir Navab, Zhongliang Jiang

**Updated**: 2025-06-26T14:20:13Z

**Summary**: Understanding medical ultrasound imaging remains a long-standing challenge due to significant visual variability caused by differences in imaging and acquisition parameters. Recent advancements in large language models (LLMs) have been used to automatically generate terminology-rich summaries orientated to clinicians with sufficient physiological knowledge. Nevertheless, the increasing demand for improved ultrasound interpretability and basic scanning guidance among non-expert users, e.g., in point-of-care settings, has not yet been explored. In this study, we first introduce the scene graph (SG) for ultrasound images to explain image content to ordinary and provide guidance for ultrasound scanning. The ultrasound SG is first computed using a transformer-based one-stage method, eliminating the need for explicit object detection. To generate a graspable image explanation for ordinary, the user query is then used to further refine the abstract SG representation through LLMs. Additionally, the predicted SG is explored for its potential in guiding ultrasound scanning toward missing anatomies within the current imaging view, assisting ordinary users in achieving more standardized and complete anatomical exploration. The effectiveness of this SG-based image explanation and scanning guidance has been validated on images from the left and right neck regions, including the carotid and thyroid, across five volunteers. The results demonstrate the potential of the method to maximally democratize ultrasound by enhancing its interpretability and usability for ordinaries.

**Link**: [arxiv](http://arxiv.org/abs/2506.19683v2),  [pdf](http://arxiv.org/pdf/2506.19683v2)

**Tags**: cs.CV cs.AI cs.LG eess.IV 



### Detecting Referring Expressions in Visually Grounded Dialogue with   Autoregressive Language Models
**Authors**: Bram Willemsen, Gabriel Skantze

**Updated**: 2025-06-26T14:14:20Z

**Summary**: In this paper, we explore the use of a text-only, autoregressive language modeling approach for the extraction of referring expressions from visually grounded dialogue. More specifically, the aim is to investigate the extent to which the linguistic context alone can inform the detection of mentions that have a (visually perceivable) referent in the visual context of the conversation. To this end, we adapt a pretrained large language model (LLM) to perform a relatively course-grained annotation of mention spans in unfolding conversations by demarcating mention span boundaries in text via next-token prediction. Our findings indicate that even when using a moderately sized LLM, relatively small datasets, and parameter-efficient fine-tuning, a text-only approach can be effective, highlighting the relative importance of the linguistic context for this task. Nevertheless, we argue that the task represents an inherently multimodal problem and discuss limitations fundamental to unimodal approaches.

**Link**: [arxiv](http://arxiv.org/abs/2506.21294v1),  [pdf](http://arxiv.org/pdf/2506.21294v1)

**Tags**: cs.CL cs.AI 



### Small Encoders Can Rival Large Decoders in Detecting Groundedness
**Authors**: Istabrak Abbes, Gabriele Prato, Quentin Fournier, Fernando Rodriguez, Alaa Boukhary, Adam Elwood, Sarath Chandar

**Updated**: 2025-06-26T14:09:41Z

**Summary**: Augmenting large language models (LLMs) with external context significantly improves their performance in natural language processing (NLP) tasks. However, LLMs struggle to answer queries reliably when the provided context lacks information, often resorting to ungrounded speculation or internal knowledge. Groundedness - generating responses strictly supported by the context - is essential for ensuring factual consistency and trustworthiness. This study focuses on detecting whether a given query is grounded in a document provided in context before the costly answer generation by LLMs. Such a detection mechanism can significantly reduce both inference time and resource consumption. We show that lightweight, task specific encoder models such as RoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy comparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in groundedness detection while reducing inference latency by orders of magnitude. The code is available at : https://github.com/chandarlab/Hallucinate-less

**Link**: [arxiv](http://arxiv.org/abs/2506.21288v1),  [pdf](http://arxiv.org/pdf/2506.21288v1)

**Tags**: cs.CL cs.AI cs.IR cs.LG 



### Thinkless: LLM Learns When to Think
**Authors**: Gongfan Fang, Xinyin Ma, Xinchao Wang

**Updated**: 2025-06-26T14:06:49Z

**Summary**: Reasoning Language Models, capable of extended chain-of-thought reasoning, have demonstrated remarkable performance on tasks requiring complex logical inference. However, applying elaborate reasoning for all queries often results in substantial computational inefficiencies, particularly when many problems admit straightforward solutions. This motivates an open question: Can LLMs learn when to think? To answer this, we propose Thinkless, a learnable framework that empowers an LLM to adaptively select between short-form and long-form reasoning, based on both task complexity and the model's ability. Thinkless is trained under a reinforcement learning paradigm and employs two control tokens, <short> for concise responses and <think> for detailed reasoning. At the core of our method is a Decoupled Group Relative Policy Optimization (DeGRPO) algorithm, which decomposes the learning objective of hybrid reasoning into two components: (1) a control token loss that governs the selection of the reasoning mode, and (2) a response loss that improves the accuracy of the generated answers. This decoupled formulation enables fine-grained control over the contributions of each objective, stabilizing training and effectively preventing collapse observed in vanilla GRPO. Empirically, on several benchmarks such as Minerva Algebra, MATH-500, and GSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% - 90%, significantly improving the efficiency of Reasoning Language Models. The code is available at https://github.com/VainF/Thinkless

**Link**: [arxiv](http://arxiv.org/abs/2505.13379v2),  [pdf](http://arxiv.org/pdf/2505.13379v2)

**Tags**: cs.CL cs.AI 



### Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via   Self-Critical Fine-Tuning
**Authors**: Xin Xu, Tianhao Chen, Fan Zhang, Wanlong Liu, Pengxiang Li, Ajay Kumar Jaiswal, Yuchen Yan, Jishan Hu, Yang Wang, Hao Chen, Shiwei Liu, Shizhe Diao, Can Yang, Lu Yin

**Updated**: 2025-06-26T14:05:45Z

**Summary**: While slow-thinking large language models (LLMs) exhibit reflection-like reasoning, commonly referred to as the "aha moment:, their ability to generate informative critiques and refine prior solutions remains limited. In this paper, we introduce Double-Checker, a principled framework designed to enhance the reasoning capabilities of slow-thinking LLMs by fostering explicit self-critique and iterative refinement of their previous solutions. By fine-tuning on our curated 1,730 self-critical instances, Double-Checker empowers long-CoT LLMs to iteratively critique and refine their outputs during inference until they evaluate their solutions as correct under self-generated critiques. We validate the efficacy of Double-Checker across a comprehensive suite of reasoning benchmarks, demonstrating that iterative self-critique significantly enhances the reasoning capabilities of long-CoT LLMs. Notably, our Double-Checker increases the pass@1 performance on challenging AIME benchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These results highlight a promising direction for developing more trustworthy and effective LLMs capable of structured self-critique.

**Link**: [arxiv](http://arxiv.org/abs/2506.21285v1),  [pdf](http://arxiv.org/pdf/2506.21285v1)

**Tags**: cs.CL 



### Wide-field Polarization Imaging and Numerical Modeling of the Coma and   Tail of Comet C/2023 A3 (Tsuchinshan-ATLAS)
**Authors**: Mirza Arnaut, Christian Wöhler, Pritish Halder, Goldy Ahuja, Shashikiran Ganesh, Megha Bhatt

**Updated**: 2025-06-26T14:05:27Z

**Summary**: Imaging polarimetry is a well-known method for analysing the structure and composition of cometary dust. Accordingly, comets are classified based on the phase angle dependent degree of linear polarization of their comae. The goal of this study is to examine the size and structure of the dust grains in the coma and in particular in the tail of the bright comet C/2023 A3 (Tsuchinshan-ATLAS) and to infer possible variations. For this purpose, we rely on the method of telescopic wide-field polarimetric imaging of the comet in order to obtain the dependence of the degree of linear polarization (DoLP) of the coma and tail on the phase angle across a broad range, using an off-the-shelf industrial grade polarization camera combined with a small telescope of short aperture ratio. These observations are accompanied by T-matrix modeling using the MSTM4 software framework for simulation of light scattering by fractal agglomerates of spherical monomer particles. Our observations indicate that the coma exhibits a high maximum DoLP of 0.34, which is further exceeded by a factor of about two by the DoLP of the comet's tail. Differences in the phase angle of maximum DoLP between coma and tail indicate a higher carbon vs.\ silicate fraction in the coma than in the tail. Our simulations are able to reproduce the observations when assuming that the dust agglomerates are formed of larger monomers in the coma than in the tail. A possible explanation of these results is that the coma monomers are coated by a thin layer of frozen volatiles, which sublimate due to solar heating when the dust moves from the coma towards the tail.

**Link**: [arxiv](http://arxiv.org/abs/2506.21284v1),  [pdf](http://arxiv.org/pdf/2506.21284v1)

**Tags**: astro-ph.EP 



### Flexible and Efficient Estimation of Causal Effects with Error-Prone   Exposures: A Control Variates Approach for Measurement Error
**Authors**: Keith Barnatchez, Rachel Nethery, Bryan E. Shepherd, Giovanni Parmigiani, Kevin P. Josey

**Updated**: 2025-06-26T14:03:03Z

**Summary**: Exposure measurement error is a ubiquitous but often overlooked challenge in causal inference with observational data. Existing methods accounting for exposure measurement error largely rely on restrictive parametric assumptions, while emerging data-adaptive estimation approaches allow for less restrictive assumptions but at the cost of flexibility, as they are typically tailored towards rigidly-defined statistical quantities. There remains a critical need for assumption-lean estimation methods that are both flexible and possess desirable theoretical properties across a variety of study designs. In this paper, we introduce a general framework for estimation of causal quantities in the presence of exposure measurement error, adapted from the control variates approach of Yang and Ding (2019). Our method can be implemented in various two-phase sampling study designs, where one obtains gold-standard exposure measurements for a small subset of the full study sample, called the validation data. The control variates framework leverages both the error-prone and error-free exposure measurements by augmenting an initial consistent estimator from the validation data with a variance reduction term formed from the full data. We show that our method inherits double-robustness properties under standard causal assumptions. Simulation studies show that our approach performs favorably compared to leading methods under various two-phase sampling schemes. We illustrate our method with observational electronic health record data on HIV outcomes from the Vanderbilt Comprehensive Care Clinic.

**Link**: [arxiv](http://arxiv.org/abs/2410.12590v2),  [pdf](http://arxiv.org/pdf/2410.12590v2)

**Tags**: stat.ME 



### HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context
**Authors**: Qize Yang, Shimin Yao, Weixuan Chen, Shenghao Fu, Detao Bai, Jiaxing Zhao, Boyuan Sun, Bowen Yin, Xihan Wei, Jingren Zhou

**Updated**: 2025-06-26T14:01:03Z

**Summary**: With the rapid evolution of multimodal large language models, the capacity to deeply understand and interpret human intentions has emerged as a critical capability, which demands detailed and thoughtful reasoning. In recent studies, Reinforcement Learning (RL) has demonstrated potential in enhancing the reasoning capabilities of Large Language Models (LLMs). Nonetheless, the challenges associated with adapting RL to multimodal data and formats remain largely unaddressed. In this paper, we identify two issues in existing multimodal reasoning models: insufficient global context understanding and shortcut problems. Insufficient context understanding can happen when a model misinterprets multimodal context, resulting in incorrect answers. The shortcut problem occurs when the model overlooks crucial clues in multimodal inputs, directly addressing the query without considering the multimodal information. To tackle these issues, we emphasize the necessity for the model to reason with a clear understanding of the global context within multimodal inputs. This global context understanding can effectively prevent the model from overlooking key multimodal cues and ensure a thorough reasoning process. To ensure the accurate interpretation of multimodal context information, we implement a context reward judged by a large language model, alongside format and accuracy rewards. Additionally, to improve complex reasoning capability, we employ the LLM to assess the logical reward, determining whether the reasoning process successfully integrates multimodal information with logical methods. We also introduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating models in understanding complex human intentions and emotions. Our proposed method demonstrates advanced performance across multiple omni-modal benchmarks compared to other open-source omni-modal models.

**Link**: [arxiv](http://arxiv.org/abs/2506.21277v1),  [pdf](http://arxiv.org/pdf/2506.21277v1)

**Tags**: cs.CV cs.CL 



### Cat and Mouse -- Can Fake Text Generation Outpace Detector Systems?
**Authors**: Andrea McGlinchey, Peter J Barclay

**Updated**: 2025-06-26T13:58:43Z

**Summary**: Large language models can produce convincing "fake text" in domains such as academic writing, product reviews, and political news. Many approaches have been investigated for the detection of artificially generated text. While this may seem to presage an endless "arms race", we note that newer LLMs use ever more parameters, training data, and energy, while relatively simple classifiers demonstrate a good level of detection accuracy with modest resources. To approach the question of whether the models' ability to beat the detectors may therefore reach a plateau, we examine the ability of statistical classifiers to identify "fake text" in the style of classical detective fiction. Over a 0.5 version increase, we found that Gemini showed an increased ability to generate deceptive text, while GPT did not. This suggests that reliable detection of fake text may remain feasible even for ever-larger models, though new model architectures may improve their deceptiveness

**Link**: [arxiv](http://arxiv.org/abs/2506.21274v1),  [pdf](http://arxiv.org/pdf/2506.21274v1)

**Tags**: cs.CL 



### Linear scaling causal discovery from high-dimensional time series by   dynamical community detection
**Authors**: Matteo Allione, Vittorio Del Tatto, Alessandro Laio

**Updated**: 2025-06-26T13:57:37Z

**Summary**: Understanding which parts of a dynamical system cause each other is extremely relevant in fundamental and applied sciences. However, inferring causal links from observational data, namely without direct manipulations of the system, is still computationally challenging, especially if the data are high-dimensional. In this study we introduce a framework for constructing causal graphs from high-dimensional time series, whose computational cost scales linearly with the number of variables. The approach is based on the automatic identification of dynamical communities, groups of variables which mutually influence each other and can therefore be described as a single node in a causal graph. These communities are efficiently identified by optimizing the Information Imbalance, a statistical quantity that assigns a weight to each putative causal variable based on its information content relative to a target variable. The communities are then ordered starting from the fully autonomous ones, whose evolution is independent from all the others, to those that are progressively dependent on other communities, building in this manner a community causal graph. We demonstrate the computational efficiency and the accuracy of our approach on time-discrete and time-continuous dynamical systems including up to 80 variables.

**Link**: [arxiv](http://arxiv.org/abs/2501.10886v2),  [pdf](http://arxiv.org/pdf/2501.10886v2)

**Tags**: physics.data-an stat.ME 



### DiLoCoX: A Low-Communication Large-Scale Training Framework for   Decentralized Cluster
**Authors**: Ji Qi, WenPeng Zhu, Li Li, Ming Wu, YingJun Wu, Wu He, Xun Gao, Jason Zeng, Michael Heinrich

**Updated**: 2025-06-26T13:45:04Z

**Summary**: The distributed training of foundation models, particularly large language models (LLMs), demands a high level of communication. Consequently, it is highly dependent on a centralized cluster with fast and reliable interconnects. Can we conduct training on slow networks and thereby unleash the power of decentralized clusters when dealing with models exceeding 100 billion parameters? In this paper, we propose DiLoCoX, a low-communication large-scale decentralized cluster training framework. It combines Pipeline Parallelism with Dual Optimizer Policy, One-Step-Delay Overlap of Communication and Local Training, and an Adaptive Gradient Compression Scheme. This combination significantly improves the scale of parameters and the speed of model pre-training. We justify the benefits of one-step-delay overlap of communication and local training, as well as the adaptive gradient compression scheme, through a theoretical analysis of convergence. Empirically, we demonstrate that DiLoCoX is capable of pre-training a 107B foundation model over a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x speedup in distributed training while maintaining negligible degradation in model convergence. To the best of our knowledge, this is the first decentralized training framework successfully applied to models with over 100 billion parameters.

**Link**: [arxiv](http://arxiv.org/abs/2506.21263v1),  [pdf](http://arxiv.org/pdf/2506.21263v1)

**Tags**: cs.LG cs.AI cs.CL 



### Ambiguities, Built-in Biases and Flaws in Big Data Insight Extraction
**Authors**: Serge Galam

**Updated**: 2025-06-26T13:43:55Z

**Summary**: I address the challenge of extracting reliable insights from large datasets using a simplified model that illustrates how hierarchical classification can distort outcomes. The model consists of discrete pixels labeled red, blue, or white. Red and blue indicate distinct properties, and white represents unclassified or ambiguous data. A macro-color is assigned only if one color holds a strict majority among the pixels. Otherwise, the aggregate is labeled white, reflecting uncertainty. This setup mimics a percolation threshold at fifty percent. Assuming direct access of the various proportions of colors is infeasible from the data, I implement a hierarchical coarse-graining procedure. Elements (first pixels, then aggregates) are recursively grouped and reclassified via local majority rules, producing ultimately a single super-aggregate whose color represents the inferred macro-property of the collection of pixels as a whole. Analytical results, supported by simulations, show that the process introduces additional white aggregates beyond white pixels, which could be initially present. These arise from groups lacking a clear majority, requiring arbitrary symmetry-breaking decisions to attribute a color to them. While each local resolution may appear minor and inconsequential, their repetitions introduce a growing systematic bias. Even with complete data, unavoidable asymmetries in local rules are shown to skew outcomes. This study highlights a critical limitation of recursive data reduction. Insight extraction is shaped not only by data quality, but by how local ambiguity is handled. That results in built-in biases. Thus, the related flaws are not due to the data, but due to structural choices made during local aggregations. Though based on a simple model, the findings expose the high likelihood of inherent flaws in widely used hierarchical classification techniques.

**Link**: [arxiv](http://arxiv.org/abs/2506.21262v1),  [pdf](http://arxiv.org/pdf/2506.21262v1)

**Tags**: physics.soc-ph cond-mat.stat-mech 



### Low multipole mapmaking for global 21-cm experiments
**Authors**: Yordan D. Ignatov, Jonathan R. Pritchard

**Updated**: 2025-06-26T13:40:32Z

**Summary**: The 21-cm global signal is obscured by very bright galactic and extra galactic foreground emissions. Typical single-spectrum fit (SSF) based methods for foreground/signal separation can result in biased estimates of the cosmological signal due to the presence of spectral oscillations induced by the interaction between chromatic beams and the spatial shape of the foregrounds. Modelling this interaction requires some amount of assumed foreground information. We present a mapmaking-based approach which describes the beam-weighted observation of the sky by multiple globally-distributed antenna experiments as an observation equation. This equation is inverted in order to estimate the low-order sky modes ($\ell\lesssim10$). The resulting chromaticity-free sky monopole is then fit with a smooth foreground function and a 21-cm model. Given the insensitivity of global 21-cm experiments to small angular scales, we rely on the mean and covariance of higher-order foreground modes being known. We show that this mapmaking-based method is capable of inferring the cosmological signal in cases where a SSF with a simple beam-factor based chromaticity correction fails, even when the foreground model used in the mapmaking method features uncertainty at the 10\% level.

**Link**: [arxiv](http://arxiv.org/abs/2506.21258v1),  [pdf](http://arxiv.org/pdf/2506.21258v1)

**Tags**: astro-ph.CO 



### The kangaroo's first hop: the early fast cooling phase of EP250108a/SN   2025kg
**Authors**: Rob A. J. Eyles-Ferris, Peter G. Jonker, Andrew J. Levan, Daniele Bjørn Malesani, Nikhil Sarin, Christopher L. Fryer, Jillian C. Rastinejad, Eric Burns, Nial R. Tanvir, Paul T. O'Brien, Wen-fai Fong, Ilya Mandel, Benjamin P. Gompertz, Charles D. Kilpatrick, Steven Bloemen, Joe S. Bright, Francesco Carotenuto, Gregory Corcoran, Laura Cotter, Paul J. Groot, Luca Izzo, Tanmoy Laskar, Antonio Martin-Carrillo, Jesse Palmerio, Maria E. Ravasio, Jan van Roestel, Andrea Saccardi, Rhaana L. C. Starling, Aishwarya Linesh Thakur, Susanna D. Vergani, Paul M. Vreeswijk, Franz E. Bauer, Sergio Campana, Jennifer A. Chacón, Ashley A. Chrimes, Stefano Covino, Joyce N. D. van Dalen, Valerio D'Elia, Massimiliano De Pasquale, Nusrin Habeeb, Dieter H. Hartmann, Agnes P. C. van Hoof, Páll Jakobsson, Yashaswi Julakanti, Giorgos Leloudas, Daniel Mata Sánchez, Christopher J. Nixon, Daniëlle L. A. Pieterse, Giavanna Pugliese, Jonathan Quirola-Vásquez, Ben C. Rayson, Ruben Salvaterra, Ben Schneider, Manuel A. P. Torres, Tayyaba Zafar

**Updated**: 2025-06-26T13:39:09Z

**Summary**: Fast X-ray transients (FXTs) are a rare and poorly understood population of events. Previously difficult to detect in real time, the launch of the Einstein Probe with its wide field X-ray telescope has led to a rapid expansion in the sample and allowed the exploration of these transients across the electromagnetic spectrum. EP250108a is a recently detected example linked to an optical counterpart, SN 2025kg, or 'the kangaroo'. Together with a companion paper (Rastinejad et al. 2025), we present our observing campaign and analysis of this event. In this letter, we focus on the early evolution of the optical counterpart over the first six days, including our measurement of the redshift of $z=0.17641$. We find that the source is well-modelled by a rapidly expanding cooling blackbody. We show the observed X-ray and radio properties are consistent with a collapsar-powered jet that is low energy ($\lesssim10^{51}$ erg) and/or fails to break out of the dense material surrounding it. While we examine the possibility that the optical emission emerges from the shock produced as the supernova ejecta expand into a dense shell of circumstellar material, due to our X-ray and radio inferences, we favour a model where it arises from a shocked cocoon resulting from the trapped jet. This makes SN 2025kg one of the few examples of this currently observationally rare event.

**Link**: [arxiv](http://arxiv.org/abs/2504.08886v4),  [pdf](http://arxiv.org/pdf/2504.08886v4)

**Tags**: astro-ph.HE 



### Zero-Shot Learning for Obsolescence Risk Forecasting
**Authors**: Elie Saad, Aya Mrabah, Mariem Besbes, Marc Zolghadri, Victor Czmil, Claude Baron, Vincent Bourgeois

**Updated**: 2025-06-26T13:23:57Z

**Summary**: Component obsolescence poses significant challenges in industries reliant on electronic components, causing increased costs and disruptions in the security and availability of systems. Accurate obsolescence risk prediction is essential but hindered by a lack of reliable data. This paper proposes a novel approach to forecasting obsolescence risk using zero-shot learning (ZSL) with large language models (LLMs) to address data limitations by leveraging domain-specific knowledge from tabular datasets. Applied to two real-world datasets, the method demonstrates effective risk prediction. A comparative evaluation of four LLMs underscores the importance of selecting the right model for specific forecasting tasks.

**Link**: [arxiv](http://arxiv.org/abs/2506.21240v1),  [pdf](http://arxiv.org/pdf/2506.21240v1)

**Tags**: cs.LG 



### Optimizing Gaussian Process Kernels Using Nested Sampling and ABC   Rejection for H(z) Reconstruction
**Authors**: Jia-yan Jiang, Kang Jiao, Tong-Jie Zhang

**Updated**: 2025-06-26T13:23:03Z

**Summary**: Recent cosmological observations have achieved high-precision measurements of the Universe's expansion history, prompting the use of nonparametric methods such as Gaussian processes (GP) regression. We apply GP regression for reconstructing the Hubble parameter using CC data, with improved covariance modeling and latest study in CC data. By comparing reconstructions in redshift space $z$ and transformed space $\log(z+1)$ , we evaluate six kernel functions using nested sampling (NS) and approximate Bayesian computation rejection (ABC rejection) methods and analyze the construction of Hubble constant $H_0$ in different models. Our analysis demonstrates that reconstructions in $\log(z+1)$ space remain physically reasonable, offering a viable alternative to conventional $z$ space approaches, while the introduction of nondiagonal covariance matrices leads to degraded reconstruction quality, suggesting that simplified diagonal forms may be preferable for reconstruction. These findings underscore the importance of task-specific kernel selection in GP-based cosmological inference. In particular, our findings suggest that careful preliminary screening of kernel functions, based on the physical quantities of interest, is essential for reliable inference in cosmological research using GP.

**Link**: [arxiv](http://arxiv.org/abs/2506.21238v1),  [pdf](http://arxiv.org/pdf/2506.21238v1)

**Tags**: astro-ph.CO 



### Enhancing Automatic Term Extraction with Large Language Models via   Syntactic Retrieval
**Authors**: Yongchan Chun, Minhyuk Kim, Dongjun Kim, Chanjun Park, Heuiseok Lim

**Updated**: 2025-06-26T13:14:52Z

**Summary**: Automatic Term Extraction (ATE) identifies domain-specific expressions that are crucial for downstream tasks such as machine translation and information retrieval. Although large language models (LLMs) have significantly advanced various NLP tasks, their potential for ATE has scarcely been examined. We propose a retrieval-based prompting strategy that, in the few-shot setting, selects demonstrations according to \emph{syntactic} rather than semantic similarity. This syntactic retrieval method is domain-agnostic and provides more reliable guidance for capturing term boundaries. We evaluate the approach in both in-domain and cross-domain settings, analyzing how lexical overlap between the query sentence and its retrieved examples affects performance. Experiments on three specialized ATE benchmarks show that syntactic retrieval improves F1-score. These findings highlight the importance of syntactic cues when adapting LLMs to terminology-extraction tasks.

**Link**: [arxiv](http://arxiv.org/abs/2506.21222v1),  [pdf](http://arxiv.org/pdf/2506.21222v1)

**Tags**: cs.CL cs.IR 



### Complexity-aware fine-tuning
**Authors**: Andrey Goncharov, Daniil Vyazhev, Petr Sychev, Edvard Khalafyan, Alexey Zaytsev

**Updated**: 2025-06-26T13:13:24Z

**Summary**: General-purpose Large Language Models (LLMs) are frequently fine-tuned through supervised fine-tuning (SFT) to enhance performance in specific domains. Better results can be achieved by distilling the chain-of-thought of a larger model at the cost of numerous expensive calls and a much greater amount of data. We propose a novel blueprint for efficient fine-tuning that uses reasoning only for complex data identified by entropy. Specifically, across two small open models ($\approx 3B$) we split the training data into complexity categories by a single token answer entropy (ROC AUC $0.73$), fine-tune large language models (LLMs) via SFT and distillation, and show that our pipeline significantly outperforms the standard SFT approach ($0.55$ vs $0.43$ average accuracy) and provides comparable with distillation performance while using $62\%$ less data ($0.55$ average accuracy for both). We publish our code and data to facilitate further research in this direction.

**Link**: [arxiv](http://arxiv.org/abs/2506.21220v1),  [pdf](http://arxiv.org/pdf/2506.21220v1)

**Tags**: cs.LG cs.CL 



### Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?
**Authors**: Haoang Chi, He Li, Wenjing Yang, Feng Liu, Long Lan, Xiaoguang Ren, Tongliang Liu, Bo Han

**Updated**: 2025-06-26T13:11:01Z

**Summary**: Causal reasoning capability is critical in advancing large language models (LLMs) toward strong artificial intelligence. While versatile LLMs appear to have demonstrated capabilities in understanding contextual causality and providing responses that obey the laws of causality, it remains unclear whether they perform genuine causal reasoning akin to humans. However, current evidence indicates the contrary. Specifically, LLMs are only capable of performing shallow (level-1) causal reasoning, primarily attributed to the causal knowledge embedded in their parameters, but they lack the capacity for genuine human-like (level-2) causal reasoning. To support this hypothesis, methodologically, we delve into the autoregression mechanism of transformer-based LLMs, revealing that it is not inherently causal. Empirically, we introduce a new causal Q&A benchmark called CausalProbe-2024, whose corpora are fresh and nearly unseen for the studied LLMs. The LLMs exhibit a significant performance drop on CausalProbe-2024 compared to earlier benchmarks, indicating the fact that they primarily engage in level-1 causal reasoning. To bridge the gap towards level-2 causal reasoning, we draw inspiration from the fact that human reasoning is usually facilitated by general knowledge and intended goals. We propose G^2-Reasoner, a method that incorporates general knowledge and goal-oriented prompts into LLMs' causal reasoning processes. Experiments demonstrate that G^2-Reasoner significantly enhances LLMs' causal reasoning capability, particularly in fresh and counterfactual contexts. This work sheds light on a new path for LLMs to advance towards genuine causal reasoning, going beyond level-1 and making strides towards level-2.

**Link**: [arxiv](http://arxiv.org/abs/2506.21215v1),  [pdf](http://arxiv.org/pdf/2506.21215v1)

**Tags**: cs.AI cs.CL cs.LG 



### TAPS: Tool-Augmented Personalisation via Structured Tagging
**Authors**: Ekaterina Taktasheva, Jeff Dalton

**Updated**: 2025-06-26T13:09:40Z

**Summary**: Recent advancements in tool-augmented large language models have enabled them to interact with external tools, enhancing their ability to perform complex user tasks. However, existing approaches overlook the role of personalisation in guiding tool use. This work investigates how user preferences can be effectively integrated into goal-oriented dialogue agents. Through extensive analysis, we identify key weaknesses in the ability of LLMs to personalise tool use. To this end, we introduce TAPS, a novel solution that enhances personalised tool use by leveraging a structured tagging tool and an uncertainty-based tool detector. TAPS significantly improves the ability of LLMs to incorporate user preferences, achieving the new state-of-the-art for open source models on the NLSI task.

**Link**: [arxiv](http://arxiv.org/abs/2506.20409v2),  [pdf](http://arxiv.org/pdf/2506.20409v2)

**Tags**: cs.CL 



### $T^3$: Multi-level Tree-based Automatic Program Repair with Large   Language Models
**Authors**: Quanming Liu, Xupeng Bu, Zhichao Yan, Ru Li

**Updated**: 2025-06-26T13:04:28Z

**Summary**: Automatic Program Repair (APR) is a core technology in software development and maintenance, with aims to enable automated defect repair with minimal human intervention. In recent years, the substantial advancements in Large Language Models (LLMs) and the Chain-of-Thought (CoT) techniques have significantly enhanced the reasoning capabilities of these models. However, due to the complex logic and multi-step reasoning ability needed, the application of CoT techniques in the APR domain remains insufficient. This study systematically evaluates the performance of several common CoT techniques in APR tasks and proposes an innovative framework $T^3$, which integrates the powerful reasoning capabilities of LLMs with tree search, effectively improving the precision of generating candidate repair solutions. Furthermore, $T^3$ provides valuable guidance for optimizing sample selection and repair strategies in APR tasks, establishing a robust framework for achieving efficient automated debugging.

**Link**: [arxiv](http://arxiv.org/abs/2506.21211v1),  [pdf](http://arxiv.org/pdf/2506.21211v1)

**Tags**: cs.SE cs.AI 



### MedPrompt: LLM-CNN Fusion with Weight Routing for Medical Image   Segmentation and Classification
**Authors**: Shadman Sobhan, Kazi Abrar Mahmud, Abduz Zami

**Updated**: 2025-06-26T12:57:41Z

**Summary**: Current medical image analysis systems are typically task-specific, requiring separate models for classification and segmentation, and lack the flexibility to support user-defined workflows. To address these challenges, we introduce MedPrompt, a unified framework that combines a few-shot prompted Large Language Model (Llama-4-17B) for high-level task planning with a modular Convolutional Neural Network (DeepFusionLab) for low-level image processing. The LLM interprets user instructions and generates structured output to dynamically route task-specific pretrained weights. This weight routing approach avoids retraining the entire framework when adding new tasks-only task-specific weights are required, enhancing scalability and deployment. We evaluated MedPrompt across 19 public datasets, covering 12 tasks spanning 5 imaging modalities. The system achieves a 97% end-to-end correctness in interpreting and executing prompt-driven instructions, with an average inference latency of 2.5 seconds, making it suitable for near real-time applications. DeepFusionLab achieves competitive segmentation accuracy (e.g., Dice 0.9856 on lungs) and strong classification performance (F1 0.9744 on tuberculosis). Overall, MedPrompt enables scalable, prompt-driven medical imaging by combining the interpretability of LLMs with the efficiency of modular CNNs.

**Link**: [arxiv](http://arxiv.org/abs/2506.21199v1),  [pdf](http://arxiv.org/pdf/2506.21199v1)

**Tags**: cs.CV eess.SP 



### LLM-Based Human-Agent Collaboration and Interaction Systems: A Survey
**Authors**: Henry Peng Zou, Wei-Chieh Huang, Yaozu Wu, Yankai Chen, Chunyu Miao, Hoang Nguyen, Yue Zhou, Weizhi Zhang, Liancheng Fang, Langzhou He, Yangning Li, Dongyuan Li, Renhe Jiang, Xue Liu, Philip S. Yu

**Updated**: 2025-06-26T12:53:30Z

**Summary**: Recent advances in large language models (LLMs) have sparked growing interest in building fully autonomous agents. However, fully autonomous LLM-based agents still face significant challenges, including limited reliability due to hallucinations, difficulty in handling complex tasks, and substantial safety and ethical risks, all of which limit their feasibility and trustworthiness in real-world applications. To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety. These human-agent collaboration systems enable humans and LLM-based agents to collaborate effectively by leveraging their complementary strengths. This paper provides the first comprehensive and structured survey of LLM-HAS. It clarifies fundamental concepts, systematically presents core components shaping these systems, including environment & profiling, human feedback, interaction types, orchestration and communication, explores emerging applications, and discusses unique challenges and opportunities arising from human-AI collaboration. By consolidating current knowledge and offering a structured overview, we aim to foster further research and innovation in this rapidly evolving interdisciplinary field. Paper lists and resources are available at https://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems.

**Link**: [arxiv](http://arxiv.org/abs/2505.00753v4),  [pdf](http://arxiv.org/pdf/2505.00753v4)

**Tags**: cs.CL cs.LG 



### No-prior Bayesian inference reIMagined: probabilistic approximations of   inferential models
**Authors**: Ryan Martin

**Updated**: 2025-06-26T12:49:59Z

**Summary**: When prior information is lacking, the go-to strategy for probabilistic inference is to combine a "default prior" and the likelihood via Bayes's theorem. Objective Bayes, (generalized) fiducial inference, etc. fall under this umbrella. This construction is natural, but the corresponding posterior distributions generally only offer limited, approximately valid uncertainty quantification. The present paper takes a reimagined approach offering posterior distributions with stronger reliability properties. The proposed construction starts with an inferential model (IM), one that takes the mathematical form of a data-driven possibility measure and features exactly valid uncertainty quantification, and then returns a so-called inner probabilistic approximation thereof. This inner probabilistic approximation inherits many of the original IM's desirable properties, including credible sets with exact coverage and asymptotic efficiency. The approximation also agrees with the familiar Bayes/fiducial solution obtained in applications where the model has a group transformation structure. A Monte Carlo method for evaluating the probabilistic approximation is presented, along with numerical illustrations.

**Link**: [arxiv](http://arxiv.org/abs/2503.19748v2),  [pdf](http://arxiv.org/pdf/2503.19748v2)

**Tags**: stat.ME math.PR math.ST stat.TH 



### Prompt-Guided Turn-Taking Prediction
**Authors**: Koji Inoue, Mikey Elmers, Yahui Fu, Zi Haur Pang, Divesh Lala, Keiko Ochi, Tatsuya Kawahara

**Updated**: 2025-06-26T12:49:07Z

**Summary**: Turn-taking prediction models are essential components in spoken dialogue systems and conversational robots. Recent approaches leverage transformer-based architectures to predict speech activity continuously and in real-time. In this study, we propose a novel model that enables turn-taking prediction to be dynamically controlled via textual prompts. This approach allows intuitive and explicit control through instructions such as "faster" or "calmer" adapting dynamically to conversational partners and contexts. The proposed model builds upon a transformer-based voice activity projection (VAP) model, incorporating textual prompt embeddings into both channel-wise transformers and a cross-channel transformer. We evaluated the feasibility of our approach using over 950 hours of human-human spoken dialogue data. Since textual prompt data for the proposed approach was not available in existing datasets, we utilized a large language model (LLM) to generate synthetic prompt sentences. Experimental results demonstrated that the proposed model improved prediction accuracy and effectively varied turn-taking timing behaviors according to the textual prompts.

**Link**: [arxiv](http://arxiv.org/abs/2506.21191v1),  [pdf](http://arxiv.org/pdf/2506.21191v1)

**Tags**: cs.CL cs.SD eess.AS 



### Survival analysis under label shift
**Authors**: Yuxiang Zong, Yanyuan Ma, Ingrid Van Keilegom

**Updated**: 2025-06-26T12:49:02Z

**Summary**: Let P represent the source population with complete data, containing covariate $\mathbf{Z}$ and response $T$, and Q the target population, where only the covariate $\mathbf{Z}$ is available. We consider a setting with both label shift and label censoring. Label shift assumes that the marginal distribution of $T$ differs between $P$ and $Q$, while the conditional distribution of $\mathbf{Z}$ given $T$ remains the same. Label censoring refers to the case where the response $T$ in $P$ is subject to random censoring. Our goal is to leverage information from the label-shifted and label-censored source population $P$ to conduct statistical inference in the target population $Q$. We propose a parametric model for $T$ given $\mathbf{Z}$ in $Q$ and estimate the model parameters by maximizing an approximate likelihood. This allows for statistical inference in $Q$ and accommodates a range of classical survival models. Under the label shift assumption, the likelihood depends not only on the unknown parameters but also on the unknown distribution of $T$ in $P$ and $\mathbf{Z}$ in $Q$, which we estimate nonparametrically. The asymptotic properties of the estimator are rigorously established and the effectiveness of the method is demonstrated through simulations and a real data application. This work is the first to combine survival analysis with label shift, offering a new research direction in this emerging topic.

**Link**: [arxiv](http://arxiv.org/abs/2506.21190v1),  [pdf](http://arxiv.org/pdf/2506.21190v1)

**Tags**: stat.ME 



### Seal Your Backdoor with Variational Defense
**Authors**: Ivan Sabolić, Matej Grcić, Siniša Šegvić

**Updated**: 2025-06-26T12:48:11Z

**Summary**: We propose VIBE, a model-agnostic framework that trains classifiers resilient to backdoor attacks. The key concept behind our approach is to treat malicious inputs and corrupted labels from the training dataset as observed random variables, while the actual clean labels are latent. VIBE then recovers the corresponding latent clean label posterior through variational inference. The resulting training procedure follows the expectation-maximization (EM) algorithm. The E-step infers the clean pseudolabels by solving an entropy-regularized optimal transport problem, while the M-step updates the classifier parameters via gradient descent. Being modular, VIBE can seamlessly integrate with recent advancements in self-supervised representation learning, which enhance its ability to resist backdoor attacks. We experimentally validate the method effectiveness against contemporary backdoor attacks on standard datasets, a large-scale setup with 1$k$ classes, and a dataset poisoned with multiple attacks. VIBE consistently outperforms previous defenses across all tested scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2503.08829v2),  [pdf](http://arxiv.org/pdf/2503.08829v2)

**Tags**: cs.LG cs.CR 



### Exact operator inference with minimal data
**Authors**: Henrik Rosenberger, Benjamin Sanderse, Giovanni Stabile

**Updated**: 2025-06-26T12:44:50Z

**Summary**: This work introduces a novel method to generate snapshot data for operator inference that guarantees the exact reconstruction of intrusive projection-based reduced-order models (ROMs). To ensure exact reconstruction, the operator inference least squares matrix must have full rank, without regularization. Existing works have achieved this full rank using heuristic strategies to generate snapshot data and a-posteriori checks on full rank, but without a guarantee of success. Our novel snapshot data generation method provides this guarantee thanks to two key ingredients: first we identify ROM states that induce full rank, then we generate snapshots corresponding to exactly these states by simulating multiple trajectories for only a single time step. This way, the number of required snapshots is minimal and orders of magnitude lower than typically reported with existing methods. The method avoids non-Markovian terms and does not require re-projection. Since the number of snapshots is minimal, the least squares problem simplifies to a linear system that is numerically more stable. In addition, because the inferred operators are exact, properties of the intrusive ROM operators such as symmetry or skew-symmetry are preserved. Numerical results for differential equations involving 2nd, 3rd and 8th order polynomials demonstrate that the novel snapshot data generation method leads to exact reconstruction of the intrusive reduced order models.

**Link**: [arxiv](http://arxiv.org/abs/2506.01244v2),  [pdf](http://arxiv.org/pdf/2506.01244v2)

**Tags**: math.NA cs.NA 65Y99, 65F22, 35R30, 65D05 



### Systematic bias in LISA ringdown analysis due to waveform inaccuracy
**Authors**: Lodovico Capuano, Massimo Vaglio, Rohit S. Chandramouli, Chantal L Pitte, Adrien Kuntz, Enrico Barausse

**Updated**: 2025-06-26T12:39:06Z

**Summary**: Inaccurate modeling of gravitational-wave signals can introduce systematic biases in the inferred source parameters. As detector sensitivities improve and signals become louder, mitigating such waveform-induced systematics becomes increasingly important. In this work, we assess the systematic biases introduced by an incomplete description of the ringdown signal from massive black hole binaries in the LISA band. Specifically, we investigate the impact of mode truncation in the ringdown template. Using a reference waveform composed of 13 modes, we establish a mode hierarchy and determine the minimum number of modes required to avoid parameter biases across a wide range of LISA sources. For typical systems with masses $\sim 10^6$--$10^7\,M_\odot$ at redshifts $z \sim 2$--$6$, we find that at least 3--6 modes are needed for accurate parameter estimation, while high-SNR events may need at least 10 modes. Our results are a window-insensitive lower bound on the minimum number of modes, as more modes may be needed depending on the choice of time-domain windowing of the post-merger signal.

**Link**: [arxiv](http://arxiv.org/abs/2506.21181v1),  [pdf](http://arxiv.org/pdf/2506.21181v1)

**Tags**: gr-qc 



### Variational Supervised Contrastive Learning
**Authors**: Ziwen Wang, Jiajun Fan, Thao Nguyen, Heng Ji, Ge Liu

**Updated**: 2025-06-26T12:27:25Z

**Summary**: Contrastive learning has proven to be highly efficient and adaptable in shaping representation spaces across diverse modalities by pulling similar samples together and pushing dissimilar ones apart. However, two key limitations persist: (1) Without explicit regulation of the embedding distribution, semantically related instances can inadvertently be pushed apart unless complementary signals guide pair selection, and (2) excessive reliance on large in-batch negatives and tailored augmentations hinders generalization. To address these limitations, we propose Variational Supervised Contrastive Learning (VarCon), which reformulates supervised contrastive learning as variational inference over latent class variables and maximizes a posterior-weighted evidence lower bound (ELBO) that replaces exhaustive pair-wise comparisons for efficient class-aware matching and grants fine-grained control over intra-class dispersion in the embedding space. Trained exclusively on image data, our experiments on CIFAR-10, CIFAR-100, ImageNet-100, and ImageNet-1K show that VarCon (1) achieves state-of-the-art performance for contrastive learning frameworks, reaching 79.36% Top-1 accuracy on ImageNet-1K and 78.29% on CIFAR-100 with a ResNet-50 encoder while converging in just 200 epochs; (2) yields substantially clearer decision boundaries and semantic organization in the embedding space, as evidenced by KNN classification, hierarchical clustering results, and transfer-learning assessments; and (3) demonstrates superior performance in few-shot learning than supervised baseline and superior robustness across various augmentation strategies.

**Link**: [arxiv](http://arxiv.org/abs/2506.07413v2),  [pdf](http://arxiv.org/pdf/2506.07413v2)

**Tags**: cs.LG cs.CV 



### Compressed and Smooth Latent Space for Text Diffusion Modeling
**Authors**: Viacheslav Meshchaninov, Egor Chimbulatov, Alexander Shabalin, Aleksandr Abramov, Dmitry Vetrov

**Updated**: 2025-06-26T12:05:13Z

**Summary**: Autoregressive language models dominate modern text generation, yet their sequential nature introduces fundamental limitations: decoding is slow, and maintaining global coherence remains challenging. Diffusion models offer a promising alternative by enabling parallel generation and flexible control; however, their application to text generation is hindered by the high dimensionality of token-level representations. We introduce Cosmos, a novel approach to text generation that operates entirely in a compressed, smooth latent space tailored specifically for diffusion. This space is learned using an autoencoder trained simultaneously for token-level reconstruction and alignment with frozen activations from a pretrained language encoder, providing robust semantic grounding and enabling effective perturbation-based augmentations. Empirically, we demonstrate that text representations can be compressed by $8\times$ while maintaining generation quality comparable to token-level diffusion models. Furthermore, increasing the latent sequence length allows Cosmos to surpass both diffusion-based and autoregressive baselines. We evaluate Cosmos on four diverse generative tasks including story generation, question generation, summarization, and detoxification and compare it with various generative paradigms. Cosmos achieves comparable or superior generation quality while offering more than $2\times$ faster inference.

**Link**: [arxiv](http://arxiv.org/abs/2506.21170v1),  [pdf](http://arxiv.org/pdf/2506.21170v1)

**Tags**: cs.CL 



### Metis-RISE: RL Incentivizes and SFT Enhances Multimodal Reasoning Model   Learning
**Authors**: Haibo Qiu, Xiaohan Lan, Fanfan Liu, Xiaohu Sun, Delian Ruan, Peng Shi, Lin Ma

**Updated**: 2025-06-26T11:45:11Z

**Summary**: Recent advancements in large language models (LLMs) have witnessed a surge in the development of advanced reasoning paradigms, which are now being integrated into multimodal large language models (MLLMs). However, existing approaches often fall short: methods solely employing reinforcement learning (RL) can struggle with sample inefficiency and activating entirely absent reasoning capabilities, while conventional pipelines that initiate with a cold-start supervised fine-tuning (SFT) phase before RL may restrict the model's exploratory capacity and face suboptimal convergence. In this work, we introduce \textbf{Metis-RISE} (\textbf{R}L \textbf{I}ncentivizes and \textbf{S}FT \textbf{E}nhances) for multimodal reasoning model learning. Unlike conventional approaches, Metis-RISE distinctively omits an initial SFT stage, beginning instead with an RL phase (e.g., using a Group Relative Policy Optimization variant) to incentivize and activate the model's latent reasoning capacity. Subsequently, the targeted SFT stage addresses two key challenges identified during RL: (1) \textit{inefficient trajectory sampling} for tasks where the model possesses but inconsistently applies correct reasoning, which we tackle using self-distilled reasoning trajectories from the RL model itself; and (2) \textit{fundamental capability absence}, which we address by injecting expert-augmented knowledge for prompts where the model entirely fails. This strategic application of RL for incentivization followed by SFT for enhancement forms the core of Metis-RISE, leading to two versions of our MLLMs (7B and 72B parameters). Evaluations on the OpenCompass Multimodal Reasoning Leaderboard demonstrate that both models achieve state-of-the-art performance among similar-sized models, with the 72B version ranking fourth overall. Please refer to our project page for open-source information.

**Link**: [arxiv](http://arxiv.org/abs/2506.13056v2),  [pdf](http://arxiv.org/pdf/2506.13056v2)

**Tags**: cs.AI cs.CV cs.LG 



### CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of   Large Language Models
**Authors**: Ping Wu, Guobin Shen, Dongcheng Zhao, Yuwei Wang, Yiting Dong, Yu Shi, Enmeng Lu, Feifei Zhao, Yi Zeng

**Updated**: 2025-06-26T11:34:33Z

**Summary**: Ensuring that Large Language Models (LLMs) align with mainstream human values and ethical norms is crucial for the safe and sustainable development of AI. Current value evaluation and alignment are constrained by Western cultural bias and incomplete domestic frameworks reliant on non-native rules; furthermore, the lack of scalable, rule-driven scenario generation methods makes evaluations costly and inadequate across diverse cultural contexts. To address these challenges, we propose a hierarchical value framework grounded in core Chinese values, encompassing three main dimensions, 12 core values, and 50 derived values. Based on this framework, we construct a large-scale Chinese Values Corpus (CVC) containing over 250,000 value rules enhanced and expanded through human annotation. Experimental results show that CVC-guided scenarios outperform direct generation ones in value boundaries and content diversity. In the evaluation across six sensitive themes (e.g., surrogacy, suicide), seven mainstream LLMs preferred CVC-generated options in over 70.5% of cases, while five Chinese human annotators showed an 87.5% alignment with CVC, confirming its universality, cultural relevance, and strong alignment with Chinese values. Additionally, we construct 400,000 rule-based moral dilemma scenarios that objectively capture nuanced distinctions in conflicting value prioritization across 17 LLMs. Our work establishes a culturally-adaptive benchmarking framework for comprehensive value evaluation and alignment, representing Chinese characteristics. All data are available at https://huggingface.co/datasets/Beijing-AISI/CVC, and the code is available at https://github.com/Beijing-AISI/CVC.

**Link**: [arxiv](http://arxiv.org/abs/2506.01495v4),  [pdf](http://arxiv.org/pdf/2506.01495v4)

**Tags**: cs.CL 



### Evaluating Randomness Assumption: A Novel Graph Theoretic Approach
**Authors**: Shriya Gehlot, Arnab Kumar Laha

**Updated**: 2025-06-26T11:27:23Z

**Summary**: Randomness or mutual independence is a fundamental assumption forming the basis of statistical inference across disciplines such as economics, finance, and management. Consequently, validating this assumption is essential for the reliable application of statistical methods. However, verifying randomness remains a challenge, as existing tests in the literature are often restricted to detecting specific types of data dependencies. In this paper, we propose a novel graph-theoretic approach to testing randomness using random interval graphs (RIGs). The key advantage of RIGs is that their properties are independent of the underlying distribution of the data, relying solely on the assumption of independence between observations. By using two key properties of RIGs-edge probability and vertex degree distribution-we develop two new randomness tests: the RIG-Edge Probability test and the RIG-Degree Distribution (RIG-DD) test. Through extensive simulations, we demonstrate that these tests can detect a broad range of dependencies, including complex phenomena such as conditional heteroskedasticity and chaotic behavior, beyond simple correlations. Furthermore, we show that the RIG-DD test outperforms most of the existing tests of randomness in the literature. We also provide real-world examples to illustrate the practical applicability of these tests.

**Link**: [arxiv](http://arxiv.org/abs/2506.21157v1),  [pdf](http://arxiv.org/pdf/2506.21157v1)

**Tags**: stat.ME 62A09 (Primary) 62G99 (Secondary) 



### Amortizing personalization in virtual brain twins
**Authors**: Nina Baldy, Marmaduke M Woodman, Viktor K Jirsa

**Updated**: 2025-06-26T11:25:14Z

**Summary**: Virtual brain twins are personalized digital models of individual human subject or patient's brains, allowing for mechanistic interpretation of neuroimaging data features. Training and inference with these models however presents a pair of challenges: large shared infrastructure do not allow for use of personal data and inference in clinical applications should not require significant resources. We introduce "anonymized personalization" to address both by expanding model priors to include personalization which under amortized inference allows training to be performed anonymously, while inference is both personalized and lightweight. We illustrate the basic approach, demonstrate reliability in an example, and discuss the impact on both experimental and computational neuroscience. Code is available at https://github.com/ins-amu/apvbt.

**Link**: [arxiv](http://arxiv.org/abs/2506.21155v1),  [pdf](http://arxiv.org/pdf/2506.21155v1)

**Tags**: q-bio.NC 



### Geometry and Perception Guided Gaussians for Multiview-consistent 3D   Generation from a Single Image
**Authors**: Pufan Li, Bi'an Du, Wei Hu

**Updated**: 2025-06-26T11:22:06Z

**Summary**: Generating realistic 3D objects from single-view images requires natural appearance, 3D consistency, and the ability to capture multiple plausible interpretations of unseen regions. Existing approaches often rely on fine-tuning pretrained 2D diffusion models or directly generating 3D information through fast network inference or 3D Gaussian Splatting, but their results generally suffer from poor multiview consistency and lack geometric detail. To takle these issues, we present a novel method that seamlessly integrates geometry and perception priors without requiring additional model training to reconstruct detailed 3D objects from a single image. Specifically, we train three different Gaussian branches initialized from the geometry prior, perception prior and Gaussian noise, respectively. The geometry prior captures the rough 3D shapes, while the perception prior utilizes the 2D pretrained diffusion model to enhance multiview information. Subsequently, we refine 3D Gaussian branches through mutual interaction between geometry and perception priors, further enhanced by a reprojection-based strategy that enforces depth consistency. Experiments demonstrate the higher-fidelity reconstruction results of our method, outperforming existing methods on novel view synthesis and 3D reconstruction, demonstrating robust and consistent 3D object generation.

**Link**: [arxiv](http://arxiv.org/abs/2506.21152v1),  [pdf](http://arxiv.org/pdf/2506.21152v1)

**Tags**: cs.CV 68 I.4.0 



### Do Large Language Models Advocate for Inferentialism?
**Authors**: Yuzuki Arai, Sho Tsugawa

**Updated**: 2025-06-26T11:03:13Z

**Summary**: The emergence of large language models (LLMs) such as ChatGPT and Claude presents new challenges for philosophy of language, particularly regarding the nature of linguistic meaning and representation. While LLMs have traditionally been understood through distributional semantics, this paper explores Robert Brandom's inferential semantics as an alternative foundational framework for understanding these systems. We examine how key features of inferential semantics -- including its anti-representationalist stance, logical expressivism, and quasi-compositional approach -- align with the architectural and functional characteristics of Transformer-based LLMs. Through analysis of the ISA (Inference, Substitution, Anaphora) approach, we demonstrate that LLMs exhibit fundamentally anti-representationalist properties in their processing of language. We further develop a consensus theory of truth appropriate for LLMs, grounded in their interactive and normative dimensions through mechanisms like RLHF. While acknowledging significant tensions between inferentialism's philosophical commitments and LLMs' sub-symbolic processing, this paper argues that inferential semantics provides valuable insights into how LLMs generate meaning without reference to external world representations. Our analysis suggests that LLMs may challenge traditional assumptions in philosophy of language, including strict compositionality and semantic externalism, though further empirical investigation is needed to fully substantiate these theoretical claims.

**Link**: [arxiv](http://arxiv.org/abs/2412.14501v2),  [pdf](http://arxiv.org/pdf/2412.14501v2)

**Tags**: cs.CL 



### How Good Are Synthetic Requirements ? Evaluating LLM-Generated Datasets   for AI4RE
**Authors**: Abdelkarim El-Hajjami, Camille Salinesi

**Updated**: 2025-06-26T10:52:07Z

**Summary**: The shortage of publicly available, labeled requirements datasets remains a major barrier to advancing Artificial Intelligence for Requirements Engineering (AI4RE). While Large Language Models offer promising capabilities for synthetic data generation, systematic approaches to control and optimize the quality of generated requirements remain underexplored. This paper presents Synthline v1, an enhanced Product Line approach for generating synthetic requirements data that extends our earlier v0 version with advanced generation strategies and curation techniques. We investigate four research questions assessing how prompting strategies, automated prompt optimization, and post-generation curation affect data quality across four classification tasks: defect detection, functional vs. non-functional, quality vs. non-quality, and security vs. non-security. Our evaluation shows that multi-sample prompting significantly boosts both utility and diversity over single-sample generation, with F1-score gains from 6 to 44 points. The use of PACE (Prompt Actor-Critic Editing) for automated prompt optimization yields task-dependent results, greatly improving functional classification (+32.5 points) but reducing performance on others. Interestingly, similarity-based curation improves diversity but often harms classification performance, indicating that some redundancy may help ML models. Most importantly, our results show that synthetic requirements can match or outperform human-authored ones for specific tasks, with synthetic data surpassing human data for security (+7.8 points) and defect classification (+15.4 points). These findings offer practical insights for AI4RE and chart a viable path to mitigating dataset scarcity through systematic synthetic generation.

**Link**: [arxiv](http://arxiv.org/abs/2506.21138v1),  [pdf](http://arxiv.org/pdf/2506.21138v1)

**Tags**: cs.SE cs.AI 



### Easily Computed Marginal Likelihoods for Multivariate Mixture Models   Using the THAMES Estimator
**Authors**: Martin Metodiev, Nicholas J. Irons, Marie Perrot-Dockès, Pierre Latouche, Adrian E. Raftery

**Updated**: 2025-06-26T10:51:45Z

**Summary**: We present a new version of the truncated harmonic mean estimator (THAMES) for univariate or multivariate mixture models. The estimator computes the marginal likelihood from Markov chain Monte Carlo (MCMC) samples, is consistent, asymptotically normal and of finite variance. In addition, it is invariant to label switching, does not require posterior samples from hidden allocation vectors, and is easily approximated, even for an arbitrarily high number of components. Its computational efficiency is based on an asymptotically optimal ordering of the parameter space, which can in turn be used to provide useful visualisations. We test it in simulation settings where the true marginal likelihood is available analytically. It performs well against state-of-the-art competitors, even in multivariate settings with a high number of components. We demonstrate its utility for inference and model selection on univariate and multivariate data sets.

**Link**: [arxiv](http://arxiv.org/abs/2504.21812v2),  [pdf](http://arxiv.org/pdf/2504.21812v2)

**Tags**: stat.ME 



### Is my Data in your AI Model? Membership Inference Test with Application   to Face Images
**Authors**: Daniel DeAlcala, Aythami Morales, Julian Fierrez, Gonzalo Mancera, Ruben Tolosana, Javier Ortega-Garcia

**Updated**: 2025-06-26T10:49:33Z

**Summary**: This article introduces the Membership Inference Test (MINT), a novel approach that aims to empirically assess if given data was used during the training of AI/ML models. Specifically, we propose two MINT architectures designed to learn the distinct activation patterns that emerge when an Audited Model is exposed to data used during its training process. These architectures are based on Multilayer Perceptrons (MLPs) and Convolutional Neural Networks (CNNs). The experimental framework focuses on the challenging task of Face Recognition, considering three state-of-the-art Face Recognition systems. Experiments are carried out using six publicly available databases, comprising over 22 million face images in total. Different experimental scenarios are considered depending on the context of the AI model to test. Our proposed MINT approach achieves promising results, with up to 90\% accuracy, indicating the potential to recognize if an AI model has been trained with specific data. The proposed MINT approach can serve to enforce privacy and fairness in several AI applications, e.g., revealing if sensitive or private data was used for training or tuning Large Language Models (LLMs).

**Link**: [arxiv](http://arxiv.org/abs/2402.09225v3),  [pdf](http://arxiv.org/pdf/2402.09225v3)

**Tags**: cs.CV cs.AI 



### GoIRL: Graph-Oriented Inverse Reinforcement Learning for Multimodal   Trajectory Prediction
**Authors**: Muleilan Pei, Shaoshuai Shi, Lu Zhang, Peiliang Li, Shaojie Shen

**Updated**: 2025-06-26T09:46:53Z

**Summary**: Trajectory prediction for surrounding agents is a challenging task in autonomous driving due to its inherent uncertainty and underlying multimodality. Unlike prevailing data-driven methods that primarily rely on supervised learning, in this paper, we introduce a novel Graph-oriented Inverse Reinforcement Learning (GoIRL) framework, which is an IRL-based predictor equipped with vectorized context representations. We develop a feature adaptor to effectively aggregate lane-graph features into grid space, enabling seamless integration with the maximum entropy IRL paradigm to infer the reward distribution and obtain the policy that can be sampled to induce multiple plausible plans. Furthermore, conditioned on the sampled plans, we implement a hierarchical parameterized trajectory generator with a refinement module to enhance prediction accuracy and a probability fusion strategy to boost prediction confidence. Extensive experimental results showcase our approach not only achieves state-of-the-art performance on the large-scale Argoverse & nuScenes motion forecasting benchmarks but also exhibits superior generalization abilities compared to existing supervised models.

**Link**: [arxiv](http://arxiv.org/abs/2506.21121v1),  [pdf](http://arxiv.org/pdf/2506.21121v1)

**Tags**: cs.CV cs.RO 



### Interpretable Hierarchical Concept Reasoning through Attention-Guided   Graph Learning
**Authors**: David Debot, Pietro Barbiero, Gabriele Dominici, Giuseppe Marra

**Updated**: 2025-06-26T08:56:55Z

**Summary**: Concept-Based Models (CBMs) are a class of deep learning models that provide interpretability by explaining predictions through high-level concepts. These models first predict concepts and then use them to perform a downstream task. However, current CBMs offer interpretability only for the final task prediction, while the concept predictions themselves are typically made via black-box neural networks. To address this limitation, we propose Hierarchical Concept Memory Reasoner (H-CMR), a new CBM that provides interpretability for both concept and task predictions. H-CMR models relationships between concepts using a learned directed acyclic graph, where edges represent logic rules that define concepts in terms of other concepts. During inference, H-CMR employs a neural attention mechanism to select a subset of these rules, which are then applied hierarchically to predict all concepts and the final task. Experimental results demonstrate that H-CMR matches state-of-the-art performance while enabling strong human interaction through concept and model interventions. The former can significantly improve accuracy at inference time, while the latter can enhance data efficiency during training when background knowledge is available.

**Link**: [arxiv](http://arxiv.org/abs/2506.21102v1),  [pdf](http://arxiv.org/pdf/2506.21102v1)

**Tags**: cs.LG cs.AI 



### Tree-based variational inference for Poisson log-normal models
**Authors**: Alexandre Chaussard, Anna Bonnet, Elisabeth Gassiat, Sylvain Le Corff

**Updated**: 2025-06-26T08:54:53Z

**Summary**: When studying ecosystems, hierarchical trees are often used to organize entities based on proximity criteria, such as the taxonomy in microbiology, social classes in geography, or product types in retail businesses, offering valuable insights into entity relationships. Despite their significance, current count-data models do not leverage this structured information. In particular, the widely used Poisson log-normal (PLN) model, known for its ability to model interactions between entities from count data, lacks the possibility to incorporate such hierarchical tree structures, limiting its applicability in domains characterized by such complexities. To address this matter, we introduce the PLN-Tree model as an extension of the PLN model, specifically designed for modeling hierarchical count data. By integrating structured variational inference techniques, we propose an adapted training procedure and establish identifiability results, enhancing both theoretical foundations and practical interpretability. Experiments on synthetic datasets and human gut microbiome data highlight generative improvements when using PLN-Tree, demonstrating the practical interest of knowledge graphs like the taxonomy in microbiome modeling. Additionally, we present a proof-of-concept implication of the identifiability results by illustrating the practical benefits of using identifiable features for classification tasks, showcasing the versatility of the framework.

**Link**: [arxiv](http://arxiv.org/abs/2406.17361v4),  [pdf](http://arxiv.org/pdf/2406.17361v4)

**Tags**: stat.ME stat.ML 



### HERMES: temporal-coHERent long-forM understanding with Episodes and   Semantics
**Authors**: Gueter Josmy Faure, Jia-Fong Yeh, Min-Hung Chen, Hung-Ting Su, Shang-Hong Lai, Winston H. Hsu

**Updated**: 2025-06-26T08:46:37Z

**Summary**: Long-form video understanding presents unique challenges that extend beyond traditional short-video analysis approaches, particularly in capturing long-range dependencies, processing redundant information efficiently, and extracting high-level semantic concepts. To address these challenges, we propose a novel approach that more accurately reflects human cognition. This paper introduces HERMES: temporal-coHERent long-forM understanding with Episodes and Semantics, featuring two versatile modules that can enhance existing video-language models or operate as a standalone system. Our Episodic COmpressor (ECO) efficiently aggregates representations from micro to semi-macro levels, reducing computational overhead while preserving temporal dependencies. Our Semantics ReTRiever (SeTR) enriches these representations with semantic information by focusing on broader context, dramatically reducing feature dimensionality while preserving relevant macro-level information. We demonstrate that these modules can be seamlessly integrated into existing SOTA models, consistently improving their performance while reducing inference latency by up to 43% and memory usage by 46%. As a standalone system, HERMES achieves state-of-the-art performance across multiple long-video understanding benchmarks in both zero-shot and fully-supervised settings.

**Link**: [arxiv](http://arxiv.org/abs/2408.17443v4),  [pdf](http://arxiv.org/pdf/2408.17443v4)

**Tags**: cs.CV cs.AI cs.CL 



### ITO-Master: Inference-Time Optimization for Audio Effects Modeling of   Music Mastering Processors
**Authors**: Junghyun Koo, Marco A. Martinez-Ramirez, Wei-Hsiang Liao, Giorgio Fabbro, Michele Mancusi, Yuki Mitsufuji

**Updated**: 2025-06-26T08:38:02Z

**Summary**: Music mastering style transfer aims to model and apply the mastering characteristics of a reference track to a target track, simulating the professional mastering process. However, existing methods apply fixed processing based on a reference track, limiting users' ability to fine-tune the results to match their artistic intent. In this paper, we introduce the ITO-Master framework, a reference-based mastering style transfer system that integrates Inference-Time Optimization (ITO) to enable finer user control over the mastering process. By optimizing the reference embedding during inference, our approach allows users to refine the output dynamically, making micro-level adjustments to achieve more precise mastering results. We explore both black-box and white-box methods for modeling mastering processors and demonstrate that ITO improves mastering performance across different styles. Through objective evaluation, subjective listening tests, and qualitative analysis using text-based conditioning with CLAP embeddings, we validate that ITO enhances mastering style similarity while offering increased adaptability. Our framework provides an effective and user-controllable solution for mastering style transfer, allowing users to refine their results beyond the initial style transfer.

**Link**: [arxiv](http://arxiv.org/abs/2506.16889v2),  [pdf](http://arxiv.org/pdf/2506.16889v2)

**Tags**: cs.SD eess.AS 



### ESMStereo: Enhanced ShuffleMixer Disparity Upsampling for Real-Time and   Accurate Stereo Matching
**Authors**: Mahmoud Tahmasebi, Saif Huq, Kevin Meehan, Marion McAfee

**Updated**: 2025-06-26T08:34:51Z

**Summary**: Stereo matching has become an increasingly important component of modern autonomous systems. Developing deep learning-based stereo matching models that deliver high accuracy while operating in real-time continues to be a major challenge in computer vision. In the domain of cost-volume-based stereo matching, accurate disparity estimation depends heavily on large-scale cost volumes. However, such large volumes store substantial redundant information and also require computationally intensive aggregation units for processing and regression, making real-time performance unattainable. Conversely, small-scale cost volumes followed by lightweight aggregation units provide a promising route for real-time performance, but lack sufficient information to ensure highly accurate disparity estimation. To address this challenge, we propose the Enhanced Shuffle Mixer (ESM) to mitigate information loss associated with small-scale cost volumes. ESM restores critical details by integrating primary features into the disparity upsampling unit. It quickly extracts features from the initial disparity estimation and fuses them with image features. These features are mixed by shuffling and layer splitting then refined through a compact feature-guided hourglass network to recover more detailed scene geometry. The ESM focuses on local contextual connectivity with a large receptive field and low computational cost, leading to the reconstruction of a highly accurate disparity map at real-time. The compact version of ESMStereo achieves an inference speed of 116 FPS on high-end GPUs and 91 FPS on the AGX Orin.

**Link**: [arxiv](http://arxiv.org/abs/2506.21091v1),  [pdf](http://arxiv.org/pdf/2506.21091v1)

**Tags**: cs.CV 



### EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for   Efficient Egocentric Perception
**Authors**: Sanjoy Chowdhury, Subrata Biswas, Sayan Nag, Tushar Nagarajan, Calvin Murdock, Ishwarya Ananthabhotla, Yijun Qian, Vamsi Krishna Ithapu, Dinesh Manocha, Ruohan Gao

**Updated**: 2025-06-26T08:09:16Z

**Summary**: Modern perception models, particularly those designed for multisensory egocentric tasks, have achieved remarkable performance but often come with substantial computational costs. These high demands pose challenges for real-world deployment, especially in resource-constrained environments. In this paper, we introduce EgoAdapt, a framework that adaptively performs cross-modal distillation and policy learning to enable efficient inference across different egocentric perception tasks, including egocentric action recognition, active speaker localization, and behavior anticipation. Our proposed policy module is adaptable to task-specific action spaces, making it broadly applicable. Experimental results on three challenging egocentric datasets EPIC-Kitchens, EasyCom, and Aria Everyday Activities demonstrate that our method significantly enhances efficiency, reducing GMACs by up to 89.09%, parameters up to 82.02%, and energy up to 9.6x, while still on-par and in many cases outperforming, the performance of corresponding state-of-the-art models.

**Link**: [arxiv](http://arxiv.org/abs/2506.21080v1),  [pdf](http://arxiv.org/pdf/2506.21080v1)

**Tags**: cs.CV cs.AI cs.LG 



### CodecSlime: Temporal Redundancy Compression of Neural Speech Codec via   Dynamic Frame Rate
**Authors**: Hankun Wang, Yiwei Guo, Chongtian Shao, Bohan Li, Xie Chen, Kai Yu

**Updated**: 2025-06-26T07:59:04Z

**Summary**: Neural speech codecs have been widely used in audio compression and various downstream tasks. Current mainstream codecs are fixed-frame-rate (FFR), which allocate the same number of tokens to every equal-duration slice. However, speech is inherently non-uniform in temporal information density. As a result, many tokens are wasted on steady-state segments like long vowels and silences. To address this mismatch, we present CodecSlime, a plugin-style method for compressing temporal redundancy through supporting dynamic frame rate (DFR) on neural speech codecs for the first time. Our method is unsupervised and architecture-agnostic, combining two key innovations, ScheDFR and Melt-and-Cool, for adapting inference and training, respectively. When integrated into a typical VQ-GAN codec backbone and operating at 40 Hz DFR ($\approx$ 600 bps), the reconstruction WER of CodecSlime is reduced by up to 46% relative to conventional FFR baselines with the same model architecture and similar bitrates, while other metrics are also competitive. CodecSlime also enables flexible trade-offs between reconstruction quality and bitrate: a single model supports inference at multiple frame rates and consistently outperforms FFR models at the corresponding frame rates. Audio samples are available at https://acadarmeria.github.io/codecslime/.

**Link**: [arxiv](http://arxiv.org/abs/2506.21074v1),  [pdf](http://arxiv.org/pdf/2506.21074v1)

**Tags**: eess.AS cs.SD 



### Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge   Graph
**Authors**: Jingwei Wang, Zai Zhang, Hao Qian, Chunjing Gan, Binbin Hu, Ziqi Liu, Zhiqiang Zhang, Jun Zhou, Bin Shi, Bo Dong

**Updated**: 2025-06-26T07:45:15Z

**Summary**: Teaching large language models (LLMs) to use tools is crucial for improving their problem-solving abilities and expanding their applications. However, effectively using tools is challenging because it requires a deep understanding of tool functionalities and user intentions. Previous methods relied mainly on LLMs to generate instruction data, but the quality of these data was often insufficient. In this paper, we propose a new method that uses knowledge graphs to generate high-quality instruction data for LLMs. Knowledge graphs are manually curated datasets rich in semantic information. We begin by extracting various query pathways from a given knowledge graph, which are transformed into a broad spectrum of user queries. We then translate the relationships between entities into actionable tools and parse the pathways of each query into detailed solution steps, thereby creating high-quality instruction data. Our experiments show that fine-tuning on just a small sample of this synthetic data can significantly improve the tool utilization and overall capabilities of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2506.21071v1),  [pdf](http://arxiv.org/pdf/2506.21071v1)

**Tags**: cs.LG cs.CL 



### MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for   Conversational Stance Detection
**Authors**: Fuqiang Niu, Genan Dai, Yisha Lu, Jiayu Liao, Xiang Li, Hu Huang, Bowen Zhang

**Updated**: 2025-06-26T06:59:30Z

**Summary**: In the realm of contemporary social media, automatic stance detection is pivotal for opinion mining, as it synthesizes and examines user perspectives on contentious topics to uncover prevailing trends and sentiments. Traditional stance detection research often targets individual instances, thereby limiting its capacity to model multi-party discussions typical in real social media scenarios. This shortcoming largely stems from the scarcity of datasets that authentically capture the dynamics of social media interactions, hindering advancements in conversational stance detection. In this paper, we introduce MT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational stance detection. To the best of our knowledge, MT2-CSD is the largest dataset available for this purpose, comprising 24,457 annotated instances and exhibiting the greatest conversational depth, thereby presenting new challenges for stance detection. To address these challenges, we propose the Large Language model enhanced Conversational Relational Attention Network (LLM-CRAN), which exploits the reasoning capabilities of LLMs to improve conversational understanding. We conduct extensive experiments to evaluate the efficacy of LLM-CRAN on the MT2-CSD dataset. The experimental results indicate that LLM-CRAN significantly outperforms strong baseline models in the task of conversational stance detection.

**Link**: [arxiv](http://arxiv.org/abs/2506.21053v1),  [pdf](http://arxiv.org/pdf/2506.21053v1)

**Tags**: cs.CL 



## Keyword: LLM Deployment 
 ### mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and   Model Selection at Scale
**Authors**: Xiaona Zhou, Constantin Brif, Ismini Lourentzou

**Updated**: 2025-06-26T17:59:58Z

**Summary**: Multivariate time series anomaly detection (MTS-AD) is critical in domains like healthcare, cybersecurity, and industrial monitoring, yet remains challenging due to complex inter-variable dependencies, temporal dynamics, and sparse anomaly labels. We introduce mTSBench, the largest benchmark to date for MTS-AD and unsupervised model selection, spanning 344 labeled time series across 19 datasets and 12 diverse application domains. mTSBench evaluates 24 anomaly detection methods, including large language model (LLM)-based detectors for multivariate time series, and systematically benchmarks unsupervised model selection techniques under standardized conditions. Consistent with prior findings, our results confirm that no single detector excels across datasets, underscoring the importance of model selection. However, even state-of-the-art selection methods remain far from optimal, revealing critical gaps. mTSBench provides a unified evaluation suite to enable rigorous, reproducible comparisons and catalyze future advances in adaptive anomaly detection and robust model selection.

**Link**: [arxiv](http://arxiv.org/abs/2506.21550v1),  [pdf](http://arxiv.org/pdf/2506.21550v1)

**Tags**: cs.LG cs.AI 



### Where to find Grokking in LLM Pretraining? Monitor   Memorization-to-Generalization without Test
**Authors**: Ziyue Li, Chenrui Fan, Tianyi Zhou

**Updated**: 2025-06-26T17:59:58Z

**Summary**: Grokking, i.e., test performance keeps improving long after training loss converged, has been recently witnessed in neural network training, making the mechanism of generalization and other emerging capabilities such as reasoning mysterious. While prior studies usually train small models on a few toy or highly-specific tasks for thousands of epochs, we conduct the first study of grokking on checkpoints during one-pass pretraining of a 7B large language model (LLM), i.e., OLMoE. We compute the training loss and evaluate generalization on diverse benchmark tasks, including math reasoning, code generation, and commonsense/domain-specific knowledge retrieval tasks.   Our study, for the first time, verifies that grokking still happens in the pretraining of large-scale foundation models, though different data may enter grokking stages asynchronously. We further demystify grokking's "emergence of generalization" by investigating LLM internal dynamics. Specifically, we find that training samples' pathways (i.e., expert choices across layers) evolve from random, instance-specific to more structured and shareable between samples during grokking. Also, the complexity of a sample's pathway reduces despite the converged loss. These indicate a memorization-to-generalization conversion, providing a mechanistic explanation of delayed generalization. In the study, we develop two novel metrics to quantify pathway distance and the complexity of a single pathway. We show their ability to predict the generalization improvement on diverse downstream tasks. They are efficient, simple to compute and solely dependent on training data. Hence, they have practical value for pretraining, enabling us to monitor the generalization performance without finetuning and test. Theoretically, we show that more structured pathways reduce model complexity and improve the generalization bound.

**Link**: [arxiv](http://arxiv.org/abs/2506.21551v1),  [pdf](http://arxiv.org/pdf/2506.21551v1)

**Tags**: cs.LG 



### PsyLite Technical Report
**Authors**: Fangjun Ding, Renyu Zhang, Xinyu Feng, Chengye Xie, Zheng Zhang, Yanting Zhang

**Updated**: 2025-06-26T17:54:42Z

**Summary**: With the rapid development of digital technology, AI-driven psychological counseling has gradually become an important research direction in the field of mental health. However, existing models still have deficiencies in dialogue safety, detailed scenario handling, and lightweight deployment. To address these issues, this study proposes PsyLite, a lightweight psychological counseling large language model agent developed based on the base model InternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation data fine-tuning and ORPO preference optimization), PsyLite enhances the model's deep-reasoning ability, psychological counseling ability, and safe dialogue ability. After deployment using Ollama and Open WebUI, a custom workflow is created with Pipelines. An innovative conditional RAG is designed to introduce crosstalk humor elements at appropriate times during psychological counseling to enhance user experience and decline dangerous requests to strengthen dialogue safety. Evaluations show that PsyLite outperforms the baseline models in the Chinese general evaluation (CEval), psychological counseling professional evaluation (CPsyCounE), and dialogue safety evaluation (SafeDialBench), particularly in psychological counseling professionalism (CPsyCounE score improvement of 47.6\%) and dialogue safety (\safe{} score improvement of 2.4\%). Additionally, the model uses quantization technology (GGUF q4\_k\_m) to achieve low hardware deployment (5GB memory is sufficient for operation), providing a feasible solution for psychological counseling applications in resource-constrained environments.

**Link**: [arxiv](http://arxiv.org/abs/2506.21536v1),  [pdf](http://arxiv.org/pdf/2506.21536v1)

**Tags**: cs.AI cs.HC 



### Exploring the Design Space of 3D MLLMs for CT Report Generation
**Authors**: Mohammed Baharoon, Jun Ma, Congyu Fang, Augustin Toma, Bo Wang

**Updated**: 2025-06-26T17:54:20Z

**Summary**: Multimodal Large Language Models (MLLMs) have emerged as a promising way to automate Radiology Report Generation (RRG). In this work, we systematically investigate the design space of 3D MLLMs, including visual input representation, projectors, Large Language Models (LLMs), and fine-tuning techniques for 3D CT report generation. We also introduce two knowledge-based report augmentation methods that improve performance on the GREEN score by up to 10\%, achieving the 2nd place on the MICCAI 2024 AMOS-MM challenge. Our results on the 1,687 cases from the AMOS-MM dataset show that RRG is largely independent of the size of LLM under the same training protocol. We also show that larger volume size does not always improve performance if the original ViT was pre-trained on a smaller volume size. Lastly, we show that using a segmentation mask along with the CT volume improves performance. The code is publicly available at https://github.com/bowang-lab/AMOS-MM-Solution

**Link**: [arxiv](http://arxiv.org/abs/2506.21535v1),  [pdf](http://arxiv.org/pdf/2506.21535v1)

**Tags**: eess.IV cs.CV cs.LG 



### "What's Up, Doc?": Analyzing How Users Seek Health Information in   Large-Scale Conversational AI Datasets
**Authors**: Akshay Paruchuri, Maryam Aziz, Rohit Vartak, Ayman Ali, Best Uchehara, Xin Liu, Ishan Chatterjee, Monica Agrawal

**Updated**: 2025-06-26T17:52:18Z

**Summary**: People are increasingly seeking healthcare information from large language models (LLMs) via interactive chatbots, yet the nature and inherent risks of these conversations remain largely unexplored. In this paper, we filter large-scale conversational AI datasets to achieve HealthChat-11K, a curated dataset of 11K real-world conversations composed of 25K user messages. We use HealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs when seeking healthcare information in order to systematically study user interactions across 21 distinct health specialties. Our analysis reveals insights into the nature of how and why users seek health information, such as common interactions, instances of incomplete context, affective behaviors, and interactions (e.g., leading questions) that can induce sycophancy, underscoring the need for improvements in the healthcare support capabilities of LLMs deployed as conversational AI. Code and artifacts to retrieve our analyses and combine them into a curated dataset can be found here: https://github.com/yahskapar/HealthChat

**Link**: [arxiv](http://arxiv.org/abs/2506.21532v1),  [pdf](http://arxiv.org/pdf/2506.21532v1)

**Tags**: cs.CL cs.AI cs.CY 



### OpenNER 1.0: Standardized Open-Access Named Entity Recognition Datasets   in 50+ Languages
**Authors**: Chester Palen-Michel, Maxwell Pickering, Maya Kruse, Jonne Sälevä, Constantine Lignos

**Updated**: 2025-06-26T17:51:40Z

**Summary**: We present OpenNER 1.0, a standardized collection of openly-available named entity recognition (NER) datasets. OpenNER contains 36 NER corpora that span 52 languages, human-annotated in varying named entity ontologies. We correct annotation format issues, standardize the original datasets into a uniform representation with consistent entity type names across corpora, and provide the collection in a structure that enables research in multilingual and multi-ontology NER. We provide baseline results using three pretrained multilingual language models and two large language models to compare the performance of recent models and facilitate future research in NER. We find that no single model is best in all languages and that significant work remains to obtain high performance from LLMs on the NER task.

**Link**: [arxiv](http://arxiv.org/abs/2412.09587v2),  [pdf](http://arxiv.org/pdf/2412.09587v2)

**Tags**: cs.CL 



### Chain-of-Sketch: Enabling Global Visual Reasoning
**Authors**: Aryo Lotfi, Enrico Fini, Samy Bengio, Moin Nabi, Emmanuel Abbe

**Updated**: 2025-06-26T17:48:33Z

**Summary**: Modern vision models have achieved remarkable success in benchmarks where local features provide critical information about the target. There is now a growing interest in tackling tasks requiring more global reasoning, where local features do not provide significant information. Minsky and Papert put forward such tasks in 1969 with their connectivity study, exposing the limitations of the perceptron model. In this paper, we introduce an expanded set of global visual datasets involving graphs, strings, mazes, and image grids. We show that large vision models still struggle to learn these tasks efficiently. Similarly, state-of-the-art multi-modal LLMs perform poorly on these datasets. We explain this learning inefficiency by means of the 'globality degree' measure. To mitigate this, we propose a method called chain-of-sketch (CoS). Similar to the chain-of-thought and scratchpad techniques used in language models, CoS breaks the original task into intermediate visual steps to help learn a complex task. In addition, we show that not all CoS strategies perform equally well. Our key insight is to impose a Markovian structure on the CoS frames. This leads to the introduction of 'inductive CoS' which achieves better out-of-distribution generalization and performs well even with smaller models compared to non-inductive variants.

**Link**: [arxiv](http://arxiv.org/abs/2410.08165v2),  [pdf](http://arxiv.org/pdf/2410.08165v2)

**Tags**: cs.LG cs.CV 



### Potemkin Understanding in Large Language Models
**Authors**: Marina Mancoridis, Bec Weeks, Keyon Vafa, Sendhil Mullainathan

**Updated**: 2025-06-26T17:41:35Z

**Summary**: Large language models (LLMs) are regularly evaluated using benchmark datasets. But what justifies making inferences about an LLM's capabilities based on its answers to a curated set of questions? This paper first introduces a formal framework to address this question. The key is to note that the benchmarks used to test LLMs -- such as AP exams -- are also those used to test people. However, this raises an implication: these benchmarks are only valid tests if LLMs misunderstand concepts in ways that mirror human misunderstandings. Otherwise, success on benchmarks only demonstrates potemkin understanding: the illusion of understanding driven by answers irreconcilable with how any human would interpret a concept. We present two procedures for quantifying the existence of potemkins: one using a specially designed benchmark in three domains, the other using a general procedure that provides a lower-bound on their prevalence. We find that potemkins are ubiquitous across models, tasks, and domains. We also find that these failures reflect not just incorrect understanding, but deeper internal incoherence in concept representations.

**Link**: [arxiv](http://arxiv.org/abs/2506.21521v1),  [pdf](http://arxiv.org/pdf/2506.21521v1)

**Tags**: cs.CL cs.AI 



### QuEST: Low-bit Diffusion Model Quantization via Efficient Selective   Finetuning
**Authors**: Haoxuan Wang, Yuzhang Shang, Zhihang Yuan, Junyi Wu, Junchi Yan, Yan Yan

**Updated**: 2025-06-26T17:36:29Z

**Summary**: The practical deployment of diffusion models is still hindered by the high memory and computational overhead. Although quantization paves a way for model compression and acceleration, existing methods face challenges in achieving low-bit quantization efficiently. In this paper, we identify imbalanced activation distributions as a primary source of quantization difficulty, and propose to adjust these distributions through weight finetuning to be more quantization-friendly. We provide both theoretical and empirical evidence supporting finetuning as a practical and reliable solution. Building on this approach, we further distinguish two critical types of quantized layers: those responsible for retaining essential temporal information and those particularly sensitive to bit-width reduction. By selectively finetuning these layers under both local and global supervision, we mitigate performance degradation while enhancing quantization efficiency. Our method demonstrates its efficacy across three high-resolution image generation tasks, obtaining state-of-the-art performance across multiple bit-width settings.

**Link**: [arxiv](http://arxiv.org/abs/2402.03666v4),  [pdf](http://arxiv.org/pdf/2402.03666v4)

**Tags**: cs.CV 



### Multi-Preference Lambda-weighted Listwise DPO for Dynamic Preference   Alignment
**Authors**: Yuhui Sun, Xiyao Wang, Zixi Li, Jinman Zhao

**Updated**: 2025-06-26T17:28:25Z

**Summary**: While large-scale unsupervised language models (LMs) capture broad world knowledge and reasoning capabilities, steering their behavior toward desired objectives remains challenging due to the lack of explicit supervision. Existing alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on training a reward model and performing reinforcement learning to align with human preferences. However, RLHF is often computationally intensive, unstable, and sensitive to hyperparameters.   To address these limitations, Direct Preference Optimization (DPO) was introduced as a lightweight and stable alternative, enabling direct alignment of language models with pairwise preference data via classification loss. However, DPO and its extensions generally assume a single static preference distribution, limiting flexibility in multi-objective or dynamic alignment settings.   In this paper, we propose a novel framework: Multi-Preference Lambda-weighted Listwise DPO, which extends DPO to incorporate multiple human preference dimensions (e.g., helpfulness, harmlessness, informativeness) and enables dynamic interpolation through a controllable simplex-weighted formulation. Our method supports both listwise preference feedback and flexible alignment across varying user intents without re-training. Empirical and theoretical analysis demonstrates that our method is as effective as traditional DPO on static objectives while offering greater generality and adaptability for real-world deployment.

**Link**: [arxiv](http://arxiv.org/abs/2506.19780v2),  [pdf](http://arxiv.org/pdf/2506.19780v2)

**Tags**: cs.LG I.2.6; I.2.7; I.5.1 



### Towards Navigation-Grade and Deployable Optomechanical Accelerometry
**Authors**: Chang Ge, Daniel Dominguez, Allison Rubenok, Michael Miller, Matt Eichenfield

**Updated**: 2025-06-26T17:26:44Z

**Summary**: We design and experimentally demonstrate an architecture for achieving navigation-grade, fiber-packaged optomechanical accelerometers that can operate with a large dynamic range, over a wide temperature range, and without sophisticated laser sources. Our accelerometer architecture is based on a novel set of design principles that take advantage of the strengths of optomechanical accelerometry while eliminating many of its historical weaknesses. Displacement readout is provided by an integrated, differential strain-sensing Mach-Zehnder interferometer (DSMZI) attached to an ultra-rigid, bulk-micromachined proof mass having a 93.4 kHz fundamental resonance frequency (22.5 pm/g displacement). Despite the extreme rigidity, the high displacement sensitivity provides an insertion loss limited 4.2 $\mu g/\sqrt{\mathrm{Hz}}$ acceleration resolution, with a straight-forward path to achieving 330 $n g/\sqrt{\mathrm{Hz}}$ by improving the fiber-to-chip coupling. Further, we show that the combination of high rigidity and intrinsic differential optical readout makes the device insensitive to the common causes of bias instability, and we measure a bias instability of 6.3 $\mu g$ at 243 seconds. The DSMZI provides a 17 nm optical bandwidth and a temperature operating range of greater than 20 $^\circ\mathrm{C}$, both orders of magnitude larger than previous demonstrations of optomechanical accelerometers. The high rigidity and large optical bandwidth yield an expected dynamic range of 165.4 dB. The combination of high acceleration resolution, high dynamic range, low bias instability, and intrinsic insensitivity to wavelength, temperature, and package stresses makes our device well suited for deployment in realistic environments demanded by real-world applications and demonstrates a path for optomechanical accelerometers to ultimately exceed the performance of all other chip-based accelerometers.

**Link**: [arxiv](http://arxiv.org/abs/2505.11751v2),  [pdf](http://arxiv.org/pdf/2505.11751v2)

**Tags**: physics.optics physics.app-ph physics.ins-det 



### Enhancing User Engagement in Socially-Driven Dialogue through   Interactive LLM Alignments
**Authors**: Jiashuo Wang, Kaitao Song, Chunpu Xu, Changhe Song, Yang Xiao, Dongsheng Li, Lili Qiu, Wenjie Li

**Updated**: 2025-06-26T17:26:17Z

**Summary**: Enhancing user engagement through interactions plays an essential role in socially-driven dialogues. While prior works have optimized models to reason over relevant knowledge or plan a dialogue act flow, the relationship between user engagement and knowledge or dialogue acts is subtle and does not guarantee user engagement in socially-driven dialogues. To this end, we enable interactive LLMs to learn user engagement by leveraging signals from the future development of conversations. Specifically, we adopt a more direct and relevant indicator of user engagement, i.e., the user's reaction related to dialogue intention after the interaction, as a reward to align interactive LLMs. To achieve this, we develop a user simulator to interact with target interactive LLMs and explore interactions between the user and the interactive LLM system via \textit{i$\times$MCTS} (\textit{M}onte \textit{C}arlo \textit{T}ree \textit{S}earch for \textit{i}nteraction). In this way, we collect a dataset containing pairs of higher and lower-quality experiences using \textit{i$\times$MCTS}, and align interactive LLMs for high-level user engagement by direct preference optimization (DPO) accordingly. Experiments conducted on two socially-driven dialogue scenarios (emotional support conversations and persuasion for good) demonstrate that our method effectively enhances user engagement in interactive LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2506.21497v1),  [pdf](http://arxiv.org/pdf/2506.21497v1)

**Tags**: cs.CL 



### Bridging Offline and Online Reinforcement Learning for LLMs
**Authors**: Jack Lanchantin, Angelica Chen, Janice Lan, Xian Li, Swarnadeep Saha, Tianlu Wang, Jing Xu, Ping Yu, Weizhe Yuan, Jason E Weston, Sainbayar Sukhbaatar, Ilia Kulikov

**Updated**: 2025-06-26T17:25:49Z

**Summary**: We investigate the effectiveness of reinforcement learning methods for finetuning large language models when transitioning from offline to semi-online to fully online regimes for both verifiable and non-verifiable tasks. Our experiments cover training on verifiable math as well as non-verifiable instruction following with a set of benchmark evaluations for both. Across these settings, we extensively compare online and semi-online Direct Preference Optimization and Group Reward Policy Optimization objectives, and surprisingly find similar performance and convergence between these variants, which all strongly outperform offline methods. We provide a detailed analysis of the training dynamics and hyperparameter selection strategies to achieve optimal results. Finally, we show that multi-tasking with verifiable and non-verifiable rewards jointly yields improved performance across both task types.

**Link**: [arxiv](http://arxiv.org/abs/2506.21495v1),  [pdf](http://arxiv.org/pdf/2506.21495v1)

**Tags**: cs.CL 



### Prompting with Phonemes: Enhancing LLMs' Multilinguality for Non-Latin   Script Languages
**Authors**: Hoang H Nguyen, Khyati Mahajan, Vikas Yadav, Julian Salazar, Philip S. Yu, Masoud Hashemi, Rishabh Maheshwary

**Updated**: 2025-06-26T17:22:53Z

**Summary**: Although multilingual LLMs have achieved remarkable performance across benchmarks, we find they continue to underperform on non-Latin script languages across contemporary LLM families. This discrepancy arises from the fact that LLMs are pretrained with orthographic scripts, which are dominated by Latin characters that obscure their shared phonology with non-Latin scripts. We propose leveraging phonemic transcriptions as complementary signals to induce script-invariant representations. Our study demonstrates that integrating phonemic signals improves performance across both non-Latin and Latin script languages, with a particularly significant impact on closing the performance gap between the two. Through detailed experiments, we show that phonemic and orthographic scripts retrieve distinct examples for in-context learning (ICL). This motivates our proposed Mixed-ICL retrieval strategy, where further aggregation from both leads to our significant performance improvements for both Latin script languages (up to 12.6%) and non-Latin script languages (up to 15.1%) compared to randomized ICL retrieval.

**Link**: [arxiv](http://arxiv.org/abs/2411.02398v3),  [pdf](http://arxiv.org/pdf/2411.02398v3)

**Tags**: cs.CL cs.AI cs.LG 



### From Web Search towards Agentic Deep Research: Incentivizing Search with   Reasoning Agents
**Authors**: Weizhi Zhang, Yangning Li, Yuanchen Bei, Junyu Luo, Guancheng Wan, Liangwei Yang, Chenxuan Xie, Yuyao Yang, Wei-Chieh Huang, Chunyu Miao, Henry Peng Zou, Xiao Luo, Yusheng Zhao, Yankai Chen, Chunkit Chan, Peilin Zhou, Xinyang Zhang, Chenwei Zhang, Jingbo Shang, Ming Zhang, Yangqiu Song, Irwin King, Philip S. Yu

**Updated**: 2025-06-26T17:18:00Z

**Summary**: Information retrieval is a cornerstone of modern knowledge acquisition, enabling billions of queries each day across diverse domains. However, traditional keyword-based search engines are increasingly inadequate for handling complex, multi-step information needs. Our position is that Large Language Models (LLMs), endowed with reasoning and agentic capabilities, are ushering in a new paradigm termed Agentic Deep Research. These systems transcend conventional information search techniques by tightly integrating autonomous reasoning, iterative retrieval, and information synthesis into a dynamic feedback loop. We trace the evolution from static web search to interactive, agent-based systems that plan, explore, and learn. We also introduce a test-time scaling law to formalize the impact of computational depth on reasoning and search. Supported by benchmark results and the rise of open-source implementations, we demonstrate that Agentic Deep Research not only significantly outperforms existing approaches, but is also poised to become the dominant paradigm for future information seeking. All the related resources, including industry products, research papers, benchmark datasets, and open-source implementations, are collected for the community in https://github.com/DavidZWZ/Awesome-Deep-Research.

**Link**: [arxiv](http://arxiv.org/abs/2506.18959v2),  [pdf](http://arxiv.org/pdf/2506.18959v2)

**Tags**: cs.IR cs.CL cs.LG 



### Efficient and Reuseable Cloud Configuration Search Using Discovery   Spaces
**Authors**: Michael Johnston, Burkhard Ringlein, Christoph Hagleitner, Alessandro Pomponio, Vassilis Vassiliadis, Christian Pinto, Srikumar Venugopal

**Updated**: 2025-06-26T16:54:39Z

**Summary**: Finding the optimal set of cloud resources to deploy a given workload at minimal cost while meeting a defined service level agreement is an active area of research. Combining tens of parameters applicable across a large selection of compute, storage, and services offered by cloud providers with similar numbers of application-specific parameters leads to configuration spaces with millions of deployment options.   In this paper, we propose Discovery Space, an abstraction that formalizes the description of workload configuration problems, and exhibits a set of characteristics required for structured, robust and distributed investigations of large search spaces. We describe a concrete implementation of the Discovery Space abstraction and show that it is generalizable across a diverse set of workloads such as Large Language Model inference and Big Data Analytics.   We demonstrate that our approach enables safe, transparent sharing of data between executions of best-of-breed optimizers increasing the efficiency of optimal configuration detection in large search spaces. We also demonstrate how Discovery Spaces enable transfer and reuse of knowledge across similar search spaces, enabling configuration search speed-ups of over 90%.

**Link**: [arxiv](http://arxiv.org/abs/2506.21467v1),  [pdf](http://arxiv.org/pdf/2506.21467v1)

**Tags**: cs.DC C.4 



### Graphs Meet AI Agents: Taxonomy, Progress, and Future Opportunities
**Authors**: Yuanchen Bei, Weizhi Zhang, Siwen Wang, Weizhi Chen, Sheng Zhou, Hao Chen, Yong Li, Jiajun Bu, Shirui Pan, Yizhou Yu, Irwin King, Fakhri Karray, Philip S. Yu

**Updated**: 2025-06-26T16:54:14Z

**Summary**: AI agents have experienced a paradigm shift, from early dominance by reinforcement learning (RL) to the rise of agents powered by large language models (LLMs), and now further advancing towards a synergistic fusion of RL and LLM capabilities. This progression has endowed AI agents with increasingly strong abilities. Despite these advances, to accomplish complex real-world tasks, agents are required to plan and execute effectively, maintain reliable memory, and coordinate smoothly with other agents. Achieving these capabilities involves contending with ever-present intricate information, operations, and interactions. In light of this challenge, data structurization can play a promising role by transforming intricate and disorganized data into well-structured forms that agents can more effectively understand and process. In this context, graphs, with their natural advantage in organizing, managing, and harnessing intricate data relationships, present a powerful data paradigm for structurization to support the capabilities demanded by advanced AI agents. To this end, this survey presents a first systematic review of how graphs can empower AI agents. Specifically, we explore the integration of graph techniques with core agent functionalities, highlight notable applications, and identify prospective avenues for future research. By comprehensively surveying this burgeoning intersection, we hope to inspire the development of next-generation AI agents equipped to tackle increasingly sophisticated challenges with graphs. Related resources are collected and continuously updated for the community in the Github link.

**Link**: [arxiv](http://arxiv.org/abs/2506.18019v2),  [pdf](http://arxiv.org/pdf/2506.18019v2)

**Tags**: cs.AI 



### PARALLELPROMPT: Extracting Parallelism from Large Language Model Queries
**Authors**: Steven Kolawole, Keshav Santhanam, Virginia Smith, Pratiksha Thaker

**Updated**: 2025-06-26T16:35:54Z

**Summary**: LLM serving systems typically treat user prompts as monolithic inputs, optimizing inference through decoding tricks or inter-query batching. However, many real-world prompts contain latent semantic parallelism--decomposable structures where subtasks can be executed independently to reduce latency while preserving meaning. We introduce PARALLELPROMPT, the first benchmark for measuring intra-query parallelism in natural user prompts. Our dataset comprises over 37,000 real-world prompts from public LLM chat logs, each annotated with a structured schema capturing task templates, shared context, and iteration inputs. These schemas are extracted using LLM-assisted prompting with rule-based multilingual validation. To evaluate the benefits of decomposition, we provide an execution suite that benchmarks serial vs. parallel strategies, measuring latency, structural adherence, and semantic fidelity. Our results show that intra-query parallelism can be successfully parsed in over 75% of curated datasets, unlocking up to 5x speedups on tasks like translation, comprehension, and comparative analysis, with minimal quality degradation. By releasing this benchmark, curation pipeline, and evaluation suite, we provide the first standardized testbed for studying structure-aware execution in LLM serving pipelines.

**Link**: [arxiv](http://arxiv.org/abs/2506.18728v2),  [pdf](http://arxiv.org/pdf/2506.18728v2)

**Tags**: cs.LG 



### Text2Cypher Across Languages: Evaluating Foundational Models Beyond   English
**Authors**: Makbule Gulcin Ozsoy, William Tai

**Updated**: 2025-06-26T16:31:10Z

**Summary**: Recent advances in large language models have enabled natural language interfaces that translate user questions into database queries, such as Text2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database accessibility, most research today focuses solely on English, with limited evaluation in other languages. This paper investigates the performance of foundational LLMs on the Text2Cypher task across multiple languages. We create and release a multilingual test set by translating English questions into Spanish and Turkish while preserving the original Cypher queries, enabling fair cross-lingual comparison. We evaluate multiple foundational models using standardized prompts and metrics. Our results show a consistent performance pattern: highest on English, then Spanish, and lowest on Turkish. We attribute this to differences in training data availability and linguistic characteristics. Additionally, we explore the impact of translating task prompts into Spanish and Turkish. Results show little to no change in evaluation metrics, suggesting prompt translation has minor impact. Our findings highlight the need for more inclusive evaluation and development in multilingual query generation. Future work includes schema localization and fine-tuning across diverse languages.

**Link**: [arxiv](http://arxiv.org/abs/2506.21445v1),  [pdf](http://arxiv.org/pdf/2506.21445v1)

**Tags**: cs.CL cs.IR 



### Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection
**Authors**: Ali Şenol, Garima Agrawal, Huan Liu

**Updated**: 2025-06-26T16:29:45Z

**Summary**: Detecting deceptive conversations on dynamic platforms is increasingly difficult due to evolving language patterns and Concept Drift (CD)-i.e., semantic or topical shifts that alter the context or intent of interactions over time. These shifts can obscure malicious intent or mimic normal dialogue, making accurate classification challenging. While Large Language Models (LLMs) show strong performance in natural language tasks, they often struggle with contextual ambiguity and hallucinations in risk-sensitive scenarios. To address these challenges, we present a Domain Knowledge (DK)-Enhanced LLM framework that integrates pretrained LLMs with structured, task-specific insights to perform fraud and concept drift detection. The proposed architecture consists of three main components: (1) a DK-LLM module to detect fake or deceptive conversations; (2) a drift detection unit (OCDD) to determine whether a semantic shift has occurred; and (3) a second DK-LLM module to classify the drift as either benign or fraudulent. We first validate the value of domain knowledge using a fake review dataset and then apply our full framework to SEConvo, a multiturn dialogue dataset that includes various types of fraud and spam attacks. Results show that our system detects fake conversations with high accuracy and effectively classifies the nature of drift. Guided by structured prompts, the LLaMA-based implementation achieves 98% classification accuracy. Comparative studies against zero-shot baselines demonstrate that incorporating domain knowledge and drift awareness significantly improves performance, interpretability, and robustness in high-stakes NLP applications.

**Link**: [arxiv](http://arxiv.org/abs/2506.21443v1),  [pdf](http://arxiv.org/pdf/2506.21443v1)

**Tags**: cs.CL cs.AI 



### Explainability of Large Language Models using SMILE: Statistical   Model-agnostic Interpretability with Local Explanations
**Authors**: Zeinab Dehghani, Mohammed Naveed Akram, Koorosh Aslansefat, Adil Khan

**Updated**: 2025-06-26T16:16:59Z

**Summary**: Large language models like GPT, LLAMA, and Claude have become incredibly powerful at generating text, but they are still black boxes, so it is hard to understand how they decide what to say. That lack of transparency can be problematic, especially in fields where trust and accountability matter. To help with this, we introduce SMILE, a new method that explains how these models respond to different parts of a prompt. SMILE is model-agnostic and works by slightly changing the input, measuring how the output changes, and then highlighting which words had the most impact. Create simple visual heat maps showing which parts of a prompt matter the most. We tested SMILE on several leading LLMs and used metrics such as accuracy, consistency, stability, and fidelity to show that it gives clear and reliable explanations. By making these models easier to understand, SMILE brings us one step closer to making AI more transparent and trustworthy.

**Link**: [arxiv](http://arxiv.org/abs/2505.21657v3),  [pdf](http://arxiv.org/pdf/2505.21657v3)

**Tags**: cs.CL cs.AI cs.LG 



### Graph Neural Network for Neutrino Physics Event Reconstruction
**Authors**: V Hewes, Adam Aurisano, Giuseppe Cerati, Jim Kowalkowski, Claire Lee, Wei-keng Liao, Daniel Grzenda, Kaushal Gumpula, Xiaohe Zhang

**Updated**: 2025-06-26T16:15:31Z

**Summary**: Liquid Argon Time Projection Chamber (LArTPC) detector technology offers a wealth of high-resolution information on particle interactions, and leveraging that information to its full potential requires sophisticated automated reconstruction techniques. This article describes NuGraph2, a Graph Neural Network (GNN) for low-level reconstruction of simulated neutrino interactions in a LArTPC detector. Simulated neutrino interactions in the MicroBooNE detector geometry are described as heterogeneous graphs, with energy depositions on each detector plane forming nodes on planar subgraphs. The network utilizes a multi-head attention message-passing mechanism to perform background filtering and semantic labelling on these graph nodes, identifying those associated with the primary physics interaction with 98.0\% efficiency and labelling them according to particle type with 94.9\% efficiency. The network operates directly on detector observables across multiple 2D representations, but utilizes a 3D-context-aware mechanism to encourage consistency between these representations. Model inference takes 0.12~s/event on a CPU, and 0.005s/event batched on a GPU. This architecture is designed to be a general-purpose solution for particle reconstruction in neutrino physics, with the potential for deployment across a broad range of detector technologies, and offers a core convolution engine that can be leveraged for a variety of tasks beyond the two described in this article.

**Link**: [arxiv](http://arxiv.org/abs/2403.11872v2),  [pdf](http://arxiv.org/pdf/2403.11872v2)

**Tags**: physics.data-an cs.LG hep-ex 



### Rethinking LLM Training through Information Geometry and Quantum Metrics
**Authors**: Riccardo Di Sipio

**Updated**: 2025-06-26T16:14:42Z

**Summary**: Optimization in large language models (LLMs) unfolds over high-dimensional parameter spaces with non-Euclidean structure. Information geometry frames this landscape using the Fisher information metric, enabling more principled learning via natural gradient descent. Though often impractical, this geometric lens clarifies phenomena such as sharp minima, generalization, and observed scaling laws. We argue that curvature-aware approaches deepen our understanding of LLM training. Finally, we speculate on quantum analogies based on the Fubini-Study metric and Quantum Fisher Information, hinting at efficient optimization in quantum-enhanced systems.

**Link**: [arxiv](http://arxiv.org/abs/2506.15830v2),  [pdf](http://arxiv.org/pdf/2506.15830v2)

**Tags**: cs.CL quant-ph I.2; I.7 



### TracLLM: A Generic Framework for Attributing Long Context LLMs
**Authors**: Yanting Wang, Wei Zou, Runpeng Geng, Jinyuan Jia

**Updated**: 2025-06-26T16:09:36Z

**Summary**: Long context large language models (LLMs) are deployed in many real-world applications such as RAG, agent, and broad LLM-integrated applications. Given an instruction and a long context (e.g., documents, PDF files, webpages), a long context LLM can generate an output grounded in the provided context, aiming to provide more accurate, up-to-date, and verifiable outputs while reducing hallucinations and unsupported claims. This raises a research question: how to pinpoint the texts (e.g., sentences, passages, or paragraphs) in the context that contribute most to or are responsible for the generated output by an LLM? This process, which we call context traceback, has various real-world applications, such as 1) debugging LLM-based systems, 2) conducting post-attack forensic analysis for attacks (e.g., prompt injection attack, knowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources to enhance the trust of users towards outputs generated by LLMs. When applied to context traceback for long context LLMs, existing feature attribution methods such as Shapley have sub-optimal performance and/or incur a large computational cost. In this work, we develop TracLLM, the first generic context traceback framework tailored to long context LLMs. Our framework can improve the effectiveness and efficiency of existing feature attribution methods. To improve the efficiency, we develop an informed search based algorithm in TracLLM. We also develop contribution score ensemble/denoising techniques to improve the accuracy of TracLLM. Our evaluation results show TracLLM can effectively identify texts in a long context that lead to the output of an LLM. Our code and data are at: https://github.com/Wang-Yanting/TracLLM.

**Link**: [arxiv](http://arxiv.org/abs/2506.04202v3),  [pdf](http://arxiv.org/pdf/2506.04202v3)

**Tags**: cs.CR cs.AI cs.LG 



### Carbon-Aware Microservice Deployment for Optimal User Experience on a   Budget
**Authors**: Kevin Kreutz, Philipp Wiesner, Monica Vitali

**Updated**: 2025-06-26T16:07:07Z

**Summary**: The carbon footprint of data centers has recently become a critical concern. So far, most carbon-aware strategies have focused on leveraging the flexibility of scheduling decisions for batch processing by shifting the time and location of workload executions. However, such approaches cannot be applied to service-oriented cloud applications, since they have to be reachable at every point in time and often at low latencies. We propose a carbon-aware approach for operating microservices under hourly carbon budgets. By choosing the most appropriate version and horizontal scaleout for each microservice, our strategy maximizes user experience and revenue while staying within budget constraints. Experiments across various application configurations and carbon budgets demonstrate that the approach adapts properly to changing workloads and carbon intensities.

**Link**: [arxiv](http://arxiv.org/abs/2506.21422v1),  [pdf](http://arxiv.org/pdf/2506.21422v1)

**Tags**: cs.DC 



### Scalable Bayesian Low-Rank Adaptation of Large Language Models via   Stochastic Variational Subspace Inference
**Authors**: Colin Samplawski, Adam D. Cobb, Manoj Acharya, Ramneet Kaur, Susmit Jha

**Updated**: 2025-06-26T15:54:45Z

**Summary**: Despite their widespread use, large language models (LLMs) are known to hallucinate incorrect information and be poorly calibrated. This makes the uncertainty quantification of these models of critical importance, especially in high-stakes domains, such as autonomy and healthcare. Prior work has made Bayesian deep learning-based approaches to this problem more tractable by performing inference over the low-rank adaptation (LoRA) parameters of a fine-tuned model. While effective, these approaches struggle to scale to larger LLMs due to requiring further additional parameters compared to LoRA. In this work we present $\textbf{Scala}$ble $\textbf{B}$ayesian $\textbf{L}$ow-Rank Adaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform Bayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By repurposing the LoRA parameters as projection matrices, we are able to map samples from this subspace into the full weight space of the LLM. This allows us to learn all the parameters of our approach using stochastic variational inference. Despite the low dimensionality of our subspace, we are able to achieve competitive performance with state-of-the-art approaches while only requiring ${\sim}1000$ additional parameters. Furthermore, it allows us to scale up to the largest Bayesian LLM to date, with four times as a many base parameters as prior work.

**Link**: [arxiv](http://arxiv.org/abs/2506.21408v1),  [pdf](http://arxiv.org/pdf/2506.21408v1)

**Tags**: cs.LG cs.AI cs.CL 



### Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented   Generation
**Authors**: Guanting Dong, Xiaoxi Li, Yuyao Zhang, Mengjie Deng

**Updated**: 2025-06-26T15:35:12Z

**Summary**: Real-world live retrieval-augmented generation (RAG) systems face significant challenges when processing user queries that are often noisy, ambiguous, and contain multiple intents. While RAG enhances large language models (LLMs) with external knowledge, current systems typically struggle with such complex inputs, as they are often trained or evaluated on cleaner data. This paper introduces Omni-RAG, a novel framework designed to improve the robustness and effectiveness of RAG systems in live, open-domain settings. Omni-RAG employs LLM-assisted query understanding to preprocess user inputs through three key modules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs with tailored prompts to denoise queries (e.g., correcting spelling errors) and decompose multi-intent queries into structured sub-queries; (2) Intent-Aware Knowledge Retrieval, which performs retrieval for each sub-query from a corpus (i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking and Generation, where a reranker (i.e., BGE) refines document selection before a final response is generated by an LLM (i.e., Falcon-10B) using a chain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG capabilities and the demands of real-world applications, such as those highlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex and noisy queries.

**Link**: [arxiv](http://arxiv.org/abs/2506.21384v1),  [pdf](http://arxiv.org/pdf/2506.21384v1)

**Tags**: cs.CL cs.AI cs.IR 



### GenFlow: Interactive Modular System for Image Generation
**Authors**: Duc-Hung Nguyen, Huu-Phuc Huynh, Minh-Triet Tran, Trung-Nghia Le

**Updated**: 2025-06-26T15:18:00Z

**Summary**: Generative art unlocks boundless creative possibilities, yet its full potential remains untapped due to the technical expertise required for advanced architectural concepts and computational workflows. To bridge this gap, we present GenFlow, a novel modular framework that empowers users of all skill levels to generate images with precision and ease. Featuring a node-based editor for seamless customization and an intelligent assistant powered by natural language processing, GenFlow transforms the complexity of workflow creation into an intuitive and accessible experience. By automating deployment processes and minimizing technical barriers, our framework makes cutting-edge generative art tools available to everyone. A user study demonstrated GenFlow's ability to optimize workflows, reduce task completion times, and enhance user understanding through its intuitive interface and adaptive features. These results position GenFlow as a groundbreaking solution that redefines accessibility and efficiency in the realm of generative art.

**Link**: [arxiv](http://arxiv.org/abs/2506.21369v1),  [pdf](http://arxiv.org/pdf/2506.21369v1)

**Tags**: cs.CV 



### Large Language Model-Powered Agent for C to Rust Code Translation
**Authors**: HoHyun Sim, Hyeonjoong Cho, Yeonghyeon Go, Zhoulai Fu, Ali Shokri, Binoy Ravindran

**Updated**: 2025-06-26T15:16:53Z

**Summary**: The C programming language has been foundational in building system-level software. However, its manual memory management model frequently leads to memory safety issues. In response, a modern system programming language, Rust, has emerged as a memory-safe alternative. Moreover, automating the C-to-Rust translation empowered by the rapid advancements of the generative capabilities of LLMs is gaining growing interest for large volumes of legacy C code. Despite some success, existing LLM-based approaches have constrained the role of LLMs to static prompt-response behavior and have not explored their agentic problem-solving capability. Applying the LLM agentic capability for the C-to-Rust translation introduces distinct challenges, as this task differs from the traditional LLM agent applications, such as math or commonsense QA domains. First, the scarcity of parallel C-to-Rust datasets hinders the retrieval of suitable code translation exemplars for in-context learning. Second, unlike math or commonsense QA, the intermediate steps required for C-to-Rust are not well-defined. Third, it remains unclear how to organize and cascade these intermediate steps to construct a correct translation trajectory. To address these challenges in the C-to-Rust translation, we propose a novel intermediate step, the Virtual Fuzzing-based equivalence Test (VFT), and an agentic planning framework, the LLM-powered Agent for C-to-Rust code translation (LAC2R). The VFT guides LLMs to identify input arguments that induce divergent behaviors between an original C function and its Rust counterpart and to generate informative diagnoses to refine the unsafe Rust code. LAC2R uses the MCTS to systematically organize the LLM-induced intermediate steps for correct translation. We experimentally demonstrated that LAC2R effectively conducts C-to-Rust translation on large-scale, real-world benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2505.15858v2),  [pdf](http://arxiv.org/pdf/2505.15858v2)

**Tags**: cs.PL cs.SE 



### Computational Design of Two-Dimensional MoSi$_2$N$_4$ Family   Field-Effect Transistor for Future Ångström-Scale CMOS Technology Nodes
**Authors**: Che Chen Tho, Zongmeng Yang, Shibo Fang, Shiying Guo, Liemao Cao, Chit Siong Lau, Fei Liu, Shengli Zhang, Jing Lu, L. K. Ang, Lain-Jong Li, Yee Sin Ang

**Updated**: 2025-06-26T15:15:56Z

**Summary**: Advancing complementary metal-oxide-semiconductor (CMOS) technology into the sub-1-nm angstr\"om-scale technology nodes is expected to involve alternative semiconductor channel materials, as silicon transistors encounter severe performance degradation at physical gate lengths below 10 nm. Two-dimensional (2D) semiconductors have emerged as strong candidates for overcoming short-channel effects due to their atomically thin bodies, which inherently suppress electrostatic leakage and improve gate control in aggressively scaled field-effect transistors (FETs). Among the growing library of 2D materials, the MoSi$_2$N$_4$ family -- a synthetic septuple-layered materials -- has attracted increasing attention for its remarkable ambient stability, suitable bandgaps, and favorable carrier transport characteristics, making it a promising platform for next-generation transistors. While experimental realization of sub-10-nm 2D FETs remains technologically demanding, computational device simulation using first-principles density functional theory combined with nonequilibrium Green's function transport simulations provide a powerful and cost-effective route for exploring the performance limits and optimal design of ultrascaled FET. This review consolidates the current progress in the computational design of MoSi$_2$N$_4$ family FETs. We review the physical properties of MoSi$_2$N$_4$ that makes them compelling candidates for transistor applications, as well as the simulated device performance and optimization strategy of MoSi$_2$N$_4$ family FETs. Finally, we identify key challenges and research gaps, and outline future directions that could accelerate the practical deployment of MoSi$_2$N$_4$ family FET in the angstr\"om-scale CMOS era.

**Link**: [arxiv](http://arxiv.org/abs/2506.21366v1),  [pdf](http://arxiv.org/pdf/2506.21366v1)

**Tags**: cond-mat.mtrl-sci cond-mat.mes-hall physics.app-ph 



### Structuralist Approach to AI Literary Criticism: Leveraging Greimas   Semiotic Square for Large Language Models
**Authors**: Fangzhou Dong, Yifan Zeng, Yingpeng Sang, Hong Shen

**Updated**: 2025-06-26T15:10:24Z

**Summary**: Large Language Models (LLMs) excel in understanding and generating text but struggle with providing professional literary criticism for works with profound thoughts and complex narratives. This paper proposes GLASS (Greimas Literary Analysis via Semiotic Square), a structured analytical framework based on Greimas Semiotic Square (GSS), to enhance LLMs' ability to conduct in-depth literary analysis. GLASS facilitates the rapid dissection of narrative structures and deep meanings in narrative works. We propose the first dataset for GSS-based literary criticism, featuring detailed analyses of 48 works. Then we propose quantitative metrics for GSS-based literary criticism using the LLM-as-a-judge paradigm. Our framework's results, compared with expert criticism across multiple works and LLMs, show high performance. Finally, we applied GLASS to 39 classic works, producing original and high-quality analyses that address existing research gaps. This research provides an AI-based tool for literary research and education, offering insights into the cognitive mechanisms underlying literary engagement.

**Link**: [arxiv](http://arxiv.org/abs/2506.21360v1),  [pdf](http://arxiv.org/pdf/2506.21360v1)

**Tags**: cs.CL 



### Semantic Preprocessing for LLM-based Malware Analysis
**Authors**: Benjamin Marais, Tony Quertier, Grégoire Barrue

**Updated**: 2025-06-26T15:09:42Z

**Summary**: In a context of malware analysis, numerous approaches rely on Artificial Intelligence to handle a large volume of data. However, these techniques focus on data view (images, sequences) and not on an expert's view. Noticing this issue, we propose a preprocessing that focuses on expert knowledge to improve malware semantic analysis and result interpretability. We propose a new preprocessing method which creates JSON reports for Portable Executable files. These reports gather features from both static and behavioral analysis, and incorporate packer signature detection, MITRE ATT\&CK and Malware Behavior Catalog (MBC) knowledge. The purpose of this preprocessing is to gather a semantic representation of binary files, understandable by malware analysts, and that can enhance AI models' explainability for malicious files analysis. Using this preprocessing to train a Large Language Model for Malware classification, we achieve a weighted-average F1-score of 0.94 on a complex dataset, representative of market reality.

**Link**: [arxiv](http://arxiv.org/abs/2506.12113v3),  [pdf](http://arxiv.org/pdf/2506.12113v3)

**Tags**: cs.CR cs.AI 



### DynamicBench: Evaluating Real-Time Report Generation in Large Language   Models
**Authors**: Jingyao Li, Hao Sun, Zile Qiao, Yong Jiang, Pengjun Xie, Fei Huang, Hong Xu, Jiaya Jia

**Updated**: 2025-06-26T14:53:44Z

**Summary**: Traditional benchmarks for large language models (LLMs) typically rely on static evaluations through storytelling or opinion expression, which fail to capture the dynamic requirements of real-time information processing in contemporary applications. To address this limitation, we present DynamicBench, a benchmark designed to evaluate the proficiency of LLMs in storing and processing up-to-the-minute data. DynamicBench utilizes a dual-path retrieval pipeline, integrating web searches with local report databases. It necessitates domain-specific knowledge, ensuring accurate responses report generation within specialized fields. By evaluating models in scenarios that either provide or withhold external documents, DynamicBench effectively measures their capability to independently process recent information or leverage contextual enhancements. Additionally, we introduce an advanced report generation system adept at managing dynamic information synthesis. Our experimental results confirm the efficacy of our approach, with our method achieving state-of-the-art performance, surpassing GPT4o in document-free and document-assisted scenarios by 7.0% and 5.8%, respectively. The code and data will be made publicly available.

**Link**: [arxiv](http://arxiv.org/abs/2506.21343v1),  [pdf](http://arxiv.org/pdf/2506.21343v1)

**Tags**: cs.LG 



### AGTCNet: A Graph-Temporal Approach for Principled Motor Imagery EEG   Classification
**Authors**: Galvin Brice S. Lim, Brian Godwin S. Lim, Argel A. Bandala, John Anthony C. Jose, Timothy Scott C. Chu, Edwin Sybingco

**Updated**: 2025-06-26T14:49:10Z

**Summary**: Brain-computer interface (BCI) technology utilizing electroencephalography (EEG) marks a transformative innovation, empowering motor-impaired individuals to engage with their environment on equal footing. Despite its promising potential, developing subject-invariant and session-invariant BCI systems remains a significant challenge due to the inherent complexity and variability of neural activity across individuals and over time, compounded by EEG hardware constraints. While prior studies have sought to develop robust BCI systems, existing approaches remain ineffective in capturing the intricate spatiotemporal dependencies within multichannel EEG signals. This study addresses this gap by introducing the attentive graph-temporal convolutional network (AGTCNet), a novel graph-temporal model for motor imagery EEG (MI-EEG) classification. Specifically, AGTCNet leverages the topographic configuration of EEG electrodes as an inductive bias and integrates graph convolutional attention network (GCAT) to jointly learn expressive spatiotemporal EEG representations. The proposed model significantly outperformed existing MI-EEG classifiers, achieving state-of-the-art performance while utilizing a compact architecture, underscoring its effectiveness and practicality for BCI deployment. With a 49.87% reduction in model size, 64.65% faster inference time, and shorter input EEG signal, AGTCNet achieved a moving average accuracy of 66.82% for subject-independent classification on the BCI Competition IV Dataset 2a, which further improved to 82.88% when fine-tuned for subject-specific classification. On the EEG Motor Movement/Imagery Dataset, AGTCNet achieved moving average accuracies of 64.14% and 85.22% for 4-class and 2-class subject-independent classifications, respectively, with further improvements to 72.13% and 90.54% for subject-specific classifications.

**Link**: [arxiv](http://arxiv.org/abs/2506.21338v1),  [pdf](http://arxiv.org/pdf/2506.21338v1)

**Tags**: cs.LG cs.HC 



### Latent Prototype Routing: Achieving Near-Perfect Load Balancing in   Mixture-of-Experts
**Authors**: Jiajie Yang

**Updated**: 2025-06-26T14:41:18Z

**Summary**: Mixture-of-Experts (MoE) architectures have emerged as a key strategy for scaling large language models (LLMs) efficiently. However, current MoE systems suffer from severe load imbalance, where only a small subset of experts is consistently activated during training and inference, leading to significant underutilization of model capacity and computational resources. In this work, we revisit expert routing through a clustering perspective and propose Latent Prototype Routing (LPR), a novel routing framework that generalizes existing approaches while promoting balanced expert utilization without compromising downstream performance. Extensive experiments across multiple open-source MoE models -- including DeepSeek-V3, Qwen3-MoE, and Mixtral -- demonstrate that LPR reduces the Gini coefficient of expert load from 0.70 to 0.035 on average, improves the min-max expert load ratio from 1e-6 to 0.70, achieving near-perfect load balancing.

**Link**: [arxiv](http://arxiv.org/abs/2506.21328v1),  [pdf](http://arxiv.org/pdf/2506.21328v1)

**Tags**: cs.LG cs.CL 



### "Who Should I Believe?": User Interpretation and Decision-Making When a   Family Healthcare Robot Contradicts Human Memory
**Authors**: Hong Wang, Natalia Calvo-Barajas, Katie Winkle, Ginevra Castellano

**Updated**: 2025-06-26T14:37:54Z

**Summary**: Advancements in robotic capabilities for providing physical assistance, psychological support, and daily health management are making the deployment of intelligent healthcare robots in home environments increasingly feasible in the near future. However, challenges arise when the information provided by these robots contradicts users' memory, raising concerns about user trust and decision-making. This paper presents a study that examines how varying a robot's level of transparency and sociability influences user interpretation, decision-making and perceived trust when faced with conflicting information from a robot. In a 2 x 2 between-subjects online study, 176 participants watched videos of a Furhat robot acting as a family healthcare assistant and suggesting a fictional user to take medication at a different time from that remembered by the user. Results indicate that robot transparency influenced users' interpretation of information discrepancies: with a low transparency robot, the most frequent assumption was that the user had not correctly remembered the time, while with the high transparency robot, participants were more likely to attribute the discrepancy to external factors, such as a partner or another household member modifying the robot's information. Additionally, participants exhibited a tendency toward overtrust, often prioritizing the robot's recommendations over the user's memory, even when suspecting system malfunctions or third-party interference. These findings highlight the impact of transparency mechanisms in robotic systems, the complexity and importance associated with system access control for multi-user robots deployed in home environments, and the potential risks of users' over reliance on robots in sensitive domains such as healthcare.

**Link**: [arxiv](http://arxiv.org/abs/2506.21322v1),  [pdf](http://arxiv.org/pdf/2506.21322v1)

**Tags**: cs.HC cs.RO 



### Multimodal LLMs for Visualization Reconstruction and Understanding
**Authors**: Can Liu, Chunlin Da, Xiaoxiao Long, Yuxiao Yang, Yu Zhang, Yong Wang

**Updated**: 2025-06-26T14:35:59Z

**Summary**: Visualizations are crucial for data communication, yet understanding them requires comprehension of both visual elements and their underlying data relationships. Current multimodal large models, while effective in natural image understanding, struggle with visualization due to their inability to decode the data-to-visual mapping rules and extract structured information. To address these challenges, we present a novel dataset and train multimodal visualization LLMs specifically designed for understanding. Our approach combines chart images with their corresponding vectorized representations, encoding schemes, and data features. The proposed vector format enables compact and accurate reconstruction of visualization content. Experimental results demonstrate significant improvements in both data extraction accuracy and chart reconstruction quality.

**Link**: [arxiv](http://arxiv.org/abs/2506.21319v1),  [pdf](http://arxiv.org/pdf/2506.21319v1)

**Tags**: cs.HC cs.CV 



### Semantic Scene Graph for Ultrasound Image Explanation and Scanning   Guidance
**Authors**: Xuesong Li, Dianye Huang, Yameng Zhang, Nassir Navab, Zhongliang Jiang

**Updated**: 2025-06-26T14:20:13Z

**Summary**: Understanding medical ultrasound imaging remains a long-standing challenge due to significant visual variability caused by differences in imaging and acquisition parameters. Recent advancements in large language models (LLMs) have been used to automatically generate terminology-rich summaries orientated to clinicians with sufficient physiological knowledge. Nevertheless, the increasing demand for improved ultrasound interpretability and basic scanning guidance among non-expert users, e.g., in point-of-care settings, has not yet been explored. In this study, we first introduce the scene graph (SG) for ultrasound images to explain image content to ordinary and provide guidance for ultrasound scanning. The ultrasound SG is first computed using a transformer-based one-stage method, eliminating the need for explicit object detection. To generate a graspable image explanation for ordinary, the user query is then used to further refine the abstract SG representation through LLMs. Additionally, the predicted SG is explored for its potential in guiding ultrasound scanning toward missing anatomies within the current imaging view, assisting ordinary users in achieving more standardized and complete anatomical exploration. The effectiveness of this SG-based image explanation and scanning guidance has been validated on images from the left and right neck regions, including the carotid and thyroid, across five volunteers. The results demonstrate the potential of the method to maximally democratize ultrasound by enhancing its interpretability and usability for ordinaries.

**Link**: [arxiv](http://arxiv.org/abs/2506.19683v2),  [pdf](http://arxiv.org/pdf/2506.19683v2)

**Tags**: cs.CV cs.AI cs.LG eess.IV 



### Detecting Referring Expressions in Visually Grounded Dialogue with   Autoregressive Language Models
**Authors**: Bram Willemsen, Gabriel Skantze

**Updated**: 2025-06-26T14:14:20Z

**Summary**: In this paper, we explore the use of a text-only, autoregressive language modeling approach for the extraction of referring expressions from visually grounded dialogue. More specifically, the aim is to investigate the extent to which the linguistic context alone can inform the detection of mentions that have a (visually perceivable) referent in the visual context of the conversation. To this end, we adapt a pretrained large language model (LLM) to perform a relatively course-grained annotation of mention spans in unfolding conversations by demarcating mention span boundaries in text via next-token prediction. Our findings indicate that even when using a moderately sized LLM, relatively small datasets, and parameter-efficient fine-tuning, a text-only approach can be effective, highlighting the relative importance of the linguistic context for this task. Nevertheless, we argue that the task represents an inherently multimodal problem and discuss limitations fundamental to unimodal approaches.

**Link**: [arxiv](http://arxiv.org/abs/2506.21294v1),  [pdf](http://arxiv.org/pdf/2506.21294v1)

**Tags**: cs.CL cs.AI 



### Small Encoders Can Rival Large Decoders in Detecting Groundedness
**Authors**: Istabrak Abbes, Gabriele Prato, Quentin Fournier, Fernando Rodriguez, Alaa Boukhary, Adam Elwood, Sarath Chandar

**Updated**: 2025-06-26T14:09:41Z

**Summary**: Augmenting large language models (LLMs) with external context significantly improves their performance in natural language processing (NLP) tasks. However, LLMs struggle to answer queries reliably when the provided context lacks information, often resorting to ungrounded speculation or internal knowledge. Groundedness - generating responses strictly supported by the context - is essential for ensuring factual consistency and trustworthiness. This study focuses on detecting whether a given query is grounded in a document provided in context before the costly answer generation by LLMs. Such a detection mechanism can significantly reduce both inference time and resource consumption. We show that lightweight, task specific encoder models such as RoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy comparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in groundedness detection while reducing inference latency by orders of magnitude. The code is available at : https://github.com/chandarlab/Hallucinate-less

**Link**: [arxiv](http://arxiv.org/abs/2506.21288v1),  [pdf](http://arxiv.org/pdf/2506.21288v1)

**Tags**: cs.CL cs.AI cs.IR cs.LG 



### Thinkless: LLM Learns When to Think
**Authors**: Gongfan Fang, Xinyin Ma, Xinchao Wang

**Updated**: 2025-06-26T14:06:49Z

**Summary**: Reasoning Language Models, capable of extended chain-of-thought reasoning, have demonstrated remarkable performance on tasks requiring complex logical inference. However, applying elaborate reasoning for all queries often results in substantial computational inefficiencies, particularly when many problems admit straightforward solutions. This motivates an open question: Can LLMs learn when to think? To answer this, we propose Thinkless, a learnable framework that empowers an LLM to adaptively select between short-form and long-form reasoning, based on both task complexity and the model's ability. Thinkless is trained under a reinforcement learning paradigm and employs two control tokens, <short> for concise responses and <think> for detailed reasoning. At the core of our method is a Decoupled Group Relative Policy Optimization (DeGRPO) algorithm, which decomposes the learning objective of hybrid reasoning into two components: (1) a control token loss that governs the selection of the reasoning mode, and (2) a response loss that improves the accuracy of the generated answers. This decoupled formulation enables fine-grained control over the contributions of each objective, stabilizing training and effectively preventing collapse observed in vanilla GRPO. Empirically, on several benchmarks such as Minerva Algebra, MATH-500, and GSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% - 90%, significantly improving the efficiency of Reasoning Language Models. The code is available at https://github.com/VainF/Thinkless

**Link**: [arxiv](http://arxiv.org/abs/2505.13379v2),  [pdf](http://arxiv.org/pdf/2505.13379v2)

**Tags**: cs.CL cs.AI 



### Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via   Self-Critical Fine-Tuning
**Authors**: Xin Xu, Tianhao Chen, Fan Zhang, Wanlong Liu, Pengxiang Li, Ajay Kumar Jaiswal, Yuchen Yan, Jishan Hu, Yang Wang, Hao Chen, Shiwei Liu, Shizhe Diao, Can Yang, Lu Yin

**Updated**: 2025-06-26T14:05:45Z

**Summary**: While slow-thinking large language models (LLMs) exhibit reflection-like reasoning, commonly referred to as the "aha moment:, their ability to generate informative critiques and refine prior solutions remains limited. In this paper, we introduce Double-Checker, a principled framework designed to enhance the reasoning capabilities of slow-thinking LLMs by fostering explicit self-critique and iterative refinement of their previous solutions. By fine-tuning on our curated 1,730 self-critical instances, Double-Checker empowers long-CoT LLMs to iteratively critique and refine their outputs during inference until they evaluate their solutions as correct under self-generated critiques. We validate the efficacy of Double-Checker across a comprehensive suite of reasoning benchmarks, demonstrating that iterative self-critique significantly enhances the reasoning capabilities of long-CoT LLMs. Notably, our Double-Checker increases the pass@1 performance on challenging AIME benchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These results highlight a promising direction for developing more trustworthy and effective LLMs capable of structured self-critique.

**Link**: [arxiv](http://arxiv.org/abs/2506.21285v1),  [pdf](http://arxiv.org/pdf/2506.21285v1)

**Tags**: cs.CL 



### HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context
**Authors**: Qize Yang, Shimin Yao, Weixuan Chen, Shenghao Fu, Detao Bai, Jiaxing Zhao, Boyuan Sun, Bowen Yin, Xihan Wei, Jingren Zhou

**Updated**: 2025-06-26T14:01:03Z

**Summary**: With the rapid evolution of multimodal large language models, the capacity to deeply understand and interpret human intentions has emerged as a critical capability, which demands detailed and thoughtful reasoning. In recent studies, Reinforcement Learning (RL) has demonstrated potential in enhancing the reasoning capabilities of Large Language Models (LLMs). Nonetheless, the challenges associated with adapting RL to multimodal data and formats remain largely unaddressed. In this paper, we identify two issues in existing multimodal reasoning models: insufficient global context understanding and shortcut problems. Insufficient context understanding can happen when a model misinterprets multimodal context, resulting in incorrect answers. The shortcut problem occurs when the model overlooks crucial clues in multimodal inputs, directly addressing the query without considering the multimodal information. To tackle these issues, we emphasize the necessity for the model to reason with a clear understanding of the global context within multimodal inputs. This global context understanding can effectively prevent the model from overlooking key multimodal cues and ensure a thorough reasoning process. To ensure the accurate interpretation of multimodal context information, we implement a context reward judged by a large language model, alongside format and accuracy rewards. Additionally, to improve complex reasoning capability, we employ the LLM to assess the logical reward, determining whether the reasoning process successfully integrates multimodal information with logical methods. We also introduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating models in understanding complex human intentions and emotions. Our proposed method demonstrates advanced performance across multiple omni-modal benchmarks compared to other open-source omni-modal models.

**Link**: [arxiv](http://arxiv.org/abs/2506.21277v1),  [pdf](http://arxiv.org/pdf/2506.21277v1)

**Tags**: cs.CV cs.CL 



### Cat and Mouse -- Can Fake Text Generation Outpace Detector Systems?
**Authors**: Andrea McGlinchey, Peter J Barclay

**Updated**: 2025-06-26T13:58:43Z

**Summary**: Large language models can produce convincing "fake text" in domains such as academic writing, product reviews, and political news. Many approaches have been investigated for the detection of artificially generated text. While this may seem to presage an endless "arms race", we note that newer LLMs use ever more parameters, training data, and energy, while relatively simple classifiers demonstrate a good level of detection accuracy with modest resources. To approach the question of whether the models' ability to beat the detectors may therefore reach a plateau, we examine the ability of statistical classifiers to identify "fake text" in the style of classical detective fiction. Over a 0.5 version increase, we found that Gemini showed an increased ability to generate deceptive text, while GPT did not. This suggests that reliable detection of fake text may remain feasible even for ever-larger models, though new model architectures may improve their deceptiveness

**Link**: [arxiv](http://arxiv.org/abs/2506.21274v1),  [pdf](http://arxiv.org/pdf/2506.21274v1)

**Tags**: cs.CL 



### DiLoCoX: A Low-Communication Large-Scale Training Framework for   Decentralized Cluster
**Authors**: Ji Qi, WenPeng Zhu, Li Li, Ming Wu, YingJun Wu, Wu He, Xun Gao, Jason Zeng, Michael Heinrich

**Updated**: 2025-06-26T13:45:04Z

**Summary**: The distributed training of foundation models, particularly large language models (LLMs), demands a high level of communication. Consequently, it is highly dependent on a centralized cluster with fast and reliable interconnects. Can we conduct training on slow networks and thereby unleash the power of decentralized clusters when dealing with models exceeding 100 billion parameters? In this paper, we propose DiLoCoX, a low-communication large-scale decentralized cluster training framework. It combines Pipeline Parallelism with Dual Optimizer Policy, One-Step-Delay Overlap of Communication and Local Training, and an Adaptive Gradient Compression Scheme. This combination significantly improves the scale of parameters and the speed of model pre-training. We justify the benefits of one-step-delay overlap of communication and local training, as well as the adaptive gradient compression scheme, through a theoretical analysis of convergence. Empirically, we demonstrate that DiLoCoX is capable of pre-training a 107B foundation model over a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x speedup in distributed training while maintaining negligible degradation in model convergence. To the best of our knowledge, this is the first decentralized training framework successfully applied to models with over 100 billion parameters.

**Link**: [arxiv](http://arxiv.org/abs/2506.21263v1),  [pdf](http://arxiv.org/pdf/2506.21263v1)

**Tags**: cs.LG cs.AI cs.CL 



### Zero-Shot Learning for Obsolescence Risk Forecasting
**Authors**: Elie Saad, Aya Mrabah, Mariem Besbes, Marc Zolghadri, Victor Czmil, Claude Baron, Vincent Bourgeois

**Updated**: 2025-06-26T13:23:57Z

**Summary**: Component obsolescence poses significant challenges in industries reliant on electronic components, causing increased costs and disruptions in the security and availability of systems. Accurate obsolescence risk prediction is essential but hindered by a lack of reliable data. This paper proposes a novel approach to forecasting obsolescence risk using zero-shot learning (ZSL) with large language models (LLMs) to address data limitations by leveraging domain-specific knowledge from tabular datasets. Applied to two real-world datasets, the method demonstrates effective risk prediction. A comparative evaluation of four LLMs underscores the importance of selecting the right model for specific forecasting tasks.

**Link**: [arxiv](http://arxiv.org/abs/2506.21240v1),  [pdf](http://arxiv.org/pdf/2506.21240v1)

**Tags**: cs.LG 



### Enhancing Automatic Term Extraction with Large Language Models via   Syntactic Retrieval
**Authors**: Yongchan Chun, Minhyuk Kim, Dongjun Kim, Chanjun Park, Heuiseok Lim

**Updated**: 2025-06-26T13:14:52Z

**Summary**: Automatic Term Extraction (ATE) identifies domain-specific expressions that are crucial for downstream tasks such as machine translation and information retrieval. Although large language models (LLMs) have significantly advanced various NLP tasks, their potential for ATE has scarcely been examined. We propose a retrieval-based prompting strategy that, in the few-shot setting, selects demonstrations according to \emph{syntactic} rather than semantic similarity. This syntactic retrieval method is domain-agnostic and provides more reliable guidance for capturing term boundaries. We evaluate the approach in both in-domain and cross-domain settings, analyzing how lexical overlap between the query sentence and its retrieved examples affects performance. Experiments on three specialized ATE benchmarks show that syntactic retrieval improves F1-score. These findings highlight the importance of syntactic cues when adapting LLMs to terminology-extraction tasks.

**Link**: [arxiv](http://arxiv.org/abs/2506.21222v1),  [pdf](http://arxiv.org/pdf/2506.21222v1)

**Tags**: cs.CL cs.IR 



### Complexity-aware fine-tuning
**Authors**: Andrey Goncharov, Daniil Vyazhev, Petr Sychev, Edvard Khalafyan, Alexey Zaytsev

**Updated**: 2025-06-26T13:13:24Z

**Summary**: General-purpose Large Language Models (LLMs) are frequently fine-tuned through supervised fine-tuning (SFT) to enhance performance in specific domains. Better results can be achieved by distilling the chain-of-thought of a larger model at the cost of numerous expensive calls and a much greater amount of data. We propose a novel blueprint for efficient fine-tuning that uses reasoning only for complex data identified by entropy. Specifically, across two small open models ($\approx 3B$) we split the training data into complexity categories by a single token answer entropy (ROC AUC $0.73$), fine-tune large language models (LLMs) via SFT and distillation, and show that our pipeline significantly outperforms the standard SFT approach ($0.55$ vs $0.43$ average accuracy) and provides comparable with distillation performance while using $62\%$ less data ($0.55$ average accuracy for both). We publish our code and data to facilitate further research in this direction.

**Link**: [arxiv](http://arxiv.org/abs/2506.21220v1),  [pdf](http://arxiv.org/pdf/2506.21220v1)

**Tags**: cs.LG cs.CL 



### Balancing Privacy, Robustness, and Efficiency in Machine Learning
**Authors**: Youssef Allouah, Rachid Guerraoui, John Stephan

**Updated**: 2025-06-26T13:12:25Z

**Summary**: This position paper argues that achieving robustness, privacy, and efficiency simultaneously in machine learning systems is infeasible under prevailing threat models. The tension between these goals arises not from algorithmic shortcomings but from structural limitations imposed by worst-case adversarial assumptions. We advocate for a systematic research agenda aimed at formalizing the robustness-privacy-efficiency trilemma, exploring how principled relaxations of threat models can unlock better trade-offs, and designing benchmarks that expose rather than obscure the compromises made. By shifting focus from aspirational universal guarantees to context-aware system design, the machine learning community can build models that are truly appropriate for real-world deployment.

**Link**: [arxiv](http://arxiv.org/abs/2312.14712v3),  [pdf](http://arxiv.org/pdf/2312.14712v3)

**Tags**: cs.LG cs.CR cs.DC 



### Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?
**Authors**: Haoang Chi, He Li, Wenjing Yang, Feng Liu, Long Lan, Xiaoguang Ren, Tongliang Liu, Bo Han

**Updated**: 2025-06-26T13:11:01Z

**Summary**: Causal reasoning capability is critical in advancing large language models (LLMs) toward strong artificial intelligence. While versatile LLMs appear to have demonstrated capabilities in understanding contextual causality and providing responses that obey the laws of causality, it remains unclear whether they perform genuine causal reasoning akin to humans. However, current evidence indicates the contrary. Specifically, LLMs are only capable of performing shallow (level-1) causal reasoning, primarily attributed to the causal knowledge embedded in their parameters, but they lack the capacity for genuine human-like (level-2) causal reasoning. To support this hypothesis, methodologically, we delve into the autoregression mechanism of transformer-based LLMs, revealing that it is not inherently causal. Empirically, we introduce a new causal Q&A benchmark called CausalProbe-2024, whose corpora are fresh and nearly unseen for the studied LLMs. The LLMs exhibit a significant performance drop on CausalProbe-2024 compared to earlier benchmarks, indicating the fact that they primarily engage in level-1 causal reasoning. To bridge the gap towards level-2 causal reasoning, we draw inspiration from the fact that human reasoning is usually facilitated by general knowledge and intended goals. We propose G^2-Reasoner, a method that incorporates general knowledge and goal-oriented prompts into LLMs' causal reasoning processes. Experiments demonstrate that G^2-Reasoner significantly enhances LLMs' causal reasoning capability, particularly in fresh and counterfactual contexts. This work sheds light on a new path for LLMs to advance towards genuine causal reasoning, going beyond level-1 and making strides towards level-2.

**Link**: [arxiv](http://arxiv.org/abs/2506.21215v1),  [pdf](http://arxiv.org/pdf/2506.21215v1)

**Tags**: cs.AI cs.CL cs.LG 



### TAPS: Tool-Augmented Personalisation via Structured Tagging
**Authors**: Ekaterina Taktasheva, Jeff Dalton

**Updated**: 2025-06-26T13:09:40Z

**Summary**: Recent advancements in tool-augmented large language models have enabled them to interact with external tools, enhancing their ability to perform complex user tasks. However, existing approaches overlook the role of personalisation in guiding tool use. This work investigates how user preferences can be effectively integrated into goal-oriented dialogue agents. Through extensive analysis, we identify key weaknesses in the ability of LLMs to personalise tool use. To this end, we introduce TAPS, a novel solution that enhances personalised tool use by leveraging a structured tagging tool and an uncertainty-based tool detector. TAPS significantly improves the ability of LLMs to incorporate user preferences, achieving the new state-of-the-art for open source models on the NLSI task.

**Link**: [arxiv](http://arxiv.org/abs/2506.20409v2),  [pdf](http://arxiv.org/pdf/2506.20409v2)

**Tags**: cs.CL 



### $T^3$: Multi-level Tree-based Automatic Program Repair with Large   Language Models
**Authors**: Quanming Liu, Xupeng Bu, Zhichao Yan, Ru Li

**Updated**: 2025-06-26T13:04:28Z

**Summary**: Automatic Program Repair (APR) is a core technology in software development and maintenance, with aims to enable automated defect repair with minimal human intervention. In recent years, the substantial advancements in Large Language Models (LLMs) and the Chain-of-Thought (CoT) techniques have significantly enhanced the reasoning capabilities of these models. However, due to the complex logic and multi-step reasoning ability needed, the application of CoT techniques in the APR domain remains insufficient. This study systematically evaluates the performance of several common CoT techniques in APR tasks and proposes an innovative framework $T^3$, which integrates the powerful reasoning capabilities of LLMs with tree search, effectively improving the precision of generating candidate repair solutions. Furthermore, $T^3$ provides valuable guidance for optimizing sample selection and repair strategies in APR tasks, establishing a robust framework for achieving efficient automated debugging.

**Link**: [arxiv](http://arxiv.org/abs/2506.21211v1),  [pdf](http://arxiv.org/pdf/2506.21211v1)

**Tags**: cs.SE cs.AI 



### MedPrompt: LLM-CNN Fusion with Weight Routing for Medical Image   Segmentation and Classification
**Authors**: Shadman Sobhan, Kazi Abrar Mahmud, Abduz Zami

**Updated**: 2025-06-26T12:57:41Z

**Summary**: Current medical image analysis systems are typically task-specific, requiring separate models for classification and segmentation, and lack the flexibility to support user-defined workflows. To address these challenges, we introduce MedPrompt, a unified framework that combines a few-shot prompted Large Language Model (Llama-4-17B) for high-level task planning with a modular Convolutional Neural Network (DeepFusionLab) for low-level image processing. The LLM interprets user instructions and generates structured output to dynamically route task-specific pretrained weights. This weight routing approach avoids retraining the entire framework when adding new tasks-only task-specific weights are required, enhancing scalability and deployment. We evaluated MedPrompt across 19 public datasets, covering 12 tasks spanning 5 imaging modalities. The system achieves a 97% end-to-end correctness in interpreting and executing prompt-driven instructions, with an average inference latency of 2.5 seconds, making it suitable for near real-time applications. DeepFusionLab achieves competitive segmentation accuracy (e.g., Dice 0.9856 on lungs) and strong classification performance (F1 0.9744 on tuberculosis). Overall, MedPrompt enables scalable, prompt-driven medical imaging by combining the interpretability of LLMs with the efficiency of modular CNNs.

**Link**: [arxiv](http://arxiv.org/abs/2506.21199v1),  [pdf](http://arxiv.org/pdf/2506.21199v1)

**Tags**: cs.CV eess.SP 



### LLM-Based Human-Agent Collaboration and Interaction Systems: A Survey
**Authors**: Henry Peng Zou, Wei-Chieh Huang, Yaozu Wu, Yankai Chen, Chunyu Miao, Hoang Nguyen, Yue Zhou, Weizhi Zhang, Liancheng Fang, Langzhou He, Yangning Li, Dongyuan Li, Renhe Jiang, Xue Liu, Philip S. Yu

**Updated**: 2025-06-26T12:53:30Z

**Summary**: Recent advances in large language models (LLMs) have sparked growing interest in building fully autonomous agents. However, fully autonomous LLM-based agents still face significant challenges, including limited reliability due to hallucinations, difficulty in handling complex tasks, and substantial safety and ethical risks, all of which limit their feasibility and trustworthiness in real-world applications. To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety. These human-agent collaboration systems enable humans and LLM-based agents to collaborate effectively by leveraging their complementary strengths. This paper provides the first comprehensive and structured survey of LLM-HAS. It clarifies fundamental concepts, systematically presents core components shaping these systems, including environment & profiling, human feedback, interaction types, orchestration and communication, explores emerging applications, and discusses unique challenges and opportunities arising from human-AI collaboration. By consolidating current knowledge and offering a structured overview, we aim to foster further research and innovation in this rapidly evolving interdisciplinary field. Paper lists and resources are available at https://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems.

**Link**: [arxiv](http://arxiv.org/abs/2505.00753v4),  [pdf](http://arxiv.org/pdf/2505.00753v4)

**Tags**: cs.CL cs.LG 



### Prompt-Guided Turn-Taking Prediction
**Authors**: Koji Inoue, Mikey Elmers, Yahui Fu, Zi Haur Pang, Divesh Lala, Keiko Ochi, Tatsuya Kawahara

**Updated**: 2025-06-26T12:49:07Z

**Summary**: Turn-taking prediction models are essential components in spoken dialogue systems and conversational robots. Recent approaches leverage transformer-based architectures to predict speech activity continuously and in real-time. In this study, we propose a novel model that enables turn-taking prediction to be dynamically controlled via textual prompts. This approach allows intuitive and explicit control through instructions such as "faster" or "calmer" adapting dynamically to conversational partners and contexts. The proposed model builds upon a transformer-based voice activity projection (VAP) model, incorporating textual prompt embeddings into both channel-wise transformers and a cross-channel transformer. We evaluated the feasibility of our approach using over 950 hours of human-human spoken dialogue data. Since textual prompt data for the proposed approach was not available in existing datasets, we utilized a large language model (LLM) to generate synthetic prompt sentences. Experimental results demonstrated that the proposed model improved prediction accuracy and effectively varied turn-taking timing behaviors according to the textual prompts.

**Link**: [arxiv](http://arxiv.org/abs/2506.21191v1),  [pdf](http://arxiv.org/pdf/2506.21191v1)

**Tags**: cs.CL cs.SD eess.AS 



### Metis-RISE: RL Incentivizes and SFT Enhances Multimodal Reasoning Model   Learning
**Authors**: Haibo Qiu, Xiaohan Lan, Fanfan Liu, Xiaohu Sun, Delian Ruan, Peng Shi, Lin Ma

**Updated**: 2025-06-26T11:45:11Z

**Summary**: Recent advancements in large language models (LLMs) have witnessed a surge in the development of advanced reasoning paradigms, which are now being integrated into multimodal large language models (MLLMs). However, existing approaches often fall short: methods solely employing reinforcement learning (RL) can struggle with sample inefficiency and activating entirely absent reasoning capabilities, while conventional pipelines that initiate with a cold-start supervised fine-tuning (SFT) phase before RL may restrict the model's exploratory capacity and face suboptimal convergence. In this work, we introduce \textbf{Metis-RISE} (\textbf{R}L \textbf{I}ncentivizes and \textbf{S}FT \textbf{E}nhances) for multimodal reasoning model learning. Unlike conventional approaches, Metis-RISE distinctively omits an initial SFT stage, beginning instead with an RL phase (e.g., using a Group Relative Policy Optimization variant) to incentivize and activate the model's latent reasoning capacity. Subsequently, the targeted SFT stage addresses two key challenges identified during RL: (1) \textit{inefficient trajectory sampling} for tasks where the model possesses but inconsistently applies correct reasoning, which we tackle using self-distilled reasoning trajectories from the RL model itself; and (2) \textit{fundamental capability absence}, which we address by injecting expert-augmented knowledge for prompts where the model entirely fails. This strategic application of RL for incentivization followed by SFT for enhancement forms the core of Metis-RISE, leading to two versions of our MLLMs (7B and 72B parameters). Evaluations on the OpenCompass Multimodal Reasoning Leaderboard demonstrate that both models achieve state-of-the-art performance among similar-sized models, with the 72B version ranking fourth overall. Please refer to our project page for open-source information.

**Link**: [arxiv](http://arxiv.org/abs/2506.13056v2),  [pdf](http://arxiv.org/pdf/2506.13056v2)

**Tags**: cs.AI cs.CV cs.LG 



### CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of   Large Language Models
**Authors**: Ping Wu, Guobin Shen, Dongcheng Zhao, Yuwei Wang, Yiting Dong, Yu Shi, Enmeng Lu, Feifei Zhao, Yi Zeng

**Updated**: 2025-06-26T11:34:33Z

**Summary**: Ensuring that Large Language Models (LLMs) align with mainstream human values and ethical norms is crucial for the safe and sustainable development of AI. Current value evaluation and alignment are constrained by Western cultural bias and incomplete domestic frameworks reliant on non-native rules; furthermore, the lack of scalable, rule-driven scenario generation methods makes evaluations costly and inadequate across diverse cultural contexts. To address these challenges, we propose a hierarchical value framework grounded in core Chinese values, encompassing three main dimensions, 12 core values, and 50 derived values. Based on this framework, we construct a large-scale Chinese Values Corpus (CVC) containing over 250,000 value rules enhanced and expanded through human annotation. Experimental results show that CVC-guided scenarios outperform direct generation ones in value boundaries and content diversity. In the evaluation across six sensitive themes (e.g., surrogacy, suicide), seven mainstream LLMs preferred CVC-generated options in over 70.5% of cases, while five Chinese human annotators showed an 87.5% alignment with CVC, confirming its universality, cultural relevance, and strong alignment with Chinese values. Additionally, we construct 400,000 rule-based moral dilemma scenarios that objectively capture nuanced distinctions in conflicting value prioritization across 17 LLMs. Our work establishes a culturally-adaptive benchmarking framework for comprehensive value evaluation and alignment, representing Chinese characteristics. All data are available at https://huggingface.co/datasets/Beijing-AISI/CVC, and the code is available at https://github.com/Beijing-AISI/CVC.

**Link**: [arxiv](http://arxiv.org/abs/2506.01495v4),  [pdf](http://arxiv.org/pdf/2506.01495v4)

**Tags**: cs.CL 



### Do Large Language Models Advocate for Inferentialism?
**Authors**: Yuzuki Arai, Sho Tsugawa

**Updated**: 2025-06-26T11:03:13Z

**Summary**: The emergence of large language models (LLMs) such as ChatGPT and Claude presents new challenges for philosophy of language, particularly regarding the nature of linguistic meaning and representation. While LLMs have traditionally been understood through distributional semantics, this paper explores Robert Brandom's inferential semantics as an alternative foundational framework for understanding these systems. We examine how key features of inferential semantics -- including its anti-representationalist stance, logical expressivism, and quasi-compositional approach -- align with the architectural and functional characteristics of Transformer-based LLMs. Through analysis of the ISA (Inference, Substitution, Anaphora) approach, we demonstrate that LLMs exhibit fundamentally anti-representationalist properties in their processing of language. We further develop a consensus theory of truth appropriate for LLMs, grounded in their interactive and normative dimensions through mechanisms like RLHF. While acknowledging significant tensions between inferentialism's philosophical commitments and LLMs' sub-symbolic processing, this paper argues that inferential semantics provides valuable insights into how LLMs generate meaning without reference to external world representations. Our analysis suggests that LLMs may challenge traditional assumptions in philosophy of language, including strict compositionality and semantic externalism, though further empirical investigation is needed to fully substantiate these theoretical claims.

**Link**: [arxiv](http://arxiv.org/abs/2412.14501v2),  [pdf](http://arxiv.org/pdf/2412.14501v2)

**Tags**: cs.CL 



### How Good Are Synthetic Requirements ? Evaluating LLM-Generated Datasets   for AI4RE
**Authors**: Abdelkarim El-Hajjami, Camille Salinesi

**Updated**: 2025-06-26T10:52:07Z

**Summary**: The shortage of publicly available, labeled requirements datasets remains a major barrier to advancing Artificial Intelligence for Requirements Engineering (AI4RE). While Large Language Models offer promising capabilities for synthetic data generation, systematic approaches to control and optimize the quality of generated requirements remain underexplored. This paper presents Synthline v1, an enhanced Product Line approach for generating synthetic requirements data that extends our earlier v0 version with advanced generation strategies and curation techniques. We investigate four research questions assessing how prompting strategies, automated prompt optimization, and post-generation curation affect data quality across four classification tasks: defect detection, functional vs. non-functional, quality vs. non-quality, and security vs. non-security. Our evaluation shows that multi-sample prompting significantly boosts both utility and diversity over single-sample generation, with F1-score gains from 6 to 44 points. The use of PACE (Prompt Actor-Critic Editing) for automated prompt optimization yields task-dependent results, greatly improving functional classification (+32.5 points) but reducing performance on others. Interestingly, similarity-based curation improves diversity but often harms classification performance, indicating that some redundancy may help ML models. Most importantly, our results show that synthetic requirements can match or outperform human-authored ones for specific tasks, with synthetic data surpassing human data for security (+7.8 points) and defect classification (+15.4 points). These findings offer practical insights for AI4RE and chart a viable path to mitigating dataset scarcity through systematic synthetic generation.

**Link**: [arxiv](http://arxiv.org/abs/2506.21138v1),  [pdf](http://arxiv.org/pdf/2506.21138v1)

**Tags**: cs.SE cs.AI 



### Is my Data in your AI Model? Membership Inference Test with Application   to Face Images
**Authors**: Daniel DeAlcala, Aythami Morales, Julian Fierrez, Gonzalo Mancera, Ruben Tolosana, Javier Ortega-Garcia

**Updated**: 2025-06-26T10:49:33Z

**Summary**: This article introduces the Membership Inference Test (MINT), a novel approach that aims to empirically assess if given data was used during the training of AI/ML models. Specifically, we propose two MINT architectures designed to learn the distinct activation patterns that emerge when an Audited Model is exposed to data used during its training process. These architectures are based on Multilayer Perceptrons (MLPs) and Convolutional Neural Networks (CNNs). The experimental framework focuses on the challenging task of Face Recognition, considering three state-of-the-art Face Recognition systems. Experiments are carried out using six publicly available databases, comprising over 22 million face images in total. Different experimental scenarios are considered depending on the context of the AI model to test. Our proposed MINT approach achieves promising results, with up to 90\% accuracy, indicating the potential to recognize if an AI model has been trained with specific data. The proposed MINT approach can serve to enforce privacy and fairness in several AI applications, e.g., revealing if sensitive or private data was used for training or tuning Large Language Models (LLMs).

**Link**: [arxiv](http://arxiv.org/abs/2402.09225v3),  [pdf](http://arxiv.org/pdf/2402.09225v3)

**Tags**: cs.CV cs.AI 



### Inside Job: Defending Kubernetes Clusters Against Network   Misconfigurations
**Authors**: Jacopo Bufalino, Jose Luis Martin-Navarro, Mario Di Francesco, Tuomas Aura

**Updated**: 2025-06-26T10:31:44Z

**Summary**: Kubernetes has emerged as the de facto standard for container orchestration. Unfortunately, its increasing popularity has also made it an attractive target for malicious actors. Despite extensive research on securing Kubernetes, little attention has been paid to the impact of network configuration on the security of application deployments. This paper addresses this gap by conducting a comprehensive analysis of network misconfigurations in a Kubernetes cluster with specific reference to lateral movement. Accordingly, we carried out an extensive evaluation of 287 open-source applications belonging to six different organizations, ranging from IT companies and public entities to non-profits. As a result, we identified 634 misconfigurations, well beyond what could be found by solutions in the state of the art. We responsibly disclosed our findings to the concerned organizations and engaged in a discussion to assess their severity. As of now, misconfigurations affecting more than thirty applications have been fixed with the mitigations we proposed.

**Link**: [arxiv](http://arxiv.org/abs/2506.21134v1),  [pdf](http://arxiv.org/pdf/2506.21134v1)

**Tags**: cs.CR cs.NI 



### Semantic-aware Digital Twin for AI-based CSI Acquisition
**Authors**: Jiajia Guo, Yiming Cui, Shi Jin

**Updated**: 2025-06-26T09:59:26Z

**Summary**: Artificial intelligence (AI) substantially enhances channel state information (CSI) acquisition performance but is limited by its reliance on single-modality information and deployment challenges, particularly in dataset collection. This paper investigates the use of semantic-aware digital twin (DT) to enhance AI-based CSI acquisition. We first briefly introduce the motivation and recent advancements in AI-driven CSI acquisition and semantic-aware DT employment for air interfaces. Then, we thoroughly explore how semantic-aware DT can bolster AI-based CSI acquisition. We categorizes the semantic-aware DT for AI-based CSI acquisition into two classes: enhancing AI-based CSI acquisition through integration with DT and using DT to aid AI-based CSI deployment. Potential integration frameworks are introduced in detail. Finally, we conclude by outlining potential research directions within the semantic-aware DT-assisted AI-based CSI acquisition.

**Link**: [arxiv](http://arxiv.org/abs/2506.21126v1),  [pdf](http://arxiv.org/pdf/2506.21126v1)

**Tags**: cs.IT math.IT 



### To what extent can current French mobile network support agricultural   robots?
**Authors**: Pierre La Rocca, Gaël Guennebaud, Aurélie Bugeau

**Updated**: 2025-06-26T08:59:33Z

**Summary**: The large-scale integration of robots in agriculture offers many promises for enhancing sustainability and increasing food production. The numerous applications of agricultural robots rely on the transmission of data via mobile network, with the amount of data depending on the services offered by the robots and the level of on-board technology. Nevertheless, infrastructure required to deploy these robots, as well as the related energy and environmental consequences, appear overlooked in the digital agriculture literature. In this study, we propose a method for assessing the additional energy consumption and carbon footprint induced by a large-scale deployment of agricultural robots. Our method also estimates the share of agricultural area that can be managed by the deployed robots with respect to network infrastructure constraints. We have applied this method to metropolitan France mobile network and agricultural parcels for five different robotic scenarios. Our results show that increasing the robot's bitrate needs leads to significant additional impacts, which increase at a pace that is poorly captured by classical linear extrapolation methods. When constraining the network to the existing sites, increased bitrate needs also comes with a rapidly decreasing manageable agricultural area.

**Link**: [arxiv](http://arxiv.org/abs/2505.10044v3),  [pdf](http://arxiv.org/pdf/2505.10044v3)

**Tags**: cs.CY 



### ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for   Real-time Community Question Answering in Industry
**Authors**: Qinwen Chen, Wenbiao Tao, Zhiwei Zhu, Mingfan Xi, Liangzhong Guo, Yuan Wang, Wei Wang, Yunshi Lan

**Updated**: 2025-06-26T08:48:16Z

**Summary**: Community Question Answering (CQA) platforms can be deemed as important knowledge bases in community, but effectively leveraging historical interactions and domain knowledge in real-time remains a challenge. Existing methods often underutilize external knowledge, fail to incorporate dynamic historical QA context, or lack memory mechanisms suited for industrial deployment. We propose ComRAG, a retrieval-augmented generation framework for real-time industrial CQA that integrates static knowledge with dynamic historical QA pairs via a centroid-based memory mechanism designed for retrieval, generation, and efficient storage. Evaluated on three industrial CQA datasets, ComRAG consistently outperforms all baselines--achieving up to 25.9% improvement in vector similarity, reducing latency by 8.7% to 23.3%, and lowering chunk growth from 20.23% to 2.06% over iterations.

**Link**: [arxiv](http://arxiv.org/abs/2506.21098v1),  [pdf](http://arxiv.org/pdf/2506.21098v1)

**Tags**: cs.CL cs.AI 



### Chain-of-Thought Enhanced Shallow Transformers for Wireless Symbol   Detection
**Authors**: Li Fan, Peng Wang, Jing Yang, Cong Shen

**Updated**: 2025-06-26T08:41:45Z

**Summary**: Transformers have shown potential in solving wireless communication problems, particularly via in-context learning (ICL), where models adapt to new tasks through prompts without requiring model updates. However, prior ICL-based Transformer models rely on deep architectures with many layers to achieve satisfactory performance, resulting in substantial storage and computational costs. In this work, we propose CHain Of thOught Symbol dEtection (CHOOSE), a CoT-enhanced shallow Transformer framework for wireless symbol detection. By introducing autoregressive latent reasoning steps within the hidden space, CHOOSE significantly improves the reasoning capacity of shallow models (1-2 layers) without increasing model depth. This design enables lightweight Transformers to achieve detection performance comparable to much deeper models, making them well-suited for deployment on resource-constrained mobile devices. Experimental results demonstrate that our approach outperforms conventional shallow Transformers and achieves performance comparable to that of deep Transformers, while maintaining storage and computational efficiency. This represents a promising direction for implementing Transformer-based algorithms in wireless receivers with limited computational resources.

**Link**: [arxiv](http://arxiv.org/abs/2506.21093v1),  [pdf](http://arxiv.org/pdf/2506.21093v1)

**Tags**: cs.LG cs.IT eess.SP math.IT stat.ML 



### EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for   Efficient Egocentric Perception
**Authors**: Sanjoy Chowdhury, Subrata Biswas, Sayan Nag, Tushar Nagarajan, Calvin Murdock, Ishwarya Ananthabhotla, Yijun Qian, Vamsi Krishna Ithapu, Dinesh Manocha, Ruohan Gao

**Updated**: 2025-06-26T08:09:16Z

**Summary**: Modern perception models, particularly those designed for multisensory egocentric tasks, have achieved remarkable performance but often come with substantial computational costs. These high demands pose challenges for real-world deployment, especially in resource-constrained environments. In this paper, we introduce EgoAdapt, a framework that adaptively performs cross-modal distillation and policy learning to enable efficient inference across different egocentric perception tasks, including egocentric action recognition, active speaker localization, and behavior anticipation. Our proposed policy module is adaptable to task-specific action spaces, making it broadly applicable. Experimental results on three challenging egocentric datasets EPIC-Kitchens, EasyCom, and Aria Everyday Activities demonstrate that our method significantly enhances efficiency, reducing GMACs by up to 89.09%, parameters up to 82.02%, and energy up to 9.6x, while still on-par and in many cases outperforming, the performance of corresponding state-of-the-art models.

**Link**: [arxiv](http://arxiv.org/abs/2506.21080v1),  [pdf](http://arxiv.org/pdf/2506.21080v1)

**Tags**: cs.CV cs.AI cs.LG 



### Post-Quantum and Blockchain-Based Attestation for Trusted FPGAs in B5G   Networks
**Authors**: Ilias Papalamprou, Nikolaos Fotos, Nikolaos Chatzivasileiadis, Anna Angelogianni, Dimosthenis Masouros, Dimitrios Soudris

**Updated**: 2025-06-26T07:58:22Z

**Summary**: The advent of 5G and beyond has brought increased performance networks, facilitating the deployment of services closer to the user. To meet performance requirements such services require specialized hardware, such as Field Programmable Gate Arrays (FPGAs). However, FPGAs are often deployed in unprotected environments, leaving the user's applications vulnerable to multiple attacks. With the rise of quantum computing, which threatens the integrity of widely-used cryptographic algorithms, the need for a robust security infrastructure is even more crucial. In this paper we introduce a hybrid hardware-software solution utilizing remote attestation to securely configure FPGAs, while integrating Post-Quantum Cryptographic (PQC) algorithms for enhanced security. Additionally, to enable trustworthiness across the whole edge computing continuum, our solution integrates a blockchain infrastructure, ensuring the secure storage of any security evidence. We evaluate the proposed secure configuration process under different PQC algorithms in two FPGA families, showcasing only 2% overheard compared to the non PQC approach.

**Link**: [arxiv](http://arxiv.org/abs/2506.21073v1),  [pdf](http://arxiv.org/pdf/2506.21073v1)

**Tags**: cs.AR 



### Bridding OT and PaaS in Edge-to-Cloud Continuum
**Authors**: Carlos J Barrios, Yves Denneulin

**Updated**: 2025-06-26T07:52:30Z

**Summary**: The Operational Technology Platform as a Service (OTPaaS) initiative provides a structured framework for the efficient management and storage of data. It ensures excellent response times while improving security, reliability, data and technology sovereignty, robustness, and energy efficiency, which are crucial for industrial transformation and data sovereignty. This paper illustrates successful deployment, adaptable application management, and various integration components catering to Edge and Cloud environments. It leverages the advantages of the Platform as a Service model and highlights key challenges that have been addressed for specific use cases.

**Link**: [arxiv](http://arxiv.org/abs/2506.21072v1),  [pdf](http://arxiv.org/pdf/2506.21072v1)

**Tags**: cs.DC cs.PF 



### Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge   Graph
**Authors**: Jingwei Wang, Zai Zhang, Hao Qian, Chunjing Gan, Binbin Hu, Ziqi Liu, Zhiqiang Zhang, Jun Zhou, Bin Shi, Bo Dong

**Updated**: 2025-06-26T07:45:15Z

**Summary**: Teaching large language models (LLMs) to use tools is crucial for improving their problem-solving abilities and expanding their applications. However, effectively using tools is challenging because it requires a deep understanding of tool functionalities and user intentions. Previous methods relied mainly on LLMs to generate instruction data, but the quality of these data was often insufficient. In this paper, we propose a new method that uses knowledge graphs to generate high-quality instruction data for LLMs. Knowledge graphs are manually curated datasets rich in semantic information. We begin by extracting various query pathways from a given knowledge graph, which are transformed into a broad spectrum of user queries. We then translate the relationships between entities into actionable tools and parse the pathways of each query into detailed solution steps, thereby creating high-quality instruction data. Our experiments show that fine-tuning on just a small sample of this synthetic data can significantly improve the tool utilization and overall capabilities of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2506.21071v1),  [pdf](http://arxiv.org/pdf/2506.21071v1)

**Tags**: cs.LG cs.CL 



### MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for   Conversational Stance Detection
**Authors**: Fuqiang Niu, Genan Dai, Yisha Lu, Jiayu Liao, Xiang Li, Hu Huang, Bowen Zhang

**Updated**: 2025-06-26T06:59:30Z

**Summary**: In the realm of contemporary social media, automatic stance detection is pivotal for opinion mining, as it synthesizes and examines user perspectives on contentious topics to uncover prevailing trends and sentiments. Traditional stance detection research often targets individual instances, thereby limiting its capacity to model multi-party discussions typical in real social media scenarios. This shortcoming largely stems from the scarcity of datasets that authentically capture the dynamics of social media interactions, hindering advancements in conversational stance detection. In this paper, we introduce MT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational stance detection. To the best of our knowledge, MT2-CSD is the largest dataset available for this purpose, comprising 24,457 annotated instances and exhibiting the greatest conversational depth, thereby presenting new challenges for stance detection. To address these challenges, we propose the Large Language model enhanced Conversational Relational Attention Network (LLM-CRAN), which exploits the reasoning capabilities of LLMs to improve conversational understanding. We conduct extensive experiments to evaluate the efficacy of LLM-CRAN on the MT2-CSD dataset. The experimental results indicate that LLM-CRAN significantly outperforms strong baseline models in the task of conversational stance detection.

**Link**: [arxiv](http://arxiv.org/abs/2506.21053v1),  [pdf](http://arxiv.org/pdf/2506.21053v1)

**Tags**: cs.CL 



### Evaluating Rare Disease Diagnostic Performance in Symptom Checkers: A   Synthetic Vignette Simulation Approach
**Authors**: Takashi Nishibayashi, Seiji Kanazawa, Kumpei Yamada

**Updated**: 2025-06-26T06:52:46Z

**Summary**: Symptom Checkers (SCs) provide medical information tailored to user symptoms. A critical challenge in SC development is preventing unexpected performance degradation for individual diseases, especially rare diseases, when updating algorithms. This risk stems from the lack of practical pre-deployment evaluation methods. For rare diseases, obtaining sufficient evaluation data from user feedback is difficult. To evaluate the impact of algorithm updates on the diagnostic performance for individual rare diseases before deployment, this study proposes and validates a novel Synthetic Vignette Simulation Approach. This approach aims to enable this essential evaluation efficiently and at a low cost. To estimate the impact of algorithm updates, we generated synthetic vignettes from disease-phenotype annotations in the Human Phenotype Ontology (HPO), a publicly available knowledge base for rare diseases curated by experts. Using these vignettes, we simulated SC interviews to predict changes in diagnostic performance. The effectiveness of this approach was validated retrospectively by comparing the predicted changes with actual performance metrics using the R-squared ($R^2$) coefficient. Our experiment, covering eight past algorithm updates for rare diseases, showed that the proposed method accurately predicted performance changes for diseases with phenotype frequency information in HPO (n=5). For these updates, we found a strong correlation for both Recall@8 change ($R^2$ = 0.83,$p$ = 0.031) and Precision@8 change ($R^2$ = 0.78,$p$ = 0.047). Our proposed method enables the pre-deployment evaluation of SC algorithm changes for individual rare diseases. This evaluation is based on a publicly available medical knowledge database created by experts, ensuring transparency and explainability for stakeholders. Additionally, SC developers can efficiently improve diagnostic performance at a low cost.

**Link**: [arxiv](http://arxiv.org/abs/2506.19750v3),  [pdf](http://arxiv.org/pdf/2506.19750v3)

**Tags**: cs.CL 



### Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning   of LLMs
**Authors**: Yaorui Shi, Sihang Li, Chang Wu, Zhiyuan Liu, Junfeng Fang, Hengxing Cai, An Zhang, Xiang Wang

**Updated**: 2025-06-26T06:52:37Z

**Summary**: Large language models have demonstrated impressive reasoning capabilities but are inherently limited by their knowledge reservoir. Retrieval-augmented reasoning mitigates this limitation by allowing LLMs to query external resources, but existing methods often retrieve irrelevant or noisy information, hindering accurate reasoning. In this paper, we propose AutoRefine, a reinforcement learning post-training framework that adopts a new ``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit knowledge refinement steps between successive search calls, enabling the model to iteratively filter, distill, and organize evidence before generating an answer. Furthermore, we incorporate tailored retrieval-specific rewards alongside answer correctness rewards using group relative policy optimization. Experiments on single-hop and multi-hop QA benchmarks demonstrate that AutoRefine significantly outperforms existing approaches, particularly in complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine issues frequent, higher-quality searches and synthesizes evidence effectively.

**Link**: [arxiv](http://arxiv.org/abs/2505.11277v3),  [pdf](http://arxiv.org/pdf/2505.11277v3)

**Tags**: cs.CL cs.AI 



### MockLLM: A Multi-Agent Behavior Collaboration Framework for Online Job   Seeking and Recruiting
**Authors**: Hongda Sun, Hongzhan Lin, Haiyu Yan, Yang Song, Xin Gao, Rui Yan

**Updated**: 2025-06-26T06:33:55Z

**Summary**: Online recruitment platforms have reshaped job-seeking and recruiting processes, driving increased demand for applications that enhance person-job matching. Traditional methods generally rely on analyzing textual data from resumes and job descriptions, limiting the dynamic, interactive aspects crucial to effective recruitment. Recent advances in Large Language Models (LLMs) have revealed remarkable potential in simulating adaptive, role-based dialogues, making them well-suited for recruitment scenarios. In this paper, we propose \textbf{MockLLM}, a novel framework to generate and evaluate mock interview interactions. The system consists of two key components: mock interview generation and two-sided evaluation in handshake protocol. By simulating both interviewer and candidate roles, MockLLM enables consistent and collaborative interactions for real-time and two-sided matching. To further improve the matching quality, MockLLM further incorporates reflection memory generation and dynamic strategy modification, refining behaviors based on previous experience. We evaluate MockLLM on real-world data Boss Zhipin, a major Chinese recruitment platform. The experimental results indicate that MockLLM outperforms existing methods in matching accuracy, scalability, and adaptability across job domains, highlighting its potential to advance candidate assessment and online recruitment.

**Link**: [arxiv](http://arxiv.org/abs/2405.18113v2),  [pdf](http://arxiv.org/pdf/2405.18113v2)

**Tags**: cs.CL cs.AI 



### SceneGenAgent: Precise Industrial Scene Generation with Coding Agent
**Authors**: Xiao Xia, Dan Zhang, Zibo Liao, Zhenyu Hou, Tianrui Sun, Jing Li, Ling Fu, Yuxiao Dong

**Updated**: 2025-06-26T06:24:08Z

**Summary**: The modeling of industrial scenes is essential for simulations in industrial manufacturing. While large language models (LLMs) have shown significant progress in generating general 3D scenes from textual descriptions, generating industrial scenes with LLMs poses a unique challenge due to their demand for precise measurements and positioning, requiring complex planning over spatial arrangement. To address this challenge, we introduce SceneGenAgent, an LLM-based agent for generating industrial scenes through C# code. SceneGenAgent ensures precise layout planning through a structured and calculable format, layout verification, and iterative refinement to meet the quantitative requirements of industrial scenarios. Experiment results demonstrate that LLMs powered by SceneGenAgent exceed their original performance, reaching up to 81.0% success rate in real-world industrial scene generation tasks and effectively meeting most scene generation requirements. To further enhance accessibility, we construct SceneInstruct, a dataset designed for fine-tuning open-source LLMs to integrate into SceneGenAgent. Experiments show that fine-tuning open-source LLMs on SceneInstruct yields significant performance improvements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our code and data are available at https://github.com/THUDM/SceneGenAgent .

**Link**: [arxiv](http://arxiv.org/abs/2410.21909v3),  [pdf](http://arxiv.org/pdf/2410.21909v3)

**Tags**: cs.CL cs.LG cs.SE 



### Little By Little: Continual Learning via Self-Activated Sparse   Mixture-of-Rank Adaptive Learning
**Authors**: Haodong Lu, Chongyang Zhao, Jason Xue, Lina Yao, Kristen Moore, Dong Gong

**Updated**: 2025-06-26T06:19:05Z

**Summary**: Continual learning (CL) with large pre-trained models is challenged by catastrophic forgetting and task interference. Existing LoRA-based Mixture-of-Experts (MoE) approaches mitigate forgetting by assigning and freezing task-specific adapters, but suffer from interference, redundancy, and ambiguous routing due to coarse adapter-level selection. However, this design introduces three key challenges: 1) Interference: Activating full LoRA experts per input leads to subspace interference and prevents selective reuse of useful components across tasks. 2) Redundancy: Newly added experts often duplicate or contradict existing knowledge due to unnecessary activation of unrelated ranks and insufficient reuse of relevant ones. 3) Ambiguity: Overlapping features across tasks confuse the router, resulting in unstable expert assignments. As more experts accumulate, earlier task routing degrades, accelerating forgetting. We propose MoRA, a Mixture-of-Rank Adaptive learning approach with self-activated and sparse rank activation for CL. Unlike mixing multiple low-rank matrices, MoRA decomposes each rank-r update into r rank-1 components, each treated as an independent expert, enabling fine-grained mixture of rank-1 expert utilization while mitigating interference and redundancy. To avoid ambiguous routing, we propose that each rank-1 expert can infer its own relevance via intermediate activations. Coupled with our proposed rank pruning and activation budgets, MoRA adaptively selects a sparse mixture of ranks per input. We validate MoRA on continual learning tasks with CLIP and large language models (LLMs), analyzing both in-domain learning and out-of-domain forgetting/generalization during fine-tuning. MoRA shows significant effectiveness on enhancing CL with PTMs, and improving generalization while mitigating forgetting.

**Link**: [arxiv](http://arxiv.org/abs/2506.21035v1),  [pdf](http://arxiv.org/pdf/2506.21035v1)

**Tags**: cs.LG 



### PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar   Coordinate Decoupling
**Authors**: Yuxuan Yue, Zukang Xu, Zhihang Yuan, Dawei Yang, Jianlong Wu, Liqiang Nie

**Updated**: 2025-06-26T06:17:49Z

**Summary**: Large Language Models (LLMs) face significant challenges in edge deployment due to their massive parameter scale. Vector Quantization (VQ), a clustering-based quantization method, serves as a prevalent solution to this issue for its extremely low-bit (even at 2-bit) and considerable accuracy. Since a vector is a quantity in mathematics and physics that has both direction and magnitude, existing VQ works typically quantize them in a coupled manner. However, we find that direction exhibits significantly greater sensitivity to quantization compared to the magnitude. For instance, when separately clustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the accuracy drop of zero-shot tasks are 46.5\% and 2.3\%, respectively. This gap even increases with the reduction of clustering centers. Further, Euclidean distance, a common metric to access vector similarities in current VQ works, places greater emphasis on reducing the magnitude error. This property is contrary to the above finding, unavoidably leading to larger quantization errors. To these ends, this paper proposes Polar Coordinate Decoupled Vector Quantization (PCDVQ), an effective and efficient VQ framework consisting of two key modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors into their polar coordinate representations and perform independent quantization of the direction and magnitude parameters.2) Distribution Aligned Codebook Construction (DACC), which optimizes the direction and magnitude codebooks in accordance with the source distribution. Experimental results show that PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\% zero-shot accuracy, establishing a novel paradigm for accurate and highly compressed LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2506.05432v2),  [pdf](http://arxiv.org/pdf/2506.05432v2)

**Tags**: cs.LG cs.AI 



### BLOCKS: Blockchain-supported Cross-Silo Knowledge Sharing for Efficient   LLM Services
**Authors**: Zhaojiacheng Zhou, Hongze Liu, Shijing Yuan, Hanning Zhang, Jiong Lou, Chentao Wu, Jie Li

**Updated**: 2025-06-26T06:16:33Z

**Summary**: The hallucination problem of Large Language Models (LLMs) has increasingly drawn attention. Augmenting LLMs with external knowledge is a promising solution to address this issue. However, due to privacy and security concerns, a vast amount of downstream task-related knowledge remains dispersed and isolated across various "silos," making it difficult to access. To bridge this knowledge gap, we propose a blockchain-based external knowledge framework that coordinates multiple knowledge silos to provide reliable foundational knowledge for large model retrieval while ensuring data security. Technically, we distill knowledge from local data into prompts and execute transactions and records on the blockchain. Additionally, we introduce a reputation mechanism and cross-validation to ensure knowledge quality and provide incentives for participation. Furthermore, we design a query generation framework that provides a direct API interface for large model retrieval. To evaluate the performance of our proposed framework, we conducted extensive experiments on various knowledge sources. The results demonstrate that the proposed framework achieves efficient LLM service knowledge sharing in blockchain environments.

**Link**: [arxiv](http://arxiv.org/abs/2506.21033v1),  [pdf](http://arxiv.org/pdf/2506.21033v1)

**Tags**: cs.DC 



### Large Language Models Acing Chartered Accountancy
**Authors**: Jatin Gupta, Akhil Sharma, Saransh Singhania, Mohammad Adnan, Sakshi Deo, Ali Imam Abidi, Keshav Gupta

**Updated**: 2025-06-26T06:10:37Z

**Summary**: Advanced intelligent systems, particularly Large Language Models (LLMs), are significantly reshaping financial practices through advancements in Natural Language Processing (NLP). However, the extent to which these models effectively capture and apply domain-specific financial knowledge remains uncertain. Addressing a critical gap in the expansive Indian financial context, this paper introduces CA-Ben, a Chartered Accountancy benchmark specifically designed to evaluate the financial, legal, and quantitative reasoning capabilities of LLMs. CA-Ben comprises structured question-answer datasets derived from the rigorous examinations conducted by the Institute of Chartered Accountants of India (ICAI), spanning foundational, intermediate, and advanced CA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1 405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated using standardized protocols. Results indicate variations in performance, with Claude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and legal reasoning. Notable challenges emerged in numerical computations and legal interpretations. The findings emphasize the strengths and limitations of current LLMs, suggesting future improvements through hybrid reasoning and retrieval-augmented generation methods, particularly for quantitative analysis and accurate legal interpretation.

**Link**: [arxiv](http://arxiv.org/abs/2506.21031v1),  [pdf](http://arxiv.org/pdf/2506.21031v1)

**Tags**: cs.CL cs.AI 



### STEP Planner: Constructing cross-hierarchical subgoal tree as an   embodied long-horizon task planner
**Authors**: Zhou Tianxing, Wang Zhirui, Ao Haojia, Chen Guangyan, Xing Boyang, Cheng Jingwen, Yang Yi, Yue Yufeng

**Updated**: 2025-06-26T06:10:02Z

**Summary**: The ability to perform reliable long-horizon task planning is crucial for deploying robots in real-world environments. However, directly employing Large Language Models (LLMs) as action sequence generators often results in low success rates due to their limited reasoning ability for long-horizon embodied tasks. In the STEP framework, we construct a subgoal tree through a pair of closed-loop models: a subgoal decomposition model and a leaf node termination model. Within this framework, we develop a hierarchical tree structure that spans from coarse to fine resolutions. The subgoal decomposition model leverages a foundation LLM to break down complex goals into manageable subgoals, thereby spanning the subgoal tree. The leaf node termination model provides real-time feedback based on environmental states, determining when to terminate the tree spanning and ensuring each leaf node can be directly converted into a primitive action. Experiments conducted in both the VirtualHome WAH-NL benchmark and on real robots demonstrate that STEP achieves long-horizon embodied task completion with success rates up to 34% (WAH-NL) and 25% (real robot) outperforming SOTA methods.

**Link**: [arxiv](http://arxiv.org/abs/2506.21030v1),  [pdf](http://arxiv.org/pdf/2506.21030v1)

**Tags**: cs.RO 



### Are Ultrathin Stents Optimal for Bifurcation Lesions? Insights from   Computational Modelling of Provisional and DK-Crush Techniques
**Authors**: Andrea Colombo, Dario Carbonaro, Mingzi Zhang, Chi Shen, Ramtin Gharleghi, Ankush Kapoor, Claudio Chiastra, Nigel Jepson, Mark Webster, Susann Beier

**Updated**: 2025-06-26T06:09:57Z

**Summary**: Complex coronary bifurcation lesions remain challenging in percutaneous coronary intervention, with stent design and deployment strategy influencing clinical outcomes. This study compares the mechanical and hemodynamic performance of the ultrathin-strut Orsiro and thin-strut Xience Sierra stent in Provisional Side Branch (PSB) and Double Kissing Crush (DKC) techniques. We used finite element analyses of bifurcation stent deployment to assess malapposition, ostium clearance, and arterial wall stress for both techniques. Computational fluid dynamics simulations quantified the luminal exposure to low Time-Averaged Endothelial Shear Stress (TAESS below 0.4 Pa) and high shear rates (above 1000 1/s). In PSB, Orsiro showed higher malapposition (13.0% vs 9.6%) but improved SB ostium clearance (77% vs 64%) and lower low-TAESS exposure (30.3% vs 33.6%) compared to Xience. Orsiro also produced higher arterial wall stresses, particularly during kissing balloon inflation. In DKC, differences in malapposition and ostium clearance diminished between stents, though Orsiro retained a hemodynamic advantage with lower low-TAESS (28.2% vs 36.3%).Stent design influenced outcomes more strongly in PSB, where anatomical interaction and platform-specific behavior impacted both structural and hemodynamic results. In DKC, procedural complexity minimized those differences, making the stenting technique the primary performance driver. Nonetheless, Orsiro consistently preserved more favorable flow conditions. These findings highlight the need to match device selection with lesion characteristics in PSB, while in DKC, optimizing procedural steps may have a greater impact than the choice of stent platform.

**Link**: [arxiv](http://arxiv.org/abs/2506.21029v1),  [pdf](http://arxiv.org/pdf/2506.21029v1)

**Tags**: physics.med-ph 



### Multimodal Prompt Alignment for Facial Expression Recognition
**Authors**: Fuyan Ma, Yiran He, Bin Sun, Shutao Li

**Updated**: 2025-06-26T05:28:57Z

**Summary**: Prompt learning has been widely adopted to efficiently adapt vision-language models (VLMs) like CLIP for various downstream tasks. Despite their success, current VLM-based facial expression recognition (FER) methods struggle to capture fine-grained textual-visual relationships, which are essential for distinguishing subtle differences between facial expressions. To address this challenge, we propose a multimodal prompt alignment framework for FER, called MPA-FER, that provides fine-grained semantic guidance to the learning process of prompted visual features, resulting in more precise and interpretable representations. Specifically, we introduce a multi-granularity hard prompt generation strategy that utilizes a large language model (LLM) like ChatGPT to generate detailed descriptions for each facial expression. The LLM-based external knowledge is injected into the soft prompts by minimizing the feature discrepancy between the soft prompts and the hard prompts. To preserve the generalization abilities of the pretrained CLIP model, our approach incorporates prototype-guided visual feature alignment, ensuring that the prompted visual features from the frozen image encoder align closely with class-specific prototypes. Additionally, we propose a cross-modal global-local alignment module that focuses on expression-relevant facial features, further improving the alignment between textual and visual features. Extensive experiments demonstrate our framework outperforms state-of-the-art methods on three FER benchmark datasets, while retaining the benefits of the pretrained model and minimizing computational costs.

**Link**: [arxiv](http://arxiv.org/abs/2506.21017v1),  [pdf](http://arxiv.org/pdf/2506.21017v1)

**Tags**: cs.CV cs.AI 



### Doppelganger Method: Breaking Role Consistency in LLM Agent via   Prompt-based Transferable Adversarial Attack
**Authors**: Daewon Kang, YeongHwan Shin, Doyeon Kim, Kyu-Hwan Jung, Meong Hi Son

**Updated**: 2025-06-26T05:18:19Z

**Summary**: Since the advent of large language models, prompt engineering now enables the rapid, low-effort creation of diverse autonomous agents that are already in widespread use. Yet this convenience raises urgent concerns about the safety, robustness, and behavioral consistency of the underlying prompts, along with the pressing challenge of preventing those prompts from being exposed to user's attempts. In this paper, we propose the ''Doppelganger method'' to demonstrate the risk of an agent being hijacked, thereby exposing system instructions and internal information. Next, we define the ''Prompt Alignment Collapse under Adversarial Transfer (PACAT)'' level to evaluate the vulnerability to this adversarial transfer attack. We also propose a ''Caution for Adversarial Transfer (CAT)'' prompt to counter the Doppelganger method. The experimental results demonstrate that the Doppelganger method can compromise the agent's consistency and expose its internal information. In contrast, CAT prompts enable effective defense against this adversarial attack.

**Link**: [arxiv](http://arxiv.org/abs/2506.14539v2),  [pdf](http://arxiv.org/pdf/2506.14539v2)

**Tags**: cs.AI cs.CR 



### VisionGuard: Synergistic Framework for Helmet Violation Detection
**Authors**: Lam-Huy Nguyen, Thinh-Phuc Nguyen, Thanh-Hai Nguyen, Gia-Huy Dinh, Minh-Triet Tran, Trung-Nghia Le

**Updated**: 2025-06-26T04:45:18Z

**Summary**: Enforcing helmet regulations among motorcyclists is essential for enhancing road safety and ensuring the effectiveness of traffic management systems. However, automatic detection of helmet violations faces significant challenges due to environmental variability, camera angles, and inconsistencies in the data. These factors hinder reliable detection of motorcycles and riders and disrupt consistent object classification. To address these challenges, we propose VisionGuard, a synergistic multi-stage framework designed to overcome the limitations of frame-wise detectors, especially in scenarios with class imbalance and inconsistent annotations. VisionGuard integrates two key components: Adaptive Labeling and Contextual Expander modules. The Adaptive Labeling module is a tracking-based refinement technique that enhances classification consistency by leveraging a tracking algorithm to assign persistent labels across frames and correct misclassifications. The Contextual Expander module improves recall for underrepresented classes by generating virtual bounding boxes with appropriate confidence scores, effectively addressing the impact of data imbalance. Experimental results show that VisionGuard improves overall mAP by 3.1% compared to baseline detectors, demonstrating its effectiveness and potential for real-world deployment in traffic surveillance systems, ultimately promoting safety and regulatory compliance.

**Link**: [arxiv](http://arxiv.org/abs/2506.21005v1),  [pdf](http://arxiv.org/pdf/2506.21005v1)

**Tags**: cs.CV 



### ALLSTaR: Automated LLM-Driven Scheduler Generation and Testing for   Intent-Based RAN
**Authors**: Maxime Elkael, Michele Polese, Reshma Prasad, Stefano Maxenti, Tommaso Melodia

**Updated**: 2025-06-26T04:31:54Z

**Summary**: The evolution toward open, programmable O-RAN and AI-RAN 6G networks creates unprecedented opportunities for Intent-Based Networking (IBN) to dynamically optimize RAN[...]. However, applying IBN effectively to the RAN scheduler [...] remains a significant challenge. Current approaches predominantly rely on coarse-grained network slicing, lacking the granularity for dynamic adaptation to individual user conditions and traffic patterns. Despite the existence of a vast body of scheduling algorithms [...], their practical utilization is hindered by implementation heterogeneity, insufficient systematic evaluation in production environments, and the complexity of developing high-performance scheduler implementations.[...] To address these limitations, we propose ALLSTaR (Automated LLm-driven Scheduler generation and Testing for intent-based RAN), a novel framework leveraging LLMs for automated, intent-driven scheduler design, implementation, and evaluation. ALLSTaR interprets NL intents, automatically generates functional scheduler code from the research literature using OCR and LLMs, and intelligently matches operator intents to the most suitable scheduler(s). Our implementation deploys these schedulers as O-RAN dApps, enabling on-the-fly deployment and testing on a production-grade, 5G-compliant testbed. This approach has enabled the largest-scale OTA experimental comparison of 18 scheduling algorithms automatically synthesized from the academic literature. The resulting performance profiles serve as the input for our Intent-Based Scheduling (IBS) framework, which dynamically selects and deploys appropriate schedulers that optimally satisfy operator intents. We validate our approach through multiple use cases unattainable with current slicing-based optimization techniques, demonstrating fine-grained control based on buffer status, physical layer conditions, and heterogeneous traffic types

**Link**: [arxiv](http://arxiv.org/abs/2505.18389v3),  [pdf](http://arxiv.org/pdf/2505.18389v3)

**Tags**: cs.NI 



### SAC: A Framework for Measuring and Inducing Personality Traits in LLMs   with Dynamic Intensity Control
**Authors**: Adithya Chittem, Aishna Shrivastava, Sai Tarun Pendela, Jagat Sesh Challa, Dhruv Kumar

**Updated**: 2025-06-26T04:12:15Z

**Summary**: Large language models (LLMs) have gained significant traction across a wide range of fields in recent years. There is also a growing expectation for them to display human-like personalities during interactions. To meet this expectation, numerous studies have proposed methods for modelling LLM personalities through psychometric evaluations. However, most existing models face two major limitations: they rely on the Big Five (OCEAN) framework, which only provides coarse personality dimensions, and they lack mechanisms for controlling trait intensity. In this paper, we address this gap by extending the Machine Personality Inventory (MPI), which originally used the Big Five model, to incorporate the 16 Personality Factor (16PF) model, allowing expressive control over sixteen distinct traits. We also developed a structured framework known as Specific Attribute Control (SAC) for evaluating and dynamically inducing trait intensity in LLMs. Our method introduces adjective-based semantic anchoring to guide trait intensity expression and leverages behavioural questions across five intensity factors: \textit{Frequency}, \textit{Depth}, \textit{Threshold}, \textit{Effort}, and \textit{Willingness}. Through experimentation, we find that modelling intensity as a continuous spectrum yields substantially more consistent and controllable personality expression compared to binary trait toggling. Moreover, we observe that changes in target trait intensity systematically influence closely related traits in psychologically coherent directions, suggesting that LLMs internalize multi-dimensional personality structures rather than treating traits in isolation. Our work opens new pathways for controlled and nuanced human-machine interactions in domains such as healthcare, education, and interviewing processes, bringing us one step closer to truly human-like social machines.

**Link**: [arxiv](http://arxiv.org/abs/2506.20993v1),  [pdf](http://arxiv.org/pdf/2506.20993v1)

**Tags**: cs.CL cs.AI cs.HC 



### Will LLMs be Professional at Fund Investment? DeepFund: A Live Arena   Perspective
**Authors**: Changlun Li, Yao Shi, Yuyu Luo, Nan Tang

**Updated**: 2025-06-26T03:57:07Z

**Summary**: Large Language Models (LLMs) have demonstrated impressive capabilities across various domains, but their effectiveness in financial decision-making remains inadequately evaluated. Current benchmarks primarily assess LLMs' understanding on financial documents rather than the ability to manage assets or dig out trading opportunities in dynamic market conditions. Despite the release of new benchmarks for evaluating diversified tasks on the financial domain, we identified four major problems in these benchmarks, which are data leakage, navel-gazing, over-intervention, and maintenance-hard. To pave the research gap, we introduce DeepFund, a comprehensive arena platform for evaluating LLM-based trading strategies in a live environment. Our approach implements a multi-agent framework where they serve as multiple key roles that realize the real-world investment decision processes. Moreover, we provide a web interface that visualizes LLMs' performance with fund investment metrics across different market conditions, enabling detailed comparative analysis. Through DeepFund, we aim to provide a more realistic and fair assessment on LLM's capabilities in fund investment, offering diversified insights and revealing their potential applications in real-world financial markets. Our code is publicly available at https://github.com/HKUSTDial/DeepFund.

**Link**: [arxiv](http://arxiv.org/abs/2503.18313v2),  [pdf](http://arxiv.org/pdf/2503.18313v2)

**Tags**: cs.MA cs.AI cs.CE cs.HC 



### WiS Platform: Enhancing Evaluation of LLM-Based Multi-Agent Systems   Through Game-Based Analysis
**Authors**: Chengwei Hu, Jianhui Zheng, Yancheng He, Hangyu Guo, Junguang Jiang, Han Zhu, Kai Sun, Yuning Jiang, Wenbo Su, Bo Zheng

**Updated**: 2025-06-26T03:55:53Z

**Summary**: Recent advancements in autonomous multi-agent systems (MAS) based on large language models (LLMs) have enhanced the application scenarios and improved the capability of LLMs to handle complex tasks. Despite demonstrating effectiveness, existing studies still evidently struggle to evaluate, analysis, and reproducibility of LLM-based MAS. In this paper, to facilitate the research on LLM-based MAS, we introduce an open, scalable, and real-time updated platform for accessing and analyzing the LLM-based MAS based on the games Who is Spy?" (WiS). Our platform is featured with three main worths: (1) a unified model evaluate interface that supports models available on Hugging Face; (2) real-time updated leaderboard for model evaluation; (3) a comprehensive evaluation covering game-winning rates, attacking, defense strategies, and reasoning of LLMs. To rigorously test WiS, we conduct extensive experiments coverage of various open- and closed-source LLMs, we find that different agents exhibit distinct and intriguing behaviors in the game. The experimental results demonstrate the effectiveness and efficiency of our platform in evaluating LLM-based MAS. Our platform and its documentation are publicly available at https://whoisspy.ai/.

**Link**: [arxiv](http://arxiv.org/abs/2412.03359v2),  [pdf](http://arxiv.org/pdf/2412.03359v2)

**Tags**: cs.AI 



### Our Coding Adventure: Using LLMs to Personalise the Narrative of a   Tangible Programming Robot for Preschoolers
**Authors**: Martin Ruskov

**Updated**: 2025-06-26T03:54:25Z

**Summary**: Finding balanced ways to employ Large Language Models (LLMs) in education is a challenge due to inherent risks of poor understanding of the technology and of a susceptible audience. This is particularly so with younger children, who are known to have difficulties with pervasive screen time. Working with a tangible programming robot called Cubetto, we propose an approach to benefit from the capabilities of LLMs by employing such models in the preparation of personalised storytelling, necessary for preschool children to get accustomed to the practice of commanding the robot. We engage in action research to develop an early version of a formalised process to rapidly prototype game stories for Cubetto. Our approach has both reproducible results, because it employs open weight models, and is model-agnostic, because we test it with 5 different LLMs. We document on one hand the process, the used materials and prompts, and on the other the learning experience and outcomes. We deem the generation successful for the intended purposes of using the results as a teacher aid. Testing the models on 4 different task scenarios, we encounter issues of consistency and hallucinations and document the corresponding evaluation process and attempts (some successful and some not) to overcome these issues. Importantly, the process does not expose children to LLMs directly. Rather, the technology is used to help teachers easily develop personalised narratives on children's preferred topics. We believe our method is adequate for preschool classes and we are planning to further experiment in real-world educational settings.

**Link**: [arxiv](http://arxiv.org/abs/2506.20982v1),  [pdf](http://arxiv.org/pdf/2506.20982v1)

**Tags**: cs.CY cs.RO K.3.1 



### Response Quality Assessment for Retrieval-Augmented Generation via   Conditional Conformal Factuality
**Authors**: Naihe Feng, Yi Sui, Shiyi Hou, Jesse C. Cresswell, Ga Wu

**Updated**: 2025-06-26T03:52:56Z

**Summary**: Existing research on Retrieval-Augmented Generation (RAG) primarily focuses on improving overall question-answering accuracy, often overlooking the quality of sub-claims within generated responses. Recent methods that attempt to improve RAG trustworthiness, such as through auto-evaluation metrics, lack probabilistic guarantees or require ground truth answers. To address these limitations, we propose Conformal-RAG, a novel framework inspired by recent applications of conformal prediction (CP) on large language models (LLMs). Conformal-RAG leverages CP and internal information from the RAG mechanism to offer statistical guarantees on response quality. It ensures group-conditional coverage spanning multiple sub-domains without requiring manual labelling of conformal sets, making it suitable for complex RAG applications. Compared to existing RAG auto-evaluation methods, Conformal-RAG offers statistical guarantees on the quality of refined sub-claims, ensuring response reliability without the need for ground truth answers. Additionally, our experiments demonstrate that by leveraging information from the RAG system, Conformal-RAG retains up to 60\% more high-quality sub-claims from the response compared to direct applications of CP to LLMs, while maintaining the same reliability guarantee.

**Link**: [arxiv](http://arxiv.org/abs/2506.20978v1),  [pdf](http://arxiv.org/pdf/2506.20978v1)

**Tags**: cs.IR H.3.3 



### Where is AIED Headed? Key Topics and Emerging Frontiers (2020-2024)
**Authors**: Shihui Feng, Huilin Zhang, Dragan Gašević

**Updated**: 2025-06-26T03:24:30Z

**Summary**: In this study, we analyze 2,398 research articles published between 2020 and 2024 across eight core venues related to the field of Artificial Intelligence in Education (AIED). Using a three-step knowledge co-occurrence network analysis, we analyze the knowledge structure of the field, the evolving knowledge clusters, and the emerging frontiers. Our findings reveal that AIED research remains strongly technically focused, with sustained themes such as intelligent tutoring systems, learning analytics, and natural language processing, alongside rising interest in large language models (LLMs) and generative artificial intelligence (GenAI). By tracking the bridging keywords over the past five years, we identify four emerging frontiers in AIED--LLMs, GenAI, multimodal learning analytics, and human-AI collaboration. The current research interests in GenAI are centered around GAI-driven personalization, self-regulated learning, feedback, assessment, motivation, and ethics.The key research interests and emerging frontiers in AIED reflect a growing emphasis on co-adaptive, human-centered AI for education. This study provides the first large-scale field-level mapping of AIED's transformation in the GenAI era and sheds light on the future research development and educational practices.

**Link**: [arxiv](http://arxiv.org/abs/2506.20971v1),  [pdf](http://arxiv.org/pdf/2506.20971v1)

**Tags**: cs.SI 



### Comparing Retrieval-Augmentation and Parameter-Efficient Fine-Tuning for   Privacy-Preserving Personalization of Large Language Models
**Authors**: Alireza Salemi, Hamed Zamani

**Updated**: 2025-06-26T03:19:56Z

**Summary**: Despite its substantial impact on various search, recommendation, and question answering tasks, privacy-preserving methods for personalizing large language models (LLMs) have received relatively limited exploration. There is one primary approach in this area through retrieval-augmented generation (RAG), which generates personalized outputs by enriching the input prompt with information retrieved from the user's personal data. This paper studies an orthogonal approach to RAG that involves learning user-dependent LLM parameters through parameter-efficient fine-tuning (PEFT). This paper presents the first systematic study for exploration of PEFT for LLM personalization and provides an extensive comparisons between RAG- and PEFT-based solutions, across a broad set of seven diverse datasets from the LaMP benchmark. Our results demonstrate that, on average, both RAG- and PEFT-based personalization methods yield 14.92% and 1.07% improvements over non-personalized LLMs, respectively. When combining RAG with PEFT, we observe a further improvement of 15.98%, highlighting the effectiveness of their integration in enhancing personalized text generation. Additionally, we identify a positive correlation between the amount of user data available and the effectiveness of PEFT. This finding suggests that RAG is particularly beneficial for cold-start users -- users with limited personal data -- while PEFT performs better when more user-specific data is available.

**Link**: [arxiv](http://arxiv.org/abs/2409.09510v2),  [pdf](http://arxiv.org/pdf/2409.09510v2)

**Tags**: cs.CL 



### Co-Design of Sensing, Communications, and Control for Low-Altitude   Wireless Networks
**Authors**: Haijia Jin, Jun Wu, Weijie Yuan, Fan Liu, Yuanhao Cui

**Updated**: 2025-06-26T03:19:27Z

**Summary**: The rapid advancement of Internet of Things (IoT) services and the evolution toward the sixth generation (6G) have positioned unmanned aerial vehicles (UAVs) as critical enablers of low-altitude wireless networks (LAWNs). This work investigates the co-design of integrated sensing, communication, and control ($\mathbf{SC^{2}}$) for multi-UAV cooperative systems with finite blocklength (FBL) transmission. In particular, the UAVs continuously monitor the state of the field robots and transmit their observations to the robot controller to ensure stable control while cooperating to localize an unknown sensing target (ST). To this end, a weighted optimization problem is first formulated by jointly considering the control and localization performance in terms of the linear quadratic regulator (LQR) cost and the determinant of the Fisher information matrix (FIM), respectively. The resultant problem, optimizing resource allocations, the UAVs' deployment positions, and multi-user scheduling, is non-convex. To circumvent this challenge, we first derive a closed-form expression of the LQR cost with respect to other variables. Subsequently, the non-convex optimization problem is decomposed into a series of sub-problems by leveraging the alternating optimization (AO) approach, in which the difference of convex functions (DC) programming and projected gradient descent (PGD) method are employed to obtain an efficient near-optimal solution. Furthermore, the convergence and computational complexity of the proposed algorithm are thoroughly analyzed. Extensive simulation results are presented to validate the effectiveness of our proposed approach compared to the benchmark schemes and reveal the trade-off between control and sensing performance.

**Link**: [arxiv](http://arxiv.org/abs/2506.20970v1),  [pdf](http://arxiv.org/pdf/2506.20970v1)

**Tags**: eess.SP 



### GuardSet-X: Massive Multi-Domain Safety Policy-Grounded Guardrail   Dataset
**Authors**: Mintong Kang, Zhaorun Chen, Chejian Xu, Jiawei Zhang, Chengquan Guo, Minzhou Pan, Ivan Revilla, Yu Sun, Bo Li

**Updated**: 2025-06-26T03:18:08Z

**Summary**: As LLMs become widespread across diverse applications, concerns about the security and safety of LLM interactions have intensified. Numerous guardrail models and benchmarks have been developed to ensure LLM content safety. However, existing guardrail benchmarks are often built upon ad hoc risk taxonomies that lack a principled grounding in standardized safety policies, limiting their alignment with real-world operational requirements. Moreover, they tend to overlook domain-specific risks, while the same risk category can carry different implications across different domains. To bridge these gaps, we introduce GuardSet-X, the first massive multi-domain safety policy-grounded guardrail dataset. GuardSet-X offers: (1) broad domain coverage across eight safety-critical domains, such as finance, law, and codeGen; (2) policy-grounded risk construction based on authentic, domain-specific safety guidelines; (3) diverse interaction formats, encompassing declarative statements, questions, instructions, and multi-turn conversations; (4) advanced benign data curation via detoxification prompting to challenge over-refusal behaviors; and (5) \textbf{attack-enhanced instances} that simulate adversarial inputs designed to bypass guardrails. Based on GuardSet-X, we benchmark 19 advanced guardrail models and uncover a series of findings, such as: (1) All models achieve varied F1 scores, with many demonstrating high variance across risk categories, highlighting their limited domain coverage and insufficient handling of domain-specific safety concerns; (2) As models evolve, their coverage of safety risks broadens, but performance on common risk categories may decrease; (3) All models remain vulnerable to optimized adversarial attacks. We believe that \dataset and the unique insights derived from our evaluations will advance the development of policy-aligned and resilient guardrail systems.

**Link**: [arxiv](http://arxiv.org/abs/2506.19054v2),  [pdf](http://arxiv.org/pdf/2506.19054v2)

**Tags**: cs.CR 



### Reward-Guided Speculative Decoding for Efficient LLM Reasoning
**Authors**: Baohao Liao, Yuhui Xu, Hanze Dong, Junnan Li, Christof Monz, Silvio Savarese, Doyen Sahoo, Caiming Xiong

**Updated**: 2025-06-26T03:14:46Z

**Summary**: We introduce Reward-Guided Speculative Decoding (RSD), a novel framework aimed at improving the efficiency of inference in large language models (LLMs). RSD synergistically combines a lightweight draft model with a more powerful target model, incorporating a controlled bias to prioritize high-reward outputs, in contrast to existing speculative decoding methods that enforce strict unbiasedness. RSD employs a process reward model to evaluate intermediate decoding steps and dynamically decide whether to invoke the target model, optimizing the trade-off between computational cost and output quality. We theoretically demonstrate that a threshold-based mixture strategy achieves an optimal balance between resource utilization and performance. Extensive evaluations on challenging reasoning benchmarks, including Olympiad-level tasks, show that RSD delivers significant efficiency gains against decoding with the target model only (up to 4.4x fewer FLOPs), while achieving significant better accuracy than parallel decoding method on average (up to +3.5). These results highlight RSD as a robust and cost-effective approach for deploying LLMs in resource-intensive scenarios. The code is available at https://github.com/BaohaoLiao/RSD.

**Link**: [arxiv](http://arxiv.org/abs/2501.19324v3),  [pdf](http://arxiv.org/pdf/2501.19324v3)

**Tags**: cs.CL cs.AI 



### ss2DNS: A Secure DNS Scheme in Stage 2
**Authors**: Ali Sadeghi Jahromi, AbdelRahman Abdou, Paul C. van Oorschot

**Updated**: 2025-06-26T03:14:28Z

**Summary**: The absence of security and privacy measures between DNS recursive resolvers and authoritative nameservers has been exploited by both on-path and off-path attackers. Although numerous security proposals have been introduced in practice and in the literature, they often face deployability barriers and/or lack a compelling set of security and privacy properties, resulting in limited adoption. We introduce ss2DNS, a novel DNS scheme designed to mitigate the security and privacy vulnerabilities in the resolution process between resolvers and authoritative nameservers, while preserving efficiency by maintaining a single round-trip. ss2DNS takes advantage of a hierarchical trust model that does not rely on entities external to DNS zones, and delegates nameserver replicas within each zone to serve zone data securely for short, renewable time intervals. This design enables real-time security properties for DNS messages without requiring the duplication of long-term private keys on replicas, thereby minimizing exposure to compromise. We implement a proof of concept of ss2DNS for evaluation and show that for server-side processing latency, resolution time, and CPU usage, ss2DNS is comparable to less-secure schemes but significantly outperforms DNS-over-TLS.

**Link**: [arxiv](http://arxiv.org/abs/2408.00968v3),  [pdf](http://arxiv.org/pdf/2408.00968v3)

**Tags**: cs.CR 



### Learning to Rank for Multiple Retrieval-Augmented Models through   Iterative Utility Maximization
**Authors**: Alireza Salemi, Hamed Zamani

**Updated**: 2025-06-26T03:06:17Z

**Summary**: This paper investigates the design of a unified search engine to serve multiple retrieval-augmented generation (RAG) agents, each with a distinct task, backbone large language model (LLM), and RAG strategy. We introduce an iterative approach where the search engine generates retrieval results for the RAG agents and gathers feedback on the quality of the retrieved documents during an offline phase. This feedback is then used to iteratively optimize the search engine using an expectation-maximization algorithm, with the goal of maximizing each agent's utility function. Additionally, we adapt this to an online setting, allowing the search engine to refine its behavior based on real-time individual agents feedback to better serve the results for each of them. Experiments on datasets from the Knowledge-Intensive Language Tasks (KILT) benchmark demonstrates that our approach significantly on average outperforms baselines across 18 RAG models. We demonstrate that our method effectively ``personalizes'' the retrieval for each RAG agent based on the collected feedback. Finally, we provide a comprehensive ablation study to explore various aspects of our method.

**Link**: [arxiv](http://arxiv.org/abs/2410.09942v2),  [pdf](http://arxiv.org/pdf/2410.09942v2)

**Tags**: cs.CL cs.IR 



### EraRAG: Efficient and Incremental Retrieval Augmented Generation for   Growing Corpora
**Authors**: Fangyuan Zhang, Zhengjun Huang, Yingli Zhou, Qintian Guo, Zhixun Li, Wensheng Luo, Di Jiang, Yixiang Fang, Xiaofang Zhou

**Updated**: 2025-06-26T03:01:33Z

**Summary**: Graph-based Retrieval-Augmented Generation (Graph-RAG) enhances large language models (LLMs) by structuring retrieval over an external corpus. However, existing approaches typically assume a static corpus, requiring expensive full-graph reconstruction whenever new documents arrive, limiting their scalability in dynamic, evolving environments. To address these limitations, we introduce EraRAG, a novel multi-layered Graph-RAG framework that supports efficient and scalable dynamic updates. Our method leverages hyperplane-based Locality-Sensitive Hashing (LSH) to partition and organize the original corpus into hierarchical graph structures, enabling efficient and localized insertions of new data without disrupting the existing topology. The design eliminates the need for retraining or costly recomputation while preserving high retrieval accuracy and low latency. Experiments on large-scale benchmarks demonstrate that EraRag achieves up to an order of magnitude reduction in update time and token consumption compared to existing Graph-RAG systems, while providing superior accuracy performance. This work offers a practical path forward for RAG systems that must operate over continually growing corpora, bridging the gap between retrieval efficiency and adaptability. Our code and data are available at https://github.com/EverM0re/EraRAG-Official.

**Link**: [arxiv](http://arxiv.org/abs/2506.20963v1),  [pdf](http://arxiv.org/pdf/2506.20963v1)

**Tags**: cs.IR cs.LG 



### Cooperative Circumnavigation for Multi-Quadrotor Systems via Onboard   Sensing
**Authors**: Xueming Liu, Lin Li, Xiang Zhou, Qingrui Zhang, Tianjiang Hu

**Updated**: 2025-06-26T02:41:55Z

**Summary**: A cooperative circumnavigation framework is proposed for multi-quadrotor systems to enclose and track a moving target without reliance on external localization systems. The distinct relationships between quadrotor-quadrotor and quadrotor-target interactions are evaluated using a heterogeneous perception strategy and corresponding state estimation algorithms. A modified Kalman filter is developed to fuse visual-inertial odometry with range measurements to enhance the accuracy of inter-quadrotor relative localization. An event-triggered distributed Kalman filter is designed to achieve robust target state estimation under visual occlusion by incorporating neighbor measurements and estimated inter-quadrotor relative positions. Using the estimation results, a cooperative circumnavigation controller is constructed, leveraging an oscillator-based autonomous formation flight strategy. We conduct extensive indoor and outdoor experiments to validate the efficiency of the proposed circumnavigation framework in occluded environments. Furthermore, a quadrotor failure experiment highlights the inherent fault tolerance property of the proposed framework, underscoring its potential for deployment in search-and-rescue operations.

**Link**: [arxiv](http://arxiv.org/abs/2506.20954v1),  [pdf](http://arxiv.org/pdf/2506.20954v1)

**Tags**: cs.RO 



### EFEAR-4D: Ego-Velocity Filtering for Efficient and Accurate 4D radar   Odometry
**Authors**: Xiaoyi Wu, Yushuai Chen, Zhan Li, Ziyang Hong, Liang Hu

**Updated**: 2025-06-26T02:41:53Z

**Summary**: Odometry is a crucial component for successfully implementing autonomous navigation, relying on sensors such as cameras, LiDARs and IMUs. However, these sensors may encounter challenges in extreme weather conditions, such as snowfall and fog. The emergence of FMCW radar technology offers the potential for robust perception in adverse conditions. As the latest generation of FWCW radars, the 4D mmWave radar provides point cloud with range, azimuth, elevation, and Doppler velocity information, despite inherent sparsity and noises in the point cloud. In this paper, we propose EFEAR-4D, an accurate, highly efficient, and learning-free method for large-scale 4D radar odometry estimation. EFEAR-4D exploits Doppler velocity information delicately for robust ego-velocity estimation, resulting in a highly accurate prior guess. EFEAR-4D maintains robustness against point-cloud sparsity and noises across diverse environments through dynamic object removal and effective region-wise feature extraction. Extensive experiments on two publicly available 4D radar datasets demonstrate state-of-the-art reliability and localization accuracy of EFEAR-4D under various conditions. Furthermore, we have collected a dataset following the same route but varying installation heights of the 4D radar, emphasizing the significant impact of radar height on point cloud quality - a crucial consideration for real-world deployments. Our algorithm and dataset will be available soon at https://github.com/CLASS-Lab/EFEAR-4D.

**Link**: [arxiv](http://arxiv.org/abs/2405.09780v2),  [pdf](http://arxiv.org/pdf/2405.09780v2)

**Tags**: cs.RO 



### Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon   Simulation
**Authors**: Chenkai Sun, Denghui Zhang, ChengXiang Zhai, Heng Ji

**Updated**: 2025-06-26T02:28:58Z

**Summary**: Given the growing influence of language model-based agents on high-stakes societal decisions, from public policy to healthcare, ensuring their beneficial impact requires understanding the far-reaching implications of their suggestions. We propose a proof-of-concept framework that projects how model-generated advice could propagate through societal systems on a macroscopic scale over time, enabling more robust alignment. To assess the long-term safety awareness of language models, we also introduce a dataset of 100 indirect harm scenarios, testing models' ability to foresee adverse, non-obvious outcomes from seemingly harmless user prompts. Our approach achieves not only over 20% improvement on the new dataset but also an average win rate exceeding 70% against strong baselines on existing safety benchmarks (AdvBench, SafeRLHF, WildGuardMix), suggesting a promising direction for safer agents.

**Link**: [arxiv](http://arxiv.org/abs/2506.20949v1),  [pdf](http://arxiv.org/pdf/2506.20949v1)

**Tags**: cs.AI cs.CL 



