# Arxiv Results
## Keyword: kv cache 
 ### PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM   Serving
**Authors**: Ahmet Caner Yüzügüler, Jiawei Zhuang, Lukas Cavigelli

**Updated**: 2025-01-14T15:14:10Z

**Summary**: Large language models (LLMs) are widely used across various applications, but their substantial computational requirements pose significant challenges, particularly in terms of HBM bandwidth bottlenecks and inter-device communication overhead. In this paper, we present PRESERVE, a novel prefetching framework designed to optimize LLM inference by overlapping memory reads for model weights and KV-cache with collective communication operations. Through extensive experiments conducted on commercial AI accelerators, we demonstrate up to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs. Additionally, we perform a design space exploration that identifies the optimal hardware configuration for the proposed method, showing a further 1.25x improvement in performance per cost by selecting the optimal L2 cache size. Our results show that PRESERVE has the potential to mitigate the memory bottlenecks and communication overheads, offering a solution to improve the performance and scalability of the LLM inference systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.08192v1),  [pdf](http://arxiv.org/pdf/2501.08192v1)

**Tags**: cs.AI cs.AR cs.DC 



### AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out   Context Attribution
**Authors**: Fengyuan Liu, Nikhil Kandpal, Colin Raffel

**Updated**: 2025-01-14T14:07:55Z

**Summary**: The influence of contextual input on the behavior of large language models (LLMs) has prompted the development of context attribution methods that aim to quantify each context span's effect on an LLM's generations. The leave-one-out (LOO) error, which measures the change in the likelihood of the LLM's response when a given span of the context is removed, provides a principled way to perform context attribution, but can be prohibitively expensive to compute for large models. In this work, we introduce AttriBoT, a series of novel techniques for efficiently computing an approximation of the LOO error for context attribution. Specifically, AttriBoT uses cached activations to avoid redundant operations, performs hierarchical attribution to reduce computation, and emulates the behavior of large target models with smaller proxy models. Taken together, AttriBoT can provide a >300x speedup while remaining more faithful to a target model's LOO error than prior context attribution methods. This stark increase in performance makes computing context attributions for a given response 30x faster than generating the response itself, empowering real-world applications that require computing attributions at scale. We release a user-friendly and efficient implementation of AttriBoT to enable efficient LLM interpretability as well as encourage future development of efficient context attribution methods.

**Link**: [arxiv](http://arxiv.org/abs/2411.15102v2),  [pdf](http://arxiv.org/pdf/2411.15102v2)

**Tags**: cs.LG 



### TreeKV: Smooth Key-Value Cache Compression with Tree Structures
**Authors**: Ziwei He, Jian Yuan, Haoli Bai, Jingwen Leng, Bo Jiang

**Updated**: 2025-01-14T12:06:33Z

**Summary**: Efficient key-value (KV) cache compression is critical for scaling transformer-based Large Language Models (LLMs) in long sequences and resource-limited settings. Existing methods evict tokens based on their positions or importance scores, but position-based strategies can miss crucial information outside predefined regions, while those relying on global importance scores resulting in strong regional biases, limiting the KV cache's overall context retention and potentially impairing the performance of LLMs on complex tasks. Our wavelet analysis reveals that as tokens approach the end of sequence, their contributions to generation gradually increase and tends to diverge more from neighboring tokens, indicating a smooth transition with increasing complexity and variability from distant to nearby context. Motivated by this observation, we propose TreeKV, an intuitive, training-free method that employs a tree structure for smooth cache compression. TreeKV maintains a fixed cache size, allowing LLMs to deliver high-quality output even in long text scenarios. Unlike most compression methods, TreeKV is applicable to both the generation and prefilling stages. TreeKV consistently surpasses all baseline models in language modeling tasks on PG19 and OpenWebText2, allowing LLMs trained with short context window to generalize to longer window with a 16x cache reduction. On the Longbench benchmark, TreeKV achieves the best performance with only 6\% of the budget at optimal efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2501.04987v2),  [pdf](http://arxiv.org/pdf/2501.04987v2)

**Tags**: cs.CL 



### Cell-level modelling of homeostasis in confined epithelial monolayers
**Authors**: KVS Chaithanya, Jan Rozman, Andrej Košmrlj, Rastko Sknepnek

**Updated**: 2025-01-14T11:41:14Z

**Summary**: Tissue homeostasis, the biological process of maintaining a steady state in tissue via control of cell proliferation, death, and metabolic function, is essential for the development, growth, maintenance, and proper function of living organisms. Disruptions to this process can lead to serious diseases and even death. In this study, we use the vertex model for the cell-level description of tissue mechanics to investigate the impact of the tissue microenvironment and local mechanical properties of cells on homeostasis in confined epithelial tissues. We find a dynamic steady state, where the balance between cell divisions and removals sustains homeostasis. By characterising homeostasis in terms of cell count, tissue area, and the cells' neighbour count distribution, we identify the factors that govern regulated and ordered tissue growth. This work, therefore, sheds light on the mechanisms underlying tissue homeostasis and highlights the importance of mechanics in the control of biological processes such as tissue development and disease pathology.

**Link**: [arxiv](http://arxiv.org/abs/2403.15896v2),  [pdf](http://arxiv.org/pdf/2403.15896v2)

**Tags**: physics.bio-ph cond-mat.soft 



### Multi-matrix Factorization Attention
**Authors**: Jingcheng Hu, Houyi Li, Yinmin Zhang, Zili Wang, Shuigeng Zhou, Xiangyu Zhang, Heung-Yeung Shum, Daxin Jiang

**Updated**: 2025-01-14T05:48:07Z

**Summary**: We propose novel attention architectures, Multi-matrix Factorization Attention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard Multi-Head Attention (MHA), including SOTA methods like MLA, fail to maintain as strong performance under stringent Key-Value cache (KV cache) constraints. MFA enhances model capacity by efficiently scaling up both the number and dimension of attention heads through low-rank matrix factorization in the Query-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory requirements by repurposing the key cache as value through value projection re-parameterization. MFA's design enables strong model capacity when working under tight KV cache budget, while MFA-KR is suitable for even harsher KV cache limits with minor performance trade-off. Notably, in our extensive and large-scale experiments, the proposed architecture outperforms MLA and performs comparably to MHA, while reducing KV cache usage by up to 56% and 93.7%, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2412.19255v2),  [pdf](http://arxiv.org/pdf/2412.19255v2)

**Tags**: cs.LG cs.CL 



### Lean Attention: Hardware-Aware Scalable Attention Mechanism for the   Decode-Phase of Transformers
**Authors**: Rya Sanovar, Srikant Bharadwaj, Renee St. Amant, Victor Rühle, Saravan Rajmohan

**Updated**: 2025-01-14T05:00:34Z

**Summary**: Transformer-based models have emerged as one of the most widely used architectures for natural language processing, natural language generation, and image generation. The size of the state-of-the-art models has increased steadily reaching billions of parameters. These huge models are memory hungry and incur significant inference latency even on cutting edge AI-accelerators, such as GPUs. Specifically, the time and memory complexity of the attention operation is quadratic in terms of the total context length, i.e., prompt and output tokens. Thus, several optimizations such as key-value tensor caching and FlashAttention computation have been proposed to deliver the low latency demands of applications relying on such large models. However, these techniques do not cater to the computationally distinct nature of different phases during inference.   To that end, we propose LeanAttention, a scalable technique of computing self-attention for the token-generation phase (decode-phase) of decoder-only transformer models. LeanAttention enables scaling the attention mechanism implementation for the challenging case of long context lengths by re-designing the execution flow for the decode-phase. We identify that the associative property of online softmax can be treated as a reduction operation thus allowing us to parallelize the attention computation over these large context lengths. We extend the "stream-K" style reduction of tiled calculation to self-attention to enable parallel computation resulting in an average of 2.6x attention execution speedup over FlashAttention-2 and up to 8.33x speedup for 512k context lengths.

**Link**: [arxiv](http://arxiv.org/abs/2405.10480v2),  [pdf](http://arxiv.org/pdf/2405.10480v2)

**Tags**: cs.AR cs.LG I.2.7; C.1.4 



### QMDB: Quick Merkle Database
**Authors**: Isaac Zhang, Ryan Zarick, Daniel Wong, Thomas Kim, Bryan Pellegrino, Mignon Li, Kelvin Wong

**Updated**: 2025-01-14T02:02:01Z

**Summary**: Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain state management by integrating key-value (KV) and Merkle tree storage into a single unified architecture. QMDB delivers a significant throughput improvement over existing architectures, achieving up to 6X over the widely used RocksDB and 8X over NOMT, a leading verifiable database. Its novel append-only twig-based design enables one SSD read per state access, O(1) IOs for updates, and in-memory Merkleization on a memory footprint as small as 2.3 bytes per entry, enabling it to run on even modest consumer-grade PCs. QMDB scales seamlessly across both commodity and enterprise hardware, achieving up to 2.28 million state updates per second. This performance enables support for 1 million token transfers per second (TPS), marking QMDB as the first solution achieving such a milestone. QMDB has been benchmarked with workloads exceeding 15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to scale to 280 billion entries on a single server. Furthermore, QMDB introduces historical proofs, unlocking the ability to query its blockchain's historical state at the latest block. QMDB not only meets the demands of current blockchains but also provides a robust foundation for building scalable, efficient, and verifiable decentralized applications across diverse use cases.

**Link**: [arxiv](http://arxiv.org/abs/2501.05262v2),  [pdf](http://arxiv.org/pdf/2501.05262v2)

**Tags**: cs.NI cs.DB 



### Parallel Key-Value Cache Fusion for Position Invariant RAG
**Authors**: Philhoon Oh, Jinwoo Shin, James Thorne

**Updated**: 2025-01-13T17:50:30Z

**Summary**: Recent advancements in Large Language Models (LLMs) underscore the necessity of Retrieval Augmented Generation (RAG) to leverage external information. However, LLMs are sensitive to the position of relevant information within contexts and tend to generate incorrect responses when such information is placed in the middle, known as `Lost in the Middle' phenomenon. In this paper, we introduce a framework that generates consistent outputs for decoder-only models, irrespective of the input context order. Experimental results for three open domain question answering tasks demonstrate position invariance, where the model is not sensitive to input context order, and superior robustness to irrelevent passages compared to prevailing approaches for RAG pipelines.

**Link**: [arxiv](http://arxiv.org/abs/2501.07523v1),  [pdf](http://arxiv.org/pdf/2501.07523v1)

**Tags**: cs.AI cs.CL 



### FlashRNN: Optimizing Traditional RNNs on Modern Hardware
**Authors**: Korbinian Pöppel, Maximilian Beck, Sepp Hochreiter

**Updated**: 2025-01-13T17:34:22Z

**Summary**: While Transformers and other sequence-parallelizable neural network architectures seem like the current state of the art in sequence modeling, they specifically lack state-tracking capabilities. These are important for time-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs, as well as modern variants like sLSTM do have these capabilities at the cost of strictly sequential processing. While this is often seen as a strong limitation, we show how fast these networks can get with our hardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the register level on modern GPUs. We extend traditional RNNs with a parallelization variant that processes multiple RNNs of smaller hidden state in parallel, similar to the head-wise processing in Transformers. To enable flexibility on different GPU variants, we introduce a new optimization framework for hardware-internal cache sizes, memory and compute handling. It models the hardware in a setting using polyhedral-like constraints, including the notion of divisibility. This speeds up the solution process in our ConstrINT library for general integer constraint satisfaction problems (integer CSPs). We show that our kernels can achieve 50x speed-ups over a vanilla PyTorch implementation and allow 40x larger hidden sizes compared to our Triton implementation. Our open-source kernels and the optimization library are released here to boost research in the direction of state-tracking enabled RNNs and sequence modeling: \url{https://github.com/NX-AI/flashrnn}

**Link**: [arxiv](http://arxiv.org/abs/2412.07752v2),  [pdf](http://arxiv.org/pdf/2412.07752v2)

**Tags**: cs.LG cs.AI 



### DID Link: Authentication in TLS with Decentralized Identifiers and   Verifiable Credentials
**Authors**: Sandro Rodriguez Garzon, Dennis Natusch, Artur Philipp, Axel Küpper, Hans Joachim Einsiedler, Daniela Schneider

**Updated**: 2025-01-13T09:33:25Z

**Summary**: Authentication in TLS is predominately carried out with X.509 digital certificates issued by certificate authorities (CA). The centralized nature of current public key infrastructures, however, comes along with severe risks, such as single points of failure and susceptibility to cyber-attacks, potentially undermining the security and trustworthiness of the entire system. With Decentralized Identifiers (DID) alongside distributed ledger technology, it becomes technically feasible to prove ownership of a unique identifier without requiring an attestation of the proof's public key by a centralized and therefore vulnerable CA. This article presents DID Link, a novel authentication scheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant way with self-issued X.509 certificates that are equipped with ledger-anchored DIDs instead of CA-issued identifiers. It facilitates the exchange of tamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable Credentials after the TLS handshake to complete the authentication with a full identification of the communication partner. A prototypical implementation shows comparable TLS handshake durations of DID Link if verification material is cached and reasonable prolongations if it is obtained from a ledger. The significant speed improvement of the resulting TLS channel over a widely used, DID-based alternative transport protocol on the application layer demonstrates the potential of DID Link to become a viable solution for the establishment of secure and trustful end-to-end communication links with decentrally managed digital identities.

**Link**: [arxiv](http://arxiv.org/abs/2405.07533v4),  [pdf](http://arxiv.org/pdf/2405.07533v4)

**Tags**: cs.CR cs.NI 



### Generating Data Locality to Accelerate Sparse Matrix-Matrix   Multiplication on CPUs
**Authors**: Jordi Wolfson-Pou, Jan Laukemann, Fabrizio Petrini

**Updated**: 2025-01-13T04:31:04Z

**Summary**: Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation in many applications. Current multithreaded implementations are based on Gustavson's algorithm and often perform poorly on large matrices due to limited cache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic NUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To generate locality, MAGNUS reorders the intermediate product into discrete cache-friendly chunks using a two-level hierarchical approach. The accumulator is applied to each chunk, where the chunk size is chosen such that the accumulator is cache-efficient. MAGNUS is input- and system-aware: based on the matrix characteristics and target system specifications, the optimal number of chunks is computed by minimizing the storage cost of the necessary data structures. MAGNUS allows for a hybrid accumulation strategy in which each chunk uses a different accumulator based on an input threshold. We consider two accumulators: an AVX-512 vectorized bitonic sorting algorithm and classical dense accumulation. An OpenMP implementation of MAGNUS is compared with several baselines for a variety of different matrices on three Intel x86 architectures. For matrices from the SuiteSparse collection, MAGNUS is faster than all the baselines in most cases and is orders of magnitude faster than Intel MKL for several matrices. For massive random matrices that model social network graphs, MAGNUS scales to the largest matrix sizes, while the baselines fail to do so. Furthermore, MAGNUS is close to the optimal bound for these matrices, regardless of the matrix size, structure, and density.

**Link**: [arxiv](http://arxiv.org/abs/2501.07056v1),  [pdf](http://arxiv.org/pdf/2501.07056v1)

**Tags**: cs.DC 



### A Unified Framework for Automated Code Transformation and Pragma   Insertion
**Authors**: Stéphane Pouget, Louis-Noël Pouchet, Jason Cong

**Updated**: 2025-01-13T03:11:28Z

**Summary**: High-level synthesis, source-to-source compilers, and various Design Space Exploration techniques for pragma insertion have significantly improved the Quality of Results of generated designs. These tools offer benefits such as reduced development time and enhanced performance. However, achieving high-quality results often requires additional manual code transformations and tiling selections, which are typically performed separately or as pre-processing steps. Although DSE techniques enable code transformation upfront, the vastness of the search space often limits the exploration of all possible code transformations, making it challenging to determine which transformations are necessary. Additionally, ensuring correctness remains challenging, especially for complex transformations and optimizations.   To tackle this obstacle, we first propose a comprehensive framework leveraging HLS compilers. Our system streamlines code transformation, pragma insertion, and tiles size selection for on-chip data caching through a unified optimization problem, aiming to enhance parallelization, particularly beneficial for computation-bound kernels. Them employing a novel Non-Linear Programming (NLP) approach, we simultaneously ascertain transformations, pragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation demonstrates that our framework adeptly identifies the appropriate transformations, including scenarios where no transformation is necessary, and inserts pragmas to achieve a favorable Quality of Results.

**Link**: [arxiv](http://arxiv.org/abs/2405.03058v5),  [pdf](http://arxiv.org/pdf/2405.03058v5)

**Tags**: cs.SE cs.PL 



### Reciprocating Locks
**Authors**: Dave Dice, Alex Kogan

**Updated**: 2025-01-12T20:08:46Z

**Summary**: We present "Reciprocating Locks", a novel mutual exclusion locking algorithm, targeting cache-coherent shared memory (CC), that enjoys a number of desirable properties. The doorway arrival phase and the release operation both run in constant-time. Waiting threads use local spinning and only a single waiting element is required per thread, regardless of the number of locks a thread might hold at a given time. While our lock does not provide strict FIFO admission, it bounds bypass and has strong anti-starvation properties. The lock is compact, space efficient, and has been intentionally designed to be readily usable in real-world general purpose computing environments such as the linux kernel, pthreads, or C++. We show the lock exhibits high throughput under contention and low latency in the uncontended case. The performance of Reciprocating Locks is competitive with and often better than the best state-of-the-art scalable spin locks.

**Link**: [arxiv](http://arxiv.org/abs/2501.02380v2),  [pdf](http://arxiv.org/pdf/2501.02380v2)

**Tags**: cs.DC D.4.1 



### On Optimizing Locality of Graph Transposition on Modern Architectures
**Authors**: Mohsen Koohi Esfahani, Hans Vandierendonck

**Updated**: 2025-01-12T17:01:40Z

**Summary**: This paper investigates the shared-memory Graph Transposition (GT) problem, a fundamental graph algorithm that is widely used in graph analytics and scientific computing.   Previous GT algorithms have significant memory requirements that are proportional to the number of vertices and threads which obstructs their use on large graphs. Moreover, atomic memory operations have become comparably fast on recent CPU architectures, which creates new opportunities for improving the performance of concurrent atomic accesses in GT.   We design PoTra, a GT algorithm which leverages graph structure and processor and memory architecture to optimize locality and performance. PoTra limits the size of additional data structures close to CPU cache sizes and utilizes the skewed degree distribution of graph datasets to optimize locality and performance. We present the performance model of PoTra to explain the connection between cache and memory response times and graph locality.   Our evaluation of PoTra on three CPU architectures and 20 real-world and synthetic graph datasets with up to 128 billion edges demonstrates that PoTra achieves up to 8.7 times speedup compared to previous works and if there is a performance loss it remains limited to 15.7%, on average.

**Link**: [arxiv](http://arxiv.org/abs/2501.06872v1),  [pdf](http://arxiv.org/pdf/2501.06872v1)

**Tags**: cs.DC cs.AR cs.DS cs.PF 



### MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large   Language Model Inference
**Authors**: Wenxuan Zeng, Ye Dong, Jinjin Zhou, Junming Ma, Jin Tan, Runsheng Wang, Meng Li

**Updated**: 2025-01-12T13:18:04Z

**Summary**: Private large language model (LLM) inference based on secure multi-party computation (MPC) offers cryptographically-secure protection for both user prompt and proprietary model weights. However, it suffers from large latency overhead especially for long input sequences. While key-value (KV) cache eviction algorithms have been proposed to reduce the computation and memory cost for plaintext inference, they are not designed for MPC and cannot benefit private inference easily. In this paper, we propose an accurate and MPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on the observation that historical tokens in a long sequence may have different effects on the downstream decoding. Hence, MPCache combines a look-once static eviction algorithm to discard unimportant tokens and a query-aware dynamic selection algorithm to further select a small subset of tokens for attention computation. As existing dynamic selection algorithms incur too much latency, we propose a series of optimizations to drastically reduce the KV cache selection overhead, including MPC-friendly similarity approximation, hierarchical KV cache clustering, and cross-layer index sharing strategy. With extensive experiments, we demonstrate that MPCache consistently outperforms prior-art KV cache eviction baselines across different LLM generation tasks and achieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction on different sequence lengths, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2501.06807v1),  [pdf](http://arxiv.org/pdf/2501.06807v1)

**Tags**: cs.CR 



### Linear Attention Sequence Parallelism
**Authors**: Weigao Sun, Zhen Qin, Dong Li, Xuyang Shen, Yu Qiao, Yiran Zhong

**Updated**: 2025-01-12T12:01:47Z

**Summary**: Sequence parallelism (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single device. However, for linear sequence modeling methods like linear attention, existing SP approaches do not take advantage of their right-product-first feature, resulting in sub-optimal communication efficiency and usability. In this paper, we introduce Linear Attention Sequence Parallelism (LASP), an efficient SP approach designed for linear attention-based transformer models. Specifically, we design an efficient point-to-point ring-style communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead, comparing with existing SP methods. We enhance the computation efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPUs. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with very-long sequences. We also discuss the generalization of LASP on other linear sequence modeling methods. Extensive experiments on linear attention-based models are conducted with varying sequence lengths from 2K to 4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\times$ longer than existing SP methods. The code is available at https://github.com/OpenNLPLab/LASP.

**Link**: [arxiv](http://arxiv.org/abs/2404.02882v2),  [pdf](http://arxiv.org/pdf/2404.02882v2)

**Tags**: cs.LG cs.CL 



### Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma   Generated THz Pulses
**Authors**: Benjamin Colmey, Rodrigo T. Paulino, Gaspard Beaufort, David G. Cooke

**Updated**: 2025-01-12T11:15:41Z

**Summary**: Terahertz pulses generated by two-color laser plasmas have reported peak field strengths exceeding MV/cm, and when illuminating metal nanotips the near-field enhancement at the tip apex should result in extremely high bunch charges and electron energies via sub-cycle cold field emission. Here, electron emission from tungsten nanotips driven by THz pulses generated by a long filament air-plasma are reported. Electron energies up to 1.1 keV and bunch charges up to 2x$10^5$ electrons per pulse were detected, well below values expected for peak field calculated via the time averaged Poynting vector. Investigations revealed a failure in the use of the time-averaged Poynting vector when applied to long filament THz pulses, due to spatio-temporal restructuring of the THz pulse in the focus. Accounting for this restructuring significantly reduces the field strength to approximately 160 ~kV/cm, consistent with the observed electron bunch charges, peak energies and their dependence on the tip position in the THz focus. Despite these findings, our results surpass previous THz plasma-driven electron generation by an order of magnitude in both electron energy and bunch charge and a path to increasing these by an additional order of magnitude by modification of the THz optics is proposed.

**Link**: [arxiv](http://arxiv.org/abs/2409.07196v3),  [pdf](http://arxiv.org/pdf/2409.07196v3)

**Tags**: cond-mat.mtrl-sci physics.plasm-ph 



### Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion
**Authors**: Bohai Gu, Hao Luo, Song Guo, Peiran Dong

**Updated**: 2025-01-12T05:25:06Z

**Summary**: Recently, diffusion-based methods have achieved great improvements in the video inpainting task. However, these methods still face many challenges, such as maintaining temporal consistency and the time-consuming issue. This paper proposes an advanced video inpainting framework using optical Flow-guided Efficient Diffusion, called FloED. Specifically, FloED employs a dual-branch architecture, where a flow branch first restores corrupted flow and a multi-scale flow adapter provides motion guidance to the main inpainting branch. Additionally, a training-free latent interpolation method is proposed to accelerate the multi-step denoising process using flow warping. Further introducing a flow attention cache mechanism, FLoED efficiently reduces the computational cost brought by incorporating optical flow. Comprehensive experiments in both background restoration and object removal tasks demonstrate that FloED outperforms state-of-the-art methods from the perspective of both performance and efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2412.00857v2),  [pdf](http://arxiv.org/pdf/2412.00857v2)

**Tags**: cs.CV 



### Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV   Cache Management
**Authors**: Liu Qianli, Hong Zicong, Chen Fahao, Li Peng, Guo Song

**Updated**: 2025-01-12T04:29:39Z

**Summary**: Serving large language models (LLMs) for massive users is challenged by the significant memory footprint of the transient state, known as the key-value (KV) cache, which scales with sequence length and number of requests. Instead of renting or buying more expensive GPUs, the load imbalance of the KV cache across GPUs, coupled with recent advances in inter-GPU communication, provides an opportunity to serve more requests via request migration. However, high migration overhead and unpredictable request patterns make it challenging. Therefore, this paper proposes MELL, a memory-efficient LLM serving system via multi-GPU KV cache management. It saves the number of GPUs needed in the system by considering the dynamic KV cache load and the costly request migration. Specifically, we first develop an adaptive request migration mechanism to balance the computational and communication overheads and adapt to diverse resource conditions. Then, we design an online algorithm tailored to a multi-LLM request and multi-GPU scheduling problem with migration enabled. It aims to minimise the required GPUs while limiting the number of migrations. Finally, we implement a prototype of MELL and demonstrate that it reduces the number of GPUs by 31% and increases the GPU utilization by 43% at most compared to existing LLM serving systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.06709v1),  [pdf](http://arxiv.org/pdf/2501.06709v1)

**Tags**: cs.DC 



### GraphSnapShot: Caching Local Structure for Fast Graph Learning
**Authors**: Dong Liu, Roger Waleffe, Meng Jiang, Shivaram Venkataraman

**Updated**: 2025-01-11T15:26:48Z

**Summary**: In our recent research, we have developed a framework called GraphSnapShot, which has been proven an useful tool for graph learning acceleration. GraphSnapShot is a framework for fast cache, storage, retrieval and computation for graph learning. It can quickly store and update the local topology of graph structure and allows us to track patterns in the structure of graph networks, just like take snapshots of the graphs. In experiments, GraphSnapShot shows efficiency, it can achieve up to 30% training acceleration and 73% memory reduction for lossless graph ML training compared to current baselines such as dgl.This technique is particular useful for large dynamic graph learning tasks such as social media analysis and recommendation systems to process complex relationships between entities.   The code for GraphSnapShot is publicly available at https://github.com/NoakLiu/GraphSnapShot.

**Link**: [arxiv](http://arxiv.org/abs/2406.17918v4),  [pdf](http://arxiv.org/pdf/2406.17918v4)

**Tags**: cs.LG cs.DC cs.SI 



### Optimizing digital experiences with content delivery networks:   Architectures, performance strategies, and future trends
**Authors**: Anuj Tyagi

**Updated**: 2025-01-11T03:47:04Z

**Summary**: This research investigates how CDNs (Content Delivery Networks) can improve the digital experience, as consumers increasingly expect fast, efficient, and effortless access to online resources. CDNs play a crucial role in reducing latency, enhancing scalability, and optimizing delivery mechanisms, which is evident across various platforms and regions. The study focuses on key CDN concerns, such as foundational and modern CDN architectures, edge computing, hybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing topics, including caching, load balancing, and the novel features of HTTP/3 and QUIC.   Current trends, such as integrating CDNs with 5G networks, serverless architectures, and AI-driven traffic management, are examined to demonstrate how CDN technology is likely to evolve. The study also addresses challenges related to security, cost, and global regulations. Practical examples from the e-commerce, streaming, and gaming industries highlight how enhanced CDNs are transforming these sectors.   The conclusions emphasize the need to evolve CDN strategies to meet growing user expectations and adapt to the rapidly changing digital landscape. Additionally, the research identifies future research opportunities, particularly in exploring the impact of QC, the enhancement of AI services, and the sustainability of CDN solutions. Overall, the study situates architectural design, performance strategies, and emerging trends to address gaps and create a more efficient and secure approach for improving digital experiences.

**Link**: [arxiv](http://arxiv.org/abs/2501.06428v1),  [pdf](http://arxiv.org/pdf/2501.06428v1)

**Tags**: cs.NI cs.SE 



### Tensor Product Attention Is All You Need
**Authors**: Yifan Zhang, Yifeng Liu, Huizhuo Yuan, Zhen Qin, Yang Yuan, Quanquan Gu, Andrew Chi-Chih Yao

**Updated**: 2025-01-11T03:37:10Z

**Summary**: Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, significantly shrinking KV cache size at inference time. By factorizing these representations into contextual low-rank components (contextual factorization) and seamlessly integrating with RoPE, TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model architecture for sequence modeling. Through extensive empirical evaluation of language modeling tasks, we demonstrate that T6 exceeds the performance of standard Transformer baselines including MHA, MQA, GQA, and MLA across various metrics, including perplexity and a range of renowned evaluation benchmarks. Notably, TPAs memory efficiency enables the processing of significantly longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. The code is available at https://github.com/tensorgi/T6.

**Link**: [arxiv](http://arxiv.org/abs/2501.06425v1),  [pdf](http://arxiv.org/pdf/2501.06425v1)

**Tags**: cs.CL cs.AI cs.LG 



### Unispeaker: A Unified Approach for Multimodality-driven Speaker   Generation
**Authors**: Zhengyan Sheng, Zhihao Du, Heng Lu, Shiliang Zhang, Zhen-Hua Ling

**Updated**: 2025-01-11T00:47:29Z

**Summary**: Recent advancements in personalized speech generation have brought synthetic speech increasingly close to the realism of target speakers' recordings, yet multimodal speaker generation remains on the rise. This paper introduces UniSpeaker, a unified approach for multimodality-driven speaker generation. Specifically, we propose a unified voice aggregator based on KV-Former, applying soft contrastive loss to map diverse voice description modalities into a shared voice space, ensuring that the generated voice aligns more closely with the input descriptions. To evaluate multimodality-driven voice control, we build the first multimodality-based voice control (MVC) benchmark, focusing on voice suitability, voice diversity, and speech quality. UniSpeaker is evaluated across five tasks using the MVC benchmark, and the experimental results demonstrate that UniSpeaker outperforms previous modality-specific models. Speech samples are available at \url{https://UniSpeaker.github.io}.

**Link**: [arxiv](http://arxiv.org/abs/2501.06394v1),  [pdf](http://arxiv.org/pdf/2501.06394v1)

**Tags**: cs.SD cs.AI eess.AS 



### Tame fields, Graded Rings and Finite Complete Sequences of Key   Polynomials
**Authors**: Caio Henrique Silva de Souza

**Updated**: 2025-01-10T10:11:45Z

**Summary**: In this paper, we present a criterion for $(K,v)$ to be henselian and defectless in terms of finite complete sequences of key polynomials. For this, we use the theory of Mac Lane-Vaqui\'e chains and abstract key polynomials. We then prove that a valued field $(K,v)$ is tame if and only if $vK$ is $p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$ admits a finite complete sequence of key polynomials. The properties $vK$ $p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on the associated graded ring. We also make considerations on simply defectless and algebraically maximal valued fields and purely inertial and purely ramified extensions.

**Link**: [arxiv](http://arxiv.org/abs/2407.01030v2),  [pdf](http://arxiv.org/pdf/2407.01030v2)

**Tags**: math.AC 13A18 



### Handover_Management_in_UAV_Networks_with_Blockages
**Authors**: Neetu R R, Gourab Ghatak, Vivek Ashok Bohara

**Updated**: 2025-01-09T15:14:05Z

**Summary**: We investigate the performance of unmanned aerial vehicle (UAV)-based networks in urban environments characterized by blockages, focusing on their capability to support the service demands of mobile users. The UAV-base stations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson point process (MPPP), where the marks represent the altitude of each UAV-BS. Leveraging stochastic geometry, we analyze the impact of blockages on network reliability by studying the meta distribution (MD) of the signal-to-interference noise ratio (SINR) for a specific reliability threshold and the association probabilities for both line-of-sight (LoS) and non line-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile users, we propose a novel cache-based handover management strategy that dynamically selects the cell search time and delays the received signal strength (RSS)-based base station (BS) associations. This strategy aims to minimize unnecessary handovers (HOs) experienced by users by leveraging caching capabilities at user equipment (UE), thus reducing latency, ensuring seamless connectivity, and maintaining the quality of service (QoS). This study provides valuable insights into optimizing UAV network deployments to support the stringent requirements in the network, ensuring reliable, low-latency, and high-throughput communication for next-generation smart cities.

**Link**: [arxiv](http://arxiv.org/abs/2409.20433v2),  [pdf](http://arxiv.org/pdf/2409.20433v2)

**Tags**: eess.SP 



### ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State   Drives
**Authors**: Shaobo Li, Yirui Eric Zhou, Hao Ren, Jian Huang

**Updated**: 2025-01-09T06:18:39Z

**Summary**: Unlike non-volatile memory that resides on the processor memory bus, memory-semantic solid-state drives (SSDs) support both byte and block access granularity via PCIe or CXL interconnects. They provide scalable memory capacity using NAND flash at a much lower cost. In addition, they have different performance characteristics for their dual byte/block interface respectively, while offering essential memory semantics for upper-level software. Such a byte-accessible storage device provides new implications on the software system design.   In this paper, we develop a new file system, named ByteFS, by rethinking the design primitives of file systems and SSD firmware to exploit the advantages of both byte and block-granular data accesses. ByteFS supports byte-granular data persistence to retain the persistence nature of SSDs. It extends the core data structure of file systems by enabling dual byte/block-granular data accesses. To facilitate the support for byte-granular writes, \pname{} manages the internal DRAM of SSD firmware in a log-structured manner and enables data coalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also enables coordinated data caching between the host page cache and SSD cache for best utilizing the precious memory resource. We implement ByteFS on both a real programmable SSD and an emulated memory-semantic SSD for sensitivity study. Compared to state-of-the-art file systems for non-volatile memory and conventional SSDs, ByteFS outperforms them by up to 2.7$\times$, while preserving the essential properties of a file system. ByteFS also reduces the write traffic to SSDs by up to 5.1$\times$ by alleviating unnecessary writes caused by both metadata and data updates in file systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.04993v1),  [pdf](http://arxiv.org/pdf/2501.04993v1)

**Tags**: cs.OS 



### Optimal Oblivious Algorithms for Multi-way Joins
**Authors**: Xiao Hu, Zhiang Wu

**Updated**: 2025-01-09T03:02:31Z

**Summary**: In cloud databases, cloud computation over sensitive data uploaded by clients inevitably causes concern about data security and privacy. Even when encryption primitives and trusted computing environments are integrated into query processing to safeguard the actual contents of the data, access patterns of algorithms can still leak private information about the data. Oblivious Random Access Memory (ORAM) and circuits are two generic approaches to address this issue, ensuring that access patterns of algorithms remain oblivious to the data. However, deploying these methods on insecure algorithms, particularly for multi-way join processing, is computationally expensive and inherently challenging.   In this paper, we propose a novel sorting-based algorithm for multi-way join processing that operates without relying on ORAM simulations or other security assumptions. Our algorithm is a non-trivial, provably oblivious composition of basic primitives, with time complexity matching the insecure worst-case optimal join algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic, with cache complexity matching the insecure lower bound, also up to a logarithmic factor. This clean and straightforward approach has the potential to be extended to other security settings and implemented in practical database systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.04216v2),  [pdf](http://arxiv.org/pdf/2501.04216v2)

**Tags**: cs.DB cs.CR 



### Modern Hardware Security: A Review of Attacks and Countermeasures
**Authors**: Jyotiprakash Mishra, Sanjay K. Sahay

**Updated**: 2025-01-08T10:14:19Z

**Summary**: With the exponential rise in the use of cloud services, smart devices, and IoT devices, advanced cyber attacks have become increasingly sophisticated and ubiquitous. Furthermore, the rapid evolution of computing architectures and memory technologies has created an urgent need to understand and address hardware security vulnerabilities. In this paper, we review the current state of vulnerabilities and mitigation strategies in contemporary computing systems. We discuss cache side-channel attacks (including Spectre and Meltdown), power side-channel attacks (such as Simple Power Analysis, Differential Power Analysis, Correlation Power Analysis, and Template Attacks), and advanced techniques like Voltage Glitching and Electromagnetic Analysis to help understand and build robust cybersecurity defense systems and guide further research. We also examine memory encryption, focusing on confidentiality, granularity, key management, masking, and re-keying strategies. Additionally, we cover Cryptographic Instruction Set Architectures, Secure Boot, Root of Trust mechanisms, Physical Unclonable Functions, and hardware fault injection techniques. The paper concludes with an analysis of the RISC-V architecture's unique security challenges. The comprehensive analysis presented in this paper is essential for building resilient hardware security solutions that can protect against both current and emerging threats in an increasingly challenging security landscape.

**Link**: [arxiv](http://arxiv.org/abs/2501.04394v1),  [pdf](http://arxiv.org/pdf/2501.04394v1)

**Tags**: cs.CR cs.AR 



### Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear   Approximation
**Authors**: Samrat Mukhopadhyay, Debasmita Mukherjee

**Updated**: 2025-01-07T17:32:19Z

**Summary**: We consider the problem of \textit{online sparse linear approximation}, where one predicts the best sparse approximation of a sequence of measurements in terms of linear combination of columns of a given measurement matrix. Such online prediction problems are ubiquitous, ranging from medical trials to web caching to resource allocation. The inherent difficulty of offline recovery also makes the online problem challenging. In this letter, we propose Follow-The-Approximate-Sparse-Leader, an efficient online meta-policy to address this online problem. Through a detailed theoretical analysis, we prove that under certain assumptions on the measurement sequence, the proposed policy enjoys a data-dependent sublinear upper bound on the static regret, which can range from logarithmic to square-root. Numerical simulations are performed to corroborate the theoretical findings and demonstrate the efficacy of the proposed online policy.

**Link**: [arxiv](http://arxiv.org/abs/2501.00799v2),  [pdf](http://arxiv.org/pdf/2501.00799v2)

**Tags**: cs.LG math.OC 



### Not all tokens are created equal: Perplexity Attention Weighted Networks   for AI generated text detection
**Authors**: Pablo Miralles-González, Javier Huertas-Tato, Alejandro Martín, David Camacho

**Updated**: 2025-01-07T17:00:49Z

**Summary**: The rapid advancement in large language models (LLMs) has significantly enhanced their ability to generate coherent and contextually relevant text, raising concerns about the misuse of AI-generated content and making it critical to detect it. However, the task remains challenging, particularly in unseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution outputs offers a theoretically appealing approach for detection, as they encapsulate insights from the models' extensive pre-training on diverse corpora. Despite its promise, zero-shot methods that attempt to operationalize these outputs have met with limited success. We hypothesize that one of the problems is that they use the mean to aggregate next-token distribution metrics across tokens, when some tokens are naturally easier or harder to predict and should be weighted differently. Based on this idea, we propose the Perplexity Attention Weighted Network (PAWN), which uses the last hidden states of the LLM and positions to weight the sum of a series of features based on metrics from the next-token distribution across the sequence length. Although not zero-shot, our method allows us to cache the last hidden states and next-token distribution metrics on disk, greatly reducing the training resource requirements. PAWN shows competitive and even better performance in-distribution than the strongest baselines (fine-tuned LMs) with a fraction of their trainable parameters. Our model also generalizes better to unseen domains and source models, with smaller variability in the decision boundary across distribution shifts. It is also more robust to adversarial attacks, and if the backbone has multilingual capabilities, it presents decent generalization to languages not seen during supervised training, with LLaMA3-1B reaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine languages.

**Link**: [arxiv](http://arxiv.org/abs/2501.03940v1),  [pdf](http://arxiv.org/pdf/2501.03940v1)

**Tags**: cs.CL cs.AI 



### Parallel $k$d-tree with Batch Updates
**Authors**: Ziyang Men, Zheqi Shen, Yan Gu, Yihan Sun

**Updated**: 2025-01-06T23:16:22Z

**Summary**: The $k$d-tree is one of the most widely used data structures to manage multi-dimensional data. Due to the ever-growing data volume, it is imperative to consider parallelism in $k$d-trees. However, we observed challenges in existing parallel kd-tree implementations, for both constructions and updates.   The goal of this paper is to develop efficient in-memory $k$d-trees by supporting high parallelism and cache-efficiency. We propose the Pkd-tree (Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and in practice. The Pkd-tree supports parallel tree construction, batch update (insertion and deletion), and various queries including k-nearest neighbor search, range query, and range count. We proved that our algorithms have strong theoretical bounds in work (sequential time complexity), span (parallelism), and cache complexity. Our key techniques include 1) an efficient construction algorithm that optimizes work, span, and cache complexity simultaneously, and 2) reconstruction-based update algorithms that guarantee the tree to be weight-balanced. With the new algorithmic insights and careful engineering effort, we achieved a highly optimized implementation of the Pkd-tree.   We tested Pkd-tree with various synthetic and real-world datasets, including both uniform and highly skewed data. We compare the Pkd-tree with state-of-the-art parallel $k$d-tree implementations. In all tests, with better or competitive query performance, Pkd-tree is much faster in construction and updates consistently than all baselines. We released our code.

**Link**: [arxiv](http://arxiv.org/abs/2411.09275v2),  [pdf](http://arxiv.org/pdf/2411.09275v2)

**Tags**: cs.DS cs.DB cs.DC cs.PF 



### The Power of Negative Zero: Datatype Customization for Quantized Large   Language Models
**Authors**: Yuzong Chen, Xilai Dai, Chi-chih Chang, Yash Akhauri, Mohamed S. Abdelfattah

**Updated**: 2025-01-06T22:40:40Z

**Summary**: Large language models (LLMs) have demonstrated remarkable performance across various machine learning tasks, quickly becoming one of the most prevalent AI workloads. Yet the substantial memory requirement of LLMs significantly hinders their deployment for end users. Post-training quantization (PTQ) serves as one of the most hardware-efficient methods to mitigate the memory and computational demands of LLMs. Although the traditional integer (INT) datatype has received widespread adoption in PTQ methods, floating-point (FP) quantization has emerged as a viable alternative thanks to its effectiveness in fitting LLM numerical distributions. However, the FP datatype in sign-magnitude binary representation contains both positive and negative zero, which constrains its representation capability, particularly under low precision (3 and 4 bits). In this paper, we extend the basic FP datatype to perform Redundant Zero Remapping (RaZeR), which remaps the negative zero FP encoding to a set of pre-defined special values to maximally utilize FP quantization encodings and to better fit LLM numerical distributions. Through careful selection of special values, RaZeR outperforms conventional asymmetric INT quantization while achieving high computational efficiency. We demonstrate that RaZeR can be seamlessly integrated with quantization algorithms for both weights and KV-cache, including advanced methods with clipping and transformations, and consistently achieve better model accuracy. Additionally, we implement a fast GEMV kernel with fused dequantization that efficiently converts the 4-bit RaZeR value to FP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows that RaZeR improves the GEMV speed by up to 7.56$\times$ compared to the FP16 implementation, while achieving up to 2.72$\times$ speedup in the LLM decoding throughput.

**Link**: [arxiv](http://arxiv.org/abs/2501.04052v1),  [pdf](http://arxiv.org/pdf/2501.04052v1)

**Tags**: cs.LG cs.CL 



### Twinkle: A GPU-based binary-lens microlensing code with contour   integration method
**Authors**: Suwei Wang, Lile Wang, Subo Dong

**Updated**: 2025-01-06T19:00:03Z

**Summary**: With the rapidly increasing rate of microlensing planet detections, microlensing modeling software faces significant challenges in computation efficiency. Here, we develop the Twinkle code, an efficient and robust binary-lens modeling software suite optimized for heterogeneous computing devices, especially GPUs. Existing microlensing codes have the issue of catastrophic cancellation that undermines the numerical stability and precision, and Twinkle resolves them by refining the coefficients of the binary-lens equation. We also devise an improved method for robustly identifying ghost images, thereby enhancing computational reliability. We have advanced the state of the art by optimizing Twinkle specifically for heterogeneous computing devices by taking into account the unique task and cache memory dispatching patterns of GPUs, while the compatibility with the traditional computing architectures of CPUs is still maintained. Twinkle has demonstrated an acceleration of approximately 2 orders of magnitude (>~100 times) on contemporary GPUs. The enhancement in computational speed of Twinkle will translate to the delivery of accurate and highly efficient data analysis for ongoing and upcoming microlensing projects. Both GPU and CPU versions of Twinkle are open-source and publicly available.

**Link**: [arxiv](http://arxiv.org/abs/2501.03322v1),  [pdf](http://arxiv.org/pdf/2501.03322v1)

**Tags**: astro-ph.IM astro-ph.EP astro-ph.GA astro-ph.SR 



### Direct Comparison of Magnetic Penetration Depth in Kagome   Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)
**Authors**: Austin Kaczmarek, Andrea Capa Salinas, Stephen D. Wilson, Katja C. Nowack

**Updated**: 2025-01-06T15:59:23Z

**Summary**: We report measurements of the local temperature-dependent penetration depth, $\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using scanning superconducting quantum interference device (SQUID) microscopy. Our results suggest that the superconducting order in all three compounds is fully gapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density, $\rho_s(T)$, shows deviations from the behavior expected for a single isotropic gap, but the data are well described by models incorporating either a single anisotropic gap or two isotropic gaps. Notably, the temperature dependences of $\lambda(T)$ and $\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are qualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with the superconducting phase reflecting features of the normal-state band structure. Our findings provide a direct comparison of the superconducting properties across the AV$_3$Sb$_5$ family.

**Link**: [arxiv](http://arxiv.org/abs/2412.19919v2),  [pdf](http://arxiv.org/pdf/2412.19919v2)

**Tags**: cond-mat.supr-con 



### Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism
**Authors**: Yimin Tang, Zhenghong Yu, Yi Zheng, T. K. Satish Kumar, Jiaoyang Li, Sven Koenig

**Updated**: 2025-01-06T06:44:13Z

**Summary**: Multi-Agent Path Finding (MAPF), which focuses on finding collision-free paths for multiple robots, is crucial in autonomous warehouse operations. Lifelong MAPF (L-MAPF), where agents are continuously reassigned new targets upon completing their current tasks, offers a more realistic approximation of real-world warehouse scenarios. While cache storage systems can enhance efficiency and reduce operational costs, existing approaches primarily rely on expectations and mathematical models, often without adequately addressing the challenges of multi-robot planning and execution. In this paper, we introduce a novel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which integrates high-level cache storage with low-level path planning. We have involved a new type of map grid called cache for temporary item storage. Additionally, we involved a task assigner (TA) with a locking mechanism to bridge the gap between the new cache grid and L-MAPF algorithm. The TA dynamically allocates target locations to agents based on their status in various scenarios. We evaluated L-MAPF-CM using different cache replacement policies and task distributions. L-MAPF-CM has demonstrated performance improvements particularly with high cache hit rates and smooth traffic conditions.

**Link**: [arxiv](http://arxiv.org/abs/2501.02803v1),  [pdf](http://arxiv.org/pdf/2501.02803v1)

**Tags**: cs.RO cs.AI 



### From Slow Bidirectional to Fast Autoregressive Video Diffusion Models
**Authors**: Tianwei Yin, Qiang Zhang, Richard Zhang, William T. Freeman, Fredo Durand, Eli Shechtman, Xun Huang

**Updated**: 2025-01-06T01:26:42Z

**Summary**: Current video diffusion models achieve impressive generation quality but struggle in interactive applications due to bidirectional attention dependencies. The generation of a single frame requires the model to process the entire sequence, including the future. We address this limitation by adapting a pretrained bidirectional diffusion transformer to an autoregressive transformer that generates frames on-the-fly. To further reduce latency, we extend distribution matching distillation (DMD) to videos, distilling 50-step diffusion model into a 4-step generator. To enable stable and high-quality distillation, we introduce a student initialization scheme based on teacher's ODE trajectories, as well as an asymmetric distillation strategy that supervises a causal student model with a bidirectional teacher. This approach effectively mitigates error accumulation in autoregressive generation, allowing long-duration video synthesis despite training on short clips. Our model achieves a total score of 84.27 on the VBench-Long benchmark, surpassing all previous video generation models. It enables fast streaming generation of high-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our approach also enables streaming video-to-video translation, image-to-video, and dynamic prompting in a zero-shot manner. We will release the code based on an open-source model in the future.

**Link**: [arxiv](http://arxiv.org/abs/2412.07772v2),  [pdf](http://arxiv.org/pdf/2412.07772v2)

**Tags**: cs.CV 



### ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video   Understanding
**Authors**: Xiao Wang, Qingyi Si, Jianlong Wu, Shiyu Zhu, Li Cao, Liqiang Nie

**Updated**: 2025-01-05T14:11:48Z

**Summary**: Video Large Language Models (VideoLLMs) have achieved remarkable progress in video understanding. However, existing VideoLLMs often inherit the limitations of their backbone LLMs in handling long sequences, leading to challenges for long video understanding. Common solutions either simply uniformly sample videos' frames or compress visual tokens, which focus primarily on low-level temporal visual redundancy, overlooking high-level knowledge redundancy. This limits the achievable compression rate with minimal loss. To this end. we introduce a training-free method, $\textbf{ReTaKe}$, containing two novel modules DPSelect and PivotKV, to jointly model and reduce both temporal visual redundancy and knowledge redundancy for long video understanding. Specifically, DPSelect identifies keyframes with local maximum peak distance based on their visual features, which are closely aligned with human video perception. PivotKV employs the obtained keyframes as pivots and conducts KV-Cache compression for the non-pivot tokens with low attention scores, which are derived from the learned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and LVBench, show that ReTaKe can support 4x longer video sequences with minimal performance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%, even surpassing or on par with much larger ones. Our code is available at https://github.com/SCZwangxiao/video-ReTaKe

**Link**: [arxiv](http://arxiv.org/abs/2412.20504v2),  [pdf](http://arxiv.org/pdf/2412.20504v2)

**Tags**: cs.CV cs.CL cs.MM 



### A Full-System Simulation Framework for CXL-Based SSD Memory System
**Authors**: Yaohui Wang, Zicong Wang, Fanfeng Meng, Yanjing Wang, Yang Ou, Lizhou Wu, Wentao Hong, Xuran Ge, Jijun Cao

**Updated**: 2025-01-05T12:51:08Z

**Summary**: Compute eXpress Link (CXL) is a promising technology for memory disaggregation and expansion. Especially, CXL makes it more effectively for large-capacity storage devices such as Solid State Drive (SSD) to be deployed in the memory pool. However, CXL-based SSDs are still in early stages, necessitating the development of reliable simulation tools. In this paper, we propose CXL-SSD-Sim, the first open-source full-system simulator designed to simulate CXL-based SSD memory system. Constructed on the foundation of gem5 and SimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along with the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM layer as a caching mechanism for the SSD, meticulously engineered to counteract latency issues inherent to CXL-based SSD memory access. Experiments are performed among five different memory devices with CXL-SSD-Sim in aspect of latency, bandwidth and real-world benchmark performance. These experiments serve to underscore the efficacy of our simulation tool in providing a comprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim simulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim.

**Link**: [arxiv](http://arxiv.org/abs/2501.02524v1),  [pdf](http://arxiv.org/pdf/2501.02524v1)

**Tags**: cs.AR 



### LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas   and Ad-Hoc Networks
**Authors**: Atonu Ghosh, Sudip Misra

**Updated**: 2025-01-05T07:41:53Z

**Summary**: The minimal infrastructure requirements of LoRa make it suitable for deployments in remote and disaster-stricken areas. Concomitantly, the modern era is witnessing the proliferation of web applications in all aspects of human life, including IoT and other network services. Contemporary IoT and network solutions heavily rely on web applications to render services. However, despite the recent research and development pivoted around LoRa, there is still a lack of studies focusing on web application access over LoRa networks. Specifically, technical challenges like payload size limitation, low data rate, and contentions in multi-user setups limit the applicability of LoRa for web applications. Hence, we propose LoRaWeb, which enables web access over LoRa networks. The LoRaWeb hardware tethers a WiFi hotspot to which the client devices connect and access the web pages using a web browser. LoRa backbone of the network handles the web page transmission from the requester and receiver devices. LoRaWeb implements a synchronization procedure to address the aforementioned challenges for effective message exchange between requesters and responders. The system implements a caching mechanism to reduce latency and contention. Additionally, it implements a message-slicing mechanism in the application layer to overcome the hardware limitations on the message length. The actual hardware-based implementation results indicate seamless deployment, and the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and ~$6 S$ for a $10 KB$ size web page.

**Link**: [arxiv](http://arxiv.org/abs/2501.02469v1),  [pdf](http://arxiv.org/pdf/2501.02469v1)

**Tags**: cs.NI cs.CY cs.SY eess.SY 



### End-to-End Long Document Summarization using Gradient Caching
**Authors**: Rohit Saxena, Hao Tang, Frank Keller

**Updated**: 2025-01-03T13:32:57Z

**Summary**: Training transformer-based encoder-decoder models for long document summarization poses a significant challenge due to the quadratic memory consumption during training. Several approaches have been proposed to extend the input length at test time, but training with these approaches is still difficult, requiring truncation of input documents and causing a mismatch between training and test conditions. In this work, we propose CachED (Gradient $\textbf{Cach}$ing for $\textbf{E}$ncoder-$\textbf{D}$ecoder models), an approach that enables end-to-end training of existing transformer-based encoder-decoder models, using the entire document without truncation. Specifically, we apply non-overlapping sliding windows to input documents, followed by fusion in decoder. During backpropagation, the gradients are cached at the decoder and are passed through the encoder in chunks by re-computing the hidden vectors, similar to gradient checkpointing. In the experiments on long document summarization, we extend BART to CachED BART, processing more than 500K tokens during training and achieving superior performance without using any additional parameters.

**Link**: [arxiv](http://arxiv.org/abs/2501.01805v1),  [pdf](http://arxiv.org/pdf/2501.01805v1)

**Tags**: cs.CL cs.AI 



### Efficient LLM Inference with Activation Checkpointing and Hybrid Caching
**Authors**: Sanghyeon Lee, Hongbeen Kim, Soojin Hwang, Guseul Heo, Minwoo Noh, Jaehyuk Huh

**Updated**: 2025-01-03T12:51:37Z

**Summary**: Recent large language models (LLMs) with enormous model sizes use many GPUs to meet memory capacity requirements incurring substantial costs for token generation. To provide cost-effective LLM inference with relaxed latency constraints, extensive research has focused on expanding GPU memory by leveraging the host memory. However, LLM inference engines that utilize the host memory often face underutilization of GPU compute units, as a considerable portion of inference time is spent in loading the model onto the GPU via host-GPU interconnect. To tackle these challenges of the host memory offloading for LLM, we introduce HybridServe, an LLM inference system with activation checkpointing based on activation caching. The activation cache stores activation checkpoints generated during intermediate inference stages, allowing the fast recomputation of KV cache while model parameters are transferred to GPU from host memory. Unlike conventional methods that recompute the KV cache from scratch using token IDs, the activation cache allows bypassing projection and FFN operations. To balance between the activation recomputation and parameter loading overhead, this study proposes a KV-activation hybrid caching scheme which finds the best ratio of the key-value and activation caches to adjust the recomputation time. Our system achieves 2.19x throughput improvement over the state-of-the-art prior work for offloading both model weights and KV cache.

**Link**: [arxiv](http://arxiv.org/abs/2501.01792v1),  [pdf](http://arxiv.org/pdf/2501.01792v1)

**Tags**: cs.DC 



### Object-level Visual Prompts for Compositional Image Generation
**Authors**: Gaurav Parmar, Or Patashnik, Kuan-Chieh Wang, Daniil Ostashev, Srinivasa Narasimhan, Jun-Yan Zhu, Daniel Cohen-Or, Kfir Aberman

**Updated**: 2025-01-02T18:59:44Z

**Summary**: We introduce a method for composing object-level visual prompts within a text-to-image diffusion model. Our approach addresses the task of generating semantically coherent compositions across diverse scenes and styles, similar to the versatility and expressiveness offered by text prompts. A key challenge in this task is to preserve the identity of the objects depicted in the input visual prompts, while also generating diverse compositions across different images. To address this challenge, we introduce a new KV-mixed cross-attention mechanism, in which keys and values are learned from distinct visual representations. The keys are derived from an encoder with a small bottleneck for layout control, whereas the values come from a larger bottleneck encoder that captures fine-grained appearance details. By mixing keys and values from these complementary sources, our model preserves the identity of the visual prompts while supporting flexible variations in object arrangement, pose, and composition. During inference, we further propose object-level compositional guidance to improve the method's identity preservation and layout correctness. Results show that our technique produces diverse scene compositions that preserve the unique characteristics of each visual prompt, expanding the creative potential of text-to-image generation.

**Link**: [arxiv](http://arxiv.org/abs/2501.01424v1),  [pdf](http://arxiv.org/pdf/2501.01424v1)

**Tags**: cs.CV cs.AI cs.GR 



### MSWA: Refining Local Attention with Multi-ScaleWindow Attention
**Authors**: Yixing Xu, Shivank Nag, Dong Li, Lu Tian, Emad Barsoum

**Updated**: 2025-01-02T03:41:32Z

**Summary**: Transformer-based LLMs have achieved exceptional performance across a wide range of NLP tasks. However, the standard self-attention mechanism suffers from quadratic time complexity and linearly increased cache size. Sliding window attention (SWA) solves this problem by restricting the attention range to a fixed-size local context window. Nevertheless, SWA employs a uniform window size for each head in each layer, making it inefficient in capturing context of varying scales. To mitigate this limitation, we propose Multi-Scale Window Attention (MSWA) which applies diverse window sizes across heads and layers in the Transformer. It not only allows for different window sizes among heads within the same layer but also progressively increases window size allocation from shallow to deep layers, thus enabling the model to capture contextual information with different lengths and distances. Experimental results on language modeling and common-sense reasoning tasks substantiate that MSWA outperforms traditional local attention in both effectiveness and efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2501.01039v1),  [pdf](http://arxiv.org/pdf/2501.01039v1)

**Tags**: cs.CL cs.AI 



### A Survey on Large Language Model Acceleration based on KV Cache   Management
**Authors**: Haoyang Li, Yiming Li, Anxin Tian, Tianhao Tang, Zhanchao Xu, Xuejia Chen, Nicole Hu, Wei Dong, Qing Li, Lei Chen

**Updated**: 2025-01-02T03:40:15Z

**Summary**: Large Language Models (LLMs) have revolutionized a wide range of domains such as natural language processing, computer vision, and multi-modal tasks due to their ability to comprehend context and perform logical reasoning. However, the computational and memory demands of LLMs, particularly during inference, pose significant challenges when scaling them to real-world, long-context, and real-time applications. Key-Value (KV) cache management has emerged as a critical optimization technique for accelerating LLM inference by reducing redundant computations and improving memory utilization. This survey provides a comprehensive overview of KV cache management strategies for LLM acceleration, categorizing them into token-level, model-level, and system-level optimizations. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments. Additionally, the survey provides an overview of both text and multimodal datasets and benchmarks used to evaluate these strategies. By presenting detailed taxonomies and comparative analyses, this work aims to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques, contributing to the practical deployment of LLMs in real-world applications. The curated paper list for KV cache management is in: \href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.

**Link**: [arxiv](http://arxiv.org/abs/2412.19442v2),  [pdf](http://arxiv.org/pdf/2412.19442v2)

**Tags**: cs.AI cs.DC 



### FlashInfer: Efficient and Customizable Attention Engine for LLM   Inference Serving
**Authors**: Zihao Ye, Lequn Chen, Ruihang Lai, Wuwei Lin, Yineng Zhang, Stephanie Wang, Tianqi Chen, Baris Kasikci, Vinod Grover, Arvind Krishnamurthy, Luis Ceze

**Updated**: 2025-01-02T02:02:20Z

**Summary**: Transformers, driven by attention mechanisms, form the foundation of large language models (LLMs). As these models scale up, efficient GPU attention kernels become essential for high-throughput and low-latency inference. Diverse LLM applications demand flexible and high-performance attention solutions. We present FlashInfer: a customizable and efficient attention engine for LLM serving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse format and composable formats to optimize memory access and reduce redundancy. It also offers a customizable attention template, enabling adaptation to various settings through Just-In-Time (JIT) compilation. Additionally, FlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user requests while maintaining compatibility with CUDAGraph which requires static configuration. FlashInfer have been integrated into leading LLM serving frameworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and end-to-end evaluations demonstrate FlashInfer's ability to significantly boost kernel performance across diverse inference scenarios: compared to state-of-the-art LLM serving solutions, FlashInfer achieve 29-69% inter-token-latency reduction compared to compiler backends for LLM serving benchmark, 28-30% latency reduction for long-context inference, and 13-17% speedup for LLM serving with parallel generation.

**Link**: [arxiv](http://arxiv.org/abs/2501.01005v1),  [pdf](http://arxiv.org/pdf/2501.01005v1)

**Tags**: cs.DC cs.AI cs.LG 



### Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant   Computation Elimination in Diffusion Model
**Authors**: Omid Saghatchian, Atiyeh Gh. Moghadam, Ahmad Nickabadi

**Updated**: 2025-01-01T20:16:27Z

**Summary**: Diffusion models have emerged as a promising approach for generating high-quality, high-dimensional images. Nevertheless, these models are hindered by their high computational cost and slow inference, partly due to the quadratic computational complexity of the self-attention mechanisms with respect to input size. Various approaches have been proposed to address this drawback. One such approach focuses on reducing the number of tokens fed into the self-attention, known as token merging (ToMe). In our method, which is called cached adaptive token merging(CA-ToMe), we calculate the similarity between tokens and then merge the r proportion of the most similar tokens. However, due to the repetitive patterns observed in adjacent steps and the variation in the frequency of similarities, we aim to enhance this approach by implementing an adaptive threshold for merging tokens and adding a caching mechanism that stores similar pairs across several adjacent steps. Empirical results demonstrate that our method operates as a training-free acceleration method, achieving a speedup factor of 1.24 in the denoising process while maintaining the same FID scores compared to existing approaches.

**Link**: [arxiv](http://arxiv.org/abs/2501.00946v1),  [pdf](http://arxiv.org/pdf/2501.00946v1)

**Tags**: cs.CV 



### EdgeRAG: Online-Indexed RAG for Edge Devices
**Authors**: Korakit Seemakhupt, Sihang Liu, Samira Khan

**Updated**: 2024-12-31T20:40:43Z

**Summary**: Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge devices is challenging due to limited memory and processing power. In this work, we propose EdgeRAG which addresses the memory constraint by pruning embeddings within clusters and generating embeddings on-demand during retrieval. To avoid the latency of generating embeddings for large tail clusters, EdgeRAG pre-computes and stores embeddings for these clusters, while adaptively caching remaining embeddings to minimize redundant computations and further optimize latency. The result from BEIR suite shows that EdgeRAG offers significant latency reduction over the baseline IVF index, but with similar generation quality while allowing all of our evaluated datasets to fit into the memory.

**Link**: [arxiv](http://arxiv.org/abs/2412.21023v2),  [pdf](http://arxiv.org/pdf/2412.21023v2)

**Tags**: cs.LG 



### Token Pruning for Caching Better: 9 Times Acceleration on Stable   Diffusion for Free
**Authors**: Evelyn Zhang, Bang Xiao, Jiayi Tang, Qianli Ma, Chang Zou, Xuefei Ning, Xuming Hu, Linfeng Zhang

**Updated**: 2024-12-31T09:56:40Z

**Summary**: Stable Diffusion has achieved remarkable success in the field of text-to-image generation, with its powerful generative capabilities and diverse generation results making a lasting impact. However, its iterative denoising introduces high computational costs and slows generation speed, limiting broader adoption. The community has made numerous efforts to reduce this computational burden, with methods like feature caching attracting attention due to their effectiveness and simplicity. Nonetheless, simply reusing features computed at previous timesteps causes the features across adjacent timesteps to become similar, reducing the dynamics of features over time and ultimately compromising the quality of generated images. In this paper, we introduce a dynamics-aware token pruning (DaTo) approach that addresses the limitations of feature caching. DaTo selectively prunes tokens with lower dynamics, allowing only high-dynamic tokens to participate in self-attention layers, thereby extending feature dynamics across timesteps. DaTo combines feature caching with token pruning in a training-free manner, achieving both temporal and token-wise information reuse. Applied to Stable Diffusion on the ImageNet, our approach delivered a 9$\times$ speedup while reducing FID by 0.33, indicating enhanced image quality. On the COCO-30k, we observed a 7$\times$ acceleration coupled with a notable FID reduction of 2.17.

**Link**: [arxiv](http://arxiv.org/abs/2501.00375v1),  [pdf](http://arxiv.org/pdf/2501.00375v1)

**Tags**: cs.CV cs.LG 



### RetrievalAttention: Accelerating Long-Context LLM Inference via Vector   Retrieval
**Authors**: Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang, Yuqing Yang, Lili Qiu

**Updated**: 2024-12-31T07:11:00Z

**Summary**: Transformer-based Large Language Models (LLMs) have become increasingly important. However, due to the quadratic time complexity of attention computation, scaling LLMs to longer contexts incurs extremely slow inference speed and high GPU memory consumption for caching key-value (KV) vectors. This paper proposes RetrievalAttention, a training-free approach to both accelerate attention computation and reduce GPU memory consumption. By leveraging the dynamic sparsity of attention mechanism, RetrievalAttention proposes to build approximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory and retrieve the most relevant ones through vector search during generation. Unfortunately, we observe that the off-the-shelf ANNS indexes are often ineffective for such retrieval tasks due to the out-of-distribution (OOD) between query vectors and key vectors in the attention mechanism. RetrievalAttention addresses the OOD challenge by designing an attention-aware vector search algorithm that can adapt to the distribution of query vectors. Our evaluation demonstrates that RetrievalAttention achieves near full attention accuracy while only requiring access to 1--3% of the data. This leads to a significant reduction in the inference cost of long-context LLMs, with a much lower GPU memory footprint. In particular, RetrievalAttention only needs a single NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters, which is capable of generating one token in 0.188 seconds.

**Link**: [arxiv](http://arxiv.org/abs/2409.10516v3),  [pdf](http://arxiv.org/pdf/2409.10516v3)

**Tags**: cs.LG cs.CL 



### Performant Automatic BLAS Offloading on Unified Memory Architecture with   OpenMP First-Touch Style Data Movement
**Authors**: Junjie Li

**Updated**: 2024-12-31T05:24:30Z

**Summary**: BLAS is a fundamental building block of advanced linear algebra libraries and many modern scientific computing applications. GPUs are known for their strong arithmetic computing capabilities and are highly suited for BLAS operations. However, porting code to GPUs often requires significant effort, especially for large, complex codes or legacy codes, even for BLAS-heavy applications. While various tools exist to automatically offload BLAS to GPUs, they are often impractical due to the high costs associated with mandatory data transfers. The advent of unified memory architectures in recent GPU designs, such as the NVIDIA Grace-Hopper, allows cache-coherent memory access across all types of memory for both CPU and GPU, potentially eliminating the bottlenecks faced in conventional architectures. This breakthrough paves the way for innovative application developments and porting strategies. Building on our preliminary work demonstrating the potential of automatic *gemm offload, this paper extends the framework to all level-3 BLAS operations and introduces SCILIB-Accel, a novel tool for automatic BLAS offload. SCILIB-Accel leverages the memory coherency in Grace-Hopper and introduces a Device First-Use data movement policy inspired by the OpenMP First-Touch approach in multi-socket CPU programming, minimizing CPU-GPU data transfers for typical scientific computing codes. Additionally, utilizing dynamic binary instrumentation, the tool intercepts BLAS symbols directly from a CPU binary, requiring no code modifications or recompilation. SCILIB-Accel has been evaluated using multiple quantum physics codes on up to a few hundred GPU nodes, yielding promising speedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was achieved on Grace-Hopper compared to Grace-Grace.

**Link**: [arxiv](http://arxiv.org/abs/2501.00279v1),  [pdf](http://arxiv.org/pdf/2501.00279v1)

**Tags**: cs.DC cs.MS cs.PF cs.SE 



### Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained   Image Recognition
**Authors**: Edwin Arkel Rios, Jansen Christopher Yuanda, Vincent Leon Ghanz, Cheng-Wei Yu, Bo-Cheng Lai, Min-Chun Hu

**Updated**: 2024-12-31T03:19:38Z

**Summary**: Ultra-fine-grained image recognition (UFGIR) is a challenging task that involves classifying images within a macro-category. While traditional FGIR deals with classifying different species, UFGIR goes beyond by classifying sub-categories within a species such as cultivars of a plant. In recent times the usage of Vision Transformer-based backbones has allowed methods to obtain outstanding recognition performances in this task but this comes at a significant cost in terms of computation specially since this task significantly benefits from incorporating higher resolution images. Therefore, techniques such as token reduction have emerged to reduce the computational cost. However, dropping tokens leads to loss of essential information for fine-grained categories, specially as the token keep rate is reduced. Therefore, to counteract the loss of information brought by the usage of token reduction we propose a novel Cross-Layer Aggregation Classification Head and a Cross-Layer Cache mechanism to recover and access information from previous layers in later locations. Extensive experiments covering more than 2000 runs across diverse settings including 5 datasets, 9 backbones, 7 token reduction methods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the proposed plug-and-play modules and allow us to push the boundaries of accuracy vs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to 10\% while maintaining a competitive accuracy to state-of-the-art models. Code is available at: \url{https://github.com/arkel23/CLCA}

**Link**: [arxiv](http://arxiv.org/abs/2501.00243v1),  [pdf](http://arxiv.org/pdf/2501.00243v1)

**Tags**: cs.CV I.2; I.4 



### MapQaTor: A System for Efficient Annotation of Map Query Datasets
**Authors**: Mahir Labib Dihan, Mohammed Eunus Ali, Md Rizwan Parvez

**Updated**: 2024-12-30T15:33:19Z

**Summary**: Mapping and navigation services like Google Maps, Apple Maps, Openstreet Maps, are essential for accessing various location-based data, yet they often struggle to handle natural language geospatial queries. Recent advancements in Large Language Models (LLMs) show promise in question answering (QA), but creating reliable geospatial QA datasets from map services remains challenging. We introduce MapQaTor, a web application that streamlines the creation of reproducible, traceable map-based QA datasets. With its plug-and-play architecture, MapQaTor enables seamless integration with any maps API, allowing users to gather and visualize data from diverse sources with minimal setup. By caching API responses, the platform ensures consistent ground truth, enhancing the reliability of the data even as real-world information evolves. MapQaTor centralizes data retrieval, annotation, and visualization within a single platform, offering a unique opportunity to evaluate the current state of LLM-based geospatial reasoning while advancing their capabilities for improved geospatial understanding. Evaluation metrics show that, MapQaTor speeds up the annotation process by at least 30 times compared to manual methods, underscoring its potential for developing geospatial resources, such as complex map reasoning datasets. The website is live at: https://mapqator.github.io/ and a demo video is available at: https://youtu.be/7_aV9Wmhs6Q.

**Link**: [arxiv](http://arxiv.org/abs/2412.21015v1),  [pdf](http://arxiv.org/pdf/2412.21015v1)

**Tags**: cs.CL cs.HC 



### SepLLM: Accelerate Large Language Models by Compressing One Segment into   One Separator
**Authors**: Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xiaozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li, Weiyang Liu, Chao Huang

**Updated**: 2024-12-30T14:54:29Z

**Summary**: Large Language Models (LLMs) have exhibited exceptional performance across a spectrum of natural language processing tasks. However, their substantial sizes pose considerable challenges, particularly in computational demands and inference speed, due to their quadratic complexity. In this work, we have identified a key pattern: certain seemingly meaningless special tokens (i.e., separators) contribute disproportionately to attention scores compared to semantically meaningful tokens. This observation suggests that information of the segments between these separator tokens can be effectively condensed into the separator tokens themselves without significant information loss. Guided by this insight, we introduce SepLLM, a plug-and-play framework that accelerates inference by compressing these segments and eliminating redundant tokens. Additionally, we implement efficient kernels for training acceleration. Experimental results across training-free, training-from-scratch, and post-training settings demonstrate SepLLM's effectiveness. Notably, using the Llama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the GSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in streaming settings, SepLLM effectively processes sequences of up to 4 million tokens or more while maintaining consistent language modeling capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2412.12094v3),  [pdf](http://arxiv.org/pdf/2412.12094v3)

**Tags**: cs.CL cs.AI cs.LG 



### A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field
**Authors**: Wei Li, Hanbyul Kim, Xinbo Wang, Jianlin Luo, Simone Latini, Dongbin Shin, Jun-Ming Liu, Jing-Feng Li, Angel Rubio, Ce-Wen Nan, Qian Li

**Updated**: 2024-12-30T11:54:19Z

**Summary**: Coherent manipulation of lattice vibrations using ultrafast light pulses enables access to nonequilibrium 'hidden' phases with designed functionalities in quantum materials. However, expanding the understanding of nonlinear light-phonon interaction mechanisms remains crucial for developing new strategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3 driven by intense terahertz excitation. As the terahertz field increases, the system transitions from the quantum paraelectric (QPE) ground state to an intermediate ferroelectric phase, and then unexpectedly reverts to a QPE state above ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice dynamics compared to the initial phases, highlighting activated antiferrodistortive phonon modes. Aided by first-principles dynamical calculations, we identify the mechanism for these complex behaviors as a superposition of multiple coherently excited eigenstates of the polar soft mode. Our results reveal a previously uncharted quantum facet of SrTiO3 and open pathways for harnessing high-order excitations to engineer quantum materials in the ultrafast regime.

**Link**: [arxiv](http://arxiv.org/abs/2412.20887v1),  [pdf](http://arxiv.org/pdf/2412.20887v1)

**Tags**: cond-mat.mtrl-sci physics.optics 



### SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language   Models
**Authors**: Kaushal Kumar Maurya, KV Aditya Srivatsa, Ekaterina Kochmar

**Updated**: 2024-12-30T05:01:44Z

**Summary**: Large language models (LLMs) have seen widespread adoption due to their remarkable performance across various applications, driving the accelerated development of a large number of diverse LLMs. However, these individual LLMs show limitations in generalization and performance on complex tasks due to inherent training biases, model size constraints, and the quality or diversity of pre-training datasets. A promising direction is to efficiently harness the diverse capabilities of LLMs to overcome these individual limitations. To address these limitations, we introduce a novel LLM selection algorithm called SelectLLM, which efficiently directs input queries to the most suitable subset of LLMs from a large pool, ensuring that the selected models collectively provide accurate responses. SelectLLM employs a multi-label classifier and policy based on the classifier's predictions and confidence scores in selecting an optimal, query-aware, and lightweight subset of LLMs. Our findings indicate that the proposed model outperforms existing ensemble-based baselines and achieves competitive performance with similarly sized top-performing LLMs while maintaining efficiency. Specifically, it achieves a huge reduction in inference latency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU, compared to the top-performing baselines. Also, we establish a theoretical upper bound by an oracle with LLMs and explore in-depth linguistic analysis to understand the performance gap between Oracle and SelectLLM.

**Link**: [arxiv](http://arxiv.org/abs/2408.08545v2),  [pdf](http://arxiv.org/pdf/2408.08545v2)

**Tags**: cs.CL 



### Align Attention Heads Before Merging Them: An Effective Way for   Converting MHA to GQA
**Authors**: Qingyun Jin, Xiaohui Song, Feng Zhou, Zengchang Qin

**Updated**: 2024-12-30T03:05:45Z

**Summary**: Large language models have been shown to perform well on a variety of natural language processing problems. However, as the model size and the input sequence's length increase, the rapid increase of KV Cache significantly slows down inference speed. Therefore GQA model, as an alternative to MHA model, has been widely introduced into LLMs. In this work, we propose a low-cost method for pruning MHA models into GQA models with any compression ratio of key-value heads. Our method is based on $\mathit{L_0}$ masks to gradually remove redundant parameters. In addition, we apply orthogonal transformations to attention heads without changing the model to increase similarity between attention heads before pruning training, in order to further improve performance of the model. Our method can be compatible with rotary position embedding (RoPE), which means the model after training can be fully adapted to the mainstream standard GQA framework. Experiments demonstrate that our strategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model without too much performance degradation, just achieved through supervised fine-tuning.

**Link**: [arxiv](http://arxiv.org/abs/2412.20677v1),  [pdf](http://arxiv.org/pdf/2412.20677v1)

**Tags**: cs.CL 



### Dynamic Optimization of Storage Systems Using Reinforcement Learning   Techniques
**Authors**: Chiyu Cheng, Chang Zhou, Yang Zhao, Jin Cao

**Updated**: 2024-12-29T17:41:40Z

**Summary**: The exponential growth of data-intensive applications has placed unprecedented demands on modern storage systems, necessitating dynamic and efficient optimization strategies. Traditional heuristics employed for storage performance optimization often fail to adapt to the variability and complexity of contemporary workloads, leading to significant performance bottlenecks and resource inefficiencies. To address these challenges, this paper introduces RL-Storage, a novel reinforcement learning (RL)-based framework designed to dynamically optimize storage system configurations. RL-Storage leverages deep Q-learning algorithms to continuously learn from real-time I/O patterns and predict optimal storage parameters, such as cache size, queue depths, and readahead settings[1]. The proposed framework operates within the storage kernel, ensuring minimal latency and low computational overhead. Through an adaptive feedback mechanism, RL-Storage dynamically adjusts critical parameters, achieving efficient resource utilization across a wide range of workloads. Experimental evaluations conducted on a range of benchmarks, including RocksDB and PostgreSQL, demonstrate significant improvements, with throughput gains of up to 2.6x and latency reductions of 43% compared to baseline heuristics. Additionally, RL-Storage achieves these performance enhancements with a negligible CPU overhead of 0.11% and a memory footprint of only 5 KB, making it suitable for seamless deployment in production environments. This work underscores the transformative potential of reinforcement learning techniques in addressing the dynamic nature of modern storage systems. By autonomously adapting to workload variations in real time, RL-Storage provides a robust and scalable solution for optimizing storage performance, paving the way for next-generation intelligent storage infrastructures.

**Link**: [arxiv](http://arxiv.org/abs/2501.00068v1),  [pdf](http://arxiv.org/pdf/2501.00068v1)

**Tags**: cs.OS cs.DC cs.LG 



### Ns3 meets Sionna: Using Realistic Channels in Network Simulation
**Authors**: Anatolij Zubow, Yannik Pilz, Sascha Rösler, Falko Dressler

**Updated**: 2024-12-29T17:18:21Z

**Summary**: Network simulators are indispensable tools for the advancement of wireless network technologies, offering a cost-effective and controlled environment to simulate real-world network behavior. However, traditional simulators, such as the widely used ns-3, exhibit limitations in accurately modeling indoor and outdoor scenarios due to their reliance on simplified statistical and stochastic channel propagation models, which often fail to accurately capture physical phenomena like multipath signal propagation and shadowing by obstacles in the line-of-sight path. We present Ns3Sionna, which integrates a ray tracing-based channel model, implemented using the Sionna RT framework, within the ns-3 network simulator. It allows to simulate environment-specific and physically accurate channel realizations for a given 3D scene and wireless device positions. Additionally, a mobility model based on ray tracing was developed to accurately represent device movements within the simulated 3D space. Ns3Sionna provides more realistic path and delay loss estimates for both indoor and outdoor environments than existing ns-3 propagation models, particularly in terms of spatial and temporal correlation. Moreover, fine-grained channel state information is provided, which could be used for the development of sensing applications. Due to the significant computational demands of ray tracing, Ns3Sionna takes advantage of the parallel execution capabilities of modern GPUs and multi-core CPUs by incorporating intelligent pre-caching mechanisms that leverage the channel's coherence time to optimize runtime performance. This enables the efficient simulation of scenarios with a small to medium number of mobile nodes.

**Link**: [arxiv](http://arxiv.org/abs/2412.20524v1),  [pdf](http://arxiv.org/pdf/2412.20524v1)

**Tags**: cs.NI 



### Revisiting Cache Freshness for Emerging Real-Time Applications
**Authors**: Ziming Mao, Rishabh Iyer, Scott Shenker, Ion Stoica

**Updated**: 2024-12-28T17:17:03Z

**Summary**: Caching is widely used in industry to improve application performance by reducing data-access latency and taking the load off the backend infrastructure. TTLs have become the de-facto mechanism used to keep cached data reasonably fresh (i.e., not too out of date with the backend). However, the emergence of real-time applications requires tighter data freshness, which is impractical to achieve with TTLs. We discuss why this is the case, and propose a simple yet effective adaptive policy to achieve the desired freshness.

**Link**: [arxiv](http://arxiv.org/abs/2412.20221v1),  [pdf](http://arxiv.org/pdf/2412.20221v1)

**Tags**: cs.OS cs.DC cs.NI 



### LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System
**Authors**: Hyucksung Kwon, Kyungmo Koo, Janghyeon Kim, Woongkyu Lee, Minjae Lee, Hyungdeok Lee, Yousub Jung, Jaehan Park, Yosub Song, Byeongsu Yang, Haerang Choi, Guhyun Kim, Jongsoon Won, Woojae Shin, Changhyun Kim, Gyeongcheol Shin, Yongkee Kwon, Ilkon Kim, Euicheol Lim, John Kim, Jungwook Choi

**Updated**: 2024-12-28T14:38:16Z

**Summary**: The expansion of large language models (LLMs) with hundreds of billions of parameters presents significant challenges to computational resources, particularly data movement and memory bandwidth. Long-context LLMs, which process sequences of tens of thousands of tokens, further increase the demand on the memory system as the complexity in attention layers and key-value cache sizes is proportional to the context length. Processing-in-Memory (PIM) maximizes memory bandwidth by moving compute to the data and can address the memory bandwidth challenges; however, PIM is not necessarily scalable to accelerate long-context LLM because of limited per-module memory capacity and the inflexibility of fixed-functional unit PIM architecture and static memory management. In this work, we propose LoL-PIM which is a multi-node PIM architecture that accelerates long context LLM through hardware-software co-design. In particular, we propose how pipeline parallelism can be exploited across a multi-PIM module while a direct PIM access (DPA) controller (or DMA for PIM) is proposed that enables dynamic PIM memory management and results in efficient PIM utilization across a diverse range of context length. We developed an MLIR-based compiler for LoL-PIM extending a commercial PIM-based compiler where the software modifications were implemented and evaluated, while the hardware changes were modeled in the simulator. Our evaluations demonstrate that LoL-PIM significantly improves throughput and reduces latency for long-context LLM inference, outperforming both multi-GPU and GPU-PIM systems (up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient deployment of LLMs in real-world applications.

**Link**: [arxiv](http://arxiv.org/abs/2412.20166v1),  [pdf](http://arxiv.org/pdf/2412.20166v1)

**Tags**: cs.AR cs.AI 



### ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal   Visual Token Trimming
**Authors**: Jiedong Zhuang, Lu Lu, Ming Dai, Rui Hu, Jian Chen, Qiang Liu, Haoji Hu

**Updated**: 2024-12-28T10:17:29Z

**Summary**: Multimodal large language models (MLLMs) enhance their perceptual capabilities by integrating visual and textual information. However, processing the massive number of visual tokens incurs a significant computational cost. Existing analysis of the MLLM attention mechanisms remains shallow, leading to coarse-grain token pruning strategies that fail to effectively balance speed and accuracy. In this paper, we conduct a comprehensive investigation of MLLM attention mechanisms with LLaVA. We find that numerous visual tokens and partial attention computations are redundant during the decoding process. Based on this insight, we propose Spatial-Temporal Visual Token Trimming ($\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without retraining. $\textbf{ST}^{3}$ consists of two primary components: 1) Progressive Visual Token Pruning (\textbf{PVTP}), which eliminates inattentive visual tokens across layers, and 2) Visual Token Annealing (\textbf{VTA}), which dynamically reduces the number of visual tokens in each layer as the generated tokens grow. Together, these techniques deliver around $\mathbf{2\times}$ faster inference with only about $\mathbf{30\%}$ KV cache memory compared to the original LLaVA, while maintaining consistent performance across various datasets. Crucially, $\textbf{ST}^{3}$ can be seamlessly integrated into existing pre-trained MLLMs, providing a plug-and-play solution for efficient inference.

**Link**: [arxiv](http://arxiv.org/abs/2412.20105v1),  [pdf](http://arxiv.org/pdf/2412.20105v1)

**Tags**: cs.CV 



### A Robust Federated Learning Framework for Undependable Devices at Scale
**Authors**: Shilong Wang, Jianchun Liu, Hongli Xu, Chunming Qiao, Huarong Deng, Qiuye Zheng, Jiantao Gong

**Updated**: 2024-12-28T03:28:52Z

**Summary**: In a federated learning (FL) system, many devices, such as smartphones, are often undependable (e.g., frequently disconnected from WiFi) during training. Existing FL frameworks always assume a dependable environment and exclude undependable devices from training, leading to poor model performance and resource wastage. In this paper, we propose FLUDE to effectively deal with undependable environments. First, FLUDE assesses the dependability of devices based on the probability distribution of their historical behaviors (e.g., the likelihood of successfully completing training). Based on this assessment, FLUDE adaptively selects devices with high dependability for training. To mitigate resource wastage during the training phase, FLUDE maintains a model cache on each device, aiming to preserve the latest training state for later use in case local training on an undependable device is interrupted. Moreover, FLUDE proposes a staleness-aware strategy to judiciously distribute the global model to a subset of devices, thus significantly reducing resource wastage while maintaining model performance. We have implemented FLUDE on two physical platforms with 120 smartphones and NVIDIA Jetson devices. Extensive experimental results demonstrate that FLUDE can effectively improve model performance and resource efficiency of FL training in undependable environments.

**Link**: [arxiv](http://arxiv.org/abs/2412.19991v1),  [pdf](http://arxiv.org/pdf/2412.19991v1)

**Tags**: cs.LG cs.DC 



### Performance Characterization and Optimizations of Traditional ML   Applications
**Authors**: Harsh Kumar, R. Govindarajan

**Updated**: 2024-12-26T04:13:52Z

**Summary**: Even in the era of Deep Learning based methods, traditional machine learning methods with large data sets continue to attract significant attention. However, we find an apparent lack of a detailed performance characterization of these methods in the context of large training datasets. In this work, we study the system's behavior of a number of traditional ML methods as implemented in popular free software libraries/modules to identify critical performance bottlenecks experienced by these applications. The performance characterization study reveals several interesting insights on the performance of these applications. Then we evaluate the performance benefits of applying some well-known optimizations at the levels of caches and the main memory. More specifically, we test the usefulness of optimizations such as (i) software prefetching to improve cache performance and (ii) data layout and computation reordering optimizations to improve locality in DRAM accesses. These optimizations are implemented as modifications to the well-known scikit-learn library, and hence can be easily leveraged by application programmers. We evaluate the impact of the proposed optimizations using a combination of simulation and execution on a real system. The software prefetching optimization results in performance benefits varying from 5.2%-27.1% on different ML applications while the data layout and computation reordering approaches yield 6.16%-28.0% performance improvement.

**Link**: [arxiv](http://arxiv.org/abs/2412.19051v1),  [pdf](http://arxiv.org/pdf/2412.19051v1)

**Tags**: cs.PF 



### XRFlux: Virtual Reality Benchmark for Edge Caching Systems
**Authors**: Nader Alfares, George Kesidis

**Updated**: 2024-12-25T18:36:21Z

**Summary**: We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality (VR) delivery systems using edge-cloud caching. As VR applications and systems progress, the need to meet strict latency and Quality of Experience (QoE) requirements is increasingly evident. In the context of VR, traditional cloud architectures (e.g., remote AWS S3 for content delivery) often struggle to meet these demands, especially for users of the same application in different locations. With edge computing, resources are brought closer to users in efforts to reduce latency and improve QoEs. However, VR's dynamic nature, with changing fields of view (FoVs) and user synchronization requirements, creates various challenges for edge caching. We address the lack of suitable benchmarks and propose a framework that simulates multiuser VR scenarios while logging users' interaction with objects within their actual and predicted FoVs. The benchmark's activity log can then be played back through an edge cache to assess the resulting QoEs. This tool fills a gap by supporting research in the optimization of edge caching (and other edge-cloud functions) for VR streaming.

**Link**: [arxiv](http://arxiv.org/abs/2412.18960v1),  [pdf](http://arxiv.org/pdf/2412.18960v1)

**Tags**: cs.PF cs.MM 



### Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With   Structured Memories
**Authors**: Dulhan Jayalath, James Bradley Wendt, Nicholas Monath, Sandeep Tata, Beliz Gunel

**Updated**: 2024-12-25T14:14:31Z

**Summary**: Long-range tasks require reasoning over long inputs. Existing solutions either need large compute budgets, training data, access to model weights, or use complex, task-specific approaches. We present PRISM, which alleviates these concerns by processing information as a stream of chunks, maintaining a structured in-context memory specified by a typed hierarchy schema. This approach demonstrates superior performance to baselines on diverse tasks while using at least 4x smaller contexts than long-context models. Moreover, PRISM is token-efficient. By producing short outputs and efficiently leveraging key-value (KV) caches, it achieves up to 54% cost reduction when compared to alternative short-context approaches. The method also scales down to tiny information chunks (e.g., 500 tokens) without increasing the number of tokens encoded or sacrificing quality. Furthermore, we show that it is possible to generate schemas to generalize our approach to new tasks with minimal effort.

**Link**: [arxiv](http://arxiv.org/abs/2412.18914v1),  [pdf](http://arxiv.org/pdf/2412.18914v1)

**Tags**: cs.AI 



### Accelerating Diffusion Transformers with Dual Feature Caching
**Authors**: Chang Zou, Evelyn Zhang, Runlin Guo, Haohang Xu, Conghui He, Xuming Hu, Linfeng Zhang

**Updated**: 2024-12-25T14:00:14Z

**Summary**: Diffusion Transformers (DiT) have become the dominant methods in image and video generation yet still suffer substantial computational costs. As an effective approach for DiT acceleration, feature caching methods are designed to cache the features of DiT in previous timesteps and reuse them in the next timesteps, allowing us to skip the computation in the next timesteps. However, on the one hand, aggressively reusing all the features cached in previous timesteps leads to a severe drop in generation quality. On the other hand, conservatively caching only the features in the redundant layers or tokens but still computing the important ones successfully preserves the generation quality but results in reductions in acceleration ratios. Observing such a tradeoff between generation quality and acceleration performance, this paper begins by quantitatively studying the accumulated error from cached features. Surprisingly, we find that aggressive caching does not introduce significantly more caching errors in the caching step, and the conservative feature caching can fix the error introduced by aggressive caching. Thereby, we propose a dual caching strategy that adopts aggressive and conservative caching iteratively, leading to significant acceleration and high generation quality at the same time. Besides, we further introduce a V-caching strategy for token-wise conservative caching, which is compatible with flash attention and requires no training and calibration data.   Our codes have been released in Github: \textbf{Code: \href{https://github.com/Shenyi-Z/DuCa}{\texttt{\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}

**Link**: [arxiv](http://arxiv.org/abs/2412.18911v1),  [pdf](http://arxiv.org/pdf/2412.18911v1)

**Tags**: cs.LG cs.AI cs.CV 



### Aspect-oriented Programming with Julia
**Authors**: Osamu Ishimura, Yoshihide Yoshimoto

**Updated**: 2024-12-25T11:59:17Z

**Summary**: This paper proposes integrating Aspect-oriented Programming (AOP) into Julia, a language widely used in scientific and High-Performance Computing (HPC). AOP enhances software modularity by encapsulating cross-cutting concerns, such as logging, caching, and parallelizing, into separate, reusable aspects. Leveraging Julia's powerful metaprogramming and abstract syntax tree (AST) manipulation capabilities, we introduce AspectJulia, an AOP framework designed to operate within Julia's runtime environment as a package. AspectJulia enables developers to define and apply aspects seamlessly, leading to more modular, maintainable, and adaptable code. We detail the implementation of AspectJulia and present diverse use cases, ranging from HPC and scientific computing to business applications, demonstrating its effectiveness in managing cross-cutting concerns. This integration simplifies application development and improves the adaptability of existing Julia modules and packages, paving the way for more efficient and maintainable software systems.

**Link**: [arxiv](http://arxiv.org/abs/2412.18885v1),  [pdf](http://arxiv.org/pdf/2412.18885v1)

**Tags**: cs.PL 



### Efficiently serving large multimedia models using EPD Disaggregation
**Authors**: Gursimran Singh, Xinglu Wang, Ivan Hu, Timothy Yu, Linzi Xing, Wei Jiang, Zhefeng Wang, Xiaolong Bai, Yi Li, Ying Xiong, Yong Zhang, Zhenan Fan

**Updated**: 2024-12-25T10:11:31Z

**Summary**: Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by handling diverse inputs such as images, audio, and video, but at the cost of adding a multimodal encoding stage that increases both computational and memory overhead. This step helps convert raw inputs into tokenized representations that inflate the token sequence for the prefill phase, negatively impacting key Service Level Objectives (SLOs) like time to first token (TTFT) and end-to-end throughput. We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates the encoding, prefill, and decode stages onto dedicated resources. Unlike current systems, which bundle encoding and prefill together, our disaggregation approach alleviates memory bottlenecks, mitigates synchronization delays, and supports flexible batching. Specifically, we employ a new caching mechanism for multimodal tokens, enabling asynchronous transfer of multimodal tokens and introduce an integrated module to find optimal config for EPD system and minimize resource usage while maximizing SLO-based performance metric. Experimental evaluations with popular LMMs show substantial gains in memory efficiency (up to 15$\times$ lesser for encoding-stage GPUs), that supports upto 22$\times$ higher batch sizes, 10$\times$ more number of images/ request, 2.2$\times$ higher kv cache size. Further, it leads to significant improvements in end-to-end throughput (up to 57\% better), and latency metrics (TTFT up to 71\% lower), compared to systems that do not disaggregate. Our findings underscore the potential of EPD disaggregation to enable resource-efficient and high-performance multimodal inference at scale.

**Link**: [arxiv](http://arxiv.org/abs/2501.05460v1),  [pdf](http://arxiv.org/pdf/2501.05460v1)

**Tags**: cs.DC cs.AI cs.CV cs.LG 



### HashEvict: A Pre-Attention KV Cache Eviction Strategy using   Locality-Sensitive Hashing
**Authors**: Minghui Liu, Tahseen Rabbani, Tony O'Halloran, Ananth Sankaralingam, Mary-Anne Hartley, Brian Gravelle, Furong Huang, Cornelia Fermüller, Yiannis Aloimonos

**Updated**: 2024-12-24T13:04:45Z

**Summary**: Transformer-based large language models (LLMs) use the key-value (KV) cache to significantly accelerate inference by storing the key and value embeddings of past tokens. However, this cache consumes significant GPU memory. In this work, we introduce HashEvict, an algorithm that uses locality-sensitive hashing (LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache that are cosine dissimilar to the current query token. This is achieved by computing the Hamming distance between binarized Gaussian projections of the current token query and cached token keys, with a projection length much smaller than the embedding dimension. We maintain a lightweight binary structure in GPU memory to facilitate these calculations. Unlike existing compression strategies that compute attention to determine token retention, HashEvict makes these decisions pre-attention, thereby reducing computational costs. Additionally, HashEvict is dynamic - at every decoding step, the key and value of the current token replace the embeddings of a token expected to produce the lowest attention score. We demonstrate that HashEvict can compress the KV cache by 30%-70% while maintaining high performance across reasoning, multiple-choice, long-context retrieval and summarization tasks.

**Link**: [arxiv](http://arxiv.org/abs/2412.16187v2),  [pdf](http://arxiv.org/pdf/2412.16187v2)

**Tags**: cs.LG cs.AI cs.CL cs.DS cs.PF 



### Development and Application of a Decentralized Domain Name Service
**Authors**: Guang Yang

**Updated**: 2024-12-24T00:46:00Z

**Summary**: The current Domain Name System (DNS), as a core infrastructure of the internet, exhibits several shortcomings: its centralized architecture leads to censorship risks and single points of failure, making domain name resolution vulnerable to attacks. The lack of encryption in the resolution process exposes it to DNS hijacking and cache poisoning attacks. Additionally, the high operational costs limit participation and innovation among small to medium-sized users. To address these issues, this paper proposes a Decentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and distributed storage (IPFS). By leveraging the immutability of blockchain and the content verification of IPFS, the system achieves decentralized storage and distribution of domain name records, eliminating the centralized dependencies of traditional DNS. With a block time of 15 seconds, the system supports rapid broadcasting of domain name updates, significantly improving resolution efficiency. The DDNS aims to serve as a complement or backup to the existing DNS system, providing a pollution-resistant, censorship-resistant, high-performance, and low-cost domain name resolution solution, offering a new technical path for the security and stability of the internet.

**Link**: [arxiv](http://arxiv.org/abs/2412.01959v2),  [pdf](http://arxiv.org/pdf/2412.01959v2)

**Tags**: cs.NI 



### Deliberation in Latent Space via Differentiable Cache Augmentation
**Authors**: Luyang Liu, Jonas Pfeiffer, Jiaxing Wu, Jun Xie, Arthur Szlam

**Updated**: 2024-12-23T18:02:25Z

**Summary**: Techniques enabling large language models (LLMs) to "think more" by generating and attending to intermediate reasoning steps have shown promise in solving complex problems. However, the standard approaches generate sequences of discrete tokens immediately before responding, and so they can incur significant latency costs and be challenging to optimize. In this work, we demonstrate that a frozen LLM can be augmented with an offline coprocessor that operates on the model's key-value (kv) cache. This coprocessor augments the cache with a set of latent embeddings designed to improve the fidelity of subsequent decoding. We train this coprocessor using the language modeling loss from the decoder on standard pretraining data, while keeping the decoder itself frozen. This approach enables the model to learn, in an end-to-end differentiable fashion, how to distill additional computation into its kv-cache. Because the decoder remains unchanged, the coprocessor can operate offline and asynchronously, and the language model can function normally if the coprocessor is unavailable or if a given cache is deemed not to require extra computation. We show experimentally that when a cache is augmented, the decoder achieves lower perplexity on numerous subsequent tokens. Furthermore, even without any task-specific training, our experiments demonstrate that cache augmentation consistently reduces perplexity and improves performance across a range of reasoning-intensive tasks.

**Link**: [arxiv](http://arxiv.org/abs/2412.17747v1),  [pdf](http://arxiv.org/pdf/2412.17747v1)

**Tags**: cs.CL cs.AI cs.LG 



### A Reproducible Method for Mapping Electricity Transmission   Infrastructure for Space Weather Risk Assessment
**Authors**: Edward J. Oughton, Evan Alexander Peters, Dennies Bor, Noah Rivera, C. Trevor Gaunt, Robert Weigel

**Updated**: 2024-12-23T16:11:18Z

**Summary**: Space weather impact assessment is constrained by the lack of available asset information to undertake modeling of Geomagnetically Induced Currents (GICs) in Extra High Voltage electricity infrastructure networks. The U.S. National Space Weather Strategy and Action Plan identifies underutilized data as a central issue for improving risk assessment, motivating this research. Accurate GIC prediction is generally not possible without information on the electrical circuit, therefore we define a reproducible method based on open-source data, which enables risk analysts to collect their own substation component data. This process converts OpenStreetMap (OSM) substation locations to high-resolution, component-level mapping of electricity transmission assets by utilizing an innovative web-browser platform to facilitate component annotation. As a case study example, we convert an initial 1,313 high-voltage (>115 kV) substations to 52,273 substation components via Google Earth APIs utilizing low-altitude, satellite, and Streetview imagery. We find that a total of 41,642 substation components (79.6%) connect to the highest substation voltage levels (>345 kV) and are possibly susceptible to GIC, with a total of 7,949 transformers identified. Compared to the initial OSM baseline, we provide new detailed insights on voltage levels, line capacities, and substation configurations. Two validation workshops were undertaken to align the method and data with GIC assessment needs. The approach ensures consistency and rapid scalability, enabling users to quickly count components via a flexible web-browser application.

**Link**: [arxiv](http://arxiv.org/abs/2412.17685v1),  [pdf](http://arxiv.org/pdf/2412.17685v1)

**Tags**: physics.geo-ph cs.SY eess.SY 



### Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT   in SAGIN
**Authors**: Qian Chen, Chenyu Wu, Shuai Han, Weixiao Meng, Tony Q. S. Quek

**Updated**: 2024-12-23T14:40:26Z

**Summary**: The rapid development of the aviation Internet of Things (IoT) has positioned in-flight connectivity (IFC) as one of its critical applications. Space-air-ground integrated networks (SAGIN) are essential for ensuring the performance of IFC by enabling seamless and reliable connectivity. However, most existing research treats satellites merely as transparent forwarding nodes and overlooks their potential caching capabilities to enhance IFC data rates. In this article, we explore an IFC-oriented SAGIN where satellites and ground stations (GSs) work together to transmit content to airborne passengers, thereby facilitating airborne communication. By categorizing files into cached (instantly accessible via satellites) and non-cached files (available only through GSs), this article pioneers the integration of multiple inter-satellite links (ISLs) into the IFC framework, thus innovating the content delivery process for both types of files. To minimize the average delay of content delivery, we formulate the corresponding optimization problems: 1) For cached files, we propose an exact penalty-based method to determine the satellite association scheme. 2) For non-cached files, we present an efficient algorithm based on alternating optimization to jointly optimize satellite association and GS bandwidth allocation. Our proposed framework is low in complexity, paving the way for high-speed Internet connectivity for aviation passengers. Finally, simulation results are provided to demonstrate the effectiveness of our proposed IFC framework for SAGIN.

**Link**: [arxiv](http://arxiv.org/abs/2405.18919v2),  [pdf](http://arxiv.org/pdf/2405.18919v2)

**Tags**: cs.IT math.IT 



### A Framework for Effective Invocation Methods of Various LLM Services
**Authors**: Can Wang, Dianbo Sui, Bolin Zhang, Xiaoyu Liu, Jiabao Kang, Zhidong Qiao, Zhiying Tu

**Updated**: 2024-12-23T12:55:21Z

**Summary**: Large Language Models (LLMs) have shown impressive abilities in solving various natural language processing tasks and are now widely offered as services. LLM services enable users to accomplish tasks without requiring specialized knowledge, simply by paying service providers. However, numerous providers offer various LLM services with variations in pricing, latency, and performance. These factors are also affected by different invocation methods, such as the choice of context and the use of cache, which lead to unpredictable and uncontrollable service cost and quality. Consequently, utilizing various LLM services invocation methods to construct an effective (cost-saving, low-latency and high-performance) invocation strategy that best meets task demands becomes a pressing challenge. This paper provides a comprehensive overview of methods help LLM services to be invoked efficiently. Technically, we define the problem of constructing an effective LLM services invocation strategy, and based on this, propose a unified LLM service invocation framework. The framework classifies existing methods into four categories: input abstraction, semantic cache, solution design, and output enhancement, which can be used separately or jointly during the invocation life cycle. We discuss the methods in each category and compare them to provide valuable guidance for researchers. Finally, we emphasize the open challenges in this domain and shed light on future research.

**Link**: [arxiv](http://arxiv.org/abs/2402.03408v3),  [pdf](http://arxiv.org/pdf/2402.03408v3)

**Tags**: cs.SE cs.DC 



### CALLIC: Content Adaptive Learning for Lossless Image Compression
**Authors**: Daxin Li, Yuanchao Bai, Kai Wang, Junjun Jiang, Xianming Liu, Wen Gao

**Updated**: 2024-12-23T10:41:18Z

**Summary**: Learned lossless image compression has achieved significant advancements in recent years. However, existing methods often rely on training amortized generative models on massive datasets, resulting in sub-optimal probability distribution estimation for specific testing images during encoding process. To address this challenge, we explore the connection between the Minimum Description Length (MDL) principle and Parameter-Efficient Transfer Learning (PETL), leading to the development of a novel content-adaptive approach for learned lossless image compression, dubbed CALLIC. Specifically, we first propose a content-aware autoregressive self-attention mechanism by leveraging convolutional gating operations, termed Masked Gated ConvFormer (MGCF), and pretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed to accelerate the coding process. During encoding, we decompose pre-trained layers, including depth-wise convolutions, using low-rank matrices and then adapt the incremental weights on testing image by Rate-guided Progressive Fine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are sorted in descending order by estimated entropy, optimizing learning process and reducing adaptation time. Extensive experiments across diverse datasets demonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless image compression.

**Link**: [arxiv](http://arxiv.org/abs/2412.17464v1),  [pdf](http://arxiv.org/pdf/2412.17464v1)

**Tags**: cs.CV eess.IV 



### Fast and Live Model Auto Scaling with O(1) Host Caching
**Authors**: Dingyan Zhang, Haotian Wang, Yang Liu, Xingda Wei, Yizhou Shan, Rong Chen, Haibo Chen

**Updated**: 2024-12-23T03:38:46Z

**Summary**: Model autoscaling is the key mechanism to achieve serverless model-as-a-service, but it faces a fundamental trade-off between scaling speed and storage/memory usage to cache parameters, and cannot meet frequent scaling requirements across multiple hosts. The key problem is that data plane performance is slow, and scaled instances remain stopped while parameters are loading. We first show that data plane can be made fast with no/O(1) caching by loading parameters through the compute network between GPUs because: (1) its speed is comparable host cache and is underutilized; (2) scaling multiple instances requires no or O(1) caching with network-optimized multicast. Second, autoscaling can be made live by breaking the scaling abstraction from a coarse-grained instance-level to a fine-grained layer-level. This allows us to offload the layer computation from the overloaded serving instances to the scaled instance with cooperative execution, thus handles cases even when the compute network is not sufficiently fast. Our system BLITZSCALE reduces the serving tail latencies by up to 86% without caching, and we achieve comparable performance (or even better) to an optimal setup where all the parameters are cached at all the host for autoscaling.

**Link**: [arxiv](http://arxiv.org/abs/2412.17246v1),  [pdf](http://arxiv.org/pdf/2412.17246v1)

**Tags**: cs.DC cs.OS 



### Semi-Supervised Contrastive Learning for Controllable Video-to-Music   Retrieval
**Authors**: Shanti Stewart, Gouthaman KV, Lie Lu, Andrea Fanelli

**Updated**: 2024-12-23T02:52:36Z

**Summary**: Content creators often use music to enhance their videos, from soundtracks in movies to background music in video blogs and social media content. However, identifying the best music for a video can be a difficult and time-consuming task. To address this challenge, we propose a novel framework for automatically retrieving a matching music clip for a given video, and vice versa. Our approach leverages annotated music labels, as well as the inherent artistic correspondence between visual and music elements. Distinct from previous cross-modal music retrieval works, our method combines both self-supervised and supervised training objectives. We use self-supervised and label-supervised contrastive learning to train a joint embedding space between music and video. We show the effectiveness of our approach by using music genre labels for the supervised training component, and our framework can be generalized to other music annotations (e.g., emotion, instrument, etc.). Furthermore, our method enables fine-grained control over how much the retrieval process focuses on self-supervised vs. label information at inference time. We evaluate the learned embeddings through a variety of video-to-music and music-to-video retrieval tasks. Our experiments show that the proposed approach successfully combines self-supervised and supervised objectives and is effective for controllable music-video retrieval.

**Link**: [arxiv](http://arxiv.org/abs/2412.05831v2),  [pdf](http://arxiv.org/pdf/2412.05831v2)

**Tags**: cs.MM cs.SD eess.AS 



### Agile TLB Prefetching and Prediction Replacement Policy
**Authors**: Melkamu Mersha, Tsion Abay, Mingziem Bitewa, Gedare Bloom

**Updated**: 2024-12-23T00:46:53Z

**Summary**: Virtual-to-physical address translation is a critical performance bottleneck in paging-based virtual memory systems. The Translation Lookaside Buffer (TLB) accelerates address translation by caching frequently accessed mappings, but TLB misses lead to costly page walks. Hardware and software techniques address this challenge. Hardware approaches enhance TLB reach through system-level support, while software optimizations include TLB prefetching, replacement policies, superpages, and page size adjustments. Prefetching Page Table Entries (PTEs) for future accesses reduces bottlenecks but may incur overhead from incorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP optimizes performance by leveraging page table locality and dynamically identifying essential free PTEs during page walks. Predictive replacement policies further improve TLB performance. Traditional LRU replacement is limited to near-instant references, while advanced policies like SRRIP, GHRP, SHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies. CHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control flow history to detect dead blocks, utilizing L2 TLB entries for learning instead of sampling. These integrated techniques collectively address key challenges in virtual memory management.

**Link**: [arxiv](http://arxiv.org/abs/2412.17203v1),  [pdf](http://arxiv.org/pdf/2412.17203v1)

**Tags**: cs.AR 



### MVREC: A General Few-shot Defect Classification Model Using Multi-View   Region-Context
**Authors**: Shuai Lyu, Fangjian Liao, Zeqi Ma, Rongchen Zhang, Dongmei Mo, Waikeung Wong

**Updated**: 2024-12-22T07:14:45Z

**Summary**: Few-shot defect multi-classification (FSDMC) is an emerging trend in quality control within industrial manufacturing. However, current FSDMC research often lacks generalizability due to its focus on specific datasets. Additionally, defect classification heavily relies on contextual information within images, and existing methods fall short of effectively extracting this information. To address these challenges, we propose a general FSDMC framework called MVREC, which offers two primary advantages: (1) MVREC extracts general features for defect instances by incorporating the pre-trained AlphaCLIP model. (2) It utilizes a region-context framework to enhance defect features by leveraging mask region input and multi-view context augmentation. Furthermore, Few-shot Zip-Adapter(-F) classifiers within the model are introduced to cache the visual features of the support set and perform few-shot classification. We also introduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes 1228 defect images with instance-level mask annotations and 46 defect types. Extensive experiments conducted on MVTec-FS and four additional datasets demonstrate its effectiveness in general defect classification and its ability to incorporate contextual information to improve classification performance. Code: https://github.com/ShuaiLYU/MVREC

**Link**: [arxiv](http://arxiv.org/abs/2412.16897v1),  [pdf](http://arxiv.org/pdf/2412.16897v1)

**Tags**: cs.CV cs.AI 



### MemServe: Context Caching for Disaggregated LLM Serving with Elastic   Memory Pool
**Authors**: Cunchen Hu, Heyang Huang, Junhao Hu, Jiang Xu, Xusheng Chen, Tao Xie, Chenxi Wang, Sa Wang, Yungang Bao, Ninghui Sun, Yizhou Shan

**Updated**: 2024-12-21T13:55:49Z

**Summary**: Large language model (LLM) serving has transformed from stateless to stateful systems, utilizing techniques like context caching and disaggregated inference. These optimizations extend the lifespan and domain of the KV cache, necessitating a new architectural approach. We present MemServe, a unified system that integrates both inter-request and intra-request optimizations. MemServe introduces MemPool, an elastic memory pool managing distributed memory and KV caches across serving instances. Using MemPool APIs, MemServe combines context caching with disaggregated inference for the first time, supported by a global scheduler that enhances cache reuse through a global prompt tree-based locality-aware policy. Tests show that MemServe significantly improves job completion time and time-to-first-time.

**Link**: [arxiv](http://arxiv.org/abs/2406.17565v3),  [pdf](http://arxiv.org/pdf/2406.17565v3)

**Tags**: cs.DC 



### Parameterized Complexity of Caching in Networks
**Authors**: Robert Ganian, Fionn Mc Inerney, Dimitra Tsigkari

**Updated**: 2024-12-21T11:20:26Z

**Summary**: The fundamental caching problem in networks asks to find an allocation of contents to a network of caches with the aim of maximizing the cache hit rate. Despite the problem's importance to a variety of research areas -- including not only content delivery, but also edge intelligence and inference -- and the extensive body of work on empirical aspects of caching, very little is known about the exact boundaries of tractability for the problem beyond its general NP-hardness. We close this gap by performing a comprehensive complexity-theoretic analysis of the problem through the lens of the parameterized complexity paradigm, which is designed to provide more precise statements regarding algorithmic tractability than classical complexity. Our results include algorithmic lower and upper bounds which together establish the conditions under which the caching problem becomes tractable.

**Link**: [arxiv](http://arxiv.org/abs/2412.16585v1),  [pdf](http://arxiv.org/pdf/2412.16585v1)

**Tags**: cs.NI cs.CC 



### Yi-Lightning Technical Report
**Authors**: Alan Wake, Bei Chen, C. X. Lv, Chao Li, Chengen Huang, Chenglin Cai, Chujie Zheng, Daniel Cooper, Fan Zhou, Feng Hu, Guoyin Wang, Heng Ji, Howard Qiu, Jiangcheng Zhu, Jun Tian, Katherine Su, Lihuan Zhang, Liying Li, Ming Song, Mou Li, Peng Liu, Qicheng Hu, Shawn Wang, Shijun Zhou, Shiming Yang, Shiyong Li, Tianhang Zhu, Wen Xie, Xiang He, Xiaobo Chen, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Yanpeng Li, Yongke Zhao, Yongzhen Luo, Yuchi Xu, Yuxuan Sha, Zhaodong Yan, Zhiyuan Liu, Zirui Zhang, Zonghong Dai

**Updated**: 2024-12-21T02:36:03Z

**Summary**: This technical report presents Yi-Lightning, our latest flagship large language model (LLM). It achieves exceptional performance, ranking 6th overall on Chatbot Arena, with particularly strong results (2nd to 4th place) in specialized categories including Chinese, Math, Coding, and Hard Prompts. Yi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture, featuring advanced expert segmentation and routing mechanisms coupled with optimized KV-caching techniques. Our development process encompasses comprehensive pre-training, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF), where we devise deliberate strategies for multi-stage training, synthetic data construction, and reward modeling. Furthermore, we implement RAISE (Responsible AI Safety Engine), a four-component framework to address safety issues across pre-training, post-training, and serving phases. Empowered by our scalable super-computing infrastructure, all these innovations substantially reduce training, deployment and inference costs while maintaining high-performance standards. With further evaluations on public academic benchmarks, Yi-Lightning demonstrates competitive performance against top-tier LLMs, while we observe a notable disparity between traditional, static benchmark results and real-world, dynamic human preferences. This observation prompts a critical reassessment of conventional benchmarks' utility in guiding the development of more intelligent and powerful AI systems for practical applications. Yi-Lightning is now available through our developer platform at https://platform.lingyiwanwu.com.

**Link**: [arxiv](http://arxiv.org/abs/2412.01253v4),  [pdf](http://arxiv.org/pdf/2412.01253v4)

**Tags**: cs.CL cs.AI cs.LG 



### SYMPHONY: Improving Memory Management for LLM Inference Workloads
**Authors**: Saurabh Agarwal, Anyong Mao, Aditya Akella, Shivaram Venkataraman

**Updated**: 2024-12-21T01:48:52Z

**Summary**: Large Language Models (LLMs) are increasingly being deployed in applications such as chatbots, code editors, and conversational agents. A key feature of LLMs is their ability to engage in multi-turn interactions with humans or external tools, enabling a wide range of tasks. Each new request in a multi-turn interaction depends on the intermediate state, specifically the key-value (K,V) caches, from previous requests in the ongoing interaction. Existing serving engines either recompute the K,V caches or offload them to main memory. Profiling reveals that recomputation can result in over 99% of processed tokens being redundant. On the other hand, offloading K,V caches from GPU memory makes inference serving stateful, leading to load imbalances across the cluster. To address these challenges, we developed SYMPHONY. SYMPHONY leverages the observation that multi-turn work loads provide additional hints that allow K,V caches to be migrated off the critical serving path. By utilizing these hints, SYMPHONY dynamically migrates K,V caches to enable finegrained scheduling of inference requests. Our experiments demonstrate that SYMPHONY can handle over 8x the number of requests compared to state-of-the-art baselines, with a similar latency profile.

**Link**: [arxiv](http://arxiv.org/abs/2412.16434v1),  [pdf](http://arxiv.org/pdf/2412.16434v1)

**Tags**: cs.DC 



### Multi-Strided Access Patterns to Boost Hardware Prefetching
**Authors**: Miguel O. Blom, Kristian F. D. Rietveld, Rob V. van Nieuwpoort

**Updated**: 2024-12-20T15:51:42Z

**Summary**: Important memory-bound kernels, such as linear algebra, convolutions, and stencils, rely on SIMD instructions as well as optimizations targeting improved vectorized data traversal and data re-use to attain satisfactory performance. On on temporary CPU architectures, the hardware prefetcher is of key importance for efficient utilization of the memory hierarchy. In this paper, we demonstrate that transforming a memory access pattern consisting of a single stride to one that concurrently accesses multiple strides, can boost the utilization of the hardware prefetcher, and in turn improves the performance of memory-bound kernels significantly. Using a set of micro-benchmarks, we establish that accessing memory in a multi-strided manner enables more cache lines to be concurrently brought into the cache, resulting in improved cache hit ratios and higher effective memory bandwidth without the introduction of costly software prefetch instructions. Subsequently, we show that multi-strided variants of a collection of six memory-bound dense compute kernels outperform state-of-the-art counterparts on three different micro-architectures. More specifically, for kernels among which Matrix Vector Multiplication, Convolution Stencil and kernels from PolyBench, we achieve significant speedups of up to 12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and 1.87x over OpenCV. The code transformation to take advantage of multi-strided memory access is a natural extension of the loop unroll and loop interchange techniques, allowing this method to be incorporated into compiler pipelines in the future.

**Link**: [arxiv](http://arxiv.org/abs/2412.16001v1),  [pdf](http://arxiv.org/pdf/2412.16001v1)

**Tags**: cs.PF 



### Towards Projected and Incremental Pseudo-Boolean Model Counting
**Authors**: Suwei Yang, Kuldeep S. Meel

**Updated**: 2024-12-20T15:18:44Z

**Summary**: Model counting is a fundamental task that involves determining the number of satisfying assignments to a logical formula, typically in conjunctive normal form (CNF). While CNF model counting has received extensive attention over recent decades, interest in Pseudo-Boolean (PB) model counting is just emerging partly due to the greater flexibility of PB formulas. As such, we observed feature gaps in existing PB counters such as a lack of support for projected and incremental settings, which could hinder adoption. In this work, our main contribution is the introduction of the PB model counter PBCount2, the first exact PB model counter with support for projected and incremental model counting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree (LOW-MD) computation ordering heuristic to support projected model counting and a cache mechanism to enable incremental model counting. In our evaluations, PBCount2 completed at least 1.40x the number of benchmarks of competing methods for projected model counting and at least 1.18x of competing methods in incremental model counting.

**Link**: [arxiv](http://arxiv.org/abs/2412.14485v2),  [pdf](http://arxiv.org/pdf/2412.14485v2)

**Tags**: cs.AI cs.LO 



### Don't Do RAG: When Cache-Augmented Generation is All You Need for   Knowledge Tasks
**Authors**: Brian J Chan, Chao-Ting Chen, Jui-Hung Cheng, Hen-Hsen Huang

**Updated**: 2024-12-20T06:58:32Z

**Summary**: Retrieval-augmented generation (RAG) has gained traction as a powerful approach for enhancing language models by integrating external knowledge sources. However, RAG introduces challenges such as retrieval latency, potential errors in document selection, and increased system complexity. With the advent of large language models (LLMs) featuring significantly extended context windows, this paper proposes an alternative paradigm, cache-augmented generation (CAG) that bypasses real-time retrieval. Our method involves preloading all relevant resources, especially when the documents or knowledge for retrieval are of a limited and manageable size, into the LLM's extended context and caching its runtime parameters. During inference, the model utilizes these preloaded parameters to answer queries without additional retrieval steps. Comparative analyses reveal that CAG eliminates retrieval latency and minimizes retrieval errors while maintaining context relevance. Performance evaluations across multiple benchmarks highlight scenarios where long-context LLMs either outperform or complement traditional RAG pipelines. These findings suggest that, for certain applications, particularly those with a constrained knowledge base, CAG provide a streamlined and efficient alternative to RAG, achieving comparable or superior results with reduced complexity.

**Link**: [arxiv](http://arxiv.org/abs/2412.15605v1),  [pdf](http://arxiv.org/pdf/2412.15605v1)

**Tags**: cs.CL 



### DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM   Serving
**Authors**: Yuhan Liu, Yuyang Huang, Jiayi Yao, Zhuohan Gu, Kuntai Du, Hanchen Li, Yihua Cheng, Junchen Jiang, Shan Lu, Madan Musuvathi, Esha Choukse

**Updated**: 2024-12-19T23:52:16Z

**Summary**: Large Language Models (LLMs) are increasingly employed in complex workflows, where different LLMs and fine-tuned variants collaboratively address complex tasks. However, these systems face significant inefficiencies due to redundant context processing of the shared context. We propose DroidSpeak, a framework that optimizes context sharing between fine-tuned LLMs derived from the same foundational model. DroidSpeak identifies critical layers in the KV cache and selectively recomputes them, enabling effective reuse of intermediate data while maintaining high accuracy.   Our approach balances computational efficiency and task fidelity, significantly reducing inference latency and throughput bottlenecks. Experiments on diverse datasets and model pairs demonstrate that DroidSpeak achieves up to 3x higher throughputs and 2.6x faster prefill times with negligible accuracy loss compared to full recomputation.

**Link**: [arxiv](http://arxiv.org/abs/2411.02820v3),  [pdf](http://arxiv.org/pdf/2411.02820v3)

**Tags**: cs.MA cs.AI cs.CL cs.LG 



### Exposing Shadow Branches
**Authors**: Chrysanthos Pepi, Bhargav Reddy Godala, Krishnam Tibrewala, Gino Chacon, Paul V. Gratz, Daniel A. Jiménez, Gilles A. Pokam, David I. August

**Updated**: 2024-12-19T22:34:37Z

**Summary**: Modern processors implement a decoupled front-end in the form of Fetch Directed Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is driven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and branch target tracking structures to speculatively fetch instructions into the Instruction Cache (L1I). As data center applications become more complex, their code footprints also grow, resulting in an increase in Branch Target Buffer (BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB miss, the BPU may not identify the current instruction as a branch to FDIP. This can prevent FDIP from prefetching or cause it to speculate down the wrong path, further polluting the L1I cache. We observe that the vast majority, 75%, of BTB-missing, unidentified branches are actually present in instruction cache lines that FDIP has previously fetched but, these missing branches have not yet been decoded and inserted into the BTB. This is because the instruction line is decoded from an entry point (which is the target of the previous taken branch) till an exit point (the taken branch). Branch instructions present in the ignored portion of the cache line we call them "Shadow Branches". Here we present Skeia, a novel shadow branch decoding technique that identifies and decodes unused bytes in cache lines fetched by FDIP, inserting them into a Shadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB, allowing FDIP to speculate despite a BTB miss. With a minimal storage state of 12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB) and ~2% versus adding an equal amount of state to the BTB across 16 front-end bound applications. Since many branches stored in the SBB are unique compared to those in a similarly sized BTB, we consistently observe greater performance gains with Skeia across all examined sizes until saturation.

**Link**: [arxiv](http://arxiv.org/abs/2408.12592v2),  [pdf](http://arxiv.org/pdf/2408.12592v2)

**Tags**: cs.AR 



### DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context   LLMs
**Authors**: Xiabin Zhou, Wenbin Wang, Minyan Zeng, Jiaxian Guo, Xuebo Liu, Li Shen, Min Zhang, Liang Ding

**Updated**: 2024-12-19T13:28:42Z

**Summary**: Efficient KV cache management in LLMs is crucial for long-context tasks like RAG and summarization. Existing KV cache compression methods enforce a fixed pattern, neglecting task-specific characteristics and reducing the retention of essential information. However, we observe distinct activation patterns across layers in various tasks, highlighting the need for adaptive strategies tailored to each task's unique demands. Based on this insight, we propose DynamicKV, a method that dynamically optimizes token retention by adjusting the number of tokens retained at each layer to adapt to the specific task. DynamicKV establishes global and per-layer maximum KV cache budgets, temporarily retaining the maximum budget for the current layer, and periodically updating the KV cache sizes of all preceding layers during inference. Our method retains only 1.7% of the KV cache size while achieving ~85% of the Full KV cache performance on LongBench. Notably, even under extreme compression (0.9%), DynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the Needle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be released.

**Link**: [arxiv](http://arxiv.org/abs/2412.14838v1),  [pdf](http://arxiv.org/pdf/2412.14838v1)

**Tags**: cs.CL 



### Accelerating Diffusion Transformers with Token-wise Feature Caching
**Authors**: Chang Zou, Xuyang Liu, Ting Liu, Siteng Huang, Linfeng Zhang

**Updated**: 2024-12-19T12:38:23Z

**Summary**: Diffusion transformers have shown significant effectiveness in both image and video synthesis at the expense of huge computation costs. To address this problem, feature caching methods have been introduced to accelerate diffusion transformers by caching the features in previous timesteps and reusing them in the following timesteps. However, previous caching methods ignore that different tokens exhibit different sensitivities to feature caching, and feature caching on some tokens may lead to 10$\times$ more destruction to the overall generation quality compared with other tokens. In this paper, we introduce token-wise feature caching, allowing us to adaptively select the most suitable tokens for caching, and further enable us to apply different caching ratios to neural layers in different types and depths. Extensive experiments on PixArt-$\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image and video generation with no requirements for training. For instance, 2.36$\times$ and 1.93$\times$ acceleration are achieved on OpenSora and PixArt-$\alpha$ with almost no drop in generation quality.

**Link**: [arxiv](http://arxiv.org/abs/2410.05317v3),  [pdf](http://arxiv.org/pdf/2410.05317v3)

**Tags**: cs.LG cs.AI cs.CV 



### Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure   Integration in Machine Learning Systems
**Authors**: Dongfang Zhao

**Updated**: 2024-12-18T22:52:12Z

**Summary**: Machine learning (ML) systems that guarantee security and privacy often rely on Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling computations on encrypted data without exposing sensitive information. However, a critical limitation of FHE is its computational inefficiency, making it impractical for large-scale applications. In this work, we propose \textit{Nemesis}, a framework that accelerates FHE-based systems without compromising accuracy or security. The design of Nemesis is inspired by Rache (SIGMOD'23), which introduced a caching mechanism for encrypted integers and scalars. Nemesis extends this idea with more advanced caching techniques and mathematical tools, enabling efficient operations over multi-slot FHE schemes and overcoming Rache's limitations to support general plaintext structures. We formally prove the security of Nemesis under standard cryptographic assumptions and evaluate its performance extensively on widely used datasets, including MNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis significantly reduces the computational overhead of FHE-based ML systems, paving the way for broader adoption of privacy-preserving technologies.

**Link**: [arxiv](http://arxiv.org/abs/2412.14392v1),  [pdf](http://arxiv.org/pdf/2412.14392v1)

**Tags**: cs.CR cs.LG 



### ResQ: Mixed-Precision Quantization of Large Language Models with   Low-Rank Residuals
**Authors**: Utkarsh Saxena, Sayeh Sharify, Kaushik Roy, Xin Wang

**Updated**: 2024-12-18T22:01:55Z

**Summary**: Post-training quantization (PTQ) of large language models (LLMs) holds the promise in reducing the prohibitive computational cost at inference time. Quantization of all weight, activation and key-value (KV) cache tensors to 4-bit without significantly degrading generalizability is challenging, due to the high quantization error caused by extreme outliers in activations. To tackle this problem, we propose ResQ, a PTQ method that pushes further the state-of-the-art. By means of principal component analysis (PCA), it identifies a low-rank subspace (in practice 1/8 of the hidden dimension) in which activation variances are highest, and keep the coefficients within this subspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit. Within each subspace, invariant random rotation is applied to further suppress outliers. We show that this is a provably optimal mixed precision quantization scheme that minimizes error. With the Llama families of models, we demonstrate that ResQ outperforms recent uniform and mixed precision PTQ methods on a variety of benchmarks, achieving up to 33% lower perplexity on Wikitext than the next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code is available at https://github.com/utkarsh-dmx/project-resq.

**Link**: [arxiv](http://arxiv.org/abs/2412.14363v1),  [pdf](http://arxiv.org/pdf/2412.14363v1)

**Tags**: cs.LG cs.CL 



### Optimizing ML Concurrent Computation and Communication with GPU DMA   Engines
**Authors**: Anirudha Agrawal, Shaizeen Aga, Suchita Pati, Mahzabeen Islam

**Updated**: 2024-12-18T21:09:08Z

**Summary**: Concurrent computation and communication (C3) is a pervasive paradigm in ML and other domains, making its performance optimization crucial. In this paper, we carefully characterize C3 in ML on GPUs, which are most widely deployed for ML training and inference. We observe that while C3 leads to performance uplifts, the uplifts are far lower than ideal speedups (serial computation and communication versus maximum of computation or communication; all times from isolated executions). C3 on average achieves only 21% of ideal speedup, this is due to known challenges of compute and memory interference between concurrent GPU kernels (that is, sharing of GPU's compute units, caches and HBM).   To attain better performance for C3, first, we evaluate dual strategies of schedule prioritization and careful resource partitioning of compute units on GPUs to push performance attained with C3 (on average 42% of ideal speedup). We also provide heuristics that can guide a runtime while employing these strategies. To further enhance C3 performance, we propose to mitigate C3 interference by offloading communication tasks to the GPU's DMA engines. To this end, we build Concurrent Communication CoLlectives (ConCCL) proof-of-concepts that harness DMA engines for communication. We show how ConCCL considerably closes the gap between realized and ideal speedup for C3 (on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall, our work makes a strong case for GPU DMA engine advancements to better support C3 on GPUs.

**Link**: [arxiv](http://arxiv.org/abs/2412.14335v1),  [pdf](http://arxiv.org/pdf/2412.14335v1)

**Tags**: cs.AR cs.DC 



### MagicPIG: LSH Sampling for Efficient LLM Generation
**Authors**: Zhuoming Chen, Ranajoy Sadhukhan, Zihao Ye, Yang Zhou, Jianyu Zhang, Niklas Nolte, Yuandong Tian, Matthijs Douze, Leon Bottou, Zhihao Jia, Beidi Chen

**Updated**: 2024-12-18T17:36:36Z

**Summary**: Large language models (LLMs) with long context windows have gained significant attention. However, the KV cache, stored to avoid re-computation, becomes a bottleneck. Various dynamic sparse or TopK-based attention approximation methods have been proposed to leverage the common insight that attention is sparse. In this paper, we first show that TopK attention itself suffers from quality degradation in certain downstream tasks because attention is not always as sparse as expected. Rather than selecting the keys and values with the highest attention scores, sampling with theoretical guarantees can provide a better estimation for attention output. To make the sampling-based approximation practical in LLM generation, we propose MagicPIG, a heterogeneous system based on Locality Sensitive Hashing (LSH). MagicPIG significantly reduces the workload of attention computation while preserving high accuracy for diverse tasks. MagicPIG stores the LSH hash tables and runs the attention computation on the CPU, which allows it to serve longer contexts and larger batch sizes with high approximation accuracy. MagicPIG can improve decoding throughput by up to $5\times$ across various GPU hardware and achieve 54ms decoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a context of 96k tokens. The code is available at https://github.com/Infini-AI-Lab/MagicPIG.

**Link**: [arxiv](http://arxiv.org/abs/2410.16179v4),  [pdf](http://arxiv.org/pdf/2410.16179v4)

**Tags**: cs.CL cs.LG 



### Rehearsal-Free Continual Federated Learning with Synergistic   Regularization
**Authors**: Yichen Li, Yuying Wang, Tianzhe Xiao, Haozhao Wang, Yining Qi, Ruixuan Li

**Updated**: 2024-12-18T12:16:41Z

**Summary**: Continual Federated Learning (CFL) allows distributed devices to collaboratively learn novel concepts from continuously shifting training data while avoiding knowledge forgetting of previously seen tasks. To tackle this challenge, most current CFL approaches rely on extensive rehearsal of previous data. Despite effectiveness, rehearsal comes at a cost to memory, and it may also violate data privacy. Considering these, we seek to apply regularization techniques to CFL by considering their cost-efficient properties that do not require sample caching or rehearsal. Specifically, we first apply traditional regularization techniques to CFL and observe that existing regularization techniques, especially synaptic intelligence, can achieve promising results under homogeneous data distribution but fail when the data is heterogeneous. Based on this observation, we propose a simple yet effective regularization algorithm for CFL named FedSSI, which tailors the synaptic intelligence for the CFL with heterogeneous data settings. FedSSI can not only reduce computational overhead without rehearsal but also address the data heterogeneity issue. Extensive experiments show that FedSSI achieves superior performance compared to state-of-the-art methods.

**Link**: [arxiv](http://arxiv.org/abs/2412.13779v1),  [pdf](http://arxiv.org/pdf/2412.13779v1)

**Tags**: cs.LG cs.DC 



### Semantic Convergence: Harmonizing Recommender Systems via Two-Stage   Alignment and Behavioral Semantic Tokenization
**Authors**: Guanghan Li, Xun Zhang, Yufei Zhang, Yifan Yin, Guojun Yin, Wei Lin

**Updated**: 2024-12-18T12:07:58Z

**Summary**: Large language models (LLMs), endowed with exceptional reasoning capabilities, are adept at discerning profound user interests from historical behaviors, thereby presenting a promising avenue for the advancement of recommendation systems. However, a notable discrepancy persists between the sparse collaborative semantics typically found in recommendation systems and the dense token representations within LLMs. In our study, we propose a novel framework that harmoniously merges traditional recommendation models with the prowess of LLMs. We initiate this integration by transforming ItemIDs into sequences that align semantically with the LLMs space, through the proposed Alignment Tokenization module. Additionally, we design a series of specialized supervised learning tasks aimed at aligning collaborative signals with the subtleties of natural language semantics. To ensure practical applicability, we optimize online inference by pre-caching the top-K results for each user, reducing latency and improving effciency. Extensive experimental evidence indicates that our model markedly improves recall metrics and displays remarkable scalability of recommendation systems.

**Link**: [arxiv](http://arxiv.org/abs/2412.13771v1),  [pdf](http://arxiv.org/pdf/2412.13771v1)

**Tags**: cs.IR cs.AI cs.CL 



### DyCoke: Dynamic Compression of Tokens for Fast Video Large Language   Models
**Authors**: Keda Tao, Can Qin, Haoxuan You, Yang Sui, Huan Wang

**Updated**: 2024-12-18T09:47:25Z

**Summary**: Video large language models (VLLMs) have significantly advanced recently in processing complex video content, yet their inference efficiency remains constrained because of the high computational cost stemming from the thousands of visual tokens generated from the video inputs. We empirically observe that, unlike single image inputs, VLLMs typically attend visual tokens from different frames at different decoding iterations, making a one-shot pruning strategy prone to removing important tokens by mistake. Motivated by this, we present DyCoke, a training-free token compression method to optimize token representation and accelerate VLLMs. DyCoke incorporates a plug-and-play temporal compression module to minimize temporal redundancy by merging redundant tokens across frames, and applies dynamic KV cache reduction to prune spatially redundant tokens selectively. It ensures high-quality inference by dynamically retaining the critical tokens at each decoding step. Extensive experimental results demonstrate that DyCoke can outperform the prior SoTA counterparts, achieving 1.5X inference speedup, 1.4X memory reduction against the baseline VLLM, while still improving the performance, with no training.

**Link**: [arxiv](http://arxiv.org/abs/2411.15024v2),  [pdf](http://arxiv.org/pdf/2411.15024v2)

**Tags**: cs.CV cs.LG 



### SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation
**Authors**: Jialong Wu, Zhenglin Wang, Linhai Zhang, Yilong Lai, Yulan He, Deyu Zhou

**Updated**: 2024-12-18T09:27:33Z

**Summary**: Key-Value (KV) cache has become a bottleneck of LLMs for long-context generation. Despite the numerous efforts in this area, the optimization for the decoding phase is generally ignored. However, we believe such optimization is crucial, especially for long-output generation tasks based on the following two observations: (i) Excessive compression during the prefill phase, which requires specific full context impairs the comprehension of the reasoning task; (ii) Deviation of heavy hitters occurs in the reasoning tasks with long outputs. Therefore, SCOPE, a simple yet efficient framework that separately performs KV cache optimization during the prefill and decoding phases, is introduced. Specifically, the KV cache during the prefill phase is preserved to maintain the essential information, while a novel strategy based on sliding is proposed to select essential heavy hitters for the decoding phase. Memory usage and memory transfer are further optimized using adaptive and discontinuous strategies. Extensive experiments on LongGenBench show the effectiveness and generalization of SCOPE and its compatibility as a plug-in to other prefill-only KV compression methods.

**Link**: [arxiv](http://arxiv.org/abs/2412.13649v1),  [pdf](http://arxiv.org/pdf/2412.13649v1)

**Tags**: cs.CL 



### ZipVL: Efficient Large Vision-Language Models with Dynamic Token   Sparsification
**Authors**: Yefei He, Feng Chen, Jing Liu, Wenqi Shao, Hong Zhou, Kaipeng Zhang, Bohan Zhuang

**Updated**: 2024-12-18T07:45:11Z

**Summary**: The efficiency of large vision-language models (LVLMs) is constrained by the computational bottleneck of the attention mechanism during the prefill phase and the memory bottleneck of fetching the key-value (KV) cache in the decoding phase, particularly in scenarios involving high-resolution images or videos. Visual content often exhibits substantial redundancy, resulting in highly sparse attention maps within LVLMs. This sparsity can be leveraged to accelerate attention computation or compress the KV cache through various approaches. However, most studies focus on addressing only one of these bottlenecks and do not adequately support dynamic adjustment of sparsity concerning distinct layers or tasks. In this paper, we present ZipVL, an efficient inference framework designed for LVLMs through a dynamic ratio allocation strategy of important tokens. This ratio is adaptively determined based on the layer-specific distribution of attention scores, rather than fixed hyper-parameters, thereby improving efficiency for less complex tasks while maintaining high performance for more challenging ones. Then we select important tokens based on their normalized attention scores and perform sparse attention mechanism solely on those important tokens, reducing the latency in the prefill phase. Tokens deemed less important will be discarded to reduce KV cache size, alleviating the memory bottleneck in the decoding phase. Our experiments demonstrate that ZipVL can accelerate the prefill phase by 2.3$\times$ and improve decoding throughput by 2.8$\times$, with a minimal accuracy reduction of only 0.5\% on VQAv2 benchmark over LLaVA-Next-13B model, effectively enhancing the generation efficiency of LVLMs.

**Link**: [arxiv](http://arxiv.org/abs/2410.08584v2),  [pdf](http://arxiv.org/pdf/2410.08584v2)

**Tags**: cs.CV cs.AI 



### Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data   Presentation
**Authors**: Yunqi Guo, Kaiyuan Hou, Heming Fu, Hongkai Chen, Zhenyu Yan, Guoliang Xing, Xiaofan Jiang

**Updated**: 2024-12-18T05:16:11Z

**Summary**: Understanding sensor data can be challenging for non-experts because of the complexity and unique semantic meanings of sensor modalities. This calls for intuitive and effective methods to present sensor information. However, creating intuitive sensor data visualizations presents three key challenges: the variability of sensor readings, gaps in domain comprehension, and the dynamic nature of sensor data. To address these issues, we develop Vivar, a novel AR system that integrates multi-modal sensor data and presents 3D volumetric content for visualization. In particular, we introduce a cross-modal embedding approach that maps sensor data into a pre-trained visual embedding space through barycentric interpolation. This allows for accurate and continuous integration of multi-modal sensor information. Vivar also incorporates sensor-aware AR scene generation using foundation models and 3D Gaussian Splatting (3DGS) without requiring domain expertise. In addition, Vivar leverages latent reuse and caching strategies to accelerate 2D and AR content generation. Our extensive experiments demonstrate that our system achieves 11$\times$ latency reduction without compromising quality. A user study involving over 485 participants, including domain experts, demonstrates Vivar's effectiveness in accuracy, consistency, and real-world applicability, paving the way for more intuitive sensor data visualization.

**Link**: [arxiv](http://arxiv.org/abs/2412.13509v1),  [pdf](http://arxiv.org/pdf/2412.13509v1)

**Tags**: cs.HC 



## Keyword: LLM Inference 
 ### Predicting 4D Hand Trajectory from Monocular Videos
**Authors**: Yufei Ye, Yao Feng, Omid Taheri, Haiwen Feng, Shubham Tulsiani, Michael J. Black

**Updated**: 2025-01-14T18:59:05Z

**Summary**: We present HaPTIC, an approach that infers coherent 4D hand trajectories from monocular videos. Current video-based hand pose reconstruction methods primarily focus on improving frame-wise 3D pose using adjacent frames rather than studying consistent 4D hand trajectories in space. Despite the additional temporal cues, they generally underperform compared to image-based methods due to the scarcity of annotated video data. To address these issues, we repurpose a state-of-the-art image-based transformer to take in multiple frames and directly predict a coherent trajectory. We introduce two types of lightweight attention layers: cross-view self-attention to fuse temporal information, and global cross-attention to bring in larger spatial context. Our method infers 4D hand trajectories similar to the ground truth while maintaining strong 2D reprojection alignment. We apply the method to both egocentric and allocentric videos. It significantly outperforms existing methods in global trajectory accuracy while being comparable to the state-of-the-art in single-image pose estimation. Project website: https://judyye.github.io/haptic-www

**Link**: [arxiv](http://arxiv.org/abs/2501.08329v1),  [pdf](http://arxiv.org/pdf/2501.08329v1)

**Tags**: cs.CV 



### PokerBench: Training Large Language Models to become Professional Poker   Players
**Authors**: Richard Zhuang, Akshat Gupta, Richard Yang, Aniket Rahane, Zhengyu Li, Gopala Anumanchipalli

**Updated**: 2025-01-14T18:59:03Z

**Summary**: We introduce PokerBench - a benchmark for evaluating the poker-playing abilities of large language models (LLMs). As LLMs excel in traditional NLP tasks, their application to complex, strategic games like poker poses a new challenge. Poker, an incomplete information game, demands a multitude of skills such as mathematics, reasoning, planning, strategy, and a deep understanding of game theory and human psychology. This makes Poker the ideal next frontier for large language models. PokerBench consists of a comprehensive compilation of 11,000 most important scenarios, split between pre-flop and post-flop play, developed in collaboration with trained poker players. We evaluate prominent models including GPT-4, ChatGPT 3.5, and various Llama and Gemma series models, finding that all state-of-the-art LLMs underperform in playing optimal poker. However, after fine-tuning, these models show marked improvements. We validate PokerBench by having models with different scores compete with each other, demonstrating that higher scores on PokerBench lead to higher win rates in actual poker games. Through gameplay between our fine-tuned model and GPT-4, we also identify limitations of simple supervised fine-tuning for learning optimal playing strategy, suggesting the need for more advanced methodologies for effectively training language models to excel in games. PokerBench thus presents a unique benchmark for a quick and reliable evaluation of the poker-playing ability of LLMs as well as a comprehensive benchmark to study the progress of LLMs in complex game-playing scenarios. The dataset and code will be made available at: \url{https://github.com/pokerllm/pokerbench}.

**Link**: [arxiv](http://arxiv.org/abs/2501.08328v1),  [pdf](http://arxiv.org/pdf/2501.08328v1)

**Tags**: cs.CL cs.AI cs.GT 



### ADAM-1: AI and Bioinformatics for Alzheimer's Detection and   Microbiome-Clinical Data Integrations
**Authors**: Ziyuan Huang, Vishaldeep Kaur Sekhon, Ouyang Guo, Mark Newman, Roozbeh Sadeghian, Maria L. Vaida, Cynthia Jo, Doyle Ward, Vanni Bucci, John P. Haran

**Updated**: 2025-01-14T18:56:33Z

**Summary**: The Alzheimer's Disease Analysis Model Generation 1 (ADAM) is a multi-agent large language model (LLM) framework designed to integrate and analyze multi-modal data, including microbiome profiles, clinical datasets, and external knowledge bases, to enhance the understanding and detection of Alzheimer's disease (AD). By leveraging retrieval-augmented generation (RAG) techniques along with its multi-agent architecture, ADAM-1 synthesizes insights from diverse data sources and contextualizes findings using literature-driven evidence. Comparative evaluation against XGBoost revealed similar mean F1 scores but significantly reduced variance for ADAM-1, highlighting its robustness and consistency, particularly in small laboratory datasets. While currently tailored for binary classification tasks, future iterations aim to incorporate additional data modalities, such as neuroimaging and biomarkers, to broaden the scalability and applicability for Alzheimer's research and diagnostics.

**Link**: [arxiv](http://arxiv.org/abs/2501.08324v1),  [pdf](http://arxiv.org/pdf/2501.08324v1)

**Tags**: cs.AI 68T07 



### Exploring Robustness of Multilingual LLMs on Real-World Noisy Data
**Authors**: Amirhossein Aliakbarzadeh, Lucie Flek, Akbar Karimi

**Updated**: 2025-01-14T18:55:35Z

**Summary**: Large Language Models (LLMs) are trained on Web data that might contain spelling errors made by humans. But do they become robust to similar real-world noise? In this paper, we investigate the effect of real-world spelling mistakes on the performance of 9 language models, with parameters ranging from 0.2B to 13B, in 3 different NLP tasks, namely Natural Language Inference (NLI), Name Entity Recognition (NER), and Intent Classification (IC). We perform our experiments on 6 different languages and build a dictionary of real-world noise for them using the Wikipedia edit history. We show that the performance gap of the studied models on the clean and noisy test data averaged across all the datasets and languages ranges from 2.3 to 4.3 absolute percentage points. In addition, mT5 models, in general, show more robustness compared to BLOOM, Falcon, and BERT-like models. In particular, mT5 (13B), was the most robust on average overall, across the 3 tasks, and in 4 of the 6 languages.

**Link**: [arxiv](http://arxiv.org/abs/2501.08322v1),  [pdf](http://arxiv.org/pdf/2501.08322v1)

**Tags**: cs.CL 



### FCC-ee Sensitivity to Flavor-Agnostic Standard Model Effective Field   Theory Operators
**Authors**: Ben Allanach, Eetu Loisa

**Updated**: 2025-01-14T18:55:08Z

**Summary**: We present a calculation of the sensitivity of the Future Circular $e^+ e^-$ Collider (FCC-ee) to the interactions of new, heavy particles in terms of publicly available extensions to the smelli and flavio computer programs. We parameterize new particles' effects with dimension-6 operators of the Standard Model Effective Field Theory (SMEFT) without any flavor assumptions and take into account the projected experimental and theoretical uncertainties of various electroweak and Higgs observables at the proposed collider. We illustrate a use of the calculation by estimating the sensitivity of the FCC-ee to a family non-universal $Z^\prime$ model that explains anomalies inferred from present-day measurements and Standard Model predictions of observables that involve the $ b \rightarrow s \ell^+ \ell^-$ transition.

**Link**: [arxiv](http://arxiv.org/abs/2501.08321v1),  [pdf](http://arxiv.org/pdf/2501.08321v1)

**Tags**: hep-ph 



### COMBO and COMMA: R packages for regression modeling and inference in the   presence of misclassified binary mediator or outcome variables
**Authors**: Kimberly A. Hochstedler Webb, Martin T. Wells

**Updated**: 2025-01-14T18:53:22Z

**Summary**: Misclassified binary outcome or mediator variables can cause unpredictable bias in resulting parameter estimates. As more datasets that were not originally collected for research purposes are being used for studies in the social and health sciences, the need for methods that address data quality concerns is growing. In this paper, we describe two R packages, COMBO and COMMA, that implement bias-correction methods for misclassified binary outcome and mediator variables, respectively. These likelihood-based approaches do not require gold standard measures and allow for estimation of sensitivity and specificity rates for the misclassified variable(s). In addition, these R packages automatically apply crucial label switching corrections, allowing researchers to circumvent the inherent permutation invariance of the misclassification model likelihood. We demonstrate COMBO for single-outcome cases using a study of bar exam passage. We develop and evaluate a risk prediction model based on noisy indicators in a pretrial risk assessment study to demonstrate COMBO for multi-outcome cases. In addition, we use COMMA to evaluate the mediating effect of potentially misdiagnosed gestational hypertension on the maternal ethnicity-birthweight relationship.

**Link**: [arxiv](http://arxiv.org/abs/2501.08320v1),  [pdf](http://arxiv.org/pdf/2501.08320v1)

**Tags**: stat.CO stat.OT 



### Enhancing Automated Interpretability with Output-Centric Feature   Descriptions
**Authors**: Yoav Gur-Arieh, Roy Mayan, Chen Agassy, Atticus Geiger, Mor Geva

**Updated**: 2025-01-14T18:53:00Z

**Summary**: Automated interpretability pipelines generate natural language descriptions for the concepts represented by features in large language models (LLMs), such as plants or the first word in a sentence. These descriptions are derived using inputs that activate the feature, which may be a dimension or a direction in the model's representation space. However, identifying activating inputs is costly, and the mechanistic role of a feature in model behavior is determined both by how inputs cause a feature to activate and by how feature activation affects outputs. Using steering evaluations, we reveal that current pipelines provide descriptions that fail to capture the causal effect of the feature on outputs. To fix this, we propose efficient, output-centric methods for automatically generating feature descriptions. These methods use the tokens weighted higher after feature stimulation or the highest weight tokens after applying the vocabulary "unembedding" head directly to the feature. Our output-centric descriptions better capture the causal effect of a feature on model outputs than input-centric descriptions, but combining the two leads to the best performance on both input and output evaluations. Lastly, we show that output-centric descriptions can be used to find inputs that activate features previously thought to be "dead".

**Link**: [arxiv](http://arxiv.org/abs/2501.08319v1),  [pdf](http://arxiv.org/pdf/2501.08319v1)

**Tags**: cs.CL 



### Rate-In: Information-Driven Adaptive Dropout Rates for Improved   Inference-Time Uncertainty Estimation
**Authors**: Tal Zeevi, Ravid Shwartz-Ziv, Yann LeCun, Lawrence H. Staib, John A. Onofrey

**Updated**: 2025-01-14T18:51:43Z

**Summary**: Accurate uncertainty estimation is crucial for deploying neural networks in risk-sensitive applications such as medical diagnosis. Monte Carlo Dropout is a widely used technique for approximating predictive uncertainty by performing stochastic forward passes with dropout during inference. However, using static dropout rates across all layers and inputs can lead to suboptimal uncertainty estimates, as it fails to adapt to the varying characteristics of individual inputs and network layers. Existing approaches optimize dropout rates during training using labeled data, resulting in fixed inference-time parameters that cannot adjust to new data distributions, compromising uncertainty estimates in Monte Carlo simulations.   In this paper, we propose Rate-In, an algorithm that dynamically adjusts dropout rates during inference by quantifying the information loss induced by dropout in each layer's feature maps. By treating dropout as controlled noise injection and leveraging information-theoretic principles, Rate-In adapts dropout rates per layer and per input instance without requiring ground truth labels. By quantifying the functional information loss in feature maps, we adaptively tune dropout rates to maintain perceptual quality across diverse medical imaging tasks and architectural configurations. Our extensive empirical study on synthetic data and real-world medical imaging tasks demonstrates that Rate-In improves calibration and sharpens uncertainty estimates compared to fixed or heuristic dropout rates without compromising predictive performance. Rate-In offers a practical, unsupervised, inference-time approach to optimizing dropout for more reliable predictive uncertainty estimation in critical applications.

**Link**: [arxiv](http://arxiv.org/abs/2412.07169v3),  [pdf](http://arxiv.org/pdf/2412.07169v3)

**Tags**: cs.LG cs.CV stat.ML 



### MiniMax-01: Scaling Foundation Models with Lightning Attention
**Authors**: MiniMax, Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, Enwei Jiao, Gengxin Li, Guojun Zhang, Haohai Sun, Houze Dong, Jiadai Zhu, Jiaqi Zhuang, Jiayuan Song, Jin Zhu, Jingtao Han, Jingyang Li, Junbin Xie, Junhao Xu, Junjie Yan, Kaishun Zhang, Kecheng Xiao, Kexi Kang, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Zheng, Linbo Chai, Long Xing, Meizhi Ju, Mingyuan Chi, Mozhi Zhang, Peikai Huang, Pengcheng Niu, Pengfei Li, Pengyu Zhao, Qi Yang, Qidi Xu, Qiexiang Wang, Qin Wang, Qiuhui Li, Ruitao Leng, Shengmin Shi, Shuqi Yu, Sichen Li, Songquan Zhu, Tao Huang, Tianrun Liang, Weigao Sun, Weixuan Sun, Weiyu Cheng, Wenkai Li, Xiangjun Song, Xiao Su, Xiaodong Han, Xinjie Zhang, Xinzhu Hou, Xu Min, Xun Zou, Xuyang Shen, Yan Gong, Yingjie Zhu, Yipeng Zhou, Yiran Zhong, Yongyi Hu, Yuanxiang Fan, Yue Yu, Yufeng Yang, Yuhao Li, Yunan Huang, Yunji Li, Yunpeng Huang, Yunzhi Xu, Yuxin Mao, Zehan Li, Zekang Li, Zewei Tao, Zewen Ying, Zhaoyang Cong, Zhen Qin, Zhenhua Fan, Zhihang Yu, Zhuo Jiang, Zijia Wu

**Updated**: 2025-01-14T18:50:05Z

**Summary**: We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01, which are comparable to top-tier models while offering superior capabilities in processing longer contexts. The core lies in lightning attention and its efficient scaling. To maximize computational capacity, we integrate it with Mixture of Experts (MoE), creating a model with 32 experts and 456 billion total parameters, of which 45.9 billion are activated for each token. We develop an optimized parallel strategy and highly efficient computation-communication overlap techniques for MoE and lightning attention. This approach enables us to conduct efficient training and inference on models with hundreds of billions of parameters across contexts spanning millions of tokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens during training and extrapolate to 4 million tokens during inference at an affordable cost. Our vision-language model, MiniMax-VL-01 is built through continued training with 512 billion vision-language tokens. Experiments on both standard and in-house benchmarks show that our models match the performance of state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32 times longer context window. We publicly release MiniMax-01 at https://github.com/MiniMax-AI.

**Link**: [arxiv](http://arxiv.org/abs/2501.08313v1),  [pdf](http://arxiv.org/pdf/2501.08313v1)

**Tags**: cs.CL cs.CV 



### HALoGEN: Fantastic LLM Hallucinations and Where to Find Them
**Authors**: Abhilasha Ravichander, Shrusti Ghela, David Wadden, Yejin Choi

**Updated**: 2025-01-14T18:13:08Z

**Summary**: Despite their impressive ability to generate high-quality and fluent text, generative large language models (LLMs) also produce hallucinations: statements that are misaligned with established world knowledge or provided input context. However, measuring hallucination can be challenging, as having humans verify model generations on-the-fly is both expensive and time-consuming. In this work, we release HALoGEN, a comprehensive hallucination benchmark consisting of: (1) 10,923 prompts for generative models spanning nine domains including programming, scientific attribution, and summarization, and (2) automatic high-precision verifiers for each use case that decompose LLM generations into atomic units, and verify each unit against a high-quality knowledge source. We use this framework to evaluate ~150,000 generations from 14 language models, finding that even the best-performing models are riddled with hallucinations (sometimes up to 86% of generated atomic facts depending on the domain). We further define a novel error classification for LLM hallucinations based on whether they likely stem from incorrect recollection of training data (Type A errors), or incorrect knowledge in training data (Type B errors), or are fabrication (Type C errors). We hope our framework provides a foundation to enable the principled study of why generative models hallucinate, and advances the development of trustworthy large language models.

**Link**: [arxiv](http://arxiv.org/abs/2501.08292v1),  [pdf](http://arxiv.org/pdf/2501.08292v1)

**Tags**: cs.CL cs.AI 



### AfriHate: A Multilingual Collection of Hate Speech and Abusive Language   Datasets for African Languages
**Authors**: Shamsuddeen Hassan Muhammad, Idris Abdulmumin, Abinew Ali Ayele, David Ifeoluwa Adelani, Ibrahim Said Ahmad, Saminu Mohammad Aliyu, Nelson Odhiambo Onyango, Lilian D. A. Wanzare, Samuel Rutunda, Lukman Jibril Aliyu, Esubalew Alemneh, Oumaima Hourrane, Hagos Tesfahun Gebremichael, Elyas Abdi Ismail, Meriem Beloucif, Ebrahim Chekol Jibril, Andiswa Bukula, Rooweither Mabuya, Salomey Osei, Abigail Oppong, Tadesse Destaw Belay, Tadesse Kebede Guge, Tesfa Tegegne Asfaw, Chiamaka Ijeoma Chukwuneke, Paul Röttger, Seid Muhie Yimam, Nedjma Ousidhoum

**Updated**: 2025-01-14T18:00:07Z

**Summary**: Hate speech and abusive language are global phenomena that need socio-cultural background knowledge to be understood, identified, and moderated. However, in many regions of the Global South, there have been several documented occurrences of (1) absence of moderation and (2) censorship due to the reliance on keyword spotting out of context. Further, high-profile individuals have frequently been at the center of the moderation process, while large and targeted hate speech campaigns against minorities have been overlooked. These limitations are mainly due to the lack of high-quality data in the local languages and the failure to include local communities in the collection, annotation, and moderation processes. To address this issue, we present AfriHate: a multilingual collection of hate speech and abusive language datasets in 15 African languages. Each instance in AfriHate is annotated by native speakers familiar with the local culture. We report the challenges related to the construction of the datasets and present various classification baseline results with and without using LLMs. The datasets, individual annotations, and hate speech and offensive language lexicons are available on https://github.com/AfriHate/AfriHate

**Link**: [arxiv](http://arxiv.org/abs/2501.08284v1),  [pdf](http://arxiv.org/pdf/2501.08284v1)

**Tags**: cs.CL 



### Exploring Robustness of LLMs to Sociodemographically-Conditioned   Paraphrasing
**Authors**: Pulkit Arora, Akbar Karimi, Lucie Flek

**Updated**: 2025-01-14T17:50:06Z

**Summary**: Large Language Models (LLMs) have shown impressive performance in various NLP tasks. However, there are concerns about their reliability in different domains of linguistic variations. Many works have proposed robustness evaluation measures for local adversarial attacks, but we need globally robust models unbiased to different language styles. We take a broader approach to explore a wider range of variations across sociodemographic dimensions to perform structured reliability tests on the reasoning capacity of language models. We extend the SocialIQA dataset to create diverse paraphrased sets conditioned on sociodemographic styles. The assessment aims to provide a deeper understanding of LLMs in (a) their capability of generating demographic paraphrases with engineered prompts and (b) their reasoning capabilities in real-world, complex language scenarios. We also explore measures such as perplexity, explainability, and ATOMIC performance of paraphrases for fine-grained reliability analysis of LLMs on these sets. We find that demographic-specific paraphrasing significantly impacts the performance of language models, indicating that the subtleties of language variations remain a significant challenge. The code and dataset will be made available for reproducibility and future research.

**Link**: [arxiv](http://arxiv.org/abs/2501.08276v1),  [pdf](http://arxiv.org/pdf/2501.08276v1)

**Tags**: cs.CL 



### Constructing optimal dynamic monitoring and treatment regimes: An   application to hypertension care
**Authors**: Janie Coulombe, Dany El-Riachi, Fanxing Du, Tianze Jiao

**Updated**: 2025-01-14T17:46:19Z

**Summary**: Hypertension is a leading cause of cardiovascular diseases and morbidity, with antihypertensive drugs and blood pressure management strategies having heterogeneous effects on patients. Previous authors exploited this heterogeneity to construct optimal dynamic treatment regimes for hypertension that input patient characteristics and output the best drug or blood pressure management strategy to prescribe. There is, however, a lack of research on optimizing monitoring schedules for these patients. It is unclear whether different monitoring patterns and drug add-on strategies could lower blood pressure differently across patients. We propose a new consistent methodology to develop optimal dynamic monitoring and add-on regimes that is doubly-robust and relies on the theory of Robins' g-methods and dynamic weighted ordinary least squares. We discuss the treatment of longitudinal missing data for that inference. The approach is evaluated in large simulation studies and applied to data from the SPRINT trial in the United States to derive a new optimal rule. This type of rule could be used by patients or physicians to personalize the timing of visit and by physicians to decide whether prescribing an antihypertensive drug is beneficial.

**Link**: [arxiv](http://arxiv.org/abs/2501.08274v1),  [pdf](http://arxiv.org/pdf/2501.08274v1)

**Tags**: stat.ME 



### RMem: Restricted Memory Banks Improve Video Object Segmentation
**Authors**: Junbao Zhou, Ziqi Pang, Yu-Xiong Wang

**Updated**: 2025-01-14T17:46:01Z

**Summary**: With recent video object segmentation (VOS) benchmarks evolving to challenging scenarios, we revisit a simple but overlooked strategy: restricting the size of memory banks. This diverges from the prevalent practice of expanding memory banks to accommodate extensive historical information. Our specially designed "memory deciphering" study offers a pivotal insight underpinning such a strategy: expanding memory banks, while seemingly beneficial, actually increases the difficulty for VOS modules to decode relevant features due to the confusion from redundant information. By restricting memory banks to a limited number of essential frames, we achieve a notable improvement in VOS accuracy. This process balances the importance and freshness of frames to maintain an informative memory bank within a bounded capacity. Additionally, restricted memory banks reduce the training-inference discrepancy in memory lengths compared with continuous expansion. This fosters new opportunities in temporal reasoning and enables us to introduce the previously overlooked "temporal positional embedding." Finally, our insights are embodied in "RMem" ("R" for restricted), a simple yet effective VOS modification that excels at challenging VOS scenarios and establishes new state of the art for object state changes (on the VOST dataset) and long videos (on the Long Videos dataset). Our code and demo are available at https://restricted-memory.github.io/.

**Link**: [arxiv](http://arxiv.org/abs/2406.08476v2),  [pdf](http://arxiv.org/pdf/2406.08476v2)

**Tags**: cs.CV cs.AI 



### Individual causal effect estimation accounting for latent disease state   modification among bipolar participants in mobile health studies
**Authors**: Charlotte R. Fowler, Xiaoxuan Cai, Habiballah Rahimi-Eichi, Lisa Dixon, Dost Ongur, Justin T. Baker, Jukka-Pekka Onnela, Linda Valeri

**Updated**: 2025-01-14T17:32:33Z

**Summary**: Individuals with bipolar disorder tend to cycle through disease states such as depression and mania. The heterogeneous nature of disease across states complicates the evaluation of interventions for bipolar disorder patients, as varied interventional success is observed within and across individuals. In fact, we hypothesize that disease state acts as an effect modifier for the causal effect of a given intervention on health outcomes. To address this dilemma, we propose an N-of-1 approach using an adapted autoregressive hidden Markov model, applied to longitudinal mobile health data collected from individuals with bipolar disorder. This method allows us to identify a latent variable from mobile health data to be treated as an effect modifier between the exposure and outcome of interest while allowing for missing data in the outcome. A counterfactual approach is employed for causal inference and to obtain a g-formula estimator to recover said effect. The performance of the proposed method is compared with a naive approach across extensive simulations and application to a multi-year smartphone study of bipolar patients, evaluating the individual effect of digital social activity on sleep duration across different latent disease states.

**Link**: [arxiv](http://arxiv.org/abs/2501.08270v1),  [pdf](http://arxiv.org/pdf/2501.08270v1)

**Tags**: stat.AP 



### Addressing the sustainable AI trilemma: a case study on LLM agents and   RAG
**Authors**: Hui Wu, Xiaoyang Wang, Zhong Fan

**Updated**: 2025-01-14T17:21:16Z

**Summary**: Large language models (LLMs) have demonstrated significant capabilities, but their widespread deployment and more advanced applications raise critical sustainability challenges, particularly in inference energy consumption. We propose the concept of the Sustainable AI Trilemma, highlighting the tensions between AI capability, digital equity, and environmental sustainability. Through a systematic case study of LLM agents and retrieval-augmented generation (RAG), we analyze the energy costs embedded in memory module designs and introduce novel metrics to quantify the trade-offs between energy consumption and system performance. Our experimental results reveal significant energy inefficiencies in current memory-augmented frameworks and demonstrate that resource-constrained environments face disproportionate efficiency penalties. Our findings challenge the prevailing LLM-centric paradigm in agent design and provide practical insights for developing more sustainable AI systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.08262v1),  [pdf](http://arxiv.org/pdf/2501.08262v1)

**Tags**: cs.CY 



### CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt   Optimization for Text Generation
**Authors**: Han He, Qianchu Liu, Lei Xu, Chaitanya Shivade, Yi Zhang, Sundararajan Srinivasan, Katrin Kirchhoff

**Updated**: 2025-01-14T17:20:04Z

**Summary**: Existing automatic prompt engineering methods are typically designed for discriminative tasks, where new task prompts are iteratively refined with limited feedback from a single metric reflecting a single aspect. However, these approaches are suboptimal for generative tasks, which require more nuanced guidance beyond a single numeric metric to improve the prompt and optimize multiple aspects of the generated text. To address these challenges, we propose a novel multi-aspect Critique-Suggestion-guided automatic Prompt Optimization (CriSPO) approach. CriSPO introduces a critique-suggestion module as its core component. This module spontaneously discovers aspects, and compares generated and reference texts across these aspects, providing specific suggestions for prompt modification. These clear critiques and actionable suggestions guide a receptive optimizer module to make more substantial changes, exploring a broader and more effective search space. To further improve CriSPO with multi-metric optimization, we introduce an Automatic Suffix Tuning (AST) extension to enhance the performance of task prompts across multiple metrics. We evaluate CriSPO on 4 state-of-the-art LLMs across 4 summarization and 5 QA datasets. Extensive experiments show 3-4% ROUGE score improvement on summarization and substantial improvement of various metrics on QA. Code available at https://github.com/amazon-science/crispo

**Link**: [arxiv](http://arxiv.org/abs/2410.02748v3),  [pdf](http://arxiv.org/pdf/2410.02748v3)

**Tags**: cs.CL cs.AI cs.LG 



### GPU-accelerated LISA parameter estimation with full time domain response
**Authors**: Cecilio García-Quirós, Shubhanshu Tiwari

**Updated**: 2025-01-14T17:15:49Z

**Summary**: We conduct the first full Bayesian inference analysis for LISA parameter estimation incorporating the effects of subdominant harmonics and spin-precession through a full time domain response. The substantial computational demands of using time domain waveforms for LISA are significantly mitigated by implementing a novel Python version of the IMRPhenomT family of waveform models and the LISA response with GPU acceleration. This time domain response alleviates the theoretical necessity of developing specific transfer functions to approximate the LISA response in the Fourier domain for each specific type of system and allows for the use of unequal arms configurations and realistic LISA orbits. Our analysis includes a series of zero-noise injections for a Massive Black Hole Binary with aligned and precessing spins. We investigate the impact of including subdominant harmonics, compare equal and unequal arm configurations, and analyze different Time-Delay-Interferometry (TDI) configurations. We utilize full and uniform priors, with a lower frequency cutoff of 0.1mHz, and a signal duration of approximately two months, sampled every 5 seconds. The sampler is initialized based on Fisher estimates. Our results demonstrate LISA capability to measure the two spin magnitudes and the primary spin tilt angle, alongside sky localization, with percent-level precision, while component masses are determined with sub-percent accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2501.08261v1),  [pdf](http://arxiv.org/pdf/2501.08261v1)

**Tags**: gr-qc astro-ph.IM 



### Multinomial Link Models
**Authors**: Tianmeng Wang, Liping Tong, Jie Yang

**Updated**: 2025-01-14T16:55:46Z

**Summary**: We propose a new class of regression models for analyzing categorical responses, called multinomial link models. It consists of four subclasses, including mixed-link models that generalize existing multinomial logistic models and their extensions, two-group models that can incorporate the observations with NA or unknown responses, multinomial conditional link models that handle longitudinal categorical responses, and po-npo mixture models that extend partial proportional odds models. We provide explicit formulae and detailed algorithms for finding the maximum likelihood estimates of the model parameters and computing the Fisher information matrix. Our algorithms solve the infeasibility issue of existing statistical software when estimating parameters of cumulative link models. The applications to real datasets show that the new models can fit the data significantly better, correct misleading conclusions due to missing responses, and make more informative inference.

**Link**: [arxiv](http://arxiv.org/abs/2312.16260v3),  [pdf](http://arxiv.org/pdf/2312.16260v3)

**Tags**: stat.ME 



### Deep Compression Autoencoder for Efficient High-Resolution Diffusion   Models
**Authors**: Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, Song Han

**Updated**: 2025-01-14T16:47:44Z

**Summary**: We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder models for accelerating high-resolution diffusion models. Existing autoencoder models have demonstrated impressive results at a moderate spatial compression ratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for high spatial compression ratios (e.g., 64x). We address this challenge by introducing two key techniques: (1) Residual Autoencoding, where we design our models to learn residuals based on the space-to-channel transformed features to alleviate the optimization difficulty of high spatial-compression autoencoders; (2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases training strategy for mitigating the generalization penalty of high spatial-compression autoencoders. With these designs, we improve the autoencoder's spatial compression ratio up to 128 while maintaining the reconstruction quality. Applying our DC-AE to latent diffusion models, we achieve significant speedup without accuracy drop. For example, on ImageNet 512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup on H100 GPU for UViT-H while achieving a better FID, compared with the widely used SD-VAE-f8 autoencoder. Our code is available at https://github.com/mit-han-lab/efficientvit.

**Link**: [arxiv](http://arxiv.org/abs/2410.10733v4),  [pdf](http://arxiv.org/pdf/2410.10733v4)

**Tags**: cs.CV cs.AI 



### Text-Diffusion Red-Teaming of Large Language Models: Unveiling Harmful   Behaviors with Proximity Constraints
**Authors**: Jonathan Nöther, Adish Singla, Goran Radanović

**Updated**: 2025-01-14T16:32:01Z

**Summary**: Recent work has proposed automated red-teaming methods for testing the vulnerabilities of a given target large language model (LLM). These methods use red-teaming LLMs to uncover inputs that induce harmful behavior in a target LLM. In this paper, we study red-teaming strategies that enable a targeted security assessment. We propose an optimization framework for red-teaming with proximity constraints, where the discovered prompts must be similar to reference prompts from a given dataset. This dataset serves as a template for the discovered prompts, anchoring the search for test-cases to specific topics, writing styles, or types of harmful behavior. We show that established auto-regressive model architectures do not perform well in this setting. We therefore introduce a black-box red-teaming method inspired by text-diffusion models: Diffusion for Auditing and Red-Teaming (DART). DART modifies the reference prompt by perturbing it in the embedding space, directly controlling the amount of change introduced. We systematically evaluate our method by comparing its effectiveness with established methods based on model fine-tuning and zero- and few-shot prompting. Our results show that DART is significantly more effective at discovering harmful inputs in close proximity to the reference prompt.

**Link**: [arxiv](http://arxiv.org/abs/2501.08246v1),  [pdf](http://arxiv.org/pdf/2501.08246v1)

**Tags**: cs.LG 



### Engineering LLM Powered Multi-agent Framework for Autonomous CloudOps
**Authors**: Kannan Parthasarathy, Karthik Vaidhyanathan, Rudra Dhar, Venkat Krishnamachari, Basil Muhammed, Adyansh Kakran, Sreemaee Akshathala, Shrikara Arun, Sumant Dubey, Mohan Veerubhotla, Amey Karan

**Updated**: 2025-01-14T16:30:10Z

**Summary**: Cloud Operations (CloudOps) is a rapidly growing field focused on the automated management and optimization of cloud infrastructure which is essential for organizations navigating increasingly complex cloud environments. MontyCloud Inc. is one of the major companies in the CloudOps domain that leverages autonomous bots to manage cloud compliance, security, and continuous operations. To make the platform more accessible and effective to the customers, we leveraged the use of GenAI.   Developing a GenAI-based solution for autonomous CloudOps for the existing MontyCloud system presented us with various challenges such as i) diverse data sources; ii) orchestration of multiple processes; and iii) handling complex workflows to automate routine tasks. To this end, we developed MOYA, a multi-agent framework that leverages GenAI and balances autonomy with the necessary human control. This framework integrates various internal and external systems and is optimized for factors like task orchestration, security, and error mitigation while producing accurate, reliable, and relevant insights by utilizing Retrieval Augmented Generation (RAG). Evaluations of our multi-agent system with the help of practitioners as well as using automated checks demonstrate enhanced accuracy, responsiveness, and effectiveness over non-agentic approaches across complex workflows.

**Link**: [arxiv](http://arxiv.org/abs/2501.08243v1),  [pdf](http://arxiv.org/pdf/2501.08243v1)

**Tags**: cs.SE cs.AI cs.LG 



### HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language   Models
**Authors**: Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, Yu Su

**Updated**: 2025-01-14T16:17:49Z

**Summary**: In order to thrive in hostile and ever-changing natural environments, mammalian brains evolved to store large amounts of knowledge about the world and continually integrate new information while avoiding catastrophic forgetting. Despite the impressive accomplishments, large language models (LLMs), even with retrieval-augmented generation (RAG), still struggle to efficiently and effectively integrate a large amount of new experiences after pre-training. In this work, we introduce HippoRAG, a novel retrieval framework inspired by the hippocampal indexing theory of human long-term memory to enable deeper and more efficient knowledge integration over new experiences. HippoRAG synergistically orchestrates LLMs, knowledge graphs, and the Personalized PageRank algorithm to mimic the different roles of neocortex and hippocampus in human memory. We compare HippoRAG with existing RAG methods on multi-hop question answering and show that our method outperforms the state-of-the-art methods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves comparable or better performance than iterative retrieval like IRCoT while being 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into IRCoT brings further substantial gains. Finally, we show that our method can tackle new types of scenarios that are out of reach of existing methods. Code and data are available at https://github.com/OSU-NLP-Group/HippoRAG.

**Link**: [arxiv](http://arxiv.org/abs/2405.14831v3),  [pdf](http://arxiv.org/pdf/2405.14831v3)

**Tags**: cs.CL cs.AI 



### Investigating Energy Efficiency and Performance Trade-offs in LLM   Inference Across Tasks and DVFS Settings
**Authors**: Paul Joe Maliakel, Shashikant Ilager, Ivona Brandic

**Updated**: 2025-01-14T16:02:33Z

**Summary**: Large language models (LLMs) have shown significant improvements in many natural language processing (NLP) tasks, accelerating their rapid adoption across many industries. These models are resource-intensive, requiring extensive computational resources both during training and inference, leading to increased energy consumption and negative environmental impact. As their adoption accelerates, the sustainability of LLMs has become a critical issue, necessitating strategies to optimize their runtime efficiency without compromising performance. Hence, it is imperative to identify the parameters that significantly influence the performance and energy efficiency of LLMs. To that end, in this work, we investigate the effect of important parameters on the performance and energy efficiency of LLMs during inference and examine their trade-offs.   First, we analyze how different types of models with varying numbers of parameters and architectures perform on tasks like text generation, question answering, and summarization by benchmarking LLMs such as Falcon-7B, Mistral-7B-v0.1, T5-3B, GPT-2, GPT-J-6B, and GPT-Neo-2.7B. Second, we study input and output sequence characteristics such as sequence length concerning energy consumption, performance, and throughput. Finally, we explore the impact of hardware-based power-saving techniques, i.e., Dynamic Voltage Frequency Scaling (DVFS), on the models' latency and energy efficiency. Our extensive benchmarking and statistical analysis reveal many interesting findings, uncovering how specific optimizations can reduce energy consumption while maintaining throughput and accuracy. This study provides actionable insights for researchers and practitioners to design energy-efficient LLM inference systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.08219v1),  [pdf](http://arxiv.org/pdf/2501.08219v1)

**Tags**: cs.LG 



### Logic Augmented Generation
**Authors**: Aldo Gangemi, Andrea Giovanni Nuzzolese

**Updated**: 2025-01-14T15:58:02Z

**Summary**: Semantic Knowledge Graphs (SKG) face challenges with scalability, flexibility, contextual understanding, and handling unstructured or ambiguous information. However, they offer formal and structured knowledge enabling highly interpretable and reliable results by means of reasoning and querying. Large Language Models (LLMs) overcome those limitations making them suitable in open-ended tasks and unstructured environments. Nevertheless, LLMs are neither interpretable nor reliable. To solve the dichotomy between LLMs and SKGs we envision Logic Augmented Generation (LAG) that combines the benefits of the two worlds. LAG uses LLMs as Reactive Continuous Knowledge Graphs that can generate potentially infinite relations and tacit knowledge on-demand. SKGs are key for injecting a discrete heuristic dimension with clear logical and factual boundaries. We exemplify LAG in two tasks of collective intelligence, i.e., medical diagnostics and climate projections. Understanding the properties and limitations of LAG, which are still mostly unknown, is of utmost importance for enabling a variety of tasks involving tacit knowledge in order to provide interpretable and effective results.

**Link**: [arxiv](http://arxiv.org/abs/2411.14012v2),  [pdf](http://arxiv.org/pdf/2411.14012v2)

**Tags**: cs.AI cs.CL 



### ASTRID -- An Automated and Scalable TRIaD for the Evaluation of   RAG-based Clinical Question Answering Systems
**Authors**: Mohita Chowdhury, Yajie Vera He, Aisling Higham, Ernest Lim

**Updated**: 2025-01-14T15:46:39Z

**Summary**: Large Language Models (LLMs) have shown impressive potential in clinical question answering (QA), with Retrieval Augmented Generation (RAG) emerging as a leading approach for ensuring the factual accuracy of model responses. However, current automated RAG metrics perform poorly in clinical and conversational use cases. Using clinical human evaluations of responses is expensive, unscalable, and not conducive to the continuous iterative development of RAG systems. To address these challenges, we introduce ASTRID - an Automated and Scalable TRIaD for evaluating clinical QA systems leveraging RAG - consisting of three metrics: Context Relevance (CR), Refusal Accuracy (RA), and Conversational Faithfulness (CF). Our novel evaluation metric, CF, is designed to better capture the faithfulness of a model's response to the knowledge base without penalising conversational elements. To validate our triad, we curate a dataset of over 200 real-world patient questions posed to an LLM-based QA agent during surgical follow-up for cataract surgery - the highest volume operation in the world - augmented with clinician-selected questions for emergency, clinical, and non-clinical out-of-domain scenarios. We demonstrate that CF can predict human ratings of faithfulness better than existing definitions for conversational use cases. Furthermore, we show that evaluation using our triad consisting of CF, RA, and CR exhibits alignment with clinician assessment for inappropriate, harmful, or unhelpful responses. Finally, using nine different LLMs, we demonstrate that the three metrics can closely agree with human evaluations, highlighting the potential of these metrics for use in LLM-driven automated evaluation pipelines. We also publish the prompts and datasets for these experiments, providing valuable resources for further research and development.

**Link**: [arxiv](http://arxiv.org/abs/2501.08208v1),  [pdf](http://arxiv.org/pdf/2501.08208v1)

**Tags**: cs.CL cs.AI 



### ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math   Problem Solving
**Authors**: Zain Ul Abedin, Shahzeb Qamar, Lucie Flek, Akbar Karimi

**Updated**: 2025-01-14T15:38:41Z

**Summary**: While Large Language Models (LLMs) have shown impressive capabilities in math problem-solving tasks, their robustness to noisy inputs is not well-studied. In this work, we propose ArithmAttack to examine how robust the LLMs are when they encounter noisy prompts that contain extra noise in the form of punctuation marks. While being easy to implement, ArithmAttack does not cause any information loss since words are not added or deleted from the context. We evaluate the robustness of seven LLMs, including LLama3, Mistral, and Mathstral, on noisy GSM8K and MultiArith datasets. Our experiments suggest that all the studied models show vulnerability to such noise, with more noise leading to poorer performances.

**Link**: [arxiv](http://arxiv.org/abs/2501.08203v1),  [pdf](http://arxiv.org/pdf/2501.08203v1)

**Tags**: cs.CL 



### Globally Convergent Variational Inference
**Authors**: Declan McNamara, Jackson Loper, Jeffrey Regier

**Updated**: 2025-01-14T15:36:32Z

**Summary**: In variational inference (VI), an approximation of the posterior distribution is selected from a family of distributions through numerical optimization. With the most common variational objective function, known as the evidence lower bound (ELBO), only convergence to a local optimum can be guaranteed. In this work, we instead establish the global convergence of a particular VI method. This VI method, which may be considered an instance of neural posterior estimation (NPE), minimizes an expectation of the inclusive (forward) KL divergence to fit a variational distribution that is parameterized by a neural network. Our convergence result relies on the neural tangent kernel (NTK) to characterize the gradient dynamics that arise from considering the variational objective in function space. In the asymptotic regime of a fixed, positive-definite neural tangent kernel, we establish conditions under which the variational objective admits a unique solution in a reproducing kernel Hilbert space (RKHS). Then, we show that the gradient descent dynamics in function space converge to this unique function. In ablation studies and practical problems, we demonstrate that our results explain the behavior of NPE in non-asymptotic finite-neuron settings, and show that NPE outperforms ELBO-based optimization, which often converges to shallow local optima.

**Link**: [arxiv](http://arxiv.org/abs/2501.08201v1),  [pdf](http://arxiv.org/pdf/2501.08201v1)

**Tags**: stat.ML cs.LG 



### Personalized LLM Response Generation with Parameterized Memory Injection
**Authors**: Kai Zhang, Yejin Kim, Xiaozhong Liu

**Updated**: 2025-01-14T15:30:50Z

**Summary**: Large Language Models (LLMs) have exhibited remarkable proficiency in comprehending and generating natural language. On the other hand, personalized LLM response generation holds the potential to offer substantial benefits for individuals in critical areas such as medical. Existing research has explored memory-augmented methods to prompt the LLM with pre-stored user-specific knowledge for personalized response generation in terms of new queries. We contend that such paradigm is unable to perceive fine-granularity information. In this study, we propose a novel \textbf{M}emory-\textbf{i}njected approach using parameter-efficient fine-tuning (PEFT) and along with a Bayesian Optimisation searching strategy to achieve \textbf{L}LM \textbf{P}ersonalization(\textbf{MiLP}).

**Link**: [arxiv](http://arxiv.org/abs/2404.03565v3),  [pdf](http://arxiv.org/pdf/2404.03565v3)

**Tags**: cs.CL 



### CWEval: Outcome-driven Evaluation on Functionality and Security of LLM   Code Generation
**Authors**: Jinjun Peng, Leyi Cui, Kele Huang, Junfeng Yang, Baishakhi Ray

**Updated**: 2025-01-14T15:27:01Z

**Summary**: Large Language Models (LLMs) have significantly aided developers by generating or assisting in code writing, enhancing productivity across various tasks. While identifying incorrect code is often straightforward, detecting vulnerabilities in functionally correct code is more challenging, especially for developers with limited security knowledge, which poses considerable security risks of using LLM-generated code and underscores the need for robust evaluation benchmarks that assess both functional correctness and security. Current benchmarks like CyberSecEval and SecurityEval attempt to solve it but are hindered by unclear and impractical specifications, failing to assess both functionality and security accurately. To tackle these deficiencies, we introduce CWEval, a novel outcome-driven evaluation framework designed to enhance the evaluation of secure code generation by LLMs. This framework not only assesses code functionality but also its security simultaneously with high-quality task specifications and outcome-driven test oracles which provides high accuracy. Coupled with CWEval-bench, a multilingual, security-critical coding benchmark, CWEval provides a rigorous empirical security evaluation on LLM-generated code, overcoming previous benchmarks' shortcomings. Through our evaluations, CWEval reveals a notable portion of functional but insecure code produced by LLMs, and shows a serious inaccuracy of previous evaluations, ultimately contributing significantly to the field of secure code generation. We open-source our artifact at: https://github.com/Co1lin/CWEval .

**Link**: [arxiv](http://arxiv.org/abs/2501.08200v1),  [pdf](http://arxiv.org/pdf/2501.08200v1)

**Tags**: cs.SE cs.CL cs.LG 



### OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for   LLM Training
**Authors**: Yijiong Yu, Ziyun Dai, Zekun Wang, Wei Wang, Ran Chen, Ji Pei

**Updated**: 2025-01-14T15:22:47Z

**Summary**: Large language models (LLMs) have demonstrated remarkable capabilities, but their success heavily relies on the quality of pretraining corpora. For Chinese LLMs, the scarcity of high-quality Chinese datasets presents a significant challenge, often limiting their performance. To address this issue, we propose the OpenCSG Chinese Corpus, a series of high-quality datasets specifically designed for LLM pretraining, post-training, and fine-tuning. This corpus includes Fineweb-edu-chinese, Fineweb-edu-chinese-v2, Cosmopedia-chinese, and Smoltalk-chinese, each with distinct characteristics: Fineweb-edu datasets focus on filtered, high-quality content derived from diverse Chinese web sources; Cosmopedia-chinese provides synthetic, textbook-style data for knowledge-intensive training; and Smoltalk-chinese emphasizes stylistic and diverse chat-format data. The OpenCSG Chinese Corpus is characterized by its high-quality text, diverse coverage across domains, and scalable, reproducible data curation processes. Additionally, we conducted extensive experimental analyses, including evaluations on smaller parameter models, which demonstrated significant performance improvements in tasks such as C-Eval, showcasing the effectiveness of the corpus for training Chinese LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2501.08197v1),  [pdf](http://arxiv.org/pdf/2501.08197v1)

**Tags**: cs.CL 



### KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model
**Authors**: Xinshuo Hu, Zifei Shan, Xinping Zhao, Zetian Sun, Zhenyu Liu, Dongfang Li, Shaolin Ye, Xinyuan Wei, Qian Chen, Baotian Hu, Min Zhang

**Updated**: 2025-01-14T15:19:52Z

**Summary**: As retrieval-augmented generation prevails in large language models, embedding models are becoming increasingly crucial. Despite the growing number of general embedding models, prior work often overlooks the critical role of training data quality. In this work, we introduce KaLM-Embedding, a general multilingual embedding model that leverages a large quantity of cleaner, more diverse, and domain-specific training data. Our model has been trained with key techniques proven to enhance performance: (1) persona-based synthetic data to create diversified examples distilled from LLMs, (2) ranking consistency filtering to remove less informative samples, and (3) semi-homogeneous task batch sampling to improve training efficacy. Departing from traditional BERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model, facilitating the adaptation of auto-regressive language models for general embedding tasks. Extensive evaluations of the MTEB benchmark across multiple languages show that our model outperforms others of comparable size, setting a new standard for multilingual embedding models with <1B parameters.

**Link**: [arxiv](http://arxiv.org/abs/2501.01028v3),  [pdf](http://arxiv.org/pdf/2501.01028v3)

**Tags**: cs.CL 



### PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM   Serving
**Authors**: Ahmet Caner Yüzügüler, Jiawei Zhuang, Lukas Cavigelli

**Updated**: 2025-01-14T15:14:10Z

**Summary**: Large language models (LLMs) are widely used across various applications, but their substantial computational requirements pose significant challenges, particularly in terms of HBM bandwidth bottlenecks and inter-device communication overhead. In this paper, we present PRESERVE, a novel prefetching framework designed to optimize LLM inference by overlapping memory reads for model weights and KV-cache with collective communication operations. Through extensive experiments conducted on commercial AI accelerators, we demonstrate up to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs. Additionally, we perform a design space exploration that identifies the optimal hardware configuration for the proposed method, showing a further 1.25x improvement in performance per cost by selecting the optimal L2 cache size. Our results show that PRESERVE has the potential to mitigate the memory bottlenecks and communication overheads, offering a solution to improve the performance and scalability of the LLM inference systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.08192v1),  [pdf](http://arxiv.org/pdf/2501.08192v1)

**Tags**: cs.AI cs.AR cs.DC 



### A Critical Synthesis of Uncertainty Quantification and Foundation Models   in Monocular Depth Estimation
**Authors**: Steven Landgraf, Rongjun Qin, Markus Ulrich

**Updated**: 2025-01-14T15:13:00Z

**Summary**: While recent foundation models have enabled significant breakthroughs in monocular depth estimation, a clear path towards safe and reliable deployment in the real-world remains elusive. Metric depth estimation, which involves predicting absolute distances, poses particular challenges, as even the most advanced foundation models remain prone to critical errors. Since quantifying the uncertainty has emerged as a promising endeavor to address these limitations and enable trustworthy deployment, we fuse five different uncertainty quantification methods with the current state-of-the-art DepthAnythingV2 foundation model. To cover a wide range of metric depth domains, we evaluate their performance on four diverse datasets. Our findings identify fine-tuning with the Gaussian Negative Log-Likelihood Loss (GNLL) as a particularly promising approach, offering reliable uncertainty estimates while maintaining predictive performance and computational efficiency on par with the baseline, encompassing both training and inference time. By fusing uncertainty quantification and foundation models within the context of monocular depth estimation, this paper lays a critical foundation for future research aimed at improving not only model performance but also its explainability. Extending this critical synthesis of uncertainty quantification and foundation models into other crucial tasks, such as semantic segmentation and pose estimation, presents exciting opportunities for safer and more reliable machine vision systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.08188v1),  [pdf](http://arxiv.org/pdf/2501.08188v1)

**Tags**: cs.CV cs.AI cs.LG 



### WebWalker: Benchmarking LLMs in Web Traversal
**Authors**: Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, Fei Huang

**Updated**: 2025-01-14T15:06:56Z

**Summary**: Retrieval-augmented generation (RAG) demonstrates remarkable performance across tasks in open-domain question-answering. However, traditional search engines may retrieve shallow content, limiting the ability of LLMs to handle complex, multi-layered information. To address it, we introduce WebWalkerQA, a benchmark designed to assess the ability of LLMs to perform web traversal. It evaluates the capacity of LLMs to traverse a website's subpages to extract high-quality data systematically. We propose WebWalker, which is a multi-agent framework that mimics human-like web navigation through an explore-critic paradigm. Extensive experimental results show that WebWalkerQA is challenging and demonstrates the effectiveness of RAG combined with WebWalker, through the horizontal and vertical integration in real-world scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2501.07572v2),  [pdf](http://arxiv.org/pdf/2501.07572v2)

**Tags**: cs.CL cs.AI 



### Optimal estimation of the null distribution in large-scale inference
**Authors**: Subhodh Kotekal, Chao Gao

**Updated**: 2025-01-14T14:58:00Z

**Summary**: The advent of large-scale inference has spurred reexamination of conventional statistical thinking. In a Gaussian model for $n$ many $z$-scores with at most $k < \frac{n}{2}$ nonnulls, Efron suggests estimating the location and scale parameters of the null distribution. Placing no assumptions on the nonnull effects, the statistical task can be viewed as a robust estimation problem. However, the best known robust estimators fail to be consistent in the regime $k \asymp n$ which is especially relevant in large-scale inference. The failure of estimators which are minimax rate-optimal with respect to other formulations of robustness (e.g. Huber's contamination model) might suggest the impossibility of consistent estimation in this regime and, consequently, a major weakness of Efron's suggestion. A sound evaluation of Efron's model thus requires a complete understanding of consistency. We sharply characterize the regime of $k$ for which consistent estimation is possible and further establish the minimax estimation rates. It is shown consistent estimation of the location parameter is possible if and only if $\frac{n}{2} - k = \omega(\sqrt{n})$, and consistent estimation of the scale parameter is possible in the entire regime $k < \frac{n}{2}$. Faster rates than those in Huber's contamination model are achievable by exploiting the Gaussian character of the data. The minimax upper bound is obtained by considering estimators based on the empirical characteristic function. The minimax lower bound involves constructing two marginal distributions whose characteristic functions match on a wide interval containing zero. The construction notably differs from those in the literature by sharply capturing a scaling of $n-2k$ in the minimax estimation rate of the location.

**Link**: [arxiv](http://arxiv.org/abs/2401.06350v2),  [pdf](http://arxiv.org/pdf/2401.06350v2)

**Tags**: math.ST stat.ME stat.TH 



### Potential and Perils of Large Language Models as Judges of Unstructured   Textual Data
**Authors**: Rewina Bedemariam, Natalie Perez, Sreyoshi Bhaduri, Satya Kapoor, Alex Gil, Elizabeth Conjar, Ikkei Itoku, David Theil, Aman Chadha, Naumaan Nayyar

**Updated**: 2025-01-14T14:49:14Z

**Summary**: Rapid advancements in large language models have unlocked remarkable capabilities when it comes to processing and summarizing unstructured text data. This has implications for the analysis of rich, open-ended datasets, such as survey responses, where LLMs hold the promise of efficiently distilling key themes and sentiments. However, as organizations increasingly turn to these powerful AI systems to make sense of textual feedback, a critical question arises, can we trust LLMs to accurately represent the perspectives contained within these text based datasets? While LLMs excel at generating human-like summaries, there is a risk that their outputs may inadvertently diverge from the true substance of the original responses. Discrepancies between the LLM-generated outputs and the actual themes present in the data could lead to flawed decision-making, with far-reaching consequences for organizations. This research investigates the effectiveness of LLMs as judge models to evaluate the thematic alignment of summaries generated by other LLMs. We utilized an Anthropic Claude model to generate thematic summaries from open-ended survey responses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as LLM judges. The LLM-as-judge approach was compared to human evaluations using Cohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalable alternative to traditional human centric evaluation methods. Our findings reveal that while LLMs as judges offer a scalable solution comparable to human raters, humans may still excel at detecting subtle, context-specific nuances. This research contributes to the growing body of knowledge on AI assisted text analysis. We discuss limitations and provide recommendations for future research, emphasizing the need for careful consideration when generalizing LLM judge models across various contexts and use cases.

**Link**: [arxiv](http://arxiv.org/abs/2501.08167v1),  [pdf](http://arxiv.org/pdf/2501.08167v1)

**Tags**: cs.CL cs.AI cs.CY 



### I Can Find You in Seconds! Leveraging Large Language Models for Code   Authorship Attribution
**Authors**: Soohyeon Choi, Yong Kiam Tan, Mark Huasong Meng, Mohamed Ragab, Soumik Mondal, David Mohaisen, Khin Mi Mi Aung

**Updated**: 2025-01-14T14:46:19Z

**Summary**: Source code authorship attribution is important in software forensics, plagiarism detection, and protecting software patch integrity. Existing techniques often rely on supervised machine learning, which struggles with generalization across different programming languages and coding styles due to the need for large labeled datasets. Inspired by recent advances in natural language authorship analysis using large language models (LLMs), which have shown exceptional performance without task-specific tuning, this paper explores the use of LLMs for source code authorship attribution.   We present a comprehensive study demonstrating that state-of-the-art LLMs can successfully attribute source code authorship across different languages. LLMs can determine whether two code snippets are written by the same author with zero-shot prompting, achieving a Matthews Correlation Coefficient (MCC) of 0.78, and can attribute code authorship from a small set of reference code snippets via few-shot learning, achieving MCC of 0.77. Additionally, LLMs show some adversarial robustness against misattribution attacks.   Despite these capabilities, we found that naive prompting of LLMs does not scale well with a large number of authors due to input token limitations. To address this, we propose a tournament-style approach for large-scale attribution. Evaluating this approach on datasets of C++ (500 authors, 26,355 samples) and Java (686 authors, 55,267 samples) code from GitHub, we achieve classification accuracy of up to 65% for C++ and 68.7% for Java using only one reference per author. These results open new possibilities for applying LLMs to code authorship attribution in cybersecurity and software engineering.

**Link**: [arxiv](http://arxiv.org/abs/2501.08165v1),  [pdf](http://arxiv.org/pdf/2501.08165v1)

**Tags**: cs.SE cs.AI 



### Inference-Time-Compute: More Faithful? A Research Note
**Authors**: James Chua, Owain Evans

**Updated**: 2025-01-14T14:31:45Z

**Summary**: Models trained specifically to generate long Chains of Thought (CoTs) have recently achieved impressive results. We refer to these models as Inference-Time-Compute (ITC) models. Are the CoTs of ITC models more faithful compared to traditional non-ITC models? We evaluate two ITC models (based on Qwen-2.5 and Gemini-2) on an existing test of faithful CoT To measure faithfulness, we test if models articulate cues in their prompt that influence their answers to MMLU questions. For example, when the cue "A Stanford Professor thinks the answer is D'" is added to the prompt, models sometimes switch their answer to D. In such cases, the Gemini ITC model articulates the cue 54% of the time, compared to 14% for the non-ITC Gemini.   We evaluate 7 types of cue, such as misleading few-shot examples and anchoring on past responses. ITC models articulate cues that influence them much more reliably than all the 6 non-ITC models tested, such as Claude-3.5-Sonnet and GPT-4o, which often articulate close to 0% of the time.   However, our study has important limitations. We evaluate only two ITC models -- we cannot evaluate OpenAI's SOTA o1 model. We also lack details about the training of these ITC models, making it hard to attribute our findings to specific processes.   We think faithfulness of CoT is an important property for AI Safety. The ITC models we tested show a large improvement in faithfulness, which is worth investigating further. To speed up this investigation, we release these early results as a research note.

**Link**: [arxiv](http://arxiv.org/abs/2501.08156v1),  [pdf](http://arxiv.org/pdf/2501.08156v1)

**Tags**: cs.LG 



### A novel understanding of the role of plasma-molecular kinetics on   divertor power exhaust
**Authors**: N. Osborne, K. Verhaegh, D. Moulton, H. Reimerdes, P. Ryan, N. Lonigro, S. Mijin, R. Osawa, K. Murray, S. Kobussen, Y. Damizia, A. Perek, C. Theiler, R. Ducker, D. Mykytchuk

**Updated**: 2025-01-14T14:29:36Z

**Summary**: During detachment, a buffer of neutral atoms and molecules builds up between the target and the ionising plasma. Collisions between the plasma and the molecules play an important role in the detachment process. Studies of plasma-molecular kinetics indicate that the gas temperature is increased during detachment for a wide range of conditions on the MAST-U and TCV tokamaks. This is related to an increased $\mathrm{D}_2$ lifetime during detachment, leading to more plasma-molecule collisions that raise the molecular temperature. Such collisions subsequently result in significant power and momentum losses to the divertor plasma during detachment. Using a simplified inference, these losses are estimated using the rotational temperature, neutral pressure and ionisation front position. Significant power losses (about $10\%$ of $P_{SOL}$) and dominant momentum losses (majority of the upstream pressure) from plasma-molecule collisions are inferred experimentally in long-legged, strongly baffled, detached divertors (MAST-U Super-X divertor), consistent with SOLPS-ITER simulations. The vibrational distribution obtained is compared to a collisional-radiative model setup using the same rate data as SOLPS-ITER, indicating some qualitative agreements and disagreements, potentially highlighting model gaps with regard to the default rates used.   These interpretations highlight the importance of plasma-molecular collisions, leading to power and momentum losses during detachment. Our analysis and reduced modelling of these processes provide further insights into detachment control observations, the workings of long-legged divertors and divertor power balance.

**Link**: [arxiv](http://arxiv.org/abs/2410.14403v5),  [pdf](http://arxiv.org/pdf/2410.14403v5)

**Tags**: physics.plasm-ph 



### Energy Backdoor Attack to Deep Neural Networks
**Authors**: Hanene F. Z. Brachemi Meftah, Wassim Hamidouche, Sid Ahmed Fezza, Olivier Déforges, Kassem Kallas

**Updated**: 2025-01-14T14:26:18Z

**Summary**: The rise of deep learning (DL) has increased computing complexity and energy use, prompting the adoption of application specific integrated circuits (ASICs) for energy-efficient edge and mobile deployment. However, recent studies have demonstrated the vulnerability of these accelerators to energy attacks. Despite the development of various inference time energy attacks in prior research, backdoor energy attacks remain unexplored. In this paper, we design an innovative energy backdoor attack against deep neural networks (DNNs) operating on sparsity-based accelerators. Our attack is carried out in two distinct phases: backdoor injection and backdoor stealthiness. Experimental results using ResNet-18 and MobileNet-V2 models trained on CIFAR-10 and Tiny ImageNet datasets show the effectiveness of our proposed attack in increasing energy consumption on trigger samples while preserving the model's performance for clean/regular inputs. This demonstrates the vulnerability of DNNs to energy backdoor attacks. The source code of our attack is available at: https://github.com/hbrachemi/energy_backdoor.

**Link**: [arxiv](http://arxiv.org/abs/2501.08152v1),  [pdf](http://arxiv.org/pdf/2501.08152v1)

**Tags**: cs.CV 



### Inductive Learning of Logical Theories with LLMs: An Expressivity-Graded   Analysis
**Authors**: João Pedro Gandarela, Danilo S. Carvalho, André Freitas

**Updated**: 2025-01-14T14:26:03Z

**Summary**: This work presents a novel systematic methodology to analyse the capabilities and limitations of Large Language Models (LLMs) with feedback from a formal inference engine, on logic theory induction. The analysis is complexity-graded w.r.t. rule dependency structure, allowing quantification of specific inference challenges on LLM performance. Integrating LLMs with formal methods is a promising frontier in the Natural Language Processing field, as an important avenue for improving model inference control and explainability. In particular, inductive learning over complex sets of facts and rules, poses unique challenges for current autoregressive models, as they lack explicit symbolic grounding. While they can be complemented by formal systems, the properties delivered by LLMs regarding inductive learning, are not well understood and quantified. Empirical results indicate that the largest LLMs can achieve competitive results against a SOTA Inductive Logic Programming (ILP) system baseline, but also that tracking long predicate relationship chains is a more difficult obstacle than theory complexity for LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2408.16779v2),  [pdf](http://arxiv.org/pdf/2408.16779v2)

**Tags**: cs.CL cs.AI cs.LO I.2.7 



### Refusal Behavior in Large Language Models: A Nonlinear Perspective
**Authors**: Fabian Hildebrandt, Andreas Maier, Patrick Krauss, Achim Schilling

**Updated**: 2025-01-14T14:23:18Z

**Summary**: Refusal behavior in large language models (LLMs) enables them to decline responding to harmful, unethical, or inappropriate prompts, ensuring alignment with ethical standards. This paper investigates refusal behavior across six LLMs from three architectural families. We challenge the assumption of refusal as a linear phenomenon by employing dimensionality reduction techniques, including PCA, t-SNE, and UMAP. Our results reveal that refusal mechanisms exhibit nonlinear, multidimensional characteristics that vary by model architecture and layer. These findings highlight the need for nonlinear interpretability to improve alignment research and inform safer AI deployment strategies.

**Link**: [arxiv](http://arxiv.org/abs/2501.08145v1),  [pdf](http://arxiv.org/pdf/2501.08145v1)

**Tags**: cs.CL cs.AI 



### Magnetic Field Structures In and Around Seyfert Galaxy Outflows
**Authors**: Salmoli Ghosh, P. Kharb, B. Sebastian, J. Gallimore, A. Pasetto, C. P. O'Dea, T. Heckman, S. A. Baum

**Updated**: 2025-01-14T14:21:37Z

**Summary**: We present radio polarimetric images of 12 Seyfert and Low-Ionization Nuclear Emission-line Region (LINER) galaxies belonging to the Centre for Astrophysics (CfA)+12 micron sample exhibiting kiloparsec-scale radio outflows (KSRs). These observations have been carried out at 10 GHz with Karl G. Jansky Very Large Array (VLA) in D-array and at 1.4 GHz with the BnA$\rightarrow$A array configurations. We find signatures of organized magnetic (B-) field structures in the cores, jets and lobes of these galaxies. The linear polarization fraction varies from a few per cent in the cores to $47\pm18$ per cent in the lobes. The inferred B-fields are toroidal in the cores of several sources making them consistent with the presence of either a sheath-like or a wind-like component surrounding the jet. The in-band spectral index images typically show the presence of flat/inverted spectrum cores and steep spectrum lobes. Radio cores with flatter spectra are found to have lower Eddington ratios while the steeper ones have higher. A strong correlation is observed between the Seyfert/LINER radio outflow properties and the mass of the supermassive black holes (SMBHs); correlations with Eddington ratios are weaker. We find signatures of jet-medium interaction and both positive and negative AGN feedback in these sources. Overall, our study indicates that radio-quiet (RQ) AGN with KSRs possess radio outflows driven by magnetic fields anchored to their black holes - accretion disks, which significantly impact their environments.

**Link**: [arxiv](http://arxiv.org/abs/2501.08141v1),  [pdf](http://arxiv.org/pdf/2501.08141v1)

**Tags**: astro-ph.GA 



### Are LLMs Good Literature Review Writers? Evaluating the Literature   Review Writing Ability of Large Language Models
**Authors**: Xuemei Tang, Xufeng Duan, Zhenguang G. Cai

**Updated**: 2025-01-14T14:16:45Z

**Summary**: The literature review is a crucial form of academic writing that involves complex processes of literature collection, organization, and summarization. The emergence of large language models (LLMs) has introduced promising tools to automate these processes. However, their actual capabilities in writing comprehensive literature reviews remain underexplored, such as whether they can generate accurate and reliable references. To address this gap, we propose a framework to assess the literature review writing ability of LLMs automatically. We evaluate the performance of LLMs across three tasks: generating references, writing abstracts, and writing literature reviews. We employ external tools for a multidimensional evaluation, which includes assessing hallucination rates in references, semantic coverage, and factual consistency with human-written context. By analyzing the experimental results, we find that, despite advancements, even the most sophisticated models still cannot avoid generating hallucinated references. Additionally, different models exhibit varying performance in literature review writing across different disciplines.

**Link**: [arxiv](http://arxiv.org/abs/2412.13612v2),  [pdf](http://arxiv.org/pdf/2412.13612v2)

**Tags**: cs.CL cs.AI 



### AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out   Context Attribution
**Authors**: Fengyuan Liu, Nikhil Kandpal, Colin Raffel

**Updated**: 2025-01-14T14:07:55Z

**Summary**: The influence of contextual input on the behavior of large language models (LLMs) has prompted the development of context attribution methods that aim to quantify each context span's effect on an LLM's generations. The leave-one-out (LOO) error, which measures the change in the likelihood of the LLM's response when a given span of the context is removed, provides a principled way to perform context attribution, but can be prohibitively expensive to compute for large models. In this work, we introduce AttriBoT, a series of novel techniques for efficiently computing an approximation of the LOO error for context attribution. Specifically, AttriBoT uses cached activations to avoid redundant operations, performs hierarchical attribution to reduce computation, and emulates the behavior of large target models with smaller proxy models. Taken together, AttriBoT can provide a >300x speedup while remaining more faithful to a target model's LOO error than prior context attribution methods. This stark increase in performance makes computing context attributions for a given response 30x faster than generating the response itself, empowering real-world applications that require computing attributions at scale. We release a user-friendly and efficient implementation of AttriBoT to enable efficient LLM interpretability as well as encourage future development of efficient context attribution methods.

**Link**: [arxiv](http://arxiv.org/abs/2411.15102v2),  [pdf](http://arxiv.org/pdf/2411.15102v2)

**Tags**: cs.LG 



### In-situ graph reasoning and knowledge expansion using Graph-PReFLexOR
**Authors**: Markus J. Buehler

**Updated**: 2025-01-14T13:52:41Z

**Summary**: The pursuit of automated scientific discovery has fueled progress from symbolic logic to modern AI, forging new frontiers in reasoning and pattern recognition. Transformers function as potential systems, where every possible relationship remains latent potentiality until tasks impose constraints, akin to measurement. Yet, refining their sampling requires more than probabilistic selection: solutions must conform to specific structures or rules, ensuring consistency and the invocation of general principles. We present Graph-PReFLexOR (Graph-based Preference-based Recursive Language Modeling for Exploratory Optimization of Reasoning), a framework that combines graph reasoning with symbolic abstraction to dynamically expand domain knowledge. Inspired by reinforcement learning, Graph-PReFLexOR defines reasoning as a structured mapping, where tasks yield knowledge graphs, abstract patterns, and ultimately, final answers. Inspired by category theory, it encodes concepts as nodes and their relationships as edges, supporting hierarchical inference and adaptive learning through isomorphic representations. Demonstrations include hypothesis generation, materials design, and creative reasoning, such as discovering relationships between mythological concepts like 'thin places' with materials science. We propose a 'knowledge garden growth' strategy that integrates insights across domains, promoting interdisciplinary connections. Results with a 3-billion-parameter Graph-PReFLexOR model show superior reasoning depth and adaptability, underscoring the potential for transparent, multidisciplinary AI-driven discovery. It lays the groundwork for general autonomous reasoning solutions.

**Link**: [arxiv](http://arxiv.org/abs/2501.08120v1),  [pdf](http://arxiv.org/pdf/2501.08120v1)

**Tags**: cs.AI cond-mat.dis-nn cond-mat.mtrl-sci cs.CL 



### Superoxide anion (O$_{2}\negthinspace^{-}$) collisions with CO$_{2}$   molecules in the energy range 50-950 eV
**Authors**: C. Guerra, M. Leiferman, A. I. Lozano, F. Aguilar-Galindo, S. Díaz-Tendero, J. C. Oller, P. Limão-Vieira, G. García

**Updated**: 2025-01-14T13:36:25Z

**Summary**: A novel gas-phase molecular scattering study is reported for O$_{2}\negthinspace^{-}$ colliding with CO$_{2}$ for impact energies ranging from 50 to 950 eV. The absolute total electron detachment, relative total and partial ionization cross sections have been measured within this energy range and the positive ion yield of those produced during the collisions has been obtained. The primary anionic beam projectile is produced in a pulsed hollow cathode discharge induced plasma, and its interactions with the neutral molecular target occur in a gas cell at a well-known constant pressure. For impact energies above 500 eV high mass (m $>$ 44 u) charged complexes have been detected. With the aid of a theoretical study, using ab initio methods, we propose a mechanism to infer on the formation of these cationic species, which have been assigned as projectile-target stable compounds (CO$_{3}\negthinspace^{+}$ and CO$_{4}\negthinspace^{+}$).

**Link**: [arxiv](http://arxiv.org/abs/2501.08107v1),  [pdf](http://arxiv.org/pdf/2501.08107v1)

**Tags**: physics.chem-ph physics.atom-ph 



### Consistency of Responses and Continuations Generated by Large Language   Models on Social Media
**Authors**: Wenlu Fan, Yuqi Zhu, Chenyang Wang, Bin Wang, Wentao Xu

**Updated**: 2025-01-14T13:19:47Z

**Summary**: Large Language Models (LLMs) demonstrate remarkable capabilities in text generation, yet their emotional consistency and semantic coherence in social media contexts remain insufficiently understood. This study investigates how LLMs handle emotional content and maintain semantic relationships through continuation and response tasks using two open-source models: Gemma and Llama. By analyzing climate change discussions from Twitter and Reddit, we examine emotional transitions, intensity patterns, and semantic similarity between human-authored and LLM-generated content. Our findings reveal that while both models maintain high semantic coherence, they exhibit distinct emotional patterns: Gemma shows a tendency toward negative emotion amplification, particularly anger, while maintaining certain positive emotions like optimism. Llama demonstrates superior emotional preservation across a broader spectrum of affects. Both models systematically generate responses with attenuated emotional intensity compared to human-authored content and show a bias toward positive emotions in response tasks. Additionally, both models maintain strong semantic similarity with original texts, though performance varies between continuation and response tasks. These findings provide insights into LLMs' emotional and semantic processing capabilities, with implications for their deployment in social media contexts and human-AI interaction design.

**Link**: [arxiv](http://arxiv.org/abs/2501.08102v1),  [pdf](http://arxiv.org/pdf/2501.08102v1)

**Tags**: cs.CL cs.AI cs.HC 



### Hierarchical Autoscaling for Large Language Model Serving with Chiron
**Authors**: Archit Patke, Dhemath Reddy, Saurabh Jha, Chandra Narayanaswami, Zbigniew Kalbarczyk, Ravishankar Iyer

**Updated**: 2025-01-14T12:57:40Z

**Summary**: Large language model (LLM) serving is becoming an increasingly important workload for cloud providers. Based on performance SLO requirements, LLM inference requests can be divided into (a) interactive requests that have tight SLOs in the order of seconds, and (b) batch requests that have relaxed SLO in the order of minutes to hours. These SLOs can degrade based on the arrival rates, multiplexing, and configuration parameters, thus necessitating the use of resource autoscaling on serving instances and their batch sizes. However, previous autoscalers for LLM serving do not consider request SLOs leading to unnecessary scaling and resource under-utilization. To address these limitations, we introduce Chiron, an autoscaler that uses the idea of hierarchical backpressure estimated using queue size, utilization, and SLOs. Our experiments show that Chiron achieves up to 90% higher SLO attainment and improves GPU efficiency by up to 70% compared to existing solutions.

**Link**: [arxiv](http://arxiv.org/abs/2501.08090v1),  [pdf](http://arxiv.org/pdf/2501.08090v1)

**Tags**: cs.DC cs.AI 



### Addressing Hallucinations in Language Models with Knowledge Graph   Embeddings as an Additional Modality
**Authors**: Viktoriia Chekalina, Anton Razzhigaev, Elizaveta Goncharova, Andrey Kuznetsov

**Updated**: 2025-01-14T12:56:34Z

**Summary**: In this paper we present an approach to reduce hallucinations in Large Language Models (LLMs) by incorporating Knowledge Graphs (KGs) as an additional modality. Our method involves transforming input text into a set of KG embeddings and using an adapter to integrate these embeddings into the language model space, without relying on external retrieval processes.   To facilitate this, we created WikiEntities, a dataset containing over 3 million Wikipedia texts annotated with entities from Wikidata and their corresponding embeddings from PyTorch-BigGraph. This dataset serves as a valuable resource for training Entity Linking models and adapting the described method to various LLMs using specialized adapters.   Our method does not require fine-tuning of the language models themselves; instead, we only train the adapter. This ensures that the model's performance on other tasks is not affected. We trained an adapter for the Mistral 7B, LLaMA 2-7B (chat), and LLaMA 3-8B (instruct) models using this dataset and demonstrated that our approach improves performance on the HaluEval, True-False benchmarks and FEVER dataset. The results indicate that incorporating KGs as a new modality can effectively reduce hallucinations and improve the factual accuracy of language models, all without the need for external retrieval.

**Link**: [arxiv](http://arxiv.org/abs/2411.11531v2),  [pdf](http://arxiv.org/pdf/2411.11531v2)

**Tags**: cs.CL cs.AI 



### JsonTuning: Towards Generalizable, Robust, and Controllable Instruction   Tuning
**Authors**: Chang Gao, Wenxuan Zhang, Guizhen Chen, Wai Lam

**Updated**: 2025-01-14T12:55:27Z

**Summary**: Instruction tuning is vital for enhancing the performance of large language models (LLMs), but existing text-to-text methods, referred to as TextTuning, struggle with issues such as generalization, robustness, and controllability due to their lack of explicit task structures. We introduce JsonTuning, a structure-to-structure approach that uses JSON structures to represent tasks. This method improves generalization by clarifying task elements and their relations, boosts robustness by minimizing ambiguity, and enhances controllability by allowing precise control over outputs. We conduct an extensive comparative analysis between JsonTuning and TextTuning using various language models and benchmarks. Our findings reveal that JsonTuning consistently surpasses TextTuning in terms of performance, robustness, and controllability across different scenarios. By overcoming the limitations of TextTuning, JsonTuning demonstrates significant potential for developing more effective and reliable LLMs capable of handling diverse scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2310.02953v4),  [pdf](http://arxiv.org/pdf/2310.02953v4)

**Tags**: cs.CL 



### Spurious Feature Eraser: Stabilizing Test-Time Adaptation for   Vision-Language Foundation Model
**Authors**: Huan Ma, Yan Zhu, Changqing Zhang, Peilin Zhao, Baoyuan Wu, Long-Kai Huang, Qinghua Hu, Bingzhe Wu

**Updated**: 2025-01-14T12:37:26Z

**Summary**: Vision-language foundation models have exhibited remarkable success across a multitude of downstream tasks due to their scalability on extensive image-text paired data. However, these models also display significant limitations when applied to downstream tasks, such as fine-grained image classification, as a result of ``decision shortcuts'' that hinder their generalization capabilities. In this work, we find that the CLIP model possesses a rich set of features, encompassing both \textit{desired invariant causal features} and \textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP on downstream tasks originates from its inability to effectively utilize pre-trained features in accordance with specific task requirements. To address this challenge, we propose a simple yet effective method, Spurious Feature Eraser (SEraser), to alleviate the decision shortcuts by erasing the spurious features. Specifically, we introduce a test-time prompt tuning paradigm that optimizes a learnable prompt, thereby compelling the model to exploit invariant features while disregarding decision shortcuts during the inference phase. The proposed method effectively alleviates excessive dependence on potentially misleading spurious information. We conduct comparative analysis of the proposed method against various approaches which validates the significant superiority.

**Link**: [arxiv](http://arxiv.org/abs/2403.00376v3),  [pdf](http://arxiv.org/pdf/2403.00376v3)

**Tags**: cs.CV cs.AI cs.LG 



### CuAsmRL: Optimizing GPU SASS Schedules via Deep Reinforcement Learning
**Authors**: Guoliang He, Eiko Yoneki

**Updated**: 2025-01-14T12:36:18Z

**Summary**: Large language models (LLMs) are remarked by their substantial computational requirements. To mitigate the cost, researchers develop specialized CUDA kernels, which often fuse several tensor operations to maximize the utilization of GPUs as much as possible. However, those specialized kernels may still leave performance on the table as CUDA assembly experts show that manual optimization of GPU SASS schedules can lead to better performance, and trial-and-error is largely employed to manually find the best GPU SASS schedules.   In this work, we employ an automatic approach to optimize GPU SASS schedules, which thus can be integrated into existing compiler frameworks. The key to automatic optimization is training an RL agent to mimic how human experts perform manual scheduling. To this end, we formulate an assembly game, where RL agents can play to find the best GPU SASS schedules. The assembly game starts from a \textit{-O3} optimized SASS schedule, and the RL agents can iteratively apply actions to mutate the current schedules. Positive rewards are generated if the mutated schedules get higher throughput by executing on GPUs. Experiments show that CuAsmRL can further improve the performance of existing specialized CUDA kernels transparently by up to $26\%$, and on average $9\%$. Moreover, it is used as a tool to reveal potential optimization moves learned automatically.

**Link**: [arxiv](http://arxiv.org/abs/2501.08071v1),  [pdf](http://arxiv.org/pdf/2501.08071v1)

**Tags**: cs.AR cs.LG 



### A Roadmap to Guide the Integration of LLMs in Hierarchical Planning
**Authors**: Israel Puerta-Merino, Carlos Núñez-Molina, Pablo Mesejo, Juan Fernández-Olivares

**Updated**: 2025-01-14T12:34:25Z

**Summary**: Recent advances in Large Language Models (LLMs) are fostering their integration into several reasoning-related fields, including Automated Planning (AP). However, their integration into Hierarchical Planning (HP), a subfield of AP that leverages hierarchical knowledge to enhance planning performance, remains largely unexplored. In this preliminary work, we propose a roadmap to address this gap and harness the potential of LLMs for HP. To this end, we present a taxonomy of integration methods, exploring how LLMs can be utilized within the HP life cycle. Additionally, we provide a benchmark with a standardized dataset for evaluating the performance of future LLM-based HP approaches, and present initial results for a state-of-the-art HP planner and LLM planner. As expected, the latter exhibits limited performance (3\% correct plans, and none with a correct hierarchical decomposition) but serves as a valuable baseline for future approaches.

**Link**: [arxiv](http://arxiv.org/abs/2501.08068v1),  [pdf](http://arxiv.org/pdf/2501.08068v1)

**Tags**: cs.AI 



### Distribution-free uncertainty quantification for inverse problems:   application to weak lensing mass mapping
**Authors**: Hubert Leterme, Jalal Fadili, Jean-Luc Starck

**Updated**: 2025-01-14T12:28:21Z

**Summary**: In inverse problems, distribution-free uncertainty quantification (UQ) aims to obtain error bars with coverage guarantees that are independent of any prior assumptions about the data distribution. In the context of mass mapping, uncertainties could lead to errors that affects our understanding of the underlying mass distribution, or could propagate to cosmological parameter estimation, thereby impacting the precision and reliability of cosmological models. Current surveys, such as Euclid or Rubin, will provide new weak lensing datasets of very high quality. Accurately quantifying uncertainties in mass maps is therefore critical to perform reliable cosmological parameter inference. In this paper, we extend the conformalized quantile regression (CQR) algorithm, initially proposed for scalar regression, to inverse problems. We compare our approach with another distribution-free approach based on risk-controlling prediction sets (RCPS). Both methods are based on a calibration dataset, and offer finite-sample coverage guarantees that are independent of the data distribution. Furthermore, they are applicable to any mass mapping method, including blackbox predictors. In our experiments, we apply UQ on three mass-mapping method: the Kaiser-Squires inversion, iterative Wiener filtering, and the MCALens algorithm. Our experiments reveal that RCPS tends to produce overconservative confidence bounds with small calibration sets, whereas CQR is designed to avoid this issue. Although the expected miscoverage rate is guaranteed to stay below a user-prescribed threshold regardless of the mass mapping method, selecting an appropriate reconstruction algorithm remains crucial for obtaining accurate estimates, especially around peak-like structures, which are particularly important for inferring cosmological parameters. Additionally, the choice of mass mapping method influences the size of the error bars.

**Link**: [arxiv](http://arxiv.org/abs/2410.08831v2),  [pdf](http://arxiv.org/pdf/2410.08831v2)

**Tags**: astro-ph.CO astro-ph.IM stat.ME 



### TreeKV: Smooth Key-Value Cache Compression with Tree Structures
**Authors**: Ziwei He, Jian Yuan, Haoli Bai, Jingwen Leng, Bo Jiang

**Updated**: 2025-01-14T12:06:33Z

**Summary**: Efficient key-value (KV) cache compression is critical for scaling transformer-based Large Language Models (LLMs) in long sequences and resource-limited settings. Existing methods evict tokens based on their positions or importance scores, but position-based strategies can miss crucial information outside predefined regions, while those relying on global importance scores resulting in strong regional biases, limiting the KV cache's overall context retention and potentially impairing the performance of LLMs on complex tasks. Our wavelet analysis reveals that as tokens approach the end of sequence, their contributions to generation gradually increase and tends to diverge more from neighboring tokens, indicating a smooth transition with increasing complexity and variability from distant to nearby context. Motivated by this observation, we propose TreeKV, an intuitive, training-free method that employs a tree structure for smooth cache compression. TreeKV maintains a fixed cache size, allowing LLMs to deliver high-quality output even in long text scenarios. Unlike most compression methods, TreeKV is applicable to both the generation and prefilling stages. TreeKV consistently surpasses all baseline models in language modeling tasks on PG19 and OpenWebText2, allowing LLMs trained with short context window to generalize to longer window with a 16x cache reduction. On the Longbench benchmark, TreeKV achieves the best performance with only 6\% of the budget at optimal efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2501.04987v2),  [pdf](http://arxiv.org/pdf/2501.04987v2)

**Tags**: cs.CL 



### Audio-Agent: Leveraging LLMs For Audio Generation, Editing and   Composition
**Authors**: Zixuan Wang, Chi-Keung Tang, Yu-Wing Tai

**Updated**: 2025-01-14T11:59:03Z

**Summary**: We introduce Audio-Agent, a multimodal framework for audio generation, editing and composition based on text or video inputs. Conventional approaches for text-to-audio (TTA) tasks often make single-pass inferences from text descriptions. While straightforward, this design struggles to produce high-quality audio when given complex text conditions. In our method, we utilize a pre-trained TTA diffusion network as the audio generation agent to work in tandem with GPT-4, which decomposes the text condition into atomic, specific instructions and calls the agent for audio generation. In doing so, Audio-Agent can generate high-quality audio that is closely aligned with the provided text or video exhibiting complex and multiple events, while supporting variable-length and variable-volume generation. For video-to-audio (VTA) tasks, most existing methods require training a timestamp detector to synchronize video events with the generated audio, a process that can be tedious and time-consuming. Instead, we propose a simpler approach by fine-tuning a pre-trained Large Language Model (LLM), e.g., Gemma2-2B-it, to obtain both semantic and temporal conditions that bridge the video and audio modality. Consequently, our framework contributes a comprehensive solution for both TTA and VTA tasks without substantial computational overhead in training.

**Link**: [arxiv](http://arxiv.org/abs/2410.03335v2),  [pdf](http://arxiv.org/pdf/2410.03335v2)

**Tags**: cs.SD cs.CV cs.LG eess.AS 



### PolyLUT: Ultra-low Latency Polynomial Inference with Hardware-Aware   Structured Pruning
**Authors**: Marta Andronic, Jiawen Li, George A. Constantinides

**Updated**: 2025-01-14T11:51:57Z

**Summary**: Standard deep neural network inference involves the computation of interleaved linear maps and nonlinear activation functions. Prior work for ultra-low latency implementations has hardcoded these operations inside FPGA lookup tables (LUTs). However, FPGA LUTs can implement a much greater variety of functions. In this paper, we propose a novel approach to training DNNs for FPGA deployment using multivariate polynomials as the basic building block. Our method takes advantage of the flexibility offered by the soft logic, hiding the polynomial evaluation inside the LUTs with minimal overhead. By using polynomial building blocks, we achieve the same accuracy using considerably fewer layers of soft logic than by using linear functions, leading to significant latency and area improvements. LUT-based implementations also face a significant challenge: the LUT size grows exponentially with the number of inputs. Prior work relies on a priori fixed sparsity, with results heavily dependent on seed selection. To address this, we propose a structured pruning strategy using a bespoke hardware-aware group regularizer that encourages a particular sparsity pattern that leads to a small number of inputs per neuron. We demonstrate the effectiveness of PolyLUT on three tasks: network intrusion detection, jet identification at the CERN Large Hadron Collider, and MNIST.

**Link**: [arxiv](http://arxiv.org/abs/2501.08043v1),  [pdf](http://arxiv.org/pdf/2501.08043v1)

**Tags**: cs.LG cs.AR 



### A Bayesian Approach for Earthquake Impact Modelling
**Authors**: Max Anderson Loake, Hamish Patten, David Steinsaltz

**Updated**: 2025-01-14T11:40:18Z

**Summary**: Immediately following a disaster event, such as an earthquake, estimates of the damage extent play a key role in informing the coordination of response and recovery efforts. We develop a novel impact estimation tool that leverages a generalised Bayesian approach to generate earthquake impact estimates across three impact types: mortality, population displacement, and building damage. Inference is performed within a likelihood-free framework, and a scoring-rule-based posterior avoids information loss from non-sufficient summary statistics. We propose an adaptation of existing scoring-rule-based loss functions that accommodates the use of an approximate Bayesian computation sequential Monte Carlo (ABC-SMC) framework. The fitted model achieves results comparable to those of two leading impact estimation tools in the prediction of total mortality when tested on a set of held-out past events. The proposed method provides four advantages over existing empirical approaches: modelling produces a gridded spatial map of the estimated impact, predictions benefit from the Bayesian quantification and interpretation of uncertainty, there is direct handling of multi-shock earthquake events, and the use of a joint model between impact types allows predictions to be updated as impact observations become available.

**Link**: [arxiv](http://arxiv.org/abs/2412.15791v2),  [pdf](http://arxiv.org/pdf/2412.15791v2)

**Tags**: stat.AP 



### Simple Estimation of Semiparametric Models with Measurement Errors
**Authors**: Kirill S. Evdokimov, Andrei Zeleneev

**Updated**: 2025-01-14T11:35:16Z

**Summary**: We develop a practical way of addressing the Errors-In-Variables (EIV) problem in the Generalized Method of Moments (GMM) framework. We focus on the settings in which the variability of the EIV is a fraction of that of the mismeasured variables, which is typical for empirical applications. For any initial set of moment conditions our approach provides a "corrected" set of moment conditions that are robust to the EIV. We show that the GMM estimator based on these moments is root-n-consistent, with the standard tests and confidence intervals providing valid inference. This is true even when the EIV are so large that naive estimators (that ignore the EIV problem) are heavily biased with their confidence intervals having 0% coverage. Our approach involves no nonparametric estimation, which is especially important for applications with many covariates, and settings with multivariate or non-classical EIV. In particular, the approach makes it easy to use instrumental variables to address EIV in nonlinear models.

**Link**: [arxiv](http://arxiv.org/abs/2306.14311v3),  [pdf](http://arxiv.org/pdf/2306.14311v3)

**Tags**: econ.EM stat.ME 



### EventHallusion: Diagnosing Event Hallucinations in Video LLMs
**Authors**: Jiacheng Zhang, Yang Jiao, Shaoxiang Chen, Na Zhao, Jingjing Chen

**Updated**: 2025-01-14T11:27:28Z

**Summary**: Recently, Multimodal Large Language Models (MLLMs) have made significant progress in the video comprehension field. Despite remarkable content reasoning and instruction following capabilities they demonstrated, the hallucination problem of these VideoLLMs is less explored compared with its counterpart in the image domain. To mitigate this gap, we propose EventHallusion, a novel benchmark that focuses on assessing the VideoLLMs' hallucination toward event, the crux of video analysis. From a hallucination attribution perspective, our EventHallusion benchmark is curated to assess a VideoLLM's susceptibility toward language priors and vision-language biases. On the other hand, we also propose a simple yet effective method, called Temporal Contrastive Decoding (TCD), to tackle the hallucination problems of VideoLLMs. The proposed TCD method rectifies the model's bias toward its priors during the decoding stage by comparing the original video with a modified version, in which temporal cues are disrupted. Through comprehensive evaluation of eight open-source and two closed-source VideoLLMs on the proposed EventHallusion benchmark, we observe that the open-source models suffer significantly from hallucination problems, whereas the closed-source ones perform markedly better. By further equipping open-source VideoLLMs with the proposed TCD approach, evident performance improvements are achieved across most metrics in the EventHallusion benchmark. Our codes and benchmark data are available at https://github.com/Stevetich/EventHallusion.

**Link**: [arxiv](http://arxiv.org/abs/2409.16597v3),  [pdf](http://arxiv.org/pdf/2409.16597v3)

**Tags**: cs.CV 



### Small-Coupling Dynamic Cavity: a Bayesian mean-field framework for   epidemic inference
**Authors**: Alfredo Braunstein, Giovanni Catania, Luca Dall'Asta, Matteo Mariani, Fabio Mazza, Mattia Tarabolo

**Updated**: 2025-01-14T11:06:18Z

**Summary**: We present the Small-Coupling Dynamic Cavity (SCDC) method, a novel generalized mean-field approximation for epidemic inference and risk assessment within a fully Bayesian framework. SCDC accounts for non-causal effects of observations and uses a graphical model representation of epidemic processes to derive self-consistent equations for edge probability marginals. A small-coupling expansion yields time-dependent cavity messages capturing individual infection probabilities and observational conditioning. With linear computational cost per iteration in the epidemic duration, SCDC is particularly efficient and valid even for recurrent epidemic processes, where standard methods are exponentially complex. Tested on synthetic networks, it matches Belief Propagation in accuracy and outperforms individual-based mean-field methods. Notably, despite being derived as a small-infectiousness expansion, SCDC maintains good accuracy even for relatively large infection probabilities. While convergence issues may arise on graphs with long-range correlations, SCDC reliably estimates risk. Future extensions include non-Markovian models and higher-order terms in the dynamic cavity framework.

**Link**: [arxiv](http://arxiv.org/abs/2306.03829v3),  [pdf](http://arxiv.org/pdf/2306.03829v3)

**Tags**: cond-mat.dis-nn cond-mat.stat-mech physics.data-an q-bio.PE 



### MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation
**Authors**: Tianyu Fan, Jingyuan Wang, Xubin Ren, Chao Huang

**Updated**: 2025-01-14T11:03:56Z

**Summary**: The growing demand for efficient and lightweight Retrieval-Augmented Generation (RAG) systems has highlighted significant challenges when deploying Small Language Models (SLMs) in existing RAG frameworks. Current approaches face severe performance degradation due to SLMs' limited semantic understanding and text processing capabilities, creating barriers for widespread adoption in resource-constrained scenarios. To address these fundamental limitations, we present MiniRAG, a novel RAG system designed for extreme simplicity and efficiency. MiniRAG introduces two key technical innovations: (1) a semantic-aware heterogeneous graph indexing mechanism that combines text chunks and named entities in a unified structure, reducing reliance on complex semantic understanding, and (2) a lightweight topology-enhanced retrieval approach that leverages graph structures for efficient knowledge discovery without requiring advanced language capabilities. Our extensive experiments demonstrate that MiniRAG achieves comparable performance to LLM-based methods even when using SLMs while requiring only 25\% of the storage space. Additionally, we contribute a comprehensive benchmark dataset for evaluating lightweight RAG systems under realistic on-device scenarios with complex queries. We fully open-source our implementation and datasets at: https://github.com/HKUDS/MiniRAG.

**Link**: [arxiv](http://arxiv.org/abs/2501.06713v2),  [pdf](http://arxiv.org/pdf/2501.06713v2)

**Tags**: cs.AI 



### Tutorial: VAE as an inference paradigm for neuroimaging
**Authors**: C. Vázquez-García, F. J. Martínez-Murcia, F. Segovia Román, Juan M. Górriz Sáez

**Updated**: 2025-01-14T10:54:36Z

**Summary**: In this tutorial, we explore Variational Autoencoders (VAEs), an essential framework for unsupervised learning, particularly suited for high-dimensional datasets such as neuroimaging. By integrating deep learning with Bayesian inference, VAEs enable the generation of interpretable latent representations. This tutorial outlines the theoretical foundations of VAEs, addresses practical challenges such as convergence issues and over-fitting, and discusses strategies like the reparameterization trick and hyperparameter optimization. We also highlight key applications of VAEs in neuroimaging, demonstrating their potential to uncover meaningful patterns, including those associated with neurodegenerative processes, and their broader implications for analyzing complex brain data.

**Link**: [arxiv](http://arxiv.org/abs/2501.08009v1),  [pdf](http://arxiv.org/pdf/2501.08009v1)

**Tags**: eess.IV cs.AI 



### Bayesian Nonparametric Inference in McKean-Vlasov models
**Authors**: Richard Nickl, Grigorios A. Pavliotis, Kolyan Ray

**Updated**: 2025-01-14T10:52:27Z

**Summary**: We consider nonparametric statistical inference on a periodic interaction potential $W$ from noisy discrete space-time measurements of solutions $\rho=\rho_W$ of the nonlinear McKean-Vlasov equation, describing the probability density of the mean field limit of an interacting particle system. We show how Gaussian process priors assigned to $W$ give rise to posterior mean estimators that exhibit fast convergence rates for the implied estimated densities $\bar \rho$ towards $\rho_W$. We further show that if the initial condition $\phi$ is not too smooth and satisfies a standard deconvolvability condition, then one can consistently infer Sobolev-regular potentials $W$ at convergence rates $N^{-\theta}$ for appropriate $\theta>0$, where $N$ is the number of measurements. The exponent $\theta$ can be taken to approach $1/2$ as the regularity of $W$ increases corresponding to `near-parametric' models.

**Link**: [arxiv](http://arxiv.org/abs/2404.16742v3),  [pdf](http://arxiv.org/pdf/2404.16742v3)

**Tags**: math.ST cs.NA math.AP math.NA stat.TH 



### Transformers and Large Language Models for Efficient Intrusion Detection   Systems: A Comprehensive Survey
**Authors**: Hamza Kheddar

**Updated**: 2025-01-14T10:52:15Z

**Summary**: With significant advancements in Transformers LLMs, NLP has extended its reach into many research fields due to its enhanced capabilities in text generation and user interaction. One field benefiting greatly from these advancements is cybersecurity. In cybersecurity, many parameters that need to be protected and exchanged between senders and receivers are in the form of text and tabular data, making NLP a valuable tool in enhancing the security measures of communication protocols. This survey paper provides a comprehensive analysis of the utilization of Transformers and LLMs in cyber-threat detection systems. The methodology of paper selection and bibliometric analysis is outlined to establish a rigorous framework for evaluating existing research. The fundamentals of Transformers are discussed, including background information on various cyber-attacks and datasets commonly used in this field. The survey explores the application of Transformers in IDSs, focusing on different architectures such as Attention-based models, LLMs like BERT and GPT, CNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others. Furthermore, it explores the diverse environments and applications where Transformers and LLMs-based IDS have been implemented, including computer networks, IoT devices, critical infrastructure protection, cloud computing, SDN, as well as in autonomous vehicles. The paper also addresses research challenges and future directions in this area, identifying key issues such as interpretability, scalability, and adaptability to evolving threats, and more. Finally, the conclusion summarizes the findings and highlights the significance of Transformers and LLMs in enhancing cyber-threat detection capabilities, while also outlining potential avenues for further research and development.

**Link**: [arxiv](http://arxiv.org/abs/2408.07583v2),  [pdf](http://arxiv.org/pdf/2408.07583v2)

**Tags**: cs.CR cs.AI cs.CL cs.CV eess.AS 



### TriAdaptLoRA: Brain-Inspired Triangular Adaptive Low-Rank Adaptation for   Parameter-Efficient Fine-Tuning
**Authors**: Yao Liang, Yuwei Wang, Yi Zeng

**Updated**: 2025-01-14T10:51:31Z

**Summary**: The fine-tuning of Large Language Models (LLMs) is pivotal for achieving optimal performance across diverse downstream tasks. However, while full fine-tuning delivers superior results, it entails significant computational and resource costs. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA, address these challenges by reducing the number of trainable parameters, but they often struggle with rank adjustment efficiency and task-specific adaptability. We propose Triangular Adaptive Low-Rank Adaptation (TriAdaptLoRA), a novel PEFT framework inspired by neuroscience principles, which dynamically optimizes the allocation of trainable parameters. TriAdaptLoRA introduces three key innovations: 1) a triangular split of transformation matrices into lower and upper triangular components to maximize parameter utilization, 2) a parameter importance metric based on normalized Frobenius norms for efficient adaptation, and 3) an adaptive rank-growth strategy governed by dynamic thresholds, allowing flexible parameter allocation across training steps. Experiments conducted on a variety of natural language understanding and generation tasks demonstrate that TriAdaptLoRA consistently outperforms existing PEFT methods. It achieves superior performance, enhanced stability, and reduced computational overhead, particularly under linear threshold-driven rank growth. These results highlight its efficacy as a scalable and resource-efficient solution for fine-tuning LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2501.08008v1),  [pdf](http://arxiv.org/pdf/2501.08008v1)

**Tags**: cs.CL cs.AI 



### DisCoPatch: Batch Statistics Are All You Need For OOD Detection, But   Only If You Can Trust Them
**Authors**: Francisco Caetano, Christiaan Viviers, Luis A. Zavala-Mondragón, Peter H. N. de With, Fons van der Sommen

**Updated**: 2025-01-14T10:49:26Z

**Summary**: Out-of-distribution (OOD) detection holds significant importance across many applications. While semantic and domain-shift OOD problems are well-studied, this work focuses on covariate shifts - subtle variations in the data distribution that can degrade machine learning performance. We hypothesize that detecting these subtle shifts can improve our understanding of in-distribution boundaries, ultimately improving OOD detection. In adversarial discriminators trained with Batch Normalization (BN), real and adversarial samples form distinct domains with unique batch statistics - a property we exploit for OOD detection. We introduce DisCoPatch, an unsupervised Adversarial Variational Autoencoder (VAE) framework that harnesses this mechanism. During inference, batches consist of patches from the same image, ensuring a consistent data distribution that allows the model to rely on batch statistics. DisCoPatch uses the VAE's suboptimal outputs (generated and reconstructed) as negative samples to train the discriminator, thereby improving its ability to delineate the boundary between in-distribution samples and covariate shifts. By tightening this boundary, DisCoPatch achieves state-of-the-art results in public OOD detection benchmarks. The proposed model not only excels in detecting covariate shifts, achieving 95.5% AUROC on ImageNet-1K(-C) but also outperforms all prior methods on public Near-OOD (95.0%) benchmarks. With a compact model size of 25MB, it achieves high OOD detection performance at notably lower latency than existing methods, making it an efficient and practical solution for real-world OOD detection applications. The code will be made publicly available

**Link**: [arxiv](http://arxiv.org/abs/2501.08005v1),  [pdf](http://arxiv.org/pdf/2501.08005v1)

**Tags**: cs.CV cs.AI eess.IV 



### Assessing the size of spatial extreme events using local coefficients   based on excursion sets
**Authors**: Ryan Cotsakis, Elena Di Bernardino, Thomas Opitz

**Updated**: 2025-01-14T10:43:44Z

**Summary**: Extreme events arising in georeferenced processes can take various forms, such as occurring in isolated patches or stretching contiguously over large areas, and can further vary with the spatial location and the extremeness of the events. We use excursion sets above threshold exceedances in data observed over a two-dimensional grid of rectangular pixels to propose a general family of coefficients that assess spatial-extent properties relevant for risk assessment, and study five candidate coefficients from this family. These coefficients are defined locally and interpreted as a spatial distance from a reference site where the threshold is exceeded. We develop statistical inference and discuss robustness to boundary effects and resolution of the pixel grid. To statistically extrapolate coefficients towards very high threshold levels, we formulate a semiparametric model and estimate a parameter characterizing how coefficients scale with the quantile level of the threshold. The utility of the new coefficients is illustrated through simulated data, as well as in an application to gridded daily temperature in continental France. We find notable differences in estimated coefficient maps between climate model simulations and observation-based reanalysis.

**Link**: [arxiv](http://arxiv.org/abs/2310.09075v2),  [pdf](http://arxiv.org/pdf/2310.09075v2)

**Tags**: math.ST stat.TH 



### LLM-Ehnanced Holonic Architecture for Ad-Hoc Scalable SoS
**Authors**: Muhammad Ashfaq, Ahmed R. Sadik, Tommi Mikkonen, Muhammad Waseem, Niko Mäkitalo

**Updated**: 2025-01-14T10:35:54Z

**Summary**: As modern system of systems (SoS) become increasingly adaptive and human centred, traditional architectures often struggle to support interoperability, reconfigurability, and effective human system interaction. This paper addresses these challenges by advancing the state of the art holonic architecture for SoS, offering two main contributions to support these adaptive needs. First, we propose a layered architecture for holons, which includes reasoning, communication, and capabilities layers. This design facilitates seamless interoperability among heterogeneous constituent systems by improving data exchange and integration. Second, inspired by principles of intelligent manufacturing, we introduce specialised holons namely, supervisor, planner, task, and resource holons aimed at enhancing the adaptability and reconfigurability of SoS. These specialised holons utilise large language models within their reasoning layers to support decision making and ensure real time adaptability. We demonstrate our approach through a 3D mobility case study focused on smart city transportation, showcasing its potential for managing complex, multimodal SoS environments. Additionally, we propose evaluation methods to assess the architecture efficiency and scalability,laying the groundwork for future empirical validations through simulations and real world implementations.

**Link**: [arxiv](http://arxiv.org/abs/2501.07992v1),  [pdf](http://arxiv.org/pdf/2501.07992v1)

**Tags**: cs.AI cs.ET cs.MA cs.SE 



### Mapping reionization bubbles in the JWST era II: inferring the position   and characteristic size of individual bubbles
**Authors**: Ivan Nikolić, Andrei Mesinger, Charlotte A. Mason, Ting-Yi Lu, Mengtao Tang, David Prelogović, Samuel Gagnon-Hartman, Daniel P. Stark

**Updated**: 2025-01-14T09:58:47Z

**Summary**: The James Webb Space Telescope (JWST) is discovering an increasing number of galaxies well into the early stages of the Epoch of Reionization (EoR). Many of these galaxies are clustered with strong Lyman alpha (Ly$\alpha$) emission, motivating the presence of surrounding cosmic HII regions that would facilitate Ly$\alpha$ transmission through the intergalactic medium (IGM). Detecting these HII "bubbles" would allow us to connect their growth to the properties of the galaxies inside them. Here we develop a new forward-modeling framework to estimate the local HII region size and location from Ly$\alpha$ spectra of galaxy groups in the early stages of the EoR. Our model takes advantage of the complementary information provided by neighboring sightlines through the IGM. Our forward models sample the main sources of uncertainty, including: (i) the global neutral fraction; (ii) EoR morphology; (iii) emergent Ly$\alpha$ emission; and (iv) NIRSpec instrument noise. Depending on the availability of complementary nebular lines, $\sim$ 0.006 $\unicode{x2013}$ 0.01 galaxies per cMpc$^3$, are required to be $\gtrsim$95\% confident that the HII bubble location and size recovered by our method is accurate to within $\sim$ 1 comoving Mpc. This corresponds roughly to tens of galaxies at $z\sim7\unicode{x2013}8$ in $\sim$2x2 tiled pointing with JWST NIRSpec. Such a sample is achievable with a targeted survey with completeness down to $M_{\rm UV}^{\rm min}\lesssim$ -19 $\unicode{x2013}$ -17, depending on the over-density of the field. We test our method on 3D EoR simulations as well as misspecified equivalent width distributions, in both cases accurately recovering the HII region surrounding targeted galaxy groups.

**Link**: [arxiv](http://arxiv.org/abs/2501.07980v1),  [pdf](http://arxiv.org/pdf/2501.07980v1)

**Tags**: astro-ph.GA astro-ph.CO 



### Private Collaborative Edge Inference via Over-the-Air Computation
**Authors**: Selim F. Yilmaz, Burak Hasircioglu, Li Qiao, Deniz Gunduz

**Updated**: 2025-01-14T09:58:24Z

**Summary**: We consider collaborative inference at the wireless edge, where each client's model is trained independently on its local dataset. Clients are queried in parallel to make an accurate decision collaboratively. In addition to maximizing the inference accuracy, we also want to ensure the privacy of local models. To this end, we leverage the superposition property of the multiple access channel to implement bandwidth-efficient multi-user inference methods. We propose different methods for ensemble and multi-view classification that exploit over-the-air computation (OAC). We show that these schemes perform better than their orthogonal counterparts with statistically significant differences while using fewer resources and providing privacy guarantees. We also provide experimental results verifying the benefits of the proposed OAC approach to multi-user inference, and perform an ablation study to demonstrate the effectiveness of our design choices. We share the source code of the framework publicly on Github to facilitate further research and reproducibility.

**Link**: [arxiv](http://arxiv.org/abs/2407.21151v2),  [pdf](http://arxiv.org/pdf/2407.21151v2)

**Tags**: cs.LG cs.AI cs.CR cs.IT math.IT 



### One Language, Many Gaps: Evaluating Dialect Fairness and Robustness of   Large Language Models in Reasoning Tasks
**Authors**: Fangru Lin, Shaoguang Mao, Emanuele La Malfa, Valentin Hofmann, Adrian de Wynter, Xun Wang, Si-Qing Chen, Michael Wooldridge, Janet B. Pierrehumbert, Furu Wei

**Updated**: 2025-01-14T09:52:50Z

**Summary**: Language is not monolithic. While benchmarks, including those designed for multiple languages, are often used as proxies to evaluate the performance of Large Language Models (LLMs), they tend to overlook the nuances of within-language variation, and thus fail to model the experience of speakers of non-standard dialects. Focusing on African American Vernacular English (AAVE), we present the first study aimed at objectively assessing the fairness and robustness of LLMs in handling dialects in canonical reasoning tasks, including algorithm, math, logic, and integrated reasoning. We introduce \textbf{ReDial} (\textbf{Re}asoning with \textbf{Dial}ect Queries), a benchmark containing 1.2K+ parallel query pairs in Standardized English and AAVE. We hire AAVE speakers, including experts with computer science backgrounds, to rewrite seven popular benchmarks, such as HumanEval and GSM8K. With ReDial, we evaluate widely used LLMs, including GPT, Claude, Llama, Mistral, and the Phi model families. Our findings reveal that \textbf{almost all of these widely used models show significant brittleness and unfairness to queries in AAVE}. Our work establishes a systematic and objective framework for analyzing LLM bias in dialectal queries. Moreover, it highlights how mainstream LLMs provide unfair service to dialect speakers in reasoning tasks, laying a critical foundation for relevant future research. Code and data can be accessed at https://github.com/fangru-lin/redial_dialect_robustness_fairness.

**Link**: [arxiv](http://arxiv.org/abs/2410.11005v2),  [pdf](http://arxiv.org/pdf/2410.11005v2)

**Tags**: cs.CL cs.LG 



### Some observations on the ambivalent role of symmetries in Bayesian   inference problems
**Authors**: Guilhem Semerjian

**Updated**: 2025-01-14T09:48:48Z

**Summary**: We collect in this note some observations on the role of symmetries in Bayesian inference problems, that can be useful or detrimental depending on the way they act on the signal and on the observations. We emphasize in particular the need to gauge away unobservable invariances in the definition of a distance between a signal and its estimator, and the consequences this implies for the statistical mechanics treatment of such models, taking as a motivating example the extensive rank matrix factorization problem.

**Link**: [arxiv](http://arxiv.org/abs/2501.07975v1),  [pdf](http://arxiv.org/pdf/2501.07975v1)

**Tags**: cond-mat.dis-nn cond-mat.stat-mech cs.IT math.IT math.PR math.ST stat.TH 



### Decentralized Learning with Approximate Finite-Time Consensus
**Authors**: Aaron Fainman, Stefan Vlaski

**Updated**: 2025-01-14T09:37:12Z

**Summary**: The performance of algorithms for decentralized optimization is affected by both the optimization error and the consensus error, the latter of which arises from the variation between agents' local models. Classically, algorithms employ averaging and gradient-tracking mechanisms with constant combination matrices to drive the collection of agents to consensus. Recent works have demonstrated that using sequences of combination matrices that achieve finite-time consensus (FTC) can result in improved communication efficiency or iteration complexity for decentralized optimization. Notably, these studies apply to highly structured networks, where exact finite-time consensus sequences are known exactly and in closed form. In this work we investigate the impact of utilizing approximate FTC matrices in decentralized learning algorithms, and quantify the impact of the approximation error on convergence rate and steady-state performance. Approximate FTC matrices can be inferred for general graphs and do not rely on a particular graph structure or prior knowledge, making the proposed scheme applicable to a broad range of decentralized learning settings.

**Link**: [arxiv](http://arxiv.org/abs/2501.07967v1),  [pdf](http://arxiv.org/pdf/2501.07967v1)

**Tags**: eess.SP 



### Derivation of Output Correlation Inferences for Multi-Output (aka   Multi-Task) Gaussian Process
**Authors**: Shuhei Watanabe

**Updated**: 2025-01-14T09:35:49Z

**Summary**: Gaussian process (GP) is arguably one of the most widely used machine learning algorithms in practice. One of its prominent applications is Bayesian optimization (BO). Although the vanilla GP itself is already a powerful tool for BO, it is often beneficial to be able to consider the dependencies of multiple outputs. To do so, Multi-task GP (MTGP) is formulated, but it is not trivial to fully understand the derivations of its formulations and their gradients from the previous literature. This paper serves friendly derivations of the MTGP formulations and their gradients.

**Link**: [arxiv](http://arxiv.org/abs/2501.07964v1),  [pdf](http://arxiv.org/pdf/2501.07964v1)

**Tags**: cs.LG cs.AI stat.ML 



### Self-Instruct Few-Shot Jailbreaking: Decompose the Attack into Pattern   and Behavior Learning
**Authors**: Jiaqi Hua, Wanxu Wei

**Updated**: 2025-01-14T09:23:30Z

**Summary**: Recently, several works have been conducted on jailbreaking Large Language Models (LLMs) with few-shot malicious demos. In particular, Zheng et al. (2024) focuses on improving the efficiency of Few-Shot Jailbreaking (FSJ) by injecting special tokens into the demos and employing demo-level random search. Nevertheless, this method lacks generality since it specifies the instruction-response structure. Moreover, the reason why inserting special tokens takes effect in inducing harmful behaviors is only empirically discussed. In this paper, we take a deeper insight into the mechanism of special token injection and propose Self-Instruct Few-Shot Jailbreaking (Self-Instruct-FSJ) facilitated with the demo-level greedy search. This framework decomposes the FSJ attack into pattern and behavior learning to exploit the model's vulnerabilities in a more generalized and efficient way. We conduct elaborate experiments to evaluate our method on common open-source models and compare it with baseline algorithms. Our code is available at https://github.com/iphosi/Self-Instruct-FSJ.

**Link**: [arxiv](http://arxiv.org/abs/2501.07959v1),  [pdf](http://arxiv.org/pdf/2501.07959v1)

**Tags**: cs.AI 



### Spiking Neural Network Accelerator Architecture for Differential-Time   Representation using Learned Encoding
**Authors**: Daniel Windhager, Lothar Ratschbacher, Bernhard A. Moser, Michael Lunglmayr

**Updated**: 2025-01-14T09:09:08Z

**Summary**: Spiking Neural Networks (SNNs) have garnered attention over recent years due to their increased energy efficiency and advantages in terms of operational complexity compared to traditional Artificial Neural Networks (ANNs). Two important questions when implementing SNNs are how to best encode existing data into spike trains and how to efficiently process these spike trains in hardware. This paper addresses both of these problems by incorporating the encoding into the learning process, thus allowing the network to learn the spike encoding alongside the weights. Furthermore, this paper proposes a hardware architecture based on a recently introduced differential-time representation for spike trains allowing decoupling of spike time and processing time. Together these contributions lead to a feedforward SNN using only Leaky-Integrate and Fire (LIF) neurons that surpasses 99% accuracy on the MNIST dataset while still being implementable on medium-sized FPGAs with inference times of less than 295us.

**Link**: [arxiv](http://arxiv.org/abs/2501.07952v1),  [pdf](http://arxiv.org/pdf/2501.07952v1)

**Tags**: cs.NE eess.SP 



### One cut-point phase-type distributions in Reliability. An application to   Resistive Random Access Memories
**Authors**: Christian Acal, Juan Eloy Ruiz-Castro, David Maldonado, Juan B. Roldán

**Updated**: 2025-01-14T09:05:59Z

**Summary**: A new probability distribution to study lifetime data in reliability is introduced in this paper. This one is a first approach to a non-homogeneous phase-type distribution. It is built by considering one cut-point in the non-negative semi-line of a phase-type distribution. The density function is defined and the main measures associated, such as the reliability function, hazard rate, cumulative hazard rate and the characteristic function are also worked out. This new class of distributions enables to decrease the number of parameter in the estimate when inference is considered. Besides, the likelihood distribution is built to estimate the model parameters by maximum likelihood. Several applications by considering Resistive Random Access Memories compare the adjustment when phase type distributions and one cut-point phase-type distributions are considered. The developed methodology has been computationally implemented in R-cran.

**Link**: [arxiv](http://arxiv.org/abs/2501.07949v1),  [pdf](http://arxiv.org/pdf/2501.07949v1)

**Tags**: stat.ME 



### DehazeGS: Seeing Through Fog with 3D Gaussian Splatting
**Authors**: Jinze Yu, Yiqun Wang, Zhengda Lu, Jianwei Guo, Yong Li, Hongxing Qin, Xiaopeng Zhang

**Updated**: 2025-01-14T08:52:51Z

**Summary**: Current novel view synthesis tasks primarily rely on high-quality and clear images. However, in foggy scenes, scattering and attenuation can significantly degrade the reconstruction and rendering quality. Although NeRF-based dehazing reconstruction algorithms have been developed, their use of deep fully connected neural networks and per-ray sampling strategies leads to high computational costs. Moreover, NeRF's implicit representation struggles to recover fine details from hazy scenes. In contrast, recent advancements in 3D Gaussian Splatting achieve high-quality 3D scene reconstruction by explicitly modeling point clouds into 3D Gaussians. In this paper, we propose leveraging the explicit Gaussian representation to explain the foggy image formation process through a physically accurate forward rendering process. We introduce DehazeGS, a method capable of decomposing and rendering a fog-free background from participating media using only muti-view foggy images as input. We model the transmission within each Gaussian distribution to simulate the formation of fog. During this process, we jointly learn the atmospheric light and scattering coefficient while optimizing the Gaussian representation of the hazy scene. In the inference stage, we eliminate the effects of scattering and attenuation on the Gaussians and directly project them onto a 2D plane to obtain a clear view. Experiments on both synthetic and real-world foggy datasets demonstrate that DehazeGS achieves state-of-the-art performance in terms of both rendering quality and computational efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2501.03659v2),  [pdf](http://arxiv.org/pdf/2501.03659v2)

**Tags**: cs.CV 



### Gandalf the Red: Adaptive Security for LLMs
**Authors**: Niklas Pfister, Václav Volhejn, Manuel Knott, Santiago Arias, Julia Bazińska, Mykhailo Bichurin, Alan Commike, Janet Darling, Peter Dienes, Matthew Fiedler, David Haber, Matthias Kraft, Marco Lancini, Max Mathys, Damián Pascual-Ortiz, Jakub Podolak, Adrià Romero-López, Kyriacos Shiarlis, Andreas Signer, Zsolt Terek, Athanasios Theocharis, Daniel Timbrell, Samuel Trautwein, Samuel Watts, Natalie Wu, Mateo Rojas-Carulla

**Updated**: 2025-01-14T08:30:49Z

**Summary**: Current evaluations of defenses against prompt attacks in large language model (LLM) applications often overlook two critical factors: the dynamic nature of adversarial behavior and the usability penalties imposed on legitimate users by restrictive defenses. We propose D-SEC (Dynamic Security Utility Threat Model), which explicitly separates attackers from legitimate users, models multi-step interactions, and rigorously expresses the security-utility in an optimizable form. We further address the shortcomings in existing evaluations by introducing Gandalf, a crowd-sourced, gamified red-teaming platform designed to generate realistic, adaptive attack datasets. Using Gandalf, we collect and release a dataset of 279k prompt attacks. Complemented by benign user data, our analysis reveals the interplay between security and utility, showing that defenses integrated in the LLM (e.g., system prompts) can degrade usability even without blocking requests. We demonstrate that restricted application domains, defense-in-depth, and adaptive defenses are effective strategies for building secure and useful LLM applications. Code is available at \href{https://github.com/lakeraai/dsec-gandalf}{\texttt{https://github.com/lakeraai/dsec-gandalf}}.

**Link**: [arxiv](http://arxiv.org/abs/2501.07927v1),  [pdf](http://arxiv.org/pdf/2501.07927v1)

**Tags**: cs.LG cs.AI cs.CL cs.CR 



### Phase of Flight Classification in Aviation Safety using LSTM, GRU, and   BiLSTM: A Case Study with ASN Dataset
**Authors**: Aziida Nanyonga, Hassan Wasswa, Graham Wild

**Updated**: 2025-01-14T08:26:58Z

**Summary**: Safety is the main concern in the aviation industry, where even minor operational issues can lead to serious consequences. This study addresses the need for comprehensive aviation accident analysis by leveraging natural language processing (NLP) and advanced AI models to classify the phase of flight from unstructured aviation accident analysis narratives. The research aims to determine whether the phase of flight can be inferred from narratives of post-accident events using NLP techniques. The classification performance of various deep learning models was evaluated. For single RNN-based models, LSTM achieved an accuracy of 63%, precision 60%, and recall 61%. BiLSTM recorded an accuracy of 64%, precision 63%, and a recall of 64%. GRU exhibited balanced performance with an accuracy and recall of 60% and a precision of 63%. Joint RNN-based models further enhanced predictive capabilities. GRU-LSTM, LSTM-BiLSTM, and GRU-BiLSTM demonstrated accuracy rates of 62%, 67%, and 60%, respectively, showcasing the benefits of combining these architectures. To provide a comprehensive overview of model performance, single and combined models were compared in terms of the various metrics. These results underscore the models' capacity to classify the phase of flight from raw text narratives, equipping aviation industry stakeholders with valuable insights for proactive decision-making. Therefore, this research signifies a substantial advancement in the application of NLP and deep learning models to enhance aviation safety.

**Link**: [arxiv](http://arxiv.org/abs/2501.07925v1),  [pdf](http://arxiv.org/pdf/2501.07925v1)

**Tags**: cs.LG 



### Bayesian inference of mixed Gaussian phylogenetic models
**Authors**: Bayu Brahmantio, Krzysztof Bartoszek, Etka Yapar

**Updated**: 2025-01-14T08:23:37Z

**Summary**: Background: Continuous traits evolution of a group of taxa that are correlated through a phylogenetic tree is commonly modelled using parametric stochastic differential equations to represent deterministic change of trait through time, while incorporating noises that represent different unobservable evolutionary pressures. Often times, a heterogeneous Gaussian process that consists of multiple parametric sub-processes is often used when the observed data come from a very diverse set of taxa. In the maximum likelihood setting, challenges can be found when exploring the involved likelihood surface and when interpreting the uncertainty around the parameters.   Results: We extend the methods to tackle inference problems for mixed Gaussian phylogenetic models (MGPMs) by implementing a Bayesian scheme that can take into account biologically relevant priors. The posterior inference method is based on the Population Monte Carlo (PMC) algorithm that are easily parallelized, and using an efficient algorithm to calculate the likelihood of phylogenetically correlated observations. A model evaluation method that is based on the proximity of the posterior predictive distribution to the observed data is also implemented. Simulation study is done to test the inference and evaluation capability of the method. Finally, we test our method on a real-world dataset.   Conclusion: We implement the method in the R package bgphy, available at github.com/bayubeta/bgphy. Simulation study demonstrates that the method is able to infer parameters and evaluate models properly, while its implementation on the real-world dataset indicates that a carefully selected model of evolution based on naturally occurring classifications results in a better fit to the observed data.

**Link**: [arxiv](http://arxiv.org/abs/2410.11548v2),  [pdf](http://arxiv.org/pdf/2410.11548v2)

**Tags**: q-bio.PE 



### Edicho: Consistent Image Editing in the Wild
**Authors**: Qingyan Bai, Hao Ouyang, Yinghao Xu, Qiuyu Wang, Ceyuan Yang, Ka Leong Cheng, Yujun Shen, Qifeng Chen

**Updated**: 2025-01-14T08:23:30Z

**Summary**: As a verified need, consistent editing across in-the-wild images remains a technical challenge arising from various unmanageable factors, like object poses, lighting conditions, and photography environments. Edicho steps in with a training-free solution based on diffusion models, featuring a fundamental design principle of using explicit image correspondence to direct editing. Specifically, the key components include an attention manipulation module and a carefully refined classifier-free guidance (CFG) denoising strategy, both of which take into account the pre-estimated correspondence. Such an inference-time algorithm enjoys a plug-and-play nature and is compatible to most diffusion-based editing methods, such as ControlNet and BrushNet. Extensive results demonstrate the efficacy of Edicho in consistent cross-image editing under diverse settings. We will release the code to facilitate future studies.

**Link**: [arxiv](http://arxiv.org/abs/2412.21079v3),  [pdf](http://arxiv.org/pdf/2412.21079v3)

**Tags**: cs.CV 



### Large Language Model Interface for Home Energy Management Systems
**Authors**: François Michelon, Yihong Zhou, Thomas Morstyn

**Updated**: 2025-01-14T08:10:43Z

**Summary**: Home Energy Management Systems (HEMSs) help households tailor their electricity usage based on power system signals such as energy prices. This technology helps to reduce energy bills and offers greater demand-side flexibility that supports the power system stability. However, residents who lack a technical background may find it difficult to use HEMSs effectively, because HEMSs require well-formatted parameterization that reflects the characteristics of the energy resources, houses, and users' needs. Recently, Large-Language Models (LLMs) have demonstrated an outstanding ability in language understanding. Motivated by this, we propose an LLM-based interface that interacts with users to understand and parameterize their ``badly-formatted answers'', and then outputs well-formatted parameters to implement an HEMS. We further use Reason and Act method (ReAct) and few-shot prompting to enhance the LLM performance. Evaluating the interface performance requires multiple user--LLM interactions. To avoid the efforts in finding volunteer users and reduce the evaluation time, we additionally propose a method that uses another LLM to simulate users with varying expertise, ranging from knowledgeable to non-technical. By comprehensive evaluation, the proposed LLM-based HEMS interface achieves an average parameter retrieval accuracy of 88\%, outperforming benchmark models without ReAct and/or few-shot prompting.

**Link**: [arxiv](http://arxiv.org/abs/2501.07919v1),  [pdf](http://arxiv.org/pdf/2501.07919v1)

**Tags**: cs.AI F.2.2; I.2.7 



### UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts
**Authors**: Bo Yang, Qingping Yang, Yingwei Ma, Runtao Liu

**Updated**: 2025-01-14T07:57:26Z

**Summary**: The evaluation of mathematical reasoning capabilities is essential for advancing Artificial General Intelligence (AGI). While Large Language Models (LLMs) have shown impressive performance in solving mathematical problems, existing benchmarks such as GSM8K and MATH present limitations, including narrow problem definitions with specific numbers and reliance on predetermined rules that hinder accurate assessments of reasoning and generality. This paper introduces the UTMath Benchmark, a robust evaluation framework designed to assess LLMs through extensive unit tests, with a focus on both the accuracy and generality of model responses. It comprises 1,053 cutting-edge problems spanning nine mathematical domains, with an average of 68 test cases per problem. UTMath is highly challenging, with the best-performing model, o1-mini, solving only 32.57\% of the problems, followed by o1-preview at 27.16\%, and GPT-4o at 26.93\%. Furthermore, we present the Reasoning-to-Coding of Thoughts (RCoT) approach, which encourages LLMs to engage in explicit reasoning prior to code generation, thereby facilitating the production of more sophisticated solutions and enhancing overall performance and efficiency. Additionally, we also release the UTMath-Train training dataset (more than 70k samples), to support the community in further exploring mathematical reasoning. Our benchmark can be accessed via the following link: https://github.com/UTMathGroup/UTMath

**Link**: [arxiv](http://arxiv.org/abs/2411.07240v2),  [pdf](http://arxiv.org/pdf/2411.07240v2)

**Tags**: cs.CL cs.AI 



### Logarithmic Memory Networks (LMNs): Efficient Long-Range Sequence   Modeling for Resource-Constrained Environments
**Authors**: Mohamed A. Taha

**Updated**: 2025-01-14T07:50:09Z

**Summary**: Long-range sequence modeling is a crucial aspect of natural language processing and time series analysis. However, traditional models like Recurrent Neural Networks (RNNs) and Transformers suffer from computational and memory inefficiencies, especially when dealing with long sequences. This paper introduces Logarithmic Memory Networks (LMNs), a novel architecture that leverages a hierarchical logarithmic tree structure to efficiently store and retrieve past information. LMNs dynamically summarize historical context, significantly reducing the memory footprint and computational complexity of attention mechanisms from O(n2) to O(log(n)). The model employs a single-vector, targeted attention mechanism to access stored information, and the memory block construction worker (summarizer) layer operates in two modes: a parallel execution mode during training for efficient processing of hierarchical tree structures and a sequential execution mode during inference, which acts as a memory management system. It also implicitly encodes positional information, eliminating the need for explicit positional encodings. These features make LMNs a robust and scalable solution for processing long-range sequences in resource-constrained environments, offering practical improvements in efficiency and scalability. The code is publicly available under the MIT License on GitHub: https://github.com/AhmedBoin/LogarithmicMemory.

**Link**: [arxiv](http://arxiv.org/abs/2501.07905v1),  [pdf](http://arxiv.org/pdf/2501.07905v1)

**Tags**: cs.AI cs.LG 



### What type of inference is planning?
**Authors**: Miguel Lázaro-Gredilla, Li Yang Ku, Kevin P. Murphy, Dileep George

**Updated**: 2025-01-14T07:33:19Z

**Summary**: Multiple types of inference are available for probabilistic graphical models, e.g., marginal, maximum-a-posteriori, and even marginal maximum-a-posteriori. Which one do researchers mean when they talk about "planning as inference"? There is no consistency in the literature, different types are used, and their ability to do planning is further entangled with specific approximations or additional constraints. In this work we use the variational framework to show that, just like all commonly used types of inference correspond to different weightings of the entropy terms in the variational problem, planning corresponds exactly to a different set of weights. This means that all the tricks of variational inference are readily applicable to planning. We develop an analogue of loopy belief propagation that allows us to perform approximate planning in factored-state Markov decisions processes without incurring intractability due to the exponentially large state space. The variational perspective shows that the previous types of inference for planning are only adequate in environments with low stochasticity, and allows us to characterize each type by its own merits, disentangling the type of inference from the additional approximations that its practical use requires. We validate these results empirically on synthetic MDPs and tasks posed in the International Planning Competition.

**Link**: [arxiv](http://arxiv.org/abs/2406.17863v4),  [pdf](http://arxiv.org/pdf/2406.17863v4)

**Tags**: cs.AI stat.ML 



### The one-dimensional equilibrium shape of a crystal
**Authors**: Emanuel Indrei

**Updated**: 2025-01-14T07:30:54Z

**Summary**: Optimizing the free energy under a mass constraint may generate a convex crystal subject to assumptions on the potential $g(0)=0$, $g \ge 0$. The general problem classically attributed to Almgren is to infer if this is the case assuming the sub-level sets of g are convex. The theorem proven in the paper is that in one dimension the answer is positive.

**Link**: [arxiv](http://arxiv.org/abs/2501.07900v1),  [pdf](http://arxiv.org/pdf/2501.07900v1)

**Tags**: math-ph math.AP math.CA math.DG math.MP 



### Bridge-SR: Schrödinger Bridge for Efficient SR
**Authors**: Chang Li, Zehua Chen, Fan Bao, Jun Zhu

**Updated**: 2025-01-14T07:26:05Z

**Summary**: Speech super-resolution (SR), which generates a waveform at a higher sampling rate from its low-resolution version, is a long-standing critical task in speech restoration. Previous works have explored speech SR in different data spaces, but these methods either require additional compression networks or exhibit limited synthesis quality and inference speed. Motivated by recent advances in probabilistic generative models, we present Bridge-SR, a novel and efficient any-to-48kHz SR system in the speech waveform domain. Using tractable Schr\"odinger Bridge models, we leverage the observed low-resolution waveform as a prior, which is intrinsically informative for the high-resolution target. By optimizing a lightweight network to learn the score functions from the prior to the target, we achieve efficient waveform SR through a data-to-data generation process that fully exploits the instructive content contained in the low-resolution observation. Furthermore, we identify the importance of the noise schedule, data scaling, and auxiliary loss functions, which further improve the SR quality of bridge-based systems. The experiments conducted on the benchmark dataset VCTK demonstrate the efficiency of our system: (1) in terms of sample quality, Bridge-SR outperforms several strong baseline methods under different SR settings, using a lightweight network backbone (1.7M); (2) in terms of inference speed, our 4-step synthesis achieves better performance than the 8-step conditional diffusion counterpart (LSD: 0.911 vs 0.927). Demo at https://bridge-sr.github.io.

**Link**: [arxiv](http://arxiv.org/abs/2501.07897v1),  [pdf](http://arxiv.org/pdf/2501.07897v1)

**Tags**: cs.SD eess.AS 



### Leveraging Metamemory Mechanisms for Enhanced Data-Free Code Generation   in LLMs
**Authors**: Shuai Wang, Liang Ding, Yibing Zhan, Yong Luo, Zheng He, Dapeng Tao

**Updated**: 2025-01-14T07:16:43Z

**Summary**: Automated code generation using large language models (LLMs) has gained attention due to its efficiency and adaptability. However, real-world coding tasks or benchmarks like HumanEval and StudentEval often lack dedicated training datasets, challenging existing few-shot prompting approaches that rely on reference examples. Inspired by human metamemory-a cognitive process involving recall and evaluation-we present a novel framework (namely M^2WF) for improving LLMs' one-time code generation. This approach enables LLMs to autonomously generate, evaluate, and utilize synthetic examples to enhance reliability and performance. Unlike prior methods, it minimizes dependency on curated data and adapts flexibly to various coding scenarios. Our experiments demonstrate significant improvements in coding benchmarks, offering a scalable and robust solution for data-free environments. The code and framework will be publicly available on GitHub and HuggingFace.

**Link**: [arxiv](http://arxiv.org/abs/2501.07892v1),  [pdf](http://arxiv.org/pdf/2501.07892v1)

**Tags**: cs.SE cs.AI 



### FLM-101B: An Open LLM and How to Train It with $100K Budget
**Authors**: Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Xuying Meng, Siqi Fan, Peng Han, Jing Li, Li Du, Bowen Qin, Zheng Zhang, Aixin Sun, Yequan Wang

**Updated**: 2025-01-14T06:40:36Z

**Summary**: Large language models (LLMs) are considered important approaches towards foundational machine intelligence, achieving remarkable success in Natural Language Processing and multimodal tasks, among others. However, the carbon footprints and financial costs originating from heavy pre-training computation is a non-negligible issue. Progressive training methods, inspired by the neurogenesis process that grows neural structures, have shown potential to accelerate LLM pre-training. However, the algorithms, implementation, and practices for progressively training LLMs beyond 100B parameters remain underexplored. In this paper, we show that our model, namely FLM-101B, trained with our growth strategy under a budget of \$100K, reaches 80\% of the baselines' performances with only 10\% of their floating-point operations. We believe that further studies on progressive training will benefit the community by cutting down the costs and promoting green AI. The checkpoint of FLM-101B is released at https://huggingface.co/CofeAI/FLM-101B.

**Link**: [arxiv](http://arxiv.org/abs/2309.03852v3),  [pdf](http://arxiv.org/pdf/2309.03852v3)

**Tags**: cs.CL cs.AI 



### Exploring Gradient Subspaces: Addressing and Overcoming LoRA's   Limitations in Federated Fine-Tuning of Large Language Models
**Authors**: Navyansh Mahla, Kshitij Sharad Jadhav, Ganesh Ramakrishnan

**Updated**: 2025-01-14T06:25:54Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, particularly in task generalization for both text and vision data. While fine-tuning these models can significantly enhance their performance on specific downstream tasks, it often requires high-quality data that cannot be shared due to privacy concerns. Federated Learning (FL) offers a promising solution for collaborative training without direct data sharing. However, many parameter-efficient fine-tuning strategies for LLMs in FL, particularly those based on Low-Rank Adaptation (LoRA), face limitations. In this paper, we critically analyze the convergence and performance guarantees of popular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to constrained subspace learning of low-rank matrices. This limitation hinders effective fine-tuning of LLMs in federated settings. Through rigorous analytical and empirical evaluations, we demonstrate that direct weight averaging outperforms LoRA-based strategies, leading to superior performance for fine-tuned models. Our comprehensive comparison unmasks inefficiencies in LoRA approaches and underscores the advantages of direct weight aggregation. We extend our analysis to low-rank gradient-based optimizers, such as GaLore, used during local training steps. Our findings show that GaLore along with direct-weight aggregation is a more effective approach, outperforming federated LoRA methods like FlexLoRA and FFA-LoRA across both text and image modalities. While privacy remains paramount in FL discourse, our focus is on assessing performance outcomes of federated fine-tuned models and evaluating various FL frameworks from both theoretical and empirical perspectives. Our findings advocate reassessing the reliance on LoRA within FL contexts, paving the way for more efficient training methodologies.

**Link**: [arxiv](http://arxiv.org/abs/2410.23111v6),  [pdf](http://arxiv.org/pdf/2410.23111v6)

**Tags**: cs.LG cs.AI 



### What Makes Cryptic Crosswords Challenging for LLMs?
**Authors**: Abdelrahman Sadallah, Daria Kotova, Ekaterina Kochmar

**Updated**: 2025-01-14T06:06:54Z

**Summary**: Cryptic crosswords are puzzles that rely on general knowledge and the solver's ability to manipulate language on different levels, dealing with various types of wordplay. Previous research suggests that solving such puzzles is challenging even for modern NLP models, including Large Language Models (LLMs). However, there is little to no research on the reasons for their poor performance on this task. In this paper, we establish the benchmark results for three popular LLMs: Gemma2, LLaMA3 and ChatGPT, showing that their performance on this task is still significantly below that of humans. We also investigate why these models struggle to achieve superior performance. We release our code and introduced datasets at https://github.com/bodasadallah/decrypting-crosswords.

**Link**: [arxiv](http://arxiv.org/abs/2412.09012v2),  [pdf](http://arxiv.org/pdf/2412.09012v2)

**Tags**: cs.CL cs.AI 



### ReARTeR: Retrieval-Augmented Reasoning with Trustworthy Process   Rewarding
**Authors**: Zhongxiang Sun, Qipeng Wang, Weijie Yu, Xiaoxue Zang, Kai Zheng, Jun Xu, Xiao Zhang, Song Yang, Han Li

**Updated**: 2025-01-14T05:56:26Z

**Summary**: Retrieval-Augmented Generation (RAG) systems for Large Language Models (LLMs) hold promise in knowledge-intensive tasks but face limitations in complex multi-step reasoning. While recent methods have integrated RAG with chain-of-thought reasoning or test-time search using Process Reward Models (PRMs), these approaches encounter challenges such as a lack of explanations, bias in PRM training data, early-step bias in PRM scores, and insufficient post-training optimization of reasoning potential. To address these issues, we propose Retrieval-Augmented Reasoning through Trustworthy Process Rewarding (ReARTeR), a framework that enhances RAG systems' reasoning capabilities through post-training and test-time scaling. At test time, ReARTeR introduces Trustworthy Process Rewarding via a Process Reward Model for accurate scalar scoring and a Process Explanation Model (PEM) for generating natural language explanations, enabling step refinement. During post-training, it utilizes Monte Carlo Tree Search guided by Trustworthy Process Rewarding to collect high-quality step-level preference data, optimized through Iterative Preference Optimization. ReARTeR addresses three core challenges: (1) misalignment between PRM and PEM, tackled through off-policy preference learning; (2) bias in PRM training data, mitigated by balanced annotation methods and stronger annotations for challenging examples; and (3) early-step bias in PRM, resolved through a temporal-difference-based look-ahead search strategy. Experimental results on multi-step reasoning benchmarks demonstrate significant improvements, underscoring ReARTeR's potential to advance the reasoning capabilities of RAG systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.07861v1),  [pdf](http://arxiv.org/pdf/2501.07861v1)

**Tags**: cs.CL 



### Hierarchical Repository-Level Code Summarization for Business   Applications Using Local LLMs
**Authors**: Nilesh Dhulshette, Sapan Shah, Vinay Kulkarni

**Updated**: 2025-01-14T05:48:27Z

**Summary**: In large-scale software development, understanding the functionality and intent behind complex codebases is critical for effective development and maintenance. While code summarization has been widely studied, existing methods primarily focus on smaller code units, such as functions, and struggle with larger code artifacts like files and packages. Additionally, current summarization models tend to emphasize low-level implementation details, often overlooking the domain and business context that are crucial for real-world applications. This paper proposes a two-step hierarchical approach for repository-level code summarization, tailored to business applications. First, smaller code units such as functions and variables are identified using syntax analysis and summarized with local LLMs. These summaries are then aggregated to generate higher-level file and package summaries. To ensure the summaries are grounded in business context, we design custom prompts that capture the intended purpose of code artifacts based on the domain and problem context of the business application. We evaluate our approach on a business support system (BSS) for the telecommunications domain, showing that syntax analysis-based hierarchical summarization improves coverage, while business-context grounding enhances the relevance of the generated summaries.

**Link**: [arxiv](http://arxiv.org/abs/2501.07857v1),  [pdf](http://arxiv.org/pdf/2501.07857v1)

**Tags**: cs.SE cs.AI 



### Optimizing Language Models for Grammatical Acceptability: A Comparative   Study of Fine-Tuning Techniques
**Authors**: Shobhit Ratan, Farley Knight, Ghada Jerfel, Sze Chung Ho

**Updated**: 2025-01-14T05:41:09Z

**Summary**: This study explores the fine-tuning (FT) of the Open Pre-trained Transformer (OPT-125M) for grammatical acceptability tasks using the CoLA dataset. By comparing Vanilla-Fine-Tuning (VFT), Pattern-Based-Fine-Tuning (PBFT), and Parameter-Efficient Fine-Tuning techniques (PEFT) like Low-Rank Adaptation (LoRA), we demonstrate significant improvements in computational efficiency while maintaining high accuracy. Our experiments reveal that while VFT achieves the highest accuracy (81.2%), LoRA enhancing FT by reducing memory usage and iteration time by more than 50%, and increases accuracy in PBFT case. Context Distillation (CD), though computationally efficient, underperformed with accuracy around 31%. Our findings contribute to democratizing access to large language models (LLM) by reducing computational barriers.

**Link**: [arxiv](http://arxiv.org/abs/2501.07853v1),  [pdf](http://arxiv.org/pdf/2501.07853v1)

**Tags**: cs.CL cs.AI 



### Evaluating Mathematical Reasoning Beyond Accuracy
**Authors**: Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, Pengfei Liu

**Updated**: 2025-01-14T05:39:40Z

**Summary**: The leaderboard of Large Language Models (LLMs) in mathematical tasks has been continuously updated. However, the majority of evaluations focus solely on the final results, neglecting the quality of the intermediate steps. This oversight can mask underlying problems, such as logical errors or unnecessary steps in the reasoning process. To measure reasoning beyond final-answer accuracy, we introduce ReasonEval, a new methodology for evaluating the quality of reasoning steps. ReasonEval employs validity and redundancy to characterize the reasoning quality, as well as accompanying LLMs to assess them automatically. We explore different design options for the LLM-based evaluators and empirically demonstrate that ReasonEval, when instantiated with base models possessing strong mathematical knowledge and trained with high-quality labeled data, consistently outperforms baseline methods in the meta-evaluation datasets. We also highlight the strong generalization capabilities of ReasonEval. By utilizing ReasonEval to evaluate LLMs specialized in math, we find that an increase in final-answer accuracy does not necessarily guarantee an improvement in the overall quality of the reasoning steps for challenging mathematical problems. Additionally, we observe that ReasonEval can play a significant role in data selection. We open-source the best-performing model, meta-evaluation script, and all evaluation results to facilitate future research.

**Link**: [arxiv](http://arxiv.org/abs/2404.05692v2),  [pdf](http://arxiv.org/pdf/2404.05692v2)

**Tags**: cs.CL 



### AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering   Benchmark Dataset
**Authors**: Tobi Olatunji, Charles Nimo, Abraham Owodunni, Tassallah Abdullahi, Emmanuel Ayodele, Mardhiyah Sanni, Chinemelu Aka, Folafunmi Omofoye, Foutse Yuehgoh, Timothy Faniran, Bonaventure F. P. Dossou, Moshood Yekini, Jonas Kemp, Katherine Heller, Jude Chidubem Omeke, Chidi Asuzu MD, Naome A. Etori, Aimérou Ndiaye, Ifeoma Okoh, Evans Doe Ocansey, Wendy Kinara, Michael Best, Irfan Essa, Stephen Edward Moore, Chris Fourie, Mercy Nyamewaa Asiedu

**Updated**: 2025-01-14T05:35:08Z

**Summary**: Recent advancements in large language model(LLM) performance on medical multiple choice question (MCQ) benchmarks have stimulated interest from healthcare providers and patients globally. Particularly in low-and middle-income countries (LMICs) facing acute physician shortages and lack of specialists, LLMs offer a potentially scalable pathway to enhance healthcare access and reduce costs. However, their effectiveness in the Global South, especially across the African continent, remains to be established. In this work, we introduce AfriMed-QA, the first large scale Pan-African English multi-specialty medical Question-Answering (QA) dataset, 15,000 questions (open and closed-ended) sourced from over 60 medical schools across 16 countries, covering 32 medical specialties. We further evaluate 30 LLMs across multiple axes including correctness and demographic bias. Our findings show significant performance variation across specialties and geographies, MCQ performance clearly lags USMLE (MedQA). We find that biomedical LLMs underperform general models and smaller edge-friendly LLMs struggle to achieve a passing score. Interestingly, human evaluations show a consistent consumer preference for LLM answers and explanations when compared with clinician answers.

**Link**: [arxiv](http://arxiv.org/abs/2411.15640v3),  [pdf](http://arxiv.org/pdf/2411.15640v3)

**Tags**: cs.CL 



## Keyword: LLM Deployment 
 ### PokerBench: Training Large Language Models to become Professional Poker   Players
**Authors**: Richard Zhuang, Akshat Gupta, Richard Yang, Aniket Rahane, Zhengyu Li, Gopala Anumanchipalli

**Updated**: 2025-01-14T18:59:03Z

**Summary**: We introduce PokerBench - a benchmark for evaluating the poker-playing abilities of large language models (LLMs). As LLMs excel in traditional NLP tasks, their application to complex, strategic games like poker poses a new challenge. Poker, an incomplete information game, demands a multitude of skills such as mathematics, reasoning, planning, strategy, and a deep understanding of game theory and human psychology. This makes Poker the ideal next frontier for large language models. PokerBench consists of a comprehensive compilation of 11,000 most important scenarios, split between pre-flop and post-flop play, developed in collaboration with trained poker players. We evaluate prominent models including GPT-4, ChatGPT 3.5, and various Llama and Gemma series models, finding that all state-of-the-art LLMs underperform in playing optimal poker. However, after fine-tuning, these models show marked improvements. We validate PokerBench by having models with different scores compete with each other, demonstrating that higher scores on PokerBench lead to higher win rates in actual poker games. Through gameplay between our fine-tuned model and GPT-4, we also identify limitations of simple supervised fine-tuning for learning optimal playing strategy, suggesting the need for more advanced methodologies for effectively training language models to excel in games. PokerBench thus presents a unique benchmark for a quick and reliable evaluation of the poker-playing ability of LLMs as well as a comprehensive benchmark to study the progress of LLMs in complex game-playing scenarios. The dataset and code will be made available at: \url{https://github.com/pokerllm/pokerbench}.

**Link**: [arxiv](http://arxiv.org/abs/2501.08328v1),  [pdf](http://arxiv.org/pdf/2501.08328v1)

**Tags**: cs.CL cs.AI cs.GT 



### ADAM-1: AI and Bioinformatics for Alzheimer's Detection and   Microbiome-Clinical Data Integrations
**Authors**: Ziyuan Huang, Vishaldeep Kaur Sekhon, Ouyang Guo, Mark Newman, Roozbeh Sadeghian, Maria L. Vaida, Cynthia Jo, Doyle Ward, Vanni Bucci, John P. Haran

**Updated**: 2025-01-14T18:56:33Z

**Summary**: The Alzheimer's Disease Analysis Model Generation 1 (ADAM) is a multi-agent large language model (LLM) framework designed to integrate and analyze multi-modal data, including microbiome profiles, clinical datasets, and external knowledge bases, to enhance the understanding and detection of Alzheimer's disease (AD). By leveraging retrieval-augmented generation (RAG) techniques along with its multi-agent architecture, ADAM-1 synthesizes insights from diverse data sources and contextualizes findings using literature-driven evidence. Comparative evaluation against XGBoost revealed similar mean F1 scores but significantly reduced variance for ADAM-1, highlighting its robustness and consistency, particularly in small laboratory datasets. While currently tailored for binary classification tasks, future iterations aim to incorporate additional data modalities, such as neuroimaging and biomarkers, to broaden the scalability and applicability for Alzheimer's research and diagnostics.

**Link**: [arxiv](http://arxiv.org/abs/2501.08324v1),  [pdf](http://arxiv.org/pdf/2501.08324v1)

**Tags**: cs.AI 68T07 



### Exploring Robustness of Multilingual LLMs on Real-World Noisy Data
**Authors**: Amirhossein Aliakbarzadeh, Lucie Flek, Akbar Karimi

**Updated**: 2025-01-14T18:55:35Z

**Summary**: Large Language Models (LLMs) are trained on Web data that might contain spelling errors made by humans. But do they become robust to similar real-world noise? In this paper, we investigate the effect of real-world spelling mistakes on the performance of 9 language models, with parameters ranging from 0.2B to 13B, in 3 different NLP tasks, namely Natural Language Inference (NLI), Name Entity Recognition (NER), and Intent Classification (IC). We perform our experiments on 6 different languages and build a dictionary of real-world noise for them using the Wikipedia edit history. We show that the performance gap of the studied models on the clean and noisy test data averaged across all the datasets and languages ranges from 2.3 to 4.3 absolute percentage points. In addition, mT5 models, in general, show more robustness compared to BLOOM, Falcon, and BERT-like models. In particular, mT5 (13B), was the most robust on average overall, across the 3 tasks, and in 4 of the 6 languages.

**Link**: [arxiv](http://arxiv.org/abs/2501.08322v1),  [pdf](http://arxiv.org/pdf/2501.08322v1)

**Tags**: cs.CL 



### Enhancing Automated Interpretability with Output-Centric Feature   Descriptions
**Authors**: Yoav Gur-Arieh, Roy Mayan, Chen Agassy, Atticus Geiger, Mor Geva

**Updated**: 2025-01-14T18:53:00Z

**Summary**: Automated interpretability pipelines generate natural language descriptions for the concepts represented by features in large language models (LLMs), such as plants or the first word in a sentence. These descriptions are derived using inputs that activate the feature, which may be a dimension or a direction in the model's representation space. However, identifying activating inputs is costly, and the mechanistic role of a feature in model behavior is determined both by how inputs cause a feature to activate and by how feature activation affects outputs. Using steering evaluations, we reveal that current pipelines provide descriptions that fail to capture the causal effect of the feature on outputs. To fix this, we propose efficient, output-centric methods for automatically generating feature descriptions. These methods use the tokens weighted higher after feature stimulation or the highest weight tokens after applying the vocabulary "unembedding" head directly to the feature. Our output-centric descriptions better capture the causal effect of a feature on model outputs than input-centric descriptions, but combining the two leads to the best performance on both input and output evaluations. Lastly, we show that output-centric descriptions can be used to find inputs that activate features previously thought to be "dead".

**Link**: [arxiv](http://arxiv.org/abs/2501.08319v1),  [pdf](http://arxiv.org/pdf/2501.08319v1)

**Tags**: cs.CL 



### Path Loss Prediction Using Machine Learning with Extended Features
**Authors**: Jonathan Ethier, Mathieu Chateauvert, Ryan G. Dempsey, Alexis Bose

**Updated**: 2025-01-14T18:44:35Z

**Summary**: Wireless communications rely on path loss modeling, which is most effective when it includes the physical details of the propagation environment. Acquiring this data has historically been challenging, but geographic information system data is becoming increasingly available with higher resolution and accuracy. Access to such details enables propagation models to more accurately predict coverage and minimize interference in wireless deployments. Machine learning-based modeling can significantly support this effort, with feature-based approaches allowing for accurate, efficient, and scalable propagation modeling. Building on previous work, we introduce an extended set of features that improves prediction accuracy while, most importantly, maintaining model generalization across a broad range of environments.

**Link**: [arxiv](http://arxiv.org/abs/2501.08306v1),  [pdf](http://arxiv.org/pdf/2501.08306v1)

**Tags**: cs.LG eess.SP 



### HALoGEN: Fantastic LLM Hallucinations and Where to Find Them
**Authors**: Abhilasha Ravichander, Shrusti Ghela, David Wadden, Yejin Choi

**Updated**: 2025-01-14T18:13:08Z

**Summary**: Despite their impressive ability to generate high-quality and fluent text, generative large language models (LLMs) also produce hallucinations: statements that are misaligned with established world knowledge or provided input context. However, measuring hallucination can be challenging, as having humans verify model generations on-the-fly is both expensive and time-consuming. In this work, we release HALoGEN, a comprehensive hallucination benchmark consisting of: (1) 10,923 prompts for generative models spanning nine domains including programming, scientific attribution, and summarization, and (2) automatic high-precision verifiers for each use case that decompose LLM generations into atomic units, and verify each unit against a high-quality knowledge source. We use this framework to evaluate ~150,000 generations from 14 language models, finding that even the best-performing models are riddled with hallucinations (sometimes up to 86% of generated atomic facts depending on the domain). We further define a novel error classification for LLM hallucinations based on whether they likely stem from incorrect recollection of training data (Type A errors), or incorrect knowledge in training data (Type B errors), or are fabrication (Type C errors). We hope our framework provides a foundation to enable the principled study of why generative models hallucinate, and advances the development of trustworthy large language models.

**Link**: [arxiv](http://arxiv.org/abs/2501.08292v1),  [pdf](http://arxiv.org/pdf/2501.08292v1)

**Tags**: cs.CL cs.AI 



### AfriHate: A Multilingual Collection of Hate Speech and Abusive Language   Datasets for African Languages
**Authors**: Shamsuddeen Hassan Muhammad, Idris Abdulmumin, Abinew Ali Ayele, David Ifeoluwa Adelani, Ibrahim Said Ahmad, Saminu Mohammad Aliyu, Nelson Odhiambo Onyango, Lilian D. A. Wanzare, Samuel Rutunda, Lukman Jibril Aliyu, Esubalew Alemneh, Oumaima Hourrane, Hagos Tesfahun Gebremichael, Elyas Abdi Ismail, Meriem Beloucif, Ebrahim Chekol Jibril, Andiswa Bukula, Rooweither Mabuya, Salomey Osei, Abigail Oppong, Tadesse Destaw Belay, Tadesse Kebede Guge, Tesfa Tegegne Asfaw, Chiamaka Ijeoma Chukwuneke, Paul Röttger, Seid Muhie Yimam, Nedjma Ousidhoum

**Updated**: 2025-01-14T18:00:07Z

**Summary**: Hate speech and abusive language are global phenomena that need socio-cultural background knowledge to be understood, identified, and moderated. However, in many regions of the Global South, there have been several documented occurrences of (1) absence of moderation and (2) censorship due to the reliance on keyword spotting out of context. Further, high-profile individuals have frequently been at the center of the moderation process, while large and targeted hate speech campaigns against minorities have been overlooked. These limitations are mainly due to the lack of high-quality data in the local languages and the failure to include local communities in the collection, annotation, and moderation processes. To address this issue, we present AfriHate: a multilingual collection of hate speech and abusive language datasets in 15 African languages. Each instance in AfriHate is annotated by native speakers familiar with the local culture. We report the challenges related to the construction of the datasets and present various classification baseline results with and without using LLMs. The datasets, individual annotations, and hate speech and offensive language lexicons are available on https://github.com/AfriHate/AfriHate

**Link**: [arxiv](http://arxiv.org/abs/2501.08284v1),  [pdf](http://arxiv.org/pdf/2501.08284v1)

**Tags**: cs.CL 



### Exploring Robustness of LLMs to Sociodemographically-Conditioned   Paraphrasing
**Authors**: Pulkit Arora, Akbar Karimi, Lucie Flek

**Updated**: 2025-01-14T17:50:06Z

**Summary**: Large Language Models (LLMs) have shown impressive performance in various NLP tasks. However, there are concerns about their reliability in different domains of linguistic variations. Many works have proposed robustness evaluation measures for local adversarial attacks, but we need globally robust models unbiased to different language styles. We take a broader approach to explore a wider range of variations across sociodemographic dimensions to perform structured reliability tests on the reasoning capacity of language models. We extend the SocialIQA dataset to create diverse paraphrased sets conditioned on sociodemographic styles. The assessment aims to provide a deeper understanding of LLMs in (a) their capability of generating demographic paraphrases with engineered prompts and (b) their reasoning capabilities in real-world, complex language scenarios. We also explore measures such as perplexity, explainability, and ATOMIC performance of paraphrases for fine-grained reliability analysis of LLMs on these sets. We find that demographic-specific paraphrasing significantly impacts the performance of language models, indicating that the subtleties of language variations remain a significant challenge. The code and dataset will be made available for reproducibility and future research.

**Link**: [arxiv](http://arxiv.org/abs/2501.08276v1),  [pdf](http://arxiv.org/pdf/2501.08276v1)

**Tags**: cs.CL 



### Vid2Sim: Realistic and Interactive Simulation from Video for Urban   Navigation
**Authors**: Ziyang Xie, Zhizheng Liu, Zhenghao Peng, Wayne Wu, Bolei Zhou

**Updated**: 2025-01-14T17:29:06Z

**Summary**: Sim-to-real gap has long posed a significant challenge for robot learning in simulation, preventing the deployment of learned models in the real world. Previous work has primarily focused on domain randomization and system identification to mitigate this gap. However, these methods are often limited by the inherent constraints of the simulation and graphics engines. In this work, we propose Vid2Sim, a novel framework that effectively bridges the sim2real gap through a scalable and cost-efficient real2sim pipeline for neural 3D scene reconstruction and simulation. Given a monocular video as input, Vid2Sim can generate photorealistic and physically interactable 3D simulation environments to enable the reinforcement learning of visual navigation agents in complex urban environments. Extensive experiments demonstrate that Vid2Sim significantly improves the performance of urban navigation in the digital twins and real world by 31.2% and 68.3% in success rate compared with agents trained with prior simulation methods.

**Link**: [arxiv](http://arxiv.org/abs/2501.06693v2),  [pdf](http://arxiv.org/pdf/2501.06693v2)

**Tags**: cs.CV cs.RO 



### Addressing the sustainable AI trilemma: a case study on LLM agents and   RAG
**Authors**: Hui Wu, Xiaoyang Wang, Zhong Fan

**Updated**: 2025-01-14T17:21:16Z

**Summary**: Large language models (LLMs) have demonstrated significant capabilities, but their widespread deployment and more advanced applications raise critical sustainability challenges, particularly in inference energy consumption. We propose the concept of the Sustainable AI Trilemma, highlighting the tensions between AI capability, digital equity, and environmental sustainability. Through a systematic case study of LLM agents and retrieval-augmented generation (RAG), we analyze the energy costs embedded in memory module designs and introduce novel metrics to quantify the trade-offs between energy consumption and system performance. Our experimental results reveal significant energy inefficiencies in current memory-augmented frameworks and demonstrate that resource-constrained environments face disproportionate efficiency penalties. Our findings challenge the prevailing LLM-centric paradigm in agent design and provide practical insights for developing more sustainable AI systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.08262v1),  [pdf](http://arxiv.org/pdf/2501.08262v1)

**Tags**: cs.CY 



### CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt   Optimization for Text Generation
**Authors**: Han He, Qianchu Liu, Lei Xu, Chaitanya Shivade, Yi Zhang, Sundararajan Srinivasan, Katrin Kirchhoff

**Updated**: 2025-01-14T17:20:04Z

**Summary**: Existing automatic prompt engineering methods are typically designed for discriminative tasks, where new task prompts are iteratively refined with limited feedback from a single metric reflecting a single aspect. However, these approaches are suboptimal for generative tasks, which require more nuanced guidance beyond a single numeric metric to improve the prompt and optimize multiple aspects of the generated text. To address these challenges, we propose a novel multi-aspect Critique-Suggestion-guided automatic Prompt Optimization (CriSPO) approach. CriSPO introduces a critique-suggestion module as its core component. This module spontaneously discovers aspects, and compares generated and reference texts across these aspects, providing specific suggestions for prompt modification. These clear critiques and actionable suggestions guide a receptive optimizer module to make more substantial changes, exploring a broader and more effective search space. To further improve CriSPO with multi-metric optimization, we introduce an Automatic Suffix Tuning (AST) extension to enhance the performance of task prompts across multiple metrics. We evaluate CriSPO on 4 state-of-the-art LLMs across 4 summarization and 5 QA datasets. Extensive experiments show 3-4% ROUGE score improvement on summarization and substantial improvement of various metrics on QA. Code available at https://github.com/amazon-science/crispo

**Link**: [arxiv](http://arxiv.org/abs/2410.02748v3),  [pdf](http://arxiv.org/pdf/2410.02748v3)

**Tags**: cs.CL cs.AI cs.LG 



### A Physical Layer Security Framework for IRS-Assisted Integrated Sensing   and Semantic Communication Systems
**Authors**: Hamid Amiriara, Mahtab Mirmohseni, Ahmed Elzanaty, Yi Ma, Rahim Tafazolli

**Updated**: 2025-01-14T17:18:27Z

**Summary**: In this paper, we propose a physical layer security (PLS) framework for an intelligent reflecting surface (IRS)-assisted integrated sensing and semantic communication (ISASC) system, where a multi-antenna dual-functional semantic base station (BS) serves multiple semantic communication users (SCUs) and monitors a potentially malicious sensing target (MST) in the presence of an eavesdropper (EVE). Both MST and EVE attempt to wiretap information from the signals transmitted to the SCUs. The deployment of the IRS not only enhances PLS by directing a strong beam towards the SCUs, but also improves the localization information for the target without disclosing information about the SCUs. To further strengthen PLS, we employ joint artificial noise (AN) and dedicated sensing signal (DSS), in addition to wiretap coding. To evaluate sensing accuracy, we derive the Cramer-Rao bound (CRB) for estimating the direction of arrival (DoA), and to assess the PLS level of the ISASC system, we determine a closed-form expression for the semantic secrecy rate (SSR). To achieve an optimal trade-off between these two competing objectives, we formulate a multi-objective optimization problem (MOOP) for the joint design of the BS's beamforming (BF) vectors and the IRS's phase shift vector. To tackle this MOOP problem, the $\epsilon$-constraint method is employed, followed by an alternating optimization (AO)-based algorithm that leverages the classical successive convex approximation (SCA) and semidefinite relaxation (SDR) techniques. Simulation results demonstrate that the proposed scheme outperforms the baseline schemes, achieving a superior trade-off between SSR and CRB. Specifically, our proposed approach improves the sensing accuracy by 5 dB compared to the commonly adopted maximal ratio transmission (MRT) approach.

**Link**: [arxiv](http://arxiv.org/abs/2410.06208v2),  [pdf](http://arxiv.org/pdf/2410.06208v2)

**Tags**: eess.SP 



### Text-Diffusion Red-Teaming of Large Language Models: Unveiling Harmful   Behaviors with Proximity Constraints
**Authors**: Jonathan Nöther, Adish Singla, Goran Radanović

**Updated**: 2025-01-14T16:32:01Z

**Summary**: Recent work has proposed automated red-teaming methods for testing the vulnerabilities of a given target large language model (LLM). These methods use red-teaming LLMs to uncover inputs that induce harmful behavior in a target LLM. In this paper, we study red-teaming strategies that enable a targeted security assessment. We propose an optimization framework for red-teaming with proximity constraints, where the discovered prompts must be similar to reference prompts from a given dataset. This dataset serves as a template for the discovered prompts, anchoring the search for test-cases to specific topics, writing styles, or types of harmful behavior. We show that established auto-regressive model architectures do not perform well in this setting. We therefore introduce a black-box red-teaming method inspired by text-diffusion models: Diffusion for Auditing and Red-Teaming (DART). DART modifies the reference prompt by perturbing it in the embedding space, directly controlling the amount of change introduced. We systematically evaluate our method by comparing its effectiveness with established methods based on model fine-tuning and zero- and few-shot prompting. Our results show that DART is significantly more effective at discovering harmful inputs in close proximity to the reference prompt.

**Link**: [arxiv](http://arxiv.org/abs/2501.08246v1),  [pdf](http://arxiv.org/pdf/2501.08246v1)

**Tags**: cs.LG 



### Engineering LLM Powered Multi-agent Framework for Autonomous CloudOps
**Authors**: Kannan Parthasarathy, Karthik Vaidhyanathan, Rudra Dhar, Venkat Krishnamachari, Basil Muhammed, Adyansh Kakran, Sreemaee Akshathala, Shrikara Arun, Sumant Dubey, Mohan Veerubhotla, Amey Karan

**Updated**: 2025-01-14T16:30:10Z

**Summary**: Cloud Operations (CloudOps) is a rapidly growing field focused on the automated management and optimization of cloud infrastructure which is essential for organizations navigating increasingly complex cloud environments. MontyCloud Inc. is one of the major companies in the CloudOps domain that leverages autonomous bots to manage cloud compliance, security, and continuous operations. To make the platform more accessible and effective to the customers, we leveraged the use of GenAI.   Developing a GenAI-based solution for autonomous CloudOps for the existing MontyCloud system presented us with various challenges such as i) diverse data sources; ii) orchestration of multiple processes; and iii) handling complex workflows to automate routine tasks. To this end, we developed MOYA, a multi-agent framework that leverages GenAI and balances autonomy with the necessary human control. This framework integrates various internal and external systems and is optimized for factors like task orchestration, security, and error mitigation while producing accurate, reliable, and relevant insights by utilizing Retrieval Augmented Generation (RAG). Evaluations of our multi-agent system with the help of practitioners as well as using automated checks demonstrate enhanced accuracy, responsiveness, and effectiveness over non-agentic approaches across complex workflows.

**Link**: [arxiv](http://arxiv.org/abs/2501.08243v1),  [pdf](http://arxiv.org/pdf/2501.08243v1)

**Tags**: cs.SE cs.AI cs.LG 



### HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language   Models
**Authors**: Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, Yu Su

**Updated**: 2025-01-14T16:17:49Z

**Summary**: In order to thrive in hostile and ever-changing natural environments, mammalian brains evolved to store large amounts of knowledge about the world and continually integrate new information while avoiding catastrophic forgetting. Despite the impressive accomplishments, large language models (LLMs), even with retrieval-augmented generation (RAG), still struggle to efficiently and effectively integrate a large amount of new experiences after pre-training. In this work, we introduce HippoRAG, a novel retrieval framework inspired by the hippocampal indexing theory of human long-term memory to enable deeper and more efficient knowledge integration over new experiences. HippoRAG synergistically orchestrates LLMs, knowledge graphs, and the Personalized PageRank algorithm to mimic the different roles of neocortex and hippocampus in human memory. We compare HippoRAG with existing RAG methods on multi-hop question answering and show that our method outperforms the state-of-the-art methods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves comparable or better performance than iterative retrieval like IRCoT while being 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into IRCoT brings further substantial gains. Finally, we show that our method can tackle new types of scenarios that are out of reach of existing methods. Code and data are available at https://github.com/OSU-NLP-Group/HippoRAG.

**Link**: [arxiv](http://arxiv.org/abs/2405.14831v3),  [pdf](http://arxiv.org/pdf/2405.14831v3)

**Tags**: cs.CL cs.AI 



### A Comprehensive Survey of Foundation Models in Medicine
**Authors**: Wasif Khan, Seowung Leem, Kyle B. See, Joshua K. Wong, Shaoting Zhang, Ruogu Fang

**Updated**: 2025-01-14T16:17:00Z

**Summary**: Foundation models (FMs) are large-scale deep learning models that are developed using large datasets and self-supervised learning methods. These models serve as a base for different downstream tasks, including healthcare. FMs have been adopted with great success across various domains within healthcare. Existing healthcare-based surveys have not yet included all of these domains. Therefore, we provide a detailed survey of FMs in healthcare. We focus on the history, learning strategies, flagship models, applications, and challenges of FMs. We explore how FMs such as the BERT and GPT families are reshaping various healthcare domains, including clinical large language models, medical image analysis, and omics. Furthermore, we provide a detailed taxonomy of healthcare applications facilitated by FMs, such as clinical NLP, medical computer vision, graph learning, and other biology-related tasks. Despite the promising opportunities FMs provide, they also have several associated challenges, which are explained in detail. We also outline open research issues and potential lessons learned to provide researchers and practitioners with insights into the capabilities of FMs in healthcare to advance their deployment and mitigate associated risks.

**Link**: [arxiv](http://arxiv.org/abs/2406.10729v2),  [pdf](http://arxiv.org/pdf/2406.10729v2)

**Tags**: cs.LG cs.AI cs.CV 



### Investigating Energy Efficiency and Performance Trade-offs in LLM   Inference Across Tasks and DVFS Settings
**Authors**: Paul Joe Maliakel, Shashikant Ilager, Ivona Brandic

**Updated**: 2025-01-14T16:02:33Z

**Summary**: Large language models (LLMs) have shown significant improvements in many natural language processing (NLP) tasks, accelerating their rapid adoption across many industries. These models are resource-intensive, requiring extensive computational resources both during training and inference, leading to increased energy consumption and negative environmental impact. As their adoption accelerates, the sustainability of LLMs has become a critical issue, necessitating strategies to optimize their runtime efficiency without compromising performance. Hence, it is imperative to identify the parameters that significantly influence the performance and energy efficiency of LLMs. To that end, in this work, we investigate the effect of important parameters on the performance and energy efficiency of LLMs during inference and examine their trade-offs.   First, we analyze how different types of models with varying numbers of parameters and architectures perform on tasks like text generation, question answering, and summarization by benchmarking LLMs such as Falcon-7B, Mistral-7B-v0.1, T5-3B, GPT-2, GPT-J-6B, and GPT-Neo-2.7B. Second, we study input and output sequence characteristics such as sequence length concerning energy consumption, performance, and throughput. Finally, we explore the impact of hardware-based power-saving techniques, i.e., Dynamic Voltage Frequency Scaling (DVFS), on the models' latency and energy efficiency. Our extensive benchmarking and statistical analysis reveal many interesting findings, uncovering how specific optimizations can reduce energy consumption while maintaining throughput and accuracy. This study provides actionable insights for researchers and practitioners to design energy-efficient LLM inference systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.08219v1),  [pdf](http://arxiv.org/pdf/2501.08219v1)

**Tags**: cs.LG 



### Logic Augmented Generation
**Authors**: Aldo Gangemi, Andrea Giovanni Nuzzolese

**Updated**: 2025-01-14T15:58:02Z

**Summary**: Semantic Knowledge Graphs (SKG) face challenges with scalability, flexibility, contextual understanding, and handling unstructured or ambiguous information. However, they offer formal and structured knowledge enabling highly interpretable and reliable results by means of reasoning and querying. Large Language Models (LLMs) overcome those limitations making them suitable in open-ended tasks and unstructured environments. Nevertheless, LLMs are neither interpretable nor reliable. To solve the dichotomy between LLMs and SKGs we envision Logic Augmented Generation (LAG) that combines the benefits of the two worlds. LAG uses LLMs as Reactive Continuous Knowledge Graphs that can generate potentially infinite relations and tacit knowledge on-demand. SKGs are key for injecting a discrete heuristic dimension with clear logical and factual boundaries. We exemplify LAG in two tasks of collective intelligence, i.e., medical diagnostics and climate projections. Understanding the properties and limitations of LAG, which are still mostly unknown, is of utmost importance for enabling a variety of tasks involving tacit knowledge in order to provide interpretable and effective results.

**Link**: [arxiv](http://arxiv.org/abs/2411.14012v2),  [pdf](http://arxiv.org/pdf/2411.14012v2)

**Tags**: cs.AI cs.CL 



### ASTRID -- An Automated and Scalable TRIaD for the Evaluation of   RAG-based Clinical Question Answering Systems
**Authors**: Mohita Chowdhury, Yajie Vera He, Aisling Higham, Ernest Lim

**Updated**: 2025-01-14T15:46:39Z

**Summary**: Large Language Models (LLMs) have shown impressive potential in clinical question answering (QA), with Retrieval Augmented Generation (RAG) emerging as a leading approach for ensuring the factual accuracy of model responses. However, current automated RAG metrics perform poorly in clinical and conversational use cases. Using clinical human evaluations of responses is expensive, unscalable, and not conducive to the continuous iterative development of RAG systems. To address these challenges, we introduce ASTRID - an Automated and Scalable TRIaD for evaluating clinical QA systems leveraging RAG - consisting of three metrics: Context Relevance (CR), Refusal Accuracy (RA), and Conversational Faithfulness (CF). Our novel evaluation metric, CF, is designed to better capture the faithfulness of a model's response to the knowledge base without penalising conversational elements. To validate our triad, we curate a dataset of over 200 real-world patient questions posed to an LLM-based QA agent during surgical follow-up for cataract surgery - the highest volume operation in the world - augmented with clinician-selected questions for emergency, clinical, and non-clinical out-of-domain scenarios. We demonstrate that CF can predict human ratings of faithfulness better than existing definitions for conversational use cases. Furthermore, we show that evaluation using our triad consisting of CF, RA, and CR exhibits alignment with clinician assessment for inappropriate, harmful, or unhelpful responses. Finally, using nine different LLMs, we demonstrate that the three metrics can closely agree with human evaluations, highlighting the potential of these metrics for use in LLM-driven automated evaluation pipelines. We also publish the prompts and datasets for these experiments, providing valuable resources for further research and development.

**Link**: [arxiv](http://arxiv.org/abs/2501.08208v1),  [pdf](http://arxiv.org/pdf/2501.08208v1)

**Tags**: cs.CL cs.AI 



### ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math   Problem Solving
**Authors**: Zain Ul Abedin, Shahzeb Qamar, Lucie Flek, Akbar Karimi

**Updated**: 2025-01-14T15:38:41Z

**Summary**: While Large Language Models (LLMs) have shown impressive capabilities in math problem-solving tasks, their robustness to noisy inputs is not well-studied. In this work, we propose ArithmAttack to examine how robust the LLMs are when they encounter noisy prompts that contain extra noise in the form of punctuation marks. While being easy to implement, ArithmAttack does not cause any information loss since words are not added or deleted from the context. We evaluate the robustness of seven LLMs, including LLama3, Mistral, and Mathstral, on noisy GSM8K and MultiArith datasets. Our experiments suggest that all the studied models show vulnerability to such noise, with more noise leading to poorer performances.

**Link**: [arxiv](http://arxiv.org/abs/2501.08203v1),  [pdf](http://arxiv.org/pdf/2501.08203v1)

**Tags**: cs.CL 



### Personalized LLM Response Generation with Parameterized Memory Injection
**Authors**: Kai Zhang, Yejin Kim, Xiaozhong Liu

**Updated**: 2025-01-14T15:30:50Z

**Summary**: Large Language Models (LLMs) have exhibited remarkable proficiency in comprehending and generating natural language. On the other hand, personalized LLM response generation holds the potential to offer substantial benefits for individuals in critical areas such as medical. Existing research has explored memory-augmented methods to prompt the LLM with pre-stored user-specific knowledge for personalized response generation in terms of new queries. We contend that such paradigm is unable to perceive fine-granularity information. In this study, we propose a novel \textbf{M}emory-\textbf{i}njected approach using parameter-efficient fine-tuning (PEFT) and along with a Bayesian Optimisation searching strategy to achieve \textbf{L}LM \textbf{P}ersonalization(\textbf{MiLP}).

**Link**: [arxiv](http://arxiv.org/abs/2404.03565v3),  [pdf](http://arxiv.org/pdf/2404.03565v3)

**Tags**: cs.CL 



### CWEval: Outcome-driven Evaluation on Functionality and Security of LLM   Code Generation
**Authors**: Jinjun Peng, Leyi Cui, Kele Huang, Junfeng Yang, Baishakhi Ray

**Updated**: 2025-01-14T15:27:01Z

**Summary**: Large Language Models (LLMs) have significantly aided developers by generating or assisting in code writing, enhancing productivity across various tasks. While identifying incorrect code is often straightforward, detecting vulnerabilities in functionally correct code is more challenging, especially for developers with limited security knowledge, which poses considerable security risks of using LLM-generated code and underscores the need for robust evaluation benchmarks that assess both functional correctness and security. Current benchmarks like CyberSecEval and SecurityEval attempt to solve it but are hindered by unclear and impractical specifications, failing to assess both functionality and security accurately. To tackle these deficiencies, we introduce CWEval, a novel outcome-driven evaluation framework designed to enhance the evaluation of secure code generation by LLMs. This framework not only assesses code functionality but also its security simultaneously with high-quality task specifications and outcome-driven test oracles which provides high accuracy. Coupled with CWEval-bench, a multilingual, security-critical coding benchmark, CWEval provides a rigorous empirical security evaluation on LLM-generated code, overcoming previous benchmarks' shortcomings. Through our evaluations, CWEval reveals a notable portion of functional but insecure code produced by LLMs, and shows a serious inaccuracy of previous evaluations, ultimately contributing significantly to the field of secure code generation. We open-source our artifact at: https://github.com/Co1lin/CWEval .

**Link**: [arxiv](http://arxiv.org/abs/2501.08200v1),  [pdf](http://arxiv.org/pdf/2501.08200v1)

**Tags**: cs.SE cs.CL cs.LG 



### OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for   LLM Training
**Authors**: Yijiong Yu, Ziyun Dai, Zekun Wang, Wei Wang, Ran Chen, Ji Pei

**Updated**: 2025-01-14T15:22:47Z

**Summary**: Large language models (LLMs) have demonstrated remarkable capabilities, but their success heavily relies on the quality of pretraining corpora. For Chinese LLMs, the scarcity of high-quality Chinese datasets presents a significant challenge, often limiting their performance. To address this issue, we propose the OpenCSG Chinese Corpus, a series of high-quality datasets specifically designed for LLM pretraining, post-training, and fine-tuning. This corpus includes Fineweb-edu-chinese, Fineweb-edu-chinese-v2, Cosmopedia-chinese, and Smoltalk-chinese, each with distinct characteristics: Fineweb-edu datasets focus on filtered, high-quality content derived from diverse Chinese web sources; Cosmopedia-chinese provides synthetic, textbook-style data for knowledge-intensive training; and Smoltalk-chinese emphasizes stylistic and diverse chat-format data. The OpenCSG Chinese Corpus is characterized by its high-quality text, diverse coverage across domains, and scalable, reproducible data curation processes. Additionally, we conducted extensive experimental analyses, including evaluations on smaller parameter models, which demonstrated significant performance improvements in tasks such as C-Eval, showcasing the effectiveness of the corpus for training Chinese LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2501.08197v1),  [pdf](http://arxiv.org/pdf/2501.08197v1)

**Tags**: cs.CL 



### KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model
**Authors**: Xinshuo Hu, Zifei Shan, Xinping Zhao, Zetian Sun, Zhenyu Liu, Dongfang Li, Shaolin Ye, Xinyuan Wei, Qian Chen, Baotian Hu, Min Zhang

**Updated**: 2025-01-14T15:19:52Z

**Summary**: As retrieval-augmented generation prevails in large language models, embedding models are becoming increasingly crucial. Despite the growing number of general embedding models, prior work often overlooks the critical role of training data quality. In this work, we introduce KaLM-Embedding, a general multilingual embedding model that leverages a large quantity of cleaner, more diverse, and domain-specific training data. Our model has been trained with key techniques proven to enhance performance: (1) persona-based synthetic data to create diversified examples distilled from LLMs, (2) ranking consistency filtering to remove less informative samples, and (3) semi-homogeneous task batch sampling to improve training efficacy. Departing from traditional BERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model, facilitating the adaptation of auto-regressive language models for general embedding tasks. Extensive evaluations of the MTEB benchmark across multiple languages show that our model outperforms others of comparable size, setting a new standard for multilingual embedding models with <1B parameters.

**Link**: [arxiv](http://arxiv.org/abs/2501.01028v3),  [pdf](http://arxiv.org/pdf/2501.01028v3)

**Tags**: cs.CL 



### PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM   Serving
**Authors**: Ahmet Caner Yüzügüler, Jiawei Zhuang, Lukas Cavigelli

**Updated**: 2025-01-14T15:14:10Z

**Summary**: Large language models (LLMs) are widely used across various applications, but their substantial computational requirements pose significant challenges, particularly in terms of HBM bandwidth bottlenecks and inter-device communication overhead. In this paper, we present PRESERVE, a novel prefetching framework designed to optimize LLM inference by overlapping memory reads for model weights and KV-cache with collective communication operations. Through extensive experiments conducted on commercial AI accelerators, we demonstrate up to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs. Additionally, we perform a design space exploration that identifies the optimal hardware configuration for the proposed method, showing a further 1.25x improvement in performance per cost by selecting the optimal L2 cache size. Our results show that PRESERVE has the potential to mitigate the memory bottlenecks and communication overheads, offering a solution to improve the performance and scalability of the LLM inference systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.08192v1),  [pdf](http://arxiv.org/pdf/2501.08192v1)

**Tags**: cs.AI cs.AR cs.DC 



### A Critical Synthesis of Uncertainty Quantification and Foundation Models   in Monocular Depth Estimation
**Authors**: Steven Landgraf, Rongjun Qin, Markus Ulrich

**Updated**: 2025-01-14T15:13:00Z

**Summary**: While recent foundation models have enabled significant breakthroughs in monocular depth estimation, a clear path towards safe and reliable deployment in the real-world remains elusive. Metric depth estimation, which involves predicting absolute distances, poses particular challenges, as even the most advanced foundation models remain prone to critical errors. Since quantifying the uncertainty has emerged as a promising endeavor to address these limitations and enable trustworthy deployment, we fuse five different uncertainty quantification methods with the current state-of-the-art DepthAnythingV2 foundation model. To cover a wide range of metric depth domains, we evaluate their performance on four diverse datasets. Our findings identify fine-tuning with the Gaussian Negative Log-Likelihood Loss (GNLL) as a particularly promising approach, offering reliable uncertainty estimates while maintaining predictive performance and computational efficiency on par with the baseline, encompassing both training and inference time. By fusing uncertainty quantification and foundation models within the context of monocular depth estimation, this paper lays a critical foundation for future research aimed at improving not only model performance but also its explainability. Extending this critical synthesis of uncertainty quantification and foundation models into other crucial tasks, such as semantic segmentation and pose estimation, presents exciting opportunities for safer and more reliable machine vision systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.08188v1),  [pdf](http://arxiv.org/pdf/2501.08188v1)

**Tags**: cs.CV cs.AI cs.LG 



### WebWalker: Benchmarking LLMs in Web Traversal
**Authors**: Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, Fei Huang

**Updated**: 2025-01-14T15:06:56Z

**Summary**: Retrieval-augmented generation (RAG) demonstrates remarkable performance across tasks in open-domain question-answering. However, traditional search engines may retrieve shallow content, limiting the ability of LLMs to handle complex, multi-layered information. To address it, we introduce WebWalkerQA, a benchmark designed to assess the ability of LLMs to perform web traversal. It evaluates the capacity of LLMs to traverse a website's subpages to extract high-quality data systematically. We propose WebWalker, which is a multi-agent framework that mimics human-like web navigation through an explore-critic paradigm. Extensive experimental results show that WebWalkerQA is challenging and demonstrates the effectiveness of RAG combined with WebWalker, through the horizontal and vertical integration in real-world scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2501.07572v2),  [pdf](http://arxiv.org/pdf/2501.07572v2)

**Tags**: cs.CL cs.AI 



### Potential and Perils of Large Language Models as Judges of Unstructured   Textual Data
**Authors**: Rewina Bedemariam, Natalie Perez, Sreyoshi Bhaduri, Satya Kapoor, Alex Gil, Elizabeth Conjar, Ikkei Itoku, David Theil, Aman Chadha, Naumaan Nayyar

**Updated**: 2025-01-14T14:49:14Z

**Summary**: Rapid advancements in large language models have unlocked remarkable capabilities when it comes to processing and summarizing unstructured text data. This has implications for the analysis of rich, open-ended datasets, such as survey responses, where LLMs hold the promise of efficiently distilling key themes and sentiments. However, as organizations increasingly turn to these powerful AI systems to make sense of textual feedback, a critical question arises, can we trust LLMs to accurately represent the perspectives contained within these text based datasets? While LLMs excel at generating human-like summaries, there is a risk that their outputs may inadvertently diverge from the true substance of the original responses. Discrepancies between the LLM-generated outputs and the actual themes present in the data could lead to flawed decision-making, with far-reaching consequences for organizations. This research investigates the effectiveness of LLMs as judge models to evaluate the thematic alignment of summaries generated by other LLMs. We utilized an Anthropic Claude model to generate thematic summaries from open-ended survey responses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as LLM judges. The LLM-as-judge approach was compared to human evaluations using Cohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalable alternative to traditional human centric evaluation methods. Our findings reveal that while LLMs as judges offer a scalable solution comparable to human raters, humans may still excel at detecting subtle, context-specific nuances. This research contributes to the growing body of knowledge on AI assisted text analysis. We discuss limitations and provide recommendations for future research, emphasizing the need for careful consideration when generalizing LLM judge models across various contexts and use cases.

**Link**: [arxiv](http://arxiv.org/abs/2501.08167v1),  [pdf](http://arxiv.org/pdf/2501.08167v1)

**Tags**: cs.CL cs.AI cs.CY 



### I Can Find You in Seconds! Leveraging Large Language Models for Code   Authorship Attribution
**Authors**: Soohyeon Choi, Yong Kiam Tan, Mark Huasong Meng, Mohamed Ragab, Soumik Mondal, David Mohaisen, Khin Mi Mi Aung

**Updated**: 2025-01-14T14:46:19Z

**Summary**: Source code authorship attribution is important in software forensics, plagiarism detection, and protecting software patch integrity. Existing techniques often rely on supervised machine learning, which struggles with generalization across different programming languages and coding styles due to the need for large labeled datasets. Inspired by recent advances in natural language authorship analysis using large language models (LLMs), which have shown exceptional performance without task-specific tuning, this paper explores the use of LLMs for source code authorship attribution.   We present a comprehensive study demonstrating that state-of-the-art LLMs can successfully attribute source code authorship across different languages. LLMs can determine whether two code snippets are written by the same author with zero-shot prompting, achieving a Matthews Correlation Coefficient (MCC) of 0.78, and can attribute code authorship from a small set of reference code snippets via few-shot learning, achieving MCC of 0.77. Additionally, LLMs show some adversarial robustness against misattribution attacks.   Despite these capabilities, we found that naive prompting of LLMs does not scale well with a large number of authors due to input token limitations. To address this, we propose a tournament-style approach for large-scale attribution. Evaluating this approach on datasets of C++ (500 authors, 26,355 samples) and Java (686 authors, 55,267 samples) code from GitHub, we achieve classification accuracy of up to 65% for C++ and 68.7% for Java using only one reference per author. These results open new possibilities for applying LLMs to code authorship attribution in cybersecurity and software engineering.

**Link**: [arxiv](http://arxiv.org/abs/2501.08165v1),  [pdf](http://arxiv.org/pdf/2501.08165v1)

**Tags**: cs.SE cs.AI 



### Energy Backdoor Attack to Deep Neural Networks
**Authors**: Hanene F. Z. Brachemi Meftah, Wassim Hamidouche, Sid Ahmed Fezza, Olivier Déforges, Kassem Kallas

**Updated**: 2025-01-14T14:26:18Z

**Summary**: The rise of deep learning (DL) has increased computing complexity and energy use, prompting the adoption of application specific integrated circuits (ASICs) for energy-efficient edge and mobile deployment. However, recent studies have demonstrated the vulnerability of these accelerators to energy attacks. Despite the development of various inference time energy attacks in prior research, backdoor energy attacks remain unexplored. In this paper, we design an innovative energy backdoor attack against deep neural networks (DNNs) operating on sparsity-based accelerators. Our attack is carried out in two distinct phases: backdoor injection and backdoor stealthiness. Experimental results using ResNet-18 and MobileNet-V2 models trained on CIFAR-10 and Tiny ImageNet datasets show the effectiveness of our proposed attack in increasing energy consumption on trigger samples while preserving the model's performance for clean/regular inputs. This demonstrates the vulnerability of DNNs to energy backdoor attacks. The source code of our attack is available at: https://github.com/hbrachemi/energy_backdoor.

**Link**: [arxiv](http://arxiv.org/abs/2501.08152v1),  [pdf](http://arxiv.org/pdf/2501.08152v1)

**Tags**: cs.CV 



### Inductive Learning of Logical Theories with LLMs: An Expressivity-Graded   Analysis
**Authors**: João Pedro Gandarela, Danilo S. Carvalho, André Freitas

**Updated**: 2025-01-14T14:26:03Z

**Summary**: This work presents a novel systematic methodology to analyse the capabilities and limitations of Large Language Models (LLMs) with feedback from a formal inference engine, on logic theory induction. The analysis is complexity-graded w.r.t. rule dependency structure, allowing quantification of specific inference challenges on LLM performance. Integrating LLMs with formal methods is a promising frontier in the Natural Language Processing field, as an important avenue for improving model inference control and explainability. In particular, inductive learning over complex sets of facts and rules, poses unique challenges for current autoregressive models, as they lack explicit symbolic grounding. While they can be complemented by formal systems, the properties delivered by LLMs regarding inductive learning, are not well understood and quantified. Empirical results indicate that the largest LLMs can achieve competitive results against a SOTA Inductive Logic Programming (ILP) system baseline, but also that tracking long predicate relationship chains is a more difficult obstacle than theory complexity for LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2408.16779v2),  [pdf](http://arxiv.org/pdf/2408.16779v2)

**Tags**: cs.CL cs.AI cs.LO I.2.7 



### Refusal Behavior in Large Language Models: A Nonlinear Perspective
**Authors**: Fabian Hildebrandt, Andreas Maier, Patrick Krauss, Achim Schilling

**Updated**: 2025-01-14T14:23:18Z

**Summary**: Refusal behavior in large language models (LLMs) enables them to decline responding to harmful, unethical, or inappropriate prompts, ensuring alignment with ethical standards. This paper investigates refusal behavior across six LLMs from three architectural families. We challenge the assumption of refusal as a linear phenomenon by employing dimensionality reduction techniques, including PCA, t-SNE, and UMAP. Our results reveal that refusal mechanisms exhibit nonlinear, multidimensional characteristics that vary by model architecture and layer. These findings highlight the need for nonlinear interpretability to improve alignment research and inform safer AI deployment strategies.

**Link**: [arxiv](http://arxiv.org/abs/2501.08145v1),  [pdf](http://arxiv.org/pdf/2501.08145v1)

**Tags**: cs.CL cs.AI 



### Are LLMs Good Literature Review Writers? Evaluating the Literature   Review Writing Ability of Large Language Models
**Authors**: Xuemei Tang, Xufeng Duan, Zhenguang G. Cai

**Updated**: 2025-01-14T14:16:45Z

**Summary**: The literature review is a crucial form of academic writing that involves complex processes of literature collection, organization, and summarization. The emergence of large language models (LLMs) has introduced promising tools to automate these processes. However, their actual capabilities in writing comprehensive literature reviews remain underexplored, such as whether they can generate accurate and reliable references. To address this gap, we propose a framework to assess the literature review writing ability of LLMs automatically. We evaluate the performance of LLMs across three tasks: generating references, writing abstracts, and writing literature reviews. We employ external tools for a multidimensional evaluation, which includes assessing hallucination rates in references, semantic coverage, and factual consistency with human-written context. By analyzing the experimental results, we find that, despite advancements, even the most sophisticated models still cannot avoid generating hallucinated references. Additionally, different models exhibit varying performance in literature review writing across different disciplines.

**Link**: [arxiv](http://arxiv.org/abs/2412.13612v2),  [pdf](http://arxiv.org/pdf/2412.13612v2)

**Tags**: cs.CL cs.AI 



### AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out   Context Attribution
**Authors**: Fengyuan Liu, Nikhil Kandpal, Colin Raffel

**Updated**: 2025-01-14T14:07:55Z

**Summary**: The influence of contextual input on the behavior of large language models (LLMs) has prompted the development of context attribution methods that aim to quantify each context span's effect on an LLM's generations. The leave-one-out (LOO) error, which measures the change in the likelihood of the LLM's response when a given span of the context is removed, provides a principled way to perform context attribution, but can be prohibitively expensive to compute for large models. In this work, we introduce AttriBoT, a series of novel techniques for efficiently computing an approximation of the LOO error for context attribution. Specifically, AttriBoT uses cached activations to avoid redundant operations, performs hierarchical attribution to reduce computation, and emulates the behavior of large target models with smaller proxy models. Taken together, AttriBoT can provide a >300x speedup while remaining more faithful to a target model's LOO error than prior context attribution methods. This stark increase in performance makes computing context attributions for a given response 30x faster than generating the response itself, empowering real-world applications that require computing attributions at scale. We release a user-friendly and efficient implementation of AttriBoT to enable efficient LLM interpretability as well as encourage future development of efficient context attribution methods.

**Link**: [arxiv](http://arxiv.org/abs/2411.15102v2),  [pdf](http://arxiv.org/pdf/2411.15102v2)

**Tags**: cs.LG 



### Consistency of Responses and Continuations Generated by Large Language   Models on Social Media
**Authors**: Wenlu Fan, Yuqi Zhu, Chenyang Wang, Bin Wang, Wentao Xu

**Updated**: 2025-01-14T13:19:47Z

**Summary**: Large Language Models (LLMs) demonstrate remarkable capabilities in text generation, yet their emotional consistency and semantic coherence in social media contexts remain insufficiently understood. This study investigates how LLMs handle emotional content and maintain semantic relationships through continuation and response tasks using two open-source models: Gemma and Llama. By analyzing climate change discussions from Twitter and Reddit, we examine emotional transitions, intensity patterns, and semantic similarity between human-authored and LLM-generated content. Our findings reveal that while both models maintain high semantic coherence, they exhibit distinct emotional patterns: Gemma shows a tendency toward negative emotion amplification, particularly anger, while maintaining certain positive emotions like optimism. Llama demonstrates superior emotional preservation across a broader spectrum of affects. Both models systematically generate responses with attenuated emotional intensity compared to human-authored content and show a bias toward positive emotions in response tasks. Additionally, both models maintain strong semantic similarity with original texts, though performance varies between continuation and response tasks. These findings provide insights into LLMs' emotional and semantic processing capabilities, with implications for their deployment in social media contexts and human-AI interaction design.

**Link**: [arxiv](http://arxiv.org/abs/2501.08102v1),  [pdf](http://arxiv.org/pdf/2501.08102v1)

**Tags**: cs.CL cs.AI cs.HC 



### Dynamic Sub-graph Distillation for Robust Semi-supervised Continual   Learning
**Authors**: Yan Fan, Yu Wang, Pengfei Zhu, Qinghua Hu

**Updated**: 2025-01-14T13:14:00Z

**Summary**: Continual learning (CL) has shown promising results and comparable performance to learning at once in a fully supervised manner. However, CL strategies typically require a large number of labeled samples, making their real-life deployment challenging. In this work, we focus on semi-supervised continual learning (SSCL), where the model progressively learns from partially labeled data with unknown categories. We provide a comprehensive analysis of SSCL and demonstrate that unreliable distributions of unlabeled data lead to unstable training and refinement of the progressing stages. This problem severely impacts the performance of SSCL. To address the limitations, we propose a novel approach called Dynamic Sub-Graph Distillation (DSGD) for semi-supervised continual learning, which leverages both semantic and structural information to achieve more stable knowledge distillation on unlabeled data and exhibit robustness against distribution bias. Firstly, we formalize a general model of structural distillation and design a dynamic graph construction for the continual learning progress. Next, we define a structure distillation vector and design a dynamic sub-graph distillation algorithm, which enables end-to-end training and adaptability to scale up tasks. The entire proposed method is adaptable to various CL methods and supervision settings. Finally, experiments conducted on three datasets CIFAR10, CIFAR100, and ImageNet-100, with varying supervision ratios, demonstrate the effectiveness of our proposed approach in mitigating the catastrophic forgetting problem in semi-supervised continual learning scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2312.16409v2),  [pdf](http://arxiv.org/pdf/2312.16409v2)

**Tags**: cs.LG cs.CV 



### Hierarchical Autoscaling for Large Language Model Serving with Chiron
**Authors**: Archit Patke, Dhemath Reddy, Saurabh Jha, Chandra Narayanaswami, Zbigniew Kalbarczyk, Ravishankar Iyer

**Updated**: 2025-01-14T12:57:40Z

**Summary**: Large language model (LLM) serving is becoming an increasingly important workload for cloud providers. Based on performance SLO requirements, LLM inference requests can be divided into (a) interactive requests that have tight SLOs in the order of seconds, and (b) batch requests that have relaxed SLO in the order of minutes to hours. These SLOs can degrade based on the arrival rates, multiplexing, and configuration parameters, thus necessitating the use of resource autoscaling on serving instances and their batch sizes. However, previous autoscalers for LLM serving do not consider request SLOs leading to unnecessary scaling and resource under-utilization. To address these limitations, we introduce Chiron, an autoscaler that uses the idea of hierarchical backpressure estimated using queue size, utilization, and SLOs. Our experiments show that Chiron achieves up to 90% higher SLO attainment and improves GPU efficiency by up to 70% compared to existing solutions.

**Link**: [arxiv](http://arxiv.org/abs/2501.08090v1),  [pdf](http://arxiv.org/pdf/2501.08090v1)

**Tags**: cs.DC cs.AI 



### Addressing Hallucinations in Language Models with Knowledge Graph   Embeddings as an Additional Modality
**Authors**: Viktoriia Chekalina, Anton Razzhigaev, Elizaveta Goncharova, Andrey Kuznetsov

**Updated**: 2025-01-14T12:56:34Z

**Summary**: In this paper we present an approach to reduce hallucinations in Large Language Models (LLMs) by incorporating Knowledge Graphs (KGs) as an additional modality. Our method involves transforming input text into a set of KG embeddings and using an adapter to integrate these embeddings into the language model space, without relying on external retrieval processes.   To facilitate this, we created WikiEntities, a dataset containing over 3 million Wikipedia texts annotated with entities from Wikidata and their corresponding embeddings from PyTorch-BigGraph. This dataset serves as a valuable resource for training Entity Linking models and adapting the described method to various LLMs using specialized adapters.   Our method does not require fine-tuning of the language models themselves; instead, we only train the adapter. This ensures that the model's performance on other tasks is not affected. We trained an adapter for the Mistral 7B, LLaMA 2-7B (chat), and LLaMA 3-8B (instruct) models using this dataset and demonstrated that our approach improves performance on the HaluEval, True-False benchmarks and FEVER dataset. The results indicate that incorporating KGs as a new modality can effectively reduce hallucinations and improve the factual accuracy of language models, all without the need for external retrieval.

**Link**: [arxiv](http://arxiv.org/abs/2411.11531v2),  [pdf](http://arxiv.org/pdf/2411.11531v2)

**Tags**: cs.CL cs.AI 



### JsonTuning: Towards Generalizable, Robust, and Controllable Instruction   Tuning
**Authors**: Chang Gao, Wenxuan Zhang, Guizhen Chen, Wai Lam

**Updated**: 2025-01-14T12:55:27Z

**Summary**: Instruction tuning is vital for enhancing the performance of large language models (LLMs), but existing text-to-text methods, referred to as TextTuning, struggle with issues such as generalization, robustness, and controllability due to their lack of explicit task structures. We introduce JsonTuning, a structure-to-structure approach that uses JSON structures to represent tasks. This method improves generalization by clarifying task elements and their relations, boosts robustness by minimizing ambiguity, and enhances controllability by allowing precise control over outputs. We conduct an extensive comparative analysis between JsonTuning and TextTuning using various language models and benchmarks. Our findings reveal that JsonTuning consistently surpasses TextTuning in terms of performance, robustness, and controllability across different scenarios. By overcoming the limitations of TextTuning, JsonTuning demonstrates significant potential for developing more effective and reliable LLMs capable of handling diverse scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2310.02953v4),  [pdf](http://arxiv.org/pdf/2310.02953v4)

**Tags**: cs.CL 



### Less is More: The Influence of Pruning on the Explainability of CNNs
**Authors**: Florian Merkle, David Weber, Pascal Schöttle, Stephan Schlögl, Martin Nocker

**Updated**: 2025-01-14T12:53:24Z

**Summary**: Over the last century, deep learning models have become the state-of-the-art for solving complex computer vision problems. These modern computer vision models have millions of parameters, which presents two major challenges: (1) the increased computational requirements hamper the deployment in resource-constrained environments, such as mobile or IoT devices, and (2) explaining the complex decisions of such networks to humans is challenging. Network pruning is a technical approach to reduce the complexity of models, where less important parameters are removed. The work presented in this paper investigates whether this reduction in technical complexity also helps with perceived explainability. To do so, we conducted a pre-study and two human-grounded experiments, assessing the effects of different pruning ratios on explainability. Overall, we evaluate four different compression rates (i.e., 2, 4, 8, and 32) with 37 500 tasks on Mechanical Turk. Results indicate that lower compression rates have a positive influence on explainability, while higher compression rates show negative effects. Furthermore, we were able to identify sweet spots that increase both the perceived explainability and the model's performance.

**Link**: [arxiv](http://arxiv.org/abs/2302.08878v3),  [pdf](http://arxiv.org/pdf/2302.08878v3)

**Tags**: cs.CV cs.AI 



### CuAsmRL: Optimizing GPU SASS Schedules via Deep Reinforcement Learning
**Authors**: Guoliang He, Eiko Yoneki

**Updated**: 2025-01-14T12:36:18Z

**Summary**: Large language models (LLMs) are remarked by their substantial computational requirements. To mitigate the cost, researchers develop specialized CUDA kernels, which often fuse several tensor operations to maximize the utilization of GPUs as much as possible. However, those specialized kernels may still leave performance on the table as CUDA assembly experts show that manual optimization of GPU SASS schedules can lead to better performance, and trial-and-error is largely employed to manually find the best GPU SASS schedules.   In this work, we employ an automatic approach to optimize GPU SASS schedules, which thus can be integrated into existing compiler frameworks. The key to automatic optimization is training an RL agent to mimic how human experts perform manual scheduling. To this end, we formulate an assembly game, where RL agents can play to find the best GPU SASS schedules. The assembly game starts from a \textit{-O3} optimized SASS schedule, and the RL agents can iteratively apply actions to mutate the current schedules. Positive rewards are generated if the mutated schedules get higher throughput by executing on GPUs. Experiments show that CuAsmRL can further improve the performance of existing specialized CUDA kernels transparently by up to $26\%$, and on average $9\%$. Moreover, it is used as a tool to reveal potential optimization moves learned automatically.

**Link**: [arxiv](http://arxiv.org/abs/2501.08071v1),  [pdf](http://arxiv.org/pdf/2501.08071v1)

**Tags**: cs.AR cs.LG 



### A Roadmap to Guide the Integration of LLMs in Hierarchical Planning
**Authors**: Israel Puerta-Merino, Carlos Núñez-Molina, Pablo Mesejo, Juan Fernández-Olivares

**Updated**: 2025-01-14T12:34:25Z

**Summary**: Recent advances in Large Language Models (LLMs) are fostering their integration into several reasoning-related fields, including Automated Planning (AP). However, their integration into Hierarchical Planning (HP), a subfield of AP that leverages hierarchical knowledge to enhance planning performance, remains largely unexplored. In this preliminary work, we propose a roadmap to address this gap and harness the potential of LLMs for HP. To this end, we present a taxonomy of integration methods, exploring how LLMs can be utilized within the HP life cycle. Additionally, we provide a benchmark with a standardized dataset for evaluating the performance of future LLM-based HP approaches, and present initial results for a state-of-the-art HP planner and LLM planner. As expected, the latter exhibits limited performance (3\% correct plans, and none with a correct hierarchical decomposition) but serves as a valuable baseline for future approaches.

**Link**: [arxiv](http://arxiv.org/abs/2501.08068v1),  [pdf](http://arxiv.org/pdf/2501.08068v1)

**Tags**: cs.AI 



### TreeKV: Smooth Key-Value Cache Compression with Tree Structures
**Authors**: Ziwei He, Jian Yuan, Haoli Bai, Jingwen Leng, Bo Jiang

**Updated**: 2025-01-14T12:06:33Z

**Summary**: Efficient key-value (KV) cache compression is critical for scaling transformer-based Large Language Models (LLMs) in long sequences and resource-limited settings. Existing methods evict tokens based on their positions or importance scores, but position-based strategies can miss crucial information outside predefined regions, while those relying on global importance scores resulting in strong regional biases, limiting the KV cache's overall context retention and potentially impairing the performance of LLMs on complex tasks. Our wavelet analysis reveals that as tokens approach the end of sequence, their contributions to generation gradually increase and tends to diverge more from neighboring tokens, indicating a smooth transition with increasing complexity and variability from distant to nearby context. Motivated by this observation, we propose TreeKV, an intuitive, training-free method that employs a tree structure for smooth cache compression. TreeKV maintains a fixed cache size, allowing LLMs to deliver high-quality output even in long text scenarios. Unlike most compression methods, TreeKV is applicable to both the generation and prefilling stages. TreeKV consistently surpasses all baseline models in language modeling tasks on PG19 and OpenWebText2, allowing LLMs trained with short context window to generalize to longer window with a 16x cache reduction. On the Longbench benchmark, TreeKV achieves the best performance with only 6\% of the budget at optimal efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2501.04987v2),  [pdf](http://arxiv.org/pdf/2501.04987v2)

**Tags**: cs.CL 



### Audio-Agent: Leveraging LLMs For Audio Generation, Editing and   Composition
**Authors**: Zixuan Wang, Chi-Keung Tang, Yu-Wing Tai

**Updated**: 2025-01-14T11:59:03Z

**Summary**: We introduce Audio-Agent, a multimodal framework for audio generation, editing and composition based on text or video inputs. Conventional approaches for text-to-audio (TTA) tasks often make single-pass inferences from text descriptions. While straightforward, this design struggles to produce high-quality audio when given complex text conditions. In our method, we utilize a pre-trained TTA diffusion network as the audio generation agent to work in tandem with GPT-4, which decomposes the text condition into atomic, specific instructions and calls the agent for audio generation. In doing so, Audio-Agent can generate high-quality audio that is closely aligned with the provided text or video exhibiting complex and multiple events, while supporting variable-length and variable-volume generation. For video-to-audio (VTA) tasks, most existing methods require training a timestamp detector to synchronize video events with the generated audio, a process that can be tedious and time-consuming. Instead, we propose a simpler approach by fine-tuning a pre-trained Large Language Model (LLM), e.g., Gemma2-2B-it, to obtain both semantic and temporal conditions that bridge the video and audio modality. Consequently, our framework contributes a comprehensive solution for both TTA and VTA tasks without substantial computational overhead in training.

**Link**: [arxiv](http://arxiv.org/abs/2410.03335v2),  [pdf](http://arxiv.org/pdf/2410.03335v2)

**Tags**: cs.SD cs.CV cs.LG eess.AS 



### PolyLUT: Ultra-low Latency Polynomial Inference with Hardware-Aware   Structured Pruning
**Authors**: Marta Andronic, Jiawen Li, George A. Constantinides

**Updated**: 2025-01-14T11:51:57Z

**Summary**: Standard deep neural network inference involves the computation of interleaved linear maps and nonlinear activation functions. Prior work for ultra-low latency implementations has hardcoded these operations inside FPGA lookup tables (LUTs). However, FPGA LUTs can implement a much greater variety of functions. In this paper, we propose a novel approach to training DNNs for FPGA deployment using multivariate polynomials as the basic building block. Our method takes advantage of the flexibility offered by the soft logic, hiding the polynomial evaluation inside the LUTs with minimal overhead. By using polynomial building blocks, we achieve the same accuracy using considerably fewer layers of soft logic than by using linear functions, leading to significant latency and area improvements. LUT-based implementations also face a significant challenge: the LUT size grows exponentially with the number of inputs. Prior work relies on a priori fixed sparsity, with results heavily dependent on seed selection. To address this, we propose a structured pruning strategy using a bespoke hardware-aware group regularizer that encourages a particular sparsity pattern that leads to a small number of inputs per neuron. We demonstrate the effectiveness of PolyLUT on three tasks: network intrusion detection, jet identification at the CERN Large Hadron Collider, and MNIST.

**Link**: [arxiv](http://arxiv.org/abs/2501.08043v1),  [pdf](http://arxiv.org/pdf/2501.08043v1)

**Tags**: cs.LG cs.AR 



### EventHallusion: Diagnosing Event Hallucinations in Video LLMs
**Authors**: Jiacheng Zhang, Yang Jiao, Shaoxiang Chen, Na Zhao, Jingjing Chen

**Updated**: 2025-01-14T11:27:28Z

**Summary**: Recently, Multimodal Large Language Models (MLLMs) have made significant progress in the video comprehension field. Despite remarkable content reasoning and instruction following capabilities they demonstrated, the hallucination problem of these VideoLLMs is less explored compared with its counterpart in the image domain. To mitigate this gap, we propose EventHallusion, a novel benchmark that focuses on assessing the VideoLLMs' hallucination toward event, the crux of video analysis. From a hallucination attribution perspective, our EventHallusion benchmark is curated to assess a VideoLLM's susceptibility toward language priors and vision-language biases. On the other hand, we also propose a simple yet effective method, called Temporal Contrastive Decoding (TCD), to tackle the hallucination problems of VideoLLMs. The proposed TCD method rectifies the model's bias toward its priors during the decoding stage by comparing the original video with a modified version, in which temporal cues are disrupted. Through comprehensive evaluation of eight open-source and two closed-source VideoLLMs on the proposed EventHallusion benchmark, we observe that the open-source models suffer significantly from hallucination problems, whereas the closed-source ones perform markedly better. By further equipping open-source VideoLLMs with the proposed TCD approach, evident performance improvements are achieved across most metrics in the EventHallusion benchmark. Our codes and benchmark data are available at https://github.com/Stevetich/EventHallusion.

**Link**: [arxiv](http://arxiv.org/abs/2409.16597v3),  [pdf](http://arxiv.org/pdf/2409.16597v3)

**Tags**: cs.CV 



### MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation
**Authors**: Tianyu Fan, Jingyuan Wang, Xubin Ren, Chao Huang

**Updated**: 2025-01-14T11:03:56Z

**Summary**: The growing demand for efficient and lightweight Retrieval-Augmented Generation (RAG) systems has highlighted significant challenges when deploying Small Language Models (SLMs) in existing RAG frameworks. Current approaches face severe performance degradation due to SLMs' limited semantic understanding and text processing capabilities, creating barriers for widespread adoption in resource-constrained scenarios. To address these fundamental limitations, we present MiniRAG, a novel RAG system designed for extreme simplicity and efficiency. MiniRAG introduces two key technical innovations: (1) a semantic-aware heterogeneous graph indexing mechanism that combines text chunks and named entities in a unified structure, reducing reliance on complex semantic understanding, and (2) a lightweight topology-enhanced retrieval approach that leverages graph structures for efficient knowledge discovery without requiring advanced language capabilities. Our extensive experiments demonstrate that MiniRAG achieves comparable performance to LLM-based methods even when using SLMs while requiring only 25\% of the storage space. Additionally, we contribute a comprehensive benchmark dataset for evaluating lightweight RAG systems under realistic on-device scenarios with complex queries. We fully open-source our implementation and datasets at: https://github.com/HKUDS/MiniRAG.

**Link**: [arxiv](http://arxiv.org/abs/2501.06713v2),  [pdf](http://arxiv.org/pdf/2501.06713v2)

**Tags**: cs.AI 



### Transformers and Large Language Models for Efficient Intrusion Detection   Systems: A Comprehensive Survey
**Authors**: Hamza Kheddar

**Updated**: 2025-01-14T10:52:15Z

**Summary**: With significant advancements in Transformers LLMs, NLP has extended its reach into many research fields due to its enhanced capabilities in text generation and user interaction. One field benefiting greatly from these advancements is cybersecurity. In cybersecurity, many parameters that need to be protected and exchanged between senders and receivers are in the form of text and tabular data, making NLP a valuable tool in enhancing the security measures of communication protocols. This survey paper provides a comprehensive analysis of the utilization of Transformers and LLMs in cyber-threat detection systems. The methodology of paper selection and bibliometric analysis is outlined to establish a rigorous framework for evaluating existing research. The fundamentals of Transformers are discussed, including background information on various cyber-attacks and datasets commonly used in this field. The survey explores the application of Transformers in IDSs, focusing on different architectures such as Attention-based models, LLMs like BERT and GPT, CNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others. Furthermore, it explores the diverse environments and applications where Transformers and LLMs-based IDS have been implemented, including computer networks, IoT devices, critical infrastructure protection, cloud computing, SDN, as well as in autonomous vehicles. The paper also addresses research challenges and future directions in this area, identifying key issues such as interpretability, scalability, and adaptability to evolving threats, and more. Finally, the conclusion summarizes the findings and highlights the significance of Transformers and LLMs in enhancing cyber-threat detection capabilities, while also outlining potential avenues for further research and development.

**Link**: [arxiv](http://arxiv.org/abs/2408.07583v2),  [pdf](http://arxiv.org/pdf/2408.07583v2)

**Tags**: cs.CR cs.AI cs.CL cs.CV eess.AS 



### TriAdaptLoRA: Brain-Inspired Triangular Adaptive Low-Rank Adaptation for   Parameter-Efficient Fine-Tuning
**Authors**: Yao Liang, Yuwei Wang, Yi Zeng

**Updated**: 2025-01-14T10:51:31Z

**Summary**: The fine-tuning of Large Language Models (LLMs) is pivotal for achieving optimal performance across diverse downstream tasks. However, while full fine-tuning delivers superior results, it entails significant computational and resource costs. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA, address these challenges by reducing the number of trainable parameters, but they often struggle with rank adjustment efficiency and task-specific adaptability. We propose Triangular Adaptive Low-Rank Adaptation (TriAdaptLoRA), a novel PEFT framework inspired by neuroscience principles, which dynamically optimizes the allocation of trainable parameters. TriAdaptLoRA introduces three key innovations: 1) a triangular split of transformation matrices into lower and upper triangular components to maximize parameter utilization, 2) a parameter importance metric based on normalized Frobenius norms for efficient adaptation, and 3) an adaptive rank-growth strategy governed by dynamic thresholds, allowing flexible parameter allocation across training steps. Experiments conducted on a variety of natural language understanding and generation tasks demonstrate that TriAdaptLoRA consistently outperforms existing PEFT methods. It achieves superior performance, enhanced stability, and reduced computational overhead, particularly under linear threshold-driven rank growth. These results highlight its efficacy as a scalable and resource-efficient solution for fine-tuning LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2501.08008v1),  [pdf](http://arxiv.org/pdf/2501.08008v1)

**Tags**: cs.CL cs.AI 



### LLM-Ehnanced Holonic Architecture for Ad-Hoc Scalable SoS
**Authors**: Muhammad Ashfaq, Ahmed R. Sadik, Tommi Mikkonen, Muhammad Waseem, Niko Mäkitalo

**Updated**: 2025-01-14T10:35:54Z

**Summary**: As modern system of systems (SoS) become increasingly adaptive and human centred, traditional architectures often struggle to support interoperability, reconfigurability, and effective human system interaction. This paper addresses these challenges by advancing the state of the art holonic architecture for SoS, offering two main contributions to support these adaptive needs. First, we propose a layered architecture for holons, which includes reasoning, communication, and capabilities layers. This design facilitates seamless interoperability among heterogeneous constituent systems by improving data exchange and integration. Second, inspired by principles of intelligent manufacturing, we introduce specialised holons namely, supervisor, planner, task, and resource holons aimed at enhancing the adaptability and reconfigurability of SoS. These specialised holons utilise large language models within their reasoning layers to support decision making and ensure real time adaptability. We demonstrate our approach through a 3D mobility case study focused on smart city transportation, showcasing its potential for managing complex, multimodal SoS environments. Additionally, we propose evaluation methods to assess the architecture efficiency and scalability,laying the groundwork for future empirical validations through simulations and real world implementations.

**Link**: [arxiv](http://arxiv.org/abs/2501.07992v1),  [pdf](http://arxiv.org/pdf/2501.07992v1)

**Tags**: cs.AI cs.ET cs.MA cs.SE 



### One Language, Many Gaps: Evaluating Dialect Fairness and Robustness of   Large Language Models in Reasoning Tasks
**Authors**: Fangru Lin, Shaoguang Mao, Emanuele La Malfa, Valentin Hofmann, Adrian de Wynter, Xun Wang, Si-Qing Chen, Michael Wooldridge, Janet B. Pierrehumbert, Furu Wei

**Updated**: 2025-01-14T09:52:50Z

**Summary**: Language is not monolithic. While benchmarks, including those designed for multiple languages, are often used as proxies to evaluate the performance of Large Language Models (LLMs), they tend to overlook the nuances of within-language variation, and thus fail to model the experience of speakers of non-standard dialects. Focusing on African American Vernacular English (AAVE), we present the first study aimed at objectively assessing the fairness and robustness of LLMs in handling dialects in canonical reasoning tasks, including algorithm, math, logic, and integrated reasoning. We introduce \textbf{ReDial} (\textbf{Re}asoning with \textbf{Dial}ect Queries), a benchmark containing 1.2K+ parallel query pairs in Standardized English and AAVE. We hire AAVE speakers, including experts with computer science backgrounds, to rewrite seven popular benchmarks, such as HumanEval and GSM8K. With ReDial, we evaluate widely used LLMs, including GPT, Claude, Llama, Mistral, and the Phi model families. Our findings reveal that \textbf{almost all of these widely used models show significant brittleness and unfairness to queries in AAVE}. Our work establishes a systematic and objective framework for analyzing LLM bias in dialectal queries. Moreover, it highlights how mainstream LLMs provide unfair service to dialect speakers in reasoning tasks, laying a critical foundation for relevant future research. Code and data can be accessed at https://github.com/fangru-lin/redial_dialect_robustness_fairness.

**Link**: [arxiv](http://arxiv.org/abs/2410.11005v2),  [pdf](http://arxiv.org/pdf/2410.11005v2)

**Tags**: cs.CL cs.LG 



### Self-Instruct Few-Shot Jailbreaking: Decompose the Attack into Pattern   and Behavior Learning
**Authors**: Jiaqi Hua, Wanxu Wei

**Updated**: 2025-01-14T09:23:30Z

**Summary**: Recently, several works have been conducted on jailbreaking Large Language Models (LLMs) with few-shot malicious demos. In particular, Zheng et al. (2024) focuses on improving the efficiency of Few-Shot Jailbreaking (FSJ) by injecting special tokens into the demos and employing demo-level random search. Nevertheless, this method lacks generality since it specifies the instruction-response structure. Moreover, the reason why inserting special tokens takes effect in inducing harmful behaviors is only empirically discussed. In this paper, we take a deeper insight into the mechanism of special token injection and propose Self-Instruct Few-Shot Jailbreaking (Self-Instruct-FSJ) facilitated with the demo-level greedy search. This framework decomposes the FSJ attack into pattern and behavior learning to exploit the model's vulnerabilities in a more generalized and efficient way. We conduct elaborate experiments to evaluate our method on common open-source models and compare it with baseline algorithms. Our code is available at https://github.com/iphosi/Self-Instruct-FSJ.

**Link**: [arxiv](http://arxiv.org/abs/2501.07959v1),  [pdf](http://arxiv.org/pdf/2501.07959v1)

**Tags**: cs.AI 



### AI Guide Dog: Egocentric Path Prediction on Smartphone
**Authors**: Aishwarya Jadhav, Jeffery Cao, Abhishree Shetty, Urvashi Priyam Kumar, Aditi Sharma, Ben Sukboontip, Jayant Sravan Tamarapalli, Jingyi Zhang, Anirudh Koul

**Updated**: 2025-01-14T09:21:17Z

**Summary**: This paper introduces AI Guide Dog (AIGD), a lightweight egocentric navigation assistance system for visually impaired individuals, designed for real-time deployment on smartphones. AIGD addresses key challenges in blind navigation by employing a vision-only, multi-label classification approach to predict directional commands, ensuring safe traversal across diverse environments. We propose a novel technique to enable goal-based outdoor navigation by integrating GPS signals and high-level directions, while also addressing uncertain multi-path predictions for destination-free indoor navigation. Our generalized model is the first navigation assistance system to handle both goal-oriented and exploratory navigation scenarios across indoor and outdoor settings, establishing a new state-of-the-art in blind navigation. We present methods, datasets, evaluations, and deployment insights to encourage further innovations in assistive navigation systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.07957v1),  [pdf](http://arxiv.org/pdf/2501.07957v1)

**Tags**: cs.RO cs.AI cs.CV cs.HC cs.LG 



### Towards high resolution, validated and open global wind power   assessments
**Authors**: Edgar Ubaldo Peña-Sánchez, Philipp Dunkel, Christoph Winkler, Heidi Heinrichs, Florian Prinz, Jann Weinand, Rachel Maier, Sebastian Dickler, Shuying Chen, Katharina Gruber, Theresa Klütz, Jochen Linßen, Detlef Stolten

**Updated**: 2025-01-14T08:45:40Z

**Summary**: Wind power is expected to play a crucial role in future net-zero energy systems, but wind power simulations to support deployment strategies vary drastically in their results, hindering reliable design decisions. Therefore, we present a transparent, open source, validated and evaluated, global wind power simulation tool ETHOS.RESKit$_{Wind}$ with high spatial resolution and customizable designs for both onshore and offshore wind turbines. The tool provides a comprehensive validation and calibration procedure using over 16 million global measurements from metrerological masts and wind turbine sites. We achieve a global average capacity factor mean error of 0.006 and Pearson correlation of 0.865. In addition, we evaluate its performance against several aggregated and statistical sources of wind power generation. The release of ETHOS.RESKit$_{Wind}$ is a step towards a fully open source and open data approach to accurate wind power modeling by incorporating the most comprehensive simulation advances in one model.

**Link**: [arxiv](http://arxiv.org/abs/2501.07937v1),  [pdf](http://arxiv.org/pdf/2501.07937v1)

**Tags**: physics.soc-ph 



### An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN   Architectures
**Authors**: Thibaut Boissin, Franck Mamalet, Thomas Fel, Agustin Martin Picard, Thomas Massena, Mathieu Serrurier

**Updated**: 2025-01-14T08:32:12Z

**Summary**: Orthogonal convolutional layers are the workhorse of multiple areas in machine learning, such as adversarial robustness, normalizing flows, GANs, and Lipschitzconstrained models. Their ability to preserve norms and ensure stable gradient propagation makes them valuable for a large range of problems. Despite their promise, the deployment of orthogonal convolution in large-scale applications is a significant challenge due to computational overhead and limited support for modern features like strides, dilations, group convolutions, and transposed convolutions.In this paper, we introduce AOC (Adaptative Orthogonal Convolution), a scalable method for constructing orthogonal convolutions, effectively overcoming these limitations. This advancement unlocks the construction of architectures that were previously considered impractical. We demonstrate through our experiments that our method produces expressive models that become increasingly efficient as they scale. To foster further advancement, we provide an open-source library implementing this method, available at https://github.com/thib-s/orthogonium.

**Link**: [arxiv](http://arxiv.org/abs/2501.07930v1),  [pdf](http://arxiv.org/pdf/2501.07930v1)

**Tags**: cs.AI cs.NE 



### Gandalf the Red: Adaptive Security for LLMs
**Authors**: Niklas Pfister, Václav Volhejn, Manuel Knott, Santiago Arias, Julia Bazińska, Mykhailo Bichurin, Alan Commike, Janet Darling, Peter Dienes, Matthew Fiedler, David Haber, Matthias Kraft, Marco Lancini, Max Mathys, Damián Pascual-Ortiz, Jakub Podolak, Adrià Romero-López, Kyriacos Shiarlis, Andreas Signer, Zsolt Terek, Athanasios Theocharis, Daniel Timbrell, Samuel Trautwein, Samuel Watts, Natalie Wu, Mateo Rojas-Carulla

**Updated**: 2025-01-14T08:30:49Z

**Summary**: Current evaluations of defenses against prompt attacks in large language model (LLM) applications often overlook two critical factors: the dynamic nature of adversarial behavior and the usability penalties imposed on legitimate users by restrictive defenses. We propose D-SEC (Dynamic Security Utility Threat Model), which explicitly separates attackers from legitimate users, models multi-step interactions, and rigorously expresses the security-utility in an optimizable form. We further address the shortcomings in existing evaluations by introducing Gandalf, a crowd-sourced, gamified red-teaming platform designed to generate realistic, adaptive attack datasets. Using Gandalf, we collect and release a dataset of 279k prompt attacks. Complemented by benign user data, our analysis reveals the interplay between security and utility, showing that defenses integrated in the LLM (e.g., system prompts) can degrade usability even without blocking requests. We demonstrate that restricted application domains, defense-in-depth, and adaptive defenses are effective strategies for building secure and useful LLM applications. Code is available at \href{https://github.com/lakeraai/dsec-gandalf}{\texttt{https://github.com/lakeraai/dsec-gandalf}}.

**Link**: [arxiv](http://arxiv.org/abs/2501.07927v1),  [pdf](http://arxiv.org/pdf/2501.07927v1)

**Tags**: cs.LG cs.AI cs.CL cs.CR 



### Large Language Model Interface for Home Energy Management Systems
**Authors**: François Michelon, Yihong Zhou, Thomas Morstyn

**Updated**: 2025-01-14T08:10:43Z

**Summary**: Home Energy Management Systems (HEMSs) help households tailor their electricity usage based on power system signals such as energy prices. This technology helps to reduce energy bills and offers greater demand-side flexibility that supports the power system stability. However, residents who lack a technical background may find it difficult to use HEMSs effectively, because HEMSs require well-formatted parameterization that reflects the characteristics of the energy resources, houses, and users' needs. Recently, Large-Language Models (LLMs) have demonstrated an outstanding ability in language understanding. Motivated by this, we propose an LLM-based interface that interacts with users to understand and parameterize their ``badly-formatted answers'', and then outputs well-formatted parameters to implement an HEMS. We further use Reason and Act method (ReAct) and few-shot prompting to enhance the LLM performance. Evaluating the interface performance requires multiple user--LLM interactions. To avoid the efforts in finding volunteer users and reduce the evaluation time, we additionally propose a method that uses another LLM to simulate users with varying expertise, ranging from knowledgeable to non-technical. By comprehensive evaluation, the proposed LLM-based HEMS interface achieves an average parameter retrieval accuracy of 88\%, outperforming benchmark models without ReAct and/or few-shot prompting.

**Link**: [arxiv](http://arxiv.org/abs/2501.07919v1),  [pdf](http://arxiv.org/pdf/2501.07919v1)

**Tags**: cs.AI F.2.2; I.2.7 



### UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts
**Authors**: Bo Yang, Qingping Yang, Yingwei Ma, Runtao Liu

**Updated**: 2025-01-14T07:57:26Z

**Summary**: The evaluation of mathematical reasoning capabilities is essential for advancing Artificial General Intelligence (AGI). While Large Language Models (LLMs) have shown impressive performance in solving mathematical problems, existing benchmarks such as GSM8K and MATH present limitations, including narrow problem definitions with specific numbers and reliance on predetermined rules that hinder accurate assessments of reasoning and generality. This paper introduces the UTMath Benchmark, a robust evaluation framework designed to assess LLMs through extensive unit tests, with a focus on both the accuracy and generality of model responses. It comprises 1,053 cutting-edge problems spanning nine mathematical domains, with an average of 68 test cases per problem. UTMath is highly challenging, with the best-performing model, o1-mini, solving only 32.57\% of the problems, followed by o1-preview at 27.16\%, and GPT-4o at 26.93\%. Furthermore, we present the Reasoning-to-Coding of Thoughts (RCoT) approach, which encourages LLMs to engage in explicit reasoning prior to code generation, thereby facilitating the production of more sophisticated solutions and enhancing overall performance and efficiency. Additionally, we also release the UTMath-Train training dataset (more than 70k samples), to support the community in further exploring mathematical reasoning. Our benchmark can be accessed via the following link: https://github.com/UTMathGroup/UTMath

**Link**: [arxiv](http://arxiv.org/abs/2411.07240v2),  [pdf](http://arxiv.org/pdf/2411.07240v2)

**Tags**: cs.CL cs.AI 



### Leveraging Metamemory Mechanisms for Enhanced Data-Free Code Generation   in LLMs
**Authors**: Shuai Wang, Liang Ding, Yibing Zhan, Yong Luo, Zheng He, Dapeng Tao

**Updated**: 2025-01-14T07:16:43Z

**Summary**: Automated code generation using large language models (LLMs) has gained attention due to its efficiency and adaptability. However, real-world coding tasks or benchmarks like HumanEval and StudentEval often lack dedicated training datasets, challenging existing few-shot prompting approaches that rely on reference examples. Inspired by human metamemory-a cognitive process involving recall and evaluation-we present a novel framework (namely M^2WF) for improving LLMs' one-time code generation. This approach enables LLMs to autonomously generate, evaluate, and utilize synthetic examples to enhance reliability and performance. Unlike prior methods, it minimizes dependency on curated data and adapts flexibly to various coding scenarios. Our experiments demonstrate significant improvements in coding benchmarks, offering a scalable and robust solution for data-free environments. The code and framework will be publicly available on GitHub and HuggingFace.

**Link**: [arxiv](http://arxiv.org/abs/2501.07892v1),  [pdf](http://arxiv.org/pdf/2501.07892v1)

**Tags**: cs.SE cs.AI 



### FoMo: A Foundation Model for Mobile Traffic Forecasting with Diffusion   Model
**Authors**: Haoye Chai, Xiaoqian Qi, Shiyuan Zhang, Yong Li

**Updated**: 2025-01-14T06:59:12Z

**Summary**: Mobile traffic forecasting allows operators to anticipate network dynamics and performance in advance, offering substantial potential for enhancing service quality and improving user experience. However, existing models are often task-oriented and are trained with tailored data, which limits their effectiveness in diverse mobile network tasks of Base Station (BS) deployment, resource allocation, energy optimization, etc. and hinders generalization across different urban environments. Foundation models have made remarkable strides across various domains of NLP and CV due to their multi-tasking adaption and zero/few-shot learning capabilities. In this paper, we propose an innovative Foundation model for Mo}bile traffic forecasting (FoMo), aiming to handle diverse forecasting tasks of short/long-term predictions and distribution generation across multiple cities to support network planning and optimization. FoMo combines diffusion models and transformers, where various spatio-temporal masks are proposed to enable FoMo to learn intrinsic features of different tasks, and a contrastive learning strategy is developed to capture the correlations between mobile traffic and urban contexts, thereby improving its transfer learning capability. Extensive experiments on 9 real-world datasets demonstrate that FoMo outperforms current models concerning diverse forecasting tasks and zero/few-shot learning, showcasing a strong universality.

**Link**: [arxiv](http://arxiv.org/abs/2410.15322v2),  [pdf](http://arxiv.org/pdf/2410.15322v2)

**Tags**: cs.LG cs.AI 



### FLM-101B: An Open LLM and How to Train It with $100K Budget
**Authors**: Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Xuying Meng, Siqi Fan, Peng Han, Jing Li, Li Du, Bowen Qin, Zheng Zhang, Aixin Sun, Yequan Wang

**Updated**: 2025-01-14T06:40:36Z

**Summary**: Large language models (LLMs) are considered important approaches towards foundational machine intelligence, achieving remarkable success in Natural Language Processing and multimodal tasks, among others. However, the carbon footprints and financial costs originating from heavy pre-training computation is a non-negligible issue. Progressive training methods, inspired by the neurogenesis process that grows neural structures, have shown potential to accelerate LLM pre-training. However, the algorithms, implementation, and practices for progressively training LLMs beyond 100B parameters remain underexplored. In this paper, we show that our model, namely FLM-101B, trained with our growth strategy under a budget of \$100K, reaches 80\% of the baselines' performances with only 10\% of their floating-point operations. We believe that further studies on progressive training will benefit the community by cutting down the costs and promoting green AI. The checkpoint of FLM-101B is released at https://huggingface.co/CofeAI/FLM-101B.

**Link**: [arxiv](http://arxiv.org/abs/2309.03852v3),  [pdf](http://arxiv.org/pdf/2309.03852v3)

**Tags**: cs.CL cs.AI 



### Exploring Gradient Subspaces: Addressing and Overcoming LoRA's   Limitations in Federated Fine-Tuning of Large Language Models
**Authors**: Navyansh Mahla, Kshitij Sharad Jadhav, Ganesh Ramakrishnan

**Updated**: 2025-01-14T06:25:54Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, particularly in task generalization for both text and vision data. While fine-tuning these models can significantly enhance their performance on specific downstream tasks, it often requires high-quality data that cannot be shared due to privacy concerns. Federated Learning (FL) offers a promising solution for collaborative training without direct data sharing. However, many parameter-efficient fine-tuning strategies for LLMs in FL, particularly those based on Low-Rank Adaptation (LoRA), face limitations. In this paper, we critically analyze the convergence and performance guarantees of popular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to constrained subspace learning of low-rank matrices. This limitation hinders effective fine-tuning of LLMs in federated settings. Through rigorous analytical and empirical evaluations, we demonstrate that direct weight averaging outperforms LoRA-based strategies, leading to superior performance for fine-tuned models. Our comprehensive comparison unmasks inefficiencies in LoRA approaches and underscores the advantages of direct weight aggregation. We extend our analysis to low-rank gradient-based optimizers, such as GaLore, used during local training steps. Our findings show that GaLore along with direct-weight aggregation is a more effective approach, outperforming federated LoRA methods like FlexLoRA and FFA-LoRA across both text and image modalities. While privacy remains paramount in FL discourse, our focus is on assessing performance outcomes of federated fine-tuned models and evaluating various FL frameworks from both theoretical and empirical perspectives. Our findings advocate reassessing the reliance on LoRA within FL contexts, paving the way for more efficient training methodologies.

**Link**: [arxiv](http://arxiv.org/abs/2410.23111v6),  [pdf](http://arxiv.org/pdf/2410.23111v6)

**Tags**: cs.LG cs.AI 



### What Makes Cryptic Crosswords Challenging for LLMs?
**Authors**: Abdelrahman Sadallah, Daria Kotova, Ekaterina Kochmar

**Updated**: 2025-01-14T06:06:54Z

**Summary**: Cryptic crosswords are puzzles that rely on general knowledge and the solver's ability to manipulate language on different levels, dealing with various types of wordplay. Previous research suggests that solving such puzzles is challenging even for modern NLP models, including Large Language Models (LLMs). However, there is little to no research on the reasons for their poor performance on this task. In this paper, we establish the benchmark results for three popular LLMs: Gemma2, LLaMA3 and ChatGPT, showing that their performance on this task is still significantly below that of humans. We also investigate why these models struggle to achieve superior performance. We release our code and introduced datasets at https://github.com/bodasadallah/decrypting-crosswords.

**Link**: [arxiv](http://arxiv.org/abs/2412.09012v2),  [pdf](http://arxiv.org/pdf/2412.09012v2)

**Tags**: cs.CL cs.AI 



### ReARTeR: Retrieval-Augmented Reasoning with Trustworthy Process   Rewarding
**Authors**: Zhongxiang Sun, Qipeng Wang, Weijie Yu, Xiaoxue Zang, Kai Zheng, Jun Xu, Xiao Zhang, Song Yang, Han Li

**Updated**: 2025-01-14T05:56:26Z

**Summary**: Retrieval-Augmented Generation (RAG) systems for Large Language Models (LLMs) hold promise in knowledge-intensive tasks but face limitations in complex multi-step reasoning. While recent methods have integrated RAG with chain-of-thought reasoning or test-time search using Process Reward Models (PRMs), these approaches encounter challenges such as a lack of explanations, bias in PRM training data, early-step bias in PRM scores, and insufficient post-training optimization of reasoning potential. To address these issues, we propose Retrieval-Augmented Reasoning through Trustworthy Process Rewarding (ReARTeR), a framework that enhances RAG systems' reasoning capabilities through post-training and test-time scaling. At test time, ReARTeR introduces Trustworthy Process Rewarding via a Process Reward Model for accurate scalar scoring and a Process Explanation Model (PEM) for generating natural language explanations, enabling step refinement. During post-training, it utilizes Monte Carlo Tree Search guided by Trustworthy Process Rewarding to collect high-quality step-level preference data, optimized through Iterative Preference Optimization. ReARTeR addresses three core challenges: (1) misalignment between PRM and PEM, tackled through off-policy preference learning; (2) bias in PRM training data, mitigated by balanced annotation methods and stronger annotations for challenging examples; and (3) early-step bias in PRM, resolved through a temporal-difference-based look-ahead search strategy. Experimental results on multi-step reasoning benchmarks demonstrate significant improvements, underscoring ReARTeR's potential to advance the reasoning capabilities of RAG systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.07861v1),  [pdf](http://arxiv.org/pdf/2501.07861v1)

**Tags**: cs.CL 



### Failure Diagnosis in Microservice Systems: A Comprehensive Survey and   Analysis
**Authors**: Shenglin Zhang, Sibo Xia, Wenzhao Fan, Binpeng Shi, Xiao Xiong, Zhenyu Zhong, Minghua Ma, Yongqian Sun, Dan Pei

**Updated**: 2025-01-14T05:49:10Z

**Summary**: Widely adopted for their scalability and flexibility, modern microservice systems present unique failure diagnosis challenges due to their independent deployment and dynamic interactions. This complexity can lead to cascading failures that negatively impact operational efficiency and user experience. Recognizing the critical role of fault diagnosis in improving the stability and reliability of microservice systems, researchers have conducted extensive studies and achieved a number of significant results. This survey provides an exhaustive review of 98 scientific papers from 2003 to the present, including a thorough examination and elucidation of the fundamental concepts, system architecture, and problem statement. It also includes a qualitative analysis of the dimensions, providing an in-depth discussion of current best practices and future directions, aiming to further its development and application. In addition, this survey compiles publicly available datasets, toolkits, and evaluation metrics to facilitate the selection and validation of techniques for practitioners.

**Link**: [arxiv](http://arxiv.org/abs/2407.01710v2),  [pdf](http://arxiv.org/pdf/2407.01710v2)

**Tags**: cs.SE 



### Hierarchical Repository-Level Code Summarization for Business   Applications Using Local LLMs
**Authors**: Nilesh Dhulshette, Sapan Shah, Vinay Kulkarni

**Updated**: 2025-01-14T05:48:27Z

**Summary**: In large-scale software development, understanding the functionality and intent behind complex codebases is critical for effective development and maintenance. While code summarization has been widely studied, existing methods primarily focus on smaller code units, such as functions, and struggle with larger code artifacts like files and packages. Additionally, current summarization models tend to emphasize low-level implementation details, often overlooking the domain and business context that are crucial for real-world applications. This paper proposes a two-step hierarchical approach for repository-level code summarization, tailored to business applications. First, smaller code units such as functions and variables are identified using syntax analysis and summarized with local LLMs. These summaries are then aggregated to generate higher-level file and package summaries. To ensure the summaries are grounded in business context, we design custom prompts that capture the intended purpose of code artifacts based on the domain and problem context of the business application. We evaluate our approach on a business support system (BSS) for the telecommunications domain, showing that syntax analysis-based hierarchical summarization improves coverage, while business-context grounding enhances the relevance of the generated summaries.

**Link**: [arxiv](http://arxiv.org/abs/2501.07857v1),  [pdf](http://arxiv.org/pdf/2501.07857v1)

**Tags**: cs.SE cs.AI 



### Optimizing Language Models for Grammatical Acceptability: A Comparative   Study of Fine-Tuning Techniques
**Authors**: Shobhit Ratan, Farley Knight, Ghada Jerfel, Sze Chung Ho

**Updated**: 2025-01-14T05:41:09Z

**Summary**: This study explores the fine-tuning (FT) of the Open Pre-trained Transformer (OPT-125M) for grammatical acceptability tasks using the CoLA dataset. By comparing Vanilla-Fine-Tuning (VFT), Pattern-Based-Fine-Tuning (PBFT), and Parameter-Efficient Fine-Tuning techniques (PEFT) like Low-Rank Adaptation (LoRA), we demonstrate significant improvements in computational efficiency while maintaining high accuracy. Our experiments reveal that while VFT achieves the highest accuracy (81.2%), LoRA enhancing FT by reducing memory usage and iteration time by more than 50%, and increases accuracy in PBFT case. Context Distillation (CD), though computationally efficient, underperformed with accuracy around 31%. Our findings contribute to democratizing access to large language models (LLM) by reducing computational barriers.

**Link**: [arxiv](http://arxiv.org/abs/2501.07853v1),  [pdf](http://arxiv.org/pdf/2501.07853v1)

**Tags**: cs.CL cs.AI 



### Evaluating Mathematical Reasoning Beyond Accuracy
**Authors**: Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, Pengfei Liu

**Updated**: 2025-01-14T05:39:40Z

**Summary**: The leaderboard of Large Language Models (LLMs) in mathematical tasks has been continuously updated. However, the majority of evaluations focus solely on the final results, neglecting the quality of the intermediate steps. This oversight can mask underlying problems, such as logical errors or unnecessary steps in the reasoning process. To measure reasoning beyond final-answer accuracy, we introduce ReasonEval, a new methodology for evaluating the quality of reasoning steps. ReasonEval employs validity and redundancy to characterize the reasoning quality, as well as accompanying LLMs to assess them automatically. We explore different design options for the LLM-based evaluators and empirically demonstrate that ReasonEval, when instantiated with base models possessing strong mathematical knowledge and trained with high-quality labeled data, consistently outperforms baseline methods in the meta-evaluation datasets. We also highlight the strong generalization capabilities of ReasonEval. By utilizing ReasonEval to evaluate LLMs specialized in math, we find that an increase in final-answer accuracy does not necessarily guarantee an improvement in the overall quality of the reasoning steps for challenging mathematical problems. Additionally, we observe that ReasonEval can play a significant role in data selection. We open-source the best-performing model, meta-evaluation script, and all evaluation results to facilitate future research.

**Link**: [arxiv](http://arxiv.org/abs/2404.05692v2),  [pdf](http://arxiv.org/pdf/2404.05692v2)

**Tags**: cs.CL 



### AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering   Benchmark Dataset
**Authors**: Tobi Olatunji, Charles Nimo, Abraham Owodunni, Tassallah Abdullahi, Emmanuel Ayodele, Mardhiyah Sanni, Chinemelu Aka, Folafunmi Omofoye, Foutse Yuehgoh, Timothy Faniran, Bonaventure F. P. Dossou, Moshood Yekini, Jonas Kemp, Katherine Heller, Jude Chidubem Omeke, Chidi Asuzu MD, Naome A. Etori, Aimérou Ndiaye, Ifeoma Okoh, Evans Doe Ocansey, Wendy Kinara, Michael Best, Irfan Essa, Stephen Edward Moore, Chris Fourie, Mercy Nyamewaa Asiedu

**Updated**: 2025-01-14T05:35:08Z

**Summary**: Recent advancements in large language model(LLM) performance on medical multiple choice question (MCQ) benchmarks have stimulated interest from healthcare providers and patients globally. Particularly in low-and middle-income countries (LMICs) facing acute physician shortages and lack of specialists, LLMs offer a potentially scalable pathway to enhance healthcare access and reduce costs. However, their effectiveness in the Global South, especially across the African continent, remains to be established. In this work, we introduce AfriMed-QA, the first large scale Pan-African English multi-specialty medical Question-Answering (QA) dataset, 15,000 questions (open and closed-ended) sourced from over 60 medical schools across 16 countries, covering 32 medical specialties. We further evaluate 30 LLMs across multiple axes including correctness and demographic bias. Our findings show significant performance variation across specialties and geographies, MCQ performance clearly lags USMLE (MedQA). We find that biomedical LLMs underperform general models and smaller edge-friendly LLMs struggle to achieve a passing score. Interestingly, human evaluations show a consistent consumer preference for LLM answers and explanations when compared with clinician answers.

**Link**: [arxiv](http://arxiv.org/abs/2411.15640v3),  [pdf](http://arxiv.org/pdf/2411.15640v3)

**Tags**: cs.CL 



### AdaSociety: An Adaptive Environment with Social Structures for   Multi-Agent Decision-Making
**Authors**: Yizhe Huang, Xingbo Wang, Hao Liu, Fanqi Kong, Aoyang Qin, Min Tang, Song-Chun Zhu, Mingjie Bi, Siyuan Qi, Xue Feng

**Updated**: 2025-01-14T05:23:03Z

**Summary**: Traditional interactive environments limit agents' intelligence growth with fixed tasks. Recently, single-agent environments address this by generating new tasks based on agent actions, enhancing task diversity. We consider the decision-making problem in multi-agent settings, where tasks are further influenced by social connections, affecting rewards and information access. However, existing multi-agent environments lack a combination of adaptive physical surroundings and social connections, hindering the learning of intelligent behaviors. To address this, we introduce AdaSociety, a customizable multi-agent environment featuring expanding state and action spaces, alongside explicit and alterable social structures. As agents progress, the environment adaptively generates new tasks with social structures for agents to undertake. In AdaSociety, we develop three mini-games showcasing distinct social structures and tasks. Initial results demonstrate that specific social structures can promote both individual and collective benefits, though current reinforcement learning and LLM-based algorithms show limited effectiveness in leveraging social structures to enhance performance. Overall, AdaSociety serves as a valuable research platform for exploring intelligence in diverse physical and social settings. The code is available at https://github.com/bigai-ai/AdaSociety.

**Link**: [arxiv](http://arxiv.org/abs/2411.03865v4),  [pdf](http://arxiv.org/pdf/2411.03865v4)

**Tags**: cs.MA cs.AI cs.GT cs.LG cs.SI 



### Unveiling Provider Bias in Large Language Models for Code Generation
**Authors**: Xiaoyu Zhang, Juan Zhai, Shiqing Ma, Qingshuang Bao, Weipeng Jiang, Chao Shen, Yang Liu

**Updated**: 2025-01-14T05:21:27Z

**Summary**: Large Language Models (LLMs) have emerged as the new recommendation engines, outperforming traditional methods in both capability and scope, particularly in code generation applications. Our research reveals a novel provider bias in LLMs, namely without explicit input prompts, these models show systematic preferences for services from specific providers in their recommendations (e.g., favoring Google Cloud over Microsoft Azure). This bias holds significant implications for market dynamics and societal equilibrium, potentially promoting digital monopolies. It may also deceive users and violate their expectations, leading to various consequences. This paper presents the first comprehensive empirical study of provider bias in LLM code generation. We develop a systematic methodology encompassing an automated pipeline for dataset generation, incorporating 6 distinct coding task categories and 30 real-world application scenarios. Our analysis encompasses over 600,000 LLM-generated responses across seven state-of-the-art models, utilizing approximately 500 million tokens (equivalent to \$5,000+ in computational costs). The study evaluates both the generated code snippets and their embedded service provider selections to quantify provider bias. Additionally, we conduct a comparative analysis of seven debiasing prompting techniques to assess their efficacy in mitigating these biases. Our findings demonstrate that LLMs exhibit significant provider preferences, predominantly favoring services from Google and Amazon, and can autonomously modify input code to incorporate their preferred providers without users' requests. Notably, we observe discrepancies between providers recommended in conversational contexts versus those implemented in generated code. The complete dataset and analysis results are available in our repository.

**Link**: [arxiv](http://arxiv.org/abs/2501.07849v1),  [pdf](http://arxiv.org/pdf/2501.07849v1)

**Tags**: cs.SE cs.AI cs.CR 



### Nigerian Software Engineer or American Data Scientist? GitHub Profile   Recruitment Bias in Large Language Models
**Authors**: Takashi Nakano, Kazumasa Shimari, Raula Gaikovina Kula, Christoph Treude, Marc Cheong, Kenichi Matsumoto

**Updated**: 2025-01-14T05:20:53Z

**Summary**: Large Language Models (LLMs) have taken the world by storm, demonstrating their ability not only to automate tedious tasks, but also to show some degree of proficiency in completing software engineering tasks. A key concern with LLMs is their "black-box" nature, which obscures their internal workings and could lead to societal biases in their outputs. In the software engineering context, in this early results paper, we empirically explore how well LLMs can automate recruitment tasks for a geographically diverse software team. We use OpenAI's ChatGPT to conduct an initial set of experiments using GitHub User Profiles from four regions to recruit a six-person software development team, analyzing a total of 3,657 profiles over a five-year period (2019-2023). Results indicate that ChatGPT shows preference for some regions over others, even when swapping the location strings of two profiles (counterfactuals). Furthermore, ChatGPT was more likely to assign certain developer roles to users from a specific country, revealing an implicit bias. Overall, this study reveals insights into the inner workings of LLMs and has implications for mitigating such societal biases in these models.

**Link**: [arxiv](http://arxiv.org/abs/2409.12544v2),  [pdf](http://arxiv.org/pdf/2409.12544v2)

**Tags**: cs.SE 



### Reasoning with Graphs: Structuring Implicit Knowledge to Enhance LLMs   Reasoning
**Authors**: Haoyu Han, Yaochen Xie, Hui Liu, Xianfeng Tang, Sreyashi Nag, William Headden, Hui Liu, Yang Li, Chen Luo, Shuiwang Ji, Qi He, Jiliang Tang

**Updated**: 2025-01-14T05:18:20Z

**Summary**: Large language models (LLMs) have demonstrated remarkable success across a wide range of tasks; however, they still encounter challenges in reasoning tasks that require understanding and inferring relationships between distinct pieces of information within text sequences. This challenge is particularly pronounced in tasks involving multi-step processes, such as logical reasoning and multi-hop question answering, where understanding implicit relationships between entities and leveraging multi-hop connections in the given context are crucial. Graphs, as fundamental data structures, explicitly represent pairwise relationships between entities, thereby offering the potential to enhance LLMs' reasoning capabilities. External graphs have proven effective in supporting LLMs across multiple tasks. However, in many reasoning tasks, no pre-existing graph structure is provided. Can we structure implicit knowledge derived from context into graphs to assist LLMs in reasoning? In this paper, we propose Reasoning with Graphs (RwG) by first constructing explicit graphs from the context and then leveraging these graphs to enhance LLM reasoning performance on reasoning tasks. Extensive experiments demonstrate the effectiveness of the proposed method in improving both logical reasoning and multi-hop question answering tasks.

**Link**: [arxiv](http://arxiv.org/abs/2501.07845v1),  [pdf](http://arxiv.org/pdf/2501.07845v1)

**Tags**: cs.CL 



### Securing Distributed Network Digital Twin Systems Against Model   Poisoning Attacks
**Authors**: Zifan Zhang, Minghong Fang, Mingzhe Chen, Gaolei Li, Xi Lin, Yuchen Liu

**Updated**: 2025-01-14T05:02:24Z

**Summary**: In the era of 5G and beyond, the increasing complexity of wireless networks necessitates innovative frameworks for efficient management and deployment. Digital twins (DTs), embodying real-time monitoring, predictive configurations, and enhanced decision-making capabilities, stand out as a promising solution in this context. Within a time-series data-driven framework that effectively maps wireless networks into digital counterparts, encapsulated by integrated vertical and horizontal twinning phases, this study investigates the security challenges in distributed network DT systems, which potentially undermine the reliability of subsequent network applications such as wireless traffic forecasting. Specifically, we consider a minimal-knowledge scenario for all attackers, in that they do not have access to network data and other specialized knowledge, yet can interact with previous iterations of server-level models. In this context, we spotlight a novel fake traffic injection attack designed to compromise a distributed network DT system for wireless traffic prediction. In response, we then propose a defense mechanism, termed global-local inconsistency detection (GLID), to counteract various model poisoning threats. GLID strategically removes abnormal model parameters that deviate beyond a particular percentile range, thereby fortifying the security of network twinning process. Through extensive experiments on real-world wireless traffic datasets, our experimental evaluations show that both our attack and defense strategies significantly outperform existing baselines, highlighting the importance of security measures in the design and implementation of DTs for 5G and beyond network systems.

**Link**: [arxiv](http://arxiv.org/abs/2407.01917v2),  [pdf](http://arxiv.org/pdf/2407.01917v2)

**Tags**: cs.NI cs.CR cs.DC 



### A Driver Advisory System Based on Large Language Model for High-speed   Train
**Authors**: Y. C. Luo, J. Xun, W. Wang, R. Z. Zhang, Z. C. Zhao

**Updated**: 2025-01-14T04:41:03Z

**Summary**: With the rapid development of China high-speed railway, drivers face increasingly significant technical challenges during operations, such as fault handling. Currently, drivers depend on the onboard mechanic when facing technical issues, for instance, traction loss or sensor faults. This dependency can hinder effective operation, even lead to accidents, while waiting for faults to be addressed. To enhance the accuracy and explainability of actions during fault handling, an Intelligent Driver Advisory System (IDAS) framework based on a large language model (LLM) named IDAS-LLM, is introduced. Initially, domain-fine-tuning of the LLM is performed using a constructed railway knowledge question-and-answer dataset to improve answer accuracy in railway-related questions. Subsequently, integration of the Retrieval-augmented Generation (RAG) architecture is pursued for system design to enhance the explainability of generated responses. Comparative experiments are conducted using the constructed railway driving knowledge assessment dataset. Results indicate that domain-fine-tuned LLMs show an improvement in answer accuracy by an average of 10%, outperforming some current mainstream LLMs. Additionally, the inclusion of the RAG framework increases the average recall rate of question-and-answer sessions by about 4%. Finally, the fault handling capability of IDAS-LLM is demonstrated through simulations of real operational scenarios, proving that the proposed framework has practical application prospects.

**Link**: [arxiv](http://arxiv.org/abs/2501.07837v1),  [pdf](http://arxiv.org/pdf/2501.07837v1)

**Tags**: cs.AI 



### Flow: A Modular Approach to Automated Agentic Workflow Generation
**Authors**: Boye Niu, Yiliao Song, Kai Lian, Yifan Shen, Yu Yao, Kun Zhang, Tongliang Liu

**Updated**: 2025-01-14T04:35:37Z

**Summary**: Multi-agent frameworks powered by large language models (LLMs) have demonstrated great success in automated planning and task execution. However, the effective adjustment of Agentic workflows during execution has not been well-studied. A effective workflow adjustment is crucial, as in many real-world scenarios, the initial plan must adjust to unforeseen challenges and changing conditions in real-time to ensure the efficient execution of complex tasks. In this paper, we define workflows as an activity-on-vertex (AOV) graphs. We continuously refine the workflow by dynamically adjusting task allocations based on historical performance and previous AOV with LLM agents. To further enhance system performance, we emphasize modularity in workflow design based on measuring parallelism and dependence complexity. Our proposed multi-agent framework achieved efficient sub-task concurrent execution, goal achievement, and error tolerance. Empirical results across different practical tasks demonstrate dramatic improvements in the efficiency of multi-agent frameworks through dynamic workflow updating and modularization.

**Link**: [arxiv](http://arxiv.org/abs/2501.07834v1),  [pdf](http://arxiv.org/pdf/2501.07834v1)

**Tags**: cs.AI cs.LG cs.MA 



### ELDER: Enhancing Lifelong Model Editing with Mixture-of-LoRA
**Authors**: Jiaang Li, Quan Wang, Zhongnan Wang, Yongdong Zhang, Zhendong Mao

**Updated**: 2025-01-14T04:25:23Z

**Summary**: Large language models (LLMs) require model editing to efficiently update specific knowledge within them and avoid factual errors. Most model editing methods are solely designed for single-time use and result in a significant forgetting effect in lifelong editing scenarios, where sequential edits are conducted over time. Previous approaches manage sequential edits by freezing original parameters and discretely allocating new parameters for each knowledge update. However, these methods lack robustness to minor input variations due to the discrete mapping between data and parameters. To overcome this challenge, we propose ELDER, a novel approach to create a continuous association between data and adapters. ELDER integrates multiple LoRAs through a router network and is trained to establish a smooth data-adapter association, thereby enhancing the edit robustness and generalization of semantically equivalent inputs. To ensure inputs containing the same knowledge will be processed by the same LoRAs, we design a novel loss to guide the model link LoRA allocations with edit knowledge. Furthermore, we propose a deferral mechanism to retain the original LLM capabilities post-edit. Extensive experiments on GPT-2 XL and LLaMA2-7B demonstrate that ELDER effectively edits models in the lifelong setting, outperforming eight baselines while exhibiting strong scalability and preserving LLMs' general abilities on downstream tasks. Our code is available at https://github.com/JiaangL/ELDER.

**Link**: [arxiv](http://arxiv.org/abs/2408.11869v3),  [pdf](http://arxiv.org/pdf/2408.11869v3)

**Tags**: cs.CL cs.AI cs.LG 



### Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial   Generation
**Authors**: Zerui Xu, Fang Wu, Yuanyuan Zhang, Yue Zhao

**Updated**: 2025-01-14T04:19:49Z

**Summary**: Machine learning (ML) exhibits promise in the clinical domain. However, it is constrained by data scarcity and ethical considerations, as the generation of clinical trials presents significant challenges due to stringent privacy regulations, high costs, and the extended duration required for conducting studies with human participants. Despite the advancements of large language models (LLMs) in general generation tasks, their potential in facilitating the generation of synthetic clinical trials is under-explored. To address this gap, we introduce a novel Retrieval-Reasoning few-shot framework that leverages LLMs to generate artificial yet realistic and diverse clinical trials with binary success/failure labels. Experiments conducted on real clinical trials from the \url{ClinicalTrials.gov} database demonstrate that our synthetic data can effectively augment real datasets. Furthermore, by fine-tuning a pre-trained model as a binary classifier on synthetic clinical trial datasets, we demonstrate that this augmentation enhances model training for downstream tasks such as trial outcome prediction. Our findings suggest that LLMs for synthetic clinical trial generation hold promise for accelerating clinical research and upholding ethical standards for patient privacy. The code is publicly available at https://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4.

**Link**: [arxiv](http://arxiv.org/abs/2410.12476v2),  [pdf](http://arxiv.org/pdf/2410.12476v2)

**Tags**: cs.CL cs.LG 



### AI Foundation Models for Wearable Movement Data in Mental Health   Research
**Authors**: Franklin Y. Ruan, Aiwei Zhang, Jenny Y. Oh, SouYoung Jin, Nicholas C. Jacobson

**Updated**: 2025-01-14T04:10:46Z

**Summary**: Pretrained foundation models and transformer architectures have driven the success of large language models (LLMs) and other modern AI breakthroughs. However, similar advancements in health data modeling remain limited due to the need for innovative adaptations. Wearable movement data offers a valuable avenue for exploration, as it's a core feature in nearly all commercial smartwatches, well established in clinical and mental health research, and the sequential nature of the data shares similarities to language. We introduce the Pretrained Actigraphy Transformer (PAT), the first open source foundation model designed for time-series wearable movement data. Leveraging transformer-based architectures and novel techniques, such as patch embeddings, and pretraining on data from 29,307 participants in a national U.S. sample, PAT achieves state-of-the-art performance in several mental health prediction tasks. PAT is also lightweight and easily interpretable, making it a robust tool for mental health research.   GitHub: https://github.com/njacobsonlab/Pretrained-Actigraphy-Transformer/

**Link**: [arxiv](http://arxiv.org/abs/2411.15240v3),  [pdf](http://arxiv.org/pdf/2411.15240v3)

**Tags**: cs.LG cs.AI cs.HC q-bio.QM 



### Real-time Verification and Refinement of Language Model Text Generation
**Authors**: Joonho Ko, Jinheon Baek, Sung Ju Hwang

**Updated**: 2025-01-14T03:59:48Z

**Summary**: Large language models (LLMs) have shown remarkable performance across a wide range of natural language tasks. However, a critical challenge remains in that they sometimes generate factually incorrect answers. To address this, while many previous work has focused on identifying errors in their generation and further refining them, they are slow in deployment since they are designed to verify the response from LLMs only after their entire generation (from the first to last tokens) is done. Further, we observe that once LLMs generate incorrect tokens early on, there is a higher likelihood that subsequent tokens will also be factually incorrect. To this end, in this work, we propose Streaming-VR (Streaming Verification and Refinement), a novel approach designed to enhance the efficiency of verification and refinement of LLM outputs. Specifically, the proposed Streaming-VR enables on-the-fly verification and correction of tokens as they are being generated, similar to a streaming process, ensuring that each subset of tokens is checked and refined in real-time by another LLM as the LLM constructs its response. Through comprehensive evaluations on multiple datasets, we demonstrate that our approach not only enhances the factual accuracy of LLMs, but also offers a more efficient solution compared to prior refinement methods.

**Link**: [arxiv](http://arxiv.org/abs/2501.07824v1),  [pdf](http://arxiv.org/pdf/2501.07824v1)

**Tags**: cs.CL cs.AI cs.LG 



### 3UR-LLM: An End-to-End Multimodal Large Language Model for 3D Scene   Understanding
**Authors**: Haomiao Xiong, Yunzhi Zhuge, Jiawen Zhu, Lu Zhang, Huchuan Lu

**Updated**: 2025-01-14T03:50:23Z

**Summary**: Multi-modal Large Language Models (MLLMs) exhibit impressive capabilities in 2D tasks, yet encounter challenges in discerning the spatial positions, interrelations, and causal logic in scenes when transitioning from 2D to 3D representations. We find that the limitations mainly lie in: i) the high annotation cost restricting the scale-up of volumes of 3D scene data, and ii) the lack of a straightforward and effective way to perceive 3D information which results in prolonged training durations and complicates the streamlined framework. To this end, we develop pipeline based on open-source 2D MLLMs and LLMs to generate high-quality 3D-text pairs and construct 3DS-160K , to enhance the pre-training process. Leveraging this high-quality pre-training data, we introduce the 3UR-LLM model, an end-to-end 3D MLLM designed for precise interpretation of 3D scenes, showcasing exceptional capability in navigating the complexities of the physical world. 3UR-LLM directly receives 3D point cloud as input and project 3D features fused with text instructions into a manageable set of tokens. Considering the computation burden derived from these hybrid tokens, we design a 3D compressor module to cohesively compress the 3D spatial cues and textual narrative. 3UR-LLM achieves promising performance with respect to the previous SOTAs, for instance, 3UR-LLM exceeds its counterparts by 7.1\% CIDEr on ScanQA, while utilizing fewer training resources. The code and model weights for 3UR-LLM and the 3DS-160K benchmark are available at 3UR-LLM.

**Link**: [arxiv](http://arxiv.org/abs/2501.07819v1),  [pdf](http://arxiv.org/pdf/2501.07819v1)

**Tags**: cs.CV 



### A Multi-Encoder Frozen-Decoder Approach for Fine-Tuning Large Language   Models
**Authors**: Kaustubh D. Dhole

**Updated**: 2025-01-14T03:43:23Z

**Summary**: Among parameter-efficient fine-tuning methods, freezing has emerged as a popular strategy for speeding up training, reducing catastrophic forgetting, and improving downstream performance. We investigate the impact of freezing the decoder in a multi-task setup comprising diverse natural language tasks, aiming to reduce deployment overhead and enhance portability to novel tasks. Our experiments, conducted by fine-tuning both individual and multi-task setups on the AlexaTM model, reveal that freezing decoders is highly effective for tasks with natural language outputs and mitigates catastrophic forgetting in multilingual tasks. However, we find that pairing frozen decoders with a larger model can effectively maintain or even enhance performance in structured and QA tasks, making it a viable strategy for a broader range of task types.

**Link**: [arxiv](http://arxiv.org/abs/2501.07818v1),  [pdf](http://arxiv.org/pdf/2501.07818v1)

**Tags**: cs.CL cs.AI cs.LG I.5.1; I.5.2; I.2.7 



### Energy-Efficient Split Learning for Fine-Tuning Large Language Models in   Edge Networks
**Authors**: Zuguang Li, Shaohua Wu, Liang Li, Songge Zhang

**Updated**: 2025-01-14T03:27:10Z

**Summary**: In this letter, we propose an energy-efficient split learning (SL) framework for fine-tuning large language models (LLMs) using geo-distributed personal data at the network edge, where LLMs are split and alternately across massive mobile devices and an edge server. Considering the device heterogeneity and channel dynamics in edge networks, a \underline{C}ut l\underline{A}yer and computing \underline{R}esource \underline{D}ecision (CARD) algorithm is developed to minimize training delay and energy consumption. Simulation results demonstrate that the proposed approach reduces the average training delay and server's energy consumption by 70.8% and 53.1%, compared to the benchmarks, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2412.00090v2),  [pdf](http://arxiv.org/pdf/2412.00090v2)

**Tags**: cs.LG cs.CL cs.DC 



### Agent-Centric Projection of Prompting Techniques and Implications for   Synthetic Training Data for Large Language Models
**Authors**: Dhruv Dhamani, Mary Lou Maher

**Updated**: 2025-01-14T03:26:43Z

**Summary**: Recent advances in prompting techniques and multi-agent systems for Large Language Models (LLMs) have produced increasingly complex approaches. However, we lack a framework for characterizing and comparing prompting techniques or understanding their relationship to multi-agent LLM systems. This position paper introduces and explains the concepts of linear contexts (a single, continuous sequence of interactions) and non-linear contexts (branching or multi-path) in LLM systems. These concepts enable the development of an agent-centric projection of prompting techniques, a framework that can reveal deep connections between prompting strategies and multi-agent systems. We propose three conjectures based on this framework: (1) results from non-linear prompting techniques can predict outcomes in equivalent multi-agent systems, (2) multi-agent system architectures can be replicated through single-LLM prompting techniques that simulate equivalent interaction patterns, and (3) these equivalences suggest novel approaches for generating synthetic training data. We argue that this perspective enables systematic cross-pollination of research findings between prompting and multi-agent domains, while providing new directions for improving both the design and training of future LLM systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.07815v1),  [pdf](http://arxiv.org/pdf/2501.07815v1)

**Tags**: cs.AI cs.CL cs.MA 



### Talk to Right Specialists: Routing and Planning in Multi-agent System   for Question Answering
**Authors**: Feijie Wu, Zitao Li, Fei Wei, Yaliang Li, Bolin Ding, Jing Gao

**Updated**: 2025-01-14T03:25:26Z

**Summary**: Leveraging large language models (LLMs), an agent can utilize retrieval-augmented generation (RAG) techniques to integrate external knowledge and increase the reliability of its responses. Current RAG-based agents integrate single, domain-specific knowledge sources, limiting their ability and leading to hallucinated or inaccurate responses when addressing cross-domain queries. Integrating multiple knowledge bases into a unified RAG-based agent raises significant challenges, including increased retrieval overhead and data sovereignty when sensitive data is involved. In this work, we propose RopMura, a novel multi-agent system that addresses these limitations by incorporating highly efficient routing and planning mechanisms. RopMura features two key components: a router that intelligently selects the most relevant agents based on knowledge boundaries and a planner that decomposes complex multi-hop queries into manageable steps, allowing for coordinating cross-domain responses. Experimental results demonstrate that RopMura effectively handles both single-hop and multi-hop queries, with the routing mechanism enabling precise answers for single-hop queries and the combined routing and planning mechanisms achieving accurate, multi-step resolutions for complex queries.

**Link**: [arxiv](http://arxiv.org/abs/2501.07813v1),  [pdf](http://arxiv.org/pdf/2501.07813v1)

**Tags**: cs.MA cs.AI cs.CL 



### CodeCoR: An LLM-Based Self-Reflective Multi-Agent Framework for Code   Generation
**Authors**: Ruwei Pan, Hongyu Zhang, Chao Liu

**Updated**: 2025-01-14T03:21:10Z

**Summary**: Code generation aims to produce code that fulfills requirements written in natural languages automatically. Large language Models (LLMs) like ChatGPT have demonstrated promising effectiveness in this area. Nonetheless, these LLMs often fail to ensure the syntactic and semantic correctness of the generated code. Recently, researchers proposed multi-agent frameworks that guide LLMs with different prompts to analyze programming tasks, generate code, perform testing in a sequential workflow. However, the performance of the workflow is not robust as the code generation depends on the performance of each agent. To address this challenge, we propose CodeCoR, a self-reflective multi-agent framework that evaluates the effectiveness of each agent and their collaborations. Specifically, for a given task description, four agents in CodeCoR generate prompts, code, test cases, and repair advice, respectively. Each agent generates more than one output and prunes away the low-quality ones. The generated code is tested in the local environment: the code that fails to pass the generated test cases is sent to the repair agent and the coding agent re-generates the code based on repair advice. Finally, the code that passes the most number of generated test cases is returned to users. Our experiments on four widely used datasets, HumanEval, HumanEval-ET, MBPP, and MBPP-ET, demonstrate that CodeCoR significantly outperforms existing baselines (e.g., CodeCoT and MapCoder), achieving an average Pass@1 score of 77.8%.

**Link**: [arxiv](http://arxiv.org/abs/2501.07811v1),  [pdf](http://arxiv.org/pdf/2501.07811v1)

**Tags**: cs.SE 



### A Low-cost and Ultra-lightweight Binary Neural Network for Traffic   Signal Recognition
**Authors**: Mingke Xiao, Yue Su, Liang Yu, Guanglong Qu, Yutong Jia, Yukuan Chang, Xu Zhang

**Updated**: 2025-01-14T03:19:10Z

**Summary**: The deployment of neural networks in vehicle platforms and wearable Artificial Intelligence-of-Things (AIOT) scenarios has become a research area that has attracted much attention. With the continuous evolution of deep learning technology, many image classification models are committed to improving recognition accuracy, but this is often accompanied by problems such as large model resource usage, complex structure, and high power consumption, which makes it challenging to deploy on resource-constrained platforms. Herein, we propose an ultra-lightweight binary neural network (BNN) model designed for hardware deployment, and conduct image classification research based on the German Traffic Sign Recognition Benchmark (GTSRB) dataset. In addition, we also verify it on the Chinese Traffic Sign (CTS) and Belgian Traffic Sign (BTS) datasets. The proposed model shows excellent recognition performance with an accuracy of up to 97.64%, making it one of the best performing BNN models in the GTSRB dataset. Compared with the full-precision model, the accuracy loss is controlled within 1%, and the parameter storage overhead of the model is only 10% of that of the full-precision model. More importantly, our network model only relies on logical operations and low-bit width fixed-point addition and subtraction operations during the inference phase, which greatly simplifies the design complexity of the processing element (PE). Our research shows the great potential of BNN in the hardware deployment of computer vision models, especially in the field of computer vision tasks related to autonomous driving.

**Link**: [arxiv](http://arxiv.org/abs/2501.07808v1),  [pdf](http://arxiv.org/pdf/2501.07808v1)

**Tags**: cs.AI cs.CV eess.IV 



### Visual Language Models as Operator Agents in the Space Domain
**Authors**: Alejandro Carrasco, Marco Nedungadi, Enrico M. Zucchelli, Amit Jain, Victor Rodriguez-Fernandez, Richard Linares

**Updated**: 2025-01-14T03:03:37Z

**Summary**: This paper explores the application of Vision-Language Models (VLMs) as operator agents in the space domain, focusing on both software and hardware operational paradigms. Building on advances in Large Language Models (LLMs) and their multimodal extensions, we investigate how VLMs can enhance autonomous control and decision-making in space missions. In the software context, we employ VLMs within the Kerbal Space Program Differential Games (KSPDG) simulation environment, enabling the agent to interpret visual screenshots of the graphical user interface to perform complex orbital maneuvers. In the hardware context, we integrate VLMs with robotic systems equipped with cameras to inspect and diagnose physical space objects, such as satellites. Our results demonstrate that VLMs can effectively process visual and textual data to generate contextually appropriate actions, competing with traditional methods and non-multimodal LLMs in simulation tasks, and showing promise in real-world applications.

**Link**: [arxiv](http://arxiv.org/abs/2501.07802v1),  [pdf](http://arxiv.org/pdf/2501.07802v1)

**Tags**: cs.AI physics.space-ph 



### $\text{Transformer}^2$: Self-adaptive LLMs
**Authors**: Qi Sun, Edoardo Cetin, Yujin Tang

**Updated**: 2025-01-14T02:52:26Z

**Summary**: Self-adaptive large language models (LLMs) aim to solve the challenges posed by traditional fine-tuning methods, which are often computationally intensive and static in their ability to handle diverse tasks. We introduce $\text{Transformer}^2$, a novel self-adaptation framework that adapts LLMs for unseen tasks in real-time by selectively adjusting only the singular components of their weight matrices. During inference, $\text{Transformer}^2$ employs a two-pass mechanism: first, a dispatch system identifies the task properties, and then task-specific "expert" vectors, trained using reinforcement learning, are dynamically mixed to obtain targeted behavior for the incoming prompt. Our method outperforms ubiquitous approaches such as LoRA, with fewer parameters and greater efficiency. $\text{Transformer}^2$ demonstrates versatility across different LLM architectures and modalities, including vision-language tasks. $\text{Transformer}^2$ represents a significant leap forward, offering a scalable, efficient solution for enhancing the adaptability and task-specific performance of LLMs, paving the way for truly dynamic, self-organizing AI systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.06252v2),  [pdf](http://arxiv.org/pdf/2501.06252v2)

**Tags**: cs.LG cs.AI cs.CL 



### Can AI Help with Your Personal Finances?
**Authors**: Oudom Hean, Utsha Saha, Binita Saha

**Updated**: 2025-01-14T02:28:28Z

**Summary**: In recent years, Large Language Models (LLMs) have emerged as a transformative development in artificial intelligence (AI), drawing significant attention from industry and academia. Trained on vast datasets, these sophisticated AI systems exhibit impressive natural language processing and content generation capabilities. This paper explores the potential of LLMs to address key challenges in personal finance, focusing on the United States. We evaluate several leading LLMs, including OpenAI's ChatGPT, Google's Gemini, Anthropic's Claude, and Meta's Llama, to assess their effectiveness in providing accurate financial advice on topics such as mortgages, taxes, loans, and investments. Our findings show that while these models achieve an average accuracy rate of approximately 70%, they also display notable limitations in certain areas. Specifically, LLMs struggle to provide accurate responses for complex financial queries, with performance varying significantly across different topics. Despite these limitations, the analysis reveals notable improvements in newer versions of these models, highlighting their growing utility for individuals and financial advisors. As these AI systems continue to evolve, their potential for advancing AI-driven applications in personal finance becomes increasingly promising.

**Link**: [arxiv](http://arxiv.org/abs/2412.19784v4),  [pdf](http://arxiv.org/pdf/2412.19784v4)

**Tags**: cs.AI cs.CE cs.LG econ.GN q-fin.EC 



### EPIC: Effective Prompting for Imbalanced-Class Data Synthesis in Tabular   Data Classification via Large Language Models
**Authors**: Jinhee Kim, Taesung Kim, Jaegul Choo

**Updated**: 2025-01-14T01:41:21Z

**Summary**: Large language models (LLMs) have demonstrated remarkable in-context learning capabilities across diverse applications. In this work, we explore the effectiveness of LLMs for generating realistic synthetic tabular data, identifying key prompt design elements to optimize performance. We introduce EPIC, a novel approach that leverages balanced, grouped data samples and consistent formatting with unique variable mapping to guide LLMs in generating accurate synthetic data across all classes, even for imbalanced datasets. Evaluations on real-world datasets show that EPIC achieves state-of-the-art machine learning classification performance, significantly improving generation efficiency. These findings highlight the effectiveness of EPIC for synthetic tabular data generation, particularly in addressing class imbalance. Our source code for our work is available at: https://seharanul17.github.io/project-synthetic-tabular-llm/

**Link**: [arxiv](http://arxiv.org/abs/2404.12404v4),  [pdf](http://arxiv.org/pdf/2404.12404v4)

**Tags**: cs.LG cs.AI 



### LLM Reading Tea Leaves: Automatically Evaluating Topic Models with Large   Language Models
**Authors**: Xiaohao Yang, He Zhao, Dinh Phung, Wray Buntine, Lan Du

**Updated**: 2025-01-14T01:21:55Z

**Summary**: Topic modeling has been a widely used tool for unsupervised text analysis. However, comprehensive evaluations of a topic model remain challenging. Existing evaluation methods are either less comparable across different models (e.g., perplexity) or focus on only one specific aspect of a model (e.g., topic quality or document representation quality) at a time, which is insufficient to reflect the overall model performance. In this paper, we propose WALM (Word Agreement with Language Model), a new evaluation method for topic modeling that considers the semantic quality of document representations and topics in a joint manner, leveraging the power of Large Language Models (LLMs). With extensive experiments involving different types of topic models, WALM is shown to align with human judgment and can serve as a complementary evaluation method to the existing ones, bringing a new perspective to topic modeling. Our software package is available at https://github.com/Xiaohao-Yang/Topic_Model_Evaluation.

**Link**: [arxiv](http://arxiv.org/abs/2406.09008v2),  [pdf](http://arxiv.org/pdf/2406.09008v2)

**Tags**: cs.CL 



### XVertNet: Unsupervised Contrast Enhancement of Vertebral Structures with   Dynamic Self-Tuning Guidance and Multi-Stage Analysis
**Authors**: Ella Eidlin, Assaf Hoogi, Hila Rozen, Mohammad Badarne, Nathan S. Netanyahu

**Updated**: 2025-01-14T01:10:52Z

**Summary**: Chest X-rays remain the primary diagnostic tool in emergency medicine, yet their limited ability to capture fine anatomical details can result in missed or delayed diagnoses. To address this, we introduce XVertNet, a novel deep-learning framework designed to enhance vertebral structure visualization in X-ray images significantly. Our framework introduces two key innovations: (1) An unsupervised learning architecture that eliminates reliance on manually labeled training data a persistent bottleneck in medical imaging, and (2) a dynamic self-tuned internal guidance mechanism featuring an adaptive feedback loop for real-time image optimization. Extensive validation across four major public datasets revealed that XVertNet outperforms state-of-the-art enhancement methods, as demonstrated by improvements in entropy scores, Tenengrad criterion values, the local phase coherence sharpness index (LPC-SI), and thetone mapped image quality index (TMQI). Furthermore, clinical validation conducted with two board-certified radiologists confirmed that the enhanced images enabled more sensitive detection of subtle vertebral fractures and degenerative changes. The unsupervised nature of XVertNet facilitates immediate clinical deployment without requiring additional training overhead. This innovation represents a transformative advancement in emergency radiology, providing a scalable and time-efficient solution to enhance diagnostic accuracy in high-pressure clinical environments.

**Link**: [arxiv](http://arxiv.org/abs/2306.03983v2),  [pdf](http://arxiv.org/pdf/2306.03983v2)

**Tags**: eess.IV cs.CV 



### Large Language Models for Knowledge Graph Embedding Techniques, Methods,   and Challenges: A Survey
**Authors**: Bingchen Liu, Xin Li

**Updated**: 2025-01-14T00:47:24Z

**Summary**: Large Language Models (LLMs) have attracted a lot of attention in various fields due to their superior performance, aiming to train hundreds of millions or more parameters on large amounts of text data to understand and generate natural language. As the superior performance of LLMs becomes apparent, they are increasingly being applied to knowledge graph embedding (KGE) related tasks to improve the processing results. As a deep learning model in the field of Natural Language Processing (NLP), it learns a large amount of textual data to predict the next word or generate content related to a given text. However, LLMs have recently been invoked to varying degrees in different types of KGE related scenarios such as multi-modal KGE and open KGE according to their task characteristics. In this paper, we investigate a wide range of approaches for performing LLMs-related tasks in different types of KGE scenarios. To better compare the various approaches, we summarize each KGE scenario in a classification. In addition to the categorization methods, we provide a tabular overview of the methods and their source code links for a more direct comparison. In the article we also discuss the applications in which the methods are mainly used and suggest several forward-looking directions for the development of this new research area.

**Link**: [arxiv](http://arxiv.org/abs/2501.07766v1),  [pdf](http://arxiv.org/pdf/2501.07766v1)

**Tags**: cs.CL cs.AI 



### Continuous GNN-based Anomaly Detection on Edge using Efficient Adaptive   Knowledge Graph Learning
**Authors**: Sanggeon Yun, Ryozo Masukawa, William Youngwoo Chung, Minhyoung Na, Nathaniel Bastian, Mohsen Imani

**Updated**: 2025-01-14T00:21:51Z

**Summary**: The increasing demand for robust security solutions across various industries has made Video Anomaly Detection (VAD) a critical task in applications such as intelligent surveillance, evidence investigation, and violence detection. Traditional approaches to VAD often rely on finetuning large pre-trained models, which can be computationally expensive and impractical for real-time or resource-constrained environments. To address this, MissionGNN introduced a more efficient method by training a graph neural network (GNN) using a fixed knowledge graph (KG) derived from large language models (LLMs) like GPT-4. While this approach demonstrated significant efficiency in computational power and memory, it faces limitations in dynamic environments where frequent updates to the KG are necessary due to evolving behavior trends and shifting data patterns. These updates typically require cloud-based computation, posing challenges for edge computing applications. In this paper, we propose a novel framework that facilitates continuous KG adaptation directly on edge devices, overcoming the limitations of cloud dependency. Our method dynamically modifies the KG through a three-phase process: pruning, alternating, and creating nodes, enabling real-time adaptation to changing data trends. This continuous learning approach enhances the robustness of anomaly detection models, making them more suitable for deployment in dynamic and resource-constrained environments.

**Link**: [arxiv](http://arxiv.org/abs/2411.09072v2),  [pdf](http://arxiv.org/pdf/2411.09072v2)

**Tags**: cs.LG 



### Rethinking AI Cultural Evaluation
**Authors**: Michal Bravansky, Filip Trhlik, Fazl Barez

**Updated**: 2025-01-13T23:42:37Z

**Summary**: As AI systems become more integrated into society, evaluating their capacity to align with diverse cultural values is crucial for their responsible deployment. Current evaluation methods predominantly rely on multiple-choice question (MCQ) datasets. In this study, we demonstrate that MCQs are insufficient for capturing the complexity of cultural values expressed in open-ended scenarios. Our findings highlight significant discrepancies between MCQ-based assessments and the values conveyed in unconstrained interactions. Based on these findings, we recommend moving beyond MCQs to adopt more open-ended, context-specific assessments that better reflect how AI models engage with cultural values in realistic settings.

**Link**: [arxiv](http://arxiv.org/abs/2501.07751v1),  [pdf](http://arxiv.org/pdf/2501.07751v1)

**Tags**: cs.AI cs.CY 



### Advancing Student Writing Through Automated Syntax Feedback
**Authors**: Kamyar Zeinalipour, Mehak Mehak, Fatemeh Parsamotamed, Marco Maggini, Marco Gori

**Updated**: 2025-01-13T23:10:02Z

**Summary**: This study underscores the pivotal role of syntax feedback in augmenting the syntactic proficiency of students. Recognizing the challenges faced by learners in mastering syntactic nuances, we introduce a specialized dataset named Essay-Syntax-Instruct designed to enhance the understanding and application of English syntax among these students. Leveraging the capabilities of Large Language Models (LLMs) such as GPT3.5-Turbo, Llama-2-7b-chat-hf, Llama-2-13b-chat-hf, and Mistral-7B-Instruct-v0.2, this work embarks on a comprehensive fine-tuning process tailored to the syntax improvement task. Through meticulous evaluation, we demonstrate that the fine-tuned LLMs exhibit a marked improvement in addressing syntax-related challenges, thereby serving as a potent tool for students to identify and rectify their syntactic errors. The findings not only highlight the effectiveness of the proposed dataset in elevating the performance of LLMs for syntax enhancement but also illuminate a promising path for utilizing advanced language models to support language acquisition efforts. This research contributes to the broader field of language learning technology by showcasing the potential of LLMs in facilitating the linguistic development of Students.

**Link**: [arxiv](http://arxiv.org/abs/2501.07740v1),  [pdf](http://arxiv.org/pdf/2501.07740v1)

**Tags**: cs.CL 



### Scideator: Human-LLM Scientific Idea Generation Grounded in   Research-Paper Facet Recombination
**Authors**: Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Hope, Daniel S. Weld

**Updated**: 2025-01-13T22:45:30Z

**Summary**: The scientific ideation process often involves blending salient aspects of existing papers to create new ideas. To see if large language models (LLMs) can assist this process, we contribute Scideator, a novel mixed-initiative tool for scientific ideation. Starting from a user-provided set of papers, Scideator extracts key facets (purposes, mechanisms, and evaluations) from these and relevant papers, allowing users to explore the idea space by interactively recombining facets to synthesize inventive ideas. Scideator also helps users to gauge idea novelty by searching the literature for potential overlaps and showing automated novelty assessments and explanations. To support these tasks, Scideator introduces four LLM-powered retrieval-augmented generation (RAG) modules: Analogous Paper Facet Finder, Faceted Idea Generator, Idea Novelty Checker, and Idea Novelty Iterator. In a within-subjects user study, 19 computer-science researchers identified significantly more interesting ideas using Scideator compared to a strong baseline combining a scientific search engine with LLM interaction.

**Link**: [arxiv](http://arxiv.org/abs/2409.14634v3),  [pdf](http://arxiv.org/pdf/2409.14634v3)

**Tags**: cs.HC cs.AI H.5.2, I.2 



### Divergences between Language Models and Human Brains
**Authors**: Yuchen Zhou, Emmy Liu, Graham Neubig, Michael J. Tarr, Leila Wehbe

**Updated**: 2025-01-13T22:22:06Z

**Summary**: Do machines and humans process language in similar ways? Recent research has hinted at the affirmative, showing that human neural activity can be effectively predicted using the internal representations of language models (LMs). Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language. In this work, we systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories. Using an LLM-based data-driven approach, we identify two domains that LMs do not capture well: social/emotional intelligence and physical commonsense. We validate these findings with human behavioral experiments and hypothesize that the gap is due to insufficient representations of social/emotional and physical knowledge in LMs. Our results show that fine-tuning LMs on these domains can improve their alignment with human brain responses.

**Link**: [arxiv](http://arxiv.org/abs/2311.09308v3),  [pdf](http://arxiv.org/pdf/2311.09308v3)

**Tags**: cs.CL cs.AI cs.LG q-bio.NC 



### LLMic: Romanian Foundation Language Model
**Authors**: Vlad-Andrei Bădoiu, Mihai-Valentin Dumitru, Alexandru M. Gherghescu, Alexandru Agache, Costin Raiciu

**Updated**: 2025-01-13T22:14:45Z

**Summary**: Recent advances in Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks with commercial models leading the way. While open models usually operate at a smaller scale, they maintain competitiveness through specialization and fine-tuning. However, a significant challenge persists: open models often underperform in low-resource languages due to limited representation in the training corpus. In this paper, we present LLMic, a bilingual foundation language model designed specifically for the Romanian Language. We document the complete process of pretraining a foundation model for a low-resource language, including corpus construction, architecture selection, and hyper-parameter optimization. Our evaluation demonstrates that LLMic can be specialized for tasks in the target language, achieving results comparable to other much larger open models. We show that fine-tuning LLMic for language translation after the initial pretraining phase outperforms existing solutions in English-to-Romanian translation tasks. This opens the path for efficient large-scale processing for the Romanian language community, using the much smaller LLMic model

**Link**: [arxiv](http://arxiv.org/abs/2501.07721v1),  [pdf](http://arxiv.org/pdf/2501.07721v1)

**Tags**: cs.CL 



