# Arxiv Results
## Keyword: kv cache 
 ### UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from   an Uncertainty-Aware Perspective
**Authors**: Jing Xiong, Jianghan Shen, Fanghua Ye, Chaofan Tao, Zhongwei Wan, Jianqiao Lu, Xun Wu, Chuanyang Zheng, Zhijiang Guo, Min Yang, Lingpeng Kong, Ngai Wong

**Updated**: 2025-09-24T16:56:17Z

**Summary**: Deploying large language models (LLMs) for long-context inference remains challenging due to their substantial memory and computational demands. While techniques such as Key-Value (KV) cache compression are designed to reduce memory usage, they often neglect the structured sparsity inherent in the relationship between hidden states and their corresponding KV cache. In this work, we explore the role of uncertainty as a potential indicator of sparsity within LLMs. We propose UNComp, an uncertainty-aware framework that leverages truncated matrix entropy to identify areas of low information content, thereby revealing sparsity patterns that can be used for adaptive compression. Unlike traditional methods that apply uniform compression, UNComp dynamically adjusts its approach to compression, guided by uncertainty measures that reflect the importance of various model components. Our analysis shows that sparsity patterns, when derived from uncertainty estimates, can be exploited to reveal special long-range dependencies, such as retrieval heads and retrieval layers. This perspective not only enhances our understanding of how compression can be optimized but also provides new insights into the inherent sparsity of LLMs during long-context inference. By focusing on uncertainty to analyze the sparsity pattern in detail, UNComp reduces the KV cache size to 4.74% of the original, achieves a 6% prefill speedup, and improves throughput by 6.4x - not only delivering strong lossless compression performance, but also validating the effectiveness of the underlying theoretical tool. We release the code at https://github.com/menik1126/UNComp.

**Link**: [arxiv](http://arxiv.org/abs/2410.03090v2),  [pdf](http://arxiv.org/pdf/2410.03090v2)

**Tags**: cs.CL cs.LG 



### Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient   LLM Inference
**Authors**: Haoyu Chen, Xue Li, Kun Qian, Yu Guan, Jin Zhao, Xin Wang

**Updated**: 2025-09-24T03:15:37Z

**Summary**: Efficiently processing the dynamics of requests, especially the context length variance, is important in Large Language Model (LLM) serving scenarios. However, there is an intrinsic trade-off: while leveraging parallelism strategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to accommodate larger context lengths, it inevitably results in degraded overall throughput. In this paper, we propose Cross-Instance Parallelism Transformation (Gyges), which adaptively adjusts the parallelism strategies of running instances to align with the dynamics of incoming requests. We design (1) a page-friendly, header-centric layout to accelerate KV cache transformations; (2) dedicated weight padding to accelerate model weight transformations; and (3) a transformation-aware scheduler to cooperatively schedule requests and parallelism transformations, optimizing the overall performance. Evaluations using real-world traces show that Gyges improves throughput by 1.75x-6.57x compared to state-of-the-art solutions.

**Link**: [arxiv](http://arxiv.org/abs/2509.19729v1),  [pdf](http://arxiv.org/pdf/2509.19729v1)

**Tags**: cs.DC 



### LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale   Architectures
**Authors**: Anders Johansson, Evan Weinberg, Christian R. Trott, Megan J. McCarthy, Stan G. Moore

**Updated**: 2025-09-24T01:32:55Z

**Summary**: Since its inception in 1995, LAMMPS has grown to be a world-class molecular dynamics code, with thousands of users, over one million lines of code, and multi-scale simulation capabilities. We discuss how LAMMPS has adapted to the modern heterogeneous computing landscape by integrating the Kokkos performance portability library into the existing C++ code. We investigate performance portability of simple pairwise, many-body reactive, and machine-learned force-field interatomic potentials. We present results on GPUs across different vendors and generations, and analyze performance trends, probing FLOPS throughput, memory bandwidths, cache capabilities, and thread-atomic operation performance. Finally, we demonstrate strong scaling on three exascale machines -- OLCF Frontier, ALCF Aurora, and NNSA El Capitan -- as well as on the CSCS Alps supercomputer, for the three potentials.

**Link**: [arxiv](http://arxiv.org/abs/2508.13523v2),  [pdf](http://arxiv.org/pdf/2508.13523v2)

**Tags**: cs.DC cs.PF physics.comp-ph C.1.4; C.2.4; C.4; D.1.3; D.3.4; E.1; I.6; I.6.8; J.2 



### Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method   for Multi-Agent Systems
**Authors**: Danilo Trombino, Vincenzo Pecorella, Alessandro de Giulii, Davide Tresoldi

**Updated**: 2025-09-23T21:46:38Z

**Summary**: Multi-agent systems (MAS) are increasingly tasked with solving complex, knowledge-intensive problems where effective agent orchestration is critical. Conventional orchestration methods rely on static agent descriptions, which often become outdated or incomplete. This limitation leads to inefficient task routing, particularly in dynamic environments where agent capabilities continuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a novel approach that augments static descriptions with dynamic, privacy-preserving relevance signals derived from each agent's internal knowledge base (KB). In the proposed framework, when static descriptions are insufficient for a clear routing decision, the orchestrator prompts the subagents in parallel. Each agent then assesses the task's relevance against its private KB, returning a lightweight ACK signal without exposing the underlying data. These collected signals populate a shared semantic cache, providing dynamic indicators of agent suitability for future queries. By combining this novel mechanism with static descriptions, our method achieves more accurate and adaptive task routing preserving agent autonomy and data confidentiality. Benchmarks show that our KBA Orchestration significantly outperforms static description-driven methods in routing precision and overall system efficiency, making it suitable for large-scale systems that require higher accuracy than standard description-driven routing.

**Link**: [arxiv](http://arxiv.org/abs/2509.19599v1),  [pdf](http://arxiv.org/pdf/2509.19599v1)

**Tags**: cs.MA cs.AI 



### From Slow Bidirectional to Fast Autoregressive Video Diffusion Models
**Authors**: Tianwei Yin, Qiang Zhang, Richard Zhang, William T. Freeman, Fredo Durand, Eli Shechtman, Xun Huang

**Updated**: 2025-09-23T21:08:03Z

**Summary**: Current video diffusion models achieve impressive generation quality but struggle in interactive applications due to bidirectional attention dependencies. The generation of a single frame requires the model to process the entire sequence, including the future. We address this limitation by adapting a pretrained bidirectional diffusion transformer to an autoregressive transformer that generates frames on-the-fly. To further reduce latency, we extend distribution matching distillation (DMD) to videos, distilling 50-step diffusion model into a 4-step generator. To enable stable and high-quality distillation, we introduce a student initialization scheme based on teacher's ODE trajectories, as well as an asymmetric distillation strategy that supervises a causal student model with a bidirectional teacher. This approach effectively mitigates error accumulation in autoregressive generation, allowing long-duration video synthesis despite training on short clips. Our model achieves a total score of 84.27 on the VBench-Long benchmark, surpassing all previous video generation models. It enables fast streaming generation of high-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our approach also enables streaming video-to-video translation, image-to-video, and dynamic prompting in a zero-shot manner.

**Link**: [arxiv](http://arxiv.org/abs/2412.07772v4),  [pdf](http://arxiv.org/pdf/2412.07772v4)

**Tags**: cs.CV 



### Decentralized Learning Strategies for Estimation Error Minimization with   Graph Neural Networks
**Authors**: Xingran Chen, Navid NaderiAlizadeh, Alejandro Ribeiro, Shirin Saeedi Bidokhti

**Updated**: 2025-09-23T20:25:15Z

**Summary**: We address the challenge of sampling and remote estimation for autoregressive Markovian processes in a multi-hop wireless network with statistically-identical agents. Agents cache the most recent samples from others and communicate over wireless collision channels governed by an underlying graph topology. Our goal is to minimize time-average estimation error and/or age of information with decentralized scalable sampling and transmission policies, considering both oblivious (where decision-making is independent of the physical processes) and non-oblivious policies (where decision-making depends on physical processes). We prove that in oblivious policies, minimizing estimation error is equivalent to minimizing the age of information. The complexity of the problem, especially the multi-dimensional action spaces and arbitrary network topologies, makes theoretical methods for finding optimal transmission policies intractable. We optimize the policies using a graphical multi-agent reinforcement learning framework, where each agent employs a permutation-equivariant graph neural network architecture. Theoretically, we prove that our proposed framework exhibits desirable transferability properties, allowing transmission policies trained on small- or moderate-size networks to be executed effectively on large-scale topologies. Numerical experiments demonstrate that (i) Our proposed framework outperforms state-of-the-art baselines; (ii) The trained policies are transferable to larger networks, and their performance gains increase with the number of agents; (iii) The training procedure withstands non-stationarity even if we utilize independent learning techniques; and, (iv) Recurrence is pivotal in both independent learning and centralized training and decentralized execution, and improves the resilience to non-stationarity in independent learning.

**Link**: [arxiv](http://arxiv.org/abs/2404.03227v3),  [pdf](http://arxiv.org/pdf/2404.03227v3)

**Tags**: eess.SP cs.LG 



### Automated Insertion of Flushes and Fences for Persistency
**Authors**: Yutong Guo, Weiyu Luo, Brian Demsky

**Updated**: 2025-09-23T18:14:21Z

**Summary**: CXL shared memory and persistent memory allow the contents of memory to persist beyond crashes. Stores to persistent or CXL memory are typically not immediately made persistent; developers must manually flush the corresponding cache lines to force the data to be written to the underlying storage. Correctly using flush and fence operations is known to be challenging. While state-of-the-art tools can find missing flush instructions, they often require bug-revealing test cases. No existing tools can ensure the absence of missing flush bugs.   In this paper, we present PMRobust, a compiler that automatically inserts flush and fence operations to ensure that code using persistent memory is free from missing flush and fence bugs. PMRobust employs a novel static analysis with optimizations that target newly allocated objects. We have evaluated PMRobust on persistent memory libraries and several persistent memory data structures and measured a geometric mean overhead of 0.26% relative to the original benchmarks with hand-placed flush and fence operations.

**Link**: [arxiv](http://arxiv.org/abs/2509.19459v1),  [pdf](http://arxiv.org/pdf/2509.19459v1)

**Tags**: cs.SE cs.PL 



### Reconstruction of a potential parameter in time-fractional diffusion   problems via a Kohn--Vogelius type functional: Theoretical aspects
**Authors**: Hamza Kahlaoui, Mourad Hrizi, Abdessamad Oulmelk, Xiangcheng Zheng, Ahmed Hendy

**Updated**: 2025-09-23T17:18:59Z

**Summary**: Of concern is the problem of reconstructing a space-dependent potential from boundary observations in the Caputo time-fractional diffusion equation, utilizing a stable and robust recovery method. We develop an algorithm to minimize the Kohn-Vogelius (KV) cost function, which measures the difference between the solutions of two excitations. The inverse potential problem is recast into an optimization problem, where the objective is to minimize a Kohn-Vogelius-type functional within a set of admissible potentials. We establish the well-posedness of this optimization problem by proving the existence and uniqueness of a minimizer and demonstrating its stability with respect to perturbations in the boundary data. Furthermore, we analyze the Fr\'echet differentiability of the KV functional and prove the Lipschitz continuity of its gradient. These theoretical results enable the development of a convergent conjugate gradient algorithm for numerical reconstruction. The effectiveness and robustness of the proposed method are confirmed through several numerical examples in both one and two dimensions, including cases with noisy data.

**Link**: [arxiv](http://arxiv.org/abs/2509.19260v1),  [pdf](http://arxiv.org/pdf/2509.19260v1)

**Tags**: math.NA cs.NA 



### CompLLM: Compression for Long Context Q&A
**Authors**: Gabriele Berton, Jayakrishnan Unnikrishnan, Son Tran, Mubarak Shah

**Updated**: 2025-09-23T16:49:43Z

**Summary**: Large Language Models (LLMs) face significant computational challenges when processing long contexts due to the quadratic complexity of self-attention. While soft context compression methods, which map input text to smaller latent representations, have shown promise, their real-world adoption is limited. Existing techniques typically compress the context as a single unit, which leads to quadratic compression complexity and an inability to reuse computations across queries with overlapping contexts. In this work, we introduce CompLLM, a soft compression technique designed for practical deployment. Instead of processing the context holistically, CompLLM divides it into segments and compresses each one independently. This simple design choice yields three critical properties: efficiency, as the compression step scales linearly with the context length; scalability, enabling models trained on short sequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and reusability, allowing compressed segments to be cached and reused across different queries. Our experiments show that with a 2x compression rate, at high context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x and reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance comparable to that obtained with the uncompressed context, and even surpasses it on very long sequences, demonstrating its effectiveness and practical utility.

**Link**: [arxiv](http://arxiv.org/abs/2509.19228v1),  [pdf](http://arxiv.org/pdf/2509.19228v1)

**Tags**: cs.CL 



### 3D Blocking for Matrix-free Smoothers in 2D Variable-Viscosity Stokes   Equations with Applications to Geodynamics
**Authors**: Marcel Ferrari, Cyrill Püntener, Alexander Sotoudeh, Niklas Viebig

**Updated**: 2025-09-23T14:25:13Z

**Summary**: We present the design, implementation, and evaluation of optimized matrix-free stencil kernels for multigrid smoothing in the incompressible Stokes equations with variable viscosity, motivated by geophysical flow problems. We investigate five smoother variants derived from different optimisation strategies: Red-Black Gauss-Seidel, Jacobi, fused Jacobi, blocked fused Jacobi, and a novel Jacobi smoother with RAS-type temporal blocking, a strategy that applies local iterations on overlapping tiles to improve cache reuse. To ensure correctness, we introduce an energy-based residual norm that balances velocity and pressure contributions, and validate all implementations using a high-contrast sinker benchmark representative of realistic geodynamic numerical models. Our performance study on NVIDIA GH200 Grace Hopper nodes of the ALPS supercomputer demonstrates that all smoothers scale well within a single NUMA domain, but the RAS-Jacobi smoother consistently achieves the best performance at higher core counts. It sustains over 90% weak-scaling efficiency up to 64 cores and delivers up to a threefold speedup compared to the C++ Jacobi baseline, owing to improved cache reuse and reduced memory traffic. These results show that temporal blocking, already employed in distributed-memory solvers to reduce communication, can also provide substantial benefits at the socket and NUMA level. This work highlights the importance of cache-aware stencil design for harnessing modern heterogeneous architectures and lays the groundwork for extending RAS-type temporal blocking strategies to three-dimensional problems and GPU accelerators.

**Link**: [arxiv](http://arxiv.org/abs/2509.19061v1),  [pdf](http://arxiv.org/pdf/2509.19061v1)

**Tags**: physics.comp-ph cs.NA math.NA 65F08, 65N55, 65N22, 76M20 G.1.8; F.2.1; D.1.3; C.1.4 



### Obelix: Mitigating Side-Channels Through Dynamic Obfuscation
**Authors**: Jan Wichelmann, Anja Rabich, Anna P"atschke, Thomas Eisenbarth

**Updated**: 2025-09-23T12:32:51Z

**Summary**: Trusted execution environments (TEEs) offer hardware-assisted means to protect code and data. However, as shown in numerous results over the years, attackers can use side-channels to leak data access patterns and even single-step the code. While the vendors are slowly introducing hardware-based countermeasures for some attacks, others will stay unaddressed. This makes a software-level countermeasure desirable, but current available solutions only address very specific attack vectors or have a narrow leakage model.   In this work, we take a holistic view at the vulnerabilities of TEEs and design a tool named Obelix, which is the first to protect both code and data against a wide range of TEE attacks, from cache attacks over single-stepping to ciphertext side-channels. We analyze the practically achievable precision of state-of-the-art single-stepping tools, and present an algorithm which uses that knowledge to divide a program into uniform code blocks, that are indistinguishable for a strong attacker. By storing these blocks and the program data in oblivious RAM, the attacker cannot follow execution, effectively protecting both secret code and data. We describe how we automate our approach to make it available for developers who are unfamiliar with side-channels. As an obfuscation tool, Obelix comes with a considerable performance overhead, but compensates this with strong security guarantees and easy applicability without requiring any expert knowledge.

**Link**: [arxiv](http://arxiv.org/abs/2509.18909v1),  [pdf](http://arxiv.org/pdf/2509.18909v1)

**Tags**: cs.CR 



### PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference
**Authors**: Hao Zhang, Mengsi Lyu, Zhuo Chen, Xingrun Xing, Yulong Ao, Yonghua Lin

**Updated**: 2025-09-23T08:31:26Z

**Summary**: Large Language Models (LLMs) demonstrate exceptional capabilities across various tasks, but their deployment is constrained by high computational and memory costs. Model pruning provides an effective means to alleviate these demands. However, existing methods often ignore the characteristics of prefill-decode (PD) disaggregation in practice. In this paper, we propose a novel pruning method for PD disaggregation inference, enabling more precise and efficient block and KV Cache pruning. Our approach constructs pruning and distillation sets to perform iterative block removal independently for the prefill and decode stages, obtaining better pruning solutions. Moreover, we introduce a token-aware cache pruning mechanism that retains all KV Cache in the prefill stage but selectively reuses entries for the first and last token sequences in selected layers during decode, reducing communication costs with minimal overhead. Extensive experiments demonstrate that our approach consistently achieves strong performance in both PD disaggregation and PD unified settings without disaggregation. Under the same (default) settings, our method achieves improved performance and faster inference, along with a 4.95$\times$ reduction in data transmission bandwidth consumption.

**Link**: [arxiv](http://arxiv.org/abs/2509.04467v3),  [pdf](http://arxiv.org/pdf/2509.04467v3)

**Tags**: cs.CL cs.AI 



### Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and   Flash
**Authors**: Fucheng Jia, Zewen Wu, Shiqi Jiang, Huiqiang Jiang, Qianxi Zhang, Yuqing Yang, Yunxin Liu, Ju Ren, Deyu Zhang, Ting Cao

**Updated**: 2025-09-23T08:24:07Z

**Summary**: Large language models (LLMs) are increasingly being deployed on mobile devices, but the limited DRAM capacity constrains the deployable model size. This paper introduces ActiveFlow, the first LLM inference framework that can achieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the scaling up of deployable model sizes. The framework is based on the novel concept of active weight DRAM-flash swapping and incorporates three novel techniques: (1) Cross-layer active weights preloading. It uses the activations from the current layer to predict the active weights of several subsequent layers, enabling computation and data loading to overlap, as well as facilitating large I/O transfers. (2) Sparsity-aware self-distillation. It adjusts the active weights to align with the dense-model output distribution, compensating for approximations introduced by contextual sparsity. (3) Active weight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation among the hot weight cache, preloaded active weights, and computation-involved weights based on available memory. Results show ActiveFlow achieves the performance-cost Pareto frontier compared to existing efficiency optimization methods.

**Link**: [arxiv](http://arxiv.org/abs/2504.08378v2),  [pdf](http://arxiv.org/pdf/2504.08378v2)

**Tags**: cs.LG 



### Static Estimation of Reuse Profiles for Arrays in Nested Loops
**Authors**: Abdur Razzak, Atanu Barai, Nandakishore Santhi, Abdel-Hameed A. Badawy

**Updated**: 2025-09-23T06:10:20Z

**Summary**: Efficient memory access patterns play a crucial role in determining the overall performance of applications by exploiting temporal and spatial locality, thus maximizing cache locality. The Reuse Distance Histogram (RDH) is a widely used metric to quantify temporal locality, measuring the distance between consecutive accesses to the same memory location. Traditionally, calculating RDH requires program execution and memory trace collection to obtain dynamic memory access behavior. This trace collection is often time-consuming, resource-intensive, and unsuitable for early-stage optimization or large-scale applications. Static prediction, on the other hand, offers a significant speedup in estimating RDH and cache hit rates. However, these approaches lack accuracy, since the predictions come without running the program and knowing the complete memory access pattern, more specifically when arrays are used inside nested loops. This paper presents a novel static analysis framework for predicting the reuse profiles of array references in programs with nested loop structures, without requiring any runtime information. By analyzing loop bounds, access patterns in smaller problem sizes, and predictive equations, our method predicts access patterns of arrays and estimates reuse distances and cache hit rate at compile time. This paper extends our previous study by incorporating more analysis and improving prediction by addressing previously unhandled reuse patterns. We evaluate our technique against a widely accepted traditional trace-driven profiling tool, Parallel Reuse Distance Analysis (PARDA). The results demonstrate that our static predictor achieves comparable accuracy while offering orders-of-magnitude improvement in the analysis speed. This work offers a practical alternative to dynamic reuse profiling and paves the way for integration into compilers and static performance modeling tools.

**Link**: [arxiv](http://arxiv.org/abs/2509.18684v1),  [pdf](http://arxiv.org/pdf/2509.18684v1)

**Tags**: cs.PF 



### CALL: Context-Aware Low-Latency Retrieval in Disk-Based Vector Databases
**Authors**: Yeonwoo Jeong, Hyunji Cho, Kyuri Park, Youngjae Kim, Sungyong Park

**Updated**: 2025-09-23T05:39:47Z

**Summary**: Embedding models capture both semantic and syntactic structures of queries, often mapping different queries to similar regions in vector space. This results in non-uniform cluster access patterns in modern disk-based vector databases. While existing approaches optimize individual queries, they overlook the impact of cluster access patterns, failing to account for the locality effects of queries that access similar clusters. This oversight increases cache miss penalty. To minimize the cache miss penalty, we propose CALL, a context-aware query grouping mechanism that organizes queries based on shared cluster access patterns. Additionally, CALL incorporates a group-aware prefetching method to minimize cache misses during transitions between query groups and latency-aware cluster loading. Experimental results show that CALL reduces the 99th percentile tail latency by up to 33% while consistently maintaining a higher cache hit ratio, substantially reducing search latency.

**Link**: [arxiv](http://arxiv.org/abs/2509.18670v1),  [pdf](http://arxiv.org/pdf/2509.18670v1)

**Tags**: cs.DB 



### VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic   Vision-Language Planning for Zero-Shot Transfer in Robot Navigation
**Authors**: Neel P. Bhatt, Yunhao Yang, Rohan Siva, Pranay Samineni, Daniel Milan, Zhangyang Wang, Ufuk Topcu

**Updated**: 2025-09-23T03:23:03Z

**Summary**: Rapid adaptation in unseen environments is essential for scalable real-world autonomy, yet existing approaches rely on exhaustive exploration or rigid navigation policies that fail to generalize. We present VLN-Zero, a two-phase vision-language navigation framework that leverages vision-language models to efficiently construct symbolic scene graphs and enable zero-shot neurosymbolic navigation. In the exploration phase, structured prompts guide VLM-based search toward informative and diverse trajectories, yielding compact scene graph representations. In the deployment phase, a neurosymbolic planner reasons over the scene graph and environmental observations to generate executable plans, while a cache-enabled execution module accelerates adaptation by reusing previously computed task-location trajectories. By combining rapid exploration, symbolic reasoning, and cache-enabled execution, the proposed framework overcomes the computational inefficiency and poor generalization of prior vision-language navigation methods, enabling robust and scalable decision-making in unseen environments. VLN-Zero achieves 2x higher success rate compared to state-of-the-art zero-shot models, outperforms most fine-tuned baselines, and reaches goal locations in half the time with 55% fewer VLM calls on average compared to state-of-the-art models across diverse environments. Codebase, datasets, and videos for VLN-Zero are available at: https://vln-zero.github.io/.

**Link**: [arxiv](http://arxiv.org/abs/2509.18592v1),  [pdf](http://arxiv.org/pdf/2509.18592v1)

**Tags**: cs.RO cs.AI cs.CV cs.LG cs.SY eess.SY 



### DOTA: Distributional Test-Time Adaptation of Vision-Language Models
**Authors**: Zongbo Han, Jialong Yang, Guangyu Wang, Junfan Li, Qianli Xu, Mike Zheng Shou, Changqing Zhang

**Updated**: 2025-09-23T01:58:07Z

**Summary**: Vision-language foundation models (VLMs), such as CLIP, exhibit remarkable performance across a wide range of tasks. However, deploying these models can be unreliable when significant distribution gaps exist between training and test data, while fine-tuning for diverse scenarios is often costly. Cache-based test-time adapters offer an efficient alternative by storing representative test samples to guide subsequent classifications. Yet, these methods typically employ naive cache management with limited capacity, leading to severe catastrophic forgetting when samples are inevitably dropped during updates. In this paper, we propose DOTA (DistributiOnal Test-time Adaptation), a simple yet effective method addressing this limitation. Crucially, instead of merely memorizing individual test samples, DOTA continuously estimates the underlying distribution of the test data stream. Test-time posterior probabilities are then computed using these dynamically estimated distributions via Bayes' theorem for adaptation. This distribution-centric approach enables the model to continually learn and adapt to the deployment environment. Extensive experiments validate that DOTA significantly mitigates forgetting and achieves state-of-the-art performance compared to existing methods.

**Link**: [arxiv](http://arxiv.org/abs/2409.19375v2),  [pdf](http://arxiv.org/pdf/2409.19375v2)

**Tags**: cs.LG cs.AI cs.CL cs.CV cs.HC 



### Foresight: Adaptive Layer Reuse for Accelerated and High-Quality   Text-to-Video Generation
**Authors**: Muhammad Adnan, Nithesh Kurella, Akhil Arunkumar, Prashant J. Nair

**Updated**: 2025-09-22T19:20:33Z

**Summary**: Diffusion Transformers (DiTs) achieve state-of-the-art results in text-to-image, text-to-video generation, and editing. However, their large model size and the quadratic cost of spatial-temporal attention over multiple denoising steps make video generation computationally expensive. Static caching mitigates this by reusing features across fixed steps but fails to adapt to generation dynamics, leading to suboptimal trade-offs between speed and quality.   We propose Foresight, an adaptive layer-reuse technique that reduces computational redundancy across denoising steps while preserving baseline performance. Foresight dynamically identifies and reuses DiT block outputs for all layers across steps, adapting to generation parameters such as resolution and denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and CogVideoX, Foresight achieves up to \latencyimprv end-to-end speedup, while maintaining video quality. The source code of Foresight is available at \href{https://github.com/STAR-Laboratory/foresight}{https://github.com/STAR-Laboratory/foresight}.

**Link**: [arxiv](http://arxiv.org/abs/2506.00329v2),  [pdf](http://arxiv.org/pdf/2506.00329v2)

**Tags**: cs.LG cs.AI cs.CV 



### Speculate Deep and Accurate: Lossless and Training-Free Acceleration for   Offloaded LLMs via Substitute Speculative Decoding
**Authors**: Pei-Shuo Wang, Jian-Jia Chen, Chun-Che Yang, Chi-Chih Chang, Ning-Chi Huang, Mohamed S. Abdelfattah, Kai-Chiang Wu

**Updated**: 2025-09-22T19:08:57Z

**Summary**: The immense model sizes of large language models (LLMs) challenge deployment on memory-limited consumer GPUs. Although model compression and parameter offloading are common strategies to address memory limitations, compression can degrade quality, and offloading maintains quality but suffers from slow inference. Speculative decoding presents a promising avenue to accelerate parameter offloading, utilizing a fast draft model to propose multiple draft tokens, which are then verified by the target LLM in parallel with a single forward pass. This method reduces the time-consuming data transfers in forward passes that involve offloaded weight transfers. Existing methods often rely on pretrained weights of the same family, but require additional training to align with custom-trained models. Moreover, approaches that involve draft model training usually yield only modest speedups. This limitation arises from insufficient alignment with the target model, preventing higher token acceptance lengths. To address these challenges and achieve greater speedups, we propose SubSpec, a plug-and-play method to accelerate parameter offloading that is lossless and training-free. SubSpec constructs a highly aligned draft model by generating low-bit quantized substitute layers from offloaded target LLM portions. Additionally, our method shares the remaining GPU-resident layers and the KV-Cache, further reducing memory overhead and enhance alignment. SubSpec achieves a high average acceptance length, delivering 9.1x speedup for Qwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for Qwen2.5 32B on popular generation benchmarks (24GB VRAM limit).

**Link**: [arxiv](http://arxiv.org/abs/2509.18344v1),  [pdf](http://arxiv.org/pdf/2509.18344v1)

**Tags**: cs.CL 



### Comparison of Adaptive plan doses using Velocity generated synthetic CT   with KV CBCT and re-planning CT
**Authors**: Sudam Masanta, Gurvinder Singh, Shefali Pahwa, Shekhar Dwivedi, Devaraju Sampathirao, Ramandeep Singh

**Updated**: 2025-09-22T18:32:32Z

**Summary**: Introduction: This study uses KV CBCT based Synthetic CT (sCT) generated through Velocity workstation and compare the target and normal tissue doses with Adaptive plan CT doses.   Methods: Thirty head and neck cancer patients undergoing Adaptive Radiation Therapy (ART) were included in this retrospective study. Initially, patient underwent treatment with the primary plan. After subsequent indications of major changes in patients' physicality and anatomy adaptive CT scans were acquired as per institutional protocol. Both the primary planning CT and the indicative cone-beam CT (CBCT) last acquired before the commencement of the adaptive treatment were imported into Velocity workstation. Rigid and deformable image registration techniques were used for the generation of a Synthetic CT (sCT). Simultaneously replanning was done on re-planning CT (rCT) for adaptive plan execution. The primary plan dose was subsequently mapped and deformed onto the Synthetic CT in Velocity workstation, allowing for a comparative dosimetric analysis between the sCT and rCT plan doses. This comparison was conducted in both Velocity and Eclipse, focusing on dose variations across different organs at risk (OARs) and the planning target volume (PTV). Additionally, dosimetric indices were evaluated to assess and validate the accuracy and quality of the synthetic CT-based dose mapping relative to adaptive planning.   Results: The dosimetric comparison between sCT and rCT stated that Mean dose for OARs and PTVs were found to be similar in the two planning and the level of confidence by using T-statistics. Collaborative research has the potential to eliminate the need of rCT as a standard requirement.   Conclusion: The sCT shows comparable CT numbers and doses to the replanning CT, suggesting it's potential as a replacement pending clinical correlation and contour adjustments.

**Link**: [arxiv](http://arxiv.org/abs/2509.18307v1),  [pdf](http://arxiv.org/pdf/2509.18307v1)

**Tags**: physics.med-ph 



### Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative   Decoding
**Authors**: Sudhanshu Agrawal, Risheek Garrepalli, Raghavv Goel, Mingu Lee, Christopher Lott, Fatih Porikli

**Updated**: 2025-09-22T17:58:21Z

**Summary**: Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs (AR-LLMs) with the potential to operate at significantly higher token generation rates. However, currently available open-source dLLMs often generate at much lower rates, typically decoding only a single token at every denoising timestep in order to maximize output quality. We present Spiffy, a speculative decoding algorithm that accelerates dLLM inference by $\mathbf{2.8{-}3.1\times}$ while provably preserving the model's output distribution. This work addresses the unique challenges involved in applying ideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes draft states by leveraging the dLLM's distribution itself in an auto-speculative manner. This approach is efficient and effective, and eliminates the overheads of training and running an independent draft model. To structure the candidate draft states, we propose a novel directed draft graph which is uniquely designed to take advantage of the bidirectional, block-wise nature of dLLM generation and can be verified in parallel by the dLLM. To further optimize the structure of these draft graphs, we introduce an efficient, offline calibration algorithm that procedurally determines high-quality graph configurations. These optimized draft graphs, enabling increased acceptance rates, lead to a significant boost in the overall speedup achieved by the system. Crucially, Spiffy is also complementary to other recent innovations in improving dLLM generation speeds such as KV-caching and multi-token unmasking. We demonstrate that when combined with such parallel decoding algorithms, Spiffy is able to effectively multiply the benefits of these methods leading to total speedups of up to $\mathbf{7.9\times}$.

**Link**: [arxiv](http://arxiv.org/abs/2509.18085v1),  [pdf](http://arxiv.org/pdf/2509.18085v1)

**Tags**: cs.LG cs.AI cs.CL 



### Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings
**Authors**: Stephen Zhang, Mustafa Khan, Vardan Papyan

**Updated**: 2025-09-22T16:16:25Z

**Summary**: Large language models (LLMs) often concentrate their attention on a few specific tokens referred to as attention sinks. Common examples include the first token, a prompt-independent sink, and punctuation tokens, which are prompt-dependent. While the tokens causing the sinks often lack direct semantic meaning, the presence of the sinks is critical for model performance, particularly under model compression and KV-caching. Despite their ubiquity, the function, semantic role, and origin of attention sinks -- especially those beyond the first token -- remain poorly understood. In this work, we conduct a comprehensive investigation demonstrating that attention sinks: catch a sequence of tokens, tag them using a common direction in embedding space, and release them back into the residual stream, where tokens are later retrieved based on the tags they have acquired. Probing experiments reveal these tags carry semantically meaningful information, such as the truth of a statement. These findings extend to reasoning models, where the mechanism spans more heads and explains greater variance in embeddings, or recent models with query-key normalization, where sinks remain just as prevalent. To encourage future theoretical analysis, we introduce a minimal problem which can be solved through the 'catch, tag, release' mechanism, and where it emerges through training.

**Link**: [arxiv](http://arxiv.org/abs/2502.00919v2),  [pdf](http://arxiv.org/pdf/2502.00919v2)

**Tags**: cs.CL cs.AI cs.LG 



### Efficient Beam Search for Large Language Models Using Trie-Based   Decoding
**Authors**: Brian J Chan, MaoXun Huang, Jui-Hung Cheng, Chao-Ting Chen, Hen-Hsen Huang

**Updated**: 2025-09-22T12:28:41Z

**Summary**: This work presents a novel trie (prefix-tree)-based parallel decoding method that addresses the memory inefficiency of batch-based beam search. By sharing a single KV cache across beams with common prefixes, our approach dramatically reduces memory usage and enables efficient decoding. We evaluated our method across three attention architectures, Multi-Head Attention (Phi-3.5-mini-instruct), Grouped Query Attention (Llama-3.1-8B-Instruct), and Sliding Window Attention (Mistral-Small-24B-Instruct-2501), using CNN/DailyMail for abstractive summarization and HumanEval for code generation. Our experiments demonstrate substantial memory savings (4--8$\times$) and up to 2.4$\times$ faster decoding, without compromising generation quality. These results highlight our method's suitability for memory-constrained environments and large-scale deployments.

**Link**: [arxiv](http://arxiv.org/abs/2502.00085v2),  [pdf](http://arxiv.org/pdf/2502.00085v2)

**Tags**: cs.CL 



### Neural Attention Search
**Authors**: Difan Deng, Marius Lindauer

**Updated**: 2025-09-22T12:03:22Z

**Summary**: We present Neural Attention Search (NAtS), a framework that automatically evaluates the importance of each token within a sequence and determines if the corresponding token can be dropped after several steps. This approach can efficiently reduce the KV cache sizes required by transformer-based models during inference and thus reduce inference costs. In this paper, we design a search space that contains three token types: (i) Global Tokens will be preserved and queried by all the following tokens. (ii) Local Tokens survive until the next global token appears. (iii) Sliding Window Tokens have an impact on the inference of a fixed size of the next following tokens. Similar to the One-Shot Neural Architecture Search approach, this token-type information can be learned jointly with the architecture weights via a learnable attention mask. Experiments on both training a new transformer from scratch and fine-tuning existing large language models show that NAtS can efficiently reduce the KV cache size required for the models while maintaining the models' performance.

**Link**: [arxiv](http://arxiv.org/abs/2502.13251v3),  [pdf](http://arxiv.org/pdf/2502.13251v3)

**Tags**: cs.CL cs.AI 



### Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming   Visual Geometry Transformers
**Authors**: Soroush Mahdi, Fardin Ayar, Ehsan Javanmardi, Manabu Tsukada, Mahdi Javanmardi

**Updated**: 2025-09-22T11:54:58Z

**Summary**: Streaming visual transformers like StreamVGGT achieve strong 3D perception but suffer from unbounded growth of key value (KV) memory, which limits scalability. We propose a training-free, inference-time token eviction policy that bounds memory by discarding redundant tokens while keeping the most informative ones. Our method uses significantly less memory with little to no drop in accuracy: on 7-Scenes with long sequences it reduces peak memory from 18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under strict memory budgets, eviction enables denser frame sampling, which improves reconstruction accuracy compared to the baseline. Experiments across video depth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and camera pose estimation (Sintel, TUM-dynamics) show that our approach closely matches StreamVGGT at a fraction of the memory and makes long-horizon streaming inference more practical.

**Link**: [arxiv](http://arxiv.org/abs/2509.17650v1),  [pdf](http://arxiv.org/pdf/2509.17650v1)

**Tags**: cs.CV 



### EpiCache: Episodic KV Cache Management for Long Conversational Question   Answering
**Authors**: Minsoo Kim, Arnav Kundu, Han-Byul Kim, Richa Dixit, Minsik Cho

**Updated**: 2025-09-25T10:24:14Z

**Summary**: Modern large language models (LLMs) extend context lengths to up to millions of tokens, enabling AI assistants to generate coherent and personalized responses grounded in long conversational histories. This ability, however, hinges on Key-Value (KV) caching, whose memory grows linearly with dialogue length and quickly becomes the bottleneck in resource-constrained environments. An active line of research for reducing memory bottleneck is KV cache compression, which seeks to limit cache size while preserving accuracy. Yet existing methods face two major limitations: (i) evicting the KV cache after full-context prefill causes unbounded peak memory, and (ii) query-dependent eviction narrows the cache to a single query, leading to failure cases in multi-turn conversations. We introduce EpiCache, a training-free KV cache management framework for long conversational question answering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth through block-wise prefill and preserves topic-relevant context via episodic KV compression, which clusters conversation history into coherent episodes and applies episode-specific KV cache eviction. We further design an adaptive layer-wise budget allocation strategy that measures each layer's sensitivity to eviction and distributes the memory budget across layers accordingly. Across three LongConvQA benchmarks, EpiCache improves accuracy by up to 40% over recent baselines, sustains near-full KV accuracy under 4-6x compression, and reduces latency and memory by up to 2.4x and 3.5x, thereby enabling efficient multi-turn interaction under strict resource constraints.

**Link**: [arxiv](http://arxiv.org/abs/2509.17396v2),  [pdf](http://arxiv.org/pdf/2509.17396v2)

**Tags**: cs.CL 



### Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory
**Authors**: Manel Lurbe, Miguel Avargues, Salvador Petit, Maria E. Gomez, Rui Yang, Guanhao Wang, Julio Sahuquillo

**Updated**: 2025-09-22T06:52:35Z

**Summary**: Emerging applications, such as big data analytics and machine learning, require increasingly large amounts of main memory, often exceeding the capacity of current commodity processors built on DRAM technology. To address this, recent research has focused on off-chip memory controllers that facilitate access to diverse memory media, each with unique density and latency characteristics. While these solutions improve memory system performance, they also exacerbate the already significant memory latency. As a result, multi-level prefetching techniques are essential to mitigate these extended latencies.   This paper investigates the advantages of prefetching across both sides of the memory system: the off-chip memory and the on-chip cache hierarchy. Our primary objective is to assess the impact of a multi-level prefetching engine on overall system performance. Additionally, we analyze the individual contribution of each prefetching level to system efficiency. To achieve this, the study evaluates two key prefetching approaches: HMC (Hybrid Memory Controller) and HMC+L1, both of which employ prefetching mechanisms commonly used by processor vendors. The HMC approach integrates a prefetcher within the off-chip hybrid memory controller, while the HMC+L1 approach combines this with additional L1 on-chip prefetchers.   Experimental results on an out-of-order execution processor show that on-chip cache prefetchers are crucial for maximizing the benefits of off-chip prefetching, which in turn further enhances performance. Specifically, the off-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and up to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher coverage to as much as 92%. Consequently, overall performance increases from 9% with the HMC approach to 12% when L1 prefetching is also employed.

**Link**: [arxiv](http://arxiv.org/abs/2509.17388v1),  [pdf](http://arxiv.org/pdf/2509.17388v1)

**Tags**: cs.DC 



### Robustifying Learning-Augmented Caching Efficiently without Compromising   1-Consistency
**Authors**: Peng Chen, Hailiang Zhao, Jiaji Zhang, Xueyan Tang, Yixuan Wang, Shuiguang Deng

**Updated**: 2025-09-25T09:49:59Z

**Summary**: The online caching problem aims to minimize cache misses when serving a sequence of requests under a limited cache size. While naive learning-augmented caching algorithms achieve ideal $1$-consistency, they lack robustness guarantees. Existing robustification methods either sacrifice $1$-consistency or introduce significant computational overhead. In this paper, we introduce Guard, a lightweight robustification framework that enhances the robustness of a broad class of learning-augmented caching algorithms to $2H_k + 2$, while preserving their $1$-consistency. Guard achieves the current best-known trade-off between consistency and robustness, with only $O(1)$ additional per-request overhead, thereby maintaining the original time complexity of the base algorithm. Extensive experiments across multiple real-world datasets and prediction models validate the effectiveness of Guard in practice.

**Link**: [arxiv](http://arxiv.org/abs/2507.16242v5),  [pdf](http://arxiv.org/pdf/2507.16242v5)

**Tags**: cs.DS cs.LG 



### Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access
**Authors**: Chaoyi Ruan, Chao Bi, Kaiwen Zheng, Ziji Shi, Xinyi Wan, Jialin Li

**Updated**: 2025-09-22T05:24:22Z

**Summary**: Large Language Model (LLM) agents tackle data-intensive tasks such as deep research and code generation. However, their effectiveness depends on frequent interactions with knowledge sources across remote clouds or regions. Such interactions can create non-trivial latency and cost bottlenecks. Existing caching solutions focus on exact-match queries, limiting their effectiveness for semantic knowledge reuse.   To address this challenge, we introduce Asteria, a novel cross-region knowledge caching architecture for LLM agents. At its core are two abstractions: Semantic Element (SE) and Semantic Retrieval Index (Sine). A semantic element captures the semantic embedding representation of an LLM query together with performance-aware metadata such as latency, cost, and staticity. Sine then provides two-stage retrieval: a vector similar index with semantic embedding for fast candidate selection and a lightweight LLM-powered semantic judger for precise validation. Atop these primitives, Asteria builds a new cache interface that includes a new semantic-aware cache hit definition, a cost-efficient eviction policy, and proactive prefetching. To reduce overhead, Asteria co-locates the small LLM judger with the main LLM using adaptive scheduling and resource sharing. Our evaluation demonstrates that Asteria delivers substantial performance improvements without compromising correctness. On representative search workloads, Asteria achieves up to a 3.6$\times$ increase in throughput by maintaining cache hit rates of over 85%, while preserving accuracy virtually identical to non-cached baselines. Asteria also improves throughput for complex coding tasks by 20%, showcasing its versatility across diverse agentic workloads.

**Link**: [arxiv](http://arxiv.org/abs/2509.17360v1),  [pdf](http://arxiv.org/pdf/2509.17360v1)

**Tags**: cs.DC 



### On efficient block Krylov-solvers for $\mathcal H^2$-matrices
**Authors**: Sven Christophersen

**Updated**: 2025-09-21T22:14:56Z

**Summary**: Hierarchical matrices provide a highly memory-efficient way of storing dense linear operators arising, for example, from boundary element methods, particularly when stored in the H^2 format. In such data-sparse representations, iterative solvers are preferred over direct ones due to the cost-efficient matrix-vector multiplications they enable. Solving multiple systems of linear equations with the same hierarchical matrix naturally leads to block methods, which in turn make heavy use of BLAS level-3 functions such as GEMM. We present an efficient implementation of H^2-matrix-vector and H^2-matrix-matrix multiplication that fully exploits the potential of modern hardware in terms of memory and cache utilization. The latter is employed to accelerate block Krylov subspace methods, which we present later as the main results of this paper.

**Link**: [arxiv](http://arxiv.org/abs/2509.17257v1),  [pdf](http://arxiv.org/pdf/2509.17257v1)

**Tags**: math.NA cs.NA 65F55, 65F08, 65F10 



### MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with   RoE
**Authors**: Soheil Zibakhsh, Mohammad Samragh, Kumari Nishu, Lauren Hannah, Arnav Kundu, Minsik Cho

**Updated**: 2025-09-21T21:05:29Z

**Summary**: The generation quality of large language models (LLMs) is often improved by utilizing inference-time sequence-level scaling methods (e.g., Chain-of-Thought). We introduce hyper-parallel scaling, a complementary framework that improves prediction quality at the token level. Hyper-parallel scaling computes and aggregates multiple output proposals for a single token from the model. We implement this concept in Mixture-of-Experts (MoE) models, which we refer to as Roster of Experts (RoE). RoE is a training-free inference algorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects controlled stochasticity into the expert routing mechanism, enabling it to sample multiple diverse experts for each token and aggregate their outputs for a more accurate final prediction.To overcome the computational cost, we introduce an efficient batching strategy and a specialized KV-caching mechanism that minimizes compute and memory overhead. For example, RoE enables a 7B MoE model to match the performance of a 10.5B MoE model while using 30% less compute for inference. These gains are achieved without any fine-tuning of model parameters.

**Link**: [arxiv](http://arxiv.org/abs/2509.17238v1),  [pdf](http://arxiv.org/pdf/2509.17238v1)

**Tags**: cs.AI cs.CL cs.ET cs.LG 



### Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse
**Authors**: Zhekai Duan, Yuan Zhang, Shikai Geng, Gaowen Liu, Joschka Boedecker, Chris Xiaoxuan Lu

**Updated**: 2025-09-21T11:48:15Z

**Summary**: Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action (VLA) models by improving performance and interpretability through intermediate reasoning steps. However, its sequential autoregressive token generation introduces significant inference latency, limiting real-time deployment. We propose Fast ECoT, an inference-time acceleration method that exploits the structured and repetitive nature of ECoT to (1) cache and reuse high-level reasoning across timesteps and (2) parallelise the generation of modular reasoning steps. Additionally, we introduce an asynchronous scheduler that decouples reasoning from action decoding, further boosting responsiveness. Fast ECoT requires no model changes or additional training and integrates easily into existing VLA pipelines. Experiments in both simulation (LIBERO) and real-world robot tasks show up to a 7.5% reduction in latency with comparable or improved task success rate and reasoning faithfulness, bringing ECoT policies closer to practical real-time deployment.

**Link**: [arxiv](http://arxiv.org/abs/2506.07639v2),  [pdf](http://arxiv.org/pdf/2506.07639v2)

**Tags**: cs.RO 



### FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline
**Authors**: Jingwei Xu, Junbin Kang, Mingkai Dong, Mingyu Liu, Lu Zhang, Shaohong Guo, Ziyan Qiu, Mingzhen You, Ziyi Tian, Anqi Yu, Tianhong Ding, Xinwei Hu, Haibo Chen

**Updated**: 2025-09-21T07:03:46Z

**Summary**: Client-side metadata caching has long been considered an effective method for accelerating metadata operations in distributed file systems (DFSs). However, we have found that client-side state (e.g., caching) is not only ineffective but also consumes valuable memory resources in the deep learning pipelines. We thus propose FalconFS, a DFS optimized for deep learning pipelines with the stateless-client architecture. Specifically, instead of performing client-side path resolution and caching, FalconFS efficiently resolves paths on the server side using hybrid metadata indexing and lazy namespace replication. FalconFS also boosts server concurrency with concurrent request merging and provides easy deployment with VFS shortcut. Evaluations against CephFS and Lustre show that FalconFS achieves up to 5.72$\times$ throughput for small file read/write and up to 12.81$\times$ throughput for deep learning model training. FalconFS has been running in Huawei autonomous driving system's production environment with 10,000 NPUs for one year.

**Link**: [arxiv](http://arxiv.org/abs/2507.10367v2),  [pdf](http://arxiv.org/pdf/2507.10367v2)

**Tags**: cs.DC cs.PF 



### SpecVLM: Fast Speculative Decoding in Vision-Language Models
**Authors**: Haiduo Huang, Fuwei Yang, Zhenhua Liu, Xuanwu Yin, Dong Li, Pengju Ren, Emad Barsoum

**Updated**: 2025-09-21T03:35:36Z

**Summary**: Speculative decoding is a powerful way to accelerate autoregressive large language models (LLMs), but directly porting it to vision-language models (VLMs) faces unique systems constraints: the prefill stage is dominated by visual tokens whose count scales with image resolution and video length, inflating both compute and memory, especially the key-value (KV) cache. We study speculative decoding for VLMs and introduce SpecVLM, a practical system that (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering 1.5--2.3x end-to-end speedups over full autoregressive inference, and (2) further accelerates VLM inference with an elastic visual compressor that adaptively selects among pruning, pooling, convolution, and resampler primitives to balance FLOPs/parameters and accuracy per input. To avoid costly offline distillation corpora, we propose an online-logit distillation protocol that trains the draft model with on-the-fly teacher logits and penultimate features using a combined cross-entropy and Smooth L1 objective, eliminating storage and preprocessing while remaining compute-efficient. This protocol reveals a training-time scaling effect: longer online training monotonically increases the draft model's average accepted length, improving speculative efficiency. Empirically, SpecVLM achieves additional acceleration, culminating in 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU, consistently over resolutions and task difficulties, while preserving the target model's output distribution (lossless decoding). Our code is available at https://github.com/haiduo/SpecVLM.

**Link**: [arxiv](http://arxiv.org/abs/2509.11815v2),  [pdf](http://arxiv.org/pdf/2509.11815v2)

**Tags**: cs.CV cs.AI 



### ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix   Caching
**Authors**: Xingyu Xiang, Raj Joshi, Yuhan Liu, Jiayi Yao, Chenxingyu Zhao, Junchen Jiang, Yang Zhou, Eddie Kohler, Minlan Yu

**Updated**: 2025-09-21T00:59:45Z

**Summary**: Distributed prefix caching accelerates long-context LLM serving by reusing KV cache entries for common context prefixes. However, KV cache fetches can become a bottleneck when network bandwidth is limited. Compression mitigates the bandwidth issue, but can degrade overall performance when decompression interferes with model computation.   We present ShadowServe, the first SmartNIC-accelerated, interference-free prefix caching system for LLM serving. ShadowServe separates a control plane on the host and a data plane fully offloaded to the SmartNIC, which eliminates interference to both host GPU and CPU. To overcome the SmartNIC's limited compute and memory resources, we design a chunked pipeline that parallelizes data plane operations across the SmartNIC's compute resources, and a minimal-copy memory management scheme that reduces memory pressure on the SmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to 2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token (TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to up to 1.35x higher throughput.

**Link**: [arxiv](http://arxiv.org/abs/2509.16857v1),  [pdf](http://arxiv.org/pdf/2509.16857v1)

**Tags**: cs.DC cs.AI cs.LG 



### MPIC: Position-Independent Multimodal Context Caching System for   Efficient MLLM Serving
**Authors**: Shiju Zhao, Junhao Hu, Rongxiao Huang, Jiaqi Zheng, Guihai Chen

**Updated**: 2025-09-20T13:54:37Z

**Summary**: The context caching technique is employed to accelerate the Multimodal Large Language Model (MLLM) inference by prevailing serving platforms currently. However, this approach merely reuses the Key-Value (KV) cache of the initial sequence of prompt, resulting in full KV cache recomputation even if the prefix differs slightly. This becomes particularly inefficient in the context of interleaved text and images, as well as multimodal retrieval-augmented generation. This paper proposes position-independent caching as a more effective approach for multimodal information management. We have designed and implemented a caching system, named MPIC, to address both system-level and algorithm-level challenges. MPIC stores the KV cache on local disks when receiving multimodal data, and calculates and loads the KV cache in parallel during inference. To mitigate accuracy degradation, we have incorporated the integrated reuse and recompute mechanism within the system. The experimental results demonstrate that MPIC can achieve up to 54\% reduction in response time and 2$\times$ improvement in throughput compared to existing context caching systems, while maintaining negligible or no accuracy loss.

**Link**: [arxiv](http://arxiv.org/abs/2502.01960v2),  [pdf](http://arxiv.org/pdf/2502.01960v2)

**Tags**: cs.LG 



### EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and   Efficient LLMs
**Authors**: Zhengge Cai, Haowen Hou

**Updated**: 2025-09-20T13:27:13Z

**Summary**: Reducing the key-value (KV) cache size is a crucial step toward enabling efficient inference in large language models (LLMs), especially under latency and memory constraints. While Multi-Head Attention (MHA) offers strong representational power, it incurs significant memory overhead. Recent work on Multi-head Latent Attention (MLA) mitigates this by compressing KV representations into a shared latent space, achieving a better trade-off between performance and cache efficiency. While MLA already achieves significant KV cache reduction, the scope for further compression remains limited without performance loss. In this paper, we propose \textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel extension of MLA that further reduces KV cache size while enhancing representational expressiveness. EG-MLA introduces a token-specific embedding gating mechanism applied in the latent space, enabling fine-grained modulation of compressed KV vectors with minimal additional computation. Compared to MHA, EG-MLA achieves over 91.6\% reduction in KV cache size with negligible performance degradation. Relative to MLA, EG-MLA consistently improves task accuracy across diverse reasoning benchmarks while achieving up to 59.9\% additional memory savings. Our theoretical analysis highlights how embedding gating induces implicit high-order interactions, and empirical evaluations demonstrate robust generalization across model scales and compression regimes. Notably, we successfully scale EG-MLA to over 1 billion parameters, demonstrating its practical viability for large-scale LLM deployment. These results establish EG-MLA as a memory- and compute-efficient attention mechanism that enables scalable, high-performance inference in modern LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2509.16686v1),  [pdf](http://arxiv.org/pdf/2509.16686v1)

**Tags**: cs.CL 



### Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and   Expressive Freestyle Portrait Animation
**Authors**: Yue Ma, Zexuan Yan, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Zhifeng Li, Wei Liu, Linfeng Zhang, Qifeng Chen

**Updated**: 2025-09-20T11:09:01Z

**Summary**: We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework for freestyle portrait animation driven by facial landmarks. The main challenges in this task are preserving the identity of the reference portrait, accurately transferring target expressions, and maintaining long-term temporal consistency while ensuring generation efficiency. To address identity preservation and accurate expression retargeting, we enhance Stable Diffusion with two key components: a expression-aware landmarks as explicit motion signals, which improve motion alignment, support exaggerated expressions, and reduce identity leakage; and a fine-grained facial loss that leverages both expression and facial masks to better capture subtle expressions and faithfully preserve the reference appearance. With these components, our model supports controllable and expressive animation across diverse portrait types, including real faces, cartoons, sculptures, and animals. However, diffusion-based frameworks typically struggle to efficiently generate long-term stable animation results, which remains a core challenge in this task. To address this, we propose a progressive generation strategy for stable long-term animation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless acceleration. These two strategies ensure that our method produces high-quality results efficiently, making it user-friendly and accessible. Finally, we introduce EmojiBench++, a more comprehensive benchmark comprising diverse portraits, driving videos, and landmark sequences. Extensive evaluations on EmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior performance in both animation quality and controllability. The code, training dataset and benchmark will be found in https://follow-your-emoji.github.io/.

**Link**: [arxiv](http://arxiv.org/abs/2509.16630v1),  [pdf](http://arxiv.org/pdf/2509.16630v1)

**Tags**: cs.CV 



### Shift Parallelism: Low-Latency, High-Throughput LLM Inference for   Dynamic Workloads
**Authors**: Mert Hidayetoglu, Aurick Qiao, Michael Wyatt, Jeff Rasley, Yuxiong He, Samyam Rajbhandari

**Updated**: 2025-09-20T01:56:25Z

**Summary**: Efficient parallelism is necessary for achieving low-latency, high-throughput inference with large language models (LLMs). Tensor parallelism (TP) is the state-of-the-art method for reducing LLM response latency, however GPU communications reduces combined token throughput. On the other hand, data parallelism (DP) obtains a higher throughput yet is slow in response latency. Best of both worlds does not exist, and it is not possible to combine TP and DP because of the KV cache variance across the parallelisms.   We notice Sequence Parallelism (SP - Ulysses in training) has similar properties as DP but with KV cache invariance. We adapt SP to inference, and combine it with TP to get the best of both worlds. Our solution: Shift Parallelism.   Shift Parallelism dynamically switches across TP and SP, and minimizes latency in low traffic without losing throughput in high traffic. The efficient GPU communications of Shift Parallelism yields up to i) 1.51x faster response in interactive workloads and ii) 50% higher throughput in batch workloads, compared to a TP-only solution.   We evaluate Shift Parallelism with real-world production traces with dynamic traffic patterns as well as synthetic benchmarking patterns across models, context sizes, and arrival rates. All results affirm the same: Shift Parallelism has a better the latency vs. throughput tradeoff than TP or DP, and hence obtains low latency without degrading throughput in dynamic workloads.

**Link**: [arxiv](http://arxiv.org/abs/2509.16495v1),  [pdf](http://arxiv.org/pdf/2509.16495v1)

**Tags**: cs.DC 



### From Coated to Uncoated: Scanning Electron Microscopy Corrections to   Estimate True Surface Pore Size in Nanoporous Membranes
**Authors**: Sima Zeinali Danalou, Dian Yu, Niher R. Sarker, Hooman Chamani, Jane Y. Howe, Patrick C. Lee, Jay R. Werber

**Updated**: 2025-09-19T23:46:08Z

**Summary**: Scanning electron microscopy (SEM) is the premier method for characterizing the nanoscale surface pores in ultrafiltration (UF) membranes and the support layers of reverse osmosis (RO) membranes. Based on SEM, the conventional understanding is that membranes typically have low surface porosities of <10%. We hypothesized that high acceleration voltage during SEM imaging and sputter metal coatings required for SEM have led to systematic underestimations of porosity and pore size. We showed that imaging a commercial UF membrane at 1, 5, and 10 kV reduced measured porosity from 10.3% (1 kV) to 6.3% (10 kV), while increasing Pt coating thickness from 1.5 to 5 nm lowered porosity by 54% for the UF membrane (12.9% to 5.8%) and 46% for an RO support (13.1% to 7.0%). To account for coating thickness, we developed a digital correction method that simulates pore dilation, enabling the pore structure to be estimated for uncoated membranes. Dilation yielded uncoated porosity values of 23% for the UF membrane and 20% for the RO support, about 3-fold greater than values observed with a 4 nm coating. Mean pore diameters were 2-fold greater for the UF membrane and 1.5-fold greater for the RO support. Critically, dilation-derived pore-size distributions agreed with low-flux dextran-retention data fitted with the Bungay-Brenner model. Our results suggest that surface porosities and pore sizes of nanoporous membranes are much larger than previously understood, with major implications for structure/transport relationships. For future nanoscale pore analysis of membranes (and other nanoporous materials), we recommend low acceleration voltage (1 kV), minimal coatings (1-2 nm), and digital dilation to account for coating artifacts

**Link**: [arxiv](http://arxiv.org/abs/2509.16471v1),  [pdf](http://arxiv.org/pdf/2509.16471v1)

**Tags**: cond-mat.mtrl-sci cs.CV physics.app-ph physics.chem-ph physics.ins-det 



### WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables
**Authors**: Hunter McCoy, Prashant Pandey

**Updated**: 2025-09-19T20:31:38Z

**Summary**: GPU hash tables are increasingly used to accelerate data processing, but their limited functionality restricts adoption in large-scale data processing applications. Current limitations include incomplete concurrency support and missing compound operations such as upserts.   This paper presents WarpSpeed, a library of high-performance concurrent GPU hash tables with a unified benchmarking framework for performance analysis. WarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and provides a rich API designed for modern GPU applications. Our evaluation uses diverse benchmarks to assess both correctness and scalability, and we demonstrate real-world impact by integrating these hash tables into three downstream applications.   We propose several optimization techniques to reduce concurrency overhead, including fingerprint-based metadata to minimize cache line probes and specialized Nvidia GPU instructions for lock-free queries. Our findings provide new insights into concurrent GPU hash table design and offer practical guidance for developing efficient, scalable data structures on modern GPUs.

**Link**: [arxiv](http://arxiv.org/abs/2509.16407v1),  [pdf](http://arxiv.org/pdf/2509.16407v1)

**Tags**: cs.DC cs.DS 



### Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision   Language Model Reasoning
**Authors**: Mingyuan Wu, Jize Jiang, Haozhen Zheng, Meitang Li, Zhaoheng Li, Beitong Tian, Bo Chen, Yongjoo Park, Minjia Zhang, Chengxiang Zhai, Klara Nahrstedt

**Updated**: 2025-09-19T17:18:26Z

**Summary**: Vision Language Models (VLMs) have achieved remarkable success in a wide range of vision applications of increasing complexity and scales, yet choosing the right VLM model size involves a trade-off between response quality and cost. While smaller VLMs are cheaper to run, they typically produce responses only marginally better than random guessing on benchmarks such as MMMU.   In this paper, we propose Cache of Thought (CoT), a master apprentice framework for collaborative inference between large and small VLMs. CoT manages high quality query results from large VLMs (master) in a cache, which are then selected via a novel multi modal retrieval and in-context learning to aid the performance of small VLMs (apprentice). We extensively evaluate CoT on various widely recognized and challenging general reasoning benchmarks, and show that CoT increases overall reasoning performance by up to 7.7% under the same budget, and specifically boosts the performance of apprentice VLMs by up to 36.6%. Our code is available at https://github.com/UIUC-MONET/Cache-of-Thoughts

**Link**: [arxiv](http://arxiv.org/abs/2502.20587v2),  [pdf](http://arxiv.org/pdf/2502.20587v2)

**Tags**: cs.LG 



### KVCompose: Efficient Structured KV Cache Compression with Composite   Tokens
**Authors**: Dmitry Akulov, Mohamed Sana, Antonio De Domenico, Tareq Si Salem, Nicola Piovesan, Fadhel Ayed

**Updated**: 2025-09-19T15:19:26Z

**Summary**: Large language models (LLMs) rely on key-value (KV) caches for efficient autoregressive decoding; however, cache size grows linearly with context length and model depth, becoming a major bottleneck in long-context inference. Prior KV cache compression methods either enforce rigid heuristics, disrupt tensor layouts with per-attention-head variability, or require specialized compute kernels.   We propose a simple, yet effective, KV cache compression framework based on attention-guided, layer-adaptive composite tokens. Our method aggregates attention scores to estimate token importance, selects head-specific tokens independently, and aligns them into composite tokens that respect the uniform cache structure required by existing inference engines. A global allocation mechanism further adapts retention budgets across layers, assigning more capacity to layers with informative tokens. This approach achieves significant memory reduction while preserving accuracy, consistently outperforming prior structured and semi-structured methods. Crucially, our approach remains fully compatible with standard inference pipelines, offering a practical and scalable solution for efficient long-context LLM deployment.

**Link**: [arxiv](http://arxiv.org/abs/2509.05165v2),  [pdf](http://arxiv.org/pdf/2509.05165v2)

**Tags**: cs.LG 



### Commissioning, characterization and first high dose rate irradiations at   a compact X-ray tube for microbeam and minibeam radiation therapy
**Authors**: Christian Petrich, Johanna Winter, Anton Dimroth, Thomas Beiser, Monika Dehn, Jessica Stolz, Jacopo Frignani, Stephanie E. Combs, Franz Schilling, Ghaleb Natour, Kurt Aulenbacher, Thomas E. Schmid, Jan J. Wilkens, Stefan Bartzsch

**Updated**: 2025-09-19T14:14:32Z

**Summary**: Minibeam and microbeam radiation therapy promise improved treatment outcomes through reduced normal tissue toxicity at better tumor control rates. The lack of suitable compact radiation sources limits the clinical application of minibeams to superficial tumors and renders it impossible for microbeams. We developed the first prototype of a compact line-focus X-ray tube (LFXT) with technology potentially suitable for clinical translation of minibeams and microbeams. We give an overview of the commissioning process preceding first operation, present optical and radiological focal spot characterization methods, and dosimetric measurements. Additionally, we report on first preclinical in vitro cell and in vivo mouse brain irradiations conducted with the LFXT prototype. The LFXT was high voltage conditioned up to 300 kV.The focal spot characterization resulted in a strongly eccentric electron distribution with a width of 72.3 $\mu$m. Dosimetry showed sharp microbeam dose profiles with steep lateral penumbras and a peak-to-valley dose ratio above 10 throughout a 70 mm thick PMMA phantom. An open-field dose rate of 4.3 Gy/s was measured at an acceleration voltage of 150 kV and a beam current of 17.4 mA at 150 mm distance from the focal spot. In vitro and in vivo experiments demonstrated the feasibility of the LFXT for minibeam and microbeam applications with field sizes of 1.5-2 cm. The mice displayed no observable side effects after whole-brain 260 $\mu$m-minibeam irradiation. We successfully constructed and commissioned the first proof-of-concept LFXT prototype. Dosimetric characterizations of the achieved microbeam field showed the superiority of the LFXT compared to conventional X-ray tubes in terms of beam quality. In future developments, the remaining limitations of the prototype will be addressed for improved minibeam and first ever microbeam radiation therapy in a clinical setting.

**Link**: [arxiv](http://arxiv.org/abs/2506.09536v2),  [pdf](http://arxiv.org/pdf/2506.09536v2)

**Tags**: physics.med-ph 



### UniGist: Towards General and Hardware-aligned Sequence-level Long   Context Compression
**Authors**: Chenlong Deng, Zhisong Zhang, Kelong Mao, Shuaiyi Li, Tianqing Fang, Hongming Zhang, Haitao Mi, Dong Yu, Zhicheng Dou

**Updated**: 2025-09-19T08:47:37Z

**Summary**: Large language models are increasingly capable of handling long-context inputs, but the memory overhead of key-value (KV) cache remains a major bottleneck for general-purpose deployment. While various compression strategies have been explored, sequence-level compression, which drops the full KV caches for certain tokens, is particularly challenging as it can lead to the loss of important contextual information. To address this, we introduce UniGist, a sequence-level long-context compression framework that efficiently preserves context information by replacing raw tokens with special compression tokens (gists) in a fine-grained manner. We adopt a chunk-free training strategy and design an efficient kernel with a gist shift trick, enabling optimized GPU training. Our scheme also supports flexible inference by allowing the actual removal of compressed tokens, resulting in real-time memory savings. Experiments across multiple long-context tasks demonstrate that UniGist significantly improves compression quality, with especially strong performance in detail-recalling tasks and long-range dependency modeling.

**Link**: [arxiv](http://arxiv.org/abs/2509.15763v1),  [pdf](http://arxiv.org/pdf/2509.15763v1)

**Tags**: cs.CL 



### CARD: A Cache-Assisted Parallel Speculative Decoding Framework via   Query-and-Correct Paradigm for Accelerating LLM Inference
**Authors**: Enyu Zhou, Kai Sheng, Hao Chen, Xin He

**Updated**: 2025-09-19T06:20:14Z

**Summary**: Speculative decoding (SD), where a draft model provides multiple candidate tokens for the target model to verify in parallel, has demonstrated significant potential for accelerating LLM inference. Yet, existing SD approaches adhere to a strict draft-then-verify paradigm, enforcing a sequential process that hampers performance and constrains the draft model's capacity. Moreover, rejecting a token in the candidate sequence invalidates all subsequent tokens, leading to wasted computation during drafting. To overcome these limitations, we propose a cache-assisted parallel speculative decoding framework called CARD, which employs a novel query-and-correct paradigm. Our approach decouples drafting from verification: the draft model populates a shared cache with candidate tokens, while the target model concurrently refines the draft's trajectory. This enables inference at near-draft-speed, effectively leveraging the draft model's efficiency without additional fine-tuning. Experimental results show that CARD significantly outperforms existing state-of-the-art methods, achieving up to a 4.83x acceleration over vanilla autoregressive decoding, with no fine-tuning required for either models.

**Link**: [arxiv](http://arxiv.org/abs/2508.04462v2),  [pdf](http://arxiv.org/pdf/2508.04462v2)

**Tags**: cs.LG 



### Optimization techniques for SQL+ML queries: A performance analysis of   real-time feature computation in OpenMLDB
**Authors**: Mashkhal A. Sidiq, Aras A. Salih, Samrand M. Hassan

**Updated**: 2025-09-19T02:27:01Z

**Summary**: In this study, we optimize SQL+ML queries on top of OpenMLDB, an open-source database that seamlessly integrates offline and online feature computations. The work used feature-rich synthetic dataset experiments in Docker, which acted like production environments that processed 100 to 500 records per batch and 6 to 12 requests per batch in parallel. Efforts have been concentrated in the areas of better query plans, cached execution plans, parallel processing, and resource management. The experimental results show that OpenMLDB can support approximately 12,500 QPS with less than 1 ms latency, outperforming SparkSQL and ClickHouse by a factor of 23 and PostgreSQL and MySQL by 3.57 times. This study assessed the impact of optimization and showed that query plan optimization accounted for 35% of the performance gains, caching for 25%, and parallel processing for 20%. These results illustrate OpenMLDB's capability for time-sensitive ML use cases, such as fraud detection, personalized recommendation, and time series forecasting. The system's modular optimization framework, which combines batch and stream processing without interference, contributes to its significant performance gain over traditional database systems, particularly in applications that require real-time feature computation and serving. This study contributes to the understanding and design of high-performance SQL+ML systems and highlights the need for specialized SQL optimization for ML workloads.

**Link**: [arxiv](http://arxiv.org/abs/2509.15529v1),  [pdf](http://arxiv.org/pdf/2509.15529v1)

**Tags**: cs.DB 



### LLM Cache Bandit Revisited: Addressing Query Heterogeneity for   Cost-Effective LLM Inference
**Authors**: Hantao Yang, Hong Xie, Defu Lian, Enhong Chen

**Updated**: 2025-09-19T01:39:08Z

**Summary**: This paper revisits the LLM cache bandit problem, with a special focus on addressing the query heterogeneity for cost-effective LLM inference. Previous works often assume uniform query sizes. Heterogeneous query sizes introduce a combinatorial structure for cache selection, making the cache replacement process more computationally and statistically challenging. We treat optimal cache selection as a knapsack problem and employ an accumulation-based strategy to effectively balance computational overhead and cache updates. In theoretical analysis, we prove that the regret of our algorithm achieves an $O(\sqrt{MNT})$ bound, improving the coefficient of $\sqrt{MN}$ compared to the $O(MN\sqrt{T})$ result in Berkeley, where $N$ is the total number of queries and $M$ is the cache size. Additionally, we also provide a problem-dependent bound, which was absent in previous works. The experiment rely on real-world data show that our algorithm reduces the total cost by approximately 12\%.

**Link**: [arxiv](http://arxiv.org/abs/2509.15515v1),  [pdf](http://arxiv.org/pdf/2509.15515v1)

**Tags**: cs.CL 



### High Voltage Delivery and Distribution for the NEXT-100 Time Projection   Chamber
**Authors**: NEXT Collaboration, C. Adams, H. Almazán, V. Álvarez, K. Bailey, R. Guenette, B. J. P. Jones, S. Johnston, K. Mistry, F. Monrabal, D. R. Nygren, B. Palmeiro, L. Rogers, J. Waldschmidt, B. Aparicio, A. I. Aranburu, L. Arazi, I. J. Arnquist, F. Auria-Luna, S. Ayet, C. D. R. Azevedo, F. Ballester, M. del Barrio-Torregrosa, A. Bayo, J. M. Benlloch-Rodríguez, F. I. G. M. Borges, A. Brodolin, S. Cárcel, A. Castillo, L. Cid, C. A. N. Conde, T. Contreras, F. P. Cossío, R. Coupe, E. Dey, G. Díaz, C. Echevarria, M. Elorza, J. Escada, R. Esteve, R. Felkai, L. M. P. Fernandes, P. Ferrario, A. L. Ferreira, F. W. Foss, Z. Freixa, J. García-Barrena, J. J. Gómez-Cadenas, J. W. R. Grocott, R. Guenette, J. Hauptman, C. A. O. Henriques, J. A. Hernando Morata, P. Herrero-Gómez, V. Herrero, C. Hervés Carrete, Y. Ifergan, F. Kellerer, L. Larizgoitia, A. Larumbe, P. Lebrun, F. Lopez, N. López-March, R. Madigan, R. D. P. Mano, A. P. Marques, J. Martín-Albo, G. Martínez-Lema, M. Martínez-Vara, R. L. Miller, J. Molina-Canteras, F. Monrabal, C. M. B. Monteiro, F. J. Mora, P. Novella, A. Nuñez, E. Oblak, J. Palacio, B. Palmeiro, A. Para, A. Pazos, J. Pelegrin, M. Pérez Maneiro, M. Querol, J. Renner, I. Rivilla, C. Rogero, B. Romeo, C. Romo-Luque, V. San Nacienciano, F. P. Santos, J. M. F. dos Santos, M. Seemann, I. Shomroni, P. A. O. C. Silva, A. Simón, S. R. Soleti, M. Sorel, J. Soto-Oton, J. M. R. Teixeira, S. Teruel-Pardo, J. F. Toledo, C. Tonnelé, S. Torelli, J. Torrent, A. Trettin, A. Usón, P. R. G. Valle, J. F. C. A. Veloso, J. Waiton, A. Yubero-Navarro

**Updated**: 2025-09-18T23:34:50Z

**Summary**: A critical element in the realization of large liquid and gas time projection chambers (TPCs) is the delivery and distribution of high voltages into and around the detector. Such experiments require of order tens of kilovolts to enable electron drift over meter-scale distances. This paper describes the design and operation of the cathode feedthrough and high voltage distribution through the field cage of the NEXT-100 experiment, an underground TPC that will search for neutrinoless double beta decay $0\nu\beta\beta$. The feedthrough has been demonstrated to hold pressures up to 20~bar and sustain voltages as high as -65~kV, and the TPC is operating stably at its design high voltages. The system has been realized within the constraints of a stringent radiopurity budget and is now being used to execute a suite of sensitive double beta decay analyses.

**Link**: [arxiv](http://arxiv.org/abs/2505.01002v3),  [pdf](http://arxiv.org/pdf/2505.01002v3)

**Tags**: physics.ins-det hep-ex 



### Language Modeling with Learned Meta-Tokens
**Authors**: Alok N. Shah, Khush Gupta, Keshav Ramji, Pratik Chaudhari

**Updated**: 2025-09-18T17:38:48Z

**Summary**: While modern Transformer-based language models (LMs) have achieved major success in multi-task generalization, they often struggle to capture long-range dependencies within their context window. This work introduces a novel approach using meta-tokens, special tokens injected during pre-training, along with a dedicated meta-attention mechanism to guide LMs to use these tokens. We pre-train a language model with a modified GPT-2 architecture equipped with meta-attention in addition to causal multi-head attention, and study the impact of these tokens on a suite of synthetic tasks. We find that data-efficient language model pre-training on fewer than 100B tokens utilizing meta-tokens and our meta-attention mechanism achieves strong performance on these tasks after fine-tuning. We suggest that these gains arise due to the meta-tokens sharpening the positional encoding. This enables them to operate as trainable, content-based landmarks, implicitly compressing preceding context and "caching" it in the meta-token. At inference-time, the meta-token points to relevant context, facilitating length generalization up to 2$\times$ its context window, even after extension with YaRN. We provide further evidence of these behaviors by visualizing model internals to study the residual stream, and assessing the compression quality by information-theoretic analysis on the rate-distortion tradeoff. Our findings suggest that pre-training LMs with meta-tokens offers a simple, data-efficient method to enhance long-context language modeling performance, while introducing new insights into the nature of their behavior towards length generalization.

**Link**: [arxiv](http://arxiv.org/abs/2509.16278v1),  [pdf](http://arxiv.org/pdf/2509.16278v1)

**Tags**: cs.CL cs.LG 



### Value-Guided KV Compression for LLMs via Approximated CUR Decomposition
**Authors**: Ayan Sengupta, Siddhant Chaudhary, Tanmoy Chakraborty

**Updated**: 2025-09-18T15:04:06Z

**Summary**: Key-value (KV) cache compression has emerged as a critical technique for reducing the memory and latency overhead of autoregressive language models during inference. Prior approaches predominantly rely on query-key attention scores to rank and evict cached tokens, assuming that attention intensity correlates with semantic importance. However, this heuristic overlooks the contribution of value vectors, which directly influence the attention output. In this paper, we propose CurDKV, a novel, value-centric KV compression method that selects keys and values based on leverage scores computed from CUR matrix decomposition. Our approach approximates the dominant subspace of the attention output $softmax(QK^T)V$, ensuring that the retained tokens best preserve the model's predictive behavior. Theoretically, we show that attention score approximation does not guarantee output preservation, and demonstrate that CUR-based selection minimizes end-to-end attention reconstruction loss. Empirically, CurDKV achieves up to 9.6% higher accuracy than state-of-the-art methods like SnapKV and ChunkKV under aggressive compression budgets on LLaMA and Mistral, while maintaining compatibility with FlashAttention and Grouped Query Attention. In addition to improved accuracy, CurDKV reduces generation latency by up to 40% at high compression, offering a practical speed-accuracy tradeoff.

**Link**: [arxiv](http://arxiv.org/abs/2509.15038v1),  [pdf](http://arxiv.org/pdf/2509.15038v1)

**Tags**: cs.CL 



### Attention Beyond Neighborhoods: Reviving Transformer for Graph   Clustering
**Authors**: Xuanting Xie, Bingheng Li, Erlin Pan, Rui Hou, Wenyu Chen, Zhao Kang

**Updated**: 2025-09-18T14:51:13Z

**Summary**: Attention mechanisms have become a cornerstone in modern neural networks, driving breakthroughs across diverse domains. However, their application to graph structured data, where capturing topological connections is essential, remains underexplored and underperforming compared to Graph Neural Networks (GNNs), particularly in the graph clustering task. GNN tends to overemphasize neighborhood aggregation, leading to a homogenization of node representations. Conversely, Transformer tends to over globalize, highlighting distant nodes at the expense of meaningful local patterns. This dichotomy raises a key question: Is attention inherently redundant for unsupervised graph learning? To address this, we conduct a comprehensive empirical analysis, uncovering the complementary weaknesses of GNN and Transformer in graph clustering. Motivated by these insights, we propose the Attentive Graph Clustering Network (AGCN) a novel architecture that reinterprets the notion that graph is attention. AGCN directly embeds the attention mechanism into the graph structure, enabling effective global information extraction while maintaining sensitivity to local topological cues. Our framework incorporates theoretical analysis to contrast AGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV cache mechanism to improve computational efficiency, and (2) a pairwise margin contrastive loss to boost the discriminative capacity of the attention space. Extensive experimental results demonstrate that AGCN outperforms state-of-the-art methods.

**Link**: [arxiv](http://arxiv.org/abs/2509.15024v1),  [pdf](http://arxiv.org/pdf/2509.15024v1)

**Tags**: cs.LG cs.AI cs.NI 



### BWCache: Accelerating Video Diffusion Transformers through Block-Wise   Caching
**Authors**: Hanshuai Cui, Zhiqing Tang, Zhifei Xu, Zhi Yao, Wenyi Zeng, Weijia Jia

**Updated**: 2025-09-18T04:57:32Z

**Summary**: Recent advancements in Diffusion Transformers (DiTs) have established them as the state-of-the-art method for video generation. However, their inherently sequential denoising process results in inevitable latency, limiting real-world applicability. Existing acceleration methods either compromise visual quality due to architectural modifications or fail to reuse intermediate features at proper granularity. Our analysis reveals that DiT blocks are the primary contributors to inference latency. Across diffusion timesteps, the feature variations of DiT blocks exhibit a U-shaped pattern with high similarity during intermediate timesteps, which suggests substantial computational redundancy. In this paper, we propose Block-Wise Caching (BWCache), a training-free method to accelerate DiT-based video generation. BWCache dynamically caches and reuses features from DiT blocks across diffusion timesteps. Furthermore, we introduce a similarity indicator that triggers feature reuse only when the differences between block features at adjacent timesteps fall below a threshold, thereby minimizing redundant computations while maintaining visual fidelity. Extensive experiments on several video diffusion models demonstrate that BWCache achieves up to 2.24$\times$ speedup with comparable visual quality.

**Link**: [arxiv](http://arxiv.org/abs/2509.13789v2),  [pdf](http://arxiv.org/pdf/2509.13789v2)

**Tags**: cs.CV cs.AI 



### Kilovolt-Class $β-Ga_2O_3$ Field-Plated Schottky Barrier Diodes with   MOCVD-Grown Intentionally $10^{15}$ $cm^{-3}$ Doped Drift Layers
**Authors**: Carl Peterson, Chinmoy Nath Saha, Rachel Kahler, Yizheng Liu, Akhila Mattapalli, Saurav Roy, Sriram Krishnamoorthy

**Updated**: 2025-09-17T20:08:53Z

**Summary**: We report on the growth optimization of intentionally low-doped ($10^{15}$ $cm^{-3}$) high-quality $\beta-Ga_2O_3$ drift layers up to 10 $\mu m$ thick via MOCVD and the fabrication of kilovolt-class field plated Schottky barrier diodes on these thick drift layers. Homoepitaxial growth was performed on (010) $10^{15}$ $cm^{-3}$ substrates using TMGa as the Ga precursor. Growth parameters were systematically optimized to determine the best conditions for high quality thick growths with the given reactor geometry. Chamber pressure was found to improve the growth rate, mobility, and roughness of the samples. Growth rates of up to 7.2 $\mu m$/hr., thicknesses of up to 10 $\mu m$, Hall mobilities of up to 176 $cm^2$/Vs, RMS roughness down to 5.45 nm, UID concentrations as low as $2 \times$ $10^{15}$ $cm^{-3}$, and controllable intentional doping down to $3 \times$ $10^{15}$ $cm^{-3}$ were achieved. Field plated Schottky barrier diodes (FP-SBDs) were fabricated on a $6.5 \times$ $10^{15}$ $cm^{-3}$ intentionally doped 10 $\mu m$ thick film to determine the electrical performance of the MOCVD-grown material. The FP-SBD was found to have current density $>$100 A/$cm^2$ at 3 V forward bias with a specific differential on resistance ($R_{on,sp}$) of 16.22 m$\Omega$.$cm^2$ and a turn on voltage of 1 V. The diodes were found to have high quality anode metal/semiconductor interfaces with an ideality factor of 1.04, close to unity. Diodes had a maximum breakdown voltage of 1.50 kV, leading to a punch-through maximum field of 2.04 MV/cm under the anode metal, which is a state-of-the-art result for SBDs on MOCVD-grown (010) drift layers.

**Link**: [arxiv](http://arxiv.org/abs/2509.14403v1),  [pdf](http://arxiv.org/pdf/2509.14403v1)

**Tags**: physics.app-ph cond-mat.mtrl-sci 



### On the Illusion of Success: An Empirical Study of Build Reruns and   Silent Failures in Industrial CI
**Authors**: Henri Aïdasso, Francis Bordeleau, Ali Tizghadam

**Updated**: 2025-09-17T18:26:29Z

**Summary**: Reliability of build outcomes is a cornerstone of effective Continuous Integration (CI). Yet in practice, developers often struggle with non-deterministic issues in the code or CI infrastructure, which undermine trust in build results. When faced with such unexpected outcomes, developers often repeatedly rerun jobs hoping for true success, but this practice is known to increase CI costs and reduce productivity. While recent studies have focused on intermittent job failures, no prior work has investigated silent failures, where build jobs are marked as successful but fail to complete all or part of their tasks. Such silent failures often go unnoticed, creating an illusion of success with detrimental consequences such as bugs escaping into production. This paper presents the first empirical study of silent failures through the practice of rerunning successful jobs. An analysis of 142,387 jobs across 81 industrial projects shows that 11% of successful jobs are rerun, with 35% of these reruns occurring after more than 24 hours. Using mixed-effects models on 32 independent variables (AUC of 85%), we identified key factors associated with reruns of successful jobs, notably testing and static analysis tasks, scripting languages like Shell, and developers prior rerun tendencies. A further analysis of 92 public issues revealed 11 categories of silent failures aligning with these factors, the most frequent being artifact operation errors, caching errors, and ignored exit codes. Overall, our findings provide valuable insights into the circumstances and causes of silent failures to raise awareness among teams, and present solutions to improve CI reliability.

**Link**: [arxiv](http://arxiv.org/abs/2509.14347v1),  [pdf](http://arxiv.org/pdf/2509.14347v1)

**Tags**: cs.SE 



### Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A   Self-Optimizing Framework
**Authors**: Kerui Huang, Shuhan Liu, Xing Hu, Tongtong Xu, Lingfeng Bao, Xin Xia

**Updated**: 2025-09-17T15:33:44Z

**Summary**: Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by prompting intermediate steps, improving accuracy and robustness in arithmetic, logic, and commonsense tasks. However, this benefit comes with high computational costs: longer outputs increase latency, memory usage, and KV-cache demands. These issues are especially critical in software engineering tasks where concise and deterministic outputs are required. To investigate these trade-offs, we conduct an empirical study based on code generation benchmarks. The results reveal that longer CoT does not always help. Excessive reasoning often causes truncation, accuracy drops, and latency up to five times higher, with failed outputs consistently longer than successful ones. These findings challenge the assumption that longer reasoning is inherently better and highlight the need for adaptive CoT control. Motivated by this, we propose SEER (Self-Enhancing Efficient Reasoning), an adaptive framework that compresses CoT while preserving accuracy. SEER combines Best-of-N sampling with task-aware adaptive filtering, dynamically adjusting thresholds based on pre-inference outputs to reduce verbosity and computational overhead. We then evaluate SEER on three software engineering tasks and one math task. On average, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation, and eliminates most infinite loops. These results demonstrate SEER as a practical method to make CoT-enhanced LLMs more efficient and robust, even under resource constraints.

**Link**: [arxiv](http://arxiv.org/abs/2509.14093v1),  [pdf](http://arxiv.org/pdf/2509.14093v1)

**Tags**: cs.SE cs.AI cs.CL 



### A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval   Prediction For Instruction Caching
**Authors**: Henry Kao, Nikhil Sreekumar, Prabhdeep Singh Soni, Ali Sedaghati, Fang Su, Bryan Chan, Maziar Goudarzi, Reza Azimi

**Updated**: 2025-09-17T14:42:38Z

**Summary**: Modern mobile CPU software pose challenges for conventional instruction cache replacement policies due to their complex runtime behavior causing high reuse distance between executions of the same instruction. Mobile code commonly suffers from large amounts of stalls in the CPU frontend and thus starvation of the rest of the CPU resources. Complexity of these applications and their code footprint are projected to grow at a rate faster than available on-chip memory due to power and area constraints, making conventional hardware-centric methods for managing instruction caches to be inadequate. We present a novel software-hardware co-design approach called TRRIP (Temperature-based Re-Reference Interval Prediction) that enables the compiler to analyze, classify, and transform code based on "temperature" (hot/cold), and to provide the hardware with a summary of code temperature information through a well-defined OS interface based on using code page attributes. TRRIP's lightweight hardware extension employs code temperature attributes to optimize the instruction cache replacement policy resulting in the eviction rate reduction of hot code. TRRIP is designed to be practical and adoptable in real mobile systems that have strict feature requirements on both the software and hardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5% resulting in geomean speedup of 3.9%, on top of RRIP cache replacement running mobile code already optimized using PGO.

**Link**: [arxiv](http://arxiv.org/abs/2509.14041v1),  [pdf](http://arxiv.org/pdf/2509.14041v1)

**Tags**: cs.AR cs.CL cs.OS cs.PF 



### SBVR: Summation of BitVector Representation for Efficient LLM   Quantization
**Authors**: Wonjun Bang, Jongseok Park, Hongseung Yu, Kyungmin Bin, Kyunghan Lee

**Updated**: 2025-09-17T13:51:27Z

**Summary**: With the advent of large language models (LLMs), numerous Post-Training Quantization (PTQ) strategies have been proposed to alleviate deployment barriers created by their enormous parameter counts. Quantization achieves compression by limiting the number of representable points in the data. Therefore, the key to achieving efficient quantization is selecting the optimal combination of representation points, or codes, for the given data. Existing PTQ solutions adopt two major approaches to this problem: Round-To-Nearest (RTN)-based methods and codebook-based methods. RTN-based methods map LLM weights onto uniformly distributed integer grids, failing to account for the Gaussian-like weight distribution of LLM weights. Codebook-based methods mitigate this issue by constructing distribution-aware codebooks; however, they suffer from random and strided memory access patterns, resulting in degraded inference speed that is exacerbated by the limited size of GPU L1 cache. To overcome these limitations, we propose a novel LLM quantization method, SBVR (Summation of BitVector Representation), that enables Gaussian-like code representation in a hardware-friendly manner for fast inference. SBVR maps weight values to non-uniform representation points whose distribution follows the actual distribution of LLM weights, enabling more accurate compression. Additionally, we design a custom CUDA kernel that allows matrix-vector multiplication directly in the SBVR format without decompression, thereby enabling high-performance execution of SBVR-compressed models. Our evaluations of SBVR on various models demonstrate state-of-the-art perplexity and accuracy benchmark performance while delivering a 2.21x- 3.04x end-to-end token-generation speedup over naive FP16 models in the 4-bit quantization regime.

**Link**: [arxiv](http://arxiv.org/abs/2509.18172v1),  [pdf](http://arxiv.org/pdf/2509.18172v1)

**Tags**: cs.LG 



### SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation
**Authors**: Jiayi Pan, Jiaming Xu, Yongkang Zhou, Guohao Dai

**Updated**: 2025-09-17T09:24:40Z

**Summary**: Feature caching has recently emerged as a promising method for diffusion model acceleration. It effectively alleviates the inefficiency problem caused by high computational requirements by caching similar features in the inference process of the diffusion model. In this paper, we analyze existing feature caching methods from the perspective of information utilization, and point out that relying solely on historical information will lead to constrained accuracy and speed performance. And we propose a novel paradigm that introduces future information via self-speculation based on the information similarity at the same time step across different iteration times. Based on this paradigm, we present \textit{SpecDiff}, a training-free multi-level feature caching strategy including a cached feature selection algorithm and a multi-level feature classification algorithm. (1) Feature selection algorithm based on self-speculative information. \textit{SpecDiff} determines a dynamic importance score for each token based on self-speculative information and historical information, and performs cached feature selection through the importance score. (2) Multi-level feature classification algorithm based on feature importance scores. \textit{SpecDiff} classifies tokens by leveraging the differences in feature importance scores and introduces a multi-level feature calculation strategy. Extensive experiments show that \textit{SpecDiff} achieves average 2.80 \times, 2.74 \times , and 3.17\times speedup with negligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow on NVIDIA A800-80GB GPU. By merging speculative and historical information, \textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing the Pareto frontier of speedup and accuracy in the efficient diffusion model inference.

**Link**: [arxiv](http://arxiv.org/abs/2509.13848v1),  [pdf](http://arxiv.org/pdf/2509.13848v1)

**Tags**: cs.CV cs.LG 



### A Framework for Multi-source Prefetching Through Adaptive Weight
**Authors**: Yoseph Berhanu Alebachew, Mulugeta Libsie

**Updated**: 2025-09-17T00:28:49Z

**Summary**: The World Wide Web has come to be a great part of our daily life, yet user observed latency is still a problem that needs a proper means of handling. Even though earlier attempts focused on caching as the chief solution to tackling this issue, its success was extremely limited. Prefetching has come to be the primary technique in supplementing caching towards soothing the latency problem associated with the contemporary Internet. However, existing approaches in prefetching are extremely limited in their ability to employ application level web document relationship which is often visible only to the content developer. This is because most approaches are access history based schemes that make future users' access prediction only based on past user access. Attempts to incorporate prefetching schemes that utilize semantic information with those that use users past access history are extremely limited in their extensibility. In this work we present a novel framework that enables integration of schemes from both worlds of prefetching without the need for a major modification to the algorithms. When there is a need/possibility to capture new application level context, a new algorithm could be developed to do so and then it can be integrated into the framework. Since each participating scheme is merely viewed as an algorithm that produces a list of candidate objects that are likely to be accessed in the near future, the framework can entertain any one of the existing prefetching schemes. With its adaptive weight management technique the framework adjusts the effect of each algorithm in the overall prediction to parallel with its observed performance so far. We have found this formwork to be less aggressive than its contemporary counterparts which is extremely important for resource constrained mobile devices that have come to be the major means of access by users of the current web.

**Link**: [arxiv](http://arxiv.org/abs/2509.13604v1),  [pdf](http://arxiv.org/pdf/2509.13604v1)

**Tags**: cs.NI 



### Bridging Cache-Friendliness and Concurrency: A Locality-Optimized   In-Memory B-Skiplist
**Authors**: Yicong Luo, Senhe Hao, Brian Wheatman, Prashant Pandey, Helen Xu

**Updated**: 2025-09-16T23:56:55Z

**Summary**: Skiplists are widely used for in-memory indexing in many key-value stores, such as RocksDB and LevelDB, due to their ease of implementation and simple concurrency control mechanisms. However, traditional skiplists suffer from poor cache locality, as they store only a single element per node, leaving performance on the table. Minimizing last-level cache misses is key to maximizing in-memory index performance, making high cache locality essential. In this paper, we present a practical concurrent B-skiplist that enhances cache locality and performance while preserving the simplicity of traditional skiplist structures and concurrency control schemes. Our key contributions include a top-down, single-pass insertion algorithm for B-skiplists and a corresponding simple and efficient top-down concurrency control scheme. On 128 threads, the proposed concurrent B-skiplist achieves between 2x-9x higher throughput compared to state-of-the-art concurrent skiplist implementations, including Facebook's concurrent skiplist from Folly and the Java ConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves competitive (0.9x-1.7x) throughput on point workloads compared to state-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a more complete picture of the performance, we also measure the latency of skiplist and tree-based indices and find that the B-skiplist achieves between 3.5x-103x lower 99% latency compared to other concurrent skiplists and between 0.85x-64x lower 99% latency compared to tree-based indices on point workloads with inserts.

**Link**: [arxiv](http://arxiv.org/abs/2507.21492v3),  [pdf](http://arxiv.org/pdf/2507.21492v3)

**Tags**: cs.DC 



### FIER: Fine-Grained and Efficient KV Cache Retrieval for Long-context LLM   Inference
**Authors**: Dongwei Wang, Zijie Liu, Song Wang, Yuxin Ren, Jianing Deng, Jingtong Hu, Tianlong Chen, Huanrui Yang

**Updated**: 2025-09-16T23:15:44Z

**Summary**: The Key-Value (KV) cache reading latency increases significantly with context lengths, hindering the efficiency of long-context LLM inference. To address this, previous works propose retaining a small fraction of KV cache based on token importance. For example, KV eviction uses static heuristics to retain tokens, while KV retrieval dynamically selects query-relevant tokens for more adaptive cache management. However, we observe that important tokens are often sparsely distributed across the long context. This sparsity makes existing page-level KV retrieval inaccurate, as each page may include irrelevant tokens and miss critical ones. In this work, we propose Fier, a \underline{Fi}ne-Grained and \underline{E}fficient KV cache \underline{R}etrieval method. Fier uses 1-bit quantized keys to estimate the importance of each token, resulting in efficient and precise retrieval. Experiments show that Fier matches full KV performance using only 11\% of the cache budget across various long-context tasks, reducing decoding latency by 1.2$\times$ to 1.5$\times$.Code is available at https://github.com/SimWangArizona/FIER

**Link**: [arxiv](http://arxiv.org/abs/2508.08256v2),  [pdf](http://arxiv.org/pdf/2508.08256v2)

**Tags**: cs.DB 



### InferLog: Accelerating LLM Inference for Online Log Parsing via   ICL-oriented Prefix Caching
**Authors**: Yilun Wang, Pengfei Chen, Haiyu Huang, Zilong He, Gou Tan, Chuanfu Zhang, Jingkai He, Zibin Zheng

**Updated**: 2025-09-16T10:33:29Z

**Summary**: Modern software systems generate massive volumes of runtime logs, necessitating efficient and accurate log parsing to enable critical downstream tasks such as anomaly detection and root cause analysis. Recently, large language models (LLMs) have achieved advanced accuracy on log parsing, but their deployment in production environments faces two major limitations: (1) the privacy risks associated with commercial LLMs, driving the adoption of local deployment, and (2) the stringent latency and throughput requirements imposed by high-volume log streams, which existing LLM-based parsers fail to meet. Although recent efforts have reduced the number of LLM queries, they overlook the high latency of the LLM invocations, where concurrent log parsing requests can cause serve performance degradation of LLM inference system.   In this study, we present InferLog, the first LLM inference optimization method for online log parsing. Our key insight is that the inference efficiency emerges as the vital bottleneck in LLM-based online log parsing, rather than parsing accuracy. InferLog accelerates inference by designing (1) A Prefix-aware ICL Refinement policy to refine the examples and permutation of in-context learning to improve the prefix caching efficiency. (2) A rapid and task-specific configuration tuning pipeline based on meta-learning to find the optimal LLM scheduling-related configuration for dynamic log parsing workloads. The experimental results based on Loghub dataset and vLLM demonstrate that InferLog significantly outperforms existing inference optimization methods and markedly accelerates the state-of-the-art LLM-based log parser without compromising parsing accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2507.08523v3),  [pdf](http://arxiv.org/pdf/2507.08523v3)

**Tags**: cs.SE 



### Topology and Fragility of European High-Voltage Networks: A   Cross-Country Comparative Analysis
**Authors**: Bálint Hartmann, Michelle T. Cirunay

**Updated**: 2025-09-16T09:54:58Z

**Summary**: Reliable electricity supply depends on the seamless operation of high-voltage grid infrastructure spanning both transmission and sub-transmission levels. Beneath this apparent uniformity lies a striking structural diversity, which leaves a clear imprint on system vulnerability. In this paper, we present harmonized topological models of the high-voltage grids of 15 European countries, integrating all elements at voltage levels above 110 kV. Topological analysis of these networks reveals a simple yet robust pattern: node degree distributions consistently follow an exponential decay, but the rate of decay varies significantly across countries. Through a detailed and systematic evaluation of network tolerance to node and edge removals, we show that the decay rate delineates the boundary between systems that are more resilient to failures and those that are prone to large-scale disruptions. Furthermore, we demonstrate that this numerical boundary is highly sensitive to which layers of the infrastructure are included in the models. To our knowledge, this study provides the first quantitative cross-country comparison of 15 European high-voltage networks, linking topological properties with vulnerability characteristics.

**Link**: [arxiv](http://arxiv.org/abs/2509.12900v1),  [pdf](http://arxiv.org/pdf/2509.12900v1)

**Tags**: eess.SY cs.SI cs.SY 



### Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use
**Authors**: Yabo Zhang, Yihan Zeng, Qingyun Li, Zhen Hu, Kavin Han, Wangmeng Zuo

**Updated**: 2025-09-16T09:22:21Z

**Summary**: Large language models (LLMs) have demonstrated strong capabilities in language understanding and reasoning, yet they remain limited when tackling real-world tasks that require up-to-date knowledge, precise operations, or specialized tool use. To address this, we propose Tool-R1, a reinforcement learning framework that enables LLMs to perform general, compositional, and multi-step tool use by generating executable Python code. Tool-R1 supports integration of user-defined tools and standard libraries, with variable sharing across steps to construct coherent workflows. An outcome-based reward function, combining LLM-based answer judgment and code execution success, guides policy optimization. To improve training efficiency, we maintain a dynamic sample queue to cache and reuse high-quality trajectories, reducing the overhead of costly online sampling. Experiments on the GAIA benchmark show that Tool-R1 substantially improves both accuracy and robustness, achieving about 10\% gain over strong baselines, with larger improvements on complex multi-step tasks. These results highlight the potential of Tool-R1 for enabling reliable and efficient tool-augmented reasoning in real-world applications. Our code will be available at https://github.com/YBYBZhang/Tool-R1.

**Link**: [arxiv](http://arxiv.org/abs/2509.12867v1),  [pdf](http://arxiv.org/pdf/2509.12867v1)

**Tags**: cs.LG cs.CV 



### Fine-Grained AI Model Caching and Downloading With Coordinated   Multipoint Broadcasting in Multi-Cell Edge Networks
**Authors**: Yang Fu, Peng Qin, Yueyue Zhang, Yifei Wang

**Updated**: 2025-09-16T09:14:15Z

**Summary**: 6G networks are envisioned to support on-demand AI model downloading to accommodate diverse inference requirements of end users. By proactively caching models at edge nodes, users can retrieve the requested models with low latency for on-device AI inference. However, the substantial size of contemporary AI models poses significant challenges for edge caching under limited storage capacity, as well as for the concurrent delivery of heterogeneous models over wireless channels. To address these challenges, we propose a fine-grained AI model caching and downloading system that exploits parameter reusability, stemming from the common practice of fine-tuning task-specific models from a shared pre-trained model with frozen parameters. This system selectively caches model parameter blocks (PBs) at edge nodes, eliminating redundant storage of reusable parameters across different cached models. Additionally, it incorporates coordinated multipoint (CoMP) broadcasting to simultaneously deliver reusable PBs to multiple users, thereby enhancing downlink spectrum utilization. Under this arrangement, we formulate a model downloading delay minimization problem to jointly optimize PB caching, migration (among edge nodes), and broadcasting beamforming. To tackle this intractable problem, we develop a distributed multi-agent learning framework that enables edge nodes to explicitly learn mutual influence among their actions, thereby facilitating cooperation. Furthermore, a data augmentation approach is proposed to adaptively generate synthetic training samples through a predictive model, boosting sample efficiency and accelerating policy learning. Both theoretical analysis and simulation experiments validate the superior convergence performance of the proposed learning framework.

**Link**: [arxiv](http://arxiv.org/abs/2509.19341v1),  [pdf](http://arxiv.org/pdf/2509.19341v1)

**Tags**: cs.NI cs.AI cs.LG 



### SAGA: Selective Adaptive Gating for Efficient and Expressive Linear   Attention
**Authors**: Yuan Cao, Dong Wang

**Updated**: 2025-09-16T08:36:05Z

**Summary**: While Transformer architecture excel at modeling long-range dependencies contributing to its widespread adoption in vision tasks the quadratic complexity of softmax-based attention mechanisms imposes a major bottleneck, particularly when processing high-resolution images. Linear attention presents a promising alternative by reformulating the attention computation from $(QK)V$ to $Q(KV)$, thereby reducing the complexity from $\mathcal{O}(N^2)$ to $\mathcal{O}(N)$ while preserving the global receptive field. However, most existing methods compress historical key-value (KV) information uniformly, which can lead to feature redundancy and the loss of directional alignment with the query (Q). This uniform compression results in low-rank $KV$ feature maps, contributing to a performance gap compared to softmax attention. To mitigate this limitation, we propose \textbf{S}elective \textbf{A}daptive \textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which introduces input-adaptive learnable gates to selectively modulate information aggregation into the $KV$ feature map. These gates enhance semantic diversity and alleviate the low-rank constraint inherent in conventional linear attention. Additionally, we propose an efficient Hadamard-product decomposition method for gate computation, which introduces no additional memory overhead. Experiments demonstrate that SAGA achieves a 1.76$\times$ improvement in throughput and a 2.69$\times$ reduction in peak GPU memory compared to PVT-T at a resolution of $1280 \times 1280$. Moreover, it improves top-1 accuracy by up to 4.4\% on the ImageNet dataset, demonstrating both computational efficiency and model effectiveness.

**Link**: [arxiv](http://arxiv.org/abs/2509.12817v1),  [pdf](http://arxiv.org/pdf/2509.12817v1)

**Tags**: cs.CV 



### Adaptive K-PackCache: Cost-Centric Data Caching in Cloud
**Authors**: Suvarthi Sarkar, Aadarshraj Sah, Poddutoori Sweeya Reddy, Aryabartta Sahu

**Updated**: 2025-09-16T07:49:41Z

**Summary**: Recent advances in data analytics have enabled the accurate prediction of user access patterns, giving rise to the idea of packed caching delivering multiple co accessed data items together as a bundle. This improves caching efficiency, as accessing one item often implies the need for others. Prior work has explored only 2 item pairwise packing. In this paper, we extend the concept to general K packing, allowing variable size bundles for improved flexibility and performance. We formulate the K PackCache problem from a content delivery network CDN operator perspective, aiming to minimize total cost comprising two components: transfer cost modeled as a base cost plus a linearly increasing term with the number of items packed, and memory rental cost for caching, which depends on how long and how much is stored. Overpacking increases cost due to low utility, underpacking leads to missed sharing opportunities. We propose an online algorithm, Adaptive K PackCache AKPC, which dynamically forms, merges, and splits data cliques based on user access patterns and content correlation. Our approach supports batch requests, enables approximate clique merging, and offers a formal competitive guarantee. Through extensive evaluation on the Netflix and Spotify datasets, AKPC reduces total cost by up to 63 and 55 percentage over online baselines, respectively, and achieves performance within 15 and 13 percentage of the optimal. This demonstrates its scalability and effectiveness for real world caching systems.

**Link**: [arxiv](http://arxiv.org/abs/2509.11156v2),  [pdf](http://arxiv.org/pdf/2509.11156v2)

**Tags**: cs.DC 



### Accelerating LLM Inference via Dynamic KV Cache Placement in   Heterogeneous Memory System
**Authors**: Yunhua Fang, Rui Xie, Asad Ul Haq, Linsen Ma, Kaoutar El Maghraoui, Naigang Wang, Meng Wang, Liu Liu, Tong Zhang

**Updated**: 2025-09-15T14:40:16Z

**Summary**: Large Language Model (LLM) inference is increasingly constrained by memory bandwidth, with frequent access to the key-value (KV) cache dominating data movement. While attention sparsity reduces some memory traffic, the relevance of past tokens varies over time, requiring the full KV cache to remain accessible and sustaining pressure on both bandwidth and capacity. With advances in interconnects such as NVLink and LPDDR5X, modern AI hardware now integrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making heterogeneous memory systems a practical solution. This work investigates dynamic KV cache placement across such systems to maximize aggregated bandwidth utilization under capacity constraints. Rather than proposing a specific scheduling policy, we formulate the placement problem mathematically and derive a theoretical upper bound, revealing substantial headroom for runtime optimization. To our knowledge, this is the first formal treatment of dynamic KV cache scheduling in heterogeneous memory systems for LLM inference.

**Link**: [arxiv](http://arxiv.org/abs/2508.13231v2),  [pdf](http://arxiv.org/pdf/2508.13231v2)

**Tags**: cs.AR cs.AI cs.PF 



### SpeCa: Accelerating Diffusion Transformers with Speculative Feature   Caching
**Authors**: Jiacheng Liu, Chang Zou, Yuanhuiyi Lyu, Fei Ren, Shaobo Wang, Kaixin Li, Linfeng Zhang

**Updated**: 2025-09-15T06:46:22Z

**Summary**: Diffusion models have revolutionized high-fidelity image and video synthesis, yet their computational demands remain prohibitive for real-time applications. These models face two fundamental challenges: strict temporal dependencies preventing parallelization, and computationally intensive forward passes required at each denoising step. Drawing inspiration from speculative decoding in large language models, we present SpeCa, a novel 'Forecast-then-verify' acceleration framework that effectively addresses both limitations. SpeCa's core innovation lies in introducing Speculative Sampling to diffusion models, predicting intermediate features for subsequent timesteps based on fully computed reference timesteps. Our approach implements a parameter-free verification mechanism that efficiently evaluates prediction reliability, enabling real-time decisions to accept or reject each prediction while incurring negligible computational overhead. Furthermore, SpeCa introduces sample-adaptive computation allocation that dynamically modulates resources based on generation complexity, allocating reduced computation for simpler samples while preserving intensive processing for complex instances. Experiments demonstrate 6.34x acceleration on FLUX with minimal quality degradation (5.5% drop), 7.3x speedup on DiT while preserving generation fidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The verification mechanism incurs minimal overhead (1.67%-3.5% of full inference costs), establishing a new paradigm for efficient diffusion model inference while maintaining generation quality even at aggressive acceleration ratios. Our codes have been released in Github: \textbf{https://github.com/Shenyi-Z/Cache4Diffusion}

**Link**: [arxiv](http://arxiv.org/abs/2509.11628v1),  [pdf](http://arxiv.org/pdf/2509.11628v1)

**Tags**: cs.LG cs.AI cs.CV 



### LogicTree: Structured Proof Exploration for Coherent and Rigorous   Logical Reasoning with Large Language Models
**Authors**: Kang He, Kaushik Roy

**Updated**: 2025-09-15T01:15:50Z

**Summary**: Large language models (LLMs) have achieved remarkable multi-step reasoning capabilities across various domains. However, LLMs still face distinct challenges in complex logical reasoning, as (1) proof-finding requires systematic exploration and the maintenance of logical coherence and (2) searching the right combination of premises at each reasoning step is inherently challenging in tasks with large premise space. To address this, we propose LogicTree, an inference-time modular framework employing algorithm-guided search to automate structured proof exploration and ensure logical coherence. Advancing beyond tree-of-thought (ToT), we incorporate caching mechanism into LogicTree to enable effective utilization of historical knowledge, preventing reasoning stagnation and minimizing redundancy. Furthermore, we address the combinatorial complexity of premise search by decomposing it into a linear process. The refined premise selection restricts subsequent inference to at most one derivation per step, enhancing reasoning granularity and enforcing strict step-by-step reasoning. Additionally, we introduce two LLM-free heuristics for premise prioritization, enabling strategic proof search. Experimental results on five datasets demonstrate that LogicTree optimally scales inference-time computation to achieve higher proof accuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6% and 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o outperforms o3-mini by 7.6% on average.

**Link**: [arxiv](http://arxiv.org/abs/2504.14089v2),  [pdf](http://arxiv.org/pdf/2504.14089v2)

**Tags**: cs.CL cs.AI cs.LG 



### FineServe: Precision-Aware KV Slab and Two-Level Scheduling for   Heterogeneous Precision LLM Serving
**Authors**: Kyungmin Bin, Seungbeom Choi, Jimyoung Son, Jieun Choi, Daseul Bae, Daehyeon Baek, Kihyo Moon, Minsung Jang, Hyojung Lee

**Updated**: 2025-09-15T00:51:47Z

**Summary**: Recent advances in Post-Training Quantization (PTQ) techniques have significantly increased demand for serving quantized large language models (LLMs), enabling higher throughput and substantially reduced memory usage with minimal accuracy loss. Quantized models address memory constraints in LLMs and enhance GPU resource utilization through efficient GPU sharing. However, quantized models have smaller KV block sizes than non-quantized models, causing limited memory efficiency due to memory fragmentation. Also, distinct resource usage patterns between quantized and non-quantized models require efficient scheduling to maximize throughput. To address these challenges, we propose FineServe, an inference serving framework for mixed-precision LLMs. FineServe's key contributions include: (1) KV Slab, a precision-aware adaptive memory management technique dynamically allocating KV cache based on model quantization characteristics, significantly reducing GPU memory fragmentation, and (2) a two-level scheduling framework comprising a global scheduler that places models to GPUs based on request rates, latency SLOs, and memory constraints and efficiency, and a local scheduler that adaptively adjusts batch sizes according to real-time request fluctuations. Experimental results demonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x higher token generation throughput compared to the state-of-the-art GPU sharing systems.

**Link**: [arxiv](http://arxiv.org/abs/2509.06261v2),  [pdf](http://arxiv.org/pdf/2509.06261v2)

**Tags**: cs.DC cs.LG 



### Multi-Layer Perceptron-Based Relay Node Selection for Next-Generation   Intelligent Delay-Tolerant Networks
**Authors**: Zhekun Huang, Milena Radenkovic

**Updated**: 2025-09-14T12:29:49Z

**Summary**: Delay Tolerant Networks (DTNs) are critical for emergency communication in highly dynamic and challenging scenarios characterized by intermittent connectivity, frequent disruptions, and unpredictable node mobility. While some protocols are widely adopted for simplicity and low overhead, their static replication strategy lacks the ability to adaptively distinguish high-quality relay nodes, often leading to inefficient and suboptimal message dissemination. To address this challenge, we propose a novel intelligent routing enhancement that integrates machine learning-based node evaluation into the Spray and Wait framework. Several dynamic, core features are extracted from simulation logs and are used to train multiple classifiers - Multi-Layer Perceptron (MLP), Support Vector Machine (SVM), and Random Forest (RF) - to predict whether a node is suitable as a relay under dynamic conditions. The trained models are deployed via a lightweight Flask-based RESTful API, enabling real-time, adaptive predictions. We implement the enhanced router MLPBasedSprayRouter, which selectively forwards messages based on the predicted relay quality. A caching mechanism is incorporated to reduce computational overhead and ensure stable, low-latency inference. Extensive experiments under realistic emergency mobility scenarios demonstrate that the proposed framework significantly improves delivery ratio while reducing average latency compared to the baseline protocols. Among all evaluated classifiers, MLP achieved the most robust performance, consistently outperforming both SVM and RF in terms of accuracy, adaptability, and inference speed. These results confirm the novelty and practicality of integrating machine learning into DTN routing, paving the way for resilient and intelligent communication systems in smart cities, disaster recovery, and other dynamic environments.

**Link**: [arxiv](http://arxiv.org/abs/2509.11239v1),  [pdf](http://arxiv.org/pdf/2509.11239v1)

**Tags**: cs.NI 



### Dislocation response to electric fields in strontium titanate: A   mesoscale indentation study
**Authors**: Alexander Frisch, Daniel Isaia, Oliver Preuß, Xufei Fang

**Updated**: 2025-09-14T09:26:44Z

**Summary**: Dislocations in perovskite oxides have drawn increasing research interest due to their potential of tuning functional properties of electroceramics. Open questions remain regarding the behavior of dislocations concerning their stability under strong externally applied electric fields. In this study, we investigate the dielectric breakdown strength of nominally undoped SrTiO3 crystals after the introduction of high-density dislocations. The dislocation-rich samples are prepared using the Brinell scratching method, and they consistently exhibit lower dielectric breakdown strength as well as a larger scatter in the breakdown probability. We also study the impact of electric field on the introduction and movement of dislocations in SrTiO3 crystals using Brinell indentation coupled with an electric field of 2 kV/mm. No changes on the dislocation plastic zone size, depth, and dislocation distribution are observed under this electric field. Based on the charge state of the dislocations in SrTiO3 as well as the electrical and thermal conductivity modified by dislocations, we discuss the forces induced by the electric field to act on the dislocations to underline the possible mechanisms for such dislocation behavior.

**Link**: [arxiv](http://arxiv.org/abs/2509.11181v1),  [pdf](http://arxiv.org/pdf/2509.11181v1)

**Tags**: cond-mat.mtrl-sci 



### AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient   Inference in LLMs
**Authors**: Santhosh G S, Saurav Prakash, Balaraman Ravindran

**Updated**: 2025-09-14T08:20:48Z

**Summary**: The quadratic complexity of the attention mechanism remains a fundamental barrier to scaling Large Language Models (LLMs) to longer contexts, creating a critical bottleneck in both computation and memory. To address this, we introduce AQUA (Attention via QUery mAgnitudes) a novel and versatile approximation strategy that significantly reduces the cost of attention with a graceful performance trade-off. Our method operates in two phases: an efficient offline step where we compute a universal, language agnostic projection matrix via SVD on a calibration dataset, and an online inference step where we project query and key vectors and dynamically select a sparse subset of dimensions based on the query's magnitude. We provide a formal theoretical analysis of AQUA, establishing the break-even point at which it becomes more computationally efficient than standard attention. Our empirical evaluations on state-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in the attention dot-product computation can be achieved with a statistically insignificant impact on performance across a wide range of benchmarks. We further showcase the versatility of AQUA by demonstrating its ability to synergistically accelerate existing token eviction methods like H2O and to directly reduce KV-cache memory size. By offering a controllable knob to balance efficiency and accuracy, AQUA provides a practical and powerful tool for making large-scale LLM inference more accessible and sustainable.

**Link**: [arxiv](http://arxiv.org/abs/2509.11155v1),  [pdf](http://arxiv.org/pdf/2509.11155v1)

**Tags**: cs.LG cs.AI cs.CL 



### Judge Q: Trainable Queries for Optimized Information Retention in KV   Cache Eviction
**Authors**: Yijun Liu, Yixuan Wang, Yuzhuang Xu, Shiyu Ji, Yang Xu, Qingfu Zhu, Wanxiang Che

**Updated**: 2025-09-13T03:34:12Z

**Summary**: Large language models (LLMs) utilize key-value (KV) cache to store historical information during sequence processing. The size of KV cache grows linearly as the length of the sequence extends, which seriously affects memory usage and decoding efficiency. Current methods for KV cache eviction typically utilize the last window from the pre-filling phase as queries to compute the KV importance scores for eviction. Although this scheme is simple to implement, it tends to overly focus on local information, potentially leading to the neglect or omission of crucial global information. To mitigate this issue, we propose Judge Q, a novel training method which incorporates a soft token list. This method only tunes the model's embedding layer at a low training cost. By concatenating the soft token list at the end of the input sequence, we train these tokens' attention map to the original input sequence to align with that of the actual decoded tokens. In this way, the queries corresponding to the soft tokens can effectively capture global information and better evaluate the importance of the keys and values within the KV cache, thus maintaining decoding quality when KV cache is evicted. Under the same eviction budget, our method exhibits less performance degradation compared to existing eviction approaches. We validate our approach through experiments conducted on models such as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks including LongBench, RULER, and Needle-in-a-Haystack. Results indicate an improvement of approximately 1 point on the LongBench and over 3 points on RULER. This proposed methodology can be seamlessly integrated into existing open-source models with minimal training overhead, thereby enhancing performance in KV cache eviction scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2509.10798v1),  [pdf](http://arxiv.org/pdf/2509.10798v1)

**Tags**: cs.CL cs.AI 



### MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging   Bit-Slice-enabled Sparsity and Repetitiveness
**Authors**: Huizheng Wang, Zichuan Wang, Zhiheng Yue, Yousheng Long, Taiquan Wei, Jianxun Yang, Yang Wang, Chao Li, Shaojun Wei, Yang Hu, Shouyi Yin

**Updated**: 2025-09-12T16:05:27Z

**Summary**: Large language models (LLMs) face significant inference latency due to inefficiencies in GEMM operations, weight access, and KV cache access, especially in real-time scenarios. This highlights the need for a versatile compute-memory efficient accelerator. Unfortunately, existing Transformer accelerators struggle to address both aspects simultaneously, as they focus on value-level processing, missing fine-grained opportunities to optimize computation and memory collaboratively. This paper introduces MCBP, a bit-grained compute-memory efficient algorithm-hardware co-design that leverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM inference. MCBP features three key innovations: 1) BS-repetitiveness-enabled computation reduction (BRCR), which eliminates redundant GEMM computations via leveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state coding (BSTC), which reduces weight access via exploiting significant sparsity in high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP), which reduces KV cache access by leveraging early-termination-based bit-grained prediction. These techniques, supported by custom accelerator designs, effectively alleviate the burden in GEMM, weight access, and KV cache access. Extensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up and 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA Transformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than Spatten, FACT and SOFA, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2509.10372v1),  [pdf](http://arxiv.org/pdf/2509.10372v1)

**Tags**: cs.AR 



### Compute Only 16 Tokens in One Timestep: Accelerating Diffusion   Transformers with Cluster-Driven Feature Caching
**Authors**: Zhixin Zheng, Xinyu Wang, Chang Zou, Shaobo Wang, Linfeng Zhang

**Updated**: 2025-09-12T14:53:45Z

**Summary**: Diffusion transformers have gained significant attention in recent years for their ability to generate high-quality images and videos, yet still suffer from a huge computational cost due to their iterative denoising process. Recently, feature caching has been introduced to accelerate diffusion transformers by caching the feature computation in previous timesteps and reusing it in the following timesteps, which leverage the temporal similarity of diffusion models while ignoring the similarity in the spatial dimension. In this paper, we introduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and complementary perspective for previous feature caching. Specifically, ClusCa performs spatial clustering on tokens in each timestep, computes only one token in each cluster and propagates their information to all the other tokens, which is able to reduce the number of tokens by over 90%. Extensive experiments on DiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image and text-to-video generation. Besides, it can be directly applied to any diffusion transformer without requirements for training. For instance, ClusCa achieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing the original model by 0.51%. The code is available at https://github.com/Shenyi-Z/Cache4Diffusion.

**Link**: [arxiv](http://arxiv.org/abs/2509.10312v1),  [pdf](http://arxiv.org/pdf/2509.10312v1)

**Tags**: cs.CV 



### XBOF: A Cost-Efficient CXL JBOF with Inter-SSD Compute Resource Sharing
**Authors**: Shushu Yi, Yuda An, Li Peng, Xiurui Pan, Qiao Li, Jieming Yin, Guangyan Zhang, Wenfei Wu, Diyu Zhou, Zhenlin Wang, Xiaolin Wang, Yingwei Luo, Ke Zhou, Jie Zhang

**Updated**: 2025-09-12T13:49:27Z

**Summary**: Enterprise SSDs integrate numerous computing resources (e.g., ARM processor and onboard DRAM) to satisfy the ever-increasing performance requirements of I/O bursts. While these resources substantially elevate the monetary costs of SSDs, the sporadic nature of I/O bursts causes severe SSD resource underutilization in just a bunch of flash (JBOF) level. Tackling this challenge, we propose XBOF, a cost-efficient JBOF design, which only reserves moderate computing resources in SSDs at low monetary cost, while achieving demanded I/O performance through efficient inter-SSD resource sharing. Specifically, XBOF first disaggregates SSD architecture into multiple disjoint parts based on their functionality, enabling fine-grained SSD internal resource management. XBOF then employs a decentralized scheme to manage these disaggregated resources and harvests the computing resources of idle SSDs to assist busy SSDs in handling I/O bursts. This idea is facilitated by the cache-coherent capability of Compute eXpress Link (CXL), with which the busy SSDs can directly utilize the harvested computing resources to accelerate metadata processing. The evaluation results show that XBOF improves SSD resource utilization by 50.4% and saves 19.0% monetary costs with a negligible performance loss, compared to existing JBOF designs.

**Link**: [arxiv](http://arxiv.org/abs/2509.10251v1),  [pdf](http://arxiv.org/pdf/2509.10251v1)

**Tags**: cs.OS 



### SvalMIZ-25 Svalbard Marginal Ice Zone Campaign 2025 -- Cruise Report
**Authors**: M. Müller, J. Rabault, C. Palerme, J. Tjernström

**Updated**: 2025-09-12T07:20:53Z

**Summary**: The coupling of weather, sea-ice, ocean, and wave forecasting systems has been a long-standing research focus to improve Arctic forecasting systems and their realism and is also a priority of international initiatives such as the WMO research project PCAPS. The goal of the Svalbard Marginal Ice Zone 2025 Campaign (SvalMIZ-25) was to observe and better understand the complex interplay between atmosphere, waves, and sea-ice in the winter Marginal Ice Zone (MIZ) in order to advance predictive skill of coupled Arctic forecasting systems. The main objective has been to set up a network of observations with a spatial distribution that allows for a representative comparison between in situ observations and gridded model data. The observed variables include air and surface temperature, sea-ice drift, and wave energy spectra. With the support of the Norwegian Coast Guard, we participated in the research cruise with KV Svalbard from 22.April - 11.May 2025. In total 21 buoys were deployed in the Marginal Ice Zone north of the Svalbard Archipelago.

**Link**: [arxiv](http://arxiv.org/abs/2509.10016v1),  [pdf](http://arxiv.org/pdf/2509.10016v1)

**Tags**: physics.ao-ph 



### LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation
**Authors**: Yiqun Shen, Song Yuan, Zhengze Zhang, Xiaoliang Wang, Daxin Jiang, Nguyen Cam-Tu

**Updated**: 2025-09-11T16:48:24Z

**Summary**: KV Cache is commonly used to accelerate LLM inference with long contexts, yet its high memory demand drives the need for cache compression. Existing compression methods, however, are largely heuristic and lack dynamic budget allocation. To address this limitation, we introduce a unified framework for cache compression by minimizing information loss in Transformer residual streams. Building on it, we analyze the layer attention output loss and derive a new metric to compare cache entries across heads, enabling layer-wise compression with dynamic head budgets. Additionally, by contrasting cross-layer information, we also achieve dynamic layer budgets. LAVa is the first unified strategy for cache eviction and dynamic budget allocation that, unlike prior methods, does not rely on training or the combination of multiple strategies. Experiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and InfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a new insight: dynamic layer budgets are crucial for generation tasks (e.g., code completion), while dynamic head budgets play a key role in extraction tasks (e.g., extractive QA). As a fully dynamic compression method, LAVa consistently maintains top performance across task types. Our code is available at https://github.com/MGDDestiny/Lava.

**Link**: [arxiv](http://arxiv.org/abs/2509.09754v1),  [pdf](http://arxiv.org/pdf/2509.09754v1)

**Tags**: cs.LG cs.AI 



### TrEnv: Transparently Share Serverless Execution Environments Across   Different Functions and Nodes
**Authors**: Jialiang Huang, Teng Ma, Zheng Liu, Sixing Lin, Kang Chen, Jinlei Jiang, Xia Liao, Yingdi Shan, Yongwei Wu, Ning Zhang, Mengting Lu, Tao Ma, Haifeng Gong, Mingxing Zhang

**Updated**: 2025-09-11T15:06:03Z

**Summary**: Serverless computing provides dynamic scalability, but its infrastructure overhead becomes a bottleneck for emerging workloads such as LLM agents, which exhibit unpredictable invocation patterns and variable resource demands. Our analysis shows that for these agents, the cost of running on serverless platforms can reach up to 70% of the cost of LLM API calls. This finding motivates the need for a more efficient, high-density serverless platform. We present TrEnv, a co-designed serverless platform that supports both container- and VM-based environments, optimized for the unique demands of LLM agents. TrEnv reduces startup latency and memory usage through repurposable sandboxes and memory templates, which enable fast reuse and restoration of execution environments. To further reduce overhead in VM-based agent workloads, TrEnv leverages browser sharing and a page cache bypassing mechanism. Evaluations show that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in container-based settings, and achieves up to 58% lower P99 latency and 61% memory savings for VM-based agents compared to state-of-the-art systems like E2B.

**Link**: [arxiv](http://arxiv.org/abs/2509.09525v1),  [pdf](http://arxiv.org/pdf/2509.09525v1)

**Tags**: cs.DC cs.OS 



### In-Loop Filtering Using Learned Look-Up Tables for Video Coding
**Authors**: Zhuoyuan Li, Jiacheng Li, Yao Li, Jialin Li, Li Li, Dong Liu, Feng Wu

**Updated**: 2025-09-11T14:34:01Z

**Summary**: In-loop filtering (ILF) is a key technology in video coding standards to reduce artifacts and enhance visual quality. Recently, neural network-based ILF schemes have achieved remarkable coding gains, emerging as a powerful candidate for next-generation video coding standards. However, the use of deep neural networks (DNN) brings significant computational and time complexity or high demands for dedicated hardware, making it challenging for general use. To address this limitation, we study a practical ILF solution by adopting look-up tables (LUTs). After training a DNN with a restricted reference range for ILF, all possible inputs are traversed, and the output values of the DNN are cached into LUTs. During the coding process, the filtering process is performed by simply retrieving the filtered pixel through locating the input pixels and interpolating between the cached values, instead of relying on heavy inference computations. In this paper, we propose a universal LUT-based ILF framework, termed LUT-ILF++. First, we introduce the cooperation of multiple kinds of filtering LUTs and propose a series of customized indexing mechanisms to enable better filtering reference perception with limited storage consumption. Second, we propose the cross-component indexing mechanism to enable the filtering of different color components jointly. Third, in order to make our solution practical for coding uses, we propose the LUT compaction scheme to enable the LUT pruning, achieving a lower storage cost of the entire solution. The proposed framework is implemented in the VVC reference software. Experimental results show that the proposed framework achieves on average 0.82%/2.97%/1.63% and 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI and RA configurations, respectively. Compared to DNN-based solutions, our proposed solution has much lower time complexity and storage cost.

**Link**: [arxiv](http://arxiv.org/abs/2509.09494v1),  [pdf](http://arxiv.org/pdf/2509.09494v1)

**Tags**: eess.IV cs.CV cs.MM 



### VFlowOpt: A Token Pruning Framework for LMMs with Visual Information   Flow-Guided Optimization
**Authors**: Sihan Yang, Runsen Xu, Chenhang Cui, Tai Wang, Dahua Lin, Jiangmiao Pang

**Updated**: 2025-09-11T12:06:49Z

**Summary**: Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging numerous visual tokens for fine-grained visual information, but this token redundancy results in significant computational costs. Previous research aimed at reducing visual tokens during inference typically leverages importance maps derived from attention scores among vision-only tokens or vision-language tokens to prune tokens across one or multiple pruning stages. Despite this progress, pruning frameworks and strategies remain simplistic and insufficiently explored, often resulting in substantial performance degradation. In this paper, we propose VFlowOpt, a token pruning framework that introduces an importance map derivation process and a progressive pruning module with a recycling mechanism. The hyperparameters of its pruning strategy are further optimized by a visual information flow-guided method. Specifically, we compute an importance map for image tokens based on their attention-derived context relevance and patch-level information entropy. We then decide which tokens to retain or prune and aggregate the pruned ones as recycled tokens to avoid potential information loss. Finally, we apply a visual information flow-guided method that regards the last token in the LMM as the most representative signal of text-visual interactions. This method minimizes the discrepancy between token representations in LMMs with and without pruning, thereby enabling superior pruning strategies tailored to different LMMs. Experiments demonstrate that VFlowOpt can prune 90% of visual tokens while maintaining comparable performance, leading to an 89% reduction in KV-Cache memory and 3.8 times faster inference.

**Link**: [arxiv](http://arxiv.org/abs/2508.05211v2),  [pdf](http://arxiv.org/pdf/2508.05211v2)

**Tags**: cs.CV 



### Universal Workers: A Vision for Eliminating Cold Starts in Serverless   Computing
**Authors**: Saman Akbari, Manfred Hauswirth

**Updated**: 2025-09-11T10:20:20Z

**Summary**: Serverless computing enables developers to deploy code without managing infrastructure, but suffers from cold start overhead when initializing new function instances. Existing solutions such as "keep-alive" or "pre-warming" are costly and unreliable under bursty workloads. We propose universal workers, which are computational units capable of executing any function with minimal initialization overhead. Based on an analysis of production workload traces, our key insight is that requests in Function-as-a-Service (FaaS) platforms show a highly skewed distribution, with most requests invoking a small subset of functions. We exploit this observation to approximate universal workers through locality groups and three-tier caching (handler, install, import). With this work, we aim to enable more efficient and scalable FaaS platforms capable of handling diverse workloads with minimal initialization overhead.

**Link**: [arxiv](http://arxiv.org/abs/2505.19880v2),  [pdf](http://arxiv.org/pdf/2505.19880v2)

**Tags**: cs.DC cs.PF 



### Spotlight Attention: Towards Efficient LLM Generation via Non-linear   Hashing-based KV Cache Retrieval
**Authors**: Wenhao Li, Yuxin Zhang, Gen Luo, Haiyuan Wan, Ziyang Gong, Fei Chao, Rongrong Ji

**Updated**: 2025-09-11T06:45:58Z

**Summary**: Reducing the key-value (KV) cache burden in Large Language Models (LLMs) significantly accelerates inference. Dynamically selecting critical KV caches during decoding helps maintain performance. Existing methods use random linear hashing to identify important tokens, but this approach is inefficient due to the orthogonal distribution of queries and keys within two narrow cones in LLMs. We introduce Spotlight Attention, a novel method that employs non-linear hashing functions to optimize the embedding distribution of queries and keys, enhancing coding efficiency and robustness. We also developed a lightweight, stable training framework using a Bradley-Terry ranking-based loss, enabling optimization of the non-linear hashing module on GPUs with 16GB memory in 8 hours. Experimental results show that Spotlight Attention drastically improves retrieval precision while shortening the length of the hash code at least 5$\times$ compared to traditional linear hashing. Finally, we exploit the computational advantages of bitwise operations by implementing specialized CUDA kernels, achieving hashing retrieval for 512K tokens in under 100$\mu$s on a single A100 GPU, with end-to-end throughput up to 3$\times$ higher than vanilla decoding.

**Link**: [arxiv](http://arxiv.org/abs/2508.19740v3),  [pdf](http://arxiv.org/pdf/2508.19740v3)

**Tags**: cs.CL 



### Bidirectional Sparse Attention for Faster Video Diffusion Training
**Authors**: Chenlu Zhan, Wen Li, Chuyu Shen, Jun Zhang, Suhui Wu, Hao Zhang

**Updated**: 2025-09-11T06:16:31Z

**Summary**: Video diffusion Transformer (DiT) models excel in generative quality but hit major computational bottlenecks when producing high-resolution, long-duration videos. The quadratic complexity of full attention leads to prohibitively high training and inference costs. Full attention inefficiency stems from two key challenges: excessive computation due to the inherent sparsity of Queries and Key-Value pairs, and redundant computation as fixed sparse patterns fail to leverage DiT's dynamic attention. To overcome this limitation, we propose a Bidirectional Sparse Attention (BSA) framework for faster video DiT training, the first to dynamically sparsify both Queries and Key-Value pairs within 3D full attention, thereby substantially improving training and inference efficiency. BSA addresses these issues through two key components. Query sparsity is optimized by selecting the most informative query tokens via semantic similarity and with a dynamic spatial-time training strategy, while KV sparsity is achieved by computing a statistical dynamic threshold to retain only the most salient KV blocks for computation. Extensive experiments demonstrate that BSA significantly accelerates DiT training across long sequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention training, while preserving or even surpassing the generative quality of full attention.

**Link**: [arxiv](http://arxiv.org/abs/2509.01085v3),  [pdf](http://arxiv.org/pdf/2509.01085v3)

**Tags**: cs.CV 



### Coherence-Aware Task Graph Modeling for Realistic Application
**Authors**: Guochu Xiong, Xiangzhong Luo, Weichen Liu

**Updated**: 2025-09-11T02:00:27Z

**Summary**: As multicore systems continue to scale, cache coherence has emerged as a critical determinant of system performance, with coherence behavior and task execution closely intertwined, reshaping inter-task dependencies. Task graph modeling provides a structured way to capture such dependencies and serves as the foundation for many system-level design strategies. However, these strategies typically rely on predefined task graphs, while many real-world applications lack explicit graphs and exhibit dynamic, data-dependent behavior, limiting the effectiveness of static approaches. To address this, several task graph modeling methods for realistic workloads have been developed. Yet, they either rely on implicit techniques that use application-specific features without producing explicit graphs, or they generate graphs tailored to fixed scheduling models, which limits generality. More importantly, they often overlook coherence interactions, creating a gap between design assumptions and actual runtime behavior. To overcome these limitations, we propose CoTAM, a Coherence-Aware Task Graph Modeling framework for realistic workloads that constructs a unified task graph reflecting runtime behavior. CoTAM analyzes the impact of coherence by decoupling its effects from overall execution, quantifies its influence through a learned weighting scheme, and infers inter-task dependencies for coherence-aware graph generation. Extensive experiments show that CoTAM outperforms implicit methods, bridging the gap between dynamic workload behavior and existing designs while demonstrating the importance of incorporating cache coherence into task graph modeling for accurate and generalizable system-level analysis.

**Link**: [arxiv](http://arxiv.org/abs/2509.09094v1),  [pdf](http://arxiv.org/pdf/2509.09094v1)

**Tags**: cs.DC 



### TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached   Responses
**Authors**: Muhammad Taha Cheema, Abeer Aamir, Khawaja Gul Muhammad, Naveed Anwar Bhatti, Ihsan Ayyub Qazi, Zafar Ayyub Qazi

**Updated**: 2025-09-10T17:59:08Z

**Summary**: Large Language Models (LLMs) process millions of queries daily, making efficient response caching a compelling optimization for reducing cost and latency. However, preserving relevance to user queries using this approach proves difficult due to the personalized nature of chatbot interactions and the limited accuracy of semantic similarity search. To address this, we present TweakLLM, a novel routing architecture that employs a lightweight LLM to dynamically adapt cached responses to incoming prompts. Through comprehensive evaluation, including user studies with side-by-side comparisons, satisfaction voting, as well as multi-agent LLM debates, we demonstrate that TweakLLM maintains response quality comparable to frontier models while significantly improving cache effectiveness. Our results across real-world datasets highlight TweakLLM as a scalable, resource-efficient caching solution for high-volume LLM deployments without compromising user experience.

**Link**: [arxiv](http://arxiv.org/abs/2507.23674v2),  [pdf](http://arxiv.org/pdf/2507.23674v2)

**Tags**: cs.LG cs.CL 



### Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer   Layer Caching
**Authors**: Siratish Sakpiboonchit

**Updated**: 2025-09-10T15:41:15Z

**Summary**: This paper presents a method to accelerate the inference process of diffusion transformer (DiT)-based text-to-speech (TTS) models by applying a selective caching mechanism to transformer layers. Specifically, I integrate SmoothCache into the F5-TTS architecture, focusing on caching outputs of self-attention and feed-forward network layers to reduce redundant computations during the denoising process. A calibration phase is introduced to analyze L1 relative errors between timesteps, guiding the selection of cache schedules that minimize quality degradation. To address the problem of inter-layer dependency, a unified caching schedule is adopted, applying the cache pattern derived from self-attention layers to both layer types. Experiments on LibriSpeech-PC and Seed-TTS datasets evaluate various cache thresholds and denoising step configurations. Results show that caching at higher denoising steps reduces inference time without compromising output quality, whereas caching at lower steps can negatively impact synthesis quality similarly to reducing the total number of denoising steps. Objective and subjective metrics confirm the effectiveness of SmoothCache in maintaining performance while improving computational efficiency. Comparisons between cached inference and reduced-step inference further highlight the benefits of selective caching, especially under high-step configurations. This work demonstrates that transformer layer caching is a practical solution for optimizing diffusion transformer-based TTS models without requiring architectural changes or retraining. Example inference results can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ .

**Link**: [arxiv](http://arxiv.org/abs/2509.08696v1),  [pdf](http://arxiv.org/pdf/2509.08696v1)

**Tags**: eess.AS cs.SD 



### BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter   1.58-bit LLM Inference
**Authors**: Wenlun Zhang, Xinyu Li, Shimpei Ando, Kentaro Yoshioka

**Updated**: 2025-09-10T12:46:29Z

**Summary**: Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy efficiency for CNNs by eliminating runtime weight updates. However, their scalability to Large Language Models (LLMs) is fundamentally constrained by their vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA series - demands more than 1,000 cm2 of silicon area even in advanced CMOS nodes. This paper presents BitROM, the first CiROM-based accelerator that overcomes this limitation through co-design with BitNet's 1.58-bit quantization model, enabling practical and efficient LLM inference at the edge. BitROM introduces three key innovations: 1) a novel Bidirectional ROM Array that stores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator optimized for ternary-weight computations; and 3) an integrated Decode-Refresh (DR) eDRAM that supports on-die KV-cache management, significantly reducing external memory access during decoding. In addition, BitROM integrates LoRA-based adapters to enable efficient transfer learning across various downstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit density of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over prior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6% reduction in external DRAM access, further enhancing deployment efficiency for LLMs in edge applications.

**Link**: [arxiv](http://arxiv.org/abs/2509.08542v1),  [pdf](http://arxiv.org/pdf/2509.08542v1)

**Tags**: cs.AR 



### Accelerating Mixture-of-Expert Inference with Adaptive Expert Split   Mechanism
**Authors**: Jiaming Yan, Jianchun Liu, Hongli Xu, Liusheng Huang

**Updated**: 2025-09-10T07:28:24Z

**Summary**: Mixture-of-Experts (MoE) has emerged as a promising architecture for modern large language models (LLMs). However, massive parameters impose heavy GPU memory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs. Offloading the expert parameters to CPU RAM offers an effective way to alleviate the VRAM requirements for MoE inference. Existing approaches typically cache a small subset of experts in VRAM and dynamically prefetch experts from RAM during inference, leading to significant degradation in inference speed due to the poor cache hit rate and substantial expert loading latency. In this work, we propose MoEpic, an efficient MoE inference system with a novel expert split mechanism. Specifically, each expert is vertically divided into two segments: top and bottom. MoEpic caches the top segment of hot experts, so that more experts will be stored under the limited VRAM budget, thereby improving the cache hit rate. During each layer's inference, MoEpic predicts and prefetches the activated experts for the next layer. Since the top segments of cached experts are exempt from fetching, the loading time is reduced, which allows efficient transfer-computation overlap. Nevertheless, the performance of MoEpic critically depends on the cache configuration (i.e., each layer's VRAM budget and expert split ratio). To this end, we propose a divide-and-conquer algorithm based on fixed-point iteration for adaptive cache configuration. Extensive experiments on popular MoE LLMs demonstrate that MoEpic can save about half of the GPU cost, while lowering the inference latency by about 37.51%-65.73% compared to the baselines.

**Link**: [arxiv](http://arxiv.org/abs/2509.08342v1),  [pdf](http://arxiv.org/pdf/2509.08342v1)

**Tags**: cs.LG cs.AI 



### EvolKV: Evolutionary KV Cache Compression for LLM Inference
**Authors**: Bohan Yu, Yekun Chai

**Updated**: 2025-09-10T06:32:49Z

**Summary**: Existing key-value (KV) cache compression methods typically rely on heuristics, such as uniform cache allocation across layers or static eviction policies, however, they ignore the critical interplays among layer-specific feature patterns and task performance, which can lead to degraded generalization. In this paper, we propose EvolKV, an adaptive framework for layer-wise, task-driven KV cache compression that jointly optimizes the memory efficiency and task performance. By reformulating cache allocation as a multi-objective optimization problem, EvolKV leverages evolutionary search to dynamically configure layer budgets while directly maximizing downstream performance. Extensive experiments on 11 tasks demonstrate that our approach outperforms all baseline methods across a wide range of KV cache budgets on long-context tasks and surpasses heuristic baselines by up to 7 percentage points on GSM8K. Notably, EvolKV achieves superior performance over the full KV cache setting on code completion while utilizing only 1.5% of the original budget, suggesting the untapped potential in learned compression strategies for KV cache budget allocation.

**Link**: [arxiv](http://arxiv.org/abs/2509.08315v1),  [pdf](http://arxiv.org/pdf/2509.08315v1)

**Tags**: cs.LG cs.CL cs.NE 



### TokenSelect: Efficient Long-Context Inference and Length Extrapolation   for LLMs via Dynamic Token-Level KV Cache Selection
**Authors**: Wei Wu, Zhuoshi Pan, Chao Wang, Liyi Chen, Yunchu Bai, Tianfu Wang, Kun Fu, Zheng Wang, Hui Xiong

**Updated**: 2025-09-09T13:30:17Z

**Summary**: Rapid advances in Large Language Models (LLMs) have spurred demand for processing extended context sequences in contemporary applications. However, this progress faces two challenges: performance degradation due to sequence lengths out-of-distribution, and excessively long inference times caused by the quadratic computational complexity of attention. These issues limit LLMs in long-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free method for efficient and accurate long-context inference. TokenSelect builds upon the observation of non-contiguous attention sparsity, using QK dot products to measure per-head KV Cache criticality at token-level. By per-head soft voting mechanism, TokenSelect selectively involves a few critical KV cache tokens in attention calculation without sacrificing accuracy. To further accelerate TokenSelect, we design the Selection Cache based on observations of consecutive Query similarity and implemented the efficient Paged Dot Product Kernel, significantly reducing the selection overhead. A comprehensive evaluation of TokenSelect demonstrates up to $23.84\times$ speedup in attention computation and up to $2.28\times$ acceleration in end-to-end latency, while providing superior performance compared to state-of-the-art long-context inference methods.

**Link**: [arxiv](http://arxiv.org/abs/2411.02886v3),  [pdf](http://arxiv.org/pdf/2411.02886v3)

**Tags**: cs.CL cs.AI cs.LG 



### DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for   Efficient MoE LLM Inference
**Authors**: Yuning Zhang, Grant Pinkert, Nan Yang, Yanli Li, Dong Yuan

**Updated**: 2025-09-09T04:00:43Z

**Summary**: Large Language Models (LLMs) have demonstrated impressive performance across a wide range of deep learning tasks. Mixture of Experts (MoE) further enhances their capabilities by increasing model width through sparsely activated expert branches, which keeps inference computation efficient. However, the large number of expert weights introduces significant GPU memory pressure, especially in resource-constrained environments such as single-GPU servers. More importantly, MoE inference consists of two fundamentally different stages: a prefill stage where most experts are activated densely, and a decode stage where only a few experts are triggered sparsely. Treating these stages with a uniform scheduling strategy often leads to suboptimal latency and memory usage. To address this, we propose DuoServe-MoE, an inference serving system that explicitly separates prefill and decode stages and applies tailored expert scheduling strategies to each. In the prefill stage, DuoServe-MoE uses a two-stream CUDA pipeline that overlaps expert weight prefetching with the computation of non-MoE layers, limiting expert residency in GPU memory. In the decode stage, a lightweight layer-level predictor trained offline from activation traces is used to prefetch only the most likely activated experts, without requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B and 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to 7.54 times while keeping peak memory usage at only 15 percent of the full model size.

**Link**: [arxiv](http://arxiv.org/abs/2509.07379v1),  [pdf](http://arxiv.org/pdf/2509.07379v1)

**Tags**: cs.DC 



### BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure   HBM Accelerators
**Authors**: Yitong Guo, Hongbo Chen, Haobin Hiroki Chen, Yukui Luo, XiaoFeng Wang, Chenghong Wang

**Updated**: 2025-09-09T00:15:05Z

**Summary**: While Trusted Execution Environments provide a strong foundation for secure cloud computing, they remain vulnerable to access pattern leakages. Oblivious Maps (OMAPs) mitigate this by fully hiding access patterns but suffer from high overhead due to randomized remapping and worst-case padding. We argue these costs are not fundamental. Modern accelerators featuring High-Bandwidth Memory (HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that eavesdropping on HBM is difficult -- even for physical attackers -- as its memory channels are sealed together with processor cores inside the same physical package. Later, Hunt et al. [NSDI'20] show that, with proper isolation, HBM can be turned into an unobservable region where both data and memory traces are hidden. This motivates a rethink of OMAP design with HBM-backed solutions to finally overcome their traditional performance limits. Building on these insights, we present BOLT, a Bandwidth Optimized, Lightning-fast OMAP accelerator that, for the first time, achieves O(1) + O(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations: (i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache to accelerate oblivious access to large host memory; (ii) a self-hosted architecture that offloads execution and memory control from the host to mitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs that maximize resource efficiency. We implement a prototype BOLT on a Xilinx U55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in initialization and query time, respectively, over state-of-the-art OMAPs, including an industry implementation from Facebook.

**Link**: [arxiv](http://arxiv.org/abs/2509.01742v2),  [pdf](http://arxiv.org/pdf/2509.01742v2)

**Tags**: cs.CR cs.AR 



### Revolutionizing Reinforcement Learning Framework for Diffusion Large   Language Models
**Authors**: Yinjie Wang, Ling Yang, Bowen Li, Ye Tian, Ke Shen, Mengdi Wang

**Updated**: 2025-09-08T17:58:06Z

**Summary**: We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language models (DLMs) that incorporates preferred inference trajectory into post-training, and is applicable across different architectures. Equipped with a diffusion-based value model that enhances training stability, we demonstrate improved reasoning performance on complex math and coding tasks. Besides, it can also be applied to adapt block-specific models to larger blocks, which improves sampling flexibility. Employing TraceRL, we derive a series of state-of-the-art diffusion language models, namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still consistently outperforms them across complex math reasoning tasks. TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical reasoning benchmarks. Through curriculum learning, we also derive the first long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1% relative accuracy gain. To facilitate reproducible research and practical applications, we release a comprehensive open-source framework for building, training, and deploying diffusion LLMs across diverse architectures. The framework integrates accelerated KV-cache techniques and inference engines for both inference and reinforcement learning, and includes implementations of various supervised fine-tuning and RL methods for mathematics, coding, and general tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL

**Link**: [arxiv](http://arxiv.org/abs/2509.06949v1),  [pdf](http://arxiv.org/pdf/2509.06949v1)

**Tags**: cs.CL 



### Amplifying Effective CXL Memory Bandwidth for LLM Inference via   Transparent Near-Data Processing
**Authors**: Rui Xie, Asad Ul Haq, Linsen Ma, Yunhua Fang, Zirak Burzin Engineer, Liu Liu, Tong Zhang

**Updated**: 2025-09-08T17:22:17Z

**Summary**: Large language model (LLM) inference is bottlenecked by the limited bandwidth of CXL-based memory used for capacity expansion. We introduce CXL-NDP, a transparent near-data processing architecture that amplifies effective CXL bandwidth without requiring changes to the CXL.mem interface or AI models. CXL-NDP integrates a precision-scalable bit-plane layout for dynamic quantization with transparent lossless compression of weights and KV caches directly within the CXL device. In end-to-end serving, CXL-NDP improves throughput by 43%, extends the maximum context length by 87%, and reduces the KV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms its practicality with a modest silicon footprint, lowering the barrier for adopting efficient, scalable CXL-based memory in generative AI infrastructure.

**Link**: [arxiv](http://arxiv.org/abs/2509.03377v2),  [pdf](http://arxiv.org/pdf/2509.03377v2)

**Tags**: cs.AR 



### X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and   Extreme KV Compression
**Authors**: Guihong Li, Mehdi Rezagholizadeh, Mingyu Yang, Vikram Appia, Emad Barsoum

**Updated**: 2025-09-08T13:34:54Z

**Summary**: Multi-head latent attention (MLA) is designed to optimize KV cache memory through low-rank key-value joint compression. Rather than caching keys and values separately, MLA stores their compressed latent representations, reducing memory overhead while maintaining the performance. While MLA improves memory efficiency without compromising language model accuracy, its major limitation lies in its integration during the pre-training phase, requiring models to be trained from scratch. This raises a key question: can we use MLA's benefits fully or partially in models that have already been pre-trained with different attention mechanisms? In this paper, we propose X-EcoMLA to deploy post training distillation to enable the upcycling of Transformer-based attention into an efficient hybrid MLA variant through lightweight post-training adaptation, bypassing the need for extensive pre-training. We demonstrate that leveraging the dark knowledge of a well-trained model can enhance training accuracy and enable extreme KV cache compression in MLA without compromising model performance. The experimental results show that our proposed method can effectively compress the KV cache while preserving the performance on the benchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression achieves the same average score by using only 3.6B training tokens and 70 GPU hours on AMD MI300, whereas a 10.6x compression have less than 0.1% average score drop with 7B training tokens and 140 GPU hours. The code for this work is available at https://github.com/AMD-AGI/AMD-Hybrid-Models.

**Link**: [arxiv](http://arxiv.org/abs/2503.11132v4),  [pdf](http://arxiv.org/pdf/2503.11132v4)

**Tags**: cs.CL 



### CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View   Synthesis
**Authors**: Xin Kong, Daniel Watson, Yannick Strümpler, Michael Niemeyer, Federico Tombari

**Updated**: 2025-09-08T11:49:51Z

**Summary**: Multi-view diffusion models have shown promise in 3D novel view synthesis, but most existing methods adopt a non-autoregressive formulation. This limits their applicability in world modeling, as they only support a fixed number of views and suffer from slow inference due to denoising all frames simultaneously. To address these limitations, we propose CausNVS, a multi-view diffusion model in an autoregressive setting, which supports arbitrary input-output view configurations and generates views sequentially. We train CausNVS with causal masking and per-frame noise, using pairwise-relative camera pose encodings (CaPE) for precise camera control. At inference time, we combine a spatially-aware sliding-window with key-value caching and noise conditioning augmentation to mitigate drift. Our experiments demonstrate that CausNVS supports a broad range of camera trajectories, enables flexible autoregressive novel view synthesis, and achieves consistently strong visual quality across diverse settings. Project page: https://kxhit.github.io/CausNVS.html.

**Link**: [arxiv](http://arxiv.org/abs/2509.06579v1),  [pdf](http://arxiv.org/pdf/2509.06579v1)

**Tags**: cs.CV 



## Keyword: LLM Inference 
 ### Stars and ionized gas in UGCA 320: a nearby gas-rich, dwarf Irregular   galaxy
**Authors**: Adebusola B. Alabi, S. Ilani Loubser, Moses K. Mogotsi, N. Zabel

**Updated**: 2025-09-24T17:59:19Z

**Summary**: UGCA 320 is a gas-rich dwarf irregular galaxy which belongs to a nearby, relatively isolated group of dwarf galaxies. Here, we combine multi-band HST imaging data with deep long-slit SALT/RSS and integral-field VLT/MUSE spectral data to study the stellar and ionized gas components of UGCA 320. Our imaging data analysis reveals a very blue (V-I~0.1 mag), flattened radial colour profile. We detect an abundance of ionized gas in UGCA 320 powered mostly by recent star formation. The stellar disc in UGCA 320 is populated predominantly by young (~120 Myr) and metal-poor (~15-30 per cent solar metallicity) stars and it rotates in the same sense as the ionized gas disc but with higher rotation velocities, and possibly in different planes. Our analysis reveals a sharp transition in the kinematic properties of the discs at radius ~10" (~0.3 kpc) and distortions in the outer disc region. We show that these features are consistent with a recent tidal interaction most likely with its close neighbour - UGCA 319. We discuss our results in the context of interacting dwarf galaxies and also show that similar inferences can be made independently from the long-slit data analysis as with the integral-field data.

**Link**: [arxiv](http://arxiv.org/abs/2509.20359v1),  [pdf](http://arxiv.org/pdf/2509.20359v1)

**Tags**: astro-ph.GA 



### Modeling Innovation Ecosystem Dynamics through Interacting Reinforced   Bernoulli Processes
**Authors**: Giacomo Aletti, Irene Crimaldi, Andrea Ghiglietti, Federico Nutarelli

**Updated**: 2025-09-24T17:53:11Z

**Summary**: Understanding how capabilities evolve into core capabilities-and how core capabilities may ossify into rigidities-is central to innovation strategy (Leonard-Barton 1992, Teece 2009). A major challenge in formalizing this process lies in the interactive nature of innovation: successes in one domain often reshape others, endogenizing specialization and complicating isolated modeling. This is especially true in ecosystems where firm capabilities and innovation outcomes hinge on managing interdependencies and complementarities (Jacobides, Cennamo and Gawer 2018, 2024).   To address this, we propose a novel formal model based on interacting reinforced Bernoulli processes. This framework captures how patent successes propagate across technological categories and how these categories co-evolve. The model is able to jointly account for several stylized facts in the empirical innovation literature, including sublinear success growth (successprobability decay), convergence of success shares across fields, and diminishing cross-category correlations over time.   Empirical validation using GLOBAL PATSTAT (1980-2018) supports the theoretical predictions. We estimate the structural parameters of the interaction matrix and we also propose a statistical procedure to make inference on the intensity of cross-category interactions under the mean-field assumption.   By endogenizing technological specialization, our model provides a strategic tool for policymakers and managers, supporting decision-making in complex, co-evolving innovation ecosystems-where targeted interventions can produce systemic effects, influencing competitive trajectories and shaping long-term patterns of specialization.

**Link**: [arxiv](http://arxiv.org/abs/2505.13364v2),  [pdf](http://arxiv.org/pdf/2505.13364v2)

**Tags**: stat.AP math.ST stat.TH 



### LEDiT: Your Length-Extrapolatable Diffusion Transformer without   Positional Encoding
**Authors**: Shen Zhang, Siyuan Liang, Yaning Tan, Zhaowei Chen, Linze Li, Ge Wu, Yuhao Chen, Shuheng Li, Zhenyu Zhao, Caihua Chen, Jiajun Liang, Yao Tang

**Updated**: 2025-09-24T17:48:25Z

**Summary**: Diffusion transformers (DiTs) struggle to generate images at resolutions higher than their training resolutions. The primary obstacle is that the explicit positional encodings(PE), such as RoPE, need extrapolating to unseen positions which degrades performance when the inference resolution differs from training. In this paper, We propose a Length-Extrapolatable Diffusion Transformer~(LEDiT) to overcome this limitation. LEDiT needs no explicit PEs, thereby avoiding PE extrapolation. The key innovation of LEDiT lies in the use of causal attention. We demonstrate that causal attention can implicitly encode global positional information and show that such information facilitates extrapolation. We further introduce a locality enhancement module, which captures fine-grained local information to complement the global coarse-grained position information encoded by causal attention. Experimental results on both conditional and text-to-image generation tasks demonstrate that LEDiT supports up to 4x resolution scaling (e.g., from 256x256 to 512x512), achieving better image quality compared to the state-of-the-art length extrapolation methods. We believe that LEDiT marks a departure from the standard RoPE-based methods and offers a promising insight into length extrapolation. Project page: https://shenzhang2145.github.io/ledit/

**Link**: [arxiv](http://arxiv.org/abs/2503.04344v3),  [pdf](http://arxiv.org/pdf/2503.04344v3)

**Tags**: cs.CV 



### Interactive Inference: A Neuromorphic Theory of Human-Computer   Interaction
**Authors**: Roel Vertegaal, Timothy Merritt, Saul Greenberg, Aneesh P. Tarun, Zhen Li, Zafeirios Fountas

**Updated**: 2025-09-24T17:46:21Z

**Summary**: Neuromorphic Human-Computer Interaction (HCI) is a theoretical approach to designing better user experiences (UX) motivated by advances in the understanding of the neurophysiology of the brain. Inspired by the neuroscientific theory of Active Inference, Interactive Inference is a first example of such approach. It offers a simplified interpretation of Active Inference that allows designers to more readily apply this theory to design and evaluation. In Interactive Inference, user behaviour is modeled as Bayesian inference on progress and goal distributions that predicts the next action. We show how the error between goal and progress distributions, or Bayesian surprise, can be modeled as a simple mean square error of the signal-to-noise ratio (SNR) of a task. The problem is that the user's capacity to process Bayesian surprise follows the logarithm of this SNR. This means errors rise quickly once average capacity is exceeded. Our model allows the quantitative analysis of performance and error using one framework that can provide real-time estimates of the mental load in users that needs to be minimized by design. We show how three basic laws of HCI, Hick's Law, Fitts' Law and the Power Law can be expressed using our model. We then test the validity of the model by empirically measuring how well it predicts human performance and error in a car following task. Results suggest that driver processing capacity indeed is a logarithmic function of the SNR of the distance to a lead car. This result provides initial evidence that Interactive Interference can be useful as a new theoretical design tool.

**Link**: [arxiv](http://arxiv.org/abs/2502.05935v4),  [pdf](http://arxiv.org/pdf/2502.05935v4)

**Tags**: cs.HC cs.IT math.IT 



### Statistical Inference Leveraging Synthetic Data with Distribution-Free   Guarantees
**Authors**: Meshi Bashari, Yonghoon Lee, Roy Maor Lotan, Edgar Dobriban, Yaniv Romano

**Updated**: 2025-09-24T17:37:14Z

**Summary**: The rapid proliferation of high-quality synthetic data -- generated by advanced AI models or collected as auxiliary data from related tasks -- presents both opportunities and challenges for statistical inference. This paper introduces a GEneral Synthetic-Powered Inference (GESPI) framework that wraps around any statistical inference procedure to safely enhance sample efficiency by combining synthetic and real data. Our framework leverages high-quality synthetic data to boost statistical power, yet adaptively defaults to the standard inference method using only real data when synthetic data is of low quality. The error of our method remains below a user-specified bound without any distributional assumptions on the synthetic data, and decreases as the quality of the synthetic data improves. This flexibility enables seamless integration with conformal prediction, risk control, hypothesis testing, and multiple testing procedures, all without modifying the base inference method. We demonstrate the benefits of our method on challenging tasks with limited labeled data, including AlphaFold protein structure prediction, and comparing large reasoning models on complex math problems.

**Link**: [arxiv](http://arxiv.org/abs/2509.20345v1),  [pdf](http://arxiv.org/pdf/2509.20345v1)

**Tags**: stat.ME cs.LG stat.ML 



### DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware   Reinforcement Learning on Imbalanced Data
**Authors**: Yuhang Zhou, Jing Zhu, Shengyi Qian, Zhuokai Zhao, Xiyao Wang, Xiaoyu Liu, Ming Li, Paiheng Xu, Wei Ai, Furong Huang

**Updated**: 2025-09-24T17:25:12Z

**Summary**: Large Language Models (LLMs) are increasingly aligned with human preferences through Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods, Group Relative Policy Optimization (GRPO) has gained attention for its simplicity and strong performance, notably eliminating the need for a learned value function. However, GRPO implicitly assumes a balanced domain distribution and uniform semantic alignment across groups, assumptions that rarely hold in real-world datasets. When applied to multi-domain, imbalanced data, GRPO disproportionately optimizes for dominant domains, neglecting underrepresented ones and resulting in poor generalization and fairness. We propose Domain-Informed Self-Consistency Policy Optimization (DISCO), a principled extension to GRPO that addresses inter-group imbalance with two key innovations. Domain-aware reward scaling counteracts frequency bias by reweighting optimization based on domain prevalence. Difficulty-aware reward scaling leverages prompt-level self-consistency to identify and prioritize uncertain prompts that offer greater learning value. Together, these strategies promote more equitable and effective policy learning across domains. Extensive experiments across multiple LLMs and skewed training distributions show that DISCO improves generalization, outperforms existing GRPO variants by 5% on Qwen3 models, and sets new state-of-the-art results on multi-domain alignment benchmarks. Our code and data are available at https://github.com/Tonyzhou98/disco_grpo.

**Link**: [arxiv](http://arxiv.org/abs/2505.15074v3),  [pdf](http://arxiv.org/pdf/2505.15074v3)

**Tags**: cs.CL cs.AI cs.LG 



### Uncovering Graph Reasoning in Decoder-only Transformers with Circuit   Tracing
**Authors**: Xinnan Dai, Chung-Hsiang Lo, Kai Guo, Shenglai Zeng, Dongsheng Luo, Jiliang Tang

**Updated**: 2025-09-24T17:25:05Z

**Summary**: Transformer-based LLMs demonstrate strong performance on graph reasoning tasks, yet their internal mechanisms remain underexplored. To uncover these reasoning process mechanisms in a fundamental and unified view, we set the basic decoder-only transformers and explain them using the circuit-tracer framework. Through this lens, we visualize reasoning traces and identify two core mechanisms in graph reasoning: token merging and structural memorization, which underlie both path reasoning and substructure extraction tasks. We further quantify these behaviors and analyze how they are influenced by graph density and model size. Our study provides a unified interpretability framework for understanding structural reasoning in decoder-only Transformers.

**Link**: [arxiv](http://arxiv.org/abs/2509.20336v1),  [pdf](http://arxiv.org/pdf/2509.20336v1)

**Tags**: cs.LG cs.AI 



### Multimodal Reference Visual Grounding
**Authors**: Yangxiao Lu, Ruosen Li, Liqiang Jing, Jikai Wang, Xinya Du, Yunhui Guo, Nicholas Ruozzi, Yu Xiang

**Updated**: 2025-09-24T17:23:48Z

**Summary**: Visual grounding focuses on detecting objects from images based on language expressions. Recent Large Vision-Language Models (LVLMs) have significantly advanced visual grounding performance by training large models with large-scale datasets. However, the problem remains challenging, especially when similar objects appear in the input image. For example, an LVLM may not be able to differentiate Diet Coke and regular Coke in an image. In this case, if additional reference images of Diet Coke and regular Coke are available, it can help the visual grounding of similar objects.   In this work, we introduce a new task named Multimodal Reference Visual Grounding (MRVG). In this task, a model has access to a set of reference images of objects in a database. Based on these reference images and a language expression, the model is required to detect a target object from a query image. We first introduce a new dataset to study the MRVG problem. Then we introduce a novel method, named MRVG-Net, to solve this visual grounding problem. We show that by efficiently using reference images with few-shot object detection and using Large Language Models (LLMs) for object matching, our method achieves superior visual grounding performance compared to the state-of-the-art LVLMs such as Qwen2.5-VL-72B. Our approach bridges the gap between few-shot detection and visual grounding, unlocking new capabilities for visual understanding, which has wide applications in robotics. Project page with our video, code, and dataset: https://irvlutd.github.io/MultiGrounding

**Link**: [arxiv](http://arxiv.org/abs/2504.02876v2),  [pdf](http://arxiv.org/pdf/2504.02876v2)

**Tags**: cs.CV cs.LG 



### Video models are zero-shot learners and reasoners
**Authors**: Thaddäus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, Robert Geirhos

**Updated**: 2025-09-24T17:17:27Z

**Summary**: The remarkable zero-shot capabilities of Large Language Models (LLMs) have propelled natural language processing from task-specific models to unified, generalist foundation models. This transformation emerged from simple primitives: large, generative models trained on web-scale data. Curiously, the same primitives apply to today's generative video models. Could video models be on a trajectory towards general-purpose vision understanding, much like LLMs developed general-purpose language understanding? We demonstrate that Veo 3 can solve a broad variety of tasks it wasn't explicitly trained for: segmenting objects, detecting edges, editing images, understanding physical properties, recognizing object affordances, simulating tool use, and more. These abilities to perceive, model, and manipulate the visual world enable early forms of visual reasoning like maze and symmetry solving. Veo's emergent zero-shot capabilities indicate that video models are on a path to becoming unified, generalist vision foundation models.

**Link**: [arxiv](http://arxiv.org/abs/2509.20328v1),  [pdf](http://arxiv.org/pdf/2509.20328v1)

**Tags**: cs.LG cs.AI cs.CV cs.RO 



### RAG Security and Privacy: Formalizing the Threat Model and Attack   Surface
**Authors**: Atousa Arzanipour, Rouzbeh Behnia, Reza Ebrahimi, Kaushik Dutta

**Updated**: 2025-09-24T17:11:35Z

**Summary**: Retrieval-Augmented Generation (RAG) is an emerging approach in natural language processing that combines large language models (LLMs) with external document retrieval to produce more accurate and grounded responses. While RAG has shown strong potential in reducing hallucinations and improving factual consistency, it also introduces new privacy and security challenges that differ from those faced by traditional LLMs. Existing research has demonstrated that LLMs can leak sensitive information through training data memorization or adversarial prompts, and RAG systems inherit many of these vulnerabilities. At the same time, reliance of RAG on an external knowledge base opens new attack surfaces, including the potential for leaking information about the presence or content of retrieved documents, or for injecting malicious content to manipulate model behavior. Despite these risks, there is currently no formal framework that defines the threat landscape for RAG systems. In this paper, we address a critical gap in the literature by proposing, to the best of our knowledge, the first formal threat model for retrieval-RAG systems. We introduce a structured taxonomy of adversary types based on their access to model components and data, and we formally define key threat vectors such as document-level membership inference and data poisoning, which pose serious privacy and integrity risks in real-world deployments. By establishing formal definitions and attack models, our work lays the foundation for a more rigorous and principled understanding of privacy and security in RAG systems.

**Link**: [arxiv](http://arxiv.org/abs/2509.20324v1),  [pdf](http://arxiv.org/pdf/2509.20324v1)

**Tags**: cs.CR cs.AI 



### DRES: Benchmarking LLMs for Disfluency Removal
**Authors**: Maria Teleki, Sai Janjur, Haoran Liu, Oliver Grabner, Ketan Verma, Thomas Docog, Xiangjue Dong, Lingfeng Shi, Cong Wang, Stephanie Birkelbach, Jason Kim, Yin Zhang, James Caverlee

**Updated**: 2025-09-24T17:08:12Z

**Summary**: Disfluencies -- such as "um," "uh," interjections, parentheticals, and edited statements -- remain a persistent challenge for speech-driven systems, degrading accuracy in command interpretation, summarization, and conversational agents. We introduce DRES (Disfluency Removal Evaluation Suite), a controlled text-level benchmark that establishes a reproducible semantic upper bound for this task. DRES builds on human-annotated Switchboard transcripts, isolating disfluency removal from ASR errors and acoustic variability. We systematically evaluate proprietary and open-source LLMs across scales, prompting strategies, and architectures. Our results reveal that (i) simple segmentation consistently improves performance, even for long-context models; (ii) reasoning-oriented models tend to over-delete fluent tokens; and (iii) fine-tuning achieves near state-of-the-art precision and recall but harms generalization abilities. We further present a set of LLM-specific error modes and offer nine practical recommendations (R1-R9) for deploying disfluency removal in speech-driven pipelines. DRES provides a reproducible, model-agnostic foundation for advancing robust spoken-language systems.

**Link**: [arxiv](http://arxiv.org/abs/2509.20321v1),  [pdf](http://arxiv.org/pdf/2509.20321v1)

**Tags**: cs.CL cs.AI eess.AS 



### Enhancing Targeted Adversarial Attacks on Large Vision-Language Models   via Intermediate Projector
**Authors**: Yiming Cao, Yanjie Li, Kaisheng Liang, Bin Xiao

**Updated**: 2025-09-24T17:02:50Z

**Summary**: The growing deployment of Large Vision-Language Models (VLMs) raises safety concerns, as adversaries may exploit model vulnerabilities to induce harmful outputs, with targeted black-box adversarial attacks posing a particularly severe threat. However, existing methods primarily maximize encoder-level global similarity, which lacks the granularity for stealthy and practical fine-grained attacks, where only specific target should be altered (e.g., modifying a car while preserving its background). Moreover, they largely neglect the projector, a key semantic bridge in VLMs for multimodal alignment. To address these limitations, we propose a novel black-box targeted attack framework that leverages the projector. Specifically, we utilize the widely adopted Querying Transformer (Q-Former) which transforms global image embeddings into fine-grained query outputs, to enhance attack effectiveness and granularity. For standard global targeted attack scenarios, we propose the Intermediate Projector Guided Attack (IPGA), which aligns Q-Former fine-grained query outputs with the target to enhance attack strength and exploits the intermediate pretrained Q-Former that is not fine-tuned for any specific Large Language Model (LLM) to improve attack transferability. For fine-grained attack scenarios, we augment IPGA with the Residual Query Alignment (RQA) module, which preserves unrelated content by constraining non-target query outputs to enhance attack granularity. Extensive experiments demonstrate that IPGA significantly outperforms baselines in global targeted attacks, and IPGA with RQA (IPGA-R) attains superior success rates and unrelated content preservation over baselines in fine-grained attacks. Our method also transfers effectively to commercial VLMs such as Google Gemini and OpenAI GPT.

**Link**: [arxiv](http://arxiv.org/abs/2508.13739v2),  [pdf](http://arxiv.org/pdf/2508.13739v2)

**Tags**: cs.CV 



### Z-Scores: A Metric for Linguistically Assessing Disfluency Removal
**Authors**: Maria Teleki, Sai Janjur, Haoran Liu, Oliver Grabner, Ketan Verma, Thomas Docog, Xiangjue Dong, Lingfeng Shi, Cong Wang, Stephanie Birkelbach, Jason Kim, Yin Zhang, James Caverlee

**Updated**: 2025-09-24T17:02:39Z

**Summary**: Evaluating disfluency removal in speech requires more than aggregate token-level scores. Traditional word-based metrics such as precision, recall, and F1 (E-Scores) capture overall performance but cannot reveal why models succeed or fail. We introduce Z-Scores, a span-level linguistically-grounded evaluation metric that categorizes system behavior across distinct disfluency types (EDITED, INTJ, PRN). Our deterministic alignment module enables robust mapping between generated text and disfluent transcripts, allowing Z-Scores to expose systematic weaknesses that word-level metrics obscure. By providing category-specific diagnostics, Z-Scores enable researchers to identify model failure modes and design targeted interventions -- such as tailored prompts or data augmentation -- yielding measurable performance improvements. A case study with LLMs shows that Z-Scores uncover challenges with INTJ and PRN disfluencies hidden in aggregate F1, directly informing model refinement strategies.

**Link**: [arxiv](http://arxiv.org/abs/2509.20319v1),  [pdf](http://arxiv.org/pdf/2509.20319v1)

**Tags**: cs.CL cs.AI eess.AS 



### A Comprehensive Evaluation of YOLO-based Deer Detection Performance on   Edge Devices
**Authors**: Bishal Adhikari, Jiajia Li, Eric S. Michel, Jacob Dykes, Te-Ming Paul Tseng, Mary Love Tagert, Dong Chen

**Updated**: 2025-09-24T17:01:50Z

**Summary**: The escalating economic losses in agriculture due to deer intrusion, estimated to be in the hundreds of millions of dollars annually in the U.S., highlight the inadequacy of traditional mitigation strategies since these methods are often labor-intensive, costly, and ineffective for modern farming systems. To overcome this, there is a critical need for intelligent, autonomous solutions which require accurate and efficient deer detection. But the progress in this field is impeded by a significant gap in the literature, mainly the lack of a domain-specific, practical dataset and limited study on the on-field deployability of deer detection systems. Addressing this gap, this study presents a comprehensive evaluation of state-of-the-art deep learning models for deer detection in challenging real-world scenarios. The contributions of this work are threefold. First, we introduce a curated, publicly available dataset of 3,095 annotated images with bounding-box annotations of deer, derived from the Idaho Cameratraps project. Second, we provide an extensive comparative analysis of 12 model variants across four recent YOLO architectures(v8, v9, v10, and v11). Finally, we benchmarked performance on a high-end NVIDIA RTX 5090 GPU and evaluated on two representative edge computing platforms: Raspberry Pi 5 and NVIDIA Jetson AGX Xavier. Results show that the real-time detection is not feasible in Raspberry Pi without hardware-specific model optimization, while NVIDIA Jetson provides greater than 30 FPS with GPU-accelerated inference on 's' and 'n' series models. This study also reveals that smaller, architecturally advanced models such as YOLOv11n, YOLOv8s, and YOLOv9s offer the optimal balance of high accuracy (AP@.5 > 0.85) and computational efficiency (FPS > 30). To support further research, both the source code and datasets are publicly available at https://github.com/WinnerBishal/track-the-deer.

**Link**: [arxiv](http://arxiv.org/abs/2509.20318v1),  [pdf](http://arxiv.org/pdf/2509.20318v1)

**Tags**: cs.CV 



### SIM-CoT: Supervised Implicit Chain-of-Thought
**Authors**: Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Jiaqi Wang, Xipeng Qiu, Dahua Lin

**Updated**: 2025-09-24T17:01:32Z

**Summary**: Implicit Chain-of-Thought (CoT) methods present a promising, token-efficient alternative to explicit CoT reasoning in Large Language Models (LLMs), but a persistent performance gap has limited the application of implicit CoT. We identify a core latent instability issue by scaling the computational budget of implicit CoT approaches: as we increase the number of implicit reasoning tokens to enhance performance, the training process often becomes unstable and collapses. Our analysis reveals that this instability arises from the latent representations becoming homogeneous and losing their semantic diversity, a failure caused by insufficient step-level supervision in existing implicit CoT approaches. To address this issue, we propose SIM-CoT, a plug-and-play training module that introduces step-level supervision to stabilize and enrich the latent reasoning space. Specifically, SIM-CoT employs an auxiliary decoder during training to align each implicit token with its corresponding explicit reasoning step, ensuring that latent states capture distinct and meaningful information. The proposed auxiliary decoder is removed during inference, preserving the computational efficiency of implicit CoT methods with no added overhead. In addition, the auxiliary decoder affords interpretability of implicit reasoning by projecting each latent token onto an explicit reasoning vocabulary, enabling per-step visualization of semantic roles and diagnosis. SIM-CoT significantly enhances both the in-domain accuracy and out-of-domain stability of various implicit CoT methods, boosting baselines like Coconut by +8.2% on GPT-2 and CODI by +3.0% on LLaMA-3.1 8B. Demonstrating strong scalability, SIM-CoT also surpasses the explicit CoT baseline on GPT-2 by 2.1% with 2.3\times greater token efficiency, while substantially closing the performance gap on larger models like LLaMA-3.1 8B.

**Link**: [arxiv](http://arxiv.org/abs/2509.20317v1),  [pdf](http://arxiv.org/pdf/2509.20317v1)

**Tags**: cs.CL cs.AI 



### A GEN AI Framework for Medical Note Generation
**Authors**: Hui Yi Leong, Yi Fan Gao, Shuai Ji, Bora Kalaycioglu, Uktu Pamuksuz

**Updated**: 2025-09-24T17:00:37Z

**Summary**: The increasing administrative burden of medical documentation, particularly through Electronic Health Records (EHR), significantly reduces the time available for direct patient care and contributes to physician burnout. To address this issue, we propose MediNotes, an advanced generative AI framework designed to automate the creation of SOAP (Subjective, Objective, Assessment, Plan) notes from medical conversations. MediNotes integrates Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and Automatic Speech Recognition (ASR) to capture and process both text and voice inputs in real time or from recorded audio, generating structured and contextually accurate medical notes. The framework also incorporates advanced techniques like Quantized Low-Rank Adaptation (QLoRA) and Parameter-Efficient Fine-Tuning (PEFT) for efficient model fine-tuning in resource-constrained environments. Additionally, MediNotes offers a query-based retrieval system, allowing healthcare providers and patients to access relevant medical information quickly and accurately. Evaluations using the ACI-BENCH dataset demonstrate that MediNotes significantly improves the accuracy, efficiency, and usability of automated medical documentation, offering a robust solution to reduce the administrative burden on healthcare professionals while improving the quality of clinical workflows.

**Link**: [arxiv](http://arxiv.org/abs/2410.01841v2),  [pdf](http://arxiv.org/pdf/2410.01841v2)

**Tags**: eess.AS cs.AI cs.CL cs.IR cs.SD 



### Efficient Fine-Tuning of Large Language Models for Automated Medical   Documentation
**Authors**: Hui Yi Leong, Yi Fan Gao, Ji Shuai, Yang Zhang, Uktu Pamuksuz

**Updated**: 2025-09-24T16:59:19Z

**Summary**: Scientific research indicates that for every hour spent in direct patient care, physicians spend nearly two additional hours on administrative tasks, particularly on electronic health records (EHRs) and desk work. This excessive administrative burden not only reduces the time available for patient care but also contributes to physician burnout and inefficiencies in healthcare delivery. To address these challenges, this study introduces MediGen, a fine-tuned large language model (LLM) designed to automate the generation of medical reports from medical dialogues. By leveraging state-of-the-art methodologies for fine-tuning open-source pretrained models, including LLaMA3-8B, MediGen achieves high accuracy in transcribing and summarizing clinical interactions. The fine-tuned LLaMA3-8B model demonstrated promising results, achieving a ROUGE score of 58% and a BERTScore-F1 of 72%, indicating its effectiveness in generating accurate and clinically relevant medical reports. These findings suggest that MediGen has the potential to significantly reduce the administrative workload on physicians, improving both healthcare efficiency and physician well-being.

**Link**: [arxiv](http://arxiv.org/abs/2409.09324v3),  [pdf](http://arxiv.org/pdf/2409.09324v3)

**Tags**: cs.CL cs.AI 



### UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from   an Uncertainty-Aware Perspective
**Authors**: Jing Xiong, Jianghan Shen, Fanghua Ye, Chaofan Tao, Zhongwei Wan, Jianqiao Lu, Xun Wu, Chuanyang Zheng, Zhijiang Guo, Min Yang, Lingpeng Kong, Ngai Wong

**Updated**: 2025-09-24T16:56:17Z

**Summary**: Deploying large language models (LLMs) for long-context inference remains challenging due to their substantial memory and computational demands. While techniques such as Key-Value (KV) cache compression are designed to reduce memory usage, they often neglect the structured sparsity inherent in the relationship between hidden states and their corresponding KV cache. In this work, we explore the role of uncertainty as a potential indicator of sparsity within LLMs. We propose UNComp, an uncertainty-aware framework that leverages truncated matrix entropy to identify areas of low information content, thereby revealing sparsity patterns that can be used for adaptive compression. Unlike traditional methods that apply uniform compression, UNComp dynamically adjusts its approach to compression, guided by uncertainty measures that reflect the importance of various model components. Our analysis shows that sparsity patterns, when derived from uncertainty estimates, can be exploited to reveal special long-range dependencies, such as retrieval heads and retrieval layers. This perspective not only enhances our understanding of how compression can be optimized but also provides new insights into the inherent sparsity of LLMs during long-context inference. By focusing on uncertainty to analyze the sparsity pattern in detail, UNComp reduces the KV cache size to 4.74% of the original, achieves a 6% prefill speedup, and improves throughput by 6.4x - not only delivering strong lossless compression performance, but also validating the effectiveness of the underlying theoretical tool. We release the code at https://github.com/menik1126/UNComp.

**Link**: [arxiv](http://arxiv.org/abs/2410.03090v2),  [pdf](http://arxiv.org/pdf/2410.03090v2)

**Tags**: cs.CL cs.LG 



### White-Basilisk: A Hybrid Model for Code Vulnerability Detection
**Authors**: Ioannis Lamprou, Alexander Shevtsov, Ioannis Arapakis, Sotiris Ioannidis

**Updated**: 2025-09-24T16:51:16Z

**Summary**: The proliferation of software vulnerabilities presents a significant challenge to cybersecurity, necessitating more effective detection methodologies. We introduce White-Basilisk, a novel approach to vulnerability detection that demonstrates superior performance while challenging prevailing assumptions in AI model scaling. Utilizing an innovative architecture that integrates Mamba layers, linear self-attention, and a Mixture of Experts framework, White-Basilisk achieves state-of-the-art results in vulnerability detection tasks with a parameter count of only 200M. The model's capacity to process sequences of unprecedented length enables comprehensive analysis of extensive codebases in a single pass, surpassing the context limitations of current Large Language Models (LLMs). White-Basilisk exhibits robust performance on imbalanced, real-world datasets, while maintaining computational efficiency that facilitates deployment across diverse organizational scales. This research not only establishes new benchmarks in code security but also provides empirical evidence that compact, efficiently designed models can outperform larger counterparts in specialized tasks, potentially redefining optimization strategies in AI development for domain-specific applications.

**Link**: [arxiv](http://arxiv.org/abs/2507.08540v3),  [pdf](http://arxiv.org/pdf/2507.08540v3)

**Tags**: cs.CR cs.AI 



### Advancing Expert Specialization for Better MoE
**Authors**: Hongcan Guo, Haolang Lu, Guoshun Nan, Bolun Chu, Jialin Zhuang, Yuan Yang, Wenhao Che, Sicong Leng, Qimei Cui, Xudong Jiang

**Updated**: 2025-09-24T16:48:33Z

**Summary**: Mixture-of-Experts (MoE) models enable efficient scaling of large language models (LLMs) by activating only a subset of experts per input. However, we observe that the commonly used auxiliary load balancing loss often leads to expert overlap and overly uniform routing, which hinders expert specialization and degrades overall performance during post-training. To address this, we propose a simple yet effective solution that introduces two complementary objectives: (1) an orthogonality loss to encourage experts to process distinct types of tokens, and (2) a variance loss to encourage more discriminative routing decisions. Gradient-level analysis demonstrates that these objectives are compatible with the existing auxiliary loss and contribute to optimizing the training process. Experimental results over various model architectures and across multiple benchmarks show that our method significantly enhances expert specialization. Notably, our method improves classic MoE baselines with auxiliary loss by up to 23.79%, while also maintaining load balancing in downstream tasks, without any architectural modifications or additional components. We will release our code to contribute to the community.

**Link**: [arxiv](http://arxiv.org/abs/2505.22323v2),  [pdf](http://arxiv.org/pdf/2505.22323v2)

**Tags**: cs.CL cs.SE 68T07 I.2.7 



### Enhancing RAG Efficiency with Adaptive Context Compression
**Authors**: Shuyu Guo, Shuo Zhang, Zhaochun Ren

**Updated**: 2025-09-24T16:41:40Z

**Summary**: Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but incurs significant inference costs due to lengthy retrieved contexts. While context compression mitigates this issue, existing methods apply fixed compression rates, over-compressing simple queries or under-compressing complex ones. We propose Adaptive Context Compression for RAG (ACC-RAG), a framework that dynamically adjusts compression rates based on input complexity, optimizing inference efficiency without sacrificing accuracy. ACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with a context selector to retain minimal sufficient information, akin to human skimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms fixed-rate methods and matches/unlocks over 4 times faster inference versus standard RAG while maintaining or improving accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2507.22931v3),  [pdf](http://arxiv.org/pdf/2507.22931v3)

**Tags**: cs.CL cs.AI 



### LLM-Powered AI Tutors with Personas for d/Deaf and Hard-of-Hearing   Online Learners
**Authors**: Haocong Cheng, Si Chen, Christopher Perdriau, Shriya Mokkapati, Yun Huang

**Updated**: 2025-09-24T16:38:38Z

**Summary**: Intelligent tutoring systems (ITS) using artificial intelligence (AI) technology have shown promise in supporting learners with diverse abilities. Large language models (LLMs) provide new opportunities to incorporate personas to AI-based tutors and support dynamic interactive dialogue. This paper explores how DHH learners interact with LLM-powered AI tutors with different experiences in DHH education as personas to identify their accessibility preferences. A user study with 16 DHH participants showed that they asked DHH-related questions based on background information and evaluated the AI tutors' cultural knowledge of the DHH communities in their responses. Participants suggested providing more transparency in each AI tutor's position within the DHH community. Participants also pointed out the lack of support in the multimodality of sign language in current LLMs. We discuss design implications to support the diverse needs in interaction between DHH users and the LLMs, such as offering supports in tuning language styles of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2411.09873v2),  [pdf](http://arxiv.org/pdf/2411.09873v2)

**Tags**: cs.HC 



### When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks   Silently Undermine Validity
**Authors**: Benjamin Feuer, Chiung-Yi Tseng, Astitwa Sarthak Lathe, Oussama Elachqar, John P Dickerson

**Updated**: 2025-09-24T16:26:47Z

**Summary**: LLM-judged benchmarks are increasingly used to evaluate complex model behaviors, yet their design introduces failure modes absent in conventional ground-truth based benchmarks. We argue that without tight objectives and verifiable constructions, benchmark rankings can produce high-confidence rankings that are in fact largely noise. We introduce two mechanisms to diagnose these issues. Schematic adherence quantifies how much of a judge's overall verdict is explained by the explicit evaluation schema, revealing unexplained variance when judges deviate from their own rubric. Psychometric validity aggregates internal consistency and discriminant validity signals to quantify irreducible uncertainty in any benchmarking run. Applying these tools to Arena-Hard Auto, we find severe schema incoherence and factor collapse across popular judges: for example, unexplained variance exceeding 90 percent for DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We also show that the ELO-style aggregation used by Arena-Hard Auto collapses and masks genuine ranking uncertainty. Our results highlight design failures that undermine validity and offer actionable principles for building better-scoped, reliability-aware LLM-judged benchmarks. We release our code at https://anonymous.4open.science/r/judgment-to-noise-947D/README.md

**Link**: [arxiv](http://arxiv.org/abs/2509.20293v1),  [pdf](http://arxiv.org/pdf/2509.20293v1)

**Tags**: cs.LG cs.AI 



### Instruction Boundary: Quantifying Biases in LLM Reasoning under Various   Coverage
**Authors**: Zipeng Ling, Yuehao Tang, Chen Huang, Shuliang Liu, Gaoyang Jiang, Shenghong Fu, Junqi Yang, Yao Wan, Jiawan Zhang, Kejia Huang, Xuming Hu

**Updated**: 2025-09-24T16:15:26Z

**Summary**: Large-language-model (LLM) reasoning has long been regarded as a powerful tool for problem solving across domains, providing non-experts with valuable advice. However, their limitations - especially those stemming from prompt design - remain underexplored. Because users may supply biased or incomplete prompts - often unintentionally - LLMs can be misled, undermining reliability and creating risks. We refer to this vulnerability as the Instruction Boundary. To investigate the phenomenon, we distill it into eight concrete facets and introduce BiasDetector, a framework that measures biases arising from three instruction types: complete, redundant, and insufficient. We evaluate several mainstream LLMs and find that, despite high headline accuracy, substantial biases persist in many downstream tasks as a direct consequence of prompt coverage. Our empirical study confirms that LLM reasoning reliability can still be significantly improved. We analyze the practical impact of these biases and outline mitigation strategies. Our findings underscore the need for developers to tackle biases and for users to craft options carefully.

**Link**: [arxiv](http://arxiv.org/abs/2509.20278v1),  [pdf](http://arxiv.org/pdf/2509.20278v1)

**Tags**: cs.CL 



### Investigating Security Implications of Automatically Generated Code on   the Software Supply Chain
**Authors**: Xiaofan Li, Xing Gao

**Updated**: 2025-09-24T16:15:17Z

**Summary**: In recent years, various software supply chain (SSC) attacks have posed significant risks to the global community. Severe consequences may arise if developers integrate insecure code snippets that are vulnerable to SSC attacks into their products. Particularly, code generation techniques, such as large language models (LLMs), have been widely utilized in the developer community. However, LLMs are known to suffer from inherent issues when generating code, including fabrication, misinformation, and reliance on outdated training data, all of which can result in serious software supply chain threats. In this paper, we investigate the security threats to the SSC that arise from these inherent issues. We examine three categories of threats, including eleven potential SSC-related threats, related to external components in source code, and continuous integration configuration files. We find some threats in LLM-generated code could enable attackers to hijack software and workflows, while some others might cause potential hidden threats that compromise the security of the software over time. To understand these security impacts and severity, we design a tool, SSCGuard, to generate 439,138 prompts based on SSC-related questions collected online, and analyze the responses of four popular LLMs from GPT and Llama. Our results show that all identified SSC-related threats persistently exist. To mitigate these risks, we propose a novel prompt-based defense mechanism, namely Chain-of-Confirmation, to reduce fabrication, and a middleware-based defense that informs users of various SSC threats.

**Link**: [arxiv](http://arxiv.org/abs/2509.20277v1),  [pdf](http://arxiv.org/pdf/2509.20277v1)

**Tags**: cs.CR cs.AI 



### Expanding the WMT24++ Benchmark with Rumantsch Grischun, Sursilvan,   Sutsilvan, Surmiran, Puter, and Vallader
**Authors**: Jannis Vamvas, Ignacio Pérez Prat, Not Battesta Soliva, Sandra Baltermia-Guetg, Andrina Beeli, Simona Beeli, Madlaina Capeder, Laura Decurtins, Gian Peder Gregori, Flavia Hobi, Gabriela Holderegger, Arina Lazzarini, Viviana Lazzarini, Walter Rosselli, Bettina Vital, Anna Rutkiewicz, Rico Sennrich

**Updated**: 2025-09-24T16:07:19Z

**Summary**: The Romansh language, spoken in Switzerland, has limited resources for machine translation evaluation. In this paper, we present a benchmark for six varieties of Romansh: Rumantsch Grischun, a supra-regional variety, and five regional varieties: Sursilvan, Sutsilvan, Surmiran, Puter, and Vallader. Our reference translations were created by human translators based on the WMT24++ benchmark, which ensures parallelism with more than 55 other languages. An automatic evaluation of existing MT systems and LLMs shows that translation out of Romansh into German is handled relatively well for all the varieties, but translation into Romansh is still challenging.

**Link**: [arxiv](http://arxiv.org/abs/2509.03148v2),  [pdf](http://arxiv.org/pdf/2509.03148v2)

**Tags**: cs.CL 



### Scan-do Attitude: Towards Autonomous CT Protocol Management using a   Large Language Model Agent
**Authors**: Xingjian Kang, Linda Vorberg, Andreas Maier, Alexander Katzmann, Oliver Taubmann

**Updated**: 2025-09-24T16:04:11Z

**Summary**: Managing scan protocols in Computed Tomography (CT), which includes adjusting acquisition parameters or configuring reconstructions, as well as selecting postprocessing tools in a patient-specific manner, is time-consuming and requires clinical as well as technical expertise. At the same time, we observe an increasing shortage of skilled workforce in radiology. To address this issue, a Large Language Model (LLM)-based agent framework is proposed to assist with the interpretation and execution of protocol configuration requests given in natural language or a structured, device-independent format, aiming to improve the workflow efficiency and reduce technologists' workload. The agent combines in-context-learning, instruction-following, and structured toolcalling abilities to identify relevant protocol elements and apply accurate modifications. In a systematic evaluation, experimental results indicate that the agent can effectively retrieve protocol components, generate device compatible protocol definition files, and faithfully implement user requests. Despite demonstrating feasibility in principle, the approach faces limitations regarding syntactic and semantic validity due to lack of a unified device API, and challenges with ambiguous or complex requests. In summary, the findings show a clear path towards LLM-based agents for supporting scan protocol management in CT imaging.

**Link**: [arxiv](http://arxiv.org/abs/2509.20270v1),  [pdf](http://arxiv.org/pdf/2509.20270v1)

**Tags**: cs.AI cs.CL 



### To Fold or Not to Fold: Graph Regularized Tensor Train for Visual Data   Completion
**Authors**: Le Xu, Lei Cheng, Ngai Wong, Yik-Chung Wu

**Updated**: 2025-09-24T15:57:10Z

**Summary**: Tensor train (TT) representation has achieved tremendous success in visual data completion tasks, especially when it is combined with tensor folding. However, folding an image or video tensor breaks the original data structure, leading to local information loss as nearby pixels may be assigned into different dimensions and become far away from each other. In this paper, to fully preserve the local information of the original visual data, we explore not folding the data tensor, and at the same time adopt graph information to regularize local similarity between nearby entries. To overcome the high computational complexity introduced by the graph-based regularization in the TT completion problem, we propose to break the original problem into multiple sub-problems with respect to each TT core fiber, instead of each TT core as in traditional methods. Furthermore, to avoid heavy parameter tuning, a sparsity promoting probabilistic model is built based on the generalized inverse Gaussian (GIG) prior, and an inference algorithm is derived under the mean-field approximation. Experiments on both synthetic data and real-world visual data show the superiority of the proposed methods.

**Link**: [arxiv](http://arxiv.org/abs/2306.11123v2),  [pdf](http://arxiv.org/pdf/2306.11123v2)

**Tags**: eess.SP cs.CV 



### AAPO: Enhancing the Reasoning Capabilities of LLMs with Advantage   Momentum
**Authors**: Jian Xiong, Jingbo Zhou, Jingyong Ye, Qiang Huang, Dejing Dou

**Updated**: 2025-09-24T15:47:49Z

**Summary**: Reinforcement learning (RL) has emerged as an effective approach for enhancing the reasoning capabilities of large language models (LLMs), especially in scenarios where supervised fine-tuning (SFT) falls short due to limited chain-of-thought (CoT) data. Among RL-based post-training methods, group relative advantage estimation, as exemplified by Group Relative Policy Optimization (GRPO), has attracted considerable attention for eliminating the dependency on the value model, thereby simplifying training compared to traditional approaches like Proximal Policy Optimization (PPO). However, we observe that exsiting group relative advantage estimation method still suffers from training inefficiencies, particularly when the estimated advantage approaches zero. To address this limitation, we propose Advantage-Augmented Policy Optimization (AAPO), a novel RL algorithm that optimizes the cross-entropy (CE) loss using advantages enhanced through a momentum-based estimation scheme. This approach effectively mitigates the inefficiencies associated with group relative advantage estimation. Experimental results on multiple mathematical reasoning benchmarks demonstrate the superior performance of AAPO.

**Link**: [arxiv](http://arxiv.org/abs/2505.14264v2),  [pdf](http://arxiv.org/pdf/2505.14264v2)

**Tags**: cs.LG cs.CL 



### Guided Reality: Generating Visually-Enriched AR Task Guidance with LLMs   and Vision Models
**Authors**: Ada Yi Zhao, Aditya Gunturu, Ellen Yi-Luen Do, Ryo Suzuki

**Updated**: 2025-09-24T15:41:44Z

**Summary**: Large language models (LLMs) have enabled the automatic generation of step-by-step augmented reality (AR) instructions for a wide range of physical tasks. However, existing LLM-based AR guidance often lacks rich visual augmentations to effectively embed instructions into spatial context for a better user understanding. We present Guided Reality, a fully automated AR system that generates embedded and dynamic visual guidance based on step-by-step instructions. Our system integrates LLMs and vision models to: 1) generate multi-step instructions from user queries, 2) identify appropriate types of visual guidance, 3) extract spatial information about key interaction points in the real world, and 4) embed visual guidance in physical space to support task execution. Drawing from a corpus of user manuals, we define five categories of visual guidance and propose an identification strategy based on the current step. We evaluate the system through a user study (N=16), completing real-world tasks and exploring the system in the wild. Additionally, four instructors shared insights on how Guided Reality could be integrated into their training workflows.

**Link**: [arxiv](http://arxiv.org/abs/2508.03547v2),  [pdf](http://arxiv.org/pdf/2508.03547v2)

**Tags**: cs.HC 



### CANDLE: A Cross-Modal Agentic Knowledge Distillation Framework for   Interpretable Sarcopenia Diagnosis
**Authors**: Yuqi Jin, Zhenhao Shuai, Zihan Hu, Weiteng Zhang, Weihao Xie, Jianwei Shuai, Xian Shen, Zhen Feng

**Updated**: 2025-09-24T15:38:14Z

**Summary**: Background and Aims: Large language models (LLMs) have shown remarkable generalization and transfer capabilities by learning from vast corpora of text and web data. Their semantic representations allow cross-task knowledge transfer and reasoning, offering promising opportunities for data-scarce and heterogeneous domains such as clinical medicine. Yet, in diagnostic tasks like sarcopenia, major challenges remain: interpretability, transparency, and deployment efficiency. Traditional machine learning (TML) models provide stable performance and feature-level attribution, ensuring traceable and auditable decision logic, but lack semantic breadth. Conversely, LLMs enable flexible inference but often function as opaque predictors. Existing integration strategies remain shallow, rarely embedding the structured reasoning of TML into LLM inference. Methods: Using sarcopenia diagnosis as a case study, SHapley Additive exPlanations (SHAP) were extracted from a baseline XGBoost model and transformed into structured, LLM-compatible representations. An actor-critic reinforcement learning (RL) strategy guided the LLM to reason over these SHAP-based inputs, producing calibrated rationales and refined decision rules. The distilled reasoning was consolidated into a structured knowledge repository and deployed via retrieval-augmented generation (RAG) for case-based inference. Results: (Omitted here.) Conclusion: By coupling SHAP-derived statistical evidence with reinforcement-trained LLM reasoning, CANDLE mitigates the interpretability-performance trade-off, enhances predictive accuracy, and preserves high decision consistency. The framework offers a scalable approach to knowledge assetization of TML models, enabling interpretable, reproducible, and clinically aligned decision support in sarcopenia and potentially broader medical domains.

**Link**: [arxiv](http://arxiv.org/abs/2507.21179v2),  [pdf](http://arxiv.org/pdf/2507.21179v2)

**Tags**: cs.LG cs.AI 



### Indirect Statistical Inference with Guaranteed Necessity and Sufficiency
**Authors**: Z Zhang, X Hu, C Lu, T Liu

**Updated**: 2025-09-24T15:35:26Z

**Summary**: This paper develops a new framework for indirect statistical inference with guaranteed necessity and sufficiency, applicable to continuous random variables. We prove that when comparing exponentially transformed order statistics from an assumed distribution with those from simulated unit exponential samples, the ranked quotients exhibit distinct asymptotics: the left segment converges to a non-degenerate distribution, while the middle and right segments degenerate to one. This yields a necessary and sufficient condition in probability for two sequences of continuous random variables to follow the same distribution. Building on this, we propose an optimization criterion based on relative errors between ordered samples. The criterion achieves its minimum if and only if the assumed and true distributions coincide, providing a second necessary and sufficient condition in optimization. These dual NS properties, rare in the literature, establish a fundamentally stronger inference framework than existing methods. Unlike classical approaches based on absolute errors (e.g., Kolmogorov-Smirnov), NSE exploits relative errors to ensure faster convergence, requires only mild approximability of the cumulative distribution function, and provides both point and interval estimates. Simulations and real-data applications confirm NSE's superior performance in preserving distributional assumptions where traditional methods fail.

**Link**: [arxiv](http://arxiv.org/abs/2509.20249v1),  [pdf](http://arxiv.org/pdf/2509.20249v1)

**Tags**: math.ST stat.TH 



### Energy Use of AI Inference: Efficiency Pathways and Test-Time Compute
**Authors**: Felipe Oviedo, Fiodar Kazhamiaka, Esha Choukse, Allen Kim, Amy Luers, Melanie Nakagawa, Ricardo Bianchini, Juan M. Lavista Ferres

**Updated**: 2025-09-24T15:32:01Z

**Summary**: As AI inference scales to billions of queries and emerging reasoning and agentic workflows increase token demand, reliable estimates of per-query energy use are increasingly important for capacity planning, emissions accounting, and efficiency prioritization. Many public estimates are inconsistent and overstate energy use, because they extrapolate from limited benchmarks and fail to reflect efficiency gains achievable at scale. In this perspective, we introduce a bottom-up methodology to estimate the per-query energy of large-scale LLM systems based on token throughput. For models running on an H100 node under realistic workloads, GPU utilization and PUE constraints, we estimate a median energy per query of 0.34 Wh (IQR: 0.18-0.67) for frontier-scale models (>200 billion parameters). These results are consistent with measurements using production-scale configurations and show that non-production estimates and assumptions can overstate energy use by 4-20x. Extending to test-time scaling scenarios with 15x more tokens per typical query, the median energy rises 13x to 4.32 Wh, indicating that targeting efficiency in this regime will deliver the largest fleet-wide savings. We quantify achievable efficiency gains at the model, serving platform, and hardware levels, finding individual median reductions of 1.5-3.5x in energy per query, while combined advances can plausibly deliver 8-20x reductions. To illustrate the system-level impact, we estimate the baseline daily energy use of a deployment serving 1 billion queries to be 0.8 GWh/day. If 10% are long queries, demand could grow to 1.8 GWh/day. With targeted efficiency interventions, it falls to 0.9 GWh/day, similar to the energy footprint of web search at that scale. This echoes how data centers historically tempered energy growth through efficiency gains during the internet and cloud build-up.

**Link**: [arxiv](http://arxiv.org/abs/2509.20241v1),  [pdf](http://arxiv.org/pdf/2509.20241v1)

**Tags**: cs.LG cs.DC 



### Matter Dipole and Hubble Tension due to Large Wavelength Perturbations
**Authors**: Gopal Kashyap, Naveen K. Singh, Pankaj Jain

**Updated**: 2025-09-25T08:38:41Z

**Summary**: We theoretically analyze the dipole anisotropy observed in the quasar distribution from the CatWISE2020 catalog. The catalog data shows a peak around $z\approx 1$, suggesting the presence of a large-scale dipole component. We explore the possibility that this dipole could be driven by primordial density fluctuations from modes that were superhorizon at the time of CMB decoupling but have since entered the horizon and become subhorizon. In particular, we consider the impact of adiabatic modes with wave numbers $k$ in the range $(10^{-4} - 4 \times 10^{-3})~\mathrm{Mpc}^{-1} $, corresponding to wavelength scales of several Gpc. Such modes can create large-scale density variations, likely causing anisotropies in the distribution of matter and, as a result, affecting the number density of observed quasars. We find that these can lead to a significant contribution to the dipole for sources up to redshifts of about 1, but are unable to explain the observed dipole. We also demonstrate that a superhorizon curvature perturbations mode, with a comoving wavenumber $k\lesssim0.3H_0$ can lead to a significant enhancement in the locally inferred Hubble constant. This effect offers a viable explanation for the observed discrepancy between local and CMB inferred measurements of $H_0$.

**Link**: [arxiv](http://arxiv.org/abs/2504.14190v3),  [pdf](http://arxiv.org/pdf/2504.14190v3)

**Tags**: astro-ph.CO 



### Context-Masked Meta-Prompting for Privacy-Preserving LLM Adaptation in   Finance
**Authors**: Sayash Raaj Hiraou

**Updated**: 2025-09-24T15:30:03Z

**Summary**: The increasing reliance on Large Language Models (LLMs) in sensitive domains like finance necessitates robust methods for privacy preservation and regulatory compliance. This paper presents an iterative meta-prompting methodology designed to optimise hard prompts without exposing proprietary or confidential context to the LLM. Through a novel regeneration process involving feeder and propagation methods, we demonstrate significant improvements in prompt efficacy. Evaluated on public datasets serving as proxies for financial tasks such as SQuAD for extractive financial Q&A, CNN/DailyMail for news summarisation, and SAMSum for client interaction summarisation, our approach, utilising GPT-3.5 Turbo, achieved a 103.87% improvement in ROUGE-L F1 for question answering. This work highlights a practical, low-cost strategy for adapting LLMs to financial applications while upholding critical privacy and auditability standards, offering a compelling case for its relevance in the evolving landscape of generative AI in finance.

**Link**: [arxiv](http://arxiv.org/abs/2407.18920v2),  [pdf](http://arxiv.org/pdf/2407.18920v2)

**Tags**: cs.CL 



### Velocity model building from seismic images using a Convolutional Neural   Operator
**Authors**: Xiao Ma, Tariq Alkhalifah

**Updated**: 2025-09-24T15:28:32Z

**Summary**: The success of building a high-resolution velocity model using machine learning is hampered by generalization limitations that often limit the success of the approach on field data. This is especially true when relying on neural operators for the mapping. Thus, we propose a novel inversion framework that relies on learning to map the velocity model to a seismic image using a Convolutional Neural Operator (CNO), and then we use optimization to invert for the velocity that matches the image. The key to the success of our network is that we use the initial and true velocity models as input in the training, then we invert for the true velocity starting from the initial velocity at inference. Specifically, we first train a neural operator to accurately learn the forward mapping from seismic velocity models to RTM images, using synthetic datasets that include high-frequency structural information. Once trained, the neural operator is embedded into an inversion loop, where its differentiable nature enables efficient gradient computation via automatic differentiation. This allows us to progressively inject high-wavenumber information from RTM images into the background velocity model, thereby improving resolution without the need for traditional adjoint-state solvers. The proposed framework is validated on both synthetic and field data. Results demonstrate that the neural operator generalizes well to real seismic scenarios, maintains high inversion accuracy, and significantly reduces computational cost. This work highlights the potential of neural operators as flexible and scalable tools for efficient, data-driven seismic imaging and inversion.

**Link**: [arxiv](http://arxiv.org/abs/2509.20238v1),  [pdf](http://arxiv.org/pdf/2509.20238v1)

**Tags**: physics.geo-ph 



### $S_8$ from Tully-Fisher, fundamental plane, and supernova distances   agree with Planck
**Authors**: Richard Stiskalek

**Updated**: 2025-09-24T15:25:11Z

**Summary**: Peculiar velocity measurements constrain the parameter combination $f\sigma_8$, the product of the linear growth rate $f$ and the fluctuation amplitude $\sigma_8$. Under the approximation that $f$ is a monotonic function of $\Omega_{\rm m}$, this can be related to $S_8 \equiv \sigma_8 \sqrt{\Omega_{\rm m}/0.3}$, enabling direct comparison with weak lensing and cosmic microwave background results. We exploit this by using three classes of direct-distance tracers -- the Tully-Fisher relation, the fundamental plane, and Type~Ia supernovae -- to infer peculiar velocities. A unified hierarchical forward model jointly calibrates each distance indicator and a linear theory reconstruction of the local Universe. This is the first consistent Bayesian analysis to combine all three major classes of distance indicators within a common framework, enabling cross-checks of systematics across diverse galaxy populations. All three tracers yield consistent values of $S_8$ that are also in agreement with Planck. Our joint constraint is $S_8 = 0.819 \pm 0.030$, with the uncertainty dominated by the 2M++ galaxy field. These results demonstrate that peculiar velocity surveys provide a robust, consistent measurement of $S_8$, and support concordance with the cosmic microwave background.

**Link**: [arxiv](http://arxiv.org/abs/2509.20235v1),  [pdf](http://arxiv.org/pdf/2509.20235v1)

**Tags**: astro-ph.CO 



### Long Video Understanding with Learnable Retrieval in Video-Language   Models
**Authors**: Jiaqi Xu, Cuiling Lan, Wenxuan Xie, Xuejin Chen, Yan Lu

**Updated**: 2025-09-24T15:24:43Z

**Summary**: The remarkable natural language understanding, reasoning, and generation capabilities of large language models (LLMs) have made them attractive for application to video understanding, utilizing video tokens as contextual input. However, employing LLMs for long video understanding presents significant challenges. The extensive number of video tokens leads to considerable computational costs for LLMs while using aggregated tokens results in loss of vision details. Moreover, the presence of abundant question-irrelevant tokens introduces noise to the video reasoning process. To address these issues, we introduce a simple yet effective learnable retrieval-based video-language model (R-VLM) for efficient long video understanding. Specifically, given a question (query) and a long video, our model identifies and selects the most relevant K video chunks and uses their associated visual tokens to serve as context for the LLM inference. This effectively reduces the number of video tokens, eliminates noise interference, and enhances system performance. We achieve this by incorporating a learnable lightweight MLP block to facilitate the efficient retrieval of question-relevant chunks, through the end-to-end training of our video-language model with a proposed soft matching loss. Our experimental results on multiple zero-shot video question answering datasets validate the effectiveness of our framework for comprehending long videos.

**Link**: [arxiv](http://arxiv.org/abs/2312.04931v3),  [pdf](http://arxiv.org/pdf/2312.04931v3)

**Tags**: cs.CV 



### Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided   Multi-Point Optimization
**Authors**: Wenhan Wu, Zheyuan Liu, Chongyang Gao, Ren Wang, Kaize Ding

**Updated**: 2025-09-24T15:23:46Z

**Summary**: Current LLM unlearning methods face a critical security vulnerability that undermines their fundamental purpose: while they appear to successfully remove sensitive or harmful knowledge, this ``forgotten" information remains precariously recoverable through relearning attacks. We identify that the root cause is that conventional methods optimizing the forgetting loss at individual data points will drive model parameters toward sharp minima in the loss landscape. In these unstable regions, even minimal parameter perturbations can drastically alter the model's behaviors. Consequently, relearning attacks exploit this vulnerability by using just a few fine-tuning samples to navigate the steep gradients surrounding these unstable regions, thereby rapidly recovering knowledge that was supposedly erased. This exposes a critical robustness gap between apparent unlearning and actual knowledge removal. To address this issue, we propose StableUN, a bi-level feedback-guided optimization framework that explicitly seeks more stable parameter regions via neighborhood-aware optimization. It integrates forgetting feedback, which uses adversarial perturbations to probe parameter neighborhoods, with remembering feedback to preserve model utility, aligning the two objectives through gradient projection. Experiments on WMDP and MUSE benchmarks demonstrate that our method is significantly more robust against both relearning and jailbreaking attacks while maintaining competitive utility performance.

**Link**: [arxiv](http://arxiv.org/abs/2509.20230v1),  [pdf](http://arxiv.org/pdf/2509.20230v1)

**Tags**: cs.LG cs.AI 



### Latent Wavelet Diffusion For Ultra-High-Resolution Image Synthesis
**Authors**: Luigi Sigillo, Shengfeng He, Danilo Comminiello

**Updated**: 2025-09-24T15:22:22Z

**Summary**: High-resolution image synthesis remains a core challenge in generative modeling, particularly in balancing computational efficiency with the preservation of fine-grained visual detail. We present Latent Wavelet Diffusion (LWD), a lightweight training framework that significantly improves detail and texture fidelity in ultra-high-resolution (2K-4K) image synthesis. LWD introduces a novel, frequency-aware masking strategy derived from wavelet energy maps, which dynamically focuses the training process on detail-rich regions of the latent space. This is complemented by a scale-consistent VAE objective to ensure high spectral fidelity. The primary advantage of our approach is its efficiency: LWD requires no architectural modifications and adds zero additional cost during inference, making it a practical solution for scaling existing models. Across multiple strong baselines, LWD consistently improves perceptual quality and FID scores, demonstrating the power of signal-driven supervision as a principled and efficient path toward high-resolution generative modeling.

**Link**: [arxiv](http://arxiv.org/abs/2506.00433v3),  [pdf](http://arxiv.org/pdf/2506.00433v3)

**Tags**: cs.CV cs.LG eess.IV 



### An Empirical Analysis of Secure Federated Learning for Autonomous   Vehicle Applications
**Authors**: Md Jueal Mia, M. Hadi Amini

**Updated**: 2025-09-24T15:16:09Z

**Summary**: Federated Learning lends itself as a promising paradigm in enabling distributed learning for autonomous vehicles applications and ensuring data privacy while enhancing and refining predictive model performance through collaborative training on edge client vehicles. However, it remains vulnerable to various categories of cyber-attacks, necessitating more robust security measures to effectively mitigate potential threats. Poisoning attacks and inference attacks are commonly initiated within the federated learning environment to compromise secure system performance. Secure aggregation can limit the disclosure of sensitive information from outsider and insider attackers of the federated learning environment. In this study, our aim is to conduct an empirical analysis on the transportation image dataset (e.g., LISA traffic light) using various secure aggregation techniques and multiparty computation in the presence of diverse categories of cyber-attacks. Multiparty computation serves as a state-of-the-art security mechanism, offering standard privacy for secure aggregation of edge autonomous vehicles local model updates through various security protocols. The presence of adversaries can mislead the autonomous vehicle learning model, leading to the misclassification of traffic lights, and resulting in detrimental impacts. This empirical study explores the resilience of various secure federated learning aggregation techniques and multiparty computation in safeguarding autonomous vehicle applications against various cyber threats during both training and inference times.

**Link**: [arxiv](http://arxiv.org/abs/2509.20223v1),  [pdf](http://arxiv.org/pdf/2509.20223v1)

**Tags**: cs.DC 



### The Cream Rises to the Top: Efficient Reranking Method for Verilog Code   Generation
**Authors**: Guang Yang, Wei Zheng, Xiang Chen, Yifan Sun, Fengji Zhang, Terry Yue Zhuo

**Updated**: 2025-09-24T15:12:21Z

**Summary**: LLMs face significant challenges in Verilog generation due to limited domain-specific knowledge. While sampling techniques improve pass@k metrics, hardware engineers need one trustworthy solution rather than uncertain candidates. To bridge this gap, we formulate it as a semantic alignment problem between requirements and Verilog implementations, and propose VCD-RNK, a discriminator model tailored for efficient Verilog code reranking. Specifically, VCD-RNKincorporates Verilog-specific reasoning by distilling expert knowledge across three dimensions: code semantic analysis, test case generation, and functional correctness assessment. By explicitly simulating the above reasoning processes during inference, VCD-RNK effectively avoids computationally intensive test execution in existing methods.

**Link**: [arxiv](http://arxiv.org/abs/2509.20215v1),  [pdf](http://arxiv.org/pdf/2509.20215v1)

**Tags**: cs.SE cs.AI cs.AR 



### Q-Palette: Fractional-Bit Quantizers Toward Optimal Bit Allocation for   Efficient LLM Deployment
**Authors**: Deokjae Lee, Hyun Oh Song

**Updated**: 2025-09-24T15:10:44Z

**Summary**: We study weight-only post-training quantization (PTQ), which quantizes the weights of a large language model (LLM) without retraining, using little or no calibration data. Weight-only PTQ is crucial for reducing the memory footprint and latency of LLM inference, especially in memory-bound, small-batch inference scenarios, such as personalized inference on edge devices. Despite its importance, irregular weight distributions with heavy-tailed outliers in LLMs complicate quantization, recently motivating rotation-based methods that transform weights into near-Gaussian distributions, which are more regular with fewer outliers, thereby reducing quantization error. In this work, we first derive the information-theoretically optimal bit allocation for Gaussianized weights under given bit budgets, revealing that fine-grained fractional-bit quantizers approaching the Gaussian distortion-rate bound are essential to achieve near-optimal quantization performance. To bridge this theoretical insight and practical implementation, we introduce Q-Palette, a versatile collection of fractional-bit quantizers that range from trellis-coded quantizers offering near-optimal distortion to simpler vector and scalar quantizers optimized for faster inference, all efficiently implemented with optimized CUDA kernels across various bitwidths. Furthermore, leveraging Q-Palette as a foundational component, we propose a novel mixed-scheme quantization framework, jointly optimizing quantizer choices and layer fusion decisions given resource constraints. The code is available at https://github.com/snu-mllab/Q-Palette.

**Link**: [arxiv](http://arxiv.org/abs/2509.20214v1),  [pdf](http://arxiv.org/pdf/2509.20214v1)

**Tags**: cs.LG cs.AI 



### Practical do-Shapley Explanations with Estimand-Agnostic Causal   Inference
**Authors**: Álvaro Parafita, Tomas Garriga, Axel Brando, Francisco J. Cazorla

**Updated**: 2025-09-24T15:04:25Z

**Summary**: Among explainability techniques, SHAP stands out as one of the most popular, but often overlooks the causal structure of the problem. In response, do-SHAP employs interventional queries, but its reliance on estimands hinders its practical application. To address this problem, we propose the use of estimand-agnostic approaches, which allow for the estimation of any identifiable query from a single model, making do-SHAP feasible on complex graphs. We also develop a novel algorithm to significantly accelerate its computation at a negligible cost, as well as a method to explain inaccessible Data Generating Processes. We demonstrate the estimation and computational performance of our approach, and validate it on two real-world datasets, highlighting its potential in obtaining reliable explanations.

**Link**: [arxiv](http://arxiv.org/abs/2509.20211v1),  [pdf](http://arxiv.org/pdf/2509.20211v1)

**Tags**: cs.LG 



### Play by the Type Rules: Inferring Constraints for LLM Functions in   Declarative Programs
**Authors**: Parker Glenn, Alfy Samuel, Daben Liu

**Updated**: 2025-09-24T15:02:33Z

**Summary**: Integrating LLM powered operators in declarative query languages allows for the combination of cheap and interpretable functions with powerful, generalizable language model reasoning. However, in order to benefit from the optimized execution of a database query language like SQL, generated outputs must align with the rules enforced by both type checkers and database contents. Current approaches address this challenge with orchestrations consisting of many LLM-based post-processing calls to ensure alignment between generated outputs and database values, introducing performance bottlenecks. We perform a study on the ability of various sized open-source language models to both parse and execute functions within a query language based on SQL, showing that small language models can excel as function executors over hybrid data sources. Then, we propose an efficient solution to enforce the well-typedness of LLM functions, demonstrating 7% accuracy improvement on a multi-hop question answering dataset with 53% improvement in latency over comparable solutions. We make our implementation available at https://github.com/parkervg/blendsql

**Link**: [arxiv](http://arxiv.org/abs/2509.20208v1),  [pdf](http://arxiv.org/pdf/2509.20208v1)

**Tags**: cs.CL cs.AI cs.DB 



### Fulcrum: Optimizing Concurrent DNN Training and Inferencing on Edge   Accelerators
**Authors**: Prashanthi S. K., Saisamarth Taluri, Pranav Gupta, Amartya Ranjan Saikia, Kunal Kumar Sahoo, Atharva Vinay Joshi, Lakshya Karwa, Kedar Dhule, Yogesh Simmhan

**Updated**: 2025-09-24T15:01:25Z

**Summary**: The proliferation of GPU accelerated edge devices like Nvidia Jetsons and the rise in privacy concerns are placing an emphasis on concurrent DNN training and inferencing on edge devices. Inference and training have different computing and QoS goals. But edge accelerators like Jetson do not support native GPU sharing and expose 1000s of power modes. This requires careful time-sharing of concurrent workloads to meet power--performance goals, while limiting costly profiling. In this paper, we design an intelligent time-slicing approach for concurrent DNN training and inferencing on Jetsons. We formulate an optimization problem to interleave training and inferencing minibatches, and decide the device power mode and inference minibatch size, while maximizing the training throughput and staying within latency and power budgets, with modest profiling costs. We propose GMD, an efficient multi-dimensional gradient descent search which profiles just $15$ power modes; and ALS, an Active Learning technique which identifies reusable Pareto-optimal power modes, but profiles $50$--$150$ power modes. We evaluate these within our Fulcrum scheduler for $273,000+$ configurations across $15$ DNN workloads. We also evaluate our strategies on dynamic arrival inference and concurrent inferences. ALS and GMD outperform simpler and more complex baselines with larger-scale profiling. Their solutions satisfy the latency and power budget for $>97\%$ of our runs, and on average are within $7\%$ of the optimal throughput.

**Link**: [arxiv](http://arxiv.org/abs/2509.20205v1),  [pdf](http://arxiv.org/pdf/2509.20205v1)

**Tags**: cs.DC 



### LLMs Reproduce Stereotypes of Sexual and Gender Minorities
**Authors**: Ruby Ostrow, Adam Lopez

**Updated**: 2025-09-24T14:58:32Z

**Summary**: A large body of research has found substantial gender bias in NLP systems. Most of this research takes a binary, essentialist view of gender: limiting its variation to the categories _men_ and _women_, conflating gender with sex, and ignoring different sexual identities. But gender and sexuality exist on a spectrum, so in this paper we study the biases of large language models (LLMs) towards sexual and gender minorities beyond binary categories. Grounding our study in a widely used social psychology model -- the Stereotype Content Model -- we demonstrate that English-language survey questions about social perceptions elicit more negative stereotypes of sexual and gender minorities from both humans and LLMs. We then extend this framework to a more realistic use case: text generation. Our analysis shows that LLMs generate stereotyped representations of sexual and gender minorities in this setting, showing that they amplify representational harms in creative writing, a widely advertised use for LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2501.05926v3),  [pdf](http://arxiv.org/pdf/2501.05926v3)

**Tags**: cs.CL 



### Procedural Environment Generation for Tool-Use Agents
**Authors**: Michael Sullivan, Mareike Hartmann, Alexander Koller

**Updated**: 2025-09-24T14:57:25Z

**Summary**: Although the power of LLM tool-use agents has ignited a flurry of recent research in this area, the curation of tool-use training data remains an open problem$-$especially for online RL training. Existing approaches to synthetic tool-use data generation tend to be non-interactive, and/or non-compositional. We introduce RandomWorld, a pipeline for the procedural generation of interactive tools and compositional tool-use data. We show that models tuned via SFT and RL on synthetic RandomWorld data improve on a range of tool-use benchmarks, and set the new SoTA for two metrics on the NESTFUL dataset. Further experiments show that downstream performance scales with the amount of RandomWorld-generated training data, opening up the possibility of further improvement through the use of entirely synthetic data.

**Link**: [arxiv](http://arxiv.org/abs/2506.11045v2),  [pdf](http://arxiv.org/pdf/2506.11045v2)

**Tags**: cs.LG I.2.7 



### Identification and Semiparametric Estimation of Conditional Means from   Aggregate Data
**Authors**: Cory McCartan, Shiro Kuriwaki

**Updated**: 2025-09-24T14:49:08Z

**Summary**: We introduce a new method for estimating the mean of an outcome variable within groups when researchers only observe the average of the outcome and group indicators across a set of aggregation units, such as geographical areas. Existing methods for this problem, also known as ecological inference, implicitly make strong assumptions about the aggregation process. We first formalize weaker conditions for identification, which motivates estimators that can efficiently control for many covariates. We propose a debiased machine learning estimator that is based on nuisance functions restricted to a partially linear form. Our estimator also admits a semiparametric sensitivity analysis for violations of the key identifying assumption, as well as asymptotically valid confidence intervals for local, unit-level estimates under additional assumptions. Simulations and validation on real-world data where ground truth is available demonstrate the advantages of our approach over existing methods. Open-source software is available which implements the proposed methods.

**Link**: [arxiv](http://arxiv.org/abs/2509.20194v1),  [pdf](http://arxiv.org/pdf/2509.20194v1)

**Tags**: stat.ME econ.EM 



### Detecting Token-Level Hallucinations Using Variance Signals: A   Reference-Free Approach
**Authors**: Keshav Kumar

**Updated**: 2025-09-24T14:48:30Z

**Summary**: Large Language Models (LLMs) have demonstrated impressive generative capabilities across diverse tasks but remain susceptible to hallucinations, confidently generated yet factually incorrect outputs. We introduce a reference-free, token-level hallucination detection framework that leverages the variance in token log-probabilities across multiple stochastic generations. Unlike prior methods that require ground-truth references or sentence-level verification, our approach is model-agnostic, interpretable, and suited for real-time or post-hoc analysis. We evaluate our method on unanswerable question prompts from the SQuAD v2 dataset and benchmark across three autoregressive models of varying scales: GPT-Neo 125M, Falcon 1B, and Mistral 7B. Through both quantitative metrics and visual diagnostics, we show that token-level variance reliably highlights instability in model outputs and correlates with hallucination patterns. Our framework is lightweight, reproducible, and adaptable to multiple domains, offering a valuable diagnostic tool for analyzing generative reliability in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2507.04137v2),  [pdf](http://arxiv.org/pdf/2507.04137v2)

**Tags**: cs.CL cs.LG 



### STAF: Leveraging LLMs for Automated Attack Tree-Based Security Test   Generation
**Authors**: Tanmay Khule, Stefan Marksteiner, Jose Alguindigue, Hannes Fuchs, Sebastian Fischmeister, Apurva Narayan

**Updated**: 2025-09-24T14:46:42Z

**Summary**: In modern automotive development, security testing is critical for safeguarding systems against increasingly advanced threats. Attack trees are widely used to systematically represent potential attack vectors, but generating comprehensive test cases from these trees remains a labor-intensive, error-prone task that has seen limited automation in the context of testing vehicular systems. This paper introduces STAF (Security Test Automation Framework), a novel approach to automating security test case generation. Leveraging Large Language Models (LLMs) and a four-step self-corrective Retrieval-Augmented Generation (RAG) framework, STAF automates the generation of executable security test cases from attack trees, providing an end-to-end solution that encompasses the entire attack surface. We particularly show the elements and processes needed to provide an LLM to actually produce sensible and executable automotive security test suites, along with the integration with an automated testing framework. We further compare our tailored approach with general purpose (vanilla) LLMs and the performance of different LLMs (namely GPT-4.1 and DeepSeek) using our approach. We also demonstrate the method of our operation step-by-step in a concrete case study. Our results show significant improvements in efficiency, accuracy, scalability, and easy integration in any workflow, marking a substantial advancement in automating automotive security testing methodologies. Using TARAs as an input for verfication tests, we create synergies by connecting two vital elements of a secure automotive development process.

**Link**: [arxiv](http://arxiv.org/abs/2509.20190v1),  [pdf](http://arxiv.org/pdf/2509.20190v1)

**Tags**: cs.CR cs.AI 



### Pagoda: An Energy and Time Roofline Study for DNN Workloads on Edge   Accelerators
**Authors**: Prashanthi S. K., Kunal Kumar Sahoo, Amartya Ranjan Saikia, Pranav Gupta, Atharva Vinay Joshi, Priyanshu Pansari, Yogesh Simmhan

**Updated**: 2025-09-24T14:46:07Z

**Summary**: Edge accelerators such as Nvidia Jetsons are becoming an integral part of the computing continuum, and are often used for DNN inferencing and training. Nvidia Jetson edge devices have $2000$+ CUDA cores within a $70$W power envelope and offer $1000$s of power modes to customize CPU, GPU and memory frequencies. Their widely varying power--performance trade-offs can be exploited for energy and power-constrained deployments. While data-driven methods to predict the power and latency of DNN workloads for edge devices exist, there is a lack of principled study to understand why edge accelerators and their power modes perform the way they do. We develop a time roofline and a novel energy roofline model for the Jetson Orin AGX for diverse power modes, and couple it with an analytical model of the compute (FLOP) and memory access (bytes) for DNN inference workloads to analyze them from first principles. These reveal unique, sometimes counter-intuitive, insights into the power and performance behavior of DNN workloads on edge accelerators, e.g., the default power mode MAXN is not the most energy efficient and time efficiency implies energy efficiency for all power modes. We also extend our analytical roofline models to DNN training. Finally, we apply these methods to tune the power mode (and hence the roofline) of the edge device to optimize the latency and energy for DNN inference, with up to $15\%$ lower energy and minimal degradation in inference time.

**Link**: [arxiv](http://arxiv.org/abs/2509.20189v1),  [pdf](http://arxiv.org/pdf/2509.20189v1)

**Tags**: cs.DC 



### Thinking Augmented Pre-training
**Authors**: Liang Wang, Nan Yang, Shaohan Huang, Li Dong, Furu Wei

**Updated**: 2025-09-25T10:55:02Z

**Summary**: This paper introduces a simple and scalable approach to improve the data efficiency of large language model (LLM) training by augmenting existing text data with thinking trajectories. The compute for pre-training LLMs has been growing at an unprecedented rate, while the availability of high-quality data remains limited. Consequently, maximizing the utility of available data constitutes a significant research challenge. A primary impediment is that certain high-quality tokens are difficult to learn given a fixed model capacity, as the underlying rationale for a single token can be exceptionally complex and deep. To address this issue, we propose Thinking augmented Pre-Training (TPT), a universal methodology that augments text with automatically generated thinking trajectories. Such augmentation effectively increases the volume of the training data and makes high-quality tokens more learnable through step-by-step reasoning and decomposition. We apply TPT across diverse training configurations up to $100$B tokens, encompassing pre-training with both constrained and abundant data, as well as mid-training from strong open-source checkpoints. Experimental results indicate that our method substantially improves the performance of LLMs across various model sizes and families. Notably, TPT enhances the data efficiency of LLM pre-training by a factor of $3$. For a $3$B parameter model, it improves the post-training performance by over $10\%$ on several challenging reasoning benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2509.20186v2),  [pdf](http://arxiv.org/pdf/2509.20186v2)

**Tags**: cs.CL cs.LG 



### Automated Multi-Agent Workflows for RTL Design
**Authors**: Amulya Bhattaram, Janani Ramamoorthy, Ranit Gupta, Diana Marculescu, Dimitrios Stamoulis

**Updated**: 2025-09-24T14:44:28Z

**Summary**: The rise of agentic AI workflows unlocks novel opportunities for computer systems design and optimization. However, for specialized domains such as program synthesis, the relative scarcity of HDL and proprietary EDA resources online compared to more common programming tasks introduces challenges, often necessitating task-specific fine-tuning, high inference costs, and manually-crafted agent orchestration. In this work, we present VeriMaAS, a multi-agent framework designed to automatically compose agentic workflows for RTL code generation. Our key insight is to integrate formal verification feedback from HDL tools directly into workflow generation, reducing the cost of gradient-based updates or prolonged reasoning traces. Our method improves synthesis performance by 5-7% for pass@k over fine-tuned baselines, while requiring only a few hundred training examples, representing an order-of-magnitude reduction in supervision cost.

**Link**: [arxiv](http://arxiv.org/abs/2509.20182v1),  [pdf](http://arxiv.org/pdf/2509.20182v1)

**Tags**: cs.AR cs.AI 



### Benchmarking Web API Integration Code Generation
**Authors**: Daniel Maninger, Leon Chemnitz, Amir Molzam Sharifloo, Jannis Brugger, Mira Mezini

**Updated**: 2025-09-24T14:36:44Z

**Summary**: API integration is a cornerstone of our digital infrastructure, enabling software systems to connect and interact. However, as shown by many studies, writing or generating correct code to invoke APIs, particularly web APIs, is challenging. Although large language models~(LLMs) have become popular in software development, their effectiveness in automating the generation of web API integration code remains unexplored. In order to address this, we present a dataset and evaluation pipeline designed to assess the ability of LLMs to generate web API invocation code. Our experiments with several open-source LLMs reveal that generating API invocations poses a significant challenge, resulting in hallucinated endpoints, incorrect argument usage, and other errors. None of the evaluated open-source models were able to solve more than 40% of the tasks.

**Link**: [arxiv](http://arxiv.org/abs/2509.20172v1),  [pdf](http://arxiv.org/pdf/2509.20172v1)

**Tags**: cs.SE cs.LG 



### Probing Gender Bias in Multilingual LLMs: A Case Study of Stereotypes in   Persian
**Authors**: Ghazal Kalhor, Behnam Bahrak

**Updated**: 2025-09-24T14:34:17Z

**Summary**: Multilingual Large Language Models (LLMs) are increasingly used worldwide, making it essential to ensure they are free from gender bias to prevent representational harm. While prior studies have examined such biases in high-resource languages, low-resource languages remain understudied. In this paper, we propose a template-based probing methodology, validated against real-world data, to uncover gender stereotypes in LLMs. As part of this framework, we introduce the Domain-Specific Gender Skew Index (DS-GSI), a metric that quantifies deviations from gender parity. We evaluate four prominent models, GPT-4o mini, DeepSeek R1, Gemini 2.0 Flash, and Qwen QwQ 32B, across four semantic domains, focusing on Persian, a low-resource language with distinct linguistic features. Our results show that all models exhibit gender stereotypes, with greater disparities in Persian than in English across all domains. Among these, sports reflect the most rigid gender biases. This study underscores the need for inclusive NLP practices and provides a framework for assessing bias in other low-resource languages.

**Link**: [arxiv](http://arxiv.org/abs/2509.20168v1),  [pdf](http://arxiv.org/pdf/2509.20168v1)

**Tags**: cs.CL 



### CyberSOCEval: Benchmarking LLMs Capabilities for Malware Analysis and   Threat Intelligence Reasoning
**Authors**: Lauren Deason, Adam Bali, Ciprian Bejean, Diana Bolocan, James Crnkovich, Ioana Croitoru, Krishna Durai, Chase Midler, Calin Miron, David Molnar, Brad Moon, Bruno Ostarcevic, Alberto Peltea, Matt Rosenberg, Catalin Sandu, Arthur Saputkin, Sagar Shah, Daniel Stan, Ernest Szocs, Shengye Wan, Spencer Whitman, Sven Krasser, Joshua Saxe

**Updated**: 2025-09-24T14:33:07Z

**Summary**: Today's cyber defenders are overwhelmed by a deluge of security alerts, threat intelligence signals, and shifting business context, creating an urgent need for AI systems to enhance operational security work. While Large Language Models (LLMs) have the potential to automate and scale Security Operations Center (SOC) operations, existing evaluations do not fully assess the scenarios most relevant to real-world defenders. This lack of informed evaluation impacts both AI developers and those applying LLMs to SOC automation. Without clear insight into LLM performance in real-world security scenarios, developers lack a north star for development, and users cannot reliably select the most effective models. Meanwhile, malicious actors are using AI to scale cyber attacks, highlighting the need for open source benchmarks to drive adoption and community-driven improvement among defenders and model developers. To address this, we introduce CyberSOCEval, a new suite of open source benchmarks within CyberSecEval 4. CyberSOCEval includes benchmarks tailored to evaluate LLMs in two tasks: Malware Analysis and Threat Intelligence Reasoning--core defensive domains with inadequate coverage in current benchmarks. Our evaluations show that larger, more modern LLMs tend to perform better, confirming the training scaling laws paradigm. We also find that reasoning models leveraging test time scaling do not achieve the same boost as in coding and math, suggesting these models have not been trained to reason about cybersecurity analysis, and pointing to a key opportunity for improvement. Finally, current LLMs are far from saturating our evaluations, showing that CyberSOCEval presents a significant challenge for AI developers to improve cyber defense capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2509.20166v1),  [pdf](http://arxiv.org/pdf/2509.20166v1)

**Tags**: cs.CR cs.AI 



### Embedding Domain Knowledge for Large Language Models via Reinforcement   Learning from Augmented Generation
**Authors**: Chaojun Nie, Jun Zhou, Guanxiang Wang, Shisong Wud, Zichen Wang

**Updated**: 2025-09-24T14:30:16Z

**Summary**: Large language models (LLMs) often exhibit limited performance on domain-specific tasks due to the natural disproportionate representation of specialized information in their training data and the static nature of these datasets. Knowledge scarcity and temporal lag create knowledge gaps for domain applications. While post-training on domain datasets can embed knowledge into models, existing approaches have some limitations. Continual Pre-Training (CPT) treats all tokens in domain documents with equal importance, failing to prioritize critical knowledge points, while supervised fine-tuning (SFT) with question-answer pairs struggles to develop the coherent knowledge structures necessary for complex reasoning tasks. To address these challenges, we propose Reinforcement Learning from Augmented Generation (RLAG). Our approach iteratively cycles between sampling generations and optimizing the model through calculated rewards, effectively embedding critical and contextually coherent domain knowledge. We select generated outputs with the highest log probabilities as the sampling result, then compute three tailored reward metrics to guide the optimization process. To comprehensively evaluate domain expertise, we assess answer accuracy and the rationality of explanations generated for correctly answered questions. Experimental results across medical, legal, astronomy, and current events datasets demonstrate that our proposed method significantly outperforms baseline approaches. Our code and data are open sourced at https://github.com/ChaojunNie/RLAG.

**Link**: [arxiv](http://arxiv.org/abs/2509.20162v1),  [pdf](http://arxiv.org/pdf/2509.20162v1)

**Tags**: cs.CL cs.AI 



### HoT: Highlighted Chain of Thought for Referencing Supporting Facts from   Inputs
**Authors**: Tin Nguyen, Logan Bolton, Mohammad Reza Taesiri, Trung Bui, Anh Totti Nguyen

**Updated**: 2025-09-24T14:26:13Z

**Summary**: An Achilles heel of Large Language Models (LLMs) is their tendency to hallucinate non-factual statements. A response mixed of factual and non-factual statements poses a challenge for humans to verify and accurately base their decisions on. To combat this problem, we propose Highlighted Chain-of-Thought Prompting (HoT), a technique for prompting LLMs to generate responses with XML tags that ground facts to those provided in the query. That is, given an input question, LLMs would first re-format the question to add XML tags highlighting key facts, and then, generate a response with highlights over the facts referenced from the input. Interestingly, in few-shot settings, HoT outperforms vanilla chain of thought prompting (CoT) on a wide range of 17 tasks from arithmetic, reading comprehension to logical reasoning. When asking humans to verify LLM responses, highlights help time-limited participants to more accurately and efficiently recognize when LLMs are correct. Yet, surprisingly, when LLMs are wrong, HoTs tend to make users believe that an answer is correct.

**Link**: [arxiv](http://arxiv.org/abs/2503.02003v4),  [pdf](http://arxiv.org/pdf/2503.02003v4)

**Tags**: cs.CL cs.HC 



### Learning hidden cascades via classification
**Authors**: Derrick Gilchrist Edward Manoharan, Anubha Goel, Alexandros Iosifidis, Henri Hansen, Juho Kanniainen

**Updated**: 2025-09-24T14:22:54Z

**Summary**: The spreading dynamics in social networks are often studied under the assumption that individuals' statuses, whether informed or infected, are fully observable. However, in many real-world situations, such statuses remain unobservable, which is crucial for determining an individual's potential to further spread the infection. While final statuses are hidden, intermediate indicators such as symptoms of infection are observable and provide useful representations of the underlying diffusion process. We propose a partial observability-aware Machine Learning framework to learn the characteristics of the spreading model. We term the method Distribution Classification, which utilizes the power of classifiers to infer the underlying transmission dynamics. Through extensive benchmarking against Approximate Bayesian Computation and GNN-based baselines, our framework consistently outperforms these state-of-the-art methods, delivering accurate parameter estimates across diverse diffusion settings while scaling efficiently to large networks. We validate the method on synthetic networks and extend the study to a real-world insider trading network, demonstrating its effectiveness in analyzing spreading phenomena where direct observation of individual statuses is not possible.

**Link**: [arxiv](http://arxiv.org/abs/2505.11228v3),  [pdf](http://arxiv.org/pdf/2505.11228v3)

**Tags**: cs.SI cs.LG 



### Affective Computing and Emotional Data: Challenges and Implications in   Privacy Regulations, The AI Act, and Ethics in Large Language Models
**Authors**: Nicola Fabiano

**Updated**: 2025-09-25T10:43:22Z

**Summary**: This paper examines the integration of emotional intelligence into artificial intelligence systems, with a focus on affective computing and the growing capabilities of Large Language Models (LLMs), such as ChatGPT and Claude, to recognize and respond to human emotions. Drawing on interdisciplinary research that combines computer science, psychology, and neuroscience, the study analyzes foundational neural architectures - CNNs for processing facial expressions and RNNs for sequential data, such as speech and text - that enable emotion recognition. It examines the transformation of human emotional experiences into structured emotional data, addressing the distinction between explicit emotional data collected with informed consent in research settings and implicit data gathered passively through everyday digital interactions. That raises critical concerns about lawful processing, AI transparency, and individual autonomy over emotional expressions in digital environments. The paper explores implications across various domains, including healthcare, education, and customer service, while addressing challenges of cultural variations in emotional expression and potential biases in emotion recognition systems across different demographic groups. From a regulatory perspective, the paper examines emotional data in the context of the GDPR and the EU AI Act frameworks, highlighting how emotional data may be considered sensitive personal data that requires robust safeguards, including purpose limitation, data minimization, and meaningful consent mechanisms.

**Link**: [arxiv](http://arxiv.org/abs/2509.20153v2),  [pdf](http://arxiv.org/pdf/2509.20153v2)

**Tags**: cs.CY cs.AI 



### PathGene: Benchmarking Driver Gene Mutations and Exon Prediction Using   Multicenter Lung Cancer Histopathology Image Dataset
**Authors**: Liangrui Pan, Qingchun Liang, Shen Zhao, Songqing Fan, Shaoliang Peng

**Updated**: 2025-09-24T14:16:30Z

**Summary**: Accurately predicting gene mutations, mutation subtypes and their exons in lung cancer is critical for personalized treatment planning and prognostic assessment. Faced with regional disparities in medical resources and the high cost of genomic assays, using artificial intelligence to infer these mutations and exon variants from routine histopathology images could greatly facilitate precision therapy. Although some prior studies have shown that deep learning can accelerate the prediction of key gene mutations from lung cancer pathology slides, their performance remains suboptimal and has so far been limited mainly to early screening tasks. To address these limitations, we have assembled PathGene, which comprises histopathology images paired with next-generation sequencing reports from 1,576 patients at the Second Xiangya Hospital, Central South University, and 448 TCGA-LUAD patients. This multi-center dataset links whole-slide images to driver gene mutation status, mutation subtypes, exon, and tumor mutational burden (TMB) status, with the goal of leveraging pathology images to predict mutations, subtypes, exon locations, and TMB for early genetic screening and to advance precision oncology. Unlike existing datasets, we provide molecular-level information related to histopathology images in PathGene to facilitate the development of biomarker prediction models. We benchmarked 11 multiple-instance learning methods on PathGene for mutation, subtype, exon, and TMB prediction tasks. These experimental methods provide valuable alternatives for early genetic screening of lung cancer patients and assisting clinicians to quickly develop personalized precision targeted treatment plans for patients. Code and data are available at https://github.com/panliangrui/NIPS2025/.

**Link**: [arxiv](http://arxiv.org/abs/2506.00096v2),  [pdf](http://arxiv.org/pdf/2506.00096v2)

**Tags**: q-bio.GN cs.AI 



### COSINUS model-independent sensitivity to the DAMA/LIBRA dark matter   signal
**Authors**: G. Angloher, M. R. Bharadwaj, A. Böhmer, M. Cababie, I. Colantoni, I. Dafinei, N. Di Marco, C. Dittmar, L. Einfalt, F. Ferella, F. Ferroni, S. Fichtinger, A. Filipponi, T. Frank, M. Friedl, Z. Ge, M. Heikinheimo, M. N. Hughes, K. Huitu, M. Kellermann, R. Maji, M. Mancuso, L. Pagnanini, F. Petricca, S. Pirro, F. Pröbst, G. Profeta, A. Puiu, F. Reindl, K. Schäffner, J. Schieck, P. Schreiner, C. Schwertner, K. Shera, M. Stahlberg, A. Stendahl, M. Stukel, C. Tresca, F. Wagner, S. Yue, V. Zema, Y. Zhu, The COSINUS Collaboration

**Updated**: 2025-09-24T14:14:34Z

**Summary**: COSINUS is a dark matter direct detection experiment using NaI crystals as cryogenic scintillating calorimeters. If no signal is observed, this will constrain the dark matter scattering rate in sodium iodide. We investigate how this constraint can be used to infer that the annual modulation signal observed in the DAMA/LIBRA experiment cannot originate from dark matter nuclear recoil events, independently of the dark matter model. We achieve this by unfolding the DAMA modulation spectrum to obtain the implied unquenched nuclear recoil spectrum, which we then compare to the expected COSINUS sensitivity. We find that assuming zero background in the signal region, a 1$\sigma$, 2$\sigma$ or 3$\sigma$ confidence limit exclusion can be obtained with 57, 130 or 250 kg day of exposure, respectively. A simple background model indicates that in the presence of background, the exposure requirements may increase by $\sim30\%$.

**Link**: [arxiv](http://arxiv.org/abs/2504.18263v2),  [pdf](http://arxiv.org/pdf/2504.18263v2)

**Tags**: hep-ph astro-ph.CO physics.ins-det 



### Enhancing Requirement Traceability through Data Augmentation Using Large   Language Models
**Authors**: Jianzhang Zhang, Jialong Zhou, Nan Niu, Chuang Liu

**Updated**: 2025-09-24T14:14:21Z

**Summary**: Requirements traceability is crucial in software engineering to ensure consistency between requirements and code. However, existing automated traceability methods are constrained by the scarcity of training data and challenges in bridging the semantic gap between artifacts. This study aims to address the data scarcity problem in requirements traceability by employing large language models (LLMs) for data augmentation. We propose a novel approach that utilizes prompt-based techniques with LLMs to generate augmented requirement-to-code trace links, thereby enhancing the training dataset. Four LLMs (Gemini 1.5 Pro, Claude 3, GPT-3.5, and GPT-4) were used, employing both zero-shot and few-shot templates. Moreover, we optimized the encoder component of the tracing model to improve its efficiency and adaptability to augmented data. The key contributions of this paper are: (1) proposing and evaluating four prompt templates for data augmentation; (2) providing a comparative analysis of four LLMs for generating trace links; (3) enhancing the model's encoder for improved adaptability to augmented datasets. Experimental results show that our approach significantly enhances model performance, achieving an F1 score improvement of up to 28.59%, thus demonstrating its effectiveness and potential for practical application.

**Link**: [arxiv](http://arxiv.org/abs/2509.20149v1),  [pdf](http://arxiv.org/pdf/2509.20149v1)

**Tags**: cs.SE 



### Cosmology inference with perturbative forward modeling at the field   level: a comparison with joint power spectrum and bispectrum analyses
**Authors**: Kazuyuki Akitsu, Marko Simonović, Shi-Fan Chen, Giovanni Cabass, Matias Zaldarriaga

**Updated**: 2025-09-24T14:12:54Z

**Summary**: We extend field-level inference to jointly constrain the cosmological parameters $\{A,\omega_{\rm cdm},H_0\}$, in both real and redshift space. Our analyses are based on mock data generated using a perturbative forward model, with noise drawn from a Gaussian distribution with a constant power spectrum. This idealized setting, where the field-level likelihood is exactly Gaussian, allows us to precisely quantify the information content in the nonlinear field on large scales. We find that field-level inference accurately recovers all cosmological parameters in both real and redshift space, with uncertainties consistent with perturbation theory expectations. We show that these error bars are comparable to those obtained from a joint power spectrum and bispectrum analysis using the same perturbative model. Finally, we perform several tests using the Gaussian field-level likelihood to fit the mock data where the true noise model is non-Gaussian, and find significant biases in the inferred cosmological parameters. These results highlight that the success of field-level inference critically depends on using the correct likelihood, which may be the primary challenge for applying this method to smaller scales even in the perturbative regime.

**Link**: [arxiv](http://arxiv.org/abs/2509.09673v2),  [pdf](http://arxiv.org/pdf/2509.09673v2)

**Tags**: astro-ph.CO 



### Inverse Reinforcement Learning with Dynamic Reward Scaling for LLM   Alignment
**Authors**: Ruoxi Cheng, Haoxuan Ma, Weixin Wang, Ranjie Duan, Jiexi Liu, Xiaoshuang Jia, Simeng Qin, Xiaochun Cao, Yang Liu, Xiaojun Jia

**Updated**: 2025-09-25T02:38:14Z

**Summary**: Alignment is vital for safely deploying large language models (LLMs). Existing techniques are either reward-based (train a reward model on preference pairs and optimize with reinforcement learning) or reward-free (directly fine-tune on ranked outputs). Recent research shows that well-tuned reward-based pipelines remain robust, and single-response demonstrations can outperform pairwise preference data. However, two challenges persist: (1) imbalanced safety datasets that overrepresent common hazards while neglecting long-tail threats; and (2) static reward models that ignore task difficulty, limiting optimization efficiency and attainable gains. We propose DR-IRL (Dynamically adjusting Rewards through Inverse Reinforcement Learning). We first train category-specific reward models using a balanced safety dataset covering seven harmful categories via IRL. Then we enhance Group Relative Policy Optimization (GRPO) by introducing dynamic reward scaling--adjusting rewards by task difficulty--data-level hardness by text encoder cosine similarity, model-level responsiveness by reward gaps. Extensive experiments across various benchmarks and LLMs demonstrate that DR-IRL outperforms all baseline methods in safety alignment while maintaining usefulness.

**Link**: [arxiv](http://arxiv.org/abs/2503.18991v5),  [pdf](http://arxiv.org/pdf/2503.18991v5)

**Tags**: cs.CL cs.AI cs.LG 



### Culture is Everywhere: A Call for Intentionally Cultural Evaluation
**Authors**: Juhyun Oh, Inha Cha, Michael Saxon, Hyunseung Lim, Shaily Bhatt, Alice Oh

**Updated**: 2025-09-24T14:02:11Z

**Summary**: The prevailing ``trivia-centered paradigm'' for evaluating the cultural alignment of large language models (LLMs) is increasingly inadequate as these models become more advanced and widely deployed. Existing approaches typically reduce culture to static facts or values, testing models via multiple-choice or short-answer questions that treat culture as isolated trivia. Such methods neglect the pluralistic and interactive realities of culture, and overlook how cultural assumptions permeate even ostensibly ``neutral'' evaluation settings. In this position paper, we argue for \textbf{intentionally cultural evaluation}: an approach that systematically examines the cultural assumptions embedded in all aspects of evaluation, not just in explicitly cultural tasks. We systematically characterize the what, how, and circumstances by which culturally contingent considerations arise in evaluation, and emphasize the importance of researcher positionality for fostering inclusive, culturally aligned NLP research. Finally, we discuss implications and future directions for moving beyond current benchmarking practices, discovering important applications that we don't know exist, and involving communities in evaluation design through HCI-inspired participatory methodologies.

**Link**: [arxiv](http://arxiv.org/abs/2509.01301v2),  [pdf](http://arxiv.org/pdf/2509.01301v2)

**Tags**: cs.CL 



### V-GameGym: Visual Game Generation for Code Large Language Models
**Authors**: Wei Zhang, Jack Yang, Renshuai Tao, Lingzheng Chai, Shawn Guo, Jiajun Wu, Xiaoming Chen, Ganqu Cui, Ning Ding, Xander Xu, Hu Wei, Bowen Zhou

**Updated**: 2025-09-24T14:01:18Z

**Summary**: Code large language models have demonstrated remarkable capabilities in programming tasks, yet current benchmarks primarily focus on single modality rather than visual game development. Most existing code-related benchmarks evaluate syntax correctness and execution accuracy, overlooking critical game-specific metrics such as playability, visual aesthetics, and user engagement that are essential for real-world deployment. To address the gap between current LLM capabilities in algorithmic problem-solving and competitive programming versus the comprehensive requirements of practical game development, we present V-GameGym, a comprehensive benchmark comprising 2,219 high-quality samples across 100 thematic clusters derived from real-world repositories, adopting a novel clustering-based curation methodology to ensure both diversity and structural completeness. Further, we introduce a multimodal evaluation framework with an automated LLM-driven pipeline for visual code synthesis using complete UI sandbox environments. Our extensive analysis reveals that V-GameGym effectively bridges the gap between code generation accuracy and practical game development workflows, providing quantifiable quality metrics for visual programming and interactive element generation.

**Link**: [arxiv](http://arxiv.org/abs/2509.20136v1),  [pdf](http://arxiv.org/pdf/2509.20136v1)

**Tags**: cs.SE 



### A Generative Conditional Distribution Equality Testing Framework and Its   Minimax Analysis
**Authors**: Siming Zheng, Meifang Lan, Tong Wang, Yuanyuan Lin

**Updated**: 2025-09-24T13:57:51Z

**Summary**: In this paper, we propose a general framework for testing the equality of the conditional distributions in a two-sample problem. This problem is most relevant to transfer learning under covariate shift. Our framework is built on neural network-based generative methods and sample splitting techniques by transforming the conditional distribution testing problem into an unconditional one. We introduce two special tests: the generative permutation-based conditional distribution equality test and the generative classification accuracy-based conditional distribution equality test. Theoretically, we establish a minimax lower bound for statistical inference in testing the equality of two conditional distributions under certain smoothness conditions. We demonstrate that the generative permutation-based conditional distribution equality test and its modified version can attain this lower bound precisely or up to some iterated logarithmic factor. Moreover, we prove the testing consistency of the generative classification accuracy-based conditional distribution equality test. We also establish the convergence rate for the learned conditional generator by deriving new results related to the recently-developed offset Rademacher complexity and approximation properties using neural networks. Empirically, we conduct numerical studies including synthetic datasets and two real-world datasets, demonstrating the effectiveness of our approach.

**Link**: [arxiv](http://arxiv.org/abs/2509.17729v2),  [pdf](http://arxiv.org/pdf/2509.17729v2)

**Tags**: cs.LG math.ST stat.ME stat.TH 



### Geometric Hyperscanning of Affect under Active Inference
**Authors**: Nicolas Hinrichs, Mahault Albarracin, Dimitris Bolis, Yuyue Jiang, Leonardo Christov-Moore, Leonhard Schilbach

**Updated**: 2025-09-24T13:50:49Z

**Summary**: Second-person neuroscience holds social cognition as embodied meaning co-regulation through reciprocal interaction, modeled here as coupled active inference with affect emerging as inference over identity-relevant surprise. Each agent maintains a self-model that tracks violations in its predictive coherence while recursively modeling the other. Valence is computed from self-model prediction error, weighted by self-relevance, and modulated by prior affective states and by what we term temporal aiming, which captures affective appraisal over time. This accommodates shifts in the self-other boundary, allowing affect to emerge at individual and dyadic levels. We propose a novel method termed geometric hyperscanning, based on the Forman-Ricci curvature, to empirically operationalize these processes: it tracks topological reconfigurations in inter-brain networks, with its entro-py serving as a proxy for affective phase transitions such as rupture, co-regulation, and re-attunement.

**Link**: [arxiv](http://arxiv.org/abs/2506.08599v4),  [pdf](http://arxiv.org/pdf/2506.08599v4)

**Tags**: q-bio.NC 



### Probability Signature: Bridging Data Semantics and Embedding Structure   in Language Models
**Authors**: Junjie Yao, Zhi-Qin John Xu

**Updated**: 2025-09-24T13:49:44Z

**Summary**: The embedding space of language models is widely believed to capture the semantic relationships; for instance, embeddings of digits often exhibit an ordered structure that corresponds to their natural sequence. However, the mechanisms driving the formation of such structures remain poorly understood. In this work, we interpret the embedding structures via the data distribution. We propose a set of probability signatures that reflect the semantic relationships among tokens. Through experiments on the composite addition tasks using the linear model and feedforward network, combined with theoretical analysis of gradient flow dynamics, we reveal that these probability signatures significantly influence the embedding structures. We further generalize our analysis to large language models (LLMs) by training the Qwen2.5 architecture on the subsets of the Pile corpus. Our results show that the probability signatures are faithfully aligned with the embedding structures, particularly in capturing strong pairwise similarities among embeddings. Our work uncovers the mechanism of how data distribution guides the formation of embedding structures, establishing a novel understanding of the relationship between embedding organization and semantic patterns.

**Link**: [arxiv](http://arxiv.org/abs/2509.20124v1),  [pdf](http://arxiv.org/pdf/2509.20124v1)

**Tags**: cs.LG 



### Can LLMs Forecast Internet Traffic from Social Media?
**Authors**: Jonatan Langlet, Mariano Scazzariello, Flavio Luciani, Marta Burocchi, Dejan Kostić, Marco Chiesa

**Updated**: 2025-09-24T13:48:14Z

**Summary**: Societal events shape the Internet's behavior. The death of a prominent public figure, a software launch, or a major sports match can trigger sudden demand surges that overwhelm peering points and content delivery networks. Although these events fall outside regular traffic patterns, forecasting systems still rely solely on those patterns and therefore miss these critical anomalies.   Thus, we argue for socio-technical systems that supplement technical measurements with an active understanding of the underlying drivers, including how events and collective behavior shape digital demands. We propose traffic forecasting using signals from public discourse, such as headlines, forums, and social media, as early demand indicators.   To validate our intuition, we present a proof-of-concept system that autonomously scrapes online discussions, infers real-world events, clusters and enriches them semantically, and correlates them with traffic measurements at a major Internet Exchange Point. This prototype predicted between 56-92% of society-driven traffic spikes after scraping a moderate amount of online discussions.   We believe this approach opens new research opportunities in cross-domain forecasting, scheduling, demand anticipation, and society-informed decision making.

**Link**: [arxiv](http://arxiv.org/abs/2509.20123v1),  [pdf](http://arxiv.org/pdf/2509.20123v1)

**Tags**: cs.NI 



### PEPS: Quantum-Inspired Reinforcement Learning for Coherent Reasoning   Traces in LLMs
**Authors**: Venkat Margapuri, Garik Kazanjian, Naren Kosaraju

**Updated**: 2025-09-24T13:29:53Z

**Summary**: Large Language Models (LLMs) often struggle with maintaining coherent multi-step reasoning traces, particularly in tasks that require a structured logical flow. This work introduces a quantum-inspired approach to address the challenge by incorporating a fidelity-based reward derived from Projected Entangled Pair States (PEPS) into Proximal Policy Optimization. Unlike prior approaches that use direct supervision or contrastive objectives, the proposed method guides learning through structural consistency, offering a novel approach to enforce global coherence in generated reasoning traces. The proposed framework is evaluated using multiple coherence-determining metrics on diverse datasets such as GSM8K, StrategyQA, and EntailmentBank spanning arithmetic, intuitive, and entailment-based reasoning. Results show that the proposed quantum-inspired approach offers significant improvements over supervised, contrastive, and pretrained baseline approaches, highlighting the effectiveness of quantum-inspired fidelity as a foundation to improve reasoning trace coherence in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2509.20105v1),  [pdf](http://arxiv.org/pdf/2509.20105v1)

**Tags**: cs.AI 



### Enabling Multi-Species Bird Classification on Low-Power Bioacoustic   Loggers
**Authors**: Stefano Ciapponi, Leonardo Mannini, Jarek Scanferla, Matteo Anderle, Elisabetta Farella

**Updated**: 2025-09-24T13:27:46Z

**Summary**: This paper introduces WrenNet, an efficient neural network enabling real-time multi-species bird audio classification on low-power microcontrollers for scalable biodiversity monitoring. We propose a semi-learnable spectral feature extractor that adapts to avian vocalizations, outperforming standard mel-scale and fully-learnable alternatives. On an expert-curated 70-species dataset, WrenNet achieves up to 90.8\% accuracy on acoustically distinctive species and 70.1\% on the full task. When deployed on an AudioMoth device ($\leq$1MB RAM), it consumes only 77mJ per inference. Moreover, the proposed model is over 16x more energy-efficient compared to Birdnet when running on a Raspberry Pi 3B+. This work demonstrates the first practical framework for continuous, multi-species acoustic monitoring on low-power edge devices.

**Link**: [arxiv](http://arxiv.org/abs/2509.20103v1),  [pdf](http://arxiv.org/pdf/2509.20103v1)

**Tags**: cs.SD cs.CE 



### Steerable Adversarial Scenario Generation through Test-Time Preference   Alignment
**Authors**: Tong Nie, Yuewen Mei, Yihong Tang, Junlin He, Jie Sun, Haotian Shi, Wei Ma, Jian Sun

**Updated**: 2025-09-24T13:27:35Z

**Summary**: Adversarial scenario generation is a cost-effective approach for safety assessment of autonomous driving systems. However, existing methods are often constrained to a single, fixed trade-off between competing objectives such as adversariality and realism. This yields behavior-specific models that cannot be steered at inference time, lacking the efficiency and flexibility to generate tailored scenarios for diverse training and testing requirements. In view of this, we reframe the task of adversarial scenario generation as a multi-objective preference alignment problem and introduce a new framework named \textbf{S}teerable \textbf{A}dversarial scenario \textbf{GE}nerator (SAGE). SAGE enables fine-grained test-time control over the trade-off between adversariality and realism without any retraining. We first propose hierarchical group-based preference optimization, a data-efficient offline alignment method that learns to balance competing objectives by decoupling hard feasibility constraints from soft preferences. Instead of training a fixed model, SAGE fine-tunes two experts on opposing preferences and constructs a continuous spectrum of policies at inference time by linearly interpolating their weights. We provide theoretical justification for this framework through the lens of linear mode connectivity. Extensive experiments demonstrate that SAGE not only generates scenarios with a superior balance of adversariality and realism but also enables more effective closed-loop training of driving policies. Project page: https://tongnie.github.io/SAGE/.

**Link**: [arxiv](http://arxiv.org/abs/2509.20102v1),  [pdf](http://arxiv.org/pdf/2509.20102v1)

**Tags**: cs.AI 



### Integrated Framework for LLM Evaluation with Answer Generation
**Authors**: Sujeong Lee, Hayoung Lee, Seongsoo Heo, Wonik Choi

**Updated**: 2025-09-24T13:20:37Z

**Summary**: Reliable evaluation of large language models is essential to ensure their applicability in practical scenarios. Traditional benchmark-based evaluation methods often rely on fixed reference answers, limiting their ability to capture important qualitative aspects of generated responses. To address these shortcomings, we propose an integrated evaluation framework called \textit{self-refining descriptive evaluation with expert-driven diagnostics}, SPEED, which utilizes specialized functional experts to perform comprehensive, descriptive analyses of model outputs. Unlike conventional approaches, SPEED actively incorporates expert feedback across multiple dimensions, including hallucination detection, toxicity assessment, and lexical-contextual appropriateness. Experimental results demonstrate that SPEED achieves robust and consistent evaluation performance across diverse domains and datasets. Additionally, by employing relatively compact expert models, SPEED demonstrates superior resource efficiency compared to larger-scale evaluators. These findings illustrate that SPEED significantly enhances fairness and interpretability in LLM evaluations, offering a promising alternative to existing evaluation methodologies.

**Link**: [arxiv](http://arxiv.org/abs/2509.20097v1),  [pdf](http://arxiv.org/pdf/2509.20097v1)

**Tags**: cs.CL cs.AI 



### Unleashing the Potential of the Semantic Latent Space in Diffusion   Models for Image Dehazing
**Authors**: Zizheng Yang, Hu Yu, Bing Li, Jinghao Zhang, Jie Huang, Feng Zhao

**Updated**: 2025-09-24T13:11:37Z

**Summary**: Diffusion models have recently been investigated as powerful generative solvers for image dehazing, owing to their remarkable capability to model the data distribution. However, the massive computational burden imposed by the retraining of diffusion models, coupled with the extensive sampling steps during the inference, limit the broader application of diffusion models in image dehazing. To address these issues, we explore the properties of hazy images in the semantic latent space of frozen pre-trained diffusion models, and propose a Diffusion Latent Inspired network for Image Dehazing, dubbed DiffLI$^2$D. Specifically, we first reveal that the semantic latent space of pre-trained diffusion models can represent the content and haze characteristics of hazy images, as the diffusion time-step changes. Building upon this insight, we integrate the diffusion latent representations at different time-steps into a delicately designed dehazing network to provide instructions for image dehazing. Our DiffLI$^2$D avoids re-training diffusion models and iterative sampling process by effectively utilizing the informative representations derived from the pre-trained diffusion models, which also offers a novel perspective for introducing diffusion models to image dehazing. Extensive experiments on multiple datasets demonstrate that the proposed method achieves superior performance to existing image dehazing methods. Code is available at https://github.com/aaaasan111/difflid.

**Link**: [arxiv](http://arxiv.org/abs/2509.20091v1),  [pdf](http://arxiv.org/pdf/2509.20091v1)

**Tags**: cs.CV 



### You Only Measure Once: On Designing Single-Shot Quantum Machine Learning   Models
**Authors**: Chen-Yu Liu, Leonardo Placidi, Kuan-Cheng Chen, Samuel Yen-Chi Chen, Gabriel Matos

**Updated**: 2025-09-24T13:08:20Z

**Summary**: Quantum machine learning (QML) models conventionally rely on repeated measurements (shots) of observables to obtain reliable predictions. This dependence on large shot budgets leads to high inference cost and time overhead, which is particularly problematic as quantum hardware access is typically priced proportionally to the number of shots. In this work we propose You Only Measure Once (Yomo), a simple yet effective design that achieves accurate inference with dramatically fewer measurements, down to the single-shot regime. Yomo replaces Pauli expectation-value outputs with a probability aggregation mechanism and introduces loss functions that encourage sharp predictions. Our theoretical analysis shows that Yomo avoids the shot-scaling limitations inherent to expectation-based models, and our experiments on MNIST and CIFAR-10 confirm that Yomo consistently outperforms baselines across different shot budgets and under simulations with depolarizing channels. By enabling accurate single-shot inference, Yomo substantially reduces the financial and computational costs of deploying QML, thereby lowering the barrier to practical adoption of QML.

**Link**: [arxiv](http://arxiv.org/abs/2509.20090v1),  [pdf](http://arxiv.org/pdf/2509.20090v1)

**Tags**: cs.LG quant-ph 



### Causal Understanding by LLMs: The Role of Uncertainty
**Authors**: Oscar Lithgow-Serrano, Vani Kanjirangat, Alessandro Antonucci

**Updated**: 2025-09-24T13:06:35Z

**Summary**: Recent papers show LLMs achieve near-random accuracy in causal relation classification, raising questions about whether such failures arise from limited pretraining exposure or deeper representational gaps. We investigate this under uncertainty-based evaluation, testing whether pretraining exposure to causal examples improves causal understanding >18K PubMed sentences -- half from The Pile corpus, half post-2024 -- across seven models (Pythia-1.4B/7B/12B, GPT-J-6B, Dolly-7B/12B, Qwen-7B). We analyze model behavior through: (i) causal classification, where the model identifies causal relationships in text, and (ii) verbatim memorization probing, where we assess whether the model prefers previously seen causal statements over their paraphrases. Models perform four-way classification (direct/conditional/correlational/no-relationship) and select between originals and their generated paraphrases. Results show almost identical accuracy on seen/unseen sentences (p > 0.05), no memorization bias (24.8% original selection), and output distribution over the possible options is almost flat, with entropic values near the maximum (1.35/1.39), confirming random guessing. Instruction-tuned models show severe miscalibration (Qwen: > 95% confidence, 32.8% accuracy, ECE=0.49). Conditional relations induce highest entropy (+11% vs. direct). These findings suggest that failures in causal understanding arise from the lack of structured causal representation, rather than insufficient exposure to causal examples during pretraining.

**Link**: [arxiv](http://arxiv.org/abs/2509.20088v1),  [pdf](http://arxiv.org/pdf/2509.20088v1)

**Tags**: cs.CL cs.AI 



### OLaPh: Optimal Language Phonemizer
**Authors**: Johannes Wirth

**Updated**: 2025-09-24T13:05:09Z

**Summary**: Phonemization, the conversion of text into phonemes, is a key step in text-to-speech. Traditional approaches use rule-based transformations and lexicon lookups, while more advanced methods apply preprocessing techniques or neural networks for improved accuracy on out-of-domain vocabulary. However, all systems struggle with names, loanwords, abbreviations, and homographs. This work presents OLaPh (Optimal Language Phonemizer), a framework that combines large lexica, multiple NLP techniques, and compound resolution with a probabilistic scoring function. Evaluations in German and English show improved accuracy over previous approaches, including on a challenging dataset. To further address unresolved cases, we train a large language model on OLaPh-generated data, which achieves even stronger generalization and performance. Together, the framework and LLM improve phonemization consistency and provide a freely available resource for future research.

**Link**: [arxiv](http://arxiv.org/abs/2509.20086v1),  [pdf](http://arxiv.org/pdf/2509.20086v1)

**Tags**: cs.CL 



### Rethinking player evaluation in sports: Goals above expectation and   beyond
**Authors**: Robert Bajons, Lucas Kook

**Updated**: 2025-09-24T13:03:15Z

**Summary**: A popular quantitative approach to evaluating player performance in sports involves comparing an observed outcome to the expected outcome ignoring player involvement, which is estimated using statistical or machine learning methods. In soccer, for instance, goals above expectation (GAX) of a player measure how often shots of this player led to a goal compared to the model-derived expected outcome of the shots. Typically, sports data analysts rely on flexible machine learning models, which are capable of handling complex nonlinear effects and feature interactions, but fail to provide valid statistical inference due to finite-sample bias and slow convergence rates. In this paper, we close this gap by presenting a framework for player evaluation with metrics derived from differences in actual and expected outcomes using flexible machine learning algorithms, which nonetheless allows for valid frequentist inference. We first show that the commonly used metrics are directly related to Rao's score test in parametric regression models for the expected outcome. Motivated by this finding and recent developments in double machine learning, we then propose the use of residualized versions of the original metrics. For GAX, the residualization step corresponds to an additional regression predicting whether a given player would take the shot under the circumstances described by the features. We further relate metrics in the proposed framework to player-specific effect estimates in interpretable semiparametric regression models, allowing us to infer directional effects, e.g., to determine players that have a positive impact on the outcome. Our primary use case are GAX in soccer. We further apply our framework to evaluate goal-stopping ability of goalkeepers, shooting skill in basketball, quarterback passing skill in American football, and injury-proneness of soccer players.

**Link**: [arxiv](http://arxiv.org/abs/2509.20083v1),  [pdf](http://arxiv.org/pdf/2509.20083v1)

**Tags**: stat.AP 



### Subsampled One-Step Estimation for Fast Statistical Inference
**Authors**: Miaomiao Su, Ruoyu Wang

**Updated**: 2025-09-24T13:00:59Z

**Summary**: Subsampling is an effective approach to alleviate the computational burden associated with large-scale datasets. Nevertheless, existing subsampling estimators incur a substantial loss in estimation efficiency compared to estimators based on the full dataset. Specifically, the convergence rate of existing subsampling estimators is typically $n^{-1/2}$ rather than $N^{-1/2}$, where $n$ and $N$ denote the subsample and full data sizes, respectively. This paper proposes a subsampled one-step (SOS) method to mitigate the estimation efficiency loss utilizing the asymptotic expansions of the subsampling and full-data estimators. The resulting SOS estimator is computationally efficient and achieves a fast convergence rate of $\max\{n^{-1}, N^{-1/2}\}$ rather than $n^{-1/2}$. We establish the asymptotic distribution of the SOS estimator, which can be non-normal in general, and construct confidence intervals on top of the asymptotic distribution. Furthermore, we prove that the SOS estimator is asymptotically normal and equivalent to the full data-based estimator when $n / \sqrt{N} \to \infty$.Simulation studies and real data analyses were conducted to demonstrate the finite sample performance of the SOS estimator. Numerical results suggest that the SOS estimator is almost as computationally efficient as the uniform subsampling estimator while achieving similar estimation efficiency to the full data-based estimator.

**Link**: [arxiv](http://arxiv.org/abs/2407.13446v2),  [pdf](http://arxiv.org/pdf/2407.13446v2)

**Tags**: stat.ME 



### Why Do Some Inputs Break Low-Bit LLM Quantization?
**Authors**: Ting-Yun Chang, Muru Zhang, Jesse Thomason, Robin Jia

**Updated**: 2025-09-24T12:57:03Z

**Summary**: Low-bit weight-only quantization significantly reduces the memory footprint of large language models (LLMs), but disproportionately affects certain examples. We analyze diverse 3-4 bit methods on LLMs ranging from 7B-70B in size and find that the quantization errors of 50 pairs of methods are strongly correlated (avg. 0.82) on FineWeb examples. Moreover, the residual stream magnitudes of full-precision models are indicative of future quantization errors. We further establish a hypothesis that relates the residual stream magnitudes to error amplification and accumulation over layers. Using LLM localization techniques, early exiting, and activation patching, we show that examples with large errors rely on precise residual activations in the late layers, and that the outputs of MLP gates play a crucial role in maintaining the perplexity. Our work reveals why certain examples result in large quantization errors and which model components are most critical for performance preservation.

**Link**: [arxiv](http://arxiv.org/abs/2506.12044v2),  [pdf](http://arxiv.org/pdf/2506.12044v2)

**Tags**: cs.LG cs.AI 



### Understanding Before Reasoning: Enhancing Chain-of-Thought with   Iterative Summarization Pre-Prompting
**Authors**: Dong-Hai Zhu, Yu-Jie Xiong, Jia-Chen Zhang, Xi-Jiong Xie, Chun-Ming Xia

**Updated**: 2025-09-24T12:56:29Z

**Summary**: Chain-of-Thought (CoT) Prompting is a dominant paradigm in Large Language Models (LLMs) to enhance complex reasoning. It guides LLMs to present multi-step reasoning, rather than generating the final answer directly. However, CoT encounters difficulties when key information required for reasoning is implicit or missing. This occurs because CoT emphasizes the sequence of reasoning steps while overlooking the early extraction of essential information. We propose a pre-prompting method called Iterative Summarization Pre-Prompting (ISP^2) to refine LLM reasoning when key information is not explicitly provided. First, entities and their corresponding descriptions are extracted to form potential key information pairs. Next, we use a reliability rating to assess these pairs, then merge the two lowest-ranked pairs into a new entity description. This process is repeated until a unique key information pair is obtained. Finally, that pair, along with the original question, is fed into LLMs to produce the answer. Extensive experiments demonstrate a 7.1% improvement compared to existing methods. Unlike traditional prompting, ISP^2 adopts an inductive approach with pre-prompting, offering flexible integration into diverse reasoning frameworks. The code is available at https://github.com/zdhgreat/ISP-2.

**Link**: [arxiv](http://arxiv.org/abs/2501.04341v2),  [pdf](http://arxiv.org/pdf/2501.04341v2)

**Tags**: cs.CL 



### From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint   Training
**Authors**: Tianqiao Liu, Xueyi Li, Hao Wang, Haoxuan Li, Zhichao Chen, Weiqi Luo, Zitao Liu

**Updated**: 2025-09-25T09:23:12Z

**Summary**: Recent advances in large language models (LLMs) have attracted significant interest in extending their capabilities to multimodal scenarios, particularly for speech-to-speech conversational systems. However, existing multimodal models handling interleaved audio and text rely on autoregressive methods, overlooking that text depends on target-target relations whereas audio depends mainly on source-target relations. In this work, we propose Text-to-Talk (TtT), a unified audio-text framework that integrates autoregressive (AR) text generation with non-autoregressive (NAR) audio diffusion in a single Transformer. By leveraging the any-order autoregressive property of absorbing discrete diffusion, our approach provides a unified training objective for text and audio. To support this hybrid generation paradigm, we design a modality-aware attention mechanism that enforces causal decoding for text while allowing bidirectional modeling within audio spans, and further introduce three training strategies that reduce train-test discrepancies. During inference, TtT employs block-wise diffusion to synthesize audio in parallel while flexibly handling variable-length outputs. Extensive experiments across Audio-QA and ASR tasks demonstrate the effectiveness of our approach, with detailed ablation studies validating each proposed component. We will open-source our models, data and code to facilitate future research in this direction.

**Link**: [arxiv](http://arxiv.org/abs/2509.20072v2),  [pdf](http://arxiv.org/pdf/2509.20072v2)

**Tags**: cs.CL 



### LLM Trainer: Automated Robotic Data Generating via Demonstration   Augmentation using LLMs
**Authors**: Abraham George, Amir Barati Farimani

**Updated**: 2025-09-24T12:40:57Z

**Summary**: We present LLM Trainer, a fully automated pipeline that leverages the world knowledge of Large Language Models (LLMs) to transform a small number of human demonstrations (as few as one) into a large robot dataset for imitation learning. Our approach decomposes demonstration generation into two steps: (1) offline demonstration annotation that extracts keyframes, salient objects, and pose-object relations; and (2) online keypose retargeting that adapts those keyframes to a new scene, given an initial observation. Using these modified keypoints, our system warps the original demonstration to generate a new trajectory, which is then executed, and the resulting demo, if successful, is saved. Because the annotation is reusable across scenes, we use Thompson sampling to optimize the annotation, significantly improving generation success rate. We evaluate our method on a range of tasks, and find that our data annotation method consistently outperforms expert-engineered baselines. We further show an ensemble policy that combines the optimized LLM feed-forward plan with a learned feedback imitation learning controller. Finally, we demonstrate hardware feasibility on a Franka Emika Panda robot. For additional materials and demonstration videos, please see the project website: https://sites.google.com/andrew.cmu.edu/llm-trainer

**Link**: [arxiv](http://arxiv.org/abs/2509.20070v1),  [pdf](http://arxiv.org/pdf/2509.20070v1)

**Tags**: cs.RO 



### MACD: Multi-Agent Clinical Diagnosis with Self-Learned Knowledge for LLM
**Authors**: Wenliang Li, Rui Yan, Xu Zhang, Li Chen, Hongji Zhu, Jing Zhao, Junjun Li, Mengru Li, Wei Cao, Zihang Jiang, Wei Wei, Kun Zhang, Shaohua Kevin Zhou

**Updated**: 2025-09-25T03:59:16Z

**Summary**: Large language models (LLMs) have demonstrated notable potential in medical applications, yet they face substantial challenges in handling complex real-world clinical diagnoses using conventional prompting methods. Current prompt engineering and multi-agent approaches typically optimize isolated inferences, neglecting the accumulation of reusable clinical experience. To address this, this study proposes a novel Multi-Agent Clinical Diagnosis (MACD) framework, which allows LLMs to self-learn clinical knowledge via a multi-agent pipeline that summarizes, refines, and applies diagnostic insights. It mirrors how physicians develop expertise through experience, enabling more focused and accurate diagnosis on key disease-specific cues. We further extend it to a MACD-human collaborative workflow, where multiple LLM-based diagnostician agents engage in iterative consultations, supported by an evaluator agent and human oversight for cases where agreement is not reached. Evaluated on 4,390 real-world patient cases across seven diseases using diverse open-source LLMs (Llama-3.1 8B/70B, DeepSeek-R1-Distill-Llama 70B), MACD significantly improves primary diagnostic accuracy, outperforming established clinical guidelines with gains up to 22.3% (MACD). On the subset of the data, it achieves performance on par with or exceeding that of human physicians (up to 16% improvement over physicians-only diagnosis). Additionally, on the MACD-human workflow, it achieves an 18.6% improvement compared to physicians-only diagnosis. Moreover, self-learned knowledge exhibits strong cross-model stability, transferability, and model-specific personalization, while the system can generate traceable rationales, enhancing explainability. Consequently, this work presents a scalable self-learning paradigm for LLM-assisted diagnosis, bridging the gap between the intrinsic knowledge of LLMs and real-world clinical practice.

**Link**: [arxiv](http://arxiv.org/abs/2509.20067v2),  [pdf](http://arxiv.org/pdf/2509.20067v2)

**Tags**: cs.AI 



### EC-LDA : Label Distribution Inference Attack against Federated Graph   Learning with Embedding Compression
**Authors**: Tong Cheng, Jie Fu, Xinpeng Ling, Huifa Li, Zhili Chen, Haifeng Qian, Junqing Gong

**Updated**: 2025-09-24T12:32:47Z

**Summary**: Graph Neural Networks (GNNs) have been widely used for graph analysis. Federated Graph Learning (FGL) is an emerging learning framework to collaboratively train graph data from various clients. Although FGL allows client data to remain localized, a malicious server can still steal client private data information through uploaded gradient. In this paper, we for the first time propose label distribution attacks (LDAs) on FGL that aim to infer the label distributions of the client-side data. Firstly, we observe that the effectiveness of LDA is closely related to the variance of node embeddings in GNNs. Next, we analyze the relation between them and propose a new attack named EC-LDA, which significantly improves the attack effectiveness by compressing node embeddings. Then, extensive experiments on node classification and link prediction tasks across six widely used graph datasets show that EC-LDA outperforms the SOTA LDAs. Specifically, EC-LDA can achieve the Cos-sim as high as 1.0 under almost all cases. Finally, we explore the robustness of EC-LDA under differential privacy protection and discuss the potential effective defense methods to EC-LDA. Our code is available at https://github.com/cheng-t/EC-LDA.

**Link**: [arxiv](http://arxiv.org/abs/2505.15140v3),  [pdf](http://arxiv.org/pdf/2505.15140v3)

**Tags**: cs.LG cs.CR 



### "Chat" between extreme mass ratio inspirals and Galactic binaries in   LISA data
**Authors**: Sviatoslav Khukhlaev, Stanislav Babak

**Updated**: 2025-09-24T12:31:25Z

**Summary**: The future space-based gravitational wave observatory, the Laser Interferometer Space Antenna, is expected to observe between a few and a few thousand extreme mass-ratio inspirals (EMRIs) per year. Due to the simultaneous presence of other gravitational wave signals in the data, it can be challenging to detect EMRIs and accurately estimate their parameters. In this work, we investigate the interaction between a gravitational signal from an EMRI and millions of signals from inspiralling Galactic white dwarf binaries. We demonstrate that bright Galactic binaries contaminate the detection and characterization of EMRIs. We perform Bayesian inference of EMRI parameters after removing resolvable Galactic binaries and confirm an accuracy comparable to that expected in Gaussian noise.

**Link**: [arxiv](http://arxiv.org/abs/2509.20062v1),  [pdf](http://arxiv.org/pdf/2509.20062v1)

**Tags**: gr-qc astro-ph.IM 



### Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens
**Authors**: Pin-Jui Ku, He Huang, Jean-Marie Lemercier, Subham Sekhar Sahoo, Zhehuai Chen, Ante Jukić

**Updated**: 2025-09-24T12:29:31Z

**Summary**: This paper introduces a discrete diffusion model (DDM) framework for text-aligned speech tokenization and reconstruction. By replacing the auto-regressive speech decoder with a discrete diffusion counterpart, our model achieves significantly better reconstruction quality, stronger ASR performance, and faster inference. We provide a comprehensive analysis of applying DDMs to speech reconstruction, examining sampler choices, inference steps, and robustness to length-scale estimation errors. Furthermore, we improve the original TASTE by systematically comparing vector quantization modules, showing that FSQ yields up to a 35% relative WER reduction and +0.14 UT-MOS improvement over RVQ for AR models, while also enhancing DDM performance. Our model generates speech in just 10 denoising steps and even supports single-step generation with only minor quality degradation.

**Link**: [arxiv](http://arxiv.org/abs/2509.20060v1),  [pdf](http://arxiv.org/pdf/2509.20060v1)

**Tags**: eess.AS 



### Joint Ex-Post Location Calibration and Radio Map Construction under   Biased Positioning Errors
**Authors**: Koki Kanzaki, Koya Sato

**Updated**: 2025-09-24T12:29:16Z

**Summary**: This paper proposes a high-accuracy radio map construction method tailored for environments where location information is affected by bursty errors. Radio maps are an effective tool for visualizing wireless environments. Although extensive research has been conducted on accurate radio map construction, most existing approaches assume noise-free location information during sensing. In practice, however, positioning errors ranging from a few to several tens of meters can arise due to device-based positioning systems (e.g., GNSS). Ignoring such errors during inference can lead to significant degradation in radio map accuracy. This study highlights that these errors often tend to be biased when using mobile devices as sensors. We introduce a novel framework that models these errors together with spatial correlation in radio propagation by embedding them as tunable parameters in the marginal log-likelihood function. This enables ex-post calibration of location uncertainty during radio map construction. Numerical results based on practical human mobility data demonstrate that the proposed method can limit RMSE degradation to approximately 0.25-0.29 dB, compared with Gaussian process regression using noise-free location data, whereas baseline methods suffer performance losses exceeding 1 dB.

**Link**: [arxiv](http://arxiv.org/abs/2509.20059v1),  [pdf](http://arxiv.org/pdf/2509.20059v1)

**Tags**: eess.SP cs.NI 



### Differentially Private Bootstrap: New Privacy Analysis and Inference   Strategies
**Authors**: Zhanyu Wang, Guang Cheng, Jordan Awan

**Updated**: 2025-09-24T12:28:52Z

**Summary**: Differentially private (DP) mechanisms protect individual-level information by introducing randomness into the statistical analysis procedure. Despite the availability of numerous DP tools, there remains a lack of general techniques for conducting statistical inference under DP. We examine a DP bootstrap procedure that releases multiple private bootstrap estimates to infer the sampling distribution and construct confidence intervals (CIs). Our privacy analysis presents new results on the privacy cost of a single DP bootstrap estimate, applicable to any DP mechanism, and identifies some misapplications of the bootstrap in the existing literature. For the composition of the DP bootstrap, we present a numerical method to compute the exact privacy cost of releasing multiple DP bootstrap estimates, and using the Gaussian-DP (GDP) framework (Dong et al., 2022), we show that the release of $B$ DP bootstrap estimates from mechanisms satisfying $(\mu/\sqrt{(2-2/\mathrm{e})B})$-GDP asymptotically satisfies $\mu$-GDP as $B$ goes to infinity. Then, we perform private statistical inference by post-processing the DP bootstrap estimates. We prove that our point estimates are consistent, our standard CIs are asymptotically valid, and both enjoy optimal convergence rates. To further improve the finite performance, we use deconvolution with DP bootstrap estimates to accurately infer the sampling distribution. We derive CIs for tasks such as population mean estimation, logistic regression, and quantile regression, and we compare them to existing methods using simulations and real-world experiments on 2016 Canada Census data. Our private CIs achieve the nominal coverage level and offer the first approach to private inference for quantile regression.

**Link**: [arxiv](http://arxiv.org/abs/2210.06140v4),  [pdf](http://arxiv.org/pdf/2210.06140v4)

**Tags**: stat.ML cs.CR cs.DS cs.LG 



### One Filters All: A Generalist Filter for State Estimation
**Authors**: Shiqi Liu, Wenhan Cao, Chang Liu, Zeyu He, Tianyi Zhang, Shengbo Eben Li

**Updated**: 2025-09-24T12:19:18Z

**Summary**: Estimating hidden states in dynamical systems, also known as optimal filtering, is a long-standing problem in various fields of science and engineering. In this paper, we introduce a general filtering framework, \textbf{LLM-Filter}, which leverages large language models (LLMs) for state estimation by embedding noisy observations with text prototypes. In various experiments for classical dynamical systems, we find that first, state estimation can significantly benefit from the reasoning knowledge embedded in pre-trained LLMs. By achieving proper modality alignment with the frozen LLM, LLM-Filter outperforms the state-of-the-art learning-based approaches. Second, we carefully design the prompt structure, System-as-Prompt (SaP), incorporating task instructions that enable the LLM to understand the estimation tasks. Guided by these prompts, LLM-Filter exhibits exceptional generalization, capable of performing filtering tasks accurately in changed or even unseen environments. We further observe a scaling-law behavior in LLM-Filter, where accuracy improves with larger model sizes and longer training times. These findings make LLM-Filter a promising foundation model of filtering.

**Link**: [arxiv](http://arxiv.org/abs/2509.20051v1),  [pdf](http://arxiv.org/pdf/2509.20051v1)

**Tags**: cs.LG cs.AI 



### Tokenization and Representation Biases in Multilingual Models on   Dialectal NLP Tasks
**Authors**: Vani Kanjirangat, Tanja Samardžić, Ljiljana Dolamic, Fabio Rinaldi

**Updated**: 2025-09-24T12:13:53Z

**Summary**: Dialectal data are characterized by linguistic variation that appears small to humans but has a significant impact on the performance of models. This dialect gap has been related to various factors (e.g., data size, economic and social factors) whose impact, however, turns out to be inconsistent. In this work, we investigate factors impacting the model performance more directly: we correlate Tokenization Parity (TP) and Information Parity (IP), as measures of representational biases in pre-trained multilingual models, with the downstream performance. We compare state-of-the-art decoder-only LLMs with encoder-based models across three tasks: dialect classification, topic classification, and extractive question answering, controlling for varying scripts (Latin vs. non-Latin) and resource availability (high vs. low). Our analysis reveals that TP is a better predictor of the performance on tasks reliant on syntactic and morphological cues (e.g., extractive QA), while IP better predicts performance in semantic tasks (e.g., topic classification). Complementary analyses, including tokenizer behavior, vocabulary coverage, and qualitative insights, reveal that the language support claims of LLMs often might mask deeper mismatches at the script or token level.

**Link**: [arxiv](http://arxiv.org/abs/2509.20045v1),  [pdf](http://arxiv.org/pdf/2509.20045v1)

**Tags**: cs.CL cs.AI 



### Language Model Evolutionary Algorithms for Recommender Systems:   Benchmarks and Algorithm Comparisons
**Authors**: Jiao Liu, Zhu Sun, Shanshan Feng, Caishun Chen, Yew-Soon Ong

**Updated**: 2025-09-24T12:12:51Z

**Summary**: In the evolutionary computing community, the remarkable language-handling capabilities and reasoning power of large language models (LLMs) have significantly enhanced the functionality of evolutionary algorithms (EAs), enabling them to tackle optimization problems involving structured language or program code. Although this field is still in its early stages, its impressive potential has led to the development of various LLM-based EAs. To effectively evaluate the performance and practical applicability of these LLM-based EAs, benchmarks with real-world relevance are essential. In this paper, we focus on LLM-based recommender systems (RSs) and introduce a benchmark problem set, named RSBench, specifically designed to assess the performance of LLM-based EAs in recommendation prompt optimization. RSBench emphasizes session-based recommendations, aiming to discover a set of Pareto optimal prompts that guide the recommendation process, providing accurate, diverse, and fair recommendations. We develop three LLM-based EAs based on established EA frameworks and experimentally evaluate their performance using RSBench. Our study offers valuable insights into the application of EAs in LLM-based RSs. Additionally, we explore key components that may influence the overall performance of the RS, providing meaningful guidance for future research on the development of LLM-based EAs in RSs. The source code of the proposed RSBench can be found at https://github.com/LiuJ-2023/RSBench/tree/main.

**Link**: [arxiv](http://arxiv.org/abs/2411.10697v3),  [pdf](http://arxiv.org/pdf/2411.10697v3)

**Tags**: cs.NE 



### Playpen: An Environment for Exploring Learning Through Conversational   Interaction
**Authors**: Nicola Horst, Davide Mazzaccara, Antonia Schmidt, Michael Sullivan, Filippo Momentè, Luca Franceschetti, Philipp Sadler, Sherzod Hakimov, Alberto Testoni, Raffaella Bernardi, Raquel Fernández, Alexander Koller, Oliver Lemon, David Schlangen, Mario Giulianelli, Alessandro Suglia

**Updated**: 2025-09-24T12:06:14Z

**Summary**: Interaction between learner and feedback-giver has come into focus recently for post-training of Large Language Models (LLMs), through the use of reward models that judge the appropriateness of a model's response. In this paper, we investigate whether Dialogue Games -- goal-directed and rule-governed activities driven predominantly by verbal actions -- can also serve as a source of feedback signals for learning. We introduce Playpen, an environment for off- and online learning through Dialogue Game self-play, and investigate a representative set of post-training methods: supervised fine-tuning; direct alignment (DPO); and reinforcement learning with GRPO. We experiment with post-training a small LLM (Llama-3.1-8B-Instruct), evaluating performance on unseen instances of training games as well as unseen games, and on standard benchmarks. We find that imitation learning through SFT improves performance on unseen instances, but negatively impacts other skills, while interactive learning with GRPO shows balanced improvements without loss of skills. We release the framework and the baseline training setups to foster research in the promising new direction of learning in (synthetic) interaction.

**Link**: [arxiv](http://arxiv.org/abs/2504.08590v3),  [pdf](http://arxiv.org/pdf/2504.08590v3)

**Tags**: cs.CL 



### Triangulating LLM Progress through Benchmarks, Games, and Cognitive   Tests
**Authors**: Filippo Momentè, Alessandro Suglia, Mario Giulianelli, Ambra Ferrari, Alexander Koller, Oliver Lemon, David Schlangen, Raquel Fernández, Raffaella Bernardi

**Updated**: 2025-09-24T12:04:54Z

**Summary**: We examine three evaluation paradigms: standard benchmarks (e.g., MMLU and BBH), interactive games (e.g., Signalling Games or Taboo), and cognitive tests (e.g., for working memory or theory of mind). First, we investigate which of the former two-benchmarks or games-is most effective at discriminating LLMs of varying quality. Then, inspired by human cognitive assessments, we compile a suite of targeted tests that measure cognitive abilities deemed essential for effective language use, and we investigate their correlation with model performance in benchmarks and games. Our analyses reveal that interactive games are superior to standard benchmarks in discriminating models. Causal and logical reasoning correlate with both static and interactive tests, while differences emerge regarding core executive functions and social/emotional skills, which correlate more with games. We advocate for the development of new interactive benchmarks and targeted cognitive tasks inspired by assessing human abilities but designed specifically for LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2502.14359v4),  [pdf](http://arxiv.org/pdf/2502.14359v4)

**Tags**: cs.CL 



### Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain   Human Label Variation
**Authors**: Beiduo Chen, Yang Janet Liu, Anna Korhonen, Barbara Plank

**Updated**: 2025-09-24T12:01:02Z

**Summary**: The recent rise of reasoning-tuned Large Language Models (LLMs)--which generate chains of thought (CoTs) before giving the final answer--has attracted significant attention and offers new opportunities for gaining insights into human label variation, which refers to plausible differences in how multiple annotators label the same data instance. Prior work has shown that LLM-generated explanations can help align model predictions with human label distributions, but typically adopt a reverse paradigm: producing explanations based on given answers. In contrast, CoTs provide a forward reasoning path that may implicitly embed rationales for each answer option, before generating the answers. We thus propose a novel LLM-based pipeline enriched with linguistically-grounded discourse segmenters to extract supporting and opposing statements for each answer option from CoTs with improved accuracy. We also propose a rank-based HLV evaluation framework that prioritizes the ranking of answers over exact scores, which instead favor direct comparison of label distributions. Our method outperforms a direct generation method as well as baselines on three datasets, and shows better alignment of ranking methods with humans, highlighting the effectiveness of our approach.

**Link**: [arxiv](http://arxiv.org/abs/2505.23368v3),  [pdf](http://arxiv.org/pdf/2505.23368v3)

**Tags**: cs.CL 



### Diffusion Classifier-Driven Reward for Offline Preference-based   Reinforcement Learning
**Authors**: Teng Pang, Bingzheng Wang, Guoqiang Wu, Yilong Yin

**Updated**: 2025-09-24T11:57:38Z

**Summary**: Offline preference-based reinforcement learning (PbRL) mitigates the need for reward definition, aligning with human preferences via preference-driven reward feedback without interacting with the environment. However, trajectory-wise preference labels are difficult to meet the precise learning of step-wise reward, thereby affecting the performance of downstream algorithms. To alleviate the insufficient step-wise reward caused by trajectory-wise preferences, we propose a novel preference-based reward acquisition method: Diffusion Preference-based Reward (DPR). DPR directly treats step-wise preference-based reward acquisition as a binary classification and utilizes the robustness of diffusion classifiers to infer step-wise rewards discriminatively. In addition, to further utilize trajectory-wise preference information, we propose Conditional Diffusion Preference-based Reward (C-DPR), which conditions on trajectory-wise preference labels to enhance reward inference. We apply the above methods to existing offline RL algorithms, and a series of experimental results demonstrate that the diffusion classifier-driven reward outperforms the previous reward acquisition method with the Bradley-Terry model.

**Link**: [arxiv](http://arxiv.org/abs/2503.01143v3),  [pdf](http://arxiv.org/pdf/2503.01143v3)

**Tags**: cs.LG 



### Resource-Efficient Adaptation of Large Language Models for Text   Embeddings via Prompt Engineering and Contrastive Fine-tuning
**Authors**: Benedikt Roth, Stephan Rappensperger, Tianming Qiu, Hamza Imamović, Julian Wörmann, Hao Shen

**Updated**: 2025-09-24T11:55:48Z

**Summary**: Large Language Models (LLMs) have become a cornerstone in Natural Language Processing (NLP), achieving impressive performance in text generation. Their token-level representations capture rich, human-aligned semantics. However, pooling these vectors into a text embedding discards crucial information. Nevertheless, many non-generative downstream tasks, such as clustering, classification, or retrieval, still depend on accurate and controllable sentence- or document-level embeddings. We explore several adaptation strategies for pre-trained, decoder-only LLMs: (i) various aggregation techniques for token embeddings, (ii) task-specific prompt engineering, and (iii) text-level augmentation via contrastive fine-tuning. Combining these components yields competitive performance on the English clustering track of the Massive Text Embedding Benchmark (MTEB). An analysis of the attention map further shows that fine-tuning shifts focus from prompt tokens to semantically relevant words, indicating more effective compression of meaning into the final hidden state. Our experiments demonstrate that LLMs can be effectively adapted as text embedding models through a combination of prompt engineering and resource-efficient contrastive fine-tuning on synthetically generated positive pairs.

**Link**: [arxiv](http://arxiv.org/abs/2507.22729v2),  [pdf](http://arxiv.org/pdf/2507.22729v2)

**Tags**: cs.CL 



## Keyword: LLM Deployment 
 ### FlyTrap: Physical Distance-Pulling Attack Towards Camera-based   Autonomous Target Tracking Systems
**Authors**: Shaoyuan Xie, Mohamad Habib Fakih, Junchi Lu, Fayzah Alshammari, Ningfei Wang, Takami Sato, Halima Bouzidi, Mohammad Abdullah Al Faruque, Qi Alfred Chen

**Updated**: 2025-09-24T17:59:54Z

**Summary**: Autonomous Target Tracking (ATT) systems, especially ATT drones, are widely used in applications such as surveillance, border control, and law enforcement, while also being misused in stalking and destructive actions. Thus, the security of ATT is highly critical for real-world applications. Under the scope, we present a new type of attack: distance-pulling attacks (DPA) and a systematic study of it, which exploits vulnerabilities in ATT systems to dangerously reduce tracking distances, leading to drone capturing, increased susceptibility to sensor attacks, or even physical collisions. To achieve these goals, we present FlyTrap, a novel physical-world attack framework that employs an adversarial umbrella as a deployable and domain-specific attack vector. FlyTrap is specifically designed to meet key desired objectives in attacking ATT drones: physical deployability, closed-loop effectiveness, and spatial-temporal consistency. Through novel progressive distance-pulling strategy and controllable spatial-temporal consistency designs, FlyTrap manipulates ATT drones in real-world setups to achieve significant system-level impacts. Our evaluations include new datasets, metrics, and closed-loop experiments on real-world white-box and even commercial ATT drones, including DJI and HoverAir. Results demonstrate FlyTrap's ability to reduce tracking distances within the range to be captured, sensor attacked, or even directly crashed, highlighting urgent security risks and practical implications for the safe deployment of ATT systems.

**Link**: [arxiv](http://arxiv.org/abs/2509.20362v1),  [pdf](http://arxiv.org/pdf/2509.20362v1)

**Tags**: cs.CR 



### Process-Informed Forecasting of Complex Thermal Dynamics in   Pharmaceutical Manufacturing
**Authors**: Ramona Rubini, Siavash Khodakarami, Aniruddha Bora, George Em Karniadakis, Michele Dassisti

**Updated**: 2025-09-24T17:42:00Z

**Summary**: Accurate time-series forecasting for complex physical systems is the backbone of modern industrial monitoring and control. While deep learning models excel at capturing complex dynamics, currently, their deployment is limited due to physical inconsistency and robustness, hence constraining their reliability in regulated environments. We introduce process-informed forecasting (PIF) models for temperature in pharmaceutical lyophilization. We investigate a wide range of models, from classical ones such as Autoregressive Integrated Moving Average Model (ARIMA) and Exponential Smoothing Model (ETS), to modern deep learning architectures, including Kolmogorov-Arnold Networks (KANs). We compare three different loss function formulations that integrate a process-informed trajectory prior: a fixed-weight loss, a dynamic uncertainty-based loss, and a Residual-Based Attention (RBA) mechanism. We evaluate all models not only for accuracy and physical consistency but also for robustness to sensor noise. Furthermore, we test the practical generalizability of the best model in a transfer learning scenario on a new process. Our results show that PIF models outperform their data-driven counterparts in terms of accuracy, physical plausibility and noise resilience. This work provides a roadmap for developing reliable and generalizable forecasting solutions for critical applications in the pharmaceutical manufacturing landscape.

**Link**: [arxiv](http://arxiv.org/abs/2509.20349v1),  [pdf](http://arxiv.org/pdf/2509.20349v1)

**Tags**: cs.LG 



### DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware   Reinforcement Learning on Imbalanced Data
**Authors**: Yuhang Zhou, Jing Zhu, Shengyi Qian, Zhuokai Zhao, Xiyao Wang, Xiaoyu Liu, Ming Li, Paiheng Xu, Wei Ai, Furong Huang

**Updated**: 2025-09-24T17:25:12Z

**Summary**: Large Language Models (LLMs) are increasingly aligned with human preferences through Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods, Group Relative Policy Optimization (GRPO) has gained attention for its simplicity and strong performance, notably eliminating the need for a learned value function. However, GRPO implicitly assumes a balanced domain distribution and uniform semantic alignment across groups, assumptions that rarely hold in real-world datasets. When applied to multi-domain, imbalanced data, GRPO disproportionately optimizes for dominant domains, neglecting underrepresented ones and resulting in poor generalization and fairness. We propose Domain-Informed Self-Consistency Policy Optimization (DISCO), a principled extension to GRPO that addresses inter-group imbalance with two key innovations. Domain-aware reward scaling counteracts frequency bias by reweighting optimization based on domain prevalence. Difficulty-aware reward scaling leverages prompt-level self-consistency to identify and prioritize uncertain prompts that offer greater learning value. Together, these strategies promote more equitable and effective policy learning across domains. Extensive experiments across multiple LLMs and skewed training distributions show that DISCO improves generalization, outperforms existing GRPO variants by 5% on Qwen3 models, and sets new state-of-the-art results on multi-domain alignment benchmarks. Our code and data are available at https://github.com/Tonyzhou98/disco_grpo.

**Link**: [arxiv](http://arxiv.org/abs/2505.15074v3),  [pdf](http://arxiv.org/pdf/2505.15074v3)

**Tags**: cs.CL cs.AI cs.LG 



### Uncovering Graph Reasoning in Decoder-only Transformers with Circuit   Tracing
**Authors**: Xinnan Dai, Chung-Hsiang Lo, Kai Guo, Shenglai Zeng, Dongsheng Luo, Jiliang Tang

**Updated**: 2025-09-24T17:25:05Z

**Summary**: Transformer-based LLMs demonstrate strong performance on graph reasoning tasks, yet their internal mechanisms remain underexplored. To uncover these reasoning process mechanisms in a fundamental and unified view, we set the basic decoder-only transformers and explain them using the circuit-tracer framework. Through this lens, we visualize reasoning traces and identify two core mechanisms in graph reasoning: token merging and structural memorization, which underlie both path reasoning and substructure extraction tasks. We further quantify these behaviors and analyze how they are influenced by graph density and model size. Our study provides a unified interpretability framework for understanding structural reasoning in decoder-only Transformers.

**Link**: [arxiv](http://arxiv.org/abs/2509.20336v1),  [pdf](http://arxiv.org/pdf/2509.20336v1)

**Tags**: cs.LG cs.AI 



### Multimodal Reference Visual Grounding
**Authors**: Yangxiao Lu, Ruosen Li, Liqiang Jing, Jikai Wang, Xinya Du, Yunhui Guo, Nicholas Ruozzi, Yu Xiang

**Updated**: 2025-09-24T17:23:48Z

**Summary**: Visual grounding focuses on detecting objects from images based on language expressions. Recent Large Vision-Language Models (LVLMs) have significantly advanced visual grounding performance by training large models with large-scale datasets. However, the problem remains challenging, especially when similar objects appear in the input image. For example, an LVLM may not be able to differentiate Diet Coke and regular Coke in an image. In this case, if additional reference images of Diet Coke and regular Coke are available, it can help the visual grounding of similar objects.   In this work, we introduce a new task named Multimodal Reference Visual Grounding (MRVG). In this task, a model has access to a set of reference images of objects in a database. Based on these reference images and a language expression, the model is required to detect a target object from a query image. We first introduce a new dataset to study the MRVG problem. Then we introduce a novel method, named MRVG-Net, to solve this visual grounding problem. We show that by efficiently using reference images with few-shot object detection and using Large Language Models (LLMs) for object matching, our method achieves superior visual grounding performance compared to the state-of-the-art LVLMs such as Qwen2.5-VL-72B. Our approach bridges the gap between few-shot detection and visual grounding, unlocking new capabilities for visual understanding, which has wide applications in robotics. Project page with our video, code, and dataset: https://irvlutd.github.io/MultiGrounding

**Link**: [arxiv](http://arxiv.org/abs/2504.02876v2),  [pdf](http://arxiv.org/pdf/2504.02876v2)

**Tags**: cs.CV cs.LG 



### Video models are zero-shot learners and reasoners
**Authors**: Thaddäus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, Robert Geirhos

**Updated**: 2025-09-24T17:17:27Z

**Summary**: The remarkable zero-shot capabilities of Large Language Models (LLMs) have propelled natural language processing from task-specific models to unified, generalist foundation models. This transformation emerged from simple primitives: large, generative models trained on web-scale data. Curiously, the same primitives apply to today's generative video models. Could video models be on a trajectory towards general-purpose vision understanding, much like LLMs developed general-purpose language understanding? We demonstrate that Veo 3 can solve a broad variety of tasks it wasn't explicitly trained for: segmenting objects, detecting edges, editing images, understanding physical properties, recognizing object affordances, simulating tool use, and more. These abilities to perceive, model, and manipulate the visual world enable early forms of visual reasoning like maze and symmetry solving. Veo's emergent zero-shot capabilities indicate that video models are on a path to becoming unified, generalist vision foundation models.

**Link**: [arxiv](http://arxiv.org/abs/2509.20328v1),  [pdf](http://arxiv.org/pdf/2509.20328v1)

**Tags**: cs.LG cs.AI cs.CV cs.RO 



### RAG Security and Privacy: Formalizing the Threat Model and Attack   Surface
**Authors**: Atousa Arzanipour, Rouzbeh Behnia, Reza Ebrahimi, Kaushik Dutta

**Updated**: 2025-09-24T17:11:35Z

**Summary**: Retrieval-Augmented Generation (RAG) is an emerging approach in natural language processing that combines large language models (LLMs) with external document retrieval to produce more accurate and grounded responses. While RAG has shown strong potential in reducing hallucinations and improving factual consistency, it also introduces new privacy and security challenges that differ from those faced by traditional LLMs. Existing research has demonstrated that LLMs can leak sensitive information through training data memorization or adversarial prompts, and RAG systems inherit many of these vulnerabilities. At the same time, reliance of RAG on an external knowledge base opens new attack surfaces, including the potential for leaking information about the presence or content of retrieved documents, or for injecting malicious content to manipulate model behavior. Despite these risks, there is currently no formal framework that defines the threat landscape for RAG systems. In this paper, we address a critical gap in the literature by proposing, to the best of our knowledge, the first formal threat model for retrieval-RAG systems. We introduce a structured taxonomy of adversary types based on their access to model components and data, and we formally define key threat vectors such as document-level membership inference and data poisoning, which pose serious privacy and integrity risks in real-world deployments. By establishing formal definitions and attack models, our work lays the foundation for a more rigorous and principled understanding of privacy and security in RAG systems.

**Link**: [arxiv](http://arxiv.org/abs/2509.20324v1),  [pdf](http://arxiv.org/pdf/2509.20324v1)

**Tags**: cs.CR cs.AI 



### DRES: Benchmarking LLMs for Disfluency Removal
**Authors**: Maria Teleki, Sai Janjur, Haoran Liu, Oliver Grabner, Ketan Verma, Thomas Docog, Xiangjue Dong, Lingfeng Shi, Cong Wang, Stephanie Birkelbach, Jason Kim, Yin Zhang, James Caverlee

**Updated**: 2025-09-24T17:08:12Z

**Summary**: Disfluencies -- such as "um," "uh," interjections, parentheticals, and edited statements -- remain a persistent challenge for speech-driven systems, degrading accuracy in command interpretation, summarization, and conversational agents. We introduce DRES (Disfluency Removal Evaluation Suite), a controlled text-level benchmark that establishes a reproducible semantic upper bound for this task. DRES builds on human-annotated Switchboard transcripts, isolating disfluency removal from ASR errors and acoustic variability. We systematically evaluate proprietary and open-source LLMs across scales, prompting strategies, and architectures. Our results reveal that (i) simple segmentation consistently improves performance, even for long-context models; (ii) reasoning-oriented models tend to over-delete fluent tokens; and (iii) fine-tuning achieves near state-of-the-art precision and recall but harms generalization abilities. We further present a set of LLM-specific error modes and offer nine practical recommendations (R1-R9) for deploying disfluency removal in speech-driven pipelines. DRES provides a reproducible, model-agnostic foundation for advancing robust spoken-language systems.

**Link**: [arxiv](http://arxiv.org/abs/2509.20321v1),  [pdf](http://arxiv.org/pdf/2509.20321v1)

**Tags**: cs.CL cs.AI eess.AS 



### Enhancing Targeted Adversarial Attacks on Large Vision-Language Models   via Intermediate Projector
**Authors**: Yiming Cao, Yanjie Li, Kaisheng Liang, Bin Xiao

**Updated**: 2025-09-24T17:02:50Z

**Summary**: The growing deployment of Large Vision-Language Models (VLMs) raises safety concerns, as adversaries may exploit model vulnerabilities to induce harmful outputs, with targeted black-box adversarial attacks posing a particularly severe threat. However, existing methods primarily maximize encoder-level global similarity, which lacks the granularity for stealthy and practical fine-grained attacks, where only specific target should be altered (e.g., modifying a car while preserving its background). Moreover, they largely neglect the projector, a key semantic bridge in VLMs for multimodal alignment. To address these limitations, we propose a novel black-box targeted attack framework that leverages the projector. Specifically, we utilize the widely adopted Querying Transformer (Q-Former) which transforms global image embeddings into fine-grained query outputs, to enhance attack effectiveness and granularity. For standard global targeted attack scenarios, we propose the Intermediate Projector Guided Attack (IPGA), which aligns Q-Former fine-grained query outputs with the target to enhance attack strength and exploits the intermediate pretrained Q-Former that is not fine-tuned for any specific Large Language Model (LLM) to improve attack transferability. For fine-grained attack scenarios, we augment IPGA with the Residual Query Alignment (RQA) module, which preserves unrelated content by constraining non-target query outputs to enhance attack granularity. Extensive experiments demonstrate that IPGA significantly outperforms baselines in global targeted attacks, and IPGA with RQA (IPGA-R) attains superior success rates and unrelated content preservation over baselines in fine-grained attacks. Our method also transfers effectively to commercial VLMs such as Google Gemini and OpenAI GPT.

**Link**: [arxiv](http://arxiv.org/abs/2508.13739v2),  [pdf](http://arxiv.org/pdf/2508.13739v2)

**Tags**: cs.CV 



### Z-Scores: A Metric for Linguistically Assessing Disfluency Removal
**Authors**: Maria Teleki, Sai Janjur, Haoran Liu, Oliver Grabner, Ketan Verma, Thomas Docog, Xiangjue Dong, Lingfeng Shi, Cong Wang, Stephanie Birkelbach, Jason Kim, Yin Zhang, James Caverlee

**Updated**: 2025-09-24T17:02:39Z

**Summary**: Evaluating disfluency removal in speech requires more than aggregate token-level scores. Traditional word-based metrics such as precision, recall, and F1 (E-Scores) capture overall performance but cannot reveal why models succeed or fail. We introduce Z-Scores, a span-level linguistically-grounded evaluation metric that categorizes system behavior across distinct disfluency types (EDITED, INTJ, PRN). Our deterministic alignment module enables robust mapping between generated text and disfluent transcripts, allowing Z-Scores to expose systematic weaknesses that word-level metrics obscure. By providing category-specific diagnostics, Z-Scores enable researchers to identify model failure modes and design targeted interventions -- such as tailored prompts or data augmentation -- yielding measurable performance improvements. A case study with LLMs shows that Z-Scores uncover challenges with INTJ and PRN disfluencies hidden in aggregate F1, directly informing model refinement strategies.

**Link**: [arxiv](http://arxiv.org/abs/2509.20319v1),  [pdf](http://arxiv.org/pdf/2509.20319v1)

**Tags**: cs.CL cs.AI eess.AS 



### A Comprehensive Evaluation of YOLO-based Deer Detection Performance on   Edge Devices
**Authors**: Bishal Adhikari, Jiajia Li, Eric S. Michel, Jacob Dykes, Te-Ming Paul Tseng, Mary Love Tagert, Dong Chen

**Updated**: 2025-09-24T17:01:50Z

**Summary**: The escalating economic losses in agriculture due to deer intrusion, estimated to be in the hundreds of millions of dollars annually in the U.S., highlight the inadequacy of traditional mitigation strategies since these methods are often labor-intensive, costly, and ineffective for modern farming systems. To overcome this, there is a critical need for intelligent, autonomous solutions which require accurate and efficient deer detection. But the progress in this field is impeded by a significant gap in the literature, mainly the lack of a domain-specific, practical dataset and limited study on the on-field deployability of deer detection systems. Addressing this gap, this study presents a comprehensive evaluation of state-of-the-art deep learning models for deer detection in challenging real-world scenarios. The contributions of this work are threefold. First, we introduce a curated, publicly available dataset of 3,095 annotated images with bounding-box annotations of deer, derived from the Idaho Cameratraps project. Second, we provide an extensive comparative analysis of 12 model variants across four recent YOLO architectures(v8, v9, v10, and v11). Finally, we benchmarked performance on a high-end NVIDIA RTX 5090 GPU and evaluated on two representative edge computing platforms: Raspberry Pi 5 and NVIDIA Jetson AGX Xavier. Results show that the real-time detection is not feasible in Raspberry Pi without hardware-specific model optimization, while NVIDIA Jetson provides greater than 30 FPS with GPU-accelerated inference on 's' and 'n' series models. This study also reveals that smaller, architecturally advanced models such as YOLOv11n, YOLOv8s, and YOLOv9s offer the optimal balance of high accuracy (AP@.5 > 0.85) and computational efficiency (FPS > 30). To support further research, both the source code and datasets are publicly available at https://github.com/WinnerBishal/track-the-deer.

**Link**: [arxiv](http://arxiv.org/abs/2509.20318v1),  [pdf](http://arxiv.org/pdf/2509.20318v1)

**Tags**: cs.CV 



### SIM-CoT: Supervised Implicit Chain-of-Thought
**Authors**: Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Jiaqi Wang, Xipeng Qiu, Dahua Lin

**Updated**: 2025-09-24T17:01:32Z

**Summary**: Implicit Chain-of-Thought (CoT) methods present a promising, token-efficient alternative to explicit CoT reasoning in Large Language Models (LLMs), but a persistent performance gap has limited the application of implicit CoT. We identify a core latent instability issue by scaling the computational budget of implicit CoT approaches: as we increase the number of implicit reasoning tokens to enhance performance, the training process often becomes unstable and collapses. Our analysis reveals that this instability arises from the latent representations becoming homogeneous and losing their semantic diversity, a failure caused by insufficient step-level supervision in existing implicit CoT approaches. To address this issue, we propose SIM-CoT, a plug-and-play training module that introduces step-level supervision to stabilize and enrich the latent reasoning space. Specifically, SIM-CoT employs an auxiliary decoder during training to align each implicit token with its corresponding explicit reasoning step, ensuring that latent states capture distinct and meaningful information. The proposed auxiliary decoder is removed during inference, preserving the computational efficiency of implicit CoT methods with no added overhead. In addition, the auxiliary decoder affords interpretability of implicit reasoning by projecting each latent token onto an explicit reasoning vocabulary, enabling per-step visualization of semantic roles and diagnosis. SIM-CoT significantly enhances both the in-domain accuracy and out-of-domain stability of various implicit CoT methods, boosting baselines like Coconut by +8.2% on GPT-2 and CODI by +3.0% on LLaMA-3.1 8B. Demonstrating strong scalability, SIM-CoT also surpasses the explicit CoT baseline on GPT-2 by 2.1% with 2.3\times greater token efficiency, while substantially closing the performance gap on larger models like LLaMA-3.1 8B.

**Link**: [arxiv](http://arxiv.org/abs/2509.20317v1),  [pdf](http://arxiv.org/pdf/2509.20317v1)

**Tags**: cs.CL cs.AI 



### A GEN AI Framework for Medical Note Generation
**Authors**: Hui Yi Leong, Yi Fan Gao, Shuai Ji, Bora Kalaycioglu, Uktu Pamuksuz

**Updated**: 2025-09-24T17:00:37Z

**Summary**: The increasing administrative burden of medical documentation, particularly through Electronic Health Records (EHR), significantly reduces the time available for direct patient care and contributes to physician burnout. To address this issue, we propose MediNotes, an advanced generative AI framework designed to automate the creation of SOAP (Subjective, Objective, Assessment, Plan) notes from medical conversations. MediNotes integrates Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and Automatic Speech Recognition (ASR) to capture and process both text and voice inputs in real time or from recorded audio, generating structured and contextually accurate medical notes. The framework also incorporates advanced techniques like Quantized Low-Rank Adaptation (QLoRA) and Parameter-Efficient Fine-Tuning (PEFT) for efficient model fine-tuning in resource-constrained environments. Additionally, MediNotes offers a query-based retrieval system, allowing healthcare providers and patients to access relevant medical information quickly and accurately. Evaluations using the ACI-BENCH dataset demonstrate that MediNotes significantly improves the accuracy, efficiency, and usability of automated medical documentation, offering a robust solution to reduce the administrative burden on healthcare professionals while improving the quality of clinical workflows.

**Link**: [arxiv](http://arxiv.org/abs/2410.01841v2),  [pdf](http://arxiv.org/pdf/2410.01841v2)

**Tags**: eess.AS cs.AI cs.CL cs.IR cs.SD 



### Efficient Fine-Tuning of Large Language Models for Automated Medical   Documentation
**Authors**: Hui Yi Leong, Yi Fan Gao, Ji Shuai, Yang Zhang, Uktu Pamuksuz

**Updated**: 2025-09-24T16:59:19Z

**Summary**: Scientific research indicates that for every hour spent in direct patient care, physicians spend nearly two additional hours on administrative tasks, particularly on electronic health records (EHRs) and desk work. This excessive administrative burden not only reduces the time available for patient care but also contributes to physician burnout and inefficiencies in healthcare delivery. To address these challenges, this study introduces MediGen, a fine-tuned large language model (LLM) designed to automate the generation of medical reports from medical dialogues. By leveraging state-of-the-art methodologies for fine-tuning open-source pretrained models, including LLaMA3-8B, MediGen achieves high accuracy in transcribing and summarizing clinical interactions. The fine-tuned LLaMA3-8B model demonstrated promising results, achieving a ROUGE score of 58% and a BERTScore-F1 of 72%, indicating its effectiveness in generating accurate and clinically relevant medical reports. These findings suggest that MediGen has the potential to significantly reduce the administrative workload on physicians, improving both healthcare efficiency and physician well-being.

**Link**: [arxiv](http://arxiv.org/abs/2409.09324v3),  [pdf](http://arxiv.org/pdf/2409.09324v3)

**Tags**: cs.CL cs.AI 



### UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from   an Uncertainty-Aware Perspective
**Authors**: Jing Xiong, Jianghan Shen, Fanghua Ye, Chaofan Tao, Zhongwei Wan, Jianqiao Lu, Xun Wu, Chuanyang Zheng, Zhijiang Guo, Min Yang, Lingpeng Kong, Ngai Wong

**Updated**: 2025-09-24T16:56:17Z

**Summary**: Deploying large language models (LLMs) for long-context inference remains challenging due to their substantial memory and computational demands. While techniques such as Key-Value (KV) cache compression are designed to reduce memory usage, they often neglect the structured sparsity inherent in the relationship between hidden states and their corresponding KV cache. In this work, we explore the role of uncertainty as a potential indicator of sparsity within LLMs. We propose UNComp, an uncertainty-aware framework that leverages truncated matrix entropy to identify areas of low information content, thereby revealing sparsity patterns that can be used for adaptive compression. Unlike traditional methods that apply uniform compression, UNComp dynamically adjusts its approach to compression, guided by uncertainty measures that reflect the importance of various model components. Our analysis shows that sparsity patterns, when derived from uncertainty estimates, can be exploited to reveal special long-range dependencies, such as retrieval heads and retrieval layers. This perspective not only enhances our understanding of how compression can be optimized but also provides new insights into the inherent sparsity of LLMs during long-context inference. By focusing on uncertainty to analyze the sparsity pattern in detail, UNComp reduces the KV cache size to 4.74% of the original, achieves a 6% prefill speedup, and improves throughput by 6.4x - not only delivering strong lossless compression performance, but also validating the effectiveness of the underlying theoretical tool. We release the code at https://github.com/menik1126/UNComp.

**Link**: [arxiv](http://arxiv.org/abs/2410.03090v2),  [pdf](http://arxiv.org/pdf/2410.03090v2)

**Tags**: cs.CL cs.LG 



### White-Basilisk: A Hybrid Model for Code Vulnerability Detection
**Authors**: Ioannis Lamprou, Alexander Shevtsov, Ioannis Arapakis, Sotiris Ioannidis

**Updated**: 2025-09-24T16:51:16Z

**Summary**: The proliferation of software vulnerabilities presents a significant challenge to cybersecurity, necessitating more effective detection methodologies. We introduce White-Basilisk, a novel approach to vulnerability detection that demonstrates superior performance while challenging prevailing assumptions in AI model scaling. Utilizing an innovative architecture that integrates Mamba layers, linear self-attention, and a Mixture of Experts framework, White-Basilisk achieves state-of-the-art results in vulnerability detection tasks with a parameter count of only 200M. The model's capacity to process sequences of unprecedented length enables comprehensive analysis of extensive codebases in a single pass, surpassing the context limitations of current Large Language Models (LLMs). White-Basilisk exhibits robust performance on imbalanced, real-world datasets, while maintaining computational efficiency that facilitates deployment across diverse organizational scales. This research not only establishes new benchmarks in code security but also provides empirical evidence that compact, efficiently designed models can outperform larger counterparts in specialized tasks, potentially redefining optimization strategies in AI development for domain-specific applications.

**Link**: [arxiv](http://arxiv.org/abs/2507.08540v3),  [pdf](http://arxiv.org/pdf/2507.08540v3)

**Tags**: cs.CR cs.AI 



### Advancing Expert Specialization for Better MoE
**Authors**: Hongcan Guo, Haolang Lu, Guoshun Nan, Bolun Chu, Jialin Zhuang, Yuan Yang, Wenhao Che, Sicong Leng, Qimei Cui, Xudong Jiang

**Updated**: 2025-09-24T16:48:33Z

**Summary**: Mixture-of-Experts (MoE) models enable efficient scaling of large language models (LLMs) by activating only a subset of experts per input. However, we observe that the commonly used auxiliary load balancing loss often leads to expert overlap and overly uniform routing, which hinders expert specialization and degrades overall performance during post-training. To address this, we propose a simple yet effective solution that introduces two complementary objectives: (1) an orthogonality loss to encourage experts to process distinct types of tokens, and (2) a variance loss to encourage more discriminative routing decisions. Gradient-level analysis demonstrates that these objectives are compatible with the existing auxiliary loss and contribute to optimizing the training process. Experimental results over various model architectures and across multiple benchmarks show that our method significantly enhances expert specialization. Notably, our method improves classic MoE baselines with auxiliary loss by up to 23.79%, while also maintaining load balancing in downstream tasks, without any architectural modifications or additional components. We will release our code to contribute to the community.

**Link**: [arxiv](http://arxiv.org/abs/2505.22323v2),  [pdf](http://arxiv.org/pdf/2505.22323v2)

**Tags**: cs.CL cs.SE 68T07 I.2.7 



### Enhancing RAG Efficiency with Adaptive Context Compression
**Authors**: Shuyu Guo, Shuo Zhang, Zhaochun Ren

**Updated**: 2025-09-24T16:41:40Z

**Summary**: Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but incurs significant inference costs due to lengthy retrieved contexts. While context compression mitigates this issue, existing methods apply fixed compression rates, over-compressing simple queries or under-compressing complex ones. We propose Adaptive Context Compression for RAG (ACC-RAG), a framework that dynamically adjusts compression rates based on input complexity, optimizing inference efficiency without sacrificing accuracy. ACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with a context selector to retain minimal sufficient information, akin to human skimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms fixed-rate methods and matches/unlocks over 4 times faster inference versus standard RAG while maintaining or improving accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2507.22931v3),  [pdf](http://arxiv.org/pdf/2507.22931v3)

**Tags**: cs.CL cs.AI 



### LLM-Powered AI Tutors with Personas for d/Deaf and Hard-of-Hearing   Online Learners
**Authors**: Haocong Cheng, Si Chen, Christopher Perdriau, Shriya Mokkapati, Yun Huang

**Updated**: 2025-09-24T16:38:38Z

**Summary**: Intelligent tutoring systems (ITS) using artificial intelligence (AI) technology have shown promise in supporting learners with diverse abilities. Large language models (LLMs) provide new opportunities to incorporate personas to AI-based tutors and support dynamic interactive dialogue. This paper explores how DHH learners interact with LLM-powered AI tutors with different experiences in DHH education as personas to identify their accessibility preferences. A user study with 16 DHH participants showed that they asked DHH-related questions based on background information and evaluated the AI tutors' cultural knowledge of the DHH communities in their responses. Participants suggested providing more transparency in each AI tutor's position within the DHH community. Participants also pointed out the lack of support in the multimodality of sign language in current LLMs. We discuss design implications to support the diverse needs in interaction between DHH users and the LLMs, such as offering supports in tuning language styles of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2411.09873v2),  [pdf](http://arxiv.org/pdf/2411.09873v2)

**Tags**: cs.HC 



### When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks   Silently Undermine Validity
**Authors**: Benjamin Feuer, Chiung-Yi Tseng, Astitwa Sarthak Lathe, Oussama Elachqar, John P Dickerson

**Updated**: 2025-09-24T16:26:47Z

**Summary**: LLM-judged benchmarks are increasingly used to evaluate complex model behaviors, yet their design introduces failure modes absent in conventional ground-truth based benchmarks. We argue that without tight objectives and verifiable constructions, benchmark rankings can produce high-confidence rankings that are in fact largely noise. We introduce two mechanisms to diagnose these issues. Schematic adherence quantifies how much of a judge's overall verdict is explained by the explicit evaluation schema, revealing unexplained variance when judges deviate from their own rubric. Psychometric validity aggregates internal consistency and discriminant validity signals to quantify irreducible uncertainty in any benchmarking run. Applying these tools to Arena-Hard Auto, we find severe schema incoherence and factor collapse across popular judges: for example, unexplained variance exceeding 90 percent for DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We also show that the ELO-style aggregation used by Arena-Hard Auto collapses and masks genuine ranking uncertainty. Our results highlight design failures that undermine validity and offer actionable principles for building better-scoped, reliability-aware LLM-judged benchmarks. We release our code at https://anonymous.4open.science/r/judgment-to-noise-947D/README.md

**Link**: [arxiv](http://arxiv.org/abs/2509.20293v1),  [pdf](http://arxiv.org/pdf/2509.20293v1)

**Tags**: cs.LG cs.AI 



### Monitoring Violations of Differential Privacy over Time
**Authors**: Önder Askin, Tim Kutta, Holger Dette

**Updated**: 2025-09-24T16:15:51Z

**Summary**: Auditing differential privacy has emerged as an important area of research that supports the design of privacy-preserving mechanisms. Privacy audits help to obtain empirical estimates of the privacy parameter, to expose flawed implementations of algorithms and to compare practical with theoretical privacy guarantees. In this work, we investigate an unexplored facet of privacy auditing: the sustained auditing of a mechanism that can go through changes during its development or deployment. Monitoring the privacy of algorithms over time comes with specific challenges. Running state-of-the-art (static) auditors repeatedly requires excessive sampling efforts, while the reliability of such methods deteriorates over time without proper adjustments. To overcome these obstacles, we present a new monitoring procedure that extracts information from the entire deployment history of the algorithm. This allows us to reduce sampling efforts, while sustaining reliable outcomes of our auditor. We derive formal guarantees with regard to the soundness of our methods and evaluate their performance for important mechanisms from the literature. Our theoretical findings and experiments demonstrate the efficacy of our approach.

**Link**: [arxiv](http://arxiv.org/abs/2509.20283v1),  [pdf](http://arxiv.org/pdf/2509.20283v1)

**Tags**: cs.CR math.ST stat.ME stat.TH 



### Instruction Boundary: Quantifying Biases in LLM Reasoning under Various   Coverage
**Authors**: Zipeng Ling, Yuehao Tang, Chen Huang, Shuliang Liu, Gaoyang Jiang, Shenghong Fu, Junqi Yang, Yao Wan, Jiawan Zhang, Kejia Huang, Xuming Hu

**Updated**: 2025-09-24T16:15:26Z

**Summary**: Large-language-model (LLM) reasoning has long been regarded as a powerful tool for problem solving across domains, providing non-experts with valuable advice. However, their limitations - especially those stemming from prompt design - remain underexplored. Because users may supply biased or incomplete prompts - often unintentionally - LLMs can be misled, undermining reliability and creating risks. We refer to this vulnerability as the Instruction Boundary. To investigate the phenomenon, we distill it into eight concrete facets and introduce BiasDetector, a framework that measures biases arising from three instruction types: complete, redundant, and insufficient. We evaluate several mainstream LLMs and find that, despite high headline accuracy, substantial biases persist in many downstream tasks as a direct consequence of prompt coverage. Our empirical study confirms that LLM reasoning reliability can still be significantly improved. We analyze the practical impact of these biases and outline mitigation strategies. Our findings underscore the need for developers to tackle biases and for users to craft options carefully.

**Link**: [arxiv](http://arxiv.org/abs/2509.20278v1),  [pdf](http://arxiv.org/pdf/2509.20278v1)

**Tags**: cs.CL 



### Investigating Security Implications of Automatically Generated Code on   the Software Supply Chain
**Authors**: Xiaofan Li, Xing Gao

**Updated**: 2025-09-24T16:15:17Z

**Summary**: In recent years, various software supply chain (SSC) attacks have posed significant risks to the global community. Severe consequences may arise if developers integrate insecure code snippets that are vulnerable to SSC attacks into their products. Particularly, code generation techniques, such as large language models (LLMs), have been widely utilized in the developer community. However, LLMs are known to suffer from inherent issues when generating code, including fabrication, misinformation, and reliance on outdated training data, all of which can result in serious software supply chain threats. In this paper, we investigate the security threats to the SSC that arise from these inherent issues. We examine three categories of threats, including eleven potential SSC-related threats, related to external components in source code, and continuous integration configuration files. We find some threats in LLM-generated code could enable attackers to hijack software and workflows, while some others might cause potential hidden threats that compromise the security of the software over time. To understand these security impacts and severity, we design a tool, SSCGuard, to generate 439,138 prompts based on SSC-related questions collected online, and analyze the responses of four popular LLMs from GPT and Llama. Our results show that all identified SSC-related threats persistently exist. To mitigate these risks, we propose a novel prompt-based defense mechanism, namely Chain-of-Confirmation, to reduce fabrication, and a middleware-based defense that informs users of various SSC threats.

**Link**: [arxiv](http://arxiv.org/abs/2509.20277v1),  [pdf](http://arxiv.org/pdf/2509.20277v1)

**Tags**: cs.CR cs.AI 



### Expanding the WMT24++ Benchmark with Rumantsch Grischun, Sursilvan,   Sutsilvan, Surmiran, Puter, and Vallader
**Authors**: Jannis Vamvas, Ignacio Pérez Prat, Not Battesta Soliva, Sandra Baltermia-Guetg, Andrina Beeli, Simona Beeli, Madlaina Capeder, Laura Decurtins, Gian Peder Gregori, Flavia Hobi, Gabriela Holderegger, Arina Lazzarini, Viviana Lazzarini, Walter Rosselli, Bettina Vital, Anna Rutkiewicz, Rico Sennrich

**Updated**: 2025-09-24T16:07:19Z

**Summary**: The Romansh language, spoken in Switzerland, has limited resources for machine translation evaluation. In this paper, we present a benchmark for six varieties of Romansh: Rumantsch Grischun, a supra-regional variety, and five regional varieties: Sursilvan, Sutsilvan, Surmiran, Puter, and Vallader. Our reference translations were created by human translators based on the WMT24++ benchmark, which ensures parallelism with more than 55 other languages. An automatic evaluation of existing MT systems and LLMs shows that translation out of Romansh into German is handled relatively well for all the varieties, but translation into Romansh is still challenging.

**Link**: [arxiv](http://arxiv.org/abs/2509.03148v2),  [pdf](http://arxiv.org/pdf/2509.03148v2)

**Tags**: cs.CL 



### Who is Responsible? The Data, Models, Users or Regulations? A   Comprehensive Survey on Responsible Generative AI for a Sustainable Future
**Authors**: Shaina Raza, Rizwan Qureshi, Anam Zahid, Safiullah Kamawal, Ferhat Sadak, Joseph Fioresi, Muhammaed Saeed, Ranjan Sapkota, Aditya Jain, Anas Zafar, Muneeb Ul Hassan, Aizan Zafar, Hasan Maqbool, Ashmal Vayani, Jia Wu, Maged Shoman

**Updated**: 2025-09-24T16:04:24Z

**Summary**: Generative AI is moving rapidly from research into real world deployment across sectors, which elevates the need for responsible development, deployment, evaluation, and governance. To address this pressing challenge, in this study, we synthesize the landscape of responsible generative AI across methods, benchmarks, and policies, and connects governance expectations to concrete engineering practice. We follow a prespecified search and screening protocol focused on post-ChatGPT era with selective inclusion of foundational work for definitions, and we conduct a narrative and thematic synthesis. Three findings emerge; First, benchmark and practice coverage is dense for bias and toxicity but relatively sparse for privacy and provenance, deepfake and media integrity risk, and system level failure in tool using and agentic settings. Second, many evaluations remain static and task local, which limits evidence portability for audit and lifecycle assurance. Third, documentation and metric validity are inconsistent, which complicates comparison across releases and domains. We outline a research and practice agenda that prioritizes adaptive and multimodal evaluation, privacy and provenance testing, deepfake risk assessment, calibration and uncertainty reporting, versioned and documented artifacts, and continuous monitoring. Limitations include reliance on public artifacts and the focus period, which may under represent capabilities reported later. The survey offers a path to align development and evaluation with governance needs and to support safe, transparent, and accountable deployment across domains. Project page: https://anas-zafar.github.io/responsible-ai.github.io , GitHub: https://github.com/anas-zafar/Responsible-AI

**Link**: [arxiv](http://arxiv.org/abs/2502.08650v5),  [pdf](http://arxiv.org/pdf/2502.08650v5)

**Tags**: cs.CY 



### Scan-do Attitude: Towards Autonomous CT Protocol Management using a   Large Language Model Agent
**Authors**: Xingjian Kang, Linda Vorberg, Andreas Maier, Alexander Katzmann, Oliver Taubmann

**Updated**: 2025-09-24T16:04:11Z

**Summary**: Managing scan protocols in Computed Tomography (CT), which includes adjusting acquisition parameters or configuring reconstructions, as well as selecting postprocessing tools in a patient-specific manner, is time-consuming and requires clinical as well as technical expertise. At the same time, we observe an increasing shortage of skilled workforce in radiology. To address this issue, a Large Language Model (LLM)-based agent framework is proposed to assist with the interpretation and execution of protocol configuration requests given in natural language or a structured, device-independent format, aiming to improve the workflow efficiency and reduce technologists' workload. The agent combines in-context-learning, instruction-following, and structured toolcalling abilities to identify relevant protocol elements and apply accurate modifications. In a systematic evaluation, experimental results indicate that the agent can effectively retrieve protocol components, generate device compatible protocol definition files, and faithfully implement user requests. Despite demonstrating feasibility in principle, the approach faces limitations regarding syntactic and semantic validity due to lack of a unified device API, and challenges with ambiguous or complex requests. In summary, the findings show a clear path towards LLM-based agents for supporting scan protocol management in CT imaging.

**Link**: [arxiv](http://arxiv.org/abs/2509.20270v1),  [pdf](http://arxiv.org/pdf/2509.20270v1)

**Tags**: cs.AI cs.CL 



### Radiation-induced Ionization Effects and Space Mission Requirements for   Silicon Photonic Mach-Zehnder Modulators
**Authors**: Kellen P. Arnold, Joel B. Slaby, Nathaniel J. Karom, Anurag R. Veluri, C. Alex Kaylor, Andrew L. Sternberg, Dennis R. Ball, Ronald D. Schrimpf, Daniel M. Fleetwood, Stephen E. Ralph, Robert A. Reed, Sharon M. Weiss

**Updated**: 2025-09-24T15:59:06Z

**Summary**: Photonic integrated circuits have become essential for meeting the growing global demand for high-capacity information processing and transport. Assessing their radiation tolerance is essential for deploying systems in radiation prone environments - including in space, high-energy particle accelerators, and defense radiation testing facilities - where the performance and compactness of photonic integrated circuits are increasingly advantageous. This work investigates the analog and digital radio frequency electro-optic performance of Mach-Zehnder modulators (MZMs) subject to 10-keV X-ray irradiation, which mimics cumulative ionization effects in space flight. Silicon photonic MZMs serve as excellent exemplars since they are interferometric devices comprised of elements common to many integrated photonic circuits. Under standard bias conditions, the irradiated MZMs exhibited significantly reduced bandwidth, a corresponding eye closure and baud rate dependent increases in the estimated error rate. The observed performance degradation is attributed to total ionizing dose effects which leads to hole trapping at the silicon/silicon dioxide waveguide interfaces as well as fast traps with energies near the conduction band edge. Notably, when MZMs were irradiated with all leads grounded, no radiation sensitivity to the electro-optic response was observed highlighting the importance of testing under standard operating conditions for ground-based radiation testing as well as on-orbit studies. Understanding the radiation induced performance degradation of MZMs and other integrated photonic devices is increasingly important for space and accelerator environments as performance requirements and deployment opportunities increase.

**Link**: [arxiv](http://arxiv.org/abs/2509.20267v1),  [pdf](http://arxiv.org/pdf/2509.20267v1)

**Tags**: physics.app-ph 



### AAPO: Enhancing the Reasoning Capabilities of LLMs with Advantage   Momentum
**Authors**: Jian Xiong, Jingbo Zhou, Jingyong Ye, Qiang Huang, Dejing Dou

**Updated**: 2025-09-24T15:47:49Z

**Summary**: Reinforcement learning (RL) has emerged as an effective approach for enhancing the reasoning capabilities of large language models (LLMs), especially in scenarios where supervised fine-tuning (SFT) falls short due to limited chain-of-thought (CoT) data. Among RL-based post-training methods, group relative advantage estimation, as exemplified by Group Relative Policy Optimization (GRPO), has attracted considerable attention for eliminating the dependency on the value model, thereby simplifying training compared to traditional approaches like Proximal Policy Optimization (PPO). However, we observe that exsiting group relative advantage estimation method still suffers from training inefficiencies, particularly when the estimated advantage approaches zero. To address this limitation, we propose Advantage-Augmented Policy Optimization (AAPO), a novel RL algorithm that optimizes the cross-entropy (CE) loss using advantages enhanced through a momentum-based estimation scheme. This approach effectively mitigates the inefficiencies associated with group relative advantage estimation. Experimental results on multiple mathematical reasoning benchmarks demonstrate the superior performance of AAPO.

**Link**: [arxiv](http://arxiv.org/abs/2505.14264v2),  [pdf](http://arxiv.org/pdf/2505.14264v2)

**Tags**: cs.LG cs.CL 



### Guided Reality: Generating Visually-Enriched AR Task Guidance with LLMs   and Vision Models
**Authors**: Ada Yi Zhao, Aditya Gunturu, Ellen Yi-Luen Do, Ryo Suzuki

**Updated**: 2025-09-24T15:41:44Z

**Summary**: Large language models (LLMs) have enabled the automatic generation of step-by-step augmented reality (AR) instructions for a wide range of physical tasks. However, existing LLM-based AR guidance often lacks rich visual augmentations to effectively embed instructions into spatial context for a better user understanding. We present Guided Reality, a fully automated AR system that generates embedded and dynamic visual guidance based on step-by-step instructions. Our system integrates LLMs and vision models to: 1) generate multi-step instructions from user queries, 2) identify appropriate types of visual guidance, 3) extract spatial information about key interaction points in the real world, and 4) embed visual guidance in physical space to support task execution. Drawing from a corpus of user manuals, we define five categories of visual guidance and propose an identification strategy based on the current step. We evaluate the system through a user study (N=16), completing real-world tasks and exploring the system in the wild. Additionally, four instructors shared insights on how Guided Reality could be integrated into their training workflows.

**Link**: [arxiv](http://arxiv.org/abs/2508.03547v2),  [pdf](http://arxiv.org/pdf/2508.03547v2)

**Tags**: cs.HC 



### CANDLE: A Cross-Modal Agentic Knowledge Distillation Framework for   Interpretable Sarcopenia Diagnosis
**Authors**: Yuqi Jin, Zhenhao Shuai, Zihan Hu, Weiteng Zhang, Weihao Xie, Jianwei Shuai, Xian Shen, Zhen Feng

**Updated**: 2025-09-24T15:38:14Z

**Summary**: Background and Aims: Large language models (LLMs) have shown remarkable generalization and transfer capabilities by learning from vast corpora of text and web data. Their semantic representations allow cross-task knowledge transfer and reasoning, offering promising opportunities for data-scarce and heterogeneous domains such as clinical medicine. Yet, in diagnostic tasks like sarcopenia, major challenges remain: interpretability, transparency, and deployment efficiency. Traditional machine learning (TML) models provide stable performance and feature-level attribution, ensuring traceable and auditable decision logic, but lack semantic breadth. Conversely, LLMs enable flexible inference but often function as opaque predictors. Existing integration strategies remain shallow, rarely embedding the structured reasoning of TML into LLM inference. Methods: Using sarcopenia diagnosis as a case study, SHapley Additive exPlanations (SHAP) were extracted from a baseline XGBoost model and transformed into structured, LLM-compatible representations. An actor-critic reinforcement learning (RL) strategy guided the LLM to reason over these SHAP-based inputs, producing calibrated rationales and refined decision rules. The distilled reasoning was consolidated into a structured knowledge repository and deployed via retrieval-augmented generation (RAG) for case-based inference. Results: (Omitted here.) Conclusion: By coupling SHAP-derived statistical evidence with reinforcement-trained LLM reasoning, CANDLE mitigates the interpretability-performance trade-off, enhances predictive accuracy, and preserves high decision consistency. The framework offers a scalable approach to knowledge assetization of TML models, enabling interpretable, reproducible, and clinically aligned decision support in sarcopenia and potentially broader medical domains.

**Link**: [arxiv](http://arxiv.org/abs/2507.21179v2),  [pdf](http://arxiv.org/pdf/2507.21179v2)

**Tags**: cs.LG cs.AI 



### Energy Use of AI Inference: Efficiency Pathways and Test-Time Compute
**Authors**: Felipe Oviedo, Fiodar Kazhamiaka, Esha Choukse, Allen Kim, Amy Luers, Melanie Nakagawa, Ricardo Bianchini, Juan M. Lavista Ferres

**Updated**: 2025-09-24T15:32:01Z

**Summary**: As AI inference scales to billions of queries and emerging reasoning and agentic workflows increase token demand, reliable estimates of per-query energy use are increasingly important for capacity planning, emissions accounting, and efficiency prioritization. Many public estimates are inconsistent and overstate energy use, because they extrapolate from limited benchmarks and fail to reflect efficiency gains achievable at scale. In this perspective, we introduce a bottom-up methodology to estimate the per-query energy of large-scale LLM systems based on token throughput. For models running on an H100 node under realistic workloads, GPU utilization and PUE constraints, we estimate a median energy per query of 0.34 Wh (IQR: 0.18-0.67) for frontier-scale models (>200 billion parameters). These results are consistent with measurements using production-scale configurations and show that non-production estimates and assumptions can overstate energy use by 4-20x. Extending to test-time scaling scenarios with 15x more tokens per typical query, the median energy rises 13x to 4.32 Wh, indicating that targeting efficiency in this regime will deliver the largest fleet-wide savings. We quantify achievable efficiency gains at the model, serving platform, and hardware levels, finding individual median reductions of 1.5-3.5x in energy per query, while combined advances can plausibly deliver 8-20x reductions. To illustrate the system-level impact, we estimate the baseline daily energy use of a deployment serving 1 billion queries to be 0.8 GWh/day. If 10% are long queries, demand could grow to 1.8 GWh/day. With targeted efficiency interventions, it falls to 0.9 GWh/day, similar to the energy footprint of web search at that scale. This echoes how data centers historically tempered energy growth through efficiency gains during the internet and cloud build-up.

**Link**: [arxiv](http://arxiv.org/abs/2509.20241v1),  [pdf](http://arxiv.org/pdf/2509.20241v1)

**Tags**: cs.LG cs.DC 



### Context-Masked Meta-Prompting for Privacy-Preserving LLM Adaptation in   Finance
**Authors**: Sayash Raaj Hiraou

**Updated**: 2025-09-24T15:30:03Z

**Summary**: The increasing reliance on Large Language Models (LLMs) in sensitive domains like finance necessitates robust methods for privacy preservation and regulatory compliance. This paper presents an iterative meta-prompting methodology designed to optimise hard prompts without exposing proprietary or confidential context to the LLM. Through a novel regeneration process involving feeder and propagation methods, we demonstrate significant improvements in prompt efficacy. Evaluated on public datasets serving as proxies for financial tasks such as SQuAD for extractive financial Q&A, CNN/DailyMail for news summarisation, and SAMSum for client interaction summarisation, our approach, utilising GPT-3.5 Turbo, achieved a 103.87% improvement in ROUGE-L F1 for question answering. This work highlights a practical, low-cost strategy for adapting LLMs to financial applications while upholding critical privacy and auditability standards, offering a compelling case for its relevance in the evolving landscape of generative AI in finance.

**Link**: [arxiv](http://arxiv.org/abs/2407.18920v2),  [pdf](http://arxiv.org/pdf/2407.18920v2)

**Tags**: cs.CL 



### Long Video Understanding with Learnable Retrieval in Video-Language   Models
**Authors**: Jiaqi Xu, Cuiling Lan, Wenxuan Xie, Xuejin Chen, Yan Lu

**Updated**: 2025-09-24T15:24:43Z

**Summary**: The remarkable natural language understanding, reasoning, and generation capabilities of large language models (LLMs) have made them attractive for application to video understanding, utilizing video tokens as contextual input. However, employing LLMs for long video understanding presents significant challenges. The extensive number of video tokens leads to considerable computational costs for LLMs while using aggregated tokens results in loss of vision details. Moreover, the presence of abundant question-irrelevant tokens introduces noise to the video reasoning process. To address these issues, we introduce a simple yet effective learnable retrieval-based video-language model (R-VLM) for efficient long video understanding. Specifically, given a question (query) and a long video, our model identifies and selects the most relevant K video chunks and uses their associated visual tokens to serve as context for the LLM inference. This effectively reduces the number of video tokens, eliminates noise interference, and enhances system performance. We achieve this by incorporating a learnable lightweight MLP block to facilitate the efficient retrieval of question-relevant chunks, through the end-to-end training of our video-language model with a proposed soft matching loss. Our experimental results on multiple zero-shot video question answering datasets validate the effectiveness of our framework for comprehending long videos.

**Link**: [arxiv](http://arxiv.org/abs/2312.04931v3),  [pdf](http://arxiv.org/pdf/2312.04931v3)

**Tags**: cs.CV 



### Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided   Multi-Point Optimization
**Authors**: Wenhan Wu, Zheyuan Liu, Chongyang Gao, Ren Wang, Kaize Ding

**Updated**: 2025-09-24T15:23:46Z

**Summary**: Current LLM unlearning methods face a critical security vulnerability that undermines their fundamental purpose: while they appear to successfully remove sensitive or harmful knowledge, this ``forgotten" information remains precariously recoverable through relearning attacks. We identify that the root cause is that conventional methods optimizing the forgetting loss at individual data points will drive model parameters toward sharp minima in the loss landscape. In these unstable regions, even minimal parameter perturbations can drastically alter the model's behaviors. Consequently, relearning attacks exploit this vulnerability by using just a few fine-tuning samples to navigate the steep gradients surrounding these unstable regions, thereby rapidly recovering knowledge that was supposedly erased. This exposes a critical robustness gap between apparent unlearning and actual knowledge removal. To address this issue, we propose StableUN, a bi-level feedback-guided optimization framework that explicitly seeks more stable parameter regions via neighborhood-aware optimization. It integrates forgetting feedback, which uses adversarial perturbations to probe parameter neighborhoods, with remembering feedback to preserve model utility, aligning the two objectives through gradient projection. Experiments on WMDP and MUSE benchmarks demonstrate that our method is significantly more robust against both relearning and jailbreaking attacks while maintaining competitive utility performance.

**Link**: [arxiv](http://arxiv.org/abs/2509.20230v1),  [pdf](http://arxiv.org/pdf/2509.20230v1)

**Tags**: cs.LG cs.AI 



### Techno-Economic analysis for Smart Hangar inspection operations through   Sensing and Localisation at scale
**Authors**: Angelos Plastropoulos, Nicolas P. Avdelidis, Argyrios Zolotas

**Updated**: 2025-09-24T15:22:32Z

**Summary**: The accuracy, resilience, and affordability of localisation are fundamental to autonomous robotic inspection within aircraft maintenance and overhaul (MRO) hangars. Hangars typically feature tall ceilings and are often made of materials such as metal. Due to its nature, it is considered a GPS-denied environment, with extensive multipath effects and stringent operational constraints that collectively create a uniquely challenging environment. This persistent gap highlights the need for domain-specific comparative studies, including rigorous cost, accuracy, and integration assessments, to inform a reliable and scalable deployment of a localisation system in the Smart Hangar. This paper presents the first techno-economic roadmap that benchmarks motion capture (MoCap), ultra-wideband (UWB), and a ceiling-mounted camera network across three operational scenarios: robot localisation, asset tracking, and surface defect detection within a 40x50 m hangar bay. A dual-layer optimisation for camera selection and positioning framework is introduced, which couples market-based camera-lens selection with an optimisation solver, producing camera layouts that minimise hardware while meeting accuracy targets. The roadmap equips MRO planners with an actionable method to balance accuracy, coverage, and budget, demonstrating that an optimised vision architecture has the potential to unlock robust and cost-effective sensing for next-generation Smart Hangars.

**Link**: [arxiv](http://arxiv.org/abs/2509.20229v1),  [pdf](http://arxiv.org/pdf/2509.20229v1)

**Tags**: cs.RO 



### Design Insights and Comparative Evaluation of a Hardware-Based   Cooperative Perception Architecture for Lane Change Prediction
**Authors**: Mohamed Manzour, Catherine M. Elias, Omar M. Shehata, Rubén Izquierdo, Miguel Ángel Sotelo

**Updated**: 2025-09-24T15:15:05Z

**Summary**: Research on lane change prediction has gained attention in the last few years. Most existing works in this area have been conducted in simulation environments or with pre-recorded datasets, these works often rely on simplified assumptions about sensing, communication, and traffic behavior that do not always hold in practice. Real-world deployments of lane-change prediction systems are relatively rare, and when they are reported, the practical challenges, limitations, and lessons learned are often under-documented. This study explores cooperative lane-change prediction through a real hardware deployment in mixed traffic and shares the insights that emerged during implementation and testing. We highlight the practical challenges we faced, including bottlenecks, reliability issues, and operational constraints that shaped the behavior of the system. By documenting these experiences, the study provides guidance for others working on similar pipelines.

**Link**: [arxiv](http://arxiv.org/abs/2509.20218v1),  [pdf](http://arxiv.org/pdf/2509.20218v1)

**Tags**: cs.AI cs.AR cs.CV cs.LG 



### The Cream Rises to the Top: Efficient Reranking Method for Verilog Code   Generation
**Authors**: Guang Yang, Wei Zheng, Xiang Chen, Yifan Sun, Fengji Zhang, Terry Yue Zhuo

**Updated**: 2025-09-24T15:12:21Z

**Summary**: LLMs face significant challenges in Verilog generation due to limited domain-specific knowledge. While sampling techniques improve pass@k metrics, hardware engineers need one trustworthy solution rather than uncertain candidates. To bridge this gap, we formulate it as a semantic alignment problem between requirements and Verilog implementations, and propose VCD-RNK, a discriminator model tailored for efficient Verilog code reranking. Specifically, VCD-RNKincorporates Verilog-specific reasoning by distilling expert knowledge across three dimensions: code semantic analysis, test case generation, and functional correctness assessment. By explicitly simulating the above reasoning processes during inference, VCD-RNK effectively avoids computationally intensive test execution in existing methods.

**Link**: [arxiv](http://arxiv.org/abs/2509.20215v1),  [pdf](http://arxiv.org/pdf/2509.20215v1)

**Tags**: cs.SE cs.AI cs.AR 



### Q-Palette: Fractional-Bit Quantizers Toward Optimal Bit Allocation for   Efficient LLM Deployment
**Authors**: Deokjae Lee, Hyun Oh Song

**Updated**: 2025-09-24T15:10:44Z

**Summary**: We study weight-only post-training quantization (PTQ), which quantizes the weights of a large language model (LLM) without retraining, using little or no calibration data. Weight-only PTQ is crucial for reducing the memory footprint and latency of LLM inference, especially in memory-bound, small-batch inference scenarios, such as personalized inference on edge devices. Despite its importance, irregular weight distributions with heavy-tailed outliers in LLMs complicate quantization, recently motivating rotation-based methods that transform weights into near-Gaussian distributions, which are more regular with fewer outliers, thereby reducing quantization error. In this work, we first derive the information-theoretically optimal bit allocation for Gaussianized weights under given bit budgets, revealing that fine-grained fractional-bit quantizers approaching the Gaussian distortion-rate bound are essential to achieve near-optimal quantization performance. To bridge this theoretical insight and practical implementation, we introduce Q-Palette, a versatile collection of fractional-bit quantizers that range from trellis-coded quantizers offering near-optimal distortion to simpler vector and scalar quantizers optimized for faster inference, all efficiently implemented with optimized CUDA kernels across various bitwidths. Furthermore, leveraging Q-Palette as a foundational component, we propose a novel mixed-scheme quantization framework, jointly optimizing quantizer choices and layer fusion decisions given resource constraints. The code is available at https://github.com/snu-mllab/Q-Palette.

**Link**: [arxiv](http://arxiv.org/abs/2509.20214v1),  [pdf](http://arxiv.org/pdf/2509.20214v1)

**Tags**: cs.LG cs.AI 



### Decentralised Multi-Manager Fund Framework
**Authors**: Arman Abgaryan, Utkarsh Sharma, Joshua Tobkin

**Updated**: 2025-09-24T15:07:08Z

**Summary**: We introduce a decentralised, algorithmic framework for permissionless, multi-strategy capital allocation via tokenised, automated vaults. The system is designed to function analogously to a multi-strategy asset management company, but implemented entirely on-chain through a modular architecture comprising four interacting layers. The first, the capitalisation layer, composed of vaults that facilitate multi-asset deposits, tokenises investor participation, and specifies high level risk limits and admissible venues for deployment. The second, the strategy layer, enables the submission of strategies by human developers or autonomous agents, creating a decentralised marketplace governed by a validation mechanism incorporating adversarial and gamified elements. The third, the execution layer, operationalises strategy deployment using the host blockchain network's services. The fourth layer, the validated allocation layer, assesses and allocates capital among validated strategies, dynamically rebalancing toward those exhibiting superior risk-adjusted performance. In the framework, each admitted strategy acts as a manager for the "fund", encapsulated in a smart contract vault that issues transferable V-Tokens, conveying fractional ownership of the real-time portfolio operated by the vault. The system is designed to be open to participation by both human and AI agents, who collectively perform the roles of capital allocators, strategy developers, and validated allocators. The resulting structure is a self-regulating asset management ecosystem capable of decentralised, cooperative optimisation across traditional and digital financial domains. This framework is facilitated by a host chain network, which offers native automation and data oracle services enabling vault entities to autonomously operate on-chain, paving the way for being self sufficient in dynamic allocation of capital.

**Link**: [arxiv](http://arxiv.org/abs/2507.00978v2),  [pdf](http://arxiv.org/pdf/2507.00978v2)

**Tags**: q-fin.GN 



### UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning
**Authors**: Zhengxi Lu, Jiabo Ye, Fei Tang, Yongliang Shen, Haiyang Xu, Ziwei Zheng, Weiming Lu, Ming Yan, Fei Huang, Jun Xiao, Yueting Zhuang

**Updated**: 2025-09-24T15:05:34Z

**Summary**: Graphical User Interface (GUI) agents have demonstrated remarkable progress in automating complex user interface interactions through reinforcement learning. However, current approaches face a fundamental dilemma: offline RL enables stable training on pre-collected trajectories, but struggles with multi-step task execution for lack of trajectory-level reward signals; online RL captures these signals through environment interaction, but suffers from sparse rewards and prohibitive deployment costs. To address it, we present Semi-online Reinforcement Learning, a novel paradigm that simulates online RL on offline trajectories. During each rollout process, we preserve the original model output within the multi-turn dialogue, where a Patch Module adaptively recovers the divergence between rollout and expert trajectories. To capture long-term training signals, Semi-online RL introduces discounted future returns into the reward computation and optimizes the policy with weighted step-level and episode-level advantages. We further introduce Semi-Online Performance (SOP), a metric that aligns better with true online performance, serving as a practical and effective proxy for real-world evaluation. Experiments show that ours Semi-online RL achieves SOTA performance among 7B models across four dynamic benchmarks, with significant gains over the base model (e.g., +12.0% on AndroidWorld, +23.8% on AITW), demonstrating significant progress in bridging the gap between offline training efficiency and online multi-turn reasoning. The code is available at https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1.

**Link**: [arxiv](http://arxiv.org/abs/2509.11543v2),  [pdf](http://arxiv.org/pdf/2509.11543v2)

**Tags**: cs.LG cs.AI 



### Play by the Type Rules: Inferring Constraints for LLM Functions in   Declarative Programs
**Authors**: Parker Glenn, Alfy Samuel, Daben Liu

**Updated**: 2025-09-24T15:02:33Z

**Summary**: Integrating LLM powered operators in declarative query languages allows for the combination of cheap and interpretable functions with powerful, generalizable language model reasoning. However, in order to benefit from the optimized execution of a database query language like SQL, generated outputs must align with the rules enforced by both type checkers and database contents. Current approaches address this challenge with orchestrations consisting of many LLM-based post-processing calls to ensure alignment between generated outputs and database values, introducing performance bottlenecks. We perform a study on the ability of various sized open-source language models to both parse and execute functions within a query language based on SQL, showing that small language models can excel as function executors over hybrid data sources. Then, we propose an efficient solution to enforce the well-typedness of LLM functions, demonstrating 7% accuracy improvement on a multi-hop question answering dataset with 53% improvement in latency over comparable solutions. We make our implementation available at https://github.com/parkervg/blendsql

**Link**: [arxiv](http://arxiv.org/abs/2509.20208v1),  [pdf](http://arxiv.org/pdf/2509.20208v1)

**Tags**: cs.CL cs.AI cs.DB 



### LLMs Reproduce Stereotypes of Sexual and Gender Minorities
**Authors**: Ruby Ostrow, Adam Lopez

**Updated**: 2025-09-24T14:58:32Z

**Summary**: A large body of research has found substantial gender bias in NLP systems. Most of this research takes a binary, essentialist view of gender: limiting its variation to the categories _men_ and _women_, conflating gender with sex, and ignoring different sexual identities. But gender and sexuality exist on a spectrum, so in this paper we study the biases of large language models (LLMs) towards sexual and gender minorities beyond binary categories. Grounding our study in a widely used social psychology model -- the Stereotype Content Model -- we demonstrate that English-language survey questions about social perceptions elicit more negative stereotypes of sexual and gender minorities from both humans and LLMs. We then extend this framework to a more realistic use case: text generation. Our analysis shows that LLMs generate stereotyped representations of sexual and gender minorities in this setting, showing that they amplify representational harms in creative writing, a widely advertised use for LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2501.05926v3),  [pdf](http://arxiv.org/pdf/2501.05926v3)

**Tags**: cs.CL 



### Procedural Environment Generation for Tool-Use Agents
**Authors**: Michael Sullivan, Mareike Hartmann, Alexander Koller

**Updated**: 2025-09-24T14:57:25Z

**Summary**: Although the power of LLM tool-use agents has ignited a flurry of recent research in this area, the curation of tool-use training data remains an open problem$-$especially for online RL training. Existing approaches to synthetic tool-use data generation tend to be non-interactive, and/or non-compositional. We introduce RandomWorld, a pipeline for the procedural generation of interactive tools and compositional tool-use data. We show that models tuned via SFT and RL on synthetic RandomWorld data improve on a range of tool-use benchmarks, and set the new SoTA for two metrics on the NESTFUL dataset. Further experiments show that downstream performance scales with the amount of RandomWorld-generated training data, opening up the possibility of further improvement through the use of entirely synthetic data.

**Link**: [arxiv](http://arxiv.org/abs/2506.11045v2),  [pdf](http://arxiv.org/pdf/2506.11045v2)

**Tags**: cs.LG I.2.7 



### Universal Camouflage Attack on Vision-Language Models for Autonomous   Driving
**Authors**: Dehong Kong, Sifan Yu, Siyuan Liang, Jiawei Liang, Jianhou Gan, Aishan Liu, Wenqi Ren

**Updated**: 2025-09-24T14:52:01Z

**Summary**: Visual language modeling for automated driving is emerging as a promising research direction with substantial improvements in multimodal reasoning capabilities. Despite its advanced reasoning abilities, VLM-AD remains vulnerable to serious security threats from adversarial attacks, which involve misleading model decisions through carefully crafted perturbations. Existing attacks have obvious challenges: 1) Physical adversarial attacks primarily target vision modules. They are difficult to directly transfer to VLM-AD systems because they typically attack low-level perceptual components. 2) Adversarial attacks against VLM-AD have largely concentrated on the digital level. To address these challenges, we propose the first Universal Camouflage Attack (UCA) framework for VLM-AD. Unlike previous methods that focus on optimizing the logit layer, UCA operates in the feature space to generate physically realizable camouflage textures that exhibit strong generalization across different user commands and model architectures. Motivated by the observed vulnerability of encoder and projection layers in VLM-AD, UCA introduces a feature divergence loss (FDL) that maximizes the representational discrepancy between clean and adversarial images. In addition, UCA incorporates a multi-scale learning strategy and adjusts the sampling ratio to enhance its adaptability to changes in scale and viewpoint diversity in real-world scenarios, thereby improving training stability. Extensive experiments demonstrate that UCA can induce incorrect driving commands across various VLM-AD models and driving scenarios, significantly surpassing existing state-of-the-art attack methods (improving 30\% in 3-P metrics). Furthermore, UCA exhibits strong attack robustness under diverse viewpoints and dynamic conditions, indicating high potential for practical deployment.

**Link**: [arxiv](http://arxiv.org/abs/2509.20196v1),  [pdf](http://arxiv.org/pdf/2509.20196v1)

**Tags**: cs.CV cs.LG 



### Detecting Token-Level Hallucinations Using Variance Signals: A   Reference-Free Approach
**Authors**: Keshav Kumar

**Updated**: 2025-09-24T14:48:30Z

**Summary**: Large Language Models (LLMs) have demonstrated impressive generative capabilities across diverse tasks but remain susceptible to hallucinations, confidently generated yet factually incorrect outputs. We introduce a reference-free, token-level hallucination detection framework that leverages the variance in token log-probabilities across multiple stochastic generations. Unlike prior methods that require ground-truth references or sentence-level verification, our approach is model-agnostic, interpretable, and suited for real-time or post-hoc analysis. We evaluate our method on unanswerable question prompts from the SQuAD v2 dataset and benchmark across three autoregressive models of varying scales: GPT-Neo 125M, Falcon 1B, and Mistral 7B. Through both quantitative metrics and visual diagnostics, we show that token-level variance reliably highlights instability in model outputs and correlates with hallucination patterns. Our framework is lightweight, reproducible, and adaptable to multiple domains, offering a valuable diagnostic tool for analyzing generative reliability in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2507.04137v2),  [pdf](http://arxiv.org/pdf/2507.04137v2)

**Tags**: cs.CL cs.LG 



### STAF: Leveraging LLMs for Automated Attack Tree-Based Security Test   Generation
**Authors**: Tanmay Khule, Stefan Marksteiner, Jose Alguindigue, Hannes Fuchs, Sebastian Fischmeister, Apurva Narayan

**Updated**: 2025-09-24T14:46:42Z

**Summary**: In modern automotive development, security testing is critical for safeguarding systems against increasingly advanced threats. Attack trees are widely used to systematically represent potential attack vectors, but generating comprehensive test cases from these trees remains a labor-intensive, error-prone task that has seen limited automation in the context of testing vehicular systems. This paper introduces STAF (Security Test Automation Framework), a novel approach to automating security test case generation. Leveraging Large Language Models (LLMs) and a four-step self-corrective Retrieval-Augmented Generation (RAG) framework, STAF automates the generation of executable security test cases from attack trees, providing an end-to-end solution that encompasses the entire attack surface. We particularly show the elements and processes needed to provide an LLM to actually produce sensible and executable automotive security test suites, along with the integration with an automated testing framework. We further compare our tailored approach with general purpose (vanilla) LLMs and the performance of different LLMs (namely GPT-4.1 and DeepSeek) using our approach. We also demonstrate the method of our operation step-by-step in a concrete case study. Our results show significant improvements in efficiency, accuracy, scalability, and easy integration in any workflow, marking a substantial advancement in automating automotive security testing methodologies. Using TARAs as an input for verfication tests, we create synergies by connecting two vital elements of a secure automotive development process.

**Link**: [arxiv](http://arxiv.org/abs/2509.20190v1),  [pdf](http://arxiv.org/pdf/2509.20190v1)

**Tags**: cs.CR cs.AI 



### Pagoda: An Energy and Time Roofline Study for DNN Workloads on Edge   Accelerators
**Authors**: Prashanthi S. K., Kunal Kumar Sahoo, Amartya Ranjan Saikia, Pranav Gupta, Atharva Vinay Joshi, Priyanshu Pansari, Yogesh Simmhan

**Updated**: 2025-09-24T14:46:07Z

**Summary**: Edge accelerators such as Nvidia Jetsons are becoming an integral part of the computing continuum, and are often used for DNN inferencing and training. Nvidia Jetson edge devices have $2000$+ CUDA cores within a $70$W power envelope and offer $1000$s of power modes to customize CPU, GPU and memory frequencies. Their widely varying power--performance trade-offs can be exploited for energy and power-constrained deployments. While data-driven methods to predict the power and latency of DNN workloads for edge devices exist, there is a lack of principled study to understand why edge accelerators and their power modes perform the way they do. We develop a time roofline and a novel energy roofline model for the Jetson Orin AGX for diverse power modes, and couple it with an analytical model of the compute (FLOP) and memory access (bytes) for DNN inference workloads to analyze them from first principles. These reveal unique, sometimes counter-intuitive, insights into the power and performance behavior of DNN workloads on edge accelerators, e.g., the default power mode MAXN is not the most energy efficient and time efficiency implies energy efficiency for all power modes. We also extend our analytical roofline models to DNN training. Finally, we apply these methods to tune the power mode (and hence the roofline) of the edge device to optimize the latency and energy for DNN inference, with up to $15\%$ lower energy and minimal degradation in inference time.

**Link**: [arxiv](http://arxiv.org/abs/2509.20189v1),  [pdf](http://arxiv.org/pdf/2509.20189v1)

**Tags**: cs.DC 



### Thinking Augmented Pre-training
**Authors**: Liang Wang, Nan Yang, Shaohan Huang, Li Dong, Furu Wei

**Updated**: 2025-09-25T10:55:02Z

**Summary**: This paper introduces a simple and scalable approach to improve the data efficiency of large language model (LLM) training by augmenting existing text data with thinking trajectories. The compute for pre-training LLMs has been growing at an unprecedented rate, while the availability of high-quality data remains limited. Consequently, maximizing the utility of available data constitutes a significant research challenge. A primary impediment is that certain high-quality tokens are difficult to learn given a fixed model capacity, as the underlying rationale for a single token can be exceptionally complex and deep. To address this issue, we propose Thinking augmented Pre-Training (TPT), a universal methodology that augments text with automatically generated thinking trajectories. Such augmentation effectively increases the volume of the training data and makes high-quality tokens more learnable through step-by-step reasoning and decomposition. We apply TPT across diverse training configurations up to $100$B tokens, encompassing pre-training with both constrained and abundant data, as well as mid-training from strong open-source checkpoints. Experimental results indicate that our method substantially improves the performance of LLMs across various model sizes and families. Notably, TPT enhances the data efficiency of LLM pre-training by a factor of $3$. For a $3$B parameter model, it improves the post-training performance by over $10\%$ on several challenging reasoning benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2509.20186v2),  [pdf](http://arxiv.org/pdf/2509.20186v2)

**Tags**: cs.CL cs.LG 



### Benchmarking Web API Integration Code Generation
**Authors**: Daniel Maninger, Leon Chemnitz, Amir Molzam Sharifloo, Jannis Brugger, Mira Mezini

**Updated**: 2025-09-24T14:36:44Z

**Summary**: API integration is a cornerstone of our digital infrastructure, enabling software systems to connect and interact. However, as shown by many studies, writing or generating correct code to invoke APIs, particularly web APIs, is challenging. Although large language models~(LLMs) have become popular in software development, their effectiveness in automating the generation of web API integration code remains unexplored. In order to address this, we present a dataset and evaluation pipeline designed to assess the ability of LLMs to generate web API invocation code. Our experiments with several open-source LLMs reveal that generating API invocations poses a significant challenge, resulting in hallucinated endpoints, incorrect argument usage, and other errors. None of the evaluated open-source models were able to solve more than 40% of the tasks.

**Link**: [arxiv](http://arxiv.org/abs/2509.20172v1),  [pdf](http://arxiv.org/pdf/2509.20172v1)

**Tags**: cs.SE cs.LG 



### Probing Gender Bias in Multilingual LLMs: A Case Study of Stereotypes in   Persian
**Authors**: Ghazal Kalhor, Behnam Bahrak

**Updated**: 2025-09-24T14:34:17Z

**Summary**: Multilingual Large Language Models (LLMs) are increasingly used worldwide, making it essential to ensure they are free from gender bias to prevent representational harm. While prior studies have examined such biases in high-resource languages, low-resource languages remain understudied. In this paper, we propose a template-based probing methodology, validated against real-world data, to uncover gender stereotypes in LLMs. As part of this framework, we introduce the Domain-Specific Gender Skew Index (DS-GSI), a metric that quantifies deviations from gender parity. We evaluate four prominent models, GPT-4o mini, DeepSeek R1, Gemini 2.0 Flash, and Qwen QwQ 32B, across four semantic domains, focusing on Persian, a low-resource language with distinct linguistic features. Our results show that all models exhibit gender stereotypes, with greater disparities in Persian than in English across all domains. Among these, sports reflect the most rigid gender biases. This study underscores the need for inclusive NLP practices and provides a framework for assessing bias in other low-resource languages.

**Link**: [arxiv](http://arxiv.org/abs/2509.20168v1),  [pdf](http://arxiv.org/pdf/2509.20168v1)

**Tags**: cs.CL 



### CyberSOCEval: Benchmarking LLMs Capabilities for Malware Analysis and   Threat Intelligence Reasoning
**Authors**: Lauren Deason, Adam Bali, Ciprian Bejean, Diana Bolocan, James Crnkovich, Ioana Croitoru, Krishna Durai, Chase Midler, Calin Miron, David Molnar, Brad Moon, Bruno Ostarcevic, Alberto Peltea, Matt Rosenberg, Catalin Sandu, Arthur Saputkin, Sagar Shah, Daniel Stan, Ernest Szocs, Shengye Wan, Spencer Whitman, Sven Krasser, Joshua Saxe

**Updated**: 2025-09-24T14:33:07Z

**Summary**: Today's cyber defenders are overwhelmed by a deluge of security alerts, threat intelligence signals, and shifting business context, creating an urgent need for AI systems to enhance operational security work. While Large Language Models (LLMs) have the potential to automate and scale Security Operations Center (SOC) operations, existing evaluations do not fully assess the scenarios most relevant to real-world defenders. This lack of informed evaluation impacts both AI developers and those applying LLMs to SOC automation. Without clear insight into LLM performance in real-world security scenarios, developers lack a north star for development, and users cannot reliably select the most effective models. Meanwhile, malicious actors are using AI to scale cyber attacks, highlighting the need for open source benchmarks to drive adoption and community-driven improvement among defenders and model developers. To address this, we introduce CyberSOCEval, a new suite of open source benchmarks within CyberSecEval 4. CyberSOCEval includes benchmarks tailored to evaluate LLMs in two tasks: Malware Analysis and Threat Intelligence Reasoning--core defensive domains with inadequate coverage in current benchmarks. Our evaluations show that larger, more modern LLMs tend to perform better, confirming the training scaling laws paradigm. We also find that reasoning models leveraging test time scaling do not achieve the same boost as in coding and math, suggesting these models have not been trained to reason about cybersecurity analysis, and pointing to a key opportunity for improvement. Finally, current LLMs are far from saturating our evaluations, showing that CyberSOCEval presents a significant challenge for AI developers to improve cyber defense capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2509.20166v1),  [pdf](http://arxiv.org/pdf/2509.20166v1)

**Tags**: cs.CR cs.AI 



### Embedding Domain Knowledge for Large Language Models via Reinforcement   Learning from Augmented Generation
**Authors**: Chaojun Nie, Jun Zhou, Guanxiang Wang, Shisong Wud, Zichen Wang

**Updated**: 2025-09-24T14:30:16Z

**Summary**: Large language models (LLMs) often exhibit limited performance on domain-specific tasks due to the natural disproportionate representation of specialized information in their training data and the static nature of these datasets. Knowledge scarcity and temporal lag create knowledge gaps for domain applications. While post-training on domain datasets can embed knowledge into models, existing approaches have some limitations. Continual Pre-Training (CPT) treats all tokens in domain documents with equal importance, failing to prioritize critical knowledge points, while supervised fine-tuning (SFT) with question-answer pairs struggles to develop the coherent knowledge structures necessary for complex reasoning tasks. To address these challenges, we propose Reinforcement Learning from Augmented Generation (RLAG). Our approach iteratively cycles between sampling generations and optimizing the model through calculated rewards, effectively embedding critical and contextually coherent domain knowledge. We select generated outputs with the highest log probabilities as the sampling result, then compute three tailored reward metrics to guide the optimization process. To comprehensively evaluate domain expertise, we assess answer accuracy and the rationality of explanations generated for correctly answered questions. Experimental results across medical, legal, astronomy, and current events datasets demonstrate that our proposed method significantly outperforms baseline approaches. Our code and data are open sourced at https://github.com/ChaojunNie/RLAG.

**Link**: [arxiv](http://arxiv.org/abs/2509.20162v1),  [pdf](http://arxiv.org/pdf/2509.20162v1)

**Tags**: cs.CL cs.AI 



### HoT: Highlighted Chain of Thought for Referencing Supporting Facts from   Inputs
**Authors**: Tin Nguyen, Logan Bolton, Mohammad Reza Taesiri, Trung Bui, Anh Totti Nguyen

**Updated**: 2025-09-24T14:26:13Z

**Summary**: An Achilles heel of Large Language Models (LLMs) is their tendency to hallucinate non-factual statements. A response mixed of factual and non-factual statements poses a challenge for humans to verify and accurately base their decisions on. To combat this problem, we propose Highlighted Chain-of-Thought Prompting (HoT), a technique for prompting LLMs to generate responses with XML tags that ground facts to those provided in the query. That is, given an input question, LLMs would first re-format the question to add XML tags highlighting key facts, and then, generate a response with highlights over the facts referenced from the input. Interestingly, in few-shot settings, HoT outperforms vanilla chain of thought prompting (CoT) on a wide range of 17 tasks from arithmetic, reading comprehension to logical reasoning. When asking humans to verify LLM responses, highlights help time-limited participants to more accurately and efficiently recognize when LLMs are correct. Yet, surprisingly, when LLMs are wrong, HoTs tend to make users believe that an answer is correct.

**Link**: [arxiv](http://arxiv.org/abs/2503.02003v4),  [pdf](http://arxiv.org/pdf/2503.02003v4)

**Tags**: cs.CL cs.HC 



### Affective Computing and Emotional Data: Challenges and Implications in   Privacy Regulations, The AI Act, and Ethics in Large Language Models
**Authors**: Nicola Fabiano

**Updated**: 2025-09-25T10:43:22Z

**Summary**: This paper examines the integration of emotional intelligence into artificial intelligence systems, with a focus on affective computing and the growing capabilities of Large Language Models (LLMs), such as ChatGPT and Claude, to recognize and respond to human emotions. Drawing on interdisciplinary research that combines computer science, psychology, and neuroscience, the study analyzes foundational neural architectures - CNNs for processing facial expressions and RNNs for sequential data, such as speech and text - that enable emotion recognition. It examines the transformation of human emotional experiences into structured emotional data, addressing the distinction between explicit emotional data collected with informed consent in research settings and implicit data gathered passively through everyday digital interactions. That raises critical concerns about lawful processing, AI transparency, and individual autonomy over emotional expressions in digital environments. The paper explores implications across various domains, including healthcare, education, and customer service, while addressing challenges of cultural variations in emotional expression and potential biases in emotion recognition systems across different demographic groups. From a regulatory perspective, the paper examines emotional data in the context of the GDPR and the EU AI Act frameworks, highlighting how emotional data may be considered sensitive personal data that requires robust safeguards, including purpose limitation, data minimization, and meaningful consent mechanisms.

**Link**: [arxiv](http://arxiv.org/abs/2509.20153v2),  [pdf](http://arxiv.org/pdf/2509.20153v2)

**Tags**: cs.CY cs.AI 



### Enhancing Requirement Traceability through Data Augmentation Using Large   Language Models
**Authors**: Jianzhang Zhang, Jialong Zhou, Nan Niu, Chuang Liu

**Updated**: 2025-09-24T14:14:21Z

**Summary**: Requirements traceability is crucial in software engineering to ensure consistency between requirements and code. However, existing automated traceability methods are constrained by the scarcity of training data and challenges in bridging the semantic gap between artifacts. This study aims to address the data scarcity problem in requirements traceability by employing large language models (LLMs) for data augmentation. We propose a novel approach that utilizes prompt-based techniques with LLMs to generate augmented requirement-to-code trace links, thereby enhancing the training dataset. Four LLMs (Gemini 1.5 Pro, Claude 3, GPT-3.5, and GPT-4) were used, employing both zero-shot and few-shot templates. Moreover, we optimized the encoder component of the tracing model to improve its efficiency and adaptability to augmented data. The key contributions of this paper are: (1) proposing and evaluating four prompt templates for data augmentation; (2) providing a comparative analysis of four LLMs for generating trace links; (3) enhancing the model's encoder for improved adaptability to augmented datasets. Experimental results show that our approach significantly enhances model performance, achieving an F1 score improvement of up to 28.59%, thus demonstrating its effectiveness and potential for practical application.

**Link**: [arxiv](http://arxiv.org/abs/2509.20149v1),  [pdf](http://arxiv.org/pdf/2509.20149v1)

**Tags**: cs.SE 



### Inverse Reinforcement Learning with Dynamic Reward Scaling for LLM   Alignment
**Authors**: Ruoxi Cheng, Haoxuan Ma, Weixin Wang, Ranjie Duan, Jiexi Liu, Xiaoshuang Jia, Simeng Qin, Xiaochun Cao, Yang Liu, Xiaojun Jia

**Updated**: 2025-09-25T02:38:14Z

**Summary**: Alignment is vital for safely deploying large language models (LLMs). Existing techniques are either reward-based (train a reward model on preference pairs and optimize with reinforcement learning) or reward-free (directly fine-tune on ranked outputs). Recent research shows that well-tuned reward-based pipelines remain robust, and single-response demonstrations can outperform pairwise preference data. However, two challenges persist: (1) imbalanced safety datasets that overrepresent common hazards while neglecting long-tail threats; and (2) static reward models that ignore task difficulty, limiting optimization efficiency and attainable gains. We propose DR-IRL (Dynamically adjusting Rewards through Inverse Reinforcement Learning). We first train category-specific reward models using a balanced safety dataset covering seven harmful categories via IRL. Then we enhance Group Relative Policy Optimization (GRPO) by introducing dynamic reward scaling--adjusting rewards by task difficulty--data-level hardness by text encoder cosine similarity, model-level responsiveness by reward gaps. Extensive experiments across various benchmarks and LLMs demonstrate that DR-IRL outperforms all baseline methods in safety alignment while maintaining usefulness.

**Link**: [arxiv](http://arxiv.org/abs/2503.18991v5),  [pdf](http://arxiv.org/pdf/2503.18991v5)

**Tags**: cs.CL cs.AI cs.LG 



### Culture is Everywhere: A Call for Intentionally Cultural Evaluation
**Authors**: Juhyun Oh, Inha Cha, Michael Saxon, Hyunseung Lim, Shaily Bhatt, Alice Oh

**Updated**: 2025-09-24T14:02:11Z

**Summary**: The prevailing ``trivia-centered paradigm'' for evaluating the cultural alignment of large language models (LLMs) is increasingly inadequate as these models become more advanced and widely deployed. Existing approaches typically reduce culture to static facts or values, testing models via multiple-choice or short-answer questions that treat culture as isolated trivia. Such methods neglect the pluralistic and interactive realities of culture, and overlook how cultural assumptions permeate even ostensibly ``neutral'' evaluation settings. In this position paper, we argue for \textbf{intentionally cultural evaluation}: an approach that systematically examines the cultural assumptions embedded in all aspects of evaluation, not just in explicitly cultural tasks. We systematically characterize the what, how, and circumstances by which culturally contingent considerations arise in evaluation, and emphasize the importance of researcher positionality for fostering inclusive, culturally aligned NLP research. Finally, we discuss implications and future directions for moving beyond current benchmarking practices, discovering important applications that we don't know exist, and involving communities in evaluation design through HCI-inspired participatory methodologies.

**Link**: [arxiv](http://arxiv.org/abs/2509.01301v2),  [pdf](http://arxiv.org/pdf/2509.01301v2)

**Tags**: cs.CL 



### V-GameGym: Visual Game Generation for Code Large Language Models
**Authors**: Wei Zhang, Jack Yang, Renshuai Tao, Lingzheng Chai, Shawn Guo, Jiajun Wu, Xiaoming Chen, Ganqu Cui, Ning Ding, Xander Xu, Hu Wei, Bowen Zhou

**Updated**: 2025-09-24T14:01:18Z

**Summary**: Code large language models have demonstrated remarkable capabilities in programming tasks, yet current benchmarks primarily focus on single modality rather than visual game development. Most existing code-related benchmarks evaluate syntax correctness and execution accuracy, overlooking critical game-specific metrics such as playability, visual aesthetics, and user engagement that are essential for real-world deployment. To address the gap between current LLM capabilities in algorithmic problem-solving and competitive programming versus the comprehensive requirements of practical game development, we present V-GameGym, a comprehensive benchmark comprising 2,219 high-quality samples across 100 thematic clusters derived from real-world repositories, adopting a novel clustering-based curation methodology to ensure both diversity and structural completeness. Further, we introduce a multimodal evaluation framework with an automated LLM-driven pipeline for visual code synthesis using complete UI sandbox environments. Our extensive analysis reveals that V-GameGym effectively bridges the gap between code generation accuracy and practical game development workflows, providing quantifiable quality metrics for visual programming and interactive element generation.

**Link**: [arxiv](http://arxiv.org/abs/2509.20136v1),  [pdf](http://arxiv.org/pdf/2509.20136v1)

**Tags**: cs.SE 



### Probability Signature: Bridging Data Semantics and Embedding Structure   in Language Models
**Authors**: Junjie Yao, Zhi-Qin John Xu

**Updated**: 2025-09-24T13:49:44Z

**Summary**: The embedding space of language models is widely believed to capture the semantic relationships; for instance, embeddings of digits often exhibit an ordered structure that corresponds to their natural sequence. However, the mechanisms driving the formation of such structures remain poorly understood. In this work, we interpret the embedding structures via the data distribution. We propose a set of probability signatures that reflect the semantic relationships among tokens. Through experiments on the composite addition tasks using the linear model and feedforward network, combined with theoretical analysis of gradient flow dynamics, we reveal that these probability signatures significantly influence the embedding structures. We further generalize our analysis to large language models (LLMs) by training the Qwen2.5 architecture on the subsets of the Pile corpus. Our results show that the probability signatures are faithfully aligned with the embedding structures, particularly in capturing strong pairwise similarities among embeddings. Our work uncovers the mechanism of how data distribution guides the formation of embedding structures, establishing a novel understanding of the relationship between embedding organization and semantic patterns.

**Link**: [arxiv](http://arxiv.org/abs/2509.20124v1),  [pdf](http://arxiv.org/pdf/2509.20124v1)

**Tags**: cs.LG 



### Temporal Smoothness-Aware Rate-Distortion Optimized 4D Gaussian   Splatting
**Authors**: Hyeongmin Lee, Kyungjune Baek

**Updated**: 2025-09-24T13:49:27Z

**Summary**: Dynamic 4D Gaussian Splatting (4DGS) effectively extends the high-speed rendering capabilities of 3D Gaussian Splatting (3DGS) to represent volumetric videos. However, the large number of Gaussians, substantial temporal redundancies, and especially the absence of an entropy-aware compression framework result in large storage requirements. Consequently, this poses significant challenges for practical deployment, efficient edge-device processing, and data transmission. In this paper, we introduce a novel end-to-end RD-optimized compression framework tailored for 4DGS, aiming to enable flexible, high-fidelity rendering across varied computational platforms. Leveraging Fully Explicit Dynamic Gaussian Splatting (Ex4DGS), one of the state-of-the-art 4DGS methods, as our baseline, we start from the existing 3DGS compression methods for compatibility while effectively addressing additional challenges introduced by the temporal axis. In particular, instead of storing motion trajectories independently per point, we employ a wavelet transform to reflect the real-world smoothness prior, significantly enhancing storage efficiency. This approach yields significantly improved compression ratios and provides a user-controlled balance between compression efficiency and rendering quality. Extensive experiments demonstrate the effectiveness of our method, achieving up to 91$\times$ compression compared to the original Ex4DGS model while maintaining high visual fidelity. These results highlight the applicability of our framework for real-time dynamic scene rendering in diverse scenarios, from resource-constrained edge devices to high-performance environments. The source code is available at https://github.com/HyeongminLEE/RD4DGS.

**Link**: [arxiv](http://arxiv.org/abs/2507.17336v2),  [pdf](http://arxiv.org/pdf/2507.17336v2)

**Tags**: cs.GR 



### Can LLMs Forecast Internet Traffic from Social Media?
**Authors**: Jonatan Langlet, Mariano Scazzariello, Flavio Luciani, Marta Burocchi, Dejan Kostić, Marco Chiesa

**Updated**: 2025-09-24T13:48:14Z

**Summary**: Societal events shape the Internet's behavior. The death of a prominent public figure, a software launch, or a major sports match can trigger sudden demand surges that overwhelm peering points and content delivery networks. Although these events fall outside regular traffic patterns, forecasting systems still rely solely on those patterns and therefore miss these critical anomalies.   Thus, we argue for socio-technical systems that supplement technical measurements with an active understanding of the underlying drivers, including how events and collective behavior shape digital demands. We propose traffic forecasting using signals from public discourse, such as headlines, forums, and social media, as early demand indicators.   To validate our intuition, we present a proof-of-concept system that autonomously scrapes online discussions, infers real-world events, clusters and enriches them semantically, and correlates them with traffic measurements at a major Internet Exchange Point. This prototype predicted between 56-92% of society-driven traffic spikes after scraping a moderate amount of online discussions.   We believe this approach opens new research opportunities in cross-domain forecasting, scheduling, demand anticipation, and society-informed decision making.

**Link**: [arxiv](http://arxiv.org/abs/2509.20123v1),  [pdf](http://arxiv.org/pdf/2509.20123v1)

**Tags**: cs.NI 



### PEPS: Quantum-Inspired Reinforcement Learning for Coherent Reasoning   Traces in LLMs
**Authors**: Venkat Margapuri, Garik Kazanjian, Naren Kosaraju

**Updated**: 2025-09-24T13:29:53Z

**Summary**: Large Language Models (LLMs) often struggle with maintaining coherent multi-step reasoning traces, particularly in tasks that require a structured logical flow. This work introduces a quantum-inspired approach to address the challenge by incorporating a fidelity-based reward derived from Projected Entangled Pair States (PEPS) into Proximal Policy Optimization. Unlike prior approaches that use direct supervision or contrastive objectives, the proposed method guides learning through structural consistency, offering a novel approach to enforce global coherence in generated reasoning traces. The proposed framework is evaluated using multiple coherence-determining metrics on diverse datasets such as GSM8K, StrategyQA, and EntailmentBank spanning arithmetic, intuitive, and entailment-based reasoning. Results show that the proposed quantum-inspired approach offers significant improvements over supervised, contrastive, and pretrained baseline approaches, highlighting the effectiveness of quantum-inspired fidelity as a foundation to improve reasoning trace coherence in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2509.20105v1),  [pdf](http://arxiv.org/pdf/2509.20105v1)

**Tags**: cs.AI 



### Cascade! Human in the loop shortcomings can increase the risk of   failures in recommender systems
**Authors**: Wm. Matthew Kennedy, Nishanshi Shukla, Cigdem Patlak, Blake Chambers, Theodora Skeadas, Tuesday, Kingsley Owadara, Aayush Dhanotiya

**Updated**: 2025-09-25T11:14:50Z

**Summary**: Recommender systems are among the most commonly deployed systems today. Systems design approaches to AI-powered recommender systems have done well to urge recommender system developers to follow more intentional data collection, curation, and management procedures. So too has the "human-in-the-loop" paradigm been widely adopted, primarily to address the issue of accountability. However, in this paper, we take the position that human oversight in recommender system design also entails novel risks that have yet to be fully described. These risks are "codetermined" by the information context in which such systems are often deployed. Furthermore, new knowledge of the shortcomings of "human-in-the-loop" practices to deliver meaningful oversight of other AI systems suggest that they may also be inadequate for achieving socially responsible recommendations. We review how the limitations of human oversight may increase the chances of a specific kind of failure: a "cascade" or "compound" failure. We then briefly explore how the unique dynamics of three common deployment contexts can make humans in the loop more likely to fail in their oversight duties. We then conclude with two recommendations.

**Link**: [arxiv](http://arxiv.org/abs/2509.20099v2),  [pdf](http://arxiv.org/pdf/2509.20099v2)

**Tags**: cs.IR cs.CY 



### Integrated Framework for LLM Evaluation with Answer Generation
**Authors**: Sujeong Lee, Hayoung Lee, Seongsoo Heo, Wonik Choi

**Updated**: 2025-09-24T13:20:37Z

**Summary**: Reliable evaluation of large language models is essential to ensure their applicability in practical scenarios. Traditional benchmark-based evaluation methods often rely on fixed reference answers, limiting their ability to capture important qualitative aspects of generated responses. To address these shortcomings, we propose an integrated evaluation framework called \textit{self-refining descriptive evaluation with expert-driven diagnostics}, SPEED, which utilizes specialized functional experts to perform comprehensive, descriptive analyses of model outputs. Unlike conventional approaches, SPEED actively incorporates expert feedback across multiple dimensions, including hallucination detection, toxicity assessment, and lexical-contextual appropriateness. Experimental results demonstrate that SPEED achieves robust and consistent evaluation performance across diverse domains and datasets. Additionally, by employing relatively compact expert models, SPEED demonstrates superior resource efficiency compared to larger-scale evaluators. These findings illustrate that SPEED significantly enhances fairness and interpretability in LLM evaluations, offering a promising alternative to existing evaluation methodologies.

**Link**: [arxiv](http://arxiv.org/abs/2509.20097v1),  [pdf](http://arxiv.org/pdf/2509.20097v1)

**Tags**: cs.CL cs.AI 



### Causal Understanding by LLMs: The Role of Uncertainty
**Authors**: Oscar Lithgow-Serrano, Vani Kanjirangat, Alessandro Antonucci

**Updated**: 2025-09-24T13:06:35Z

**Summary**: Recent papers show LLMs achieve near-random accuracy in causal relation classification, raising questions about whether such failures arise from limited pretraining exposure or deeper representational gaps. We investigate this under uncertainty-based evaluation, testing whether pretraining exposure to causal examples improves causal understanding >18K PubMed sentences -- half from The Pile corpus, half post-2024 -- across seven models (Pythia-1.4B/7B/12B, GPT-J-6B, Dolly-7B/12B, Qwen-7B). We analyze model behavior through: (i) causal classification, where the model identifies causal relationships in text, and (ii) verbatim memorization probing, where we assess whether the model prefers previously seen causal statements over their paraphrases. Models perform four-way classification (direct/conditional/correlational/no-relationship) and select between originals and their generated paraphrases. Results show almost identical accuracy on seen/unseen sentences (p > 0.05), no memorization bias (24.8% original selection), and output distribution over the possible options is almost flat, with entropic values near the maximum (1.35/1.39), confirming random guessing. Instruction-tuned models show severe miscalibration (Qwen: > 95% confidence, 32.8% accuracy, ECE=0.49). Conditional relations induce highest entropy (+11% vs. direct). These findings suggest that failures in causal understanding arise from the lack of structured causal representation, rather than insufficient exposure to causal examples during pretraining.

**Link**: [arxiv](http://arxiv.org/abs/2509.20088v1),  [pdf](http://arxiv.org/pdf/2509.20088v1)

**Tags**: cs.CL cs.AI 



### OLaPh: Optimal Language Phonemizer
**Authors**: Johannes Wirth

**Updated**: 2025-09-24T13:05:09Z

**Summary**: Phonemization, the conversion of text into phonemes, is a key step in text-to-speech. Traditional approaches use rule-based transformations and lexicon lookups, while more advanced methods apply preprocessing techniques or neural networks for improved accuracy on out-of-domain vocabulary. However, all systems struggle with names, loanwords, abbreviations, and homographs. This work presents OLaPh (Optimal Language Phonemizer), a framework that combines large lexica, multiple NLP techniques, and compound resolution with a probabilistic scoring function. Evaluations in German and English show improved accuracy over previous approaches, including on a challenging dataset. To further address unresolved cases, we train a large language model on OLaPh-generated data, which achieves even stronger generalization and performance. Together, the framework and LLM improve phonemization consistency and provide a freely available resource for future research.

**Link**: [arxiv](http://arxiv.org/abs/2509.20086v1),  [pdf](http://arxiv.org/pdf/2509.20086v1)

**Tags**: cs.CL 



### Why Do Some Inputs Break Low-Bit LLM Quantization?
**Authors**: Ting-Yun Chang, Muru Zhang, Jesse Thomason, Robin Jia

**Updated**: 2025-09-24T12:57:03Z

**Summary**: Low-bit weight-only quantization significantly reduces the memory footprint of large language models (LLMs), but disproportionately affects certain examples. We analyze diverse 3-4 bit methods on LLMs ranging from 7B-70B in size and find that the quantization errors of 50 pairs of methods are strongly correlated (avg. 0.82) on FineWeb examples. Moreover, the residual stream magnitudes of full-precision models are indicative of future quantization errors. We further establish a hypothesis that relates the residual stream magnitudes to error amplification and accumulation over layers. Using LLM localization techniques, early exiting, and activation patching, we show that examples with large errors rely on precise residual activations in the late layers, and that the outputs of MLP gates play a crucial role in maintaining the perplexity. Our work reveals why certain examples result in large quantization errors and which model components are most critical for performance preservation.

**Link**: [arxiv](http://arxiv.org/abs/2506.12044v2),  [pdf](http://arxiv.org/pdf/2506.12044v2)

**Tags**: cs.LG cs.AI 



### Understanding Before Reasoning: Enhancing Chain-of-Thought with   Iterative Summarization Pre-Prompting
**Authors**: Dong-Hai Zhu, Yu-Jie Xiong, Jia-Chen Zhang, Xi-Jiong Xie, Chun-Ming Xia

**Updated**: 2025-09-24T12:56:29Z

**Summary**: Chain-of-Thought (CoT) Prompting is a dominant paradigm in Large Language Models (LLMs) to enhance complex reasoning. It guides LLMs to present multi-step reasoning, rather than generating the final answer directly. However, CoT encounters difficulties when key information required for reasoning is implicit or missing. This occurs because CoT emphasizes the sequence of reasoning steps while overlooking the early extraction of essential information. We propose a pre-prompting method called Iterative Summarization Pre-Prompting (ISP^2) to refine LLM reasoning when key information is not explicitly provided. First, entities and their corresponding descriptions are extracted to form potential key information pairs. Next, we use a reliability rating to assess these pairs, then merge the two lowest-ranked pairs into a new entity description. This process is repeated until a unique key information pair is obtained. Finally, that pair, along with the original question, is fed into LLMs to produce the answer. Extensive experiments demonstrate a 7.1% improvement compared to existing methods. Unlike traditional prompting, ISP^2 adopts an inductive approach with pre-prompting, offering flexible integration into diverse reasoning frameworks. The code is available at https://github.com/zdhgreat/ISP-2.

**Link**: [arxiv](http://arxiv.org/abs/2501.04341v2),  [pdf](http://arxiv.org/pdf/2501.04341v2)

**Tags**: cs.CL 



### From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint   Training
**Authors**: Tianqiao Liu, Xueyi Li, Hao Wang, Haoxuan Li, Zhichao Chen, Weiqi Luo, Zitao Liu

**Updated**: 2025-09-25T09:23:12Z

**Summary**: Recent advances in large language models (LLMs) have attracted significant interest in extending their capabilities to multimodal scenarios, particularly for speech-to-speech conversational systems. However, existing multimodal models handling interleaved audio and text rely on autoregressive methods, overlooking that text depends on target-target relations whereas audio depends mainly on source-target relations. In this work, we propose Text-to-Talk (TtT), a unified audio-text framework that integrates autoregressive (AR) text generation with non-autoregressive (NAR) audio diffusion in a single Transformer. By leveraging the any-order autoregressive property of absorbing discrete diffusion, our approach provides a unified training objective for text and audio. To support this hybrid generation paradigm, we design a modality-aware attention mechanism that enforces causal decoding for text while allowing bidirectional modeling within audio spans, and further introduce three training strategies that reduce train-test discrepancies. During inference, TtT employs block-wise diffusion to synthesize audio in parallel while flexibly handling variable-length outputs. Extensive experiments across Audio-QA and ASR tasks demonstrate the effectiveness of our approach, with detailed ablation studies validating each proposed component. We will open-source our models, data and code to facilitate future research in this direction.

**Link**: [arxiv](http://arxiv.org/abs/2509.20072v2),  [pdf](http://arxiv.org/pdf/2509.20072v2)

**Tags**: cs.CL 



### LLM Trainer: Automated Robotic Data Generating via Demonstration   Augmentation using LLMs
**Authors**: Abraham George, Amir Barati Farimani

**Updated**: 2025-09-24T12:40:57Z

**Summary**: We present LLM Trainer, a fully automated pipeline that leverages the world knowledge of Large Language Models (LLMs) to transform a small number of human demonstrations (as few as one) into a large robot dataset for imitation learning. Our approach decomposes demonstration generation into two steps: (1) offline demonstration annotation that extracts keyframes, salient objects, and pose-object relations; and (2) online keypose retargeting that adapts those keyframes to a new scene, given an initial observation. Using these modified keypoints, our system warps the original demonstration to generate a new trajectory, which is then executed, and the resulting demo, if successful, is saved. Because the annotation is reusable across scenes, we use Thompson sampling to optimize the annotation, significantly improving generation success rate. We evaluate our method on a range of tasks, and find that our data annotation method consistently outperforms expert-engineered baselines. We further show an ensemble policy that combines the optimized LLM feed-forward plan with a learned feedback imitation learning controller. Finally, we demonstrate hardware feasibility on a Franka Emika Panda robot. For additional materials and demonstration videos, please see the project website: https://sites.google.com/andrew.cmu.edu/llm-trainer

**Link**: [arxiv](http://arxiv.org/abs/2509.20070v1),  [pdf](http://arxiv.org/pdf/2509.20070v1)

**Tags**: cs.RO 



### A Novel Short-Term Anomaly Prediction for IIoT with Software Defined   Twin Network
**Authors**: Bilal Dalgic, Betul Sen, Muge Erel-Ozcevik

**Updated**: 2025-09-24T12:37:18Z

**Summary**: Secure monitoring and dynamic control in an IIoT environment are major requirements for current development goals. We believe that dynamic, secure monitoring of the IIoT environment can be achieved through integration with the Software-Defined Network (SDN) and Digital Twin (DT) paradigms. The current literature lacks implementation details for SDN-based DT and time-aware intelligent model training for short-term anomaly detection against IIoT threats. Therefore, we have proposed a novel framework for short-term anomaly detection that uses an SDN-based DT. Using a comprehensive dataset, time-aware labeling of features, and a comprehensive evaluation of various machine learning models, we propose a novel SD-TWIN-based anomaly detection algorithm. According to the performance of a new real-time SD-TWIN deployment, the GPU- accelerated LightGBM model is particularly effective, achieving a balance of high recall and strong classification performance.

**Link**: [arxiv](http://arxiv.org/abs/2509.20068v1),  [pdf](http://arxiv.org/pdf/2509.20068v1)

**Tags**: cs.NI cs.LG cs.SE 



### MACD: Multi-Agent Clinical Diagnosis with Self-Learned Knowledge for LLM
**Authors**: Wenliang Li, Rui Yan, Xu Zhang, Li Chen, Hongji Zhu, Jing Zhao, Junjun Li, Mengru Li, Wei Cao, Zihang Jiang, Wei Wei, Kun Zhang, Shaohua Kevin Zhou

**Updated**: 2025-09-25T03:59:16Z

**Summary**: Large language models (LLMs) have demonstrated notable potential in medical applications, yet they face substantial challenges in handling complex real-world clinical diagnoses using conventional prompting methods. Current prompt engineering and multi-agent approaches typically optimize isolated inferences, neglecting the accumulation of reusable clinical experience. To address this, this study proposes a novel Multi-Agent Clinical Diagnosis (MACD) framework, which allows LLMs to self-learn clinical knowledge via a multi-agent pipeline that summarizes, refines, and applies diagnostic insights. It mirrors how physicians develop expertise through experience, enabling more focused and accurate diagnosis on key disease-specific cues. We further extend it to a MACD-human collaborative workflow, where multiple LLM-based diagnostician agents engage in iterative consultations, supported by an evaluator agent and human oversight for cases where agreement is not reached. Evaluated on 4,390 real-world patient cases across seven diseases using diverse open-source LLMs (Llama-3.1 8B/70B, DeepSeek-R1-Distill-Llama 70B), MACD significantly improves primary diagnostic accuracy, outperforming established clinical guidelines with gains up to 22.3% (MACD). On the subset of the data, it achieves performance on par with or exceeding that of human physicians (up to 16% improvement over physicians-only diagnosis). Additionally, on the MACD-human workflow, it achieves an 18.6% improvement compared to physicians-only diagnosis. Moreover, self-learned knowledge exhibits strong cross-model stability, transferability, and model-specific personalization, while the system can generate traceable rationales, enhancing explainability. Consequently, this work presents a scalable self-learning paradigm for LLM-assisted diagnosis, bridging the gap between the intrinsic knowledge of LLMs and real-world clinical practice.

**Link**: [arxiv](http://arxiv.org/abs/2509.20067v2),  [pdf](http://arxiv.org/pdf/2509.20067v2)

**Tags**: cs.AI 



### One Filters All: A Generalist Filter for State Estimation
**Authors**: Shiqi Liu, Wenhan Cao, Chang Liu, Zeyu He, Tianyi Zhang, Shengbo Eben Li

**Updated**: 2025-09-24T12:19:18Z

**Summary**: Estimating hidden states in dynamical systems, also known as optimal filtering, is a long-standing problem in various fields of science and engineering. In this paper, we introduce a general filtering framework, \textbf{LLM-Filter}, which leverages large language models (LLMs) for state estimation by embedding noisy observations with text prototypes. In various experiments for classical dynamical systems, we find that first, state estimation can significantly benefit from the reasoning knowledge embedded in pre-trained LLMs. By achieving proper modality alignment with the frozen LLM, LLM-Filter outperforms the state-of-the-art learning-based approaches. Second, we carefully design the prompt structure, System-as-Prompt (SaP), incorporating task instructions that enable the LLM to understand the estimation tasks. Guided by these prompts, LLM-Filter exhibits exceptional generalization, capable of performing filtering tasks accurately in changed or even unseen environments. We further observe a scaling-law behavior in LLM-Filter, where accuracy improves with larger model sizes and longer training times. These findings make LLM-Filter a promising foundation model of filtering.

**Link**: [arxiv](http://arxiv.org/abs/2509.20051v1),  [pdf](http://arxiv.org/pdf/2509.20051v1)

**Tags**: cs.LG cs.AI 



### Tokenization and Representation Biases in Multilingual Models on   Dialectal NLP Tasks
**Authors**: Vani Kanjirangat, Tanja Samardžić, Ljiljana Dolamic, Fabio Rinaldi

**Updated**: 2025-09-24T12:13:53Z

**Summary**: Dialectal data are characterized by linguistic variation that appears small to humans but has a significant impact on the performance of models. This dialect gap has been related to various factors (e.g., data size, economic and social factors) whose impact, however, turns out to be inconsistent. In this work, we investigate factors impacting the model performance more directly: we correlate Tokenization Parity (TP) and Information Parity (IP), as measures of representational biases in pre-trained multilingual models, with the downstream performance. We compare state-of-the-art decoder-only LLMs with encoder-based models across three tasks: dialect classification, topic classification, and extractive question answering, controlling for varying scripts (Latin vs. non-Latin) and resource availability (high vs. low). Our analysis reveals that TP is a better predictor of the performance on tasks reliant on syntactic and morphological cues (e.g., extractive QA), while IP better predicts performance in semantic tasks (e.g., topic classification). Complementary analyses, including tokenizer behavior, vocabulary coverage, and qualitative insights, reveal that the language support claims of LLMs often might mask deeper mismatches at the script or token level.

**Link**: [arxiv](http://arxiv.org/abs/2509.20045v1),  [pdf](http://arxiv.org/pdf/2509.20045v1)

**Tags**: cs.CL cs.AI 



### Language Model Evolutionary Algorithms for Recommender Systems:   Benchmarks and Algorithm Comparisons
**Authors**: Jiao Liu, Zhu Sun, Shanshan Feng, Caishun Chen, Yew-Soon Ong

**Updated**: 2025-09-24T12:12:51Z

**Summary**: In the evolutionary computing community, the remarkable language-handling capabilities and reasoning power of large language models (LLMs) have significantly enhanced the functionality of evolutionary algorithms (EAs), enabling them to tackle optimization problems involving structured language or program code. Although this field is still in its early stages, its impressive potential has led to the development of various LLM-based EAs. To effectively evaluate the performance and practical applicability of these LLM-based EAs, benchmarks with real-world relevance are essential. In this paper, we focus on LLM-based recommender systems (RSs) and introduce a benchmark problem set, named RSBench, specifically designed to assess the performance of LLM-based EAs in recommendation prompt optimization. RSBench emphasizes session-based recommendations, aiming to discover a set of Pareto optimal prompts that guide the recommendation process, providing accurate, diverse, and fair recommendations. We develop three LLM-based EAs based on established EA frameworks and experimentally evaluate their performance using RSBench. Our study offers valuable insights into the application of EAs in LLM-based RSs. Additionally, we explore key components that may influence the overall performance of the RS, providing meaningful guidance for future research on the development of LLM-based EAs in RSs. The source code of the proposed RSBench can be found at https://github.com/LiuJ-2023/RSBench/tree/main.

**Link**: [arxiv](http://arxiv.org/abs/2411.10697v3),  [pdf](http://arxiv.org/pdf/2411.10697v3)

**Tags**: cs.NE 



### Playpen: An Environment for Exploring Learning Through Conversational   Interaction
**Authors**: Nicola Horst, Davide Mazzaccara, Antonia Schmidt, Michael Sullivan, Filippo Momentè, Luca Franceschetti, Philipp Sadler, Sherzod Hakimov, Alberto Testoni, Raffaella Bernardi, Raquel Fernández, Alexander Koller, Oliver Lemon, David Schlangen, Mario Giulianelli, Alessandro Suglia

**Updated**: 2025-09-24T12:06:14Z

**Summary**: Interaction between learner and feedback-giver has come into focus recently for post-training of Large Language Models (LLMs), through the use of reward models that judge the appropriateness of a model's response. In this paper, we investigate whether Dialogue Games -- goal-directed and rule-governed activities driven predominantly by verbal actions -- can also serve as a source of feedback signals for learning. We introduce Playpen, an environment for off- and online learning through Dialogue Game self-play, and investigate a representative set of post-training methods: supervised fine-tuning; direct alignment (DPO); and reinforcement learning with GRPO. We experiment with post-training a small LLM (Llama-3.1-8B-Instruct), evaluating performance on unseen instances of training games as well as unseen games, and on standard benchmarks. We find that imitation learning through SFT improves performance on unseen instances, but negatively impacts other skills, while interactive learning with GRPO shows balanced improvements without loss of skills. We release the framework and the baseline training setups to foster research in the promising new direction of learning in (synthetic) interaction.

**Link**: [arxiv](http://arxiv.org/abs/2504.08590v3),  [pdf](http://arxiv.org/pdf/2504.08590v3)

**Tags**: cs.CL 



### Triangulating LLM Progress through Benchmarks, Games, and Cognitive   Tests
**Authors**: Filippo Momentè, Alessandro Suglia, Mario Giulianelli, Ambra Ferrari, Alexander Koller, Oliver Lemon, David Schlangen, Raquel Fernández, Raffaella Bernardi

**Updated**: 2025-09-24T12:04:54Z

**Summary**: We examine three evaluation paradigms: standard benchmarks (e.g., MMLU and BBH), interactive games (e.g., Signalling Games or Taboo), and cognitive tests (e.g., for working memory or theory of mind). First, we investigate which of the former two-benchmarks or games-is most effective at discriminating LLMs of varying quality. Then, inspired by human cognitive assessments, we compile a suite of targeted tests that measure cognitive abilities deemed essential for effective language use, and we investigate their correlation with model performance in benchmarks and games. Our analyses reveal that interactive games are superior to standard benchmarks in discriminating models. Causal and logical reasoning correlate with both static and interactive tests, while differences emerge regarding core executive functions and social/emotional skills, which correlate more with games. We advocate for the development of new interactive benchmarks and targeted cognitive tasks inspired by assessing human abilities but designed specifically for LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2502.14359v4),  [pdf](http://arxiv.org/pdf/2502.14359v4)

**Tags**: cs.CL 



### Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain   Human Label Variation
**Authors**: Beiduo Chen, Yang Janet Liu, Anna Korhonen, Barbara Plank

**Updated**: 2025-09-24T12:01:02Z

**Summary**: The recent rise of reasoning-tuned Large Language Models (LLMs)--which generate chains of thought (CoTs) before giving the final answer--has attracted significant attention and offers new opportunities for gaining insights into human label variation, which refers to plausible differences in how multiple annotators label the same data instance. Prior work has shown that LLM-generated explanations can help align model predictions with human label distributions, but typically adopt a reverse paradigm: producing explanations based on given answers. In contrast, CoTs provide a forward reasoning path that may implicitly embed rationales for each answer option, before generating the answers. We thus propose a novel LLM-based pipeline enriched with linguistically-grounded discourse segmenters to extract supporting and opposing statements for each answer option from CoTs with improved accuracy. We also propose a rank-based HLV evaluation framework that prioritizes the ranking of answers over exact scores, which instead favor direct comparison of label distributions. Our method outperforms a direct generation method as well as baselines on three datasets, and shows better alignment of ranking methods with humans, highlighting the effectiveness of our approach.

**Link**: [arxiv](http://arxiv.org/abs/2505.23368v3),  [pdf](http://arxiv.org/pdf/2505.23368v3)

**Tags**: cs.CL 



### MARG: MAstering Risky Gap Terrains for Legged Robots with Elevation   Mapping
**Authors**: Yinzhao Dong, Ji Ma, Liu Zhao, Wanyue Li, Peng Lu

**Updated**: 2025-09-24T12:00:34Z

**Summary**: Deep Reinforcement Learning (DRL) controllers for quadrupedal locomotion have demonstrated impressive performance on challenging terrains, allowing robots to execute complex skills such as climbing, running, and jumping. However, existing blind locomotion controllers often struggle to ensure safety and efficient traversal through risky gap terrains, which are typically highly complex, requiring robots to perceive terrain information and select appropriate footholds during locomotion accurately. Meanwhile, existing perception-based controllers still present several practical limitations, including a complex multi-sensor deployment system and expensive computing resource requirements. This paper proposes a DRL controller named MAstering Risky Gap Terrains (MARG), which integrates terrain maps and proprioception to dynamically adjust the action and enhance the robot's stability in these tasks. During the training phase, our controller accelerates policy optimization by selectively incorporating privileged information (e.g., center of mass, friction coefficients) that are available in simulation but unmeasurable directly in real-world deployments due to sensor limitations. We also designed three foot-related rewards to encourage the robot to explore safe footholds. More importantly, a terrain map generation (TMG) model is proposed to reduce the drift existing in mapping and provide accurate terrain maps using only one LiDAR, providing a foundation for zero-shot transfer of the learned policy. The experimental results indicate that MARG maintains stability in various risky terrain tasks.

**Link**: [arxiv](http://arxiv.org/abs/2509.20036v1),  [pdf](http://arxiv.org/pdf/2509.20036v1)

**Tags**: cs.RO 



### Resource-Efficient Adaptation of Large Language Models for Text   Embeddings via Prompt Engineering and Contrastive Fine-tuning
**Authors**: Benedikt Roth, Stephan Rappensperger, Tianming Qiu, Hamza Imamović, Julian Wörmann, Hao Shen

**Updated**: 2025-09-24T11:55:48Z

**Summary**: Large Language Models (LLMs) have become a cornerstone in Natural Language Processing (NLP), achieving impressive performance in text generation. Their token-level representations capture rich, human-aligned semantics. However, pooling these vectors into a text embedding discards crucial information. Nevertheless, many non-generative downstream tasks, such as clustering, classification, or retrieval, still depend on accurate and controllable sentence- or document-level embeddings. We explore several adaptation strategies for pre-trained, decoder-only LLMs: (i) various aggregation techniques for token embeddings, (ii) task-specific prompt engineering, and (iii) text-level augmentation via contrastive fine-tuning. Combining these components yields competitive performance on the English clustering track of the Massive Text Embedding Benchmark (MTEB). An analysis of the attention map further shows that fine-tuning shifts focus from prompt tokens to semantically relevant words, indicating more effective compression of meaning into the final hidden state. Our experiments demonstrate that LLMs can be effectively adapted as text embedding models through a combination of prompt engineering and resource-efficient contrastive fine-tuning on synthetically generated positive pairs.

**Link**: [arxiv](http://arxiv.org/abs/2507.22729v2),  [pdf](http://arxiv.org/pdf/2507.22729v2)

**Tags**: cs.CL 



### Meeseeks: A Feedback-Driven, Iterative Self-Correction Benchmark   evaluating LLMs' Instruction Following Capability
**Authors**: Jiaming wang, Yunke Zhao, Peng Ding, Jun Kuang, Yibin Shen, Zhe Tang, Yilin Jin, ZongYu Wang, Xiaoyu Li, Xuezhi Cao, Xunliang Cai

**Updated**: 2025-09-24T11:52:17Z

**Summary**: The capability to precisely adhere to instructions is a cornerstone for Large Language Models (LLMs) to function as dependable agents in real-world scenarios. However, confronted with complex prompts, LLMs frequently encounter difficulties in fulfilling all specified requirements within a single response. Drawing inspiration from recent advancements in Chain-of-Thought (CoT) prompting and self-correction methodologies, we introduce Meeseeks (The name is inspired by Mr. Meeseeks from "Rick and Morty," a character renowned for efficiently accomplishing assigned tasks. See: https://en.wikipedia.org/wiki/Mr._Meeseeks), a fully automated iterative instruction-following benchmark equipped with an integrated feedback mechanism. Meeseeks identifies erroneous components in model responses and provides corresponding feedback accurately, thereby iteratively guiding the model toward self-correction. The dataset contains over 700 curated instances annotated by 32 distinct capability tags in Chinese and English. Extensive experimental results reveal that different state-of-the-art commercial and open-source LLMs exhibit vastly disparate performance, and even after 20 turns of iterative feedback-driven self-correction, nearly all models demonstrate suboptimal performance. We conducted comprehensive analysis from both macro and instance levels, uncovering numerous common issues prevalent in current state-of-the-art models, as well as several counterintuitive phenomena. We've open-sourced our work on https://github.com/ADoublLEN/Meeseeks.

**Link**: [arxiv](http://arxiv.org/abs/2504.21625v5),  [pdf](http://arxiv.org/pdf/2504.21625v5)

**Tags**: cs.CL 



### ASC-SW: Atrous strip convolution network with sliding windows
**Authors**: Cheng Liu, Fan Zhu, Yifeng Xu, Baoru Huang, Mohd Rizal Arshad

**Updated**: 2025-09-24T11:46:54Z

**Summary**: With the rapid development of lightweight visual neural network architectures, traditional high-performance vision models have undergone significant compression, enhancing their computational and energy efficiency and enabling deployment on resource-constrained edge devices. In order to enable the mobile robot to avoid the ground wires, we propose a visual-assisted navigation framework called Atrous Strip Convolution Sliding Window (ASC-SW). This framework compensates for the limitations of traditional light detection and range (LiDAR) sensors to detect ground-level obstacles such as wires. A lightweight and efficient segmentation model, Atrous Strip Convolution Network (ASCnet) was proposed, for detecting deformable linear objects (DLOs). Atrous Strip Convolution Spatial Pyramid Pooling (ASCSPP) is designed to extract DLOs features effectively. Atrous Strip Convolution is integrated into ASCSPP to accurately identify the linear structure of DLOs with low computational cost. Additionally, a Sliding Window (SW) post processing module is proposed to denoise the output in complex environments, improving recognition accuracy. ASC-SW achieves 75.3% MIoU at 217 FPS on a self-built real world dataset and real-robot experiment was demonstrated that our proposed framework. It can be successfully verified on the real-robot on the edge device(Jetson platform) at that were originally inoperable.

**Link**: [arxiv](http://arxiv.org/abs/2507.12744v2),  [pdf](http://arxiv.org/pdf/2507.12744v2)

**Tags**: cs.RO 



### To Trust Or Not To Trust Your Vision-Language Model's Prediction
**Authors**: Hao Dong, Moru Liu, Jian Liang, Eleni Chatzi, Olga Fink

**Updated**: 2025-09-24T11:40:31Z

**Summary**: Vision-Language Models (VLMs) have demonstrated strong capabilities in aligning visual and textual modalities, enabling a wide range of applications in multimodal understanding and generation. While they excel in zero-shot and transfer learning scenarios, VLMs remain susceptible to misclassification, often yielding confident yet incorrect predictions. This limitation poses a significant risk in safety-critical domains, where erroneous predictions can lead to severe consequences. In this work, we introduce TrustVLM, a training-free framework designed to address the critical challenge of estimating when VLM's predictions can be trusted. Motivated by the observed modality gap in VLMs and the insight that certain concepts are more distinctly represented in the image embedding space, we propose a novel confidence-scoring function that leverages this space to improve misclassification detection. We rigorously evaluate our approach across 17 diverse datasets, employing 4 architectures and 2 VLMs, and demonstrate state-of-the-art performance, with improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 compared to existing baselines. By improving the reliability of the model without requiring retraining, TrustVLM paves the way for safer deployment of VLMs in real-world applications. The code is available at https://github.com/EPFL-IMOS/TrustVLM.

**Link**: [arxiv](http://arxiv.org/abs/2505.23745v2),  [pdf](http://arxiv.org/pdf/2505.23745v2)

**Tags**: cs.CV cs.AI cs.LG 



### Embodied AI: From LLMs to World Models
**Authors**: Tongtong Feng, Xin Wang, Yu-Gang Jiang, Wenwu Zhu

**Updated**: 2025-09-24T11:37:48Z

**Summary**: Embodied Artificial Intelligence (AI) is an intelligent system paradigm for achieving Artificial General Intelligence (AGI), serving as the cornerstone for various applications and driving the evolution from cyberspace to physical systems. Recent breakthroughs in Large Language Models (LLMs) and World Models (WMs) have drawn significant attention for embodied AI. On the one hand, LLMs empower embodied AI via semantic reasoning and task decomposition, bringing high-level natural language instructions and low-level natural language actions into embodied cognition. On the other hand, WMs empower embodied AI by building internal representations and future predictions of the external world, facilitating physical law-compliant embodied interactions. As such, this paper comprehensively explores the literature in embodied AI from basics to advances, covering both LLM driven and WM driven works. In particular, we first present the history, key technologies, key components, and hardware systems of embodied AI, as well as discuss its development via looking from unimodal to multimodal angle. We then scrutinize the two burgeoning fields of embodied AI, i.e., embodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs, meticulously delineating their indispensable roles in end-to-end embodied cognition and physical laws-driven embodied interactions. Building upon the above advances, we further share our insights on the necessity of the joint MLLM-WM driven embodied AI architecture, shedding light on its profound significance in enabling complex tasks within physical worlds. In addition, we examine representative applications of embodied AI, demonstrating its wide applicability in real-world scenarios. Last but not least, we point out future research directions of embodied AI that deserve further investigation.

**Link**: [arxiv](http://arxiv.org/abs/2509.20021v1),  [pdf](http://arxiv.org/pdf/2509.20021v1)

**Tags**: cs.AI cs.CL cs.RO 



### Combating the Memory Walls: Optimization Pathways for Long-Context   Agentic LLM Inference
**Authors**: Haoran Wu, Can Xiao, Jiayi Nie, Xuan Guo, Binglei Lou, Jeffrey T. H. Wong, Zhiwen Mo, Cheng Zhang, Przemyslaw Forys, Wayne Luk, Hongxiang Fan, Jianyi Cheng, Timothy M. Jones, Rika Antonova, Robert Mullins, Aaron Zhao

**Updated**: 2025-09-24T11:31:37Z

**Summary**: LLMs now form the backbone of AI agents for a diverse array of applications, including tool use, command-line agents, and web or computer use agents. These agentic LLM inference tasks are fundamentally different from chatbot-focused inference -- they often have much larger context lengths to capture complex, prolonged inputs, such as entire webpage DOMs or complicated tool call trajectories. This, in turn, generates significant off-chip memory traffic for the underlying hardware at the inference stage and causes the workload to be constrained by two memory walls, namely the bandwidth and capacity memory walls, preventing the on-chip compute units from achieving high utilization.   In this paper, we introduce PLENA, a hardware-software co-designed system that applies three core optimization pathways to tackle these challenges. PLENA includes an efficient hardware implementation of compute and memory units supporting an asymmetric quantization scheme. PLENA also features a novel flattened systolic array architecture that has native support for FlashAttention to tackle these memory walls in the scenario of inference serving for long-context LLMs. Additionally, PLENA is developed with a complete stack, including a custom ISA, a compiler, a cycle-emulated simulator, and an automated design space exploration flow. The simulated results show that PLENA achieves up to 8.5x higher utilization than existing accelerators, and delivers 2.24x higher throughput than the A100 GPU and 3.85x higher throughput than the TPU v6e, under the same multiplier count and memory settings. The full PLENA system will also be open-sourced.

**Link**: [arxiv](http://arxiv.org/abs/2509.09505v2),  [pdf](http://arxiv.org/pdf/2509.09505v2)

**Tags**: cs.AR 



### Soft Tokens, Hard Truths
**Authors**: Natasha Butt, Ariel Kwiatkowski, Ismail Labiad, Julia Kempe, Yann Ollivier

**Updated**: 2025-09-24T11:28:42Z

**Summary**: The use of continuous instead of discrete tokens during the Chain-of-Thought (CoT) phase of reasoning LLMs has garnered attention recently, based on the intuition that a continuous mixture of discrete tokens could simulate a superposition of several reasoning paths simultaneously. Theoretical results have formally proven that continuous tokens have much greater expressivity and can solve specific problems more efficiently. However, practical use of continuous tokens has been limited by strong training difficulties: previous works either just use continuous tokens at inference time on a pre-trained discrete-token model, or must distill the continuous CoT from ground-truth discrete CoTs and face computational costs that limit the CoT to very few tokens.   This is the first work introducing a scalable method to learn continuous CoTs via reinforcement learning (RL), without distilling from reference discrete CoTs. We use "soft" tokens: mixtures of tokens together with noise on the input embedding to provide RL exploration. Computational overhead is minimal, enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs match discrete-token CoTs for pass@1 and surpass them for pass@32, showing greater CoT diversity. In systematic comparisons, the best-performing scenario is to train with continuous CoT tokens then use discrete tokens for inference, meaning the "soft" models can be deployed in a standard way. Finally, we show continuous CoT RL training better preserves the predictions of the base model on out-of-domain tasks, thus providing a softer touch to the base model.

**Link**: [arxiv](http://arxiv.org/abs/2509.19170v2),  [pdf](http://arxiv.org/pdf/2509.19170v2)

**Tags**: cs.CL cs.AI cs.LG 



### Lidar-based Tracking of Traffic Participants with Sensor Nodes in   Existing Urban Infrastructure
**Authors**: Simon Schäfer, Bassam Alrifaee, Ehsan Hashemi

**Updated**: 2025-09-24T11:28:10Z

**Summary**: This paper presents a lidar-only state estimation and tracking framework, along with a roadside sensing unit for integration with existing urban infrastructure. Urban deployments demand scalable, real-time tracking solutions, yet traditional remote sensing remains costly and computationally intensive, especially under perceptually degraded conditions. Our sensor node couples a single lidar with an edge computing unit and runs a computationally efficient, GPU-free observer that simultaneously estimates object state, class, dimensions, and existence probability. The pipeline performs: (i) state updates via an extended Kalman filter, (ii) dimension estimation using a 1D grid-map/Bayesian update, (iii) class updates via a lookup table driven by the most probable footprint, and (iv) existence estimation from track age and bounding-box consistency. Experiments in dynamic urban-like scenes with diverse traffic participants demonstrate real-time performance and high precision: The complete end-to-end pipeline finishes within \SI{100}{\milli\second} for \SI{99.88}{\%} of messages, with an excellent detection rate. Robustness is further confirmed under simulated wind and sensor vibration. These results indicate that reliable, real-time roadside tracking is feasible on CPU-only edge hardware, enabling scalable, privacy-friendly deployments within existing city infrastructure. The framework integrates with existing poles, traffic lights, and buildings, reducing deployment costs and simplifying large-scale urban rollouts and maintenance efforts.

**Link**: [arxiv](http://arxiv.org/abs/2509.20009v1),  [pdf](http://arxiv.org/pdf/2509.20009v1)

**Tags**: cs.RO 



### DiffNator: Generating Structured Explanations of Time-Series Differences
**Authors**: Kota Dohi, Tomoya Nishida, Harsh Purohit, Takashi Endo, Yohei Kawaguchi

**Updated**: 2025-09-24T11:27:07Z

**Summary**: In many IoT applications, the central interest lies not in individual sensor signals but in their differences, yet interpreting such differences requires expert knowledge. We propose DiffNator, a framework for structured explanations of differences between two time series. We first design a JSON schema that captures the essential properties of such differences. Using the Time-series Observations of Real-world IoT (TORI) dataset, we generate paired sequences and train a model that combine a time-series encoder with a frozen LLM to output JSON-formatted explanations. Experimental results show that DiffNator generates accurate difference explanations and substantially outperforms both a visual question answering (VQA) baseline and a retrieval method using a pre-trained time-series encoder.

**Link**: [arxiv](http://arxiv.org/abs/2509.20007v1),  [pdf](http://arxiv.org/pdf/2509.20007v1)

**Tags**: cs.CL 



### The Knowledge-Behaviour Disconnect in LLM-based Chatbots
**Authors**: Jan Broersen

**Updated**: 2025-09-24T11:24:49Z

**Summary**: Large language model-based artificial conversational agents (like ChatGPT) give answers to all kinds of questions, and often enough these answers are correct. Just on the basis of that capacity alone, we may attribute knowledge to them. But do these models use this knowledge as a basis for their own conversational behaviour? I argue this is not the case, and I will refer to this failure as a `disconnect'. I further argue this disconnect is fundamental in the sense that with more data and more training of the LLM on which a conversational chatbot is based, it will not disappear. The reason is, as I will claim, that the core technique used to train LLMs does not allow for the establishment of the connection we are after. The disconnect reflects a fundamental limitation on the capacities of LLMs, and explains the source of hallucinations. I will furthermore consider the ethical version of the disconnect (ethical conversational knowledge not being aligned with ethical conversational behaviour), since in this domain researchers have come up with several additional techniques to influence a chatbot's behaviour. I will discuss how these techniques do nothing to solve the disconnect and can make it worse.

**Link**: [arxiv](http://arxiv.org/abs/2509.20004v1),  [pdf](http://arxiv.org/pdf/2509.20004v1)

**Tags**: cs.CL cs.AI 



### Incentivizing Reasoning for Advanced Instruction-Following of Large   Language Models
**Authors**: Yulei Qin, Gang Li, Zongyi Li, Zihan Xu, Yuchen Shi, Zhekai Lin, Xiao Cui, Ke Li, Xing Sun

**Updated**: 2025-09-24T11:19:59Z

**Summary**: Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose RAIF, a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Evaluation on OOD constraints also confirms the generalizability of our RAIF. Codes and data are available at https://github.com/yuleiqin/RAIF.   Keywords: reinforcement learning with verifiable rewards (RLVR), instruction following, complex instructions

**Link**: [arxiv](http://arxiv.org/abs/2506.01413v7),  [pdf](http://arxiv.org/pdf/2506.01413v7)

**Tags**: cs.CV cs.AI cs.CL cs.LG 



### LLMs as a synthesis between symbolic and distributed approaches to   language
**Authors**: Gemma Boleda

**Updated**: 2025-09-24T11:06:37Z

**Summary**: Since the middle of the 20th century, a fierce battle is being fought between symbolic and distributed approaches to language and cognition. The success of deep learning models, and LLMs in particular, has been alternatively taken as showing that the distributed camp has won, or dismissed as an irrelevant engineering development. In this position paper, I argue that deep learning models for language actually represent a synthesis between the two traditions. This is because 1) deep learning architectures allow for both distributed/continuous/fuzzy and symbolic/discrete/categorical-like representations and processing; 2) models trained on language make use of this flexibility. In particular, I review recent research in interpretability that showcases how a substantial part of morphosyntactic knowledge is encoded in a near-discrete fashion in LLMs. This line of research suggests that different behaviors arise in an emergent fashion, and models flexibly alternate between the two modes (and everything in between) as needed. This is possibly one of the main reasons for their wild success; and it makes them particularly interesting for the study of language. Is it time for peace?

**Link**: [arxiv](http://arxiv.org/abs/2502.11856v2),  [pdf](http://arxiv.org/pdf/2502.11856v2)

**Tags**: cs.CL 



### TrojanRobot: Physical-world Backdoor Attacks Against VLM-based Robotic   Manipulation
**Authors**: Xianlong Wang, Hewen Pan, Hangtao Zhang, Minghui Li, Shengshan Hu, Ziqi Zhou, Lulu Xue, Aishan Liu, Yunpeng Jiang, Leo Yu Zhang, Xiaohua Jia

**Updated**: 2025-09-24T11:02:49Z

**Summary**: Robotic manipulation in the physical world is increasingly empowered by \textit{large language models} (LLMs) and \textit{vision-language models} (VLMs), leveraging their understanding and perception capabilities. Recently, various attacks against such robotic policies have been proposed, with backdoor attacks drawing considerable attention for their high stealth and strong persistence capabilities. However, existing backdoor efforts are limited to simulators and suffer from physical-world realization. To address this, we propose \textit{TrojanRobot}, a highly stealthy and broadly effective robotic backdoor attack in the physical world. Specifically, we introduce a module-poisoning approach by embedding a backdoor module into the modular robotic policy, enabling backdoor control over the policy's visual perception module thereby backdooring the entire robotic policy. Our vanilla implementation leverages a backdoor-finetuned VLM to serve as the backdoor module. To enhance its generalization in physical environments, we propose a prime implementation, leveraging the LVLM-as-a-backdoor paradigm and developing three types of prime attacks, \ie, \textit{permutation}, \textit{stagnation}, and \textit{intentional} attacks, thus achieving finer-grained backdoors. Extensive experiments on the UR3e manipulator with 18 task instructions using robotic policies based on four VLMs demonstrate the broad effectiveness and physical-world stealth of TrojanRobot. Our attack's video demonstrations are available via a github link https://trojanrobot.github.io.

**Link**: [arxiv](http://arxiv.org/abs/2411.11683v6),  [pdf](http://arxiv.org/pdf/2411.11683v6)

**Tags**: cs.RO cs.AI 



### First Data of the 3000 km$^2$ Radio Detector at the Pierre Auger   Observatory
**Authors**: Bjarni Pont

**Updated**: 2025-09-24T10:47:20Z

**Summary**: In this contribution, we present the status and first data from the Radio Detector (RD) at the Pierre Auger Observatory, consisting of $1660$ radio antennas deployed across the $3000$ km$^2$ surface detector array. These antennas measure the radio emission from extensive air showers in the $30-80$ MHz band, enabling electromagnetic energy measurements for air showers with zenith angles above $65\deg$. Combined with the muonic measurements from the water-Cherenkov detectors (WCDs), this allows mass composition studies at the highest energies. The large-scale deployment of the RD began in November 2023 and was completed in November 2024. A full end-to-end calibration shows consistency between Galactic and in-lab calibration to better than $5$\% and includes continuous monitoring for hardware failures, ensuring, for example, antenna alignment within $5\deg$. We present the first data, demonstrating a strong correlation between the electromagnetic energy measured by the RD and the total shower energy measured by the WCD, confirming that the detector chain - including triggering, data readout, absolute calibration, and reconstruction is well understood. We highlight a particularly impressive $32$ EeV shower at a zenith angle of $85\deg$, producing a $50$ km-long radio footprint, showcasing the unique capabilities of this detector.

**Link**: [arxiv](http://arxiv.org/abs/2509.19984v1),  [pdf](http://arxiv.org/pdf/2509.19984v1)

**Tags**: astro-ph.IM hep-ph 



### Markov Decision Processes under External Temporal Processes
**Authors**: Ranga Shaarad Ayyagari, Revanth Raj Eega, Ambedkar Dukkipati

**Updated**: 2025-09-24T10:44:26Z

**Summary**: Reinforcement Learning Algorithms are predominantly developed for stationary environments, and the limited literature that considers nonstationary environments often involves specific assumptions about changes that can occur in transition probability matrices and reward functions. Considering that real-world applications involve environments that continuously evolve due to various external events, and humans make decisions by discerning patterns in historical events, this study investigates Markov Decision Processes under the influence of an external temporal process. We establish the conditions under which the problem becomes tractable, allowing it to be addressed by considering only a finite history of events, based on the properties of the perturbations introduced by the exogenous process. We propose and theoretically analyze a policy iteration algorithm to tackle this problem, which learns policies contingent upon the current state of the environment, as well as a finite history of prior events of the exogenous process. We show that such an algorithm is not guaranteed to converge. However, we provide a guarantee for policy improvement in regions of the state space determined by the approximation error induced by considering tractable policies and value functions. We also establish the sample complexity of least-squares policy evaluation and policy improvement algorithms that consider approximations due to the incorporation of only a finite history of temporal events. While our results are applicable to general discrete-time processes satisfying certain conditions on the rate of decay of the influence of their events, we further analyze the case of discrete-time Hawkes processes with Gaussian marks. We performed experiments to demonstrate our findings for policy evaluation and deployment in traditional control environments.

**Link**: [arxiv](http://arxiv.org/abs/2305.16056v4),  [pdf](http://arxiv.org/pdf/2305.16056v4)

**Tags**: cs.LG cs.AI 



### LASER: Stratified Selective Sampling for Instruction Tuning with   Dedicated Scoring Strategy
**Authors**: Paramita Mirza, Lucas Weber, Fabian Küch

**Updated**: 2025-09-24T10:42:38Z

**Summary**: Recent work shows that post-training datasets for LLMs can be substantially downsampled without noticeably deteriorating performance. However, data selection often incurs high computational costs or is limited to narrow domains. In this paper, we demonstrate that data selection can be both -- efficient and universal -- by using a multi-step pipeline in which we efficiently bin data points into groups, estimate quality using specialized models, and score difficulty with a robust, lightweight method. Task-based categorization allows us to control the composition of our final data -- crucial for finetuning multi-purpose models. To guarantee diversity, we improve upon previous work using embedding models and a clustering algorithm. This integrated strategy enables high-performance fine-tuning with minimal overhead.

**Link**: [arxiv](http://arxiv.org/abs/2505.22157v3),  [pdf](http://arxiv.org/pdf/2505.22157v3)

**Tags**: cs.CL 



### Date Fragments: A Hidden Bottleneck of Tokenization for Temporal   Reasoning
**Authors**: Gagan Bhatia, Maxime Peyrard, Wei Zhao

**Updated**: 2025-09-24T10:35:24Z

**Summary**: Modern BPE tokenizers often split calendar dates into meaningless fragments, e.g., 20250312 $\rightarrow$ 202, 503, 12, inflating token counts and obscuring the inherent structure needed for robust temporal reasoning. In this work, we (1) introduce a simple yet interpretable metric, termed date fragmentation ratio, that measures how faithfully a tokenizer preserves multi-digit date components; (2) release DateAugBench, a suite of 6500 examples spanning three temporal reasoning tasks: context-based date resolution, format-invariance puzzles, and date arithmetic across historical, contemporary, and future time periods; and (3) through layer-wise probing and causal attention-hop analyses, uncover an emergent date-abstraction mechanism whereby large language models stitch together the fragments of month, day, and year components for temporal reasoning. Our experiments show that excessive fragmentation correlates with accuracy drops of up to 10 points on uncommon dates like historical and futuristic dates. Further, we find that the larger the model, the faster the emergent date abstraction that heals date fragments is accomplished. Lastly, we observe a reasoning path that LLMs follow to assemble date fragments, typically differing from human interpretation (year $\rightarrow$ month $\rightarrow$ day). Our datasets and code are made publicly available \href{https://github.com/gagan3012/date-fragments}{here}.

**Link**: [arxiv](http://arxiv.org/abs/2505.16088v3),  [pdf](http://arxiv.org/pdf/2505.16088v3)

**Tags**: cs.CL cs.AI 



### SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation   via Multi-Modal Emotion Embedding
**Authors**: Phyo Thet Yee, Dimitrios Kollias, Sudeepta Mishra, Abhinav Dhall

**Updated**: 2025-09-24T10:21:29Z

**Summary**: Audio-driven talking face generation has received growing interest, particularly for applications requiring expressive and natural human-avatar interaction. However, most existing emotion-aware methods rely on a single modality (either audio or image) for emotion embedding, limiting their ability to capture nuanced affective cues. Additionally, most methods condition on a single reference image, restricting the model's ability to represent dynamic changes in actions or attributes across time. To address these issues, we introduce SynchroRaMa, a novel framework that integrates a multi-modal emotion embedding by combining emotional signals from text (via sentiment analysis) and audio (via speech-based emotion recognition and audio-derived valence-arousal features), enabling the generation of talking face videos with richer and more authentic emotional expressiveness and fidelity. To ensure natural head motion and accurate lip synchronization, SynchroRaMa includes an audio-to-motion (A2M) module that generates motion frames aligned with the input audio. Finally, SynchroRaMa incorporates scene descriptions generated by Large Language Model (LLM) as additional textual input, enabling it to capture dynamic actions and high-level semantic attributes. Conditioning the model on both visual and textual cues enhances temporal consistency and visual realism. Quantitative and qualitative experiments on benchmark datasets demonstrate that SynchroRaMa outperforms the state-of-the-art, achieving improvements in image quality, expression preservation, and motion realism. A user study further confirms that SynchroRaMa achieves higher subjective ratings than competing methods in overall naturalness, motion diversity, and video smoothness. Our project page is available at <https://novicemm.github.io/synchrorama>.

**Link**: [arxiv](http://arxiv.org/abs/2509.19965v1),  [pdf](http://arxiv.org/pdf/2509.19965v1)

**Tags**: cs.CV 



### Data Fusion for Full-Range Response Reconstruction via Diffusion Models
**Authors**: Wingho Feng, Quanwang Li, Chen Wang, Jian-sheng Fan

**Updated**: 2025-09-24T09:40:29Z

**Summary**: Accurately capturing the full-range response of structures is crucial in structural health monitoring (SHM) for ensuring safety and operational integrity. However, limited sensor deployment due to cost, accessibility, or scale often hinders comprehensive monitoring. This paper presents a generative data fusion framework utilizing diffusion models, to reconstruct the full-range structural response from sparse and heterogeneous sensor measurements. We incorporate Diffusion Posterior Sampling (DPS) into the reconstruction framework, using sensor measurements as probabilistic constraints to guide the sampling process. Three forward models are designed: Direct Observation Mapping (DOM), Channel-based Observation Mapping (COM), and Neural Network Forward Model (NNFM), enabling flexible adaptation to different sensor placement conditions and reconstruction targets. The proposed framework is validated on a steel plate shear wall exhibiting nonlinear responses. By simultaneously sampling 100 realizations and averaging them as the ensemble prediction result, the three forward models achieve Weighted Mean Absolute Percentage Errors of 1.62% (DOM), 3.27% (COM), and 3.49% (NNFM). Sensitivity analyses further demonstrate robust performance under varying hyperparameters, sensor configurations, and noise levels. The proposed framework shows new possibilities for probabilistic modeling and decision-making in SHM by harnessing the capabilities of diffusion models, offering a novel data fusion approach for full-range monitoring of structures.

**Link**: [arxiv](http://arxiv.org/abs/2502.00795v2),  [pdf](http://arxiv.org/pdf/2502.00795v2)

**Tags**: cs.CE 



### Documentation Retrieval Improves Planning Language Generation
**Authors**: Renxiang Wang, Li Zhang

**Updated**: 2025-09-24T09:38:48Z

**Summary**: Certain strong LLMs have shown promise for zero-shot formal planning by generating planning languages like PDDL. Yet, performance of most open-source models under 50B parameters has been reported to be close to zero due to the low-resource nature of these languages. We significantly improve their performance via a series of lightweight pipelines that integrates documentation retrieval with modular code generation and error refinement. With models like Llama-4-Maverick, our best pipeline improves plan correctness from 0\% to over 80\% on the common BlocksWorld domain. However, while syntactic errors are substantially reduced, semantic errors persist in more challenging domains, revealing fundamental limitations in current models' reasoning capabilities.\footnote{Our code and data can be found at https://github.com/Nangxxxxx/PDDL-RAG

**Link**: [arxiv](http://arxiv.org/abs/2509.19931v1),  [pdf](http://arxiv.org/pdf/2509.19931v1)

**Tags**: cs.IR F.2.2; I.2.7 



### MMSE-Calibrated Few-Shot Prompting for Alzheimer's Detection
**Authors**: Jana Sweidan, Mounim A. El-Yacoubi, Nasredine Semmar

**Updated**: 2025-09-24T09:33:46Z

**Summary**: Prompting large language models is a training-free method for detecting Alzheimer's disease from speech transcripts. Using the ADReSS dataset, we revisit zero-shot prompting and study few-shot prompting with a class-balanced protocol using nested interleave and a strict schema, sweeping up to 20 examples per class. We evaluate two variants achieving state-of-the-art prompting results. (i) MMSE-Proxy Prompting: each few-shot example carries a probability anchored to Mini-Mental State Examination bands via a deterministic mapping, enabling AUC computing; this reaches 0.82 accuracy and 0.86 AUC (ii) Reasoning-augmented Prompting: few-shot examples pool is generated with a multimodal LLM (GPT-5) that takes as input the Cookie Theft image, transcript, and MMSE to output a reasoning and MMSE-aligned probability; evaluation remains transcript-only and reaches 0.82 accuracy and 0.83 AUC. To our knowledge, this is the first ADReSS study to anchor elicited probabilities to MMSE and to use multimodal construction to improve interpretability.

**Link**: [arxiv](http://arxiv.org/abs/2509.19926v1),  [pdf](http://arxiv.org/pdf/2509.19926v1)

**Tags**: cs.LG 



