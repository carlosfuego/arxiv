# Arxiv Results
## Keyword: kv cache 
 ### Buffer replay enhances the robustness of multimodal learning under missing-modality
**Authors**: Hongye Zhu, Xuan Liu, Yanwen Ba, Jingye Xue, Shigeng Zhang

**Updated**: 2025-11-28T10:55:31Z

**Summary**: Missing modalities consistently lead to significant performance degradation in multimodal models. Existing approaches either synthesize missing modalities at high computational cost or apply prompt-based fine-tuning that relies only on adjacent-layer features and overlooks long-distance contextual information, which may offer additional tolerance to errors when one or more modalities are missing. To address this, we introduce REplay Prompting (REP): (1) construct modality-wise feature buffers via a residual bypass to cache early-layer representations and replay them in deeper layers, mitigating information loss as network depth increases; (2) employ a private-shared feature decoupling strategy, where private buffers preserve modality-specific signals and shared buffers encode cross-modal semantics; and (3) design a task-aware dynamic initialization mechanism to configure these buffers differently, improving stability and generalization under diverse missing-modality conditions. Experiments on vision-language, vision-language-audio, and temporal multimodal benchmarks demonstrate that REP consistently outperforms prior methods under both single- and multi-modality missing scenarios, while introducing only negligible parameter overhead. These results establish REP as a lightweight and effective paradigm for robust multimodal learning in challenging missing-modality environments.

**Link**: [arxiv](https://arxiv.org/abs/2511.23070v1),  [pdf](https://arxiv.org/pdf/2511.23070v1)

**Tags**: cs.CV cs.LG 



### Cohet: A CXL-Driven Coherent Heterogeneous Computing Framework with Hardware-Calibrated Full-System Simulation
**Authors**: Yanjing Wang, Lizhou Wu, Sunfeng Gao, Yibo Tang, Junhui Luo, Zicong Wang, Yang Ou, Dezun Dong, Nong Xiao, Mingche Lai

**Updated**: 2025-11-28T09:22:37Z

**Summary**: Conventional heterogeneous computing systems built on PCIe interconnects suffer from inefficient fine-grained host-device interactions and complex programming models. In recent years, many proprietary and open cache-coherent interconnect standards have emerged, among which compute express link (CXL) prevails in the open-standard domain after acquiring several competing solutions. Although CXL-based coherent heterogeneous computing holds the potential to fundamentally transform the collaborative computing mode of CPUs and XPUs, research in this direction remains hampered by the scarcity of available CXL-supported platforms, immature software/hardware ecosystems, and unclear application prospects. This paper presents Cohet, the first CXL-driven coherent heterogeneous computing framework. Cohet decouples the compute and memory resources to form unbiased CPU and XPU pools which share a single unified and coherent memory pool. It exposes a standard malloc/mmap interface to both CPU and XPU compute threads, leaving the OS dealing with smart memory allocation and management of heterogeneous resources. To facilitate Cohet research, we also present a full-system cycle-level simulator named SimCXL, which is capable of modeling all CXL sub-protocols and device types. SimCXL has been rigorously calibrated against a real CXL testbed with various CXL memory and accelerators, showing an average simulation error of 3%. Our evaluation reveals that CXL.cache reduces latency by 68% and increases bandwidth by 14.4x compared to DMA transfers at cacheline granularity. Building upon these insights, we demonstrate the benefits of Cohet with two killer apps, which are remote atomic operation (RAO) and remote procedure call (RPC). Compared to PCIe-NIC design, CXL-NIC achieves a 5.5 to 40.2x speedup for RAO offloading and an average speedup of 1.86x for RPC (de)serialization offloading.

**Link**: [arxiv](https://arxiv.org/abs/2511.23011v1),  [pdf](https://arxiv.org/pdf/2511.23011v1)

**Tags**: cs.AR 



### BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation
**Authors**: Zeyu Zhang, Shuning Chang, Yuanyu He, Yizeng Han, Jiasheng Tang, Fan Wang, Bohan Zhuang

**Updated**: 2025-11-28T08:25:59Z

**Summary**: Generating minute-long videos is a critical step toward developing world models, providing a foundation for realistic extended scenes and advanced AI simulators. The emerging semi-autoregressive (block diffusion) paradigm integrates the strengths of diffusion and autoregressive models, enabling arbitrary-length video generation and improving inference efficiency through KV caching and parallel sampling. However, it yet faces two enduring challenges: (i) KV-cache-induced long-horizon error accumulation, and (ii) the lack of fine-grained long-video benchmarks and coherence-aware metrics. To overcome these limitations, we propose BlockVid, a novel block diffusion framework equipped with semantic-aware sparse KV cache, an effective training strategy called Block Forcing, and dedicated chunk-wise noise scheduling and shuffling to reduce error propagation and enhance temporal consistency. We further introduce LV-Bench, a fine-grained benchmark for minute-long videos, complete with new metrics evaluating long-range coherence. Extensive experiments on VBench and LV-Bench demonstrate that BlockVid consistently outperforms existing methods in generating high-quality, coherent minute-long videos. In particular, it achieves a 22.2% improvement on VDE Subject and a 19.4% improvement on VDE Clarity in LV-Bench over the state of the art approaches. Project website: https://ziplab.co/BlockVid. Inferix (Code): https://github.com/alibaba-damo-academy/Inferix.

**Link**: [arxiv](https://arxiv.org/abs/2511.22973v1),  [pdf](https://arxiv.org/pdf/2511.22973v1)

**Tags**: cs.CV 



### The Immutable Tensor Architecture: A Pure Dataflow Approach for Secure, Energy-Efficient AI Inference
**Authors**: Fang Li

**Updated**: 2025-11-28T05:36:12Z

**Summary**: The deployment of Large Language Models (LLMs) on consumer edge devices is throttled by the "Memory Wall" -- the prohibitive bandwidth and energy cost of fetching gigabytes of model weights from DRAM for every token generated. Current architectures (GPUs, NPUs) treat model weights as mutable software data, incurring massive energy penalties to maintain general-purpose programmability. We propose The Immutable Tensor Architecture (ITA), a paradigm shift that treats model weights not as data, but as physical circuit topology. By encoding parameters directly into the metal interconnects and logic of mature-node ASICs (28nm/40nm), ITA eliminates the memory hierarchy entirely. We present a "Split-Brain" system design where a host CPU manages dynamic KV-cache operations while the ITA ASIC acts as a stateless, ROM-embedded dataflow engine.

**Link**: [arxiv](https://arxiv.org/abs/2511.22889v1),  [pdf](https://arxiv.org/pdf/2511.22889v1)

**Tags**: cs.AR 



### Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems
**Authors**: Shashwat Jaiswal, Shrikara Arun, Anjaly Parayil, Ankur Mallick, Spyros Mastorakis, Alind Khare, Chloi Alverti, Renee St Amant, Chetan Bansal, Victor Rühle, Josep Torrellas

**Updated**: 2025-11-28T05:04:02Z

**Summary**: Low-Rank Adaptation (LoRA) has become the de facto method for parameter-efficient fine-tuning of large language models (LLMs), enabling rapid adaptation to diverse domains. In production, LoRA-based models are served at scale, creating multi-tenant environments with hundreds of adapters sharing a base model. However, state-of-the-art serving systems co-batch heterogeneous adapters without accounting for rank (size) variability, leading to severe performance skew, which ultimately requires adding more GPUs to satisfy service-level objectives (SLOs). Existing optimizations, focused on loading, caching, and kernel execution, ignore this heterogeneity, leaving GPU resources underutilized. We present LoRAServe, a workload-aware dynamic adapter placement and routing framework designed to tame rank diversity in LoRA serving. By dynamically rebalancing adapters across GPUs and leveraging GPU Direct RDMA for remote access, LoRAServe maximizes throughput and minimizes tail latency under real-world workload drift. Evaluations on production traces from Company X show that LoRAServe elicits up to 2$\times$ higher throughput, up to 9$\times$ lower TTFT, while using up to 50% fewer GPUs under SLO constraints compared to state-of-the-art systems.

**Link**: [arxiv](https://arxiv.org/abs/2511.22880v1),  [pdf](https://arxiv.org/pdf/2511.22880v1)

**Tags**: cs.DC cs.AI cs.LG 



### GLOW: Global Illumination-Aware Inverse Rendering of Indoor Scenes Captured with Dynamic Co-Located Light & Camera
**Authors**: Jiaye Wu, Saeed Hadadan, Geng Lin, Peihan Tu, Matthias Zwicker, David Jacobs, Roni Sengupta

**Updated**: 2025-11-28T03:24:12Z

**Summary**: Inverse rendering of indoor scenes remains challenging due to the ambiguity between reflectance and lighting, exacerbated by inter-reflections among multiple objects. While natural illumination-based methods struggle to resolve this ambiguity, co-located light-camera setups offer better disentanglement as lighting can be easily calibrated via Structure-from-Motion. However, such setups introduce additional complexities like strong inter-reflections, dynamic shadows, near-field lighting, and moving specular highlights, which existing approaches fail to handle. We present GLOW, a Global Illumination-aware Inverse Rendering framework designed to address these challenges. GLOW integrates a neural implicit surface representation with a neural radiance cache to approximate global illumination, jointly optimizing geometry and reflectance through carefully designed regularization and initialization. We then introduce a dynamic radiance cache that adapts to sharp lighting discontinuities from near-field motion, and a surface-angle-weighted radiometric loss to suppress specular artifacts common in flashlight captures. Experiments show that GLOW substantially outperforms prior methods in material reflectance estimation under both natural and co-located illumination.

**Link**: [arxiv](https://arxiv.org/abs/2511.22857v1),  [pdf](https://arxiv.org/pdf/2511.22857v1)

**Tags**: cs.CV 



### CacheTrap: Injecting Trojans in LLMs without Leaving any Traces in Inputs or Weights
**Authors**: Mohaiminul Al Nahian, Abeer Matar A. Almalky, Gamana Aragonda, Ranyang Zhou, Sabbir Ahmed, Dmitry Ponomarev, Li Yang, Shaahin Angizi, Adnan Siraj Rakin

**Updated**: 2025-11-27T18:30:19Z

**Summary**: Adversarial weight perturbation has emerged as a concerning threat to LLMs that either use training privileges or system-level access to inject adversarial corruption in model weights. With the emergence of innovative defensive solutions that place system- and algorithm-level checks and corrections in the input and weight spaces, these perturbations are increasingly susceptible to defenses. This work develops a novel perspective on Trojan attacks that generates an attacker-designed model output while leaving no attack traces on the inputs or weights. Such an attack space can be unlocked through corruption of the key-value (KV) cache. In this paper, we introduce CacheTrap, a novel Trojan attack that corrupts the value vectors stored in the KV cache. These vectors capture the dynamic activations for specific token positions and therefore constitute a natural surface for transient, inference-time trigger insertion. The transient nature of these KV values and their dependence on victim input imply additional constraints on our attack, such as a lack of knowledge of the victim's data or domain application, and, consequently, a lack of gradient information. The objective of the proposed CacheTrap is to develop a vulnerable KV bit-searching algorithm so that, once the attack employs the identified bit-flip as a trigger, the model generates targeted behavior, e.g., classifying inputs towards the target class. Moreover, CacheTrap is a data- and gradient-free attack which also has no impact on the model's utility. Our evaluation demonstrates that the proposed attack enables the first successful Trojan attack on LLMs with a single bit flip in the KV cache. In addition, the data-independent nature of the attack ensures that once the attacker identifies the vulnerable bit index, the location remains constant and can be transferred to a wide range of victim tasks/datasets/queries with no overhead.

**Link**: [arxiv](https://arxiv.org/abs/2511.22681v1),  [pdf](https://arxiv.org/pdf/2511.22681v1)

**Tags**: cs.CR 



### 3RSeT: Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison
**Authors**: Elham Cheshmikhani, Hamed Farbeh, Hossein Asad

**Updated**: 2025-11-27T15:39:45Z

**Summary**: Recent development in memory technologies has introduced Spin-Transfer Torque Magnetic RAM (STT-MRAM) as the most promising replacement for SRAMs in on-chip cache memories. Besides its lower leakage power, higher density, immunity to radiation-induced particles, and non-volatility, an unintentional bit flip during read operation, referred to as read disturbance error, is a severe reliability challenge in STT-MRAM caches. One major source of read disturbance error in STT-MRAM caches is simultaneous accesses to all tags for parallel comparison operation in a cache set, which has not been addressed in previous work. This paper first demonstrates that high read accesses to tag array extremely increase the read disturbance rate and then proposes a low-cost scheme, so-called Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison (3RSeT), to reduce the error rate by eliminating a significant portion of tag reads. 3RSeT proactively disables the tags that have no chance for hit, using low significant bits of the tags on each access request. Our evaluations using gem5 full-system cycle-accurate simulator show that 3RSeT reduces the read disturbance rate in the tag array by 71.8%, which results in 3.6x improvement in Mean Time To Failure (MTTF). In addition, the energy consumption is reduced by 62.1% without compromising performance and with less than 0.4% area overhead.

**Link**: [arxiv](https://arxiv.org/abs/2511.22551v1),  [pdf](https://arxiv.org/pdf/2511.22551v1)

**Tags**: cs.AR cs.ET cs.PF 



### Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration
**Authors**: Mengyu Yang, Yanming Yang, Chenyi Xu, Chenxi Song, Yufan Zuo, Tong Zhao, Ruibo Li, Chi Zhang

**Updated**: 2025-11-27T15:13:32Z

**Summary**: Diffusion models have achieved impressive generative quality across modalities like 2D images, videos, and 3D shapes, but their inference remains computationally expensive due to the iterative denoising process. While recent caching-based methods effectively reuse redundant computations to speed up 2D and video generation, directly applying these techniques to 3D diffusion models can severely disrupt geometric consistency. In 3D synthesis, even minor numerical errors in cached latent features accumulate, causing structural artifacts and topological inconsistencies. To overcome this limitation, we propose Fast3Dcache, a training-free geometry-aware caching framework that accelerates 3D diffusion inference while preserving geometric fidelity. Our method introduces a Predictive Caching Scheduler Constraint (PCSC) to dynamically determine cache quotas according to voxel stabilization patterns and a Spatiotemporal Stability Criterion (SSC) to select stable features for reuse based on velocity magnitude and acceleration criterion. Comprehensive experiments show that Fast3Dcache accelerates inference significantly, achieving up to a 27.12% speed-up and a 54.8% reduction in FLOPs, with minimal degradation in geometric quality as measured by Chamfer Distance (2.48%) and F-Score (1.95%).

**Link**: [arxiv](https://arxiv.org/abs/2511.22533v1),  [pdf](https://arxiv.org/pdf/2511.22533v1)

**Tags**: cs.CV 



### Enhancing Trustworthiness with Mixed Precision: Benchmarks, Opportunities, and Challenges
**Authors**: Guanxi Lu, Hao Mark Chen, Zhiqiang Que, Wayne Luk, Hongxiang Fan

**Updated**: 2025-11-27T14:17:43Z

**Summary**: Large language models (LLMs) have shown promising performance across various tasks. However, their autoregressive decoding process poses significant challenges for efficient deployment on existing AI hardware. Quantization alleviates memory and compute pressure by compressing weights, activations, and KV caches to low precisions while preserving generation quality. However, existing quantization frameworks typically focus on perplexity or classification accuracy, often omitting critical trustworthiness metrics. This gap introduces risks when applying quantized LLMs to downstream high-stakes domains such as finance and healthcare. In this work, we systematically investigate the impact of quantization on four trustworthiness metrics (adversarial robustness, fairness, machine ethics, and out-of-distribution robustness) and identify the instability across compression ratios and quantization methods. Building on these observations, we develop a novel precision-ensemble voting approach that leverages predictions from mixed-precision variants of the same model and consistently improves performance by up to $5.8\%$ on trustworthiness metrics. Our results highlight the importance of considering trustworthiness when developing model compression techniques and point to research opportunities at the intersection of compression and trustworthiness for safety-critical applications.

**Link**: [arxiv](https://arxiv.org/abs/2511.22483v1),  [pdf](https://arxiv.org/pdf/2511.22483v1)

**Tags**: cs.LG 



### OmniInfer: System-Wide Acceleration Techniques for Optimizing LLM Serving Throughput and Latency
**Authors**: Jun Wang, Yunxiang Yao, Wenwei Kuang, Runze Mao, Zhenhao Sun, Zhuang Tao, Ziyang Zhang, Dengyu Li, Jiajun Chen, Zhili Wang, Kai Cui, Congzhi Cai, Longwen Lan, Ken Zhang

**Updated**: 2025-11-27T14:13:47Z

**Summary**: Large Language Models drive a wide range of modern AI applications but impose substantial challenges on large-scale serving systems due to intensive computation, strict latency constraints, and throughput bottlenecks. We introduce OmniInfer, a unified system-level acceleration framework designed to maximize end-to-end serving efficiency through fine-grained optimization of expert placement, cache compression, and scheduling. OmniInfer integrates three complementary components: OmniPlacement for load-aware Mixture-of-Experts scheduling, OmniAttn for sparse attention acceleration, and OmniProxy for disaggregation-aware request scheduling. Built atop vLLM, OmniInfer delivers system-wide performance gains through adaptive resource disaggregation, efficient sparsity exploitation, and global coordination across prefill and decode phases. Evaluated on DeepSeek-R1 within a 10-node Ascend 910C cluster, OmniInfer achieves 616 QPM, where the unified framework reduces TPOT by 36\%, and the superimposition of OmniProxy further slashes TTFT by 38\%. The project is open-sourced at [this https URL](https://gitee.com/omniai/omniinfer).

**Link**: [arxiv](https://arxiv.org/abs/2511.22481v1),  [pdf](https://arxiv.org/pdf/2511.22481v1)

**Tags**: cs.DC 



### EfficientNav: Towards On-Device Object-Goal Navigation with Navigation Map Caching and Retrieval
**Authors**: Zebin Yang, Sunjian Zheng, Tong Xie, Tianshi Xu, Bo Yu, Fan Wang, Jie Tang, Shaoshan Liu, Meng Li

**Updated**: 2025-11-27T13:54:42Z

**Summary**: Object-goal navigation (ObjNav) tasks an agent with navigating to the location of a specific object in an unseen environment. Embodied agents equipped with large language models (LLMs) and online constructed navigation maps can perform ObjNav in a zero-shot manner. However, existing agents heavily rely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small LLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to limited model capacity for understanding complex navigation maps, which prevents deploying ObjNav on local devices. At the same time, the long prompt introduced by the navigation map description will cause high planning latency on local devices. In this paper, we propose EfficientNav to enable on-device efficient LLM-based zero-shot ObjNav. To help the smaller LLMs better understand the environment, we propose semantics-aware memory retrieval to prune redundant information in navigation maps. To reduce planning latency, we propose discrete memory caching and attention-based memory clustering to efficiently save and re-use the KV cache. Extensive experimental results demonstrate that EfficientNav achieves 11.1% improvement in success rate on HM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time latency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our code is available on https://github.com/PKU-SEC-Lab/EfficientNav.

**Link**: [arxiv](https://arxiv.org/abs/2510.18546v2),  [pdf](https://arxiv.org/pdf/2510.18546v2)

**Tags**: cs.RO cs.AI 



### Semantic-Aware Caching for Efficient Image Generation in Edge Computing
**Authors**: Hanshuai Cui, Zhiqing Tang, Zhi Yao, Weijia Ji, Wei Zhao

**Updated**: 2025-11-27T12:58:25Z

**Summary**: Text-to-image generation employing diffusion models has attained significant popularity due to its capability to produce high-quality images that adhere to textual prompts. However, the integration of diffusion models faces critical challenges into resource-constrained mobile and edge environments because it requires multiple denoising steps from the original random noise. A practical way to speed up denoising is to initialize the process with a noised reference image that is similar to the target, since both images share similar layouts, structures, and details, allowing for fewer denoising steps. Based on this idea, we present CacheGenius, a hybrid image generation system in edge computing that accelerates generation by combining text-toimage and image-to-image workflows. It generates images from user text prompts using cached reference images. CacheGenius introduces a semantic-aware classified storage scheme and a request-scheduling algorithm that ensures semantic alignment between references and targets. To ensure sustained performance, it employs a cache maintenance policy that proactively evicts obsolete entries via correlation analysis. Evaluated in a distributed edge computing system, CacheGenius reduces generation latency by 41% and computational costs by 48% relative to baselines, while maintaining competitive evaluation metrics.

**Link**: [arxiv](https://arxiv.org/abs/2511.22421v1),  [pdf](https://arxiv.org/pdf/2511.22421v1)

**Tags**: cs.NI 



### PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel
**Authors**: Jinjun Yi, Zhixin Zhao, Yitao Hu, Ke Yan, Weiwei Sun, Hao Wang, Laiping Zhao, Yuhao Zhang, Wenxin Li, Keqiu Li

**Updated**: 2025-11-27T11:10:30Z

**Summary**: LLM serving is increasingly dominated by decode attention, which is a memory-bound operation due to massive KV cache loading from global memory. Meanwhile, real-world workloads exhibit substantial, hierarchical shared prefixes across requests (e.g., system prompts, tools/templates, RAG). Existing attention implementations fail to fully exploit prefix sharing: *one-query-per-CTA* execution repeatedly loads shared prefix KV cache, while *one-size-fits-all* tiling leaves on-chip resources idle and exacerbates bubbles for uneven KV lengths. These choices amplify memory bandwidth pressure and stall memory-bound decode attention.   This paper introduces PAT, a prefix-aware attention kernel implementation for LLM decoding that organizes execution with a pack-forward-merge paradigm. PAT packs queries by shared prefix to reduce repeated memory accesses, runs a customized multi-tile kernel to achieve high resource efficiency. It further applies practical multi-stream forwarding and KV splitting to reduce resource bubbles. The final merge performs online softmax with negligible overhead. We implement PAT as an off-the-shelf plugin for vLLM. Evaluation on both real-world and synthetic workloads shows that PAT reduces attention latency by 67.4% on average and TPOT by 13.6-83.4% under the same configurations against state-of-the-art attention kernels.

**Link**: [arxiv](https://arxiv.org/abs/2511.22333v1),  [pdf](https://arxiv.org/pdf/2511.22333v1)

**Tags**: cs.DC cs.CL 



### MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte Carlo Tree Search
**Authors**: Shuozhi Yuan, Limin Chen, Miaomiao Yuan, Zhao Jin

**Updated**: 2025-11-27T09:37:42Z

**Summary**: Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at translating natural language questions into SQL queries. While recent advances in large language models have greatly improved performance, most existing approaches depend on models with tens of billions of parameters or costly APIs, limiting their applicability in resource-constrained environments. For real world, especially on edge devices, it is crucial for Text-to-SQL to ensure cost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL is of great practical significance. However, smaller LLMs often struggle with complicated user instruction, redundant schema linking or syntax correctness. To address these challenges, we propose MCTS-SQL, a novel framework that uses Monte Carlo Tree Search to guide SQL generation through multi-step refinement. Since the light-weight models' weak performance of single-shot prediction, we generate better results through several trials with feedback. However, directly applying MCTS-based methods inevitably leads to significant time and computational overhead. Driven by this issue, we propose a token-level prefix-cache mechanism that stores prior information during iterations, effectively improved the execution speed. Experiments results on the SPIDER and BIRD benchmarks demonstrate the effectiveness of our approach. Using a small open-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When leveraging a more powerful model Gemini 2.5 to explore the performance upper bound, we achieved results competitive with the SOTA. Our findings demonstrate that even small models can be effectively deployed in practical Text-to-SQL systems with the right strategy.

**Link**: [arxiv](https://arxiv.org/abs/2501.16607v3),  [pdf](https://arxiv.org/pdf/2501.16607v3)

**Tags**: cs.DB cs.AI cs.CL cs.PL 



### KeepKV: Achieving Periodic Lossless KV Cache Compression for Efficient LLM Inference
**Authors**: Yuxuan Tian, Zihan Wang, Yebo Peng, Aomufei Yuan, Zhiming Wang, Bairen Yi, Xin Liu, Yong Cui, Tong Yang

**Updated**: 2025-11-27T07:44:35Z

**Summary**: Efficient inference of large language models (LLMs) is hindered by an ever-growing key-value (KV) cache, making KV cache compression a critical research direction. Traditional methods selectively evict less important KV cache entries, which leads to information loss and hallucinations. Recently, merging-based strategies have been explored to retain more information by merging KV pairs that would be discarded; however, these existing approaches inevitably introduce inconsistencies in attention distributions before and after merging, causing degraded generation quality. To overcome this challenge, we propose KeepKV, a novel adaptive KV cache merging method designed to preserve performance under strict memory constraints, achieving single-step lossless compression and providing error bounds for multi-step compression. KeepKV introduces the Electoral Votes mechanism that records merging history and adaptively adjusts attention scores. Moreover, it further leverages a novel Zero Inference-Perturbation Merging method, compensating for attention loss resulting from cache merging. Extensive experiments on various benchmarks and LLM architectures demonstrate that KeepKV substantially reduces memory usage while successfully retaining essential context information, achieving over 2x inference throughput improvement and maintaining superior generation quality even with only 10% KV cache budgets.

**Link**: [arxiv](https://arxiv.org/abs/2504.09936v2),  [pdf](https://arxiv.org/pdf/2504.09936v2)

**Tags**: cs.LG cs.AI cs.CL 



### Statistical Independence Aware Caching for LLM Workflows
**Authors**: Yihan Dai, Dimitrios Stamatios Bouras, Haoxiang Jia, Sergey Mechtaev

**Updated**: 2025-11-27T05:16:28Z

**Summary**: Large language models (LLMs) inference is both expensive and slow. Local caching of responses offers a practical solution to reduce the cost and latency of LLM queries. In research contexts, caching also enhances reproducibility and provides flexibility for experimentation. However, naive reuse of cached responses compromises statistical independence, a critical property for probabilistic workflows. In applications of LLM for code, it underpins performance metrics such as Pass@k and uncertainty estimation, as well as algorithms like program repair loops and retries. Existing LLM caching systems lack ways to enforce statistical independence constraints. To address this, we introduce Mnimi, a cache design pattern that supports modular LLM workflows while ensuring statistical integrity at the component level. Its core innovation lies in encapsulating statistical constraints within the type of LLM references, allowing users to manage and transform these types according to the scope and requirements of their algorithm. We implemented this design pattern in Python using a combination of decorators and iterators over infinite sequences. A case study on SpecFix, an recent automated program specification repair system, highlights how Mnimi improves reproducibility, ease of debugging, time and cost efficiency while preserving statistical correctness.

**Link**: [arxiv](https://arxiv.org/abs/2511.22118v1),  [pdf](https://arxiv.org/pdf/2511.22118v1)

**Tags**: cs.SE 



### Clock2Q+: A Simple and Efficient Replacement Algorithm for Metadata Cache in VMware vSAN
**Authors**: Yiyan Zhai, Bintang Dwi Marthen, Sarath Balivada, Vamsi Sudhakar Bojji, Eric Knauft, Jitender Rohilla, Jiaqi Zuo, Quanxing Liu, Maxime Austruy, Wenguang Wang, Juncheng Yang

**Updated**: 2025-11-26T22:34:26Z

**Summary**: Cache replacement algorithms are critical building blocks of storage systems. This paper examines the characteristics of metadata caches and argues that they inherently exhibit correlated references, even when the corresponding data accesses do not contain correlated references. The presence of correlated references reduces the effectiveness of cache replacement algorithms because these references are often mistakenly categorized as hot blocks. Clock2Q+ is specifically designed for metadata caches and has been implemented in vSAN and VDFS, two flagship storage products of VMware by Broadcom. Similar to S3-FIFO, Clock2Q+ uses three queues; however, Clock2Q+ introduces a correlation window in the Small FIFO queue, where blocks in this window do not set the reference bit. This simple enhancement allows Clock2Q+ to outperform state-of-the-art replacement algorithms. Compared to S3-FIFO, the second-best performing algorithm, Clock2Q+ achieves up to a 28.5% lower miss ratio on metadata traces. Clock2Q+ possesses the essential properties required for large-scale storage systems: it has low CPU overhead on cache hits, low memory overhead, scales efficiently to multiple CPUs, and is both easy to tune and implement. Additionally, Clock2Q+ outperforms state-of-the-art cache replacement algorithms on data traces as well.

**Link**: [arxiv](https://arxiv.org/abs/2511.21958v1),  [pdf](https://arxiv.org/pdf/2511.21958v1)

**Tags**: cs.DC 



### Diagonal Scaling: A Multi-Dimensional Resource Model and Optimization Framework for Distributed Databases
**Authors**: Shahir Abdullah, Syed Rohit Zaman

**Updated**: 2025-11-26T17:36:15Z

**Summary**: Modern cloud databases present scaling as a binary decision: scale-out by adding nodes or scale-up by increasing per-node resources. This one-dimensional view is limiting because database performance, cost, and coordination overhead emerge from the joint interaction of horizontal elasticity and per-node CPU, memory, network bandwidth, and storage IOPS. As a result, systems often overreact to load spikes, underreact to memory pressure, or oscillate between suboptimal states. We introduce the Scaling Plane, a two-dimensional model in which each distributed database configuration is represented as a point (H, V), with H denoting node count and V a vector of resources. Over this plane, we define smooth approximations of latency, throughput, coordination overhead, and monetary cost, providing a unified view of performance trade-offs. We show analytically and empirically that optimal scaling trajectories frequently lie along diagonal paths: sequences of joint horizontal and vertical adjustments that simultaneously exploit cluster parallelism and per-node improvements. To compute such actions, we propose DIAGONALSCALE, a discrete local-search algorithm that evaluates horizontal, vertical, and diagonal moves in the Scaling Plane and selects the configuration minimizing a multi-objective function subject to SLA constraints. Using synthetic surfaces, microbenchmarks, and experiments on distributed SQL and KV systems, we demonstrate that diagonal scaling reduces p95 latency by up to 40 percent, lowers cost-per-query by up to 37 percent, and reduces rebalancing by 2 to 5 times compared to horizontal-only and vertical-only autoscaling. Our results highlight the need for multi-dimensional scaling models and provide a foundation for next-generation autoscaling in cloud database systems.

**Link**: [arxiv](https://arxiv.org/abs/2511.21612v1),  [pdf](https://arxiv.org/pdf/2511.21612v1)

**Tags**: cs.DC 



### Modeling the Effect of Data Redundancy on Speedup in MLFMA Near-Field Computation
**Authors**: Morteza Sadeghi

**Updated**: 2025-11-26T16:01:32Z

**Summary**: The near-field (P2P) operator in the Multilevel Fast Multipole Algorithm (MLFMA) is a performance bottleneck on GPUs due to poor memory locality. This work introduces data redundancy to improve spatial locality by reducing memory access dispersion. For validation of results, we propose an analytical model based on a Locality metric that combines data volume and access dispersion to predict speedup trends without hardware-specific profiling. The approach is validated on two MLFMA-based applications: an electromagnetic solver (DBIM-MLFMA) with regular structure, and a stellar dynamics code (PhotoNs-2.0) with irregular particle distribution. Results show up to 7X kernel speedup due to improved cache behavior. However, increased data volume raises overheads in data restructuring, limiting end-to-end application speedup to 1.04X. While the model cannot precisely predict absolute speedups, it reliably captures performance trends across different problem sizes and densities. The technique is injectable into existing implementations with minimal code changes. This work demonstrates that data redundancy can enhance GPU performance for P2P operator, provided locality gains outweigh data movement costs.

**Link**: [arxiv](https://arxiv.org/abs/2511.21535v1),  [pdf](https://arxiv.org/pdf/2511.21535v1)

**Tags**: cs.DC cs.PF 



### Subjective Depth and Timescale Transformers: Learning Where and When to Compute
**Authors**: Frederico Wieser, Martin Benfeghoul, Haitham Bou Ammar, Jun Wang, Zafeirios Fountas

**Updated**: 2025-11-26T14:00:18Z

**Summary**: The rigid, uniform allocation of computation in standard Transformer (TF) architectures can limit their efficiency and scalability, particularly for large-scale models and long sequences. Addressing this, we introduce Subjective Depth Transformers (SDT) and Subjective Timescale Transformers (STT), two distinct architectures that leverage Bayesian surprise signals to dynamically route computation, learning where and when to compute within decoder-only TFs. SDT augments a decoder-only stack with alternating Decision and Dynamic layers: a Decision layer computes a full block 'posterior' and a lightweight 'prior,' while a Dynamic layer employs fixed-capacity Top-K routing based on Bayesian surprise (Expected and Unexpected Change), maintaining a static compute graph. STT extends this conditional computation to the temporal domain: a transition network predicts residual updates, forming a temporal 'change hypothesis' that informs a router to dynamically execute or bypass TF blocks for each token, managing KV-cache contributions. Both architectures exhibit the predicted shift from novelty to prediction driven gating over training, suggesting alignment with surprise based principles. While operating at reduced capacity, they offer preliminary insights into the compute-accuracy trade-offs of conditional computation. The proposed architectures establish a flexible framework for efficiency, reducing self-attention computation by 75% and KV-cache requirements by 50% within each compute skipping layer, setting a pathway for more efficient models.

**Link**: [arxiv](https://arxiv.org/abs/2511.21408v1),  [pdf](https://arxiv.org/pdf/2511.21408v1)

**Tags**: cs.LG cs.AI cs.CL cs.IT 



### RIA: A Ranking-Infused Approach for Optimized listwise CTR Prediction
**Authors**: Guoxiao Zhang, Tan Qu, Ao Li, DongLin Ni, Qianlong Xie, Xingxing Wang

**Updated**: 2025-11-26T13:45:10Z

**Summary**: Reranking improves recommendation quality by modeling item interactions. However, existing methods often decouple ranking and reranking, leading to weak listwise evaluation models that suffer from combinatorial sparsity and limited representational power under strict latency constraints. In this paper, we propose RIA (Ranking-Infused Architecture), a unified, end-to-end framework that seamlessly integrates pointwise and listwise evaluation. RIA introduces four key components: (1) the User and Candidate DualTransformer (UCDT) for fine-grained user-item-context modeling; (2) the Context-aware User History and Target (CUHT) module for position-sensitive preference learning; (3) the Listwise Multi-HSTU (LMH) module to capture hierarchical item dependencies; and (4) the Embedding Cache (EC) module to bridge efficiency and effectiveness during inference. By sharing representations across ranking and reranking, RIA enables rich contextual knowledge transfer while maintaining low latency. Extensive experiments show that RIA outperforms state-of-the-art models on both public and industrial datasets, achieving significant gains in AUC and LogLoss. Deployed in Meituan advertising system, RIA yields a +1.69% improvement in Click-Through Rate (CTR) and a +4.54% increase in Cost Per Mille (CPM) in online A/B tests.

**Link**: [arxiv](https://arxiv.org/abs/2511.21394v1),  [pdf](https://arxiv.org/pdf/2511.21394v1)

**Tags**: cs.IR cs.AI 



### Toward Secure Content-Centric Approaches for 5G-Based IoT: Advances and Emerging Trends
**Authors**: Ghada Jaber, Mohamed Ali Zormati, Walid Cavelius, Louka Chapiro, Mohamed El Ahmadi

**Updated**: 2025-11-26T12:34:53Z

**Summary**: The convergence of the Internet of Things (IoT) and 5G technologies is transforming modern communication systems by enabling massive connectivity, low latency, and high-speed data transmission. In this evolving landscape, Content-Centric Networking (CCN) is emerging as a promising alternative to traditional Internet Protocol (IP)-based architectures. CCN offers advantages such as in-network caching, scalability, and efficient content dissemination, all of which are particularly well-suited to the constraints of the IoT. However, deploying content-centric approaches in 5G-based IoT environments introduces significant security challenges. Key concerns include content authentication, data integrity, privacy protection, and resilience against attacks such as spoofing and cache poisoning. Such issues are exacerbated by the distributed, mobile, and heterogeneous nature of IoT and 5G systems. In this survey, we review and classify existing security solutions for content-centric architectures in IoT-5G scenarios. We highlight current trends, identify limitations in existing approaches, and outline future research directions with a focus on lightweight and adaptive security mechanisms.

**Link**: [arxiv](https://arxiv.org/abs/2511.21336v1),  [pdf](https://arxiv.org/pdf/2511.21336v1)

**Tags**: cs.NI 



### DynamicAdaptiveClimb: Adaptive Cache Replacement with Dynamic Resizing
**Authors**: Daniel Berend, Shlomi Dolev, Sweta Kumari, Dhruv Mishra, Marina Kogan-Sadetsky, Archit Somani

**Updated**: 2025-11-26T10:02:24Z

**Summary**: Efficient cache management is critical for optimizing the system performance, and numerous caching mechanisms have been proposed, each exploring various insertion and eviction strategies. In this paper, we present AdaptiveClimb and its extension, DynamicAdaptiveClimb, two novel cache replacement policies that leverage lightweight, cache adaptation to outperform traditional approaches. Unlike classic Least Recently Used (LRU) and Incremental Rank Progress (CLIMB) policies, AdaptiveClimb dynamically adjusts the promotion distance (jump) of the cached objects based on recent hit and miss patterns, requiring only a single tunable parameter and no per-item statistics. This enables rapid adaptation to changing access distributions while maintaining low overhead. Building on this foundation, DynamicAdaptiveClimb further enhances adaptability by automatically tuning the cache size in response to workload demands. Our comprehensive evaluation across a diverse set of real-world traces, including 1067 traces from 6 different datasets, demonstrates that DynamicAdaptiveClimb consistently achieves substantial speedups and higher hit ratios compared to other state-of-the-art algorithms. In particular, our approach achieves up to a 29% improvement in hit ratio and a substantial reduction in miss penalties compared to the FIFO baseline. Furthermore, it outperforms the next-best contenders, AdaptiveClimb and SIEVE [43], by approximately 10% to 15%, especially in environments characterized by fluctuating working set sizes. These results highlight the effectiveness of our approach in delivering efficient performance, making it well-suited for modern, dynamic caching environments.

**Link**: [arxiv](https://arxiv.org/abs/2511.21235v1),  [pdf](https://arxiv.org/pdf/2511.21235v1)

**Tags**: cs.OS eess.SY 



### Generative Early Stage Ranking
**Authors**: Juhee Hong, Meng Liu, Shengzhi Wang, Xiaoheng Mao, Huihui Cheng, Leon Gao, Christopher Leung, Jin Zhou, Chandra Mouli Sekar, Zhao Zhu, Ruochen Liu, Tuan Trieu, Dawei Sun, Jeet Kanjani, Rui Li, Jing Qian, Xuan Cao, Minjie Fan, Mingze Gao

**Updated**: 2025-11-26T06:29:18Z

**Summary**: Large-scale recommendations commonly adopt a multi-stage cascading ranking system paradigm to balance effectiveness and efficiency. Early Stage Ranking (ESR) systems utilize the "user-item decoupling" approach, where independently learned user and item representations are only combined at the final layer. While efficient, this design is limited in effectiveness, as it struggles to capture fine-grained user-item affinities and cross-signals. To address these, we propose the Generative Early Stage Ranking (GESR) paradigm, introducing the Mixture of Attention (MoA) module which leverages diverse attention mechanisms to bridge the effectiveness gap: the Hard Matching Attention (HMA) module encodes explicit cross-signals by computing raw match counts between user and item features; the Target-Aware Self Attention module generates target-aware user representations conditioned on the item, enabling more personalized learning; and the Cross Attention modules facilitate early and more enriched interactions between user-item features. MoA's specialized attention encodings are further refined in the final layer through a Multi-Logit Parameterized Gating (MLPG) module, which integrates the newly learned embeddings via gating and produces secondary logits that are fused with the primary logit. To address the efficiency and latency challenges, we have introduced a comprehensive suite of optimization techniques. These span from custom kernels that maximize the capabilities of the latest hardware to efficient serving solutions powered by caching mechanisms. The proposed GESR paradigm has shown substantial improvements in topline metrics, engagement, and consumption tasks, as validated by both offline and online experiments. To the best of our knowledge, this marks the first successful deployment of full target-aware attention sequence modeling within an ESR stage at such a scale.

**Link**: [arxiv](https://arxiv.org/abs/2511.21095v1),  [pdf](https://arxiv.org/pdf/2511.21095v1)

**Tags**: cs.LG 



### A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency
**Authors**: Sihyeong Park, Sungryeol Jeon, Chaelyn Lee, Seokhun Jeon, Byung-Soo Kim, Jemin Lee

**Updated**: 2025-11-26T05:49:31Z

**Summary**: Large language models (LLMs) are widely applied in chatbots, code generators, and search engines. Workload such as chain-of-throught, complex reasoning, agent services significantly increase the inference cost by invoke the model repeatedly. Optimization methods such as parallelism, compression, and caching have been adopted to reduce costs, but the diverse service requirements make it hard to select the right method. Recently, specialized LLM inference engines have emerged as a key component for integrating the optimization methods into service-oriented infrastructures. However, a systematic study on inference engines is still lacking.This paper provides a comprehensive evaluation of 25 open-source and commercial inference engines. We examine each inference engine in terms of ease-of-use, ease-of-deployment, general-purpose support, scalability, and suitability for throughput- and latency-aware computation. Furthermore, we explore the design goals of each inference engine by investigating the optimization techniques it supports. In addition, we assess the ecosystem maturity of open source inference engines and handle the performance and cost policy of commercial solutions.We outline future research directions that include support for complex LLM-based services, support of various hardware, and enhanced security, offering practical guidance to researchers and developers in selecting and designing optimized LLM inference engines. We also provide a public repository to continually track developments in this fast-evolving field: \href{https://github.com/sihyeong/Awesome-LLM-Inference-Engine}{https://github.com/sihyeong/Awesome-LLM-Inference-Engine}.

**Link**: [arxiv](https://arxiv.org/abs/2505.01658v3),  [pdf](https://arxiv.org/pdf/2505.01658v3)

**Tags**: cs.CL 



### No Request Left Behind: Tackling Heterogeneity in Long-Context LLM Inference with Medha
**Authors**: Amey Agrawal, Haoran Qiu, Junda Chen, Íñigo Goiri, Chaojie Zhang, Rayyan Shahid, Ramachandran Ramjee, Alexey Tumanov, Esha Choukse

**Updated**: 2025-11-26T01:43:40Z

**Summary**: Deploying million-token Large Language Models (LLMs) is challenging because production workloads are highly heterogeneous, mixing short queries and long documents. This heterogeneity, combined with the quadratic complexity of attention, creates severe convoy effects where long-running requests stall short, interactive ones, degrading system responsiveness. We present Medha, a serving system that eliminates these convoys by introducing fine-grained, preemptive scheduling to LLM inference.   Medha makes preemption practical with a co-designed set of mechanisms -- including Adaptive Chunking and Stream Pipeline Parallel that overcome the perceived inefficiencies and scaling challenges of chunking. Additionally, we present a new parallelism strategy KV-Cache Parallelism to reduce the decode latency and afford interactivity despite very long context. These mechanisms are orchestrated by a Length-Aware Relative Slack (LARS) scheduler, a deadline and heterogeneity-aware scheduling policy that prevents both the convoy effect and the starvation that plagues simpler policies. Under a heterogeneous workload, Medha improves throughput by 5.7x while reducing median and 99th percentile latency by 30x and 174x, respectively, compared to state-of-the-art non-preemptive systems.

**Link**: [arxiv](https://arxiv.org/abs/2409.17264v5),  [pdf](https://arxiv.org/pdf/2409.17264v5)

**Tags**: cs.LG cs.DC 



### Biembeddings of Archdeacon type: their full automorphism group and their number
**Authors**: Simone Costa

**Updated**: 2025-11-25T21:02:20Z

**Summary**: Archdeacon, in his seminal paper $[1]$, defined the concept of Heffter array in order to provide explicit constructions of $\mathbb{Z}_{v}$-regular biembeddings of complete graphs $K_v$ into orientable surfaces.   In this paper, we first introduce the quasi-Heffter arrays as a generalization of the concept of Heffer array and we show that, in this context, we can define a $2$-colorable embedding of Archdeacon type of the complete multipartite graph $K_{\frac{v}{t}\times t}$ into an orientable surface. Then, our main goal is to study the full automorphism groups of these embeddings: here we are able to prove, using a probabilistic approach, that, almost always, this group is exactly $\mathbb{Z}_{v}$.   As an application of this result, given a positive integer $t\not\equiv 0\pmod{4}$, we prove that there are, for infinitely many pairs of $v$ and $k$, at least $(1-o(1)) \frac{(\frac{v-t}{2})!}{φ(v)} $ non-isomorphic biembeddings of $K_{\frac{v}{t}\times t}$ whose face lengths are multiples of $k$. Here $φ(\cdot)$ denotes the Euler's totient function. Moreover, in case $t=1$ and $v$ is a prime, almost all these embeddings define faces that are all of the same length $kv$, i.e. we have a more than exponential number of non-isomorphic $kv$-gonal biembeddings of $K_{v}$.

**Link**: [arxiv](https://arxiv.org/abs/2205.02066v3),  [pdf](https://arxiv.org/pdf/2205.02066v3)

**Tags**: math.CO 



### Length-MAX Tokenizer for Language Models
**Authors**: Dong Dong, Weijie Su

**Updated**: 2025-11-25T20:56:56Z

**Summary**: We introduce a new tokenizer for language models that minimizes the average tokens per character, thereby reducing the number of tokens needed to represent text during training and to generate text during inference. Our method, which we refer to as the Length-MAX tokenizer, obtains its vocabulary by casting a length-weighted objective maximization as a graph partitioning problem and developing a greedy approximation algorithm. On FineWeb and diverse domains, it yields 14--18\% fewer tokens than Byte Pair Encoding (BPE) across vocabulary sizes from 10K to 50K, and the reduction is 13.0\% when the size is 64K. Training GPT-2 models at 124M, 355M, and 1.3B parameters from scratch with five runs each shows 18.5\%, 17.2\%, and 18.5\% fewer steps, respectively, to reach a fixed validation loss, and 13.7\%, 12.7\%, and 13.7\% lower inference latency, together with a 16\% throughput gain at 124M, while consistently improving on downstream tasks including reducing LAMBADA perplexity by 11.7\% and enhancing HellaSwag accuracy by 4.3\%. Moreover, the Length-MAX tokenizer achieves 99.62\% vocabulary coverage and the out-of-vocabulary rate remains low at 0.12\% on test sets. These results demonstrate that optimizing for average token length, rather than frequency alone, offers an effective approach to more efficient language modeling without sacrificing -- and often improving -- downstream performance. The tokenizer is compatible with production systems and reduces embedding and KV-cache memory by 18\% at inference.

**Link**: [arxiv](https://arxiv.org/abs/2511.20849v1),  [pdf](https://arxiv.org/pdf/2511.20849v1)

**Tags**: cs.CL cs.AI cs.LG 



### Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout
**Authors**: Hidir Yesiltepe, Tuna Han Salih Meral, Adil Kaan Akan, Kaan Oktay, Pinar Yanardag

**Updated**: 2025-11-25T18:59:46Z

**Summary**: Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce $\infty$-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish $\infty$-RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that $\infty$-RoPE consistently surpasses previous autoregressive models in overall VBench scores.

**Link**: [arxiv](https://arxiv.org/abs/2511.20649v1),  [pdf](https://arxiv.org/pdf/2511.20649v1)

**Tags**: cs.CV 



### Block Cascading: Training Free Acceleration of Block-Causal Video Models
**Authors**: Hmrishav Bandyopadhyay, Nikhil Pinnaparaju, Rahim Entezari, Jim Scott, Yi-Zhe Song, Varun Jampani

**Updated**: 2025-11-25T15:52:58Z

**Summary**: Block-causal video generation faces a stark speed-quality trade-off: small 1.3B models manage only 16 FPS while large 14B models crawl at 4.5 FPS, forcing users to choose between responsiveness and quality. Block Cascading significantly mitigates this trade-off through training-free parallelization. Our key insight: future video blocks do not need fully denoised current blocks to begin generation. By starting block generation with partially denoised context from predecessors, we transform sequential pipelines into parallel cascades where multiple blocks denoise simultaneously. With 5 GPUs exploiting temporal parallelism, we achieve ~2x acceleration across all model scales: 1.3B models accelerate from 16 to 30 FPS, 14B models from 4.5 to 12.5 FPS. Beyond inference speed, Block Cascading eliminates overhead from KV-recaching (of ~200ms) during context switches for interactive generation. Extensive evaluations validated against multiple block-causal pipelines demonstrate no significant loss in generation quality when switching from block-causal to Block Cascading pipelines for inference. Project Page: https://hmrishavbandy.github.io/block_cascading_page/

**Link**: [arxiv](https://arxiv.org/abs/2511.20426v1),  [pdf](https://arxiv.org/pdf/2511.20426v1)

**Tags**: cs.CV cs.AI 



### Pickle Prefetcher: Programmable and Scalable Last-Level Cache Prefetcher
**Authors**: Hoa Nguyen, Pongstorn Maidee, Jason Lowe-Power, Alireza Kaviani

**Updated**: 2025-11-25T06:36:32Z

**Summary**: Modern high-performance architectures employ large last-level caches (LLCs). While large LLCs can reduce average memory access latency for workloads with a high degree of locality, they can also increase latency for workloads with irregular memory access patterns. Prefetchers are widely used to reduce memory latency by prefetching data into the cache hierarchy before it is accessed by the core. However, existing prediction-based prefetchers often struggle with irregular memory access patterns, which are especially prevalent in modern applications. This paper introduces the Pickle Prefetcher, a programmable and scalable LLC prefetcher designed to handle independent irregular memory access patterns effectively. Instead of relying on static heuristics or complex prediction algorithms, Pickle Prefetcher allows software to define its own prefetching strategies using a simple programming interface without expanding the instruction set architecture (ISA). By trading the logic complexity of hardware prediction for software programmability, Pickle Prefetcher can adapt to a wide range of access patterns without requiring extensive hardware resources for prediction. This allows the prefetcher to dedicate its resources to scheduling and issuing timely prefetch requests. Graph applications are an example where the memory access pattern is irregular but easily predictable by software. Through extensive evaluations of the Pickle Prefetcher on gem5 full-system simulations, we demonstrate tha Pickle Prefetcher significantly outperforms traditional prefetching techniques. Our results show that Pickle Prefetcher achieves speedups of up to 1.74x on the GAPBS breadth-first search (BFS) implementation over a baseline system. When combined with private cache prefetchers, Pickle Prefetcher provides up to a 1.40x speedup over systems using only private cache prefetchers.

**Link**: [arxiv](https://arxiv.org/abs/2511.19973v1),  [pdf](https://arxiv.org/pdf/2511.19973v1)

**Tags**: cs.AR 



### ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for Scalable Recommendation System
**Authors**: Youngsuk Kim, Junghwan Lim, Hyuk-Jae Lee, Chae Eun Rhee

**Updated**: 2025-11-25T05:43:27Z

**Summary**: Although deep learning-based personalized recommendation systems provide qualified recommendations, they strain data center resources. The main bottleneck is the embedding layer, which is highly memory-intensive due to its sparse, irregular access patterns to embeddings. Recent near-memory processing (NMP) and processing-in-memory (PIM) architectures have addressed these issues by exploiting parallelism within memory. However, as model sizes increase year by year and can exceed server capacity, inference on single-node servers becomes challenging, necessitating the integration of model compression. Various algorithms have been proposed for model size reduction, but they come at the cost of increased memory access and CPU-PIM communication. We present ProactivePIM, a PIM system tailored for weight-sharing algorithms, a family of compression methods that decompose an embedding table into compact subtables, such as QR-trick and TT-Rec. Our analysis shows that embedding layer execution with weight-sharing algorithms increases memory access and incurs CPU-PIM communication. We also find that these algorithms exhibit unique data locality characteristics, which we name intra-GnR locality. ProactivePIM accelerates weight-sharing algorithms by utilizing a heterogeneous HBM-DIMM memory architecture with integration of a two-level PIM system of base-die PIM (bd-PIM) and bank-group PIM (bg-PIM) inside the HBM. To gain further speedup, ProactivePIM prefetches embeddings with high intra-GnR locality into an SRAM cache within bg-PIM and eliminates the CPU-PIM communication through duplication of target subtables across bank groups. With additional optimization techniques, our design effectively accelerates weight-sharing algorithms, achieving 2.22x and 2.15x speedup in QR-trick and TT-Rec, respectively, compared to the baseline architecture.

**Link**: [arxiv](https://arxiv.org/abs/2402.04032v6),  [pdf](https://arxiv.org/pdf/2402.04032v6)

**Tags**: cs.AR cs.AI 



### Understanding and Optimizing Multi-Stage AI Inference Pipelines
**Authors**: Abhimanyu Rajeshkumar Bambhaniya, Hanjiang Wu, Suvinay Subramanian, Sudarshan Srinivasan, Souvik Kundu, Amir Yazdanbakhsh, Midhilesh Elavazhagan, Madhu Kumar, Tushar Krishna

**Updated**: 2025-11-25T04:36:10Z

**Summary**: The rapid evolution of Large Language Models (LLMs) has driven the need for increasingly sophisticated inference pipelines and hardware platforms. Modern LLM serving extends beyond traditional prefill-decode workflows, incorporating multi-stage processes such as Retrieval Augmented Generation (RAG), key-value (KV) cache retrieval, dynamic model routing, and multi step reasoning. These stages exhibit diverse computational demands, requiring distributed systems that integrate GPUs, ASICs, CPUs, and memory-centric architectures. However, existing simulators lack the fidelity to model these heterogeneous, multi-engine workflows, limiting their ability to inform architectural decisions.   To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM inference Execution Simulator. HERMES models diverse request stages; including RAG, KV retrieval, reasoning, prefill, and decode across complex hardware hierarchies. HERMES supports heterogeneous clients executing multiple models concurrently unlike prior frameworks while incorporating advanced batching strategies and multi-level memory hierarchies. By integrating real hardware traces with analytical modeling, HERMES captures critical trade-offs such as memory bandwidth contention, inter-cluster communication latency, and batching efficiency in hybrid CPU-accelerator deployments. Through case studies, we explore the impact of reasoning stages on end-to-end latency, optimal batching strategies for hybrid pipelines, and the architectural implications of remote KV cache retrieval. HERMES empowers system designers to navigate the evolving landscape of LLM inference, providing actionable insights into optimizing hardware-software co-design for next-generation AI workloads.

**Link**: [arxiv](https://arxiv.org/abs/2504.09775v4),  [pdf](https://arxiv.org/pdf/2504.09775v4)

**Tags**: cs.AR cs.AI cs.DC cs.LG 



### On 10x Better Scalability: KV Stores Scale Up KV Cache
**Authors**: Weiping Yu, Ye Jiarui, He Mengke, Junfeng Liu, Siqiang Luo

**Updated**: 2025-11-25T02:03:54Z

**Summary**: Large language models (LLMs) rely on Key-Value (KV) cache to reduce time-to-first-token (TTFT) latency, but existing disk-based KV cache systems using file-per-object layouts suffer from severe scalability bottlenecks due to file system metadata overhead, I/O inefficiency, and poor spatial locality. This paper presents SGLANG-LSM, a database-inspired system that leverages Log-Structured Merge-tree (LSM-tree) architectures for scalable KV cache management. SGLANG-LSM implements a layered system design with three coordinated components: (1) a prefix-preserving storage engine that maintains token sequence locality while efficiently storing large KV cache tensors through key-value separation, (2) an adaptive controller that dynamically optimizes LSM-tree configurations based on shifting workload characteristics, and (3) runtime services including batch operations and automatic resource management for production deployment. Evaluation on large-scale dynamic workloads demonstrates that SGLANG-LSM significantly improves cache hits by up to 143% and reduces TTFT by up to 24% compared to state-of-the-art systems, representing the first systematic application of database storage architectures to large-scale LLM cache management.

**Link**: [arxiv](https://arxiv.org/abs/2511.16138v2),  [pdf](https://arxiv.org/pdf/2511.16138v2)

**Tags**: cs.DB 



### Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation
**Authors**: Inferix Team, Tianyu Feng, Yizeng Han, Jiahao He, Yuanyu He, Xi Lin, Teng Liu, Hanfeng Lu, Jiasheng Tang, Wei Wang, Zhiyuan Wang, Jichao Wu, Mingyang Yang, Yinghao Yu, Zeyu Zhang, Bohan Zhuang

**Updated**: 2025-11-25T01:45:04Z

**Summary**: World models serve as core simulators for fields such as agentic AI, embodied AI, and gaming, capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception, understanding, and reasoning, paving the way for a new paradigm that moves beyond current LLM-centric vision foundation models. A key breakthrough empowering them is the semi-autoregressive (block-diffusion) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in block-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation.   Therefore, Inferix is specifically designed as a next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang) and from classic video diffusion models (such as xDiTs). Inferix further enhances its offering with interactive video streaming and profiling, enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench, a new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration.

**Link**: [arxiv](https://arxiv.org/abs/2511.20714v1),  [pdf](https://arxiv.org/pdf/2511.20714v1)

**Tags**: cs.CV cs.AI 



### Computer-aided Characterization of Fundamental Limits of Coded Caching with Linear Coding
**Authors**: Niccolò Brembilla, Yinbin Ma, Pietro Belotti, Federico Malucelli, Daniela Tuninetti

**Updated**: 2025-11-24T19:14:34Z

**Summary**: Inspired by prior work by Tian and by Cao and Xu, this paper presents an efficient computer-aided framework to characterize the fundamental limits of coded caching systems under the constraint of linear coding. The proposed framework considers non-Shannon-type inequalities which are valid for representable polymatroids (and hence for linear codes), and leverages symmetric structure and problem-specific constraints of coded caching to reduce the complexity of the linear program. The derived converse bounds are tighter compared to previous known analytic methods, and prove the optimality of some achievable memory-load tradeoff points under the constraint of linear coding placement and delivery. These results seem to indicate that small, structured demand subsets combined with minimal common information constructions may be sufficient to characterize optimal tradeoffs under linear coding.

**Link**: [arxiv](https://arxiv.org/abs/2511.19639v1),  [pdf](https://arxiv.org/pdf/2511.19639v1)

**Tags**: cs.IT 



### CDLM: Consistency Diffusion Language Models For Faster Sampling
**Authors**: Minseo Kim, Chenfeng Xu, Coleman Hooper, Harman Singh, Ben Athiwaratkun, Ce Zhang, Kurt Keutzer, Amir Gholami

**Updated**: 2025-11-24T16:21:25Z

**Summary**: Diffusion Language Models (DLMs) offer a promising parallel generation paradigm but suffer from slow inference due to numerous refinement steps and the inability to use standard KV caching. We introduce CDLM (Consistency Diffusion Language Models), a training-based acceleration method that simultaneously tackles both bottlenecks. CDLM integrates consistency modeling to drastically reduce the number of required sampling steps by enabling multi-token finalization. Furthermore, we enforce a block-wise causal attention mask during fine-tuning, making the model fully compatible with KV caching. Experiments show CDLM achieves 3.6x-14.5x lower latency while maintaining competitive accuracy on math and coding tasks. The full training and evaluation code is available at https://github.com/SqueezeAILab/CDLM.

**Link**: [arxiv](https://arxiv.org/abs/2511.19269v1),  [pdf](https://arxiv.org/pdf/2511.19269v1)

**Tags**: cs.LG cs.CL 



### Orchestrating Dual-Boundaries: An Arithmetic Intensity Inspired Acceleration Framework for Diffusion Language Models
**Authors**: Linye Wei, Wenjue Chen, Pingzhi Tang, Xiaotian Guo, Le Ye, Runsheng Wang, Meng Li

**Updated**: 2025-11-24T13:36:54Z

**Summary**: Diffusion-based large language models (dLLMs) have recently gained significant attention for their exceptional performance and inherent potential for parallel decoding. Existing frameworks further enhance its inference efficiency by enabling KV caching. However, its bidirectional attention mechanism necessitates periodic cache refreshes that interleave prefill and decoding phases, both contributing substantial inference cost and constraining achievable speedup. Inspired by the heterogeneous arithmetic intensity of the prefill and decoding phases, we propose ODB-dLLM, a framework that orchestrates dual-boundaries to accelerate dLLM inference. In the prefill phase, we find that the predefined fixed response length introduces heavy yet redundant computational overhead, which affects efficiency. To alleviate this, ODB-dLLM incorporates an adaptive length prediction mechanism that progressively reduces prefill overhead and unnecessary computation. In the decoding phase, we analyze the computational characteristics of dLLMs and propose a dLLM-specific jump-share speculative decoding method to enhance efficiency by reducing the number of decoding iterations. Experimental results demonstrate that ODB-dLLM achieves 46-162x and 2.63-6.30x speedups over the baseline dLLM and Fast-dLLM, respectively, while simultaneously mitigating the accuracy degradation in existing acceleration frameworks.

**Link**: [arxiv](https://arxiv.org/abs/2511.21759v1),  [pdf](https://arxiv.org/pdf/2511.21759v1)

**Tags**: cs.CL cs.LG 



### SWAN: Sparse Winnowed Attention for Reduced Inference Memory via Decompression-Free KV-Cache Compression
**Authors**: Santhosh G S, Saurav Prakash, Balaraman Ravindran

**Updated**: 2025-11-24T09:41:24Z

**Summary**: Large Language Models (LLMs) face a significant bottleneck during autoregressive inference due to the massive memory footprint of the Key-Value (KV) cache. Existing compression techniques like token eviction, quantization, or other low-rank methods often risk information loss, have fixed limits, or introduce significant computational overhead from explicit decompression steps. In this work, we introduce SWAN, a novel, fine-tuning-free framework that eliminates this overhead. Our method uses an offline orthogonal matrix to rotate and prune the KV-cache, which is then used directly in the attention computation without any reconstruction. Our extensive experiments demonstrate that SWAN, augmented with a small dense buffer, offers a robust trade-off, maintaining performance close to the uncompressed baseline even at aggressive 50-60% memory savings per-token on KV-cache. A key advantage is its runtime-tunable compression level, allowing operators to dynamically adjust the memory footprint, a flexibility absent in methods requiring fixed offline configurations. This combination of a decompression-free design, high performance under compression, and adaptability makes SWAN a practical and efficient solution for serving LLMs with long contexts.

**Link**: [arxiv](https://arxiv.org/abs/2511.18936v1),  [pdf](https://arxiv.org/pdf/2511.18936v1)

**Tags**: cs.LG cs.AI cs.CL 



### SlimCaching: Edge Caching of Mixture-of-Experts for Distributed Inference
**Authors**: Qian Chen, Xianhao Chen, Kaibin Huang

**Updated**: 2025-11-24T09:35:35Z

**Summary**: Mixture-of-Experts (MoE) models improve the scalability of large language models (LLMs) by activating only a small subset of relevant experts per input. However, the sheer number of expert networks in an MoE model introduces a significant storage burden for an edge device. To address this challenge, we consider a scenario where experts are dispersed across an edge network for distributed inference. Based on the popular Top-$K$ expert selection strategy, we formulate a latency minimization problem by optimizing expert caching on edge servers under storage constraints. When $K=1$, the problem reduces to a monotone submodular maximization problem with knapsack constraints, for which we design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee. For the general case where $K \geq 1$, expert co-activation within the same MoE layer introduces non-submodularity, which renders greedy methods ineffective. To tackle this issue, we propose a successive greedy decomposition method to decompose the original problem into a series of subproblems, with each being solved by a dynamic programming approach. Furthermore, we design an accelerated algorithm based on the max-convolution technique to obtain the approximate solution with a provable guarantee in polynomial time. Simulation results on various MoE models demonstrate that our method significantly reduces inference latency compared to existing baselines.

**Link**: [arxiv](https://arxiv.org/abs/2507.06567v2),  [pdf](https://arxiv.org/pdf/2507.06567v2)

**Tags**: cs.LG cs.DC cs.NI 



### MagicWorld: Interactive Geometry-driven Video World Exploration
**Authors**: Guangyuan Li, Siming Zheng, Shuolin Xu, Jinwei Chen, Bo Li, Xiaobin Hu, Lei Zhao, Peng-Tao Jiang

**Updated**: 2025-11-24T08:41:28Z

**Summary**: Recent interactive video world model methods generate scene evolution conditioned on user instructions. Although they achieve impressive results, two key limitations remain. First, they fail to fully exploit the correspondence between instruction-driven scene motion and the underlying 3D geometry, which results in structural instability under viewpoint changes. Second, they easily forget historical information during multi-step interaction, resulting in error accumulation and progressive drift in scene semantics and structure. To address these issues, we propose MagicWorld, an interactive video world model that integrates 3D geometric priors and historical retrieval. MagicWorld starts from a single scene image, employs user actions to drive dynamic scene evolution, and autoregressively synthesizes continuous scenes. We introduce the Action-Guided 3D Geometry Module (AG3D), which constructs a point cloud from the first frame of each interaction and the corresponding action, providing explicit geometric constraints for viewpoint transitions and thereby improving structural consistency. We further propose History Cache Retrieval (HCR) mechanism, which retrieves relevant historical frames during generation and injects them as conditioning signals, helping the model utilize past scene information and mitigate error accumulation. Experimental results demonstrate that MagicWorld achieves notable improvements in scene stability and continuity across interaction iterations.

**Link**: [arxiv](https://arxiv.org/abs/2511.18886v1),  [pdf](https://arxiv.org/pdf/2511.18886v1)

**Tags**: cs.CV 



### Mitigating Long-Tail Bias in HOI Detection via Adaptive Diversity Cache
**Authors**: Yuqiu Jiang, Xiaozhen Qiao, Tianyu Mei, Haojian Huang, Yifan Chen, Ye Zheng, Zhe Sun

**Updated**: 2025-11-24T06:30:08Z

**Summary**: Human-Object Interaction (HOI) detection is a fundamental task in computer vision, empowering machines to comprehend human-object relationships in diverse real-world scenarios. Recent advances in VLMs have significantly improved HOI detection by leveraging rich cross-modal representations. However, most existing VLM-based approaches rely heavily on additional training or prompt tuning, resulting in substantial computational overhead and limited scalability, particularly in long-tailed scenarios where rare interactions are severely underrepresented. In this paper, we propose the Adaptive Diversity Cache (ADC) module, a novel training-free and plug-and-play mechanism designed to mitigate long-tail bias in HOI detection. ADC constructs class-specific caches that accumulate high-confidence and diverse feature representations during inference. The method incorporates frequency-aware cache adaptation that favors rare categories and is designed to enable robust prediction calibration without requiring additional training or fine-tuning. Extensive experiments on HICO-DET and V-COCO datasets show that ADC consistently improves existing HOI detectors, achieving up to +8.57\% mAP gain on rare categories and +4.39\% on the full dataset, demonstrating its effectiveness in mitigating long-tail bias while preserving overall performance.

**Link**: [arxiv](https://arxiv.org/abs/2511.18811v1),  [pdf](https://arxiv.org/pdf/2511.18811v1)

**Tags**: cs.CV cs.AI 



### SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning
**Authors**: Lingkun Long, Rubing Yang, Yushi Huang, Desheng Hui, Ao Zhou, Jianlei Yang

**Updated**: 2025-11-24T06:26:25Z

**Summary**: Long-context inference for Large Language Models (LLMs) is heavily limited by high computational demands. While several existing methods optimize attention computation, they still process the full set of hidden states at each layer, limiting overall efficiency. In this work, we propose SlimInfer, an innovative framework that aims to accelerate inference by directly pruning less critical prompt tokens during the forward pass. Our key insight is an information diffusion phenomenon: As information from critical tokens propagates through layers, it becomes distributed across the entire sequence. This diffusion process suggests that LLMs can maintain their semantic integrity when excessive tokens, even including these critical ones, are pruned in hidden states. Motivated by this, SlimInfer introduces a dynamic fine-grained pruning mechanism that accurately removes redundant tokens of hidden state at intermediate layers. This layer-wise pruning naturally enables an asynchronous KV cache manager that prefetches required token blocks without complex predictors, reducing both memory usage and I/O costs. Extensive experiments show that SlimInfer can achieve up to $\mathbf{2.53\times}$ time-to-first-token (TTFT) speedup and $\mathbf{1.88\times}$ end-to-end latency reduction for LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on LongBench. Our code is available at https://github.com/Longxmas/SlimInfer.

**Link**: [arxiv](https://arxiv.org/abs/2508.06447v2),  [pdf](https://arxiv.org/pdf/2508.06447v2)

**Tags**: cs.CL 



### GVD-TG: Topological Graph based on Fast Hierarchical GVD Sampling for Robot Exploration
**Authors**: Yanbin Li, Canran Xiao, Shenghai Yuan, Peilai Yu, Ziruo Li, Zhiguo Zhang, Wenzheng Chi, Wei Zhang

**Updated**: 2025-11-24T03:02:39Z

**Summary**: Topological maps are more suitable than metric maps for robotic exploration tasks. However, real-time updating of accurate and detail-rich environmental topological maps remains a challenge. This paper presents a topological map updating method based on the Generalized Voronoi Diagram (GVD). First, the newly observed areas are denoised to avoid low-efficiency GVD nodes misleading the topological structure. Subsequently, a multi-granularity hierarchical GVD generation method is designed to control the sampling granularity at both global and local levels. This not only ensures the accuracy of the topological structure but also enhances the ability to capture detail features, reduces the probability of path backtracking, and ensures no overlap between GVDs through the maintenance of a coverage map, thereby improving GVD utilization efficiency. Second, a node clustering method with connectivity constraints and a connectivity method based on a switching mechanism are designed to avoid the generation of unreachable nodes and erroneous nodes caused by obstacle attraction. A special cache structure is used to store all connectivity information, thereby improving exploration efficiency. Finally, to address the issue of frontiers misjudgment caused by obstacles within the scope of GVD units, a frontiers extraction method based on morphological dilation is designed to effectively ensure the reachability of frontiers. On this basis, a lightweight cost function is used to assess and switch to the next viewpoint in real time. This allows the robot to quickly adjust its strategy when signs of path backtracking appear, thereby escaping the predicament and increasing exploration flexibility. And the performance of system for exploration task is verified through comparative tests with SOTA methods.

**Link**: [arxiv](https://arxiv.org/abs/2511.18708v1),  [pdf](https://arxiv.org/pdf/2511.18708v1)

**Tags**: cs.RO 



### Development of a projectile charge state analyzer and 10 kV bipolar power supply for MeV energy ion - atom/molecule collision experiments
**Authors**: Sandeep Bajrangi Bari, Sahan Raghava Sykam, Ranojit Das, Rohit Tyagi, Aditya H. Kelkar

**Updated**: 2025-11-24T03:02:09Z

**Summary**: We have developed a post-collision projectile charge state analyzer (CSA) for detecting the charge state of the projectile ion following ion-atom/molecule collision. The design of the analyzer, based on electrostatic parallel plate deflector was simulated using SIMION ion optics package. We have also developed a 10 kV bipolar programmable power supply to bias the CSA electrodes. The CSA and the power supply, both, were tested in collision studies using MeV energy ion beam of proton and carbon ions at the 1.7 MV tandetron accelerator facility at IIT Kanpur.

**Link**: [arxiv](https://arxiv.org/abs/2511.18707v1),  [pdf](https://arxiv.org/pdf/2511.18707v1)

**Tags**: physics.atom-ph 



### Kitty: Accurate and Efficient 2-bit KV Cache Quantization with Dynamic Channel-wise Precision Boost
**Authors**: Haojun Xia, Xiaoxia Wu, Jisen Li, Robert Wu, Junxiong Wang, Jue Wang, Chenxi Li, Aman Singhal, Alay Dilipbhai Shah, Alpay Ariyak, Donglin Zhuang, Zhongzhu Zhou, Ben Athiwaratkun, Zhen Zheng, Shuaiwen Leon Song

**Updated**: 2025-11-23T22:54:48Z

**Summary**: The KV cache is a dominant memory bottleneck for LLM inference. While 4-bit KV quantization preserves accuracy, 2-bit often degrades it, especially on long-context reasoning. We close this gap via an algorithm-system co-design for mixed-precision KV caching: Kitty. On the algorithm side, extensive experiments show that Dynamic Channel-wise Precision Boost -- which ranks Key-cache channels by sensitivity and keeps only a small fraction at higher precision -- maintains near-zero loss in accuracy drop while approaching 2-bit memory. The main challenge is handling dynamic 4-bit channel boosts while keeping the page layout coalesced and the dequantization uniform, with no scattered reads or hard-coded masks. Kitty addresses these issues by decompose each mixed-precision Key page into two tensors with unified 2-bit precision. Based on this, Kitty provides a page-centric KV layout, Triton-compatible page dequantization kernels, and a lightweight runtime pipeline that preserves coalescing and avoids divergence. Across seven tasks and two model families (Qwen3, LLaMA3), Kitty cuts KV memory by nearly 8x with negligible accuracy loss, enabling up to 8x larger batches and 2.1x-4.1x higher throughput under the same memory budget. We release the full implementation of Kitty at https://github.com/Summer-Summer/Kitty.

**Link**: [arxiv](https://arxiv.org/abs/2511.18643v1),  [pdf](https://arxiv.org/pdf/2511.18643v1)

**Tags**: cs.LG cs.AI 



### KV-Efficient VLA: A Method to Speed up Vision Language Models with RNN-Gated Chunked KV Cache
**Authors**: Wanshun Xu, Long Zhuang, Lianlei Shan

**Updated**: 2025-11-23T17:07:12Z

**Summary**: Vision-Language-Action (VLA) models offer a unified framework for robotic perception and control, but their ability to scale to real-world, long-horizon tasks is limited by the high computational cost of attention and the large memory required for storing key-value (KV) pairs during inference, particularly when retaining historical image tokens as context. Recent methods have focused on scaling backbone architectures to improve generalization, with less emphasis on addressing inference inefficiencies essential for real-time use. In this work, we present KV-Efficient VLA, a model-agnostic memory compression approach designed to address these limitations by introducing a lightweight mechanism to selectively retain high-utility context. Our method partitions the KV cache into fixed-size chunks and employs a recurrent gating module to summarize and filter the historical context according to learned utility scores. This design aims to preserve recent fine-grained detail while aggressively pruning stale, low-relevance memory. Based on experiments, our approach can yield an average of 24.6% FLOPs savings, 1.34x inference speedup, and 1.87x reduction in KV memory. Our method integrates seamlessly into recent VLA stacks, enabling scalable inference without modifying downstream control logic.

**Link**: [arxiv](https://arxiv.org/abs/2509.21354v2),  [pdf](https://arxiv.org/pdf/2509.21354v2)

**Tags**: cs.CV cs.AI 



### CrossJEPA: Cross-Modal Joint-Embedding Predictive Architecture for Efficient 3D Representation Learning from 2D Images
**Authors**: Avishka Perera, Kumal Hewagamage, Saeedha Nazar, Kavishka Abeywardana, Hasitha Gallella, Ranga Rodrigo, Mohamed Afham

**Updated**: 2025-11-23T12:40:04Z

**Summary**: Image-to-point cross-modal learning has emerged to address the scarcity of large-scale 3D datasets in 3D representation learning. However, current methods that leverage 2D data often result in large, slow-to-train models, making them computationally expensive and difficult to deploy in resource-constrained environments. The architecture design of such models is therefore critical, determining their performance, memory footprint, and compute efficiency. The Joint-embedding Predictive Architecture (JEPA) has gained wide popularity in self-supervised learning for its simplicity and efficiency, but has been under-explored in cross-modal settings, partly due to the misconception that masking is intrinsic to JEPA. In this light, we propose CrossJEPA, a simple Cross-modal Joint Embedding Predictive Architecture that harnesses the knowledge of an image foundation model and trains a predictor to infer embeddings of specific rendered 2D views from corresponding 3D point clouds, thereby introducing a JEPA-style pretraining strategy beyond masking. By conditioning the predictor on cross-domain projection information, CrossJEPA purifies the supervision signal from semantics exclusive to the target domain. We further exploit the frozen teacher design with a one-time target embedding caching mechanism, yielding amortized efficiency. CrossJEPA achieves a new state-of-the-art in linear probing on the synthetic ModelNet40 (94.2%) and the real-world ScanObjectNN (88.3%) benchmarks, using only 14.1M pretraining parameters (8.5M in the point encoder), and about 6 pretraining hours on a standard single GPU. These results position CrossJEPA as a performant, memory-efficient, and fast-to-train framework for 3D representation learning via knowledge distillation. We analyze CrossJEPA intuitively, theoretically, and empirically, and extensively ablate our design choices. Code will be made available.

**Link**: [arxiv](https://arxiv.org/abs/2511.18424v1),  [pdf](https://arxiv.org/pdf/2511.18424v1)

**Tags**: cs.CV 



### MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models
**Authors**: Xiyang Wu, Zongxia Li, Jihui Jin, Guangyao Shi, Gouthaman KV, Vishnu Raj, Nilotpal Sinha, Jingxi Chen, Fan Du, Dinesh Manocha

**Updated**: 2025-11-23T09:43:44Z

**Summary**: Vision Language Models (VLMs) perform well on standard video tasks but struggle with physics-driven reasoning involving motion dynamics and spatial interactions. This limitation reduces their ability to interpret real or AI-generated content (AIGC) videos and to generate physically consistent content. We present an approach that addresses this gap by translating physical-world context cues into interpretable representations aligned with VLMs' perception, comprehension, and reasoning. We introduce MASS-Bench, a comprehensive benchmark consisting of 4,350 real-world and AIGC videos and 8,361 free-form video question-answering pairs focused on physics-related comprehension tasks, with detailed annotations including visual detections, sub-segment grounding, and full-sequence 3D motion tracking of entities. We further present MASS, a model-agnostic method that injects spatial-temporal signals into the VLM language space via depth-based 3D encoding and visual grounding, coupled with a motion tracker for object dynamics. To strengthen cross-modal alignment and reasoning, we apply reinforcement fine-tuning. Experiments and ablations show that our refined VLMs outperform comparable and larger baselines, as well as prior state-of-the-art models, by 8.7% and 6.0%, achieving performance comparable to close-source SoTA VLMs such as Gemini-2.5-Flash on physics reasoning and comprehension. These results validate the effectiveness of our approach.

**Link**: [arxiv](https://arxiv.org/abs/2511.18373v1),  [pdf](https://arxiv.org/pdf/2511.18373v1)

**Tags**: cs.CV 



### Breaking the Bottleneck with DiffuApriel: High-Throughput Diffusion LMs with Mamba Backbone
**Authors**: Vaibhav Singh, Oleksiy Ostapenko, Pierre-André Noël, Torsten Scholak

**Updated**: 2025-11-23T05:32:34Z

**Summary**: Diffusion-based language models have recently emerged as a promising alternative to autoregressive generation, yet their reliance on Transformer backbones limits inference efficiency due to quadratic attention and KV-cache overhead. In this work, we introduce DiffuApriel, a masked diffusion language model built on a bidirectional Mamba backbone that combines the diffusion objective with linear-time sequence modeling. DiffuApriel matches the performance of Transformer-based diffusion models while achieving up to 4.4x higher inference throughput for long sequences with a 1.3B model. We further propose DiffuApriel-H, a hybrid variant that interleaves attention and mamba layers, offering up to 2.6x throughput improvement with balanced global and local context modeling. Our results demonstrate that bidirectional state-space architectures serve as strong denoisers in masked diffusion LMs, providing a practical and scalable foundation for faster, memory-efficient text generation.

**Link**: [arxiv](https://arxiv.org/abs/2511.15927v2),  [pdf](https://arxiv.org/pdf/2511.15927v2)

**Tags**: cs.LG cs.AI 



### MIDAS: Adaptive Proxy Middleware for Mitigating Metadata Hotspots in HPC I/O at Scale
**Authors**: Sangam Ghimire, Nigam Niraula, Nirjal Bhurtel, Paribartan Timalsina, Bishal Neupane, James Bhattarai, Sudan Jha

**Updated**: 2025-11-22T17:05:38Z

**Summary**: Metadata hotspots remain one of the key obstacles to scalable Input/Output (I/O) in both High-Performance Computing (HPC) and cloud-scale storage environments. Situations such as job start-ups, checkpoint storms, or heavily skewed namespace access can trigger thousands of concurrent metadata requests against a small subset of servers. The result is long queues, inflated tail latencies, and reduced system throughput. Prior efforts including static namespace partitioning, backend-specific extensions, and kernel-level modifications address parts of the problem, but they often prove too rigid, intrusive to deploy, or unstable under shifting workloads. We present MIDAS, an adaptive middleware layer that operates transparently between clients and metadata servers, requiring no changes to kernels or storage backends. The design brings together three mechanisms: (i) a namespace-aware load balancer that enhances consistent hashing with power-of-d sampling informed by live telemetry, (ii) a cooperative caching layer that preserves backend semantics through leases, invalidations, or adaptive timeouts, and (iii) a self-stabilizing control loop that dynamically adjusts routing aggressiveness and cache lifetimes while avoiding oscillations under bursty workloads. Analysis of the model and controlled experiments show that MIDAS reduces average queue lengths by roughly 23% and mitigates worst-case hotspots by up to 80% when compared to round-robin scheduling. These findings highlight that a stability-aware, middleware-based strategy can provide backend-agnostic improvements to metadata management, enabling better scalability in bursty scenarios, more predictable tail latencies, and stronger overall system performance.

**Link**: [arxiv](https://arxiv.org/abs/2511.18124v1),  [pdf](https://arxiv.org/pdf/2511.18124v1)

**Tags**: cs.DC 



### SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation
**Authors**: Jiayi Pan, Jiaming Xu, Yongkang Zhou, Guohao Dai

**Updated**: 2025-11-22T06:28:24Z

**Summary**: Feature caching has recently emerged as a promising method for diffusion model acceleration. It effectively alleviates the inefficiency problem caused by high computational requirements by caching similar features in the inference process of the diffusion model. In this paper, we analyze existing feature caching methods from the perspective of information utilization, and point out that relying solely on historical information will lead to constrained accuracy and speed performance. And we propose a novel paradigm that introduces future information via self-speculation based on the information similarity at the same time step across different iteration times. Based on this paradigm, we present \textit{SpecDiff}, a training-free multi-level feature caching strategy including a cached feature selection algorithm and a multi-level feature classification algorithm. (1) Feature selection algorithm based on self-speculative information. \textit{SpecDiff} determines a dynamic importance score for each token based on self-speculative information and historical information, and performs cached feature selection through the importance score. (2) Multi-level feature classification algorithm based on feature importance scores. \textit{SpecDiff} classifies tokens by leveraging the differences in feature importance scores and introduces a multi-level feature calculation strategy. Extensive experiments show that \textit{SpecDiff} achieves average 2.80 \times, 2.74 \times , and 3.17\times speedup with negligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow on NVIDIA A800-80GB GPU. By merging speculative and historical information, \textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing the Pareto frontier of speedup and accuracy in the efficient diffusion model inference.

**Link**: [arxiv](https://arxiv.org/abs/2509.13848v2),  [pdf](https://arxiv.org/pdf/2509.13848v2)

**Tags**: cs.CV cs.LG 



### Pre-cache: A Microarchitectural Solution to prevent Meltdown and Spectre
**Authors**: Subhash Sethumurugan, Hari Cherupalli, Kangjie Lu, John Sartori

**Updated**: 2025-11-21T19:23:27Z

**Summary**: Recent work has shown that out-of-order and speculative execution mechanisms used to increase performance in the majority of processors expose the processors to critical attacks. These attacks, called Meltdown and Spectre, exploit the side effects of performance-enhancing features in modern microprocessors to expose secret data through side channels in the microarchitecture. The well known implementations of these attacks exploit cache-based side channels since they are the least noisy channels to exfiltrate data. While some software patches attempted to mitigate these attacks, they are ad-hoc and only try to fix the side effects of the vulnerabilites. They may also impose a performance overhead of up to 30%. In this paper, we present a microarchitecture-based solution for Meltdown and Spectre that addresses the vulnerabilities exploited by the attacks. Our solution prevents flushed instructions from exposing data to the cache. Our approach can also be extended to other memory structures in the microarchitecture thereby preventing variants of the attacks which exploit these memory structures. We further identify two new variant attacks based on exploiting the side effects of speculative and out-of-order execution and show how our solution can be used to prevent these attacks. Evaluation results show that our microarchitectural solution not only restores secure out-of-order and speculative execution, but also has relatively low overhead and does not significantly impact performance for most applications.

**Link**: [arxiv](https://arxiv.org/abs/2511.17726v1),  [pdf](https://arxiv.org/pdf/2511.17726v1)

**Tags**: cs.CR cs.AR 



### Ferroelectric Nematic Liquid Crystals as Charge Boosters for Triboelectric Nanogenerators
**Authors**: Jia-Yao Ye, Susanta Chakraborty, Karthick Subramani, Xing-Zhou Tang, Yan-Nan Xie, Bing-Xiang Li

**Updated**: 2025-11-21T12:27:53Z

**Summary**: Driven by growing demand for clean energy, triboelectric nanogenerators (TENGs) have emerged as promising self-powered systems, yet achieving high charge density remains a critical challenge. In polymer dielectrics, triboelectricity can be further amplified by incorporating high-dielectric and polar materials for functional adaptability. Conventional dielectrics, including liquid crystals (LCs), offer limited improvement for triboelectrification, whereas the breakthrough ferroelectric nematic liquid crystals (NF-LCs), with giant spontaneous polarization and high dielectric constant, act as highly effective charge boosters. Here, we introduce NF-LC (DIO) as a functional filler in a PVDF-based TENG. Increasing DIO content progressively grows the electroactive phase and effective polarization in PVDF and defines a marked improvement in TENGs electrical performances through favourable dipolar alignment and strengthened charge-trap effects. The optimized composite film achieves an impressive open-circuit voltage of 1.1 kV, short-circuit current of 50 micro Amp, and power density of 110 W/m2 - seven times higher than pure PVDF. The device exhibits excellent charge-storage capability, powers over 500 LEDs without power management. This work establishes NF-LC-based TENGs as a new platform for high-performance self-powered energy harvesting, linking soft matter physics with applied energy technology.

**Link**: [arxiv](https://arxiv.org/abs/2511.17202v1),  [pdf](https://arxiv.org/pdf/2511.17202v1)

**Tags**: physics.app-ph cond-mat.mtrl-sci 



### RASTP: Representation-Aware Semantic Token Pruning for Generative Recommendation with Semantic Identifiers
**Authors**: Tianyu Zhan, Kairui Fu, Zheqi Lv, Shengyu Zhang

**Updated**: 2025-11-21T04:39:32Z

**Summary**: Generative recommendation systems typically leverage Semantic Identifiers (SIDs), which represent each item as a sequence of tokens that encode semantic information. However, representing item ID with multiple SIDs significantly increases input sequence length, which is a major determinant of computational complexity and memory consumption. While existing efforts primarily focus on optimizing attention computation and KV cache, we propose RASTP (Representation-Aware Semantic Token Pruning), which directly prunes less informative tokens in the input sequence. Specifically, RASTP evaluates token importance by combining semantic saliency, measured via representation magnitude, and attention centrality, derived from cumulative attention weights. Since RASTP dynamically prunes low-information or irrelevant semantic tokens, experiments on three real-world Amazon datasets show that RASTP reduces training time by 26.7\%, while maintaining or slightly improving recommendation performance. The code has been open-sourced at https://github.com/Yuzt-zju/RASTP.

**Link**: [arxiv](https://arxiv.org/abs/2511.16943v1),  [pdf](https://arxiv.org/pdf/2511.16943v1)

**Tags**: cs.IR cs.AI 



### Revisiting Multimodal KV Cache Compression: A Frequency-Domain-Guided Outlier-KV-Aware Approach
**Authors**: Yaoxin Yang, Peng Ye, Xudong Tan, Chongjun Tu, Maosen Zhao, Jia Hao, Tao Chen

**Updated**: 2025-11-20T20:25:34Z

**Summary**: Multimodal large language models suffer from substantial inference overhead since multimodal KV Cache grows proportionally with the visual input length. Existing multimodal KV Cache compression methods mostly rely on attention score to reduce cache size, which makes them are incompatible with established efficient attention kernels (e.g., FlashAttention) and ignores the contribution of value vectors to the attention output. In this work, we revisit multimodal KV Cache compression from the perspective of the KV matrices' distribution. First, we observe that frequency-domain energy of multimodal KV matrices is predominantly concentrated in low-frequency and extract this principal energy via a low-pass filter. Further, we find that removing KV pairs that deviate substantially from this principal energy leads to a pronounced performance drop, which we define as Outlier KVs. Considering Outlier KVs are more likely to encode features critical for inference, we propose FlashCache, a frequency-domain-guided, Outlier-KV-aware KV Cache compression framework. First, we introduce an Outlier KV Recognition Module that models the principal component of multimodal KV matrices in the frequency domain and preferentially retains KV pairs that significantly deviate from it. Furthermore, Dynamic Budget Allocation Module is designed to adaptively determine the per-layer KV Cache size to retain more Outlier KVs. Experiments on multiple MLLMs and benchmarks demonstrate that FlashCache outperforms state-of-the-art multimoal KV compression methods, achieving up to 1.69 times faster decoding with 80% lower KV memory usage while maintaining task performance.

**Link**: [arxiv](https://arxiv.org/abs/2511.16786v1),  [pdf](https://arxiv.org/pdf/2511.16786v1)

**Tags**: cs.LG cs.AI cs.CV 



### Adapt-As-You-Walk Through the Clouds: Training-Free Online Test-Time Adaptation of 3D Vision-Language Foundation Models
**Authors**: Mehran Tamjidi, Hamidreza Dastmalchi, Mohammadreza Alimoradijazi, Ali Cheraghian, Aijun An, Morteza Saberi

**Updated**: 2025-11-20T19:08:56Z

**Summary**: 3D Vision-Language Foundation Models (VLFMs) have shown strong generalization and zero-shot recognition capabilities in open-world point cloud processing tasks. However, these models often underperform in practical scenarios where data are noisy, incomplete, or drawn from a different distribution than the training data. To address this, we propose Uni-Adapter, a novel training-free online test-time adaptation (TTA) strategy for 3D VLFMs based on dynamic prototype learning. We define a 3D cache to store class-specific cluster centers as prototypes, which are continuously updated to capture intra-class variability in heterogeneous data distributions. These dynamic prototypes serve as anchors for cache-based logit computation via similarity scoring. Simultaneously, a graph-based label smoothing module captures inter-prototype similarities to enforce label consistency among similar prototypes. Finally, we unify predictions from the original 3D VLFM and the refined 3D cache using entropy-weighted aggregation for reliable adaptation. Without retraining, Uni-Adapter effectively mitigates distribution shifts, achieving state-of-the-art performance on diverse 3D benchmarks over different 3D VLFMs, improving ModelNet-40C by 10.55%, ScanObjectNN-C by 8.26%, and ShapeNet-C by 4.49% over the source 3D VLFMs. Project page: https://mehran-tam.github.io/Uni-Adapter

**Link**: [arxiv](https://arxiv.org/abs/2511.15311v2),  [pdf](https://arxiv.org/pdf/2511.15311v2)

**Tags**: cs.CV 



### Progressive Supernet Training for Efficient Visual Autoregressive Modeling
**Authors**: Xiaoyue Chen, Yuling Shi, Kaiyuan Li, Huandong Wang, Yong Li, Xiaodong Gu, Xinlei Chen, Mingbao Lin

**Updated**: 2025-11-20T16:59:24Z

**Summary**: Visual Auto-Regressive (VAR) models significantly reduce inference steps through the "next-scale" prediction paradigm. However, progressive multi-scale generation incurs substantial memory overhead due to cumulative KV caching, limiting practical deployment.   We observe a scale-depth asymmetric dependency in VAR: early scales exhibit extreme sensitivity to network depth, while later scales remain robust to depth reduction. Inspired by this, we propose VARiant: by equidistant sampling, we select multiple subnets ranging from 16 to 2 layers from the original 30-layer VAR-d30 network. Early scales are processed by the full network, while later scales utilize subnet. Subnet and the full network share weights, enabling flexible depth adjustment within a single model.   However, weight sharing between subnet and the entire network can lead to optimization conflicts. To address this, we propose a progressive training strategy that breaks through the Pareto frontier of generation quality for both subnets and the full network under fixed-ratio training, achieving joint optimality.   Experiments on ImageNet demonstrate that, compared to the pretrained VAR-d30 (FID 1.95), VARiant-d16 and VARiant-d8 achieve nearly equivalent quality (FID 2.05/2.12) while reducing memory consumption by 40-65%. VARiant-d2 achieves 3.5 times speedup and 80% memory reduction at moderate quality cost (FID 2.97). In terms of deployment, VARiant's single-model architecture supports zero-cost runtime depth switching and provides flexible deployment options from high quality to extreme efficiency, catering to diverse application scenarios.

**Link**: [arxiv](https://arxiv.org/abs/2511.16546v1),  [pdf](https://arxiv.org/pdf/2511.16546v1)

**Tags**: cs.CV 



### KVTuner: Sensitivity-Aware Layer-Wise Mixed-Precision KV Cache Quantization for Efficient and Nearly Lossless LLM Inference
**Authors**: Xing Li, Zeyu Xing, Yiming Li, Linping Qu, Hui-Ling Zhen, Wulong Liu, Yiwu Yao, Sinno Jialin Pan, Mingxuan Yuan

**Updated**: 2025-11-20T15:25:17Z

**Summary**: KV cache quantization can improve Large Language Models (LLMs) inference throughput and latency in long contexts and large batch-size scenarios while preserving LLMs effectiveness. However, current methods have three unsolved issues: overlooking layer-wise sensitivity to KV cache quantization, high overhead of online fine-grained decision-making, and low flexibility to different LLMs and constraints. Therefore, we theoretically analyze the inherent correlation of layer-wise transformer attention patterns to KV cache quantization errors and study why key cache is generally more important than value cache for quantization error reduction. We further propose a simple yet effective framework KVTuner to adaptively search for the optimal hardware-friendly layer-wise KV quantization precision pairs for coarse-grained KV cache with multi-objective optimization and directly utilize the offline searched configurations during online inference. To reduce the computational cost of offline calibration, we utilize the intra-layer KV precision pair pruning and inter-layer clustering to reduce the search space. Experimental results show that we can achieve nearly lossless 3.25-bit mixed precision KV cache quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive models like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum inference throughput can be improved by 21.25\% compared with KIVI-KV8 quantization over various context lengths. Our code and searched configurations are available at https://github.com/cmd2001/KVTuner.

**Link**: [arxiv](https://arxiv.org/abs/2502.04420v5),  [pdf](https://arxiv.org/pdf/2502.04420v5)

**Tags**: cs.LG cs.AI cs.CL 



### Optimizing 3D Gaussian Splattering for Mobile GPUs
**Authors**: Md Musfiqur Rahman Sanim, Zhihao Shu, Bahram Afsharmanesh, AmirAli Mirian, Jiexiong Guan, Wei Niu, Bin Ren, Gagan Agrawal

**Updated**: 2025-11-20T12:25:26Z

**Summary**: Image-based 3D scene reconstruction, which transforms multi-view images into a structured 3D representation of the surrounding environment, is a common task across many modern applications. 3D Gaussian Splatting (3DGS) is a new paradigm to address this problem and offers considerable efficiency as compared to the previous methods. Motivated by this, and considering various benefits of mobile device deployment (data privacy, operating without internet connectivity, and potentially faster responses), this paper develops Texture3dgs, an optimized mapping of 3DGS for a mobile GPU. A critical challenge in this area turns out to be optimizing for the two-dimensional (2D) texture cache, which needs to be exploited for faster executions on mobile GPUs. As a sorting method dominates the computations in 3DGS on mobile platforms, the core of Texture3dgs is a novel sorting algorithm where the processing, data movement, and placement are highly optimized for 2D memory. The properties of this algorithm are analyzed in view of a cost model for the texture cache. In addition, we accelerate other steps of the 3DGS algorithm through improved variable layout design and other optimizations. End-to-end evaluation shows that Texture3dgs delivers up to 4.1$\times$ and 1.7$\times$ speedup for the sorting and overall 3D scene reconstruction, respectively -- while also reducing memory usage by up to 1.6$\times$ -- demonstrating the effectiveness of our design for efficient mobile 3D scene reconstruction.

**Link**: [arxiv](https://arxiv.org/abs/2511.16298v1),  [pdf](https://arxiv.org/pdf/2511.16298v1)

**Tags**: cs.CV cs.GR 



### AMS-KV: Adaptive KV Caching in Multi-Scale Visual Autoregressive Transformers
**Authors**: Boxun Xu, Yu Wang, Zihu Wang, Peng Li

**Updated**: 2025-11-20T05:10:12Z

**Summary**: Visual autoregressive modeling (VAR) via next-scale prediction has emerged as a scalable image generation paradigm. While Key and Value (KV) caching in large language models (LLMs) has been extensively studied, next-scale prediction presents unique challenges, and KV caching design for next-scale based VAR transformers remains largely unexplored. A major bottleneck is the excessive KV memory growth with the increasing number of scales-severely limiting scalability. Our systematic investigation reveals that: (1) Attending to tokens from local scales significantly contributes to generation quality (2) Allocating a small amount of memory for the coarsest scales, termed as condensed scales, stabilizes multi-scale image generation (3) Strong KV similarity across finer scales is predominantly observed in cache-efficient layers, whereas cache-demanding layers exhibit weaker inter-scale similarity. Based on the observations, we introduce AMS-KV, a scale-adaptive KV caching policy for next-scale prediction in VAR models. AMS-KV prioritizes storing KVs from condensed and local scales, preserving the most relevant tokens to maintain generation quality. It further optimizes KV cache utilization and computational efficiency identifying cache-demanding layers through inter-scale similarity analysis. Compared to the vanilla next-scale prediction-based VAR models, AMS-KV reduces KV cache usage by up to 84.83% and self-attention latency by 60.48%. Moreover, when the baseline VAR-d30 model encounters out-of-memory failures at a batch size of 128, AMS-KV enables stable scaling to a batch size of 256 with improved throughput.

**Link**: [arxiv](https://arxiv.org/abs/2511.16047v1),  [pdf](https://arxiv.org/pdf/2511.16047v1)

**Tags**: cs.CV 



### Train Short, Infer Long: Speech-LLM Enables Zero-Shot Streamable Joint ASR and Diarization on Long Audio
**Authors**: Mohan Shi, Xiong Xiao, Ruchao Fan, Shaoshi Ling, Jinyu Li

**Updated**: 2025-11-20T05:07:13Z

**Summary**: Joint automatic speech recognition (ASR) and speaker diarization aim to answer the question "who spoke what" in multi-speaker scenarios. In this paper, we present an end-to-end speech large language model (Speech-LLM) for Joint strEamable DIarization and aSr (JEDIS-LLM). The model is trained only on short audio under 20s but is capable of streamable inference on long-form audio without additional training. This is achieved by introducing a Speaker Prompt Cache (SPC) with an on-the-fly update mechanism during chunk-wise streaming inference, inspired by the autoregressive nature of LLMs. The SPC also allows the seamless use of pre-enrolled speaker profiles which is common in many scenarios like meeting transcription. To further enhance diarization capability, we incorporate word-level speaker supervision into the speech encoder during training. Experimental results demonstrate that our system outperforms strong baselines, including Sortformer and Meta-Cat in the local setting on audio up to 20s, and DiarizationLM on long-form audio, despite being fully end-to-end and streamable while DiarizationLM follows a cascaded offline pipeline. To the best of our knowledge, this is the first work enabling zero-shot streamable joint ASR and diarization on long audio using a Speech-LLM trained only on short audio, achieving state-of-the-art performance.

**Link**: [arxiv](https://arxiv.org/abs/2511.16046v1),  [pdf](https://arxiv.org/pdf/2511.16046v1)

**Tags**: eess.AS 



### Data Layout Polymorphism for Bounding Volume Hierarchies
**Authors**: Christophe Gyurgyik, Alexander J Root, Fredrik Kjolstad

**Updated**: 2025-11-20T04:37:32Z

**Summary**: Bounding volume hierarchies are ubiquitous acceleration structures in graphics, scientific computing, and data analytics. Their performance depends critically on data layout choices that affect cache utilization, memory bandwidth, and vectorization -- increasingly dominant factors in modern computing. Yet, in most programming systems, these layout choices are hopelessly entangled with the traversal logic. This entanglement prevents developers from independently optimizing data layouts and algorithms across different contexts, perpetuating a false dichotomy between performance and portability. We introduce Scion, a domain-specific language and compiler for specifying the data layouts of bounding volume hierarchies independent of tree traversal algorithms. We show that Scion can express a broad spectrum of layout optimizations used in high performance computing while remaining architecture-agnostic. We demonstrate empirically that Pareto-optimal layouts (along performance and memory footprint axes) vary across algorithms, architectures, and workload characteristics. Through systematic design exploration, we also identify a novel ray tracing layout that combines optimization techniques from prior work, achieving Pareto-optimality across diverse architectures and scenes.

**Link**: [arxiv](https://arxiv.org/abs/2511.15028v2),  [pdf](https://arxiv.org/pdf/2511.15028v2)

**Tags**: cs.PL 



### Material processing by laser-plasma-filament-guided high voltage discharges
**Authors**: Kristian Cvecek, Markus Döring, Alexander Romboy, Johannes Heberle, Michael Schmidt

**Updated**: 2025-11-19T17:39:12Z

**Summary**: We investigate ablation experiments performed by laser-plasma-filament guided electrical discharges at high-voltages of up to 145 kV. The guiding was accomplished via fs-laser-generated plasma filaments across a gap of 201 mm of air onto steel 1.3343 samples. This method combines remote material processing and enables the steering and deflection of high voltage discharges with the ease-of-use of remote laser processing technology. We observe an increase of the per-pulse-(and-discharge)-ablated volume by a factor of 1.67 over an ablation regime when the discharges are not present and up to a factor of 12.5 over the case when neither discharges nor plasma filaments, only a loosely focused laser beam, are present.

**Link**: [arxiv](https://arxiv.org/abs/2511.15651v1),  [pdf](https://arxiv.org/pdf/2511.15651v1)

**Tags**: physics.plasm-ph physics.app-ph physics.optics 



### EPSO: A Caching-Based Efficient Superoptimizer for BPF Bytecode
**Authors**: Qian Zhu, Yuxuan Liu, Ziyuan Zhu, Shangqing Liu, Lei Bu

**Updated**: 2025-11-19T16:21:20Z

**Summary**: Extended Berkeley Packet Filter (eBPF) allows developers to extend Linux kernel functionality without modifying its source code. To ensure system safety, an in-kernel safety checker, the verifier, enforces strict safety constraints (for example, a limited program size) on eBPF programs loaded into the kernel. These constraints, combined with eBPF's performance-critical use cases, make effective optimization essential. However, existing compilers (such as Clang) offer limited optimization support, and many semantics-preserving transformations are rejected by the verifier, which makes handcrafted optimization rule design both challenging and limited in effectiveness. Superoptimization overcomes the limitations of rule-based methods by automatically discovering optimal transformations, but its high computational cost limits scalability. To address this, we propose EPSO, a caching-based superoptimizer that discovers rewrite rules via offline superoptimization and reuses them to achieve high-quality optimizations with minimal runtime overhead. We evaluate EPSO on benchmarks from the Linux kernel and several eBPF-based projects, including Cilium, Katran, hXDP, Sysdig, Tetragon, and Tracee. EPSO discovers 795 rewrite rules and achieves up to 68.87 percent (average 24.37 percent) reduction in program size compared to Clang's output, outperforming the state-of-the-art BPF optimizer K2 on all benchmarks and Merlin on 92.68 percent of them. Additionally, EPSO reduces program runtime by an average of 6.60 percent, improving throughput and lowering latency in network applications.

**Link**: [arxiv](https://arxiv.org/abs/2511.15589v1),  [pdf](https://arxiv.org/pdf/2511.15589v1)

**Tags**: cs.SE cs.PL 



### B+ANN: A Fast Billion-Scale Disk-based Nearest-Neighbor Index
**Authors**: Selim Furkan Tekin, Rajesh Bordawekar

**Updated**: 2025-11-19T15:50:28Z

**Summary**: Storing and processing of embedding vectors by specialized Vector databases (VDBs) has become the linchpin in building modern AI pipelines. Most current VDBs employ variants of a graph-based ap- proximate nearest-neighbor (ANN) index algorithm, HNSW, to an- swer semantic queries over stored vectors. Inspite of its wide-spread use, the HNSW algorithm suffers from several issues: in-memory design and implementation, random memory accesses leading to degradation in cache behavior, limited acceleration scope due to fine-grained pairwise computations, and support of only semantic similarity queries. In this paper, we present a novel disk-based ANN index, B+ANN, to address these issues: it first partitions input data into blocks containing semantically similar items, then builds an B+ tree variant to store blocks both in-memory and on disks, and finally, enables hybrid edge- and block-based in-memory traversals. As demonstrated by our experimantal evaluation, the proposed B+ANN disk-based index improves both quality (Recall value), and execution performance (Queries per second/QPS) over HNSW, by improving spatial and temporal locality for semantic operations, reducing cache misses (19.23% relative gain), and decreasing the memory consumption and disk-based build time by 24x over the DiskANN algorithm. Finally, it enables dissimilarity queries, which are not supported by similarity-oriented ANN indices.

**Link**: [arxiv](https://arxiv.org/abs/2511.15557v1),  [pdf](https://arxiv.org/pdf/2511.15557v1)

**Tags**: cs.DB cs.AI cs.DS 



### A Data-driven ML Approach for Maximizing Performance in LLM-Adapter Serving
**Authors**: Ferran Agullo, Joan Oliveras, Chen Wang, Alberto Gutierrez-Torre, Olivier Tardieu, Alaa Youssef, Jordi Torres, Josep Ll. Berral

**Updated**: 2025-11-19T13:36:14Z

**Summary**: With the rapid adoption of Large Language Models (LLMs), LLM-adapters have become increasingly common, providing lightweight specialization of large-scale models. Serving hundreds or thousands of these adapters on a single GPU allows request aggregation, increasing throughput, but may also cause request starvation if GPU memory limits are exceeded. To address this issue, this study focuses on determining the joint configuration of concurrent and parallel adapters that maximizes GPU throughput without inducing starvation, given heterogeneous adapter and traffic properties. We propose a data-driven ML approach leveraging interpretable models to tackle this caching problem and introduce the first Digital Twin capable of reproducing an LLM-adapter serving system, enabling efficient training data generation. Experiments with the vLLM framework and LoRA adapters show that the Digital Twin reproduces throughput within 5.1% of real results, while the ML approach predicts optimal numbers of concurrent and parallel adapters with an error of at most 7.2% under heterogeneous, real-world workloads. The code is publicly available at https://github.com/FerranAgulloLopez/GPULLMAdapterOptimization.

**Link**: [arxiv](https://arxiv.org/abs/2508.08343v3),  [pdf](https://arxiv.org/pdf/2508.08343v3)

**Tags**: cs.PF cs.AI cs.CL 



### ANTS: Adaptive Negative Textual Space Shaping for OOD Detection via Test-Time MLLM Understanding and Reasoning
**Authors**: Wenjie Zhu, Yabin Zhang, Xin Jin, Wenjun Zeng, Lei Zhang

**Updated**: 2025-11-19T13:23:53Z

**Summary**: The introduction of negative labels (NLs) has proven effective in enhancing Out-of-Distribution (OOD) detection. However, existing methods often lack an understanding of OOD images, making it difficult to construct an accurate negative space. Furthermore, the absence of negative labels semantically similar to ID labels constrains their capability in near-OOD detection. To address these issues, we propose shaping an Adaptive Negative Textual Space (ANTS) by leveraging the understanding and reasoning capabilities of multimodal large language models (MLLMs). Specifically, we cache images likely to be OOD samples from the historical test images and prompt the MLLM to describe these images, generating expressive negative sentences that precisely characterize the OOD distribution and enhance far-OOD detection. For the near-OOD setting, where OOD samples resemble the in-distribution (ID) subset, we cache the subset of ID classes that are visually similar to historical test images and then leverage MLLM reasoning to generate visually similar negative labels tailored to this subset, effectively reducing false negatives and improving near-OOD detection. To balance these two types of negative textual spaces, we design an adaptive weighted score that enables the method to handle different OOD task settings (near-OOD and far-OOD), making it highly adaptable in open environments. On the ImageNet benchmark, our ANTS significantly reduces the FPR95 by 3.1\%, establishing a new state-of-the-art. Furthermore, our method is training-free and zero-shot, enabling high scalability.

**Link**: [arxiv](https://arxiv.org/abs/2509.03951v3),  [pdf](https://arxiv.org/pdf/2509.03951v3)

**Tags**: cs.CV 



### DARE: An Irregularity-Tolerant Matrix Processing Unit with a Densifying ISA and Filtered Runahead Execution
**Authors**: Xin Yang, Xin Fan, Zengshi Wang, Jun Han

**Updated**: 2025-11-19T11:52:03Z

**Summary**: Deep Neural Networks (DNNs) are widely applied across domains and have shown strong effectiveness. As DNN workloads increasingly run on CPUs, dedicated Matrix Processing Units (MPUs) and Matrix Instruction Set Architectures (ISAs) have been introduced. At the same time, sparsity techniques are widely adopted in algorithms to reduce computational cost.   Despite these advances, insufficient hardware-algorithm co-optimization leads to suboptimal performance. On the memory side, sparse DNNs incur irregular access patterns that cause high cache miss rates. While runahead execution is a promising prefetching technique, its direct application to MPUs is often ineffective due to significant prefetch redundancy. On the compute side, stride constraints in current Matrix ISAs prevent the densification of multiple logically related sparse operations, resulting in poor utilization of MPU processing elements.   To address these irregularities, we propose DARE, an irregularity-tolerant MPU with a Densifying ISA and filtered Runahead Execution. DARE extends the ISA to support densifying sparse operations and equips a lightweight runahead mechanism with filtering capability. Experimental results show that DARE improves performance by 1.04$\times$ to 4.44$\times$ and increases energy efficiency by 1.00$\times$ to 22.8$\times$ over the baseline, with 3.91$\times$ lower hardware overhead than NVR.

**Link**: [arxiv](https://arxiv.org/abs/2511.15367v1),  [pdf](https://arxiv.org/pdf/2511.15367v1)

**Tags**: cs.AR 



### Enabling MoE on the Edge via Importance-Driven Expert Scheduling
**Authors**: Guoying Zhu, Meng Li, Haipeng Dai, Xuechen Liu, Weijun Wang, Keran Li, Jun xiao, Ligeng Chen, Wei Wang

**Updated**: 2025-11-19T06:48:57Z

**Summary**: The Mixture of Experts (MoE) architecture has emerged as a key technique for scaling Large Language Models by activating only a subset of experts per query. Deploying MoE on consumer-grade edge hardware, however, is constrained by limited device memory, making dynamic expert offloading essential. Unlike prior work that treats offloading purely as a scheduling problem, we leverage expert importance to guide decisions, substituting low-importance activated experts with functionally similar ones already cached in GPU memory, thereby preserving accuracy. As a result, this design reduces memory usage and data transfer, while largely eliminating PCIe overhead. In addition, we introduce a scheduling policy that maximizes the reuse ratio of GPU-cached experts, further boosting efficiency. Extensive evaluations show that our approach delivers 48% lower decoding latency with over 60% expert cache hit rate, while maintaining nearly lossless accuracy.

**Link**: [arxiv](https://arxiv.org/abs/2508.18983v2),  [pdf](https://arxiv.org/pdf/2508.18983v2)

**Tags**: cs.AI 



### PIM or CXL-PIM? Understanding Architectural Trade-offs Through Large-Scale Benchmarking
**Authors**: I-Ting Lee, Bao-Kai Wang, Liang-Chi Chen, Wen Sheng Lim, Da-Wei Chang, Yu-Ming Chang, Chieng-Chung Ho

**Updated**: 2025-11-19T04:13:40Z

**Summary**: Processing-in-memory (PIM) reduces data movement by executing near memory, but our large-scale characterization on real PIM hardware shows that end-to-end performance is often limited by disjoint host and device address spaces that force explicit staging transfers. In contrast, CXL-PIM provides a unified address space and cache-coherent access at the cost of higher access latency. These opposing interface models create workload-dependent tradeoffs that are not captured by small-scale studies. This work presents a side-by-side, large-scale comparison of PIM and CXL-PIM using measurements from real PIM hardware and trace-driven CXL modeling. We identify when unified-address access amortizes link latency enough to overcome transfer bottlenecks, and when tightly coupled PIM remains preferable. Our results reveal phase- and dataset-size regimes in which the relative ranking between the two architectures reverses, offering practical guidance for future near-memory system design.

**Link**: [arxiv](https://arxiv.org/abs/2511.14400v2),  [pdf](https://arxiv.org/pdf/2511.14400v2)

**Tags**: cs.ET cs.PF 



### A Comprehensive Study on Visual Token Redundancy for Discrete Diffusion-based Multimodal Large Language Models
**Authors**: Duo Li, Zuhao Yang, Xiaoqin Zhang, Ling Shao, Shijian Lu

**Updated**: 2025-11-19T04:13:36Z

**Summary**: Discrete diffusion-based multimodal large language models (dMLLMs) have emerged as a promising alternative to autoregressive MLLMs thanks to their advantages in parallel decoding and bidirectional context modeling, but most existing dMLLMs incur significant computational overhead during inference due to the full-sequence attention computation in each denoising step. Pioneer studies attempt to resolve this issue from a modality-agnostic perspective via key-value cache optimization or efficient sampling but most of them overlook modality-specific visual token redundancy. In this work, we conduct a comprehensive study on how visual token redundancy evolves with different dMLLM architectures and tasks and how visual token pruning affects dMLLM responses and efficiency. Specifically, our study reveals that visual redundancy emerges only in from-scratch dMLLMs while handling long-answer tasks. In addition, we validate that visual token pruning introduces non-negligible information loss in dMLLMs and only from-scratch dMLLMs can recover the lost information progressively during late denoising steps. Furthermore, our study shows that layer-skipping is promising for accelerating AR-to-diffusion dMLLMs, whereas progressive or late-step pruning is more effective for from-scratch dMLLMs. Overall, this work offers a new perspective on efficiency optimization for dMLLMs, greatly advancing their applicability across various multimodal understanding tasks.

**Link**: [arxiv](https://arxiv.org/abs/2511.15098v1),  [pdf](https://arxiv.org/pdf/2511.15098v1)

**Tags**: cs.CV 



### Electric-Field-Dependent Thermal Conductivity in Fresh and Aged Bulk Single Crystalline $\mathrm{BaTiO_3}$
**Authors**: Fanghao Zhang, Guanchun Rui, Yujie Quan, Shantal Adajian, Matthew Delmont, Q. M. Zhang, Bolin Liao

**Updated**: 2025-11-18T23:42:50Z

**Summary**: Active thermal management requires advances in thermal switching materials, whose thermal conductivity responds to external stimuli. The electric field, as one of the most convenient and effective stimuli, has shown great potential in tuning the thermal conductivity of ferroelectric materials. While previous studies on electric-field-induced ferroelectric thermal switching have primarily focused on thin films and bulk solid solutions with strong extrinsic interface and defect scatterings, bulk single crystals, which can offer clear insights into intrinsic thermal switching mechanisms, have received comparatively less attention. Here, we demonstrate electric-field-induced thermal switching in bulk single-crystalline $\mathrm{BaTiO_3}$ (BTO) at room temperature and elucidate the critical role of domain evolution and aging in governing heat transport. Using a customized steady-state platform with in-situ electric fields up to $\pm$10 kV/cm, we observe a modulation of thermal conductivity up to 35% in fresh BTO driven by polarization reorientation and domain restructuring. First-principles finite-temperature lattice-dynamics calculations confirm that the switching behavior primarily originates from anisotropic phonon transport associated with domain configuration rather than strain-induced changes in phonon velocities. We further reveal that both ambient aging and controlled thermal aging can enhance the switching contrast through the formation and alignment of defect dipoles that modulate phonon-defect scattering. These results establish defect-domain interactions as a powerful design parameter for ferroelectric thermal switches and demonstrate a versatile experimental platform for exploring field-tunable heat transport and phase behavior in bulk functional materials.

**Link**: [arxiv](https://arxiv.org/abs/2511.14974v1),  [pdf](https://arxiv.org/pdf/2511.14974v1)

**Tags**: cond-mat.mtrl-sci 



### OmniSparse: Training-Aware Fine-Grained Sparse Attention for Long-Video MLLMs
**Authors**: Feng Chen, Yefei He, Shaoxuan He, Yuanyu He, Jing Liu, Lequan Lin, Akide Liu, Zhaoyang Li, Jiyuan Zhang, Zhenbang Sun, Bohan Zhuang, Qi Wu

**Updated**: 2025-11-18T23:07:41Z

**Summary**: Existing sparse attention methods primarily target inference-time acceleration by selecting critical tokens under predefined sparsity patterns. However, they often fail to bridge the training-inference gap and lack the capacity for fine-grained token selection across multiple dimensions such as queries, key-values (KV), and heads, leading to suboptimal performance and limited acceleration gains. In this paper, we introduce OmniSparse, a training-aware fine-grained sparse attention framework for long-video MLLMs, which operates in both training and inference with dynamic token budget allocation. Specifically, OmniSparse contains three adaptive and complementary mechanisms: (1) query selection via lazy-active classification, retaining active queries that capture broad semantic similarity while discarding most lazy ones that focus on limited local context and exhibit high functional redundancy; (2) KV selection with head-level dynamic budget allocation, where a shared budget is determined based on the flattest head and applied uniformly across all heads to ensure attention recall; and (3) KV cache slimming to reduce head-level redundancy by selectively fetching visual KV cache according to the head-level decoding query pattern. Experimental results show that OmniSparse matches the performance of full attention while achieving up to 2.7x speedup during prefill and 2.4x memory reduction during decoding.

**Link**: [arxiv](https://arxiv.org/abs/2511.12201v2),  [pdf](https://arxiv.org/pdf/2511.12201v2)

**Tags**: cs.CV 



### RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based Sequence Modeling
**Authors**: Xiuying Wei, Anunay Yadav, Razvan Pascanu, Caglar Gulcehre

**Updated**: 2025-11-18T22:29:13Z

**Summary**: Transformers have become the cornerstone of modern large-scale language models, but their reliance on softmax attention poses a computational bottleneck at both training and inference. Recurrent models offer high efficiency, but compressing the full sequence into a fixed-size and holistic representation can suffer from memory degradation in long contexts and limit fine-grained retrieval. To address this, we propose RAT, an intermediate design that bridges the efficiency of RNNs and capacity of attention. RAT partitions the input into chunks, applies recurrence within each chunk for local dependencies, and softmax-based attention across chunks for long-range interactions. This design mitigates memory degradation and enables direct access to distant tokens, while retaining computational efficiency. Empirically, with a chunk size of 16, the RAT block achieves a 7$\times$ improvement in training speed for 100K sequence length and 9$times$ in generation at the 4K position, while maintaining similar performance compared to standard attention. We demonstrate this by training 1.3B parameter models from scratch and performing large-scale evaluations, including short- and long-context benchmarks, as well as supervised fine-tuning~(SFT). We further propose a hybrid architecture that interleaves RAT with local attention. By combining efficient long-range modeling with strong local interactions, this hybrid design not only improves inference speed and reduces cache memory usage, but also consistently enhances performance and shows the overall best results. Code is available at https://github.com/CLAIRE-Labo/RAT.

**Link**: [arxiv](https://arxiv.org/abs/2507.04416v3),  [pdf](https://arxiv.org/pdf/2507.04416v3)

**Tags**: cs.CL 



### CPSL: Representing Volumetric Video via Content-Promoted Scene Layers
**Authors**: Kaiyuan Hu, Yili Jin, Junhua Liu, Xize Duan, Hong Kang, Xue Liu

**Updated**: 2025-11-18T21:26:13Z

**Summary**: Volumetric video enables immersive and interactive visual experiences by supporting free viewpoint exploration and realistic motion parallax. However, existing volumetric representations from explicit point clouds to implicit neural fields, remain costly in capture, computation, and rendering, which limits their scalability for on-demand video and reduces their feasibility for real-time communication.   To bridge this gap, we propose Content-Promoted Scene Layers (CPSL), a compact 2.5D video representation that brings the perceptual benefits of volumetric video to conventional 2D content. Guided by per-frame depth and content saliency, CPSL decomposes each frame into a small set of geometry-consistent layers equipped with soft alpha bands and an edge-depth cache that jointly preserve occlusion ordering and boundary continuity. These lightweight, 2D-encodable assets enable parallax-corrected novel-view synthesis via depth-weighted warping and front-to-back alpha compositing, bypassing expensive 3D reconstruction. Temporally, CPSL maintains inter-frame coherence using motion-guided propagation and per-layer encoding, supporting real-time playback with standard video codecs. Across multiple benchmarks, CPSL achieves superior perceptual quality and boundary fidelity compared with layer-based and neural-field baselines while reducing storage and rendering cost by several folds. Our approach offer a practical path from 2D video to scalable 2.5D immersive media.

**Link**: [arxiv](https://arxiv.org/abs/2511.14927v1),  [pdf](https://arxiv.org/pdf/2511.14927v1)

**Tags**: cs.CV cs.MM 



### SilverTorch: A Unified Model-based System to Democratize Large-Scale Recommendation on GPUs
**Authors**: Bi Xue, Hong Wu, Lei Chen, Chao Yang, Yiming Ma, Fei Ding, Zhen Wang, Liang Wang, Xiaoheng Mao, Ke Huang, Xialu Li, Peng Xia, Rui Jian, Yanli Zhao, Yanzun Huang, Yijie Deng, Harry Tran, Ryan Chang, Min Yu, Eric Dong, Jiazhou Wang, Qianqian Zhang, Keke Zhai, Hongzhang Yin, Pawel Garbacki, Zheng Fang, Yiyi Pan, Min Ni, Yang Liu

**Updated**: 2025-11-18T20:00:19Z

**Summary**: Serving deep learning based recommendation models (DLRM) at scale is challenging. Existing systems rely on CPU-based ANN indexing and filtering services, suffering from non-negligible costs and forgoing joint optimization opportunities. Such inefficiency makes them difficult to support more complex model architectures, such as learned similarities and multi-task retrieval.   In this paper, we propose SilverTorch, a model-based system for serving recommendation models on GPUs. SilverTorch unifies model serving by replacing standalone indexing and filtering services with layers of served models. We propose a Bloom index algorithm on GPUs for feature filtering and a tensor-native fused Int8 ANN kernel on GPUs for nearest neighbor search. We further co-design the ANN search index and filtering index to reduce GPU memory utilization and eliminate unnecessary computation. Benefit from SilverTorch's serving paradigm, we introduce a OverArch scoring layer and a Value Model to aggregate results across multi-tasks. These advancements improve the accuracy for retrieval and enable future studies for serving more complex models. For ranking, SilverTorch's design accelerates item embedding calculation by caching the pre-calculated embeddings inside the serving model.   Our evaluation on the industry-scale datasets show that SilverTorch achieves up to 5.6x lower latency and 23.7x higher throughput compared to the state-of-the-art approaches. We also demonstrate that SilverTorch's solution is 13.35x more cost-efficient than CPU-based solution while improving accuracy via serving more complex models. SilverTorch serves over hundreds of models online across major products and recommends contents for billions of daily active users.

**Link**: [arxiv](https://arxiv.org/abs/2511.14881v1),  [pdf](https://arxiv.org/pdf/2511.14881v1)

**Tags**: cs.IR 



### Cloud-Native Vector Search: A Comprehensive Performance Analysis
**Authors**: Zhaoheng Li, Wei Ding, Silu Huang, Zikang Wang, Yuanjin Lin, Ke Wu, Yongjoo Park, Jianjun Chen

**Updated**: 2025-11-18T18:50:15Z

**Summary**: Vector search has been widely employed in recommender system and retrieval-augmented-generation pipelines, commonly performed with vector indexes to efficiently find similar items in large datasets. Recent growths in both data and task complexity have motivated placing vector indexes onto remote storage -- cloud-native vector search, which cloud providers have recently introduced services for. Yet, despite varying workload characteristics and various available vector index forms, providers default to using cluster-based indexes, which on paper do adapt well to differences between disk and cloud-based environment: their fetch granularities and lack of notable intra-query dependencies aligns with the large optimal fetch sizes and minimizes costly round-trips (i.e., as opposed to graph-based indexes) to remote storage, respectively.   This paper systematically studies cloud-native vector search: What and how should indexes be built and used for on-cloud vector search? We analyze bottlenecks of two common index classes, cluster and graph indexes, on remote storage, and show that despite current standardized adoption of cluster indexes on the cloud, graph indexes are favored in workloads requiring high concurrency and recall, or operating on high-dimensional data or large datatypes. We further find that on-cloud search demands significantly different indexing and search parameterizations versus on-disk search for optimal performance. Finally, we incorporate existing cloud-based caching setups into vector search and find that certain index optimizations work against caching, and study how this can be mitigated to maximize gains under various available cache sizes.

**Link**: [arxiv](https://arxiv.org/abs/2511.14748v1),  [pdf](https://arxiv.org/pdf/2511.14748v1)

**Tags**: cs.DB 



### FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation
**Authors**: Yunfeng Wu, Jiayi Song, Zhenxiong Tan, Zihao He, Songhua Liu

**Updated**: 2025-11-18T17:56:04Z

**Summary**: The quadratic time and memory complexity of the attention mechanism in modern Transformer based video generators makes end-to-end training for ultra high resolution videos prohibitively expensive. Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation. At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail. However, naive local window attention, unfortunately, often leads to repetitive content and exhibits a lack of global coherence in the generated results. To overcome this challenge, we devise a dual-path pipeline that backs up window attention with a novel cross-attention override strategy, enabling the semantic content produced by local attention to be guided by another branch with a full receptive field and, therefore, ensuring holistic consistency. Furthermore, to improve efficiency, we incorporate a cross-attention caching strategy for this branch to avoid the frequent computation of full 3D attention. Extensive experiments demonstrate that our method delivers ultra-high-resolution videos with fine-grained visual details and high efficiency in a training-free paradigm. Meanwhile, it achieves superior performance on VBench, even compared to training-based alternatives, with competitive or improved efficiency. Codes are available at: https://github.com/WillWu111/FreeSwim

**Link**: [arxiv](https://arxiv.org/abs/2511.14712v1),  [pdf](https://arxiv.org/pdf/2511.14712v1)

**Tags**: cs.CV 



### Near-Lossless Model Compression Enables Longer Context Inference in DNA Large Language Models
**Authors**: Rui Zhu, Xiaopu Zhou, Haixu Tang, Stephen W. Scherer, Lucila Ohno-Machado

**Updated**: 2025-11-18T17:29:39Z

**Summary**: Trained on massive cross-species DNA corpora, DNA large language models (LLMs) learn the fundamental "grammar" and evolutionary patterns of genomic sequences. This makes them powerful priors for DNA sequence modeling, particularly over long ranges. However, two major constraints hinder their use in practice: the quadratic computational cost of self-attention and the growing memory required for key-value (KV) caches during autoregressive decoding. These constraints force the use of heuristics such as fixed-window truncation or sliding windows, which compromise fidelity on ultra-long sequences by discarding distant information. We introduce FOCUS (Feature-Oriented Compression for Ultra-long Self-attention), a progressive context-compression module that can be plugged into pretrained DNA LLMs. FOCUS combines the established k-mer representation in genomics with learnable hierarchical compression: it inserts summary tokens at k-mer granularity and progressively compresses attention key and value activations across multiple Transformer layers, retaining only the summary KV states across windows while discarding ordinary-token KV. A shared-boundary windowing scheme yields a stationary cross-window interface that propagates long-range information with minimal loss. We validate FOCUS on an Evo-2-based DNA LLM fine-tuned on GRCh38 chromosome 1 with self-supervised training and randomized compression schedules to promote robustness across compression ratios. On held-out human chromosomes, FOCUS achieves near-lossless fidelity: compressing a 1 kb context into only 10 summary tokens (about 100x) shifts the average per-nucleotide probability by only about 0.0004. Compared to a baseline without compression, FOCUS reduces KV-cache memory and converts effective inference scaling from O(N^2) to near-linear O(N), enabling about 100x longer inference windows on commodity GPUs with near-lossless fidelity.

**Link**: [arxiv](https://arxiv.org/abs/2511.14694v1),  [pdf](https://arxiv.org/pdf/2511.14694v1)

**Tags**: q-bio.GN cs.AI cs.LG q-bio.PE 



### Scalable Enforcement of Fine Grained Access Control Policies in Relational Database Management Systems
**Authors**: Anadi Shakya, Primal Pappachan, David Maier, Roberto Yus, Sharad Mehrotra, Johann-Christoph Freytag

**Updated**: 2025-11-18T16:20:47Z

**Summary**: The proliferation of smart technologies and evolving privacy regulations such as the GDPR and CPRA has increased the need to manage fine-grained access control (FGAC) policies in database management systems (DBMSs). Existing approaches to enforcing FGAC policies do not scale to thousands of policies, leading to degraded query performance and reduced system effectiveness. We present Sieve, a middleware for relational DBMSs that combines query rewriting and caching to optimize FGAC policy enforcement. Sieve rewrites a query with guarded expressions that group and filter policies and can efficiently use indexes in the DBMS. It also integrates a caching mechanism with an effective replacement strategy and a refresh mechanism to adapt to dynamic workloads. Experiments on two DBMSs with real and synthetic datasets show that Sieve scales to large datasets and policy corpora, maintaining low query latency and system load and improving policy evaluation performance by between 2x and 10x on workloads with 200 to 1,200 policies. The caching extension further improves query performance by between 6 and 22 percent under dynamic workloads, especially with larger cache sizes. These results highlight Sieve's applicability for real-time access control in smart environments and its support for efficient, scalable management of user preferences and privacy policies.

**Link**: [arxiv](https://arxiv.org/abs/2511.14629v1),  [pdf](https://arxiv.org/pdf/2511.14629v1)

**Tags**: cs.DB 



### CLO: Efficient LLM Inference System with CPU-Light KVCache Offloading via Algorithm-System Co-Design
**Authors**: Jiawei Yi, Ping Gong, Youhui Bai, Jiaqi Ruan, Shengnan Wang, Pengcheng Wang, Haibo Wang, Weiguang Wang, Xia Zhu, Feng Wu, Cheng Li

**Updated**: 2025-11-18T14:03:21Z

**Summary**: The growth of million-token LLMs exposes the scalability limits of inference systems, where the KVCache dominates memory usage and data transfer overhead. Recent offloading systems migrate the KVCache to CPU memory and incorporate top-k attention to reduce the volume of data transferred from the CPU, while further applying system-level optimizations such as on-GPU caching and prefetching to lower transfer overhead. However, they overlook the CPU bottleneck in three aspects: (1) substantial overhead of fine-grained dynamic cache management performed on the CPU side, (2) significant transfer overhead from poor PCIe bandwidth utilization caused by heavy gathering operations at the CPU side, and (3) GPU runtime bubbles introduced by coarse-grained CPU-centric synchronization. To address these challenges, we propose CLO, a CPU-light KVCache offloading system via algorithm-system co-design. CLO features: (1) a coarse-grained head-wise approximate on-GPU caching strategy with negligible cache management cost, (2) seamless combination of data prefetching and on-GPU persistent caching for lower transfer overhead, (3) a zero-copy transfer engine to fully exploit PCIe bandwidth, and a GPU-centric synchronization method to eliminate GPU stalls. Evaluation on two widely-used LLMs demonstrates that CLO achieves comparable accuracy to state-of-the-art systems, while substantially minimizing CPU overhead, fully utilizing PCIe bandwidth, thus improving decoding throughput by 9.3%-66.6%. Our results highlight that algorithm-system co-design is essential for memory-constrained LLM inference on modern GPU platforms. We open source CLO at https://github.com/CommediaJW/CLO.

**Link**: [arxiv](https://arxiv.org/abs/2511.14510v1),  [pdf](https://arxiv.org/pdf/2511.14510v1)

**Tags**: cs.LG 



### Magnetic atoms with a large electric dipole moment
**Authors**: Johannes Seifert, Sid C. Wright, Boris G. Sartakov, Giacomo Valtolina, Gerard Meijer

**Updated**: 2025-11-18T07:58:55Z

**Summary**: We experimentally show that an electric dipole moment of more than 1 Debye can be induced in the dysprosium (Dy) atom, in a long-lived state that is about 17513 cm$^{-1}$ above the ground state. This metastable state is part of a strongly coupled opposite-parity doublet. Using optically detected microwave spectroscopy in an atomic beam, we determine the approximately 1.12 cm$^{-1}$ doublet spacing for the five stable bosonic isotopes of Dy with kHz-level accuracy. From the shift of the microwave transition frequency in low electric fields (below 150 V/cm) and from optical spectra in high electric fields (up to 150 kV/cm), a reduced transition dipole moment of 7.65 $\pm$ 0.05 Debye between the doublet states is extracted. In high electric fields the doublet interacts with a third state at 17727 cm$^{-1}$, that connects to the ground state via an electric-dipole transition. The three-state Stark interaction enables preparation of Dy atoms in the metastable state via single-photon excitation from the ground state.

**Link**: [arxiv](https://arxiv.org/abs/2511.14225v1),  [pdf](https://arxiv.org/pdf/2511.14225v1)

**Tags**: physics.atom-ph cond-mat.quant-gas 



### Rethinking Token-wise Feature Caching: Accelerating Diffusion Transformers with Dual Feature Caching
**Authors**: Chang Zou, Evelyn Zhang, Runlin Guo, Haohang Xu, Conghui He, Xuming Hu, Linfeng Zhang

**Updated**: 2025-11-18T06:21:13Z

**Summary**: Diffusion Transformers (DiT) have become the dominant methods in image and video generation yet still suffer substantial computational costs. As an effective approach for DiT acceleration, feature caching methods are designed to cache the features of DiT in previous timesteps and reuse them in the next timesteps, allowing us to skip the computation in the next timesteps. Among them, token-wise feature caching has been introduced to perform different caching ratios for different tokens in DiTs, aiming to skip the computation for unimportant tokens while still computing the important ones. In this paper, we propose to carefully check the effectiveness in token-wise feature caching with the following two questions: (1) Is it really necessary to compute the so-called "important" tokens in each step? (2) Are so-called important tokens really important? Surprisingly, this paper gives some counter-intuition answers, demonstrating that consistently computing the selected ``important tokens'' in all steps is not necessary. The selection of the so-called ``important tokens'' is often ineffective, and even sometimes shows inferior performance than random selection. Based on these observations, this paper introduces dual feature caching referred to as DuCa, which performs aggressive caching strategy and conservative caching strategy iteratively and selects the tokens for computing randomly. Extensive experimental results demonstrate the effectiveness of our method in DiT, PixArt, FLUX, and OpenSora, demonstrating significant improvements than the previous token-wise feature caching.

**Link**: [arxiv](https://arxiv.org/abs/2412.18911v2),  [pdf](https://arxiv.org/pdf/2412.18911v2)

**Tags**: cs.LG cs.AI cs.CV 



### CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation
**Authors**: Hasan Akgul, Mari Eplik, Javier Rojas, Aina Binti Abdullah, Pieter van der Merwe

**Updated**: 2025-11-18T06:18:28Z

**Summary**: We present CoSense-LLM, an edge-first framework that turns continuous multimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and lightweight vision) into compact, verifiable semantic tokens and coordinates with large language models under explicit latency, energy, bandwidth, and privacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight encoder that aligns sensor embeddings with language and compresses them into short discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer that grounds generation in site specific policies and notes; (iii) PromptRouter, a cost and uncertainty aware policy that selects edge only generation, edge plus retrieval, or compact cloud escalation; and (iv) Secure Execution, an auditable redaction path that enforces data minimization so raw waveforms never leave the device. The system works with modern serving optimizations, including paged or streaming KV caches, FlashAttention style kernels, speculative decoding, and quantized LoRA adapters, and supports on device personalization and federated updates under non IID drift. Across home, office, and clinic deployments, CoSense-LLM delivers grounded explanations while meeting tight service level objectives: it sustains sub second (p95) end to end latency on edge dominant paths, reduces inter tier token and bandwidth costs by preferring local retrieval grounded responses, and preserves privacy by transmitting only discrete codes and redacted metadata. Ablations show that Edge-RAG improves factual consistency and reduces contradictions, calibrated uncertainty enables selective abstention and controlled escalations, and KV plus decoding accelerators lower energy per decision. The results support an edge first design that treats semantics, privacy, and predictable latency as co equal goals for large model deployments in interference prone environments.

**Link**: [arxiv](https://arxiv.org/abs/2510.19670v3),  [pdf](https://arxiv.org/pdf/2510.19670v3)

**Tags**: cs.CL 



### AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models
**Authors**: Yuhua Jiang, Shuang Cheng, Yan Ding, Feifei Gao, Biqing Qi

**Updated**: 2025-11-18T05:21:11Z

**Summary**: Vision-language-action (VLA) models have recently emerged as a powerful paradigm for building generalist robots. However, traditional VLA models that generate actions through flow matching (FM) typically rely on rigid and uniform time schedules, i.e., synchronous FM (SFM). Without action context awareness and asynchronous self-correction, SFM becomes unstable in long-horizon tasks, where a single action error can cascade into failure. In this work, we propose asynchronous flow matching VLA (AsyncVLA), a novel framework that introduces temporal flexibility in asynchronous FM (AFM) and enables self-correction in action generation. AsyncVLA breaks from the vanilla SFM in VLA models by generating the action tokens in a non-uniform time schedule with action context awareness. Besides, our method introduces the confidence rater to extract confidence of the initially generated actions, enabling the model to selectively refine inaccurate action tokens before execution. Moreover, we propose a unified training procedure for SFM and AFM that endows a single model with both modes, improving KV-cache utilization. Extensive experiments on robotic manipulation benchmarks demonstrate that AsyncVLA is data-efficient and exhibits self-correction ability. AsyncVLA achieves state-of-the-art results across general embodied evaluations due to its asynchronous generation in AFM. Our code is available at https://github.com/YuhuaJiang2002/AsyncVLA.

**Link**: [arxiv](https://arxiv.org/abs/2511.14148v1),  [pdf](https://arxiv.org/pdf/2511.14148v1)

**Tags**: cs.RO cs.AI cs.LG 



### 10Cache: Heterogeneous Resource-Aware Tensor Caching and Migration for LLM Training
**Authors**: Sabiha Afroz, Redwan Ibne Seraj Khan, Hadeel Albahar, Jingoo Han, Ali R. Butt

**Updated**: 2025-11-18T04:17:44Z

**Summary**: Training large language models (LLMs) in the cloud faces growing memory bottlenecks due to the limited capacity and high cost of GPUs. While GPU memory offloading to CPU and NVMe has made large-scale training more feasible, existing approaches suffer from high tensor migration latency and suboptimal device memory utilization, ultimately increasing training time and cloud costs. To address these challenges, we present 10Cache, a resource-aware tensor caching and migration system that accelerates LLM training by intelligently coordinating memory usage across GPU, CPU, and NVMe tiers. 10Cache profiles tensor execution order to construct prefetch policies, allocates memory buffers in pinned memory based on tensor size distributions, and reuses memory buffers to minimize allocation overhead.   Designed for cloud-scale deployments, 10Cache improves memory efficiency and reduces reliance on high-end GPUs. Across diverse LLM workloads, it achieves up to 2x speedup in training time, improves GPU cache hit rate by up to 86.6x, and increases CPU/GPU memory utilization by up to 2.15x and 1.33x, respectively, compared to state-of-the-art offloading methods. These results demonstrate that 10Cache is a practical and scalable solution for optimizing LLM training throughput and resource efficiency in cloud environments.

**Link**: [arxiv](https://arxiv.org/abs/2511.14124v1),  [pdf](https://arxiv.org/pdf/2511.14124v1)

**Tags**: cs.DC cs.LG 



### Quantifying and Mitigating Selection Bias in LLMs: A Transferable LoRA Fine-Tuning and Efficient Majority Voting Approach
**Authors**: Blessed Guda, Lawrence Francis, Gabrial Zencha Ashungafac, Carlee Joe-Wong, Moise Busogi

**Updated**: 2025-11-17T21:31:37Z

**Summary**: Multiple Choice Question (MCQ) answering is a widely used method for evaluating the performance of Large Language Models (LLMs). However, LLMs often exhibit selection bias in MCQ tasks, where their choices are influenced by factors like answer position or option symbols rather than the content. This bias undermines the reliability of MCQ as an evaluation framework. Most existing selection bias metrics require answer labels and measure divergences between prediction and answer distributions, but do not fully capture the consistency of a model's predictions across different orderings of answer choices. Existing selection bias mitigation strategies have notable limitations: majority voting, though effective, is computationally prohibitive; calibration-based methods require validation sets and often fail to generalize across datasets. To address these gaps, we propose three key contributions: (1) a new unsupervised label-free Permutation Bias Metric (PBM) that directly quantifies inconsistencies in model predictions across answer permutations, providing a more precise measure of selection bias, (2) an efficient majority voting approach called Batch Question-Context KV caching (BaQCKV), to significantly reduce computational costs while preserving bias mitigation effectiveness, and (3) an unsupervised Low-Rank Adaptation (LoRA-1) fine-tuning strategy based on our proposed metric and the BaQCKV that mitigates selection bias, providing a computationally efficient alternative that maintains model generalizability. Experiments across multiple MCQ benchmarks demonstrate that our approaches reduce bias, increasing consistency in accuracy while minimizing computational costs.

**Link**: [arxiv](https://arxiv.org/abs/2511.21709v1),  [pdf](https://arxiv.org/pdf/2511.21709v1)

**Tags**: cs.CL cs.AI cs.IR 



### TZ-LLM: Protecting On-Device Large Language Models with Arm TrustZone
**Authors**: Xunjie Wang, Jiacheng Shi, Zihan Zhao, Yang Yu, Zhichao Hua, Jinyu Gu

**Updated**: 2025-11-17T18:59:20Z

**Summary**: Large Language Models (LLMs) deployed on mobile devices offer benefits like user privacy and reduced network latency, but introduce a significant security risk: the leakage of proprietary models to end users.   To mitigate this risk, we propose a system design for protecting on-device LLMs using Arm Trusted Execution Environment (TEE), TrustZone. Our system addresses two primary challenges: (1) The dilemma between memory efficiency and fast inference (caching model parameters within TEE memory). (2) The lack of efficient and secure Neural Processing Unit (NPU) time-sharing between Rich Execution Environment (REE) and TEE.   Our approach incorporates two key innovations. First, we employ pipelined restoration, leveraging the deterministic memory access patterns of LLM inference to prefetch parameters on demand, hiding memory allocation, I/O and decryption latency under computation time. Second, we introduce a co-driver design, creating a minimal data plane NPU driver in the TEE that collaborates with the full-fledged REE driver. This reduces the TEE TCB size and eliminates control plane reinitialization overhead during NPU world switches.   We implemented our system on the emerging OpenHarmony OS and the llama.cpp inference framework, and evaluated it with various LLMs on an Arm Rockchip device. Compared to a strawman TEE baseline lacking our optimizations, our system reduces TTFT by up to 90.9% and increases decoding speed by up to 23.2%.

**Link**: [arxiv](https://arxiv.org/abs/2511.13717v1),  [pdf](https://arxiv.org/pdf/2511.13717v1)

**Tags**: cs.CR 



### QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention
**Authors**: Hyunwoo Oh, Hanning Chen, Sanggeon Yun, Yang Ni, Wenjun Huang, Tamoghno Das, Suyeon Jang, Mohsen Imani

**Updated**: 2025-11-17T18:34:04Z

**Summary**: Deformable transformers deliver state-of-the-art detection but map poorly to hardware due to irregular memory access and low arithmetic intensity. We introduce QUILL, a schedule-aware accelerator that turns deformable attention into cache-friendly, single-pass work. At its core, Distance-based Out-of-Order Querying (DOOQ) orders queries by spatial proximity; the look-ahead drives a region prefetch into an alternate buffer--forming a schedule-aware prefetch loop that overlaps memory and compute. A fused MSDeformAttn engine executes interpolation, Softmax, aggregation, and the final projection (W''m) in one pass without spilling intermediates, while small tensors are kept on-chip and surrounding dense layers run on integrated GEMMs. Implemented as RTL and evaluated end-to-end, QUILL achieves up to 7.29x higher throughput and 47.3x better energy efficiency than an RTX 4090, and exceeds prior accelerators by 3.26-9.82x in throughput and 2.01-6.07x in energy efficiency. With mixed-precision quantization, accuracy tracks FP32 within <=0.9 AP across Deformable and Sparse DETR variants. By converting sparsity into locality--and locality into utilization--QUILL delivers consistent, end-to-end speedups.

**Link**: [arxiv](https://arxiv.org/abs/2511.13679v1),  [pdf](https://arxiv.org/pdf/2511.13679v1)

**Tags**: cs.AR cs.CV cs.LG 



### AutoSAGE: Input-Aware CUDA Scheduling for Sparse GNN Aggregation (SpMM/SDDMM) and CSR Attention
**Authors**: Aleksandar Stankovic

**Updated**: 2025-11-17T18:25:51Z

**Summary**: Sparse GNN aggregations (CSR SpMM/SDDMM) vary widely in performance with degree skew, feature width, and GPU micro-architecture. We present AutoSAGE, an input-aware CUDA scheduler that chooses tiling and mapping per input using a lightweight estimate refined by on-device micro-probes, with a guardrail that safely falls back to vendor kernels and a persistent cache for deterministic replay. AutoSAGE covers SpMM and SDDMM and composes into a CSR attention pipeline (SDDMM -> row-softmax -> SpMM). On Reddit and OGBN-Products, it matches vendor baselines at bandwidth-bound feature widths and finds gains at small widths; on synthetic sparsity and skew stress tests it achieves up to 4.7x kernel-level speedups. We release CUDA sources, Python bindings, a reproducible harness, and replayable cache logs.

**Link**: [arxiv](https://arxiv.org/abs/2511.17594v1),  [pdf](https://arxiv.org/pdf/2511.17594v1)

**Tags**: cs.LG cs.PF 



### CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding
**Authors**: Shrenik Patel, Daivik Patel

**Updated**: 2025-11-17T17:56:14Z

**Summary**: Long-form video question answering (VQA) overwhelms current vision-language models (VLMs) because attention and key-value (KV) caches grow with runtime, forcing either expensive inference or near-sighted sliding windows. We introduce CacheFlow, a training-free pipeline that pairs Dynamic Token Dropping (DTD) with a compressive long-term memory. DTD prunes per-patch tokens online via cosine similarity to the previous frame, and surviving tokens are packed into fixed-size blocks. This online, per-frame processing makes our approach fundamentally suited for live streaming VQA. As blocks are processed, each one's keys are summarized by a tiny recurrent encoder to form a retrieval index, while the block's full KV pairs are offloaded and later rehydrated for generation, preserving answer fidelity. At inference, a consensus-based retrieval mechanism retrieves only the Top-K most relevant blocks and attends over both the retrieved and local context for precise, long-range reasoning. CacheFlow is drop-in, architecture-agnostic, and requires no fine-tuning. Experiments on both offline and streaming VQA benchmarks demonstrate that CacheFlow outperforms current strong baselines, while processing up to 87% less tokens. Our dual approach enables VLMs to be both efficient and context-aware, paving the way for practical long-form video understanding.

**Link**: [arxiv](https://arxiv.org/abs/2511.13644v1),  [pdf](https://arxiv.org/pdf/2511.13644v1)

**Tags**: cs.CV 



### VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping
**Authors**: Haotian Dong, Ye Li, Rongwei Lu, Chen Tang, Shu-Tao Xia, Zhi Wang

**Updated**: 2025-11-17T16:50:58Z

**Summary**: Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its "draft one step, then verify one step" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.

**Link**: [arxiv](https://arxiv.org/abs/2511.13587v1),  [pdf](https://arxiv.org/pdf/2511.13587v1)

**Tags**: cs.CV cs.AI 



### Microwave-acoustic-driven power electronics
**Authors**: Liyang Jin, Zichen Xi, Joseph G. Thomas, Jun Ji, Yuanzhi Zhang, Nuo Chen, Yizheng Zhu, Linbo Shao, Liyan Zhu

**Updated**: 2025-11-17T14:25:37Z

**Summary**: Electrical isolation is critical to ensure safety and minimize electromagnetic interference (EMI), yet existing methods struggle to simultaneously transmit power and signals through a unified channel. Here we demonstrate a mechanically-isolated gate driver based on microwave-frequency surface acoustic wave (SAW) device on lithium niobate that achieves galvanic isolation of 2.75 kV with ultralow isolation capacitance (0.032 pF) over 1.25 mm mechanical propagation length, delivering 13.4 V open-circuit voltage and 44.4 mA short-circuit current. We demonstrate isolated gate driving for a gallium nitride (GaN) high-electron-mobility transistor, achieving a turn-on time of 108.8 ns comparable to commercial drivers and validate its operation in a buck converter. In addition, our SAW device operates over an ultrawide temperature range from 0.5 K (-272.6 °C) to 544 K (271 °C). The microwave-frequency SAW devices offer inherent EMI immunity and potential for heterogeneous integration on multiple semiconductor platforms, enabling compact, high-performance isolated power and signal transmission in advanced power electronics.

**Link**: [arxiv](https://arxiv.org/abs/2511.13412v1),  [pdf](https://arxiv.org/pdf/2511.13412v1)

**Tags**: eess.SY physics.app-ph 



### Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query
**Authors**: Yixuan Wang, Shiyu Ji, Yijun Liu, Yuzhuang Xu, Yang Xu, Qingfu Zhu, Wanxiang Che

**Updated**: 2025-11-17T13:29:25Z

**Summary**: Large language models (LLMs) rely on key-value cache (KV cache) to accelerate decoding by reducing redundant computations. However, the KV cache memory usage grows substantially with longer text sequences, posing challenges for efficient deployment. Existing KV cache eviction methods prune tokens using prefilling-stage attention scores, causing inconsistency with actual inference queries, especially under tight memory budgets. In this paper, we propose Lookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost pseudo lookahead queries to better approximate the true decoding-stage queries. By using these lookahead queries as the observation window for importance estimation, LAQ achieves more consistent and accurate KV cache eviction aligned with real inference scenarios. Experimental results on LongBench and Needle-in-a-Haystack benchmarks show that LAQ outperforms existing methods across various budget levels, achieving a 1 $\sim$ 4 point improvement on LongBench under limited cache budget. Moreover, LAQ is complementary to existing approaches and can be flexibly combined to yield further improvements.

**Link**: [arxiv](https://arxiv.org/abs/2505.20334v2),  [pdf](https://arxiv.org/pdf/2505.20334v2)

**Tags**: cs.CL cs.AI 



### Whistledown: Combining User-Level Privacy with Conversational Coherence in LLMs
**Authors**: Chelsea McMurray, Hayder Tirmazi

**Updated**: 2025-11-17T12:56:33Z

**Summary**: Users increasingly rely on large language models (LLMs) for personal, emotionally charged, and socially sensitive conversations. However, prompts sent to cloud-hosted models can contain personally identifiable information (PII) that users do not want logged, retained, or leaked. We observe this to be especially acute when users discuss friends, coworkers, or adversaries, i.e., when they spill the tea. Enterprises face the same challenge when they want to use LLMs for internal communication and decision-making.   In this whitepaper, we present Whistledown, a best-effort privacy layer that modifies prompts before they are sent to the LLM. Whistledown combines pseudonymization and $ε$-local differential privacy ($ε$-LDP) with transformation caching to provide best-effort privacy protection without sacrificing conversational utility. Whistledown is designed to have low compute and memory overhead, allowing it to be deployed directly on a client's device in the case of individual users. For enterprise users, Whistledown is deployed centrally within a zero-trust gateway that runs on an enterprise's trusted infrastructure. Whistledown requires no changes to the existing APIs of popular LLM providers.

**Link**: [arxiv](https://arxiv.org/abs/2511.13319v1),  [pdf](https://arxiv.org/pdf/2511.13319v1)

**Tags**: cs.CR cs.AI 



### Compress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers
**Authors**: Woomin Song, Sai Muralidhar Jayanthi, Srikanth Ronanki, Kanthashree Mysore Sathyendra, Jinwoo Shin, Aram Galstyan, Shubham Katiyar, Sravan Babu Bodapati

**Updated**: 2025-11-17T12:29:07Z

**Summary**: As large language models increasingly gain popularity in real-world applications, processing extremely long contexts, often exceeding the model's pre-trained context limits, has emerged as a critical challenge. While existing approaches to efficient long-context processing show promise, recurrent compression-based methods struggle with information preservation, whereas random access approaches require substantial memory resources. We introduce REFORM, a novel inference framework that efficiently handles long contexts through a two-phase approach. First, it incrementally processes input chunks while maintaining a compressed KV cache, constructs cross-layer context embeddings, and utilizes early exit strategy for improved efficiency. Second, it identifies and gathers essential tokens via similarity matching and selectively recomputes the KV cache. Compared to baselines, REFORM achieves over 52% and 34% performance gains on RULER and BABILong respectively at 1M context length. It also outperforms baselines on Infinite-Bench, RepoEval, and MM-NIAH, demonstrating flexibility across diverse tasks and domains. Additionally, REFORM reduces inference time by 30% and peak memory usage by 5%, achieving both efficiency and superior performance.

**Link**: [arxiv](https://arxiv.org/abs/2506.01215v2),  [pdf](https://arxiv.org/pdf/2506.01215v2)

**Tags**: cs.CL cs.LG 



### Hogwild! Inference: Parallel LLM Generation via Concurrent Attention
**Authors**: Gleb Rodionov, Roman Garipov, Alina Shutova, George Yakushev, Erik Schultheis, Vage Egiazarian, Anton Sinitsin, Denis Kuznedelev, Dan Alistarh

**Updated**: 2025-11-17T11:11:28Z

**Summary**: Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM "workers" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the LLM instances to come up with their own collaboration strategy for the problem at hand, all the while "seeing" each other's memory in the concurrent KV cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with "instant" access to each other's memory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning.

**Link**: [arxiv](https://arxiv.org/abs/2504.06261v4),  [pdf](https://arxiv.org/pdf/2504.06261v4)

**Tags**: cs.LG cs.CL 



### A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning
**Authors**: Liuyi Jin, Pasan Gunawardena, Amran Haroon, Runzhi Wang, Sangwoo Lee, Radu Stoleru, Michael Middleton, Zepeng Huo, Jeeeun Kim, Jason Moats

**Updated**: 2025-11-17T07:27:52Z

**Summary**: Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present EMSGlass, a smart-glasses system powered by EMSNet, the first multimodal multitask model for Emergency Medical Services (EMS), and EMSServe, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates text, vital signs, and scene images to construct a unified real-time understanding of EMS incidents. Trained on real-world multimodal EMS datasets, EMSNet simultaneously supports up to five critical EMS tasks with superior accuracy compared to state-of-the-art unimodal baselines. Built on top of PyTorch, EMSServe introduces a modality-aware model splitter and a feature caching mechanism, achieving adaptive and efficient inference across heterogeneous hardware while addressing the challenge of asynchronous modality arrival in the field. By optimizing multimodal inference execution in EMS scenarios, EMSServe achieves 1.9x -- 11.7x speedup over direct PyTorch multimodal inference. A user study evaluation with six professional EMTs demonstrates that EMSGlass enhances real-time situational awareness, decision-making speed, and operational efficiency through intuitive on-glass interaction. In addition, qualitative insights from the user study provide actionable directions for extending EMSGlass toward next-generation AI-enabled EMS systems, bridging multimodal intelligence with real-world emergency response workflows.

**Link**: [arxiv](https://arxiv.org/abs/2511.13078v1),  [pdf](https://arxiv.org/pdf/2511.13078v1)

**Tags**: cs.LG eess.AS eess.IV 



## Keyword: LLM Inference 
 ### Thinking by Doing: Building Efficient World Model Reasoning in LLMs via Multi-turn Interaction
**Authors**: Bao Shu, Yan Cai, Jianjian Sun, Chunrui Han, En Yu, Liang Zhao, Jingcheng Hu, Yinmin Zhang, Haoran Lv, Yuang Peng, Zheng Ge, Xiangyu Zhang, Daxin Jiang, Xiangyu Yue

**Updated**: 2025-11-28T18:59:47Z

**Summary**: Developing robust world model reasoning is crucial for large language model (LLM) agents to plan and interact in complex environments. While multi-turn interaction offers a superior understanding of environmental dynamics via authentic feedback, current approaches often impose a rigid reasoning process, which constrains the model's active learning, ultimately hindering efficient world model reasoning. To address these issues, we explore world-model internalization through efficient interaction and active reasoning (WMAct), which liberates the model from structured reasoning, allowing the model to shape thinking directly through its doing, and achieves effective and efficient world model reasoning with two key mechanisms: (1) a reward rescaling mechanism adjusting outcome reward based on action efficacy to incentivize redundancy reduction and purposeful interaction; (2) an interaction frequency annealing strategy to progressively reduce the maximum allowed interaction turns, which compels the model to condense its learning and internalize environmental dynamics rather than over-relying on environmental cues. Our experiments on Sokoban, Maze, and Taxi show that WMAct yields effective world model reasoning capable of resolving tasks in a single turn that previously required multiple interactions and fosters strong transferability to complex environments, improving performance on a suite of reasoning benchmarks.

**Link**: [arxiv](https://arxiv.org/abs/2511.23476v1),  [pdf](https://arxiv.org/pdf/2511.23476v1)

**Tags**: cs.AI 



### ThetaEvolve: Test-time Learning on Open Problems
**Authors**: Yiping Wang, Shao-Rong Su, Zhiyuan Zeng, Eva Xu, Liliang Ren, Xinyu Yang, Zeyi Huang, Xuehai He, Luyao Ma, Baolin Peng, Hao Cheng, Pengcheng He, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, Yelong Shen

**Updated**: 2025-11-28T18:58:14Z

**Summary**: Recent advances in large language models (LLMs) have enabled breakthroughs in mathematical discovery, exemplified by AlphaEvolve, a closed-source system that evolves programs to improve bounds on open problems. However, it relies on ensembles of frontier LLMs to achieve new bounds and is a pure inference system that models cannot internalize the evolving strategies. We introduce ThetaEvolve, an open-source framework that simplifies and extends AlphaEvolve to efficiently scale both in-context learning and Reinforcement Learning (RL) at test time, allowing models to continually learn from their experiences in improving open optimization problems. ThetaEvolve features a single LLM, a large program database for enhanced exploration, batch sampling for higher throughput, lazy penalties to discourage stagnant outputs, and optional reward shaping for stable training signals, etc. ThetaEvolve is the first evolving framework that enable a small open-source model, like DeepSeek-R1-0528-Qwen3-8B, to achieve new best-known bounds on open problems (circle packing and first auto-correlation inequality) mentioned in AlphaEvolve. Besides, across two models and four open tasks, we find that ThetaEvolve with RL at test-time consistently outperforms inference-only baselines, and the model indeed learns evolving capabilities, as the RL-trained checkpoints demonstrate faster progress and better final performance on both trained target task and other unseen tasks. We release our code publicly: https://github.com/ypwang61/ThetaEvolve

**Link**: [arxiv](https://arxiv.org/abs/2511.23473v1),  [pdf](https://arxiv.org/pdf/2511.23473v1)

**Tags**: cs.LG cs.CL 



### Spectral analysis of the Koopman operator recovers Hamiltonian parameters in open quantum systems
**Authors**: Jorge E. Pérez-García, Carlos Colchero, Julio C. Gutiérrez-Vega

**Updated**: 2025-11-28T18:57:15Z

**Summary**: An accurate identification of Hamiltonian parameters is essential for modeling and control of open quantum systems. In this work, a novel data-driven method for inferring such parameters from first-moment dynamics is presented using an open 2D quantum harmonic oscillator as an example. The method relies on the discrete spectrum of the Koopman operator to obtain such parameters, which is obtained using the multichannel Hankel Alternative View of Koopman (mHAVOK) algorithm; a theoretical connection of such affirmation is presented. The method is tested on the expected value of noiseless quadratures, retrieving oscillation frequencies, damping rates, nonlinear Kerr shifts, qubit-photon coupling strengths of a Jaynes-Cummings interaction, and a modulated frequency of a time-dependent Hamiltonian. The majority of the recovered parameters remained within 5% of their true values. When compared to Fourier and matrix-pencil estimators, our approach yields lower errors for dynamics with strong dissipation. Overall, these findings suggest that Koopman operator theory provides a practical framework for studying quantum dynamical systems.

**Link**: [arxiv](https://arxiv.org/abs/2511.23470v1),  [pdf](https://arxiv.org/pdf/2511.23470v1)

**Tags**: quant-ph math-ph 



### Kinetic Mixing and the Phantom Illusion: Axion-Dilaton Quintessence in Light of DESI DR2
**Authors**: Michael W. Toomey, Ellie Hughes, Mikhail M. Ivanov, James M. Sullivan

**Updated**: 2025-11-28T18:54:22Z

**Summary**: Recent results from DESI BAO analyses suggest that dark energy may not be a cosmological constant and is in fact dynamical. Furthermore, the data suggest that the equation of state may have been in the phantom regime in the distant past, recently undergoing a phantom crossing. In this work, we investigate whether this preference can be realized within a kinetically mixed axion-dilaton (KMIX) quintessence model, a string-motivated system in which an axion-like field couples exponentially to a dilaton-like (moduli) field. Crucially, KMIX can appear phantom in a standard Chevallier-Polarski-Linder (CPL) based analysis. To confront the model with data, we develop a fast pipeline based on normalizing flows that (i) learns a theory-informed prior on $(w_0,w_a)$ from KMIX realizations and (ii) provides an inverse mapping from CPL parameters back to the physical KMIX parameters. By importance-sampling pre-computed CPL chains using this framework, we effectively transform generic phenomenological constraints into direct, computationally efficient constraints on the underlying KMIX theory, avoiding the prohibitive cost of full parameter space exploration. Applied to Planck+DESI DR2 BAO measurements, our framework finds support for KMIX at $2.5σ$ compared to the base CPL fit at $3.1σ$, demonstrating that KMIX may account for the DESI preference without invoking true phantom behavior. When additionally including Type Ia supernovae data, we find that the preference remains above $3σ$ for Union3 and DES Y5, but drops to $2.1σ$ with Pantheon+. The latter, combined with the DESI full-shape power spectrum and bispectrum data, further reduces the preference to $1.7σ$. Ultimately, should the DESI deviation persist with future data, KMIX may offer a theoretically well-motivated explanation for the phantom-like signatures inferred from phenomenological fits.

**Link**: [arxiv](https://arxiv.org/abs/2511.23463v1),  [pdf](https://arxiv.org/pdf/2511.23463v1)

**Tags**: astro-ph.CO hep-ph 



### NegBLEURT Forest: Leveraging Inconsistencies for Detecting Jailbreak Attacks
**Authors**: Lama Sleem, Jerome Francois, Lujun Li, Nathan Foucher, Niccolo Gentile, Radu State

**Updated**: 2025-11-28T18:49:16Z

**Summary**: Jailbreak attacks designed to bypass safety mechanisms pose a serious threat by prompting LLMs to generate harmful or inappropriate content, despite alignment with ethical guidelines. Crafting universal filtering rules remains difficult due to their inherent dependence on specific contexts. To address these challenges without relying on threshold calibration or model fine-tuning, this work introduces a semantic consistency analysis between successful and unsuccessful responses, demonstrating that a negation-aware scoring approach captures meaningful patterns. Building on this insight, a novel detection framework called NegBLEURT Forest is proposed to evaluate the degree of alignment between outputs elicited by adversarial prompts and expected safe behaviors. It identifies anomalous responses using the Isolation Forest algorithm, enabling reliable jailbreak detection. Experimental results show that the proposed method consistently achieves top-tier performance, ranking first or second in accuracy across diverse models using the crafted dataset, while competing approaches exhibit notable sensitivity to model and data variations.

**Link**: [arxiv](https://arxiv.org/abs/2511.11784v2),  [pdf](https://arxiv.org/pdf/2511.11784v2)

**Tags**: cs.CR cs.AI 



### The Price of Progress: Algorithmic Efficiency and the Falling Cost of AI Inference
**Authors**: Hans Gundlach, Jayson Lynch, Matthias Mertens, Neil Thompson

**Updated**: 2025-11-28T18:47:33Z

**Summary**: Language models have seen enormous progress on advanced benchmarks in recent years, but much of this progress has only been possible by using more costly models. Benchmarks may therefore present a warped picture of progress in practical capabilities per dollar. To remedy this, we use data from Artificial Analysis and Epoch AI to form the largest dataset of current and historical prices to run benchmarks to date. We find that the price for a given level of benchmark performance has decreased remarkably fast, around $5\times$ to $10\times$ per year, for frontier models on knowledge, reasoning, math, and software engineering benchmarks. These reductions in the cost of AI inference are due to economic forces, hardware efficiency improvements, and algorithmic efficiency improvements. Isolating out open models to control for competition effects and dividing by hardware price declines, we estimate that algorithmic efficiency progress is around $3\times$ per year. Finally, we recommend that evaluators both publicize and take into account the price of benchmarking as an essential part of measuring the real-world impact of AI.

**Link**: [arxiv](https://arxiv.org/abs/2511.23455v1),  [pdf](https://arxiv.org/pdf/2511.23455v1)

**Tags**: cs.LG cs.AI cs.CY 



### Accelerated Execution of Bayesian Neural Networks using a Single Probabilistic Forward Pass and Code Generation
**Authors**: Bernhard Klein, Falk Selker, Hendrik Borras, Sophie Steger, Franz Pernkopf, Holger Fröning

**Updated**: 2025-11-28T18:35:20Z

**Summary**: Machine learning models perform well across domains such as diagnostics, weather forecasting, NLP, and autonomous driving, but their limited uncertainty handling restricts use in safety-critical settings. Traditional neural networks often fail to detect out-of-domain (OOD) data and may output confident yet incorrect predictions. Bayesian neural networks (BNNs) address this by providing probabilistic estimates, but incur high computational cost because predictions require sampling weight distributions and multiple forward passes. The Probabilistic Forward Pass (PFP) offers a highly efficient approximation to Stochastic Variational Inference (SVI) by assuming Gaussian-distributed weights and activations, enabling fully analytic uncertainty propagation and replacing sampling with a single deterministic forward pass. We present an end-to-end pipeline for training, compiling, optimizing, and deploying PFP-based BNNs on embedded ARM CPUs. Using the TVM deep learning compiler, we implement a dedicated library of Gaussian-propagating operators for multilayer perceptrons and convolutional neural networks, combined with manual and automated tuning strategies. Ablation studies show that PFP consistently outperforms SVI in computational efficiency, achieving speedups of up to 4200x for small mini-batches. PFP-BNNs match SVI-BNNs on Dirty-MNIST in accuracy, uncertainty estimation, and OOD detection while greatly reducing compute cost. These results highlight the potential of combining Bayesian approximations with code generation to enable efficient BNN deployment on resource-constrained systems.

**Link**: [arxiv](https://arxiv.org/abs/2511.23440v1),  [pdf](https://arxiv.org/pdf/2511.23440v1)

**Tags**: cs.LG cs.AR cs.DC stat.ML 



### Towards Continuous Intelligence Growth: Self-Training, Continual Learning, and Dual-Scale Memory in SuperIntelliAgent
**Authors**: Jianzhe Lin, Zeyu Pan, Yun Zhu, Ruiqi Song, Jining Yang

**Updated**: 2025-11-28T18:32:49Z

**Summary**: We introduce SuperIntelliAgent, an agentic learning framework that couples a trainable small diffusion model (the learner) with a frozen large language model (the verifier) to enable continual intelligence growth through self-supervised interaction. Unlike conventional supervised fine-tuning, SuperIntelliAgent learns autonomously without annotation: the learner generates candidate outputs, the verifier evaluates them through step-by-step reasoning, and their interaction produces chosen/rejected pairs for Direct Preference Optimization (DPO). This converts each input into a pseudo-training signal for continual improvement. The framework integrates dual-scale memory: short-term in-context memory that preserves reasoning traces across refinement cycles, and long-term memory that consolidates acquired knowledge through lightweight on-the-fly fine-tuning. A replay buffer retains samples that show verifiable progress and replays them as auxiliary supervision, reinforcing recent learning while forming adaptive curricula. SuperIntelliAgent is infrastructure-agnostic and can be plugged into existing agentic frameworks while turning ordinary inference loops into a lifelong optimization process. We posit that pairing a trainable learner with a reasoning-capable verifier forms a minimal reliable unit of growing intelligence, as paired feedback and partial-history replay yield richer learning curricula and stronger preference alignment. With a small number of automatically generated DPO pairs, the learner improves across all benchmarks, indicating that this mechanism provides a promising direction for continual intelligence accumulation and real-world deployment.

**Link**: [arxiv](https://arxiv.org/abs/2511.23436v1),  [pdf](https://arxiv.org/pdf/2511.23436v1)

**Tags**: cs.AI 



### Getting it right: Methods for risk ratios and risk differences cluster randomized trials with a small number of clusters
**Authors**: Shifeng Sun, Xueqi Wang, Zhuoran Hou, Elizabeth L. Turner

**Updated**: 2025-11-28T18:18:10Z

**Summary**: Most cluster randomized trials (CRTs) randomize fewer than 30-40 clusters in total. When performing inference for such ``small'' CRTs, it is important to use methods that appropriately account for the small sample size. When the generalized estimating equations (GEE) approach is used for analysis of ``small'' CRTs, the robust variance estimator from GEE is biased downward and therefore bias-corrected standard errors should be used. Moreover, in order to avoid inflated Type I error, an appropriate bias-corrected standard error should be paired with the t- rather than Z-statistic when making inference about a single-parameter intervention effect. Although several bias-correction methods (including Kauermann and Carroll (KC), Mancl and DeRouen (MD), Morel, Bokossa, and Neerchal (MBN), and the average of KC and MD (AVG)) have been evaluated for inference for odds ratios, their finite-sample behavior in ``small'' CRTs with few clusters has not been thoroughly investigated for risk ratios and risk differences. The current article aims to fill the gap by including analysis via binomial, Poisson and Gaussian models and for a broad spectrum of scenarios. Analysis is via binomial and Poisson models (using log and identity link for risk and differences measures, respectively). We additionally explore the use of Gaussian models with identity link for risk differences and adopt the "modified" approach for analysis with misspecified Poisson and Gaussian models. We consider a broad spectrum of scenarios including for rare outcomes, small cluster sizes, high intracluster correlations (ICCs), and high coefficients of variation (CVs) of cluster size.

**Link**: [arxiv](https://arxiv.org/abs/2511.23419v1),  [pdf](https://arxiv.org/pdf/2511.23419v1)

**Tags**: stat.ME 



### Detection of the Pairwise Kinematic Sunyaev-Zel'dovich Effect and Pairwise Velocity with DESI DR1 Galaxies and ACT DR6 and Planck CMB Data
**Authors**: Yulin Gong, Patricio A. Gallardo, Rachel Bean, Jenna Moore, Eve M. Vavagiakis, Nicholas Battaglia, Boryana Hadzhiyska, Yun-Hsin Hsu, Jessica Nicole Aguilar, Steven Ahlen, Davide Bianchi, David Brooks, Todd Claybaugh, Rebecca Canning, Mark Devlin, Peter Doel, Axel de la Macorra, Simone Ferraro, Andreu Font-Ribera, Jaime E. Forero-Romero, Enrique Gaztañaga, Gaston Gutierrez, Satya Gontcho A Gontcho, Julien Guy, Klaus Honscheid, Cullan Howlett, R. Henry Liu, Mustapha Ishak, Dick Joyce, Anthony Kremin, Claire Lamman, Michael Levi, Martin Landriau, Marc Manera, Aaron Meisner, Ramon Miquel, Michael D. Niemack, Seshadri Nadathur, Will Percival, Francisco Prada, Graziano Rossi, Bernardita Ried Guachalla, Eusebio Sanchez, Hee-Jong Seo, David Sprayberry, David Schlegel, Cristóbal Sifón, Michael Schubnell, Joseph Harry Silber, Gregory Tarlé, Benjamin Alan Weaver, Rongpu Zhou, Hu Zou

**Updated**: 2025-11-28T18:12:40Z

**Summary**: We present a 9.3-sigma detection of the pairwise kinematic Sunyaev-Zeldovich (kSZ) effect by combining a sample of 913,286 Luminous Red Galaxies (LRGs) from the Dark Energy Spectroscopic Instrument Data Release 1 (DESI DR1) catalog and co-added Atacama Cosmology Telescope (ACT DR6) and Planck cosmic microwave background (CMB) temperature maps. This represents the highest-significance pairwise kSZ measurement to date. The analysis uses three ACT CMB temperature maps: co-added 150 GHz, total frequency maps, and a component-separated Internal Linear Combination (ILC) map, all of which cover 19,000 square degrees of the sky from Advanced ACTPol observations conducted between 2017 and 2022. Comparison of the results of these three maps serves as a consistency check for potential foreground contamination that may depend on the observation frequency. An estimate of the best-fit mass-averaged optical depth is obtained by comparing the pairwise kSZ curve with the linear-theory prediction of the pairwise velocity under the best-fit Planck cosmology, and is compared with predictions from simulations. This estimate serves as a reference point for future comparisons with thermal SZ-derived optical depth measurements for the same DESI cluster samples, which will be presented in a companion paper. Finally, we employ a machine-learning approach trained on simulations to estimate the optical depth for 456,803 DESI LRG-identified clusters within the simulated mass range (greater than about 1e13 solar masses). These are combined with the measured kSZ signal to infer the individual cluster peculiar velocities, providing the opportunity to constrain the behavior of gravity and the dark sector over a range of cosmic scales and epochs.

**Link**: [arxiv](https://arxiv.org/abs/2511.23417v1),  [pdf](https://arxiv.org/pdf/2511.23417v1)

**Tags**: astro-ph.CO 



### Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards
**Authors**: Charles Arnal, Gaëtan Narozniak, Vivien Cabannes, Yunhao Tang, Julia Kempe, Remi Munos

**Updated**: 2025-11-28T18:12:20Z

**Summary**: Reinforcement learning (RL) is increasingly used to align large language models (LLMs). Off-policy methods offer greater implementation simplicity and data efficiency than on-policy techniques, but often result in suboptimal performance. In this work, we study the intermediate range of algorithms between off-policy RL and supervised fine-tuning by analyzing a simple off-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with $r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$ emphasizes high-reward samples, while raising it penalizes low-reward ones more heavily. We first provide a theoretical analysis of this off-policy REINFORCE algorithm, showing that when the baseline $V$ lower-bounds the expected reward, the algorithm enjoys a policy improvement guarantee. Our analysis reveals that while on-policy updates can safely leverage both positive and negative signals, off-policy updates benefit from focusing more on positive rewards than on negative ones. We validate our findings experimentally in a controlled stochastic bandit setting and through fine-tuning state-of-the-art LLMs on reasoning tasks.

**Link**: [arxiv](https://arxiv.org/abs/2506.20520v2),  [pdf](https://arxiv.org/pdf/2506.20520v2)

**Tags**: cs.LG cs.CL 



### Evaluating LLMs for One-Shot Patching of Real and Artificial Vulnerabilities
**Authors**: Aayush Garg, Zanis Ali Khan, Renzo Degiovanni, Qiang Tang

**Updated**: 2025-11-28T18:03:47Z

**Summary**: Automated vulnerability patching is crucial for software security, and recent advancements in Large Language Models (LLMs) present promising capabilities for automating this task. However, existing research has primarily assessed LLMs using publicly disclosed vulnerabilities, leaving their effectiveness on related artificial vulnerabilities largely unexplored. In this study, we empirically evaluate the patching effectiveness and complementarity of several prominent LLMs, such as OpenAI's GPT variants, LLaMA, DeepSeek, and Mistral models, using both real and artificial vulnerabilities. Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities. Our results reveal that LLMs patch real vulnerabilities more effectively compared to artificial ones. Additionally, our analysis reveals significant variability across LLMs in terms of overlapping (multiple LLMs patching the same vulnerabilities) and complementarity (vulnerabilities patched exclusively by a single LLM), emphasizing the importance of selecting appropriate LLMs for effective vulnerability patching.

**Link**: [arxiv](https://arxiv.org/abs/2511.23408v1),  [pdf](https://arxiv.org/pdf/2511.23408v1)

**Tags**: cs.CR cs.AI cs.SE 



### LFM2 Technical Report
**Authors**: Alexander Amini, Anna Banaszak, Harold Benoit, Arthur Böök, Tarek Dakhran, Song Duong, Alfred Eng, Fernando Fernandes, Marc Härkönen, Anne Harrington, Ramin Hasani, Saniya Karwa, Yuri Khrustalev, Maxime Labonne, Mathias Lechner, Valentine Lechner, Simon Lee, Zetian Li, Noel Loo, Jacob Marks, Edoardo Mosca, Samuel J. Paech, Paul Pak, Rom N. Parnichkun, Alex Quach, Ryan Rogers, Daniela Rus, Nayan Saxena, Bettina Schlager, Tim Seyde, Jimmy T. H. Smith, Aditya Tadimeti, Neehal Tumma

**Updated**: 2025-11-28T17:56:35Z

**Summary**: We present LFM2, a family of Liquid Foundation Models designed for efficient on-device deployment and strong task capabilities. Using hardware-in-the-loop architecture search under edge latency and memory constraints, we obtain a compact hybrid backbone that combines gated short convolutions with a small number of grouped query attention blocks, delivering up to 2x faster prefill and decode on CPUs compared to similarly sized models. The LFM2 family covers 350M-8.3B parameters, including dense models (350M, 700M, 1.2B, 2.6B) and a mixture-of-experts variant (8.3B total, 1.5B active), all with 32K context length. LFM2's training pipeline includes a tempered, decoupled Top-K knowledge distillation objective that avoids support mismatch; curriculum learning with difficulty-ordered data; and a three-stage post-training recipe of supervised fine-tuning, length-normalized preference optimization, and model merging. Pre-trained on 10-12T tokens, LFM2 models achieve strong results across diverse benchmarks; for example, LFM2-2.6B reaches 79.56% on IFEval and 82.41% on GSM8K. We further build multimodal and retrieval variants: LFM2-VL for vision-language tasks, LFM2-Audio for speech, and LFM2-ColBERT for retrieval. LFM2-VL supports tunable accuracy-latency tradeoffs via token-efficient visual processing, while LFM2-Audio separates audio input and output pathways to enable real-time speech-to-speech interaction competitive with models 3x larger. LFM2-ColBERT provides a low-latency encoder for queries and documents, enabling high-performance retrieval across multiple languages. All models are released with open weights and deployment packages for ExecuTorch, llama.cpp, and vLLM, making LFM2 a practical base for edge applications that need fast, memory-efficient inference and strong task capabilities.

**Link**: [arxiv](https://arxiv.org/abs/2511.23404v1),  [pdf](https://arxiv.org/pdf/2511.23404v1)

**Tags**: cs.LG cs.AI 



### Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization
**Authors**: Filip Sondej, Yushi Yang, Mikołaj Kniejski, Marcel Windys

**Updated**: 2025-11-28T17:35:49Z

**Summary**: Language models can retain dangerous knowledge and skills even after extensive safety fine-tuning, posing both misuse and misalignment risks. Recent studies show that even specialized unlearning methods can be easily reversed. To address this, we systematically evaluate many existing and novel components of unlearning methods and identify ones crucial for irreversible unlearning.   We introduce Disruption Masking, a technique in which we only allow updating weights, where the signs of the unlearning gradient and the retaining gradient are the same. This ensures all updates are non-disruptive.   Additionally, we identify the need for normalizing the unlearning gradients, and also confirm the usefulness of meta-learning. We combine these insights into MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and validate its effectiveness at preventing the recovery of dangerous capabilities. MUDMAN outperforms the prior TAR method by 40%, setting a new state-of-the-art for robust unlearning.

**Link**: [arxiv](https://arxiv.org/abs/2506.12484v5),  [pdf](https://arxiv.org/pdf/2506.12484v5)

**Tags**: cs.LG cs.AI cs.CL 



### Bounded-Error Quantum Simulation via Hamiltonian and Lindbladian Learning
**Authors**: Tristan Kraft, Manoj K. Joshi, William Lam, Tobias Olsacher, Florian Kranzl, Johannes Franke, Lata Kh Joshi, Rainer Blatt, Augusto Smerzi, Daniel Stilck França, Benoît Vermersch, Barbara Kraus, Christian F. Roos, Peter Zoller

**Updated**: 2025-11-28T17:33:59Z

**Summary**: Analog Quantum Simulators offer a route to exploring strongly correlated many-body dynamics beyond classical computation, but their predictive power remains limited by the absence of quantitative error estimation. Establishing rigorous uncertainty bounds is essential for elevating such devices from qualitative demonstrations to quantitative scientific tools. Here we introduce a general framework for bounded-error quantum simulation, which provides predictions for many-body observables with experimentally quantifiable uncertainties. The approach combines Hamiltonian and Lindbladian Learning--a statistically rigorous inference of the coherent and dissipative generators governing the dynamics--with the propagation of their uncertainties into the simulated observables, yielding confidence bounds directly derived from experimental data. We demonstrate this framework on trapped-ion quantum simulators implementing long-range Ising interactions with up to 51 ions, and validate it where classical comparison is possible. We analyze error bounds on two levels. First, we learn an open-system model from experimental data collected in an initial time window of quench dynamics, simulate the corresponding master equation, and quantitatively verify consistency between theoretical predictions and measured dynamics at long times. Second, we establish error bounds directly from experimental measurements alone, without relying on classical simulation--crucial for entering regimes of quantum advantage. The learned models reproduce the experimental evolution within the predicted bounds, demonstrating quantitative reliability and internal consistency. Bounded-error quantum simulation provides a scalable foundation for trusted analog quantum computation, bridging the gap between experimental platforms and predictive many-body physics. The techniques presented here directly extend to digital quantum simulation.

**Link**: [arxiv](https://arxiv.org/abs/2511.23392v1),  [pdf](https://arxiv.org/pdf/2511.23392v1)

**Tags**: quant-ph 



### Hierarchical AI-Meteorologist: LLM-Agent System for Multi-Scale and Explainable Weather Forecast Reporting
**Authors**: Daniil Sukhorukov, Andrei Zakharov, Nikita Glazkov, Katsiaryna Yanchanka, Vladimir Kirilin, Maxim Dubovitsky, Roman Sultimov, Yuri Maksimov, Ilya Makarov

**Updated**: 2025-11-28T17:27:06Z

**Summary**: We present the Hierarchical AI-Meteorologist, an LLM-agent system that generates explainable weather reports using a hierarchical forecast reasoning and weather keyword generation. Unlike standard approaches that treat forecasts as flat time series, our framework performs multi-scale reasoning across hourly, 6-hour, and daily aggregations to capture both short-term dynamics and long-term trends. Its core reasoning agent converts structured meteorological inputs into coherent narratives while simultaneously extracting a few keywords effectively summarizing the dominant meteorological events. These keywords serve as semantic anchors for validating consistency, temporal coherence and factual alignment of the generated reports. Using OpenWeather and Meteostat data, we demonstrate that hierarchical context and keyword-based validation substantially improve interpretability and robustness of LLM-generated weather narratives, offering a reproducible framework for semantic evaluation of automated meteorological reporting and advancing agent-based scientific reasoning.

**Link**: [arxiv](https://arxiv.org/abs/2511.23387v1),  [pdf](https://arxiv.org/pdf/2511.23387v1)

**Tags**: cs.AI 



### PLRD: Partially Linear Regression Discontinuity Inference
**Authors**: Aditya Ghosh, Guido Imbens, Stefan Wager

**Updated**: 2025-11-28T17:23:05Z

**Summary**: Regression discontinuity designs have become one of the most popular research designs in empirical economics. We argue, however, that widely used approaches to building confidence intervals in regression discontinuity designs exhibit suboptimal behavior in practice: In a simulation study calibrated to high-profile applications of regression discontinuity designs, existing methods either have systematic under-coverage or have wider-than-necessary intervals. We propose a new approach, partially linear regression discontinuity inference (PLRD), and find it to address shortcomings of existing methods: Throughout our experiments, confidence intervals built using PLRD are both valid and short. We also provide large-sample guarantees for PLRD under smoothness assumptions.

**Link**: [arxiv](https://arxiv.org/abs/2503.09907v2),  [pdf](https://arxiv.org/pdf/2503.09907v2)

**Tags**: econ.EM stat.ME 



### AugGen: Augmenting Task-Based Learning in Professional Creative Software with LLM-Generated Scaffolded UIs
**Authors**: Yimeng Liu, Misha Sra

**Updated**: 2025-11-28T17:22:43Z

**Summary**: Professional creative software often presents steep learning curves due to complex interfaces, lack of structured task-aware guidance, and unfamiliar domain terminology. To address these challenges and augment user learning experience, we introduce AugGen, a method for generating scaffolded user interfaces that simplify interface complexity and support task-based learning. With the user's task, our method surfaces task-relevant tools to reduce distracting features, organizes the tools around task workflow stages to offer execution guidance, connects tools with domain concepts to foster learning engagement, and progressively discloses advanced features to manage learning progress. To evaluate the method, we used our LLM-assisted pipeline to generate two task-specific scaffolded UIs and deployed them in Blender, our professional 3D modeling testbed. We invited both beginner (N=32) and expert (N=8) users to evaluate our implemented interfaces. Results show that the scaffolded interfaces significantly reduced user-perceived task load, enhanced task performance via embedded guidance, and augmented concept learning during task execution.

**Link**: [arxiv](https://arxiv.org/abs/2511.23379v1),  [pdf](https://arxiv.org/pdf/2511.23379v1)

**Tags**: cs.HC 



### Is Passive Expertise-Based Personalization Enough? A Case Study in AI-Assisted Test-Taking
**Authors**: Li Siyan, Jason Zhang, Akash Maharaj, Yuanming Shi, Yunyao Li

**Updated**: 2025-11-28T17:21:41Z

**Summary**: Novice and expert users have different systematic preferences in task-oriented dialogues. However, whether catering to these preferences actually improves user experience and task performance remains understudied. To investigate the effects of expertise-based personalization, we first built a version of an enterprise AI assistant with passive personalization. We then conducted a user study where participants completed timed exams, aided by the two versions of the AI assistant. Preliminary results indicate that passive personalization helps reduce task load and improve assistant perception, but reveal task-specific limitations that can be addressed through providing more user agency. These findings underscore the importance of combining active and passive personalization to optimize user experience and effectiveness in enterprise task-oriented environments.

**Link**: [arxiv](https://arxiv.org/abs/2511.23376v1),  [pdf](https://arxiv.org/pdf/2511.23376v1)

**Tags**: cs.HC cs.CL 



### Functional Program Synthesis with Higher-Order Functions and Recursion Schemes
**Authors**: Matheus Campos Fernandes

**Updated**: 2025-11-28T17:02:01Z

**Summary**: Program synthesis is the process of generating a computer program following a set of specifications, such as a set of input-output examples. It can be modeled as a search problem in which the search space is the set of all valid programs. As the search space is vast, brute force is usually not feasible, and search heuristics, such as genetic programming, also have difficulty navigating it without guidance. This text presents 2 novel GP algorithms that synthesize pure, typed, and functional programs: HOTGP and Origami. HOTGP uses strong types and a functional grammar, synthesizing Haskell code, with support for higher-order functions, $λ$-functions, and parametric polymorphism. Experimental results show that HOTGP is competitive with the state of the art. Additionally, Origami is an algorithm that tackles the challenge of effectively handling loops and recursion by exploring Recursion Schemes, in which the programs are composed of well-defined templates with only a few parts that need to be synthesized. The first implementation of Origami can synthesize solutions in several Recursion Schemes and data structures, being competitive with other GP methods in the literature, as well as LLMs. The latest version of Origami employs a novel procedure, called AC/DC, designed to improve the search-space exploration. It achieves considerable improvement over its previous version by raising success rates on every problem. Compared to similar methods in the literature, it has the highest count of problems solved with success rates of $100\%$, $\geq 75\%$, and $\geq 25\%$ across all benchmarks. In $18\%$ of all benchmark problems, it stands as the only method to reach $100\%$ success rate, being the first known approach to achieve it on any problem in PSB2. It also demonstrates competitive performance to LLMs, achieving the highest overall win-rate against Copilot among all GP methods.

**Link**: [arxiv](https://arxiv.org/abs/2511.23354v1),  [pdf](https://arxiv.org/pdf/2511.23354v1)

**Tags**: cs.NE 



### ParaGate: Parasitic-Driven Domain Adaptation Transfer Learning for Netlist Performance Prediction
**Authors**: Bin Sun, Jingyi Zhou, Jianan Mu, Zhiteng Chao, Tianmeng Yang, Ziyue Xu, Jing Ye, Huawei Li

**Updated**: 2025-11-28T16:49:25Z

**Summary**: In traditional EDA flows, layout-level performance metrics are only obtainable after placement and routing, hindering global optimization at earlier stages. Although some neural-network-based solutions predict layout-level performance directly from netlists, they often face generalization challenges due to the black-box heuristics of commercial placement-and-routing tools, which create disparate data across designs. To this end, we propose ParaGate, a three-step cross-stage prediction framework that infers layout-level timing and power from netlists. First, we propose a two-phase transfer-learning approach to predict parasitic parameters, pre-training on mid-scale circuits and fine-tuning on larger ones to capture extreme conditions. Next, we rely on EDA tools for timing analysis, offloading the long-path numerical reasoning. Finally, ParaGate performs global calibration using subgraph features. Experiments show that ParaGate achieves strong generalization with minimal fine-tuning data: on openE906, its arrival-time R2 from 0.119 to 0.897. These results demonstrate that ParaGate could provide guidance for global optimization in the synthesis and placement stages.

**Link**: [arxiv](https://arxiv.org/abs/2511.23340v1),  [pdf](https://arxiv.org/pdf/2511.23340v1)

**Tags**: cs.LG cs.AI 



### LLMAID: Identifying AI Capabilities in Android Apps with LLMs
**Authors**: Pei Liu, Terry Zhuo, Jiawei Deng, Thong James, Shidong Pan, Sherry Xu, Zhenchang Xing, Qinghua Lu, Xiaoning Du, Hongyu Zhang

**Updated**: 2025-11-28T16:36:03Z

**Summary**: Recent advancements in artificial intelligence (AI) and its widespread integration into mobile software applications have received significant attention, highlighting the growing prominence of AI capabilities in modern software systems. However, the inherent hallucination and reliability issues of AI continue to raise persistent concerns. Consequently, application users and regulators increasingly ask critical questions such as: Does the application incorporate AI capabilities? and What specific types of AI functionalities are embedded? Preliminary efforts have been made to identify AI capabilities in mobile software; however, existing approaches mainly rely on manual inspection and rule-based heuristics. These methods are not only costly and time-consuming but also struggle to adapt advanced AI techniques.   To address the limitations of existing methods, we propose LLMAID (Large Language Model for AI Discovery). LLMAID includes four main tasks: (1) candidate extraction, (2) knowledge base interaction, (3) AI capability analysis and detection, and (4) AI service summarization. We apply LLMAID to a dataset of 4,201 Android applications and demonstrate that it identifies 242% more real-world AI apps than state-of-the-art rule-based approaches. Our experiments show that LLM4AID achieves high precision and recall, both exceeding 90%, in detecting AI-related components. Additionally, a user study indicates that developers find the AI service summaries generated by LLMAID to be more informative and preferable to the original app descriptions. Finally, we leverage LLMAID to perform an empirical analysis of AI capabilities across Android apps. The results reveal a strong concentration of AI functionality in computer vision (54.80%), with object detection emerging as the most common task (25.19%).

**Link**: [arxiv](https://arxiv.org/abs/2511.19059v2),  [pdf](https://arxiv.org/pdf/2511.19059v2)

**Tags**: cs.SE 



### Comment on "Repair of DNA Double-Strand Breaks Leaves Heritable Impairment to Genome Function"
**Authors**: Yi Wang, Shu-Feng Zhou

**Updated**: 2025-11-28T16:22:46Z

**Summary**: Bantele and colleagues recently reported that repair of a single CRISPR/Cas9-induced DNA double-strand break (DSB) in the c-MYC topologically associated domain leads to a persistent depletion of chromatin interactions and long-term transcriptional attenuation across multiple generations of human cells. They interpret this observation as evidence for a previously unrecognized principle--"chromatin fatigue"--in which DSB repair generates a stable architectural defect that acts as a heritable impairment to genome function. Such an idea, if correct, would carry profound implications for genome biology, epigenetic inheritance, cancer evolution, aging, and the safety of therapeutic genome editing. However, our detailed reassessment of the experimental design, underlying assumptions, and data interpretation reveals that the evidence provided is inadequate to support these sweeping conclusions. Instead, the observed outcomes are more plausibly explained by a combination of Cas9 persistence, off-target DNA damage, repair-factor retention, MYC enhancer plasticity, and the well-documented genomic instability of HeLa cells. The study does not demonstrate mechanistic causality, does not exclude simpler explanations, and does not provide data consistent with true chromatin memory or heritable architectural change. Moreover, its statistical inferences are based on noisy measurements that fall within expected variability of unstable oncogenic loci. Here, we present a comprehensive critical analysis showing that the proposed model of chromatin fatigue is unsupported by the available evidence. We offer a corrected interpretation in which the chromatin landscape experiences a temporary, repair-associated perturbation that resolves without leaving enduring or heritable impairment.

**Link**: [arxiv](https://arxiv.org/abs/2511.11986v2),  [pdf](https://arxiv.org/pdf/2511.11986v2)

**Tags**: q-bio.CB q-bio.SC 



### Impact of Resonance, Raman, and Thomson Scattering on Hydrogen Line Formation in Little Red Dots
**Authors**: Seok-Jun Chang, Max Gronke, Jorryt Matthee, Charlotte Mason

**Updated**: 2025-11-28T16:21:25Z

**Summary**: Little Red Dots (LRDs) are compact sources at $z>5$ discovered through JWST spectroscopy. Their spectra exhibit broad Balmer emission lines ($\gtrsim1000\rm~km~s^{-1}$), alongside absorption features and a pronounced Balmer break -- evidence for a dense, neutral hydrogen medium with the $n=2$ state. When interpreted as arising from AGN broad-line regions, inferred black hole masses from local scaling relations exceed expectations given their stellar masses, challenging models of early black hole-galaxy co-evolution. However, radiative transfer effects in dense media may also impact the formation of hydrogen emission lines. We model three scattering processes shaping hydrogen line profiles: resonance scattering by hydrogen in the $n=2$ state, Raman scattering of UV radiation by ground-state hydrogen, and Thomson scattering by free electrons. Using 3D Monte Carlo radiative transfer simulations with multi-branching resonance transitions, we examine their imprint on line shapes and ratios. Resonance scattering produces strong deviations from Case B flux ratios, clear differences between H$α$ and H$β$, and encodes gas kinematics in line profiles but cannot broaden H$β$ due to conversion to Pa$α$. While Raman scattering can yield broad wings, scattering of UV continuum is disfavored given the absence of strong FWHM variations across transitions. Raman scattering of higher Lyman-series emission can produce H$α$/H$β$ wing width ratios of $\gtrsim1.28$, agreeing with observations. Thomson scattering can reproduce the observed $\gtrsim1000~\rm km\, s^{-1}$ wings under plausible conditions, e.g., $T_{\rm e} \sim 10^4\rm \, K$ and $N_{\rm e}\sim10^{24}\rm~cm^{-2}$ -- and lead to black hole mass overestimates by factors $\gtrsim10$. Our results provide a framework for interpreting hydrogen lines in LRDs and similar systems.

**Link**: [arxiv](https://arxiv.org/abs/2508.08768v2),  [pdf](https://arxiv.org/pdf/2508.08768v2)

**Tags**: astro-ph.GA 



### Toward Honest Language Models for Deductive Reasoning
**Authors**: Jiarui Liu, Kaustubh Dhole, Yingheng Wang, Haoyang Wen, Sarah Zhang, Haitao Mao, Gaotang Li, Neeraj Varshney, Jingguo Liu, Xiaoman Pan

**Updated**: 2025-11-28T16:20:50Z

**Summary**: Deductive reasoning is the process of deriving conclusions strictly from the given premises, without relying on external knowledge. We define honesty in this setting as a model's ability to respond only when the conclusion is logically entailed by the premises, and to abstain otherwise. However, current language models often fail to reason honestly, producing unwarranted answers when the input is insufficient. To study this challenge, we formulate honest deductive reasoning as multi-step tasks where models must either derive the correct conclusion or abstain. We curate two datasets from graph structures, one for linear algebra and one for logical inference, and introduce unanswerable cases by randomly perturbing an edge in half of the instances. We find that prompting and existing training methods, including GRPO with or without supervised fine-tuning initialization, struggle on these tasks. In particular, GRPO optimize only for final task outcomes, leaving models vulnerable to collapse when negative rewards dominate early training. To address this, we propose ACNCHOR, a reinforcement learning method that injects ground truth trajectories into rollouts, preventing early training collapse. Our results demonstrate that this method stabilizes learning and significantly improves the overall reasoning performance, underscoring the importance of training dynamics for enabling honest deductive reasoning in language models.

**Link**: [arxiv](https://arxiv.org/abs/2511.09222v4),  [pdf](https://arxiv.org/pdf/2511.09222v4)

**Tags**: cs.CL 



### Do LLM-judges Align with Human Relevance in Cranfield-style Recommender Evaluation?
**Authors**: Gustavo Penha, Aleksandr V. Petrov, Claudia Hauff, Enrico Palumbo, Ali Vardasbi, Edoardo D'Amico, Francesco Fabbri, Alice Wang, Praveen Chandar, Henrik Lindstrom, Hugues Bouchard, Mounia Lalmas

**Updated**: 2025-11-28T16:10:39Z

**Summary**: Evaluating recommender systems remains a long-standing challenge, as offline methods based on historical user interactions and train-test splits often yield unstable and inconsistent results due to exposure bias, popularity bias, sampled evaluations, and missing-not-at-random patterns. In contrast, textual document retrieval benefits from robust, standardized evaluation via Cranfield-style test collections, which combine pooled relevance judgments with controlled setups. While recent work shows that adapting this methodology to recommender systems is feasible, constructing such collections remains costly due to the need for manual relevance judgments, thus limiting scalability. This paper investigates whether Large Language Models (LLMs) can serve as reliable automatic judges to address these scalability challenges. Using the ML-32M-ext Cranfield-style movie recommendation collection, we first examine the limitations of existing evaluation methodologies. Then we explore the alignment and the recommender systems ranking agreement between the LLM-judge and human provided relevance labels. We find that incorporating richer item metadata and longer user histories improves alignment, and that LLM-judge yields high agreement with human-based rankings (Kendall's tau = 0.87). Finally, an industrial case study in the podcast recommendation domain demonstrates the practical value of LLM-judge for model selection. Overall, our results show that LLM-judge is a viable and scalable approach for evaluating recommender systems.

**Link**: [arxiv](https://arxiv.org/abs/2511.23312v1),  [pdf](https://arxiv.org/pdf/2511.23312v1)

**Tags**: cs.IR 



### FLIMs: Fault Localization Interference Mutants, Definition, Recognition and Mitigation
**Authors**: Hengyuan Liu, Zheng Li, Donghua Wang, Yankai Wu, Xiang Chen, Yong Liu

**Updated**: 2025-11-28T16:00:44Z

**Summary**: Mutation-based Fault Localization (MBFL) has been widely explored for automated software debugging, leveraging artificial mutants to identify faulty code entities. However, MBFL faces significant challenges due to interference mutants generated from non-faulty code entities but can be killed by failing tests. These mutants mimic the test sensitivity behaviors of real faulty code entities and weaken the effectiveness of fault localization. To address this challenge, we introduce the concept of Fault Localization Interference Mutants (FLIMs) and conduct a theoretical analysis based on the Reachability, Infection, Propagation, and Revealability (RIPR) model, identifying four distinct interference causes. Building on this, we propose a novel approach to semantically recognize and mitigate FLIMs using LLM-based semantic analysis, enhanced by fine-tuning techniques and confidence estimation strategies to address LLM output instability. The recognized FLIMs are then mitigated by refining the suspiciousness scores calculated from MBFL techniques. We integrate FLIM recognition and mitigation into the MBFL workflow, developing MBFL-FLIM, a fault localization framework that enhances MBFL's effectiveness by reducing misleading interference while preserving real fault-revealing information. Our empirical experiments on the Defects4J benchmark with 395 program versions using eight LLMs demonstrate MBFL-FLIM's superiority over traditional SBFL and MBFL methods, advanced dynamic feature-based approaches, and recent LLM-based fault localization techniques. Specifically, MBFL-FLIM achieves an average improvement of 44 faults in the Top-1 metric, representing a significant enhancement over baseline methods. Further evaluation confirms MBFL-FLIM's robust performance in multi-fault scenarios, with ablation experiments validating the contributions of the fine-tuning and confidence estimation components.

**Link**: [arxiv](https://arxiv.org/abs/2511.23302v1),  [pdf](https://arxiv.org/pdf/2511.23302v1)

**Tags**: cs.SE 



### CAMA: Enhancing Mathematical Reasoning in Large Language Models with Causal Knowledge
**Authors**: Lei Zan, Keli Zhang, Ruichu Cai, Lujia Pan

**Updated**: 2025-11-28T15:58:34Z

**Summary**: Large Language Models (LLMs) have demonstrated strong performance across a wide range of tasks, yet they still struggle with complex mathematical reasoning, a challenge fundamentally rooted in deep structural dependencies. To address this challenge, we propose \textbf{CA}usal \textbf{MA}thematician (\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit, reusable mathematical structure. In the learning stage, CAMA first constructs the \textbf{M}athematical \textbf{C}ausal \textbf{G}raph (\textbf{MCG}), a high-level representation of solution strategies, by combining LLM priors with causal discovery algorithms applied to a corpus of question-solution pairs. The resulting MCG encodes essential knowledge points and their causal dependencies. To better align the graph with downstream reasoning tasks, CAMA further refines the MCG through iterative feedback derived from a selected subset of the question-solution pairs. In the reasoning stage, given a new question, CAMA dynamically extracts a task-relevant subgraph from the MCG, conditioned on both the question content and the LLM's intermediate reasoning trace. This subgraph, which encodes the most pertinent knowledge points and their causal dependencies, is then injected back into the LLM to guide its reasoning process. Empirical results on real-world datasets show that CAMA significantly improves LLM performance on challenging mathematical problems. Furthermore, our experiments demonstrate that structured guidance consistently outperforms unstructured alternatives, and that incorporating asymmetric causal relationships yields greater improvements than using symmetric associations alone.

**Link**: [arxiv](https://arxiv.org/abs/2508.02583v4),  [pdf](https://arxiv.org/pdf/2508.02583v4)

**Tags**: cs.AI cs.LG 



### SafeHumanoid: VLM-RAG-driven Control of Upper Body Impedance for Humanoid Robot
**Authors**: Yara Mahmoud, Jeffrin Sam, Nguyen Khang, Marcelino Fernando, Issatay Tokmurziyev, Miguel Altamirano Cabrera, Muhammad Haris Khan, Artem Lykov, Dzmitry Tsetserukou

**Updated**: 2025-11-28T15:58:26Z

**Summary**: Safe and trustworthy Human Robot Interaction (HRI) requires robots not only to complete tasks but also to regulate impedance and speed according to scene context and human proximity. We present SafeHumanoid, an egocentric vision pipeline that links Vision Language Models (VLMs) with Retrieval-Augmented Generation (RAG) to schedule impedance and velocity parameters for a humanoid robot. Egocentric frames are processed by a structured VLM prompt, embedded and matched against a curated database of validated scenarios, and mapped to joint-level impedance commands via inverse kinematics. We evaluate the system on tabletop manipulation tasks with and without human presence, including wiping, object handovers, and liquid pouring. The results show that the pipeline adapts stiffness, damping, and speed profiles in a context-aware manner, maintaining task success while improving safety. Although current inference latency (up to 1.4 s) limits responsiveness in highly dynamic settings, SafeHumanoid demonstrates that semantic grounding of impedance control is a viable path toward safer, standard-compliant humanoid collaboration.

**Link**: [arxiv](https://arxiv.org/abs/2511.23300v1),  [pdf](https://arxiv.org/pdf/2511.23300v1)

**Tags**: cs.RO 



### Entropy Rectifying Guidance for Diffusion and Flow Models
**Authors**: Tariq Berrada Ifriqi, Adriana Romero-Soriano, Michal Drozdzal, Jakob Verbeek, Karteek Alahari

**Updated**: 2025-11-28T15:44:11Z

**Summary**: Guidance techniques are commonly used in diffusion and flow models to improve image quality and input consistency for conditional generative tasks such as class-conditional and text-to-image generation. In particular, classifier-free guidance (CFG) is the most widely adopted guidance technique. It results, however, in trade-offs across quality, diversity and consistency: improving some at the expense of others. While recent work has shown that it is possible to disentangle these factors to some extent, such methods come with an overhead of requiring an additional (weaker) model, or require more forward passes per sampling step. In this paper, we propose Entropy Rectifying Guidance (ERG), a simple and effective guidance method based on inference-time changes in the attention mechanism of state-of-the-art diffusion transformer architectures, which allows for simultaneous improvements over image quality, diversity and prompt consistency. ERG is more general than CFG and similar guidance techniques, as it extends to unconditional sampling. We show that ERG results in significant improvements in various tasks, including text-to-image, class-conditional and unconditional image generation. We also show that ERG can be seamlessly combined with other recent guidance methods such as CADS and APG, further improving generation results.

**Link**: [arxiv](https://arxiv.org/abs/2504.13987v2),  [pdf](https://arxiv.org/pdf/2504.13987v2)

**Tags**: cs.CV cs.AI 



### MCP vs RAG vs NLWeb vs HTML: A Comparison of the Effectiveness and Efficiency of Different Agent Interfaces to the Web (Technical Report)
**Authors**: Aaron Steiner, Ralph Peeters, Christian Bizer

**Updated**: 2025-11-28T15:32:15Z

**Summary**: Large language model agents are increasingly used to automate web tasks such as product search, offer comparison, and checkout. Current research explores different interfaces through which these agents interact with websites, including traditional HTML browsing, retrieval-augmented generation (RAG) over pre-crawled content, communication via Web APIs using the Model Context Protocol (MCP), and natural-language querying through the NLWeb interface. However, no prior work has compared these four architectures within a single controlled environment using identical tasks.   To address this gap, we introduce a testbed consisting of four simulated e-shops, each offering its products via HTML, MCP, and NLWeb interfaces. For each interface (HTML, RAG, MCP, and NLWeb) we develop specialized agents that perform the same sets of tasks, ranging from simple product searches and price comparisons to complex queries for complementary or substitute products and checkout processes. We evaluate the agents using GPT 4.1, GPT 5, GPT 5 mini, and Claude Sonnet 4 as underlying LLM. Our evaluation shows that the RAG, MCP and NLWeb agents outperform HTML on both effectiveness and efficiency. Averaged over all tasks, F1 rises from 0.67 for HTML to between 0.75 and 0.77 for the other agents. Token usage falls from about 241k for HTML to between 47k and 140k per task. The runtime per task drops from 291 seconds to between 50 and 62 seconds. The best overall configuration is RAG with GPT 5 achieving an F1 score of 0.87 and a completion rate of 0.79. Also taking cost into consideration, RAG with GPT 5 mini offers a good compromise between API usage fees and performance. Our experiments show the choice of the interaction interface has a substantial impact on both the effectiveness and efficiency of LLM-based web agents.

**Link**: [arxiv](https://arxiv.org/abs/2511.23281v1),  [pdf](https://arxiv.org/pdf/2511.23281v1)

**Tags**: cs.CL 



### Beyond Curve Fitting: Neuro-Symbolic Agents for Context-Aware Epidemic Forecasting
**Authors**: Joongwon Chae, Runming Wang, Chen Xiong, Gong Yunhan, Lian Zhang, Ji Jiansong, Dongmei Yu, Peiwu Qin

**Updated**: 2025-11-28T15:29:26Z

**Summary**: Effective surveillance of hand, foot and mouth disease (HFMD) requires forecasts accounting for epidemiological patterns and contextual drivers like school calendars and weather. While classical models and recent foundation models (e.g., Chronos, TimesFM) incorporate covariates, they often lack the semantic reasoning to interpret the causal interplay between conflicting drivers. In this work, we propose a two-agent framework decoupling contextual interpretation from probabilistic forecasting. An LLM "event interpreter" processes heterogeneous signals-including school schedules, meteorological summaries, and reports-into a scalar transmission-impact signal. A neuro-symbolic core then combines this with historical case counts to produce calibrated probabilistic forecasts. We evaluate the framework on real-world HFMD datasets from Hong Kong (2023-2024) and Lishui, China (2024). Compared to traditional and foundation-model baselines, our approach achieves competitive point forecasting accuracy while providing robust 90% prediction intervals (coverage 0.85-1.00) and human-interpretable rationales. Our results suggest that structurally integrating domain knowledge through LLMs can match state-of-the-art performance while yielding context-aware forecasts that align with public health workflows. Code is available at https://github.com/jw-chae/forecast_MED .

**Link**: [arxiv](https://arxiv.org/abs/2511.23276v1),  [pdf](https://arxiv.org/pdf/2511.23276v1)

**Tags**: cs.LG cs.MA 



### Conjugate Generalised Bayesian Inference for Discrete Doubly Intractable Problems
**Authors**: William Laplante, Matias Altamirano, Jeremias Knoblauch, Andrew Duncan, François-Xavier Briol

**Updated**: 2025-11-28T15:28:45Z

**Summary**: Doubly intractable problems occur when both the likelihood and the posterior are available only in unnormalised form, with computationally intractable normalisation constants. Bayesian inference then typically requires direct approximation of the posterior through specialised and typically expensive MCMC methods. In this paper, we provide a computationally efficient alternative in the form of a novel generalised Bayesian posterior that allows for conjugate inference within the class of exponential family models for discrete data. We derive theoretical guarantees to characterise the asymptotic behaviour of the generalised posterior, supporting its use for inference. The method is evaluated on a range of challenging intractable exponential family models, including the Conway-Maxwell-Poisson graphical model of multivariate count data, autoregressive discrete time series models, and Markov random fields such as the Ising and Potts models. The computational gains are significant; in our experiments, the method is between 10 and 6000 times faster than state-of-the-art Bayesian computational methods.

**Link**: [arxiv](https://arxiv.org/abs/2511.23275v1),  [pdf](https://arxiv.org/pdf/2511.23275v1)

**Tags**: stat.ME 



### Network Inversion for Uncertainty-Aware Out-of-Distribution Detection
**Authors**: Pirzada Suhail, Rehna Afroz, Gouranga Bala, Amit Sethi

**Updated**: 2025-11-28T15:28:21Z

**Summary**: Out-of-distribution (OOD) detection and uncertainty estimation (UE) are critical components for building safe machine learning systems, especially in real-world scenarios where unexpected inputs are inevitable. However the two problems have, until recently, separately been addressed. In this work, we propose a novel framework that combines network inversion with classifier training to simultaneously address both OOD detection and uncertainty estimation. For a standard n-class classification task, we extend the classifier to an (n+1)-class model by introducing a "garbage" class, initially populated with random gaussian noise to represent outlier inputs. After each training epoch, we use network inversion to reconstruct input images corresponding to all output classes that initially appear as noisy and incoherent and are therefore excluded to the garbage class for retraining the classifier. This cycle of training, inversion, and exclusion continues iteratively till the inverted samples begin to resemble the in-distribution data more closely, with a significant drop in the uncertainty, suggesting that the classifier has learned to carve out meaningful decision boundaries while sanitising the class manifolds by pushing OOD content into the garbage class. During inference, this training scheme enables the model to effectively detect and reject OOD samples by classifying them into the garbage class. Furthermore, the confidence scores associated with each prediction can be used to estimate uncertainty for both in-distribution and OOD inputs. Our approach is scalable, interpretable, and does not require access to external OOD datasets or post-hoc calibration techniques while providing a unified solution to the dual challenges of OOD detection and uncertainty estimation.

**Link**: [arxiv](https://arxiv.org/abs/2505.23448v2),  [pdf](https://arxiv.org/pdf/2505.23448v2)

**Tags**: cs.LG cs.CV 



### Behavior-Equivalent Token: Single-Token Replacement for Long Prompts in LLMs
**Authors**: Jiancheng Dong, Pengyue Jia, Jingyu Peng, Maolin Wang, Yuhao Wang, Lixin Su, Xin Sun, Shuaiqiang Wang, Dawei Yin, Xiangyu Zhao

**Updated**: 2025-11-28T15:22:52Z

**Summary**: Carefully engineered system prompts play a critical role in guiding the behavior of LLM agents, but their considerable length introduces significant drawbacks, including increased inference latency, higher computational cost, and reduced effective context length. This raises the question of whether such lengthy prompts can be replaced by a drastically reduced number of tokens while preserving their behavioral effect on downstream tasks. To enable this, we propose a lightweight three-stage training framework that learns a single prompt-specific Behavior-Equivalent token ([BE]). The framework first trains [BE] to encode the natural-language content of the original system prompt via reconstruction, and then distills the prompt 's downstream behavior into this single token. Importantly, our method requires no access to model internals, no auxiliary compression models, and no labeled responses. Empirical evaluations on three datasets show that a single [BE] token achieves up to a 3000x reduction in prompt length, while retaining about 98% of the downstream performance of the original system prompts. This substantially reduces inference cost and leaves almost the entire context window available for user inputs.

**Link**: [arxiv](https://arxiv.org/abs/2511.23271v1),  [pdf](https://arxiv.org/pdf/2511.23271v1)

**Tags**: cs.CL 



### Unlocking Multilingual Reasoning Capability of LLMs and LVLMs through Representation Engineering
**Authors**: Qiming Li, Xiaocheng Feng, Yixuan Ma, Zekai Ye, Ruihan Chen, Xiachong Feng, Bing Qin

**Updated**: 2025-11-28T14:40:27Z

**Summary**: Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) demonstrate strong reasoning capabilities, yet their performance in English significantly outperforms that in low-resource languages, raising fairness concerns in multilingual applications. Existing approaches either rely on costly multilingual training or employ prompting with external translation tools, both of which are resource-intensive and sensitive to translation quality. To address these limitations, we propose a training-free inference-time method to enhance Multilingual Reasoning capabilities via Representation Engineering (MRRE) without using any additional training data or tools. MRRE sequentially injects two precomputed vectors at specific layers during inference processing: cross-lingual reasoning enhancement vectors, which steer non-English reasoning representations toward English space to unlock multilingual reasoning, and target-language output anchoring vectors, which restore the distribution of the target language to preserve input-output language consistency. Comprehensive experiments across six advanced LLMs and LVLMs on four reasoning benchmarks demonstrate that MRRE consistently enhances non-English reasoning by an average gain of 5.48% and up to 7.54% in low-resource languages (Thai and Swahili), while improving input-output language consistency by 3.78%.

**Link**: [arxiv](https://arxiv.org/abs/2511.23231v1),  [pdf](https://arxiv.org/pdf/2511.23231v1)

**Tags**: cs.CV 



### Continual Learning of Domain Knowledge from Human Feedback in Text-to-SQL
**Authors**: Thomas Cook, Kelly Patel, Sivapriya Vellaichamy, Udari Madhushani Sehwag, Saba Rahimi, Zhen Zeng, Sumitra Ganesh

**Updated**: 2025-11-28T14:40:27Z

**Summary**: Large Language Models (LLMs) can generate SQL queries from natural language questions but struggle with database-specific schemas and tacit domain knowledge. We introduce a framework for continual learning from human feedback in text-to-SQL, where a learning agent receives natural language feedback to refine queries and distills the revealed knowledge for reuse on future tasks. This distilled knowledge is stored in a structured memory, enabling the agent to improve execution accuracy over time. We design and evaluate multiple variations of a learning agent architecture that vary in how they capture and retrieve past experiences. Experiments on the BIRD benchmark Dev set show that memory-augmented agents, particularly the Procedural Agent, achieve significant accuracy gains and error reduction by leveraging human-in-the-loop feedback. Our results highlight the importance of transforming tacit human expertise into reusable knowledge, paving the way for more adaptive, domain-aware text-to-SQL systems that continually learn from a human-in-the-loop.

**Link**: [arxiv](https://arxiv.org/abs/2511.10674v2),  [pdf](https://arxiv.org/pdf/2511.10674v2)

**Tags**: cs.CL cs.AI cs.DB 



### SlotFlow: Amortized Trans-Dimensional Inference with Slot-Based Normalizing Flows
**Authors**: Niklas Houba, Giovanni Giarda, Lorenzo Speri

**Updated**: 2025-11-28T14:38:01Z

**Summary**: Inferring the number of distinct components contributing to an observation, while simultaneously estimating their parameters, remains a long-standing challenge across signal processing, astrophysics, and neuroscience. Classical trans-dimensional Bayesian methods such as Reversible Jump Markov Chain Monte Carlo (RJMCMC) provide asymptotically exact inference but can be computationally expensive. Instead, modern deep learning provides a faster alternative to inference but typically assume fixed component counts, sidestepping the core challenge of trans-dimensionality. To address this, we introduce SlotFlow, a deep learning architecture for trans-dimensional amortized inference. The architecture processes time-series observations, which we represent jointly in the frequency and time domains through parallel encoders. A classifier produces a distribution over component counts K, and its MAP estimate specifies the number of slots instantiated. Each slot is parameterized by a shared conditional normalizing flow trained via permutation-invariant Hungarian matching. On sinusoidal decomposition with up to 10 overlapping components and Gaussian noise, SlotFlow achieves 99.85% cardinality accuracy and well-calibrated parameter posteriors, with systematic biases well below one posterior standard deviation. Direct comparison with RJMCMC shows close agreement in amplitude and phase, with Wasserstein distances $W_2 < 0.01$ and $< 0.03$, indicating that shared global context captures inter-component structure despite a factorized posterior. Frequency posteriors remain centered but exhibit 2-3x broader intervals, consistent with an encoder bottleneck in retaining long-baseline phase coherence. The method delivers a $\sim 10^6\times$ speedup over RJMCMC, suggesting applicability to time-critical workflows in gravitational-wave astronomy, neural spike sorting, and object-centric vision.

**Link**: [arxiv](https://arxiv.org/abs/2511.23228v1),  [pdf](https://arxiv.org/pdf/2511.23228v1)

**Tags**: astro-ph.IM gr-qc physics.data-an 



### TWEO: Transformers Without Extreme Outliers Enables FP8 Training And Quantization For Dummies
**Authors**: Guang Liang, Jie Shao, Ningyuan Tang, Xinyao Liu, Jianxin Wu

**Updated**: 2025-11-28T14:33:21Z

**Summary**: Native FP8 support in modern hardware is essential for training large Transformers, but is severely hindered by extreme activation outliers. Existing solutions either rely on complex mixed-precision engineering or invasive architectural modifications. This paper fundamentally challenges the conventional wisdom that outliers are data-driven. We demonstrate that extreme outliers are a data-independent, mechanically-produced artifact of training, originating from specific structural properties of the weight matrices (i.e., colinearity). Based on this insight, we propose TWEO (Transformers Without Extreme Outliers), a novel, non-invasive loss function. TWEO effectively prevents extreme outliers via a very simple loss term, which reduces outliers from 10000+ to less than 20. TWEO then enables full-model FP8 pre-training with neither engineering tricks nor architectural changes for both LLM and ViT. When standard FP8 training catastrophically collapses, TWEO achieves performance comparable to the BF16 baseline while delivering a 36% increase in training throughput. Also, TWEO enables a new quantization paradigm. Hardware-friendly W8A8 per-tensor static quantization of LLMs, previously considered completely unusable due to outliers, achieves SOTA performance for the first time on TWEO-trained models.

**Link**: [arxiv](https://arxiv.org/abs/2511.23225v1),  [pdf](https://arxiv.org/pdf/2511.23225v1)

**Tags**: cs.CL cs.CV cs.LG 



### OmniRouter: Budget and Performance Controllable Multi-LLM Routing
**Authors**: Kai Mei, Wujiang Xu, Minghao Guo, Shuhang Lin, Yongfeng Zhang

**Updated**: 2025-11-28T14:29:56Z

**Summary**: Large language models (LLMs) deliver superior performance but require substantial computational resources and operate with relatively low efficiency, while smaller models can efficiently handle simpler tasks with fewer resources. LLM routing is a crucial paradigm that dynamically selects the most suitable large language models from a pool of candidates to process diverse inputs, ensuring optimal resource utilization while maintaining response quality. Existing routing frameworks typically model this as a locally optimal decision-making problem, selecting the presumed best-fit LLM for each query individually, which overlooks global budget constraints, resulting in ineffective resource allocation. To tackle this problem, we introduce OmniRouter, a fundamentally controllable routing framework for multi-LLM serving. Instead of making per-query greedy choices, OmniRouter models the routing task as a constrained optimization problem, assigning models that minimize total cost while ensuring the required performance level. Specifically, a hybrid retrieval-augmented predictor is designed to predict the capabilities and costs of LLMs. After obtaining the predicted cost and performance, we utilize a constrained optimizer for cost-optimal assignments that employs Lagrangian dual decomposition with adaptive multipliers. It iteratively converges toward the globally optimal query-model allocation, dynamically balancing latency minimization against quality thresholds while adhering to heterogeneous capacity constraints. Experiments show that OmniRouter achieves up to 6.30% improvement in response accuracy while simultaneously reducing computational costs by at least 10.15% compared to competitive router baselines. The code and the dataset are available at https://github.com/dongyuanjushi/OmniRouter.

**Link**: [arxiv](https://arxiv.org/abs/2502.20576v6),  [pdf](https://arxiv.org/pdf/2502.20576v6)

**Tags**: cs.DB cs.CL 



### Instruction Tuning of Large Language Models for Tabular Data Generation-in One Day
**Authors**: Milad Abdollahzadeh, Abdul Raheem, Zilong Zhao, Uzair Javaid, Kevin Yee, Nalam Venkata Abhishek, Tram Truong-Huu, Biplab Sikdar

**Updated**: 2025-11-28T14:26:46Z

**Summary**: Tabular instruction tuning has emerged as a promising research direction for improving LLMs understanding of tabular data. However, the majority of existing works only consider question-answering and reasoning tasks over tabular data, leaving tabular data generation largely unnoticed. In this work, for the first time, we explore the efficacy of instruction tuning in improving LLMs tabular data generation capabilities. More specifically, given the high data and computation requirements of tabular instruction tuning, we aim to address the possibility of instruction tuning for tabular data generation with limited data and computational resources. To achieve this, we first create a high-quality instruction dataset for tabular data, enabling efficient LLM comprehension. We then instruction-tune an open-source LLM (Llama3.1-8B-Instruct) on the training set of this dataset to improve its tabular data generation performance. Our experimental results show that by using our high-quality dataset and instruction-tuning on only 7K instructions with an A100 GPU, for less than 6 hours, we achieve tabular data generation performance on par with the most capable commercial LLM, GPT-4o.

**Link**: [arxiv](https://arxiv.org/abs/2511.23220v1),  [pdf](https://arxiv.org/pdf/2511.23220v1)

**Tags**: cs.CV 



### Comparing Variable Selection and Model Averaging Methods for Logistic Regression
**Authors**: Nikola Sekulovski, František Bartoš, Don van den Bergh, Giuseppe Arena, Henrik R. Godmann, Vipasha Goyal, Julius M. Pfadt, Maarten Marsman, Adrian E. Raftery

**Updated**: 2025-11-28T14:21:15Z

**Summary**: Model uncertainty is a central challenge in statistical models for binary outcomes such as logistic regression, arising when it is unclear which predictors should be included in the model. Many methods have been proposed to address this issue for logistic regression, but their relative performance under realistic conditions remains poorly understood. We therefore conducted a preregistered, simulation-based comparison of 28 established methods for variable selection and inference under model uncertainty, using 11 empirical datasets spanning a range of sample sizes and numbers of predictors, in cases both with and without separation. We found that Bayesian model averaging methods based on g-priors, particularly with g = max(n, p^2), show the strongest overall performance when separation is absent. When separation occurs, penalized likelihood approaches, especially the LASSO, provide the most stable results, while Bayesian model averaging with the local empirical Bayes (EB-local) prior is competitive in both situations. These findings offer practical guidance for applied researchers on how to effectively address model uncertainty in logistic regression in modern empirical and machine learning research.

**Link**: [arxiv](https://arxiv.org/abs/2511.23216v1),  [pdf](https://arxiv.org/pdf/2511.23216v1)

**Tags**: stat.ME 



### Mina: A Multilingual LLM-Powered Legal Assistant Agent for Bangladesh for Empowering Access to Justice
**Authors**: Azmine Toushik Wasi, Wahid Faisal, Mst Rafia Islam

**Updated**: 2025-11-28T14:20:39Z

**Summary**: Bangladesh's low-income population faces major barriers to affordable legal advice due to complex legal language, procedural opacity, and high costs. Existing AI legal assistants lack Bengali-language support and jurisdiction-specific adaptation, limiting their effectiveness. To address this, we developed Mina, a multilingual LLM-based legal assistant tailored for the Bangladeshi context. It employs multilingual embeddings and a RAG-based chain-of-tools framework for retrieval, reasoning, translation, and document generation, delivering context-aware legal drafts, citations, and plain-language explanations via an interactive chat interface. Evaluated by law faculty from leading Bangladeshi universities across all stages of the 2022 and 2023 Bangladesh Bar Council Exams, Mina scored 75-80% in Preliminary MCQs, Written, and simulated Viva Voce exams, matching or surpassing average human performance and demonstrating clarity, contextual understanding, and sound legal reasoning. Even under a conservative upper bound, Mina operates at just 0.12-0.61% of typical legal consultation costs in Bangladesh, yielding a 99.4-99.9\% cost reduction relative to human-provided services. These results confirm its potential as a low-cost, multilingual AI assistant that automates key legal tasks and scales access to justice, offering a real-world case study on building domain-specific, low-resource systems and addressing challenges of multilingual adaptation, efficiency, and sustainable public-service AI deployment.

**Link**: [arxiv](https://arxiv.org/abs/2511.08605v2),  [pdf](https://arxiv.org/pdf/2511.08605v2)

**Tags**: cs.CL cs.CY cs.HC cs.MA cs.MM 



### GAPS: Guiding Dynamic Android Analysis with Static Path Synthesis
**Authors**: Samuele Doria, Eleonora Losiouk

**Updated**: 2025-11-28T14:19:04Z

**Summary**: Dynamically resolving method reachability in Android applications remains a critical and largely unsolved problem. Despite notable advancements in GUI testing and static call graph construction, current tools are insufficient for reliably driving execution toward specific target methods, especially those not embedded in a graphical component (e.g., libraries' methods), a capability essential for tasks such as vulnerability validation, debugging, and behavioral analysis.   We present GAPS (Graph-based Automated Path Synthesizer), the first system that integrates static, method-guided call graph analysis with dynamic, interaction-driven execution. GAPS performs a lightweight backward traversal of the call graph, guided by data-flow analysis, to reconstruct paths reaching the target methods. These paths are then translated into instructions that guide runtime app exploration.   On the AndroTest benchmark, GAPS statically identifies paths to reach 88.24\% of the target methods in just 4.27 seconds per app and dynamically reaches 57.44\% of them. In contrast, state-of-the-art dynamic interaction tools show significantly lower reachability over three runs: APE, one of the best model-based GUI testers, achieves 12.82\%, while GoalExplorer, a hybrid analysis tool, reaches 9.69\%, and Guardian, an LLM-based UI automator, reaches 17.12\%. Static analysis tools also fall short: FlowDroid and DroidReach identify paths to reach 58.81\% and 9.48\% of the targets, requiring 35.06 seconds and 23.46 seconds per app, respectively.   Finally, an evaluation on the 50 most downloaded real-world apps demonstrates GAPS's practical utility in analyzing security-critical code under a realistic scenario. With an average static analysis time of 278.9 seconds, GAPS statically reconstructs paths to 62.03\% of the target methods and dynamically reaches 59.86\% of them.

**Link**: [arxiv](https://arxiv.org/abs/2511.23213v1),  [pdf](https://arxiv.org/pdf/2511.23213v1)

**Tags**: cs.SE 



### Asymptotic Theory and Phase Transitions for Variable Importance in Quantile Regression Forests
**Authors**: Tomoshige Nakamura, Hiroshi Shiraishi

**Updated**: 2025-11-28T14:18:05Z

**Summary**: Quantile Regression Forests (QRF) are widely used for non-parametric conditional quantile estimation, yet statistical inference for variable importance measures remains challenging due to the non-smoothness of the loss function and the complex bias-variance trade-off. In this paper, we develop a asymptotic theory for variable importance defined as the difference in pinball loss risks. We first establish the asymptotic normality of the QRF estimator by handling the non-differentiable pinball loss via Knight's identity. Second, we uncover a "phase transition" phenomenon governed by the subsampling rate $β$ (where $s \asymp n^β$). We prove that in the bias-dominated regime ($β\ge 1/2$), which corresponds to large subsample sizes typically favored in practice to maximize predictive accuracy, standard inference breaks down as the estimator converges to a deterministic bias constant rather than a zero-mean normal distribution. Finally, we derive the explicit analytic form of this asymptotic bias and discuss the theoretical feasibility of restoring valid inference via analytic bias correction. Our results highlight a fundamental trade-off between predictive performance and inferential validity, providing a theoretical foundation for understanding the intrinsic limitations of random forest inference in high-dimensional settings.

**Link**: [arxiv](https://arxiv.org/abs/2511.23212v1),  [pdf](https://arxiv.org/pdf/2511.23212v1)

**Tags**: stat.ML cs.LG math.ST 



### A Design-Based Matching Framework for Staggered Adoption with Time-Varying Confounding
**Authors**: Suehyun Kim, Kwonsang Lee

**Updated**: 2025-11-28T14:16:01Z

**Summary**: Causal inference in longitudinal datasets has long been challenging due to dynamic treatment adoption and confounding by time-varying covariates. Prior work either fails to account for heterogeneity across treatment adoption cohorts and treatment timings or relies on modeling assumptions. In this paper, we develop a novel design-based framework for inference on group- and time-specific treatment effects in panel data with staggered treatment adoption. We establish identification results for causal effects under this structure and introduce corresponding estimators, together with a block bootstrap procedure for estimating the covariance matrix and testing the homogeneity of group-time treatment effects. To implement the framework in practice, we propose the Reverse-Time Nested Matching algorithm, which constructs matched strata by pairing units from different adoption cohorts in a way that ensures comparability of covariate histories at each treatment time. Applying the algorithm to the Netflix-IPTV dataset, we find that while Netflix subscription does not significantly affect total IPTV viewing time, it does negatively affect VoD usage. We also provide statistical evidence that the causal effects of Netflix subscription may vary even within the same treatment cohort or across the same outcome and event times.

**Link**: [arxiv](https://arxiv.org/abs/2511.23208v1),  [pdf](https://arxiv.org/pdf/2511.23208v1)

**Tags**: stat.ME 



### Quantifying the Privacy-Utility Trade-off in GPS-based Daily Stress Recognition using Semantic Features
**Authors**: Hoang Khang Phan, Nhat Tan Le

**Updated**: 2025-11-28T14:04:00Z

**Summary**: Psychological stress is a widespread issue that significantly impacts student well-being and academic performance. Effective remote stress recognition is crucial, yet existing methods often rely on wearable devices or GPS-based clustering techniques that pose privacy risks. In this study, we introduce a novel, end-to-end privacy-enhanced framework for semantic location encoding using a self-hosted OSM engine and an LLM-bootstrapped static map. We rigorously quantify the privacy-utility trade-off and demonstrate (via LOSO validation) that our Privacy-Aware (PA) model achieves performance statistically indistinguishable from a non-private model, proving that utility does not require sacrificing privacy. Feature importance analysis highlights that recreational activity time, working time, and travel time play a significant role in stress recognition.

**Link**: [arxiv](https://arxiv.org/abs/2511.23200v1),  [pdf](https://arxiv.org/pdf/2511.23200v1)

**Tags**: cs.CR cs.HC 



### iSeal: Encrypted Fingerprinting for Reliable LLM Ownership Verification
**Authors**: Zixun Xiong, Gaoyi Wu, Qingyang Yu, Mingyu Derek Ma, Lingfeng Yao, Miao Pan, Xiaojiang Du, Hao Wang

**Updated**: 2025-11-28T13:58:50Z

**Summary**: Given the high cost of large language model (LLM) training from scratch, safeguarding LLM intellectual property (IP) has become increasingly crucial. As the standard paradigm for IP ownership verification, LLM fingerprinting thus plays a vital role in addressing this challenge. Existing LLM fingerprinting methods verify ownership by extracting or injecting model-specific features. However, they overlook potential attacks during the verification process, leaving them ineffective when the model thief fully controls the LLM's inference process. In such settings, attackers may share prompt-response pairs to enable fingerprint unlearning or manipulate outputs to evade exact-match verification. We propose iSeal, the first fingerprinting method designed for reliable verification when the model thief controls the suspected LLM in an end-to-end manner. It injects unique features into both the model and an external module, reinforced by an error-correction mechanism and a similarity-based verification strategy. These components are resistant to verification-time attacks, including collusion-based fingerprint unlearning and response manipulation, backed by both theoretical analysis and empirical results. iSeal achieves 100 percent Fingerprint Success Rate (FSR) on 12 LLMs against more than 10 attacks, while baselines fail under unlearning and response manipulations.

**Link**: [arxiv](https://arxiv.org/abs/2511.08905v2),  [pdf](https://arxiv.org/pdf/2511.08905v2)

**Tags**: cs.CR cs.AI 



### Obstruction reasoning for robotic grasping
**Authors**: Runyu Jiao, Matteo Bortolon, Francesco Giuliari, Alice Fasoli, Sergio Povoli, Guofeng Mei, Yiming Wang, Fabio Poiesi

**Updated**: 2025-11-28T13:53:12Z

**Summary**: Successful robotic grasping in cluttered environments not only requires a model to visually ground a target object but also to reason about obstructions that must be cleared beforehand. While current vision-language embodied reasoning models show emergent spatial understanding, they remain limited in terms of obstruction reasoning and accessibility planning. To bridge this gap, we present UNOGrasp, a learning-based vision-language model capable of performing visually-grounded obstruction reasoning to infer the sequence of actions needed to unobstruct the path and grasp the target object. We devise a novel multi-step reasoning process based on obstruction paths originated by the target object. We anchor each reasoning step with obstruction-aware visual cues to incentivize reasoning capability. UNOGrasp combines supervised and reinforcement finetuning through verifiable reasoning rewards. Moreover, we construct UNOBench, a large-scale dataset for both training and benchmarking, based on MetaGraspNetV2, with over 100k obstruction paths annotated by humans with obstruction ratios, contact points, and natural-language instructions. Extensive experiments and real-robot evaluations show that UNOGrasp significantly improves obstruction reasoning and grasp success across both synthetic and real-world environments, outperforming generalist and proprietary alternatives. Project website: https://tev-fbk.github.io/UnoGrasp/.

**Link**: [arxiv](https://arxiv.org/abs/2511.23186v1),  [pdf](https://arxiv.org/pdf/2511.23186v1)

**Tags**: cs.RO cs.AI cs.CV 



### A new approach for simulating PBH formation from generic curvature fluctuations with the Misner-Sharp formalism
**Authors**: Albert Escrivà

**Updated**: 2025-11-28T13:52:12Z

**Summary**: Primordial Black Holes (PBHs) may have formed in the early Universe due to the collapse of super-horizon curvature fluctuations. Simulations of PBH formation have been essential for inferring the initial conditions that lead to black hole formation and for studying their properties and impact on our Universe. The Misner-Sharp formalism is commonly used as a standard approach for these simulations. Recently, type-II fluctuations, characterized by a non-monotonic areal radius, have gained interest. In the standard Misner-Sharp approach for simulating PBH formation with these fluctuations, the evolution equations exhibit divergent terms ($0/0$), which complicate and prevent the simulations. We formulate a new approach to overcome this issue in a simple manner by using the trace of the extrinsic curvature as an auxiliary variable, allowing simulations of type-II fluctuations within the Misner-Sharp formalism. Using a set of standard exponential-shaped curvature profiles, we apply and test our new approach and numerical code based on pseudospectral methods to study the time evolution of the gravitational collapse, threshold values of type A/B PBHs and PBH mass. Interestingly, we identify cases of type-II fluctuations that do not necessarily result in PBH formation.

**Link**: [arxiv](https://arxiv.org/abs/2504.05813v3),  [pdf](https://arxiv.org/pdf/2504.05813v3)

**Tags**: astro-ph.CO gr-qc hep-th 



### HPSU: A Benchmark for Human-Level Perception in Real-World Spoken Speech Understanding
**Authors**: Chen Li, Peiji Yang, Yicheng Zhong, Jianxing Yu, Zhisheng Wang, Zihao Gou, Wenqing Chen, Jian Yin

**Updated**: 2025-11-28T13:45:40Z

**Summary**: Recent advances in Speech Large Language Models (Speech LLMs) have led to great progress in speech understanding tasks such as Automatic Speech Recognition (ASR) and Speech Emotion Recognition (SER). However, whether these models can achieve human-level auditory perception, particularly in terms of their ability to comprehend latent intentions and implicit emotions in real-world spoken language, remains underexplored. To this end, we introduce the Human-level Perception in Spoken Speech Understanding (HPSU), a new benchmark for fully evaluating the human-level perceptual and understanding capabilities of Speech LLMs. HPSU comprises over 20,000 expert-validated spoken language understanding samples in English and Chinese. It establishes a comprehensive evaluation framework by encompassing a spectrum of tasks, ranging from basic speaker attribute recognition to complex inference of latent intentions and implicit emotions. To address the issues of data scarcity and high cost of manual annotation in real-world scenarios, we developed a semi-automatic annotation process. This process fuses audio, textual, and visual information to enable precise speech understanding and labeling, thus enhancing both annotation efficiency and quality. We systematically evaluate various open-source and proprietary Speech LLMs. The results demonstrate that even top-performing models still fall considerably short of human capabilities in understanding genuine spoken interactions. Consequently, HPSU will be useful for guiding the development of Speech LLMs toward human-level perception and cognition.

**Link**: [arxiv](https://arxiv.org/abs/2511.23178v1),  [pdf](https://arxiv.org/pdf/2511.23178v1)

**Tags**: cs.SD 



### LockForge: Automating Paper-to-Code for Logic Locking with Multi-Agent Reasoning LLMs
**Authors**: Akashdeep Saha, Zeng Wang, Prithwish Basu Roy, Johann Knechtel, Ozgur Sinanoglu, Ramesh Karri

**Updated**: 2025-11-28T13:42:32Z

**Summary**: Despite rapid progress in logic locking (LL), reproducibility remains a challenge as codes are rarely made public. We present LockForge, a first-of-its-kind, multi-agent large language model (LLM) framework that turns LL descriptions in papers into executable and tested code. LockForge provides a carefully crafted pipeline realizing forethought, implementation, iterative refinement, and a multi-stage validation, all to systematically bridge the gap between prose and practice for complex LL schemes. For validation, we devise (i) an LLM-as-Judge stage with a scoring system considering behavioral checks, conceptual mechanisms, structural elements, and reproducibility on benchmarks, and (ii) an independent LLM-as-Examiner stage for ground-truth assessment. We apply LockForge to 10 seminal LL schemes, many of which lack reference implementations. Our evaluation on multiple SOTA LLMs, including ablation studies, reveals the significant complexity of the task. We show that an advanced reasoning model and a sophisticated, multi-stage framework like LockForge are required. We release all implementations and benchmarks, providing a reproducible and fair foundation for evaluation of further LL research.

**Link**: [arxiv](https://arxiv.org/abs/2511.18531v2),  [pdf](https://arxiv.org/pdf/2511.18531v2)

**Tags**: cs.CR cs.PL 



### Are LLMs Good Safety Agents or a Propaganda Engine?
**Authors**: Neemesh Yadav, Francesco Ortu, Jiarui Liu, Joeun Yook, Bernhard Schölkopf, Rada Mihalcea, Alberto Cazzaniga, Zhijing Jin

**Updated**: 2025-11-28T13:36:00Z

**Summary**: Large Language Models (LLMs) are trained to refuse to respond to harmful content. However, systematic analyses of whether this behavior is truly a reflection of its safety policies or an indication of political censorship, that is practiced globally by countries, is lacking. Differentiating between safety influenced refusals or politically motivated censorship is hard and unclear. For this purpose we introduce PSP, a dataset built specifically to probe the refusal behaviors in LLMs from an explicitly political context. PSP is built by formatting existing censored content from two data sources, openly available on the internet: sensitive prompts in China generalized to multiple countries, and tweets that have been censored in various countries. We study: 1) impact of political sensitivity in seven LLMs through data-driven (making PSP implicit) and representation-level approaches (erasing the concept of politics); and, 2) vulnerability of models on PSP through prompt injection attacks (PIAs). Associating censorship with refusals on content with masked implicit intent, we find that most LLMs perform some form of censorship. We conclude with summarizing major attributes that can cause a shift in refusal distributions across models and contexts of different countries.

**Link**: [arxiv](https://arxiv.org/abs/2511.23174v1),  [pdf](https://arxiv.org/pdf/2511.23174v1)

**Tags**: cs.CL 



### Development of systematic uncertainty-aware neural network trainings for binned-likelihood analyses at the LHC
**Authors**: CMS Collaboration

**Updated**: 2025-11-28T13:26:42Z

**Summary**: We propose a neural network training method capable of accounting for the effects of systematic variations of the data model in the training process and describe its extension towards neural network multiclass classification. The procedure is evaluated on the realistic case of the measurement of Higgs boson production via gluon fusion and vector boson fusion in the $ττ$ decay channel at the CMS experiment. The neural network output functions are used to infer the signal strengths for inclusive production of Higgs bosons as well as for their production via gluon fusion and vector boson fusion. We observe improvements of 12 and 16% in the uncertainty in the signal strengths for gluon and vector-boson fusion, respectively, compared with a conventional neural network training based on cross-entropy.

**Link**: [arxiv](https://arxiv.org/abs/2502.13047v2),  [pdf](https://arxiv.org/pdf/2502.13047v2)

**Tags**: hep-ex hep-ph physics.data-an 



### Energy-Efficient Vision Transformer Inference for Edge-AI Deployment
**Authors**: Nursultan Amanzhol, Jurn-Gyu Park

**Updated**: 2025-11-28T13:24:08Z

**Summary**: The growing deployment of Vision Transformers (ViTs) on energy-constrained devices requires evaluation methods that go beyond accuracy alone. We present a two-stage pipeline for assessing ViT energy efficiency that combines device-agnostic model selection with device-related measurements. We benchmark 13 ViT models on ImageNet-1K and CIFAR-10, running inference on NVIDIA Jetson TX2 (edge device) and an NVIDIA RTX 3050 (mobile GPU). The device-agnostic stage uses the NetScore metric for screening; the device-related stage ranks models with the Sustainable Accuracy Metric (SAM). Results show that hybrid models such as LeViT_Conv_192 reduce energy by up to 53% on TX2 relative to a ViT baseline (e.g., SAM5=1.44 on TX2/CIFAR-10), while distilled models such as TinyViT-11M_Distilled excel on the mobile GPU (e.g., SAM5=1.72 on RTX 3050/CIFAR-10 and SAM5=0.76 on RTX 3050/ImageNet-1K).

**Link**: [arxiv](https://arxiv.org/abs/2511.23166v1),  [pdf](https://arxiv.org/pdf/2511.23166v1)

**Tags**: cs.LG 



### Amplifiers or Equalizers? A Longitudinal Study of LLM Evolution in Software Engineering Project-Based Learning
**Authors**: Hana Kataoka, Jialong Li, Yutaka Matsuno

**Updated**: 2025-11-28T13:05:23Z

**Summary**: As LLMs reshape software development, integrating LLM-augmented practices into SE education has become imperative. While existing studies explore LLMs' educational use in introductory programming or isolated SE tasks, their impact in more open-ended Project-Based Learning (PBL) remains unexplored. This paper introduces a two-year longitudinal study comparing a 2024 (using early free LLMs, $n$=48) and 2025 (using the latest paid LLMs, $n$=46) cohort. Our findings suggest the latest powerful LLMs' dual role: they act as "equalizers," boosting average performance even for programming-weak students, providing opportunities for more authentic SE practices; yet also as "amplifiers," dramatically widening absolute performance gaps, creating new pedagogical challenges for addressing educational inequities.

**Link**: [arxiv](https://arxiv.org/abs/2511.23157v1),  [pdf](https://arxiv.org/pdf/2511.23157v1)

**Tags**: cs.SE cs.HC 



### Look Where It Matters: Training-Free Ultra-HR Remote Sensing VQA via Adaptive Zoom Search
**Authors**: Yunqi Zhou, Chengjie Jiang, Chun Yuan, Jing Li

**Updated**: 2025-11-28T13:03:57Z

**Summary**: With advances in satellite constellations, sensor technologies, and imaging pipelines, ultra-high-resolution (Ultra-HR) remote sensing imagery is becoming increasingly widespread. However, current remote sensing foundation models are ill-suited to such inputs: full-image encoding exhausts token and memory budgets, while resize-based preprocessing loses fine-grained and answer-critical details. In this context, guiding the model look where it matters before prediction becomes crucial. Therefore, we present ZoomSearch, a training-free, plug-and-play pipeline that decouples 'where to look' from 'how to answer' for Ultra-HR Remote Sensing Visual Question Answering (RS-VQA). ZoomSearch combines Adaptive Multi-Branch Zoom Search, which performs a hierarchical search over image patches to localize query-relevant regions, with Layout-Aware Patch Reassembly, which reorganizes the selected patches into a compact, layout-faithful canvas. We conduct comprehensive experiments on Ultra-HR RS-VQA benchmarks MME-RealWorld-RS and LRS-VQA, comparing against (i) strong general foundation models, (ii) remote sensing foundation models, (iii) Ultra-HR RS-VQA methods, and (iv) plug-and-play search-based VQA methods. When integrated with LLaVA-ov, ZoomSearch attains state-of-the-art accuracy across diverse tasks, improving the LLaVA-ov baseline by 26.3% on LRS-VQA and 114.8% on MME-RealWorld-RS. Meanwhile, it achieves much higher inference efficiency, outperforming prior search-based methods by 20%~44% in speed.

**Link**: [arxiv](https://arxiv.org/abs/2511.20460v2),  [pdf](https://arxiv.org/pdf/2511.20460v2)

**Tags**: cs.CV 



### Biased by Design: Leveraging AI Biases to Enhance Critical Thinking of News Readers
**Authors**: Liudmila Zavolokina, Kilian Sprenkamp, Zoya Katashinskaya, Daniel Gordon Jones

**Updated**: 2025-11-28T13:02:51Z

**Summary**: This paper explores the design of a propaganda detection tool using Large Language Models (LLMs). Acknowledging the inherent biases in AI models, especially in political contexts, we investigate how these biases might be leveraged to enhance critical thinking in news consumption. Countering the typical view of AI biases as detrimental, our research proposes strategies of user choice and personalization in response to a user's political stance, applying psychological concepts of confirmation bias and cognitive dissonance. We present findings from a qualitative user study, offering insights and design recommendations (bias awareness, personalization and choice, and gradual introduction of diverse perspectives) for AI tools in propaganda detection.

**Link**: [arxiv](https://arxiv.org/abs/2504.14522v4),  [pdf](https://arxiv.org/pdf/2504.14522v4)

**Tags**: cs.HC cs.AI 



### Activation Quantization of Vision Encoders Needs Prefixing Registers
**Authors**: Seunghyeon Kim, Jinho Kim, Taesun Yeom, Wonpyo Park, Kyuyeun Kim, Jaeho Lee

**Updated**: 2025-11-28T12:54:42Z

**Summary**: Transformer-based vision encoders -- such as CLIP -- are central to multimodal intelligence, powering applications from autonomous web agents to robotic control. Since these applications often demand real-time processing of massive visual data, reducing the inference cost of vision encoders is critical. Quantization offers a practical path, but remains challenging even at 8-bit precision due to massive-scale activations (i.e., outliers). In this work, we propose $\textit{RegCache}$, a training-free algorithm that mitigates outliers in large-scale pretrained vision encoders and serves as a plug-in module that can be applied on top of other quantization methods. The proposed RegCache introduces outlier-prone yet semantically meaningless prefix tokens to the target vision encoder, which prevents other tokens from having outliers. Notably, we observe that outliers in vision encoders behave differently from those in language models, motivating two technical innovations: middle-layer prefixing and token deletion. Experiments show that our method consistently improves the accuracy of quantized models across both text-supervised and self-supervised vision encoders.

**Link**: [arxiv](https://arxiv.org/abs/2510.04547v3),  [pdf](https://arxiv.org/pdf/2510.04547v3)

**Tags**: cs.LG cs.CV 



### InstanceV: Instance-Level Video Generation
**Authors**: Yuheng Chen, Teng Hu, Jiangning Zhang, Zhucun Xue, Ran Yi, Lizhuang Ma

**Updated**: 2025-11-28T12:52:37Z

**Summary**: Recent advances in text-to-video diffusion models have enabled the generation of high-quality videos conditioned on textual descriptions. However, most existing text-to-video models rely solely on textual conditions, lacking general fine-grained controllability over video generation. To address this challenge, we propose InstanceV, a video generation framework that enables i) instance-level control and ii) global semantic consistency. Specifically, with the aid of proposed Instance-aware Masked Cross-Attention mechanism, InstanceV maximizes the utilization of additional instance-level grounding information to generate correctly attributed instances at designated spatial locations. To improve overall consistency, We introduce the Shared Timestep-Adaptive Prompt Enhancement module, which connects local instances with global semantics in a parameter-efficient manner. Furthermore, we incorporate Spatially-Aware Unconditional Guidance during both training and inference to alleviate the disappearance of small instances. Finally, we propose a new benchmark, named InstanceBench, which combines general video quality metrics with instance-aware metrics for more comprehensive evaluation on instance-level video generation. Extensive experiments demonstrate that InstanceV not only achieves remarkable instance-level controllability in video generation, but also outperforms existing state-of-the-art models in both general quality and instance-aware metrics across qualitative and quantitative evaluations.

**Link**: [arxiv](https://arxiv.org/abs/2511.23146v1),  [pdf](https://arxiv.org/pdf/2511.23146v1)

**Tags**: cs.CV 



### Automated Generation of MDPs Using Logic Programming and LLMs for Robotic Applications
**Authors**: Enrico Saccon, Davide De Martini, Matteo Saveriano, Edoardo Lamon, Luigi Palopoli, Marco Roveri

**Updated**: 2025-11-28T12:48:30Z

**Summary**: We present a novel framework that integrates Large Language Models (LLMs) with automated planning and formal verification to streamline the creation and use of Markov Decision Processes (MDP). Our system leverages LLMs to extract structured knowledge in the form of a Prolog knowledge base from natural language (NL) descriptions. It then automatically constructs an MDP through reachability analysis, and synthesises optimal policies using the Storm model checker. The resulting policy is exported as a state-action table for execution. We validate the framework in three human-robot interaction scenarios, demonstrating its ability to produce executable policies with minimal manual effort. This work highlights the potential of combining language models with formal methods to enable more accessible and scalable probabilistic planning in robotics.

**Link**: [arxiv](https://arxiv.org/abs/2511.23143v1),  [pdf](https://arxiv.org/pdf/2511.23143v1)

**Tags**: cs.RO cs.AI 



### Multi-chain Graph Refinement and Selection for Reliable Reasoning in Large Language Models
**Authors**: Yujiao Yang, Jing Lian, Linhui Li

**Updated**: 2025-11-28T12:35:16Z

**Summary**: The complex reasoning ability of Large Language Models (LLMs) poses a critical bottleneck for their practical applications. Test-time expansion methods such as Tree-of-Thought (ToT) and Graph-of-Thought (GoT) enhance reasoning by introducing intermediate reasoning structures, tree search, or graph-based exploration mechanisms. However, their reasoning strategies suffer from limited diversity, redundant search branches, and inadequate integration and error correction across heterogeneous reasoning paths. To address these limitations, we propose a novel reasoning framework called Multi-chain Graph Refinement & Selection (MGRS), which first generates multiple diverse reasoning trajectories for a given problem, refines candidate responses using a composite self- and cross-verification strategy, then constructs a reasoning relation graph and estimates the success rate of intermediate nodes, and finally computes cumulative success rates to select the most reliable answer and corresponding reasoning trajectory. Experimental results demonstrate that MGRS significantly advances both the reasoning capability and computational efficiency of reasoning enhancement methods. Across six benchmark datasets spanning four distinct tasks, MGRS achieves an average accuracy of 82.9%, outperforming state-of-the-art baselines by a clear margin of 2.1%. Remarkably, on the 24-point game, MGRS attains 100% accuracy for the first time, while delivering a 13.6x speed-up compared to the leading Forest of Thoughts framework.

**Link**: [arxiv](https://arxiv.org/abs/2511.23136v1),  [pdf](https://arxiv.org/pdf/2511.23136v1)

**Tags**: cs.CL cs.AI 



### A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space
**Authors**: Huijie Liu, Shuhao Cui, Haoxiang Cao, Shuai Ma, Kai Wu, Guoliang Kang

**Updated**: 2025-11-28T12:30:49Z

**Summary**: Innovative visual stylization is a cornerstone of artistic creation, yet generating novel and consistent visual styles remains a significant challenge. Existing generative approaches typically rely on lengthy textual prompts, reference images, or parameter-efficient fine-tuning to guide style-aware image generation, but often struggle with style consistency, limited creativity, and complex style representations. In this paper, we affirm that a style is worth one numerical code by introducing the novel task, code-to-style image generation, which produces images with novel, consistent visual styles conditioned solely on a numerical style code. To date, this field has only been primarily explored by the industry (e.g., Midjourney), with no open-source research from the academic community. To fill this gap, we propose CoTyle, the first open-source method for this task. Specifically, we first train a discrete style codebook from a collection of images to extract style embeddings. These embeddings serve as conditions for a text-to-image diffusion model (T2I-DM) to generate stylistic images. Subsequently, we train an autoregressive style generator on the discrete style embeddings to model their distribution, allowing the synthesis of novel style embeddings. During inference, a numerical style code is mapped to a unique style embedding by the style generator, and this embedding guides the T2I-DM to generate images in the corresponding style. Unlike existing methods, our method offers unparalleled simplicity and diversity, unlocking a vast space of reproducible styles from minimal input. Extensive experiments validate that CoTyle effectively turns a numerical code into a style controller, demonstrating a style is worth one code.

**Link**: [arxiv](https://arxiv.org/abs/2511.10555v5),  [pdf](https://arxiv.org/pdf/2511.10555v5)

**Tags**: cs.CV cs.AI 



### Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy
**Authors**: Teng Hu, Zhentao Yu, Guozhen Zhang, Zihan Su, Zhengguang Zhou, Youliang Zhang, Yuan Zhou, Qinglin Lu, Ran Yi

**Updated**: 2025-11-28T12:25:17Z

**Summary**: The synthesis of synchronized audio-visual content is a key challenge in generative AI, with open-source models facing challenges in robust audio-video alignment. Our analysis reveals that this issue is rooted in three fundamental challenges of the joint diffusion process: (1) Correspondence Drift, where concurrently evolving noisy latents impede stable learning of alignment; (2) inefficient global attention mechanisms that fail to capture fine-grained temporal cues; and (3) the intra-modal bias of conventional Classifier-Free Guidance (CFG), which enhances conditionality but not cross-modal synchronization. To overcome these challenges, we introduce Harmony, a novel framework that mechanistically enforces audio-visual synchronization. We first propose a Cross-Task Synergy training paradigm to mitigate drift by leveraging strong supervisory signals from audio-driven video and video-driven audio generation tasks. Then, we design a Global-Local Decoupled Interaction Module for efficient and precise temporal-style alignment. Finally, we present a novel Synchronization-Enhanced CFG (SyncCFG) that explicitly isolates and amplifies the alignment signal during inference. Extensive experiments demonstrate that Harmony establishes a new state-of-the-art, significantly outperforming existing methods in both generation fidelity and, critically, in achieving fine-grained audio-visual synchronization.

**Link**: [arxiv](https://arxiv.org/abs/2511.21579v2),  [pdf](https://arxiv.org/pdf/2511.21579v2)

**Tags**: cs.CV 



### Joint Optimization of Pilot Length, Pilot Assignment, and Power Allocation for Cell-free MIMO Systems with Graph Neural Networks
**Authors**: Yao Peng, Tingting Liu, Chenyang Yang

**Updated**: 2025-11-28T12:21:28Z

**Summary**: In user-centric cell-free multi-antenna systems, pilot contamination degrades spectral efficiency (SE) severely. To mitigate pilot contamination, existing works jointly optimize pilot assignment and power allocation by assuming fixed pilot length, which fail to balance pilot overhead against the contamination. To maximize net-SE, we jointly optimize pilot length, pilot assignment, and power allocation with deep learning. Since the pilot length is a variable, the size of pilot assignment matrix is unknown during the optimization. To cope with the challenge, we design size-generalizable graph neural networks (GNNs). We prove that pilot assignment policy is a one-to-many mapping, and improperly designed GNNs cannot learn the optimal policy. We tackle this issue by introducing feature enhancement. To improve learning performance, we design a contamination-aware attention mechanism for the GNNs. Given that pilot assignment and power allocation respectively depend on large- and small-scale channels, we develop a dual-timescale GNN framework to explore the potential. To reduce inference time, a single-timescale GNN is also designed. Simulation results show that the designed GNNs outperform existing methods in terms of net-SE, training complexity, and inference time, and can be well generalized across problem scales and channels.

**Link**: [arxiv](https://arxiv.org/abs/2511.23128v1),  [pdf](https://arxiv.org/pdf/2511.23128v1)

**Tags**: eess.SP 



### Unsupervised Variational Acoustic Clustering
**Authors**: Luan Vinícius Fiorio, Bruno Defraene, Johan David, Frans Widdershoven, Wim van Houtum, Ronald M. Aarts

**Updated**: 2025-11-28T12:17:20Z

**Summary**: We propose an unsupervised variational acoustic clustering model for clustering audio data in the time-frequency domain. The model leverages variational inference, extended to an autoencoder framework, with a Gaussian mixture model as a prior for the latent space. Specifically designed for audio applications, we introduce a convolutional-recurrent variational autoencoder optimized for efficient time-frequency processing. Our experimental results considering a spoken digits dataset demonstrate a significant improvement in accuracy and clustering performance compared to traditional methods, showcasing the model's enhanced ability to capture complex audio patterns.

**Link**: [arxiv](https://arxiv.org/abs/2503.18579v2),  [pdf](https://arxiv.org/pdf/2503.18579v2)

**Tags**: eess.AS eess.SP 



### Evolutionary Discovery of Heuristic Policies for Traffic Signal Control
**Authors**: Ruibing Wang, Shuhan Guo, Zeen Li, Zhen Wang, Quanming Yao

**Updated**: 2025-11-28T12:11:17Z

**Summary**: Traffic Signal Control (TSC) involves a challenging trade-off: classic heuristics are efficient but oversimplified, while Deep Reinforcement Learning (DRL) achieves high performance yet suffers from poor generalization and opaque policies. Online Large Language Models (LLMs) provide general reasoning but incur high latency and lack environment-specific optimization. To address these issues, we propose Temporal Policy Evolution for Traffic (\textbf{\method{}}), which uses LLMs as an evolution engine to derive specialized heuristic policies. The framework introduces two key modules: (1) Structured State Abstraction (SSA), converting high-dimensional traffic data into temporal-logical facts for reasoning; and (2) Credit Assignment Feedback (CAF), tracing flawed micro-decisions to poor macro-outcomes for targeted critique. Operating entirely at the prompt level without training, \method{} yields lightweight, robust policies optimized for specific traffic environments, outperforming both heuristics and online LLM actors.

**Link**: [arxiv](https://arxiv.org/abs/2511.23122v1),  [pdf](https://arxiv.org/pdf/2511.23122v1)

**Tags**: cs.AI 



### Dripper: Token-Efficient Main HTML Extraction with a Lightweight LM
**Authors**: Mengjie Liu, Jiahui Peng, Pei Chu, Jiantao Qiu, Ren Ma, He Zhu, Rui Min, Lindong Lu, Wenchang Ning, Linfeng Hou, Kaiwen Liu, Yuan Qu, Zhenxiang Li, Chao Xu, Zhongying Tu, Wentao Zhang, Conghui He

**Updated**: 2025-11-28T12:04:46Z

**Summary**: Accurately and efficiently extracting main content from general web pages is of great significance for obtaining training data for large models. Using well-pre-trained decoder-only generative language models offers excellent document comprehension capabilities, thereby effectively enhancing parsing quality. However, it remains constrained by issues such as context window length, inference cost, and format hallucination. We present Dripper, an efficient HTML main content extraction framework powered by lightweight language models, which addresses these challenges through four key innovations: (1) We design a specialized HTML simplification algorithm that reduces input token count to 22\% compared to raw HTML while preserving critical structural information; (2) We reformulate main content extraction as a semantic block sequence classification task, significantly reducing inference cost; (3) We introduce a controlled decoding mechanism that strictly constrains the output space through logits processors, effectively eliminating hallucination issues common in small-scale models; (4) We propose WebMainBench, an evaluation dataset containing over 7,800 web pages with meticulously human-annotated main content extraction labels. Experimental results demonstrate that using only a 0.6B parameter model, Dripper achieves state-of-the-art performance across all evaluation benchmarks and outperforms all baseline methods, attaining an ROUGE-N F1 score of 81.58\%( 83.13\% with fall-back strategy) on our proposed WebMainBench dataset.

**Link**: [arxiv](https://arxiv.org/abs/2511.23119v1),  [pdf](https://arxiv.org/pdf/2511.23119v1)

**Tags**: cs.CL 



### A Spectral Koopman Approximation Framework for Stochastic Reaction Networks
**Authors**: Ankit Gupta, Mustafa Khammash

**Updated**: 2025-11-28T11:57:17Z

**Summary**: Stochastic reaction networks (SRNs) are a general class of continuous-time Markov jump processes used to model a wide range of systems, including biochemical dynamics in single cells, ecological and epidemiological populations, and queueing or communication networks. Yet analyzing their dynamics remains challenging because these processes are high-dimensional and their transient behavior can vary substantially across different initial molecular or population states. Here we introduce a spectral framework for the stochastic Koopman operator that provides a tractable, low-dimensional representation of SRN dynamics over continuous time, together with computable error estimates. By exploiting the compactness of the Koopman operator, we recover dominant spectral modes directly from simulated or experimental data, enabling efficient prediction of moments, event probabilities, and other summary statistics across all initial states. We further derive continuous-time parameter sensitivities and cross-spectral densities, offering new tools for probing noise structure and frequency-domain behavior. We demonstrate the approach on biologically relevant systems, including synthetic intracellular feedback controllers, stochastic oscillators, and inference of initial-state distributions from high-temporal-resolution flow cytometry. Together, these results establish spectral Koopman analysis as a powerful and general framework for studying stochastic dynamical systems across the biological, ecological, and computational sciences.

**Link**: [arxiv](https://arxiv.org/abs/2511.23114v1),  [pdf](https://arxiv.org/pdf/2511.23114v1)

**Tags**: q-bio.MN math.PR q-bio.QM 



### db-SP: Accelerating Sparse Attention for Visual Generative Models with Dual-Balanced Sequence Parallelism
**Authors**: Siqi Chen, Ke Hong, Tianchen Zhao, Ruiqi Xie, Zhenhua Zhu, Xudong Zhang, Yu Wang

**Updated**: 2025-11-28T11:55:46Z

**Summary**: Scaling Diffusion Transformer (DiT) inference via sequence parallelism is critical for reducing latency in visual generation, but is severely hampered by workload imbalance when applied to models employing block-wise sparse attention. The imbalance stems from the inherent variation in sparsity across attention heads and the irregular distribution of dense blocks within the sparse mask, when sequence parallelism is applied along the head dimension (as in Ulysses) or the block dimension (as in Ring Attention). In this paper, we formalize a sparse imbalance ratio to quantify the imbalance, and propose db-SP, a sparsity-aware sequence parallelism technique that tackles the challenge. db-SP contains a dual-level partitioning approach that achieves near-perfect workload balance at both the head and block levels with negligible overhead. Furthermore, to handle the evolving sparsity patterns across denoising steps and layers, db-SP dynamically determines the parallel degrees for the head and block dimensions at runtime. Experimental results demonstrate that db-SP delivers an end-to-end speedup of 1.25x and an attention-specific speedup of 1.40x over state-of-the-art sequence parallel methods on average. Code is available at https://github.com/thu-nics/db-SP.

**Link**: [arxiv](https://arxiv.org/abs/2511.23113v1),  [pdf](https://arxiv.org/pdf/2511.23113v1)

**Tags**: cs.CV cs.LG 



### Consequences of radially correlated rotation curves for galaxy mass models
**Authors**: Helena Chase, Diego Dado, Katherine E. Harborne, Kyle A. Oman

**Updated**: 2025-11-28T11:49:33Z

**Summary**: Consecutive points in rotation curve measurements are correlated with each other, but this is usually ignored when constructing galaxy mass models. We apply the data-driven approach proposed by Posti (2022) to include the characteristic amplitude and scale length of such correlations as `nuisance parameters'. We construct mass models for $134$ galaxies from the SPARC rotation curve compilation with Navarro-Frenk-White (NFW) and pseudo-isothermal sphere (pISO) models for the dark halo. Allowing for correlations in the rotation curves generally improves the goodness of fit for both halo models, often yielding a formally good fit ($χ^2_\mathrm{r}\approx 1$) and model uncertainties that seem more representative of the constraining power of the data. For both halo models the inference on the typical correlation amplitude and scale length are very similar and physically plausible, $\sim 20\,\mathrm{km}\,\mathrm{s}^{-1}$ and $\sim 5\,\mathrm{kpc}$, respectively. The parametric form that we use to describe the correlations is intentionally simple, and our fitting approach makes the parameters describing possible correlations prone to `absorbing' other systematic errors, so we regard these estimates as upper limits. Without allowing for correlations we find a statistical preference for the pISO over the NFW model for $88$/$134$ galaxies; this preference essentially disappears when correlations are allowed for. Accounting for correlations in rotation curves when constructing mass models fundamentally affects how they are interpreted, highlighting an important systematic uncertainty that affects evidence for cusps or cores in dark matter haloes.

**Link**: [arxiv](https://arxiv.org/abs/2511.23106v1),  [pdf](https://arxiv.org/pdf/2511.23106v1)

**Tags**: astro-ph.GA 



### Mind Reading or Misreading? LLMs on the Big Five Personality Test
**Authors**: Francesco Di Cursi, Chiara Boldrini, Marco Conti, Andrea Passarella

**Updated**: 2025-11-28T11:40:30Z

**Summary**: We evaluate large language models (LLMs) for automatic personality prediction from text under the binary Five Factor Model (BIG5). Five models -- including GPT-4 and lightweight open-source alternatives -- are tested across three heterogeneous datasets (Essays, MyPersonality, Pandora) and two prompting strategies (minimal vs. enriched with linguistic and psychological cues). Enriched prompts reduce invalid outputs and improve class balance, but also introduce a systematic bias toward predicting trait presence. Performance varies substantially: Openness and Agreeableness are relatively easier to detect, while Extraversion and Neuroticism remain challenging. Although open-source models sometimes approach GPT-4 and prior benchmarks, no configuration yields consistently reliable predictions in zero-shot binary settings. Moreover, aggregate metrics such as accuracy and macro-F1 mask significant asymmetries, with per-class recall offering clearer diagnostic value. These findings show that current out-of-the-box LLMs are not yet suitable for APPT, and that careful coordination of prompt design, trait framing, and evaluation metrics is essential for interpretable results.

**Link**: [arxiv](https://arxiv.org/abs/2511.23101v1),  [pdf](https://arxiv.org/pdf/2511.23101v1)

**Tags**: cs.CL cs.AI 



### Inference for quantile-parametrized families via CDF confidence bands
**Authors**: Srijan Chattopadhyay, Siddhaarth Sarkar, Arun Kumar Kuchibhotla

**Updated**: 2025-11-28T11:19:27Z

**Summary**: Quantile-based distribution families are an important subclass of parametric families, capable of exhibiting a wide range of behaviors using very few parameters. These parametric models present significant challenges for classical methods, since the CDF and density do not have a closed-form expression. Furthermore, approximate maximum likelihood estimation and related procedures may yield non-$\sqrt{n}$ and non-normal asymptotics over regions of the parameter space, making bootstrap and resampling techniques unreliable. We develop a novel inference framework that constructs confidence sets by inverting distribution-free confidence bands for the empirical CDF through the known quantile function. Our proposed inference procedure provides a principled and assumption-lean alternative in this setting, requiring no distributional assumptions beyond the parametric model specification and avoiding the computational and theoretical difficulties associated with likelihood-based methods for these complex parametric families. We demonstrate our framework on Tukey Lambda and generalized Lambda distributions, evaluate its performance through simulation studies, and illustrate its practical utility with an application to both a small-sample dataset (Twin Study) and a large-sample dataset (Spanish household incomes).

**Link**: [arxiv](https://arxiv.org/abs/2511.23086v1),  [pdf](https://arxiv.org/pdf/2511.23086v1)

**Tags**: stat.ME 



### A General Bayesian Nonparametric Approach for Estimating Population-Level and Conditional Causal Effects
**Authors**: Yongseok Hur, Joonhyuk Jung, Juhee Lee

**Updated**: 2025-11-28T11:17:48Z

**Summary**: We propose a Bayesian nonparametric (BNP) approach to causal inference using observational data consisting of outcome, treatment, and a set of confounders. The conditional distribution of the outcome given treatment and confounders is modeled flexibly using a dependent nonparametric mixture model, in which both the atoms and the weights vary with the confounders. The proposed BNP model is well suited for causal inference problems, as it does not rely on parametric assumptions about how the conditional distribution depends on the confounders. In particular, the model effectively adjusts for confounding and improves the modeling of treatment effect heterogeneity, leading to more accurate estimation of both the average treatment effect (ATE) and heterogeneous treatment effects (HTE). Posterior inference under the proposed model is computationally efficient due to the use of data augmentation. Extensive evaluations demonstrate that the proposed model offers competitive or superior performance compared to a wide range of recent methods spanning various statistical approaches, including Bayesian additive regression tree (BART) models, which are well known for their strong empirical performance. More importantly, the model provides fully probabilistic inference on quantities of interest that other methods cannot easily provide, using their posterior distributions.

**Link**: [arxiv](https://arxiv.org/abs/2511.23085v1),  [pdf](https://arxiv.org/pdf/2511.23085v1)

**Tags**: stat.ME 



### AutoPatch: Multi-Agent Framework for Patching Real-World CVE Vulnerabilities
**Authors**: Minjae Seo, Wonwoo Choi, Myoungsung You, Seungwon Shin

**Updated**: 2025-11-28T11:15:57Z

**Summary**: Large Language Models (LLMs) have emerged as promising tools in software development, enabling automated code generation and analysis. However, their knowledge is limited to a fixed cutoff date, making them prone to generating code vulnerable to newly disclosed CVEs. Frequent fine-tuning with new CVE sets is costly, and existing LLM-based approaches focus on oversimplified CWE examples and require providing explicit bug locations to LLMs, limiting their ability to patch complex real-world vulnerabilities. To address these limitations, we propose AutoPatch, a multi-agent framework designed to patch vulnerable LLM-generated code, particularly those introduced after the LLMs' knowledge cutoff. AutoPatch integrates Retrieval-Augmented Generation (RAG) with a structured database of recently disclosed vulnerabilities, comprising 525 code snippets derived from 75 high-severity CVEs across real-world systems such as the Linux kernel and Chrome. AutoPatch combines semantic and taint analysis to identify the most relevant CVE and leverages enhanced Chain-of-Thought (CoT) reasoning to construct enriched prompts for verification and patching. Our unified similarity model, which selects the most relevant vulnerabilities, achieves 90.4 percent accuracy in CVE matching. AutoPatch attains 89.5 percent F1-score for vulnerability verification and 95.0 percent accuracy in patching, while being over 50x more cost-efficient than traditional fine-tuning approaches.

**Link**: [arxiv](https://arxiv.org/abs/2505.04195v2),  [pdf](https://arxiv.org/pdf/2505.04195v2)

**Tags**: cs.CR 



### REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance
**Authors**: Chuyi Kong, Gao Wei, Jing Ma, Hongzhan Lin, Yaxin Fan

**Updated**: 2025-11-28T11:08:27Z

**Summary**: The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.

**Link**: [arxiv](https://arxiv.org/abs/2511.20233v2),  [pdf](https://arxiv.org/pdf/2511.20233v2)

**Tags**: cs.CL 



### TRACE: Temporally Reliable Anatomically-Conditioned 3D CT Generation with Enhanced Efficiency
**Authors**: Minye Shao, Xingyu Miao, Haoran Duan, Zeyu Wang, Jingkun Chen, Yawen Huang, Xian Wu, Jingjing Deng, Yang Long, Yefeng Zheng

**Updated**: 2025-11-28T10:52:25Z

**Summary**: 3D medical image generation is essential for data augmentation and patient privacy, calling for reliable and efficient models suited for clinical practice. However, current methods suffer from limited anatomical fidelity, restricted axial length, and substantial computational cost, placing them beyond reach for regions with limited resources and infrastructure. We introduce TRACE, a framework that generates 3D medical images with spatiotemporal alignment using a 2D multimodal-conditioned diffusion approach. TRACE models sequential 2D slices as video frame pairs, combining segmentation priors and radiology reports for anatomical alignment, incorporating optical flow to sustain temporal coherence. During inference, an overlapping-frame strategy links frame pairs into a flexible length sequence, reconstructed into a spatiotemporally and anatomically aligned 3D volume. Experimental results demonstrate that TRACE effectively balances computational efficiency with preserving anatomical fidelity and spatiotemporal consistency. Code is available at: https://github.com/VinyehShaw/TRACE.

**Link**: [arxiv](https://arxiv.org/abs/2507.00802v2),  [pdf](https://arxiv.org/pdf/2507.00802v2)

**Tags**: cs.CV 



### An empirical estimate of the electricity supply curve from market outcomes
**Authors**: Jorge Sánchez Canales, Alice Lixuan Xu, Chiara Fusar Bassini, Lynn H. Kaack, Lion Hirth

**Updated**: 2025-11-28T10:49:35Z

**Summary**: Researchers and electricity sector practitioners frequently require the supply curve of electricity markets and the price elasticity of supply for purposes such as price forecasting, policy analyses or market power assessment. It is common practice to construct supply curves from engineering data such as installed capacity and fuel prices. In this study, we propose a data-driven methodology to estimate the supply curve of electricity market empirically, i.e. from observed prices and quantities without further modeling assumptions. Due to the massive swings in fuel prices during the European energy crisis, a central task is detecting periods of stable supply curves. To this end, we implement two alternative clustering methods, one based on the fundamental drivers of electricity supply and the other directly on observed market outcomes. We apply our methods to the German electricity market between 2019 and 2024. We find that both approaches identify almost identical regimes shifts, supporting the idea of stable supply regimes stemming from stable drivers. Supply conditions are often stable for extended periods, but evolved rapidly during the energy crisis, triggering a rapid succession of regimes. Fuel prices were the dominant drivers of regime shifts, while conventional plant availability and the nuclear phase-out play a comparatively minor role. Our approach produces empirical supply curves suitable for causal inference and counterfactual analysis of market outcomes.

**Link**: [arxiv](https://arxiv.org/abs/2511.23068v1),  [pdf](https://arxiv.org/pdf/2511.23068v1)

**Tags**: econ.EM 



### Progressive Localisation in Localist LLMs
**Authors**: Joachim Diederich

**Updated**: 2025-11-28T10:44:50Z

**Summary**: This paper demonstrates that progressive localization, the gradual increase of attention locality from early distributed layers to late localized layers, represents the optimal architecture for creating interpretable large language models (LLMs) while preserving performance. Through systematic experimentation with GPT-2 fine-tuned on The Psychology of Artificial Superintelligence, we evaluate seven locality configurations ranging from fully distributed to strictly localist, with five progressive schedules implementing polynomial increases (linear through quintic). We investigate whether interpretability constraints can be aligned with natural semantic structure while being applied strategically across network depth. We demonstrate that progressive semantic localization, combining adaptive semantic block partitioning with steep polynomial locality schedules, achieves near-baseline language modeling performance while providing interpretable attention patterns. Multiple independent training runs with different random seeds establish that results are statistically robust and highly reproducible. The approach dramatically outperforms both fixed-window localization and naive uniform locality constraints. Analysis reveals that maintaining flexibility through low-fidelity constraints preserves model capacity while providing interpretability benefits, and that steep schedules concentrating locality in decision-critical final layers while preserving distributed learning in early layers achieve near-baseline attention distribution characteristics. These findings demonstrate that interpretability mechanisms should align with semantic structure to achieve practical performance-interpretability tradeoffs for trustworthy AI systems.

**Link**: [arxiv](https://arxiv.org/abs/2511.18375v2),  [pdf](https://arxiv.org/pdf/2511.18375v2)

**Tags**: cs.AI 



### Conveying Imagistic Thinking in TCM Translation: A Prompt Engineering and LLM-Based Evaluation Framework
**Authors**: Jiatong Han

**Updated**: 2025-11-28T10:35:10Z

**Summary**: Traditional Chinese Medicine theory is built on imagistic thinking, in which medical principles and diagnostic and therapeutic logic are structured through metaphor and metonymy. However, existing English translations largely rely on literal rendering, making it difficult for target-language readers to reconstruct the underlying conceptual networks and apply them in clinical practice. This study adopted a human-in-the-loop framework and selected four passages from the medical canon Huangdi Neijing that are fundamental in theory. Through prompt-based cognitive scaffolding, DeepSeek V3.1 was guided to identify metaphor and metonymy in the source text and convey the theory in translation. In the evaluation stage, ChatGPT 5 Pro and Gemini 2.5 Pro were instructed by prompts to simulate three types of real-world readers. Human translations, baseline model translations, and prompt-adjusted translations were scored by the simulated readers across five cognitive dimensions, followed by structured interviews and Interpretative Phenomenological Analysis. Results show that the prompt-adjusted LLM translations perform best across all five dimensions, with high cross-model and cross-role consistency. The interview themes reveal differences between human and machine translation, effective strategies for metaphor and metonymy transfer, and readers' cognitive preferences. This study provides a cognitive, efficient and replicable HITL methodological pathway for translation of ancient, concept-dense texts like TCM.

**Link**: [arxiv](https://arxiv.org/abs/2511.23059v1),  [pdf](https://arxiv.org/pdf/2511.23059v1)

**Tags**: cs.CL cs.AI 



### FIELDS: Face reconstruction with accurate Inference of Expression using Learning with Direct Supervision
**Authors**: Chen Ling, Henglin Shi, Hedvig Kjellström

**Updated**: 2025-11-28T10:30:52Z

**Summary**: Facial expressions convey the bulk of emotional information in human communication, yet existing 3D face reconstruction methods often miss subtle affective details due to reliance on 2D supervision and lack of 3D ground truth. We propose FIELDS (Face reconstruction with accurate Inference of Expression using Learning with Direct Supervision) to address these limitations by extending self-supervised 2D image consistency cues with direct 3D expression parameter supervision and an auxiliary emotion recognition branch. Our encoder is guided by authentic expression parameters from spontaneous 4D facial scans, while an intensity-aware emotion loss encourages the 3D expression parameters to capture genuine emotion content without exaggeration. This dual-supervision strategy bridges the 2D/3D domain gap and mitigates expression-intensity bias, yielding high-fidelity 3D reconstructions that preserve subtle emotional cues. From a single image, FIELDS produces emotion-rich face models with highly realistic expressions, significantly improving in-the-wild facial expression recognition performance without sacrificing naturalness.

**Link**: [arxiv](https://arxiv.org/abs/2511.21245v2),  [pdf](https://arxiv.org/pdf/2511.21245v2)

**Tags**: cs.CV 



### MindPower: Enabling Theory-of-Mind Reasoning in VLM-based Embodied Agents
**Authors**: Ruoxuan Zhang, Qiyun Zheng, Zhiyu Zhou, Ziqi Liao, Siyu Wu, Jian-Yu Jiang-Lin, Bin Wen, Hongxia Xie, Jianlong Fu, Wen-Huang Cheng

**Updated**: 2025-11-28T10:24:44Z

**Summary**: Theory of Mind (ToM) refers to the ability to infer others' mental states, such as beliefs, desires, and intentions. Current vision-language embodied agents lack ToM-based decision-making, and existing benchmarks focus solely on human mental states while ignoring the agent's own perspective, hindering coherent decision and action generation. To address this, we propose MindPower, a Robot-Centric framework integrating Perception, Mental Reasoning, Decision Making and Action. Given multimodal inputs, MindPower first perceives the environment and human states, then performs ToM Reasoning to model both self and others, and finally generates decisions and actions guided by inferred mental states. Furthermore, we introduce Mind-Reward, a novel optimization objective that encourages VLMs to produce consistent ToM Reasoning and behavior. Our model outperforms GPT-4o by 12.77% in decision making and 12.49% in action generation.

**Link**: [arxiv](https://arxiv.org/abs/2511.23055v1),  [pdf](https://arxiv.org/pdf/2511.23055v1)

**Tags**: cs.AI 



### Deep Reinforcement Learning for Drone Route Optimization in Post-Disaster Road Assessment
**Authors**: Huatian Gong, Jiuh-Biing Sheu, Zheng Wang, Xiaoguang Yang, Ran Yan

**Updated**: 2025-11-28T10:23:25Z

**Summary**: Rapid post-disaster road damage assessment is critical for effective emergency response, yet traditional optimization methods suffer from excessive computational time and require domain knowledge for algorithm design, making them unsuitable for time-sensitive disaster scenarios. This study proposes an attention-based encoder-decoder model (AEDM) for rapid drone routing decision in post-disaster road damage assessment. The method employs deep reinforcement learning to determine high-quality drone assessment routes without requiring algorithmic design knowledge. A network transformation method is developed to convert link-based routing problems into equivalent node-based formulations, while a synthetic road network generation technique addresses the scarcity of large-scale training datasets. The model is trained using policy optimization with multiple optima (POMO) with multi-task learning capabilities to handle diverse parameter combinations. Experimental results demonstrate two key strengths of AEDM: it outperforms commercial solvers by 20--71\% and traditional heuristics by 23--35\% in solution quality, while achieving rapid inference (1--2 seconds) versus 100--2,000 seconds for traditional methods. The model exhibits strong generalization across varying problem scales, drone numbers, and time constraints, consistently outperforming baseline methods on unseen parameter distributions and real-world road networks. The proposed method effectively balances computational efficiency with solution quality, making it particularly suitable for time-critical disaster response applications where rapid decision-making is essential for saving lives. The source code for AEDM is publicly available at https://github.com/PJ-HTU/AEDM-for-Post-disaster-road-assessment.

**Link**: [arxiv](https://arxiv.org/abs/2509.01886v2),  [pdf](https://arxiv.org/pdf/2509.01886v2)

**Tags**: cs.LG 



### Social Perceptions of English Spelling Variation on Twitter: A Comparative Analysis of Human and LLM Responses
**Authors**: Dong Nguyen, Laura Rosseel

**Updated**: 2025-11-28T10:06:18Z

**Summary**: Spelling variation (e.g. funnnn vs. fun) can influence the social perception of texts and their writers: we often have various associations with different forms of writing (is the text informal? does the writer seem young?). In this study, we focus on the social perception of spelling variation in online writing in English and study to what extent this perception is aligned between humans and large language models (LLMs). Building on sociolinguistic methodology, we compare LLM and human ratings on three key social attributes of spelling variation (formality, carefulness, age). We find generally strong correlations in the ratings between humans and LLMs. However, notable differences emerge when we analyze the distribution of ratings and when comparing between different types of spelling variation.

**Link**: [arxiv](https://arxiv.org/abs/2511.23041v1),  [pdf](https://arxiv.org/pdf/2511.23041v1)

**Tags**: cs.CL 



### Time Extrapolation with Graph Convolutional Autoencoder and Tensor Train Decomposition
**Authors**: Yuanhong Chen, Federico Pichi, Zhen Gao, Gianluigi Rozza

**Updated**: 2025-11-28T09:59:17Z

**Summary**: Graph autoencoders have gained attention in nonlinear reduced-order modeling of parameterized partial differential equations defined on unstructured grids. Despite they provide a geometrically consistent way of treating complex domains, applying such architectures to parameterized dynamical systems for temporal prediction beyond the training data, i.e. the extrapolation regime, is still a challenging task due to the simultaneous need of temporal causality and generalizability in the parametric space. In this work, we explore the integration of graph convolutional autoencoders (GCAs) with tensor train (TT) decomposition and Operator Inference (OpInf) to develop a time-consistent reduced-order model. In particular, high-fidelity snapshots are represented as a combination of parametric, spatial, and temporal cores via TT decomposition, while OpInf is used to learn the evolution of the latter. Moreover, we enhance the generalization performance by developing a multi-fidelity two-stages approach in the framework of Deep Operator Networks (DeepONet), treating the spatial and temporal cores as the trunk networks, and the parametric core as the branch network. Numerical results, including heat-conduction, advection-diffusion and vortex-shedding phenomena, demonstrate great performance in effectively learning the dynamic in the extrapolation regime for complex geometries, also in comparison with state-of-the-art approaches e.g. MeshGraphNets.

**Link**: [arxiv](https://arxiv.org/abs/2511.23037v1),  [pdf](https://arxiv.org/pdf/2511.23037v1)

**Tags**: math.NA cs.LG 



### Chiplet-Based RISC-V SoC with Modular AI Acceleration
**Authors**: Suhas Suresh Bharadwaj, Prerana Ramkumar

**Updated**: 2025-11-28T09:49:31Z

**Summary**: Achieving high performance, energy efficiency, and cost-effectiveness while maintaining architectural flexibility is a critical challenge in the development and deployment of edge AI devices. Monolithic SoC designs struggle with this complex balance mainly due to low manufacturing yields (below 16%) at advanced 360 mm^2 process nodes. This paper presents a novel chiplet-based RISC-V SoC architecture that addresses these limitations through modular AI acceleration and intelligent system level optimization. Our proposed design integrates 4 different key innovations in a 30mm x 30mm silicon interposer: adaptive cross-chiplet Dynamic Voltage and Frequency Scaling (DVFS); AI-aware Universal Chiplet Interconnect Express (UCIe) protocol extensions featuring streaming flow control units and compression-aware transfers; distributed cryptographic security across heterogeneous chiplets; and intelligent sensor-driven load migration. The proposed architecture integrates a 7nm RISC-V CPU chiplet with dual 5nm AI accelerators (15 TOPS INT8 each), 16GB HBM3 memory stacks, and dedicated power management controllers. Experimental results across industry standard benchmarks like MobileNetV2, ResNet-50 and real-time video processing demonstrate significant performance improvements. The AI-optimized configuration achieves ~14.7% latency reduction, 17.3% throughput improvement, and 16.2% power reduction compared to previous basic chiplet implementations. These improvements collectively translate to a 40.1% efficiency gain corresponding to ~3.5 mJ per MobileNetV2 inference (860 mW/244 images/s), while maintaining sub-5ms real-time capability across all experimented workloads. These performance upgrades demonstrate that modular chiplet designs can achieve near-monolithic computational density while enabling cost efficiency, scalability and upgradeability, crucial for next-generation edge AI device applications.

**Link**: [arxiv](https://arxiv.org/abs/2509.18355v4),  [pdf](https://arxiv.org/pdf/2509.18355v4)

**Tags**: cs.AR cs.AI 



### Efficient Reasoning via Thought-Training and Thought-Free Inference
**Authors**: Canhui Wu, Qiong Cao, Chao Xue, Wei Xi, Xiaodong He

**Updated**: 2025-11-28T09:49:18Z

**Summary**: Recent advances in large language models (LLMs) have leveraged explicit Chain-of-Thought (CoT) prompting to improve reasoning accuracy. However, most existing methods primarily focus on compressing verbose reasoning outputs. These Long-to-Short transformations aim to improve efficiency, but require a large amount of short CoT data. In this work, we introduce \textbf{3TF} (\textbf{T}hought-\textbf{T}raining and \textbf{T}hought-\textbf{F}ree inference), a framework for efficient reasoning that takes a Short-to-Long perspective. We first train a hybrid model that can operate in both reasoning and non-reasoning modes, and then further train it on CoT-annotated data to internalize structured reasoning, while enforcing concise, thought-free outputs at inference time using the no-reasoning mode. Unlike compression-based approaches, 3TF improves the reasoning quality of non-reasoning outputs, enabling models to perform rich internal reasoning implicitly while keeping external outputs short. Empirically, 3TF-trained models obtain large improvements on reasoning benchmarks under thought-free inference, demonstrating that high quality reasoning can be learned and executed implicitly without explicit step-by-step generation.

**Link**: [arxiv](https://arxiv.org/abs/2511.03408v3),  [pdf](https://arxiv.org/pdf/2511.03408v3)

**Tags**: cs.CL 



### Resistant Topology Inference in Consensus Networks: A Feedback-Based Design
**Authors**: Yushan Li, Jiabao He, Dimos V. Dimarogonas

**Updated**: 2025-11-28T09:41:56Z

**Summary**: Consensus networks are widely deployed in numerous civil and industrial applications. However, the process of reaching a common consensus among nodes can unintentionally reveal the network's topology to external observers by appropriate inference techniques. This paper investigates a feedback-based resistant inference design to prevent the topology from being inferred using data, while preserving the original consensus convergence. First, we characterize the conditions to preserve the original consensus, and introduce the ''accurate inference'' notion, which accounts for both the uniqueness of the solution to topology inference (solvability) and the deviation from the original topology (accuracy). Then, we employ invariant subspace analysis to characterize the solvability. Even when unique inference remains possible, we provide necessary and sufficient conditions for the feedback design to induce inaccurate inference, and give a Laplacian structure based distributed design. Simulations validate the effectiveness of the method.

**Link**: [arxiv](https://arxiv.org/abs/2511.23023v1),  [pdf](https://arxiv.org/pdf/2511.23023v1)

**Tags**: eess.SY 



### Masked Diffusion for Generative Recommendation
**Authors**: Kulin Shah, Bhuvesh Kumar, Neil Shah, Liam Collins

**Updated**: 2025-11-28T09:36:26Z

**Summary**: Generative recommendation (GR) with semantic IDs (SIDs) has emerged as a promising alternative to traditional recommendation approaches due to its performance gains, capitalization on semantic information provided through language model embeddings, and inference and storage efficiency. Existing GR with SIDs works frame the probability of a sequence of SIDs corresponding to a user's interaction history using autoregressive modeling. While this has led to impressive next item prediction performances in certain settings, these autoregressive GR with SIDs models suffer from expensive inference due to sequential token-wise decoding, potentially inefficient use of training data and bias towards learning short-context relationships among tokens. Inspired by recent breakthroughs in NLP, we propose to instead model and learn the probability of a user's sequence of SIDs using masked diffusion. Masked diffusion employs discrete masking noise to facilitate learning the sequence distribution, and models the probability of masked tokens as conditionally independent given the unmasked tokens, allowing for parallel decoding of the masked tokens. We demonstrate through thorough experiments that our proposed method consistently outperforms autoregressive modeling. This performance gap is especially pronounced in data-constrained settings and in terms of coarse-grained recall, consistent with our intuitions. Moreover, our approach allows the flexibility of predicting multiple SIDs in parallel during inference while maintaining superior performance to autoregressive modeling.

**Link**: [arxiv](https://arxiv.org/abs/2511.23021v1),  [pdf](https://arxiv.org/pdf/2511.23021v1)

**Tags**: cs.LG cs.IR 



### ExoJAX Retrievals of VLT/CRIRES Spectra of Luhman 16AB: C/O Ratios and Systematic Uncertainties
**Authors**: Hibiki Yama, Kento Masuda, Yui Kawashima, Hajime Kawahara

**Updated**: 2025-11-28T09:33:28Z

**Summary**: We present atmospheric retrievals of the benchmark brown dwarf binary Luhman 16AB using high-resolution VLT/CRIRES spectra and the differentiable framework ExoJAX. We derive elemental abundances and temperature-pressure ($T$-$P$) profiles while explicitly testing the robustness of the results against major sources of systematic uncertainty. We first perform retrievals with a power-law $T$-$P$ profile and assess the sensitivity of inferred molecular abundances and C/O ratios to different CO line lists (ExoMol, HITEMP with air- and H2-broadening). We then introduce a flexible Gaussian process-based $T$-$P$ profile, allowing a non-parametric characterization of the thermal structure and a more conservative treatment of uncertainties. For both components, we infer C/O ratios of about 0.67, slightly above solar, with line list systematics at the 7 percent level emerging as the dominant source of uncertainty, whereas assumptions about $T$-$P$ parameterization or photometric variability play a lesser role. The retrieved $T$-$P$ profiles and molecular abundances are broadly consistent with atmospheric models and equilibrium chemistry. Our results establish Luhman 16AB as a key anchor for substellar C/O measurements, demonstrate the utility of flexible $T$-$P$ modeling in high-resolution retrievals, and highlight the importance of systematic tests -- particularly line list uncertainties -- for robust comparisons between brown dwarfs and giant exoplanets.

**Link**: [arxiv](https://arxiv.org/abs/2511.23018v1),  [pdf](https://arxiv.org/pdf/2511.23018v1)

**Tags**: astro-ph.EP astro-ph.SR 



### Maritime Activities Observed Through Open-Access Positioning Data: Moving and Stationary Vessels in the Baltic Sea
**Authors**: Moritz Hütten

**Updated**: 2025-11-28T09:30:33Z

**Summary**: Understanding past and present maritime activity patterns is critical for navigation safety, environmental assessment, and commercial operations. An increasing number of services now openly provide positioning data from the Automatic Identification System (AIS) via ground-based receivers. We show that coastal vessel activity can be reconstructed from open access data with high accuracy, even with limited data quality and incomplete receiver coverage. For three months of open AIS data in the Baltic Sea from August to October 2024, we present (i) cleansing and reconstruction methods to improve the data quality, and (ii) a journey model that converts AIS message data into vessel counts, traffic estimates, and spatially resolved vessel density at a resolution of $\sim$400 m. Vessel counts are provided, along with their uncertainties, for both moving and stationary activity. Vessel density maps also enable the identification of port locations, and we infer the most crowded and busiest coastal areas in the Baltic Sea. We find that on average, $\gtrsim$4000 vessels simultaneously operate in the Baltic Sea, and more than 300 vessels enter or leave the area each day. Our results agree within 20\% with previous studies relying on proprietary data.

**Link**: [arxiv](https://arxiv.org/abs/2511.23016v1),  [pdf](https://arxiv.org/pdf/2511.23016v1)

**Tags**: cs.CE cs.CY cs.LG 



### RvLLM: LLM Runtime Verification with Domain Knowledge
**Authors**: Yedi Zhang, Sun Yi Emma, Annabelle Lee Jia En, Jin Song Dong

**Updated**: 2025-11-28T09:26:03Z

**Summary**: Large language models (LLMs) have emerged as a dominant AI paradigm due to their exceptional text understanding and generation capabilities. However, their tendency to generate inconsistent or erroneous outputs challenges their reliability, especially in high-stakes domains requiring accuracy and trustworthiness. Existing research primarily focuses on detecting and mitigating model misbehavior in general-purpose scenarios, often overlooking the potential of integrating domain-specific knowledge. In this work, we advance misbehavior detection by incorporating domain knowledge. The core idea is to design a general specification language that enables domain experts to customize domain-specific predicates in a lightweight and intuitive manner, supporting later runtime verification of LLM outputs. To achieve this, we design a novel specification language, ESL, and introduce a runtime verification framework, RvLLM, to validate LLM output against domain-specific constraints defined in ESL. We evaluate RvLLM on three representative tasks: violation detection against Singapore Rapid Transit Systems Act, numerical comparison, and inequality solving. Experimental results demonstrate that RvLLM effectively detects erroneous outputs across various LLMs in a lightweight and flexible manner. The results reveal that despite their impressive capabilities, LLMs remain prone to low-level errors due to limited interpretability and a lack of formal guarantees during inference, and our framework offers a potential long-term solution by leveraging expert domain knowledge to rigorously and efficiently verify LLM outputs.

**Link**: [arxiv](https://arxiv.org/abs/2505.18585v3),  [pdf](https://arxiv.org/pdf/2505.18585v3)

**Tags**: cs.AI cs.CL cs.LO 



### Joint Bayesian Inference of Parameter and Discretization Error Uncertainties in ODE Models
**Authors**: Shoji Toyota, Yuto Miyatake

**Updated**: 2025-11-28T09:22:07Z

**Summary**: We address the problem of Bayesian inference for parameters in ordinary differential equation (ODE) models based on observational data. Conventional approaches in this setting typically rely on numerical solvers such as the Euler or Runge-Kutta methods. However, these methods generally do not account for the discretization error induced by discretizing the ODE model. We propose a Bayesian inference framework for ODE models that explicitly quantifies discretization errors. Our method models discretization error as a random variable and performs Bayesian inference on both ODE parameters and variances of the randomized discretization errors, referred to as the discretization error variance. A key idea of our approach is the introduction of a Markov prior on the temporal evolution of the discretization error variances, enabling the inference problem to be formulated as a state-space model. Furthermore, we propose a specific form of the Markov prior that arises naturally from standard discretization error analysis. This prior depends on the step size in the numerical solver, and we discuss its asymptotic property in the limit as the step size approaches zero. Numerical experiments illustrate that the proposed method can simultaneously quantify uncertainties in both the ODE parameters and the discretization errors, and can produce posterior distributions over the parameters with broader support by accounting for discretization error.

**Link**: [arxiv](https://arxiv.org/abs/2511.23010v1),  [pdf](https://arxiv.org/pdf/2511.23010v1)

**Tags**: stat.ME 



### Incorporating neutron star physics into gravitational wave inference with neural priors
**Authors**: Thibeau Wouters, Peter T. H. Pang, Tim Dietrich, Chris Van Den Broeck

**Updated**: 2025-11-28T08:47:58Z

**Summary**: Bayesian inference, widely used in gravitational-wave parameter estimation, depends on the choice of priors, i.e., on our previously existing knowledge. However, to investigate neutron star mergers, priors are often chosen in an agnostic way, leaving valuable information from nuclear physics and independent observations of neutron stars unused. In this work, we propose to encode information on neutron star physics into data-driven prior distributions constructed with normalizing flows, referred to as neural priors. These priors take input from constraints on the nuclear equation of state and neutron star population models. Applied to GW170817, GW190425, and GW230529, we highlight two contributions of the framework. First, we demonstrate its ability to provide source classification and to enable model selection of equation of state constraints for loud signals such as GW170817, directly from the gravitational-wave data. Second, we obtain narrower constraints on the source properties through these informed priors. As a result, the neural priors consistently recover higher luminosity distances compared to agnostic priors. Our method paves the way for classifying future ambiguous low-mass mergers observed through gravitational waves and for continuously incorporating advances in our understanding of neutron star properties into gravitational-wave data analysis.

**Link**: [arxiv](https://arxiv.org/abs/2511.22987v1),  [pdf](https://arxiv.org/pdf/2511.22987v1)

**Tags**: astro-ph.HE gr-qc hep-ph 



### ShoppingComp: Are LLMs Really Ready for Your Shopping Cart?
**Authors**: Huaixiao Tou, Ying Zeng, Cong Ma, Muzhi Li, Minghao Li, Weijie Yuan, He Zhang, Kai Jia

**Updated**: 2025-11-28T08:32:54Z

**Summary**: We present ShoppingComp, a challenging real-world benchmark for rigorously evaluating LLM-powered shopping agents on three core capabilities: precise product retrieval, expert-level report generation, and safety critical decision making. Unlike prior e-commerce benchmarks, ShoppingComp introduces highly complex tasks under the principle of guaranteeing real products and ensuring easy verifiability, adding a novel evaluation dimension for identifying product safety hazards alongside recommendation accuracy and report quality. The benchmark comprises 120 tasks and 1,026 scenarios, curated by 35 experts to reflect authentic shopping needs. Results reveal stark limitations of current LLMs: even state-of-the-art models achieve low performance (e.g., 11.22% for GPT-5, 3.92% for Gemini-2.5-Flash). These findings highlight a substantial gap between research benchmarks and real-world deployment, where LLMs make critical errors such as failure to identify unsafe product usage or falling for promotional misinformation, leading to harmful recommendations. ShoppingComp fills the gap and thus establishes a new standard for advancing reliable and practical agents in e-commerce.

**Link**: [arxiv](https://arxiv.org/abs/2511.22978v1),  [pdf](https://arxiv.org/pdf/2511.22978v1)

**Tags**: cs.CL 



### An LLM-Assisted Multi-Agent Control Framework for Roll-to-Roll Manufacturing Systems
**Authors**: Jiachen Li, Shihao Li, Christopher Martin, Zijun Chen, Dongmei Chen, Wei Li

**Updated**: 2025-11-28T08:30:12Z

**Summary**: Roll-to-roll manufacturing requires precise tension and velocity control to ensure product quality, yet controller commissioning and adaptation remain time-intensive processes dependent on expert knowledge. This paper presents an LLM-assisted multi-agent framework that automates control system design and adaptation for R2R systems while maintaining safety. The framework operates through five phases: system identification from operational data, automated controller selection and tuning, sim-to-real adaptation with safety verification, continuous monitoring with diagnostic capabilities, and periodic model refinement. Experimental validation on a R2R system demonstrates successful tension regulation and velocity tracking under significant model uncertainty, with the framework achieving performance convergence through iterative adaptation. The approach reduces manual tuning effort while providing transparent diagnostic information for maintenance planning, offering a practical pathway for integrating AI-assisted automation in manufacturing control systems.

**Link**: [arxiv](https://arxiv.org/abs/2511.22975v1),  [pdf](https://arxiv.org/pdf/2511.22975v1)

**Tags**: eess.SY 



### BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation
**Authors**: Zeyu Zhang, Shuning Chang, Yuanyu He, Yizeng Han, Jiasheng Tang, Fan Wang, Bohan Zhuang

**Updated**: 2025-11-28T08:25:59Z

**Summary**: Generating minute-long videos is a critical step toward developing world models, providing a foundation for realistic extended scenes and advanced AI simulators. The emerging semi-autoregressive (block diffusion) paradigm integrates the strengths of diffusion and autoregressive models, enabling arbitrary-length video generation and improving inference efficiency through KV caching and parallel sampling. However, it yet faces two enduring challenges: (i) KV-cache-induced long-horizon error accumulation, and (ii) the lack of fine-grained long-video benchmarks and coherence-aware metrics. To overcome these limitations, we propose BlockVid, a novel block diffusion framework equipped with semantic-aware sparse KV cache, an effective training strategy called Block Forcing, and dedicated chunk-wise noise scheduling and shuffling to reduce error propagation and enhance temporal consistency. We further introduce LV-Bench, a fine-grained benchmark for minute-long videos, complete with new metrics evaluating long-range coherence. Extensive experiments on VBench and LV-Bench demonstrate that BlockVid consistently outperforms existing methods in generating high-quality, coherent minute-long videos. In particular, it achieves a 22.2% improvement on VDE Subject and a 19.4% improvement on VDE Clarity in LV-Bench over the state of the art approaches. Project website: https://ziplab.co/BlockVid. Inferix (Code): https://github.com/alibaba-damo-academy/Inferix.

**Link**: [arxiv](https://arxiv.org/abs/2511.22973v1),  [pdf](https://arxiv.org/pdf/2511.22973v1)

**Tags**: cs.CV 



### Training-Free Loosely Speculative Decoding: Accepting Semantically Correct Drafts Beyond Exact Match
**Authors**: Jinze Li, Yixing Xu, Guanchen Li, Shuo Yang, Jinfeng Xu, Xuanwu Yin, Dong Li, Edith C. H. Ngai, Emad Barsoum

**Updated**: 2025-11-28T08:23:30Z

**Summary**: Large language models (LLMs) achieve strong performance across diverse tasks but suffer from high inference latency due to their autoregressive generation. Speculative Decoding (SPD) mitigates this issue by verifying candidate tokens in parallel from a smaller draft model, yet its strict exact-match verification discards many semantically valid continuations. Moreover, existing training-based SPD methods often suffer from performance degradation on out-of-distribution (OOD) tasks. To this end, we propose Training-Free Loosely Speculative Decoding (FLy), a novel method that loosens the rigid verification criterion by leveraging the target model's self-corrective behavior to judge whether a draft-target mismatch remains semantically valid. FLy introduces a two-tier mechanism: an entropy-level gate that identifies whether the current token allows multiple plausible alternatives or is nearly deterministic, and a token-level deferred window that distinguishes genuine errors from differently worded yet semantically correct variants. To further reduce latency, we design a multi-level acceleration strategy that accelerates not only the target model but also the drafter itself. Owing to its training-free design, FLy composes seamlessly with arbitrary draft-target pairs and generalizes across models and domains without hyperparameter re-tuning. Experiments show that FLy preserves more than 99% of the target model's accuracy while achieving an average 2.81x speedup on Llama-3.1-70B-Instruct and 5.07x speedup on the 405B variant. Notably, on out-of-domain datasets, our method remains highly effective and outperforms the training-based method EAGLE-3 by 1.62x.

**Link**: [arxiv](https://arxiv.org/abs/2511.22972v1),  [pdf](https://arxiv.org/pdf/2511.22972v1)

**Tags**: cs.CL 



### Experts are all you need: A Composable Framework for Large Language Model Inference
**Authors**: Shrihari Sridharan, Sourjya Roy, Anand Raghunathan, Kaushik Roy

**Updated**: 2025-11-28T08:00:16Z

**Summary**: Large Language Models (LLMs) have achieved state-of-the-art accuracies in a variety of natural language processing (NLP) tasks. However, this success comes at the cost of increased model sizes which leads to additional computational burden. Mixture of Experts (MoEs) overcome this bottleneck by decoupling model capacity from computation by only activating a subset of parameters or "experts". However, these models require joint pretraining of these experts along with the router and do not model multi-step reasoning. In contrast, multi-agent frameworks improve reasoning by decomposing complex problems into modular subtasks. However, these frameworks rely on sequential "plan--act--observe" loops, which introduce significant latency. Our work, Comp-LLM, addresses these challenges by introducing a composable inference framework that enables cross-expert collaboration via an explicit sub-query dependency graph. Comp-LLM consists of three components: (1) A Sub-query Generator that decomposes an input query, assigns each sub-query to an appropriate expert using embedding similarity, and constructs a dependency graph; (2) A Query Executor that processes nodes in the graph and identifies opportunities for parallelism based on dependencies and resource constraints; and (3) A Response Aggregator that synthesizes intermediate expert responses into a coherent final answer. Across several benchmarks, Comp-LLM achieves up to 11.01% accuracy improvement over monolithic LLMs of similar size, while offering 1.67x--3.56x reduction in model size with no significant degradation relative to the largest model in its family. Additionally, Comp-LLM provides 1.1x--1.7x latency improvement compared to sequential sub-query processing.

**Link**: [arxiv](https://arxiv.org/abs/2511.22955v1),  [pdf](https://arxiv.org/pdf/2511.22955v1)

**Tags**: cs.LG 



### InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training
**Authors**: Pengkai Wang, Linus, Pengwei Liu, Zhijie Sang, Congkai Xie, Hongxia Yang

**Updated**: 2025-11-28T07:52:44Z

**Summary**: Reinforcement learning has powered many of the recent breakthroughs in large language models, especially for tasks where rewards can be computed automatically, such as code generation. However, these methods deteriorate in open-ended domains like medical consultation, where feedback is inherently ambiguous, highly context-dependent, and cannot be reduced to a reliable scalar signal. In such settings, RL must either rely on supervision-intensive reward models that often fail to generalize, or it falls into pathological behaviors such as reward hacking - an especially troubling risk for high-stakes medical dialogue. To address these limitations, we introduce ORBIT, an open-ended rubric-based incremental training framework for high-stakes medical dialogue. ORBIT integrates synthetic dialogue generation with dynamically constructed rubrics that serve as adaptive guides for incremental RL. Instead of relying on external medical knowledge bases or handcrafted rule sets, ORBIT uses rubric-driven feedback to steer the learning process. Its judge component can be instantiated with general-purpose instruction-following LLMs, removing the need for any task-specific fine-tuning. Applied to the Qwen3-4B-Instruct model, ORBIT raises the HealthBench-Hard score from 7.0 to 27.5 using only 2k training samples, achieving SOTA performance for models at this scale. With larger rubric datasets, ORBIT-trained models further compete with the strongest open-source baselines on HealthBench-Hard. Our analysis shows that rubric-guided RL consistently improves consultation quality across diverse medical scenarios. We also apply such rubric generation and training pipeline to InfoBench, where ORBIT enhances instruction-following performance, highlighting the generality of rubric-based feedback.

**Link**: [arxiv](https://arxiv.org/abs/2510.15859v3),  [pdf](https://arxiv.org/pdf/2510.15859v3)

**Tags**: cs.CL cs.AI 



## Keyword: LLM Deployment 
 ### Thinking by Doing: Building Efficient World Model Reasoning in LLMs via Multi-turn Interaction
**Authors**: Bao Shu, Yan Cai, Jianjian Sun, Chunrui Han, En Yu, Liang Zhao, Jingcheng Hu, Yinmin Zhang, Haoran Lv, Yuang Peng, Zheng Ge, Xiangyu Zhang, Daxin Jiang, Xiangyu Yue

**Updated**: 2025-11-28T18:59:47Z

**Summary**: Developing robust world model reasoning is crucial for large language model (LLM) agents to plan and interact in complex environments. While multi-turn interaction offers a superior understanding of environmental dynamics via authentic feedback, current approaches often impose a rigid reasoning process, which constrains the model's active learning, ultimately hindering efficient world model reasoning. To address these issues, we explore world-model internalization through efficient interaction and active reasoning (WMAct), which liberates the model from structured reasoning, allowing the model to shape thinking directly through its doing, and achieves effective and efficient world model reasoning with two key mechanisms: (1) a reward rescaling mechanism adjusting outcome reward based on action efficacy to incentivize redundancy reduction and purposeful interaction; (2) an interaction frequency annealing strategy to progressively reduce the maximum allowed interaction turns, which compels the model to condense its learning and internalize environmental dynamics rather than over-relying on environmental cues. Our experiments on Sokoban, Maze, and Taxi show that WMAct yields effective world model reasoning capable of resolving tasks in a single turn that previously required multiple interactions and fosters strong transferability to complex environments, improving performance on a suite of reasoning benchmarks.

**Link**: [arxiv](https://arxiv.org/abs/2511.23476v1),  [pdf](https://arxiv.org/pdf/2511.23476v1)

**Tags**: cs.AI 



### ThetaEvolve: Test-time Learning on Open Problems
**Authors**: Yiping Wang, Shao-Rong Su, Zhiyuan Zeng, Eva Xu, Liliang Ren, Xinyu Yang, Zeyi Huang, Xuehai He, Luyao Ma, Baolin Peng, Hao Cheng, Pengcheng He, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, Yelong Shen

**Updated**: 2025-11-28T18:58:14Z

**Summary**: Recent advances in large language models (LLMs) have enabled breakthroughs in mathematical discovery, exemplified by AlphaEvolve, a closed-source system that evolves programs to improve bounds on open problems. However, it relies on ensembles of frontier LLMs to achieve new bounds and is a pure inference system that models cannot internalize the evolving strategies. We introduce ThetaEvolve, an open-source framework that simplifies and extends AlphaEvolve to efficiently scale both in-context learning and Reinforcement Learning (RL) at test time, allowing models to continually learn from their experiences in improving open optimization problems. ThetaEvolve features a single LLM, a large program database for enhanced exploration, batch sampling for higher throughput, lazy penalties to discourage stagnant outputs, and optional reward shaping for stable training signals, etc. ThetaEvolve is the first evolving framework that enable a small open-source model, like DeepSeek-R1-0528-Qwen3-8B, to achieve new best-known bounds on open problems (circle packing and first auto-correlation inequality) mentioned in AlphaEvolve. Besides, across two models and four open tasks, we find that ThetaEvolve with RL at test-time consistently outperforms inference-only baselines, and the model indeed learns evolving capabilities, as the RL-trained checkpoints demonstrate faster progress and better final performance on both trained target task and other unseen tasks. We release our code publicly: https://github.com/ypwang61/ThetaEvolve

**Link**: [arxiv](https://arxiv.org/abs/2511.23473v1),  [pdf](https://arxiv.org/pdf/2511.23473v1)

**Tags**: cs.LG cs.CL 



### NegBLEURT Forest: Leveraging Inconsistencies for Detecting Jailbreak Attacks
**Authors**: Lama Sleem, Jerome Francois, Lujun Li, Nathan Foucher, Niccolo Gentile, Radu State

**Updated**: 2025-11-28T18:49:16Z

**Summary**: Jailbreak attacks designed to bypass safety mechanisms pose a serious threat by prompting LLMs to generate harmful or inappropriate content, despite alignment with ethical guidelines. Crafting universal filtering rules remains difficult due to their inherent dependence on specific contexts. To address these challenges without relying on threshold calibration or model fine-tuning, this work introduces a semantic consistency analysis between successful and unsuccessful responses, demonstrating that a negation-aware scoring approach captures meaningful patterns. Building on this insight, a novel detection framework called NegBLEURT Forest is proposed to evaluate the degree of alignment between outputs elicited by adversarial prompts and expected safe behaviors. It identifies anomalous responses using the Isolation Forest algorithm, enabling reliable jailbreak detection. Experimental results show that the proposed method consistently achieves top-tier performance, ranking first or second in accuracy across diverse models using the crafted dataset, while competing approaches exhibit notable sensitivity to model and data variations.

**Link**: [arxiv](https://arxiv.org/abs/2511.11784v2),  [pdf](https://arxiv.org/pdf/2511.11784v2)

**Tags**: cs.CR cs.AI 



### Accelerated Execution of Bayesian Neural Networks using a Single Probabilistic Forward Pass and Code Generation
**Authors**: Bernhard Klein, Falk Selker, Hendrik Borras, Sophie Steger, Franz Pernkopf, Holger Fröning

**Updated**: 2025-11-28T18:35:20Z

**Summary**: Machine learning models perform well across domains such as diagnostics, weather forecasting, NLP, and autonomous driving, but their limited uncertainty handling restricts use in safety-critical settings. Traditional neural networks often fail to detect out-of-domain (OOD) data and may output confident yet incorrect predictions. Bayesian neural networks (BNNs) address this by providing probabilistic estimates, but incur high computational cost because predictions require sampling weight distributions and multiple forward passes. The Probabilistic Forward Pass (PFP) offers a highly efficient approximation to Stochastic Variational Inference (SVI) by assuming Gaussian-distributed weights and activations, enabling fully analytic uncertainty propagation and replacing sampling with a single deterministic forward pass. We present an end-to-end pipeline for training, compiling, optimizing, and deploying PFP-based BNNs on embedded ARM CPUs. Using the TVM deep learning compiler, we implement a dedicated library of Gaussian-propagating operators for multilayer perceptrons and convolutional neural networks, combined with manual and automated tuning strategies. Ablation studies show that PFP consistently outperforms SVI in computational efficiency, achieving speedups of up to 4200x for small mini-batches. PFP-BNNs match SVI-BNNs on Dirty-MNIST in accuracy, uncertainty estimation, and OOD detection while greatly reducing compute cost. These results highlight the potential of combining Bayesian approximations with code generation to enable efficient BNN deployment on resource-constrained systems.

**Link**: [arxiv](https://arxiv.org/abs/2511.23440v1),  [pdf](https://arxiv.org/pdf/2511.23440v1)

**Tags**: cs.LG cs.AR cs.DC stat.ML 



### Towards Continuous Intelligence Growth: Self-Training, Continual Learning, and Dual-Scale Memory in SuperIntelliAgent
**Authors**: Jianzhe Lin, Zeyu Pan, Yun Zhu, Ruiqi Song, Jining Yang

**Updated**: 2025-11-28T18:32:49Z

**Summary**: We introduce SuperIntelliAgent, an agentic learning framework that couples a trainable small diffusion model (the learner) with a frozen large language model (the verifier) to enable continual intelligence growth through self-supervised interaction. Unlike conventional supervised fine-tuning, SuperIntelliAgent learns autonomously without annotation: the learner generates candidate outputs, the verifier evaluates them through step-by-step reasoning, and their interaction produces chosen/rejected pairs for Direct Preference Optimization (DPO). This converts each input into a pseudo-training signal for continual improvement. The framework integrates dual-scale memory: short-term in-context memory that preserves reasoning traces across refinement cycles, and long-term memory that consolidates acquired knowledge through lightweight on-the-fly fine-tuning. A replay buffer retains samples that show verifiable progress and replays them as auxiliary supervision, reinforcing recent learning while forming adaptive curricula. SuperIntelliAgent is infrastructure-agnostic and can be plugged into existing agentic frameworks while turning ordinary inference loops into a lifelong optimization process. We posit that pairing a trainable learner with a reasoning-capable verifier forms a minimal reliable unit of growing intelligence, as paired feedback and partial-history replay yield richer learning curricula and stronger preference alignment. With a small number of automatically generated DPO pairs, the learner improves across all benchmarks, indicating that this mechanism provides a promising direction for continual intelligence accumulation and real-world deployment.

**Link**: [arxiv](https://arxiv.org/abs/2511.23436v1),  [pdf](https://arxiv.org/pdf/2511.23436v1)

**Tags**: cs.AI 



### Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards
**Authors**: Charles Arnal, Gaëtan Narozniak, Vivien Cabannes, Yunhao Tang, Julia Kempe, Remi Munos

**Updated**: 2025-11-28T18:12:20Z

**Summary**: Reinforcement learning (RL) is increasingly used to align large language models (LLMs). Off-policy methods offer greater implementation simplicity and data efficiency than on-policy techniques, but often result in suboptimal performance. In this work, we study the intermediate range of algorithms between off-policy RL and supervised fine-tuning by analyzing a simple off-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with $r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$ emphasizes high-reward samples, while raising it penalizes low-reward ones more heavily. We first provide a theoretical analysis of this off-policy REINFORCE algorithm, showing that when the baseline $V$ lower-bounds the expected reward, the algorithm enjoys a policy improvement guarantee. Our analysis reveals that while on-policy updates can safely leverage both positive and negative signals, off-policy updates benefit from focusing more on positive rewards than on negative ones. We validate our findings experimentally in a controlled stochastic bandit setting and through fine-tuning state-of-the-art LLMs on reasoning tasks.

**Link**: [arxiv](https://arxiv.org/abs/2506.20520v2),  [pdf](https://arxiv.org/pdf/2506.20520v2)

**Tags**: cs.LG cs.CL 



### Evaluating LLMs for One-Shot Patching of Real and Artificial Vulnerabilities
**Authors**: Aayush Garg, Zanis Ali Khan, Renzo Degiovanni, Qiang Tang

**Updated**: 2025-11-28T18:03:47Z

**Summary**: Automated vulnerability patching is crucial for software security, and recent advancements in Large Language Models (LLMs) present promising capabilities for automating this task. However, existing research has primarily assessed LLMs using publicly disclosed vulnerabilities, leaving their effectiveness on related artificial vulnerabilities largely unexplored. In this study, we empirically evaluate the patching effectiveness and complementarity of several prominent LLMs, such as OpenAI's GPT variants, LLaMA, DeepSeek, and Mistral models, using both real and artificial vulnerabilities. Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities. Our results reveal that LLMs patch real vulnerabilities more effectively compared to artificial ones. Additionally, our analysis reveals significant variability across LLMs in terms of overlapping (multiple LLMs patching the same vulnerabilities) and complementarity (vulnerabilities patched exclusively by a single LLM), emphasizing the importance of selecting appropriate LLMs for effective vulnerability patching.

**Link**: [arxiv](https://arxiv.org/abs/2511.23408v1),  [pdf](https://arxiv.org/pdf/2511.23408v1)

**Tags**: cs.CR cs.AI cs.SE 



### LFM2 Technical Report
**Authors**: Alexander Amini, Anna Banaszak, Harold Benoit, Arthur Böök, Tarek Dakhran, Song Duong, Alfred Eng, Fernando Fernandes, Marc Härkönen, Anne Harrington, Ramin Hasani, Saniya Karwa, Yuri Khrustalev, Maxime Labonne, Mathias Lechner, Valentine Lechner, Simon Lee, Zetian Li, Noel Loo, Jacob Marks, Edoardo Mosca, Samuel J. Paech, Paul Pak, Rom N. Parnichkun, Alex Quach, Ryan Rogers, Daniela Rus, Nayan Saxena, Bettina Schlager, Tim Seyde, Jimmy T. H. Smith, Aditya Tadimeti, Neehal Tumma

**Updated**: 2025-11-28T17:56:35Z

**Summary**: We present LFM2, a family of Liquid Foundation Models designed for efficient on-device deployment and strong task capabilities. Using hardware-in-the-loop architecture search under edge latency and memory constraints, we obtain a compact hybrid backbone that combines gated short convolutions with a small number of grouped query attention blocks, delivering up to 2x faster prefill and decode on CPUs compared to similarly sized models. The LFM2 family covers 350M-8.3B parameters, including dense models (350M, 700M, 1.2B, 2.6B) and a mixture-of-experts variant (8.3B total, 1.5B active), all with 32K context length. LFM2's training pipeline includes a tempered, decoupled Top-K knowledge distillation objective that avoids support mismatch; curriculum learning with difficulty-ordered data; and a three-stage post-training recipe of supervised fine-tuning, length-normalized preference optimization, and model merging. Pre-trained on 10-12T tokens, LFM2 models achieve strong results across diverse benchmarks; for example, LFM2-2.6B reaches 79.56% on IFEval and 82.41% on GSM8K. We further build multimodal and retrieval variants: LFM2-VL for vision-language tasks, LFM2-Audio for speech, and LFM2-ColBERT for retrieval. LFM2-VL supports tunable accuracy-latency tradeoffs via token-efficient visual processing, while LFM2-Audio separates audio input and output pathways to enable real-time speech-to-speech interaction competitive with models 3x larger. LFM2-ColBERT provides a low-latency encoder for queries and documents, enabling high-performance retrieval across multiple languages. All models are released with open weights and deployment packages for ExecuTorch, llama.cpp, and vLLM, making LFM2 a practical base for edge applications that need fast, memory-efficient inference and strong task capabilities.

**Link**: [arxiv](https://arxiv.org/abs/2511.23404v1),  [pdf](https://arxiv.org/pdf/2511.23404v1)

**Tags**: cs.LG cs.AI 



### Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization
**Authors**: Filip Sondej, Yushi Yang, Mikołaj Kniejski, Marcel Windys

**Updated**: 2025-11-28T17:35:49Z

**Summary**: Language models can retain dangerous knowledge and skills even after extensive safety fine-tuning, posing both misuse and misalignment risks. Recent studies show that even specialized unlearning methods can be easily reversed. To address this, we systematically evaluate many existing and novel components of unlearning methods and identify ones crucial for irreversible unlearning.   We introduce Disruption Masking, a technique in which we only allow updating weights, where the signs of the unlearning gradient and the retaining gradient are the same. This ensures all updates are non-disruptive.   Additionally, we identify the need for normalizing the unlearning gradients, and also confirm the usefulness of meta-learning. We combine these insights into MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and validate its effectiveness at preventing the recovery of dangerous capabilities. MUDMAN outperforms the prior TAR method by 40%, setting a new state-of-the-art for robust unlearning.

**Link**: [arxiv](https://arxiv.org/abs/2506.12484v5),  [pdf](https://arxiv.org/pdf/2506.12484v5)

**Tags**: cs.LG cs.AI cs.CL 



### Hierarchical AI-Meteorologist: LLM-Agent System for Multi-Scale and Explainable Weather Forecast Reporting
**Authors**: Daniil Sukhorukov, Andrei Zakharov, Nikita Glazkov, Katsiaryna Yanchanka, Vladimir Kirilin, Maxim Dubovitsky, Roman Sultimov, Yuri Maksimov, Ilya Makarov

**Updated**: 2025-11-28T17:27:06Z

**Summary**: We present the Hierarchical AI-Meteorologist, an LLM-agent system that generates explainable weather reports using a hierarchical forecast reasoning and weather keyword generation. Unlike standard approaches that treat forecasts as flat time series, our framework performs multi-scale reasoning across hourly, 6-hour, and daily aggregations to capture both short-term dynamics and long-term trends. Its core reasoning agent converts structured meteorological inputs into coherent narratives while simultaneously extracting a few keywords effectively summarizing the dominant meteorological events. These keywords serve as semantic anchors for validating consistency, temporal coherence and factual alignment of the generated reports. Using OpenWeather and Meteostat data, we demonstrate that hierarchical context and keyword-based validation substantially improve interpretability and robustness of LLM-generated weather narratives, offering a reproducible framework for semantic evaluation of automated meteorological reporting and advancing agent-based scientific reasoning.

**Link**: [arxiv](https://arxiv.org/abs/2511.23387v1),  [pdf](https://arxiv.org/pdf/2511.23387v1)

**Tags**: cs.AI 



### AugGen: Augmenting Task-Based Learning in Professional Creative Software with LLM-Generated Scaffolded UIs
**Authors**: Yimeng Liu, Misha Sra

**Updated**: 2025-11-28T17:22:43Z

**Summary**: Professional creative software often presents steep learning curves due to complex interfaces, lack of structured task-aware guidance, and unfamiliar domain terminology. To address these challenges and augment user learning experience, we introduce AugGen, a method for generating scaffolded user interfaces that simplify interface complexity and support task-based learning. With the user's task, our method surfaces task-relevant tools to reduce distracting features, organizes the tools around task workflow stages to offer execution guidance, connects tools with domain concepts to foster learning engagement, and progressively discloses advanced features to manage learning progress. To evaluate the method, we used our LLM-assisted pipeline to generate two task-specific scaffolded UIs and deployed them in Blender, our professional 3D modeling testbed. We invited both beginner (N=32) and expert (N=8) users to evaluate our implemented interfaces. Results show that the scaffolded interfaces significantly reduced user-perceived task load, enhanced task performance via embedded guidance, and augmented concept learning during task execution.

**Link**: [arxiv](https://arxiv.org/abs/2511.23379v1),  [pdf](https://arxiv.org/pdf/2511.23379v1)

**Tags**: cs.HC 



### Is Passive Expertise-Based Personalization Enough? A Case Study in AI-Assisted Test-Taking
**Authors**: Li Siyan, Jason Zhang, Akash Maharaj, Yuanming Shi, Yunyao Li

**Updated**: 2025-11-28T17:21:41Z

**Summary**: Novice and expert users have different systematic preferences in task-oriented dialogues. However, whether catering to these preferences actually improves user experience and task performance remains understudied. To investigate the effects of expertise-based personalization, we first built a version of an enterprise AI assistant with passive personalization. We then conducted a user study where participants completed timed exams, aided by the two versions of the AI assistant. Preliminary results indicate that passive personalization helps reduce task load and improve assistant perception, but reveal task-specific limitations that can be addressed through providing more user agency. These findings underscore the importance of combining active and passive personalization to optimize user experience and effectiveness in enterprise task-oriented environments.

**Link**: [arxiv](https://arxiv.org/abs/2511.23376v1),  [pdf](https://arxiv.org/pdf/2511.23376v1)

**Tags**: cs.HC cs.CL 



### Design, modelling and experimental validation of bipenniform shape memory alloy-based linear actuator integrable with hydraulic stroke amplification mechanism
**Authors**: Kanhaiya Lal Chaurasiya, Ruchira Kumar Pradhan, Yashaswi Sinha, Shivam Gupta, Ujjain Kumar Bidila, Digambar Killedar, Kapil Das Sahu, Bishakh Bhattacharya

**Updated**: 2025-11-28T17:18:25Z

**Summary**: The increasing industrial demand for alternative actuators over conventional electromagnetism-based systems having limited efficiency, bulky size, complex design due to in-built gear-train mechanisms, and high production and amortization costs necessitates the innovation in new actuator development. Integrating bio-inspired design principles into linear actuators could bring forth the next generation of adaptive and energy efficient smart material-based actuation systems. The present study amalgamates the advantages of bipenniform architecture, which generates high force in the given physiological region and a high power-to-weight ratio of shape memory alloy (SMA), into a novel bio-inspired SMA-based linear actuator. A mathematical model of a multi-layered bipenniform configuration-based SMA actuator was developed and validated experimentally. The current research also caters to the incorporation of failure mitigation strategies using design failure mode and effects analysis along with the experimental assessment of the performance of the developed actuator. The system has been benchmarked against an industry-developed stepper motor-driven actuator. It has shown promising results generating an actuation force of 257 N with 15 V input voltage, meeting the acceptable range for actuation operation. It further exhibits about 67% reduction in the weight of the drive mechanism, with 80% lesser component, 32% cost reduction, and 19% energy savings and similar envelope dimensions for assembly compatibility with dampers and louvers for easy onsite deployment. The study introduces SMA coil-based actuator as an advanced design that can be deployed for high force-high stroke applications. The bio-inspired SMA-based linear actuator has applications ranging from building automation controls to lightweight actuation systems for space robotics and medical prosthesis.

**Link**: [arxiv](https://arxiv.org/abs/2511.23372v1),  [pdf](https://arxiv.org/pdf/2511.23372v1)

**Tags**: cs.RO 



### Functional Program Synthesis with Higher-Order Functions and Recursion Schemes
**Authors**: Matheus Campos Fernandes

**Updated**: 2025-11-28T17:02:01Z

**Summary**: Program synthesis is the process of generating a computer program following a set of specifications, such as a set of input-output examples. It can be modeled as a search problem in which the search space is the set of all valid programs. As the search space is vast, brute force is usually not feasible, and search heuristics, such as genetic programming, also have difficulty navigating it without guidance. This text presents 2 novel GP algorithms that synthesize pure, typed, and functional programs: HOTGP and Origami. HOTGP uses strong types and a functional grammar, synthesizing Haskell code, with support for higher-order functions, $λ$-functions, and parametric polymorphism. Experimental results show that HOTGP is competitive with the state of the art. Additionally, Origami is an algorithm that tackles the challenge of effectively handling loops and recursion by exploring Recursion Schemes, in which the programs are composed of well-defined templates with only a few parts that need to be synthesized. The first implementation of Origami can synthesize solutions in several Recursion Schemes and data structures, being competitive with other GP methods in the literature, as well as LLMs. The latest version of Origami employs a novel procedure, called AC/DC, designed to improve the search-space exploration. It achieves considerable improvement over its previous version by raising success rates on every problem. Compared to similar methods in the literature, it has the highest count of problems solved with success rates of $100\%$, $\geq 75\%$, and $\geq 25\%$ across all benchmarks. In $18\%$ of all benchmark problems, it stands as the only method to reach $100\%$ success rate, being the first known approach to achieve it on any problem in PSB2. It also demonstrates competitive performance to LLMs, achieving the highest overall win-rate against Copilot among all GP methods.

**Link**: [arxiv](https://arxiv.org/abs/2511.23354v1),  [pdf](https://arxiv.org/pdf/2511.23354v1)

**Tags**: cs.NE 



### Performance Evaluation of Multi-Armed Bandit Algorithms for Wi-Fi Channel Access
**Authors**: Miguel Casasnovas, Francesc Wilhelmi, Richard Combes, Maksymilian Wojnar, Katarzyna Kosek-Szott, Szymon Szott, Anders Jonsson, Luis Esteve, Boris Bellalta

**Updated**: 2025-11-28T17:00:06Z

**Summary**: The adoption of dynamic, self-learning solutions for real-time wireless network optimization has recently gained significant attention due to the limited adaptability of existing protocols. This paper investigates multi-armed bandit (MAB) strategies as a data-driven approach for decentralized, online channel access optimization in Wi-Fi, targeting dynamic channel access settings: primary channel, channel width, and contention window (CW) adjustment. Key design aspects are examined, including the adoption of joint versus factorial action spaces, the inclusion of contextual information, and the nature of the action-selection strategy (optimism-driven, unimodal, or randomized). State-of-the-art algorithms and a proposed lightweight contextual approach, E-RLB, are evaluated through simulations. Results show that contextual and optimism-driven strategies consistently achieve the highest performance and fastest adaptation under recurrent conditions. Unimodal structures require careful graph construction to ensure that the unimodality assumption holds. Randomized exploration, adopted in the proposed E-RLB, can induce disruptive parameter reallocations, especially in multi-player settings. Decomposing the action space across several specialized agents accelerates convergence but increases sensitivity to randomized exploration and demands coordination under shared rewards to avoid correlated learning. Finally, despite its inherent inefficiencies from epsilon-greedy exploration, E-RLB demonstrates effective adaptation and learning, highlighting its potential as a viable low-complexity solution for realistic dynamic deployments.

**Link**: [arxiv](https://arxiv.org/abs/2511.23352v1),  [pdf](https://arxiv.org/pdf/2511.23352v1)

**Tags**: cs.NI 



### LLMAID: Identifying AI Capabilities in Android Apps with LLMs
**Authors**: Pei Liu, Terry Zhuo, Jiawei Deng, Thong James, Shidong Pan, Sherry Xu, Zhenchang Xing, Qinghua Lu, Xiaoning Du, Hongyu Zhang

**Updated**: 2025-11-28T16:36:03Z

**Summary**: Recent advancements in artificial intelligence (AI) and its widespread integration into mobile software applications have received significant attention, highlighting the growing prominence of AI capabilities in modern software systems. However, the inherent hallucination and reliability issues of AI continue to raise persistent concerns. Consequently, application users and regulators increasingly ask critical questions such as: Does the application incorporate AI capabilities? and What specific types of AI functionalities are embedded? Preliminary efforts have been made to identify AI capabilities in mobile software; however, existing approaches mainly rely on manual inspection and rule-based heuristics. These methods are not only costly and time-consuming but also struggle to adapt advanced AI techniques.   To address the limitations of existing methods, we propose LLMAID (Large Language Model for AI Discovery). LLMAID includes four main tasks: (1) candidate extraction, (2) knowledge base interaction, (3) AI capability analysis and detection, and (4) AI service summarization. We apply LLMAID to a dataset of 4,201 Android applications and demonstrate that it identifies 242% more real-world AI apps than state-of-the-art rule-based approaches. Our experiments show that LLM4AID achieves high precision and recall, both exceeding 90%, in detecting AI-related components. Additionally, a user study indicates that developers find the AI service summaries generated by LLMAID to be more informative and preferable to the original app descriptions. Finally, we leverage LLMAID to perform an empirical analysis of AI capabilities across Android apps. The results reveal a strong concentration of AI functionality in computer vision (54.80%), with object detection emerging as the most common task (25.19%).

**Link**: [arxiv](https://arxiv.org/abs/2511.19059v2),  [pdf](https://arxiv.org/pdf/2511.19059v2)

**Tags**: cs.SE 



### Do LLM-judges Align with Human Relevance in Cranfield-style Recommender Evaluation?
**Authors**: Gustavo Penha, Aleksandr V. Petrov, Claudia Hauff, Enrico Palumbo, Ali Vardasbi, Edoardo D'Amico, Francesco Fabbri, Alice Wang, Praveen Chandar, Henrik Lindstrom, Hugues Bouchard, Mounia Lalmas

**Updated**: 2025-11-28T16:10:39Z

**Summary**: Evaluating recommender systems remains a long-standing challenge, as offline methods based on historical user interactions and train-test splits often yield unstable and inconsistent results due to exposure bias, popularity bias, sampled evaluations, and missing-not-at-random patterns. In contrast, textual document retrieval benefits from robust, standardized evaluation via Cranfield-style test collections, which combine pooled relevance judgments with controlled setups. While recent work shows that adapting this methodology to recommender systems is feasible, constructing such collections remains costly due to the need for manual relevance judgments, thus limiting scalability. This paper investigates whether Large Language Models (LLMs) can serve as reliable automatic judges to address these scalability challenges. Using the ML-32M-ext Cranfield-style movie recommendation collection, we first examine the limitations of existing evaluation methodologies. Then we explore the alignment and the recommender systems ranking agreement between the LLM-judge and human provided relevance labels. We find that incorporating richer item metadata and longer user histories improves alignment, and that LLM-judge yields high agreement with human-based rankings (Kendall's tau = 0.87). Finally, an industrial case study in the podcast recommendation domain demonstrates the practical value of LLM-judge for model selection. Overall, our results show that LLM-judge is a viable and scalable approach for evaluating recommender systems.

**Link**: [arxiv](https://arxiv.org/abs/2511.23312v1),  [pdf](https://arxiv.org/pdf/2511.23312v1)

**Tags**: cs.IR 



### Hard-Constrained Neural Networks with Physics-Embedded Architecture for Residual Dynamics Learning and Invariant Enforcement in Cyber-Physical Systems
**Authors**: Enzo Nicolás Spotorno, Josafat Leal Filho, Antônio Augusto Fröhlich

**Updated**: 2025-11-28T16:06:24Z

**Summary**: This paper presents a framework for physics-informed learning in complex cyber-physical systems governed by differential equations with both unknown dynamics and algebraic invariants. First, we formalize the Hybrid Recurrent Physics-Informed Neural Network (HRPINN), a general-purpose architecture that embeds known physics as a hard structural constraint within a recurrent integrator to learn only residual dynamics. Second, we introduce the Projected HRPINN (PHRPINN), a novel extension that integrates a predict-project mechanism to strictly enforce algebraic invariants by design. The framework is supported by a theoretical analysis of its representational capacity. We validate HRPINN on a real-world battery prognostics DAE and evaluate PHRPINN on a suite of standard constrained benchmarks. The results demonstrate the framework's potential for achieving high accuracy and data efficiency, while also highlighting critical trade-offs between physical consistency, computational cost, and numerical stability, providing practical guidance for its deployment.

**Link**: [arxiv](https://arxiv.org/abs/2511.23307v1),  [pdf](https://arxiv.org/pdf/2511.23307v1)

**Tags**: cs.LG cs.AI 



### FLIMs: Fault Localization Interference Mutants, Definition, Recognition and Mitigation
**Authors**: Hengyuan Liu, Zheng Li, Donghua Wang, Yankai Wu, Xiang Chen, Yong Liu

**Updated**: 2025-11-28T16:00:44Z

**Summary**: Mutation-based Fault Localization (MBFL) has been widely explored for automated software debugging, leveraging artificial mutants to identify faulty code entities. However, MBFL faces significant challenges due to interference mutants generated from non-faulty code entities but can be killed by failing tests. These mutants mimic the test sensitivity behaviors of real faulty code entities and weaken the effectiveness of fault localization. To address this challenge, we introduce the concept of Fault Localization Interference Mutants (FLIMs) and conduct a theoretical analysis based on the Reachability, Infection, Propagation, and Revealability (RIPR) model, identifying four distinct interference causes. Building on this, we propose a novel approach to semantically recognize and mitigate FLIMs using LLM-based semantic analysis, enhanced by fine-tuning techniques and confidence estimation strategies to address LLM output instability. The recognized FLIMs are then mitigated by refining the suspiciousness scores calculated from MBFL techniques. We integrate FLIM recognition and mitigation into the MBFL workflow, developing MBFL-FLIM, a fault localization framework that enhances MBFL's effectiveness by reducing misleading interference while preserving real fault-revealing information. Our empirical experiments on the Defects4J benchmark with 395 program versions using eight LLMs demonstrate MBFL-FLIM's superiority over traditional SBFL and MBFL methods, advanced dynamic feature-based approaches, and recent LLM-based fault localization techniques. Specifically, MBFL-FLIM achieves an average improvement of 44 faults in the Top-1 metric, representing a significant enhancement over baseline methods. Further evaluation confirms MBFL-FLIM's robust performance in multi-fault scenarios, with ablation experiments validating the contributions of the fine-tuning and confidence estimation components.

**Link**: [arxiv](https://arxiv.org/abs/2511.23302v1),  [pdf](https://arxiv.org/pdf/2511.23302v1)

**Tags**: cs.SE 



### CAMA: Enhancing Mathematical Reasoning in Large Language Models with Causal Knowledge
**Authors**: Lei Zan, Keli Zhang, Ruichu Cai, Lujia Pan

**Updated**: 2025-11-28T15:58:34Z

**Summary**: Large Language Models (LLMs) have demonstrated strong performance across a wide range of tasks, yet they still struggle with complex mathematical reasoning, a challenge fundamentally rooted in deep structural dependencies. To address this challenge, we propose \textbf{CA}usal \textbf{MA}thematician (\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit, reusable mathematical structure. In the learning stage, CAMA first constructs the \textbf{M}athematical \textbf{C}ausal \textbf{G}raph (\textbf{MCG}), a high-level representation of solution strategies, by combining LLM priors with causal discovery algorithms applied to a corpus of question-solution pairs. The resulting MCG encodes essential knowledge points and their causal dependencies. To better align the graph with downstream reasoning tasks, CAMA further refines the MCG through iterative feedback derived from a selected subset of the question-solution pairs. In the reasoning stage, given a new question, CAMA dynamically extracts a task-relevant subgraph from the MCG, conditioned on both the question content and the LLM's intermediate reasoning trace. This subgraph, which encodes the most pertinent knowledge points and their causal dependencies, is then injected back into the LLM to guide its reasoning process. Empirical results on real-world datasets show that CAMA significantly improves LLM performance on challenging mathematical problems. Furthermore, our experiments demonstrate that structured guidance consistently outperforms unstructured alternatives, and that incorporating asymmetric causal relationships yields greater improvements than using symmetric associations alone.

**Link**: [arxiv](https://arxiv.org/abs/2508.02583v4),  [pdf](https://arxiv.org/pdf/2508.02583v4)

**Tags**: cs.AI cs.LG 



### PoseAdapt: Sustainable Human Pose Estimation via Continual Learning Benchmarks and Toolkit
**Authors**: Muhammad Saif Ullah Khan, Didier Stricker

**Updated**: 2025-11-28T15:34:42Z

**Summary**: Human pose estimators are typically retrained from scratch or naively fine-tuned whenever keypoint sets, sensing modalities, or deployment domains change--an inefficient, compute-intensive practice that rarely matches field constraints. We present PoseAdapt, an open-source framework and benchmark suite for continual pose model adaptation. PoseAdapt defines domain-incremental and class-incremental tracks that simulate realistic changes in density, lighting, and sensing modality, as well as skeleton growth. The toolkit supports two workflows: (i) Strategy Benchmarking, which lets researchers implement continual learning (CL) methods as plugins and evaluate them under standardized protocols; and (ii) Model Adaptation, which allows practitioners to adapt strong pretrained models to new tasks with minimal supervision. We evaluate representative regularization-based methods in single-step and sequential settings. Benchmarks enforce a fixed lightweight backbone, no access to past data, and tight per-step budgets. This isolates adaptation strategy effects, highlighting the difficulty of maintaining accuracy under strict resource limits. PoseAdapt connects modern CL techniques with practical pose estimation needs, enabling adaptable models that improve over time without repeated full retraining.

**Link**: [arxiv](https://arxiv.org/abs/2409.20469v2),  [pdf](https://arxiv.org/pdf/2409.20469v2)

**Tags**: cs.CV 



### Closing the Generalization Gap in Parameter-efficient Federated Edge Learning
**Authors**: Xinnong Du, Zhonghao Lyu, Xiaowen Cao, Chunyang Wen, Shuguang Cui, Jie Xu

**Updated**: 2025-11-28T15:34:09Z

**Summary**: Federated edge learning (FEEL) provides a promising foundation for edge artificial intelligence (AI) by enabling collaborative model training while preserving data privacy. However, limited and heterogeneous local datasets, as well as resource-constrained deployment, severely degrade both model generalization and resource utilization, leading to a compromised learning performance. Therefore, we propose a parameter-efficient FEEL framework that jointly leverages model pruning and client selection to tackle such challenges. First, we derive an information-theoretic generalization statement that characterizes the discrepancy between training and testing function losses and embed it into the convergence analysis. It reveals that a larger local generalization statement can undermine the global convergence. Then, we formulate a generalization-aware average squared gradient norm bound minimization problem, by jointly optimizing the pruning ratios, client selection, and communication-computation resources under energy and delay constraints. Despite its non-convexity, the resulting mixed-integer problem is efficiently solved via an alternating optimization algorithm. Extensive experiments demonstrate that the proposed design achieves superior learning performance than state-of-the-art baselines, validating the effectiveness of coupling generalization-aware analysis with system-level optimization for efficient FEEL.

**Link**: [arxiv](https://arxiv.org/abs/2511.23282v1),  [pdf](https://arxiv.org/pdf/2511.23282v1)

**Tags**: cs.LG cs.DC cs.IT 



### MCP vs RAG vs NLWeb vs HTML: A Comparison of the Effectiveness and Efficiency of Different Agent Interfaces to the Web (Technical Report)
**Authors**: Aaron Steiner, Ralph Peeters, Christian Bizer

**Updated**: 2025-11-28T15:32:15Z

**Summary**: Large language model agents are increasingly used to automate web tasks such as product search, offer comparison, and checkout. Current research explores different interfaces through which these agents interact with websites, including traditional HTML browsing, retrieval-augmented generation (RAG) over pre-crawled content, communication via Web APIs using the Model Context Protocol (MCP), and natural-language querying through the NLWeb interface. However, no prior work has compared these four architectures within a single controlled environment using identical tasks.   To address this gap, we introduce a testbed consisting of four simulated e-shops, each offering its products via HTML, MCP, and NLWeb interfaces. For each interface (HTML, RAG, MCP, and NLWeb) we develop specialized agents that perform the same sets of tasks, ranging from simple product searches and price comparisons to complex queries for complementary or substitute products and checkout processes. We evaluate the agents using GPT 4.1, GPT 5, GPT 5 mini, and Claude Sonnet 4 as underlying LLM. Our evaluation shows that the RAG, MCP and NLWeb agents outperform HTML on both effectiveness and efficiency. Averaged over all tasks, F1 rises from 0.67 for HTML to between 0.75 and 0.77 for the other agents. Token usage falls from about 241k for HTML to between 47k and 140k per task. The runtime per task drops from 291 seconds to between 50 and 62 seconds. The best overall configuration is RAG with GPT 5 achieving an F1 score of 0.87 and a completion rate of 0.79. Also taking cost into consideration, RAG with GPT 5 mini offers a good compromise between API usage fees and performance. Our experiments show the choice of the interaction interface has a substantial impact on both the effectiveness and efficiency of LLM-based web agents.

**Link**: [arxiv](https://arxiv.org/abs/2511.23281v1),  [pdf](https://arxiv.org/pdf/2511.23281v1)

**Tags**: cs.CL 



### RetryGuard: Preventing Self-Inflicted Retry Storms in Cloud Microservices Applications
**Authors**: Jhonatan Tavori, Anat Bremler-Barr, Hanoch Levy, Ofek Lavi

**Updated**: 2025-11-28T15:31:25Z

**Summary**: Modern cloud applications are built on independent, diverse microservices, offering scalability, flexibility, and usage-based billing. However, the structural design of these varied services, along with their reliance on auto-scalers for dynamic internet traffic, introduces significant coordination challenges. As we demonstrate in this paper, common default retry patterns used between misaligned services can turn into retry storms which drive up resource usage and costs, leading to self-inflicted Denial-of-Wallet (DoW) scenarios. To overcome these problems we introduce RetryGuard, a distributed framework for productive control of retry patterns across interdependent microservices. By managing retry policy on a per-service basis and making parallel decisions, RetryGuard prevents retry storms, curbs resource contention, and mitigates escalating operational costs. RetryGuard makes its decisions based on an analytic model that captures the relationships among retries, throughput (rejections), delays, and costs. Experimental results show that RetryGuard significantly reduces resource usage and costs compared to AWS standard and advanced retry policies. We further demonstrate its scalability and superior performance in a more complex Kubernetes deployment with the Istio service mesh, where it achieves substantial improvements.

**Link**: [arxiv](https://arxiv.org/abs/2511.23278v1),  [pdf](https://arxiv.org/pdf/2511.23278v1)

**Tags**: cs.NI cs.CR cs.DC 



### Beyond Curve Fitting: Neuro-Symbolic Agents for Context-Aware Epidemic Forecasting
**Authors**: Joongwon Chae, Runming Wang, Chen Xiong, Gong Yunhan, Lian Zhang, Ji Jiansong, Dongmei Yu, Peiwu Qin

**Updated**: 2025-11-28T15:29:26Z

**Summary**: Effective surveillance of hand, foot and mouth disease (HFMD) requires forecasts accounting for epidemiological patterns and contextual drivers like school calendars and weather. While classical models and recent foundation models (e.g., Chronos, TimesFM) incorporate covariates, they often lack the semantic reasoning to interpret the causal interplay between conflicting drivers. In this work, we propose a two-agent framework decoupling contextual interpretation from probabilistic forecasting. An LLM "event interpreter" processes heterogeneous signals-including school schedules, meteorological summaries, and reports-into a scalar transmission-impact signal. A neuro-symbolic core then combines this with historical case counts to produce calibrated probabilistic forecasts. We evaluate the framework on real-world HFMD datasets from Hong Kong (2023-2024) and Lishui, China (2024). Compared to traditional and foundation-model baselines, our approach achieves competitive point forecasting accuracy while providing robust 90% prediction intervals (coverage 0.85-1.00) and human-interpretable rationales. Our results suggest that structurally integrating domain knowledge through LLMs can match state-of-the-art performance while yielding context-aware forecasts that align with public health workflows. Code is available at https://github.com/jw-chae/forecast_MED .

**Link**: [arxiv](https://arxiv.org/abs/2511.23276v1),  [pdf](https://arxiv.org/pdf/2511.23276v1)

**Tags**: cs.LG cs.MA 



### Behavior-Equivalent Token: Single-Token Replacement for Long Prompts in LLMs
**Authors**: Jiancheng Dong, Pengyue Jia, Jingyu Peng, Maolin Wang, Yuhao Wang, Lixin Su, Xin Sun, Shuaiqiang Wang, Dawei Yin, Xiangyu Zhao

**Updated**: 2025-11-28T15:22:52Z

**Summary**: Carefully engineered system prompts play a critical role in guiding the behavior of LLM agents, but their considerable length introduces significant drawbacks, including increased inference latency, higher computational cost, and reduced effective context length. This raises the question of whether such lengthy prompts can be replaced by a drastically reduced number of tokens while preserving their behavioral effect on downstream tasks. To enable this, we propose a lightweight three-stage training framework that learns a single prompt-specific Behavior-Equivalent token ([BE]). The framework first trains [BE] to encode the natural-language content of the original system prompt via reconstruction, and then distills the prompt 's downstream behavior into this single token. Importantly, our method requires no access to model internals, no auxiliary compression models, and no labeled responses. Empirical evaluations on three datasets show that a single [BE] token achieves up to a 3000x reduction in prompt length, while retaining about 98% of the downstream performance of the original system prompts. This substantially reduces inference cost and leaves almost the entire context window available for user inputs.

**Link**: [arxiv](https://arxiv.org/abs/2511.23271v1),  [pdf](https://arxiv.org/pdf/2511.23271v1)

**Tags**: cs.CL 



### BanglaSentNet: An Explainable Hybrid Deep Learning Framework for Multi-Aspect Sentiment Analysis with Cross-Domain Transfer Learning
**Authors**: Ariful Islam, Md Rifat Hossen, Tanvir Mahmud

**Updated**: 2025-11-28T15:17:22Z

**Summary**: Multi-aspect sentiment analysis of Bangla e-commerce reviews remains challenging due to limited annotated datasets, morphological complexity, code-mixing phenomena, and domain shift issues, affecting 300 million Bangla-speaking users. Existing approaches lack explainability and cross-domain generalization capabilities crucial for practical deployment. We present BanglaSentNet, an explainable hybrid deep learning framework integrating LSTM, BiLSTM, GRU, and BanglaBERT through dynamic weighted ensemble learning for multi-aspect sentiment classification. We introduce a dataset of 8,755 manually annotated Bangla product reviews across four aspects (Quality, Service, Price, Decoration) from major Bangladeshi e-commerce platforms. Our framework incorporates SHAP-based feature attribution and attention visualization for transparent insights. BanglaSentNet achieves 85% accuracy and 0.88 F1-score, outperforming standalone deep learning models by 3-7% and traditional approaches substantially. The explainability suite achieves 9.4/10 interpretability score with 87.6% human agreement. Cross-domain transfer learning experiments reveal robust generalization: zero-shot performance retains 67-76% effectiveness across diverse domains (BanglaBook reviews, social media, general e-commerce, news headlines); few-shot learning with 500-1000 samples achieves 90-95% of full fine-tuning performance, significantly reducing annotation costs. Real-world deployment demonstrates practical utility for Bangladeshi e-commerce platforms, enabling data-driven decision-making for pricing optimization, service improvement, and customer experience enhancement. This research establishes a new state-of-the-art benchmark for Bangla sentiment analysis, advances ensemble learning methodologies for low-resource languages, and provides actionable solutions for commercial applications.

**Link**: [arxiv](https://arxiv.org/abs/2511.23264v1),  [pdf](https://arxiv.org/pdf/2511.23264v1)

**Tags**: cs.LG cs.CL 



### Unrepeated White Rabbit Time Synchronisation over a 300 km Optical Fibre Link
**Authors**: Ben Amies-King, Marco Lucamarini

**Updated**: 2025-11-28T15:02:46Z

**Summary**: White Rabbit (WR) technology provides a commercially-available off-the-shelf solution for time synchronisation with sub-nanosecond accuracy and picosecond-level precision over optical fibre links typically spanning tens of kilometres. Such high-performance time dissemination can support a variety of applications, including position, navigation and timing (PNT), financial transactions, metrology, as well as entanglement and quantum key distribution (QKD). Demonstrations of WR over significantly longer distances remain few and far between, particularly in scenarios where intermediate amplification is unavailable, such as stretches of long-haul underwater fibre. In this work, we report the longest unrepeated deployment of WR to date, achieving time synchronisation over a 300 km (51.34 dB) single-span optical fibre link, even in highly asymmetrical configurations, with 99.86% uptime, whilst maintaining picosecond-level precision and sub-nanosecond accuracy. This was achieved through careful selection and optimisation of the components deployed at the link's end points. By leveraging standard telecom fibre and off-the-shelf hardware, our results pave the way for a scalable and standardised timing backbone for large-scale quantum networks, offering a practical route toward time distribution in future heterogeneous quantum communication systems.

**Link**: [arxiv](https://arxiv.org/abs/2511.23254v1),  [pdf](https://arxiv.org/pdf/2511.23254v1)

**Tags**: quant-ph physics.optics 



### One-Shot Secure Aggregation: A Hybrid Cryptographic Protocol for Private Federated Learning in IoT
**Authors**: Imraul Emmaka, Tran Viet Xuan Phuong

**Updated**: 2025-11-28T15:01:26Z

**Summary**: Federated Learning (FL) offers a promising approach to collaboratively train machine learning models without centralizing raw data, yet its scalability is often throttled by excessive communication overhead. This challenge is magnified in Internet of Things (IoT) environments, where devices face stringent bandwidth, latency, and energy constraints. Conventional secure aggregation protocols, while essential for protecting model updates, frequently require multiple interaction rounds, large payload sizes, and per-client costs rendering them impractical for many edge deployments.   In this work, we present Hyb-Agg, a lightweight and communication-efficient secure aggregation protocol that integrates Multi-Key CKKS (MK-CKKS) homomorphic encryption with Elliptic Curve Diffie-Hellman (ECDH)-based additive masking. Hyb-Agg reduces the secure aggregation process to a single, non-interactive client-to-server transmission per round, ensuring that per-client communication remains constant regardless of the number of participants. This design eliminates partial decryption exchanges, preserves strong privacy under the RLWE, CDH, and random oracle assumptions, and maintains robustness against collusion by the server and up to $N-2$ clients.   We implement and evaluate Hyb-Agg on both high-performance and resource-constrained devices, including a Raspberry Pi 4, demonstrating that it delivers sub-second execution times while achieving a constant communication expansion factor of approximately 12x over plaintext size. By directly addressing the communication bottleneck, Hyb-Agg enables scalable, privacy-preserving federated learning that is practical for real-world IoT deployments.

**Link**: [arxiv](https://arxiv.org/abs/2511.23252v1),  [pdf](https://arxiv.org/pdf/2511.23252v1)

**Tags**: cs.CR cs.AI 



### Synthetic Industrial Object Detection: GenAI vs. Feature-Based Methods
**Authors**: Jose Moises Araya-Martinez, Adrián Sanchis Reig, Gautham Mohan, Sarvenaz Sardari, Jens Lambrecht, Jörg Krüger

**Updated**: 2025-11-28T14:51:08Z

**Summary**: Reducing the burden of data generation and annotation remains a major challenge for the cost-effective deployment of machine learning in industrial and robotics settings. While synthetic rendering is a promising solution, bridging the sim-to-real gap often requires expert intervention. In this work, we benchmark a range of domain randomization (DR) and domain adaptation (DA) techniques, including feature-based methods, generative AI (GenAI), and classical rendering approaches, for creating contextualized synthetic data without manual annotation. Our evaluation focuses on the effectiveness and efficiency of low-level and high-level feature alignment, as well as a controlled diffusion-based DA method guided by prompts generated from real-world contexts. We validate our methods on two datasets: a proprietary industrial dataset (automotive and logistics) and a public robotics dataset. Results show that if render-based data with enough variability is available as seed, simpler feature-based methods, such as brightness-based and perceptual hashing filtering, outperform more complex GenAI-based approaches in both accuracy and resource efficiency. Perceptual hashing consistently achieves the highest performance, with mAP50 scores of 98% and 67% on the industrial and robotics datasets, respectively. Additionally, GenAI methods present significant time overhead for data generation at no apparent improvement of sim-to-real mAP values compared to simpler methods. Our findings offer actionable insights for efficiently bridging the sim-to-real gap, enabling high real-world performance from models trained exclusively on synthetic data.

**Link**: [arxiv](https://arxiv.org/abs/2511.23241v1),  [pdf](https://arxiv.org/pdf/2511.23241v1)

**Tags**: cs.CV 



### Incorporating Ephemeral Traffic Waves in A Data-Driven Framework for Microsimulation in CARLA
**Authors**: Alex Richardson, Azhar Hasan, Gabor Karsai, Jonathan Sprinkle

**Updated**: 2025-11-28T14:45:20Z

**Summary**: This paper introduces a data-driven traffic microsimulation framework in CARLA that reconstructs real-world wave dynamics using high-fidelity time-space data from the I-24 MOTION testbed. Calibration of road networks in microsimulators to reproduce ephemeral phenomena such as traffic waves for large-scale simulation is a process that is fraught with challenges. This work reconsiders the existence of the traffic state data as boundary conditions on an ego vehicle moving through previously recorded traffic data, rather than reproducing those traffic phenomena in a calibrated microsim. Our approach is to autogenerate a 1 mile highway segment corresponding to I-24, and use the I-24 data to power a cosimulation module that injects traffic information into the simulation. The CARLA and cosimulation simulations are centered around an ego vehicle sampled from the empirical data, with autogeneration of "visible" traffic within the longitudinal range of the ego vehicle. Boundary control beyond these visible ranges is achieved using ghost cells behind (upstream) and ahead (downstream) of the ego vehicle. Unlike prior simulation work that focuses on local car-following behavior or abstract geometries, our framework targets full time-space diagram fidelity as the validation objective. Leveraging CARLA's rich sensor suite and configurable vehicle dynamics, we simulate wave formation and dissipation in both low-congestion and high-congestion scenarios for qualitative analysis. The resulting emergent behavior closely mirrors that of real traffic, providing a novel cosimulation framework for evaluating traffic control strategies, perception-driven autonomy, and future deployment of wave mitigation solutions. Our work bridges microscopic modeling with physical experimental data, enabling the first perceptually realistic, boundary-driven simulation of empirical traffic wave phenomena in CARLA.

**Link**: [arxiv](https://arxiv.org/abs/2511.23236v1),  [pdf](https://arxiv.org/pdf/2511.23236v1)

**Tags**: cs.RO cs.ET 



### Unlocking Multilingual Reasoning Capability of LLMs and LVLMs through Representation Engineering
**Authors**: Qiming Li, Xiaocheng Feng, Yixuan Ma, Zekai Ye, Ruihan Chen, Xiachong Feng, Bing Qin

**Updated**: 2025-11-28T14:40:27Z

**Summary**: Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) demonstrate strong reasoning capabilities, yet their performance in English significantly outperforms that in low-resource languages, raising fairness concerns in multilingual applications. Existing approaches either rely on costly multilingual training or employ prompting with external translation tools, both of which are resource-intensive and sensitive to translation quality. To address these limitations, we propose a training-free inference-time method to enhance Multilingual Reasoning capabilities via Representation Engineering (MRRE) without using any additional training data or tools. MRRE sequentially injects two precomputed vectors at specific layers during inference processing: cross-lingual reasoning enhancement vectors, which steer non-English reasoning representations toward English space to unlock multilingual reasoning, and target-language output anchoring vectors, which restore the distribution of the target language to preserve input-output language consistency. Comprehensive experiments across six advanced LLMs and LVLMs on four reasoning benchmarks demonstrate that MRRE consistently enhances non-English reasoning by an average gain of 5.48% and up to 7.54% in low-resource languages (Thai and Swahili), while improving input-output language consistency by 3.78%.

**Link**: [arxiv](https://arxiv.org/abs/2511.23231v1),  [pdf](https://arxiv.org/pdf/2511.23231v1)

**Tags**: cs.CV 



### Continual Learning of Domain Knowledge from Human Feedback in Text-to-SQL
**Authors**: Thomas Cook, Kelly Patel, Sivapriya Vellaichamy, Udari Madhushani Sehwag, Saba Rahimi, Zhen Zeng, Sumitra Ganesh

**Updated**: 2025-11-28T14:40:27Z

**Summary**: Large Language Models (LLMs) can generate SQL queries from natural language questions but struggle with database-specific schemas and tacit domain knowledge. We introduce a framework for continual learning from human feedback in text-to-SQL, where a learning agent receives natural language feedback to refine queries and distills the revealed knowledge for reuse on future tasks. This distilled knowledge is stored in a structured memory, enabling the agent to improve execution accuracy over time. We design and evaluate multiple variations of a learning agent architecture that vary in how they capture and retrieve past experiences. Experiments on the BIRD benchmark Dev set show that memory-augmented agents, particularly the Procedural Agent, achieve significant accuracy gains and error reduction by leveraging human-in-the-loop feedback. Our results highlight the importance of transforming tacit human expertise into reusable knowledge, paving the way for more adaptive, domain-aware text-to-SQL systems that continually learn from a human-in-the-loop.

**Link**: [arxiv](https://arxiv.org/abs/2511.10674v2),  [pdf](https://arxiv.org/pdf/2511.10674v2)

**Tags**: cs.CL cs.AI cs.DB 



### TWEO: Transformers Without Extreme Outliers Enables FP8 Training And Quantization For Dummies
**Authors**: Guang Liang, Jie Shao, Ningyuan Tang, Xinyao Liu, Jianxin Wu

**Updated**: 2025-11-28T14:33:21Z

**Summary**: Native FP8 support in modern hardware is essential for training large Transformers, but is severely hindered by extreme activation outliers. Existing solutions either rely on complex mixed-precision engineering or invasive architectural modifications. This paper fundamentally challenges the conventional wisdom that outliers are data-driven. We demonstrate that extreme outliers are a data-independent, mechanically-produced artifact of training, originating from specific structural properties of the weight matrices (i.e., colinearity). Based on this insight, we propose TWEO (Transformers Without Extreme Outliers), a novel, non-invasive loss function. TWEO effectively prevents extreme outliers via a very simple loss term, which reduces outliers from 10000+ to less than 20. TWEO then enables full-model FP8 pre-training with neither engineering tricks nor architectural changes for both LLM and ViT. When standard FP8 training catastrophically collapses, TWEO achieves performance comparable to the BF16 baseline while delivering a 36% increase in training throughput. Also, TWEO enables a new quantization paradigm. Hardware-friendly W8A8 per-tensor static quantization of LLMs, previously considered completely unusable due to outliers, achieves SOTA performance for the first time on TWEO-trained models.

**Link**: [arxiv](https://arxiv.org/abs/2511.23225v1),  [pdf](https://arxiv.org/pdf/2511.23225v1)

**Tags**: cs.CL cs.CV cs.LG 



### OmniRouter: Budget and Performance Controllable Multi-LLM Routing
**Authors**: Kai Mei, Wujiang Xu, Minghao Guo, Shuhang Lin, Yongfeng Zhang

**Updated**: 2025-11-28T14:29:56Z

**Summary**: Large language models (LLMs) deliver superior performance but require substantial computational resources and operate with relatively low efficiency, while smaller models can efficiently handle simpler tasks with fewer resources. LLM routing is a crucial paradigm that dynamically selects the most suitable large language models from a pool of candidates to process diverse inputs, ensuring optimal resource utilization while maintaining response quality. Existing routing frameworks typically model this as a locally optimal decision-making problem, selecting the presumed best-fit LLM for each query individually, which overlooks global budget constraints, resulting in ineffective resource allocation. To tackle this problem, we introduce OmniRouter, a fundamentally controllable routing framework for multi-LLM serving. Instead of making per-query greedy choices, OmniRouter models the routing task as a constrained optimization problem, assigning models that minimize total cost while ensuring the required performance level. Specifically, a hybrid retrieval-augmented predictor is designed to predict the capabilities and costs of LLMs. After obtaining the predicted cost and performance, we utilize a constrained optimizer for cost-optimal assignments that employs Lagrangian dual decomposition with adaptive multipliers. It iteratively converges toward the globally optimal query-model allocation, dynamically balancing latency minimization against quality thresholds while adhering to heterogeneous capacity constraints. Experiments show that OmniRouter achieves up to 6.30% improvement in response accuracy while simultaneously reducing computational costs by at least 10.15% compared to competitive router baselines. The code and the dataset are available at https://github.com/dongyuanjushi/OmniRouter.

**Link**: [arxiv](https://arxiv.org/abs/2502.20576v6),  [pdf](https://arxiv.org/pdf/2502.20576v6)

**Tags**: cs.DB cs.CL 



### Instruction Tuning of Large Language Models for Tabular Data Generation-in One Day
**Authors**: Milad Abdollahzadeh, Abdul Raheem, Zilong Zhao, Uzair Javaid, Kevin Yee, Nalam Venkata Abhishek, Tram Truong-Huu, Biplab Sikdar

**Updated**: 2025-11-28T14:26:46Z

**Summary**: Tabular instruction tuning has emerged as a promising research direction for improving LLMs understanding of tabular data. However, the majority of existing works only consider question-answering and reasoning tasks over tabular data, leaving tabular data generation largely unnoticed. In this work, for the first time, we explore the efficacy of instruction tuning in improving LLMs tabular data generation capabilities. More specifically, given the high data and computation requirements of tabular instruction tuning, we aim to address the possibility of instruction tuning for tabular data generation with limited data and computational resources. To achieve this, we first create a high-quality instruction dataset for tabular data, enabling efficient LLM comprehension. We then instruction-tune an open-source LLM (Llama3.1-8B-Instruct) on the training set of this dataset to improve its tabular data generation performance. Our experimental results show that by using our high-quality dataset and instruction-tuning on only 7K instructions with an A100 GPU, for less than 6 hours, we achieve tabular data generation performance on par with the most capable commercial LLM, GPT-4o.

**Link**: [arxiv](https://arxiv.org/abs/2511.23220v1),  [pdf](https://arxiv.org/pdf/2511.23220v1)

**Tags**: cs.CV 



### Mina: A Multilingual LLM-Powered Legal Assistant Agent for Bangladesh for Empowering Access to Justice
**Authors**: Azmine Toushik Wasi, Wahid Faisal, Mst Rafia Islam

**Updated**: 2025-11-28T14:20:39Z

**Summary**: Bangladesh's low-income population faces major barriers to affordable legal advice due to complex legal language, procedural opacity, and high costs. Existing AI legal assistants lack Bengali-language support and jurisdiction-specific adaptation, limiting their effectiveness. To address this, we developed Mina, a multilingual LLM-based legal assistant tailored for the Bangladeshi context. It employs multilingual embeddings and a RAG-based chain-of-tools framework for retrieval, reasoning, translation, and document generation, delivering context-aware legal drafts, citations, and plain-language explanations via an interactive chat interface. Evaluated by law faculty from leading Bangladeshi universities across all stages of the 2022 and 2023 Bangladesh Bar Council Exams, Mina scored 75-80% in Preliminary MCQs, Written, and simulated Viva Voce exams, matching or surpassing average human performance and demonstrating clarity, contextual understanding, and sound legal reasoning. Even under a conservative upper bound, Mina operates at just 0.12-0.61% of typical legal consultation costs in Bangladesh, yielding a 99.4-99.9\% cost reduction relative to human-provided services. These results confirm its potential as a low-cost, multilingual AI assistant that automates key legal tasks and scales access to justice, offering a real-world case study on building domain-specific, low-resource systems and addressing challenges of multilingual adaptation, efficiency, and sustainable public-service AI deployment.

**Link**: [arxiv](https://arxiv.org/abs/2511.08605v2),  [pdf](https://arxiv.org/pdf/2511.08605v2)

**Tags**: cs.CL cs.CY cs.HC cs.MA cs.MM 



### GAPS: Guiding Dynamic Android Analysis with Static Path Synthesis
**Authors**: Samuele Doria, Eleonora Losiouk

**Updated**: 2025-11-28T14:19:04Z

**Summary**: Dynamically resolving method reachability in Android applications remains a critical and largely unsolved problem. Despite notable advancements in GUI testing and static call graph construction, current tools are insufficient for reliably driving execution toward specific target methods, especially those not embedded in a graphical component (e.g., libraries' methods), a capability essential for tasks such as vulnerability validation, debugging, and behavioral analysis.   We present GAPS (Graph-based Automated Path Synthesizer), the first system that integrates static, method-guided call graph analysis with dynamic, interaction-driven execution. GAPS performs a lightweight backward traversal of the call graph, guided by data-flow analysis, to reconstruct paths reaching the target methods. These paths are then translated into instructions that guide runtime app exploration.   On the AndroTest benchmark, GAPS statically identifies paths to reach 88.24\% of the target methods in just 4.27 seconds per app and dynamically reaches 57.44\% of them. In contrast, state-of-the-art dynamic interaction tools show significantly lower reachability over three runs: APE, one of the best model-based GUI testers, achieves 12.82\%, while GoalExplorer, a hybrid analysis tool, reaches 9.69\%, and Guardian, an LLM-based UI automator, reaches 17.12\%. Static analysis tools also fall short: FlowDroid and DroidReach identify paths to reach 58.81\% and 9.48\% of the targets, requiring 35.06 seconds and 23.46 seconds per app, respectively.   Finally, an evaluation on the 50 most downloaded real-world apps demonstrates GAPS's practical utility in analyzing security-critical code under a realistic scenario. With an average static analysis time of 278.9 seconds, GAPS statically reconstructs paths to 62.03\% of the target methods and dynamically reaches 59.86\% of them.

**Link**: [arxiv](https://arxiv.org/abs/2511.23213v1),  [pdf](https://arxiv.org/pdf/2511.23213v1)

**Tags**: cs.SE 



### Pathryoshka: Compressing Pathology Foundation Models via Multi-Teacher Knowledge Distillation with Nested Embeddings
**Authors**: Christian Grashei, Christian Brechenmacher, Rao Muhammad Umer, Jingsong Liu, Carsten Marr, Ewa Szczurek, Peter J. Schüffler

**Updated**: 2025-11-28T14:11:31Z

**Summary**: Pathology foundation models (FMs) have driven significant progress in computational pathology. However, these high-performing models can easily exceed a billion parameters and produce high-dimensional embeddings, thus limiting their applicability for research or clinical use when computing resources are tight. Here, we introduce Pathryoshka, a multi-teacher distillation framework inspired by RADIO distillation and Matryoshka Representation Learning to reduce pathology FM sizes while allowing for adaptable embedding dimensions. We evaluate our framework with a distilled model on ten public pathology benchmarks with varying downstream tasks. Compared to its much larger teachers, Pathryoshka reduces the model size by 86-92% at on-par performance. It outperforms state-of-the-art single-teacher distillation models of comparable size by a median margin of 7.0 in accuracy. By enabling efficient local deployment without sacrificing accuracy or representational richness, Pathryoshka democratizes access to state-of-the-art pathology FMs for the broader research and clinical community.

**Link**: [arxiv](https://arxiv.org/abs/2511.23204v1),  [pdf](https://arxiv.org/pdf/2511.23204v1)

**Tags**: cs.CV 



### Quantifying the Privacy-Utility Trade-off in GPS-based Daily Stress Recognition using Semantic Features
**Authors**: Hoang Khang Phan, Nhat Tan Le

**Updated**: 2025-11-28T14:04:00Z

**Summary**: Psychological stress is a widespread issue that significantly impacts student well-being and academic performance. Effective remote stress recognition is crucial, yet existing methods often rely on wearable devices or GPS-based clustering techniques that pose privacy risks. In this study, we introduce a novel, end-to-end privacy-enhanced framework for semantic location encoding using a self-hosted OSM engine and an LLM-bootstrapped static map. We rigorously quantify the privacy-utility trade-off and demonstrate (via LOSO validation) that our Privacy-Aware (PA) model achieves performance statistically indistinguishable from a non-private model, proving that utility does not require sacrificing privacy. Feature importance analysis highlights that recreational activity time, working time, and travel time play a significant role in stress recognition.

**Link**: [arxiv](https://arxiv.org/abs/2511.23200v1),  [pdf](https://arxiv.org/pdf/2511.23200v1)

**Tags**: cs.CR cs.HC 



### iSeal: Encrypted Fingerprinting for Reliable LLM Ownership Verification
**Authors**: Zixun Xiong, Gaoyi Wu, Qingyang Yu, Mingyu Derek Ma, Lingfeng Yao, Miao Pan, Xiaojiang Du, Hao Wang

**Updated**: 2025-11-28T13:58:50Z

**Summary**: Given the high cost of large language model (LLM) training from scratch, safeguarding LLM intellectual property (IP) has become increasingly crucial. As the standard paradigm for IP ownership verification, LLM fingerprinting thus plays a vital role in addressing this challenge. Existing LLM fingerprinting methods verify ownership by extracting or injecting model-specific features. However, they overlook potential attacks during the verification process, leaving them ineffective when the model thief fully controls the LLM's inference process. In such settings, attackers may share prompt-response pairs to enable fingerprint unlearning or manipulate outputs to evade exact-match verification. We propose iSeal, the first fingerprinting method designed for reliable verification when the model thief controls the suspected LLM in an end-to-end manner. It injects unique features into both the model and an external module, reinforced by an error-correction mechanism and a similarity-based verification strategy. These components are resistant to verification-time attacks, including collusion-based fingerprint unlearning and response manipulation, backed by both theoretical analysis and empirical results. iSeal achieves 100 percent Fingerprint Success Rate (FSR) on 12 LLMs against more than 10 attacks, while baselines fail under unlearning and response manipulations.

**Link**: [arxiv](https://arxiv.org/abs/2511.08905v2),  [pdf](https://arxiv.org/pdf/2511.08905v2)

**Tags**: cs.CR cs.AI 



### HPSU: A Benchmark for Human-Level Perception in Real-World Spoken Speech Understanding
**Authors**: Chen Li, Peiji Yang, Yicheng Zhong, Jianxing Yu, Zhisheng Wang, Zihao Gou, Wenqing Chen, Jian Yin

**Updated**: 2025-11-28T13:45:40Z

**Summary**: Recent advances in Speech Large Language Models (Speech LLMs) have led to great progress in speech understanding tasks such as Automatic Speech Recognition (ASR) and Speech Emotion Recognition (SER). However, whether these models can achieve human-level auditory perception, particularly in terms of their ability to comprehend latent intentions and implicit emotions in real-world spoken language, remains underexplored. To this end, we introduce the Human-level Perception in Spoken Speech Understanding (HPSU), a new benchmark for fully evaluating the human-level perceptual and understanding capabilities of Speech LLMs. HPSU comprises over 20,000 expert-validated spoken language understanding samples in English and Chinese. It establishes a comprehensive evaluation framework by encompassing a spectrum of tasks, ranging from basic speaker attribute recognition to complex inference of latent intentions and implicit emotions. To address the issues of data scarcity and high cost of manual annotation in real-world scenarios, we developed a semi-automatic annotation process. This process fuses audio, textual, and visual information to enable precise speech understanding and labeling, thus enhancing both annotation efficiency and quality. We systematically evaluate various open-source and proprietary Speech LLMs. The results demonstrate that even top-performing models still fall considerably short of human capabilities in understanding genuine spoken interactions. Consequently, HPSU will be useful for guiding the development of Speech LLMs toward human-level perception and cognition.

**Link**: [arxiv](https://arxiv.org/abs/2511.23178v1),  [pdf](https://arxiv.org/pdf/2511.23178v1)

**Tags**: cs.SD 



### LockForge: Automating Paper-to-Code for Logic Locking with Multi-Agent Reasoning LLMs
**Authors**: Akashdeep Saha, Zeng Wang, Prithwish Basu Roy, Johann Knechtel, Ozgur Sinanoglu, Ramesh Karri

**Updated**: 2025-11-28T13:42:32Z

**Summary**: Despite rapid progress in logic locking (LL), reproducibility remains a challenge as codes are rarely made public. We present LockForge, a first-of-its-kind, multi-agent large language model (LLM) framework that turns LL descriptions in papers into executable and tested code. LockForge provides a carefully crafted pipeline realizing forethought, implementation, iterative refinement, and a multi-stage validation, all to systematically bridge the gap between prose and practice for complex LL schemes. For validation, we devise (i) an LLM-as-Judge stage with a scoring system considering behavioral checks, conceptual mechanisms, structural elements, and reproducibility on benchmarks, and (ii) an independent LLM-as-Examiner stage for ground-truth assessment. We apply LockForge to 10 seminal LL schemes, many of which lack reference implementations. Our evaluation on multiple SOTA LLMs, including ablation studies, reveals the significant complexity of the task. We show that an advanced reasoning model and a sophisticated, multi-stage framework like LockForge are required. We release all implementations and benchmarks, providing a reproducible and fair foundation for evaluation of further LL research.

**Link**: [arxiv](https://arxiv.org/abs/2511.18531v2),  [pdf](https://arxiv.org/pdf/2511.18531v2)

**Tags**: cs.CR cs.PL 



### Local Hybrid Retrieval-Augmented Document QA
**Authors**: Paolo Astrino

**Updated**: 2025-11-28T13:42:31Z

**Summary**: Organizations handling sensitive documents face a critical dilemma: adopt cloud-based AI systems that offer powerful question-answering capabilities but compromise data privacy, or maintain local processing that ensures security but delivers poor accuracy. We present a question-answering system that resolves this trade-off by combining semantic understanding with keyword precision, operating entirely on local infrastructure without internet access. Our approach demonstrates that organizations can achieve competitive accuracy on complex queries across legal, scientific, and conversational documents while keeping all data on their machines. By balancing two complementary retrieval strategies and using consumer-grade hardware acceleration, the system delivers reliable answers with minimal errors, letting banks, hospitals, and law firms adopt conversational document AI without transmitting proprietary information to external providers. This work establishes that privacy and performance need not be mutually exclusive in enterprise AI deployment.

**Link**: [arxiv](https://arxiv.org/abs/2511.10297v2),  [pdf](https://arxiv.org/pdf/2511.10297v2)

**Tags**: cs.CL 



### Are LLMs Good Safety Agents or a Propaganda Engine?
**Authors**: Neemesh Yadav, Francesco Ortu, Jiarui Liu, Joeun Yook, Bernhard Schölkopf, Rada Mihalcea, Alberto Cazzaniga, Zhijing Jin

**Updated**: 2025-11-28T13:36:00Z

**Summary**: Large Language Models (LLMs) are trained to refuse to respond to harmful content. However, systematic analyses of whether this behavior is truly a reflection of its safety policies or an indication of political censorship, that is practiced globally by countries, is lacking. Differentiating between safety influenced refusals or politically motivated censorship is hard and unclear. For this purpose we introduce PSP, a dataset built specifically to probe the refusal behaviors in LLMs from an explicitly political context. PSP is built by formatting existing censored content from two data sources, openly available on the internet: sensitive prompts in China generalized to multiple countries, and tweets that have been censored in various countries. We study: 1) impact of political sensitivity in seven LLMs through data-driven (making PSP implicit) and representation-level approaches (erasing the concept of politics); and, 2) vulnerability of models on PSP through prompt injection attacks (PIAs). Associating censorship with refusals on content with masked implicit intent, we find that most LLMs perform some form of censorship. We conclude with summarizing major attributes that can cause a shift in refusal distributions across models and contexts of different countries.

**Link**: [arxiv](https://arxiv.org/abs/2511.23174v1),  [pdf](https://arxiv.org/pdf/2511.23174v1)

**Tags**: cs.CL 



### Energy-Efficient Vision Transformer Inference for Edge-AI Deployment
**Authors**: Nursultan Amanzhol, Jurn-Gyu Park

**Updated**: 2025-11-28T13:24:08Z

**Summary**: The growing deployment of Vision Transformers (ViTs) on energy-constrained devices requires evaluation methods that go beyond accuracy alone. We present a two-stage pipeline for assessing ViT energy efficiency that combines device-agnostic model selection with device-related measurements. We benchmark 13 ViT models on ImageNet-1K and CIFAR-10, running inference on NVIDIA Jetson TX2 (edge device) and an NVIDIA RTX 3050 (mobile GPU). The device-agnostic stage uses the NetScore metric for screening; the device-related stage ranks models with the Sustainable Accuracy Metric (SAM). Results show that hybrid models such as LeViT_Conv_192 reduce energy by up to 53% on TX2 relative to a ViT baseline (e.g., SAM5=1.44 on TX2/CIFAR-10), while distilled models such as TinyViT-11M_Distilled excel on the mobile GPU (e.g., SAM5=1.72 on RTX 3050/CIFAR-10 and SAM5=0.76 on RTX 3050/ImageNet-1K).

**Link**: [arxiv](https://arxiv.org/abs/2511.23166v1),  [pdf](https://arxiv.org/pdf/2511.23166v1)

**Tags**: cs.LG 



### Amplifiers or Equalizers? A Longitudinal Study of LLM Evolution in Software Engineering Project-Based Learning
**Authors**: Hana Kataoka, Jialong Li, Yutaka Matsuno

**Updated**: 2025-11-28T13:05:23Z

**Summary**: As LLMs reshape software development, integrating LLM-augmented practices into SE education has become imperative. While existing studies explore LLMs' educational use in introductory programming or isolated SE tasks, their impact in more open-ended Project-Based Learning (PBL) remains unexplored. This paper introduces a two-year longitudinal study comparing a 2024 (using early free LLMs, $n$=48) and 2025 (using the latest paid LLMs, $n$=46) cohort. Our findings suggest the latest powerful LLMs' dual role: they act as "equalizers," boosting average performance even for programming-weak students, providing opportunities for more authentic SE practices; yet also as "amplifiers," dramatically widening absolute performance gaps, creating new pedagogical challenges for addressing educational inequities.

**Link**: [arxiv](https://arxiv.org/abs/2511.23157v1),  [pdf](https://arxiv.org/pdf/2511.23157v1)

**Tags**: cs.SE cs.HC 



### Biased by Design: Leveraging AI Biases to Enhance Critical Thinking of News Readers
**Authors**: Liudmila Zavolokina, Kilian Sprenkamp, Zoya Katashinskaya, Daniel Gordon Jones

**Updated**: 2025-11-28T13:02:51Z

**Summary**: This paper explores the design of a propaganda detection tool using Large Language Models (LLMs). Acknowledging the inherent biases in AI models, especially in political contexts, we investigate how these biases might be leveraged to enhance critical thinking in news consumption. Countering the typical view of AI biases as detrimental, our research proposes strategies of user choice and personalization in response to a user's political stance, applying psychological concepts of confirmation bias and cognitive dissonance. We present findings from a qualitative user study, offering insights and design recommendations (bias awareness, personalization and choice, and gradual introduction of diverse perspectives) for AI tools in propaganda detection.

**Link**: [arxiv](https://arxiv.org/abs/2504.14522v4),  [pdf](https://arxiv.org/pdf/2504.14522v4)

**Tags**: cs.HC cs.AI 



### Automated Generation of MDPs Using Logic Programming and LLMs for Robotic Applications
**Authors**: Enrico Saccon, Davide De Martini, Matteo Saveriano, Edoardo Lamon, Luigi Palopoli, Marco Roveri

**Updated**: 2025-11-28T12:48:30Z

**Summary**: We present a novel framework that integrates Large Language Models (LLMs) with automated planning and formal verification to streamline the creation and use of Markov Decision Processes (MDP). Our system leverages LLMs to extract structured knowledge in the form of a Prolog knowledge base from natural language (NL) descriptions. It then automatically constructs an MDP through reachability analysis, and synthesises optimal policies using the Storm model checker. The resulting policy is exported as a state-action table for execution. We validate the framework in three human-robot interaction scenarios, demonstrating its ability to produce executable policies with minimal manual effort. This work highlights the potential of combining language models with formal methods to enable more accessible and scalable probabilistic planning in robotics.

**Link**: [arxiv](https://arxiv.org/abs/2511.23143v1),  [pdf](https://arxiv.org/pdf/2511.23143v1)

**Tags**: cs.RO cs.AI 



### Multi-chain Graph Refinement and Selection for Reliable Reasoning in Large Language Models
**Authors**: Yujiao Yang, Jing Lian, Linhui Li

**Updated**: 2025-11-28T12:35:16Z

**Summary**: The complex reasoning ability of Large Language Models (LLMs) poses a critical bottleneck for their practical applications. Test-time expansion methods such as Tree-of-Thought (ToT) and Graph-of-Thought (GoT) enhance reasoning by introducing intermediate reasoning structures, tree search, or graph-based exploration mechanisms. However, their reasoning strategies suffer from limited diversity, redundant search branches, and inadequate integration and error correction across heterogeneous reasoning paths. To address these limitations, we propose a novel reasoning framework called Multi-chain Graph Refinement & Selection (MGRS), which first generates multiple diverse reasoning trajectories for a given problem, refines candidate responses using a composite self- and cross-verification strategy, then constructs a reasoning relation graph and estimates the success rate of intermediate nodes, and finally computes cumulative success rates to select the most reliable answer and corresponding reasoning trajectory. Experimental results demonstrate that MGRS significantly advances both the reasoning capability and computational efficiency of reasoning enhancement methods. Across six benchmark datasets spanning four distinct tasks, MGRS achieves an average accuracy of 82.9%, outperforming state-of-the-art baselines by a clear margin of 2.1%. Remarkably, on the 24-point game, MGRS attains 100% accuracy for the first time, while delivering a 13.6x speed-up compared to the leading Forest of Thoughts framework.

**Link**: [arxiv](https://arxiv.org/abs/2511.23136v1),  [pdf](https://arxiv.org/pdf/2511.23136v1)

**Tags**: cs.CL cs.AI 



### Evolutionary Discovery of Heuristic Policies for Traffic Signal Control
**Authors**: Ruibing Wang, Shuhan Guo, Zeen Li, Zhen Wang, Quanming Yao

**Updated**: 2025-11-28T12:11:17Z

**Summary**: Traffic Signal Control (TSC) involves a challenging trade-off: classic heuristics are efficient but oversimplified, while Deep Reinforcement Learning (DRL) achieves high performance yet suffers from poor generalization and opaque policies. Online Large Language Models (LLMs) provide general reasoning but incur high latency and lack environment-specific optimization. To address these issues, we propose Temporal Policy Evolution for Traffic (\textbf{\method{}}), which uses LLMs as an evolution engine to derive specialized heuristic policies. The framework introduces two key modules: (1) Structured State Abstraction (SSA), converting high-dimensional traffic data into temporal-logical facts for reasoning; and (2) Credit Assignment Feedback (CAF), tracing flawed micro-decisions to poor macro-outcomes for targeted critique. Operating entirely at the prompt level without training, \method{} yields lightweight, robust policies optimized for specific traffic environments, outperforming both heuristics and online LLM actors.

**Link**: [arxiv](https://arxiv.org/abs/2511.23122v1),  [pdf](https://arxiv.org/pdf/2511.23122v1)

**Tags**: cs.AI 



### Neuro-Symbolic Constrained Optimization for Cloud Application Deployment via Graph Neural Networks and Satisfiability Modulo Theory
**Authors**: Madalina Erascu

**Updated**: 2025-11-28T11:53:08Z

**Summary**: This paper proposes a novel hybrid neuro-symbolic framework for the optimal and scalable deployment of component-based applications in the Cloud. The challenge of efficiently mapping application components to virtual machines (VMs) across diverse VM Offers from Cloud Providers is formalized as a constrained optimization problem (COP), considering both general and application-specific constraints. Due to the NP-hard nature and scalability limitations of exact solvers, we introduce a machine learning-enhanced approach where graph neural networks (GNNs) are trained on small-scale deployment instances and their predictions are used as soft constraints within the Z3 SMT solver. The deployment problem is recast as a graph edge classification task over a heterogeneous graph, combining relational embeddings with constraint reasoning. Our framework is validated through several realistic case studies, each highlighting different constraint profiles. Experimental results confirm that incorporating GNN predictions improves solver scalability and often preserves or even improves cost-optimality. This work demonstrates the practical benefits of neuro-symbolic coupling for Cloud infrastructure planning and contributes a reusable methodology for general NP-hard problems.

**Link**: [arxiv](https://arxiv.org/abs/2511.23109v1),  [pdf](https://arxiv.org/pdf/2511.23109v1)

**Tags**: cs.LO 



### Mind Reading or Misreading? LLMs on the Big Five Personality Test
**Authors**: Francesco Di Cursi, Chiara Boldrini, Marco Conti, Andrea Passarella

**Updated**: 2025-11-28T11:40:30Z

**Summary**: We evaluate large language models (LLMs) for automatic personality prediction from text under the binary Five Factor Model (BIG5). Five models -- including GPT-4 and lightweight open-source alternatives -- are tested across three heterogeneous datasets (Essays, MyPersonality, Pandora) and two prompting strategies (minimal vs. enriched with linguistic and psychological cues). Enriched prompts reduce invalid outputs and improve class balance, but also introduce a systematic bias toward predicting trait presence. Performance varies substantially: Openness and Agreeableness are relatively easier to detect, while Extraversion and Neuroticism remain challenging. Although open-source models sometimes approach GPT-4 and prior benchmarks, no configuration yields consistently reliable predictions in zero-shot binary settings. Moreover, aggregate metrics such as accuracy and macro-F1 mask significant asymmetries, with per-class recall offering clearer diagnostic value. These findings show that current out-of-the-box LLMs are not yet suitable for APPT, and that careful coordination of prompt design, trait framing, and evaluation metrics is essential for interpretable results.

**Link**: [arxiv](https://arxiv.org/abs/2511.23101v1),  [pdf](https://arxiv.org/pdf/2511.23101v1)

**Tags**: cs.CL cs.AI 



### AutoPatch: Multi-Agent Framework for Patching Real-World CVE Vulnerabilities
**Authors**: Minjae Seo, Wonwoo Choi, Myoungsung You, Seungwon Shin

**Updated**: 2025-11-28T11:15:57Z

**Summary**: Large Language Models (LLMs) have emerged as promising tools in software development, enabling automated code generation and analysis. However, their knowledge is limited to a fixed cutoff date, making them prone to generating code vulnerable to newly disclosed CVEs. Frequent fine-tuning with new CVE sets is costly, and existing LLM-based approaches focus on oversimplified CWE examples and require providing explicit bug locations to LLMs, limiting their ability to patch complex real-world vulnerabilities. To address these limitations, we propose AutoPatch, a multi-agent framework designed to patch vulnerable LLM-generated code, particularly those introduced after the LLMs' knowledge cutoff. AutoPatch integrates Retrieval-Augmented Generation (RAG) with a structured database of recently disclosed vulnerabilities, comprising 525 code snippets derived from 75 high-severity CVEs across real-world systems such as the Linux kernel and Chrome. AutoPatch combines semantic and taint analysis to identify the most relevant CVE and leverages enhanced Chain-of-Thought (CoT) reasoning to construct enriched prompts for verification and patching. Our unified similarity model, which selects the most relevant vulnerabilities, achieves 90.4 percent accuracy in CVE matching. AutoPatch attains 89.5 percent F1-score for vulnerability verification and 95.0 percent accuracy in patching, while being over 50x more cost-efficient than traditional fine-tuning approaches.

**Link**: [arxiv](https://arxiv.org/abs/2505.04195v2),  [pdf](https://arxiv.org/pdf/2505.04195v2)

**Tags**: cs.CR 



### REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance
**Authors**: Chuyi Kong, Gao Wei, Jing Ma, Hongzhan Lin, Yaxin Fan

**Updated**: 2025-11-28T11:08:27Z

**Summary**: The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.

**Link**: [arxiv](https://arxiv.org/abs/2511.20233v2),  [pdf](https://arxiv.org/pdf/2511.20233v2)

**Tags**: cs.CL 



### Progressive Localisation in Localist LLMs
**Authors**: Joachim Diederich

**Updated**: 2025-11-28T10:44:50Z

**Summary**: This paper demonstrates that progressive localization, the gradual increase of attention locality from early distributed layers to late localized layers, represents the optimal architecture for creating interpretable large language models (LLMs) while preserving performance. Through systematic experimentation with GPT-2 fine-tuned on The Psychology of Artificial Superintelligence, we evaluate seven locality configurations ranging from fully distributed to strictly localist, with five progressive schedules implementing polynomial increases (linear through quintic). We investigate whether interpretability constraints can be aligned with natural semantic structure while being applied strategically across network depth. We demonstrate that progressive semantic localization, combining adaptive semantic block partitioning with steep polynomial locality schedules, achieves near-baseline language modeling performance while providing interpretable attention patterns. Multiple independent training runs with different random seeds establish that results are statistically robust and highly reproducible. The approach dramatically outperforms both fixed-window localization and naive uniform locality constraints. Analysis reveals that maintaining flexibility through low-fidelity constraints preserves model capacity while providing interpretability benefits, and that steep schedules concentrating locality in decision-critical final layers while preserving distributed learning in early layers achieve near-baseline attention distribution characteristics. These findings demonstrate that interpretability mechanisms should align with semantic structure to achieve practical performance-interpretability tradeoffs for trustworthy AI systems.

**Link**: [arxiv](https://arxiv.org/abs/2511.18375v2),  [pdf](https://arxiv.org/pdf/2511.18375v2)

**Tags**: cs.AI 



### Conveying Imagistic Thinking in TCM Translation: A Prompt Engineering and LLM-Based Evaluation Framework
**Authors**: Jiatong Han

**Updated**: 2025-11-28T10:35:10Z

**Summary**: Traditional Chinese Medicine theory is built on imagistic thinking, in which medical principles and diagnostic and therapeutic logic are structured through metaphor and metonymy. However, existing English translations largely rely on literal rendering, making it difficult for target-language readers to reconstruct the underlying conceptual networks and apply them in clinical practice. This study adopted a human-in-the-loop framework and selected four passages from the medical canon Huangdi Neijing that are fundamental in theory. Through prompt-based cognitive scaffolding, DeepSeek V3.1 was guided to identify metaphor and metonymy in the source text and convey the theory in translation. In the evaluation stage, ChatGPT 5 Pro and Gemini 2.5 Pro were instructed by prompts to simulate three types of real-world readers. Human translations, baseline model translations, and prompt-adjusted translations were scored by the simulated readers across five cognitive dimensions, followed by structured interviews and Interpretative Phenomenological Analysis. Results show that the prompt-adjusted LLM translations perform best across all five dimensions, with high cross-model and cross-role consistency. The interview themes reveal differences between human and machine translation, effective strategies for metaphor and metonymy transfer, and readers' cognitive preferences. This study provides a cognitive, efficient and replicable HITL methodological pathway for translation of ancient, concept-dense texts like TCM.

**Link**: [arxiv](https://arxiv.org/abs/2511.23059v1),  [pdf](https://arxiv.org/pdf/2511.23059v1)

**Tags**: cs.CL cs.AI 



### Geometry-Consistent 4D Gaussian Splatting for Sparse-Input Dynamic View Synthesis
**Authors**: Yiwei Li, Jiannong Cao, Penghui Ruan, Divya Saxena, Songye Zhu, Yinfeng Cao

**Updated**: 2025-11-28T10:11:48Z

**Summary**: Gaussian Splatting has been considered as a novel way for view synthesis of dynamic scenes, which shows great potential in AIoT applications such as digital twins. However, recent dynamic Gaussian Splatting methods significantly degrade when only sparse input views are available, limiting their applicability in practice. The issue arises from the incoherent learning of 4D geometry as input views decrease. This paper presents GC-4DGS, a novel framework that infuses geometric consistency into 4D Gaussian Splatting (4DGS), offering real-time and high-quality dynamic scene rendering from sparse input views. While learning-based Multi-View Stereo (MVS) and monocular depth estimators (MDEs) provide geometry priors, directly integrating these with 4DGS yields suboptimal results due to the ill-posed nature of sparse-input 4D geometric optimization. To address these problems, we introduce a dynamic consistency checking strategy to reduce estimation uncertainties of MVS across spacetime. Furthermore, we propose a global-local depth regularization approach to distill spatiotemporal-consistent geometric information from monocular depths, thereby enhancing the coherent geometry and appearance learning within the 4D volume. Extensive experiments on the popular N3DV and Technicolor datasets validate the effectiveness of GC-4DGS in rendering quality without sacrificing efficiency. Notably, our method outperforms RF-DeRF, the latest dynamic radiance field tailored for sparse-input dynamic view synthesis, and the original 4DGS by 2.62dB and 1.58dB in PSNR, respectively, with seamless deployability on resource-constrained IoT edge devices.

**Link**: [arxiv](https://arxiv.org/abs/2511.23044v1),  [pdf](https://arxiv.org/pdf/2511.23044v1)

**Tags**: cs.CV 



### Social Perceptions of English Spelling Variation on Twitter: A Comparative Analysis of Human and LLM Responses
**Authors**: Dong Nguyen, Laura Rosseel

**Updated**: 2025-11-28T10:06:18Z

**Summary**: Spelling variation (e.g. funnnn vs. fun) can influence the social perception of texts and their writers: we often have various associations with different forms of writing (is the text informal? does the writer seem young?). In this study, we focus on the social perception of spelling variation in online writing in English and study to what extent this perception is aligned between humans and large language models (LLMs). Building on sociolinguistic methodology, we compare LLM and human ratings on three key social attributes of spelling variation (formality, carefulness, age). We find generally strong correlations in the ratings between humans and LLMs. However, notable differences emerge when we analyze the distribution of ratings and when comparing between different types of spelling variation.

**Link**: [arxiv](https://arxiv.org/abs/2511.23041v1),  [pdf](https://arxiv.org/pdf/2511.23041v1)

**Tags**: cs.CL 



### Chiplet-Based RISC-V SoC with Modular AI Acceleration
**Authors**: Suhas Suresh Bharadwaj, Prerana Ramkumar

**Updated**: 2025-11-28T09:49:31Z

**Summary**: Achieving high performance, energy efficiency, and cost-effectiveness while maintaining architectural flexibility is a critical challenge in the development and deployment of edge AI devices. Monolithic SoC designs struggle with this complex balance mainly due to low manufacturing yields (below 16%) at advanced 360 mm^2 process nodes. This paper presents a novel chiplet-based RISC-V SoC architecture that addresses these limitations through modular AI acceleration and intelligent system level optimization. Our proposed design integrates 4 different key innovations in a 30mm x 30mm silicon interposer: adaptive cross-chiplet Dynamic Voltage and Frequency Scaling (DVFS); AI-aware Universal Chiplet Interconnect Express (UCIe) protocol extensions featuring streaming flow control units and compression-aware transfers; distributed cryptographic security across heterogeneous chiplets; and intelligent sensor-driven load migration. The proposed architecture integrates a 7nm RISC-V CPU chiplet with dual 5nm AI accelerators (15 TOPS INT8 each), 16GB HBM3 memory stacks, and dedicated power management controllers. Experimental results across industry standard benchmarks like MobileNetV2, ResNet-50 and real-time video processing demonstrate significant performance improvements. The AI-optimized configuration achieves ~14.7% latency reduction, 17.3% throughput improvement, and 16.2% power reduction compared to previous basic chiplet implementations. These improvements collectively translate to a 40.1% efficiency gain corresponding to ~3.5 mJ per MobileNetV2 inference (860 mW/244 images/s), while maintaining sub-5ms real-time capability across all experimented workloads. These performance upgrades demonstrate that modular chiplet designs can achieve near-monolithic computational density while enabling cost efficiency, scalability and upgradeability, crucial for next-generation edge AI device applications.

**Link**: [arxiv](https://arxiv.org/abs/2509.18355v4),  [pdf](https://arxiv.org/pdf/2509.18355v4)

**Tags**: cs.AR cs.AI 



### Efficient Reasoning via Thought-Training and Thought-Free Inference
**Authors**: Canhui Wu, Qiong Cao, Chao Xue, Wei Xi, Xiaodong He

**Updated**: 2025-11-28T09:49:18Z

**Summary**: Recent advances in large language models (LLMs) have leveraged explicit Chain-of-Thought (CoT) prompting to improve reasoning accuracy. However, most existing methods primarily focus on compressing verbose reasoning outputs. These Long-to-Short transformations aim to improve efficiency, but require a large amount of short CoT data. In this work, we introduce \textbf{3TF} (\textbf{T}hought-\textbf{T}raining and \textbf{T}hought-\textbf{F}ree inference), a framework for efficient reasoning that takes a Short-to-Long perspective. We first train a hybrid model that can operate in both reasoning and non-reasoning modes, and then further train it on CoT-annotated data to internalize structured reasoning, while enforcing concise, thought-free outputs at inference time using the no-reasoning mode. Unlike compression-based approaches, 3TF improves the reasoning quality of non-reasoning outputs, enabling models to perform rich internal reasoning implicitly while keeping external outputs short. Empirically, 3TF-trained models obtain large improvements on reasoning benchmarks under thought-free inference, demonstrating that high quality reasoning can be learned and executed implicitly without explicit step-by-step generation.

**Link**: [arxiv](https://arxiv.org/abs/2511.03408v3),  [pdf](https://arxiv.org/pdf/2511.03408v3)

**Tags**: cs.CL 



### An AI-Enabled Hybrid Cyber-Physical Framework for Adaptive Control in Smart Grids
**Authors**: Muhammad Siddique, Sohaib Zafar

**Updated**: 2025-11-28T09:30:33Z

**Summary**: Evolving smart grids require flexible and adaptive control methods. A harmonized hybrid cyber-physical framework, which considers both physical and cyber layers and ensures adaptability, is one of the critical challenges to enable sustainable and scalable smart grids. This paper proposes a three-layer (physical, cyber, control) architecture, with an energy management system as the core of the system. Adaptive Dynamic Programming(ADP) and Artificial Intelligence-based optimization techniques are used for sustainability and scalability. The deployment is considered under two contingencies: Cloud Independent and cloud-assisted. They allow us to test the proposed model under a low-latency localized decision scenario and also under a centralized control scenario. The architecture is simulated on a standard IEEE 33-Bus system, yielding positive results. The proposed framework can ensure grid stability, optimize dispatch, and respond to ever-changing grid dynamics.

**Link**: [arxiv](https://arxiv.org/abs/2511.21590v2),  [pdf](https://arxiv.org/pdf/2511.21590v2)

**Tags**: cs.LG 



### RvLLM: LLM Runtime Verification with Domain Knowledge
**Authors**: Yedi Zhang, Sun Yi Emma, Annabelle Lee Jia En, Jin Song Dong

**Updated**: 2025-11-28T09:26:03Z

**Summary**: Large language models (LLMs) have emerged as a dominant AI paradigm due to their exceptional text understanding and generation capabilities. However, their tendency to generate inconsistent or erroneous outputs challenges their reliability, especially in high-stakes domains requiring accuracy and trustworthiness. Existing research primarily focuses on detecting and mitigating model misbehavior in general-purpose scenarios, often overlooking the potential of integrating domain-specific knowledge. In this work, we advance misbehavior detection by incorporating domain knowledge. The core idea is to design a general specification language that enables domain experts to customize domain-specific predicates in a lightweight and intuitive manner, supporting later runtime verification of LLM outputs. To achieve this, we design a novel specification language, ESL, and introduce a runtime verification framework, RvLLM, to validate LLM output against domain-specific constraints defined in ESL. We evaluate RvLLM on three representative tasks: violation detection against Singapore Rapid Transit Systems Act, numerical comparison, and inequality solving. Experimental results demonstrate that RvLLM effectively detects erroneous outputs across various LLMs in a lightweight and flexible manner. The results reveal that despite their impressive capabilities, LLMs remain prone to low-level errors due to limited interpretability and a lack of formal guarantees during inference, and our framework offers a potential long-term solution by leveraging expert domain knowledge to rigorously and efficiently verify LLM outputs.

**Link**: [arxiv](https://arxiv.org/abs/2505.18585v3),  [pdf](https://arxiv.org/pdf/2505.18585v3)

**Tags**: cs.AI cs.CL cs.LO 



### Ovis-Image Technical Report
**Authors**: Guo-Hua Wang, Liangfu Cao, Tianyu Cui, Minghao Fu, Xiaohao Chen, Pengxin Zhan, Jianshan Zhao, Lan Li, Bowen Fu, Jiaqi Liu, Qing-Guo Chen

**Updated**: 2025-11-28T08:42:31Z

**Summary**: We introduce $\textbf{Ovis-Image}$, a 7B text-to-image model specifically optimized for high-quality text rendering, designed to operate efficiently under stringent computational constraints. Built upon our previous Ovis-U1 framework, Ovis-Image integrates a diffusion-based visual decoder with the stronger Ovis 2.5 multimodal backbone, leveraging a text-centric training pipeline that combines large-scale pre-training with carefully tailored post-training refinements. Despite its compact architecture, Ovis-Image achieves text rendering performance on par with significantly larger open models such as Qwen-Image and approaches closed-source systems like Seedream and GPT4o. Crucially, the model remains deployable on a single high-end GPU with moderate memory, narrowing the gap between frontier-level text rendering and practical deployment. Our results indicate that combining a strong multimodal backbone with a carefully designed, text-focused training recipe is sufficient to achieve reliable bilingual text rendering without resorting to oversized or proprietary models.

**Link**: [arxiv](https://arxiv.org/abs/2511.22982v1),  [pdf](https://arxiv.org/pdf/2511.22982v1)

**Tags**: cs.CV cs.AI 



### ShoppingComp: Are LLMs Really Ready for Your Shopping Cart?
**Authors**: Huaixiao Tou, Ying Zeng, Cong Ma, Muzhi Li, Minghao Li, Weijie Yuan, He Zhang, Kai Jia

**Updated**: 2025-11-28T08:32:54Z

**Summary**: We present ShoppingComp, a challenging real-world benchmark for rigorously evaluating LLM-powered shopping agents on three core capabilities: precise product retrieval, expert-level report generation, and safety critical decision making. Unlike prior e-commerce benchmarks, ShoppingComp introduces highly complex tasks under the principle of guaranteeing real products and ensuring easy verifiability, adding a novel evaluation dimension for identifying product safety hazards alongside recommendation accuracy and report quality. The benchmark comprises 120 tasks and 1,026 scenarios, curated by 35 experts to reflect authentic shopping needs. Results reveal stark limitations of current LLMs: even state-of-the-art models achieve low performance (e.g., 11.22% for GPT-5, 3.92% for Gemini-2.5-Flash). These findings highlight a substantial gap between research benchmarks and real-world deployment, where LLMs make critical errors such as failure to identify unsafe product usage or falling for promotional misinformation, leading to harmful recommendations. ShoppingComp fills the gap and thus establishes a new standard for advancing reliable and practical agents in e-commerce.

**Link**: [arxiv](https://arxiv.org/abs/2511.22978v1),  [pdf](https://arxiv.org/pdf/2511.22978v1)

**Tags**: cs.CL 



### An LLM-Assisted Multi-Agent Control Framework for Roll-to-Roll Manufacturing Systems
**Authors**: Jiachen Li, Shihao Li, Christopher Martin, Zijun Chen, Dongmei Chen, Wei Li

**Updated**: 2025-11-28T08:30:12Z

**Summary**: Roll-to-roll manufacturing requires precise tension and velocity control to ensure product quality, yet controller commissioning and adaptation remain time-intensive processes dependent on expert knowledge. This paper presents an LLM-assisted multi-agent framework that automates control system design and adaptation for R2R systems while maintaining safety. The framework operates through five phases: system identification from operational data, automated controller selection and tuning, sim-to-real adaptation with safety verification, continuous monitoring with diagnostic capabilities, and periodic model refinement. Experimental validation on a R2R system demonstrates successful tension regulation and velocity tracking under significant model uncertainty, with the framework achieving performance convergence through iterative adaptation. The approach reduces manual tuning effort while providing transparent diagnostic information for maintenance planning, offering a practical pathway for integrating AI-assisted automation in manufacturing control systems.

**Link**: [arxiv](https://arxiv.org/abs/2511.22975v1),  [pdf](https://arxiv.org/pdf/2511.22975v1)

**Tags**: eess.SY 



### Training-Free Loosely Speculative Decoding: Accepting Semantically Correct Drafts Beyond Exact Match
**Authors**: Jinze Li, Yixing Xu, Guanchen Li, Shuo Yang, Jinfeng Xu, Xuanwu Yin, Dong Li, Edith C. H. Ngai, Emad Barsoum

**Updated**: 2025-11-28T08:23:30Z

**Summary**: Large language models (LLMs) achieve strong performance across diverse tasks but suffer from high inference latency due to their autoregressive generation. Speculative Decoding (SPD) mitigates this issue by verifying candidate tokens in parallel from a smaller draft model, yet its strict exact-match verification discards many semantically valid continuations. Moreover, existing training-based SPD methods often suffer from performance degradation on out-of-distribution (OOD) tasks. To this end, we propose Training-Free Loosely Speculative Decoding (FLy), a novel method that loosens the rigid verification criterion by leveraging the target model's self-corrective behavior to judge whether a draft-target mismatch remains semantically valid. FLy introduces a two-tier mechanism: an entropy-level gate that identifies whether the current token allows multiple plausible alternatives or is nearly deterministic, and a token-level deferred window that distinguishes genuine errors from differently worded yet semantically correct variants. To further reduce latency, we design a multi-level acceleration strategy that accelerates not only the target model but also the drafter itself. Owing to its training-free design, FLy composes seamlessly with arbitrary draft-target pairs and generalizes across models and domains without hyperparameter re-tuning. Experiments show that FLy preserves more than 99% of the target model's accuracy while achieving an average 2.81x speedup on Llama-3.1-70B-Instruct and 5.07x speedup on the 405B variant. Notably, on out-of-domain datasets, our method remains highly effective and outperforms the training-based method EAGLE-3 by 1.62x.

**Link**: [arxiv](https://arxiv.org/abs/2511.22972v1),  [pdf](https://arxiv.org/pdf/2511.22972v1)

**Tags**: cs.CL 



### Experts are all you need: A Composable Framework for Large Language Model Inference
**Authors**: Shrihari Sridharan, Sourjya Roy, Anand Raghunathan, Kaushik Roy

**Updated**: 2025-11-28T08:00:16Z

**Summary**: Large Language Models (LLMs) have achieved state-of-the-art accuracies in a variety of natural language processing (NLP) tasks. However, this success comes at the cost of increased model sizes which leads to additional computational burden. Mixture of Experts (MoEs) overcome this bottleneck by decoupling model capacity from computation by only activating a subset of parameters or "experts". However, these models require joint pretraining of these experts along with the router and do not model multi-step reasoning. In contrast, multi-agent frameworks improve reasoning by decomposing complex problems into modular subtasks. However, these frameworks rely on sequential "plan--act--observe" loops, which introduce significant latency. Our work, Comp-LLM, addresses these challenges by introducing a composable inference framework that enables cross-expert collaboration via an explicit sub-query dependency graph. Comp-LLM consists of three components: (1) A Sub-query Generator that decomposes an input query, assigns each sub-query to an appropriate expert using embedding similarity, and constructs a dependency graph; (2) A Query Executor that processes nodes in the graph and identifies opportunities for parallelism based on dependencies and resource constraints; and (3) A Response Aggregator that synthesizes intermediate expert responses into a coherent final answer. Across several benchmarks, Comp-LLM achieves up to 11.01% accuracy improvement over monolithic LLMs of similar size, while offering 1.67x--3.56x reduction in model size with no significant degradation relative to the largest model in its family. Additionally, Comp-LLM provides 1.1x--1.7x latency improvement compared to sequential sub-query processing.

**Link**: [arxiv](https://arxiv.org/abs/2511.22955v1),  [pdf](https://arxiv.org/pdf/2511.22955v1)

**Tags**: cs.LG 



### InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training
**Authors**: Pengkai Wang, Linus, Pengwei Liu, Zhijie Sang, Congkai Xie, Hongxia Yang

**Updated**: 2025-11-28T07:52:44Z

**Summary**: Reinforcement learning has powered many of the recent breakthroughs in large language models, especially for tasks where rewards can be computed automatically, such as code generation. However, these methods deteriorate in open-ended domains like medical consultation, where feedback is inherently ambiguous, highly context-dependent, and cannot be reduced to a reliable scalar signal. In such settings, RL must either rely on supervision-intensive reward models that often fail to generalize, or it falls into pathological behaviors such as reward hacking - an especially troubling risk for high-stakes medical dialogue. To address these limitations, we introduce ORBIT, an open-ended rubric-based incremental training framework for high-stakes medical dialogue. ORBIT integrates synthetic dialogue generation with dynamically constructed rubrics that serve as adaptive guides for incremental RL. Instead of relying on external medical knowledge bases or handcrafted rule sets, ORBIT uses rubric-driven feedback to steer the learning process. Its judge component can be instantiated with general-purpose instruction-following LLMs, removing the need for any task-specific fine-tuning. Applied to the Qwen3-4B-Instruct model, ORBIT raises the HealthBench-Hard score from 7.0 to 27.5 using only 2k training samples, achieving SOTA performance for models at this scale. With larger rubric datasets, ORBIT-trained models further compete with the strongest open-source baselines on HealthBench-Hard. Our analysis shows that rubric-guided RL consistently improves consultation quality across diverse medical scenarios. We also apply such rubric generation and training pipeline to InfoBench, where ORBIT enhances instruction-following performance, highlighting the generality of rubric-based feedback.

**Link**: [arxiv](https://arxiv.org/abs/2510.15859v3),  [pdf](https://arxiv.org/pdf/2510.15859v3)

**Tags**: cs.CL cs.AI 



### Atom of Thoughts for Markov LLM Test-Time Scaling
**Authors**: Fengwei Teng, Quan Shi, Zhaoyang Yu, Jiayi Zhang, Chenglin Wu, Yuyu Luo, Zhijiang Guo

**Updated**: 2025-11-28T07:41:57Z

**Summary**: Large Language Models (LLMs) achieve superior performance through training-time scaling, and test-time scaling further enhances their capabilities by conducting effective reasoning during inference. However, as the scale of reasoning increases, existing test-time scaling methods suffer from accumulated historical information, which not only wastes computational resources but also interferes with effective reasoning. To address this issue, we observe that complex reasoning can be achieved by solving a series of independent and self-contained subquestions. These subquestions are essentially \textit{atomic questions}, exhibiting the memoryless property similar to Markov processes. Based on this observation, we propose Atom of Thoughts (\our), where each state transition consists of decomposing the current question into a dependency-based directed acyclic graph and contracting its subquestions, forming a simplified question that maintains answer equivalence with the original problem. This answer preservation enables the iterative \textit{decomposition-contraction} process to naturally form a meaningful Markov reasoning process. Furthermore, these atomic states can be seamlessly integrated into existing test-time scaling methods, enabling \our to serve as a plug-in enhancement for improving reasoning capabilities. Experiments across six benchmarks demonstrate the effectiveness of \our both as a standalone framework and a plug-in enhancement. Notably, on HotpotQA, when applied to gpt-4o-mini, \our achieves an \textbf{80.6\%} F1 score, surpassing o3-mini by \textbf{3.4\%} and DeepSeek-R1 by \textbf{10.6\%}. The code is available at \href{https://github.com/qixucen/atom}{https://github.com/qixucen/atom}.

**Link**: [arxiv](https://arxiv.org/abs/2502.12018v3),  [pdf](https://arxiv.org/pdf/2502.12018v3)

**Tags**: cs.CL cs.AI cs.LG 



### Visual Puns from Idioms: An Iterative LLM-T2IM-MLLM Framework
**Authors**: Kelaiti Xiao, Liang Yang, Dongyu Zhang, Paerhati Tulajiang, Hongfei Lin

**Updated**: 2025-11-28T07:30:58Z

**Summary**: We study idiom-based visual puns--images that align an idiom's literal and figurative meanings--and present an iterative framework that coordinates a large language model (LLM), a text-to-image model (T2IM), and a multimodal LLM (MLLM) for automatic generation and evaluation. Given an idiom, the system iteratively (i) generates detailed visual prompts, (ii) synthesizes an image, (iii) infers the idiom from the image, and (iv) refines the prompt until recognition succeeds or a step limit is reached. Using 1,000 idioms as inputs, we synthesize a corresponding dataset of visual pun images with paired prompts, enabling benchmarking of both generation and understanding. Experiments across 10 LLMs, 10 MLLMs, and one T2IM (Qwen-Image) show that MLLM choice is the primary performance driver: GPT achieves the highest accuracies, Gemini follows, and the best open-source MLLM (Gemma) is competitive with some closed models. On the LLM side, Claude attains the strongest average performance for prompt generation.

**Link**: [arxiv](https://arxiv.org/abs/2511.22943v1),  [pdf](https://arxiv.org/pdf/2511.22943v1)

**Tags**: cs.CL cs.CV 



### RAG-Empowered LLM-Driven Dynamic Radio Resource Management in Open 6G RAN
**Authors**: Onur Salan, Burak Çırağ, Onur Sever, İbrahim Hökelek, Ali Görçin, Hakan Ali Çırpan

**Updated**: 2025-11-28T07:20:25Z

**Summary**: Implications of the advancements in the area of artificial intelligence to the wireless communications is extremely significant, especially in terms of resource management. In this paper, a Retrieval-Augmented Generation (RAG)-empowered Large Language Model (ReLLM)-driven dynamic radio resource management framework for Open Radio Access Network (O-RAN) inspired 6G networks is proposed. The introduced methodology leverages the ReLLM framework to interpret both historical and real-time network data, enabling adaptive control of network slices. The ReLLM is founded on two specialized agents, one is responsible for proactively detecting service level agreement (SLA) violations by continuously monitoring and estimating slice-specific performance metrics, and the other one is responsible for dynamically reallocating physical resource blocks when the SLA violation probability exceeds a pre-defined threshold. The primary objective of this dual-agent design is to minimize unnecessary LLM inference calls while satisfying the SLA requirements of the slices, thereby improving computational and energy efficiency. The proposed ReLLM framework is implemented and validated on an end-to-end O-RAN testbed built upon open-source OpenAirInterface emulators. The experimental results demonstrate that the LLM approach with its reduced token consumption feature maintains a near-zero drop ratio for the low-priority slice while simultaneously satisfying acceptable latency performance for the high-priority slice. The ReLLM-driven design improves reliability and SLA compliance, confirming its practicality for real-world O-RAN testbeds and its potential applicability to future 6G networks.

**Link**: [arxiv](https://arxiv.org/abs/2511.22933v1),  [pdf](https://arxiv.org/pdf/2511.22933v1)

**Tags**: eess.SP 



### FedAPTA: Federated Multi-task Learning in Computing Power Networks with Adaptive Layer-wise Pruning and Task-aware Aggregation
**Authors**: Zhen Yu, Yachao Yuan, Jin Wang, Zhipeng Cheng, Jianhua Hu

**Updated**: 2025-11-28T07:07:55Z

**Summary**: Federated Learning (FL) has shown considerable promise in Computing Power Networks (CPNs) for privacy protection, efficient data utilization, and dynamic collaboration. Although it offers practical benefits, applying FL in CPNs continues to encounter a major obstacle, i.e., multi-task deployment. However, existing work mainly focuses on mitigating FL's computation and communication overhead of a single task while overlooking the computing resource wastage issue of heterogeneous devices across multiple tasks in FL under CPNs. To tackle this, we design FedAPTA, a federated multi-task learning framework in CPNs. FedAPTA alleviates computing resource wastage through the developed layer-wise model pruning technique, which reduces local model size while considering both data and device heterogeneity. To aggregate structurally heterogeneous local models of different tasks, we introduce a heterogeneous model recovery strategy and a task-aware model aggregation method that enables the aggregation through infilling local model architecture with the shared global model and clustering local models according to their specific tasks. We deploy FedAPTA on a realistic FL platform and benchmark it against nine SOTA FL methods. The experimental outcomes demonstrate that the proposed FedAPTA considerably outperforms the state-of-the-art FL methods by up to 4.23%. Our code is available at https://github.com/Zhenzovo/FedCPN.

**Link**: [arxiv](https://arxiv.org/abs/2508.02230v2),  [pdf](https://arxiv.org/pdf/2508.02230v2)

**Tags**: cs.DC 



### Artwork Interpretation with Vision Language Models: A Case Study on Emotions and Emotion Symbols
**Authors**: Sebastian Padó, Kerstin Thomas

**Updated**: 2025-11-28T07:04:09Z

**Summary**: Emotions are a fundamental aspect of artistic expression. Due to their abstract nature, there is a broad spectrum of emotion realization in artworks. These are subject to historical change and their analysis requires expertise in art history. In this article, we investigate which aspects of emotional expression can be detected by current (2025) vision language models (VLMs). We present a case study of three VLMs (Llava-Llama and two Qwen models) in which we ask these models four sets of questions of increasing complexity about artworks (general content, emotional content, expression of emotions, and emotion symbols) and carry out a qualitative expert evaluation. We find that the VLMs recognize the content of the images surprisingly well and often also which emotions they depict and how they are expressed. The models perform best for concrete images but fail for highly abstract or highly symbolic images. Reliable recognition of symbols remains fundamentally difficult. Furthermore, the models continue to exhibit the well-known LLM weakness of providing inconsistent answers to related questions.

**Link**: [arxiv](https://arxiv.org/abs/2511.22929v1),  [pdf](https://arxiv.org/pdf/2511.22929v1)

**Tags**: cs.CV cs.CL 



### AgentShield: Make MAS more secure and efficient
**Authors**: Kaixiang Wang, Zhaojiacheng Zhou, Bunyod Suvonov, Jiong Lou, Jie LI

**Updated**: 2025-11-28T06:55:50Z

**Summary**: Large Language Model (LLM)-based Multi-Agent Systems (MAS) offer powerful cooperative reasoning but remain vulnerable to adversarial attacks, where compromised agents can undermine the system's overall performance. Existing defenses either depend on single trusted auditors, creating single points of failure, or sacrifice efficiency for robustness. To resolve this tension, we propose \textbf{AgentShield}, a distributed framework for efficient, decentralized auditing. AgentShield introduces a novel three-layer defense: \textbf{(i) Critical Node Auditing} prioritizes high-influence agents via topological analysis; \textbf{(ii) Light Token Auditing} implements a cascade protocol using lightweight sentry models for rapid discriminative verification; and \textbf{(iii) Two-Round Consensus Auditing} triggers heavyweight arbiters only upon uncertainty to ensure global agreement. This principled design optimizes the robustness-efficiency trade-off. Experiments demonstrate that AgentShield achieves a 92.5\% recovery rate and reduces auditing overhead by over 70\% compared to existing methods, maintaining high collaborative accuracy across diverse MAS topologies and adversarial scenarios.

**Link**: [arxiv](https://arxiv.org/abs/2511.22924v1),  [pdf](https://arxiv.org/pdf/2511.22924v1)

**Tags**: cs.MA cs.AI cs.CR 



### CO-QLink: Cryogenic Optical Link for Scalable Quantum Computing Systems and High-Performance Cryogenic Computing Systems
**Authors**: Zheng Chang, Siqi Zhang, Wenqiang Huang, Tian Tian, Qichun Liu, Tiefu Li, Nan Qi, Yuanjin Zheng, Zhihua Wang, Yanshu Guo, Hanjun Jiang

**Updated**: 2025-11-28T06:47:27Z

**Summary**: Cryogenic systems necessitate extensive data transmission between room-temperature and cryogenic environments, as well as within the cryogenic temperature domain. High-speed, low-power data transmission is pivotal to enabling the deployment of larger-scale cryogenic systems, including the scalable quantum computing systems and the high-performance cryogenic computing systems fully immersed in liquid nitrogen. In contrast to wireline and microwave links, optical communication links are emerging as a solution characterized by high data rates, high energy efficiency, low signal attenuation, absence of thermal conduction, and superior scalability. In this work, a 4K heat-insulated high-speed (56Gbps) low-power (1.6pJ/b) transceiver (TRX) that achieves a complete link between 4K systems and room temperature (RT) equipment is presented. Copackaged with a PIN photodiode (PD), the RX uses an inverter-based analog front-end and an analog half-rate clock data recovery loop. Connecting to a Mach-Zehnder modulator (MZM), the TX contains a voltage-mode driver with current-mode injection for low-power output-swing-boosting and 3-tap feed-forward equalization (FFE). This link has been demonstrated in the control and readout of a complete superconducting quantum computing system.

**Link**: [arxiv](https://arxiv.org/abs/2511.22920v1),  [pdf](https://arxiv.org/pdf/2511.22920v1)

**Tags**: quant-ph eess.SP 



### RIS-Assisted Physical Layer Security: Artificial Noise-Driven Optimization and Measurements
**Authors**: Ahmet Muaz Aktas, Sefa Kayraklik, Sultangali Arzykulov, Galymzhan Nauryzbayev, Ibrahim Hokelek, Ali Gorcin

**Updated**: 2025-11-28T06:32:35Z

**Summary**: Reconfigurable intelligent surface (RIS) has emerged as a key enabler for providing signal coverage, energy efficiency, reliable communication, and physical layer security (PLS) in next-generation wireless communication networks. This paper investigates an artificial noise (AN)-driven RIS-assisted secure communication system. The RIS is partitioned into two segments, where the first segment is configured to direct the communication signal (CS) toward the legitimate user (Bob), and the other one is configured to steer the AN toward the eavesdropper (Eve). To this end, iterative and discrete Fourier transform-based algorithms are developed for practical RIS phase shift optimization. The power allocation between the CS and the AN signals is optimized in such a way that the secrecy capacity (SC) is maximized while limiting Eve's channel capacity. The proposed PLS framework is evaluated through both simulations and software defined radio based testbed experiments. The results demonstrate promising improvements in the SC, highlighting the potential of AN-driven RIS-assisted PLS for practical deployments.

**Link**: [arxiv](https://arxiv.org/abs/2511.22910v1),  [pdf](https://arxiv.org/pdf/2511.22910v1)

**Tags**: eess.SP 



### Language-conditioned world model improves policy generalization by reading environmental descriptions
**Authors**: Anh Nguyen, Stefan Lee

**Updated**: 2025-11-28T06:13:27Z

**Summary**: To interact effectively with humans in the real world, it is important for agents to understand language that describes the dynamics of the environment--that is, how the environment behaves--rather than just task instructions specifying "what to do". Understanding this dynamics-descriptive language is important for human-agent interaction and agent behavior. Recent work address this problem using a model-based approach: language is incorporated into a world model, which is then used to learn a behavior policy. However, these existing methods either do not demonstrate policy generalization to unseen games or rely on limiting assumptions. For instance, assuming that the latency induced by inference-time planning is tolerable for the target task or expert demonstrations are available. Expanding on this line of research, we focus on improving policy generalization from a language-conditioned world model while dropping these assumptions. We propose a model-based reinforcement learning approach, where a language-conditioned world model is trained through interaction with the environment, and a policy is learned from this model--without planning or expert demonstrations. Our method proposes Language-aware Encoder for Dreamer World Model (LED-WM) built on top of DreamerV3. LED-WM features an observation encoder that uses an attention mechanism to explicitly ground language descriptions to entities in the observation. We show that policies trained with LED-WM generalize more effectively to unseen games described by novel dynamics and language compared to other baselines in several settings in two environments: MESSENGER and MESSENGER-WM.To highlight how the policy can leverage the trained world model before real-world deployment, we demonstrate the policy can be improved through fine-tuning on synthetic test trajectories generated by the world model.

**Link**: [arxiv](https://arxiv.org/abs/2511.22904v1),  [pdf](https://arxiv.org/pdf/2511.22904v1)

**Tags**: cs.CL cs.LG 



### The Immutable Tensor Architecture: A Pure Dataflow Approach for Secure, Energy-Efficient AI Inference
**Authors**: Fang Li

**Updated**: 2025-11-28T05:36:12Z

**Summary**: The deployment of Large Language Models (LLMs) on consumer edge devices is throttled by the "Memory Wall" -- the prohibitive bandwidth and energy cost of fetching gigabytes of model weights from DRAM for every token generated. Current architectures (GPUs, NPUs) treat model weights as mutable software data, incurring massive energy penalties to maintain general-purpose programmability. We propose The Immutable Tensor Architecture (ITA), a paradigm shift that treats model weights not as data, but as physical circuit topology. By encoding parameters directly into the metal interconnects and logic of mature-node ASICs (28nm/40nm), ITA eliminates the memory hierarchy entirely. We present a "Split-Brain" system design where a host CPU manages dynamic KV-cache operations while the ITA ASIC acts as a stateless, ROM-embedded dataflow engine.

**Link**: [arxiv](https://arxiv.org/abs/2511.22889v1),  [pdf](https://arxiv.org/pdf/2511.22889v1)

**Tags**: cs.AR 



### SlotVLA: Towards Modeling of Object-Relation Representations in Robotic Manipulation
**Authors**: Taisei Hanyu, Nhat Chung, Huy Le, Toan Nguyen, Yuki Ikebe, Anthony Gunderman, Duy Nguyen Ho Minh, Khoa Vo, Tung Kieu, Kashu Yamazaki, Chase Rainwater, Anh Nguyen, Ngan Le

**Updated**: 2025-11-28T05:35:21Z

**Summary**: Inspired by how humans reason over discrete objects and their relationships, we explore whether compact object-centric and object-relation representations can form a foundation for multitask robotic manipulation. Most existing robotic multitask models rely on dense embeddings that entangle both object and background cues, raising concerns about both efficiency and interpretability. In contrast, we study object-relation-centric representations as a pathway to more structured, efficient, and explainable visuomotor control. Our contributions are two-fold. First, we introduce LIBERO+, a fine-grained benchmark dataset designed to enable and evaluate object-relation reasoning in robotic manipulation. Unlike prior datasets, LIBERO+ provides object-centric annotations that enrich demonstrations with box- and mask-level labels as well as instance-level temporal tracking, supporting compact and interpretable visuomotor representations. Second, we propose SlotVLA, a slot-attention-based framework that captures both objects and their relations for action decoding. It uses a slot-based visual tokenizer to maintain consistent temporal object representations, a relation-centric decoder to produce task-relevant embeddings, and an LLM-driven module that translates these embeddings into executable actions. Experiments on LIBERO+ demonstrate that object-centric slot and object-relation slot representations drastically reduce the number of required visual tokens, while providing competitive generalization. Together, LIBERO+ and SlotVLA provide a compact, interpretable, and effective foundation for advancing object-relation-centric robotic manipulation.

**Link**: [arxiv](https://arxiv.org/abs/2511.06754v2),  [pdf](https://arxiv.org/pdf/2511.06754v2)

**Tags**: cs.RO cs.CV 



### Adversarial Training for Process Reward Models
**Authors**: Gurusha Juneja, Deepak Nathani, William Yang Wang

**Updated**: 2025-11-28T05:32:01Z

**Summary**: Process Reward Models (PRMs) enhance reasoning ability of LLMs by providing step-level supervision. However, their widespread adoption is limited due to expensive manual step-level annotation and poor generalization of static training data to novel errors. We introduce Adversarially Trained PRMs (\texttt{APRM}), where a Generator ($G$) learns to produce reasoning errors to deceive a PRM ($R$), while $R$ concurrently learns to detect them. This interaction yields progressively harder negatives for $R$, improving its robustness and generalization to novel errors without requiring manual step-level labels. Averaged across diverse mathematical reasoning benchmarks, \texttt{APRM} improves solver accuracy by $+3.4$ percentage points (pp) over the strongest PRM baseline. \texttt{APRM} achieves gains of $+5.3$ pp on out-of-distribution tasks.

**Link**: [arxiv](https://arxiv.org/abs/2511.22888v1),  [pdf](https://arxiv.org/pdf/2511.22888v1)

**Tags**: cs.LG cs.AI 



### Un-mixing Test-time Adaptation under Heterogeneous Data Streams
**Authors**: Zixian Su, Jingwei Guo, Xi Yang, Qiufeng Wang, Kaizhu Huang

**Updated**: 2025-11-28T05:25:45Z

**Summary**: Deploying deep models in real-world scenarios remains challenging due to significant performance drops under distribution shifts between training and deployment environments. Test-Time Adaptation (TTA) has recently emerged as a promising solution, enabling on-the-fly model adaptation. However, its effectiveness deteriorates in the presence of mixed distribution shifts -- common in practical settings -- where multiple target domains coexist. In this paper, we study TTA under mixed distribution shifts and move beyond conventional whole-batch adaptation paradigms. By revisiting distribution shifts from a spectral perspective, we find that the heterogeneity across latent domains is often pronounced in Fourier space. In particular, high-frequency components encode domain-specific variations, which facilitates clearer separation of samples from different distributions. Motivated by this observation, we propose to un-mix heterogeneous data streams using high-frequency domain cues, making diverse shift patterns more tractable. To this end, we propose Frequency-based Decentralized Adaptation (FreDA), a novel framework that decomposes globally heterogeneous data stream into locally homogeneous clusters in the Fourier space. It leverages decentralized learning and augmentation strategies to robustly adapt under mixed domain shifts. Extensive experiments across various environments (corrupted, natural, and medical) show the superiority of our method over the state-of-the-arts.

**Link**: [arxiv](https://arxiv.org/abs/2411.15173v4),  [pdf](https://arxiv.org/pdf/2411.15173v4)

**Tags**: cs.LG cs.AI 



### InsightEval: An Expert-Curated Benchmark for Assessing Insight Discovery in LLM-Driven Data Agents
**Authors**: Zhenghao Zhu, Yuanfeng Song, Xin Chen, Chengzhong Liu, Yakun Cui, Caleb Chen Cao, Sirui Han, Yike Guo

**Updated**: 2025-11-28T05:19:24Z

**Summary**: Data analysis has become an indispensable part of scientific research. To discover the latent knowledge and insights hidden within massive datasets, we need to perform deep exploratory analysis to realize their full value. With the advent of large language models (LLMs) and multi-agent systems, more and more researchers are making use of these technologies for insight discovery. However, there are few benchmarks for evaluating insight discovery capabilities. As one of the most comprehensive existing frameworks, InsightBench also suffers from many critical flaws: format inconsistencies, poorly conceived objectives, and redundant insights. These issues may significantly affect the quality of data and the evaluation of agents. To address these issues, we thoroughly investigate shortcomings in InsightBench and propose essential criteria for a high-quality insight benchmark. Regarding this, we develop a data-curation pipeline to construct a new dataset named InsightEval. We further introduce a novel metric to measure the exploratory performance of agents. Through extensive experiments on InsightEval, we highlight prevailing challenges in automated insight discovery and raise some key findings to guide future research in this promising direction.

**Link**: [arxiv](https://arxiv.org/abs/2511.22884v1),  [pdf](https://arxiv.org/pdf/2511.22884v1)

**Tags**: cs.AI 



### FEANEL: A Benchmark for Fine-Grained Error Analysis in K-12 English Writing
**Authors**: Jingheng Ye, Shen Wang, Jiaqi Chen, Hebin Wang, Deqing Zou, Yanyu Zhu, Jiwei Tang, Hai-Tao Zheng, Ruitong Liu, Haoyang Li, Yanfeng Wang, Qingsong Wen

**Updated**: 2025-11-28T05:17:45Z

**Summary**: Large Language Models (LLMs) have transformed artificial intelligence, offering profound opportunities for educational applications. However, their ability to provide fine-grained educational feedback for K-12 English writing remains underexplored. In this paper, we challenge the error analysis and pedagogical skills of LLMs by introducing the problem of Fine-grained Error Analysis for English Learners and present the Fine-grained Error ANalysis for English Learners (FEANEL) Benchmark. The benchmark comprises 1,000 essays written by elementary and secondary school students, and a well-developed English writing error taxonomy. Each error is annotated by language education experts and categorized by type, severity, and explanatory feedback, using a part-of-speech-based taxonomy they co-developed. We evaluate state-of-the-art LLMs on the FEANEL Benchmark to explore their error analysis and pedagogical abilities. Experimental results reveal significant gaps in current LLMs' ability to perform fine-grained error analysis, highlighting the need for advancements in particular methods for educational applications.

**Link**: [arxiv](https://arxiv.org/abs/2511.22883v1),  [pdf](https://arxiv.org/pdf/2511.22883v1)

**Tags**: cs.CL 



### Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems
**Authors**: Shashwat Jaiswal, Shrikara Arun, Anjaly Parayil, Ankur Mallick, Spyros Mastorakis, Alind Khare, Chloi Alverti, Renee St Amant, Chetan Bansal, Victor Rühle, Josep Torrellas

**Updated**: 2025-11-28T05:04:02Z

**Summary**: Low-Rank Adaptation (LoRA) has become the de facto method for parameter-efficient fine-tuning of large language models (LLMs), enabling rapid adaptation to diverse domains. In production, LoRA-based models are served at scale, creating multi-tenant environments with hundreds of adapters sharing a base model. However, state-of-the-art serving systems co-batch heterogeneous adapters without accounting for rank (size) variability, leading to severe performance skew, which ultimately requires adding more GPUs to satisfy service-level objectives (SLOs). Existing optimizations, focused on loading, caching, and kernel execution, ignore this heterogeneity, leaving GPU resources underutilized. We present LoRAServe, a workload-aware dynamic adapter placement and routing framework designed to tame rank diversity in LoRA serving. By dynamically rebalancing adapters across GPUs and leveraging GPU Direct RDMA for remote access, LoRAServe maximizes throughput and minimizes tail latency under real-world workload drift. Evaluations on production traces from Company X show that LoRAServe elicits up to 2$\times$ higher throughput, up to 9$\times$ lower TTFT, while using up to 50% fewer GPUs under SLO constraints compared to state-of-the-art systems.

**Link**: [arxiv](https://arxiv.org/abs/2511.22880v1),  [pdf](https://arxiv.org/pdf/2511.22880v1)

**Tags**: cs.DC cs.AI cs.LG 



### JBE-QA: Japanese Bar Exam QA Dataset for Assessing Legal Domain Knowledge
**Authors**: Zhihan Cao, Fumihito Nishino, Hiroaki Yamada, Nguyen Ha Thanh, Yusuke Miyao, Ken Satoh

**Updated**: 2025-11-28T04:16:17Z

**Summary**: We introduce JBE-QA, a Japanese Bar Exam Question-Answering dataset to evaluate large language models' legal knowledge. Derived from the multiple-choice (tanto-shiki) section of the Japanese bar exam (2015-2024), JBE-QA provides the first comprehensive benchmark for Japanese legal-domain evaluation of LLMs. It covers the Civil Code, the Penal Code, and the Constitution, extending beyond the Civil Code focus of prior Japanese resources. Each question is decomposed into independent true/false judgments with structured contextual fields. The dataset contains 3,464 items with balanced labels. We evaluate 26 LLMs, including proprietary, open-weight, Japanese-specialised, and reasoning models. Our results show that proprietary models with reasoning enabled perform best, and the Constitution questions are generally easier than the Civil Code or the Penal Code questions.

**Link**: [arxiv](https://arxiv.org/abs/2511.22869v1),  [pdf](https://arxiv.org/pdf/2511.22869v1)

**Tags**: cs.CL 



### ARIAL: An Agentic Framework for Document VQA with Precise Answer Localization
**Authors**: Ahmad Mohammadshirazi, Pinaki Prasad Guha Neogi, Dheeraj Kulshrestha, Rajiv Ramnath

**Updated**: 2025-11-28T04:10:44Z

**Summary**: Document Visual Question Answering (VQA) requires models to not only extract accurate textual answers but also precisely localize them within document images, a capability critical for interpretability in high-stakes applications. However, existing systems achieve strong textual accuracy while producing unreliable spatial grounding, or sacrifice performance for interpretability. We present ARIAL (Agentic Reasoning for Interpretable Answer Localization), a modular framework that orchestrates specialized tools through an LLM-based planning agent to achieve both precise answer extraction and reliable spatial grounding. ARIAL decomposes Document VQA into structured subtasks: OCR-based text extraction with TrOCR, retrieval-augmented context selection using semantic search, answer generation via a fine-tuned Gemma 3-27B model, and explicit bounding-box localization through text-to-region alignment. This modular architecture produces transparent reasoning traces, enabling tool-level auditability and independent component optimization. We evaluate ARIAL on four benchmarks (DocVQA, FUNSD, CORD, and SROIE) using both textual accuracy (ANLS) and spatial precision (mAP at IoU 0.50 to 0.95). ARIAL achieves state-of-the-art results across all datasets: 88.7 ANLS and 50.1 mAP on DocVQA, 90.0 ANLS and 50.3 mAP on FUNSD, 85.5 ANLS and 60.2 mAP on CORD, and 93.1 ANLS on SROIE, surpassing the previous best method (DLaVA) by +2.8 ANLS and +3.9 mAP on DocVQA. Our work demonstrates how agentic orchestration of specialized tools can simultaneously improve performance and interpretability, providing a pathway toward trustworthy, explainable document AI systems.

**Link**: [arxiv](https://arxiv.org/abs/2511.18192v2),  [pdf](https://arxiv.org/pdf/2511.18192v2)

**Tags**: cs.CV cs.AI 



### Physics Steering: Causal Control of Cross-Domain Concepts in a Physics Foundation Model
**Authors**: Rio Alexa Fear, Payel Mukhopadhyay, Michael McCabe, Alberto Bietti, Miles Cranmer

**Updated**: 2025-11-28T04:04:02Z

**Summary**: Recent advances in mechanistic interpretability have revealed that large language models (LLMs) develop internal representations corresponding not only to concrete entities but also distinct, human-understandable abstract concepts and behaviour. Moreover, these hidden features can be directly manipulated to steer model behaviour. However, it remains an open question whether this phenomenon is unique to models trained on inherently structured data (ie. language, images) or if it is a general property of foundation models. In this work, we investigate the internal representations of a large physics-focused foundation model. Inspired by recent work identifying single directions in activation space for complex behaviours in LLMs, we extract activation vectors from the model during forward passes over simulation datasets for different physical regimes. We then compute "delta" representations between the two regimes. These delta tensors act as concept directions in activation space, encoding specific physical features. By injecting these concept directions back into the model during inference, we can steer its predictions, demonstrating causal control over physical behaviours, such as inducing or removing some particular physical feature from a simulation. These results suggest that scientific foundation models learn generalised representations of physical principles. They do not merely rely on superficial correlations and patterns in the simulations. Our findings open new avenues for understanding and controlling scientific foundation models and has implications for AI-enabled scientific discovery.

**Link**: [arxiv](https://arxiv.org/abs/2511.20798v2),  [pdf](https://arxiv.org/pdf/2511.20798v2)

**Tags**: cs.LG cs.AI physics.comp-ph 



### DistTrain: Addressing Model and Data Heterogeneity with Disaggregated Training for Multimodal Large Language Models
**Authors**: Zili Zhang, Yinmin Zhong, Yimin Jiang, Hanpeng Hu, Jianjian Sun, Zheng Ge, Yibo Zhu, Daxin Jiang, Xin Jin

**Updated**: 2025-11-28T03:45:05Z

**Summary**: Multimodal large language models (LLMs) empower LLMs to ingest inputs and generate outputs in multiple forms, such as text, image, and audio. However, the integration of multiple modalities introduces heterogeneity in both the model and training data, creating unique systems challenges.   We propose DistTrain, a disaggregated training system for multimodal LLMs. DistTrain incorporates two novel disaggregation techniques to address model and data heterogeneity, respectively. The first is disaggregated model orchestration, which separates the training for modality encoder, LLM backbone, and modality generator. This allows the three components to adaptively and independently orchestrate their resources and parallelism configurations. The second is disaggregated data preprocessing, which decouples data preprocessing from training. This eliminates resource contention between preprocessing and training, and enables efficient data reordering to mitigate stragglers within and between microbatches caused by data heterogeneity. We evaluate DistTrain across different sizes of multimodal LLMs on a large-scale production cluster. The experimental results show that DistTrain achieves 54.7% Model FLOPs Utilization (MFU) when training a 72B multimodal LLM on 1172 GPUs and outperforms Megatron-LM by up to 2.2x on training throughput.

**Link**: [arxiv](https://arxiv.org/abs/2408.04275v4),  [pdf](https://arxiv.org/pdf/2408.04275v4)

**Tags**: cs.DC 



### Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning
**Authors**: Wenjin Liu, Haoran Luo, Xueyuan Lin, Haoming Liu, Tiesunlong Shen, Jiapu Wang, Rui Mao, Erik Cambria

**Updated**: 2025-11-28T03:35:49Z

**Summary**: Recently, advanced large language models (LLMs) have emerged at an increasingly rapid pace. However, when faced with complex problems, most users are often unable to provide accurate and effective prompts to interact with LLMs, thus limiting the performance of LLMs. To address this challenge, we propose Prompt-R1, an end-to-end reinforcement learning framework that uses a small-scale LLM to collaborate with large-scale LLMs, replacing user interaction to solve problems better. This collaboration is cast as a multi-turn prompt interaction, where the small-scale LLM thinks and generates prompts, and the large-scale LLM performs complex reasoning. A dual-constrained reward is designed to optimize for correctness, generation quality, and reasoning accuracy. Prompt-R1 provides a plug-and-play framework that supports both inference and training with various large-scale LLMs. Experiments on multiple public datasets show that Prompt-R1 significantly outperforms baseline models across tasks. Our code is publicly available at https://github.com/QwenQKing/Prompt-R1.

**Link**: [arxiv](https://arxiv.org/abs/2511.01016v3),  [pdf](https://arxiv.org/pdf/2511.01016v3)

**Tags**: cs.CL 



### DOPO: A Dynamic PD-Disaggregation Architecture for Maximizing Goodput in LLM Inference Serving
**Authors**: Junhan Liao, Minxian Xu, Wanyi Zheng, Yan Wang, Kejiang Ye, Rajkumar Buyya, Chengzhong Xu

**Updated**: 2025-11-28T03:31:23Z

**Summary**: To meet strict Service-Level Objectives (SLOs),contemporary Large Language Models (LLMs) decouple the prefill and decoding stages and place them on separate GPUs to mitigate the distinct bottlenecks inherent to each phase. However, the heterogeneity of LLM workloads causes producerconsumer imbalance between the two instance types in such disaggregated architecture. To address this problem, we propose DOPD (Dynamic Optimal Prefill/Decoding), a dynamic LLM inference system that adjusts instance allocations to achieve an optimal prefill-to-decoding (P/D) ratio based on real-time load monitoring. Combined with an appropriate request-scheduling policy, DOPD effectively resolves imbalances between prefill and decoding instances and mitigates resource allocation mismatches due to mixed-length requests under high concurrency. Experimental evaluations show that, compared with vLLM and DistServe (representative aggregation-based and disaggregationbased approaches), DOPD improves overall system goodput by up to 1.5X, decreases P90 time-to-first-token (TTFT) by up to 67.5%, and decreases P90 time-per-output-token (TPOT) by up to 22.8%. Furthermore, our dynamic P/D adjustment technique performs proactive reconfiguration based on historical load, achieving over 99% SLOs attainment while using less additional resources.

**Link**: [arxiv](https://arxiv.org/abs/2511.20982v2),  [pdf](https://arxiv.org/pdf/2511.20982v2)

**Tags**: cs.DC 



### Self Iterative Label Refinement via Robust Unlabeled Learning
**Authors**: Hikaru Asano, Tadashi Kozuno, Yukino Baba

**Updated**: 2025-11-28T03:29:20Z

**Summary**: Recent advances in large language models (LLMs) have yielded impressive performance on various tasks, yet they often depend on high-quality feedback that can be costly. Self-refinement methods attempt to leverage LLMs' internal evaluation mechanisms with minimal human supervision; however, these approaches frequently suffer from inherent biases and overconfidence, especially in domains where the models lack sufficient internal knowledge, resulting in performance degradation. As an initial step toward enhancing self-refinement for broader applications, we introduce an iterative refinement pipeline that employs the Unlabeled-Unlabeled learning framework to improve LLM-generated pseudo-labels for classification tasks. By exploiting two unlabeled datasets with differing positive class ratios, our approach iteratively denoises and refines the initial pseudo-labels, thereby mitigating the adverse effects of internal biases with minimal human supervision. Evaluations on diverse datasets, including low-resource language corpora, patent classifications, and protein structure categorizations, demonstrate that our method consistently outperforms both initial LLM's classification performance and the self-refinement approaches by cutting-edge models (e.g., GPT-4o and DeepSeek-R1). Moreover, we experimentally confirm that our refined classifier facilitates effective post-training alignment for safety in LLMs and demonstrate successful self-refinement in generative tasks as well.\footnote{Our code is available at https://github.com/HikaruAsano/self-iterative-label-refinement.}

**Link**: [arxiv](https://arxiv.org/abs/2502.12565v2),  [pdf](https://arxiv.org/pdf/2502.12565v2)

**Tags**: cs.CL 



### RAG System for Supporting Japanese Litigation Procedures: Faithful Response Generation Complying with Legal Norms
**Authors**: Yuya Ishihara, Atsushi Keyaki, Hiroaki Yamada, Ryutaro Ohara, Mihoko Sumida

**Updated**: 2025-11-28T03:28:27Z

**Summary**: This study discusses the essential components that a Retrieval-Augmented Generation (RAG)-based LLM system should possess in order to support Japanese medical litigation procedures complying with legal norms. In litigation, expert commissioners, such as physicians, architects, accountants, and engineers, provide specialized knowledge to help judges clarify points of dispute. When considering the substitution of these expert roles with a RAG-based LLM system, the constraint of strict adherence to legal norms is imposed. Specifically, three requirements arise: (1) the retrieval module must retrieve appropriate external knowledge relevant to the disputed issues in accordance with the principle prohibiting the use of private knowledge, (2) the responses generated must originate from the context provided by the RAG and remain faithful to that context, and (3) the retrieval module must reference external knowledge with appropriate timestamps corresponding to the issues at hand. This paper discusses the design of a RAG-based LLM system that satisfies these requirements.

**Link**: [arxiv](https://arxiv.org/abs/2511.22858v1),  [pdf](https://arxiv.org/pdf/2511.22858v1)

**Tags**: cs.CL cs.IR 



### Two-Stage Distributionally Robust Optimization Framework for Secure Communications in Aerial-RIS Systems
**Authors**: Zhongming Feng, Qiling Gao, Zeping Sui, Yun Lin, Michail Matthaiou

**Updated**: 2025-11-28T03:20:19Z

**Summary**: This letter proposes a two-stage distributionally robust optimization (DRO) framework for secure deployment and beamforming in an aerial reconfigurable intelligent surface (A-RIS) assisted millimeter-wave system. To account for multi-timescale uncertainties arising from user mobility, imperfect channel state information (CSI), and hardware impairments, our approach decouples the long-term unmanned aerial vehicle (UAV) placement from the per-slot beamforming design. By employing the conditional value-at-risk (CVaR) as a distribution-free risk metric, a low-complexity algorithm is developed, which combines a surrogate model for efficient deployment with an alternating optimization (AO) scheme for robust real-time beamforming. Simulation results validate that the proposed DRO-CVaR framework significantly enhances the tail-end secrecy spectral efficiency and maintains a lower outage probability compared to benchmark schemes, especially under severe uncertainty conditions.

**Link**: [arxiv](https://arxiv.org/abs/2511.22855v1),  [pdf](https://arxiv.org/pdf/2511.22855v1)

**Tags**: cs.IR cs.IT 



### Structured Prompting Enables More Robust Evaluation of Language Models
**Authors**: Asad Aali, Muhammad Ahmed Mohsin, Vasiliki Bikia, Arnav Singhvi, Richard Gaus, Suhana Bedi, Hejie Cui, Miguel Fuentes, Alyssa Unell, Yifan Mai, Jordan Cahoon, Michael Pfeffer, Roxana Daneshjou, Sanmi Koyejo, Emily Alsentzer, Christopher Potts, Nigam H. Shah, Akshay S. Chaudhari

**Updated**: 2025-11-28T03:16:43Z

**Summary**: As language models (LMs) are increasingly adopted across domains, high-quality benchmarking frameworks that accurately estimate performance are essential for guiding deployment decisions. While frameworks such as Holistic Evaluation of Language Models (HELM) enable broad evaluation across tasks, they often rely on fixed prompts that fail to generalize across LMs, yielding unrepresentative performance estimates. Unless we approximate each LM's ceiling (maximum achievable via changes to the prompt), we risk underestimating performance. Declarative prompting frameworks, such as DSPy, offer a scalable alternative to manual prompt engineering by crafting structured prompts that can be optimized per task. However, such frameworks have not been systematically evaluated across established benchmarks. We present a reproducible DSPy+HELM framework that introduces structured prompting methods which elicit reasoning, enabling more accurate LM benchmarking. Using four prompting methods, we evaluate four frontier LMs across seven benchmarks (general/medical domain) against existing HELM baseline scores. We find that without structured prompting: (i) HELM underestimates LM performance (by 4% average), (ii) performance estimates vary more across benchmarks ($+$2% standard deviation), (iii) performance gaps are misrepresented (leaderboard rankings flip on 3/7 benchmarks), and (iv) introducing chain-of-thought reduces LM sensitivity to prompt design (smaller $Δ$ across prompts). To our knowledge, this is the first benchmarking study to systematically integrate structured prompting into an established evaluation framework, demonstrating how scalable performance-ceiling approximation yields more robust, decision-useful benchmarks. We open-source (i) DSPy+HELM Integration (https://github.com/stanford-crfm/helm/pull/3893) and (ii) Prompt Optimization Pipeline (https://github.com/StanfordMIMI/dspy-helm).

**Link**: [arxiv](https://arxiv.org/abs/2511.20836v2),  [pdf](https://arxiv.org/pdf/2511.20836v2)

**Tags**: cs.CL cs.AI cs.LG 



### PerfMamba: Performance Analysis and Pruning of Selective State Space Models
**Authors**: Abdullah Al Asif, Mobina Kashaniyan, Sixing Yu, Juan Pablo Muñoz, Ali Jannesari

**Updated**: 2025-11-28T03:09:15Z

**Summary**: Recent advances in sequence modeling have introduced selective SSMs as promising alternatives to Transformer architectures, offering theoretical computational efficiency and sequence processing advantages. A comprehensive understanding of selective SSMs in runtime behavior, resource utilization patterns, and scaling characteristics still remains unexplored, thus obstructing their optimal deployment and further architectural improvements. This paper presents a thorough empirical study of Mamba-1 and Mamba-2, systematically profiled for performance to assess the design principles that contribute to their efficiency in state-space modeling. A detailed analysis of computation patterns, memory access, I/O characteristics, and scaling properties was performed for sequence lengths ranging from 64 to 16384 tokens. Our findings show that the SSM component, a central part of the selective SSM architecture, demands a significant portion of computational resources compared to other components in the Mamba block. Based on these insights, we propose a pruning technique that selectively removes low-activity states within the SSM component, achieving measurable throughput and memory gains while maintaining accuracy within a moderate pruning regime. This approach results in performance improvements across varying sequence lengths, achieving a 1.14x speedup and reducing memory usage by 11.50\%. These results offer valuable guidance for designing more efficient SSM architectures that can be applied to a wide range of real-world applications.

**Link**: [arxiv](https://arxiv.org/abs/2511.22849v1),  [pdf](https://arxiv.org/pdf/2511.22849v1)

**Tags**: cs.LG 



### Domain adaptation of large language models for geotechnical applications
**Authors**: Lei Fan, Fangxue Liu, Cheng Chen

**Updated**: 2025-11-28T02:43:59Z

**Summary**: The rapid advancement of large language models (LLMs) is transforming opportunities in geotechnical engineering, where workflows rely on complex, text-rich data. While general-purpose LLMs demonstrate strong reasoning capabilities, their effectiveness in geotechnical applications is constrained by limited exposure to specialized terminology and domain logic. Thus, domain adaptation, tailoring general LLMs for geotechnical use, has become essential. This paper presents the first systematic review of LLM adaptation and application in geotechnical contexts. It critically examines four key adaptation strategies, including prompt engineering, retrieval augmented generation, domain-adaptive pretraining, and fine-tuning, and evaluates their comparative benefits, limitations, and implementation trends. This review synthesizes current applications spanning geological interpretation, subsurface characterization, design analysis, numerical modeling, risk assessment, and geotechnical education. Findings show that domain-adapted LLMs substantially improve reasoning accuracy, automation, and interpretability, yet remain limited by data scarcity, validation challenges, and explainability concerns. Future research directions are also suggested. This review establishes a critical foundation for developing geotechnically literate LLMs and guides researchers and practitioners in advancing the digital transformation of geotechnical engineering.

**Link**: [arxiv](https://arxiv.org/abs/2507.05613v3),  [pdf](https://arxiv.org/pdf/2507.05613v3)

**Tags**: cs.AI 



### Breaking the Visual Shortcuts in Multimodal Knowledge-Based Visual Question Answering
**Authors**: Dosung Lee, Sangwon Jung, Boyoung Kim, Minyoung Kim, Sungyeon Kim, Junyoung Sung, Paul Hongsuck Seo

**Updated**: 2025-11-28T02:22:06Z

**Summary**: Existing Multimodal Knowledge-Based Visual Question Answering (MKB-VQA) benchmarks suffer from "visual shortcuts", as the query image typically matches the primary subject entity of the target document. We demonstrate that models can exploit these shortcuts, achieving comparable results using visual cues alone. To address this, we introduce Relational Entity Text-Image kNowledge Augmented (RETINA) benchmark, automatically constructed using an LLM-driven pipeline, consisting of 120k training and 2k human-curated test set. RETINA contains queries referencing secondary subjects (i.e. related entities) and pairs them with images of these related entities, removing the visual shortcut. When evaluated on RETINA existing models show significantly degraded performance, confirming their reliance on the shortcut. Furthermore, we propose Multi-Image MultImodal Retriever (MIMIR), which enriches document embeddings by augmenting images of multiple related entities, effectively handling RETINA, unlike prior work that uses only a single image per document. Our experiments validate the limitations of existing benchmarks and demonstrate the effectiveness of RETINA and MIMIR. Our project is available at: Project Page.

**Link**: [arxiv](https://arxiv.org/abs/2511.22843v1),  [pdf](https://arxiv.org/pdf/2511.22843v1)

**Tags**: cs.CV 



### Benford's Curse: Tracing Digit Bias to Numerical Hallucination in LLMs
**Authors**: Jiandong Shao, Yao Lu, Jianfei Yang

**Updated**: 2025-11-28T02:05:33Z

**Summary**: Large Language Models (LLMs) exhibit impressive performance on complex reasoning tasks, yet they frequently fail on basic numerical problems, producing incorrect outputs. Inspired by Benford's Law, a statistical pattern in which lower digits occur more frequently as leading digits, we hypothesize that the skewed digit distributions in web-collected corpora may be learned by LLMs during pretraining, leading to biased numerical generation. To investigate the hypothesis, we first examine whether digits frequencies in pretraining corpus (OLMo2) follows Benford's law. We then construct an evaluation benchmark in which the ground-truth digits are uniformly distributed within each of the seven numerical reasoning tasks. Our evaluation results demonstrate that leading open-source LLMs show a consistent pattern of digit bias that resembles Benford's law. Through logit-lens tracing and neuron-level dissection, we identify that this bias arises predominantly from a small subset of highly digit-selective feed-forward network (FFN) neurons in the deeper layers. Finally, we demonstrate that pruning these neurons mitigates imbalanced overgeneration and partially corrects erroneous outputs, providing causal evidence that fine-grained pretraining digit bias can propagate into model behavior. Our findings reveal a fundamental connection between corpus-level statistics and symbolic failure modes in LLMs, offering a new lens for diagnosing and mitigating hallucinations in numerical tasks.

**Link**: [arxiv](https://arxiv.org/abs/2506.01734v2),  [pdf](https://arxiv.org/pdf/2506.01734v2)

**Tags**: cs.CL 



### Structured Multi-Step Reasoning for Entity Matching Using Large Language Model
**Authors**: Rohan Bopardikar, Jin Wang, Jia Zou

**Updated**: 2025-11-28T01:33:06Z

**Summary**: Entity matching is a fundamental task in data cleaning and data integration. With the rapid adoption of large language models (LLMs), recent studies have explored zero-shot and few-shot prompting to improve entity matching accuracy. However, most existing approaches rely on single-step prompting and offer limited investigation into structured reasoning strategies. In this work, we investigate how to enhance LLM-based entity matching by decomposing the matching process into multiple explicit reasoning stages. We propose a three-step framework that first identifies matched and unmatched tokens between two records, then determines the attributes most influential to the matching decision, and finally predicts whether the records refer to the same real-world entity. In addition, we explore a debate-based strategy that contrasts supporting and opposing arguments to improve decision robustness. We evaluate our approaches against multiple existing baselines on several real-world entity matching benchmark datasets. Experimental results demonstrate that structured multi-step reasoning can improve matching performance in several cases, while also highlighting remaining challenges and opportunities for further refinement of reasoning-guided LLM approaches.

**Link**: [arxiv](https://arxiv.org/abs/2511.22832v1),  [pdf](https://arxiv.org/pdf/2511.22832v1)

**Tags**: cs.DB 



