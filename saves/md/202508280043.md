# Arxiv Results
## Keyword: kv cache 
 ### VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D   Space
**Authors**: Lin Li, Zehuan Huang, Haoran Feng, Gengxiong Zhuang, Rui Chen, Chunchao Guo, Lu Sheng

**Updated**: 2025-08-26T17:59:47Z

**Summary**: 3D local editing of specified regions is crucial for game industry and robot interaction. Recent methods typically edit rendered multi-view images and then reconstruct 3D models, but they face challenges in precisely preserving unedited regions and overall coherence. Inspired by structured 3D generative models, we propose VoxHammer, a novel training-free approach that performs precise and coherent editing in 3D latent space. Given a 3D model, VoxHammer first predicts its inversion trajectory and obtains its inverted latents and key-value tokens at each timestep. Subsequently, in the denoising and editing phase, we replace the denoising features of preserved regions with the corresponding inverted latents and cached key-value tokens. By retaining these contextual features, this approach ensures consistent reconstruction of preserved areas and coherent integration of edited parts. To evaluate the consistency of preserved regions, we constructed Edit3D-Bench, a human-annotated dataset comprising hundreds of samples, each with carefully labeled 3D editing regions. Experiments demonstrate that VoxHammer significantly outperforms existing methods in terms of both 3D consistency of preserved regions and overall quality. Our method holds promise for synthesizing high-quality edited paired data, thereby laying the data foundation for in-context 3D generation. See our project page at https://huanngzh.github.io/VoxHammer-Page/.

**Link**: [arxiv](http://arxiv.org/abs/2508.19247v1),  [pdf](http://arxiv.org/pdf/2508.19247v1)

**Tags**: cs.CV 



### Enabling MoE on the Edge via Importance-Driven Expert Scheduling
**Authors**: Guoying Zhu, Meng Li, Haipeng Dai, Xuechen Liu, Weijun Wang, Keran Li, Jun xiao, Ligeng Chen, Wei Wang

**Updated**: 2025-08-26T12:32:09Z

**Summary**: The Mixture of Experts (MoE) architecture has emerged as a key technique for scaling Large Language Models by activating only a subset of experts per query. Deploying MoE on consumer-grade edge hardware, however, is constrained by limited device memory, making dynamic expert offloading essential. Unlike prior work that treats offloading purely as a scheduling problem, we leverage expert importance to guide decisions, substituting low-importance activated experts with functionally similar ones already cached in GPU memory, thereby preserving accuracy. As a result, this design reduces memory usage and data transfer, while largely eliminating PCIe overhead. In addition, we introduce a scheduling policy that maximizes the reuse ratio of GPU-cached experts, further boosting efficiency. Extensive evaluations show that our approach delivers 48% lower decoding latency with over 60% expert cache hit rate, while maintaining nearly lossless accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2508.18983v1),  [pdf](http://arxiv.org/pdf/2508.18983v1)

**Tags**: cs.AI 



### Rethinking Caching for LLM Serving Systems: Beyond Traditional   Heuristics
**Authors**: Jungwoo Kim, Minsang Kim, Jaeheon Lee, Chanwoo Moon, Heejin Kim, Taeho Hwang, Woosuk Chung, Yeseong Kim, Sungjin Lee

**Updated**: 2025-08-26T07:09:09Z

**Summary**: Serving Large Language Models (LLMs) at scale requires meeting strict Service Level Objectives (SLOs) under severe computational and memory constraints. Nevertheless, traditional caching strategies fall short: exact-matching and prefix caches neglect query semantics, while state-of-the-art semantic caches remain confined to traditional intuitions, offering little conceptual departure. Building on this, we present SISO, a semantic caching system that redefines efficiency for LLM serving. SISO introduces centroid-based caching to maximize coverage with minimal memory, locality-aware replacement to preserve high-value entries, and dynamic thresholding to balance accuracy and latency under varying workloads. Across diverse datasets, SISO delivers up to 1.71$\times$ higher hit ratios and consistently stronger SLO attainment compared to state-of-the-art systems.

**Link**: [arxiv](http://arxiv.org/abs/2508.18736v1),  [pdf](http://arxiv.org/pdf/2508.18736v1)

**Tags**: cs.DB cs.LG 



### Apple Intelligence Foundation Language Models: Tech Report 2025
**Authors**: Ethan Li, Anders Boesen Lindbo Larsen, Chen Zhang, Xiyou Zhou, Jun Qin, Dian Ang Yap, Narendran Raghavan, Xuankai Chang, Margit Bowler, Eray Yildiz, John Peebles, Hannah Gillis Coleman, Matteo Ronchi, Peter Gray, Keen You, Anthony Spalvieri-Kruse, Ruoming Pang, Reed Li, Yuli Yang, Emad Soroush, Zhiyun Lu, Crystal Xiao, Rong Situ, Jordan Huffaker, David Griffiths, Zaid Ahmed, Peng Zhang, Daniel Parilla, Asaf Liberman, Jennifer Mallalieu, Parsa Mazaheri, Qibin Chen, Manjot Bilkhu, Aonan Zhang, Eric Wang, Dave Nelson, Michael FitzMaurice, Thomas Voice, Jeremy Liu, Josh Shaffer, Shiwen Zhao, Prasanth Yadla, Farzin Rasteh, Pengsheng Guo, Arsalan Farooq, Jeremy Snow, Stephen Murphy, Tao Lei, Minsik Cho, George Horrell, Sam Dodge, Lindsay Hislop, Sumeet Singh, Alex Dombrowski, Aiswarya Raghavan, Sasha Sirovica, Mandana Saebi, Faye Lao, Max Lam, TJ Lu, Zhaoyang Xu, Karanjeet Singh, Marc Kirchner, David Mizrahi, Rajat Arora, Haotian Zhang, Henry Mason, Lawrence Zhou, Yi Hua, Ankur Jain, Felix Bai, Joseph Astrauskas, Floris Weers, Josh Gardner, Mira Chiang, Yi Zhang, Pulkit Agrawal, Tony Sun, Quentin Keunebroek, Matthew Hopkins, Bugu Wu, Tao Jia, Chen Chen, Xingyu Zhou, Nanzhu Wang, Peng Liu, Ruixuan Hou, Rene Rauch, Yuan Gao, Afshin Dehghan, Jonathan Janke, Zirui Wang, Cha Chen, Xiaoyi Ren, Feng Nan, Josh Elman, Dong Yin, Yusuf Goren, Jeff Lai, Yiran Fei, Syd Evans, Muyang Yu, Guoli Yin, Yi Qin, Erin Feldman, Isha Garg, Aparna Rajamani, Karla Vega, Walker Cheng, TJ Collins, Hans Han, Raul Rea Menacho, Simon Yeung, Sophy Lee, Phani Mutyala, Ying-Chang Cheng, Zhe Gan, Sprite Chu, Justin Lazarow, Alessandro Pappalardo, Federico Scozzafava, Jing Lu, Erik Daxberger, Laurent Duchesne, Jen Liu, David Güera, Stefano Ligas, Mary Beth Kery, Brent Ramerth, Ciro Sannino, Marcin Eichner, Haoshuo Huang, Rui Qian, Moritz Schwarzer-Becker, David Riazati, Mingfei Gao, Bailin Wang, Jack Cackler, Yang Lu, Ransen Niu, John Dennison, Guillaume Klein, Jeffrey Bigham, Deepak Gopinath, Navid Shiee, Darren Botten, Guillaume Tartavel, Alex Guillen Garcia, Sam Xu, Victoria MönchJuan Haladjian, Zi-Yi Dou, Matthias Paulik, Adolfo Lopez Mendez, Zhen Li, Hong-You Chen, Chao Jia, Dhaval Doshi, Zhengdong Zhang, Raunak Manjani, Aaron Franklin, Zhile Ren, David Chen, Artsiom Peshko, Nandhitha Raghuram, Hans Hao, Jiulong Shan, Kavya Nerella, Ramsey Tantawi, Vivek Kumar, Saiwen Wang, Brycen Wershing, Bhuwan Dhingra, Dhruti Shah, Ob Adaranijo, Xin Zheng, Tait Madsen, Hadas Kotek, Chang Liu, Yin Xia, Hanli Li, Suma Jayaram, Yanchao Sun, Ahmed Fakhry, Vasileios Saveris, Dustin Withers, Yanghao Li, Alp Aygar, Andres Romero Mier Y Teran, Kaiwei Huang, Mark Lee, Xiujun Li, Yuhong Li, Tyler Johnson, Jay Tang, Joseph Yitan Cheng, Futang Peng, Andrew Walkingshaw, Lucas Guibert, Abhishek Sharma, Cheng Shen, Piotr Maj, Yasutaka Tanaka, You-Cyuan Jhang, Vivian Ma, Tommi Vehvilainen, Kelvin Zou, Jeff Nichols, Matthew Lei, David Qiu, Yihao Qian, Gokul Santhanam, Wentao Wu, Yena Han, Dominik Moritz, Haijing Fu, Mingze Xu, Vivek Rathod, Jian Liu, Louis D'hauwe, Qin Ba, Haitian Sun, Haoran Yan, Philipp Dufter, Anh Nguyen, Yihao Feng, Emma Wang, Keyu He, Rahul Nair, Sanskruti Shah, Jiarui Lu, Patrick Sonnenberg, Jeremy Warner, Yuanzhi Li, Bowen Pan, Ziyi Zhong, Joe Zhou, Sam Davarnia, Olli Saarikivi, Irina Belousova, Rachel Burger, Shang-Chen Wu, Di Feng, Bas Straathof, James Chou, Yuanyang Zhang, Marco Zuliani, Eduardo Jimenez, Abhishek Sundararajan, Xianzhi Du, Chang Lan, Nilesh Shahdadpuri, Peter Grasch, Sergiu Sima, Josh Newnham, Varsha Paidi, Jianyu Wang, Kaelen Haag, Alex Braunstein, Daniele Molinari, Richard Wei, Brenda Yang, Nicholas Lusskin, Joanna Arreaza-Taylor, Meng Cao, Nicholas Seidl, Simon Wang, Jiaming Hu, Yiping Ma, Mengyu Li, Kieran Liu, Hang Su, Sachin Ravi, Chong Wang, Xin Wang, Kevin Smith, Haoxuan You, Binazir Karimzadeh, Rui Li, Jinhao Lei, Wei Fang, Alec Doane, Sam Wiseman, Ismael Fernandez, Jane Li, Andrew Hansen, Javier Movellan, Christopher Neubauer, Hanzhi Zhou, Chris Chaney, Nazir Kamaldin, Valentin Wolf, Fernando Bermúdez-Medina, Joris Pelemans, Peter Fu, Howard Xing, Xiang Kong, Wayne Shan, Gabriel Jacoby-Cooper, Dongcai Shen, Tom Gunter, Guillaume Seguin, Fangping Shi, Shiyu Li, Yang Xu, Areeba Kamal, Dan Masi, Saptarshi Guha, Qi Zhu, Jenna Thibodeau, Changyuan Zhang, Rebecca Callahan, Charles Maalouf, Wilson Tsao, Boyue Li, Qingqing Cao, Naomy Sabo, Cheng Leong, Yi Wang, Anupama Mann Anupama, Colorado Reed, Kenneth Jung, Zhifeng Chen, Mohana Prasad Sathya Moorthy, Yifei He, Erik Hornberger, Devi Krishna, Senyu Tong, Michael, Lee, David Haldimann, Yang Zhao, Bowen Zhang, Chang Gao, Chris Bartels, Sushma Rao, Nathalie Tran, Simon Lehnerer, Co Giang, Patrick Dong, Junting Pan, Biyao Wang, Dongxu Li, Mehrdad Farajtabar, Dongseong Hwang, Grace Duanmu, Eshan Verma, Sujeeth Reddy, Qi Shan, Hongbin Gao, Nan Du, Pragnya Sridhar, Forrest Huang, Yingbo Wang, Nikhil Bhendawade, Diane Zhu, Sai Aitharaju, Fred Hohman, Lauren Gardiner, Chung-Cheng Chiu, Yinfei Yang, Alper Kokmen, Frank Chu, Ke Ye, Kaan Elgin, Oron Levy, John Park, Donald Zhang, Eldon Schoop, Nina Wenzel, Michael Booker, Hyunjik Kim, Chinguun Erdenebileg, Nan Dun, Eric Liang Yang, Priyal Chhatrapati, Vishaal Mahtani, Haiming Gang, Kohen Chia, Deepa Seshadri, Donghan Yu, Yan Meng, Kelsey Peterson, Zhen Yang, Yongqiang Wang, Carina Peng, Doug Kang, Anuva Agarwal, Albert Antony, Juan Lao Tebar, Albin Madappally Jose, Regan Poston, Andy De Wang, Gerard Casamayor, Elmira Amirloo, Violet Yao, Wojciech Kryscinski, Kun Duan, Lezhi L

**Updated**: 2025-08-26T04:02:11Z

**Summary**: We introduce two multilingual, multimodal foundation language models that power Apple Intelligence features across Apple devices and services: i a 3B-parameter on-device model optimized for Apple silicon through architectural innovations such as KV-cache sharing and 2-bit quantization-aware training; and ii a scalable server model built on a novel Parallel-Track Mixture-of-Experts PT-MoE transformer that combines track parallelism, mixture-of-experts sparse computation, and interleaved global-local attention to deliver high quality with competitive cost on Apple's Private Cloud Compute platform. Both models are trained on large-scale multilingual and multimodal datasets sourced via responsible web crawling, licensed corpora, and high-quality synthetic data, then further refined with supervised fine-tuning and reinforcement learning on a new asynchronous platform. The resulting models support several additional languages while understanding images and executing tool calls. In public benchmarks and human evaluations, both the server model and the on-device model match or surpass comparably sized open baselines.   A new Swift-centric Foundation Models framework exposes guided generation, constrained tool calling, and LoRA adapter fine-tuning, allowing developers to integrate these capabilities with a few lines of code. The latest advancements in Apple Intelligence models are grounded in our Responsible AI approach with safeguards like content filtering and locale-specific evaluation, as well as our commitment to protecting our users' privacy with innovations like Private Cloud Compute.

**Link**: [arxiv](http://arxiv.org/abs/2507.13575v2),  [pdf](http://arxiv.org/pdf/2507.13575v2)

**Tags**: cs.LG cs.AI 



### Physical Autoregressive Model for Robotic Manipulation without Action   Pretraining
**Authors**: Zijian Song, Sihan Qin, Tianshui Chen, Liang Lin, Guangrun Wang

**Updated**: 2025-08-26T03:23:53Z

**Summary**: The scarcity of manipulation data has motivated the use of pretrained large models from other modalities in robotics. In this work, we build upon autoregressive video generation models to propose a Physical Autoregressive Model (PAR), where physical tokens combine frames and actions to represent the joint evolution of the robot and its environment. PAR leverages the world knowledge embedded in video pretraining to understand physical dynamics without requiring action pretraining, enabling accurate video prediction and consistent action trajectories. It also adopts a DiT-based de-tokenizer to model frames and actions as continuous tokens, mitigating quantization errors and facilitating mutual enhancement. Furthermore, we incorporate a causal mask with inverse kinematics, parallel training, and the KV-cache mechanism to further improve performance and efficiency. Experiments on the ManiSkill benchmark show that PAR achieves a 100\% success rate on the PushCube task, matches the performance of action-pretrained baselines on other tasks, and accurately predicts future videos with tightly aligned action trajectories. These findings underscore a promising direction for robotic manipulation by transferring world knowledge from autoregressive video pretraining. The project page is here: https://hcplab-sysu.github.io/PhysicalAutoregressiveModel/

**Link**: [arxiv](http://arxiv.org/abs/2508.09822v3),  [pdf](http://arxiv.org/pdf/2508.09822v3)

**Tags**: cs.CV 



### Krul: Efficient State Restoration for Multi-turn Conversations with   Dynamic Cross-layer KV Sharing
**Authors**: Junyi Wen, Junyuan Liang, Zicong Hong, Wuhui Chen, Ting Cai, Zibin Zheng

**Updated**: 2025-08-26T01:55:27Z

**Summary**: Efficient state restoration in multi-turn conversations with large language models (LLMs) remains a critical challenge, primarily due to the overhead of recomputing or loading full key-value (KV) caches for all historical tokens. To address this, existing approaches compress KV caches across adjacent layers with highly similar attention patterns. However, these methods often apply a fixed compression scheme across all conversations, selecting the same layer pairs for compression without considering conversation-specific attention dynamics. This static strategy overlooks variability in attention pattern similarity across different conversations, which can lead to noticeable accuracy degradation.   We present Krul, a multi-turn LLM inference system that enables accurate and efficient KV cache restoration. Krul dynamically selects compression strategies based on attention similarity across layer pairs and uses a recomputation-loading pipeline to restore the KV cache. It introduces three key innovations: 1) a preemptive compression strategy selector to preserve critical context for future conversation turns and selects a customized strategy for the conversation; 2) a token-wise heterogeneous attention similarity estimator to mitigate the attention similarity computation and storage overhead during model generation; 3) a bubble-free restoration scheduler to reduce potential bubbles brought by the imbalance of recomputing and loading stream due to compressed KV caches. Empirical evaluations on real-world tasks demonstrate that Krul achieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x reduction in KV cache storage compared to state-of-the-art methods without compromising generation quality.

**Link**: [arxiv](http://arxiv.org/abs/2507.08045v2),  [pdf](http://arxiv.org/pdf/2507.08045v2)

**Tags**: cs.CL cs.AI 



### AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration
**Authors**: Zhuoping Yang, Jinming Zhuang, Xingzhen Chen, Alex K. Jones, Peipei Zhou

**Updated**: 2025-08-26T01:45:34Z

**Summary**: GPUs are critical for compute-intensive applications, yet emerging workloads such as recommender systems, graph analytics, and data analytics often exceed GPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as external memory, and the GPU-centric approach enables GPU threads to directly issue NVMe requests, further avoiding CPU intervention. However, current GPU-centric approaches adopt synchronous I/O, forcing threads to stall during long communication delays.   We propose AGILE, a lightweight asynchronous GPU-centric I/O library that eliminates deadlock risks and integrates a flexible HBM-based software cache. AGILE overlaps computation and I/O, improving performance by up to 1.88$\times$ across workloads with diverse computation-to-communication ratios. Compared to BaM on DLRM, AGILE achieves up to 1.75$\times$ speedup through efficient design and overlapping; on graph applications, AGILE reduces software cache overhead by up to 3.12$\times$ and NVMe I/O overhead by up to 2.85$\times$; AGILE also lowers per-thread register usage by up to 1.32$\times$.

**Link**: [arxiv](http://arxiv.org/abs/2504.19365v3),  [pdf](http://arxiv.org/pdf/2504.19365v3)

**Tags**: cs.DC 



### Strata: Hierarchical Context Caching for Long Context Language Model   Serving
**Authors**: Zhiqiang Xie, Ziyi Xu, Mark Zhao, Yuwei An, Vikram Sharma Mailthody, Scott Mahlke, Michael Garland, Christos Kozyrakis

**Updated**: 2025-08-26T00:09:03Z

**Summary**: Large Language Models (LLMs) with expanding context windows face significant performance hurdles. While caching key-value (KV) states is critical for avoiding redundant computation, the storage footprint of long-context caches quickly exceeds GPU memory capacity, forcing production systems to adopt hierarchical caching across memory hierarchies. However, transferring large cached contexts back to the GPU introduces severe performance bottlenecks: fragmented I/O from paged layouts prevents full bandwidth utilization, and existing schedulers fail to account for cache-loading delays, leaving systems loading-bound rather than compute-bound. We present Strata, a hierarchical context caching framework designed for efficient long context LLM serving. Strata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling GPU and CPU memory layouts and employs cache-aware request scheduling to balance compute with I/O latency and overlapping unavoidable stalls with complementary tasks. Built on SGLang and deployed in production, Strata achieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache and 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without degrading short-context performance.

**Link**: [arxiv](http://arxiv.org/abs/2508.18572v1),  [pdf](http://arxiv.org/pdf/2508.18572v1)

**Tags**: cs.DC 



### Real-time 3D Visualization of Radiance Fields on Light Field Displays
**Authors**: Jonghyun Kim, Cheng Sun, Michael Stengel, Matthew Chan, Andrew Russell, Jaehyun Jung, Wil Braithwaite, Shalini De Mello, David Luebke

**Updated**: 2025-08-25T22:21:04Z

**Summary**: Radiance fields have revolutionized photo-realistic 3D scene visualization by enabling high-fidelity reconstruction of complex environments, making them an ideal match for light field displays. However, integrating these technologies presents significant computational challenges, as light field displays require multiple high-resolution renderings from slightly shifted viewpoints, while radiance fields rely on computationally intensive volume rendering. In this paper, we propose a unified and efficient framework for real-time radiance field rendering on light field displays. Our method supports a wide range of radiance field representations, including NeRFs, 3D Gaussian Splatting, and Sparse Voxels, within a shared architecture based on a single-pass plane sweeping strategy and caching of shared, non-directional components. The framework generalizes across different scene formats without retraining, and avoids redundant computation across views. We further demonstrate a real-time interactive application on a Looking Glass display, achieving 200+ FPS at 512p across 45 views, enabling seamless, immersive 3D interaction. On standard benchmarks, our method achieves up to 22x speedup compared to independently rendering each view, while preserving image quality.

**Link**: [arxiv](http://arxiv.org/abs/2508.18540v1),  [pdf](http://arxiv.org/pdf/2508.18540v1)

**Tags**: cs.GR eess.IV 



### DiskJoin: Large-scale Vector Similarity Join with SSD
**Authors**: Yanqi Chen, Xiao Yan, Alexandra Meliou, Eric Lo

**Updated**: 2025-08-25T21:07:52Z

**Summary**: Similarity join--a widely used operation in data science--finds all pairs of items that have distance smaller than a threshold. Prior work has explored distributed computation methods to scale similarity join to large data volumes but these methods require a cluster deployment, and efficiency suffers from expensive inter-machine communication. On the other hand, disk-based solutions are more cost-effective by using a single machine and storing the large dataset on high-performance external storage, such as NVMe SSDs, but in these methods the disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin, the first disk-based similarity join algorithm that can process billion-scale vector datasets efficiently on a single machine. DiskJoin improves disk I/O by tailoring the data access patterns to avoid repetitive accesses and read amplification. It also uses main memory as a dynamic cache and carefully manages cache eviction to improve cache hit rate and reduce disk retrieval time. For further acceleration, we adopt a probabilistic pruning technique that can effectively prune a large number of vector pairs from computation. Our evaluation on real-world, large-scale datasets shows that DiskJoin significantly outperforms alternatives, achieving speedups from 50x to 1000x.

**Link**: [arxiv](http://arxiv.org/abs/2508.18494v1),  [pdf](http://arxiv.org/pdf/2508.18494v1)

**Tags**: cs.DB 



### SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study
**Authors**: Yang Xiang, Fernando García-Redondo, Arvind Sharma, Van Dai Nguyen, Andrea Fantini, Philippe Matagne, Siddharth Rao, Subhali Subhechha, Lynn Verschueren, Mohammed Aftab Baig, Marie Garcia Bardon, Geert Hellings

**Updated**: 2025-08-25T17:41:13Z

**Summary**: This work explores the cross-node scaling potential of SOT-MRAM for last-level caches (LLCs) under heterogeneous system scaling paradigm. We perform extensive Design-Technology Co-Optimization (DTCO) exercises to evaluate the bitcell footprint for different cell configurations at a representative 7 nm technology and to assess their implications on read and write power-performance. We crucially identify the MTJ routing struggle in conventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary bitcell area scaling challenge and propose to use BEOL read selectors (BEOL RSs) that enable (10 -- 40) % bitcell area reduction and eventually match sub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet the required SOT switching current, provided the magnetic free layer properties be engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This is particularly to attribute to their (i) more available Si fins for write transistor and (ii) lower bitline resistance at reduced cell width. We nevertheless underscore the read tradeoff associated with BEOL RSs, with the low-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the imperfectly rectifying diode selectors suffering (2.5 -- 5)$\times$ energy cost relative to 2T1R. This article thus highlights the realistic prospects and hurdles of BEOL RSs towards holistic power-performance-area scaling of SOT-MRAM.

**Link**: [arxiv](http://arxiv.org/abs/2508.18250v1),  [pdf](http://arxiv.org/pdf/2508.18250v1)

**Tags**: cs.ET 



### MARM: Unlocking the Future of Recommendation Systems through Memory   Augmentation and Scalable Complexity
**Authors**: Xiao Lv, Jiangxia Cao, Shijie Guan, Xiaoyou Zhou, Zhiguang Qi, Yaqiang Zang, Ming Li, Ben Wang, Kun Gai, Guorui Zhou

**Updated**: 2025-08-25T15:48:28Z

**Summary**: Scaling-law has guided the language model designing for past years, however, it is worth noting that the scaling laws of NLP cannot be directly applied to RecSys due to the following reasons: (1) The amount of training samples and model parameters is typically not the bottleneck for the model. Our recommendation system can generate over 50 billion user samples daily, and such a massive amount of training data can easily allow our model parameters to exceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the stability and robustness of the recommendation system, it is essential to control computational complexity FLOPs carefully. Considering the above differences with LLM, we can draw a conclusion that: for a RecSys model, compared to model parameters, the computational complexity FLOPs is a more expensive factor that requires careful control. In this paper, we propose our milestone work, MARM (Memory Augmented Recommendation Model), which explores a new cache scaling-laws successfully.

**Link**: [arxiv](http://arxiv.org/abs/2411.09425v3),  [pdf](http://arxiv.org/pdf/2411.09425v3)

**Tags**: cs.IR N/A 



### ILRe: Intermediate Layer Retrieval for Context Compression in Causal   Language Models
**Authors**: Manlai Liang, Mandi Liu, Jiangzhou Ji, Huaijun Li, Haobo Yang, Yaohan He, Jinlong Li

**Updated**: 2025-08-25T10:59:02Z

**Summary**: Large Language Models (LLMs) have demonstrated success across many benchmarks. However, they still exhibit limitations in long-context scenarios, primarily due to their short effective context length, quadratic computational complexity, and high memory overhead when processing lengthy inputs. To mitigate these issues, we introduce a novel context compression pipeline, called Intermediate Layer Retrieval (ILRe), which determines one intermediate decoder layer offline, encodes context by streaming chunked prefill only up to that layer, and recalls tokens by the attention scores between the input query and full key cache in that specified layer. In particular, we propose a multi-pooling kernels allocating strategy in the token recalling process to maintain the completeness of semantics. Our approach not only reduces the prefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance comparable to or better than the full context in the long context scenarios. Without additional post training or operator development, ILRe can process a single $1M$ tokens request in less than half a minute (speedup $\approx 180\times$) and scores RULER-$1M$ benchmark of $\approx 79.8$ with model Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.

**Link**: [arxiv](http://arxiv.org/abs/2508.17892v1),  [pdf](http://arxiv.org/pdf/2508.17892v1)

**Tags**: cs.CL cs.LG 



### SuperGen: An Efficient Ultra-high-resolution Video Generation System   with Sketching and Tiling
**Authors**: Fanjiang Ye, Zepeng Zhao, Yi Mu, Jucheng Shen, Renjie Li, Kaijian Wang, Desen Sun, Saurabh Agarwal, Myungjin Lee, Triston Cao, Aditya Akella, Arvind Krishnamurthy, T. S. Eugene Ng, Zhengzhong Tu, Yuke Wang

**Updated**: 2025-08-25T07:49:17Z

**Summary**: Diffusion models have recently achieved remarkable success in generative tasks (e.g., image and video generation), and the demand for high-quality content (e.g., 2K/4K videos) is rapidly increasing across various domains. However, generating ultra-high-resolution videos on existing standard-resolution (e.g., 720p) platforms remains challenging due to the excessive re-training requirements and prohibitively high computational and memory costs. To this end, we introduce SuperGen, an efficient tile-based framework for ultra-high-resolution video generation. SuperGen features a novel training-free algorithmic innovation with tiling to successfully support a wide range of resolutions without additional training efforts while significantly reducing both memory footprint and computational complexity. Moreover, SuperGen incorporates a tile-tailored, adaptive, region-aware caching strategy that accelerates video generation by exploiting redundancy across denoising steps and spatial regions. SuperGen also integrates cache-guided, communication-minimized tile parallelism for enhanced throughput and minimized latency. Evaluations demonstrate that SuperGen harvests the maximum performance gains while achieving high output quality across various benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2508.17756v1),  [pdf](http://arxiv.org/pdf/2508.17756v1)

**Tags**: cs.LG cs.SY eess.SY 



### OmniCache: A Trajectory-Oriented Global Perspective on Training-Free   Cache Reuse for Diffusion Transformer Models
**Authors**: Huanpeng Chu, Wei Wu, Guanyu Fen, Yutao Zhang

**Updated**: 2025-08-25T03:07:02Z

**Summary**: Diffusion models have emerged as a powerful paradigm for generative tasks such as image synthesis and video generation, with Transformer architectures further enhancing performance. However, the high computational cost of diffusion Transformers-stemming from a large number of sampling steps and complex per-step computations-presents significant challenges for real-time deployment. In this paper, we introduce OmniCache, a training-free acceleration method that exploits the global redundancy inherent in the denoising process. Unlike existing methods that determine caching strategies based on inter-step similarities and tend to prioritize reusing later sampling steps, our approach originates from the sampling perspective of DIT models. We systematically analyze the model's sampling trajectories and strategically distribute cache reuse across the entire sampling process. This global perspective enables more effective utilization of cached computations throughout the diffusion trajectory, rather than concentrating reuse within limited segments of the sampling procedure. In addition, during cache reuse, we dynamically estimate the corresponding noise and filter it out to reduce its impact on the sampling direction. Extensive experiments demonstrate that our approach accelerates the sampling process while maintaining competitive generative quality, offering a promising and practical solution for efficient deployment of diffusion-based generative models.

**Link**: [arxiv](http://arxiv.org/abs/2508.16212v2),  [pdf](http://arxiv.org/pdf/2508.16212v2)

**Tags**: cs.CV cs.AI cs.LG 



### ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters   at Scale
**Authors**: Ge Shi, Hanieh Sadri, Qian Wang, Yu Zhang, Ying Xiong, Yong Zhang, Zhenan Fan

**Updated**: 2025-08-25T03:05:16Z

**Summary**: Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large language models to enhance their task-specific performance by selectively tuning the top-activated experts for the task. Serving these fine-tuned models at scale is challenging: deploying merged models in isolation is prohibitively resource-hungry, while existing multi-adapter serving systems with LoRA-style additive updates are incompatible with ESFT's expert-oriented paradigm. We present ExpertWeave, a system that serves multiple ESFT adapters concurrently over a single shared MoE base model, drastically reducing the memory footprint and improving resource utilization. To seamlessly integrate into existing inference pipelines for MoE models with non-intrusive modifications and minimal latency overhead, ExpertWeave introduces a virtual-memory-assisted expert weight manager that co-locates base-model and adapter experts without incurring memory overhead from fragmentation, and a fused kernel for batched rerouting to enable lightweight redirection of tokens to the appropriate experts at runtime. Our evaluations show that ExpertWeave can simultaneously serve multiple adapters of a 16B MoE model on a single accelerator where the baseline runs out of memory, or provides up to 94x more KV cache capacity and achieves up to 18% higher throughput while using comparable resources, all without compromising model accuracy. ExpertWeave maintains low overhead even when scaling to 20 adapters, with a 4-11% latency increase compared with serving the base model alone. Source code will be released soon.

**Link**: [arxiv](http://arxiv.org/abs/2508.17624v1),  [pdf](http://arxiv.org/pdf/2508.17624v1)

**Tags**: cs.DC 



### TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated   Prefill and Decode Inference
**Authors**: Xiaojuan Tang, Fanxu Meng, Pingzhi Tang, Yuxuan Wang, Di Yin, Xing Sun, Muhan Zhang

**Updated**: 2025-08-25T02:24:20Z

**Summary**: Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses key-value states into a low-rank latent vector, caching only this vector to reduce memory. In tensor parallelism (TP), however, attention heads are computed across multiple devices, and each device must load the full cache, eroding the advantage of MLA over Grouped Query Attention (GQA). We propose Tensor-Parallel Latent Attention (TPLA): a scheme that partitions both the latent representation and each head's input dimension across devices, performs attention independently per shard, and then combines results with an all-reduce. TPLA preserves the benefits of a compressed KV cache while unlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in TPLA still leverages the full latent representation, maintaining stronger representational capacity. TPLA is drop-in compatible with models pre-trained using MLA: it supports MLA-style prefilling and enables efficient tensor-parallel decoding without retraining. Applying simple orthogonal transforms -- e.g., the Hadamard transform or PCA -- before TP slicing further mitigates cross-shard interference, yielding minimal accuracy degradation. By reducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x and 1.93x speedups, respectively, at a 32K-token context length while maintaining performance on commonsense and LongBench benchmarks. TPLA can be implemented with FlashAttention-3, enabling practical end-to-end acceleration.

**Link**: [arxiv](http://arxiv.org/abs/2508.15881v2),  [pdf](http://arxiv.org/pdf/2508.15881v2)

**Tags**: cs.LG cs.AI 



### Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD   NPUs
**Authors**: Aadesh Deshmukh, Venkata Yaswanth Raparti, Samuel Hsu

**Updated**: 2025-08-25T01:33:18Z

**Summary**: Transformer-based deep learning models are increasingly deployed on energy, and DRAM bandwidth constrained devices such as laptops and gaming consoles, which presents significant challenges in meeting the latency requirements of the models. The industry is turning to neural processing units (NPUs) for superior performance-per-watt (perf/watt); however, efficiently mapping dynamic attention layers to the NPUs remains a challenging task. For optimizing perf/watt, AMD XDNA NPUs employ software managed caches and share system memory with host. This requires substantial engineering effort to unlock efficient tiling, buffer allocation, and data movement to extract the maximum efficiency from the device. This paper introduces Zen-Attention, a framework that optimizes DRAM bandwidth utilization in the attention layer of models by systematically exploring the complex design space of layer folding, tiling, and data-movement on the interconnect, and the tensor layouts to come up with an optimal solution. Our evaluation includes comparative analysis of end-to-end model latency and specific attention latency in each model. We demonstrate how the framework enhances mapping capabilities by varying input dimensions, which require padding and masking in the attention block. For representative transformer models, the Zen-Attention Framework achieves up to 4x improvement in the latency of the attention block and up to 32% improvement in end-to-end network latency compared to the baseline Unfolded- approaches.

**Link**: [arxiv](http://arxiv.org/abs/2508.17593v1),  [pdf](http://arxiv.org/pdf/2508.17593v1)

**Tags**: cs.DC 



### RT-Cache: Training-Free Retrieval for Real-Time Manipulation
**Authors**: Owen Kwon, Abraham George, Alison Bartsch, Amir Barati Farimani

**Updated**: 2025-08-25T00:15:27Z

**Summary**: Real robots are expected to repeat the same behavior in new environments with very little new data, yet modern controllers either incur heavy per-step inference or require deployment-time fine-tuning. We propose RT-Cache, a training-free retrieval-as-control pipeline that caches diverse image action trajectories in a unified vector memory and, at test time, embeds the current frame to retrieve and replay multi-step snippets, replacing per-step model calls. A hierarchical search keeps lookups sub-second at million scale, shifting cost from compute to storage and enabling real-time control on modest GPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher success and lower completion time than strong retrieval baselines (approximately x2 higher success and ~30% faster in our settings), and a single-episode anchoring study shows immediate adaptation to a more complex, contact-rich task without fine-tuning. RT-Cache turns experience into an append-only memory, offering a simple, scalable path to few-shot deployment today and a foundation for multimodal keys and optional integration with high-level policies. Project page: https://rt-cache.github.io/.

**Link**: [arxiv](http://arxiv.org/abs/2505.09040v3),  [pdf](http://arxiv.org/pdf/2505.09040v3)

**Tags**: cs.RO cs.AI cs.CV cs.LG 



### PRISM: Efficient Long-Range Reasoning With Short-Context LLMs
**Authors**: Dulhan Jayalath, James Bradley Wendt, Nicholas Monath, Sandeep Tata, Beliz Gunel

**Updated**: 2025-08-24T22:09:57Z

**Summary**: Long-range tasks demand reasoning over long inputs. However, existing solutions are limited, e.g., long-context models require large compute budgets, parameter-efficient fine-tuning (PEFT) needs training data, and retrieval-augmented generation (RAG) entails complex task-specific designs. Though in-context approaches overcome many of these issues, methods with short-context LLMs are inefficient, trading context for processing more tokens. We introduce PRISM, a highly token-efficient in-context method based on structured schemas that outperforms baselines on diverse tasks with 4x shorter contexts. This approach produces concise outputs and efficiently leverages key-value (KV) caches to reduce costs by up to 54%. PRISM scales down to tiny contexts without increasing costs or sacrificing quality, and generalizes to new tasks with minimal effort by generating schemas from task descriptions.

**Link**: [arxiv](http://arxiv.org/abs/2412.18914v3),  [pdf](http://arxiv.org/pdf/2412.18914v3)

**Tags**: cs.AI 



### Evaluating Compiler Optimization Impacts on zkVM Performance
**Authors**: Thomas Gassmann, Stefanos Chaliasos, Thodoris Sotiropoulos, Zhendong Su

**Updated**: 2025-08-24T20:51:06Z

**Summary**: Zero-knowledge proofs (ZKPs) are the cornerstone of programmable cryptography. They enable (1) privacy-preserving and verifiable computation across blockchains, and (2) an expanding range of off-chain applications such as credential schemes. Zero-knowledge virtual machines (zkVMs) lower the barrier by turning ZKPs into a drop-in backend for standard compilation pipelines. This lets developers write proof-generating programs in conventional languages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits. However, these VMs inherit compiler infrastructures tuned for traditional architectures rather than for proof systems. In particular, standard compiler optimizations assume features that are absent in zkVMs, including cache locality, branch prediction, or instruction-level parallelism. Therefore, their impact on proof generation is questionable.   We present the first systematic study of the impact of compiler optimizations on zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an unoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero and SP1). While standard LLVM optimization levels do improve zkVM performance (over 40\%), their impact is far smaller than on traditional CPUs, since their decisions rely on hardware features rather than proof constraints. Guided by a fine-grained pass-level analysis, we~\emph{slightly} refine a small set of LLVM passes to be zkVM-aware, improving zkVM execution time by up to 45\% (average +4.6\% on RISC Zero, +1\% on SP1) and achieving consistent proving-time gains. Our work highlights the potential of compiler-level optimizations for zkVM performance and opens new direction for zkVM-specific passes, backends, and superoptimizers.

**Link**: [arxiv](http://arxiv.org/abs/2508.17518v1),  [pdf](http://arxiv.org/pdf/2508.17518v1)

**Tags**: cs.PF 



### Practical Insertion-Only Convex Hull
**Authors**: Ivor van der Hoog, Henrik Reinstädtler, Eva Rotenberg

**Updated**: 2025-08-24T19:28:22Z

**Summary**: Convex hull data structures are fundamental in computational geometry. We study insertion-only data structures, supporting various containment and intersection queries. When $P$ is sorted by $x$- or $y$-coordinate, convex hulls can be constructed in linear time using classical algorithms such as Graham scan. We investigate a variety of methods tailored to the insertion-only setting. We explore a broad selection of trade-offs involving robustness, memory access patterns, and space usage, providing an extensive evaluation of both existing and novel techniques. Logarithmic-time methods rely on pointer-based tree structures, which suffer in practice due to poor memory locality. Motivated by this, we develop a vector-based solution inspired by Overmars' logarithmic method. Our structure has worse asymptotic bounds, supporting queries in $O(\log^2 n)$ time, but stores data in $O(\log n)$ contiguous vectors, greatly improving cache performance.   Through empirical evaluation on real-world and synthetic data sets, we uncover surprising trends. Let $h$ denote the size of the convex hull. We show that a na\"ive $O(h)$ insertion-only algorithm based on Graham scan consistently outperforms both theoretical and practical state-of-the-art methods under realistic workloads, even on data sets with rather large convex hulls. While tree-based methods with $O(\log h)$ update times offer solid theoretical guarantees, they are never optimal in practice. In contrast, our vector-based logarithmic method, despite its theoretically inferior bounds, is highly competitive across all tested scenarios. It is optimal whenever the convex hull becomes large.

**Link**: [arxiv](http://arxiv.org/abs/2508.17496v1),  [pdf](http://arxiv.org/pdf/2508.17496v1)

**Tags**: cs.CG 



### TreePO: Bridging the Gap of Policy Optimization and Efficacy and   Inference Efficiency with Heuristic Tree-based Modeling
**Authors**: Yizhi Li, Qingshui Gu, Zhoufutu Wen, Ziniu Li, Tianshun Xing, Shuyue Guo, Tianyu Zheng, Xin Zhou, Xingwei Qu, Wangchunshu Zhou, Zheng Zhang, Wei Shen, Qian Liu, Chenghua Lin, Jian Yang, Ge Zhang, Wenhao Huang

**Updated**: 2025-08-24T16:52:37Z

**Summary**: Recent advancements in aligning large language models via reinforcement learning have achieved remarkable gains in solving complex reasoning problems, but at the cost of expensive on-policy rollouts and limited exploration of diverse reasoning paths. In this work, we introduce TreePO, involving a self-guided rollout algorithm that views sequence generation as a tree-structured searching process. Composed of dynamic tree sampling policy and fixed-length segment decoding, TreePO leverages local uncertainty to warrant additional branches. By amortizing computation across common prefixes and pruning low-value paths early, TreePO essentially reduces the per-update compute burden while preserving or enhancing exploration diversity. Key contributions include: (1) a segment-wise sampling algorithm that alleviates the KV cache burden through contiguous segments and spawns new branches along with an early-stop mechanism; (2) a tree-based segment-level advantage estimation that considers both global and local proximal policy optimization. and (3) analysis on the effectiveness of probability and quality-driven dynamic divergence and fallback strategy. We empirically validate the performance gain of TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours from 22\% up to 43\% of the sampling design for the trained models, meanwhile showing up to 40\% reduction at trajectory-level and 35\% at token-level sampling compute for the existing models. While offering a free lunch of inference efficiency, TreePO reveals a practical path toward scaling RL-based post-training with fewer samples and less compute. Home page locates at https://m-a-p.ai/TreePO.

**Link**: [arxiv](http://arxiv.org/abs/2508.17445v1),  [pdf](http://arxiv.org/pdf/2508.17445v1)

**Tags**: cs.LG cs.CL 



### TinySR: Pruning Diffusion for Real-World Image Super-Resolution
**Authors**: Linwei Dong, Qingnan Fan, Yuhang Yu, Qi Zhang, Jinwei Chen, Yawei Luo, Changqing Zou

**Updated**: 2025-08-24T16:17:33Z

**Summary**: Real-world image super-resolution (Real-ISR) focuses on recovering high-quality images from low-resolution inputs that suffer from complex degradations like noise, blur, and compression. Recently, diffusion models (DMs) have shown great potential in this area by leveraging strong generative priors to restore fine details. However, their iterative denoising process incurs high computational overhead, posing challenges for real-time applications. Although one-step distillation methods, such as OSEDiff and TSD-SR, offer faster inference, they remain fundamentally constrained by their large, over-parameterized model architectures. In this work, we present TinySR, a compact yet effective diffusion model specifically designed for Real-ISR that achieves real-time performance while maintaining perceptual quality. We introduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy to facilitate more effective decision-making in depth pruning. We achieve VAE compression through channel pruning, attention removal and lightweight SepConv. We eliminate time- and prompt-related modules and perform pre-caching techniques to further speed up the model. TinySR significantly reduces computational cost and model size, achieving up to 5.68x speedup and 83% parameter reduction compared to its teacher TSD-SR, while still providing high quality results.

**Link**: [arxiv](http://arxiv.org/abs/2508.17434v1),  [pdf](http://arxiv.org/pdf/2508.17434v1)

**Tags**: cs.CV 



### DiCache: Let Diffusion Model Determine Its Own Cache
**Authors**: Jiazi Bu, Pengyang Ling, Yujie Zhou, Yibin Wang, Yuhang Zang, Tong Wu, Dahua Lin, Jiaqi Wang

**Updated**: 2025-08-24T13:30:00Z

**Summary**: Recent years have witnessed the rapid development of acceleration techniques for diffusion models, especially caching-based acceleration methods. These studies seek to answer two fundamental questions: "When to cache" and "How to use cache", typically relying on predefined empirical laws or dataset-level priors to determine the timing of caching and utilizing handcrafted rules for leveraging multi-step caches. However, given the highly dynamic nature of the diffusion process, they often exhibit limited generalizability and fail on outlier samples. In this paper, a strong correlation is revealed between the variation patterns of the shallow-layer feature differences in the diffusion model and those of final model outputs. Moreover, we have observed that the features from different model layers form similar trajectories. Based on these observations, we present DiCache, a novel training-free adaptive caching strategy for accelerating diffusion models at runtime, answering both when and how to cache within a unified framework. Specifically, DiCache is composed of two principal components: (1) Online Probe Profiling Scheme leverages a shallow-layer online probe to obtain a stable prior for the caching error in real time, enabling the model to autonomously determine caching schedules. (2) Dynamic Cache Trajectory Alignment combines multi-step caches based on shallow-layer probe feature trajectory to better approximate the current feature, facilitating higher visual quality. Extensive experiments validate DiCache's capability in achieving higher efficiency and improved visual fidelity over state-of-the-art methods on various leading diffusion models including WAN 2.1, HunyuanVideo for video generation, and Flux for image generation.

**Link**: [arxiv](http://arxiv.org/abs/2508.17356v1),  [pdf](http://arxiv.org/pdf/2508.17356v1)

**Tags**: cs.CV 



### TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained   Elastic Long-Context LLM Serving
**Authors**: Bingyang Wu, Zili Zhang, Yinmin Zhong, Guanzhe Huang, Yibo Zhu, Xuanzhe Liu, Xin Jin

**Updated**: 2025-08-24T05:45:16Z

**Summary**: Prefix caching is crucial to accelerate multi-turn interactions and requests with shared prefixes. At the cluster level, existing prefix caching systems are tightly coupled with request scheduling to optimize cache efficiency and computation performance together, leading to load imbalance, data redundancy, and memory fragmentation of caching systems across instances. To address these issues, memory pooling is promising to shield the scheduler from the underlying cache management so that it can focus on the computation optimization. However, because existing prefix caching systems only transfer increasingly longer prefix caches between instances, they cannot achieve low-latency memory pooling.   To address these problems, we propose a unified segment-level prefix cache pool, TokenLake. It uses a declarative cache interface to expose requests' query tensors, prefix caches, and cache-aware operations to TokenLake for efficient pooling. Powered by this abstraction, TokenLake can manage prefix cache at the segment level with a heavy-hitter-aware load balancing algorithm to achieve better cache load balance, deduplication, and defragmentation. TokenLake also transparently minimizes the communication volume of query tensors and new caches. Based on TokenLake, the scheduler can schedule requests elastically by using existing techniques without considering prefix cache management. Evaluations on real-world workloads show that TokenLake can improve throughput by up to 2.6$\times$ and 2.0$\times$ and boost hit rate by 2.0$\times$ and 2.1$\times$, compared to state-of-the-art cache-aware routing and cache-centric PD-disaggregation solutions, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2508.17219v1),  [pdf](http://arxiv.org/pdf/2508.17219v1)

**Tags**: cs.DC cs.LG 



### DPad: Efficient Diffusion Language Models with Suffix Dropout
**Authors**: Xinhua Chen, Sitao Huang, Cong Guo, Chiyue Wei, Yintao He, Jianyi Zhang, Hai "Helen" Li, Yiran Chen

**Updated**: 2025-08-23T20:28:45Z

**Summary**: Diffusion-based Large Language Models (dLLMs) parallelize text generation by framing decoding as a denoising process, but suffer from high computational overhead since they predict all future suffix tokens at each step while retaining only a small fraction. We propose Diffusion Scratchpad (DPad), a training-free method that restricts attention to a small set of nearby suffix tokens, preserving fidelity while eliminating redundancy. DPad integrates two strategies: (i) a sliding window, which maintains a fixed-length suffix window, and (ii) distance-decay dropout, which deterministically removes distant suffix tokens before attention computation. This simple design is compatible with existing optimizations such as prefix caching and can be implemented with only a few lines of code. Comprehensive evaluations across multiple benchmarks on LLaDA-1.5 and Dream models demonstrate that DPad delivers up to $\mathbf{61.4\times}$ speedup over vanilla dLLMs while maintaining comparable accuracy, highlighting its potential for efficient and scalable long-sequence inference. Our code is available at https://github.com/Crys-Chen/DPad.

**Link**: [arxiv](http://arxiv.org/abs/2508.14148v2),  [pdf](http://arxiv.org/pdf/2508.14148v2)

**Tags**: cs.CL cs.LG 



### MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices
**Authors**: Nishant Gavhane, Arush Mehrotra, Rohit Chawla, Peter Proenca

**Updated**: 2025-08-23T20:28:32Z

**Summary**: The deployment of large-scale Mixture-of-Experts (MoE) models on edge devices presents significant challenges due to memory constraints. While MoE architectures enable efficient utilization of computational resources by activating only a subset of experts per inference, they require careful memory management to operate efficiently in resource-constrained environments. Traditional heuristic-based expert caching strategies such as MoE-Infinity struggle to maintain high cache hit rates as models parameters scale. In this work, we introduce MoE-Beyond, a learning-based expert activation predictor trained to predict expert activations during autoregressive decoding. By framing the task as a multi-label sequence prediction problem, we train a lightweight transformer model on 66 million expert activation traces extracted from LDJnr-Puffin dataset [5] using DeepSeek-V2-Chat-Lite MoE. Our predictor generalizes effectively across unseen prompts from WebGLM-QA dataset [6], achieving 97.5% accuracy and an 86.6% F1-score. Simulation results show that MoE-Beyond improves GPU cache hit rate from 17% to 72% when only 10% of experts fit in GPU cache, outperforming heuristic baselines.

**Link**: [arxiv](http://arxiv.org/abs/2508.17137v1),  [pdf](http://arxiv.org/pdf/2508.17137v1)

**Tags**: cs.LG 



### VQL: An End-to-End Context-Aware Vector Quantization Attention for   Ultra-Long User Behavior Modeling
**Authors**: Kaiyuan Li, Yongxiang Tang, Yanhua Cheng, Yong Bai, Yanxiang Zeng, Chao Wang, Xialong Liu, Peng Jiang

**Updated**: 2025-08-23T19:58:18Z

**Summary**: In large-scale recommender systems, ultra-long user behavior sequences encode rich signals of evolving interests. Extending sequence length generally improves accuracy, but directly modeling such sequences in production is infeasible due to latency and memory constraints. Existing solutions fall into two categories: (1) top-k retrieval, which truncates the sequence and may discard most attention mass when L >> k; and (2) encoder-based compression, which preserves coverage but often over-compresses and fails to incorporate key context such as temporal gaps or target-aware signals. Neither class achieves a good balance of low-loss compression, context awareness, and efficiency.   We propose VQL, a context-aware Vector Quantization Attention framework for ultra-long behavior modeling, with three innovations. (1) Key-only quantization: only attention keys are quantized, while values remain intact; we prove that softmax normalization yields an error bound independent of sequence length, and a codebook loss directly supervises quantization quality. This also enables L-free inference via offline caches. (2) Multi-scale quantization: attention heads are partitioned into groups, each with its own small codebook, which reduces quantization error while keeping cache size fixed. (3) Efficient context injection: static features (e.g., item category, modality) are directly integrated, and relative position is modeled via a separable temporal kernel. All context is injected without enlarging the codebook, so cached representations remain query-independent.   Experiments on three large-scale datasets (KuaiRand-1K, KuaiRec, TMALL) show that VQL consistently outperforms strong baselines, achieving higher accuracy while reducing inference latency, establishing a new state of the art in balancing accuracy and efficiency for ultra-long sequence recommendation.

**Link**: [arxiv](http://arxiv.org/abs/2508.17125v1),  [pdf](http://arxiv.org/pdf/2508.17125v1)

**Tags**: cs.IR 



### Learned Structure in CARTRIDGES: Keys as Shareable Routers in   Self-Studied Representations
**Authors**: Maurizio Diaz

**Updated**: 2025-08-23T14:20:06Z

**Summary**: A bottleneck for long-context LLM inference is the linearly growing KV cache. Recent work has proposed CARTRIDGES, an approach which leverages offline compute to train a much smaller KV cache than is typically required for a full document (up to 40x less memory usage at inference time). In this paper, we present the first mechanistic exploration of the learned CARTRIDGE key-value cache structure. In particular, we propose that (1) CARTRIDGE keys act as stable, shareable retrieval routers for the compressed corpora and (2) most of the learned compression occurs within the CARTRIDGE value vectors. We present empirical evidence of our routing theory across tasks, model families, and model sizes; for example, we can ablate the learned CARTRIDGE key vectors between tasks with little performance loss. Finally, we propose a slight improvement in initialization called Sampled Chunk Initialization (SCI). We suggest that SCI can lead to faster CARTRIDGE convergence than previously demonstrated in the literature. Our findings lay the groundwork for broader empirical study of CARTRIDGE training optimization which may be crucial for further scaling.

**Link**: [arxiv](http://arxiv.org/abs/2508.17032v1),  [pdf](http://arxiv.org/pdf/2508.17032v1)

**Tags**: cs.LG 



### HiCache: Training-free Acceleration of Diffusion Models via Hermite   Polynomial-based Feature Caching
**Authors**: Liang Feng, Shikang Zheng, Jiacheng Liu, Yuqi Lin, Qinming Zhou, Peiliang Cai, Xinyu Wang, Junjie Chen, Chang Zou, Yue Ma, Linfeng Zhang

**Updated**: 2025-08-23T10:35:16Z

**Summary**: Diffusion models have achieved remarkable success in content generation but suffer from prohibitive computational costs due to iterative sampling. While recent feature caching methods tend to accelerate inference through temporal extrapolation, these methods still suffer from server quality loss due to the failure in modeling the complex dynamics of feature evolution. To solve this problem, this paper presents HiCache, a training-free acceleration framework that fundamentally improves feature prediction by aligning mathematical tools with empirical properties. Our key insight is that feature derivative approximations in Diffusion Transformers exhibit multivariate Gaussian characteristics, motivating the use of Hermite polynomials-the potentially theoretically optimal basis for Gaussian-correlated processes. Besides, We further introduce a dual-scaling mechanism that ensures numerical stability while preserving predictive accuracy. Extensive experiments demonstrate HiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding baseline quality, maintaining strong performance across text-to-image, video generation, and super-resolution tasks. Core implementation is provided in the appendix, with complete code to be released upon acceptance.

**Link**: [arxiv](http://arxiv.org/abs/2508.16984v1),  [pdf](http://arxiv.org/pdf/2508.16984v1)

**Tags**: cs.CV 



### Enhancing Memory Efficiency in Large Language Model Training Through   Chronos-aware Pipeline Parallelism
**Authors**: Xinyuan Lin, Chenlu Li, Zongle Huang, Chunyu Wang, Bo Xiao, Huazhong Yang, Shishi Duan, Yongpan Liu

**Updated**: 2025-08-23T08:40:52Z

**Summary**: Larger model sizes and longer sequence lengths have empowered the Large Language Model (LLM) to achieve outstanding performance across various domains. However, this progress brings significant storage capacity challenges for LLM pretraining. High Bandwidth Memory (HBM) is expensive and requires more advanced packaging technologies for capacity expansion, creating an urgent need for memory-efficient scheduling strategies. Yet, prior pipeline parallelism schedules have primarily focused on reducing bubble overhead, often neglecting memory efficiency and lacking compatibility with other memory-efficient strategies. Consequently, these methods struggle to meet the storage demands of storage capacity for next-generation LLM. This work presents ChronosPipe, a Chronos-aware pipeline parallelism for memory-efficient LLM pretraining. The core insight of ChronosPipe is to treat HBM as a fast but small 'cache,' optimizing and exploiting temporal locality within LLM pretraining to enhance HBM utilization. ChronosPipe introduces a pipeline scheduling strategy, Chronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal locality of activations. Additionally, it leverages Chronos-Recomp and Chronos-Offload to efficiently harness the intrinsic temporal locality of activations and weights in Deep Neural Networks. Experiment results show that ChronosPipe can expand the trainable model size by 2.4x while maintaining comparable throughput, achieving 1.5x better than the 1F1B strategy combined with recomputation.

**Link**: [arxiv](http://arxiv.org/abs/2503.03182v2),  [pdf](http://arxiv.org/pdf/2503.03182v2)

**Tags**: cs.DC 



### SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long   Sequences
**Authors**: Jungyoub Cha, Hyunjong Kim, Sungzoon Cho

**Updated**: 2025-08-22T08:45:04Z

**Summary**: Speculative decoding is a widely adopted technique for accelerating inference in large language models (LLMs), but its performance degrades on long inputs due to increased attention cost and reduced draft accuracy. We introduce SpecExtend, a drop-in enhancement that improves the performance of speculative decoding on long sequences without any additional training. First, SpecExtend integrates efficient attention mechanisms such as FlashAttention and Hybrid Tree Attention into both the draft and target models. To improve draft accuracy and speed on long inputs without retraining, we propose Cross-model Retrieval, a novel KV cache eviction strategy that uses the target model's attention scores to dynamically select relevant context for the draft model. Extensive evaluations on three long-context understanding datasets show that SpecExtend accelerates standard tree-based speculative decoding by up to 2.22x for inputs up to 16K tokens, providing an effective solution for speculative decoding of long sequences. Our code is available at https://github.com/jycha98/SpecExtend .

**Link**: [arxiv](http://arxiv.org/abs/2505.20776v2),  [pdf](http://arxiv.org/pdf/2505.20776v2)

**Tags**: cs.CL cs.AI cs.LG I.2.7; C.4 



### Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion   Transformers
**Authors**: Shikang Zheng, Liang Feng, Xinyu Wang, Qinming Zhou, Peiliang Cai, Chang Zou, Jiacheng Liu, Yuqi Lin, Junjie Chen, Yue Ma, Linfeng Zhang

**Updated**: 2025-08-22T08:34:03Z

**Summary**: Diffusion Transformers (DiTs) have demonstrated exceptional performance in high-fidelity image and video generation. To reduce their substantial computational costs, feature caching techniques have been proposed to accelerate inference by reusing hidden representations from previous timesteps. However, current methods often struggle to maintain generation quality at high acceleration ratios, where prediction errors increase sharply due to the inherent instability of long-step forecasting. In this work, we adopt an ordinary differential equation (ODE) perspective on the hidden-feature sequence, modeling layer representations along the trajectory as a feature-ODE. We attribute the degradation of existing caching strategies to their inability to robustly integrate historical features under large skipping intervals. To address this, we propose FoCa (Forecast-then-Calibrate), which treats feature caching as a feature-ODE solving problem. Extensive experiments on image synthesis, video generation, and super-resolution tasks demonstrate the effectiveness of FoCa, especially under aggressive acceleration. Without additional training, FoCa achieves near-lossless speedups of 5.50 times on FLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high quality with a 4.53 times speedup on DiT.

**Link**: [arxiv](http://arxiv.org/abs/2508.16211v1),  [pdf](http://arxiv.org/pdf/2508.16211v1)

**Tags**: cs.CV 



### Joint Cache Placement and Routing in Satellite-Terrestrial Edge   Computing Network: A GNN-Enabled DRL Approach
**Authors**: Yuhao Zheng, Ting You, Kejia Peng, Chang Liu

**Updated**: 2025-08-22T07:57:28Z

**Summary**: In this letter, we investigate the problem of joint content caching and routing in satellite-terrestrial edge computing networks (STECNs) to improve caching service for geographically distributed users. To handle the challenges arising from dynamic low Earth orbit (LEO) satellite topologies and heterogeneous content demands, we propose a learning-based framework that integrates graph neural networks (GNNs) with deep reinforcement learning (DRL). The satellite network is represented as a dynamic graph, where GNNs are embedded within the DRL agent to capture spatial and topological dependencies and support routing-aware decision-making. The caching strategy is optimized by formulating the problem as a Markov decision process (MDP) and applying soft actor-critic (SAC) algorithm. Simulation results demonstrate that our approach significantly improves the delivery success rate and reduces communication traffic cost.

**Link**: [arxiv](http://arxiv.org/abs/2508.16184v1),  [pdf](http://arxiv.org/pdf/2508.16184v1)

**Tags**: cs.NI 



### CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing
**Authors**: Yixuan Wang, Haoyu Qiao, Lujun Li, Qingfu Zhu, Wanxiang Che

**Updated**: 2025-08-22T06:55:45Z

**Summary**: Large Language Models (LLMs) confront significant memory challenges due to the escalating KV cache with increasing sequence length. As a crucial technique, existing cross-layer KV cache sharing methods either necessitate modified model architectures with subsequent pre-training or incur significant performance degradation at high compression rates. To mitigate these challenges, we propose CommonKV, a training-free method for cross-layer KV cache compression through adjacent parameters sharing. Inspired by the high similarity observed in cross-layer hidden states, we utilize Singular Value Decomposition (SVD) to achieve weight sharing across adjacent parameters, resulting in a more easily mergeable latent KV cache. Furthermore, we also introduce an adaptive budget allocation strategy. It dynamically assigns compression budgets based on cosine similarity, ensuring that dissimilar caches are not over-compressed. Experiments across multiple backbone models and benchmarks including LongBench and Ruler demonstrate that the proposed method consistently outperforms existing low-rank and cross-layer approaches at various compression ratios. Moreover, we find that the benefits of CommonKV are orthogonal to other quantization and eviction methods. By integrating these approaches, we can ultimately achieve a 98\% compression ratio without significant performance loss.

**Link**: [arxiv](http://arxiv.org/abs/2508.16134v1),  [pdf](http://arxiv.org/pdf/2508.16134v1)

**Tags**: cs.LG cs.AI 



### Lightweight and Fast Real-time Image Enhancement via Decomposition of   the Spatial-aware Lookup Tables
**Authors**: Wontae Kim, Keuntek Lee, Nam Ik Cho

**Updated**: 2025-08-22T06:28:24Z

**Summary**: The image enhancement methods based on 3D lookup tables (3D LUTs) efficiently reduce both model size and runtime by interpolating pre-calculated values at the vertices. However, the 3D LUT methods have a limitation due to their lack of spatial information, as they convert color values on a point-by-point basis. Although spatial-aware 3D LUT methods address this limitation, they introduce additional modules that require a substantial number of parameters, leading to increased runtime as image resolution increases. To address this issue, we propose a method for generating image-adaptive LUTs by focusing on the redundant parts of the tables. Our efficient framework decomposes a 3D LUT into a linear sum of low-dimensional LUTs and employs singular value decomposition (SVD). Furthermore, we enhance the modules for spatial feature fusion to be more cache-efficient. Extensive experimental results demonstrate that our model effectively decreases both the number of parameters and runtime while maintaining spatial awareness and performance.

**Link**: [arxiv](http://arxiv.org/abs/2508.16121v1),  [pdf](http://arxiv.org/pdf/2508.16121v1)

**Tags**: eess.IV cs.CV 



### Dynamic Optimization of Storage Systems Using Reinforcement Learning   Techniques
**Authors**: Chiyu Cheng, Chang Zhou, Yang Zhao

**Updated**: 2025-08-22T03:36:44Z

**Summary**: The exponential growth of data-intensive applications has placed unprecedented demands on modern storage systems, necessitating dynamic and efficient optimization strategies. Traditional heuristics employed for storage performance optimization often fail to adapt to the variability and complexity of contemporary workloads, leading to significant performance bottlenecks and resource inefficiencies. To address these challenges, this paper introduces RL-Storage, a novel reinforcement learning (RL)-based framework designed to dynamically optimize storage system configurations. RL-Storage leverages deep Q-learning algorithms to continuously learn from real-time I/O patterns and predict optimal storage parameters, such as cache size, queue depths, and readahead settings[1].This work underscores the transformative potential of reinforcement learning techniques in addressing the dynamic nature of modern storage systems. By autonomously adapting to workload variations in real time, RL-Storage provides a robust and scalable solution for optimizing storage performance, paving the way for next-generation intelligent storage infrastructures.

**Link**: [arxiv](http://arxiv.org/abs/2501.00068v2),  [pdf](http://arxiv.org/pdf/2501.00068v2)

**Tags**: cs.OS cs.DC cs.LG 



### Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based   Side-Channel Attacks on Fully Associative Randomized Caches
**Authors**: Chris Cao, Gururaj Saileshwar

**Updated**: 2025-08-21T22:45:06Z

**Summary**: Recent work presented at USENIX Security 2025 (SEC'25) claims that occupancy-based attacks can recover AES keys from the MIRAGE randomized cache. In this paper, we examine these claims and find that they arise from a modeling flaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of MIRAGE uses a constant seed to initialize the random number generator used for global evictions in MIRAGE, causing every AES encryption they trace to evict the same deterministic sequence of cache lines. This artificially creates a highly repeatable timing pattern that is not representative of a realistic implementation of MIRAGE, where eviction sequences vary randomly between encryptions. When we instead randomize the eviction seed for each run, reflecting realistic operation, the correlation between AES T-table accesses and attacker runtimes disappears, and the attack fails. These findings show that the reported leakage is an artifact of incorrect modeling, and not an actual vulnerability in MIRAGE.

**Link**: [arxiv](http://arxiv.org/abs/2508.10431v2),  [pdf](http://arxiv.org/pdf/2508.10431v2)

**Tags**: cs.CR 



### Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of   Vision-Language Models
**Authors**: Xinyu Chen, Haotian Zhai, Can Zhang, Xiupeng Shi, Ruirui Li

**Updated**: 2025-08-21T20:13:40Z

**Summary**: In zero-shot setting, test-time adaptation adjusts pre-trained models using unlabeled data from the test phase to enhance performance on unknown test distributions. Existing cache-enhanced TTA methods rely on a low-entropy criterion to select samples for prototype construction, assuming intra-class compactness. However, low-entropy samples may be unreliable under distribution shifts, and the resulting prototypes may not ensure compact intra-class distributions. This study identifies a positive correlation between cache-enhanced performance and intra-class compactness. Based on this observation, we propose a Multi-Cache enhanced Prototype-based Test-Time Adaptation (MCP) featuring three caches: an entropy cache for initializing prototype representations with low-entropy samples, an align cache for integrating visual and textual information to achieve compact intra-class distributions, and a negative cache for prediction calibration using high-entropy samples. We further developed MCP++, a framework incorporating cross-modal prototype alignment and residual learning, introducing prototype residual fine-tuning. Comparative and ablation experiments across 15 downstream tasks demonstrate that the proposed method and framework achieve state-of-the-art generalization performance. Project Page available at: https://zhaihaotian.github.io/MCP-ICCV25/

**Link**: [arxiv](http://arxiv.org/abs/2508.01225v2),  [pdf](http://arxiv.org/pdf/2508.01225v2)

**Tags**: cs.CV cs.AI 



### HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO   Serving and Fast Scaling
**Authors**: Zahra Yousefijamarani, Xinglu Wang, Qian Wang, Morgan Lindsay Heisler, Taha Shabani, Niloofar Gholipour, Parham Yassini, Hong Chang, Kan Chen, Qiantao Zhang, Xiaolong Bai, Jiannan Wang, Ying Xiong, Yong Zhang, Zhenan Fan

**Updated**: 2025-08-21T18:40:20Z

**Summary**: Modern large language model (LLM) serving systems face challenges from highly variable requests with diverse lengths, priorities, and stage-specific service-level objectives (SLOs). Meeting these requires real-time scheduling, rapid and cost-effective scaling, and support for both collocated and disaggregated Prefill/Decode (P/D) architectures.   We present \textbf{HyperFlexis}, a unified LLM serving system that integrates algorithmic and system-level innovations to jointly optimize scheduling and scaling under multiple SLOs. It features a multi-SLO-aware scheduler that leverages budget estimation and request prioritization to ensure proactive SLO compliance for both new and ongoing requests. The system supports prefill- and decode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV cache transfers. It also enables cost-effective scaling decisions, prefill-decode instance linking during scaling, and rapid P/D role transitions. To accelerate scaling and reduce cold-start latency, a device-to-device (D2D) weight transfer mechanism is proposed that lowers weight loading overhead by up to \textbf{19.39$\times$}. These optimizations allow the system to achieve up to \textbf{4.44$\times$} higher SLO attainment, \textbf{65.82\%} lower request latency, and cost parity with state-of-the-art baselines. The code will be released soon.

**Link**: [arxiv](http://arxiv.org/abs/2508.15919v1),  [pdf](http://arxiv.org/pdf/2508.15919v1)

**Tags**: cs.DC cs.AI 



### StreamMem: Query-Agnostic KV Cache Memory for Streaming Video   Understanding
**Authors**: Yanlai Yang, Zhuokai Zhao, Satya Narayan Shukla, Aashu Singh, Shlok Kumar Mishra, Lizhu Zhang, Mengye Ren

**Updated**: 2025-08-21T16:56:29Z

**Summary**: Multimodal large language models (MLLMs) have made significant progress in visual-language reasoning, but their ability to efficiently handle long videos remains limited. Despite recent advances in long-context MLLMs, storing and attending to the key-value (KV) cache for long visual contexts incurs substantial memory and computational overhead. Existing visual compression methods require either encoding the entire visual context before compression or having access to the questions in advance, which is impractical for long video understanding and multi-turn conversational settings. In this work, we propose StreamMem, a query-agnostic KV cache memory mechanism for streaming video understanding. Specifically, StreamMem encodes new video frames in a streaming manner, compressing the KV cache using attention scores between visual tokens and generic query tokens, while maintaining a fixed-size KV memory to enable efficient question answering (QA) in memory-constrained, long-video scenarios. Evaluation on three long video understanding and two streaming video question answering benchmarks shows that StreamMem achieves state-of-the-art performance in query-agnostic KV cache compression and is competitive with query-aware compression approaches.

**Link**: [arxiv](http://arxiv.org/abs/2508.15717v1),  [pdf](http://arxiv.org/pdf/2508.15717v1)

**Tags**: cs.CV cs.AI 



### GoVector: An I/O-Efficient Caching Strategy for High-Dimensional Vector   Nearest Neighbor Search
**Authors**: Yijie Zhou, Shengyuan Lin, Shufeng Gong, Song Yu, Shuhao Fan, Yanfeng Zhang, Ge Yu

**Updated**: 2025-08-21T16:21:46Z

**Summary**: Graph-based high-dimensional vector indices have become a mainstream solution for large-scale approximate nearest neighbor search (ANNS). However, their substantial memory footprint often requires storage on secondary devices, where frequent on-demand loading of graph and vector data leads to I/O becoming the dominant bottleneck, accounting for over 90\% of query latency. Existing static caching strategies mitigate this issue only in the initial navigation phase by preloading entry points and multi-hop neighbors, but they fail in the second phase where query-dependent nodes must be dynamically accessed to achieve high recall. We propose GoVector, an I/O-efficient caching strategy tailored for disk-based graph indices. GoVector combines (1) a static cache that stores entry points and frequently accessed neighbors, and (2) a dynamic cache that adaptively captures nodes with high spatial locality during the second search phase. To further align storage layout with similarity-driven search patterns, GoVector reorders nodes on disk so that similar vectors are colocated on the same or adjacent pages, thereby improving locality and reducing I/O overhead. Extensive experiments on multiple public datasets show that GoVector achieves substantial performance improvements. At 90% recall, it reduces I/O operations by 46% on average, increases query throughput by 1.73x, and lowers query latency by 42% compared to state-of-the-art disk-based graph indexing systems.

**Link**: [arxiv](http://arxiv.org/abs/2508.15694v1),  [pdf](http://arxiv.org/pdf/2508.15694v1)

**Tags**: cs.DB 



### CausalMesh: A Formally Verified Causal Cache for Stateful Serverless   Computing
**Authors**: Haoran Zhang, Zihao Zhang, Shuai Mu, Sebastian Angel, Vincent Liu

**Updated**: 2025-08-21T15:25:30Z

**Summary**: Stateful serverless workflows consist of multiple serverless functions that access state on a remote database. Developers sometimes add a cache layer between the serverless runtime and the database to improve I/O latency. However, in a serverless environment, functions in the same workflow may be scheduled to different nodes with different caches, which can cause non-intuitive anomalies. This paper presents CausalMesh, a novel approach to causally consistent caching in environments where a computation may migrate from one machine to another, such as in serverless computing. CausalMesh is the first cache system that supports coordination-free and abort-free read/write operations and read transactions when clients roam among multiple servers. CausalMesh also supports read-write transactional causal consistency in the presence of client roaming, but at the cost of abort-freedom.   We have formally verified CausalMesh's protocol in Dafny, and our experimental evaluation shows that CausalMesh has lower latency and higher throughput than existing proposals

**Link**: [arxiv](http://arxiv.org/abs/2508.15647v1),  [pdf](http://arxiv.org/pdf/2508.15647v1)

**Tags**: cs.DC 



### GATEBLEED: Exploiting On-Core Accelerator Power Gating for High   Performance & Stealthy Attacks on AI
**Authors**: Joshua Kalyanapu, Farshad Dizani, Darsh Asher, Azam Ghanbari, Rosario Cammarota, Aydin Aysu, Samira Mirbagher Ajorpaz

**Updated**: 2025-08-21T14:58:12Z

**Summary**: As power consumption from AI training and inference continues to increase, AI accelerators are being integrated directly into the CPU. Intel's Advanced Matrix Extensions (AMX) is one such example, debuting on the 4th generation Intel Xeon Scalable CPU. We discover a timing side and covert channel, GATEBLEED, caused by the aggressive power gating utilized to keep the CPU within operating limits. We show that the GATEBLEED side channel is a threat to AI privacy as many ML models such as transformers and CNNs make critical computationally-heavy decisions based on private values like confidence thresholds and routing logits. Timing delays from selective powering down of AMX components mean that each matrix multiplication is a potential leakage point when executed on the AMX accelerator. Our research identifies over a dozen potential gadgets across popular ML libraries (HuggingFace, PyTorch, TensorFlow, etc.), revealing that they can leak sensitive and private information. GATEBLEED poses a risk for local and remote timing inference, even under previous protective measures. GATEBLEED can be used as a high performance, stealthy remote covert channel and a generic magnifier for timing transmission channels, capable of bypassing traditional cache defenses to leak arbitrary memory addresses and evading state of the art microarchitectural attack detectors under realistic network conditions and system configurations in which previous attacks fail. We implement an end-to-end microarchitectural inference attack on a transformer model optimized with Intel AMX, achieving a membership inference accuracy of 81% and a precision of 0.89. In a CNN-based or transformer-based mixture-of-experts model optimized with Intel AMX, we leak expert choice with 100% accuracy. To our knowledge, this is the first side-channel attack on AI privacy that exploits hardware optimizations.

**Link**: [arxiv](http://arxiv.org/abs/2507.17033v2),  [pdf](http://arxiv.org/pdf/2507.17033v2)

**Tags**: cs.CR 



### Efficient Mixed-Precision Large Language Model Inference with TurboMind
**Authors**: Li Zhang, Youhe Jiang, Guoliang He, Xin Chen, Han Lv, Qian Yao, Fangcheng Fu, Kai Chen

**Updated**: 2025-08-21T14:24:52Z

**Summary**: Mixed-precision inference techniques reduce the memory and computational demands of Large Language Models (LLMs) by applying hybrid precision formats to model weights, activations, and KV caches. This work introduces mixed-precision LLM inference techniques that encompass (i) systematic memory and compute optimization across hierarchical storage and tensor core architectures, and (ii) comprehensive end-to-end mixed-precision optimization across diverse precision formats and hardware configurations. Our approach features two novel mixed-precision pipelines designed for optimal hardware utilization: a General Matrix Multiply (GEMM) pipeline that optimizes matrix operations through offline weight packing and online acceleration, and an attention pipeline that enables efficient attention computation with arbitrary Query, Key, and Value precision combinations. The key implementation of the pipelines includes (i) hardware-aware weight packing for automatic format optimization, (ii) adaptive head alignment for efficient attention computation, (iii) instruction-level parallelism for memory hierarchy exploitation, and (iv) KV memory loading pipeline for enhanced inference efficiency. We conduct comprehensive evaluations across 16 popular LLMs and 4 representative GPU architectures. Results demonstrate that our approach achieves up to 61% lower serving latency (30% on average) and up to 156% higher throughput (58% on average) in mixed-precision workloads compared to existing mixed-precision frameworks, establishing consistent performance improvements across all tested configurations and hardware types. This work is integrated into TurboMind, a high-performance inference engine of the LMDeploy project, which is open-sourced and publicly available at https://github.com/InternLM/lmdeploy.

**Link**: [arxiv](http://arxiv.org/abs/2508.15601v1),  [pdf](http://arxiv.org/pdf/2508.15601v1)

**Tags**: cs.DC cs.PF 



### Time-Optimal Directed q-Analysis
**Authors**: Felix Windisch, Florian Unger

**Updated**: 2025-08-21T13:57:09Z

**Summary**: Directed q-analysis is a recent extension of q-analysis, an established method for extracting structure from networks, to directed graphs. Until recently, a lack of efficient algorithms heavily restricted the application of this technique: Previous approaches scale with the square of the input size, which is also the maximal size of the output, rendering such approaches worst-case optimal. In practice, output sizes of relevant networks are usually far from the worst case, a fact that could be exploited by an (efficient) output-sensitive algorithm. We develop such an algorithm and formally describe it in detail. The key insight, obtained by carefully studying various approaches to directed q-analysis and how they relate to each other, is that inverting the order of computation leads to significant complexity gains. Targeted precomputation and caching tactics further reduce the introduced overhead, enough to achieve (under mild assumptions) a time complexity that is linear in output size. The resulting algorithm for performing directed q-analysis is shown to be time-optimal.

**Link**: [arxiv](http://arxiv.org/abs/2508.15583v1),  [pdf](http://arxiv.org/pdf/2508.15583v1)

**Tags**: cs.DS 



### QVecOpt: An Efficient Storage and Computing Opti-mization Framework for   Large-scale Quantum State Simulation
**Authors**: Mingyang Yu, Haorui Yang, Donglin Wang, Desheng Kong, Ji Du, Yulong Fu, Jing Xu

**Updated**: 2025-08-21T13:24:13Z

**Summary**: In response to the challenges in large-scale quantum state simulation on classical computing platforms, including memory limits, frequent disk I/O, and high computational complexity, this study builds upon a previously proposed hierarchical storage-based quantum simulation system and introduces an optimization framework, the Quantum Vector Optimization Framework (QVecOpt). QVecOpt integrates four strategies: amplitude pairing, cache optimization, block storage optimization, and parallel optimization. These collectively enhance state vector storage and computational scheduling. The amplitude pairing mechanism locates relevant amplitude pairs via bitwise XOR, reducing traversal complexity of single-qubit gates from $O(2^n)$ to $O(1)$. Cache optimization pre-allocates buffers and loads only required data, cutting disk I/O. Block storage optimization partitions the state vector for on-demand loading and local updates, reducing redundant access. Parallel optimization distributes the state vector across nodes for collaborative computation, achieving near-linear speedup. Complexity analysis shows that, compared with hierarchical storage simulation, the method reduces state vector traversals for single-qubit gates from $2^n$ to 1, removing the main bottleneck. It also lowers computational and I/O complexity from $O(2^n)$ to $O(2^n/C)$ and $O(2^n/B)$. In simulations of 16-29 qubits, efficiency improves nearly tenfold, breaking the memory bottleneck of existing tools and enabling high-bit quantum circuit simulations beyond traditional methods. This work provides an efficient, scalable solution for classical simulation of large-scale quantum computation with significant academic and practical value.

**Link**: [arxiv](http://arxiv.org/abs/2508.15545v1),  [pdf](http://arxiv.org/pdf/2508.15545v1)

**Tags**: cs.ET 



### 3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt
**Authors**: Lukas Höllein, Aljaž Božič, Michael Zollhöfer, Matthias Nießner

**Updated**: 2025-08-21T12:52:11Z

**Summary**: We present 3DGS-LM, a new method that accelerates the reconstruction of 3D Gaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored Levenberg-Marquardt (LM). Existing methods reduce the optimization time by decreasing the number of Gaussians or by improving the implementation of the differentiable rasterizer. However, they still rely on the ADAM optimizer to fit Gaussian parameters of a scene in thousands of iterations, which can take up to an hour. To this end, we change the optimizer to LM that runs in conjunction with the 3DGS differentiable rasterizer. For efficient GPU parallization, we propose a caching data structure for intermediate gradients that allows us to efficiently calculate Jacobian-vector products in custom CUDA kernels. In every LM iteration, we calculate update directions from multiple image subsets using these kernels and combine them in a weighted mean. Overall, our method is 20% faster than the original 3DGS while obtaining the same reconstruction quality. Our optimization is also agnostic to other methods that acclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.

**Link**: [arxiv](http://arxiv.org/abs/2409.12892v2),  [pdf](http://arxiv.org/pdf/2409.12892v2)

**Tags**: cs.CV 



### TrimCaching: Parameter-sharing Edge Caching for AI Model Downloading
**Authors**: Guanqiao Qu, Zheng Lin, Qian Chen, Jian Li, Fangming Liu, Xianhao Chen, Kaibin Huang

**Updated**: 2025-08-21T11:43:48Z

**Summary**: Next-generation mobile networks are expected to facilitate fast AI model downloading to end users. By caching models on edge servers, mobile networks can deliver models to end users with low latency, resulting in a paradigm of edge model caching. In this paper, we develop a novel model placement framework, called parameter-sharing model caching (TrimCaching). TrimCaching exploits the key observation that a wide range of AI models, such as convolutional neural networks or large language models, can share a significant proportion of parameter blocks containing reusable knowledge, thereby improving storage efficiency. To this end, we formulate a parameter-sharing model placement problem to maximize the cache hit ratio in multi-edge wireless networks by balancing the fundamental tradeoff between storage efficiency and service latency. We show that the formulated problem is a submodular maximization problem with submodular constraints, for which no polynomial-time approximation algorithm exists. To tackle this challenge, we study an important special case, where a small fixed number of parameter blocks are shared across models, which often holds in practice. In such a case, a polynomial-time algorithm with a $\left(1-\epsilon\right)/2$-approximation guarantee is developed. Subsequently, we address the original problem for the general case by developing a greedy algorithm. Simulation results demonstrate that the proposed TrimCaching framework significantly improves the cache hit ratio compared with state-of-the-art content caching without exploiting shared parameters in AI models.

**Link**: [arxiv](http://arxiv.org/abs/2404.14204v3),  [pdf](http://arxiv.org/pdf/2404.14204v3)

**Tags**: cs.NI 



### Gorgeous: Revisiting the Data Layout for Disk-Resident High-Dimensional   Vector Search
**Authors**: Peiqi Yin, Xiao Yan, Qihui Zhou, Hui Li, Xiaolu Li, Lin Zhang, Meiling Wang, Xin Yao, James Cheng

**Updated**: 2025-08-21T06:26:18Z

**Summary**: Similarity-based vector search underpins many important applications, but a key challenge is processing massive vector datasets (e.g., in TBs). To reduce costs, some systems utilize SSDs as the primary data storage. They employ a proximity graph, which connects similar vectors to form a graph and is the state-of-the-art index for vector search. However, these systems are hindered by sub-optimal data layouts that fail to effectively utilize valuable memory space to reduce disk access and suffer from poor locality for accessing disk-resident data. Through extensive profiling and analysis, we found that the structure of the proximity graph index is accessed more frequently than the vectors themselves, yet existing systems do not distinguish between the two. To address this problem, we design the Gorgeous system with the principle of prioritizing graph structure over vectors. Specifically, Gorgeous features a memory cache that keeps the adjacency lists of graph nodes to improve cache hits and a disk block format that explicitly stores neighbors' adjacency lists along with a vector to enhance data locality. Experimental results show that Gorgeous consistently outperforms two state-of-the-art disk-based systems for vector search, boosting average query throughput by over 60% and reducing query latency by over 35%.

**Link**: [arxiv](http://arxiv.org/abs/2508.15290v1),  [pdf](http://arxiv.org/pdf/2508.15290v1)

**Tags**: cs.DB 



### SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache   Channel Pruning
**Authors**: Huanxuan Liao, Yixing Xu, Shizhu He, Guanchen Li, Xuanwu Yin, Dong Li, Emad Barsoum, Jun Zhao, Kang Liu

**Updated**: 2025-08-21T03:48:28Z

**Summary**: Long-context inference in large language models (LLMs) is increasingly constrained by the KV cache bottleneck: memory usage grows linearly with sequence length, while attention computation scales quadratically. Existing approaches address this issue by compressing the KV cache along the temporal axis through strategies such as token eviction or merging to reduce memory and computational overhead. However, these methods often neglect fine-grained importance variations across feature dimensions (i.e., the channel axis), thereby limiting their ability to effectively balance efficiency and model accuracy. In reality, we observe that channel saliency varies dramatically across both queries and positions: certain feature channels carry near-zero information for a given query, while others spike in relevance. To address this oversight, we propose SPARK, a training-free plug-and-play method that applies unstructured sparsity by pruning KV at the channel level, while dynamically restoring the pruned entries during attention score computation. Notably, our approach is orthogonal to existing KV compression and quantization techniques, making it compatible for integration with them to achieve further acceleration. By reducing channel-level redundancy, SPARK enables processing of longer sequences within the same memory budget. For sequences of equal length, SPARK not only preserves or improves model accuracy but also reduces KV cache storage by over 30% compared to eviction-based methods. Furthermore, even with an aggressive pruning ratio of 80%, SPARK maintains performance with less degradation than 5% compared to the baseline eviction method, demonstrating its robustness and effectiveness. Our code will be available at https://github.com/Xnhyacinth/SparK.

**Link**: [arxiv](http://arxiv.org/abs/2508.15212v1),  [pdf](http://arxiv.org/pdf/2508.15212v1)

**Tags**: cs.CL cs.AI cs.LG 



### MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in   Mixture-of-Experts LLMs
**Authors**: Ruyi Ding, Tianhong Xu, Xinyi Shen, Aidong Adam Ding, Yunsi Fei

**Updated**: 2025-08-20T20:02:35Z

**Summary**: The transformer architecture has become a cornerstone of modern AI, fueling remarkable progress across applications in natural language processing, computer vision, and multimodal learning. As these models continue to scale explosively for performance, implementation efficiency remains a critical challenge. Mixture of Experts (MoE) architectures, selectively activating specialized subnetworks (experts), offer a unique balance between model accuracy and computational cost. However, the adaptive routing in MoE architectures, where input tokens are dynamically directed to specialized experts based on their semantic meaning inadvertently opens up a new attack surface for privacy breaches. These input-dependent activation patterns leave distinctive temporal and spatial traces in hardware execution, which adversaries could exploit to deduce sensitive user data. In this work, we propose MoEcho, discovering a side channel analysis based attack surface that compromises user privacy on MoE based systems. Specifically, in MoEcho, we introduce four novel architectural side channels on different computing platforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and Performance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting these vulnerabilities, we propose four attacks that effectively breach user privacy in large language models (LLMs) and vision language models (VLMs) based on MoE architectures: Prompt Inference Attack, Response Reconstruction Attack, Visual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first runtime architecture level security analysis of the popular MoE structure common in modern transformers, highlighting a serious security and privacy threat and calling for effective and timely safeguards when harnessing MoE based models for developing efficient large scale AI services.

**Link**: [arxiv](http://arxiv.org/abs/2508.15036v1),  [pdf](http://arxiv.org/pdf/2508.15036v1)

**Tags**: cs.CR cs.AI 



### Rethinking the Potential of Layer Freezing for Efficient DNN Training
**Authors**: Chence Yang, Ci Zhang, Lei Lu, Qitao Tan, Sheng Li, Ao Li, Xulong Tang, Shaoyi Huang, Jinzhen Wang, Guoming Li, Jundong Li, Xiaoming Zhai, Jin Lu, Geng Yuan

**Updated**: 2025-08-20T19:54:41Z

**Summary**: With the growing size of deep neural networks and datasets, the computational costs of training have significantly increased. The layer-freezing technique has recently attracted great attention as a promising method to effectively reduce the cost of network training. However, in traditional layer-freezing methods, frozen layers are still required for forward propagation to generate feature maps for unfrozen layers, limiting the reduction of computation costs. To overcome this, prior works proposed a hypothetical solution, which caches feature maps from frozen layers as a new dataset, allowing later layers to train directly on stored feature maps. While this approach appears to be straightforward, it presents several major challenges that are severely overlooked by prior literature, such as how to effectively apply augmentations to feature maps and the substantial storage overhead introduced. If these overlooked challenges are not addressed, the performance of the caching method will be severely impacted and even make it infeasible. This paper is the first to comprehensively explore these challenges and provides a systematic solution. To improve training accuracy, we propose \textit{similarity-aware channel augmentation}, which caches channels with high augmentation sensitivity with a minimum additional storage cost. To mitigate storage overhead, we incorporate lossy data compression into layer freezing and design a \textit{progressive compression} strategy, which increases compression rates as more layers are frozen, effectively reducing storage costs. Finally, our solution achieves significant reductions in training cost while maintaining model accuracy, with a minor time overhead. Additionally, we conduct a comprehensive evaluation of freezing and compression strategies, providing insights into optimizing their application for efficient DNN training.

**Link**: [arxiv](http://arxiv.org/abs/2508.15033v1),  [pdf](http://arxiv.org/pdf/2508.15033v1)

**Tags**: cs.LG 



### Diverse Negative Sampling for Implicit Collaborative Filtering
**Authors**: Yueqing Xuan, Kacper Sokol, Mark Sanderson, Jeffrey Chan

**Updated**: 2025-08-20T06:48:54Z

**Summary**: Implicit collaborative filtering recommenders are usually trained to learn user positive preferences. Negative sampling, which selects informative negative items to form negative training data, plays a crucial role in this process. Since items are often clustered in the latent space, existing negative sampling strategies normally oversample negative items from the dense regions. This leads to homogeneous negative data and limited model expressiveness. In this paper, we propose Diverse Negative Sampling (DivNS), a novel approach that explicitly accounts for diversity in negative training data during the negative sampling process. DivNS first finds hard negative items with large preference scores and constructs user-specific caches that store unused but highly informative negative samples. Then, its diversity-augmented sampler selects a diverse subset of negative items from the cache while ensuring dissimilarity from the user's hard negatives. Finally, a synthetic negatives generator combines the selected diverse negatives with hard negatives to form more effective training data. The resulting synthetic negatives are both informative and diverse, enabling recommenders to learn a broader item space and improve their generalisability. Extensive experiments on four public datasets demonstrate the effectiveness of DivNS in improving recommendation quality while maintaining computational efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2508.14468v1),  [pdf](http://arxiv.org/pdf/2508.14468v1)

**Tags**: cs.IR 



### You Only Evaluate Once: A Tree-based Rerank Method at Meituan
**Authors**: Shuli Wang, Yinqiu Huang, Changhao Li, Yuan Zhou, Yonggang Liu, Yongqiang Zhang, Yinhua Zhu, Haitao Wang, Xingxing Wang

**Updated**: 2025-08-20T04:36:25Z

**Summary**: Reranking plays a crucial role in modern recommender systems by capturing the mutual influences within the list. Due to the inherent challenges of combinatorial search spaces, most methods adopt a two-stage search paradigm: a simple General Search Unit (GSU) efficiently reduces the candidate space, and an Exact Search Unit (ESU) effectively selects the optimal sequence. These methods essentially involve making trade-offs between effectiveness and efficiency, while suffering from a severe \textbf{inconsistency problem}, that is, the GSU often misses high-value lists from ESU. To address this problem, we propose YOLOR, a one-stage reranking method that removes the GSU while retaining only the ESU. Specifically, YOLOR includes: (1) a Tree-based Context Extraction Module (TCEM) that hierarchically aggregates multi-scale contextual features to achieve "list-level effectiveness", and (2) a Context Cache Module (CCM) that enables efficient feature reuse across candidate permutations to achieve "permutation-level efficiency". Extensive experiments across public and industry datasets validate YOLOR's performance, and we have successfully deployed YOLOR on the Meituan food delivery platform.

**Link**: [arxiv](http://arxiv.org/abs/2508.14420v1),  [pdf](http://arxiv.org/pdf/2508.14420v1)

**Tags**: cs.IR 



### H2EAL: Hybrid-Bonding Architecture with Hybrid Sparse Attention for   Efficient Long-Context LLM Inference
**Authors**: Zizhuo Fu, Xiaotian Guo, Wenxuan Zeng, Shuzhang Zhong, Yadong Zhang, Peiyu Chen, Runsheng Wang, Le Ye, Meng Li

**Updated**: 2025-08-20T03:42:37Z

**Summary**: Large language models (LLMs) have demonstrated remarkable proficiency in a wide range of natural language processing applications. However, the high energy and latency overhead induced by the KV cache limits the edge deployment, especially for long contexts. Emerging hybrid bonding (HB) technology has been proposed as a promising alternative to conventional near-memory processing (NMP) architectures, offering improved bandwidth efficiency and lower power consumption while exhibiting characteristics of distributed memory. In this paper, we propose H2EAL, a hybrid bonding-based accelerator with sparse attention algorithm-hardware co-design for efficient LLM inference at the edge. At the algorithm level, we propose a hybrid sparse attention scheme with static and dynamic sparsity for different heads to fully leverage the sparsity with high accuracy. At the hardware level, we co-design the hardware to support hybrid sparse attention and propose memory-compute co-placement to address the distributed memory bottleneck. Since different attention heads exhibit different sparse patterns and the attention structure often mismatches the HB architecture, we further develop a load-balancing scheduler with parallel tiled attention to address workload imbalance and optimize the mapping strategy. Extensive experiments demonstrate H2EAL achieves 5.20~48.21x speedup and 6.22~73.48x energy efficiency improvement over baseline HB implementation, with a negligible average accuracy drop of 0.87% on multiple benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2508.16653v1),  [pdf](http://arxiv.org/pdf/2508.16653v1)

**Tags**: cs.PF 



### Scavenger+: Revisiting Space-Time Tradeoffs in Key-Value Separated   LSM-trees
**Authors**: Jianshun Zhang, Fang Wang, Jiaxin Ou, Yi Wang, Ming Zhao, Sheng Qiu, Junxun Huang, Baoquan Li, Peng Fang, Dan Feng

**Updated**: 2025-08-19T15:26:36Z

**Summary**: Key-Value Stores (KVS) based on log-structured merge-trees (LSM-trees) are widely used in storage systems but face significant challenges, such as high write amplification caused by compaction. KV-separated LSM-trees address write amplification but introduce significant space amplification, a critical concern in cost-sensitive scenarios. Garbage collection (GC) can reduce space amplification, but existing strategies are often inefficient and fail to account for workload characteristics. Moreover, current key-value (KV) separated LSM-trees overlook the space amplification caused by the index LSM-tree. In this paper, we systematically analyze the sources of space amplification in KV-separated LSM-trees and propose Scavenger+, which achieves a better performance-space trade-off. Scavenger+ introduces (1) an I/O-efficient garbage collection scheme to reduce I/O overhead, (2) a space-aware compaction strategy based on compensated size to mitigate index-induced space amplification, and (3) a dynamic GC scheduler that adapts to system load to make better use of CPU and storage resources. Extensive experiments demonstrate that Scavenger+ significantly improves write performance and reduces space amplification compared to state-of-the-art KV-separated LSM-trees, including BlobDB, Titan, and TerarkDB.

**Link**: [arxiv](http://arxiv.org/abs/2508.13935v1),  [pdf](http://arxiv.org/pdf/2508.13935v1)

**Tags**: cs.DB 



### Scavenger: Better Space-Time Trade-Offs for Key-Value Separated   LSM-trees
**Authors**: Jianshun Zhang, Fang Wang, Sheng Qiu, Yi Wang, Jiaxin Ou, Junxun Huang, Baoquan Li, Peng Fang, Dan Feng

**Updated**: 2025-08-19T15:08:39Z

**Summary**: Key-Value Stores (KVS) implemented with log-structured merge-tree (LSM-tree) have gained widespread acceptance in storage systems. Nonetheless, a significant challenge arises in the form of high write amplification due to the compaction process. While KV-separated LSM-trees successfully tackle this issue, they also bring about substantial space amplification problems, a concern that cannot be overlooked in cost-sensitive scenarios. Garbage collection (GC) holds significant promise for space amplification reduction, yet existing GC strategies often fall short in optimization performance, lacking thorough consideration of workload characteristics. Additionally, current KV-separated LSM-trees also ignore the adverse effect of the space amplification in the index LSM-tree. In this paper, we systematically analyze the sources of space amplification of KV-separated LSM-trees and introduce Scavenger, which achieves a better trade-off between performance and space amplification. Scavenger initially proposes an I/O-efficient garbage collection scheme to reduce I/O overhead and incorporates a space-aware compaction strategy based on compensated size to minimize the space amplification of index LSM-trees. Extensive experiments show that Scavenger significantly improves write performance and achieves lower space amplification than other KV-separated LSM-trees (including BlobDB, Titan, and TerarkDB).

**Link**: [arxiv](http://arxiv.org/abs/2508.13909v1),  [pdf](http://arxiv.org/pdf/2508.13909v1)

**Tags**: cs.DB 



### Tight Inter-Core Cache Contention Analysis for WCET Estimation on   Multicore Systems
**Authors**: Shuai Zhao, Jieyu Jiang, Shenlin Cai, Yaowei Liang, Chen Jie, Yinjie Fang, Wei Zhang, Guoquan Zhang, Yaoyao Gu, Xiang Xiao, Wei Qin, Xiangzhen Ouyang, Wanli Chang

**Updated**: 2025-08-19T14:30:41Z

**Summary**: WCET (Worst-Case Execution Time) estimation on multicore architecture is particularly challenging mainly due to the complex accesses over cache shared by multiple cores. Existing analysis identifies possible contentions between parallel tasks by leveraging the partial order of the tasks or their program regions. Unfortunately, they overestimate the number of cache misses caused by a remote block access without considering the actual cache state and the number of accesses. This paper reports a new analysis for inter-core cache contention. Based on the order of program regions in a task, we first identify memory references that could be affected if a remote access occurs in a region. Afterwards, a fine-grained contention analysis is constructed that computes the number of cache misses based on the access quantity of local and remote blocks. We demonstrate that the overall inter-core cache interference of a task can be obtained via dynamic programming. Experiments show that compared to existing methods, the proposed analysis reduces inter-core cache interference and WCET estimations by 52.31% and 8.94% on average, without significantly increasing computation overhead.

**Link**: [arxiv](http://arxiv.org/abs/2508.13863v1),  [pdf](http://arxiv.org/pdf/2508.13863v1)

**Tags**: cs.SE 



### Zobrist Hash-based Duplicate Detection in Symbolic Regression
**Authors**: Bogdan Burlacu

**Updated**: 2025-08-19T14:18:16Z

**Summary**: Symbolic regression encompasses a family of search algorithms that aim to discover the best fitting function for a set of data without requiring an a priori specification of the model structure. The most successful and commonly used technique for symbolic regression is Genetic Programming (GP), an evolutionary search method that evolves a population of mathematical expressions through the mechanism of natural selection. In this work we analyze the efficiency of the evolutionary search in GP and show that many points in the search space are re-visited and re-evaluated multiple times by the algorithm, leading to wasted computational effort. We address this issue by introducing a caching mechanism based on the Zobrist hash, a type of hashing frequently used in abstract board games for the efficient construction and subsequent update of transposition tables. We implement our caching approach using the open-source framework Operon and demonstrate its performance on a selection of real-world regression problems, where we observe up to 34\% speedups without any detrimental effects on search quality. The hashing approach represents a straightforward way to improve runtime performance while also offering some interesting possibilities for adjusting search strategy based on cached information.

**Link**: [arxiv](http://arxiv.org/abs/2508.13859v1),  [pdf](http://arxiv.org/pdf/2508.13859v1)

**Tags**: cs.NE 



### INDS: Incremental Named Data Streaming for Real-Time Point Cloud Video
**Authors**: Ruonan Chai, Yixiang Zhu, Xinjiao Li, Jiawei Li, Zili Meng, Dirk Kutscher

**Updated**: 2025-08-19T11:54:30Z

**Summary**: Real-time streaming of point cloud video, characterized by massive data volumes and high sensitivity to packet loss, remains a key challenge for immersive applications under dynamic network conditions. While connection-oriented protocols such as TCP and more modern alternatives like QUIC alleviate some transport-layer inefficiencies, including head-of-line blocking, they still retain a coarse-grained, segment-based delivery model and a centralized control loop that limit fine-grained adaptation and effective caching. We introduce INDS (Incremental Named Data Streaming), an adaptive streaming framework based on Information-Centric Networking (ICN) that rethinks delivery for hierarchical, layered media. INDS leverages the Octree structure of point cloud video and expressive content naming to support progressive, partial retrieval of enhancement layers based on consumer bandwidth and decoding capability. By combining time-windows with Group-of-Frames (GoF), INDS's naming scheme supports fine-grained in-network caching and facilitates efficient multi-user data reuse. INDS can be deployed as an overlay, remaining compatible with QUIC-based transport infrastructure as well as future Media-over-QUIC (MoQ) architectures, without requiring changes to underlying IP networks. Our prototype implementation shows up to 80% lower delay, 15-50% higher throughput, and 20-30% increased cache hit rates compared to state-of-the-art DASH-style systems. Together, these results establish INDS as a scalable, cache-friendly solution for real-time point cloud streaming under variable and lossy conditions, while its compatibility with MoQ overlays further positions it as a practical, forward-compatible architecture for emerging immersive media systems.

**Link**: [arxiv](http://arxiv.org/abs/2508.13756v1),  [pdf](http://arxiv.org/pdf/2508.13756v1)

**Tags**: cs.MM C.2.1; C.2.4; H.5.1 



### CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint   Caching and Resource-Aware Graph Partitioning
**Authors**: Xianfeng Song, Yi Zou, Zheng Shi

**Updated**: 2025-08-19T10:21:33Z

**Summary**: Graph Neural Networks (GNNs) have shown remarkable capabilities in processing graph-structured data prevalent in various real-world applications. However, the scalability of full-batch GNN training becomes severely limited by high communication overhead and load imbalance in distributed environments. In this paper, we present CaPGNN, a novel framework for efficient parallel full-batch GNN training on single-server with multi-GPU, designed specifically to reduce redundant inter-GPU communication and balance computational workloads. We propose a joint adaptive caching algorithm that leverages both CPU and GPU memory to significantly reduce the repetitive transmission of vertex features across partitions. Additionally, we introduce a resource-aware graph partitioning algorithm that adjusts subgraph sizes dynamically according to the heterogeneous computational and communication capacities of GPUs. Extensive experiments on large-scale benchmark datasets demonstrate that CaPGNN effectively reduces communication costs by up to 96% and accelerates GNN training by up to 12.7 times compared to state-of-the-art approaches. Our results highlight the potential of adaptive caching and resource-aware partitioning to facilitate scalable, efficient, and practical deployment of full-batch GNN training in distributed computing environments.

**Link**: [arxiv](http://arxiv.org/abs/2508.13716v1),  [pdf](http://arxiv.org/pdf/2508.13716v1)

**Tags**: cs.DC 



### SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units   with Precision Recovery
**Authors**: Weicheng Xue, Baisong Xu, Kai Yang, Yongxiang Liu, Dengdeng Fan, Pengxiang Xu, Yonghong Tian

**Updated**: 2025-08-19T09:13:13Z

**Summary**: Low-precision matrix engines, such as FP16 cube, offer high throughput but lack support for full-precision computation. In this work, we propose SGEMM-cube, a high-performance algorithm for emulating FP32 general matrix-matrix multiplication (GEMM) using only FP16 computation units on a representative AI accelerator. The method decomposes each FP32 operand into two FP16 values and compensates for numerical errors through a tunable scaling strategy. A detailed analysis of numerical errors, including underflow conditions and precision loss, guides the selection of scaling parameters to preserve up to 22 bits of mantissa accuracy. We further investigate the effect of computation order on accuracy and demonstrate that a term-wise accumulation scheme improves numerical stability over conventional FP32 GEMM in low-exponent regimes. Finally, a cache-aware blocking strategy and double-buffered pipeline are introduced to overlap memory transfers with computation, enabling SGEMM-cube to achieve up to 77\% of the theoretical FP32-equivalent peak performance on Ascend 910A NPU lacking native FP32 support. Extensive numerical experiments confirm that our method not only recovers the accuracy of native FP32 GEMM but also exhibits superior numerical stability under certain conditions, due to its structured and error-aware computation order.

**Link**: [arxiv](http://arxiv.org/abs/2507.23387v3),  [pdf](http://arxiv.org/pdf/2507.23387v3)

**Tags**: cs.DC 



### LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale   Architectures
**Authors**: Anders Johansson, Evan Weinberg, Christian R. Trott, Megan J. McCarthy, Stan G. Moore

**Updated**: 2025-08-19T05:27:53Z

**Summary**: Since its inception in 1995, LAMMPS has grown to be a world-class molecular dynamics code, with thousands of users, over one million lines of code, and multi-scale simulation capabilities. We discuss how LAMMPS has adapted to the modern heterogeneous computing landscape by integrating the Kokkos performance portability library into the existing C++ code. We investigate performance portability of simple pairwise, many-body reactive, and machine-learned force-field interatomic potentials. We present results on GPUs across different vendors and generations, and analyze performance trends, probing FLOPS throughput, memory bandwidths, cache capabilities, and thread-atomic operation performance. Finally, we demonstrate strong scaling on all current US exascale machines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the three potentials.

**Link**: [arxiv](http://arxiv.org/abs/2508.13523v1),  [pdf](http://arxiv.org/pdf/2508.13523v1)

**Tags**: cs.DC cs.PF physics.comp-ph C.1.4; C.2.4; C.4; D.1.3; D.3.4; E.1; I.6; I.6.8; J.2 



### Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated   Diffusion Transformers
**Authors**: Wongi Jeong, Kyungryeol Lee, Hoigi Seo, Se Young Chun

**Updated**: 2025-08-19T03:13:39Z

**Summary**: Diffusion transformers have emerged as an alternative to U-net-based diffusion models for high-fidelity image and video generation, offering superior scalability. However, their heavy computation remains a major obstacle to real-world deployment. Existing acceleration methods primarily exploit the temporal dimension such as reusing cached features across diffusion timesteps. Here, we propose Region-Adaptive Latent Upsampling (RALU), a training-free framework that accelerates inference along spatial dimension. RALU performs mixed-resolution sampling across three stages: 1) low-resolution denoising latent diffusion to efficiently capture global semantic structure, 2) region-adaptive upsampling on specific regions prone to artifacts at full-resolution, and 3) all latent upsampling at full-resolution for detail refinement. To stabilize generations across resolution transitions, we leverage noise-timestep rescheduling to adapt the noise level across varying resolutions. Our method significantly reduces computation while preserving image quality by achieving up to 7.0$\times$ speed-up on FLUX and 3.0$\times$ on Stable Diffusion 3 with minimal degradation. Furthermore, RALU is complementary to existing temporal accelerations such as caching methods, thus can be seamlessly integrated to further reduce inference latency without compromising generation quality.

**Link**: [arxiv](http://arxiv.org/abs/2507.08422v2),  [pdf](http://arxiv.org/pdf/2507.08422v2)

**Tags**: cs.CV eess.IV 



### Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A   first-principles DFT+$U$+$V$ study
**Authors**: Indukuru Ramesh Reddy, Sayandeep Ghosh, Bongjae Kim, Chang-Jong Kang

**Updated**: 2025-08-19T01:38:23Z

**Summary**: Nonlocal Coulomb interactions play a crucial role in stabilizing distinct electronic phases in kagome materials. In this work, we systematically investigate the effects of on-site ($U$) and inter-site ($V$) Coulomb interactions on the electronic structure and stability of charge-density-wave (CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory (DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and stability of CDW phases, whereas $U$ suppresses these phases, highlighting a fundamental competition between local and nonlocal Coulomb interactions. By directly comparing our theoretical results with angle-resolved photoemission spectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that accurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings establish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable insights into the correlated electronic states in kagome metals and serving as a foundation for future explorations of correlation-driven phenomena in related materials.

**Link**: [arxiv](http://arxiv.org/abs/2504.17995v2),  [pdf](http://arxiv.org/pdf/2504.17995v2)

**Tags**: cond-mat.str-el 



### Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data   Analysis
**Authors**: Ayoub Ben Chaliah, Hela Dellagi

**Updated**: 2025-08-18T21:58:18Z

**Summary**: We present Datarus-R1-14B, a 14 B-parameter open-weights language model fine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and graduate-level problem solver. Datarus is trained not on isolated question-answer pairs but on full analytical trajectories including reasoning steps, code execution, error traces, self-corrections, and final conclusions, all captured in a ReAct-style notebook format spanning finance, medicine, numerical analysis, and other quantitative domains. Our training pipeline combines (i) a trajectory-centric synthetic data generator that yielded 144 000 tagged notebook episodes, (ii) a dual-reward framework blending a lightweight tag-based structural signal with a Hierarchical Reward Model (HRM) that scores both single-step soundness and end-to-end coherence, and (iii) a memory-optimized implementation of Group Relative Policy Optimization (GRPO) featuring KV-cache reuse, sequential generation, and reference-model sharding. A cosine curriculum smoothly shifts emphasis from structural fidelity to semantic depth, reducing the format collapse and verbosity that often plague RL-aligned LLMs. A central design choice in Datarus is it dual reasoning interface. In agentic mode the model produces ReAct-tagged steps that invoke Python tools to execute real code; in reflection mode it outputs compact Chain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On demanding postgraduate-level problems, Datarus exhibits an "AHA-moment" pattern: it sketches hypotheses, revises them once or twice, and converges avoiding the circular, token-inflating loops common to contemporary systems. Across standard public benchmarks Datarus surpasses similar size models and even reaches the level of larger reasoning models such as QwQ-32B achieving up to 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting 18-49% fewer tokens per solution.

**Link**: [arxiv](http://arxiv.org/abs/2508.13382v1),  [pdf](http://arxiv.org/pdf/2508.13382v1)

**Tags**: cs.CL cs.AI 



### AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical   Manufacturing Scale-Up
**Authors**: Sakhinana Sagar Srinivas, Shivam Gupta, Venkataramana Runkana

**Updated**: 2025-08-18T16:52:22Z

**Summary**: Recent advances in generative AI have accelerated the discovery of novel chemicals and materials. However, scaling these discoveries to industrial production remains a major bottleneck due to the synthesis gap -- the need to develop entirely new manufacturing processes. This challenge requires detailed engineering blueprints: PFDs for equipment layouts and material/energy flows, and PIDs for process plant operations. Current AI systems cannot yet reliably generate these critical engineering schematics, creating a fundamental obstacle to manufacturing scale-up of novel discoveries. We present a closed-loop, physics-aware framework for automated generation of industrially viable PFDs and PIDs. The framework integrates three key components: (1) domain-specialized small language models (SLMs) trained for auto-generation of PFDs and PIDs, (2) a hierarchical knowledge graph containing process flow and instrumentation descriptions for 1,020+ chemicals for Graph Retrieval-Augmented Generation (GRAG), and (3) an open-source chemical process simulator for modeling, simulation, optimization, and analysis of novel chemical processes. The SLMs are trained through a multi-stage pipeline on synthetic datasets, with process simulator-in-the-loop validation ensuring feasibility. To enhance computational efficiency, the framework implements structural pruning (width and depth) guided by importance heuristics to reduce language model size while preserving accuracy, followed by advanced inference optimizations including FlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization, and Test-Time Inference Scaling. Experimental results demonstrate that our framework generates simulator-validated process descriptions with high fidelity.

**Link**: [arxiv](http://arxiv.org/abs/2505.24584v3),  [pdf](http://arxiv.org/pdf/2505.24584v3)

**Tags**: cs.LG cs.AI cs.IR 



### Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning   Models
**Authors**: Ruikang Liu, Yuxuan Sun, Manyi Zhang, Haoli Bai, Xianzhi Yu, Tiezheng Yu, Chun Yuan, Lu Hou

**Updated**: 2025-08-18T16:06:09Z

**Summary**: Recent advancements in reasoning language models have demonstrated remarkable performance in complex tasks, but their extended chain-of-thought reasoning process increases inference overhead. While quantization has been widely adopted to reduce the inference cost of large language models, its impact on reasoning models remains understudied. In this paper, we conduct the first systematic study on quantized reasoning models, evaluating the open-sourced DeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B parameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache, and activation quantization using state-of-the-art algorithms at varying bit-widths, with extensive evaluation across mathematical (AIME, MATH-500), scientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our findings reveal that while lossless quantization can be achieved with W8A8 or W4A16 quantization, lower bit-widths introduce significant accuracy risks. We further identify model size, model origin, and task difficulty as critical determinants of performance. Contrary to expectations, quantized models do not exhibit increased output lengths. In addition, strategically scaling the model sizes or reasoning steps can effectively enhance the performance. All quantized models and codes are open-sourced in https://github.com/ruikangliu/Quantized-Reasoning-Models.

**Link**: [arxiv](http://arxiv.org/abs/2504.04823v2),  [pdf](http://arxiv.org/pdf/2504.04823v2)

**Tags**: cs.CL cs.AI 



### Some optimization possibilities in data plane programming
**Authors**: Altangerel Gereltsetseg, Tejfel Máté

**Updated**: 2025-08-18T09:41:28Z

**Summary**: Software-defined networking (SDN) technology aims to create a highly flexible network by decoupling control plane and the data plane and programming them independently. There has been a lot of research on improving and optimizing the control plane, and data plane programming is a relatively new concept, so study on it is one of the hot topics for researchers. At the 2019 Dagstuhl Seminar, well-known scientists on computer networking discussed challenges and problems in the field of data plane programming that need to be addressed over the next 10 years. Based on this seminar issues and papers review, we suggested some possible solutions which are for optimizing data plane to improve packet processing performance and link utilization. The suggestions include (i) enriching data plane language with asynchronous external function, (ii) compression based on payload size, (iii) in-network caching for fast packet processing, and (iv) offloading external functions to an additional thread, virtual machine (VM) or server, etc. In addition, we implemented some of these in the P4 data plane language to illustrate the practicality.

**Link**: [arxiv](http://arxiv.org/abs/2508.12767v1),  [pdf](http://arxiv.org/pdf/2508.12767v1)

**Tags**: cs.NI 



### Dissecting CPU-GPU Unified Physical Memory on AMD MI300A APUs
**Authors**: Jacob Wahlgren, Gabin Schieffer, Ruimin Shi, Edgar A. León, Roger Pearce, Maya Gokhale, Ivy Peng

**Updated**: 2025-08-18T09:06:49Z

**Summary**: Discrete GPUs are a cornerstone of HPC and data center systems, requiring management of separate CPU and GPU memory spaces. Unified Virtual Memory (UVM) has been proposed to ease the burden of memory management; however, at a high cost in performance. The recent introduction of AMD's MI300A Accelerated Processing Units (APUs)--as deployed in the El Capitan supercomputer--enables HPC systems featuring integrated CPU and GPU with Unified Physical Memory (UPM) for the first time. This work presents the first comprehensive characterization of the UPM architecture on MI300A. We first analyze the UPM system properties, including memory latency, bandwidth, and coherence overhead. We then assess the efficiency of the system software in memory allocation, page fault handling, TLB management, and Infinity Cache utilization. We propose a set of porting strategies for transforming applications for the UPM architecture and evaluate six applications on the MI300A APU. Our results show that applications on UPM using the unified memory model can match or outperform those in the explicitly managed model--while reducing memory costs by up to 44%.

**Link**: [arxiv](http://arxiv.org/abs/2508.12743v1),  [pdf](http://arxiv.org/pdf/2508.12743v1)

**Tags**: cs.DC cs.PF 



### MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration
**Authors**: Yuanxin Wei, Lansong Diao, Bujiao Chen, Shenggan Cheng, Zhengping Qian, Wenyuan Yu, Nong Xiao, Wei Lin, Jiangsu Du

**Updated**: 2025-08-18T07:49:33Z

**Summary**: Leveraging the Transformer architecture and the diffusion process, video DiT models have emerged as a dominant approach for high-quality video generation. However, their multi-step iterative denoising process incurs high computational cost and inference latency. Caching, a widely adopted optimization method in DiT models, leverages the redundancy in the diffusion process to skip computations in different granularities (e.g., step, cfg, block). Nevertheless, existing caching methods are limited to single-granularity strategies, struggling to balance generation quality and inference speed in a flexible manner. In this work, we propose MixCache, a training-free caching-based framework for efficient video DiT inference. It first distinguishes the interference and boundary between different caching strategies, and then introduces a context-aware cache triggering strategy to determine when caching should be enabled, along with an adaptive hybrid cache decision strategy for dynamically selecting the optimal caching granularity. Extensive experiments on diverse models demonstrate that, MixCache can significantly accelerate video generation (e.g., 1.94$\times$ speedup on Wan 14B, 1.97$\times$ speedup on HunyuanVideo) while delivering both superior generation quality and inference efficiency compared to baseline methods.

**Link**: [arxiv](http://arxiv.org/abs/2508.12691v1),  [pdf](http://arxiv.org/pdf/2508.12691v1)

**Tags**: cs.GR cs.CV cs.LG 



### Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for   NGINX
**Authors**: Aayush Gupta, Arpit Bhayani

**Updated**: 2025-08-17T20:01:12Z

**Summary**: Web proxies such as NGINX commonly rely on least-recently-used (LRU) eviction, which is size agnostic and can thrash under periodic bursts and mixed object sizes. We introduce Cold-RL, a learned eviction policy for NGINX that replaces LRU's forced-expire path with a dueling Deep Q-Network served by an ONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL samples the K least-recently-used objects, extracts six lightweight features (age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT), and requests a bitmask of victims; a hard timeout of 500 microseconds triggers immediate fallback to native LRU. Policies are trained offline by replaying NGINX access logs through a cache simulator with a simple reward: a retained object earns one point if it is hit again before TTL expiry. We compare against LRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial workloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538, a 146 percent improvement over the best classical baseline; at 100 MB, from 0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods (about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th percentile eviction latency within budget. To our knowledge, this is the first reinforcement learning eviction policy integrated into NGINX with strict SLOs.

**Link**: [arxiv](http://arxiv.org/abs/2508.12485v1),  [pdf](http://arxiv.org/pdf/2508.12485v1)

**Tags**: cs.LG cs.AI cs.DB cs.NI C.2.4; C.4; D.4.2; I.2.6 



### Accelerating LLM Inference via Dynamic KV Cache Placement in   Heterogeneous Memory System
**Authors**: Yunhua Fang, Rui Xie, Asad Ul Haq, Linsen Ma, Kaoutar El Maghraoui, Naigang Wang, Meng Wang, Liu Liu, Tong Zhang

**Updated**: 2025-08-17T19:07:08Z

**Summary**: Large Language Model (LLM) inference is increasingly constrained by memory bandwidth, with frequent access to the key-value (KV) cache dominating data movement. While attention sparsity reduces some memory traffic, the relevance of past tokens varies over time, requiring the full KV cache to remain accessible and sustaining pressure on both bandwidth and capacity. With advances in interconnects such as NVLink and LPDDR5X, modern AI hardware now integrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making heterogeneous memory systems a practical solution. This work investigates dynamic KV cache placement across such systems to maximize aggregated bandwidth utilization under capacity constraints. Rather than proposing a specific scheduling policy, we formulate the placement problem mathematically and derive a theoretical upper bound, revealing substantial headroom for runtime optimization. To our knowledge, this is the first formal treatment of dynamic KV cache scheduling in heterogeneous memory systems for LLM inference.

**Link**: [arxiv](http://arxiv.org/abs/2508.13231v1),  [pdf](http://arxiv.org/pdf/2508.13231v1)

**Tags**: cs.AR cs.AI cs.PF 



### ZigzagAttention: Efficient Long-Context Inference with Exclusive   Retrieval and Streaming Heads
**Authors**: Zhuorui Liu, Chen Zhang, Dawei Song

**Updated**: 2025-08-17T15:48:50Z

**Summary**: With the rapid development of large language models (LLMs), handling long context has become one of the vital abilities in LLMs. Such long-context ability is accompanied by difficulties in deployment, especially due to the increased consumption of KV cache. There is certain work aiming to optimize the memory footprint of KV cache, inspired by the observation that attention heads can be categorized into retrieval heads that are of great significance and streaming heads that are of less significance. Typically, identifying the streaming heads and and waiving the KV cache in the streaming heads would largely reduce the overhead without hurting the performance that much. However, since employing both retrieval and streaming heads in one layer decomposes one large round of attention computation into two small ones, it may unexpectedly bring extra latency on accessing and indexing tensors. Based on this intuition, we impose an important improvement to the identification process of retrieval and streaming heads, in which we design a criterion that enforces exclusively retrieval or streaming heads gathered in one unique layer. In this way, we further eliminate the extra latency and only incur negligible performance degradation. Our method named \textsc{ZigzagAttention} is competitive among considered baselines owing to reduced latency and comparable performance.

**Link**: [arxiv](http://arxiv.org/abs/2508.12407v1),  [pdf](http://arxiv.org/pdf/2508.12407v1)

**Tags**: cs.CL 



### Enhancement of the energy storage and electrocaloric effect performances   in 0.4 BCZT 0.6 BSTSn medium entropy ceramic prepared by sol gel method
**Authors**: S. Khardazi, Z. Gargar, A. Lyubchyk, O. Zakir, D. Mezzane, M. Amjoud, A. Alimoussa, Z. Kutnjak

**Updated**: 2025-08-17T13:05:52Z

**Summary**: Based on the traditional polycrystalline ferroelectric Ba0.85Ca0.15Zr0.10Ti0.90O3, the 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6 Ba0.9Sr0.1Ti0.9Sn0.1O3 medium entropy material with good energy storage and electrocaloric effect performances is designed and synthesized by the solgel method. The structural, dielectric, energy storage and electrocaloric effect properties of the prepared sample were studied. The findings demonstrate that the 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6 Ba0.9Sr0.1Ti0.9Sn0.1O3 ceramic simultaneously has a significant recoverable energy storage density of 255.4 mJ/cm3, an efficiency of 67.9%, a large ECE temperature change of 1.36 K, and a high ECE responsivity of 0.453 K.mm/kV under a low electric field of 30 kV/cm. Moreover, excellent temperature stability of Wrec (less than 10%) was achieved in the investigated sample 0.4BCZT 0.6BSTSn.

**Link**: [arxiv](http://arxiv.org/abs/2508.12357v1),  [pdf](http://arxiv.org/pdf/2508.12357v1)

**Tags**: cond-mat.mtrl-sci 



### CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based   Token Eviction
**Authors**: Raghavv Goel, Junyoung Park, Mukul Gagrani, Dalton Jones, Matthew Morse, Harper Langston, Mingu Lee, Chris Lott

**Updated**: 2025-08-16T23:41:48Z

**Summary**: While long context support of large language models has extended their abilities, it also incurs challenges in memory and compute which becomes crucial bottlenecks in resource-restricted devices. Token eviction, a widely adopted post-training methodology designed to alleviate the bottlenecks by evicting less important tokens from the cache, typically uses attention scores as proxy metrics for token importance. However, one major limitation of attention score as a token-wise importance metrics is that it lacks the information about contribution of tokens to the attention output. In this paper, we propose a simple eviction criterion based on the contribution of cached tokens to attention outputs. Our method, CAOTE, optimizes for eviction error due to token eviction, by seamlessly integrating attention scores and value vectors. This is the first method which uses value tokens on top of attention-based eviction scores in closed-form. Additionally, CAOTE can act as a meta-heuristic method with flexible usage with any token eviction method. We show that CAOTE, when combined with the state-of-the-art attention score-based methods, always improves accuracies on the downstream task, indicating the importance of leveraging information from values during token eviction process.

**Link**: [arxiv](http://arxiv.org/abs/2504.14051v5),  [pdf](http://arxiv.org/pdf/2504.14051v5)

**Tags**: cs.LG 



### Memory-Augmented Transformers: A Systematic Review from Neuroscience   Principles to Enhanced Model Architectures
**Authors**: Parsa Omidi, Xingshuai Huang, Axel Laborieux, Bahareh Nikpour, Tianyu Shi, Armaghan Eshaghi

**Updated**: 2025-08-16T03:17:35Z

**Summary**: Memory is fundamental to intelligence, enabling learning, reasoning, and adaptability across biological and artificial systems. While Transformer architectures excel at sequence modeling, they face critical limitations in long-range context retention, continual learning, and knowledge integration. This review presents a unified framework bridging neuroscience principles, including dynamic multi-timescale memory, selective attention, and consolidation, with engineering advances in Memory-Augmented Transformers. We organize recent progress through three taxonomic dimensions: functional objectives (context extension, reasoning, knowledge integration, adaptation), memory representations (parameter-encoded, state-based, explicit, hybrid), and integration mechanisms (attention fusion, gated control, associative retrieval). Our analysis of core memory operations (reading, writing, forgetting, and capacity management) reveals a shift from static caches toward adaptive, test-time learning systems. We identify persistent challenges in scalability and interference, alongside emerging solutions including hierarchical buffering and surprise-gated updates. This synthesis provides a roadmap toward cognitively-inspired, lifelong-learning Transformer architectures.

**Link**: [arxiv](http://arxiv.org/abs/2508.10824v2),  [pdf](http://arxiv.org/pdf/2508.10824v2)

**Tags**: cs.LG cs.CL 



### KV-Auditor: Auditing Local Differential Privacy for Correlated Key-Value   Estimation
**Authors**: Jingnan Xu, Leixia Wang, Xiaofeng Meng

**Updated**: 2025-08-15T14:17:24Z

**Summary**: To protect privacy for data-collection-based services, local differential privacy (LDP) is widely adopted due to its rigorous theoretical bound on privacy loss. However, mistakes in complex theoretical analysis or subtle implementation errors may undermine its practical guarantee. To address this, auditing is crucial to confirm that LDP protocols truly protect user data. However, existing auditing methods, though, mainly target machine learning and federated learning tasks based on centralized differentially privacy (DP), with limited attention to LDP. Moreover, the few studies on LDP auditing focus solely on simple frequency estimation task for discrete data, leaving correlated key-value data - which requires both discrete frequency estimation for keys and continuous mean estimation for values - unexplored.   To bridge this gap, we propose KV-Auditor, a framework for auditing LDP-based key-value estimation mechanisms by estimating their empirical privacy lower bounds. Rather than traditional LDP auditing methods that relies on binary output predictions, KV-Auditor estimates this lower bound by analyzing unbounded output distributions, supporting continuous data. Specifically, we classify state-of-the-art LDP key-value mechanisms into interactive and non-interactive types. For non-interactive mechanisms, we propose horizontal KV-Auditor for small domains with sufficient samples and vertical KV-Auditor for large domains with limited samples. For interactive mechanisms, we design a segmentation strategy to capture incremental privacy leakage across iterations. Finally, we perform extensive experiments to validate the effectiveness of our approach, offering insights for optimizing LDP-based key-value estimators.

**Link**: [arxiv](http://arxiv.org/abs/2508.11495v1),  [pdf](http://arxiv.org/pdf/2508.11495v1)

**Tags**: cs.CR 



### Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless   Edge-Device Networks
**Authors**: Rui Bao, Nan Xue, Yaping Sun, Zhiyong Chen

**Updated**: 2025-08-15T07:55:05Z

**Summary**: The integration of wireless communications and Large Language Models (LLMs) is poised to unlock ubiquitous intelligent services, yet deploying them in wireless edge-device collaborative environments presents a critical trade-off between inference quality and end-to-end latency. A fundamental mismatch exists between task complexity and resource allocation: offloading simple queries invites prohibitive latency, while on-device models lack the capacity for demanding computations. To address this challenge, we propose a dynamic, quality-latency aware routing framework that orchestrates inference between a lightweight model on the mobile device and a powerful model on the edge server. Our framework employs two distinct cost models: for single-turn queries, it fuses a BERT-predicted semantic score with communication and computation overheads; for multi-turn dialogues, it further quantifies context-aware costs arising from model switching and KV-cache management. While maintaining full inference quality, extensive experiments demonstrate that our framework cuts average response latency by 5-15% and reduces large model invocations by 10-20% against competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2508.11291v1),  [pdf](http://arxiv.org/pdf/2508.11291v1)

**Tags**: cs.IT cs.AI cs.LG math.IT 



### UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?
**Authors**: Mukund Choudhary, KV Aditya Srivatsa, Gaurja Aeron, Antara Raaghavi Bhattacharya, Dang Khoa Dang Dinh, Ikhlasul Akmal Hanif, Daria Kotova, Ekaterina Kochmar, Monojit Choudhury

**Updated**: 2025-08-15T06:53:28Z

**Summary**: Large language models (LLMs) have demonstrated potential in reasoning tasks, but their performance on linguistics puzzles remains consistently poor. These puzzles, often derived from Linguistics Olympiad (LO) contests, provide a minimal contamination environment to assess LLMs' linguistic reasoning abilities across low-resource languages. This work analyses LLMs' performance on 629 problems across 41 low-resource languages by labelling each with linguistically informed features to unveil weaknesses. Our analyses show that LLMs struggle with puzzles involving higher morphological complexity and perform better on puzzles involving linguistic features that are also found in English. We also show that splitting words into morphemes as a pre-processing step improves solvability, indicating a need for more informed and language-specific tokenisers. These findings thus offer insights into some challenges in linguistic reasoning and modelling of low-resource languages.

**Link**: [arxiv](http://arxiv.org/abs/2508.11260v1),  [pdf](http://arxiv.org/pdf/2508.11260v1)

**Tags**: cs.CL 



### ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal   Parallelism
**Authors**: Zedong Liu, Shenggan Cheng, Guangming Tan, Yang You, Dingwen Tao

**Updated**: 2025-08-15T04:27:30Z

**Summary**: Multimodal large language models (MLLMs) extend LLMs to handle images, videos, and audio by incorporating feature extractors and projection modules. However, these additional components -- combined with complex inference pipelines and heterogeneous workloads -- introduce significant inference overhead. Therefore, efficiently serving MLLMs remains a major challenge. Current tightly coupled serving architectures struggle to distinguish between mixed request types or adapt parallelism strategies to different inference stages, leading to increased time-to-first-token (TTFT) latency and poor resource utilization. To address this, we introduce Elastic Multimodal Parallelism (EMP), a new serving paradigm that elastically adapts to resource heterogeneity across request types and inference stages. Building upon EMP, we develop ElasticMM, an MLLM serving system that (1) separates requests into independent modality groups with dynamic resource allocation via a modality-aware load balancer; (2) decouples inference stages and enables parallelism adjustment and adaptive scaling via elastic partition scheduling; and (3) improves inference efficiency through unified multimodal prefix caching and non-blocking encoding. Experiments on diverse real-world datasets show that ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by up to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level objectives (SLOs).

**Link**: [arxiv](http://arxiv.org/abs/2507.10069v2),  [pdf](http://arxiv.org/pdf/2507.10069v2)

**Tags**: cs.DC cs.LG 



### A Survey on Diffusion Language Models
**Authors**: Tianyi Li, Mingda Chen, Bowei Guo, Zhiqiang Shen

**Updated**: 2025-08-14T17:47:22Z

**Summary**: Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs.

**Link**: [arxiv](http://arxiv.org/abs/2508.10875v1),  [pdf](http://arxiv.org/pdf/2508.10875v1)

**Tags**: cs.CL cs.AI cs.LG 



### FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference
**Authors**: Guangda Liu, Chengwei Li, Zhenyu Ning, Minyi Guo, Jieru Zhao

**Updated**: 2025-08-14T16:12:44Z

**Summary**: Large language models (LLMs) have been widely deployed with rapidly expanding context windows to support increasingly demanding applications. However, long contexts pose significant deployment challenges, primarily due to the KV cache whose size grows proportionally with context length. While KV cache compression methods are proposed to address this issue, KV dropping methods incur considerable accuracy loss, and KV retrieval methods suffer from significant efficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization framework to enhance KV retrieval efficiency while preserving accuracy. On the algorithm side, FreeKV introduces speculative retrieval to shift the KV selection and recall processes out of the critical path, combined with fine-grained correction to ensure accuracy. On the system side, FreeKV employs hybrid KV layouts across CPU and GPU memory to eliminate fragmented data transfers, and leverages double-buffered streamed recall to further improve efficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy across various scenarios and models, delivering up to 13$\times$ speedup compared to SOTA KV retrieval methods.

**Link**: [arxiv](http://arxiv.org/abs/2505.13109v2),  [pdf](http://arxiv.org/pdf/2505.13109v2)

**Tags**: cs.LG cs.AI cs.CL 



### BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit   KV Cache
**Authors**: Dayou Du, Shijie Cao, Jianyi Cheng, Luo Mai, Ting Cao, Mao Yang

**Updated**: 2025-08-14T15:37:43Z

**Summary**: The rise of long-context Large Language Models (LLMs) amplifies memory and bandwidth demands during autoregressive decoding, as the Key-Value (KV) cache grows with each generated token. Low-bit KV-cache quantization (e.g., 4-bit or 2-bit) can reduce memory footprint while preserving accuracy, but existing systems suffer from slow decoding due to their exclusive reliance on CUDA cores, neglecting Tensor Cores (the primary source of compute on modern GPUs). We present BitDecoding, a new long-context LLM inference system with a low-bit KV cache. BitDecoding enables efficient low-bit KV-cache decoding by cooperatively leveraging CUDA cores and Tensor Cores. It introduces methods for automatically inducing optimized layouts to exploit Tensor Cores, along with warp-level parallelization strategies for dequantization. For unified system support, BitDecoding includes a query transformation module supporting diverse attention variants, a quantization kernel that supports both tensor-wise and channel-wise scaling used in various quantization algorithms with high performance, and a dequantization kernel with a software-defined pipeline to coordinate CUDA and Tensor Cores execution for mixed-precision operations. Evaluated on RTX 4090, A100, and H100, BitDecoding accelerates decoding by up to 7.5x, 4.8x, and 8.9x, respectively, over FP16 FlashDecoding-v2, and surpasses the state-of-the-art low-bit system QServe by up to 4.3x. On LLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding latency by 3x, showing substantial improvements for long-context generation. The code is available at https://github.com/DD-DuDa/BitDecoding.

**Link**: [arxiv](http://arxiv.org/abs/2503.18773v2),  [pdf](http://arxiv.org/pdf/2503.18773v2)

**Tags**: cs.AR cs.AI cs.CL cs.PF 



### EVCtrl: Efficient Control Adapter for Visual Generation
**Authors**: Zixiang Yang, Yue Ma, Yinhan Zhang, Shanhui Mo, Dongrui Liu, Linfeng Zhang

**Updated**: 2025-08-14T14:11:48Z

**Summary**: Visual generation includes both image and video generation, training probabilistic models to create coherent, diverse, and semantically faithful content from scratch. While early research focused on unconditional sampling, practitioners now demand controllable generation that allows precise specification of layout, pose, motion, or style. While ControlNet grants precise spatial-temporal control, its auxiliary branch markedly increases latency and introduces redundant computation in both uncontrolled regions and denoising steps, especially for video. To address this problem, we introduce EVCtrl, a lightweight, plug-and-play control adapter that slashes overhead without retraining the model. Specifically, we propose a spatio-temporal dual caching strategy for sparse control information. For spatial redundancy, we first profile how each layer of DiT-ControlNet responds to fine-grained control, then partition the network into global and local functional zones. A locality-aware cache focuses computation on the local zones that truly need the control signal, skipping the bulk of redundant computation in global regions. For temporal redundancy, we selectively omit unnecessary denoising steps to improve efficiency. Extensive experiments on CogVideo-Controlnet, Wan2.1-Controlnet, and Flux demonstrate that our method is effective in image and video control generation without the need for training. For example, it achieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and Wan2.1-Controlnet, respectively, with almost no degradation in generation quality.Codes are available in the supplementary materials.

**Link**: [arxiv](http://arxiv.org/abs/2508.10963v1),  [pdf](http://arxiv.org/pdf/2508.10963v1)

**Tags**: cs.CV 



### SurfaceLogicKV: Surface and Logic Attention Behaviors are All You Need   for Robust KV Cache Compression
**Authors**: Mengjie Li, William J. Song

**Updated**: 2025-08-14T14:08:58Z

**Summary**: The increasing input sequence length in Large Language Models (LLMs) puts significant pressure on key-value (KV) cache storage, making efficient inference challenging. Explicitly distinguishing attention behavior into our self-defined surface memorization and logic construction reveals essential roles in long-context reasoning. We observe that an individual attention head can display various behaviors, with nearly 98.5% effectively ignoring completely irrelevant information. The remaining 1.5% behaves as logic construction, and 0.5% behaves as surface memorization. Based on layer- and head-wise integration, we propose a novel two-stage SurfaceLogicKV method to utilize these attention behaviors for KV Cache compression. As a result, it achieves improved compressing robustness while maintaining competitive performance across various tasks and long sequences compared to baselines or even FullKV in some specific situations

**Link**: [arxiv](http://arxiv.org/abs/2508.15806v1),  [pdf](http://arxiv.org/pdf/2508.15806v1)

**Tags**: cs.CL cs.AI 



### Routing and Wavelength Assignment with Minimal Attack Radius for QKD   Networks
**Authors**: Mengyao Li, Qiaolun Zhang, Zongshuai Yang, Stefano Bregni, Alberto Gatto, Raouf Boutaba, Massimo Tornatore

**Updated**: 2025-08-14T13:10:43Z

**Summary**: Quantum Key Distribution (QKD) can distribute keys with guaranteed security but remains susceptible to key exchange interruption due to physical-layer threats, such as high-power jamming attacks. To address this challenge, we first introduce a novel metric, namely Maximum Number of Affected Requests (maxNAR), to quantify the worst-case impact of a single physical-layer attack, and then we investigate a new problem of Routing and Wavelength Assignment with Minimal Attack Radius (RWA-MAR). We formulate the problem using an Integer Linear Programming (ILP) model and propose a scalable heuristic to efficiently minimize maxNAR. Our approach incorporates key caching through Quantum Key Pools (QKPs) to enhance resilience and optimize resource utilization. Moreover, we model the impact of different QKD network architectures, employing Optical Bypass (OB) for optical switching of quantum channels and Trusted Relay (TR) for secure key forwarding. Moreover, a tunable parameter is designed in the heuristic to guide the preference for OB or TR, offering enhanced adaptability and dynamic control in diverse network scenarios. Simulation results confirm that our method significantly outperforms the baseline in terms of security and scalability.

**Link**: [arxiv](http://arxiv.org/abs/2508.10613v1),  [pdf](http://arxiv.org/pdf/2508.10613v1)

**Tags**: quant-ph cs.NI 



### Yan: Foundational Interactive Video Generation
**Authors**: Deheng Ye, Fangyun Zhou, Jiacheng Lv, Jianqi Ma, Jun Zhang, Junyan Lv, Junyou Li, Minwen Deng, Mingyu Yang, Qiang Fu, Wei Yang, Wenkai Lv, Yangbin Yu, Yewen Wang, Yonghang Guan, Zhihao Hu, Zhongbin Fang, Zhongqian Sun

**Updated**: 2025-08-14T10:26:51Z

**Summary**: We present Yan, a foundational framework for interactive video generation, covering the entire pipeline from simulation and generation to editing. Specifically, Yan comprises three core modules. AAA-level Simulation: We design a highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based shift-window denoising inference process, achieving real-time 1080P/60FPS interactive simulation. Multi-Modal Generation: We introduce a hierarchical autoregressive caption method that injects game-specific knowledge into open-domain multi-modal video diffusion models (VDMs), then transforming the VDM into a frame-wise, action-controllable, real-time infinite interactive video generator. Notably, when the textual and visual prompts are sourced from different domains, the model demonstrates strong generalization, allowing it to blend and compose the style and mechanics across domains flexibly according to user prompts. Multi-Granularity Editing: We propose a hybrid model that explicitly disentangles interactive mechanics simulation from visual rendering, enabling multi-granularity video content editing during interaction through text. Collectively, Yan offers an integration of these modules, pushing interactive video generation beyond isolated capabilities toward a comprehensive AI-driven interactive creation paradigm, paving the way for the next generation of creative tools, media, and entertainment. The project page is: https://greatx3.github.io/Yan/.

**Link**: [arxiv](http://arxiv.org/abs/2508.08601v3),  [pdf](http://arxiv.org/pdf/2508.08601v3)

**Tags**: cs.CV cs.AI 



### ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic   Parallelism in LLMs
**Authors**: Keyu Chen, Zhifeng Shen, Daohai Yu, Haoqian Wu, Wei Wen, Jianfeng He, Ruizhi Qiao, Xing Sun

**Updated**: 2025-08-14T09:04:56Z

**Summary**: The increasing scale and complexity of large language models (LLMs) pose significant inference latency challenges, primarily due to their autoregressive decoding paradigm characterized by the sequential nature of next-token prediction. By re-examining the outputs of autoregressive models, we observed that some segments exhibit parallelizable structures, which we term intrinsic parallelism. Decoding each parallelizable branch simultaneously (i.e. parallel decoding) can significantly improve the overall inference speed of LLMs. In this paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which addresses two core challenges: automated construction of parallelizable data and efficient parallel decoding mechanism. More specifically, we introduce a non-invasive pipeline that automatically extracts and validates parallelizable structures from the responses of autoregressive models. To empower efficient adaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which enables seamless transitions between serial and parallel decoding modes while maintaining a reusable KV cache, maximizing computational efficiency. Extensive evaluations across General Tasks, Retrieval-Augmented Generation, Mathematical Reasoning, demonstrate that ASPD achieves unprecedented performance in both effectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up to 3.19x speedup (1.85x on average) while maintaining response quality within 1% difference compared to autoregressive models, realizing significant acceleration without compromising generation quality. Our framework sets a groundbreaking benchmark for efficient LLM parallel inference, paving the way for its deployment in latency-sensitive applications such as AI-powered customer service bots and answer retrieval engines.

**Link**: [arxiv](http://arxiv.org/abs/2508.08895v2),  [pdf](http://arxiv.org/pdf/2508.08895v2)

**Tags**: cs.CL cs.AI 



### NanoControl: A Lightweight Framework for Precise and Efficient Control   in Diffusion Transformer
**Authors**: Shanyuan Liu, Jian Zhu, Junda Lu, Yue Gong, Liuzhuozheng Li, Bo Cheng, Yuhang Ma, Liebucha Wu, Xiaoyu Wu, Dawei Leng, Yuhui Yin

**Updated**: 2025-08-14T07:54:44Z

**Summary**: Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in text-to-image synthesis. However, in the domain of controllable text-to-image generation using DiTs, most existing methods still rely on the ControlNet paradigm originally designed for UNet-based diffusion models. This paradigm introduces significant parameter overhead and increased computational costs. To address these challenges, we propose the Nano Control Diffusion Transformer (NanoControl), which employs Flux as the backbone network. Our model achieves state-of-the-art controllable text-to-image generation performance while incurring only a 0.024\% increase in parameter count and a 0.029\% increase in GFLOPs, thus enabling highly efficient controllable generation. Specifically, rather than duplicating the DiT backbone for control, we design a LoRA-style (low-rank adaptation) control module that directly learns control signals from raw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation mechanism that integrates condition-specific key-value information into the backbone in a simple yet highly effective manner, facilitating deep fusion of conditional features. Extensive benchmark experiments demonstrate that NanoControl significantly reduces computational overhead compared to conventional control approaches, while maintaining superior generation quality and achieving improved controllability.

**Link**: [arxiv](http://arxiv.org/abs/2508.10424v1),  [pdf](http://arxiv.org/pdf/2508.10424v1)

**Tags**: cs.CV 



### XQuant: Breaking the Memory Wall for LLM Inference with KV Cache   Rematerialization
**Authors**: Aditya Tomar, Coleman Hooper, Minjae Lee, Haocheng Xi, Rishabh Tiwari, Wonjun Kang, Luca Manolache, Michael W. Mahoney, Kurt Keutzer, Amir Gholami

**Updated**: 2025-08-14T06:52:38Z

**Summary**: Although LLM inference has emerged as a critical workload for many downstream applications, efficiently inferring LLMs is challenging due to the substantial memory footprint and bandwidth requirements. In parallel, compute capabilities have steadily outpaced both memory capacity and bandwidth over the last few decades, a trend that remains evident in modern GPU hardware and exacerbates the challenge of LLM inference. As such, new algorithms are emerging that trade increased computation for reduced memory operations. To that end, we present XQuant, which takes advantage of this trend, enabling an order-of-magnitude reduction in memory consumption through low-bit quantization with substantial accuracy benefits relative to state-of-the-art KV cache quantization methods. We accomplish this by quantizing and caching the layer input activations X, instead of using standard KV caching, and then rematerializing the Keys and Values on-the-fly during inference. This results in an immediate 2$\times$ memory savings compared to KV caching. By applying XQuant, we achieve up to $\sim 7.7\times$ memory savings with $<0.1$ perplexity degradation compared to the FP16 baseline. Furthermore, our approach leverages the fact that X values are similar across layers. Building on this observation, we introduce XQuant-CL, which exploits the cross-layer similarity in the X embeddings for extreme compression. Across different models, XQuant-CL attains up to 10$\times$ memory savings relative to the FP16 baseline with only 0.01 perplexity degradation, and 12.5$\times$ memory savings with only $0.1$ perplexity degradation. XQuant exploits the rapidly increasing compute capabilities of hardware platforms to eliminate the memory bottleneck, while surpassing state-of-the-art KV cache quantization methods and achieving near-FP16 accuracy across a wide range of models.

**Link**: [arxiv](http://arxiv.org/abs/2508.10395v1),  [pdf](http://arxiv.org/pdf/2508.10395v1)

**Tags**: cs.LG 



### RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache   Compression
**Authors**: Payman Behnam, Yaosheng Fu, Ritchie Zhao, Po-An Tsai, Zhiding Yu, Alexey Tumanov

**Updated**: 2025-08-13T17:55:58Z

**Summary**: Transformer-based Large Language Models rely critically on the KV cache to efficiently handle extended contexts during the decode phase. Yet, the size of the KV cache grows proportionally with the input length, burdening both memory bandwidth and capacity as decoding progresses. To address this challenge, we present RocketKV, a training-free KV cache compression strategy containing two consecutive stages. In the first stage, it performs coarse-grain permanent KV cache eviction on the input sequence tokens. In the second stage, it adopts a hybrid sparse attention method to conduct fine-grain top-k sparse attention, approximating the attention scores by leveraging both head and sequence dimensionality reductions. We show that RocketKV provides a compression ratio of up to 400$\times$, end-to-end speedup of up to 3.7$\times$ as well as peak memory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU compared to the full KV cache baseline, while achieving negligible accuracy loss on a variety of long-context tasks. We also propose a variant of RocketKV for multi-turn scenarios, which consistently outperforms other existing methods and achieves accuracy nearly on par with an oracle top-k attention scheme. The source code is available here: https://github.com/NVlabs/RocketKV.

**Link**: [arxiv](http://arxiv.org/abs/2502.14051v3),  [pdf](http://arxiv.org/pdf/2502.14051v3)

**Tags**: cs.CL cs.LG 



### Re-thinking Memory-Bound Limitations in CGRAs
**Authors**: Xiangfeng Liu, Zhe Jiang, Anzhen Zhu, Xiaomeng Han, Mingsong Lyu, Qingxu Deng, Nan Guan

**Updated**: 2025-08-27T12:13:45Z

**Summary**: Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators commonly employed to boost performance in workloads with iterative structures. Existing research typically focuses on compiler or architecture optimizations aimed at improving CGRA performance, energy efficiency, flexibility, and area utilization, under the idealistic assumption that kernels can access all data from Scratchpad Memory (SPM). However, certain complex workloads-particularly in fields like graph analytics, irregular database operations, and specialized forms of high-performance computing (e.g., unstructured mesh simulations)-exhibit irregular memory access patterns that hinder CGRA utilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To address this challenge, we conduct a thorough analysis of the underlying causes of performance degradation, then propose a redesigned memory subsystem and refine the memory model. With both microarchitectural and theoretical optimization, our solution can effectively manage irregular memory accesses through CGRA-specific runahead execution mechanism and cache reconfiguration techniques. Our results demonstrate that we can achieve performance comparable to the original SPM-only system while requiring only 1.27% of the storage size. The runahead execution mechanism achieves an average 3.04x speedup (up to 6.91x), with cache reconfiguration technique providing an additional 6.02% improvement, significantly enhancing CGRA performance for irregular memory access patterns.

**Link**: [arxiv](http://arxiv.org/abs/2508.09570v2),  [pdf](http://arxiv.org/pdf/2508.09570v2)

**Tags**: cs.AR B.3.0; B.6.0 



### Performant Automatic BLAS Offloading on Unified Memory Architecture with   OpenMP First-Touch Style Data Movement
**Authors**: Junjie Li

**Updated**: 2025-08-13T06:13:36Z

**Summary**: BLAS is a fundamental building block of advanced linear algebra libraries and many modern scientific computing applications. GPUs are known for their strong arithmetic computing capabilities and are highly suited for BLAS operations. However, porting code to GPUs often requires significant effort, especially for large, complex codes or legacy codes, even for BLAS-heavy applications. While various tools exist to automatically offload BLAS to GPUs, they are often impractical due to the high costs associated with mandatory data transfers. The advent of unified memory architectures in recent GPU designs, such as the NVIDIA Grace-Hopper, allows cache-coherent memory access across all types of memory for both CPU and GPU, potentially eliminating the bottlenecks faced in conventional architectures. This breakthrough paves the way for innovative application developments and porting strategies. Building on our preliminary work demonstrating the potential of automatic *gemm offload, this paper extends the framework to all level-3 BLAS operations and introduces SCILIB-Accel, a novel tool for automatic BLAS offload. SCILIB-Accel leverages the memory coherency in Grace-Hopper and introduces a Device First-Use data movement policy inspired by the OpenMP First-Touch approach in multi-socket CPU programming, minimizing CPU-GPU data transfers for typical scientific computing codes. Additionally, utilizing dynamic binary instrumentation, the tool intercepts BLAS symbols directly from a CPU binary, requiring no code modifications or recompilation. SCILIB-Accel has been evaluated using multiple quantum physics codes on up to a few hundred GPU nodes, yielding promising speedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was achieved on Grace-Hopper compared to Grace-Grace.

**Link**: [arxiv](http://arxiv.org/abs/2501.00279v4),  [pdf](http://arxiv.org/pdf/2501.00279v4)

**Tags**: cs.DC cs.MS cs.PF cs.SE 



### Advancing Reliable Test-Time Adaptation of Vision-Language Models under   Visual Variations
**Authors**: Yiwen Liang, Hui Chen, Yizhe Xiong, Zihan Zhou, Mengyao Lyu, Zijia Lin, Shuaicheng Niu, Sicheng Zhao, Jungong Han, Guiguang Ding

**Updated**: 2025-08-13T04:24:56Z

**Summary**: Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but struggle with distribution shifts in downstream tasks when labeled data is unavailable, which has motivated the development of Test-Time Adaptation (TTA) to improve VLMs' performance during inference without annotations. Among various TTA approaches, cache-based methods show promise by preserving historical knowledge from low-entropy samples in a dynamic cache and fostering efficient adaptation. However, these methods face two critical reliability challenges: (1) entropy often becomes unreliable under distribution shifts, causing error accumulation in the cache and degradation in adaptation performance; (2) the final predictions may be unreliable due to inflexible decision boundaries that fail to accommodate large downstream shifts. To address these challenges, we propose a Reliable Test-time Adaptation (ReTA) method that integrates two complementary strategies to enhance reliability from two perspectives. First, to mitigate the unreliability of entropy as a sample selection criterion for cache construction, we introduce Consistency-aware Entropy Reweighting (CER), which incorporates consistency constraints to weight entropy during cache updating. While conventional approaches rely solely on low entropy for cache prioritization and risk introducing noise, our method leverages predictive consistency to maintain a high-quality cache and facilitate more robust adaptation. Second, we present Diversity-driven Distribution Calibration (DDC), which models class-wise text embeddings as multivariate Gaussian distributions, enabling adaptive decision boundaries for more accurate predictions across visually diverse content. Extensive experiments demonstrate that ReTA consistently outperforms state-of-the-art methods, particularly under real-world distribution shifts. Code: https://github.com/Evelyn1ywliang/ReTA.

**Link**: [arxiv](http://arxiv.org/abs/2507.09500v2),  [pdf](http://arxiv.org/pdf/2507.09500v2)

**Tags**: cs.CV 



### The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM   Serving Systems
**Authors**: Linke Song, Zixuan Pang, Wenhao Wang, Zihao Wang, XiaoFeng Wang, Hongbo Chen, Wei Song, Yier Jin, Dan Meng, Rui Hou

**Updated**: 2025-08-13T04:03:10Z

**Summary**: The wide deployment of Large Language Models (LLMs) has given rise to strong demands for optimizing their inference performance. Today's techniques serving this purpose primarily focus on reducing latency and improving throughput through algorithmic and hardware enhancements, while largely overlooking their privacy side effects, particularly in a multi-user environment. In our research, for the first time, we discovered a set of new timing side channels in LLM systems, arising from shared caches and GPU memory allocations, which can be exploited to infer both confidential system prompts and those issued by other users. These vulnerabilities echo security challenges observed in traditional computing systems, highlighting an urgent need to address potential information leakage in LLM serving infrastructures. In this paper, we report novel attack strategies designed to exploit such timing side channels inherent in LLM deployments, specifically targeting the Key-Value (KV) cache and semantic cache widely used to enhance LLM inference performance. Our approach leverages timing measurements and classification models to detect cache hits, allowing an adversary to infer private prompts with high accuracy. We also propose a token-by-token search algorithm to efficiently recover shared prompt prefixes in the caches, showing the feasibility of stealing system prompts and those produced by peer users. Our experimental studies on black-box testing of popular online LLM services demonstrate that such privacy risks are completely realistic, with significant consequences. Our findings underscore the need for robust mitigation to protect LLM systems against such emerging threats.

**Link**: [arxiv](http://arxiv.org/abs/2409.20002v4),  [pdf](http://arxiv.org/pdf/2409.20002v4)

**Tags**: cs.CR 



### Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache   in LLM Inference
**Authors**: Zhifan Luo, Shuo Shao, Su Zhang, Lijing Zhou, Yuke Hu, Chenxu Zhao, Zhihao Liu, Zhan Qin

**Updated**: 2025-08-13T02:48:25Z

**Summary**: The Key-Value (KV) cache, which stores intermediate attention computations (Key and Value pairs) to avoid redundant calculations, is a fundamental mechanism for accelerating Large Language Model (LLM) inference. However, this efficiency optimization introduces significant yet underexplored privacy risks. This paper provides the first comprehensive analysis of these vulnerabilities, demonstrating that an attacker can reconstruct sensitive user inputs directly from the KV-cache. We design and implement three distinct attack vectors: a direct Inversion Attack, a more broadly applicable and potent Collision Attack, and a semantic-based Injection Attack. These methods demonstrate the practicality and severity of KV-cache privacy leakage issues. To mitigate this, we propose KV-Cloak, a novel, lightweight, and efficient defense mechanism. KV-Cloak uses a reversible matrix-based obfuscation scheme, combined with operator fusion, to secure the KV-cache. Our extensive experiments show that KV-Cloak effectively thwarts all proposed attacks, reducing reconstruction quality to random noise. Crucially, it achieves this robust security with virtually no degradation in model accuracy and minimal performance overhead, offering a practical solution for trustworthy LLM deployment.

**Link**: [arxiv](http://arxiv.org/abs/2508.09442v1),  [pdf](http://arxiv.org/pdf/2508.09442v1)

**Tags**: cs.CR cs.AI cs.CL 



### Design and Simulation of 6T SRAM Array
**Authors**: Justin London

**Updated**: 2025-08-13T01:39:09Z

**Summary**: Conventional 6T SRAM is used in microprocessors in the cache memory design. The basic 6T SRAM cell and a 6 bit memory array layout are designed in LEdit. The design and analysis of key SRAM components, sense amplifiers, decoders, write drivers and precharge circuits are also provided. The pulse voltage waveforms generated for read and write operations as well as Q and Qbar nodes are simulated in LTSpice. Parasitic capacitances are extracted and their impact on the waveforms analyzed. Static noise margin, propagation delays, and power dissipation are calculated. Comparison of SRAM read and write operational performance using CMOS transistors is made with edge-triggered D flip flops. If certain size area and ratio constraints are satisfied, the 6T cell with CMOS transistors will possess stability, speed, and power efficiency. Both theoretical and simulated results are given.

**Link**: [arxiv](http://arxiv.org/abs/2508.09419v1),  [pdf](http://arxiv.org/pdf/2508.09419v1)

**Tags**: eess.SY cs.SY 



## Keyword: LLM Inference 
 ### Cohort-Aware Agents for Individualized Lung Cancer Risk Prediction Using   a Retrieval-Augmented Model Selection Framework
**Authors**: Chongyu Qu, Allen J. Luna, Thomas Z. Li, Junchao Zhu, Junlin Guo, Juming Xiong, Kim L. Sandler, Bennett A. Landman, Yuankai Huo

**Updated**: 2025-08-26T17:59:53Z

**Summary**: Accurate lung cancer risk prediction remains challenging due to substantial variability across patient populations and clinical settings -- no single model performs best for all cohorts. To address this, we propose a personalized lung cancer risk prediction agent that dynamically selects the most appropriate model for each patient by combining cohort-specific knowledge with modern retrieval and reasoning techniques. Given a patient's CT scan and structured metadata -- including demographic, clinical, and nodule-level features -- the agent first performs cohort retrieval using FAISS-based similarity search across nine diverse real-world cohorts to identify the most relevant patient population from a multi-institutional database. Second, a Large Language Model (LLM) is prompted with the retrieved cohort and its associated performance metrics to recommend the optimal prediction algorithm from a pool of eight representative models, including classical linear risk models (e.g., Mayo, Brock), temporally-aware models (e.g., TD-VIT, DLSTM), and multi-modal computer vision-based approaches (e.g., Liao, Sybil, DLS, DLI). This two-stage agent pipeline -- retrieval via FAISS and reasoning via LLM -- enables dynamic, cohort-aware risk prediction personalized to each patient's profile. Building on this architecture, the agent supports flexible and cohort-driven model selection across diverse clinical populations, offering a practical path toward individualized risk assessment in real-world lung cancer screening.

**Link**: [arxiv](http://arxiv.org/abs/2508.14940v2),  [pdf](http://arxiv.org/pdf/2508.14940v2)

**Tags**: cs.LG 



### StepWiser: Stepwise Generative Judges for Wiser Reasoning
**Authors**: Wei Xiong, Wenting Zhao, Weizhe Yuan, Olga Golovneva, Tong Zhang, Jason Weston, Sainbayar Sukhbaatar

**Updated**: 2025-08-26T17:45:05Z

**Summary**: As models increasingly leverage multi-step reasoning strategies to solve complex problems, supervising the logical validity of these intermediate steps has become a critical research challenge. Process reward models address this by providing step-by-step feedback, but current approaches have two major drawbacks: they typically function as classifiers without providing explanations, and their reliance on supervised fine-tuning with static datasets limits generalization. Inspired by recent advances, we reframe stepwise reward modeling from a classification task to a reasoning task itself. We thus propose a generative judge that reasons about the policy model's reasoning steps (i.e., meta-reasons), outputting thinking tokens before delivering a final verdict. Our model, StepWiser, is trained by reinforcement learning using relative outcomes of rollouts. We show it provides (i) better judgment accuracy on intermediate steps than existing methods; (ii) can be used to improve the policy model at training time; and (iii) improves inference-time search.

**Link**: [arxiv](http://arxiv.org/abs/2508.19229v1),  [pdf](http://arxiv.org/pdf/2508.19229v1)

**Tags**: cs.AI cs.CL 



### Generative Interfaces for Language Models
**Authors**: Jiaqi Chen, Yanzhe Zhang, Yutong Zhang, Yijia Shao, Diyi Yang

**Updated**: 2025-08-26T17:43:20Z

**Summary**: Large language models (LLMs) are increasingly seen as assistants, copilots, and consultants, capable of supporting a wide range of tasks through natural conversation. However, most systems remain constrained by a linear request-response format that often makes interactions inefficient in multi-turn, information-dense, and exploratory tasks. To address these limitations, we propose Generative Interfaces for Language Models, a paradigm in which LLMs respond to user queries by proactively generating user interfaces (UIs) that enable more adaptive and interactive engagement. Our framework leverages structured interface-specific representations and iterative refinements to translate user queries into task-specific UIs. For systematic evaluation, we introduce a multidimensional assessment framework that compares generative interfaces with traditional chat-based ones across diverse tasks, interaction patterns, and query types, capturing functional, interactive, and emotional aspects of user experience. Results show that generative interfaces consistently outperform conversational ones, with humans preferring them in over 70% of cases. These findings clarify when and why users favor generative interfaces, paving the way for future advancements in human-AI interaction.

**Link**: [arxiv](http://arxiv.org/abs/2508.19227v1),  [pdf](http://arxiv.org/pdf/2508.19227v1)

**Tags**: cs.CL cs.AI cs.HC 



### From Intents to Conversations: Generating Intent-Driven Dialogues with   Contrastive Learning for Multi-Turn Classification
**Authors**: Junhua Liu, Yong Keat Tan, Bin Fu, Kwan Hui Lim

**Updated**: 2025-08-26T17:22:52Z

**Summary**: In conversational AI systems, a critical challenge in training effective multi-turn intent classification models lies in the generation of large-scale, domain-specific, multilingual dialogue datasets. In this paper, we introduce Chain-of-Intent, a novel framework that integrates Hidden Markov Models (HMMs) with Large Language Models (LLMs) to generate intent-driven, context-aware dialogues through self-play. Our method first extracts domain-specific intent transition patterns from real-world e-commerce chat logs, which guide the modeling of turn-level dynamics and intent sequences. LLMs are then employed to parameterize the emission probabilities of HMMs, enabling the generation of natural, coherent utterances aligned with predicted intents and dialogue context. We further propose MINT-CL, a multi-task contrastive learning framework for multi-turn intent classification, which improves performance while reducing dependence on large-scale annotated datasets. Empirical results demonstrate that our approach outperforms competitive baselines in both dialogue generation quality and classification accuracy, particularly in multilingual settings. To facilitate future research, we release MINT-E, a comprehensive, multilingual, intent-aware multi-turn dialogue corpus derived from the e-commerce domain. The reproduced source code and dataset are available at https://github.com/junhua/chain-of-intent.

**Link**: [arxiv](http://arxiv.org/abs/2411.14252v2),  [pdf](http://arxiv.org/pdf/2411.14252v2)

**Tags**: cs.CL cs.AI 



### Astrophysics informed Gaussian processes for gravitational-wave   populations: Evidence for the onset of the pair-instability supernova mass   gap
**Authors**: Ignacio Magaña Hernandez, Antonella Palmese

**Updated**: 2025-08-26T17:13:20Z

**Summary**: We analyze binary black hole (BBH) mergers from the latest Gravitational Wave Transient Catalog (GWTC-3) using a flexible, non-parametric framework to infer the underlying black hole mass distribution. Our model employs Gaussian Processes (GPs) with astrophysically motivated priors to represent the mass distribution, assuming both component masses are independently drawn from a common mass function. This approach enables us to capture complex features in the population without relying on rigid parametric forms. Motivated by predictions from binary stellar evolution, we focus on the presence of a mass gap in the range $40-120~ M_\odot$, attributed to the pair-instability supernova (PISN) and pulsational PISN (PPISN) processes. Using our GP-based model, we find for the first time, strong evidence for the onset of this mass gap, locating its lower edge at approximately $45-60 ~M_\odot$. We observe a suppression in the BBH merger rate when comparing the latest constraints against our GP model, by a factor of $10-60$ at $\sim60~M_\odot$, corresponding to the minimum of the mass gap. Additionally, we find evidence of a subpopulation of mergers populating the mass gap around $70 ~M_\odot$, which we argue is due to hierarchical mergers, as well as of a feature in the $40-50~ M_\odot$ range, albeit with low significance, which we attribute to the predicted PPISN build up. We discuss the astrophysical implications of this result in light of GW231123, a recently reported BBH merger with a total mass potentially above the PISN gap, suggesting the need for revised models of massive stellar evolution or alternative formation channels.

**Link**: [arxiv](http://arxiv.org/abs/2508.19208v1),  [pdf](http://arxiv.org/pdf/2508.19208v1)

**Tags**: astro-ph.HE 



### Bridging the Editing Gap in LLMs: FineEdit for Precise and Targeted Text   Modifications
**Authors**: Yiming Zeng, Wanhao Yu, Zexin Li, Tao Ren, Yu Ma, Jinghan Cao, Xiyan Chen, Tingting Yu

**Updated**: 2025-08-26T17:11:42Z

**Summary**: Large Language Models (LLMs) have significantly advanced natural language processing, demonstrating strong capabilities in tasks such as text generation, summarization, and reasoning. Recently, their potential for automating precise text editing tasks across specialized domains, such as programming code, LaTeX, and structured database languages, has gained attention. However, current state-of-the-art LLMs still struggle with executing precise, instruction-driven edits, particularly when structural accuracy and strict adherence to domain conventions are required. To address these challenges, we introduce InstrEditBench, an automated benchmark dataset comprising over 30,000 structured editing tasks spanning diverse domains, including Wikipedia articles, LaTeX documents, source code, and database languages. Using this benchmark, we develop FineEdit, a specialized editing model explicitly trained for accurate, context-aware text modifications. Experimental evaluations demonstrate that FineEdit outperforms state-of-the-art models, achieving improvements of approximately 10\% over Gemini models on single-turn edits, up to 30\% over Llama-3.2-3B, and exceeding Mistral-7B-OpenOrca performance by over 40\% on direct editing tasks. FineEdit also effectively generalizes to realistic multi-turn editing scenarios, highlighting its practical applicability. To facilitate further research and reproducibility, we release FineEdit at https://github.com/StuRinDQB/FineEdit} and https://huggingface.co/datasets/YimingZeng/FineEdit_bench.

**Link**: [arxiv](http://arxiv.org/abs/2502.13358v3),  [pdf](http://arxiv.org/pdf/2502.13358v3)

**Tags**: cs.CL 



### Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and   Reasoning
**Authors**: Alan Li, Yixin Liu, Arpan Sarkar, Doug Downey, Arman Cohan

**Updated**: 2025-08-26T17:04:23Z

**Summary**: Scientific problem solving poses unique challenges for LLMs, requiring both deep domain knowledge and the ability to apply such knowledge through complex reasoning. While automated scientific reasoners hold great promise for assisting human scientists, there is currently no widely adopted holistic benchmark for evaluating scientific reasoning, and few approaches systematically disentangle the distinct roles of knowledge and reasoning in these tasks. To address these gaps, we introduce SciReas, a diverse suite of existing benchmarks for scientific reasoning tasks, and SciReas-Pro, a selective subset that requires more complex reasoning. Our holistic evaluation surfaces insights about scientific reasoning performance that remain hidden when relying on individual benchmarks alone. We then propose KRUX, a probing framework for studying the distinct roles of reasoning and knowledge in scientific tasks. Combining the two, we conduct an in-depth analysis that yields several key findings: (1) Retrieving task-relevant knowledge from model parameters is a critical bottleneck for LLMs in scientific reasoning; (2) Reasoning models consistently benefit from external knowledge added in-context on top of the reasoning enhancement; (3) Enhancing verbalized reasoning improves LLMs' ability to surface task-relevant knowledge. Finally, we conduct a lightweight analysis, comparing our science-focused data composition with concurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline for scientific reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2508.19202v1),  [pdf](http://arxiv.org/pdf/2508.19202v1)

**Tags**: cs.CL 



### Understanding Tool-Integrated Reasoning
**Authors**: Heng Lin, Zhongwen Xu

**Updated**: 2025-08-26T17:03:46Z

**Summary**: We study why Tool-Integrated Reasoning (TIR) makes Large Language Models (LLMs) more capable. While LLMs integrated with tools like Python code interpreters show great promise, a principled theory explaining why this paradigm is effective has been missing. This work provides the first formal proof that TIR fundamentally expands an LLM's capabilities. We demonstrate that tools enable a strict expansion of the model's empirical and feasible support, breaking the capability ceiling of pure-text models by unlocking problem-solving strategies that are otherwise impossible or intractably verbose. To guide model behavior without compromising training stability and performance, we also introduce Advantage Shaping Policy Optimization (ASPO), a novel algorithm that directly modifies the advantage function to guide the policy behavior. We conduct comprehensive experiments on challenging mathematical benchmarks, leveraging a Python interpreter as the external tool. Our results show that the TIR model decisively outperforms its pure-text counterpart on the pass@k metric. Crucially, this advantage is not confined to computationally-intensive problems but extends to those requiring significant abstract insight. We further identify the emergent cognitive patterns that illustrate how models learn to think with tools. Finally, we report improved tool usage behavior with early code invocation and much more interactive turns with ASPO. Overall, our work provides the first principled explanation for TIR's success, shifting the focus from the mere fact that tools work to why and how they enable more powerful reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2508.19201v1),  [pdf](http://arxiv.org/pdf/2508.19201v1)

**Tags**: cs.LG cs.AI stat.ML 



### The Ramon Llull's Thinking Machine for Automated Ideation
**Authors**: Xinran Zhao, Boyuan Zheng, Chenglei Si, Haofei Yu, Ken Liu, Runlong Zhou, Ruochen Li, Tong Chen, Xiang Li, Yiming Zhang, Tongshuang Wu

**Updated**: 2025-08-26T17:03:43Z

**Summary**: This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for generating knowledge through symbolic recombination - as a conceptual foundation for building a modern Llull's thinking machine for research ideation. Our approach defines three compositional axes: Theme (e.g., efficiency, adaptivity), Domain (e.g., question answering, machine translation), and Method (e.g., adversarial training, linear attention). These elements represent high-level abstractions common in scientific work - motivations, problem settings, and technical approaches - and serve as building blocks for LLM-driven exploration. We mine elements from human experts or conference papers and show that prompting LLMs with curated combinations produces research ideas that are diverse, relevant, and grounded in current literature. This modern thinking machine offers a lightweight, interpretable tool for augmenting scientific creativity and suggests a path toward collaborative ideation between humans and AI.

**Link**: [arxiv](http://arxiv.org/abs/2508.19200v1),  [pdf](http://arxiv.org/pdf/2508.19200v1)

**Tags**: cs.AI cs.CL 



### Prompt-based Dynamic Token Pruning for Efficient Segmentation of Medical   Images
**Authors**: Pallabi Dutta, Anubhab Maity, Sushmita Mitra

**Updated**: 2025-08-26T17:02:00Z

**Summary**: The high computational demands of Vision Transformers (ViTs) in processing a large number of tokens often constrain their practical application in analyzing medical images. This research proposes a Prompt-driven Adaptive Token ({\it PrATo}) pruning method to selectively reduce the processing of irrelevant tokens in the segmentation pipeline. The prompt-based spatial prior helps to rank the tokens according to their relevance. Tokens with low-relevance scores are down-weighted, ensuring that only the relevant ones are propagated for processing across subsequent stages. This data-driven pruning strategy improves segmentation accuracy and inference speed by allocating computational resources to essential regions. The proposed framework is integrated with several state-of-the-art models to facilitate the elimination of irrelevant tokens, thereby enhancing computational efficiency while preserving segmentation accuracy. The experimental results show a reduction of $\sim$ 35-55% tokens; thus reducing the computational costs relative to baselines. Cost-effective medical image processing, using our framework, facilitates real-time diagnosis by expanding its applicability in resource-constrained environments.

**Link**: [arxiv](http://arxiv.org/abs/2506.16369v2),  [pdf](http://arxiv.org/pdf/2506.16369v2)

**Tags**: cs.CV 



### Unraveling the temporal dependence of ecological interaction measures
**Authors**: Javier Aguilar, Samir Suweis, Amos Maritan, Sandro Azaele

**Updated**: 2025-08-26T17:01:34Z

**Summary**: Species interactions (ranging from direct predator prey relationships to indirect effects mediated by the environment) are central to ecosystem balance and biodiversity. While empirical methods for measuring these interactions exist, their interpretability and limitations remain unclear. Here we examine the empirical matrix of pairwise interactions, a widely used tool, and analyze its temporal variability. We show that apparent fluctuations in interaction strength (and even shifts in interaction signs, often interpreted as transitions between competition and facilitation) can arise intrinsically from population dynamics with fixed ecological roles. Experimental protocols further shape these estimates: the duration of observation and the type of setup in microbial growth studies (e.g., chemostats, batch cultures, or resource conditions) systematically affect measured interactions. Considering interactions across timescales enhances interpretability: short-term measurements primarily capture direct species couplings, whereas long-term observations increasingly reflect indirect community feedback. Taken together, these results establish short duration inferences, obtained either directly or extrapolated, as a principled way to disentangle direct from indirect interactions. Building on this insight, we propose a model inference approach that leverages multiple short time series rather than extended longitudinal datasets.

**Link**: [arxiv](http://arxiv.org/abs/2508.19197v1),  [pdf](http://arxiv.org/pdf/2508.19197v1)

**Tags**: q-bio.PE physics.bio-ph 



### Pixie: Fast and Generalizable Supervised Learning of 3D Physics from   Pixels
**Authors**: Long Le, Ryan Lucas, Chen Wang, Chuhao Chen, Dinesh Jayaraman, Eric Eaton, Lingjie Liu

**Updated**: 2025-08-26T16:57:07Z

**Summary**: Inferring the physical properties of 3D scenes from visual information is a critical yet challenging task for creating interactive and realistic virtual worlds. While humans intuitively grasp material characteristics such as elasticity or stiffness, existing methods often rely on slow, per-scene optimization, limiting their generalizability and application. To address this problem, we introduce PIXIE, a novel method that trains a generalizable neural network to predict physical properties across multiple scenes from 3D visual features purely using supervised losses. Once trained, our feed-forward network can perform fast inference of plausible material fields, which coupled with a learned static scene representation like Gaussian Splatting enables realistic physics simulation under external forces. To facilitate this research, we also collected PIXIEVERSE, one of the largest known datasets of paired 3D assets and physic material annotations. Extensive evaluations demonstrate that PIXIE is about 1.46-4.39x better and orders of magnitude faster than test-time optimization methods. By leveraging pretrained visual features like CLIP, our method can also zero-shot generalize to real-world scenes despite only ever been trained on synthetic data. https://pixie-3d.github.io/

**Link**: [arxiv](http://arxiv.org/abs/2508.17437v2),  [pdf](http://arxiv.org/pdf/2508.17437v2)

**Tags**: cs.CV 



### Separating Intent from Execution: A Probabilistic Approach to Pitch   Location Accuracy
**Authors**: Matt Ludwig, Ryan S. Brill, Abraham J. Wyner

**Updated**: 2025-08-26T16:44:30Z

**Summary**: Control has long been recognized as a critical component of pitcher performance, reflecting a pitcher's ability to execute pitches in alignment with his intended targets. However, accurately inferring a pitcher's intentions presents a persistent challenge. Traditional metrics typically rely on uniformity assumptions, inferring intent based on the behavior of a ``typical'' pitcher across similar situations. In this study, we propose an alternative, individualized approach to measuring control, one that eschews such assumptions in favor of personalized inference. We estimate a pitcher's intended location on a pitch-by-pitch basis, conditioning on both individual tendencies and specific game contexts. This allows us to assess control by comparing the actual pitch location to the inferred intended target, thereby aligning measurement more closely with the unique strategies of each pitcher. We introduce xCTRL, a novel metric that quantifies control as the distance between a pitch's actual location and its estimated intended location. We find that xCTRL exhibits strong stability and greater predictive power than existing control metrics. By capturing pitcher-specific intent, xCTRL enhances our understanding of control and offers a more intuitive and accurate representation of pitching performance.

**Link**: [arxiv](http://arxiv.org/abs/2508.19184v1),  [pdf](http://arxiv.org/pdf/2508.19184v1)

**Tags**: stat.AP 



### TL-Training: A Task-Feature-Based Framework for Training Large Language   Models in Tool Use
**Authors**: Junjie Ye, Yilong Wu, Sixian Li, Yuming Yang, Zhiheng Xi, Tao Gui, Qi Zhang, Xuanjing Huang, Peng Wang, Zhongchao Shi, Jianping Fan, Zhengyin Du

**Updated**: 2025-08-26T16:40:49Z

**Summary**: Large language models (LLMs) achieve remarkable advancements by leveraging tools to interact with environments, a critical step toward generalized AI. However, the standard supervised fine-tuning (SFT) approach, which relies on large-scale datasets, often overlooks task-specific characteristics in tool use, leading to performance bottlenecks. To address this issue, we analyze three existing LLMs and uncover key insights: training data can inadvertently impede tool-use behavior, token importance is distributed unevenly, and errors in tool calls fall into a small set of categories. Building on these findings, we propose~\emph{TL-Training}, a task-feature-based framework that mitigates the effects of suboptimal training data, dynamically adjusts token weights to prioritize key tokens during SFT, and incorporates a robust reward mechanism tailored to error categories, optimized through proximal policy optimization. We validate TL-Training by training CodeLLaMA-2-7B and evaluating it on four open-source test sets. Our results demonstrate that the LLM trained by our method matches or surpasses both open- and closed-source LLMs in tool-use performance using only 1,217 training data points. Additionally, our method enhances robustness in noisy environments and improves general task performance, offering a scalable and efficient paradigm for tool-use training in LLMs. Code and data are available at https://github.com/Junjie-Ye/TL-Training.

**Link**: [arxiv](http://arxiv.org/abs/2412.15495v2),  [pdf](http://arxiv.org/pdf/2412.15495v2)

**Tags**: cs.CL cs.AI 



### MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection
**Authors**: Zhihao Zhang, Abhinav Kumar, Girish Chandar Ganesan, Xiaoming Liu

**Updated**: 2025-08-26T16:31:32Z

**Summary**: Accurately predicting 3D attributes is crucial for monocular 3D object detection (Mono3D), with depth estimation posing the greatest challenge due to the inherent ambiguity in mapping 2D images to 3D space. While existing methods leverage multiple depth cues (e.g., estimating depth uncertainty, modeling depth error) to improve depth accuracy, they overlook that accurate depth prediction requires conditioning on other 3D attributes, as these attributes are intrinsically inter-correlated through the 3D to 2D projection, which ultimately limits overall accuracy and stability. Inspired by Chain-of-Thought (CoT) in large language models (LLMs), this paper proposes MonoCoP, which leverages a Chain-of-Prediction (CoP) to predict attributes sequentially and conditionally via three key designs. First, it employs a lightweight AttributeNet (AN) for each 3D attribute to learn attribute-specific features. Next, MonoCoP constructs an explicit chain to propagate these learned features from one attribute to the next. Finally, MonoCoP uses a residual connection to aggregate features for each attribute along the chain, ensuring that later attribute predictions are conditioned on all previously processed attributes without forgetting the features of earlier ones. Experimental results show that our MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI leaderboard without requiring additional data and further surpasses existing methods on the Waymo and nuScenes frontal datasets.

**Link**: [arxiv](http://arxiv.org/abs/2505.04594v5),  [pdf](http://arxiv.org/pdf/2505.04594v5)

**Tags**: cs.CV 



### Applications of compact multipliers to algebrability of   $(\ell_{\infty}\setminus c_0)\cup\{0\}$ and $(B(\ell_2(\mathbb{N}))\setminus   K(\ell_2(\mathbb{N}))\cup \{ 0\}.$
**Authors**: Willian Franca, Jorge J. Garcés

**Updated**: 2025-08-26T16:26:01Z

**Summary**: In present work we deal with the class $\mathcal{C}=\mathcal{C}_1\cup \mathcal{C}_2$ where $\mathcal{C}_1$ (respectively, $\mathcal{C}_2$) is formed by all separable Uniform algebras (respectively, separable commutative C$^*$-algebras) with no compact elements. For a given algebra $A$ in $\mathcal{C}_1$ (respectively, $A$ in $\mathcal{C}_2$) we show that $A$ is isometrically isomorphic as algebra (respectively, as C$^*$-algebra) to a subalgebra $M$ of $\ell_{\infty}$ with $M\subset (\ell_{\infty}\setminus c_0)\cup\{0\}.$ Under the additional assumption that $A$ is non-unital we verify that there exists a copy of $M(A)$ (the multipliers algebra of $A$ which is non-separable) inside $(\ell_{\infty}\setminus c_0)\cup\{0\}$.   For an infinitely generated abelian C$^*$-algebra $B,$ we study the least cardinality possible of a system of generators ($gen_{C^*}(B)$). In fact we deduce that $gen_{C^*}(B)$ coincides with the smallest cardinal number $n$ such that an embedding of $\Delta(B)$ (= the spectrum of $B$) in $\mathbb{R}^n$ exists - The finitely generated version of this result was proved by Nagisa. In addition, we introduce new concepts of algebrability in terms of $gen_{C^*}(B)$ ($(C^*)$-genalgebrability) and its natural variations.   From our methods we infer that there is $^*$-isomorphic copy of $\ell_{\infty}$ in $(\ell_{\infty}\setminus c_0)\cup\{0\}$. In particular, $(\ell_{\infty}\setminus c_0)\cup\{0\}$ contains a copy of every separable Banach space. Moreover, all the positive answers of this work holds if we replace the set $(\ell_{\infty}\setminus c_0)\cup\{0\}$ with $(B(\ell_2(\mathbb{N}))\setminus K(\ell_2(\mathbb{N}))\cup \{ 0\}.$

**Link**: [arxiv](http://arxiv.org/abs/2508.19174v1),  [pdf](http://arxiv.org/pdf/2508.19174v1)

**Tags**: math.FA math.OA Primary Primary: 46L05, 47L40, 46B87. Secondary: 46B45, 46B26 



### MATRIX: Multi-Agent simulaTion fRamework for safe Interactions and   conteXtual clinical conversational evaluation
**Authors**: Ernest Lim, Yajie Vera He, Jared Joselowitz, Kate Preston, Mohita Chowdhury, Louis Williams, Aisling Higham, Katrina Mason, Mariane Melo, Tom Lawton, Yan Jia, Ibrahim Habli

**Updated**: 2025-08-26T16:12:12Z

**Summary**: Despite the growing use of large language models (LLMs) in clinical dialogue systems, existing evaluations focus on task completion or fluency, offering little insight into the behavioral and risk management requirements essential for safety-critical systems. This paper presents MATRIX (Multi-Agent simulaTion fRamework for safe Interactions and conteXtual clinical conversational evaluation), a structured, extensible framework for safety-oriented evaluation of clinical dialogue agents.   MATRIX integrates three components: (1) a safety-aligned taxonomy of clinical scenarios, expected system behaviors and failure modes derived through structured safety engineering methods; (2) BehvJudge, an LLM-based evaluator for detecting safety-relevant dialogue failures, validated against expert clinician annotations; and (3) PatBot, a simulated patient agent capable of producing diverse, scenario-conditioned responses, evaluated for realism and behavioral fidelity with human factors expertise, and a patient-preference study.   Across three experiments, we show that MATRIX enables systematic, scalable safety evaluation. BehvJudge with Gemini 2.5-Pro achieves expert-level hazard detection (F1 0.96, sensitivity 0.999), outperforming clinicians in a blinded assessment of 240 dialogues. We also conducted one of the first realism analyses of LLM-based patient simulation, showing that PatBot reliably simulates realistic patient behavior in quantitative and qualitative evaluations. Using MATRIX, we demonstrate its effectiveness in benchmarking five LLM agents across 2,100 simulated dialogues spanning 14 hazard scenarios and 10 clinical domains.   MATRIX is the first framework to unify structured safety engineering with scalable, validated conversational AI evaluation, enabling regulator-aligned safety auditing. We release all evaluation tools, prompts, structured scenarios, and datasets.

**Link**: [arxiv](http://arxiv.org/abs/2508.19163v1),  [pdf](http://arxiv.org/pdf/2508.19163v1)

**Tags**: cs.AI cs.HC cs.MA 68T50, 68T42, 92C50, 68Q60 I.2.0; J.3 



### From Coverage to Consequences: BMI, Health Behaviors, and Self-rated   Health After Medicaid Contraction
**Authors**: Md Twfiqur Rahman

**Updated**: 2025-08-26T16:06:22Z

**Summary**: Leveraging Tennessee's 2005 Medicaid contraction, I study the impact of losing public health insurance on body weight and relevant health behaviors. Using Behavioral Risk Factor Surveillance System (BRFSS) data from 1997 to 2010, I estimate synthetic difference-in-differences models. The estimates suggest that the reform increased Body Mass Index by 0.38 points and the overweight or obesity prevalence (BMI$\geq$25) by $\sim$4\% among Tennessean childless adults. My findings -- a 21\% increase in the share of childless adults reporting ``poor'' health status (the lowest level on the five-point scale), a reduction in Medicaid-reimbursed utilization of pain and anti-inflammatory medications, and a reduction in participation in moderate physical activities -- suggest that worsening unmanaged health conditions may be a key pathway through which coverage loss affected weight gain. Additionally, my analysis offers practical guidance for conducting robust inference in single treated cluster settings with limited pre-treatment data.

**Link**: [arxiv](http://arxiv.org/abs/2508.19155v1),  [pdf](http://arxiv.org/pdf/2508.19155v1)

**Tags**: econ.GN q-fin.EC 



### GeNet: A Multimodal LLM-Based Co-Pilot for Network Topology and   Configuration
**Authors**: Beni Ifland, Elad Duani, Rubin Krief, Miro Ohana, Aviram Zilberman, Andres Murillo, Ofir Manor, Ortal Lavi, Hikichi Kenji, Asaf Shabtai, Yuval Elovici, Rami Puzis

**Updated**: 2025-08-26T15:53:39Z

**Summary**: Communication network engineering in enterprise environments is traditionally a complex, time-consuming, and error-prone manual process. Most research on network engineering automation has concentrated on configuration synthesis, often overlooking changes in the physical network topology. This paper introduces GeNet, a multimodal co-pilot for enterprise network engineers. GeNet is a novel framework that leverages a large language model (LLM) to streamline network design workflows. It uses visual and textual modalities to interpret and update network topologies and device configurations based on user intents. GeNet was evaluated on enterprise network scenarios adapted from Cisco certification exercises. Our results demonstrate GeNet's ability to interpret network topology images accurately, potentially reducing network engineers' efforts and accelerating network design processes in enterprise environments. Furthermore, we show the importance of precise topology understanding when handling intents that require modifications to the network's topology.

**Link**: [arxiv](http://arxiv.org/abs/2407.08249v2),  [pdf](http://arxiv.org/pdf/2407.08249v2)

**Tags**: cs.NI cs.AI 



### A Bag of Tricks for Efficient Implicit Neural Point Clouds
**Authors**: Florian Hahlbohm, Linus Franke, Leon Overkämping, Paula Wespe, Susana Castillo, Martin Eisemann, Marcus Magnor

**Updated**: 2025-08-26T15:49:10Z

**Summary**: Implicit Neural Point Cloud (INPC) is a recent hybrid representation that combines the expressiveness of neural fields with the efficiency of point-based rendering, achieving state-of-the-art image quality in novel view synthesis. However, as with other high-quality approaches that query neural networks during rendering, the practical usability of INPC is limited by comparatively slow rendering. In this work, we present a collection of optimizations that significantly improve both the training and inference performance of INPC without sacrificing visual fidelity. The most significant modifications are an improved rasterizer implementation, more effective sampling techniques, and the incorporation of pre-training for the convolutional neural network used for hole-filling. Furthermore, we demonstrate that points can be modeled as small Gaussians during inference to further improve quality in extrapolated, e.g., close-up views of the scene. We design our implementations to be broadly applicable beyond INPC and systematically evaluate each modification in a series of experiments. Our optimized INPC pipeline achieves up to 25% faster training, 2x faster rendering, and 20% reduced VRAM usage paired with slight image quality improvements.

**Link**: [arxiv](http://arxiv.org/abs/2508.19140v1),  [pdf](http://arxiv.org/pdf/2508.19140v1)

**Tags**: cs.GR cs.CV cs.LG 



### ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context
**Authors**: Victoria R. Li, Yida Chen, Naomi Saphra

**Updated**: 2025-08-26T15:44:42Z

**Summary**: While the biases of language models in production are extensively documented, the biases of their guardrails have been neglected. This paper studies how contextual information about the user influences the likelihood of an LLM to refuse to execute a request. By generating user biographies that offer ideological and demographic information, we find a number of biases in guardrail sensitivity on GPT-3.5. Younger, female, and Asian-American personas are more likely to trigger a refusal guardrail when requesting censored or illegal information. Guardrails are also sycophantic, refusing to comply with requests for a political position the user is likely to disagree with. We find that certain identity groups and seemingly innocuous information, e.g., sports fandom, can elicit changes in guardrail sensitivity similar to direct statements of political ideology. For each demographic category and even for American football team fandom, we find that ChatGPT appears to infer a likely political ideology and modify guardrail behavior accordingly.

**Link**: [arxiv](http://arxiv.org/abs/2407.06866v3),  [pdf](http://arxiv.org/pdf/2407.06866v3)

**Tags**: cs.CL cs.AI 



### ZeST: an LLM-based Zero-Shot Traversability Navigation for Unknown   Environments
**Authors**: Shreya Gummadi, Mateus V. Gasparino, Gianluca Capezzuto, Marcelo Becker, Girish Chowdhary

**Updated**: 2025-08-26T15:30:19Z

**Summary**: The advancement of robotics and autonomous navigation systems hinges on the ability to accurately predict terrain traversability. Traditional methods for generating datasets to train these prediction models often involve putting robots into potentially hazardous environments, posing risks to equipment and safety. To solve this problem, we present ZeST, a novel approach leveraging visual reasoning capabilities of Large Language Models (LLMs) to create a traversability map in real-time without exposing robots to danger. Our approach not only performs zero-shot traversability and mitigates the risks associated with real-world data collection but also accelerates the development of advanced navigation systems, offering a cost-effective and scalable solution. To support our findings, we present navigation results, in both controlled indoor and unstructured outdoor environments. As shown in the experiments, our method provides safer navigation when compared to other state-of-the-art methods, constantly reaching the final goal.

**Link**: [arxiv](http://arxiv.org/abs/2508.19131v1),  [pdf](http://arxiv.org/pdf/2508.19131v1)

**Tags**: cs.RO cs.AI cs.CV 



### A Survey on Data Selection for LLM Instruction Tuning
**Authors**: Bolin Zhang, Jiahao Wang, Qianlong Du, Jiajun Zhang, Zhiying Tu, Dianhui Chu

**Updated**: 2025-08-26T15:28:36Z

**Summary**: Instruction tuning is a vital step of training large language models (LLMs), so how to enhance the effect of instruction tuning has received increased attention. Existing works indicate that the quality of the dataset is more crucial than the quantity during instruction tuning of LLMs. Therefore, recently a lot of studies focus on exploring the methods of selecting high-quality subset from instruction datasets, aiming to reduce training costs and enhance the instruction-following capabilities of LLMs. This paper presents a comprehensive survey on data selection for LLM instruction tuning. Firstly, we introduce the wildly used instruction datasets. Then, we propose a new taxonomy of the data selection methods and provide a detailed introduction of recent advances, and the evaluation strategies and results of data selection methods are also elaborated in detail. Finally, we emphasize the open challenges and present new frontiers of this task.

**Link**: [arxiv](http://arxiv.org/abs/2402.05123v3),  [pdf](http://arxiv.org/pdf/2402.05123v3)

**Tags**: cs.CL 



### An Ontology-Driven Graph RAG for Legal Norms: A Hierarchical, Temporal,   and Deterministic Approach
**Authors**: Hudson de Martim

**Updated**: 2025-08-26T15:27:25Z

**Summary**: Retrieval-Augmented Generation (RAG) systems in the legal domain face a critical challenge: standard, flat-text retrieval is blind to the hierarchical, diachronic, and causal structure of law, leading to anachronistic and unreliable answers. This paper introduces an ontology-driven Graph RAG framework designed to overcome these limitations. We ground our knowledge graph in a formal, LRMoo-inspired model that distinguishes abstract legal Works from their versioned Expressions. We model temporal states as efficient aggregations that reuse the versioned expressions (CTVs) of unchanged components, and we reify legislative events as first-class Action nodes to make causality explicit and queryable. This structured backbone enables a unified, planner-guided query strategy that applies explicit policies to deterministically resolve complex requests for (i) point-in-time retrieval, (ii) hierarchical impact analysis, and (iii) auditable provenance reconstruction. Through a case study on the Brazilian Constitution, we demonstrate how this approach provides a verifiable, temporally-correct substrate for LLMs, enabling higher-order analytical capabilities while drastically reducing the risk of factual errors. The result is a practical framework for building more trustworthy and explainable legal AI systems.

**Link**: [arxiv](http://arxiv.org/abs/2505.00039v4),  [pdf](http://arxiv.org/pdf/2505.00039v4)

**Tags**: cs.CL cs.AI cs.IR 



### Exploring the Robustness of Language Models for Tabular Question   Answering via Attention Analysis
**Authors**: Kushal Raj Bhandari, Sixue Xing, Soham Dan, Jianxi Gao

**Updated**: 2025-08-26T15:27:18Z

**Summary**: Large Language Models (LLMs), already shown to ace various unstructured text comprehension tasks, have also remarkably been shown to tackle table (structured) comprehension tasks without specific training. Building on earlier studies of LLMs for tabular tasks, we probe how in-context learning (ICL), model scale, instruction tuning, and domain bias affect Tabular QA (TQA) robustness by testing LLMs, under diverse augmentations and perturbations, on diverse domains: Wikipedia-based $\textbf{WTQ}$, financial $\textbf{TAT-QA}$, and scientific $\textbf{SCITAB}$. Although instruction tuning and larger, newer LLMs deliver stronger, more robust TQA performance, data contamination and reliability issues, especially on $\textbf{WTQ}$, remain unresolved. Through an in-depth attention analysis, we reveal a strong correlation between perturbation-induced shifts in attention dispersion and the drops in performance, with sensitivity peaking in the model's middle layers. We highlight the need for improved interpretable methodologies to develop more reliable LLMs for table comprehension. Through an in-depth attention analysis, we reveal a strong correlation between perturbation-induced shifts in attention dispersion and performance drops, with sensitivity peaking in the model's middle layers. Based on these findings, we argue for the development of structure-aware self-attention mechanisms and domain-adaptive processing techniques to improve the transparency, generalization, and real-world reliability of LLMs on tabular data.

**Link**: [arxiv](http://arxiv.org/abs/2406.12719v4),  [pdf](http://arxiv.org/pdf/2406.12719v4)

**Tags**: cs.CL cs.AI 



### StagFormer: Time Staggering Transformer Decoding for RunningLayers In   Parallel
**Authors**: Dylan Cutler, Arun Kandoor, Nishanth Dikkala, Nikunj Saunshi, Xin Wang, Rina Panigrahy

**Updated**: 2025-08-26T15:21:49Z

**Summary**: Decoding in a Transformer based language model is inherently sequential as a token's embedding needs to pass through all the layers in the network before the generation of the next token can begin. In this work, we propose a new architecture StagFormer (Staggered Transformer), which staggers execution along the sequence axis and thereby enables parallelizing the decoding process along the depth of the model. We achieve this by breaking the dependency of the token representation at time step $i$ in layer $l$ upon the representations of tokens until time step $i$ from layer $l-1$. Instead, we stagger the execution and only allow a dependency on token representations until time step $i-1$. The later sections of the Transformer still get access to the "rich" representations from the prior section but only from those token positions which are one time step behind. StagFormer allows for different sections of the model to be executed in parallel yielding a potential speedup in decoding while being quality neutral in our simulations. We also explore many natural extensions of this idea. We present how weight-sharing across the different sections being staggered can be more practical in settings with limited memory. We explore the efficacy of using a bounded window attention to pass information from one section to another which helps drive further latency gains for some applications. We also explore the scalability of the staggering idea over more than 2 sections of the Transformer. Finally, we show how one can approximate a recurrent model during inference using weight-sharing. This variant can lead to substantial gains in quality for short generations while being neutral in its latency impact.

**Link**: [arxiv](http://arxiv.org/abs/2501.15665v2),  [pdf](http://arxiv.org/pdf/2501.15665v2)

**Tags**: cs.LG cs.AI 



### MCI-GRU: Stock Prediction Model Based on Multi-Head Cross-Attention and   Improved GRU
**Authors**: Peng Zhu, Yuante Li, Yifan Hu, Sheng Xiang, Qinyuan Liu, Dawei Cheng, Yuqi Liang

**Updated**: 2025-08-26T15:18:44Z

**Summary**: As financial markets grow increasingly complex in the big data era, accurate stock prediction has become more critical. Traditional time series models, such as GRUs, have been widely used but often struggle to capture the intricate nonlinear dynamics of markets, particularly in the flexible selection and effective utilization of key historical information. Recently, methods like Graph Neural Networks and Reinforcement Learning have shown promise in stock prediction but require high data quality and quantity, and they tend to exhibit instability when dealing with data sparsity and noise. Moreover, the training and inference processes for these models are typically complex and computationally expensive, limiting their broad deployment in practical applications. Existing approaches also generally struggle to capture unobservable latent market states effectively, such as market sentiment and expectations, microstructural factors, and participant behavior patterns, leading to an inadequate understanding of market dynamics and subsequently impact prediction accuracy. To address these challenges, this paper proposes a stock prediction model, MCI-GRU, based on a multi-head cross-attention mechanism and an improved GRU. First, we enhance the GRU model by replacing the reset gate with an attention mechanism, thereby increasing the model's flexibility in selecting and utilizing historical information. Second, we design a multi-head cross-attention mechanism for learning unobservable latent market state representations, which are further enriched through interactions with both temporal features and cross-sectional features. Finally, extensive experiments on four main stock markets show that the proposed method outperforms SOTA techniques across multiple metrics. Additionally, its successful application in real-world fund management operations confirms its effectiveness and practicality.

**Link**: [arxiv](http://arxiv.org/abs/2410.20679v3),  [pdf](http://arxiv.org/pdf/2410.20679v3)

**Tags**: q-fin.ST cs.LG q-fin.CP 



### SecureV2X: An Efficient and Privacy-Preserving System for   Vehicle-to-Everything (V2X) Applications
**Authors**: Joshua Lee, Ali Arastehfard, Weiran Liu, Xuegang Ban, Yuan Hong

**Updated**: 2025-08-26T15:17:46Z

**Summary**: Autonomous driving and V2X technologies have developed rapidly in the past decade, leading to improved safety and efficiency in modern transportation. These systems interact with extensive networks of vehicles, roadside infrastructure, and cloud resources to support their machine learning capabilities. However, the widespread use of machine learning in V2X systems raises issues over the privacy of the data involved. This is particularly concerning for smart-transit and driver safety applications which can implicitly reveal user locations or explicitly disclose medical data such as EEG signals. To resolve these issues, we propose SecureV2X, a scalable, multi-agent system for secure neural network inferences deployed between the server and each vehicle. Under this setting, we study two multi-agent V2X applications: secure drowsiness detection, and secure red-light violation detection. Our system achieves strong performance relative to baselines, and scales efficiently to support a large number of secure computation interactions simultaneously. For instance, SecureV2X is $9.4 \times$ faster, requires $143\times$ fewer computational rounds, and involves $16.6\times$ less communication on drowsiness detection compared to other secure systems. Moreover, it achieves a runtime nearly $100\times$ faster than state-of-the-art benchmarks in object detection tasks for red light violation detection.

**Link**: [arxiv](http://arxiv.org/abs/2508.19115v1),  [pdf](http://arxiv.org/pdf/2508.19115v1)

**Tags**: cs.CR cs.AI E.3; I.2.6; I.5.1; F.1.2 



### DELIVER: A System for LLM-Guided Coordinated Multi-Robot Pickup and   Delivery using Voronoi-Based Relay Planning
**Authors**: Alkesh K. Srivastava, Jared Michael Levin, Alexander Derrico, Philip Dames

**Updated**: 2025-08-26T15:17:08Z

**Summary**: We present DELIVER (Directed Execution of Language-instructed Item Via Engineered Relay), a fully integrated framework for cooperative multi-robot pickup and delivery driven by natural language commands. DELIVER unifies natural language understanding, spatial decomposition, relay planning, and motion execution to enable scalable, collision-free coordination in real-world settings. Given a spoken or written instruction, a lightweight instance of LLaMA3 interprets the command to extract pickup and delivery locations. The environment is partitioned using a Voronoi tessellation to define robot-specific operating regions. Robots then compute optimal relay points along shared boundaries and coordinate handoffs. A finite-state machine governs each robot's behavior, enabling robust execution. We implement DELIVER on the MultiTRAIL simulation platform and validate it in both ROS2-based Gazebo simulations and real-world hardware using TurtleBot3 robots. Empirical results show that DELIVER maintains consistent mission cost across varying team sizes while reducing per-agent workload by up to 55% compared to a single-agent system. Moreover, the number of active relay agents remains low even as team size increases, demonstrating the system's scalability and efficient agent utilization. These findings underscore DELIVER's modular and extensible architecture for language-guided multi-robot coordination, advancing the frontiers of cyber-physical system integration.

**Link**: [arxiv](http://arxiv.org/abs/2508.19114v1),  [pdf](http://arxiv.org/pdf/2508.19114v1)

**Tags**: cs.RO cs.MA 



### Hybrid Deep Searcher: Integrating Parallel and Sequential Search   Reasoning
**Authors**: Dayoon Ko, Jihyuk Kim, Haeju Park, Sohyeon Kim, Dahyun Lee, Yongrae Jo, Gunhee Kim, Moontae Lee, Kyungjae Lee

**Updated**: 2025-08-26T15:15:17Z

**Summary**: Large reasoning models (LRMs) have demonstrated strong performance in complex, multi-step reasoning tasks. Existing methods enhance LRMs by sequentially integrating external knowledge retrieval; models iteratively generate queries, retrieve external information, and progressively reason over this information. However, purely sequential querying increases inference latency and context length, diminishing coherence and potentially reducing accuracy. To address these limitations, we introduce HDS-QA (Hybrid Deep Search QA), a synthetic dataset automatically generated from Natural Questions, explicitly designed to train LRMs to distinguish parallelizable from sequential queries. HDS-QA comprises hybrid-hop questions that combine parallelizable independent subqueries (executable simultaneously) and sequentially dependent subqueries (requiring step-by-step resolution), along with synthetic reasoning-querying-retrieval paths involving parallel queries. We fine-tune an LRM using HDS-QA, naming the model HybridDeepSearcher, which outperforms state-of-the-art baselines across multiple benchmarks, notably achieving +15.9 and +11.5 F1 on FanOutQA and a subset of BrowseComp, respectively, both requiring comprehensive and exhaustive search. Experimental results highlight two key advantages: HybridDeepSearcher reaches comparable accuracy with fewer search turns, significantly reducing inference latency, and it effectively scales as more turns are permitted. These results demonstrate the efficiency, scalability, and effectiveness of explicitly training LRMs to leverage hybrid parallel and sequential querying.

**Link**: [arxiv](http://arxiv.org/abs/2508.19113v1),  [pdf](http://arxiv.org/pdf/2508.19113v1)

**Tags**: cs.AI 



### Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary   Perception in LVLMs
**Authors**: Zhikai Ding, Shiyu Ni, Keping Bi

**Updated**: 2025-08-26T15:14:19Z

**Summary**: Large vision-language models (LVLMs) demonstrate strong visual question answering (VQA) capabilities but are shown to hallucinate. A reliable model should perceive its knowledge boundaries-knowing what it knows and what it does not. This paper investigates LVLMs' perception of their knowledge boundaries by evaluating three types of confidence signals: probabilistic confidence, answer consistency-based confidence, and verbalized confidence. Experiments on three LVLMs across three VQA datasets show that, although LVLMs possess a reasonable perception level, there is substantial room for improvement. Among the three confidences, probabilistic and consistency-based signals are more reliable indicators, while verbalized confidence often leads to overconfidence. To enhance LVLMs' perception, we adapt several established confidence calibration methods from Large Language Models (LLMs) and propose three effective methods. Additionally, we compare LVLMs with their LLM counterparts, finding that jointly processing visual and textual inputs decreases question-answering performance but reduces confidence, resulting in an improved perception level compared to LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2508.19111v1),  [pdf](http://arxiv.org/pdf/2508.19111v1)

**Tags**: cs.CL 



### Label Set Optimization via Activation Distribution Kurtosis for   Zero-shot Classification with Generative Models
**Authors**: Yue Li, Zhixue Zhao, Carolina Scarton

**Updated**: 2025-08-26T15:09:51Z

**Summary**: In-context learning (ICL) performance is highly sensitive to prompt design, yet the impact of class label options (e.g. lexicon or order) in zero-shot classification remains underexplored. This study proposes LOADS (Label set Optimization via Activation Distribution kurtosiS), a post-hoc method for selecting optimal label sets in zero-shot ICL with large language models (LLMs). LOADS is built upon the observations in our empirical analysis, the first to systematically examine how label option design (i.e., lexical choice, order, and elaboration) impacts classification performance. This analysis shows that the lexical choice of the labels in the prompt (such as agree vs. support in stance classification) plays an important role in both model performance and model's sensitivity to the label order. A further investigation demonstrates that optimal label words tend to activate fewer outlier neurons in LLMs' feed-forward networks. LOADS then leverages kurtosis to measure the neuron activation distribution for label selection, requiring only a single forward pass without gradient propagation or labelled data. The LOADS-selected label words consistently demonstrate effectiveness for zero-shot ICL across classification tasks, datasets, models and languages, achieving maximum performance gain from 0.54 to 0.76 compared to the conventional approach of using original dataset label words.

**Link**: [arxiv](http://arxiv.org/abs/2410.19195v2),  [pdf](http://arxiv.org/pdf/2410.19195v2)

**Tags**: cs.CL 



### Learning in Repeated Multi-Objective Stackelberg Games with Payoff   Manipulation
**Authors**: Phurinut Srisawad, Juergen Branke, Long Tran-Thanh

**Updated**: 2025-08-26T15:07:29Z

**Summary**: We study payoff manipulation in repeated multi-objective Stackelberg games, where a leader may strategically influence a follower's deterministic best response, e.g., by offering a share of their own payoff. We assume that the follower's utility function, representing preferences over multiple objectives, is unknown but linear, and its weight parameter must be inferred through interaction. This introduces a sequential decision-making challenge for the leader, who must balance preference elicitation with immediate utility maximisation. We formalise this problem and propose manipulation policies based on expected utility (EU) and long-term expected utility (longEU), which guide the leader in selecting actions and offering incentives that trade off short-term gains with long-term impact. We prove that under infinite repeated interactions, longEU converges to the optimal manipulation. Empirical results across benchmark environments demonstrate that our approach improves cumulative leader utility while promoting mutually beneficial outcomes, all without requiring explicit negotiation or prior knowledge of the follower's utility function.

**Link**: [arxiv](http://arxiv.org/abs/2508.14705v2),  [pdf](http://arxiv.org/pdf/2508.14705v2)

**Tags**: cs.GT cs.AI 



### CLEAR: Continuous Latent Autoregressive Modeling for High-quality and   Low-latency Speech Synthesis
**Authors**: Chun Yat Wu, Jiajun Deng, Guinan Li, Qiuqiang Kong, Simon Lui

**Updated**: 2025-08-26T14:59:30Z

**Summary**: Autoregressive (AR) language models have emerged as powerful solutions for zero-shot text-to-speech (TTS) synthesis, capable of generating natural speech from a few seconds of audio prompts. However, conventional AR-based TTS systems relying on discrete audio tokens face the challenge of lossy compression during tokenization, requiring longer discrete token sequences to capture the same information as continuous ones, which adds inference latency and complicates AR modeling. To address this challenge, this paper proposes the Continuous Latent Autoregressive model (CLEAR), a unified zero-shot TTS framework that directly models continuous audio representations. More specifically, CLEAR introduces an enhanced variational autoencoder with shortcut connections, which achieves a high compression ratio to map waveforms into compact continuous latents. A lightweight MLP-based rectified flow head that operates independently for each hidden state is presented to model the continuous latent probability distribution, and trained jointly with the AR model within a single-stage framework. Experiments show that the proposed zero-shot CLEAR TTS can synthesize high-quality speech with low latency. Compared to state-of-the-art (SOTA) TTS models, CLEAR delivers competitive performance in robustness, speaker similarity and naturalness, while offering a lower real-time factor (RTF). In particular, CLEAR achieves SOTA results on the LibriSpeech test-clean dataset, with a word error rate of 1.88\% and an RTF of 0.29. Moreover, CLEAR facilitates streaming speech synthesis with a first-frame delay of 96ms, while maintaining high-quality speech synthesis.

**Link**: [arxiv](http://arxiv.org/abs/2508.19098v1),  [pdf](http://arxiv.org/pdf/2508.19098v1)

**Tags**: eess.AS 



### Reasoning LLMs in the Medical Domain: A Literature Survey
**Authors**: Armin Berger, Sarthak Khanna, David Berghaus, Rafet Sifa

**Updated**: 2025-08-26T14:59:19Z

**Summary**: The emergence of advanced reasoning capabilities in Large Language Models (LLMs) marks a transformative development in healthcare applications. Beyond merely expanding functional capabilities, these reasoning mechanisms enhance decision transparency and explainability-critical requirements in medical contexts. This survey examines the transformation of medical LLMs from basic information retrieval tools to sophisticated clinical reasoning systems capable of supporting complex healthcare decisions. We provide a thorough analysis of the enabling technological foundations, with a particular focus on specialized prompting techniques like Chain-of-Thought and recent breakthroughs in Reinforcement Learning exemplified by DeepSeek-R1. Our investigation evaluates purpose-built medical frameworks while also examining emerging paradigms such as multi-agent collaborative systems and innovative prompting architectures. The survey critically assesses current evaluation methodologies for medical validation and addresses persistent challenges in field interpretation limitations, bias mitigation strategies, patient safety frameworks, and integration of multimodal clinical data. Through this survey, we seek to establish a roadmap for developing reliable LLMs that can serve as effective partners in clinical practice and medical research.

**Link**: [arxiv](http://arxiv.org/abs/2508.19097v1),  [pdf](http://arxiv.org/pdf/2508.19097v1)

**Tags**: cs.AI 



### Trustworthy Agents for Electronic Health Records through Confidence   Estimation
**Authors**: Yongwoo Song, Minbyul Jeong, Mujeen Sung

**Updated**: 2025-08-26T14:59:04Z

**Summary**: Large language models (LLMs) show promise for extracting information from Electronic Health Records (EHR) and supporting clinical decisions. However, deployment in clinical settings faces challenges due to hallucination risks. We propose Hallucination Controlled Accuracy at k% (HCAcc@k%), a novel metric quantifying the accuracy-reliability trade-off at varying confidence thresholds. We introduce TrustEHRAgent, a confidence-aware agent incorporating stepwise confidence estimation for clinical question answering. Experiments on MIMIC-III and eICU datasets show TrustEHRAgent outperforms baselines under strict reliability constraints, achieving improvements of 44.23%p and 25.34%p at HCAcc@70% while baseline methods fail at these thresholds. These results highlight limitations of traditional accuracy metrics in evaluating healthcare AI agents. Our work contributes to developing trustworthy clinical agents that deliver accurate information or transparently express uncertainty when confidence is low.

**Link**: [arxiv](http://arxiv.org/abs/2508.19096v1),  [pdf](http://arxiv.org/pdf/2508.19096v1)

**Tags**: cs.AI 



### Accretion onto supermassive and intermediate mass black holes in   cosmological simulations
**Authors**: Rainer Weinberger, Aklant Bhowmick, Laura Blecha, Greg Bryan, Johannes Buchner, Lars Hernquist, Julie Hlavacek-Larrondo, Volker Springel

**Updated**: 2025-08-26T14:51:49Z

**Summary**: Accretion is the dominant contribution to the cosmic massive black hole density in the Universe today. Yet, modelling it in cosmological simulations is challenging due to the dynamic range involved, as well as the theoretical uncertainties of the underlying mechanisms driving accretion from galactic to black hole horizon scales. We present a simple, flexible parametrization for gas inflows onto massive black holes in order to manage this uncertainty in large-volume cosmological simulations. This is done as part of the "Learning the Universe'' collaboration, which aims to jointly infer the initial conditions and physical processes governing the evolution of the Universe using a Bayesian forward-modelling approach. To allow such a forward-modelling, we update the prescription for accretion with a two-parameter free-fall based inflow estimate that allows for a radius-dependent inflow rate and add a simple model for unresolved accretion disks. We use uniform resolution cosmological hydrodynamical simulations and the IllustrisTNG framework to study the massive black hole population and its dependence on the introduced model parameters. Once the parameters of the accretion formula are chosen to result in a roughly similar redshift zero black hole mass density, the differences caused by the details in the accretion formula are moderate in the supermassive black hole regime, indicating that it is difficult to distinguish between accretion mechanisms based on luminous active galactic nuclei powered by supermassive black holes. Applying the same models to intermediate mass black holes at high redshift, however, reveals significantly different accretion rates in high redshift, moderate luminosity active galactic nuclei and different frequencies and mass distributions of intermediate mass black hole mergers for the same black hole formation model.

**Link**: [arxiv](http://arxiv.org/abs/2502.13241v2),  [pdf](http://arxiv.org/pdf/2502.13241v2)

**Tags**: astro-ph.GA astro-ph.CO 



### It's All About In-Context Learning! Teaching Extremely Low-Resource   Languages to LLMs
**Authors**: Yue Li, Zhixue Zhao, Carolina Scarton

**Updated**: 2025-08-26T14:51:10Z

**Summary**: Extremely low-resource languages, especially those written in rare scripts, as shown in Figure 1, remain largely unsupported by large language models (LLMs). This is due in part to compounding factors such as the lack of training data. This paper delivers the first comprehensive analysis of whether LLMs can acquire such languages purely via in-context learning (ICL), with or without auxiliary alignment signals, and how these methods compare to parameter-efficient fine-tuning (PEFT). We systematically evaluate 20 under-represented languages across three state-of-the-art multilingual LLMs. Our findings highlight the limitation of PEFT when both language and its script are extremely under-represented by the LLM. In contrast, zero-shot ICL with language alignment is impressively effective on extremely low-resource languages, while few-shot ICL or PEFT is more beneficial for languages relatively better represented by LLMs. For LLM practitioners working on extremely low-resource languages, we summarise guidelines grounded by our results on adapting LLMs to low-resource languages, e.g., avoiding fine-tuning a multilingual model on languages of unseen scripts.

**Link**: [arxiv](http://arxiv.org/abs/2508.19089v1),  [pdf](http://arxiv.org/pdf/2508.19089v1)

**Tags**: cs.CL 



### Steerable Scene Generation with Post Training and Inference-Time Search
**Authors**: Nicholas Pfaff, Hongkai Dai, Sergey Zakharov, Shun Iwase, Russ Tedrake

**Updated**: 2025-08-26T14:49:48Z

**Summary**: Training robots in simulation requires diverse 3D scenes that reflect the specific challenges of downstream tasks. However, scenes that satisfy strict task requirements, such as high-clutter environments with plausible spatial arrangement, are rare and costly to curate manually. Instead, we generate large-scale scene data using procedural models that approximate realistic environments for robotic manipulation, and adapt it to task-specific goals. We do this by training a unified diffusion-based generative model that predicts which objects to place from a fixed asset library, along with their SE(3) poses. This model serves as a flexible scene prior that can be adapted using reinforcement learning-based post training, conditional generation, or inference-time search, steering generation toward downstream objectives even when they differ from the original data distribution. Our method enables goal-directed scene synthesis that respects physical feasibility and scales across scene types. We introduce a novel MCTS-based inference-time search strategy for diffusion models, enforce feasibility via projection and simulation, and release a dataset of over 44 million SE(3) scenes spanning five diverse environments. Website with videos, code, data, and model weights: https://steerable-scene-generation.github.io/

**Link**: [arxiv](http://arxiv.org/abs/2505.04831v2),  [pdf](http://arxiv.org/pdf/2505.04831v2)

**Tags**: cs.RO cs.GR cs.LG 



### APT-LLM: Exploiting Arbitrary-Precision Tensor Core Computing for LLM   Acceleration
**Authors**: Shaobo Ma, Chao Fang, Haikuo Shao, Zhongfeng Wang

**Updated**: 2025-08-26T14:48:29Z

**Summary**: Large language models (LLMs) have revolutionized AI applications, yet their enormous computational demands severely limit deployment and real-time performance. Quantization methods can help reduce computational costs, however, attaining the extreme efficiency associated with ultra-low-bit quantized LLMs at arbitrary precision presents challenges on GPUs. This is primarily due to the limited support for GPU Tensor Cores, inefficient memory management, and inflexible kernel optimizations. To tackle these challenges, we propose a comprehensive acceleration scheme for arbitrary precision LLMs, namely APT-LLM. Firstly, we introduce a novel data format, bipolar-INT, which allows for efficient and lossless conversion with signed INT, while also being more conducive to parallel computation. We also develop a matrix multiplication (MatMul) method allowing for arbitrary precision by dismantling and reassembling matrices at the bit level. This method provides flexible precision and optimizes the utilization of GPU Tensor Cores. In addition, we propose a memory management system focused on data recovery, which strategically employs fast shared memory to substantially increase kernel execution speed and reduce memory access latency. Finally, we develop a kernel mapping method that dynamically selects the optimal configurable hyperparameters of kernels for varying matrix sizes, enabling optimal performance across different LLM architectures and precision settings. In LLM inference, APT-LLM achieves up to a 3.99$\times$ speedup compared to FP16 baselines and a 2.16$\times$ speedup over NVIDIA CUTLASS INT4 acceleration on RTX 3090. On RTX 4090 and H800, APT-LLM achieves up to 2.44$\times$ speedup over FP16 and 1.65$\times$ speedup over CUTLASS integer baselines.

**Link**: [arxiv](http://arxiv.org/abs/2508.19087v1),  [pdf](http://arxiv.org/pdf/2508.19087v1)

**Tags**: cs.LG cs.AI cs.AR 



### Federated Fine-Tuning of Sparsely-Activated Large Language Models on   Resource-Constrained Devices
**Authors**: Fahao Chen, Jie Wan, Peng Li, Zhou Su, Dongxiao Yu

**Updated**: 2025-08-26T14:39:00Z

**Summary**: Federated fine-tuning of Mixture-of-Experts (MoE)-based large language models (LLMs) is challenging due to their massive computational requirements and the resource constraints of participants. Existing working attempts to fill this gap through model quantization, computation offloading, or expert pruning. However, they cannot achieve desired performance due to impractical system assumptions and a lack of consideration for MoE-specific characteristics. In this paper, we propose FLUX, a system designed to enable federated fine-tuning of MoE-based LLMs across participants with constrained computing resources (e.g., consumer-grade GPUs), aiming to minimize time-to-accuracy. FLUX introduces three key innovations: (1) quantization-based local profiling to estimate expert activation with minimal overhead, (2) adaptive layer-aware expert merging to reduce resource consumption while preserving accuracy, and (3) dynamic expert role assignment using an exploration-exploitation strategy to balance tuning and non-tuning experts. Extensive experiments on LLaMA-MoE and DeepSeek-MoE with multiple benchmark datasets demonstrate that FLUX significantly outperforms existing methods, achieving up to 4.75X speedup in time-to-accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2508.19078v1),  [pdf](http://arxiv.org/pdf/2508.19078v1)

**Tags**: cs.DC cs.AI 



### HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive   Global-Local Guidance
**Authors**: Ziyue Li, Yuan Chang, Gaihong Yu, Xiaoqiu Le

**Updated**: 2025-08-26T14:37:48Z

**Summary**: Large language model (LLM)-based agents have demonstrated remarkable capabilities in decision-making tasks, but struggle significantly with complex, long-horizon planning scenarios. This arises from their lack of macroscopic guidance, causing disorientation and failures in complex tasks, as well as insufficient continuous oversight during execution, rendering them unresponsive to environmental changes and prone to deviations. To tackle these challenges, we introduce HiPlan, a hierarchical planning framework that provides adaptive global-local guidance to boost LLM-based agents'decision-making. HiPlan decomposes complex tasks into milestone action guides for general direction and step-wise hints for detailed actions. During the offline phase, we construct a milestone library from expert demonstrations, enabling structured experience reuse by retrieving semantically similar tasks and milestones. In the execution phase, trajectory segments from past milestones are dynamically adapted to generate step-wise hints that align current observations with the milestone objectives, bridging gaps and correcting deviations. Extensive experiments across two challenging benchmarks demonstrate that HiPlan substantially outperforms strong baselines, and ablation studies validate the complementary benefits of its hierarchical components.

**Link**: [arxiv](http://arxiv.org/abs/2508.19076v1),  [pdf](http://arxiv.org/pdf/2508.19076v1)

**Tags**: cs.CL cs.AI 



### SmartBench: Is Your LLM Truly a Good Chinese Smartphone Assistant?
**Authors**: Xudong Lu, Haohao Gao, Renshou Wu, Shuai Ren, Xiaoxin Chen, Hongsheng Li, Fangyuan Li

**Updated**: 2025-08-26T14:34:00Z

**Summary**: Large Language Models (LLMs) have become integral to daily life, especially advancing as intelligent assistants through on-device deployment on smartphones. However, existing LLM evaluation benchmarks predominantly focus on objective tasks like mathematics and coding in English, which do not necessarily reflect the practical use cases of on-device LLMs in real-world mobile scenarios, especially for Chinese users. To address these gaps, we introduce SmartBench, the first benchmark designed to evaluate the capabilities of on-device LLMs in Chinese mobile contexts. We analyze functionalities provided by representative smartphone manufacturers and divide them into five categories: text summarization, text Q&A, information extraction, content creation, and notification management, further detailed into 20 specific tasks. For each task, we construct high-quality datasets comprising 50 to 200 question-answer pairs that reflect everyday mobile interactions, and we develop automated evaluation criteria tailored for these tasks. We conduct comprehensive evaluations of on-device LLMs and MLLMs using SmartBench and also assess their performance after quantized deployment on real smartphone NPUs. Our contributions provide a standardized framework for evaluating on-device LLMs in Chinese, promoting further development and optimization in this critical area. Code and data will be available at https://github.com/vivo-ai-lab/SmartBench.

**Link**: [arxiv](http://arxiv.org/abs/2503.06029v2),  [pdf](http://arxiv.org/pdf/2503.06029v2)

**Tags**: cs.CL cs.LG 



### An LLM-powered Natural-to-Robotic Language Translation Framework with   Correctness Guarantees
**Authors**: ZhenDong Chen, ZhanShang Nie, ShiXing Wan, JunYi Li, YongTian Cheng, Shuai Zhao

**Updated**: 2025-08-26T14:32:49Z

**Summary**: The Large Language Models (LLM) are increasingly being deployed in robotics to generate robot control programs for specific user tasks, enabling embodied intelligence. Existing methods primarily focus on LLM training and prompt design that utilize LLMs to generate executable programs directly from user tasks in natural language. However, due to the inconsistency of the LLMs and the high complexity of the tasks, such best-effort approaches often lead to tremendous programming errors in the generated code, which significantly undermines the effectiveness especially when the light-weight LLMs are applied. This paper introduces a natural-robotic language translation framework that (i) provides correctness verification for generated control programs and (ii) enhances the performance of LLMs in program generation via feedback-based fine-tuning for the programs. To achieve this, a Robot Skill Language (RSL) is proposed to abstract away from the intricate details of the control programs, bridging the natural language tasks with the underlying robot skills. Then, the RSL compiler and debugger are constructed to verify RSL programs generated by the LLM and provide error feedback to the LLM for refining the outputs until being verified by the compiler. This provides correctness guarantees for the LLM-generated programs before being offloaded to the robots for execution, significantly enhancing the effectiveness of LLM-powered robotic applications. Experiments demonstrate NRTrans outperforms the existing method under a range of LLMs and tasks, and achieves a high success rate for light-weight LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2508.19074v1),  [pdf](http://arxiv.org/pdf/2508.19074v1)

**Tags**: cs.RO cs.AI cs.PL 



### Can Structured Templates Facilitate LLMs in Tackling Harder Tasks? : An   Exploration of Scaling Laws by Difficulty
**Authors**: Zhichao Yang, Zhaoxin Fan, Gen Li, Yuanze Hu, Xinyu Wang, Ye Qiu, Xin Wang, Yifan Sun, Wenjun Wu

**Updated**: 2025-08-26T14:26:32Z

**Summary**: Structured, procedural reasoning is essential for Large Language Models (LLMs), especially in mathematics. While post-training methods have improved LLM performance, they still fall short in capturing deep procedural logic on complex tasks. To tackle the issue, in this paper, we first investigate this limitation and uncover a novel finding: a Scaling Law by Difficulty, which reveals that model performance follows a U-shaped curve with respect to training data complexity -- excessive low-difficulty data impedes abstraction, while high-difficulty data significantly enhances reasoning ability. Motivated by this, we propose the Structured Solution Template (SST) framework, which uses solution templates and a curriculum of varied difficulty to explicitly teach procedural reasoning. Specifically, SST comprises (1) fine-tuning with structured solution-template chains and dynamically weighted loss to prioritize procedural logic, (2) prompt-time injection of solution templates as cognitive scaffolds to guide inference, and (3) integrated curriculum fine-tuning that explicitly teaches the model to self-plan - execute - self-correct. Experiments on GSM8K, AIME24, and new Dynamic En benchmark show that SST significantly improves both accuracy and efficiency, especially on harder problems.

**Link**: [arxiv](http://arxiv.org/abs/2508.19069v1),  [pdf](http://arxiv.org/pdf/2508.19069v1)

**Tags**: cs.AI 



### Is ozone a reliable proxy for molecular oxygen? III. The impact of   CH$_4$ on the O$_2$-O$_3$ relationship for Earth-like atmospheres
**Authors**: Thea Kozakis, João M. Mendonça, Lars A. Buchhave, Luisa M. Lara

**Updated**: 2025-08-26T14:21:28Z

**Summary**: In the search for life in the Universe, molecular oxygen (O$_2$) combined with a reducing species, such as methane (CH$_4$), is considered a promising disequilibrium biosignature. In cases where it would be difficult or impossible to detect O$_2$ (e.g., mid-IR or low O$_2$ levels), it has been suggested that ozone (O$_3$), the photochemical product of O$_2$, could be used as a proxy for determining the abundance of O$_2$. As the O$_2$-O$_3$ relationship is nonlinear, the goal of this series of papers is to explore how it would change for different host stars and atmospheric compositions and learning how to use O$_3$ to infer O$_2$. We used photochemistry and climate modeling to further explore the O$_2$-O$_3$ relationship by modeling Earth-like planets with the present atmospheric level (PAL) of O$_2$ between 0.01% to 150% along with high and low CH$_4$ abundances of 1000% and 10% PAL, respectively. Methane is of interest not only because it is a biosignature, but also the source of hydrogen atoms for hydrogen oxide (HO$_x$) which destroys O$_3$ through catalytic cycles and acts as a catalyst for the smog mechanism of O$_3$ formation in the lower atmosphere. We find varying CH$_4$ causes changes to the O$_2$-O$_3$ relationship in ways that are highly dependent on both host star and O$_2$ abundance. A striking result for high CH$_4$ models in high O$_2$ atmospheres around hotter hosts is that enough CH$_4$ is efficiently converted into H$_2$O to significantly impact stratospheric temperatures, and therefore the formation/destruction rates of O$_3$. Changes in HO$_x$ also influenced the HO$_x$ catalytic cycle and smog O$_3$, causing variations in harmful UV reaching the surface as well as changes in the 9.6~$\mu$m O$_3$ feature in emission spectra. This demonstrates the need to explore the O$_2$-O$_3$ relationship in order to use O$_3$ as a reliable proxy for O$_2$ in future observations.

**Link**: [arxiv](http://arxiv.org/abs/2508.19062v1),  [pdf](http://arxiv.org/pdf/2508.19062v1)

**Tags**: astro-ph.EP 



### No Label Left Behind: A Unified Surface Defect Detection Model for all   Supervision Regimes
**Authors**: Blaž Rolih, Matic Fučka, Danijel Skočaj

**Updated**: 2025-08-26T14:20:21Z

**Summary**: Surface defect detection is a critical task across numerous industries, aimed at efficiently identifying and localising imperfections or irregularities on manufactured components. While numerous methods have been proposed, many fail to meet industrial demands for high performance, efficiency, and adaptability. Existing approaches are often constrained to specific supervision scenarios and struggle to adapt to the diverse data annotations encountered in real-world manufacturing processes, such as unsupervised, weakly supervised, mixed supervision, and fully supervised settings. To address these challenges, we propose SuperSimpleNet, a highly efficient and adaptable discriminative model built on the foundation of SimpleNet. SuperSimpleNet incorporates a novel synthetic anomaly generation process, an enhanced classification head, and an improved learning procedure, enabling efficient training in all four supervision scenarios, making it the first model capable of fully leveraging all available data annotations. SuperSimpleNet sets a new standard for performance across all scenarios, as demonstrated by its results on four challenging benchmark datasets. Beyond accuracy, it is very fast, achieving an inference time below 10 ms. With its ability to unify diverse supervision paradigms while maintaining outstanding speed and reliability, SuperSimpleNet represents a promising step forward in addressing real-world manufacturing challenges and bridging the gap between academic research and industrial applications. Code: https://github.com/blaz-r/SuperSimpleNet

**Link**: [arxiv](http://arxiv.org/abs/2508.19060v1),  [pdf](http://arxiv.org/pdf/2508.19060v1)

**Tags**: cs.CV cs.AI 



### An Agentic System for Rare Disease Diagnosis with Traceable Reasoning
**Authors**: Weike Zhao, Chaoyi Wu, Yanjie Fan, Xiaoman Zhang, Pengcheng Qiu, Yuze Sun, Xiao Zhou, Yanfeng Wang, Xin Sun, Ya Zhang, Yongguo Yu, Kun Sun, Weidi Xie

**Updated**: 2025-08-26T14:13:25Z

**Summary**: Rare diseases collectively affect over 300 million individuals worldwide, yet timely and accurate diagnosis remains a pervasive challenge. This is largely due to their clinical heterogeneity, low individual prevalence, and the limited familiarity most clinicians have with rare conditions. Here, we introduce DeepRare, the first rare disease diagnosis agentic system powered by a large language model (LLM), capable of processing heterogeneous clinical inputs. The system generates ranked diagnostic hypotheses for rare diseases, each accompanied by a transparent chain of reasoning that links intermediate analytic steps to verifiable medical evidence.   DeepRare comprises three key components: a central host with a long-term memory module; specialized agent servers responsible for domain-specific analytical tasks integrating over 40 specialized tools and web-scale, up-to-date medical knowledge sources, ensuring access to the most current clinical information. This modular and scalable design enables complex diagnostic reasoning while maintaining traceability and adaptability. We evaluate DeepRare on eight datasets. The system demonstrates exceptional diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013 diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15 methods, like traditional bioinformatics diagnostic tools, LLMs, and other agentic systems, achieving an average Recall@1 score of 57.18% and surpassing the second-best method (Reasoning LLM) by a substantial margin of 23.79 percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at Recall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of reasoning chains by clinical experts achieves 95.40% agreements. Furthermore, the DeepRare system has been implemented as a user-friendly web application http://raredx.cn/doctor.

**Link**: [arxiv](http://arxiv.org/abs/2506.20430v2),  [pdf](http://arxiv.org/pdf/2506.20430v2)

**Tags**: cs.CL cs.AI cs.CV cs.MA 



### SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge   Understanding of LLMs
**Authors**: Zhiqiang Liu, Enpei Niu, Yin Hua, Mengshu Sun, Lei Liang, Huajun Chen, Wen Zhang

**Updated**: 2025-08-26T14:11:22Z

**Summary**: Although large language models (LLMs) have made significant progress in understanding Structured Knowledge (SK) like KG and Table, existing evaluations for SK understanding are non-rigorous (i.e., lacking evaluations of specific capabilities) and focus on a single type of SK. Therefore, we aim to propose a more comprehensive and rigorous structured knowledge understanding benchmark to diagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a Structured Knowledge Augmented QA Benchmark that encompasses four widely used structured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a three-stage pipeline to construct SKA-Bench instances, which includes a question, an answer, positive knowledge units, and noisy knowledge units. To evaluate the SK understanding capabilities of LLMs in a fine-grained manner, we expand the instances into four fundamental ability testbeds: Noise Robustness, Order Insensitivity, Information Integration, and Negative Rejection. Empirical evaluations on 8 representative LLMs, including the advanced DeepSeek-R1, indicate that existing LLMs still face significant challenges in understanding structured knowledge, and their performance is influenced by factors such as the amount of noise, the order of knowledge units, and hallucination phenomenon. Our dataset and code are available at https://github.com/Lza12a/SKA-Bench.

**Link**: [arxiv](http://arxiv.org/abs/2507.17178v2),  [pdf](http://arxiv.org/pdf/2507.17178v2)

**Tags**: cs.CL cs.AI 



### Architecting Clinical Collaboration: Multi-Agent Reasoning Systems for   Multimodal Medical VQA
**Authors**: Karishma Thakrar, Shreyas Basavatia, Akshay Daftardar

**Updated**: 2025-08-26T14:02:57Z

**Summary**: Dermatological care via telemedicine often lacks the rich context of in-person visits. Clinicians must make diagnoses based on a handful of images and brief descriptions, without the benefit of physical exams, second opinions, or reference materials. While many medical AI systems attempt to bridge these gaps with domain-specific fine-tuning, this work hypothesized that mimicking clinical reasoning processes could offer a more effective path forward. This study tested seven vision-language models on medical visual question answering across six configurations: baseline models, fine-tuned variants, and both augmented with either reasoning layers that combine multiple model perspectives, analogous to peer consultation, or retrieval-augmented generation that incorporates medical literature at inference time, serving a role similar to reference-checking. While fine-tuning degraded performance in four of seven models with an average 30% decrease, baseline models collapsed on test data. Clinical-inspired architectures, meanwhile, achieved up to 70% accuracy, maintaining performance on unseen data while generating explainable, literature-grounded outputs critical for clinical adoption. These findings demonstrate that medical AI succeeds by reconstructing the collaborative and evidence-based practices fundamental to clinical diagnosis.

**Link**: [arxiv](http://arxiv.org/abs/2507.05520v3),  [pdf](http://arxiv.org/pdf/2507.05520v3)

**Tags**: cs.AI 



### A Survey on Causal Discovery: Theory and Practice
**Authors**: Alessio Zanga, Elif Ozkirimli, Fabio Stella

**Updated**: 2025-08-26T14:02:48Z

**Summary**: Understanding the laws that govern a phenomenon is the core of scientific progress. This is especially true when the goal is to model the interplay between different aspects in a causal fashion. Indeed, causal inference itself is specifically designed to quantify the underlying relationships that connect a cause to its effect. Causal discovery is a branch of the broader field of causality in which causal graphs are recovered from data (whenever possible), enabling the identification and estimation of causal effects. In this paper, we explore recent advancements in causal discovery in a unified manner, provide a consistent overview of existing algorithms developed under different settings, report useful tools and data, present real-world applications to understand why and how these methods can be fruitfully exploited.

**Link**: [arxiv](http://arxiv.org/abs/2305.10032v2),  [pdf](http://arxiv.org/pdf/2305.10032v2)

**Tags**: cs.AI 



### A Concurrent Modular Agent: Framework for Autonomous LLM Agents
**Authors**: Norihiro Maruyama, Takahide Yoshida, Hiroki Sato, Atsushi Masumori, Johnsmith, Takashi Ikegami

**Updated**: 2025-08-26T13:58:31Z

**Summary**: We introduce the Concurrent Modular Agent (CMA), a framework that orchestrates multiple Large-Language-Model (LLM)-based modules that operate fully asynchronously yet maintain a coherent and fault-tolerant behavioral loop. This framework addresses long-standing difficulties in agent architectures by letting intention emerge from language-mediated interactions among autonomous processes. This approach enables flexible, adaptive, and context-dependent behavior through the combination of concurrently executed modules that offload reasoning to an LLM, inter-module communication, and a single shared global state.We consider this approach to be a practical realization of Minsky's Society of Mind theory. We demonstrate the viability of our system through two practical use-case studies. The emergent properties observed in our system suggest that complex cognitive phenomena like self-awareness may indeed arise from the organized interaction of simpler processes, supporting Minsky-Society of Mind concept and opening new avenues for artificial intelligence research. The source code for our work is available at: https://github.com/AlternativeMachine/concurrent-modular-agent.

**Link**: [arxiv](http://arxiv.org/abs/2508.19042v1),  [pdf](http://arxiv.org/pdf/2508.19042v1)

**Tags**: cs.AI 



### Large Language Model Aided QoS Prediction for Service Recommendation
**Authors**: Huiying Liu, Zekun Zhang, Honghao Li, Qilin Wu, Yiwen Zhang

**Updated**: 2025-08-26T13:55:02Z

**Summary**: Large language models (LLMs) have seen rapid improvement in the recent years, and have been used in a wider range of applications. After being trained on large text corpus, LLMs obtain the capability of extracting rich features from textual data. Such capability is potentially useful for the web service recommendation task, where the web users and services have intrinsic attributes that can be described using natural language sentences and are useful for recommendation. In this paper, we explore the possibility and practicality of using LLMs for web service recommendation. We propose the large language model aided QoS prediction (llmQoS) model, which use LLMs to extract useful information from attributes of web users and services via descriptive sentences. This information is then used in combination with the QoS values of historical interactions of users and services, to predict QoS values for any given user-service pair. On the WSDream dataset, llmQoS is shown to overcome the data sparsity issue inherent to the QoS prediction problem, and outperforms comparable baseline models consistently.

**Link**: [arxiv](http://arxiv.org/abs/2408.02223v3),  [pdf](http://arxiv.org/pdf/2408.02223v3)

**Tags**: cs.LG cs.DC 



### Investigating Advanced Reasoning of Large Language Models via Black-Box   Interaction
**Authors**: Congchi Yin, Tianyi Wu, Yankai Shu, Alex Gu, Yunhan Wang, Jun Shao, Xun Jiang, Piji Li

**Updated**: 2025-08-26T13:54:17Z

**Summary**: Existing tasks fall short in evaluating reasoning ability of Large Language Models (LLMs) in an interactive, unknown environment. This deficiency leads to the isolated assessment of deductive, inductive, and abductive reasoning, neglecting the integrated reasoning process that is indispensable for humans discovery of real world. We introduce a novel evaluation paradigm, \textit{black-box interaction}, to tackle this challenge. A black-box is defined by a hidden function that maps a specific set of inputs to outputs. LLMs are required to unravel the hidden function behind the black-box by interacting with it in given exploration turns, and reasoning over observed input-output pairs. Leveraging this idea, we build the \textsc{Oracle} benchmark which comprises 6 types of black-box task and 96 black-boxes. 19 modern LLMs are benchmarked. o3 ranks first in 5 of the 6 tasks, achieving over 70\% accuracy on most easy black-boxes. But it still struggles with some hard black-box tasks, where its average performance drops below 40\%. Further analysis indicates a universal difficulty among LLMs: They lack the high-level planning capability to develop efficient and adaptive exploration strategies for hypothesis refinement.

**Link**: [arxiv](http://arxiv.org/abs/2508.19035v1),  [pdf](http://arxiv.org/pdf/2508.19035v1)

**Tags**: cs.AI 



### MovieCORE: COgnitive REasoning in Movies
**Authors**: Gueter Josmy Faure, Min-Hung Chen, Jia-Fong Yeh, Ying Cheng, Hung-Ting Su, Yung-Hao Tang, Shang-Hong Lai, Winston H. Hsu

**Updated**: 2025-08-26T13:43:45Z

**Summary**: This paper introduces MovieCORE, a novel video question answering (VQA) dataset designed to probe deeper cognitive understanding of movie content. Unlike existing datasets that focus on surface-level comprehension, MovieCORE emphasizes questions that engage System-2 thinking while remaining specific to the video material. We present an innovative agentic brainstorming approach, utilizing multiple large language models (LLMs) as thought agents to generate and refine high-quality question-answer pairs. To evaluate dataset quality, we develop a set of cognitive tests assessing depth, thought-provocation potential, and syntactic complexity. We also propose a comprehensive evaluation scheme for assessing VQA model performance on deeper cognitive tasks. To address the limitations of existing video-language models (VLMs), we introduce an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves model reasoning capabilities post-training by up to 25%. Our work contributes to advancing movie understanding in AI systems and provides valuable insights into the capabilities and limitations of current VQA models when faced with more challenging, nuanced questions about cinematic content. Our project page, dataset and code can be found at https://joslefaure.github.io/assets/html/moviecore.html.

**Link**: [arxiv](http://arxiv.org/abs/2508.19026v1),  [pdf](http://arxiv.org/pdf/2508.19026v1)

**Tags**: cs.CL cs.AI cs.CV 



### General Intelligence Requires Reward-based Pretraining
**Authors**: Seungwook Han, Jyothish Pari, Samuel J. Gershman, Pulkit Agrawal

**Updated**: 2025-08-26T13:31:48Z

**Summary**: Large Language Models (LLMs) have demonstrated impressive real-world utility, exemplifying artificial useful intelligence (AUI). However, their ability to reason adaptively and robustly -- the hallmarks of artificial general intelligence (AGI) -- remains fragile. While LLMs seemingly succeed in commonsense reasoning, programming, and mathematics, they struggle to generalize algorithmic understanding across novel contexts. Our experiments with algorithmic tasks in esoteric programming languages reveal that LLM's reasoning overfits to the training data and is limited in its transferability. We hypothesize that the core issue underlying such limited transferability is the coupling of reasoning and knowledge in LLMs.   To transition from AUI to AGI, we propose disentangling knowledge and reasoning through three key directions: (1) pretaining to reason using RL from scratch as an alternative to the widely used next-token prediction pretraining, (2) using a curriculum of synthetic tasks to ease the learning of a reasoning prior for RL that can then be transferred to natural language tasks, and (3) learning more generalizable reasoning functions using a small context window to reduce exploiting spurious correlations between tokens. Such a reasoning system coupled with a trained retrieval system and a large external memory bank as a knowledge store can overcome several limitations of existing architectures at learning to reason in novel scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2502.19402v3),  [pdf](http://arxiv.org/pdf/2502.19402v3)

**Tags**: cs.LG 



### Network inference via approximate Bayesian computation. Illustration on   a stochastic multi-population neural mass model
**Authors**: Susanne Ditlevsen, Massimiliano Tamborrino, Irene Tubikanec

**Updated**: 2025-08-26T13:29:53Z

**Summary**: In this article, we propose an adapted sequential Monte Carlo approximate Bayesian computation (SMC-ABC) algorithm for network inference in coupled stochastic differential equations (SDEs) used for multivariate time series modeling. Our approach is motivated by neuroscience, specifically the challenge of estimating brain connectivity before and during epileptic seizures. To this end, we make four key contributions. First, we introduce a 6N-dimensional SDE to model the activity of N coupled neuronal populations, extending the (single-population) stochastic Jansen and Rit neural mass model used to describe human electroencephalography (EEG) rhythms, particularly epileptic activity. Second, we construct a reliable and efficient numerical splitting scheme for the model simulation. Third, we apply the proposed adapted SMC-ABC algorithm to the neural mass model and validate it on different types of simulated data. Compared to standard SMC-ABC, our approach significantly reduces computational cost by requiring fewer model simulations to reach the desired posterior region, thanks to the inclusion of binary parameters describing the presence or absence of coupling directions. Finally, we apply our method to real multi-channel EEG data, uncovering potential similarities in patients' brain activities across different epileptic seizures, as well as differences between pre-seizure and seizure periods.

**Link**: [arxiv](http://arxiv.org/abs/2306.15787v4),  [pdf](http://arxiv.org/pdf/2306.15787v4)

**Tags**: stat.ME cs.NA math.NA 



### Project-Probe-Aggregate: Efficient Fine-Tuning for Group Robustness
**Authors**: Beier Zhu, Jiequan Cui, Hanwang Zhang, Chi Zhang

**Updated**: 2025-08-26T13:19:52Z

**Summary**: While image-text foundation models have succeeded across diverse downstream tasks, they still face challenges in the presence of spurious correlations between the input and label. To address this issue, we propose a simple three-step approach,Project-Probe-Aggregate (PPA), that enables parameter-efficient fine-tuning for foundation models without relying on group annotations. Building upon the failure-based debiasing scheme, our method, PPA, improves its two key components: minority samples identification and the robust training algorithm. Specifically, we first train biased classifiers by projecting image features onto the nullspace of class proxies from text encoders. Next, we infer group labels using the biased classifier and probe group targets with prior correction. Finally, we aggregate group weights of each class to produce the debiased classifier. Our theoretical analysis shows that our PPA enhances minority group identification and is Bayes optimal for minimizing the balanced group error, mitigating spurious correlations. Extensive experimental results confirm the effectiveness of our PPA: it outperforms the state-of-the-art by an average worst-group accuracy while requiring less than 0.01% tunable parameters without training group labels.

**Link**: [arxiv](http://arxiv.org/abs/2503.09487v3),  [pdf](http://arxiv.org/pdf/2503.09487v3)

**Tags**: cs.CV 



### Sense of Self and Time in Borderline Personality. A Comparative   Robustness Study with Generative AI
**Authors**: Marcin Moskalewicz, Anna Sterna, Marek Pokropski, Paula Flores

**Updated**: 2025-08-26T13:13:47Z

**Summary**: This study examines the capacity of large language models (LLMs) to support phenomenological qualitative analysis of first-person experience in Borderline Personality Disorder (BPD), understood as a disorder of temporality and selfhood. Building on a prior human-led thematic analysis of 24 inpatients' life-story interviews, we compared three LLMs (OpenAI GPT-4o, Google Gemini 2.5 Pro, Anthropic Claude Opus 4) prompted to mimic the interpretative style of the original investigators. The models were evaluated with blinded and non-blinded expert judges in phenomenology and clinical psychology. Assessments included semantic congruence, Jaccard coefficients, and multidimensional validity ratings (credibility, coherence, substantiveness, and groundness in data). Results showed variable overlap with the human analysis, from 0 percent in GPT to 42 percent in Claude and 58 percent in Gemini, and a low Jaccard coefficient (0.21-0.28). However, the models recovered themes omitted by humans. Gemini's output most closely resembled the human analysis, with validity scores significantly higher than GPT and Claude (p < 0.0001), and was judged as human by blinded experts. All scores strongly correlated (R > 0.78) with the quantity of text and words per theme, highlighting both the variability and potential of AI-augmented thematic analysis to mitigate human interpretative bias.

**Link**: [arxiv](http://arxiv.org/abs/2508.19008v1),  [pdf](http://arxiv.org/pdf/2508.19008v1)

**Tags**: cs.AI 



### Seal Your Backdoor with Variational Defense
**Authors**: Ivan Sabolić, Matej Grcić, Siniša Šegvić

**Updated**: 2025-08-26T13:09:35Z

**Summary**: We propose VIBE, a model-agnostic framework that trains classifiers resilient to backdoor attacks. The key concept behind our approach is to treat malicious inputs and corrupted labels from the training dataset as observed random variables, while the actual clean labels are latent. VIBE then recovers the corresponding latent clean label posterior through variational inference. The resulting training procedure follows the expectation-maximization (EM) algorithm. The E-step infers the clean pseudolabels by solving an entropy-regularized optimal transport problem, while the M-step updates the classifier parameters via gradient descent. Being modular, VIBE can seamlessly integrate with recent advancements in self-supervised representation learning, which enhance its ability to resist backdoor attacks. We experimentally validate the method effectiveness against contemporary backdoor attacks on standard datasets, a large-scale setup with 1$k$ classes, and a dataset poisoned with multiple attacks. VIBE consistently outperforms previous defenses across all tested scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2503.08829v3),  [pdf](http://arxiv.org/pdf/2503.08829v3)

**Tags**: cs.LG cs.CR 



### Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A   Framework and Benchmark
**Authors**: Yuxuan Cai, Yipeng Hao, Jie Zhou, Hang Yan, Zhikai Lei, Rui Zhen, Zhenhua Han, Yutao Yang, Junsong Li, Qianjun Pan, Tianyu Huai, Qin Chen, Xin Li, Kai Chen, Bo Zhang, Xipeng Qiu, Liang He

**Updated**: 2025-08-26T13:04:28Z

**Summary**: As AI advances toward general intelligence, the focus is shifting from systems optimized for static tasks to creating open-ended agents that learn continuously. In this paper, we introduce Experience-driven Lifelong Learning (ELL), a framework for building self-evolving agents capable of continuous growth through real-world interaction. The framework is built on four core principles: (1) Experience Exploration: Agents learn through continuous, self-motivated interaction with dynamic environments, navigating interdependent tasks and generating rich experiential trajectories. (2) Long-term Memory: Agents preserve and structure historical knowledge, including personal experiences, domain expertise, and commonsense reasoning, into a persistent memory system. (3) Skill Learning: Agents autonomously improve by abstracting recurring patterns from experience into reusable skills, which are actively refined and validated for application in new tasks. (4) Knowledge Internalization: Agents internalize explicit and discrete experiences into implicit and intuitive capabilities as "second nature".   We also introduce StuLife, a benchmark dataset for ELL that simulates a student's holistic college journey, from enrollment to academic and personal development, across three core phases and ten detailed sub-scenarios. StuLife is designed around three key paradigm shifts: From Passive to Proactive, From Context to Memory, and From Imitation to Learning. In this dynamic environment, agents must acquire and distill practical skills and maintain persistent memory to make decisions based on evolving state variables. StuLife provides a comprehensive platform for evaluating lifelong learning capabilities, including memory retention, skill transfer, and self-motivated behavior. Beyond evaluating SOTA LLMs on the StuLife benchmark, we also explore the role of context engineering in advancing AGI.

**Link**: [arxiv](http://arxiv.org/abs/2508.19005v1),  [pdf](http://arxiv.org/pdf/2508.19005v1)

**Tags**: cs.AI cs.CL 



### Estimating the average treatment effect in cluster-randomized trials   with misclassified outcomes and non-random validation subsets
**Authors**: Dane Isenberg, Nandita Mitra, Steven C. Marcus, Rinad S. Beidas, Kristin A. Linn

**Updated**: 2025-08-26T13:00:45Z

**Summary**: Randomized trials are viewed as the benchmark for assessing causal effects of treatments on outcomes of interest. Nonetheless, challenges such as measurement error can undermine the standard causal assumptions for randomized trials. In ASPIRE, a cluster-randomized trial, pediatric primary care clinics were assigned to one of two treatments aimed at promoting clinician delivery of a secure firearm program to parents during well-child visits. A key outcome of interest is thus parent receipt of the program at each visit. Clinicians documented program delivery in patients' electronic health records for all visits, but their reporting is a proxy measure for the parent receipt outcome. Parents were also surveyed to report directly on program receipt after their child's visit; however, only a small subset of them completed the survey. Here, we develop a causal inference framework for a binary outcome that is subject to misclassification through silver-standard measures (clinician reports), but gold-standard measures (parent reports) are only available for a non-random internal validation subset. We propose a method for identifying the average treatment effect (ATE) that addresses the risk of bias due to misclassification and non-random validation selection, even when the outcome (parent receipt) may directly impact selection propensity (survey responsiveness). We show that ATE estimation relies on specifying the relationship between the gold- and silver-standard outcome measures in the validation subset, which may depend on treatment and covariates. Additionally, the clustered design is reflected in our causal assumptions and in our cluster-robust approach to estimation of the ATE. Simulation studies demonstrate acceptable finite-sample operating characteristics of our ATE estimator, supporting its application to ASPIRE.

**Link**: [arxiv](http://arxiv.org/abs/2508.18137v2),  [pdf](http://arxiv.org/pdf/2508.18137v2)

**Tags**: stat.ME 



### Bayesian Joint Modeling of Zero-Inflated Longitudinal Data and Survival   with a Cure Fraction: Application to AIDS Data
**Authors**: Taban Baghfalaki, Mojtaba Ganjali

**Updated**: 2025-08-26T13:00:02Z

**Summary**: We propose a comprehensive Bayesian joint modeling framework for zero-inflated longitudinal count data and time-to-event outcomes, explicitly incorporating a cure fraction to account for subjects who never experience the event. The longitudinal sub-model employs a flexible mixed-effects Hurdle model, with distributional options including zero-inflated Poisson and zero-inflated negative binomial, accommodating excess zeros and overdispersion common in count data. The survival component is modeled using a Cox proportional hazards model combined with a mixture cure model to distinguish cured from susceptible individuals. To link the longitudinal and survival processes, we include a linear combination of current longitudinal values as predictors in the survival model. Inference is performed via Hamiltonian Monte Carlo, enabling efficient and robust parameter estimation. The joint model supports dynamic predictions, facilitating real-time risk assessment and personalized medicine. Model performance and estimation accuracy are validated through simulation studies. Finally, we illustrate the methodology using a real-world HIV cohort dataset, demonstrating its practical utility in predicting patient survival outcomes and supporting personalized treatment decisions. Our results highlight the benefits of integrating complex longitudinal count data with survival information in clinical research.

**Link**: [arxiv](http://arxiv.org/abs/2508.19001v1),  [pdf](http://arxiv.org/pdf/2508.19001v1)

**Tags**: stat.ME stat.AP 



### LLM-Enhanced Linear Autoencoders for Recommendation
**Authors**: Jaewan Moon, Seongmin Park, Jongwuk Lee

**Updated**: 2025-08-26T12:59:55Z

**Summary**: Large language models (LLMs) have been widely adopted to enrich the semantic representation of textual item information in recommender systems. However, existing linear autoencoders (LAEs) that incorporate textual information rely on sparse word co-occurrence patterns, limiting their ability to capture rich textual semantics. To address this, we propose L3AE, the first integration of LLMs into the LAE framework. L3AE effectively integrates the heterogeneous knowledge of textual semantics and user-item interactions through a two-phase optimization strategy. (i) L3AE first constructs a semantic item-to-item correlation matrix from LLM-derived item representations. (ii) It then learns an item-to-item weight matrix from collaborative signals while distilling semantic item correlations as regularization. Notably, each phase of L3AE is optimized through closed-form solutions, ensuring global optimality and computational efficiency. Extensive experiments demonstrate that L3AE consistently outperforms state-of-the-art LLM-enhanced models on three benchmark datasets, achieving gains of 27.6% in Recall@20 and 39.3% in NDCG@20. The source code is available at https://github.com/jaewan7599/L3AE_CIKM2025.

**Link**: [arxiv](http://arxiv.org/abs/2508.13500v2),  [pdf](http://arxiv.org/pdf/2508.13500v2)

**Tags**: cs.IR cs.AI cs.CL cs.LG 



### RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation   and Language Generation for Explainable QA Hallucination Detection
**Authors**: Yiming Huang, Junyan Zhang, Zihao Wang, Biquan Bie, Yunzhong Qiu, Yi R. Fung, Xinlei He

**Updated**: 2025-08-26T12:55:45Z

**Summary**: Large Language Models (LLMs) have become powerful, but hallucinations remain a vital obstacle to their trustworthy use. While previous works improved the capability of hallucination detection by measuring uncertainty, they all lack the ability to explain the provenance behind why hallucinations occur, i.e., which part of the inputs tends to trigger hallucinations. Recent works on the prompt attack indicate that uncertainty exists in semantic propagation, where attention mechanisms gradually fuse local token information into high-level semantics across layers. Meanwhile, uncertainty also emerges in language generation, due to its probability-based selection of high-level semantics for sampled generations. Based on that, we propose RePPL to recalibrate uncertainty measurement by these two aspects, which dispatches explainable uncertainty scores to each token and aggregates in Perplexity-style Log-Average form as total score. Experiments show that our method achieves the best comprehensive detection performance across various QA datasets on advanced models (average AUC of 0.833), and our method is capable of producing token-level uncertainty scores as explanations for the hallucination. Leveraging these scores, we preliminarily find the chaotic pattern of hallucination and showcase its promising usage.

**Link**: [arxiv](http://arxiv.org/abs/2505.15386v2),  [pdf](http://arxiv.org/pdf/2505.15386v2)

**Tags**: cs.CL cs.AI 



### MOSA: Mixtures of Simple Adapters Outperform Monolithic Approaches in   LLM-based Multilingual ASR
**Authors**: Junjie Li, Jing Peng, Yangui Fang, Shuai Wang, Kai Yu

**Updated**: 2025-08-26T12:54:23Z

**Summary**: End-to-end multilingual ASR aims to transcribe speech from different languages into corresponding text, but is often limited by scarce multilingual data. LLM-based ASR aligns speech encoder outputs with LLM input space via a projector and has achieved notable success. However, prior work mainly improves performance by increasing data, with little focus on cross-lingual knowledge sharing. Moreover, a single complex projector struggles to capture both shared and language-specific features effectively. In this work, we propose MOSA (Mixture of Simple Adapters), leveraging a Mixture-of-Experts mechanism to combine lightweight adapters that learn shared and language-specific knowledge. This enables better utilization of high-resource language data to support low-resource languages, mitigating data scarcity issues. Experimental results show that MOSA-Base achieves a 15.4\% relative reduction in average WER compared to the Baseline-Base and consistently outperforms it across all languages. Remarkably, MOSA-Base surpasses the Baseline-Base even when trained with only 60\% of its parameters. Similarly, MOSA-Large outperforms the Baseline-Large in average WER and demonstrates greater robustness to data imbalance. Ablation studies further indicate that MOSA is more effective at handling individual languages and learning both language-specific and shared linguistic knowledge. These findings support that, in LLM-based ASR, a mixture of simple adapters is more effective than a single, complex adapter design.

**Link**: [arxiv](http://arxiv.org/abs/2508.18998v1),  [pdf](http://arxiv.org/pdf/2508.18998v1)

**Tags**: eess.AS 



### GitTaskBench: A Benchmark for Code Agents Solving Real-World Tasks   Through Code Repository Leveraging
**Authors**: Ziyi Ni, Huacan Wang, Shuo Zhang, Shuo Lu, Ziyang He, Wang You, Zhenheng Tang, Yuntao Du, Bill Sun, Hongzhang Liu, Sen Hu, Ronghao Chen, Bo Li, Xin Li, Chen Hu, Binxing Jiao, Daxin Jiang, Pin Lyu

**Updated**: 2025-08-26T12:48:05Z

**Summary**: Beyond scratch coding, exploiting large-scale code repositories (e.g., GitHub) for practical tasks is vital in real-world software development, yet current benchmarks rarely evaluate code agents in such authentic, workflow-driven scenarios. To bridge this gap, we introduce GitTaskBench, a benchmark designed to systematically assess this capability via 54 realistic tasks across 7 modalities and 7 domains. Each task pairs a relevant repository with an automated, human-curated evaluation harness specifying practical success criteria. Beyond measuring execution and task success, we also propose the alpha-value metric to quantify the economic benefit of agent performance, which integrates task success rates, token cost, and average developer salaries. Experiments across three state-of-the-art agent frameworks with multiple advanced LLMs show that leveraging code repositories for complex task solving remains challenging: even the best-performing system, OpenHands+Claude 3.7, solves only 48.15% of tasks. Error analysis attributes over half of failures to seemingly mundane yet critical steps like environment setup and dependency resolution, highlighting the need for more robust workflow management and increased timeout preparedness. By releasing GitTaskBench, we aim to drive progress and attention toward repository-aware code reasoning, execution, and deployment -- moving agents closer to solving complex, end-to-end real-world tasks. The benchmark and code are open-sourced at https://github.com/QuantaAlpha/GitTaskBench.

**Link**: [arxiv](http://arxiv.org/abs/2508.18993v1),  [pdf](http://arxiv.org/pdf/2508.18993v1)

**Tags**: cs.SE cs.AI 



### Automatic Prompt Optimization with Prompt Distillation
**Authors**: Viktor N. Zhuravlev, Artur R. Khairullin, Ernest A. Dyagin, Alena N. Sitkina, Nikita I. Kulin

**Updated**: 2025-08-26T12:46:58Z

**Summary**: Autoprompting is the process of automatically selecting optimized prompts for language models, which is gaining popularity due to the rapid development of prompt engineering driven by extensive research in the field of large language models (LLMs). This paper presents DistillPrompt -- a novel autoprompting method based on large language models that employs a multi-stage integration of task-specific information into prompts using training data. DistillPrompt utilizes distillation, compression, and aggregation operations to explore the prompt space more thoroughly. The method was tested on different datasets for text classification and generation tasks using the t-lite-instruct-0.1 language model. The results demonstrate a significant average improvement (e.g., 20.12% across the entire dataset compared to Grips) in key metrics over existing methods in the field, establishing DistillPrompt as one of the most effective non-gradient approaches in autoprompting.

**Link**: [arxiv](http://arxiv.org/abs/2508.18992v1),  [pdf](http://arxiv.org/pdf/2508.18992v1)

**Tags**: cs.CL cs.AI cs.LG 



### Truth or Twist? Optimal Model Selection for Reliable Label Flipping   Evaluation in LLM-based Counterfactuals
**Authors**: Qianli Wang, Van Bach Nguyen, Nils Feldhus, Luis Felipe Villa-Arenas, Christin Seifert, Sebastian Möller, Vera Schmitt

**Updated**: 2025-08-27T14:04:13Z

**Summary**: Counterfactual examples are widely employed to enhance the performance and robustness of large language models (LLMs) through counterfactual data augmentation (CDA). However, the selection of the judge model used to evaluate label flipping, the primary metric for assessing the validity of generated counterfactuals for CDA, yields inconsistent results. To decipher this, we define four types of relationships between the counterfactual generator and judge models: being the same model, belonging to the same model family, being independent models, and having an distillation relationship. Through extensive experiments involving two state-of-the-art LLM-based methods, three datasets, four generator models, and 15 judge models, complemented by a user study (n = 90), we demonstrate that judge models with an independent, non-fine-tuned relationship to the generator model provide the most reliable label flipping evaluations. Relationships between the generator and judge models, which are closely aligned with the user study for CDA, result in better model performance and robustness. Nevertheless, we find that the gap between the most effective judge models and the results obtained from the user study remains considerably large. This suggests that a fully automated pipeline for CDA may be inadequate and requires human intervention.

**Link**: [arxiv](http://arxiv.org/abs/2505.13972v3),  [pdf](http://arxiv.org/pdf/2505.13972v3)

**Tags**: cs.CL 



### The Double-edged Sword of LLM-based Data Reconstruction: Understanding   and Mitigating Contextual Vulnerability in Word-level Differential Privacy   Text Sanitization
**Authors**: Stephen Meisenbacher, Alexandra Klymenko, Andreea-Elena Bodea, Florian Matthes

**Updated**: 2025-08-26T12:22:45Z

**Summary**: Differentially private text sanitization refers to the process of privatizing texts under the framework of Differential Privacy (DP), providing provable privacy guarantees while also empirically defending against adversaries seeking to harm privacy. Despite their simplicity, DP text sanitization methods operating at the word level exhibit a number of shortcomings, among them the tendency to leave contextual clues from the original texts due to randomization during sanitization $\unicode{x2013}$ this we refer to as $\textit{contextual vulnerability}$. Given the powerful contextual understanding and inference capabilities of Large Language Models (LLMs), we explore to what extent LLMs can be leveraged to exploit the contextual vulnerability of DP-sanitized texts. We expand on previous work not only in the use of advanced LLMs, but also in testing a broader range of sanitization mechanisms at various privacy levels. Our experiments uncover a double-edged sword effect of LLM-based data reconstruction attacks on privacy and utility: while LLMs can indeed infer original semantics and sometimes degrade empirical privacy protections, they can also be used for good, to improve the quality and privacy of DP-sanitized texts. Based on our findings, we propose recommendations for using LLM data reconstruction as a post-processing step, serving to increase privacy protection by thinking adversarially.

**Link**: [arxiv](http://arxiv.org/abs/2508.18976v1),  [pdf](http://arxiv.org/pdf/2508.18976v1)

**Tags**: cs.CR cs.CL 



### MultiRef: Controllable Image Generation with Multiple Visual References
**Authors**: Ruoxi Chen, Dongping Chen, Siyuan Wu, Sinan Wang, Shiyun Lang, Petr Sushko, Gaoyang Jiang, Yao Wan, Ranjay Krishna

**Updated**: 2025-08-26T12:18:14Z

**Summary**: Visual designers naturally draw inspiration from multiple visual references, combining diverse elements and aesthetic principles to create artwork. However, current image generative frameworks predominantly rely on single-source inputs -- either text prompts or individual reference images. In this paper, we focus on the task of controllable image generation using multiple visual references. We introduce MultiRef-bench, a rigorous evaluation framework comprising 990 synthetic and 1,000 real-world samples that require incorporating visual content from multiple reference images. The synthetic samples are synthetically generated through our data engine RefBlend, with 10 reference types and 33 reference combinations. Based on RefBlend, we further construct a dataset MultiRef containing 38k high-quality images to facilitate further research. Our experiments across three interleaved image-text models (i.e., OmniGen, ACE, and Show-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that even state-of-the-art systems struggle with multi-reference conditioning, with the best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in real-world cases on average compared to the golden answer. These findings provide valuable directions for developing more flexible and human-like creative tools that can effectively integrate multiple sources of visual inspiration. The dataset is publicly available at: https://multiref.github.io/.

**Link**: [arxiv](http://arxiv.org/abs/2508.06905v3),  [pdf](http://arxiv.org/pdf/2508.06905v3)

**Tags**: cs.CV 



### Enhancing compact convolutional transformers with super attention
**Authors**: Simpenzwe Honore Leandre, Natenaile Asmamaw Shiferaw, Dillip Rout

**Updated**: 2025-08-26T12:00:38Z

**Summary**: In this paper, we propose a vision model that adopts token mixing, sequence-pooling, and convolutional tokenizers to achieve state-of-the-art performance and efficient inference in fixed context-length tasks. In the CIFAR100 benchmark, our model significantly improves the baseline of the top 1% and top 5% validation accuracy from 36.50% to 46.29% and 66.33% to 76.31%, while being more efficient than the Scaled Dot Product Attention (SDPA) transformers when the context length is less than the embedding dimension and only 60% the size. In addition, the architecture demonstrates high training stability and does not rely on techniques such as data augmentation like mixup, positional embeddings, or learning rate scheduling. We make our code available on Github.

**Link**: [arxiv](http://arxiv.org/abs/2508.18960v1),  [pdf](http://arxiv.org/pdf/2508.18960v1)

**Tags**: cs.CV cs.LG 



### From Confidence to Collapse in LLM Factual Robustness
**Authors**: Alina Fastowski, Bardh Prenkaj, Gjergji Kasneci

**Updated**: 2025-08-26T11:54:16Z

**Summary**: Ensuring the robustness of factual knowledge in LLMs is critical for reliable applications in tasks such as question answering and reasoning. However, existing evaluation methods predominantly focus on performance-based metrics, often investigating from the perspective of prompt perturbations, which captures only the externally triggered side of knowledge robustness. To bridge this gap, we introduce a principled approach to measure factual robustness from the perspective of the generation process by analyzing token distribution entropy in combination with temperature scaling sensitivity. These two factors build the Factual Robustness Score (FRS), a novel metric which quantifies the stability of a fact against perturbations in decoding conditions, given its initial uncertainty. To validate our approach, we conduct extensive experiments on 5 LLMs across 3 closed-book QA datasets (SQuAD, TriviaQA, and HotpotQA). We show that factual robustness varies significantly -- smaller models report an FRS of $0.76$, larger ones $0.93$ -- with accuracy degrading by ~$60\%$ under increased uncertainty. These insights demonstrate how entropy and temperature scaling impact factual accuracy, and lay a foundation for developing more robust knowledge retention and retrieval in future models.

**Link**: [arxiv](http://arxiv.org/abs/2508.16267v2),  [pdf](http://arxiv.org/pdf/2508.16267v2)

**Tags**: cs.CL cs.AI 



### Interleaving Large Language Models for Compiler Testing
**Authors**: Yunbo Ni, Shaohua Li

**Updated**: 2025-08-26T11:49:58Z

**Summary**: Testing compilers with AI models, especially large language models (LLMs), has shown great promise. However, current approaches struggle with two key problems: The generated programs for testing compilers are often too simple, and extensive testing with the LLMs is computationally expensive. In this paper, we propose a novel compiler testing framework that decouples the testing process into two distinct phases: an offline phase and an online phase. In the offline phase, we use LLMs to generate a collection of small but feature-rich code pieces. In the online phase, we reuse these code pieces by strategically combining them to build high-quality and valid test programs, which are then used to test compilers.   We implement this idea in a tool, LegoFuzz, for testing C compilers. The results are striking: we found 66 bugs in GCC and LLVM, the most widely used C compilers. Almost half of the bugs are miscompilation bugs, which are serious and hard-to-find bugs that none of the existing LLM-based tools could find. We believe this efficient design opens up new possibilities for using AI models in software testing beyond just C compilers.

**Link**: [arxiv](http://arxiv.org/abs/2508.18955v1),  [pdf](http://arxiv.org/pdf/2508.18955v1)

**Tags**: cs.SE 



### Novel Approaches to Artificial Intelligence Development Based on the   Nearest Neighbor Method
**Authors**: I. I. Priezzhev, D. A. Danko, A. V. Shubin

**Updated**: 2025-08-26T11:49:42Z

**Summary**: Modern neural network technologies, including large language models, have achieved remarkable success in various applied artificial intelligence applications, however, they face a range of fundamental limitations. Among them are hallucination effects, high computational complexity of training and inference, costly fine-tuning, and catastrophic forgetting issues. These limitations significantly hinder the use of neural networks in critical areas such as medicine, industrial process management, and scientific research. This article proposes an alternative approach based on the nearest neighbors method with hierarchical clustering structures. Employing the k-nearest neighbors algorithm significantly reduces or completely eliminates hallucination effects while simplifying model expansion and fine-tuning without the need for retraining the entire network. To overcome the high computational load of the k-nearest neighbors method, the paper proposes using tree-like data structures based on Kohonen self-organizing maps, thereby greatly accelerating nearest neighbor searches. Tests conducted on handwritten digit recognition and simple subtitle translation tasks confirmed the effectiveness of the proposed approach. With only a slight reduction in accuracy, the nearest neighbor search time was reduced hundreds of times compared to exhaustive search methods. The proposed method features transparency and interpretability, closely aligns with human cognitive mechanisms, and demonstrates potential for extensive use in tasks requiring high reliability and explainable results.

**Link**: [arxiv](http://arxiv.org/abs/2508.18953v1),  [pdf](http://arxiv.org/pdf/2508.18953v1)

**Tags**: cs.AI I.2.6; I.2.8; I.5.1 



### Energy-Based Flow Matching for Generating 3D Molecular Structure
**Authors**: Wenyin Zhou, Christopher Iliffe Sprague, Vsevolod Viliuga, Matteo Tadiello, Arne Elofsson, Hossein Azizpour

**Updated**: 2025-08-26T11:42:57Z

**Summary**: Molecular structure generation is a fundamental problem that involves determining the 3D positions of molecules' constituents. It has crucial biological applications, such as molecular docking, protein folding, and molecular design. Recent advances in generative modeling, such as diffusion models and flow matching, have made great progress on these tasks by modeling molecular conformations as a distribution. In this work, we focus on flow matching and adopt an energy-based perspective to improve training and inference of structure generation models. Our view results in a mapping function, represented by a deep network, that is directly learned to \textit{iteratively} map random configurations, i.e. samples from the source distribution, to target structures, i.e. points in the data manifold. This yields a conceptually simple and empirically effective flow matching setup that is theoretically justified and has interesting connections to fundamental properties such as idempotency and stability, as well as the empirically useful techniques such as structure refinement in AlphaFold. Experiments on protein docking as well as protein backbone generation consistently demonstrate the method's effectiveness, where it outperforms recent baselines of task-associated flow matching and diffusion models, using a similar computational budget.

**Link**: [arxiv](http://arxiv.org/abs/2508.18949v1),  [pdf](http://arxiv.org/pdf/2508.18949v1)

**Tags**: cs.LG 



### Unlocking New Paths for Science with Extreme-Mass-Ratio Inspirals:   Machine Learning-Enhanced MCMC for Accurate Parameter Inversion
**Authors**: Bo Liang, Chang Liu, Hanlin Song, Zhenwei Lyu, Minghui Du, Peng Xu, Ziren Luo, Sensen He, Haohao Gu, Tianyu Zhao, Manjia Liang Yuxiang Xu, Li-e Qiang, Mingming Sun, Wei-Liang Qian

**Updated**: 2025-08-26T11:42:44Z

**Summary**: The detection of gravitational waves from extreme-mass-ratio inspirals (EMRIs) in space-borne antennas like Taiji and LISA promises deep insights into strong-field gravity and black hole physics. However, the complex, highly degenerate, and non-convex likelihood landscapes characteristic of EMRI parameter spaces pose severe challenges for conventional Markov chain Monte Carlo (MCMC) methods. Under realistic instrumental noise and broad priors, these methods demand impractical computational costs but are prone to becoming trapped in local maxima, leading to biased and unreliable parameter estimates. To address this, we introduce Flow-Matching Markov Chain Monte Carlo (FM-MCMC), a novel Bayesian framework that integrates continuous normalizing flows (CNFs) with parallel tempering MCMC (PTMCMC). By generating high-likelihood regions via CNFs and refining them through PTMCMC, FM-MCMC enables robust exploration of the nontrivial parameter spaces, while achieving orders-of-magnitude improvement in computational efficiency and, more importantly, ensuring statistically reliable and unbiased inference. By enabling real-time, unbiased parameter inference, FM-MCMC could unlock the full scientific potential of EMRI observations, and would serve as a scalable pipeline for precision gravitational-wave astronomy.

**Link**: [arxiv](http://arxiv.org/abs/2508.00348v2),  [pdf](http://arxiv.org/pdf/2508.00348v2)

**Tags**: gr-qc astro-ph.CO astro-ph.IM physics.space-ph 



### Integrating Large Language Model for Improved Causal Discovery
**Authors**: Taiyu Ban, Lyuzhou Chen, Derui Lyu, Xiangyu Wang, Qinrui Zhu, Qiang Tu, Huanhuan Chen

**Updated**: 2025-08-26T11:41:14Z

**Summary**: Recovering the structure of causal graphical models from observational data is an essential yet challenging task for causal discovery in scientific scenarios. Domain-specific causal discovery usually relies on expert validation or prior analysis to improve the reliability of recovered causality, which is yet limited by the scarcity of expert resources. Recently, Large Language Models (LLM) have been used for causal analysis across various domain-specific scenarios, suggesting its potential as autonomous expert roles in guiding data-based structure learning. However, integrating LLMs into causal discovery faces challenges due to inaccuracies in LLM-based reasoning on revealing the actual causal structure. To address this challenge, we propose an error-tolerant LLM-driven causal discovery framework. The error-tolerant mechanism is designed three-fold with sufficient consideration on potential inaccuracies. In the LLM-based reasoning process, an accuracy-oriented prompting strategy restricts causal analysis to a reliable range. Next, a knowledge-to-structure transition aligns LLM-derived causal statements with structural causal interactions. In the structure learning process, the goodness-of-fit to data and adherence to LLM-derived priors are balanced to further address prior inaccuracies. Evaluation of eight real-world causal structures demonstrates the efficacy of our LLM-driven approach in improving data-based causal discovery, along with its robustness to inaccurate LLM-derived priors. Codes are available at https://github.com/tyMadara/LLM-CD.

**Link**: [arxiv](http://arxiv.org/abs/2306.16902v2),  [pdf](http://arxiv.org/pdf/2306.16902v2)

**Tags**: cs.AI 



### LLMs in the SOC: An Empirical Study of Human-AI Collaboration in   Security Operations Centres
**Authors**: Ronal Singh, Shahroz Tariq, Fatemeh Jalalvand, Mohan Baruwal Chhetri, Surya Nepal, Cecile Paris, Martin Lochner

**Updated**: 2025-08-26T11:40:02Z

**Summary**: The integration of Large Language Models (LLMs) into Security Operations Centres (SOCs) presents a transformative, yet still evolving, opportunity to reduce analyst workload through human-AI collaboration. However, their real-world application in SOCs remains underexplored. To address this gap, we present a longitudinal study of 3,090 analyst queries from 45 SOC analysts over 10 months. Our analysis reveals that analysts use LLMs as on-demand aids for sensemaking and context-building, rather than for making high-stakes determinations, preserving analyst decision authority. The majority of queries are related to interpreting low-level telemetry (e.g., commands) and refining technical communication through short (1-3 turn) interactions. Notably, 93% of queries align with established cybersecurity competencies (NICE Framework), underscoring the relevance of LLM use for SOC-related tasks. Despite variations in tasks and engagement, usage trends indicate a shift from occasional exploration to routine integration, with growing adoption and sustained use among a subset of analysts. We find that LLMs function as flexible, on-demand cognitive aids that augment, rather than replace, SOC expertise. Our study provides actionable guidance for designing context-aware, human-centred AI assistance in security operations, highlighting the need for further in-the-wild research on real-world analyst-LLM collaboration, challenges, and impacts.

**Link**: [arxiv](http://arxiv.org/abs/2508.18947v1),  [pdf](http://arxiv.org/pdf/2508.18947v1)

**Tags**: cs.CR 



### PanoHair: Detailed Hair Strand Synthesis on Volumetric Heads
**Authors**: Shashikant Verma, Shanmuganathan Raman

**Updated**: 2025-08-26T11:36:14Z

**Summary**: Achieving realistic hair strand synthesis is essential for creating lifelike digital humans, but producing high-fidelity hair strand geometry remains a significant challenge. Existing methods require a complex setup for data acquisition, involving multi-view images captured in constrained studio environments. Additionally, these methods have longer hair volume estimation and strand synthesis times, which hinder efficiency. We introduce PanoHair, a model that estimates head geometry as signed distance fields using knowledge distillation from a pre-trained generative teacher model for head synthesis. Our approach enables the prediction of semantic segmentation masks and 3D orientations specifically for the hair region of the estimated geometry. Our method is generative and can generate diverse hairstyles with latent space manipulations. For real images, our approach involves an inversion process to infer latent codes and produces visually appealing hair strands, offering a streamlined alternative to complex multi-view data acquisition setups. Given the latent code, PanoHair generates a clean manifold mesh for the hair region in under 5 seconds, along with semantic and orientation maps, marking a significant improvement over existing methods, as demonstrated in our experiments.

**Link**: [arxiv](http://arxiv.org/abs/2508.18944v1),  [pdf](http://arxiv.org/pdf/2508.18944v1)

**Tags**: cs.GR cs.CV 



### EnerSwap: Large-Scale, Privacy-First Automated Market Maker for V2G   Energy Trading
**Authors**: Ahmed Mounsf Rafik Bendada, Yacine Ghamri-Doudane

**Updated**: 2025-08-26T11:31:05Z

**Summary**: With the rapid growth of Electric Vehicle (EV) technology, EVs are destined to shape the future of transportation. The large number of EVs facilitates the development of the emerging vehicle-to-grid (V2G) technology, which realizes bidirectional energy exchanges between EVs and the power grid. This has led to the setting up of electricity markets that are usually confined to a small geographical location, often with a small number of participants. Usually, these markets are manipulated by intermediaries responsible for collecting bids from prosumers, determining the market-clearing price, incorporating grid constraints, and accounting for network losses. While centralized models can be highly efficient, they grant excessive power to the intermediary by allowing them to gain exclusive access to prosumers \textquotesingle price preferences. This opens the door to potential market manipulation and raises significant privacy concerns for users, such as the location of energy providers. This lack of protection exposes users to potential risks, as untrustworthy servers and malicious adversaries can exploit this information to infer trading activities and real identities. This work proposes a secure, decentralized exchange market built on blockchain technology, utilizing a privacy-preserving Automated Market Maker (AMM) model to offer open and fair, and equal access to traders, and mitigates the most common trading-manipulation attacks. Additionally, it incorporates a scalable architecture based on geographical dynamic sharding, allowing for efficient resource allocation and improved performance as the market grows.

**Link**: [arxiv](http://arxiv.org/abs/2508.18942v1),  [pdf](http://arxiv.org/pdf/2508.18942v1)

**Tags**: cs.CR 



### VISION: Robust and Interpretable Code Vulnerability Detection Leveraging   Counterfactual Augmentation
**Authors**: David Egea, Barproda Halder, Sanghamitra Dutta

**Updated**: 2025-08-26T11:20:39Z

**Summary**: Automated detection of vulnerabilities in source code is an essential cybersecurity challenge, underpinning trust in digital systems and services. Graph Neural Networks (GNNs) have emerged as a promising approach as they can learn structural and logical code relationships in a data-driven manner. However, their performance is severely constrained by training data imbalances and label noise. GNNs often learn 'spurious' correlations from superficial code similarities, producing detectors that fail to generalize well to unseen real-world data. In this work, we propose a unified framework for robust and interpretable vulnerability detection, called VISION, to mitigate spurious correlations by systematically augmenting a counterfactual training dataset. Counterfactuals are samples with minimal semantic modifications but opposite labels. Our framework includes: (i) generating counterfactuals by prompting a Large Language Model (LLM); (ii) targeted GNN training on paired code examples with opposite labels; and (iii) graph-based interpretability to identify the crucial code statements relevant for vulnerability predictions while ignoring spurious ones. We find that VISION reduces spurious learning and enables more robust, generalizable detection, improving overall accuracy (from 51.8% to 97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group accuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20 vulnerability. We further demonstrate gains using proposed metrics: intra-class attribution variance, inter-class attribution distance, and node score dependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (real and counterfactual) from the high-impact CWE-20 category. Finally, VISION advances transparent and trustworthy AI-based cybersecurity systems through interactive visualization for human-in-the-loop analysis.

**Link**: [arxiv](http://arxiv.org/abs/2508.18933v1),  [pdf](http://arxiv.org/pdf/2508.18933v1)

**Tags**: cs.AI 



### Waver: Wave Your Way to Lifelike Video Generation
**Authors**: Yifu Zhang, Hao Yang, Yuqi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei, Yi Jiang, Bingyue Peng, Zehuan Yuan

**Updated**: 2025-08-26T10:56:04Z

**Summary**: We present Waver, a high-performance foundation model for unified image and video generation. Waver can directly generate videos with durations ranging from 5 to 10 seconds at a native resolution of 720p, which are subsequently upscaled to 1080p. The model simultaneously supports text-to-video (T2V), image-to-video (I2V), and text-to-image (T2I) generation within a single, integrated framework. We introduce a Hybrid Stream DiT architecture to enhance modality alignment and accelerate training convergence. To ensure training data quality, we establish a comprehensive data curation pipeline and manually annotate and train an MLLM-based video quality model to filter for the highest-quality samples. Furthermore, we provide detailed training and inference recipes to facilitate the generation of high-quality videos. Building on these contributions, Waver excels at capturing complex motion, achieving superior motion amplitude and temporal consistency in video synthesis. Notably, it ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial Analysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming existing open-source models and matching or surpassing state-of-the-art commercial solutions. We hope this technical report will help the community more efficiently train high-quality video generation models and accelerate progress in video generation technologies. Official page: https://github.com/FoundationVision/Waver.

**Link**: [arxiv](http://arxiv.org/abs/2508.15761v2),  [pdf](http://arxiv.org/pdf/2508.15761v2)

**Tags**: cs.CV 



### Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement   Learning for General LLM Reasoning
**Authors**: Yang Zhou, Sunzhu Li, Shunyu Liu, Wenkai Fang, Jiale Zhao, Jingwen Yang, Jianwei Lv, Kongcheng Zhang, Yihe Zhou, Hengtong Lu, Wei Chen, Yan Xie, Mingli Song

**Updated**: 2025-08-26T10:52:15Z

**Summary**: Recent advances in Large Language Models (LLMs) have underscored the potential of Reinforcement Learning (RL) to facilitate the emergence of reasoning capabilities. Despite the encouraging results, a fundamental dilemma persists as RL improvement relies on learning from high-quality samples, yet the exploration for such samples remains bounded by the inherent limitations of LLMs. This, in effect, creates an undesirable cycle in which what cannot be explored cannot be learned. In this work, we propose Rubric-Scaffolded Reinforcement Learning (RuscaRL), a novel instructional scaffolding framework designed to break the exploration bottleneck for general LLM reasoning. Specifically, RuscaRL introduces checklist-style rubrics as (1) explicit scaffolding for exploration during rollout generation, where different rubrics are provided as external guidance within task instructions to steer diverse high-quality responses. This guidance is gradually decayed over time, encouraging the model to internalize the underlying reasoning patterns; (2) verifiable rewards for exploitation during model training, where we can obtain robust LLM-as-a-Judge scores using rubrics as references, enabling effective RL on general reasoning tasks. Extensive experiments demonstrate the superiority of the proposed RuscaRL across various benchmarks, effectively expanding reasoning boundaries under the best-of-N evaluation. Notably, RuscaRL significantly boosts Qwen2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500, surpassing GPT-4.1. Furthermore, our fine-tuned variant on Qwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading LLMs including OpenAI-o3. This work is still in progress, and we will release the code, the models, and the datasets soon.

**Link**: [arxiv](http://arxiv.org/abs/2508.16949v2),  [pdf](http://arxiv.org/pdf/2508.16949v2)

**Tags**: cs.LG cs.AI 



### DESAMO: A Device for Elder-Friendly Smart Homes Powered by Embedded LLM   with Audio Modality
**Authors**: Youngwon Choi, Donghyuk Jung, Hwayeon Kim

**Updated**: 2025-08-26T10:44:33Z

**Summary**: We present DESAMO, an on-device smart home system for elder-friendly use powered by Audio LLM, that supports natural and private interactions. While conventional voice assistants rely on ASR-based pipelines or ASR-LLM cascades, often struggling with the unclear speech common among elderly users and unable to handle non-speech audio, DESAMO leverages an Audio LLM to process raw audio input directly, enabling a robust understanding of user intent and critical events, such as falls or calls for help.

**Link**: [arxiv](http://arxiv.org/abs/2508.18918v1),  [pdf](http://arxiv.org/pdf/2508.18918v1)

**Tags**: cs.HC cs.SD eess.AS 



### Simulations in Statistical Workflows
**Authors**: Paul-Christian Bürkner, Marvin Schmitt, Stefan T. Radev

**Updated**: 2025-08-26T10:24:11Z

**Summary**: Simulations play important and diverse roles in statistical workflows, for example, in model specification, checking, validation, and even directly in model inference. Over the past decades, the application areas and overall potential of simulations in statistical workflows have expanded significantly, driven by the development of new simulation-based algorithms and exponentially increasing computational resources. In this paper, we examine past and current trends in the field and offer perspectives on how simulations may shape the future of statistical practice.

**Link**: [arxiv](http://arxiv.org/abs/2503.24011v2),  [pdf](http://arxiv.org/pdf/2503.24011v2)

**Tags**: stat.CO 



### Generative Artificial Intelligence and Agents in Research and Teaching
**Authors**: Jussi S. Jauhiainen, Aurora Toppari

**Updated**: 2025-08-26T10:23:02Z

**Summary**: This study provides a comprehensive analysis of the development, functioning, and application of generative artificial intelligence (GenAI) and large language models (LLMs), with an emphasis on their implications for research and education. It traces the conceptual evolution from artificial intelligence (AI) through machine learning (ML) and deep learning (DL) to transformer architectures, which constitute the foundation of contemporary generative systems. Technical aspects, including prompting strategies, word embeddings, and probabilistic sampling methods (temperature, top-k, and top-p), are examined alongside the emergence of autonomous agents. These elements are considered in relation to both the opportunities they create and the limitations and risks they entail.   The work critically evaluates the integration of GenAI across the research process, from ideation and literature review to research design, data collection, analysis, interpretation, and dissemination. While particular attention is given to geographical research, the discussion extends to wider academic contexts. A parallel strand addresses the pedagogical applications of GenAI, encompassing course and lesson design, teaching delivery, assessment, and feedback, with geography education serving as a case example.   Central to the analysis are the ethical, social, and environmental challenges posed by GenAI. Issues of bias, intellectual property, governance, and accountability are assessed, alongside the ecological footprint of LLMs and emerging technological strategies for mitigation. The concluding section considers near- and long-term futures of GenAI, including scenarios of sustained adoption, regulation, and potential decline. By situating GenAI within both scholarly practice and educational contexts, the study contributes to critical debates on its transformative potential and societal responsibilities.

**Link**: [arxiv](http://arxiv.org/abs/2508.16701v2),  [pdf](http://arxiv.org/pdf/2508.16701v2)

**Tags**: cs.CY cs.AI 



### Interactive Evaluation of Large Language Models for Multi-Requirement   Software Engineering Tasks
**Authors**: Dimitrios Rontogiannis, Maxime Peyrard, Nicolas Baldwin, Martin Josifoski, Robert West, Dimitrios Gunopulos

**Updated**: 2025-08-26T10:22:37Z

**Summary**: Standard single-turn, static benchmarks fall short in evaluating the nuanced capabilities of Large Language Models (LLMs) on complex tasks such as software engineering. In this work, we propose a novel interactive evaluation framework that assesses LLMs on multi-requirement programming tasks through structured, feedback-driven dialogue. Each task is modeled as a requirement dependency graph, and an ``interviewer'' LLM, aware of the ground-truth solution, provides minimal, targeted hints to an ``interviewee'' model to help correct errors and fulfill target constraints. This dynamic protocol enables fine-grained diagnostic insights into model behavior, uncovering strengths and systematic weaknesses that static benchmarks fail to measure. We build on DevAI, a benchmark of 55 curated programming tasks, by adding ground-truth solutions and evaluating the relevance and utility of interviewer hints through expert annotation. Our results highlight the importance of dynamic evaluation in advancing the development of collaborative code-generating agents.

**Link**: [arxiv](http://arxiv.org/abs/2508.18905v1),  [pdf](http://arxiv.org/pdf/2508.18905v1)

**Tags**: cs.AI 



### The Influence of Human-inspired Agentic Sophistication in LLM-driven   Strategic Reasoners
**Authors**: Vince Trencsenyi, Agnieszka Mensfelt, Kostas Stathis

**Updated**: 2025-08-26T10:19:45Z

**Summary**: The rapid rise of large language models (LLMs) has shifted artificial intelligence (AI) research toward agentic systems, motivating the use of weaker and more flexible notions of agency. However, this shift raises key questions about the extent to which LLM-based agents replicate human strategic reasoning, particularly in game-theoretic settings. In this context, we examine the role of agentic sophistication in shaping artificial reasoners' performance by evaluating three agent designs: a simple game-theoretic model, an unstructured LLM-as-agent model, and an LLM integrated into a traditional agentic framework. Using guessing games as a testbed, we benchmarked these agents against human participants across general reasoning patterns and individual role-based objectives. Furthermore, we introduced obfuscated game scenarios to assess agents' ability to generalise beyond training distributions. Our analysis, covering over 2000 reasoning samples across 25 agent configurations, shows that human-inspired cognitive structures can enhance LLM agents' alignment with human strategic behaviour. Still, the relationship between agentic design complexity and human-likeness is non-linear, highlighting a critical dependence on underlying LLM capabilities and suggesting limits to simple architectural augmentation.

**Link**: [arxiv](http://arxiv.org/abs/2505.09396v2),  [pdf](http://arxiv.org/pdf/2505.09396v2)

**Tags**: cs.AI cs.MA 



### Debate-to-Detect: Reformulating Misinformation Detection as a Real-World   Debate with Large Language Models
**Authors**: Chen Han, Wenzhen Zheng, Xijin Tang

**Updated**: 2025-08-26T10:08:51Z

**Summary**: The proliferation of misinformation in digital platforms reveals the limitations of traditional detection methods, which mostly rely on static classification and fail to capture the intricate process of real-world fact-checking. Despite advancements in Large Language Models (LLMs) that enhance automated reasoning, their application to misinformation detection remains hindered by issues of logical inconsistency and superficial verification. In response, we introduce Debate-to-Detect (D2D), a novel Multi-Agent Debate (MAD) framework that reformulates misinformation detection as a structured adversarial debate. Inspired by fact-checking workflows, D2D assigns domain-specific profiles to each agent and orchestrates a five-stage debate process, including Opening Statement, Rebuttal, Free Debate, Closing Statement, and Judgment. To transcend traditional binary classification, D2D introduces a multi-dimensional evaluation mechanism that assesses each claim across five distinct dimensions: Factuality, Source Reliability, Reasoning Quality, Clarity, and Ethics. Experiments with GPT-4o on two datasets demonstrate significant improvements over baseline methods, and the case study highlight D2D's capability to iteratively refine evidence while improving decision transparency, representing a substantial advancement towards interpretable misinformation detection. The code will be released publicly after the official publication.

**Link**: [arxiv](http://arxiv.org/abs/2505.18596v4),  [pdf](http://arxiv.org/pdf/2505.18596v4)

**Tags**: cs.CL cs.AI 



### pyFAST: A Modular PyTorch Framework for Time Series Modeling with   Multi-source and Sparse Data
**Authors**: Zhijin Wang, Senzhen Wu, Yue Hu, Xiufeng Liu

**Updated**: 2025-08-26T10:05:47Z

**Summary**: Modern time series analysis demands frameworks that are flexible, efficient, and extensible. However, many existing Python libraries exhibit limitations in modularity and in their native support for irregular, multi-source, or sparse data. We introduce pyFAST, a research-oriented PyTorch framework that explicitly decouples data processing from model computation, fostering a cleaner separation of concerns and facilitating rapid experimentation. Its data engine is engineered for complex scenarios, supporting multi-source loading, protein sequence handling, efficient sequence- and patch-level padding, dynamic normalization, and mask-based modeling for both imputation and forecasting. pyFAST integrates LLM-inspired architectures for the alignment-free fusion of sparse data sources and offers native sparse metrics, specialized loss functions, and flexible exogenous data fusion. Training utilities include batch-based streaming aggregation for evaluation and device synergy to maximize computational efficiency. A comprehensive suite of classical and deep learning models (Linears, CNNs, RNNs, Transformers, and GNNs) is provided within a modular architecture that encourages extension. Released under the MIT license at GitHub, pyFAST provides a compact yet powerful platform for advancing time series research and applications.

**Link**: [arxiv](http://arxiv.org/abs/2508.18891v1),  [pdf](http://arxiv.org/pdf/2508.18891v1)

**Tags**: cs.LG cs.AI 



### HAEPO: History-Aggregated Exploratory Policy Optimization
**Authors**: Gaurish Trivedi, Alakh Sharma, Kartikey Singh Bhandari, Dhruv Kumar, Pratik Narang, Jagat Sesh Challa

**Updated**: 2025-08-26T09:59:44Z

**Summary**: Exploration is essential in modern learning, from reinforcement learning environments with small neural policies to large language models (LLMs). Existing work, such as DPO, leverages full sequence log-likelihoods to capture an entire trajectory of the model's decisions, while methods like GRPO aggregate per-token ratios into a trajectory-level update. However, both often limit exploration on long-horizon tasks. We introduce History-Aggregated Exploratory Policy Optimization (HAEPO), a history-aware exploratory loss to combat these shortcomings. HAEPO compresses each trajectory into the sum of its logarithmic probabilities (a cumulative logarithmic likelihood), and applies a Plackett-Luce softmax across trajectories to obtain normalized weights proportional to their returns, thus encouraging broader exploration. We add entropy regularization to stabilize the aggressive updates to prevent premature collapse and a soft KL penalty relative to a frozen copy of the previous (reference) policy. Empirically, HAEPO converges fast, explores thoroughly, aligns closely with true rewards, and demonstrates robust learning behavior better or at par with PPO, GRPO, and DPO across diverse tasks. Thus, HAEPO provides a stable and interpretable framework by explicitly leveraging full-trajectory history while balancing exploration and stability.

**Link**: [arxiv](http://arxiv.org/abs/2508.18884v1),  [pdf](http://arxiv.org/pdf/2508.18884v1)

**Tags**: cs.LG cs.AI 



### Judicial Requirements for Generative AI in Legal Reasoning
**Authors**: Eljas Linna, Tuula Linna

**Updated**: 2025-08-26T09:56:26Z

**Summary**: Large Language Models (LLMs) are being integrated into professional domains, yet their limitations in high-stakes fields like law remain poorly understood. This paper defines the core capabilities that an AI system must possess to function as a reliable reasoning tool in judicial decision-making. Using the IRAC (Issue-Rule-Application-Conclusion) model as an analytical framework, the study focuses on the most challenging phases of legal adjudication: determining the applicable Rule (R) and performing the Application (A) of that rule to the facts of a case. From a judicial perspective, the analysis deconstructs legal reasoning into a series of core requirements, including the ability to select the correct legal framework across jurisdictions, generate sound arguments based on the doctrine of legal sources, distinguish ratio decidendi from obiter dictum in case law, resolve ambiguity arising from general clauses like "reasonableness", manage conflicting legal provisions, and correctly apply the burden of proof. The paper then maps various AI enhancement mechanisms, such as Retrieval-Augmented Generation (RAG), multi-agent systems, and neuro-symbolic AI, to these requirements, assessing their potential to bridge the gap between the probabilistic nature of LLMs and the rigorous, choice-driven demands of legal interpretation. The findings indicate that while these techniques can address specific challenges, significant challenges remain, particularly in tasks requiring discretion and transparent, justifiable reasoning. Our paper concludes that the most effective current role for AI in law is a dual one: as a high-volume assistant for simple, repetitive cases and as a sophisticated "sparring partner" for human experts in complex matters.

**Link**: [arxiv](http://arxiv.org/abs/2508.18880v1),  [pdf](http://arxiv.org/pdf/2508.18880v1)

**Tags**: cs.AI 



### PointFix: Learning to Fix Domain Bias for Robust Online Stereo   Adaptation
**Authors**: Kwonyoung Kim, Jungin Park, Jiyoung Lee, Dongbo Min, Kwanghoon Sohn

**Updated**: 2025-08-26T09:55:56Z

**Summary**: Online stereo adaptation tackles the domain shift problem, caused by different environments between synthetic (training) and real (test) datasets, to promptly adapt stereo models in dynamic real-world applications such as autonomous driving. However, previous methods often fail to counteract particular regions related to dynamic objects with more severe environmental changes. To mitigate this issue, we propose to incorporate an auxiliary point-selective network into a meta-learning framework, called PointFix, to provide a robust initialization of stereo models for online stereo adaptation. In a nutshell, our auxiliary network learns to fix local variants intensively by effectively back-propagating local information through the meta-gradient for the robust initialization of the baseline model. This network is model-agnostic, so can be used in any kind of architectures in a plug-and-play manner. We conduct extensive experiments to verify the effectiveness of our method under three adaptation settings such as short-, mid-, and long-term sequences. Experimental results show that the proper initialization of the base stereo model by the auxiliary network enables our learning paradigm to achieve state-of-the-art performance at inference.

**Link**: [arxiv](http://arxiv.org/abs/2207.13340v2),  [pdf](http://arxiv.org/pdf/2207.13340v2)

**Tags**: cs.CV cs.LG 



### Optimization of Latent-Space Compression using Game-Theoretic Techniques   for Transformer-Based Vector Search
**Authors**: Kushagra Agrawal, Nisharg Nargund, Oishani Banerjee

**Updated**: 2025-08-26T09:51:02Z

**Summary**: Vector similarity search plays a pivotal role in modern information retrieval systems, especially when powered by transformer-based embeddings. However, the scalability and efficiency of such systems are often hindered by the high dimensionality of latent representations. In this paper, we propose a novel game-theoretic framework for optimizing latent-space compression to enhance both the efficiency and semantic utility of vector search. By modeling the compression strategy as a zero-sum game between retrieval accuracy and storage efficiency, we derive a latent transformation that preserves semantic similarity while reducing redundancy. We benchmark our method against FAISS, a widely-used vector search library, and demonstrate that our approach achieves a significantly higher average similarity (0.9981 vs. 0.5517) and utility (0.8873 vs. 0.5194), albeit with a modest increase in query time. This trade-off highlights the practical value of game-theoretic latent compression in high-utility, transformer-based search applications. The proposed system can be seamlessly integrated into existing LLM pipelines to yield more semantically accurate and computationally efficient retrieval.

**Link**: [arxiv](http://arxiv.org/abs/2508.18877v1),  [pdf](http://arxiv.org/pdf/2508.18877v1)

**Tags**: cs.IR cs.AI cs.LG 



### Empowering Computing Education Researchers Through LLM-Assisted Content   Analysis
**Authors**: Laurie Gale, Sebastian Mateos Nicolajsen

**Updated**: 2025-08-26T09:46:59Z

**Summary**: Computing education research (CER) is often instigated by practitioners wanting to improve both their own and the wider discipline's teaching practice. However, the latter is often difficult as many researchers lack the colleagues, resources, or capacity to conduct research that is generalisable or rigorous enough to advance the discipline. As a result, research methods that enable sense-making with larger volumes of qualitative data, while not increasing the burden on the researcher, have significant potential within CER.   In this discussion paper, we propose such a method for conducting rigorous analysis on large volumes of textual data, namely a variation of LLM-assisted content analysis (LACA). This method combines content analysis with the use of large language models, empowering researchers to conduct larger-scale research which they would otherwise not be able to perform. Using a computing education dataset, we illustrate how LACA could be applied in a reproducible and rigorous manner. We believe this method has potential in CER, enabling more generalisable findings from a wider range of research. This, together with the development of similar methods, can help to advance both the practice and research quality of the CER discipline.

**Link**: [arxiv](http://arxiv.org/abs/2508.18872v1),  [pdf](http://arxiv.org/pdf/2508.18872v1)

**Tags**: cs.CL 



### ReflectivePrompt: Reflective evolution in autoprompting algorithms
**Authors**: Viktor N. Zhuravlev, Artur R. Khairullin, Ernest A. Dyagin, Alena N. Sitkina, Nikita I. Kulin

**Updated**: 2025-08-26T09:46:20Z

**Summary**: Autoprompting is the process of automatically selecting optimized prompts for language models, which has been gaining popularity with the rapid advancement of prompt engineering, driven by extensive research in the field of large language models (LLMs). This paper presents ReflectivePrompt - a novel autoprompting method based on evolutionary algorithms that employs a reflective evolution approach for more precise and comprehensive search of optimal prompts. ReflectivePrompt utilizes short-term and long-term reflection operations before crossover and elitist mutation to enhance the quality of the modifications they introduce. This method allows for the accumulation of knowledge obtained throughout the evolution process and updates it at each epoch based on the current population. ReflectivePrompt was tested on 33 datasets for classification and text generation tasks using open-access large language models: t-lite-instruct-0.1 and gemma3-27b-it. The method demonstrates, on average, a significant improvement (e.g., 28% on BBH compared to EvoPrompt) in metrics relative to current state-of-the-art approaches, thereby establishing itself as one of the most effective solutions in evolutionary algorithm-based autoprompting.

**Link**: [arxiv](http://arxiv.org/abs/2508.18870v1),  [pdf](http://arxiv.org/pdf/2508.18870v1)

**Tags**: cs.CL cs.AI cs.LG 



### Harnessing Meta-Learning for Controllable Full-Frame Video Stabilization
**Authors**: Muhammad Kashif Ali, Eun Woo Im, Dongjin Kim, Tae Hyun Kim, Vivek Gupta, Haonan Luo, Tianrui Li

**Updated**: 2025-08-26T09:38:29Z

**Summary**: Video stabilization remains a fundamental problem in computer vision, particularly pixel-level synthesis solutions for video stabilization, which synthesize full-frame outputs, add to the complexity of this task. These methods aim to enhance stability while synthesizing full-frame videos, but the inherent diversity in motion profiles and visual content present in each video sequence makes robust generalization with fixed parameters difficult. To address this, we present a novel method that improves pixel-level synthesis video stabilization methods by rapidly adapting models to each input video at test time. The proposed approach takes advantage of low-level visual cues available during inference to improve both the stability and visual quality of the output. Notably, the proposed rapid adaptation achieves significant performance gains even with a single adaptation pass. We further propose a jerk localization module and a targeted adaptation strategy, which focuses the adaptation on high-jerk segments for maximizing stability with fewer adaptation steps. The proposed methodology enables modern stabilizers to overcome the longstanding SOTA approaches while maintaining the full frame nature of the modern methods, while offering users with control mechanisms akin to classical approaches. Extensive experiments on diverse real-world datasets demonstrate the versatility of the proposed method. Our approach consistently improves the performance of various full-frame synthesis models in both qualitative and quantitative terms, including results on downstream applications.

**Link**: [arxiv](http://arxiv.org/abs/2508.18859v1),  [pdf](http://arxiv.org/pdf/2508.18859v1)

**Tags**: cs.CV 



### Think before you fit: parameter identifiability, sensitivity and   uncertainty in systems biology models
**Authors**: Simon P. Preston, Richard D. Wilkinson, Richard H. Clayton, Mike J. Chappell, Gary R. Mirams

**Updated**: 2025-08-26T09:34:05Z

**Summary**: Reliable predictions from systems biology models require knowing whether parameters can be estimated from available data, and with what certainty. Identifiability analysis reveals whether parameters are learnable in principle (structural identifiability) and in practice (practical identifiability). We introduce the core ideas using linear models, highlighting how experimental design and output sensitivity shape identifiability. In nonlinear models, identifiability can vary with parameter values, motivating global and simulation-based approaches. We summarise computational methods for assessing identifiability noting that weakly identifiable parameters can undermine predictions beyond the calibration dataset. Strategies to improve identifiability include measuring different outputs, refining model structure, and adding prior knowledge. Far from a technical afterthought, identifiability determines the limits of inference and prediction. Recognising and addressing identifiability is essential for building models that are not only well-fitted to data, but also capable of delivering predictions with robust, quantifiable uncertainty.

**Link**: [arxiv](http://arxiv.org/abs/2508.18853v1),  [pdf](http://arxiv.org/pdf/2508.18853v1)

**Tags**: stat.ME math.ST q-bio.QM stat.AP stat.TH 62P10, 92BXX 



### ClusterFusion: Expanding Operator Fusion Scope for LLM Inference via   Cluster-Level Collective Primitive
**Authors**: Xinhao Luo, Zihan Liu, Yangjie Zhou, Shihan Fang, Ziyu Huang, Yu Feng, Chen Zhang, Shixuan Sun, Zhenzhe Zheng, Jingwen Leng, Minyi Guo

**Updated**: 2025-08-26T09:29:23Z

**Summary**: Large language model (LLM) decoding suffers from high latency due to fragmented execution across operators and heavy reliance on off-chip memory for data exchange and reduction. This execution model limits opportunities for fusion and incurs significant memory traffic and kernel launch overhead. While modern architectures such as NVIDIA Hopper provide distributed shared memory and low-latency intra-cluster interconnects, they expose only low-level data movement instructions, lacking structured abstractions for collective on-chip communication. To bridge this software-hardware gap, we introduce two cluster-level communication primitives, ClusterReduce and ClusterGather, which abstract common communication patterns and enable structured, high-speed data exchange and reduction between thread blocks within a cluster, allowing intermediate results to be on-chip without involving off-chip memory. Building on these abstractions, we design ClusterFusion, an execution framework that schedules communication and computation jointly to expand operator fusion scope by composing decoding stages such as QKV Projection, Attention, and Output Projection into a single fused kernels. Evaluations on H100 GPUs show that ClusterFusion outperforms state-of-the-art inference frameworks by 1.61x on average in end-to-end latency across different models and configurations. The source code is available at https://github.com/xinhao-luo/ClusterFusion.

**Link**: [arxiv](http://arxiv.org/abs/2508.18850v1),  [pdf](http://arxiv.org/pdf/2508.18850v1)

**Tags**: cs.DC cs.AI 



## Keyword: LLM Deployment 
 ### Cohort-Aware Agents for Individualized Lung Cancer Risk Prediction Using   a Retrieval-Augmented Model Selection Framework
**Authors**: Chongyu Qu, Allen J. Luna, Thomas Z. Li, Junchao Zhu, Junlin Guo, Juming Xiong, Kim L. Sandler, Bennett A. Landman, Yuankai Huo

**Updated**: 2025-08-26T17:59:53Z

**Summary**: Accurate lung cancer risk prediction remains challenging due to substantial variability across patient populations and clinical settings -- no single model performs best for all cohorts. To address this, we propose a personalized lung cancer risk prediction agent that dynamically selects the most appropriate model for each patient by combining cohort-specific knowledge with modern retrieval and reasoning techniques. Given a patient's CT scan and structured metadata -- including demographic, clinical, and nodule-level features -- the agent first performs cohort retrieval using FAISS-based similarity search across nine diverse real-world cohorts to identify the most relevant patient population from a multi-institutional database. Second, a Large Language Model (LLM) is prompted with the retrieved cohort and its associated performance metrics to recommend the optimal prediction algorithm from a pool of eight representative models, including classical linear risk models (e.g., Mayo, Brock), temporally-aware models (e.g., TD-VIT, DLSTM), and multi-modal computer vision-based approaches (e.g., Liao, Sybil, DLS, DLI). This two-stage agent pipeline -- retrieval via FAISS and reasoning via LLM -- enables dynamic, cohort-aware risk prediction personalized to each patient's profile. Building on this architecture, the agent supports flexible and cohort-driven model selection across diverse clinical populations, offering a practical path toward individualized risk assessment in real-world lung cancer screening.

**Link**: [arxiv](http://arxiv.org/abs/2508.14940v2),  [pdf](http://arxiv.org/pdf/2508.14940v2)

**Tags**: cs.LG 



### Generative Interfaces for Language Models
**Authors**: Jiaqi Chen, Yanzhe Zhang, Yutong Zhang, Yijia Shao, Diyi Yang

**Updated**: 2025-08-26T17:43:20Z

**Summary**: Large language models (LLMs) are increasingly seen as assistants, copilots, and consultants, capable of supporting a wide range of tasks through natural conversation. However, most systems remain constrained by a linear request-response format that often makes interactions inefficient in multi-turn, information-dense, and exploratory tasks. To address these limitations, we propose Generative Interfaces for Language Models, a paradigm in which LLMs respond to user queries by proactively generating user interfaces (UIs) that enable more adaptive and interactive engagement. Our framework leverages structured interface-specific representations and iterative refinements to translate user queries into task-specific UIs. For systematic evaluation, we introduce a multidimensional assessment framework that compares generative interfaces with traditional chat-based ones across diverse tasks, interaction patterns, and query types, capturing functional, interactive, and emotional aspects of user experience. Results show that generative interfaces consistently outperform conversational ones, with humans preferring them in over 70% of cases. These findings clarify when and why users favor generative interfaces, paving the way for future advancements in human-AI interaction.

**Link**: [arxiv](http://arxiv.org/abs/2508.19227v1),  [pdf](http://arxiv.org/pdf/2508.19227v1)

**Tags**: cs.CL cs.AI cs.HC 



### From Intents to Conversations: Generating Intent-Driven Dialogues with   Contrastive Learning for Multi-Turn Classification
**Authors**: Junhua Liu, Yong Keat Tan, Bin Fu, Kwan Hui Lim

**Updated**: 2025-08-26T17:22:52Z

**Summary**: In conversational AI systems, a critical challenge in training effective multi-turn intent classification models lies in the generation of large-scale, domain-specific, multilingual dialogue datasets. In this paper, we introduce Chain-of-Intent, a novel framework that integrates Hidden Markov Models (HMMs) with Large Language Models (LLMs) to generate intent-driven, context-aware dialogues through self-play. Our method first extracts domain-specific intent transition patterns from real-world e-commerce chat logs, which guide the modeling of turn-level dynamics and intent sequences. LLMs are then employed to parameterize the emission probabilities of HMMs, enabling the generation of natural, coherent utterances aligned with predicted intents and dialogue context. We further propose MINT-CL, a multi-task contrastive learning framework for multi-turn intent classification, which improves performance while reducing dependence on large-scale annotated datasets. Empirical results demonstrate that our approach outperforms competitive baselines in both dialogue generation quality and classification accuracy, particularly in multilingual settings. To facilitate future research, we release MINT-E, a comprehensive, multilingual, intent-aware multi-turn dialogue corpus derived from the e-commerce domain. The reproduced source code and dataset are available at https://github.com/junhua/chain-of-intent.

**Link**: [arxiv](http://arxiv.org/abs/2411.14252v2),  [pdf](http://arxiv.org/pdf/2411.14252v2)

**Tags**: cs.CL cs.AI 



### Route-and-Execute: Auditable Model-Card Matching and Specialty-Level   Deployment
**Authors**: Shayan Vassef, Soorya Ram Shimegekar, Abhay Goyal, Koustuv Saha, Pi Zonooz, Navin Kumar

**Updated**: 2025-08-26T17:13:21Z

**Summary**: Clinical workflows are fragmented as a patchwork of scripts and task-specific networks that often handle triage, task selection, and model deployment. These pipelines are rarely streamlined for data science pipeline, reducing efficiency and raising operational costs. Workflows also lack data-driven model identification (from imaging/tabular inputs) and standardized delivery of model outputs. In response, we present a practical, healthcare-first framework that uses a single vision-language model (VLM) in two complementary roles. First (Solution 1), the VLM acts as an aware model-card matcher that routes an incoming image to the appropriate specialist model via a three-stage workflow (modality -> primary abnormality -> model-card id). Checks are provided by (i) stagewise prompts that allow early exit via None/Normal/Other and (ii) a stagewise answer selector that arbitrates between the top-2 candidates at each stage, reducing the chance of an incorrect selection and aligning the workflow with clinical risk tolerance. Second (Solution 2), we fine-tune the VLM on specialty-specific datasets ensuring a single model covers multiple downstream tasks within each specialty, maintaining performance while simplifying deployment. Across gastroenterology, hematology, ophthalmology, and pathology, our single-model deployment matches or approaches specialized baselines.   Compared with pipelines composed of many task-specific agents, this approach shows that one VLM can both decide and do. It may reduce effort by data scientists, shorten monitoring, increase the transparency of model selection (with per-stage justifications), and lower integration overhead.

**Link**: [arxiv](http://arxiv.org/abs/2508.16839v2),  [pdf](http://arxiv.org/pdf/2508.16839v2)

**Tags**: cs.AI 



### Bridging the Editing Gap in LLMs: FineEdit for Precise and Targeted Text   Modifications
**Authors**: Yiming Zeng, Wanhao Yu, Zexin Li, Tao Ren, Yu Ma, Jinghan Cao, Xiyan Chen, Tingting Yu

**Updated**: 2025-08-26T17:11:42Z

**Summary**: Large Language Models (LLMs) have significantly advanced natural language processing, demonstrating strong capabilities in tasks such as text generation, summarization, and reasoning. Recently, their potential for automating precise text editing tasks across specialized domains, such as programming code, LaTeX, and structured database languages, has gained attention. However, current state-of-the-art LLMs still struggle with executing precise, instruction-driven edits, particularly when structural accuracy and strict adherence to domain conventions are required. To address these challenges, we introduce InstrEditBench, an automated benchmark dataset comprising over 30,000 structured editing tasks spanning diverse domains, including Wikipedia articles, LaTeX documents, source code, and database languages. Using this benchmark, we develop FineEdit, a specialized editing model explicitly trained for accurate, context-aware text modifications. Experimental evaluations demonstrate that FineEdit outperforms state-of-the-art models, achieving improvements of approximately 10\% over Gemini models on single-turn edits, up to 30\% over Llama-3.2-3B, and exceeding Mistral-7B-OpenOrca performance by over 40\% on direct editing tasks. FineEdit also effectively generalizes to realistic multi-turn editing scenarios, highlighting its practical applicability. To facilitate further research and reproducibility, we release FineEdit at https://github.com/StuRinDQB/FineEdit} and https://huggingface.co/datasets/YimingZeng/FineEdit_bench.

**Link**: [arxiv](http://arxiv.org/abs/2502.13358v3),  [pdf](http://arxiv.org/pdf/2502.13358v3)

**Tags**: cs.CL 



### Optimizing Highway Traffic Flow in Mixed Autonomy: A Multiagent   Truncated Rollout Approach
**Authors**: Lu Liu, Chi Xie, Xi Xiong

**Updated**: 2025-08-26T17:04:25Z

**Summary**: The development of connected and autonomous vehicles (CAVs) offers substantial opportunities to enhance traffic efficiency. However, in mixed autonomy environments where CAVs coexist with human-driven vehicles (HDVs), achieving efficient coordination among CAVs remains challenging due to heterogeneous driving behaviors. To address this, this paper proposes a multiagent truncated rollout approach that enhances CAV speed coordination to improve highway throughput while reducing computational overhead. In this approach, a traffic density evolution equation is formulated that comprehensively accounts for the presence or absence of CAVs, and a distributed coordination control framework is established accordingly. By incorporating kinematic information from neighbor agents and employing an agent-by-agent sequential solution mechanism, our method enables explicit cooperation among CAVs. Furthermore, we introduce a truncated rollout scheme that adaptively shortens the optimization horizon based on the evaluation of control sequences. This significantly reduces the time complexity, thereby improving real-time performance and scalability. Theoretical analysis provides rigorous guarantees on the stability and performance improvement of the system. Simulations conducted on real-world bottleneck scenarios demonstrate that, in large-scale mixed traffic flows, the proposed method outperforms conventional model predictive control methods by reducing both the average travel time in the bottleneck area and overall computational time, highlighting its strong potential for practical deployment.

**Link**: [arxiv](http://arxiv.org/abs/2508.19203v1),  [pdf](http://arxiv.org/pdf/2508.19203v1)

**Tags**: cs.MA 



### Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and   Reasoning
**Authors**: Alan Li, Yixin Liu, Arpan Sarkar, Doug Downey, Arman Cohan

**Updated**: 2025-08-26T17:04:23Z

**Summary**: Scientific problem solving poses unique challenges for LLMs, requiring both deep domain knowledge and the ability to apply such knowledge through complex reasoning. While automated scientific reasoners hold great promise for assisting human scientists, there is currently no widely adopted holistic benchmark for evaluating scientific reasoning, and few approaches systematically disentangle the distinct roles of knowledge and reasoning in these tasks. To address these gaps, we introduce SciReas, a diverse suite of existing benchmarks for scientific reasoning tasks, and SciReas-Pro, a selective subset that requires more complex reasoning. Our holistic evaluation surfaces insights about scientific reasoning performance that remain hidden when relying on individual benchmarks alone. We then propose KRUX, a probing framework for studying the distinct roles of reasoning and knowledge in scientific tasks. Combining the two, we conduct an in-depth analysis that yields several key findings: (1) Retrieving task-relevant knowledge from model parameters is a critical bottleneck for LLMs in scientific reasoning; (2) Reasoning models consistently benefit from external knowledge added in-context on top of the reasoning enhancement; (3) Enhancing verbalized reasoning improves LLMs' ability to surface task-relevant knowledge. Finally, we conduct a lightweight analysis, comparing our science-focused data composition with concurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline for scientific reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2508.19202v1),  [pdf](http://arxiv.org/pdf/2508.19202v1)

**Tags**: cs.CL 



### Understanding Tool-Integrated Reasoning
**Authors**: Heng Lin, Zhongwen Xu

**Updated**: 2025-08-26T17:03:46Z

**Summary**: We study why Tool-Integrated Reasoning (TIR) makes Large Language Models (LLMs) more capable. While LLMs integrated with tools like Python code interpreters show great promise, a principled theory explaining why this paradigm is effective has been missing. This work provides the first formal proof that TIR fundamentally expands an LLM's capabilities. We demonstrate that tools enable a strict expansion of the model's empirical and feasible support, breaking the capability ceiling of pure-text models by unlocking problem-solving strategies that are otherwise impossible or intractably verbose. To guide model behavior without compromising training stability and performance, we also introduce Advantage Shaping Policy Optimization (ASPO), a novel algorithm that directly modifies the advantage function to guide the policy behavior. We conduct comprehensive experiments on challenging mathematical benchmarks, leveraging a Python interpreter as the external tool. Our results show that the TIR model decisively outperforms its pure-text counterpart on the pass@k metric. Crucially, this advantage is not confined to computationally-intensive problems but extends to those requiring significant abstract insight. We further identify the emergent cognitive patterns that illustrate how models learn to think with tools. Finally, we report improved tool usage behavior with early code invocation and much more interactive turns with ASPO. Overall, our work provides the first principled explanation for TIR's success, shifting the focus from the mere fact that tools work to why and how they enable more powerful reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2508.19201v1),  [pdf](http://arxiv.org/pdf/2508.19201v1)

**Tags**: cs.LG cs.AI stat.ML 



### The Ramon Llull's Thinking Machine for Automated Ideation
**Authors**: Xinran Zhao, Boyuan Zheng, Chenglei Si, Haofei Yu, Ken Liu, Runlong Zhou, Ruochen Li, Tong Chen, Xiang Li, Yiming Zhang, Tongshuang Wu

**Updated**: 2025-08-26T17:03:43Z

**Summary**: This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for generating knowledge through symbolic recombination - as a conceptual foundation for building a modern Llull's thinking machine for research ideation. Our approach defines three compositional axes: Theme (e.g., efficiency, adaptivity), Domain (e.g., question answering, machine translation), and Method (e.g., adversarial training, linear attention). These elements represent high-level abstractions common in scientific work - motivations, problem settings, and technical approaches - and serve as building blocks for LLM-driven exploration. We mine elements from human experts or conference papers and show that prompting LLMs with curated combinations produces research ideas that are diverse, relevant, and grounded in current literature. This modern thinking machine offers a lightweight, interpretable tool for augmenting scientific creativity and suggests a path toward collaborative ideation between humans and AI.

**Link**: [arxiv](http://arxiv.org/abs/2508.19200v1),  [pdf](http://arxiv.org/pdf/2508.19200v1)

**Tags**: cs.AI cs.CL 



### Get Global Guarantees: On the Probabilistic Nature of Perturbation   Robustness
**Authors**: Wenchuan Mu, Kwan Hui Lim

**Updated**: 2025-08-26T16:41:04Z

**Summary**: In safety-critical deep learning applications, robustness measures the ability of neural models that handle imperceptible perturbations in input data, which may lead to potential safety hazards. Existing pre-deployment robustness assessment methods typically suffer from significant trade-offs between computational cost and measurement precision, limiting their practical utility. To address these limitations, this paper conducts a comprehensive comparative analysis of existing robustness definitions and associated assessment methodologies. We propose tower robustness to evaluate robustness, which is a novel, practical metric based on hypothesis testing to quantitatively evaluate probabilistic robustness, enabling more rigorous and efficient pre-deployment assessments. Our extensive comparative evaluation illustrates the advantages and applicability of our proposed approach, thereby advancing the systematic understanding and enhancement of model robustness in safety-critical deep learning applications.

**Link**: [arxiv](http://arxiv.org/abs/2508.19183v1),  [pdf](http://arxiv.org/pdf/2508.19183v1)

**Tags**: cs.LG 



### TL-Training: A Task-Feature-Based Framework for Training Large Language   Models in Tool Use
**Authors**: Junjie Ye, Yilong Wu, Sixian Li, Yuming Yang, Zhiheng Xi, Tao Gui, Qi Zhang, Xuanjing Huang, Peng Wang, Zhongchao Shi, Jianping Fan, Zhengyin Du

**Updated**: 2025-08-26T16:40:49Z

**Summary**: Large language models (LLMs) achieve remarkable advancements by leveraging tools to interact with environments, a critical step toward generalized AI. However, the standard supervised fine-tuning (SFT) approach, which relies on large-scale datasets, often overlooks task-specific characteristics in tool use, leading to performance bottlenecks. To address this issue, we analyze three existing LLMs and uncover key insights: training data can inadvertently impede tool-use behavior, token importance is distributed unevenly, and errors in tool calls fall into a small set of categories. Building on these findings, we propose~\emph{TL-Training}, a task-feature-based framework that mitigates the effects of suboptimal training data, dynamically adjusts token weights to prioritize key tokens during SFT, and incorporates a robust reward mechanism tailored to error categories, optimized through proximal policy optimization. We validate TL-Training by training CodeLLaMA-2-7B and evaluating it on four open-source test sets. Our results demonstrate that the LLM trained by our method matches or surpasses both open- and closed-source LLMs in tool-use performance using only 1,217 training data points. Additionally, our method enhances robustness in noisy environments and improves general task performance, offering a scalable and efficient paradigm for tool-use training in LLMs. Code and data are available at https://github.com/Junjie-Ye/TL-Training.

**Link**: [arxiv](http://arxiv.org/abs/2412.15495v2),  [pdf](http://arxiv.org/pdf/2412.15495v2)

**Tags**: cs.CL cs.AI 



### MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection
**Authors**: Zhihao Zhang, Abhinav Kumar, Girish Chandar Ganesan, Xiaoming Liu

**Updated**: 2025-08-26T16:31:32Z

**Summary**: Accurately predicting 3D attributes is crucial for monocular 3D object detection (Mono3D), with depth estimation posing the greatest challenge due to the inherent ambiguity in mapping 2D images to 3D space. While existing methods leverage multiple depth cues (e.g., estimating depth uncertainty, modeling depth error) to improve depth accuracy, they overlook that accurate depth prediction requires conditioning on other 3D attributes, as these attributes are intrinsically inter-correlated through the 3D to 2D projection, which ultimately limits overall accuracy and stability. Inspired by Chain-of-Thought (CoT) in large language models (LLMs), this paper proposes MonoCoP, which leverages a Chain-of-Prediction (CoP) to predict attributes sequentially and conditionally via three key designs. First, it employs a lightweight AttributeNet (AN) for each 3D attribute to learn attribute-specific features. Next, MonoCoP constructs an explicit chain to propagate these learned features from one attribute to the next. Finally, MonoCoP uses a residual connection to aggregate features for each attribute along the chain, ensuring that later attribute predictions are conditioned on all previously processed attributes without forgetting the features of earlier ones. Experimental results show that our MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI leaderboard without requiring additional data and further surpasses existing methods on the Waymo and nuScenes frontal datasets.

**Link**: [arxiv](http://arxiv.org/abs/2505.04594v5),  [pdf](http://arxiv.org/pdf/2505.04594v5)

**Tags**: cs.CV 



### MATRIX: Multi-Agent simulaTion fRamework for safe Interactions and   conteXtual clinical conversational evaluation
**Authors**: Ernest Lim, Yajie Vera He, Jared Joselowitz, Kate Preston, Mohita Chowdhury, Louis Williams, Aisling Higham, Katrina Mason, Mariane Melo, Tom Lawton, Yan Jia, Ibrahim Habli

**Updated**: 2025-08-26T16:12:12Z

**Summary**: Despite the growing use of large language models (LLMs) in clinical dialogue systems, existing evaluations focus on task completion or fluency, offering little insight into the behavioral and risk management requirements essential for safety-critical systems. This paper presents MATRIX (Multi-Agent simulaTion fRamework for safe Interactions and conteXtual clinical conversational evaluation), a structured, extensible framework for safety-oriented evaluation of clinical dialogue agents.   MATRIX integrates three components: (1) a safety-aligned taxonomy of clinical scenarios, expected system behaviors and failure modes derived through structured safety engineering methods; (2) BehvJudge, an LLM-based evaluator for detecting safety-relevant dialogue failures, validated against expert clinician annotations; and (3) PatBot, a simulated patient agent capable of producing diverse, scenario-conditioned responses, evaluated for realism and behavioral fidelity with human factors expertise, and a patient-preference study.   Across three experiments, we show that MATRIX enables systematic, scalable safety evaluation. BehvJudge with Gemini 2.5-Pro achieves expert-level hazard detection (F1 0.96, sensitivity 0.999), outperforming clinicians in a blinded assessment of 240 dialogues. We also conducted one of the first realism analyses of LLM-based patient simulation, showing that PatBot reliably simulates realistic patient behavior in quantitative and qualitative evaluations. Using MATRIX, we demonstrate its effectiveness in benchmarking five LLM agents across 2,100 simulated dialogues spanning 14 hazard scenarios and 10 clinical domains.   MATRIX is the first framework to unify structured safety engineering with scalable, validated conversational AI evaluation, enabling regulator-aligned safety auditing. We release all evaluation tools, prompts, structured scenarios, and datasets.

**Link**: [arxiv](http://arxiv.org/abs/2508.19163v1),  [pdf](http://arxiv.org/pdf/2508.19163v1)

**Tags**: cs.AI cs.HC cs.MA 68T50, 68T42, 92C50, 68Q60 I.2.0; J.3 



### GeNet: A Multimodal LLM-Based Co-Pilot for Network Topology and   Configuration
**Authors**: Beni Ifland, Elad Duani, Rubin Krief, Miro Ohana, Aviram Zilberman, Andres Murillo, Ofir Manor, Ortal Lavi, Hikichi Kenji, Asaf Shabtai, Yuval Elovici, Rami Puzis

**Updated**: 2025-08-26T15:53:39Z

**Summary**: Communication network engineering in enterprise environments is traditionally a complex, time-consuming, and error-prone manual process. Most research on network engineering automation has concentrated on configuration synthesis, often overlooking changes in the physical network topology. This paper introduces GeNet, a multimodal co-pilot for enterprise network engineers. GeNet is a novel framework that leverages a large language model (LLM) to streamline network design workflows. It uses visual and textual modalities to interpret and update network topologies and device configurations based on user intents. GeNet was evaluated on enterprise network scenarios adapted from Cisco certification exercises. Our results demonstrate GeNet's ability to interpret network topology images accurately, potentially reducing network engineers' efforts and accelerating network design processes in enterprise environments. Furthermore, we show the importance of precise topology understanding when handling intents that require modifications to the network's topology.

**Link**: [arxiv](http://arxiv.org/abs/2407.08249v2),  [pdf](http://arxiv.org/pdf/2407.08249v2)

**Tags**: cs.NI cs.AI 



### ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context
**Authors**: Victoria R. Li, Yida Chen, Naomi Saphra

**Updated**: 2025-08-26T15:44:42Z

**Summary**: While the biases of language models in production are extensively documented, the biases of their guardrails have been neglected. This paper studies how contextual information about the user influences the likelihood of an LLM to refuse to execute a request. By generating user biographies that offer ideological and demographic information, we find a number of biases in guardrail sensitivity on GPT-3.5. Younger, female, and Asian-American personas are more likely to trigger a refusal guardrail when requesting censored or illegal information. Guardrails are also sycophantic, refusing to comply with requests for a political position the user is likely to disagree with. We find that certain identity groups and seemingly innocuous information, e.g., sports fandom, can elicit changes in guardrail sensitivity similar to direct statements of political ideology. For each demographic category and even for American football team fandom, we find that ChatGPT appears to infer a likely political ideology and modify guardrail behavior accordingly.

**Link**: [arxiv](http://arxiv.org/abs/2407.06866v3),  [pdf](http://arxiv.org/pdf/2407.06866v3)

**Tags**: cs.CL cs.AI 



### ZeST: an LLM-based Zero-Shot Traversability Navigation for Unknown   Environments
**Authors**: Shreya Gummadi, Mateus V. Gasparino, Gianluca Capezzuto, Marcelo Becker, Girish Chowdhary

**Updated**: 2025-08-26T15:30:19Z

**Summary**: The advancement of robotics and autonomous navigation systems hinges on the ability to accurately predict terrain traversability. Traditional methods for generating datasets to train these prediction models often involve putting robots into potentially hazardous environments, posing risks to equipment and safety. To solve this problem, we present ZeST, a novel approach leveraging visual reasoning capabilities of Large Language Models (LLMs) to create a traversability map in real-time without exposing robots to danger. Our approach not only performs zero-shot traversability and mitigates the risks associated with real-world data collection but also accelerates the development of advanced navigation systems, offering a cost-effective and scalable solution. To support our findings, we present navigation results, in both controlled indoor and unstructured outdoor environments. As shown in the experiments, our method provides safer navigation when compared to other state-of-the-art methods, constantly reaching the final goal.

**Link**: [arxiv](http://arxiv.org/abs/2508.19131v1),  [pdf](http://arxiv.org/pdf/2508.19131v1)

**Tags**: cs.RO cs.AI cs.CV 



### A Survey on Data Selection for LLM Instruction Tuning
**Authors**: Bolin Zhang, Jiahao Wang, Qianlong Du, Jiajun Zhang, Zhiying Tu, Dianhui Chu

**Updated**: 2025-08-26T15:28:36Z

**Summary**: Instruction tuning is a vital step of training large language models (LLMs), so how to enhance the effect of instruction tuning has received increased attention. Existing works indicate that the quality of the dataset is more crucial than the quantity during instruction tuning of LLMs. Therefore, recently a lot of studies focus on exploring the methods of selecting high-quality subset from instruction datasets, aiming to reduce training costs and enhance the instruction-following capabilities of LLMs. This paper presents a comprehensive survey on data selection for LLM instruction tuning. Firstly, we introduce the wildly used instruction datasets. Then, we propose a new taxonomy of the data selection methods and provide a detailed introduction of recent advances, and the evaluation strategies and results of data selection methods are also elaborated in detail. Finally, we emphasize the open challenges and present new frontiers of this task.

**Link**: [arxiv](http://arxiv.org/abs/2402.05123v3),  [pdf](http://arxiv.org/pdf/2402.05123v3)

**Tags**: cs.CL 



### Sharing is Caring: Analysis of Hybrid Network Sharing Strategies for   Energy Efficient Multi-Operator Cellular Systems
**Authors**: Laura Finarelli, Maoquan Ni, Michela Meo, Falko Dressler, Gianluca Rizzo

**Updated**: 2025-08-26T15:28:07Z

**Summary**: This paper introduces a novel analytical framework for evaluating energy-efficient, QoS-aware network-sharing strategies in cellular networks. Leveraging stochastic geometry, our framework enables the systematic assessment of network performance across a range of sharing paradigms, including both conventional single-operator scenarios and advanced hybrid strategies that enable full integration and cooperation among multiple mobile network operators. Our framework incorporates diverse user densities, rate requirements, and energy consumption models to ensure comprehensive analysis. Applying our results to real-world datasets from French mobile network operators, we demonstrate that hybrid network sharing can yield substantial energy savings, up to $35\%$, while maintaining quality of service. Furthermore, our results allow us to characterizing how the benefits of network sharing vary as a function of the geographical and functional characteristics of the deployment area. These findings highlight the potential of collaborative sharing strategies to enhance operational efficiency and sustainability in next-generation cellular networks.

**Link**: [arxiv](http://arxiv.org/abs/2508.19130v1),  [pdf](http://arxiv.org/pdf/2508.19130v1)

**Tags**: cs.NI 



### Space-Time Coded RIS-Assisted Wireless Systems with Practical Reflection   Models: Error Rate Analysis and Negative Moment-Based Optimization with   Saddle Point Approximation
**Authors**: Tayfun Yilmaz, Haci Ilhan, Ibrahim Hokelek

**Updated**: 2025-08-26T15:27:57Z

**Summary**: RIS-assisted communication has recently attracted significant attention for enhancing wireless performance in challenging environments, making accurate error analysis under practical hardware constraints crucial for future multi-antenna systems. This paper presents a theoretical framework for SER analysis of RIS-assisted multiple antenna systems employing OSTBC under practical reflection models with amplitude-dependent and quantized phase responses. By exploiting the Gramian structure of the cascaded channel f, we derive exact MGF expressions of the nonzero eigenvalue of f'f for small RIS sizes. For large-scale RIS deployments, where closed-form analysis becomes intractable, we employ Saddle Point Approximation to approximate the eigenvalue distribution. Using these results, we derive unified SER expressions using exact and SPA-based MGF formulations, applicable to arbitrary RIS sizes, phase configuration, and both identical and non-identical amplitude responses. Extensive Monte Carlo simulations confirm the accuracy of the proposed SER expressions, demonstrating very close agreement for all configurations.

**Link**: [arxiv](http://arxiv.org/abs/2508.19129v1),  [pdf](http://arxiv.org/pdf/2508.19129v1)

**Tags**: eess.SP 



### An Ontology-Driven Graph RAG for Legal Norms: A Hierarchical, Temporal,   and Deterministic Approach
**Authors**: Hudson de Martim

**Updated**: 2025-08-26T15:27:25Z

**Summary**: Retrieval-Augmented Generation (RAG) systems in the legal domain face a critical challenge: standard, flat-text retrieval is blind to the hierarchical, diachronic, and causal structure of law, leading to anachronistic and unreliable answers. This paper introduces an ontology-driven Graph RAG framework designed to overcome these limitations. We ground our knowledge graph in a formal, LRMoo-inspired model that distinguishes abstract legal Works from their versioned Expressions. We model temporal states as efficient aggregations that reuse the versioned expressions (CTVs) of unchanged components, and we reify legislative events as first-class Action nodes to make causality explicit and queryable. This structured backbone enables a unified, planner-guided query strategy that applies explicit policies to deterministically resolve complex requests for (i) point-in-time retrieval, (ii) hierarchical impact analysis, and (iii) auditable provenance reconstruction. Through a case study on the Brazilian Constitution, we demonstrate how this approach provides a verifiable, temporally-correct substrate for LLMs, enabling higher-order analytical capabilities while drastically reducing the risk of factual errors. The result is a practical framework for building more trustworthy and explainable legal AI systems.

**Link**: [arxiv](http://arxiv.org/abs/2505.00039v4),  [pdf](http://arxiv.org/pdf/2505.00039v4)

**Tags**: cs.CL cs.AI cs.IR 



### Exploring the Robustness of Language Models for Tabular Question   Answering via Attention Analysis
**Authors**: Kushal Raj Bhandari, Sixue Xing, Soham Dan, Jianxi Gao

**Updated**: 2025-08-26T15:27:18Z

**Summary**: Large Language Models (LLMs), already shown to ace various unstructured text comprehension tasks, have also remarkably been shown to tackle table (structured) comprehension tasks without specific training. Building on earlier studies of LLMs for tabular tasks, we probe how in-context learning (ICL), model scale, instruction tuning, and domain bias affect Tabular QA (TQA) robustness by testing LLMs, under diverse augmentations and perturbations, on diverse domains: Wikipedia-based $\textbf{WTQ}$, financial $\textbf{TAT-QA}$, and scientific $\textbf{SCITAB}$. Although instruction tuning and larger, newer LLMs deliver stronger, more robust TQA performance, data contamination and reliability issues, especially on $\textbf{WTQ}$, remain unresolved. Through an in-depth attention analysis, we reveal a strong correlation between perturbation-induced shifts in attention dispersion and the drops in performance, with sensitivity peaking in the model's middle layers. We highlight the need for improved interpretable methodologies to develop more reliable LLMs for table comprehension. Through an in-depth attention analysis, we reveal a strong correlation between perturbation-induced shifts in attention dispersion and performance drops, with sensitivity peaking in the model's middle layers. Based on these findings, we argue for the development of structure-aware self-attention mechanisms and domain-adaptive processing techniques to improve the transparency, generalization, and real-world reliability of LLMs on tabular data.

**Link**: [arxiv](http://arxiv.org/abs/2406.12719v4),  [pdf](http://arxiv.org/pdf/2406.12719v4)

**Tags**: cs.CL cs.AI 



### MCI-GRU: Stock Prediction Model Based on Multi-Head Cross-Attention and   Improved GRU
**Authors**: Peng Zhu, Yuante Li, Yifan Hu, Sheng Xiang, Qinyuan Liu, Dawei Cheng, Yuqi Liang

**Updated**: 2025-08-26T15:18:44Z

**Summary**: As financial markets grow increasingly complex in the big data era, accurate stock prediction has become more critical. Traditional time series models, such as GRUs, have been widely used but often struggle to capture the intricate nonlinear dynamics of markets, particularly in the flexible selection and effective utilization of key historical information. Recently, methods like Graph Neural Networks and Reinforcement Learning have shown promise in stock prediction but require high data quality and quantity, and they tend to exhibit instability when dealing with data sparsity and noise. Moreover, the training and inference processes for these models are typically complex and computationally expensive, limiting their broad deployment in practical applications. Existing approaches also generally struggle to capture unobservable latent market states effectively, such as market sentiment and expectations, microstructural factors, and participant behavior patterns, leading to an inadequate understanding of market dynamics and subsequently impact prediction accuracy. To address these challenges, this paper proposes a stock prediction model, MCI-GRU, based on a multi-head cross-attention mechanism and an improved GRU. First, we enhance the GRU model by replacing the reset gate with an attention mechanism, thereby increasing the model's flexibility in selecting and utilizing historical information. Second, we design a multi-head cross-attention mechanism for learning unobservable latent market state representations, which are further enriched through interactions with both temporal features and cross-sectional features. Finally, extensive experiments on four main stock markets show that the proposed method outperforms SOTA techniques across multiple metrics. Additionally, its successful application in real-world fund management operations confirms its effectiveness and practicality.

**Link**: [arxiv](http://arxiv.org/abs/2410.20679v3),  [pdf](http://arxiv.org/pdf/2410.20679v3)

**Tags**: q-fin.ST cs.LG q-fin.CP 



### DELIVER: A System for LLM-Guided Coordinated Multi-Robot Pickup and   Delivery using Voronoi-Based Relay Planning
**Authors**: Alkesh K. Srivastava, Jared Michael Levin, Alexander Derrico, Philip Dames

**Updated**: 2025-08-26T15:17:08Z

**Summary**: We present DELIVER (Directed Execution of Language-instructed Item Via Engineered Relay), a fully integrated framework for cooperative multi-robot pickup and delivery driven by natural language commands. DELIVER unifies natural language understanding, spatial decomposition, relay planning, and motion execution to enable scalable, collision-free coordination in real-world settings. Given a spoken or written instruction, a lightweight instance of LLaMA3 interprets the command to extract pickup and delivery locations. The environment is partitioned using a Voronoi tessellation to define robot-specific operating regions. Robots then compute optimal relay points along shared boundaries and coordinate handoffs. A finite-state machine governs each robot's behavior, enabling robust execution. We implement DELIVER on the MultiTRAIL simulation platform and validate it in both ROS2-based Gazebo simulations and real-world hardware using TurtleBot3 robots. Empirical results show that DELIVER maintains consistent mission cost across varying team sizes while reducing per-agent workload by up to 55% compared to a single-agent system. Moreover, the number of active relay agents remains low even as team size increases, demonstrating the system's scalability and efficient agent utilization. These findings underscore DELIVER's modular and extensible architecture for language-guided multi-robot coordination, advancing the frontiers of cyber-physical system integration.

**Link**: [arxiv](http://arxiv.org/abs/2508.19114v1),  [pdf](http://arxiv.org/pdf/2508.19114v1)

**Tags**: cs.RO cs.MA 



### Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary   Perception in LVLMs
**Authors**: Zhikai Ding, Shiyu Ni, Keping Bi

**Updated**: 2025-08-26T15:14:19Z

**Summary**: Large vision-language models (LVLMs) demonstrate strong visual question answering (VQA) capabilities but are shown to hallucinate. A reliable model should perceive its knowledge boundaries-knowing what it knows and what it does not. This paper investigates LVLMs' perception of their knowledge boundaries by evaluating three types of confidence signals: probabilistic confidence, answer consistency-based confidence, and verbalized confidence. Experiments on three LVLMs across three VQA datasets show that, although LVLMs possess a reasonable perception level, there is substantial room for improvement. Among the three confidences, probabilistic and consistency-based signals are more reliable indicators, while verbalized confidence often leads to overconfidence. To enhance LVLMs' perception, we adapt several established confidence calibration methods from Large Language Models (LLMs) and propose three effective methods. Additionally, we compare LVLMs with their LLM counterparts, finding that jointly processing visual and textual inputs decreases question-answering performance but reduces confidence, resulting in an improved perception level compared to LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2508.19111v1),  [pdf](http://arxiv.org/pdf/2508.19111v1)

**Tags**: cs.CL 



### Label Set Optimization via Activation Distribution Kurtosis for   Zero-shot Classification with Generative Models
**Authors**: Yue Li, Zhixue Zhao, Carolina Scarton

**Updated**: 2025-08-26T15:09:51Z

**Summary**: In-context learning (ICL) performance is highly sensitive to prompt design, yet the impact of class label options (e.g. lexicon or order) in zero-shot classification remains underexplored. This study proposes LOADS (Label set Optimization via Activation Distribution kurtosiS), a post-hoc method for selecting optimal label sets in zero-shot ICL with large language models (LLMs). LOADS is built upon the observations in our empirical analysis, the first to systematically examine how label option design (i.e., lexical choice, order, and elaboration) impacts classification performance. This analysis shows that the lexical choice of the labels in the prompt (such as agree vs. support in stance classification) plays an important role in both model performance and model's sensitivity to the label order. A further investigation demonstrates that optimal label words tend to activate fewer outlier neurons in LLMs' feed-forward networks. LOADS then leverages kurtosis to measure the neuron activation distribution for label selection, requiring only a single forward pass without gradient propagation or labelled data. The LOADS-selected label words consistently demonstrate effectiveness for zero-shot ICL across classification tasks, datasets, models and languages, achieving maximum performance gain from 0.54 to 0.76 compared to the conventional approach of using original dataset label words.

**Link**: [arxiv](http://arxiv.org/abs/2410.19195v2),  [pdf](http://arxiv.org/pdf/2410.19195v2)

**Tags**: cs.CL 



### Safe Reinforcement Learning in Black-Box Environments via Adaptive   Shielding
**Authors**: Daniel Bethell, Simos Gerasimou, Radu Calinescu, Calum Imrie

**Updated**: 2025-08-26T15:01:38Z

**Summary**: Empowering safe exploration of reinforcement learning (RL) agents during training is a critical challenge towards their deployment in many real-world scenarios. When prior knowledge of the domain or task is unavailable, training RL agents in unknown, black-box environments presents an even greater safety risk. We introduce ADVICE (Adaptive Shielding with a Contrastive Autoencoder), a novel post-shielding technique that distinguishes safe and unsafe features of state-action pairs during training, and uses this knowledge to protect the RL agent from executing actions that yield likely hazardous outcomes. Our comprehensive experimental evaluation against state-of-the-art safe RL exploration techniques shows that ADVICE significantly reduces safety violations (approx 50%) during training, with a competitive outcome reward compared to other techniques.

**Link**: [arxiv](http://arxiv.org/abs/2405.18180v3),  [pdf](http://arxiv.org/pdf/2405.18180v3)

**Tags**: cs.AI cs.LG 



### Reasoning LLMs in the Medical Domain: A Literature Survey
**Authors**: Armin Berger, Sarthak Khanna, David Berghaus, Rafet Sifa

**Updated**: 2025-08-26T14:59:19Z

**Summary**: The emergence of advanced reasoning capabilities in Large Language Models (LLMs) marks a transformative development in healthcare applications. Beyond merely expanding functional capabilities, these reasoning mechanisms enhance decision transparency and explainability-critical requirements in medical contexts. This survey examines the transformation of medical LLMs from basic information retrieval tools to sophisticated clinical reasoning systems capable of supporting complex healthcare decisions. We provide a thorough analysis of the enabling technological foundations, with a particular focus on specialized prompting techniques like Chain-of-Thought and recent breakthroughs in Reinforcement Learning exemplified by DeepSeek-R1. Our investigation evaluates purpose-built medical frameworks while also examining emerging paradigms such as multi-agent collaborative systems and innovative prompting architectures. The survey critically assesses current evaluation methodologies for medical validation and addresses persistent challenges in field interpretation limitations, bias mitigation strategies, patient safety frameworks, and integration of multimodal clinical data. Through this survey, we seek to establish a roadmap for developing reliable LLMs that can serve as effective partners in clinical practice and medical research.

**Link**: [arxiv](http://arxiv.org/abs/2508.19097v1),  [pdf](http://arxiv.org/pdf/2508.19097v1)

**Tags**: cs.AI 



### Trustworthy Agents for Electronic Health Records through Confidence   Estimation
**Authors**: Yongwoo Song, Minbyul Jeong, Mujeen Sung

**Updated**: 2025-08-26T14:59:04Z

**Summary**: Large language models (LLMs) show promise for extracting information from Electronic Health Records (EHR) and supporting clinical decisions. However, deployment in clinical settings faces challenges due to hallucination risks. We propose Hallucination Controlled Accuracy at k% (HCAcc@k%), a novel metric quantifying the accuracy-reliability trade-off at varying confidence thresholds. We introduce TrustEHRAgent, a confidence-aware agent incorporating stepwise confidence estimation for clinical question answering. Experiments on MIMIC-III and eICU datasets show TrustEHRAgent outperforms baselines under strict reliability constraints, achieving improvements of 44.23%p and 25.34%p at HCAcc@70% while baseline methods fail at these thresholds. These results highlight limitations of traditional accuracy metrics in evaluating healthcare AI agents. Our work contributes to developing trustworthy clinical agents that deliver accurate information or transparently express uncertainty when confidence is low.

**Link**: [arxiv](http://arxiv.org/abs/2508.19096v1),  [pdf](http://arxiv.org/pdf/2508.19096v1)

**Tags**: cs.AI 



### First Steps Toward the Development of a Straight-Line Reference   Alignment System for Future Accelerators at CERN Using Pseudo-Nondiffracting   Layer Beams
**Authors**: Martin Dušek, Sebastian Figura, Jakub Michal Polak, Solomon William Kamugasa, Dirk Mergelkuhl, Witold Niewiem, Štěpán Kunc, Jean-Christophe Gayde, Miroslav Šulc

**Updated**: 2025-08-26T14:58:02Z

**Summary**: This paper presents experimental results that allow for the performance evaluation of a straight-line reference alignment system based on pseudo-nondiffracting Layer beams. Sensors, developed specifically for this system, feature four linear CMOS chips and a square aperture. This allows for simultaneous measurements along the beam path without disrupting the laser reference. Measurements, conducted over a distance of 2 m from the first to the last sensor, were compared with a laser tracker measurement to assess the sensor performance. The alignment reference generated by the Layer Beams exhibited a repeatability and reproducibility root-mean-square error (RMSE) of less than 30 ${\mu}$m. The relative alignment precision for a known displacement was validated with a standard deviation of 4.3 ${\mu}$m. The results highlight the underlying sources of noise, which are induced mainly by the cover glass, the protective film of the pixels, and the dark noise of the CMOS chips. Solutions to address these challenges are proposed. Additionally, a proof-of-concept for future development of a radiation-hard sensor utilizing optical fiber matrices is demonstrated. The RMSE of the reference position detection introduced by the fiber matrix remained below 1.3 ${\mu}$m. This would allow the sensor to be used reliably in high-radiation environments typical for accelerator facilities. This study serves as a foundational step toward developing a robust straight-line reference alignment system based on pseudo-nondiffracting Layer beams intended for deployment in the accelerator facilities.

**Link**: [arxiv](http://arxiv.org/abs/2506.15234v2),  [pdf](http://arxiv.org/pdf/2506.15234v2)

**Tags**: physics.ins-det physics.app-ph physics.optics 



### Building an Open CGRA Ecosystem for Agile Innovation
**Authors**: Rohan Juneja, Pranav Dangi, Thilini Kaushalya Bandara, Zhaoying Li, Dhananjaya Wijerathne, Li-Shiuan Peh, Tulika Mitra

**Updated**: 2025-08-26T14:51:25Z

**Summary**: Modern computing workloads, particularly in AI and edge applications, demand hardware-software co-design to meet aggressive performance and energy targets. Such co-design benefits from open and agile platforms that replace closed, vertically integrated development with modular, community-driven ecosystems. Coarse-Grained Reconfigurable Architectures (CGRAs), with their unique balance of flexibility and efficiency are particularly well-suited for this paradigm. When built on open-source hardware generators and software toolchains, CGRAs provide a compelling foundation for architectural exploration, cross-layer optimization, and real-world deployment. In this paper, we will present an open CGRA ecosystem that we have developed to support agile innovation across the stack. Our contributions include HyCUBE, a CGRA with a reconfigurable single-cycle multi-hop interconnect for efficient data movement; PACE, which embeds a power-efficient HyCUBE within a RISC-V SoC targeting edge computing; and Morpher, a fully open-source, architecture-adaptive CGRA design framework that supports design space exploration, compilation, simulation, and validation. By embracing openness at every layer, we aim to lower barriers to innovation, enable reproducible research, and demonstrate how CGRAs can anchor the next wave of agile hardware development. We will conclude with a call for a unified abstraction layer for CGRAs and spatial accelerators, one that decouples hardware specialization from software development. Such a representation would unlock architectural portability, compiler innovation, and a scalable, open foundation for spatial computing.

**Link**: [arxiv](http://arxiv.org/abs/2508.19090v1),  [pdf](http://arxiv.org/pdf/2508.19090v1)

**Tags**: cs.AR 



### It's All About In-Context Learning! Teaching Extremely Low-Resource   Languages to LLMs
**Authors**: Yue Li, Zhixue Zhao, Carolina Scarton

**Updated**: 2025-08-26T14:51:10Z

**Summary**: Extremely low-resource languages, especially those written in rare scripts, as shown in Figure 1, remain largely unsupported by large language models (LLMs). This is due in part to compounding factors such as the lack of training data. This paper delivers the first comprehensive analysis of whether LLMs can acquire such languages purely via in-context learning (ICL), with or without auxiliary alignment signals, and how these methods compare to parameter-efficient fine-tuning (PEFT). We systematically evaluate 20 under-represented languages across three state-of-the-art multilingual LLMs. Our findings highlight the limitation of PEFT when both language and its script are extremely under-represented by the LLM. In contrast, zero-shot ICL with language alignment is impressively effective on extremely low-resource languages, while few-shot ICL or PEFT is more beneficial for languages relatively better represented by LLMs. For LLM practitioners working on extremely low-resource languages, we summarise guidelines grounded by our results on adapting LLMs to low-resource languages, e.g., avoiding fine-tuning a multilingual model on languages of unseen scripts.

**Link**: [arxiv](http://arxiv.org/abs/2508.19089v1),  [pdf](http://arxiv.org/pdf/2508.19089v1)

**Tags**: cs.CL 



### APT-LLM: Exploiting Arbitrary-Precision Tensor Core Computing for LLM   Acceleration
**Authors**: Shaobo Ma, Chao Fang, Haikuo Shao, Zhongfeng Wang

**Updated**: 2025-08-26T14:48:29Z

**Summary**: Large language models (LLMs) have revolutionized AI applications, yet their enormous computational demands severely limit deployment and real-time performance. Quantization methods can help reduce computational costs, however, attaining the extreme efficiency associated with ultra-low-bit quantized LLMs at arbitrary precision presents challenges on GPUs. This is primarily due to the limited support for GPU Tensor Cores, inefficient memory management, and inflexible kernel optimizations. To tackle these challenges, we propose a comprehensive acceleration scheme for arbitrary precision LLMs, namely APT-LLM. Firstly, we introduce a novel data format, bipolar-INT, which allows for efficient and lossless conversion with signed INT, while also being more conducive to parallel computation. We also develop a matrix multiplication (MatMul) method allowing for arbitrary precision by dismantling and reassembling matrices at the bit level. This method provides flexible precision and optimizes the utilization of GPU Tensor Cores. In addition, we propose a memory management system focused on data recovery, which strategically employs fast shared memory to substantially increase kernel execution speed and reduce memory access latency. Finally, we develop a kernel mapping method that dynamically selects the optimal configurable hyperparameters of kernels for varying matrix sizes, enabling optimal performance across different LLM architectures and precision settings. In LLM inference, APT-LLM achieves up to a 3.99$\times$ speedup compared to FP16 baselines and a 2.16$\times$ speedup over NVIDIA CUTLASS INT4 acceleration on RTX 3090. On RTX 4090 and H800, APT-LLM achieves up to 2.44$\times$ speedup over FP16 and 1.65$\times$ speedup over CUTLASS integer baselines.

**Link**: [arxiv](http://arxiv.org/abs/2508.19087v1),  [pdf](http://arxiv.org/pdf/2508.19087v1)

**Tags**: cs.LG cs.AI cs.AR 



### Federated Fine-Tuning of Sparsely-Activated Large Language Models on   Resource-Constrained Devices
**Authors**: Fahao Chen, Jie Wan, Peng Li, Zhou Su, Dongxiao Yu

**Updated**: 2025-08-26T14:39:00Z

**Summary**: Federated fine-tuning of Mixture-of-Experts (MoE)-based large language models (LLMs) is challenging due to their massive computational requirements and the resource constraints of participants. Existing working attempts to fill this gap through model quantization, computation offloading, or expert pruning. However, they cannot achieve desired performance due to impractical system assumptions and a lack of consideration for MoE-specific characteristics. In this paper, we propose FLUX, a system designed to enable federated fine-tuning of MoE-based LLMs across participants with constrained computing resources (e.g., consumer-grade GPUs), aiming to minimize time-to-accuracy. FLUX introduces three key innovations: (1) quantization-based local profiling to estimate expert activation with minimal overhead, (2) adaptive layer-aware expert merging to reduce resource consumption while preserving accuracy, and (3) dynamic expert role assignment using an exploration-exploitation strategy to balance tuning and non-tuning experts. Extensive experiments on LLaMA-MoE and DeepSeek-MoE with multiple benchmark datasets demonstrate that FLUX significantly outperforms existing methods, achieving up to 4.75X speedup in time-to-accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2508.19078v1),  [pdf](http://arxiv.org/pdf/2508.19078v1)

**Tags**: cs.DC cs.AI 



### HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive   Global-Local Guidance
**Authors**: Ziyue Li, Yuan Chang, Gaihong Yu, Xiaoqiu Le

**Updated**: 2025-08-26T14:37:48Z

**Summary**: Large language model (LLM)-based agents have demonstrated remarkable capabilities in decision-making tasks, but struggle significantly with complex, long-horizon planning scenarios. This arises from their lack of macroscopic guidance, causing disorientation and failures in complex tasks, as well as insufficient continuous oversight during execution, rendering them unresponsive to environmental changes and prone to deviations. To tackle these challenges, we introduce HiPlan, a hierarchical planning framework that provides adaptive global-local guidance to boost LLM-based agents'decision-making. HiPlan decomposes complex tasks into milestone action guides for general direction and step-wise hints for detailed actions. During the offline phase, we construct a milestone library from expert demonstrations, enabling structured experience reuse by retrieving semantically similar tasks and milestones. In the execution phase, trajectory segments from past milestones are dynamically adapted to generate step-wise hints that align current observations with the milestone objectives, bridging gaps and correcting deviations. Extensive experiments across two challenging benchmarks demonstrate that HiPlan substantially outperforms strong baselines, and ablation studies validate the complementary benefits of its hierarchical components.

**Link**: [arxiv](http://arxiv.org/abs/2508.19076v1),  [pdf](http://arxiv.org/pdf/2508.19076v1)

**Tags**: cs.CL cs.AI 



### SmartBench: Is Your LLM Truly a Good Chinese Smartphone Assistant?
**Authors**: Xudong Lu, Haohao Gao, Renshou Wu, Shuai Ren, Xiaoxin Chen, Hongsheng Li, Fangyuan Li

**Updated**: 2025-08-26T14:34:00Z

**Summary**: Large Language Models (LLMs) have become integral to daily life, especially advancing as intelligent assistants through on-device deployment on smartphones. However, existing LLM evaluation benchmarks predominantly focus on objective tasks like mathematics and coding in English, which do not necessarily reflect the practical use cases of on-device LLMs in real-world mobile scenarios, especially for Chinese users. To address these gaps, we introduce SmartBench, the first benchmark designed to evaluate the capabilities of on-device LLMs in Chinese mobile contexts. We analyze functionalities provided by representative smartphone manufacturers and divide them into five categories: text summarization, text Q&A, information extraction, content creation, and notification management, further detailed into 20 specific tasks. For each task, we construct high-quality datasets comprising 50 to 200 question-answer pairs that reflect everyday mobile interactions, and we develop automated evaluation criteria tailored for these tasks. We conduct comprehensive evaluations of on-device LLMs and MLLMs using SmartBench and also assess their performance after quantized deployment on real smartphone NPUs. Our contributions provide a standardized framework for evaluating on-device LLMs in Chinese, promoting further development and optimization in this critical area. Code and data will be available at https://github.com/vivo-ai-lab/SmartBench.

**Link**: [arxiv](http://arxiv.org/abs/2503.06029v2),  [pdf](http://arxiv.org/pdf/2503.06029v2)

**Tags**: cs.CL cs.LG 



### An LLM-powered Natural-to-Robotic Language Translation Framework with   Correctness Guarantees
**Authors**: ZhenDong Chen, ZhanShang Nie, ShiXing Wan, JunYi Li, YongTian Cheng, Shuai Zhao

**Updated**: 2025-08-26T14:32:49Z

**Summary**: The Large Language Models (LLM) are increasingly being deployed in robotics to generate robot control programs for specific user tasks, enabling embodied intelligence. Existing methods primarily focus on LLM training and prompt design that utilize LLMs to generate executable programs directly from user tasks in natural language. However, due to the inconsistency of the LLMs and the high complexity of the tasks, such best-effort approaches often lead to tremendous programming errors in the generated code, which significantly undermines the effectiveness especially when the light-weight LLMs are applied. This paper introduces a natural-robotic language translation framework that (i) provides correctness verification for generated control programs and (ii) enhances the performance of LLMs in program generation via feedback-based fine-tuning for the programs. To achieve this, a Robot Skill Language (RSL) is proposed to abstract away from the intricate details of the control programs, bridging the natural language tasks with the underlying robot skills. Then, the RSL compiler and debugger are constructed to verify RSL programs generated by the LLM and provide error feedback to the LLM for refining the outputs until being verified by the compiler. This provides correctness guarantees for the LLM-generated programs before being offloaded to the robots for execution, significantly enhancing the effectiveness of LLM-powered robotic applications. Experiments demonstrate NRTrans outperforms the existing method under a range of LLMs and tasks, and achieves a high success rate for light-weight LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2508.19074v1),  [pdf](http://arxiv.org/pdf/2508.19074v1)

**Tags**: cs.RO cs.AI cs.PL 



### Can Structured Templates Facilitate LLMs in Tackling Harder Tasks? : An   Exploration of Scaling Laws by Difficulty
**Authors**: Zhichao Yang, Zhaoxin Fan, Gen Li, Yuanze Hu, Xinyu Wang, Ye Qiu, Xin Wang, Yifan Sun, Wenjun Wu

**Updated**: 2025-08-26T14:26:32Z

**Summary**: Structured, procedural reasoning is essential for Large Language Models (LLMs), especially in mathematics. While post-training methods have improved LLM performance, they still fall short in capturing deep procedural logic on complex tasks. To tackle the issue, in this paper, we first investigate this limitation and uncover a novel finding: a Scaling Law by Difficulty, which reveals that model performance follows a U-shaped curve with respect to training data complexity -- excessive low-difficulty data impedes abstraction, while high-difficulty data significantly enhances reasoning ability. Motivated by this, we propose the Structured Solution Template (SST) framework, which uses solution templates and a curriculum of varied difficulty to explicitly teach procedural reasoning. Specifically, SST comprises (1) fine-tuning with structured solution-template chains and dynamically weighted loss to prioritize procedural logic, (2) prompt-time injection of solution templates as cognitive scaffolds to guide inference, and (3) integrated curriculum fine-tuning that explicitly teaches the model to self-plan - execute - self-correct. Experiments on GSM8K, AIME24, and new Dynamic En benchmark show that SST significantly improves both accuracy and efficiency, especially on harder problems.

**Link**: [arxiv](http://arxiv.org/abs/2508.19069v1),  [pdf](http://arxiv.org/pdf/2508.19069v1)

**Tags**: cs.AI 



### An Agentic System for Rare Disease Diagnosis with Traceable Reasoning
**Authors**: Weike Zhao, Chaoyi Wu, Yanjie Fan, Xiaoman Zhang, Pengcheng Qiu, Yuze Sun, Xiao Zhou, Yanfeng Wang, Xin Sun, Ya Zhang, Yongguo Yu, Kun Sun, Weidi Xie

**Updated**: 2025-08-26T14:13:25Z

**Summary**: Rare diseases collectively affect over 300 million individuals worldwide, yet timely and accurate diagnosis remains a pervasive challenge. This is largely due to their clinical heterogeneity, low individual prevalence, and the limited familiarity most clinicians have with rare conditions. Here, we introduce DeepRare, the first rare disease diagnosis agentic system powered by a large language model (LLM), capable of processing heterogeneous clinical inputs. The system generates ranked diagnostic hypotheses for rare diseases, each accompanied by a transparent chain of reasoning that links intermediate analytic steps to verifiable medical evidence.   DeepRare comprises three key components: a central host with a long-term memory module; specialized agent servers responsible for domain-specific analytical tasks integrating over 40 specialized tools and web-scale, up-to-date medical knowledge sources, ensuring access to the most current clinical information. This modular and scalable design enables complex diagnostic reasoning while maintaining traceability and adaptability. We evaluate DeepRare on eight datasets. The system demonstrates exceptional diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013 diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15 methods, like traditional bioinformatics diagnostic tools, LLMs, and other agentic systems, achieving an average Recall@1 score of 57.18% and surpassing the second-best method (Reasoning LLM) by a substantial margin of 23.79 percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at Recall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of reasoning chains by clinical experts achieves 95.40% agreements. Furthermore, the DeepRare system has been implemented as a user-friendly web application http://raredx.cn/doctor.

**Link**: [arxiv](http://arxiv.org/abs/2506.20430v2),  [pdf](http://arxiv.org/pdf/2506.20430v2)

**Tags**: cs.CL cs.AI cs.CV cs.MA 



### SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge   Understanding of LLMs
**Authors**: Zhiqiang Liu, Enpei Niu, Yin Hua, Mengshu Sun, Lei Liang, Huajun Chen, Wen Zhang

**Updated**: 2025-08-26T14:11:22Z

**Summary**: Although large language models (LLMs) have made significant progress in understanding Structured Knowledge (SK) like KG and Table, existing evaluations for SK understanding are non-rigorous (i.e., lacking evaluations of specific capabilities) and focus on a single type of SK. Therefore, we aim to propose a more comprehensive and rigorous structured knowledge understanding benchmark to diagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a Structured Knowledge Augmented QA Benchmark that encompasses four widely used structured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a three-stage pipeline to construct SKA-Bench instances, which includes a question, an answer, positive knowledge units, and noisy knowledge units. To evaluate the SK understanding capabilities of LLMs in a fine-grained manner, we expand the instances into four fundamental ability testbeds: Noise Robustness, Order Insensitivity, Information Integration, and Negative Rejection. Empirical evaluations on 8 representative LLMs, including the advanced DeepSeek-R1, indicate that existing LLMs still face significant challenges in understanding structured knowledge, and their performance is influenced by factors such as the amount of noise, the order of knowledge units, and hallucination phenomenon. Our dataset and code are available at https://github.com/Lza12a/SKA-Bench.

**Link**: [arxiv](http://arxiv.org/abs/2507.17178v2),  [pdf](http://arxiv.org/pdf/2507.17178v2)

**Tags**: cs.CL cs.AI 



### Private Quantum Database
**Authors**: Giancarlo Gatti, Rihan Hai

**Updated**: 2025-08-26T14:11:22Z

**Summary**: Quantum databases open an exciting new frontier in data management by offering privacy guarantees that classical systems cannot match. Traditional engines tackle user privacy, which hides the records being queried, or data privacy, which prevents a user from learning more than she has queried. We propose a quantum database that protects both by leveraging quantum mechanics: when the user measures her chosen basis, the superposition collapses and the unqueried rows become physically inaccessible. We encode relational tables as a sequence of Quantum Random Access Codes (QRACs) over mutually unbiased bases (MUBs), transmit a bounded number of quantum states, and let a single, destructive measurement reconstruct only the selected tuple. This allows us to preserve data privacy and user privacy at once without trusted hardware or heavyweight cryptography. Moreover, we envision a novel hybrid quantum-classical architecture ready for early deployment, which ensures compatibility with the limitations of today's Noisy Intermediate-Scale Quantum devices.

**Link**: [arxiv](http://arxiv.org/abs/2508.19055v1),  [pdf](http://arxiv.org/pdf/2508.19055v1)

**Tags**: quant-ph cs.DB 



### A Concurrent Modular Agent: Framework for Autonomous LLM Agents
**Authors**: Norihiro Maruyama, Takahide Yoshida, Hiroki Sato, Atsushi Masumori, Johnsmith, Takashi Ikegami

**Updated**: 2025-08-26T13:58:31Z

**Summary**: We introduce the Concurrent Modular Agent (CMA), a framework that orchestrates multiple Large-Language-Model (LLM)-based modules that operate fully asynchronously yet maintain a coherent and fault-tolerant behavioral loop. This framework addresses long-standing difficulties in agent architectures by letting intention emerge from language-mediated interactions among autonomous processes. This approach enables flexible, adaptive, and context-dependent behavior through the combination of concurrently executed modules that offload reasoning to an LLM, inter-module communication, and a single shared global state.We consider this approach to be a practical realization of Minsky's Society of Mind theory. We demonstrate the viability of our system through two practical use-case studies. The emergent properties observed in our system suggest that complex cognitive phenomena like self-awareness may indeed arise from the organized interaction of simpler processes, supporting Minsky-Society of Mind concept and opening new avenues for artificial intelligence research. The source code for our work is available at: https://github.com/AlternativeMachine/concurrent-modular-agent.

**Link**: [arxiv](http://arxiv.org/abs/2508.19042v1),  [pdf](http://arxiv.org/pdf/2508.19042v1)

**Tags**: cs.AI 



### Large Language Model Aided QoS Prediction for Service Recommendation
**Authors**: Huiying Liu, Zekun Zhang, Honghao Li, Qilin Wu, Yiwen Zhang

**Updated**: 2025-08-26T13:55:02Z

**Summary**: Large language models (LLMs) have seen rapid improvement in the recent years, and have been used in a wider range of applications. After being trained on large text corpus, LLMs obtain the capability of extracting rich features from textual data. Such capability is potentially useful for the web service recommendation task, where the web users and services have intrinsic attributes that can be described using natural language sentences and are useful for recommendation. In this paper, we explore the possibility and practicality of using LLMs for web service recommendation. We propose the large language model aided QoS prediction (llmQoS) model, which use LLMs to extract useful information from attributes of web users and services via descriptive sentences. This information is then used in combination with the QoS values of historical interactions of users and services, to predict QoS values for any given user-service pair. On the WSDream dataset, llmQoS is shown to overcome the data sparsity issue inherent to the QoS prediction problem, and outperforms comparable baseline models consistently.

**Link**: [arxiv](http://arxiv.org/abs/2408.02223v3),  [pdf](http://arxiv.org/pdf/2408.02223v3)

**Tags**: cs.LG cs.DC 



### Investigating Advanced Reasoning of Large Language Models via Black-Box   Interaction
**Authors**: Congchi Yin, Tianyi Wu, Yankai Shu, Alex Gu, Yunhan Wang, Jun Shao, Xun Jiang, Piji Li

**Updated**: 2025-08-26T13:54:17Z

**Summary**: Existing tasks fall short in evaluating reasoning ability of Large Language Models (LLMs) in an interactive, unknown environment. This deficiency leads to the isolated assessment of deductive, inductive, and abductive reasoning, neglecting the integrated reasoning process that is indispensable for humans discovery of real world. We introduce a novel evaluation paradigm, \textit{black-box interaction}, to tackle this challenge. A black-box is defined by a hidden function that maps a specific set of inputs to outputs. LLMs are required to unravel the hidden function behind the black-box by interacting with it in given exploration turns, and reasoning over observed input-output pairs. Leveraging this idea, we build the \textsc{Oracle} benchmark which comprises 6 types of black-box task and 96 black-boxes. 19 modern LLMs are benchmarked. o3 ranks first in 5 of the 6 tasks, achieving over 70\% accuracy on most easy black-boxes. But it still struggles with some hard black-box tasks, where its average performance drops below 40\%. Further analysis indicates a universal difficulty among LLMs: They lack the high-level planning capability to develop efficient and adaptive exploration strategies for hypothesis refinement.

**Link**: [arxiv](http://arxiv.org/abs/2508.19035v1),  [pdf](http://arxiv.org/pdf/2508.19035v1)

**Tags**: cs.AI 



### MovieCORE: COgnitive REasoning in Movies
**Authors**: Gueter Josmy Faure, Min-Hung Chen, Jia-Fong Yeh, Ying Cheng, Hung-Ting Su, Yung-Hao Tang, Shang-Hong Lai, Winston H. Hsu

**Updated**: 2025-08-26T13:43:45Z

**Summary**: This paper introduces MovieCORE, a novel video question answering (VQA) dataset designed to probe deeper cognitive understanding of movie content. Unlike existing datasets that focus on surface-level comprehension, MovieCORE emphasizes questions that engage System-2 thinking while remaining specific to the video material. We present an innovative agentic brainstorming approach, utilizing multiple large language models (LLMs) as thought agents to generate and refine high-quality question-answer pairs. To evaluate dataset quality, we develop a set of cognitive tests assessing depth, thought-provocation potential, and syntactic complexity. We also propose a comprehensive evaluation scheme for assessing VQA model performance on deeper cognitive tasks. To address the limitations of existing video-language models (VLMs), we introduce an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves model reasoning capabilities post-training by up to 25%. Our work contributes to advancing movie understanding in AI systems and provides valuable insights into the capabilities and limitations of current VQA models when faced with more challenging, nuanced questions about cinematic content. Our project page, dataset and code can be found at https://joslefaure.github.io/assets/html/moviecore.html.

**Link**: [arxiv](http://arxiv.org/abs/2508.19026v1),  [pdf](http://arxiv.org/pdf/2508.19026v1)

**Tags**: cs.CL cs.AI cs.CV 



### General Intelligence Requires Reward-based Pretraining
**Authors**: Seungwook Han, Jyothish Pari, Samuel J. Gershman, Pulkit Agrawal

**Updated**: 2025-08-26T13:31:48Z

**Summary**: Large Language Models (LLMs) have demonstrated impressive real-world utility, exemplifying artificial useful intelligence (AUI). However, their ability to reason adaptively and robustly -- the hallmarks of artificial general intelligence (AGI) -- remains fragile. While LLMs seemingly succeed in commonsense reasoning, programming, and mathematics, they struggle to generalize algorithmic understanding across novel contexts. Our experiments with algorithmic tasks in esoteric programming languages reveal that LLM's reasoning overfits to the training data and is limited in its transferability. We hypothesize that the core issue underlying such limited transferability is the coupling of reasoning and knowledge in LLMs.   To transition from AUI to AGI, we propose disentangling knowledge and reasoning through three key directions: (1) pretaining to reason using RL from scratch as an alternative to the widely used next-token prediction pretraining, (2) using a curriculum of synthetic tasks to ease the learning of a reasoning prior for RL that can then be transferred to natural language tasks, and (3) learning more generalizable reasoning functions using a small context window to reduce exploiting spurious correlations between tokens. Such a reasoning system coupled with a trained retrieval system and a large external memory bank as a knowledge store can overcome several limitations of existing architectures at learning to reason in novel scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2502.19402v3),  [pdf](http://arxiv.org/pdf/2502.19402v3)

**Tags**: cs.LG 



### Sense of Self and Time in Borderline Personality. A Comparative   Robustness Study with Generative AI
**Authors**: Marcin Moskalewicz, Anna Sterna, Marek Pokropski, Paula Flores

**Updated**: 2025-08-26T13:13:47Z

**Summary**: This study examines the capacity of large language models (LLMs) to support phenomenological qualitative analysis of first-person experience in Borderline Personality Disorder (BPD), understood as a disorder of temporality and selfhood. Building on a prior human-led thematic analysis of 24 inpatients' life-story interviews, we compared three LLMs (OpenAI GPT-4o, Google Gemini 2.5 Pro, Anthropic Claude Opus 4) prompted to mimic the interpretative style of the original investigators. The models were evaluated with blinded and non-blinded expert judges in phenomenology and clinical psychology. Assessments included semantic congruence, Jaccard coefficients, and multidimensional validity ratings (credibility, coherence, substantiveness, and groundness in data). Results showed variable overlap with the human analysis, from 0 percent in GPT to 42 percent in Claude and 58 percent in Gemini, and a low Jaccard coefficient (0.21-0.28). However, the models recovered themes omitted by humans. Gemini's output most closely resembled the human analysis, with validity scores significantly higher than GPT and Claude (p < 0.0001), and was judged as human by blinded experts. All scores strongly correlated (R > 0.78) with the quantity of text and words per theme, highlighting both the variability and potential of AI-augmented thematic analysis to mitigate human interpretative bias.

**Link**: [arxiv](http://arxiv.org/abs/2508.19008v1),  [pdf](http://arxiv.org/pdf/2508.19008v1)

**Tags**: cs.AI 



### Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A   Framework and Benchmark
**Authors**: Yuxuan Cai, Yipeng Hao, Jie Zhou, Hang Yan, Zhikai Lei, Rui Zhen, Zhenhua Han, Yutao Yang, Junsong Li, Qianjun Pan, Tianyu Huai, Qin Chen, Xin Li, Kai Chen, Bo Zhang, Xipeng Qiu, Liang He

**Updated**: 2025-08-26T13:04:28Z

**Summary**: As AI advances toward general intelligence, the focus is shifting from systems optimized for static tasks to creating open-ended agents that learn continuously. In this paper, we introduce Experience-driven Lifelong Learning (ELL), a framework for building self-evolving agents capable of continuous growth through real-world interaction. The framework is built on four core principles: (1) Experience Exploration: Agents learn through continuous, self-motivated interaction with dynamic environments, navigating interdependent tasks and generating rich experiential trajectories. (2) Long-term Memory: Agents preserve and structure historical knowledge, including personal experiences, domain expertise, and commonsense reasoning, into a persistent memory system. (3) Skill Learning: Agents autonomously improve by abstracting recurring patterns from experience into reusable skills, which are actively refined and validated for application in new tasks. (4) Knowledge Internalization: Agents internalize explicit and discrete experiences into implicit and intuitive capabilities as "second nature".   We also introduce StuLife, a benchmark dataset for ELL that simulates a student's holistic college journey, from enrollment to academic and personal development, across three core phases and ten detailed sub-scenarios. StuLife is designed around three key paradigm shifts: From Passive to Proactive, From Context to Memory, and From Imitation to Learning. In this dynamic environment, agents must acquire and distill practical skills and maintain persistent memory to make decisions based on evolving state variables. StuLife provides a comprehensive platform for evaluating lifelong learning capabilities, including memory retention, skill transfer, and self-motivated behavior. Beyond evaluating SOTA LLMs on the StuLife benchmark, we also explore the role of context engineering in advancing AGI.

**Link**: [arxiv](http://arxiv.org/abs/2508.19005v1),  [pdf](http://arxiv.org/pdf/2508.19005v1)

**Tags**: cs.AI cs.CL 



### LLM-Enhanced Linear Autoencoders for Recommendation
**Authors**: Jaewan Moon, Seongmin Park, Jongwuk Lee

**Updated**: 2025-08-26T12:59:55Z

**Summary**: Large language models (LLMs) have been widely adopted to enrich the semantic representation of textual item information in recommender systems. However, existing linear autoencoders (LAEs) that incorporate textual information rely on sparse word co-occurrence patterns, limiting their ability to capture rich textual semantics. To address this, we propose L3AE, the first integration of LLMs into the LAE framework. L3AE effectively integrates the heterogeneous knowledge of textual semantics and user-item interactions through a two-phase optimization strategy. (i) L3AE first constructs a semantic item-to-item correlation matrix from LLM-derived item representations. (ii) It then learns an item-to-item weight matrix from collaborative signals while distilling semantic item correlations as regularization. Notably, each phase of L3AE is optimized through closed-form solutions, ensuring global optimality and computational efficiency. Extensive experiments demonstrate that L3AE consistently outperforms state-of-the-art LLM-enhanced models on three benchmark datasets, achieving gains of 27.6% in Recall@20 and 39.3% in NDCG@20. The source code is available at https://github.com/jaewan7599/L3AE_CIKM2025.

**Link**: [arxiv](http://arxiv.org/abs/2508.13500v2),  [pdf](http://arxiv.org/pdf/2508.13500v2)

**Tags**: cs.IR cs.AI cs.CL cs.LG 



### RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation   and Language Generation for Explainable QA Hallucination Detection
**Authors**: Yiming Huang, Junyan Zhang, Zihao Wang, Biquan Bie, Yunzhong Qiu, Yi R. Fung, Xinlei He

**Updated**: 2025-08-26T12:55:45Z

**Summary**: Large Language Models (LLMs) have become powerful, but hallucinations remain a vital obstacle to their trustworthy use. While previous works improved the capability of hallucination detection by measuring uncertainty, they all lack the ability to explain the provenance behind why hallucinations occur, i.e., which part of the inputs tends to trigger hallucinations. Recent works on the prompt attack indicate that uncertainty exists in semantic propagation, where attention mechanisms gradually fuse local token information into high-level semantics across layers. Meanwhile, uncertainty also emerges in language generation, due to its probability-based selection of high-level semantics for sampled generations. Based on that, we propose RePPL to recalibrate uncertainty measurement by these two aspects, which dispatches explainable uncertainty scores to each token and aggregates in Perplexity-style Log-Average form as total score. Experiments show that our method achieves the best comprehensive detection performance across various QA datasets on advanced models (average AUC of 0.833), and our method is capable of producing token-level uncertainty scores as explanations for the hallucination. Leveraging these scores, we preliminarily find the chaotic pattern of hallucination and showcase its promising usage.

**Link**: [arxiv](http://arxiv.org/abs/2505.15386v2),  [pdf](http://arxiv.org/pdf/2505.15386v2)

**Tags**: cs.CL cs.AI 



### MOSA: Mixtures of Simple Adapters Outperform Monolithic Approaches in   LLM-based Multilingual ASR
**Authors**: Junjie Li, Jing Peng, Yangui Fang, Shuai Wang, Kai Yu

**Updated**: 2025-08-26T12:54:23Z

**Summary**: End-to-end multilingual ASR aims to transcribe speech from different languages into corresponding text, but is often limited by scarce multilingual data. LLM-based ASR aligns speech encoder outputs with LLM input space via a projector and has achieved notable success. However, prior work mainly improves performance by increasing data, with little focus on cross-lingual knowledge sharing. Moreover, a single complex projector struggles to capture both shared and language-specific features effectively. In this work, we propose MOSA (Mixture of Simple Adapters), leveraging a Mixture-of-Experts mechanism to combine lightweight adapters that learn shared and language-specific knowledge. This enables better utilization of high-resource language data to support low-resource languages, mitigating data scarcity issues. Experimental results show that MOSA-Base achieves a 15.4\% relative reduction in average WER compared to the Baseline-Base and consistently outperforms it across all languages. Remarkably, MOSA-Base surpasses the Baseline-Base even when trained with only 60\% of its parameters. Similarly, MOSA-Large outperforms the Baseline-Large in average WER and demonstrates greater robustness to data imbalance. Ablation studies further indicate that MOSA is more effective at handling individual languages and learning both language-specific and shared linguistic knowledge. These findings support that, in LLM-based ASR, a mixture of simple adapters is more effective than a single, complex adapter design.

**Link**: [arxiv](http://arxiv.org/abs/2508.18998v1),  [pdf](http://arxiv.org/pdf/2508.18998v1)

**Tags**: eess.AS 



### GitTaskBench: A Benchmark for Code Agents Solving Real-World Tasks   Through Code Repository Leveraging
**Authors**: Ziyi Ni, Huacan Wang, Shuo Zhang, Shuo Lu, Ziyang He, Wang You, Zhenheng Tang, Yuntao Du, Bill Sun, Hongzhang Liu, Sen Hu, Ronghao Chen, Bo Li, Xin Li, Chen Hu, Binxing Jiao, Daxin Jiang, Pin Lyu

**Updated**: 2025-08-26T12:48:05Z

**Summary**: Beyond scratch coding, exploiting large-scale code repositories (e.g., GitHub) for practical tasks is vital in real-world software development, yet current benchmarks rarely evaluate code agents in such authentic, workflow-driven scenarios. To bridge this gap, we introduce GitTaskBench, a benchmark designed to systematically assess this capability via 54 realistic tasks across 7 modalities and 7 domains. Each task pairs a relevant repository with an automated, human-curated evaluation harness specifying practical success criteria. Beyond measuring execution and task success, we also propose the alpha-value metric to quantify the economic benefit of agent performance, which integrates task success rates, token cost, and average developer salaries. Experiments across three state-of-the-art agent frameworks with multiple advanced LLMs show that leveraging code repositories for complex task solving remains challenging: even the best-performing system, OpenHands+Claude 3.7, solves only 48.15% of tasks. Error analysis attributes over half of failures to seemingly mundane yet critical steps like environment setup and dependency resolution, highlighting the need for more robust workflow management and increased timeout preparedness. By releasing GitTaskBench, we aim to drive progress and attention toward repository-aware code reasoning, execution, and deployment -- moving agents closer to solving complex, end-to-end real-world tasks. The benchmark and code are open-sourced at https://github.com/QuantaAlpha/GitTaskBench.

**Link**: [arxiv](http://arxiv.org/abs/2508.18993v1),  [pdf](http://arxiv.org/pdf/2508.18993v1)

**Tags**: cs.SE cs.AI 



### Automatic Prompt Optimization with Prompt Distillation
**Authors**: Viktor N. Zhuravlev, Artur R. Khairullin, Ernest A. Dyagin, Alena N. Sitkina, Nikita I. Kulin

**Updated**: 2025-08-26T12:46:58Z

**Summary**: Autoprompting is the process of automatically selecting optimized prompts for language models, which is gaining popularity due to the rapid development of prompt engineering driven by extensive research in the field of large language models (LLMs). This paper presents DistillPrompt -- a novel autoprompting method based on large language models that employs a multi-stage integration of task-specific information into prompts using training data. DistillPrompt utilizes distillation, compression, and aggregation operations to explore the prompt space more thoroughly. The method was tested on different datasets for text classification and generation tasks using the t-lite-instruct-0.1 language model. The results demonstrate a significant average improvement (e.g., 20.12% across the entire dataset compared to Grips) in key metrics over existing methods in the field, establishing DistillPrompt as one of the most effective non-gradient approaches in autoprompting.

**Link**: [arxiv](http://arxiv.org/abs/2508.18992v1),  [pdf](http://arxiv.org/pdf/2508.18992v1)

**Tags**: cs.CL cs.AI cs.LG 



### Real-Time Sounding in ISAC networks: Design and Implementation of a   Multi-Node Testbed with Synchronized Airborne and Ground-Based Sensors
**Authors**: Julia Beuster, Carsten Andrich, Sebastian Giehl, Marc Miranda, Lorenz Mohr, Dieter Novotny, Tom Kaufmann, Christian Schneider, Reiner S. Thomä

**Updated**: 2025-08-26T12:41:01Z

**Summary**: As integrated sensing and communication (ISAC) capabilities become more prevalent in the mobile 6G radio landscape, there is a substantial opportunity to enhance situational awareness across diverse applications through multi-static radar sensing within meshed ISAC networks. To facilitate the development and testing of detection and localization algorithms across diverse scenarios, this paper introduces a synchronized distributed channel sounding testbed with airborne and ground-based multi-channel transceiver nodes with centimeter-level positioning accuracy enabled by real-time kinematic (RTK) and inertial navigation system (INS) data. Our modular experimental measurement system is designed to include stationary sensor nodes and light-weight to medium-weight mobile nodes deployable on unmanned aerial vehicles (UAVs), cars, pedestrians, and cyclists. Utilizing commercial off-the-shelf (COTS) hardware, specifically software defined radios (SDRs), the testbed encourages reproducibility in academic research laboratories. We detail the individual modules and integration steps required to achieve the specified performance. The testbed's capabilities are validated through a real-world measurement campaign, including stationary and flying sensor nodes, aimed at detecting radar targets such as vertical take-off and landing (VTOL) aircrafts, small hexacopters, cars and vulnerable road users (VRUs) in air-to-air (A2A) and air-to-ground (A2G) scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2506.00624v2),  [pdf](http://arxiv.org/pdf/2506.00624v2)

**Tags**: eess.SP 



### Truth or Twist? Optimal Model Selection for Reliable Label Flipping   Evaluation in LLM-based Counterfactuals
**Authors**: Qianli Wang, Van Bach Nguyen, Nils Feldhus, Luis Felipe Villa-Arenas, Christin Seifert, Sebastian Möller, Vera Schmitt

**Updated**: 2025-08-27T14:04:13Z

**Summary**: Counterfactual examples are widely employed to enhance the performance and robustness of large language models (LLMs) through counterfactual data augmentation (CDA). However, the selection of the judge model used to evaluate label flipping, the primary metric for assessing the validity of generated counterfactuals for CDA, yields inconsistent results. To decipher this, we define four types of relationships between the counterfactual generator and judge models: being the same model, belonging to the same model family, being independent models, and having an distillation relationship. Through extensive experiments involving two state-of-the-art LLM-based methods, three datasets, four generator models, and 15 judge models, complemented by a user study (n = 90), we demonstrate that judge models with an independent, non-fine-tuned relationship to the generator model provide the most reliable label flipping evaluations. Relationships between the generator and judge models, which are closely aligned with the user study for CDA, result in better model performance and robustness. Nevertheless, we find that the gap between the most effective judge models and the results obtained from the user study remains considerably large. This suggests that a fully automated pipeline for CDA may be inadequate and requires human intervention.

**Link**: [arxiv](http://arxiv.org/abs/2505.13972v3),  [pdf](http://arxiv.org/pdf/2505.13972v3)

**Tags**: cs.CL 



### The Double-edged Sword of LLM-based Data Reconstruction: Understanding   and Mitigating Contextual Vulnerability in Word-level Differential Privacy   Text Sanitization
**Authors**: Stephen Meisenbacher, Alexandra Klymenko, Andreea-Elena Bodea, Florian Matthes

**Updated**: 2025-08-26T12:22:45Z

**Summary**: Differentially private text sanitization refers to the process of privatizing texts under the framework of Differential Privacy (DP), providing provable privacy guarantees while also empirically defending against adversaries seeking to harm privacy. Despite their simplicity, DP text sanitization methods operating at the word level exhibit a number of shortcomings, among them the tendency to leave contextual clues from the original texts due to randomization during sanitization $\unicode{x2013}$ this we refer to as $\textit{contextual vulnerability}$. Given the powerful contextual understanding and inference capabilities of Large Language Models (LLMs), we explore to what extent LLMs can be leveraged to exploit the contextual vulnerability of DP-sanitized texts. We expand on previous work not only in the use of advanced LLMs, but also in testing a broader range of sanitization mechanisms at various privacy levels. Our experiments uncover a double-edged sword effect of LLM-based data reconstruction attacks on privacy and utility: while LLMs can indeed infer original semantics and sometimes degrade empirical privacy protections, they can also be used for good, to improve the quality and privacy of DP-sanitized texts. Based on our findings, we propose recommendations for using LLM data reconstruction as a post-processing step, serving to increase privacy protection by thinking adversarially.

**Link**: [arxiv](http://arxiv.org/abs/2508.18976v1),  [pdf](http://arxiv.org/pdf/2508.18976v1)

**Tags**: cs.CR cs.CL 



### MultiRef: Controllable Image Generation with Multiple Visual References
**Authors**: Ruoxi Chen, Dongping Chen, Siyuan Wu, Sinan Wang, Shiyun Lang, Petr Sushko, Gaoyang Jiang, Yao Wan, Ranjay Krishna

**Updated**: 2025-08-26T12:18:14Z

**Summary**: Visual designers naturally draw inspiration from multiple visual references, combining diverse elements and aesthetic principles to create artwork. However, current image generative frameworks predominantly rely on single-source inputs -- either text prompts or individual reference images. In this paper, we focus on the task of controllable image generation using multiple visual references. We introduce MultiRef-bench, a rigorous evaluation framework comprising 990 synthetic and 1,000 real-world samples that require incorporating visual content from multiple reference images. The synthetic samples are synthetically generated through our data engine RefBlend, with 10 reference types and 33 reference combinations. Based on RefBlend, we further construct a dataset MultiRef containing 38k high-quality images to facilitate further research. Our experiments across three interleaved image-text models (i.e., OmniGen, ACE, and Show-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that even state-of-the-art systems struggle with multi-reference conditioning, with the best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in real-world cases on average compared to the golden answer. These findings provide valuable directions for developing more flexible and human-like creative tools that can effectively integrate multiple sources of visual inspiration. The dataset is publicly available at: https://multiref.github.io/.

**Link**: [arxiv](http://arxiv.org/abs/2508.06905v3),  [pdf](http://arxiv.org/pdf/2508.06905v3)

**Tags**: cs.CV 



### From Confidence to Collapse in LLM Factual Robustness
**Authors**: Alina Fastowski, Bardh Prenkaj, Gjergji Kasneci

**Updated**: 2025-08-26T11:54:16Z

**Summary**: Ensuring the robustness of factual knowledge in LLMs is critical for reliable applications in tasks such as question answering and reasoning. However, existing evaluation methods predominantly focus on performance-based metrics, often investigating from the perspective of prompt perturbations, which captures only the externally triggered side of knowledge robustness. To bridge this gap, we introduce a principled approach to measure factual robustness from the perspective of the generation process by analyzing token distribution entropy in combination with temperature scaling sensitivity. These two factors build the Factual Robustness Score (FRS), a novel metric which quantifies the stability of a fact against perturbations in decoding conditions, given its initial uncertainty. To validate our approach, we conduct extensive experiments on 5 LLMs across 3 closed-book QA datasets (SQuAD, TriviaQA, and HotpotQA). We show that factual robustness varies significantly -- smaller models report an FRS of $0.76$, larger ones $0.93$ -- with accuracy degrading by ~$60\%$ under increased uncertainty. These insights demonstrate how entropy and temperature scaling impact factual accuracy, and lay a foundation for developing more robust knowledge retention and retrieval in future models.

**Link**: [arxiv](http://arxiv.org/abs/2508.16267v2),  [pdf](http://arxiv.org/pdf/2508.16267v2)

**Tags**: cs.CL cs.AI 



### Interleaving Large Language Models for Compiler Testing
**Authors**: Yunbo Ni, Shaohua Li

**Updated**: 2025-08-26T11:49:58Z

**Summary**: Testing compilers with AI models, especially large language models (LLMs), has shown great promise. However, current approaches struggle with two key problems: The generated programs for testing compilers are often too simple, and extensive testing with the LLMs is computationally expensive. In this paper, we propose a novel compiler testing framework that decouples the testing process into two distinct phases: an offline phase and an online phase. In the offline phase, we use LLMs to generate a collection of small but feature-rich code pieces. In the online phase, we reuse these code pieces by strategically combining them to build high-quality and valid test programs, which are then used to test compilers.   We implement this idea in a tool, LegoFuzz, for testing C compilers. The results are striking: we found 66 bugs in GCC and LLVM, the most widely used C compilers. Almost half of the bugs are miscompilation bugs, which are serious and hard-to-find bugs that none of the existing LLM-based tools could find. We believe this efficient design opens up new possibilities for using AI models in software testing beyond just C compilers.

**Link**: [arxiv](http://arxiv.org/abs/2508.18955v1),  [pdf](http://arxiv.org/pdf/2508.18955v1)

**Tags**: cs.SE 



### SIREN: Software Identification and Recognition in HPC Systems
**Authors**: Thomas Jakobsche, Fredrik Robertsén, Jessica R. Jones, Utz-Uwe Haus, Florina M. Ciorba

**Updated**: 2025-08-26T11:44:30Z

**Summary**: HPC systems use monitoring and operational data analytics to ensure efficiency, performance, and orderly operations. Application-specific insights are crucial for analyzing the increasing complexity and diversity of HPC workloads, particularly through the identification of unknown software and recognition of repeated executions, which facilitate system optimization and security improvements. However, traditional identification methods using job or file names are unreliable for arbitrary user-provided names (a.out). Fuzzy hashing of executables detects similarities despite changes in executable version or compilation approach while preserving privacy and file integrity, overcoming these limitations. We introduce SIREN, a process-level data collection framework for software identification and recognition. SIREN improves observability in HPC by enabling analysis of process metadata, environment information, and executable fuzzy hashes. Findings from a first opt-in deployment campaign on LUMI show SIREN's ability to provide insights into software usage, recognition of repeated executions of known applications, and similarity-based identification of unknown applications.

**Link**: [arxiv](http://arxiv.org/abs/2508.18950v1),  [pdf](http://arxiv.org/pdf/2508.18950v1)

**Tags**: cs.DC 



### Integrating Large Language Model for Improved Causal Discovery
**Authors**: Taiyu Ban, Lyuzhou Chen, Derui Lyu, Xiangyu Wang, Qinrui Zhu, Qiang Tu, Huanhuan Chen

**Updated**: 2025-08-26T11:41:14Z

**Summary**: Recovering the structure of causal graphical models from observational data is an essential yet challenging task for causal discovery in scientific scenarios. Domain-specific causal discovery usually relies on expert validation or prior analysis to improve the reliability of recovered causality, which is yet limited by the scarcity of expert resources. Recently, Large Language Models (LLM) have been used for causal analysis across various domain-specific scenarios, suggesting its potential as autonomous expert roles in guiding data-based structure learning. However, integrating LLMs into causal discovery faces challenges due to inaccuracies in LLM-based reasoning on revealing the actual causal structure. To address this challenge, we propose an error-tolerant LLM-driven causal discovery framework. The error-tolerant mechanism is designed three-fold with sufficient consideration on potential inaccuracies. In the LLM-based reasoning process, an accuracy-oriented prompting strategy restricts causal analysis to a reliable range. Next, a knowledge-to-structure transition aligns LLM-derived causal statements with structural causal interactions. In the structure learning process, the goodness-of-fit to data and adherence to LLM-derived priors are balanced to further address prior inaccuracies. Evaluation of eight real-world causal structures demonstrates the efficacy of our LLM-driven approach in improving data-based causal discovery, along with its robustness to inaccurate LLM-derived priors. Codes are available at https://github.com/tyMadara/LLM-CD.

**Link**: [arxiv](http://arxiv.org/abs/2306.16902v2),  [pdf](http://arxiv.org/pdf/2306.16902v2)

**Tags**: cs.AI 



### LLMs in the SOC: An Empirical Study of Human-AI Collaboration in   Security Operations Centres
**Authors**: Ronal Singh, Shahroz Tariq, Fatemeh Jalalvand, Mohan Baruwal Chhetri, Surya Nepal, Cecile Paris, Martin Lochner

**Updated**: 2025-08-26T11:40:02Z

**Summary**: The integration of Large Language Models (LLMs) into Security Operations Centres (SOCs) presents a transformative, yet still evolving, opportunity to reduce analyst workload through human-AI collaboration. However, their real-world application in SOCs remains underexplored. To address this gap, we present a longitudinal study of 3,090 analyst queries from 45 SOC analysts over 10 months. Our analysis reveals that analysts use LLMs as on-demand aids for sensemaking and context-building, rather than for making high-stakes determinations, preserving analyst decision authority. The majority of queries are related to interpreting low-level telemetry (e.g., commands) and refining technical communication through short (1-3 turn) interactions. Notably, 93% of queries align with established cybersecurity competencies (NICE Framework), underscoring the relevance of LLM use for SOC-related tasks. Despite variations in tasks and engagement, usage trends indicate a shift from occasional exploration to routine integration, with growing adoption and sustained use among a subset of analysts. We find that LLMs function as flexible, on-demand cognitive aids that augment, rather than replace, SOC expertise. Our study provides actionable guidance for designing context-aware, human-centred AI assistance in security operations, highlighting the need for further in-the-wild research on real-world analyst-LLM collaboration, challenges, and impacts.

**Link**: [arxiv](http://arxiv.org/abs/2508.18947v1),  [pdf](http://arxiv.org/pdf/2508.18947v1)

**Tags**: cs.CR 



### VISION: Robust and Interpretable Code Vulnerability Detection Leveraging   Counterfactual Augmentation
**Authors**: David Egea, Barproda Halder, Sanghamitra Dutta

**Updated**: 2025-08-26T11:20:39Z

**Summary**: Automated detection of vulnerabilities in source code is an essential cybersecurity challenge, underpinning trust in digital systems and services. Graph Neural Networks (GNNs) have emerged as a promising approach as they can learn structural and logical code relationships in a data-driven manner. However, their performance is severely constrained by training data imbalances and label noise. GNNs often learn 'spurious' correlations from superficial code similarities, producing detectors that fail to generalize well to unseen real-world data. In this work, we propose a unified framework for robust and interpretable vulnerability detection, called VISION, to mitigate spurious correlations by systematically augmenting a counterfactual training dataset. Counterfactuals are samples with minimal semantic modifications but opposite labels. Our framework includes: (i) generating counterfactuals by prompting a Large Language Model (LLM); (ii) targeted GNN training on paired code examples with opposite labels; and (iii) graph-based interpretability to identify the crucial code statements relevant for vulnerability predictions while ignoring spurious ones. We find that VISION reduces spurious learning and enables more robust, generalizable detection, improving overall accuracy (from 51.8% to 97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group accuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20 vulnerability. We further demonstrate gains using proposed metrics: intra-class attribution variance, inter-class attribution distance, and node score dependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (real and counterfactual) from the high-impact CWE-20 category. Finally, VISION advances transparent and trustworthy AI-based cybersecurity systems through interactive visualization for human-in-the-loop analysis.

**Link**: [arxiv](http://arxiv.org/abs/2508.18933v1),  [pdf](http://arxiv.org/pdf/2508.18933v1)

**Tags**: cs.AI 



### Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement   Learning for General LLM Reasoning
**Authors**: Yang Zhou, Sunzhu Li, Shunyu Liu, Wenkai Fang, Jiale Zhao, Jingwen Yang, Jianwei Lv, Kongcheng Zhang, Yihe Zhou, Hengtong Lu, Wei Chen, Yan Xie, Mingli Song

**Updated**: 2025-08-26T10:52:15Z

**Summary**: Recent advances in Large Language Models (LLMs) have underscored the potential of Reinforcement Learning (RL) to facilitate the emergence of reasoning capabilities. Despite the encouraging results, a fundamental dilemma persists as RL improvement relies on learning from high-quality samples, yet the exploration for such samples remains bounded by the inherent limitations of LLMs. This, in effect, creates an undesirable cycle in which what cannot be explored cannot be learned. In this work, we propose Rubric-Scaffolded Reinforcement Learning (RuscaRL), a novel instructional scaffolding framework designed to break the exploration bottleneck for general LLM reasoning. Specifically, RuscaRL introduces checklist-style rubrics as (1) explicit scaffolding for exploration during rollout generation, where different rubrics are provided as external guidance within task instructions to steer diverse high-quality responses. This guidance is gradually decayed over time, encouraging the model to internalize the underlying reasoning patterns; (2) verifiable rewards for exploitation during model training, where we can obtain robust LLM-as-a-Judge scores using rubrics as references, enabling effective RL on general reasoning tasks. Extensive experiments demonstrate the superiority of the proposed RuscaRL across various benchmarks, effectively expanding reasoning boundaries under the best-of-N evaluation. Notably, RuscaRL significantly boosts Qwen2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500, surpassing GPT-4.1. Furthermore, our fine-tuned variant on Qwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading LLMs including OpenAI-o3. This work is still in progress, and we will release the code, the models, and the datasets soon.

**Link**: [arxiv](http://arxiv.org/abs/2508.16949v2),  [pdf](http://arxiv.org/pdf/2508.16949v2)

**Tags**: cs.LG cs.AI 



### DESAMO: A Device for Elder-Friendly Smart Homes Powered by Embedded LLM   with Audio Modality
**Authors**: Youngwon Choi, Donghyuk Jung, Hwayeon Kim

**Updated**: 2025-08-26T10:44:33Z

**Summary**: We present DESAMO, an on-device smart home system for elder-friendly use powered by Audio LLM, that supports natural and private interactions. While conventional voice assistants rely on ASR-based pipelines or ASR-LLM cascades, often struggling with the unclear speech common among elderly users and unable to handle non-speech audio, DESAMO leverages an Audio LLM to process raw audio input directly, enabling a robust understanding of user intent and critical events, such as falls or calls for help.

**Link**: [arxiv](http://arxiv.org/abs/2508.18918v1),  [pdf](http://arxiv.org/pdf/2508.18918v1)

**Tags**: cs.HC cs.SD eess.AS 



### Generative Artificial Intelligence and Agents in Research and Teaching
**Authors**: Jussi S. Jauhiainen, Aurora Toppari

**Updated**: 2025-08-26T10:23:02Z

**Summary**: This study provides a comprehensive analysis of the development, functioning, and application of generative artificial intelligence (GenAI) and large language models (LLMs), with an emphasis on their implications for research and education. It traces the conceptual evolution from artificial intelligence (AI) through machine learning (ML) and deep learning (DL) to transformer architectures, which constitute the foundation of contemporary generative systems. Technical aspects, including prompting strategies, word embeddings, and probabilistic sampling methods (temperature, top-k, and top-p), are examined alongside the emergence of autonomous agents. These elements are considered in relation to both the opportunities they create and the limitations and risks they entail.   The work critically evaluates the integration of GenAI across the research process, from ideation and literature review to research design, data collection, analysis, interpretation, and dissemination. While particular attention is given to geographical research, the discussion extends to wider academic contexts. A parallel strand addresses the pedagogical applications of GenAI, encompassing course and lesson design, teaching delivery, assessment, and feedback, with geography education serving as a case example.   Central to the analysis are the ethical, social, and environmental challenges posed by GenAI. Issues of bias, intellectual property, governance, and accountability are assessed, alongside the ecological footprint of LLMs and emerging technological strategies for mitigation. The concluding section considers near- and long-term futures of GenAI, including scenarios of sustained adoption, regulation, and potential decline. By situating GenAI within both scholarly practice and educational contexts, the study contributes to critical debates on its transformative potential and societal responsibilities.

**Link**: [arxiv](http://arxiv.org/abs/2508.16701v2),  [pdf](http://arxiv.org/pdf/2508.16701v2)

**Tags**: cs.CY cs.AI 



### Interactive Evaluation of Large Language Models for Multi-Requirement   Software Engineering Tasks
**Authors**: Dimitrios Rontogiannis, Maxime Peyrard, Nicolas Baldwin, Martin Josifoski, Robert West, Dimitrios Gunopulos

**Updated**: 2025-08-26T10:22:37Z

**Summary**: Standard single-turn, static benchmarks fall short in evaluating the nuanced capabilities of Large Language Models (LLMs) on complex tasks such as software engineering. In this work, we propose a novel interactive evaluation framework that assesses LLMs on multi-requirement programming tasks through structured, feedback-driven dialogue. Each task is modeled as a requirement dependency graph, and an ``interviewer'' LLM, aware of the ground-truth solution, provides minimal, targeted hints to an ``interviewee'' model to help correct errors and fulfill target constraints. This dynamic protocol enables fine-grained diagnostic insights into model behavior, uncovering strengths and systematic weaknesses that static benchmarks fail to measure. We build on DevAI, a benchmark of 55 curated programming tasks, by adding ground-truth solutions and evaluating the relevance and utility of interviewer hints through expert annotation. Our results highlight the importance of dynamic evaluation in advancing the development of collaborative code-generating agents.

**Link**: [arxiv](http://arxiv.org/abs/2508.18905v1),  [pdf](http://arxiv.org/pdf/2508.18905v1)

**Tags**: cs.AI 



### The Influence of Human-inspired Agentic Sophistication in LLM-driven   Strategic Reasoners
**Authors**: Vince Trencsenyi, Agnieszka Mensfelt, Kostas Stathis

**Updated**: 2025-08-26T10:19:45Z

**Summary**: The rapid rise of large language models (LLMs) has shifted artificial intelligence (AI) research toward agentic systems, motivating the use of weaker and more flexible notions of agency. However, this shift raises key questions about the extent to which LLM-based agents replicate human strategic reasoning, particularly in game-theoretic settings. In this context, we examine the role of agentic sophistication in shaping artificial reasoners' performance by evaluating three agent designs: a simple game-theoretic model, an unstructured LLM-as-agent model, and an LLM integrated into a traditional agentic framework. Using guessing games as a testbed, we benchmarked these agents against human participants across general reasoning patterns and individual role-based objectives. Furthermore, we introduced obfuscated game scenarios to assess agents' ability to generalise beyond training distributions. Our analysis, covering over 2000 reasoning samples across 25 agent configurations, shows that human-inspired cognitive structures can enhance LLM agents' alignment with human strategic behaviour. Still, the relationship between agentic design complexity and human-likeness is non-linear, highlighting a critical dependence on underlying LLM capabilities and suggesting limits to simple architectural augmentation.

**Link**: [arxiv](http://arxiv.org/abs/2505.09396v2),  [pdf](http://arxiv.org/pdf/2505.09396v2)

**Tags**: cs.AI cs.MA 



### Adaptive 6G Networks-in-Network Management for Industrial Applications
**Authors**: Daniel Lindenschmitt, Paul Seehofer, Marius Schmitz, Jan Mertes, Roland Bless, Martina Zitterbart, Jan C. Aurich, Hans D. Schotten

**Updated**: 2025-08-26T10:19:16Z

**Summary**: This paper presents the application of Dynamic Spectrum Management (DSM) for future 6G industrial networks, establishing an efficient controller for the Networks-in-Network (NiN) concept. The proposed architecture integrates nomadic as well as static sub-networks (SNs with diverse Quality of Service (QoS) requirements within the coverage area of an overlayer network, managed by a centralized spectrum manager (SM). Control plane connectivity between the SNs and the DSM is ensured by the self-organizing KIRA routing protocol. The demonstrated system enables scalable, zero-touch connectivity and supports nomadic SNs through seamless discovery and reconfiguration. SNs are implemented for modular Industrial Internet of Things (IIoT) scenarios, as well as for mission-critical control loops and for logistics or nomadic behavior. The DSM framework dynamically adapts spectrum allocation to meet real-time demands while ensuring reliable operation. The demonstration highlights the potential of DSM and NiNs to support flexible, dense, and heterogeneous wireless deployments in reconfigurable manufacturing environments.

**Link**: [arxiv](http://arxiv.org/abs/2508.18902v1),  [pdf](http://arxiv.org/pdf/2508.18902v1)

**Tags**: cs.NI 



### Interpretable Decision-Making for End-to-End Autonomous Driving
**Authors**: Mona Mirzaie, Bodo Rosenhahn

**Updated**: 2025-08-26T10:14:16Z

**Summary**: Trustworthy AI is mandatory for the broad deployment of autonomous vehicles. Although end-to-end approaches derive control commands directly from raw data, interpreting these decisions remains challenging, especially in complex urban scenarios. This is mainly attributed to very deep neural networks with non-linear decision boundaries, making it challenging to grasp the logic behind AI-driven decisions. This paper presents a method to enhance interpretability while optimizing control commands in autonomous driving. To address this, we propose loss functions that promote the interpretability of our model by generating sparse and localized feature maps. The feature activations allow us to explain which image regions contribute to the predicted control command. We conduct comprehensive ablation studies on the feature extraction step and validate our method on the CARLA benchmarks. We also demonstrate that our approach improves interpretability, which correlates with reducing infractions, yielding a safer, high-performance driving model. Notably, our monocular, non-ensemble model surpasses the top-performing approaches from the CARLA Leaderboard by achieving lower infraction scores and the highest route completion rate, all while ensuring interpretability.

**Link**: [arxiv](http://arxiv.org/abs/2508.18898v1),  [pdf](http://arxiv.org/pdf/2508.18898v1)

**Tags**: cs.CV cs.AI cs.LG cs.RO 



### Perception Gaps in Risk, Benefit, and Value Between Experts and Public   Challenge Socially Accepted AI
**Authors**: Philipp Brauner, Felix Glawe, Gian Luca Liehner, Luisa Vervier, Martina Ziefle

**Updated**: 2025-08-26T10:08:51Z

**Summary**: Artificial Intelligence (AI) is reshaping many societal domains, raising critical questions about its risks, benefits, and the potential misalignment between public and academic perspectives. This study examines how the general public (N=1110) -- individuals who interact with or are impacted by AI technologies -- and academic AI experts (N=119) -- those elites shaping AI development -- perceive AI's capabilities and impact across 71 scenarios. These scenarios span domains such as sustainability, healthcare, job performance, societal inequality, art, and warfare. Participants evaluated these scenarios across four dimensions using the psychometric model: likelihood, perceived risk and benefit, and overall value (or sentiment). The results suggest significant differences: experts consistently anticipate higher probabilities, perceive lower risks, report greater benefits, and express more positive sentiment toward AI compared to the non-experts. Moreover, both groups apply different weighting schemes: experts discount risk more heavily relative to benefit than non-experts. Visual mappings of these evaluations uncover areas convergent evaluations (e.g., AI performing medical diagnoses or criminal use) as well as tension points (e.g., decision of legal cases, political decision making), highlighting areas where communication and policy interventions may be needed. These findings underscore a critical translational challenge: if AI research and deployment are to align with societal priorities, the perception gap between developers and the public must be better understood and addressed. Our results provide an empirical foundation for value-sensitive AI governance and trust-building strategies across stakeholder groups.

**Link**: [arxiv](http://arxiv.org/abs/2412.01459v2),  [pdf](http://arxiv.org/pdf/2412.01459v2)

**Tags**: cs.CY cs.AI cs.HC 



### Debate-to-Detect: Reformulating Misinformation Detection as a Real-World   Debate with Large Language Models
**Authors**: Chen Han, Wenzhen Zheng, Xijin Tang

**Updated**: 2025-08-26T10:08:51Z

**Summary**: The proliferation of misinformation in digital platforms reveals the limitations of traditional detection methods, which mostly rely on static classification and fail to capture the intricate process of real-world fact-checking. Despite advancements in Large Language Models (LLMs) that enhance automated reasoning, their application to misinformation detection remains hindered by issues of logical inconsistency and superficial verification. In response, we introduce Debate-to-Detect (D2D), a novel Multi-Agent Debate (MAD) framework that reformulates misinformation detection as a structured adversarial debate. Inspired by fact-checking workflows, D2D assigns domain-specific profiles to each agent and orchestrates a five-stage debate process, including Opening Statement, Rebuttal, Free Debate, Closing Statement, and Judgment. To transcend traditional binary classification, D2D introduces a multi-dimensional evaluation mechanism that assesses each claim across five distinct dimensions: Factuality, Source Reliability, Reasoning Quality, Clarity, and Ethics. Experiments with GPT-4o on two datasets demonstrate significant improvements over baseline methods, and the case study highlight D2D's capability to iteratively refine evidence while improving decision transparency, representing a substantial advancement towards interpretable misinformation detection. The code will be released publicly after the official publication.

**Link**: [arxiv](http://arxiv.org/abs/2505.18596v4),  [pdf](http://arxiv.org/pdf/2505.18596v4)

**Tags**: cs.CL cs.AI 



### pyFAST: A Modular PyTorch Framework for Time Series Modeling with   Multi-source and Sparse Data
**Authors**: Zhijin Wang, Senzhen Wu, Yue Hu, Xiufeng Liu

**Updated**: 2025-08-26T10:05:47Z

**Summary**: Modern time series analysis demands frameworks that are flexible, efficient, and extensible. However, many existing Python libraries exhibit limitations in modularity and in their native support for irregular, multi-source, or sparse data. We introduce pyFAST, a research-oriented PyTorch framework that explicitly decouples data processing from model computation, fostering a cleaner separation of concerns and facilitating rapid experimentation. Its data engine is engineered for complex scenarios, supporting multi-source loading, protein sequence handling, efficient sequence- and patch-level padding, dynamic normalization, and mask-based modeling for both imputation and forecasting. pyFAST integrates LLM-inspired architectures for the alignment-free fusion of sparse data sources and offers native sparse metrics, specialized loss functions, and flexible exogenous data fusion. Training utilities include batch-based streaming aggregation for evaluation and device synergy to maximize computational efficiency. A comprehensive suite of classical and deep learning models (Linears, CNNs, RNNs, Transformers, and GNNs) is provided within a modular architecture that encourages extension. Released under the MIT license at GitHub, pyFAST provides a compact yet powerful platform for advancing time series research and applications.

**Link**: [arxiv](http://arxiv.org/abs/2508.18891v1),  [pdf](http://arxiv.org/pdf/2508.18891v1)

**Tags**: cs.LG cs.AI 



### HAEPO: History-Aggregated Exploratory Policy Optimization
**Authors**: Gaurish Trivedi, Alakh Sharma, Kartikey Singh Bhandari, Dhruv Kumar, Pratik Narang, Jagat Sesh Challa

**Updated**: 2025-08-26T09:59:44Z

**Summary**: Exploration is essential in modern learning, from reinforcement learning environments with small neural policies to large language models (LLMs). Existing work, such as DPO, leverages full sequence log-likelihoods to capture an entire trajectory of the model's decisions, while methods like GRPO aggregate per-token ratios into a trajectory-level update. However, both often limit exploration on long-horizon tasks. We introduce History-Aggregated Exploratory Policy Optimization (HAEPO), a history-aware exploratory loss to combat these shortcomings. HAEPO compresses each trajectory into the sum of its logarithmic probabilities (a cumulative logarithmic likelihood), and applies a Plackett-Luce softmax across trajectories to obtain normalized weights proportional to their returns, thus encouraging broader exploration. We add entropy regularization to stabilize the aggressive updates to prevent premature collapse and a soft KL penalty relative to a frozen copy of the previous (reference) policy. Empirically, HAEPO converges fast, explores thoroughly, aligns closely with true rewards, and demonstrates robust learning behavior better or at par with PPO, GRPO, and DPO across diverse tasks. Thus, HAEPO provides a stable and interpretable framework by explicitly leveraging full-trajectory history while balancing exploration and stability.

**Link**: [arxiv](http://arxiv.org/abs/2508.18884v1),  [pdf](http://arxiv.org/pdf/2508.18884v1)

**Tags**: cs.LG cs.AI 



### Judicial Requirements for Generative AI in Legal Reasoning
**Authors**: Eljas Linna, Tuula Linna

**Updated**: 2025-08-26T09:56:26Z

**Summary**: Large Language Models (LLMs) are being integrated into professional domains, yet their limitations in high-stakes fields like law remain poorly understood. This paper defines the core capabilities that an AI system must possess to function as a reliable reasoning tool in judicial decision-making. Using the IRAC (Issue-Rule-Application-Conclusion) model as an analytical framework, the study focuses on the most challenging phases of legal adjudication: determining the applicable Rule (R) and performing the Application (A) of that rule to the facts of a case. From a judicial perspective, the analysis deconstructs legal reasoning into a series of core requirements, including the ability to select the correct legal framework across jurisdictions, generate sound arguments based on the doctrine of legal sources, distinguish ratio decidendi from obiter dictum in case law, resolve ambiguity arising from general clauses like "reasonableness", manage conflicting legal provisions, and correctly apply the burden of proof. The paper then maps various AI enhancement mechanisms, such as Retrieval-Augmented Generation (RAG), multi-agent systems, and neuro-symbolic AI, to these requirements, assessing their potential to bridge the gap between the probabilistic nature of LLMs and the rigorous, choice-driven demands of legal interpretation. The findings indicate that while these techniques can address specific challenges, significant challenges remain, particularly in tasks requiring discretion and transparent, justifiable reasoning. Our paper concludes that the most effective current role for AI in law is a dual one: as a high-volume assistant for simple, repetitive cases and as a sophisticated "sparring partner" for human experts in complex matters.

**Link**: [arxiv](http://arxiv.org/abs/2508.18880v1),  [pdf](http://arxiv.org/pdf/2508.18880v1)

**Tags**: cs.AI 



### Optimization of Latent-Space Compression using Game-Theoretic Techniques   for Transformer-Based Vector Search
**Authors**: Kushagra Agrawal, Nisharg Nargund, Oishani Banerjee

**Updated**: 2025-08-26T09:51:02Z

**Summary**: Vector similarity search plays a pivotal role in modern information retrieval systems, especially when powered by transformer-based embeddings. However, the scalability and efficiency of such systems are often hindered by the high dimensionality of latent representations. In this paper, we propose a novel game-theoretic framework for optimizing latent-space compression to enhance both the efficiency and semantic utility of vector search. By modeling the compression strategy as a zero-sum game between retrieval accuracy and storage efficiency, we derive a latent transformation that preserves semantic similarity while reducing redundancy. We benchmark our method against FAISS, a widely-used vector search library, and demonstrate that our approach achieves a significantly higher average similarity (0.9981 vs. 0.5517) and utility (0.8873 vs. 0.5194), albeit with a modest increase in query time. This trade-off highlights the practical value of game-theoretic latent compression in high-utility, transformer-based search applications. The proposed system can be seamlessly integrated into existing LLM pipelines to yield more semantically accurate and computationally efficient retrieval.

**Link**: [arxiv](http://arxiv.org/abs/2508.18877v1),  [pdf](http://arxiv.org/pdf/2508.18877v1)

**Tags**: cs.IR cs.AI cs.LG 



### Empowering Computing Education Researchers Through LLM-Assisted Content   Analysis
**Authors**: Laurie Gale, Sebastian Mateos Nicolajsen

**Updated**: 2025-08-26T09:46:59Z

**Summary**: Computing education research (CER) is often instigated by practitioners wanting to improve both their own and the wider discipline's teaching practice. However, the latter is often difficult as many researchers lack the colleagues, resources, or capacity to conduct research that is generalisable or rigorous enough to advance the discipline. As a result, research methods that enable sense-making with larger volumes of qualitative data, while not increasing the burden on the researcher, have significant potential within CER.   In this discussion paper, we propose such a method for conducting rigorous analysis on large volumes of textual data, namely a variation of LLM-assisted content analysis (LACA). This method combines content analysis with the use of large language models, empowering researchers to conduct larger-scale research which they would otherwise not be able to perform. Using a computing education dataset, we illustrate how LACA could be applied in a reproducible and rigorous manner. We believe this method has potential in CER, enabling more generalisable findings from a wider range of research. This, together with the development of similar methods, can help to advance both the practice and research quality of the CER discipline.

**Link**: [arxiv](http://arxiv.org/abs/2508.18872v1),  [pdf](http://arxiv.org/pdf/2508.18872v1)

**Tags**: cs.CL 



### Energy-Efficient Precoding for Dense VCSEL-Based OWC Systems Under a   Cooperative Broadcast Model
**Authors**: Hossein Safi, Asim Ihsan, Hossien B. Eldeeb, Bastien Bechadergue, Iman Tavakkolnia, Harald Haas

**Updated**: 2025-08-26T09:46:42Z

**Summary**: As 6G and beyond aim for sustainable, high-capacity wireless connectivity, optical wireless communication (OWC) has emerged as a compelling solution.Recent advances in vertical-cavity surface-emitting laser (VCSEL) arrays have significantly enhanced OWC performance, enabling high-speed, low-power data transmission. However, dense VCSEL deployments introduce challenges related to interference and energy efficiency (EE). This paper proposes a scalable precoding framework for EE maximization in fully cooperative VCSEL-based OWC broadcast systems. We formulate a non-convex optimization problem to design the precoding matrix under practical optical constraints while guaranteeing minimum user rates. To solve this, we apply Dinkelbach's method to handle the fractional objective and the inner approximation technique to iteratively convexify and solve the problem. Simulation results show that our approach consistently outperforms regularized zero-forcing in terms of EE, particularly in large-scale deployments, demonstrating its potential for next-generation sustainable dense OWC networks.

**Link**: [arxiv](http://arxiv.org/abs/2508.18871v1),  [pdf](http://arxiv.org/pdf/2508.18871v1)

**Tags**: physics.optics 



### ReflectivePrompt: Reflective evolution in autoprompting algorithms
**Authors**: Viktor N. Zhuravlev, Artur R. Khairullin, Ernest A. Dyagin, Alena N. Sitkina, Nikita I. Kulin

**Updated**: 2025-08-26T09:46:20Z

**Summary**: Autoprompting is the process of automatically selecting optimized prompts for language models, which has been gaining popularity with the rapid advancement of prompt engineering, driven by extensive research in the field of large language models (LLMs). This paper presents ReflectivePrompt - a novel autoprompting method based on evolutionary algorithms that employs a reflective evolution approach for more precise and comprehensive search of optimal prompts. ReflectivePrompt utilizes short-term and long-term reflection operations before crossover and elitist mutation to enhance the quality of the modifications they introduce. This method allows for the accumulation of knowledge obtained throughout the evolution process and updates it at each epoch based on the current population. ReflectivePrompt was tested on 33 datasets for classification and text generation tasks using open-access large language models: t-lite-instruct-0.1 and gemma3-27b-it. The method demonstrates, on average, a significant improvement (e.g., 28% on BBH compared to EvoPrompt) in metrics relative to current state-of-the-art approaches, thereby establishing itself as one of the most effective solutions in evolutionary algorithm-based autoprompting.

**Link**: [arxiv](http://arxiv.org/abs/2508.18870v1),  [pdf](http://arxiv.org/pdf/2508.18870v1)

**Tags**: cs.CL cs.AI cs.LG 



### ClusterFusion: Expanding Operator Fusion Scope for LLM Inference via   Cluster-Level Collective Primitive
**Authors**: Xinhao Luo, Zihan Liu, Yangjie Zhou, Shihan Fang, Ziyu Huang, Yu Feng, Chen Zhang, Shixuan Sun, Zhenzhe Zheng, Jingwen Leng, Minyi Guo

**Updated**: 2025-08-26T09:29:23Z

**Summary**: Large language model (LLM) decoding suffers from high latency due to fragmented execution across operators and heavy reliance on off-chip memory for data exchange and reduction. This execution model limits opportunities for fusion and incurs significant memory traffic and kernel launch overhead. While modern architectures such as NVIDIA Hopper provide distributed shared memory and low-latency intra-cluster interconnects, they expose only low-level data movement instructions, lacking structured abstractions for collective on-chip communication. To bridge this software-hardware gap, we introduce two cluster-level communication primitives, ClusterReduce and ClusterGather, which abstract common communication patterns and enable structured, high-speed data exchange and reduction between thread blocks within a cluster, allowing intermediate results to be on-chip without involving off-chip memory. Building on these abstractions, we design ClusterFusion, an execution framework that schedules communication and computation jointly to expand operator fusion scope by composing decoding stages such as QKV Projection, Attention, and Output Projection into a single fused kernels. Evaluations on H100 GPUs show that ClusterFusion outperforms state-of-the-art inference frameworks by 1.61x on average in end-to-end latency across different models and configurations. The source code is available at https://github.com/xinhao-luo/ClusterFusion.

**Link**: [arxiv](http://arxiv.org/abs/2508.18850v1),  [pdf](http://arxiv.org/pdf/2508.18850v1)

**Tags**: cs.DC cs.AI 



### ConfTuner: Training Large Language Models to Express Their Confidence   Verbally
**Authors**: Yibo Li, Miao Xiong, Jiaying Wu, Bryan Hooi

**Updated**: 2025-08-26T09:25:32Z

**Summary**: Large Language Models (LLMs) are increasingly deployed in high-stakes domains such as science, law, and healthcare, where accurate expressions of uncertainty are essential for reliability and trust. However, current LLMs are often observed to generate incorrect answers with high confidence, a phenomenon known as "overconfidence". Recent efforts have focused on calibrating LLMs' verbalized confidence: i.e., their expressions of confidence in text form, such as "I am 80% confident that...". Existing approaches either rely on prompt engineering or fine-tuning with heuristically generated uncertainty estimates, both of which have limited effectiveness and generalizability. Motivated by the notion of proper scoring rules for calibration in classical machine learning models, we introduce ConfTuner, a simple and efficient fine-tuning method that introduces minimal overhead and does not require ground-truth confidence scores or proxy confidence estimates. ConfTuner relies on a new loss function, tokenized Brier score, which we theoretically prove to be a proper scoring rule, intuitively meaning that it "correctly incentivizes the model to report its true probability of being correct". ConfTuner improves calibration across diverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our results further show that better-calibrated confidence enables downstream gains in self-correction and model cascade, advancing the development of trustworthy LLM systems. The code is available at https://github.com/liushiliushi/ConfTuner.

**Link**: [arxiv](http://arxiv.org/abs/2508.18847v1),  [pdf](http://arxiv.org/pdf/2508.18847v1)

**Tags**: cs.CL cs.AI 



### Arrows of Math Reasoning Data Synthesis for Large Language Models:   Diversity, Complexity and Correctness
**Authors**: Sirui Chen, Changxin Tian, Binbin Hu, Kunlong Chen, Ziqi Liu, Zhiqiang Zhang, Jun Zhou

**Updated**: 2025-08-26T09:01:50Z

**Summary**: Enhancing the mathematical reasoning of large language models (LLMs) demands high-quality training data, yet conventional methods face critical challenges in scalability, cost, and data reliability. To address these limitations, we propose a novel program-assisted synthesis framework that systematically generates a high-quality mathematical corpus with guaranteed diversity, complexity, and correctness. This framework integrates mathematical knowledge systems and domain-specific tools to create executable programs. These programs are then translated into natural language problem-solution pairs and vetted by a bilateral validation mechanism that verifies solution correctness against program outputs and ensures program-problem consistency. We have generated 12.3 million such problem-solving triples. Experiments demonstrate that models fine-tuned on our data significantly improve their inference capabilities, achieving state-of-the-art performance on several benchmark datasets and showcasing the effectiveness of our synthesis approach.

**Link**: [arxiv](http://arxiv.org/abs/2508.18824v1),  [pdf](http://arxiv.org/pdf/2508.18824v1)

**Tags**: cs.CL 



### PCR-CA: Parallel Codebook Representations with Contrastive Alignment for   Multiple-Category App Recommendation
**Authors**: Bin Tan, Wangyao Ge, Yidi Wang, Xin Liu, Jeff Burtoft, Hao Fan, Hui Wang

**Updated**: 2025-08-27T07:58:08Z

**Summary**: Modern app store recommender systems struggle with multiple-category apps, as traditional taxonomies fail to capture overlapping semantics, leading to suboptimal personalization. We propose PCR-CA (Parallel Codebook Representations with Contrastive Alignment), an end-to-end framework for improved CTR prediction. PCR-CA first extracts compact multimodal embeddings from app text, then introduces a Parallel Codebook VQ-AE module that learns discrete semantic representations across multiple codebooks in parallel -- unlike hierarchical residual quantization (RQ-VAE). This design enables independent encoding of diverse aspects (e.g., gameplay, art style), better modeling multiple-category semantics. To bridge semantic and collaborative signals, we employ a contrastive alignment loss at both the user and item levels, enhancing representation learning for long-tail items. Additionally, a dual-attention fusion mechanism combines ID-based and semantic features to capture user interests, especially for long-tail apps. Experiments on a large-scale dataset show PCR-CA achieves a +0.76% AUC improvement over strong baselines, with +2.15% AUC gains for long-tail apps. Online A/B testing further validates our approach, showing a +10.52% lift in CTR and a +16.30% improvement in CVR, demonstrating PCR-CA's effectiveness in real-world deployment. The new framework has now been fully deployed on the Microsoft Store.

**Link**: [arxiv](http://arxiv.org/abs/2508.18166v3),  [pdf](http://arxiv.org/pdf/2508.18166v3)

**Tags**: cs.IR cs.LG 



### LLM-based Contrastive Self-Supervised AMR Learning with Masked Graph   Autoencoders for Fake News Detection
**Authors**: Shubham Gupta, Shraban Kumar Chatterjee, Suman Kundu

**Updated**: 2025-08-26T08:58:35Z

**Summary**: The proliferation of misinformation in the digital age has led to significant societal challenges. Existing approaches often struggle with capturing long-range dependencies, complex semantic relations, and the social dynamics influencing news dissemination. Furthermore, these methods require extensive labelled datasets, making their deployment resource-intensive. In this study, we propose a novel self-supervised misinformation detection framework that integrates both complex semantic relations using Abstract Meaning Representation (AMR) and news propagation dynamics. We introduce an LLM-based graph contrastive loss (LGCL) that utilizes negative anchor points generated by a Large Language Model (LLM) to enhance feature separability in a zero-shot manner. To incorporate social context, we employ a multi view graph masked autoencoder, which learns news propagation features from social context graph. By combining these semantic and propagation-based features, our approach effectively differentiates between fake and real news in a self-supervised manner. Extensive experiments demonstrate that our self-supervised framework achieves superior performance compared to other state-of-the-art methodologies, even with limited labelled datasets while improving generalizability.

**Link**: [arxiv](http://arxiv.org/abs/2508.18819v1),  [pdf](http://arxiv.org/pdf/2508.18819v1)

**Tags**: cs.CL cs.SI 



### A Dynamic Approach to Collaborative Document Writing (Full Version)
**Authors**: Avital Finanser, Nimrod Talmon

**Updated**: 2025-08-26T08:53:33Z

**Summary**: We introduce a model for collaborative text aggregation in which an agent community coauthors a document, modeled as an unordered collection of paragraphs, using a dynamic mechanism: agents propose paragraphs and vote on those suggested by others. We formalize the setting and explore its realizations, concentrating on voting mechanisms that aggregate votes into a single, dynamic document. We focus on two desiderata: the eventual stability of the process and its expected social welfare. Following an impossibility result, we describe several aggregation methods and report on agent-based simulations that utilize natural language processing (NLP) and large-language models (LLMs) to model agents and their contexts. Using these simulations, we demonstrate promising results regarding the possibility of rapid convergence to a high social welfare collaborative text.

**Link**: [arxiv](http://arxiv.org/abs/2508.17489v2),  [pdf](http://arxiv.org/pdf/2508.17489v2)

**Tags**: cs.GT 



### SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on   Multimodal Large Language Models
**Authors**: Xiangyu Dong, Haoran Zhao, Jiang Gao, Haozhou Li, Xiaoguang Ma, Yaoming Zhou, Fuhai Chen, Juan Liu

**Updated**: 2025-08-26T08:52:15Z

**Summary**: Recent advances in vision-language navigation (VLN) were mainly attributed to emerging large language models (LLMs). These methods exhibited excellent generalization capabilities in instruction understanding and task reasoning. However, they were constrained by the fixed knowledge bases and reasoning abilities of LLMs, preventing fully incorporating experiential knowledge and thus resulting in a lack of efficient evolutionary capacity. To address this, we drew inspiration from the evolution capabilities of natural agents, and proposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the ability to continuously evolve during testing. To the best of our knowledge, it was the first time that an multimodal LLM-powered self-evolving VLN framework was proposed. Specifically, SE-VLN comprised three core modules, i.e., a hierarchical memory module to transfer successful and failure cases into reusable knowledge, a retrieval-augmented thought-based reasoning module to retrieve experience and enable multi-step decision-making, and a reflection module to realize continual evolution. Comprehensive tests illustrated that the SE-VLN achieved navigation success rates of 57% and 35.2% in unseen environments, representing absolute performance improvements of 23.9% and 15.0% over current state-of-the-art methods on R2R and REVERSE datasets, respectively. Moreover, the SE-VLN showed performance improvement with increasing experience repository, elucidating its great potential as a self-evolving agent framework for VLN.

**Link**: [arxiv](http://arxiv.org/abs/2507.13152v3),  [pdf](http://arxiv.org/pdf/2507.13152v3)

**Tags**: cs.CV cs.AI cs.RO 



### Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with   Capability in Mind for Better Reasoning
**Authors**: Shangziqi Zhao, Jiahao Yuan, Guisong Yang, Usman Naseem

**Updated**: 2025-08-26T08:50:21Z

**Summary**: Long chain-of-thought (Long-CoT) reasoning improves accuracy in LLMs, yet its verbose, self-reflective style often hinders effective distillation into small language models (SLMs). We revisit Long-CoT compression through the lens of capability alignment and ask: Can pruning improve reasoning? We propose Prune-on-Logic, a structure-aware framework that transforms Long-CoT into logic graphs and selectively prunes low-utility reasoning steps under self-verification constraints. Through systematic analysis across three pruning strategies - targeting entire chains, core reasoning, and verification - we find that verification pruning consistently improves accuracy while reducing token usage, whereas reasoning or indiscriminate pruning degrades performance. Our study reveals that effective pruning aligns supervision with model capacity rather than merely shortening inputs. Gains hold across tasks, model scales, and CoT capability, with larger models benefiting more from pruning due to richer but more redundant reasoning. Our empirical findings highlight pruning as a structural optimization strategy for aligning CoT reasoning with SLM capacity.

**Link**: [arxiv](http://arxiv.org/abs/2505.14582v2),  [pdf](http://arxiv.org/pdf/2505.14582v2)

**Tags**: cs.CL 



### STARec: An Efficient Agent Framework for Recommender Systems via   Autonomous Deliberate Reasoning
**Authors**: Chenghao Wu, Ruiyang Ren, Junjie Zhang, Ruirui Wang, Zhongrui Ma, Qi Ye, Wayne Xin Zhao

**Updated**: 2025-08-26T08:47:58Z

**Summary**: While modern recommender systems are instrumental in navigating information abundance, they remain fundamentally limited by static user modeling and reactive decision-making paradigms. Current large language model (LLM)-based agents inherit these shortcomings through their overreliance on heuristic pattern matching, yielding recommendations prone to shallow correlation bias, limited causal inference, and brittleness in sparse-data scenarios. We introduce STARec, a slow-thinking augmented agent framework that endows recommender systems with autonomous deliberative reasoning capabilities. Each user is modeled as an agent with parallel cognitions: fast response for immediate interactions and slow reasoning that performs chain-of-thought rationales. To cultivate intrinsic slow thinking, we develop anchored reinforcement training - a two-stage paradigm combining structured knowledge distillation from advanced reasoning models with preference-aligned reward shaping. This hybrid approach scaffolds agents in acquiring foundational capabilities (preference summarization, rationale generation) while enabling dynamic policy adaptation through simulated feedback loops. Experiments on MovieLens 1M and Amazon CDs benchmarks demonstrate that STARec achieves substantial performance gains compared with state-of-the-art baselines, despite using only 0.4% of the full training data.

**Link**: [arxiv](http://arxiv.org/abs/2508.18812v1),  [pdf](http://arxiv.org/pdf/2508.18812v1)

**Tags**: cs.AI 



### Near-Field Challenges in Ultra-Wideband ISAC: Beamforming Strategies and   System Insights
**Authors**: Yonghwi Kim, Sang-Hyun Park, Siyun Yang, Kai-Kit Wong, Linglong Dai, Chan-Byoung Chae

**Updated**: 2025-08-26T08:41:30Z

**Summary**: The shift toward sixth-generation (6G) wireless networks places integrated sensing and communications (ISAC) at the core of future applications such as autonomous driving, extended reality, and smart manufacturing. However, the combination of large antenna arrays and ultra-wide bandwidths brings near-field propagation effects and beam squint to the forefront, fundamentally challenging traditional far-field designs. True time delay units (TTDs) offer a potential solution, but their cost and hardware complexity limit scalability. In this article, we present practical beamforming strategies for near-field ultra-wideband ISAC systems. We explore codebook designs across analog and digital domains that mitigate beam squint, ensure reliable user coverage, and enhance sensing accuracy. We further validate these approaches through large-scale system-level simulations, including 3D map-based evaluations that reflect real-world urban environments. Our results demonstrate how carefully designed beamforming can balance communication throughput with sensing performance, achieving reliable coverage and efficient resource use even under severe near-field conditions. We conclude by highlighting open challenges in hardware, algorithms, and system integration, pointing toward research directions that will shape the deployment of 6G-ready ISAC networks.

**Link**: [arxiv](http://arxiv.org/abs/2508.18810v1),  [pdf](http://arxiv.org/pdf/2508.18810v1)

**Tags**: eess.SP 



### A Survey on Cloud-Edge-Terminal Collaborative Intelligence in AIoT   Networks
**Authors**: Jiaqi Wu, Jing Liu, Yang Liu, Lixu Wang, Zehua Wang, Wei Chen, Zijian Tian, Richard Yu, Victor C. M. Leung

**Updated**: 2025-08-26T08:38:01Z

**Summary**: The proliferation of Internet of things (IoT) devices in smart cities, transportation, healthcare, and industrial applications, coupled with the explosive growth of AI-driven services, has increased demands for efficient distributed computing architectures and networks, driving cloud-edge-terminal collaborative intelligence (CETCI) as a fundamental paradigm within the artificial intelligence of things (AIoT) community. With advancements in deep learning, large language models (LLMs), and edge computing, CETCI has made significant progress with emerging AIoT applications, moving beyond isolated layer optimization to deployable collaborative intelligence systems for AIoT (CISAIOT), a practical research focus in AI, distributed computing, and communications. This survey describes foundational architectures, enabling technologies, and scenarios of CETCI paradigms, offering a tutorial-style review for CISAIOT beginners. We systematically analyze architectural components spanning cloud, edge, and terminal layers, examining core technologies including network virtualization, container orchestration, and software-defined networking, while presenting categorizations of collaboration paradigms that cover task offloading, resource allocation, and optimization across heterogeneous infrastructures. Furthermore, we explain intelligent collaboration learning frameworks by reviewing advances in federated learning, distributed deep learning, edge-cloud model evolution, and reinforcement learning-based methods. Finally, we discuss challenges (e.g., scalability, heterogeneity, interoperability) and future trends (e.g., 6G+, agents, quantum computing, digital twin), highlighting how integration of distributed computing and communication can address open issues and guide development of robust, efficient, and secure collaborative AIoT systems.

**Link**: [arxiv](http://arxiv.org/abs/2508.18803v1),  [pdf](http://arxiv.org/pdf/2508.18803v1)

**Tags**: cs.NI cs.AI 



### Robust and Label-Efficient Deep Waste Detection
**Authors**: Hassan Abid, Khan Muhammad, Muhammad Haris Khan

**Updated**: 2025-08-26T08:34:04Z

**Summary**: Effective waste sorting is critical for sustainable recycling, yet AI research in this domain continues to lag behind commercial systems due to limited datasets and reliance on legacy object detectors. In this work, we advance AI-driven waste detection by establishing strong baselines and introducing an ensemble-based semi-supervised learning framework. We first benchmark state-of-the-art Open-Vocabulary Object Detection (OVOD) models on the real-world ZeroWaste dataset, demonstrating that while class-only prompts perform poorly, LLM-optimized prompts significantly enhance zero-shot accuracy. Next, to address domain-specific limitations, we fine-tune modern transformer-based detectors, achieving a new baseline of 51.6 mAP. We then propose a soft pseudo-labeling strategy that fuses ensemble predictions using spatial and consensus-aware weighting, enabling robust semi-supervised training. Applied to the unlabeled ZeroWaste-s subset, our pseudo-annotations achieve performance gains that surpass fully supervised training, underscoring the effectiveness of scalable annotation pipelines. Our work contributes to the research community by establishing rigorous baselines, introducing a robust ensemble-based pseudo-labeling pipeline, generating high-quality annotations for the unlabeled ZeroWaste-s subset, and systematically evaluating OVOD models under real-world waste sorting conditions. Our code is available at: https://github.com/h-abid97/robust-waste-detection.

**Link**: [arxiv](http://arxiv.org/abs/2508.18799v1),  [pdf](http://arxiv.org/pdf/2508.18799v1)

**Tags**: cs.CV 



### CASP: An evaluation dataset for formal verification of C code
**Authors**: Niclas Hertzberg, Merlijn Sevenhuijsen, Liv Kåreborn, Anna Lokrantz

**Updated**: 2025-08-26T08:30:34Z

**Summary**: Recent developments in Large Language Models (LLMs) have shown promise in automating code generation, yet the generated programs lack rigorous correctness guarantees. Formal verification can address this shortcoming, but requires expertise and is time-consuming to apply. Currently, there is no dataset of verified C code paired with formal specifications that enables systematic benchmarking in this space. To fill this gap, we present a curated evaluation dataset of C code paired with formal specifications written in ANSI/ISO C Specification Language (ACSL). We develop a multi-stage filtering process to carefully extract 506 pairs of C code and formal specifications from The Stack 1 and The Stack 2. We first identify C files annotated with formal languages. Then, we ensure that the annotated C files formally verify, and employ LLMs to improve non-verifying files. Furthermore, we post-process the remaining files into pairs of C code and ACSL specifications, where each specification-implementation pair is formally verified using Frama-C. To ensure the quality of the pairs, a manual inspection is conducted to confirm the correctness of every pair. The resulting dataset of C-ACSL specification pairs (CASP) provides a foundation for benchmarking and further research on integrating automated code generation with verified correctness.

**Link**: [arxiv](http://arxiv.org/abs/2508.18798v1),  [pdf](http://arxiv.org/pdf/2508.18798v1)

**Tags**: cs.FL 



### CausalMACE: Causality Empowered Multi-Agents in Minecraft Cooperative   Tasks
**Authors**: Qi Chai, Zhang Zheng, Junlong Ren, Deheng Ye, Zichuan Lin, Hao Wang

**Updated**: 2025-08-26T08:29:05Z

**Summary**: Minecraft, as an open-world virtual interactive environment, has become a prominent platform for research on agent decision-making and execution. Existing works primarily adopt a single Large Language Model (LLM) agent to complete various in-game tasks. However, for complex tasks requiring lengthy sequences of actions, single-agent approaches often face challenges related to inefficiency and limited fault tolerance. Despite these issues, research on multi-agent collaboration remains scarce. In this paper, we propose CausalMACE, a holistic causality planning framework designed to enhance multi-agent systems, in which we incorporate causality to manage dependencies among subtasks. Technically, our proposed framework introduces two modules: an overarching task graph for global task planning and a causality-based module for dependency management, where inherent rules are adopted to perform causal intervention. Experimental results demonstrate our approach achieves state-of-the-art performance in multi-agent cooperative tasks of Minecraft.

**Link**: [arxiv](http://arxiv.org/abs/2508.18797v1),  [pdf](http://arxiv.org/pdf/2508.18797v1)

**Tags**: cs.AI 



### Insights into User Interface Innovations from a Design Thinking Workshop   at deRSE25
**Authors**: Maximilian Frank, Simon Lund

**Updated**: 2025-08-26T08:11:50Z

**Summary**: Large Language Models have become widely adopted tools due to their versatile capabilities, yet their user interfaces remain limited, often following rigid, linear interaction paradigms. In this paper, we present insights from a design thinking workshop held at the deRSE25 conference aiming at collaboratively developing innovative user interface concepts for LLMs. During the workshop, participants identified common use cases, evaluated the strengths and shortcomings of current LLM interfaces, and created visualizations of new interaction concepts emphasizing flexible context management, dynamic conversation branching, and enhanced mechanisms for user control. We describe how these participant-generated ideas advanced our own whiteboard-based UI approach. The ongoing development of this interface is guided by the human-centered design process - an iterative, user-focused methodology that emphasizes continuous refinement through user feedback. Broader implications for future LLM interface development are discussed, advocating for increased attention to UI innovation grounded in user-centered design principles.

**Link**: [arxiv](http://arxiv.org/abs/2508.18784v1),  [pdf](http://arxiv.org/pdf/2508.18784v1)

**Tags**: cs.HC cs.AI 



### ST-Raptor: LLM-Powered Semi-Structured Table Question Answering
**Authors**: Zirui Tang, Boyu Niu, Xuanhe Zhou, Boxiu Li, Wei Zhou, Jiannan Wang, Guoliang Li, Xinyi Zhang, Fan Wu

**Updated**: 2025-08-26T08:10:55Z

**Summary**: Semi-structured tables, widely used in real-world applications (e.g., financial reports, medical records, transactional orders), often involve flexible and complex layouts (e.g., hierarchical headers and merged cells). These tables generally rely on human analysts to interpret table layouts and answer relevant natural language questions, which is costly and inefficient. To automate the procedure, existing methods face significant challenges. First, methods like NL2SQL require converting semi-structured tables into structured ones, which often causes substantial information loss. Second, methods like NL2Code and multi-modal LLM QA struggle to understand the complex layouts of semi-structured tables and cannot accurately answer corresponding questions. To this end, we propose ST-Raptor, a tree-based framework for semi-structured table question answering using large language models. First, we introduce the Hierarchical Orthogonal Tree (HO-Tree), a structural model that captures complex semi-structured table layouts, along with an effective algorithm for constructing the tree. Second, we define a set of basic tree operations to guide LLMs in executing common QA tasks. Given a user question, ST-Raptor decomposes it into simpler sub-questions, generates corresponding tree operation pipelines, and conducts operation-table alignment for accurate pipeline execution. Third, we incorporate a two-stage verification mechanism: forward validation checks the correctness of execution steps, while backward validation evaluates answer reliability by reconstructing queries from predicted answers. To benchmark the performance, we present SSTQA, a dataset of 764 questions over 102 real-world semi-structured tables. Experiments show that ST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code is available at https://github.com/weAIDB/ST-Raptor.

**Link**: [arxiv](http://arxiv.org/abs/2508.18190v2),  [pdf](http://arxiv.org/pdf/2508.18190v2)

**Tags**: cs.AI cs.DB cs.IR 



### Survey on Monocular Metric Depth Estimation
**Authors**: Jiuling Zhang

**Updated**: 2025-08-26T08:10:41Z

**Summary**: Monocular Depth Estimation (MDE) enables spatial understanding, 3D reconstruction, and autonomous navigation, yet deep learning approaches often predict only relative depth without a consistent metric scale. This limitation reduces reliability in applications such as visual SLAM, precise 3D modeling, and view synthesis. Monocular Metric Depth Estimation (MMDE) overcomes this challenge by producing depth maps with absolute scale, ensuring geometric consistency and enabling deployment without additional calibration. This survey reviews the evolution of MMDE, from geometry-based methods to state-of-the-art deep models, with emphasis on the datasets that drive progress. Key benchmarks, including KITTI, NYU-D, ApolloScape, and TartanAir, are examined in terms of modality, scene type, and application domain. Methodological advances are analyzed, covering domain generalization, boundary preservation, and the integration of synthetic and real data. Techniques such as unsupervised and semi-supervised learning, patch-based inference, architectural innovations, and generative modeling are evaluated for their strengths and limitations. By synthesizing current progress, highlighting the importance of high-quality datasets, and identifying open challenges, this survey provides a structured reference for advancing MMDE and supporting its adoption in real-world computer vision systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.11841v4),  [pdf](http://arxiv.org/pdf/2501.11841v4)

**Tags**: cs.CV 



### Controllable Conversational Theme Detection Track at DSTC 12
**Authors**: Igor Shalyminov, Hang Su, Jake Vincent, Siffi Singh, Jason Cai, James Gung, Raphael Shu, Saab Mansour

**Updated**: 2025-08-26T08:10:01Z

**Summary**: Conversational analytics has been on the forefront of transformation driven by the advances in Speech and Natural Language Processing techniques. Rapid adoption of Large Language Models (LLMs) in the analytics field has taken the problems that can be automated to a new level of complexity and scale. In this paper, we introduce Theme Detection as a critical task in conversational analytics, aimed at automatically identifying and categorizing topics within conversations. This process can significantly reduce the manual effort involved in analyzing expansive dialogs, particularly in domains like customer support or sales. Unlike traditional dialog intent detection, which often relies on a fixed set of intents for downstream system logic, themes are intended as a direct, user-facing summary of the conversation's core inquiry. This distinction allows for greater flexibility in theme surface forms and user-specific customizations. We pose Controllable Conversational Theme Detection problem as a public competition track at Dialog System Technology Challenge (DSTC) 12 -- it is framed as joint clustering and theme labeling of dialog utterances, with the distinctive aspect being controllability of the resulting theme clusters' granularity achieved via the provided user preference data. We give an overview of the problem, the associated dataset and the evaluation metrics, both automatic and human. Finally, we discuss the participant teams' submissions and provide insights from those. The track materials (data and code) are openly available in the GitHub repository.

**Link**: [arxiv](http://arxiv.org/abs/2508.18783v1),  [pdf](http://arxiv.org/pdf/2508.18783v1)

**Tags**: cs.CL 



### Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical   Error Correction
**Authors**: Yilin Li, Xunjian Yin, Yilin Chen, Xiaojun Wan

**Updated**: 2025-08-26T08:04:04Z

**Summary**: Grammatical error correction is a significant task in NLP. Traditional methods based on encoder-decoder models have achieved certain success, but the application of LLMs in this field is still underexplored. Current research predominantly relies on supervised fine-tuning to train LLMs to directly generate the corrected sentence, which limits the model's powerful reasoning ability. To address this limitation, we propose a novel framework based on Rule-Based RL. Through experiments on the Chinese datasets, our Rule-Based RL framework achieves \textbf{state-of-the-art }performance, with a notable increase in \textbf{recall}. This result clearly highlights the advantages of using RL to steer LLMs, offering a more controllable and reliable paradigm for future development in GEC.

**Link**: [arxiv](http://arxiv.org/abs/2508.18780v1),  [pdf](http://arxiv.org/pdf/2508.18780v1)

**Tags**: cs.CL cs.AI 



### YuLan-OneSim: Towards the Next Generation of Social Simulator with Large   Language Models
**Authors**: Lei Wang, Heyang Gao, Xiaohe Bo, Xu Chen, Ji-Rong Wen

**Updated**: 2025-08-26T08:03:56Z

**Summary**: Leveraging large language model (LLM) based agents to simulate human social behaviors has recently gained significant attention. In this paper, we introduce a novel social simulator called YuLan-OneSim. Compared to previous works, YuLan-OneSim distinguishes itself in five key aspects: (1) Code-free scenario construction: Users can simply describe and refine their simulation scenarios through natural language interactions with our simulator. All simulation code is automatically generated, significantly reducing the need for programming expertise. (2) Comprehensive default scenarios: We implement 50 default simulation scenarios spanning 8 domains, including economics, sociology, politics, psychology, organization, demographics, law, and communication, broadening access for a diverse range of social researchers. (3) Evolvable simulation: Our simulator is capable of receiving external feedback and automatically fine-tuning the backbone LLMs, significantly enhancing the simulation quality. (4) Large-scale simulation: By developing a fully responsive agent framework and a distributed simulation architecture, our simulator can handle up to 100,000 agents, ensuring more stable and reliable simulation results. (5) AI social researcher: Leveraging the above features, we develop an AI social researcher. Users only need to propose a research topic, and the AI researcher will automatically analyze the input, construct simulation environments, summarize results, generate technical reports, review and refine the reports--completing the social science research loop. To demonstrate the advantages of YuLan-OneSim, we conduct experiments to evaluate the quality of the automatically generated scenarios, the reliability, efficiency, and scalability of the simulation process, as well as the performance of the AI social researcher.

**Link**: [arxiv](http://arxiv.org/abs/2505.07581v3),  [pdf](http://arxiv.org/pdf/2505.07581v3)

**Tags**: cs.AI cs.CY 



### ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large   Language Models
**Authors**: Qianyu He, Siyu Yuan, Xuefeng Li, Mingxuan Wang, Jiangjie Chen

**Updated**: 2025-08-26T07:57:28Z

**Summary**: Large language models (LLMs) with chain-of-thought reasoning have demonstrated remarkable problem-solving capabilities, but controlling their computational effort remains a significant challenge for practical deployment. Recent proprietary systems like OpenAI's gpt-oss series have introduced discrete operational modes for intuitive reasoning control, but the open-source community has largely failed to achieve such capabilities. In this paper, we introduce ThinkDial, the first open-recipe end-to-end framework that successfully implements gpt-oss-style controllable reasoning through discrete operational modes. Our system enables seamless switching between three distinct reasoning regimes: High mode (full reasoning capability), Medium mode (50 percent token reduction with <10 percent performance degradation), and Low mode (75 percent token reduction with <15 percent performance degradation). We achieve this through an end-to-end training paradigm that integrates budget-mode control throughout the entire pipeline: budget-mode supervised fine-tuning that embeds controllable reasoning capabilities directly into the learning process, and two-phase budget-aware reinforcement learning with adaptive reward shaping. Extensive experiments demonstrate that ThinkDial achieves target compression-performance trade-offs with clear response length reductions while maintaining performance thresholds. The framework also exhibits strong generalization capabilities on out-of-distribution tasks.

**Link**: [arxiv](http://arxiv.org/abs/2508.18773v1),  [pdf](http://arxiv.org/pdf/2508.18773v1)

**Tags**: cs.CL 



### Fingerprint Vector: Enabling Scalable and Efficient Model Fingerprint   Transfer via Vector Addition
**Authors**: Zhenhua Xu, Qichen Liu, Zhebo Wang, Wenpeng Xing, Dezhang Kong, Mohan Li, Meng Han

**Updated**: 2025-08-26T07:56:10Z

**Summary**: Backdoor-based fingerprinting has emerged as an effective technique for tracing the ownership of large language models. However, in real-world deployment scenarios, developers often instantiate multiple downstream models from a shared base model, and applying fingerprinting to each variant individually incurs prohibitive computational overhead. While inheritance-based approaches -- where fingerprints are embedded into the base model and expected to persist through fine-tuning -- appear attractive, they suffer from three key limitations: late-stage fingerprinting, fingerprint instability, and interference with downstream adaptation. To address these challenges, we propose a novel mechanism called the Fingerprint Vector. Our method first embeds a fingerprint into the base model via backdoor-based fine-tuning, then extracts a task-specific parameter delta as a fingerprint vector by computing the difference between the fingerprinted and clean models. This vector can be directly added to any structurally compatible downstream model, allowing the fingerprint to be transferred post hoc without additional fine-tuning. Extensive experiments show that Fingerprint Vector achieves comparable or superior performance to direct injection across key desiderata. It maintains strong effectiveness across diverse model architectures as well as mainstream downstream variants within the same family. It also preserves harmlessness and robustness in most cases. Even when slight robustness degradation is observed, the impact remains within acceptable bounds and is outweighed by the scalability benefits of our approach.

**Link**: [arxiv](http://arxiv.org/abs/2409.08846v3),  [pdf](http://arxiv.org/pdf/2409.08846v3)

**Tags**: cs.CR cs.CL cs.LG 



