# Arxiv Results
## Keyword: kv cache 
 ### WDMoE: Wireless Distributed Mixture of Experts for Large Language Models
**Authors**: Nan Xue, Yaping Sun, Zhiyong Chen, Meixia Tao, Xiaodong Xu, Liang Qian, Shuguang Cui, Wenjun Zhang, Ping Zhang

**Updated**: 2024-11-11T02:48:00Z

**Summary**: Large Language Models (LLMs) have achieved significant success in various natural language processing tasks, but the role of wireless networks in supporting LLMs has not been thoroughly explored. In this paper, we propose a wireless distributed Mixture of Experts (WDMoE) architecture to enable collaborative deployment of LLMs across edge servers at the base station (BS) and mobile devices in wireless networks. Specifically, we decompose the MoE layer in LLMs by placing the gating network and the preceding neural network layer at BS, while distributing the expert networks among the devices. This deployment leverages the parallel inference capabilities of expert networks on mobile devices, effectively utilizing the limited computing and caching resources of these devices. Accordingly, we develop a performance metric for WDMoE-based LLMs, which accounts for both model capability and latency. To minimize the latency while maintaining accuracy, we jointly optimize expert selection and bandwidth allocation based on the performance metric. Moreover, we build a hardware testbed using NVIDIA Jetson kits to validate the effectiveness of WDMoE. Both theoretical simulations and practical hardware experiments demonstrate that the proposed method can significantly reduce the latency without compromising LLM performance.

**Link**: [arxiv](http://arxiv.org/abs/2411.06681v1),  [pdf](http://arxiv.org/pdf/2411.06681v1)

**Tags**: cs.LG cs.AI cs.DC cs.IT math.IT 



### Anchor Attention, Small Cache: Code Generation with Large Language   Models
**Authors**: Xiangyu Zhang, Yu Zhou, Guang Yang, Harald C. Gall, Taolue Chen

**Updated**: 2024-11-11T02:47:05Z

**Summary**: The development of large language models (LLMs) has revolutionized automated code generation. However, their high demand of computation resources has hindered a broader deployment and raised environmental concerns. A common strategy for diminishing computational demands is to cache Key-Value (KV) states from the attention mechanism which is adopted predominately by mainstream LLMs. It can mitigate the need of repeated attention computations, but brings significant memory overhead. Current practices in NLP often use sparse attention which may, unfortunately, lead to substantial inaccuracies, or hallucinations, in code generation tasks. In this paper, we analyze the attention weights distribution within code generation models via an empirical study, uncovering a sparsity pattern, i.e., the aggregation of information at specific anchor points. Based on this observation, we propose a novel approach, AnchorCoder, which features token-wise anchor attention designed to extract and compress the contextual information, and layer-wise anchor attention enabling cross-layer communication to mitigate the issue of excessive superposition caused by the compression. The extensive experiments across multiple benchmark datasets confirm the effectiveness of AnchorCoder, which can consistently achieve a significant (at least 70%) reduction in KV cache requirements, while preserving the majority of model's performance.

**Link**: [arxiv](http://arxiv.org/abs/2411.06680v1),  [pdf](http://arxiv.org/pdf/2411.06680v1)

**Tags**: cs.SE 68N19 D.2.3 



### An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning
**Authors**: Dong Li, Aijia Zhang, Junqi Gao, Biqing Qi

**Updated**: 2024-11-11T01:53:14Z

**Summary**: Incremental graph learning has gained significant attention for its ability to address the catastrophic forgetting problem in graph representation learning. However, traditional methods often rely on a large number of labels for node classification, which is impractical in real-world applications. This makes few-shot incremental learning on graphs a pressing need. Current methods typically require extensive training samples from meta-learning to build memory and perform intensive fine-tuning of GNN parameters, leading to high memory consumption and potential loss of previously learned knowledge. To tackle these challenges, we introduce Mecoin, an efficient method for building and maintaining memory. Mecoin employs Structured Memory Units to cache prototypes of learned categories, as well as Memory Construction Modules to update these prototypes for new categories through interactions between the nodes and the cached prototypes. Additionally, we have designed a Memory Representation Adaptation Module to store probabilities associated with each class prototype, reducing the need for parameter fine-tuning and lowering the forgetting rate. When a sample matches its corresponding class prototype, the relevant probabilities are retrieved from the MRaM. Knowledge is then distilled back into the GNN through a Graph Knowledge Distillation Module, preserving the model's memory. We analyze the effectiveness of Mecoin in terms of generalization error and explore the impact of different distillation strategies on model performance through experiments and VC-dimension analysis. Compared to other related works, Mecoin shows superior performance in accuracy and forgetting rate. Our code is publicly available on the https://github.com/Arvin0313/Mecoin-GFSCIL.git .

**Link**: [arxiv](http://arxiv.org/abs/2411.06659v1),  [pdf](http://arxiv.org/pdf/2411.06659v1)

**Tags**: cs.LG cs.AI 



### GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for   Dynamic Graph Processing
**Authors**: Hongfu Li

**Updated**: 2024-11-10T15:58:07Z

**Summary**: An efficient data structure is fundamental to meeting the growing demands in dynamic graph processing. However, the dual requirements for graph computation efficiency (with contiguous structures) and graph update efficiency (with linked list-like structures) present a conflict in the design principles of graph structures. After experimental studies of existing state-of-the-art dynamic graph structures, we observe that the overhead of cache misses accounts for a major portion of the graph computation time. This paper presents GastCoCo, a system with graph storage and coroutine-based prefetch co-design. By employing software prefetching via stackless coroutines and introducing a prefetch-friendly data structure CBList, GastCoCo significantly alleviates the performance degradation caused by cache misses. Our results show that GastCoCo outperforms state-of-the-art graph storage systems by 1.3x - 180x in graph updates and 1.4x - 41.1x in graph computation.

**Link**: [arxiv](http://arxiv.org/abs/2312.14396v4),  [pdf](http://arxiv.org/pdf/2312.14396v4)

**Tags**: cs.DB 



### Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion   Prior
**Authors**: Tanvir Mahmud, Mustafa Munir, Radu Marculescu, Diana Marculescu

**Updated**: 2024-11-10T10:08:37Z

**Summary**: Video-to-video synthesis poses significant challenges in maintaining character consistency, smooth temporal transitions, and preserving visual quality during fast motion. While recent fully cross-frame self-attention mechanisms have improved character consistency across multiple frames, they come with high computational costs and often include redundant operations, especially for videos with higher frame rates. To address these inefficiencies, we propose an adaptive motion-guided cross-frame attention mechanism that selectively reduces redundant computations. This enables a greater number of cross-frame attentions over more frames within the same computational budget, thereby enhancing both video quality and temporal coherence. Our method leverages optical flow to focus on moving regions while sparsely attending to stationary areas, allowing for the joint editing of more frames without increasing computational demands. Traditional frame interpolation techniques struggle with motion blur and flickering in intermediate frames, which compromises visual fidelity. To mitigate this, we introduce KV-caching for jointly edited frames, reusing keys and values across intermediate frames to preserve visual quality and maintain temporal consistency throughout the video. With our adaptive cross-frame self-attention approach, we achieve a threefold increase in the number of keyframes processed compared to existing methods, all within the same computational budget as fully cross-frame attention baselines. This results in significant improvements in prediction accuracy and temporal consistency, outperforming state-of-the-art approaches. Code will be made publicly available at https://github.com/tanvir-utexas/AdaVE/tree/main

**Link**: [arxiv](http://arxiv.org/abs/2406.04873v2),  [pdf](http://arxiv.org/pdf/2406.04873v2)

**Tags**: cs.CV cs.AI 



### LSMGraph: A High-Performance Dynamic Graph Storage System with   Multi-Level CSR
**Authors**: Song Yu, Shufeng Gong, Qian Tao, Sijie Shen, Yanfeng Zhang, Wenyuan Yu, Pengxi Liu, Zhixin Zhang, Hongfu Li, Xiaojian Luo, Ge Yu, Jingren Zhou

**Updated**: 2024-11-10T08:31:18Z

**Summary**: The growing volume of graph data may exhaust the main memory. It is crucial to design a disk-based graph storage system to ingest updates and analyze graphs efficiently. However, existing dynamic graph storage systems suffer from read or write amplification and face the challenge of optimizing both read and write performance simultaneously. To address this challenge, we propose LSMGraph, a novel dynamic graph storage system that combines the write-friendly LSM-tree and the read-friendly CSR. It leverages the multi-level structure of LSM-trees to optimize write performance while utilizing the compact CSR structures embedded in the LSM-trees to boost read performance. LSMGraph uses a new memory structure, MemGraph, to efficiently cache graph updates and uses a multi-level index to speed up reads within the multi-level structure. Furthermore, LSMGraph incorporates a vertex-grained version control mechanism to mitigate the impact of LSM-tree compaction on read performance and ensure the correctness of concurrent read and write operations. Our evaluation shows that LSMGraph significantly outperforms state-of-the-art (graph) storage systems on both graph update and graph analytical workloads.

**Link**: [arxiv](http://arxiv.org/abs/2411.06392v1),  [pdf](http://arxiv.org/pdf/2411.06392v1)

**Tags**: cs.DB 



### EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in   LLM Serving
**Authors**: Haiying Shen, Tanmoy Sen

**Updated**: 2024-11-10T05:12:51Z

**Summary**: As Large Language Models (LLMs) continue to grow, reducing costs and alleviating GPU demands has become increasingly critical. However, existing schedulers primarily target either GPU compute or Key-Value Cache (KVC) utilization, failing to fully optimize both GPU compute and KVC usage during each iteration or guarantee timely KVC allocations when needed. To address these challenges, we conducted a trace-based experimental analysis and made insightful observations, leading to the design of a system called EcoServe. EcoServe maximizes multi-resource utilization while ensuring service-level objective (SLO) guarantees in LLM serving. To enable adding prompts to a batch to maximize GPU utilization in each iteration, EcoServe maintains separate waiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It batches GTs with the same predicted response lengths (RL) to save scheduling time and allocates KVC space for the predicted RL to avoid KVC allocation failures. It further has a novel KVC pipelining method, allowing sharing allocated but unused KVC space to enhance KVC utilization. In addition, it prioritizes queued requests that occupy more KVC to release KVC earlier and satisfy request service-level-objective (SLO). Experimental results demonstrate that EcoServe increases throughput by up to 4$\times$ with the same level of latency, generates up to 91\% lower job completion time and up to 91\% higher SLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs used in DistServe by up to 78\% while maintaining the same level of goodput.

**Link**: [arxiv](http://arxiv.org/abs/2411.06364v1),  [pdf](http://arxiv.org/pdf/2411.06364v1)

**Tags**: cs.DC 



### Eigen Attention: Attention in Low-Rank Space for KV Cache Compression
**Authors**: Utkarsh Saxena, Gobinda Saha, Sakshi Choudhary, Kaushik Roy

**Updated**: 2024-11-08T16:29:33Z

**Summary**: Large language models (LLMs) represent a groundbreaking advancement in the domain of natural language processing due to their impressive reasoning abilities. Recently, there has been considerable interest in increasing the context lengths for these models to enhance their applicability to complex tasks. However, at long context lengths and large batch sizes, the key-value (KV) cache, which stores the attention keys and values, emerges as the new bottleneck in memory usage during inference. To address this, we propose Eigen Attention, which performs the attention operation in a low-rank space, thereby reducing the KV cache memory overhead. Our proposed approach is orthogonal to existing KV cache compression techniques and can be used synergistically with them. Through extensive experiments over OPT, MPT, and Llama model families, we demonstrate that Eigen Attention results in up to 40% reduction in KV cache sizes and up to 60% reduction in attention operation latency with minimal drop in performance. Code is available at https://github.com/UtkarshSaxena1/EigenAttn.

**Link**: [arxiv](http://arxiv.org/abs/2408.05646v2),  [pdf](http://arxiv.org/pdf/2408.05646v2)

**Tags**: cs.LG cs.AI cs.CL 



### AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing   and Data Locality
**Authors**: Ilias Bournias, Lukas Cavigelli, Georgios Zacharopoulos

**Updated**: 2024-11-08T13:24:01Z

**Summary**: Large Language Model (LLM) inference on large-scale systems is expected to dominate future cloud infrastructures. Efficient LLM inference in cloud environments with numerous AI accelerators is challenging, necessitating extensive optimizations for optimal performance. Current systems batch prefill and decoding to boost throughput but encounter latency issues, while others disaggregate these phases, leading to resource underutilization. We propose AcceLLM, a novel method addressing latency and load balancing, inspired by the cache data management. It strategically utilizes redundant data to enhance inference via load balancing and optimal hardware use. Simulated evaluations on Nvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art systems up to 30% in latency and efficiency, handling diverse workloads effectively.

**Link**: [arxiv](http://arxiv.org/abs/2411.05555v1),  [pdf](http://arxiv.org/pdf/2411.05555v1)

**Tags**: cs.DC 



### GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic   Embedding Caching
**Authors**: Sajal Regmi, Chetan Phakami Pun

**Updated**: 2024-11-08T02:21:19Z

**Summary**: Large Language Models (LLMs), such as GPT (Radford et al., 2019), have significantly advanced artificial intelligence by enabling sophisticated natural language understanding and generation. However, the high computational and financial costs associated with frequent API calls to these models present a substantial bottleneck, especially for applications like customer service chatbots that handle repetitive queries. In this paper, we introduce GPT Semantic Cache, a method that leverages semantic caching of query embeddings in in-memory storage (Redis). By storing embeddings of user queries, our approach efficiently identifies semantically similar questions, allowing for the retrieval of pre-generated responses without redundant API calls to the LLM. This technique reduces operational costs and improves response times, enhancing the efficiency of LLM-powered applications.

**Link**: [arxiv](http://arxiv.org/abs/2411.05276v1),  [pdf](http://arxiv.org/pdf/2411.05276v1)

**Tags**: cs.LG 



### Loki: Low-rank Keys for Efficient Sparse Attention
**Authors**: Prajwal Singhania, Siddharth Singh, Shwai He, Soheil Feizi, Abhinav Bhatele

**Updated**: 2024-11-07T18:58:50Z

**Summary**: Inference on large language models (LLMs) can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in LLM inference contributes significantly to these costs, which has sparked an interest in approximating the self-attention computation to reduce such costs. In this work, we propose to approximate self-attention by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to speed up the attention computation due to reduced data movement (load/store) and compute costs while maintaining the efficacy of the models better than other popular approximation methods.

**Link**: [arxiv](http://arxiv.org/abs/2406.02542v2),  [pdf](http://arxiv.org/pdf/2406.02542v2)

**Tags**: cs.LG 



### BitNet a4.8: 4-bit Activations for 1-bit LLMs
**Authors**: Hongyu Wang, Shuming Ma, Furu Wei

**Updated**: 2024-11-07T18:41:50Z

**Summary**: Recent research on the 1-bit Large Language Models (LLMs), such as BitNet b1.58, presents a promising direction for reducing the inference cost of LLMs while maintaining their performance. In this work, we introduce BitNet a4.8, enabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid quantization and sparsification strategy to mitigate the quantization errors introduced by the outlier channels. Specifically, we utilize 4-bit activations for inputs to the attention and feed-forward network layers, while sparsifying intermediate states followed with 8-bit quantization. Extensive experiments demonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58 with equivalent training costs, while being faster in inference with enabling 4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of parameters and supports 3-bit KV cache, further enhancing the efficiency of large-scale LLM deployment and inference.

**Link**: [arxiv](http://arxiv.org/abs/2411.04965v1),  [pdf](http://arxiv.org/pdf/2411.04965v1)

**Tags**: cs.CL cs.LG 



### Adaptive Caching for Faster Video Generation with Diffusion Transformers
**Authors**: Kumara Kahatapitiya, Haozhe Liu, Sen He, Ding Liu, Menglin Jia, Chenyang Zhang, Michael S. Ryoo, Tian Xie

**Updated**: 2024-11-07T17:06:32Z

**Summary**: Generating temporally-consistent high-fidelity videos can be computationally expensive, especially over longer temporal spans. More-recent Diffusion Transformers (DiTs) -- despite making significant headway in this context -- have only heightened such challenges as they rely on larger models and heavier attention mechanisms, resulting in slower inference speeds. In this paper, we introduce a training-free method to accelerate video DiTs, termed Adaptive Caching (AdaCache), which is motivated by the fact that "not all videos are created equal": meaning, some videos require fewer denoising steps to attain a reasonable quality than others. Building on this, we not only cache computations through the diffusion process, but also devise a caching schedule tailored to each video generation, maximizing the quality-latency trade-off. We further introduce a Motion Regularization (MoReg) scheme to utilize video information within AdaCache, essentially controlling the compute allocation based on motion content. Altogether, our plug-and-play contributions grant significant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video generation) without sacrificing the generation quality, across multiple video DiT baselines.

**Link**: [arxiv](http://arxiv.org/abs/2411.02397v2),  [pdf](http://arxiv.org/pdf/2411.02397v2)

**Tags**: cs.CV 



### JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial   Cyber-Physical Systems
**Authors**: Geng Sun, Jiaxu Wu, Long He, Jiacheng Wang, Dusit Niyato, Abbas Jamalipour, Shiwen Mao

**Updated**: 2024-11-07T14:59:44Z

**Summary**: In the era of the sixth generation (6G) and industrial Internet of Things (IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of sensor devices and computing-intensive tasks. To address the limited resources of IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) has emerged as a promising solution, providing flexible and cost-effective services in close proximity of IIoT sensor devices (ISDs). However, leveraging aerial MEC to meet the delay-sensitive and computation-intensive requirements of the ISDs could face several challenges, including the limited communication, computation and caching (3C) resources, stringent offloading requirements for 3C services, and constrained on-board energy of UAVs. To address these issues, we first present a collaborative aerial MEC-assisted ICPS architecture by incorporating the computing capabilities of the macro base station (MBS) and UAVs. We then formulate a service delay minimization optimization problem (SDMOP). Since the SDMOP is proved to be an NP-hard problem, we propose a joint computation offloading, caching, communication resource allocation, computation resource allocation, and UAV trajectory control approach (JC5A). Specifically, JC5A consists of a block successive upper bound minimization method of multipliers (BSUMM) for computation offloading and service caching, a convex optimization-based method for communication and computation resource allocation, and a successive convex approximation (SCA)-based method for UAV trajectory control. Moreover, we theoretically prove the convergence and polynomial complexity of JC5A. Simulation results demonstrate that the proposed approach can achieve superior system performance compared to the benchmark approaches and algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2411.04762v1),  [pdf](http://arxiv.org/pdf/2411.04762v1)

**Tags**: cs.NI eess.SP 



### CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot   Classification
**Authors**: Qijie Wang, Guandu Liu, Bin Wang

**Updated**: 2024-11-07T09:33:40Z

**Summary**: Recent advances in vision-language foundational models, such as CLIP, have demonstrated significant strides in zero-shot classification. However, the extensive parameterization of models like CLIP necessitates a resource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have introduced training-free methods aimed at bolstering the efficacy of downstream tasks. While these approaches incorporate support sets to maintain data distribution consistency between knowledge cache and test sets, they often fall short in terms of generalization on the test set, particularly when faced with test data exhibiting substantial distributional variations. In this work, we present CapS-Adapter, an innovative method that employs a caption-based support set, effectively harnessing both image and caption features to exceed existing state-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly constructs support sets that closely mirror target distributions, utilizing instance-level distribution features extracted from multimodal large models. By leveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances predictive accuracy through the use of multimodal support sets. Our method achieves outstanding zero-shot classification results across 19 benchmark datasets, improving accuracy by 2.19\% over the previous leading method. Our contributions are substantiated through extensive validation on multiple benchmark datasets, demonstrating superior performance and robust generalization capabilities. Our code is made publicly available at https://github.com/WLuLi/CapS-Adapter.

**Link**: [arxiv](http://arxiv.org/abs/2405.16591v2),  [pdf](http://arxiv.org/pdf/2405.16591v2)

**Tags**: cs.CV 



### HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO   Computation Redundancy
**Authors**: Shuqing Luo, Jie Peng, Pingzhi Li, Tianlong Chen

**Updated**: 2024-11-07T06:40:40Z

**Summary**: Mixture-of-Experts (MoE) has emerged as a practical approach to scale up parameters for the Transformer model to achieve better generalization while maintaining a sub-linear increase in computation overhead. Current MoE models are mainly built with expert parallelism on distributed devices. However, it usually depends on homogeneous devices to deploy and suffers from heavy communication overhead and computation redundancy. In this paper, we explore developing a \texttt{H}eterogeneous-aware \texttt{EX}pert \texttt{A}llocation framework, \textbf{\texttt{HEXA-MoE}}, with significantly enhanced computing efficiency. It contains two components: ($1$) \textit{Expert-Specific Operators}. We replace the typical general matrix multiplication or grouped matrix multiplication interfaces with our operators, which allows the computing to be performed in an in-place manner with \textbf{ZERO} redundancy. ($2$) \textit{Adaptive Data- and Model-Centric Configurations} for different workload scales. Specifically, we introduce a pipeline-shared cache on each device to tackle the heavy memory consumption in the existing data-centric MoE library. Comprehensive experiments on the Swin-MoE benchmark consistently reveal the effectiveness of our \texttt{HEXA-MoE} framework, i.e., reducing $10\%\sim48\%$ memory consumption and achieving $0.5\sim4.3\times$ speed up compared to current state-of-the-art MoE libraries. Furthermore, we examine our \texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric settings. Promising results show that employing optimal parallel configuration with \texttt{HEXA-MoE} on heterogeneous devices can substantially minimize overall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.

**Link**: [arxiv](http://arxiv.org/abs/2411.01288v2),  [pdf](http://arxiv.org/pdf/2411.01288v2)

**Tags**: cs.DC 



### Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated   Parameters by Tencent
**Authors**: Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang, Jonny Han, Xiaobo Shu, Jiahao Bu, Zhongzhi Chen, Xuemeng Huang, Fengzong Lian, Saiyong Yang, Jianfeng Yan, Yuyuan Zeng, Xiaoqin Ren, Chao Yu, Lulu Wu, Yue Mao, Jun Xia, Tao Yang, Suncong Zheng, Kan Wu, Dian Jiao, Jinbao Xue, Xipeng Zhang, Decheng Wu, Kai Liu, Dengpeng Wu, Guanghui Xu, Shaohua Chen, Shuang Chen, Xiao Feng, Yigeng Hong, Junqiang Zheng, Chengcheng Xu, Zongwei Li, Xiong Kuang, Jianglu Hu, Yiqi Chen, Yuchi Deng, Guiyang Li, Ao Liu, Chenchen Zhang, Shihui Hu, Zilong Zhao, Zifan Wu, Yao Ding, Weichao Wang, Han Liu, Roberts Wang, Hao Fei, Peijie Yu, Ze Zhao, Xun Cao, Hai Wang, Fusheng Xiang, Mengyuan Huang, Zhiyuan Xiong, Bin Hu, Xuebin Hou, Lei Jiang, Jianqiang Ma, Jiajia Wu, Yaping Deng, Yi Shen, Qian Wang, Weijie Liu, Jie Liu, Meng Chen, Liang Dong, Weiwen Jia, Hu Chen, Feifei Liu, Rui Yuan, Huilin Xu, Zhenxiang Yan, Tengfei Cao, Zhichao Hu, Xinhua Feng, Dong Du, Tinghao Yu, Yangyu Tao, Feng Zhang, Jianchen Zhu, Chengzhong Xu, Xirui Li, Chong Zha, Wen Ouyang, Yinben Xia, Xiang Li, Zekun He, Rongpeng Chen, Jiawei Song, Ruibin Chen, Fan Jiang, Chongqing Zhao, Bo Wang, Hao Gong, Rong Gan, Winston Hu, Zhanhui Kang, Yong Yang, Yuhong Liu, Di Wang, Jie Jiang

**Updated**: 2024-11-06T09:15:27Z

**Summary**: In this paper, we introduce Hunyuan-Large, which is currently the largest open-source Transformer-based mixture of experts model, with a total of 389 billion parameters and 52 billion activation parameters, capable of handling up to 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior performance across various benchmarks including language understanding and generation, logical reasoning, mathematical problem-solving, coding, long-context, and aggregated tasks, where it outperforms LLama3.1-70B and exhibits comparable performance when compared to the significantly larger LLama3.1-405B model. Key practice of Hunyuan-Large include large-scale synthetic data that is orders larger than in previous literature, a mixed expert routing strategy, a key-value cache compression technique, and an expert-specific learning rate strategy. Additionally, we also investigate the scaling laws and learning rate schedule of mixture of experts models, providing valuable insights and guidances for future model development and optimization. The code and checkpoints of Hunyuan-Large are released to facilitate future innovations and applications.   Codes: https://github.com/Tencent/Hunyuan-Large   Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large

**Link**: [arxiv](http://arxiv.org/abs/2411.02265v3),  [pdf](http://arxiv.org/pdf/2411.02265v3)

**Tags**: cs.CL cs.AI 



### Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model   Training Pipelines via Memoization-Awareness
**Authors**: Abdelmajid Essofi, Ridwan Salahuddeen, Munachiso Nwadike, Elnura Zhalieva, Kun Zhang, Eric Xing, Willie Neiswanger, Qirong Ho

**Updated**: 2024-11-06T07:53:04Z

**Summary**: The training or fine-tuning of machine learning, vision, and language models is often implemented as a pipeline: a sequence of stages encompassing data preparation, model training and evaluation. In this paper, we exploit pipeline structures to reduce the cost of hyperparameter tuning for model training/fine-tuning, which is particularly valuable for language models given their high costs in GPU-days. We propose a "memoization-aware" Bayesian Optimization (BO) algorithm, EEIPU, that works in tandem with a pipeline caching system, allowing it to evaluate significantly more hyperparameter candidates per GPU-day than other tuning algorithms. The result is better-quality hyperparameters in the same amount of search time, or equivalently, reduced search time to reach the same hyperparameter quality. In our benchmarks on machine learning (model ensembles), vision (convolutional architecture) and language (T5 architecture) pipelines, we compare EEIPU against recent BO algorithms: EEIPU produces an average of $103\%$ more hyperparameter candidates (within the same budget), and increases the validation metric by an average of $108\%$ more than other algorithms (where the increase is measured starting from the end of warm-up iterations).

**Link**: [arxiv](http://arxiv.org/abs/2411.03731v1),  [pdf](http://arxiv.org/pdf/2411.03731v1)

**Tags**: cs.LG stat.ML 



### The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM   Serving Systems
**Authors**: Linke Song, Zixuan Pang, Wenhao Wang, Zihao Wang, XiaoFeng Wang, Hongbo Chen, Wei Song, Yier Jin, Dan Meng, Rui Hou

**Updated**: 2024-11-06T07:12:55Z

**Summary**: The wide deployment of Large Language Models (LLMs) has given rise to strong demands for optimizing their inference performance. Today's techniques serving this purpose primarily focus on reducing latency and improving throughput through algorithmic and hardware enhancements, while largely overlooking their privacy side effects, particularly in a multi-user environment. In our research, for the first time, we discovered a set of new timing side channels in LLM systems, arising from shared caches and GPU memory allocations, which can be exploited to infer both confidential system prompts and those issued by other users. These vulnerabilities echo security challenges observed in traditional computing systems, highlighting an urgent need to address potential information leakage in LLM serving infrastructures. In this paper, we report novel attack strategies designed to exploit such timing side channels inherent in LLM deployments, specifically targeting the Key-Value (KV) cache and semantic cache widely used to enhance LLM inference performance. Our approach leverages timing measurements and classification models to detect cache hits, allowing an adversary to infer private prompts with high accuracy. We also propose a token-by-token search algorithm to efficiently recover shared prompt prefixes in the caches, showing the feasibility of stealing system prompts and those produced by peer users. Our experimental studies on black-box testing of popular online LLM services demonstrate that such privacy risks are completely realistic, with significant consequences. Our findings underscore the need for robust mitigation to protect LLM systems against such emerging threats.

**Link**: [arxiv](http://arxiv.org/abs/2409.20002v2),  [pdf](http://arxiv.org/pdf/2409.20002v2)

**Tags**: cs.CR 



### HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE   Inference
**Authors**: Peng Tang, Jiacheng Liu, Xiaofeng Hou, Yifei Pu, Jing Wang, Pheng-Ann Heng, Chao Li, Minyi Guo

**Updated**: 2024-11-06T01:49:45Z

**Summary**: The Mixture-of-Experts (MoE) architecture has demonstrated significant advantages in the era of Large Language Models (LLMs), offering enhanced capabilities with reduced inference costs. However, deploying MoE-based LLMs on memoryconstrained edge devices remains challenging due to their substantial memory requirements. While existing expertoffloading methods alleviate the memory requirements, they often incur significant expert-loading costs or compromise model accuracy. We present HOBBIT, a mixed precision expert offloading system to enable flexible and efficient MoE inference. Our key insight is that dynamically replacing less critical cache-miss experts with low precision versions can substantially reduce expert-loading latency while preserving model accuracy. HOBBIT introduces three innovative techniques that map the natural hierarchy of MoE computation: (1) a token-level dynamic expert loading mechanism, (2) a layer-level adaptive expert prefetching technique, and (3) a sequence-level multidimensional expert caching policy. These innovations fully leverage the benefits of mixedprecision expert inference. By implementing HOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate its performance across different edge devices with representative MoE models. The results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding compared to state-of-the-art MoE offloading systems.

**Link**: [arxiv](http://arxiv.org/abs/2411.01433v2),  [pdf](http://arxiv.org/pdf/2411.01433v2)

**Tags**: cs.LG cs.DC 



### ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression
**Authors**: Rui Xie, Linsen Ma, Alex Zhong, Feng Chen, Tong Zhang

**Updated**: 2024-11-05T15:22:11Z

**Summary**: As a core component in modern data centers, key-value cache provides high-throughput and low-latency services for high-speed data processing. The effectiveness of a key-value cache relies on its ability of accommodating the needed data. However, expanding the cache capacity is often more difficult than commonly expected because of many practical constraints, such as server costs, cooling issues, rack space, and even human resource expenses. A potential solution is compression, which virtually extends the cache capacity by condensing data in cache. In practice, this seemingly simple idea has not gained much traction in key-value cache system design, due to several critical issues: the compression-unfriendly index structure, severe read/write amplification, wasteful decompression operations, and heavy computing cost. This paper presents a hybrid DRAM-SSD cache design to realize a systematic integration of data compression in key-value cache. By treating compression as an essential component, we have redesigned the indexing structure, data management, and leveraged the emerging computational SSD hardware for collaborative optimizations. We have developed a prototype, called ZipCache. Our experimental results show that ZipCache can achieve up to 72.4% higher throughput and 42.4% lower latency, while reducing the write amplification by up to 26.2 times.

**Link**: [arxiv](http://arxiv.org/abs/2411.03174v1),  [pdf](http://arxiv.org/pdf/2411.03174v1)

**Tags**: cs.DB 



### Wireless Edge Content Broadcast via Integrated Terrestrial and   Non-terrestrial Networks
**Authors**: Feng Wang, Giovanni Geraci, Lingxiang Li, Peng Wang, Tony Q. S. Quek

**Updated**: 2024-11-05T08:34:44Z

**Summary**: Non-terrestrial networks (NTN) have emerged as a transformative solution to bridge the digital divide and deliver essential services to remote and underserved areas. In this context, low Earth orbit (LEO) satellite constellations offer remarkable potential for efficient cache content broadcast in remote regions, thereby extending the reach of digital services. In this paper, we introduce a novel approach to optimize wireless edge content placement using NTN. Despite wide coverage, the varying NTN transmission capabilities must be carefully aligned with each content placement to maximize broadcast efficiency. In this paper, we introduce a novel approach to optimize wireless edge content placement using NTN, positioning NTN as a complement to TN for achieving optimal content broadcasting. Specifically, we dynamically select content for placement via NTN links. This selection is based on popularity and suitability for delivery through NTN, while considering the orbital motion of LEO satellites. Our system-level case studies, based on a practical LEO constellation, demonstrate the significant improvement in placement speed compared to existing methods, which neglect network mobility. We also demonstrate that NTN links significantly outperform standalone wireless TN solutions, particularly in the early stages of content delivery. This advantage is amplified when there is a higher correlation of content popularity across geographical regions.

**Link**: [arxiv](http://arxiv.org/abs/2308.05591v3),  [pdf](http://arxiv.org/pdf/2308.05591v3)

**Tags**: eess.SY cs.IT cs.NI cs.SY eess.SP math.IT 



### TokenSelect: Efficient Long-Context Inference and Length Extrapolation   for LLMs via Dynamic Token-Level KV Cache Selection
**Authors**: Wei Wu, Zhuoshi Pan, Chao Wang, Liyi Chen, Yunchu Bai, Kun Fu, Zheng Wang, Hui Xiong

**Updated**: 2024-11-05T07:56:24Z

**Summary**: With the development of large language models (LLMs), the ability to handle longer contexts has become a key capability for Web applications such as cross-document understanding and LLM-powered search systems. However, this progress faces two major challenges: performance degradation due to sequence lengths out-of-distribution, and excessively long inference times caused by the quadratic computational complexity of attention. These issues hinder the application of LLMs in long-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic, training-free method for efficient and accurate long-context inference. TokenSelect builds upon the observation of non-contiguous attention sparsity, using Query-Key dot products to measure per-head KV Cache criticality at token-level. By per-head soft voting mechanism, TokenSelect selectively involves a small number of critical KV cache tokens in the attention calculation without sacrificing accuracy. To further accelerate TokenSelect, we designed the Selection Cache based on observations of consecutive Query similarity and implemented efficient dot product kernel, significantly reducing the overhead of token selection. A comprehensive evaluation of TokenSelect demonstrates up to 23.84x speedup in attention computation and up to 2.28x acceleration in end-to-end latency, while providing superior performance compared to state-of-the-art long-context inference methods.

**Link**: [arxiv](http://arxiv.org/abs/2411.02886v1),  [pdf](http://arxiv.org/pdf/2411.02886v1)

**Tags**: cs.CL cs.AI cs.LG 



### DroidSpeak: Enhancing Cross-LLM Communication
**Authors**: Yuhan Liu, Esha Choukse, Shan Lu, Junchen Jiang, Madan Musuvathi

**Updated**: 2024-11-05T05:41:41Z

**Summary**: In multi-agent systems utilizing Large Language Models (LLMs), communication between agents traditionally relies on natural language. This communication often includes the full context of the query so far, which can introduce significant prefill-phase latency, especially with long contexts.   We introduce DroidSpeak, a novel framework to target this cross-LLM communication by leveraging the reuse of intermediate data, such as input embeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the need to reprocess entire contexts for fine-tuned versions of the same foundational model. This approach allows faster context integration while maintaining the quality of task performance. Experimental evaluations demonstrate DroidSpeak's ability to significantly accelerate inter-agent communication, achieving up to a 2.78x speedup in prefill latency with negligible loss in accuracy. Our findings underscore the potential to create more efficient and scalable multi-agent systems.

**Link**: [arxiv](http://arxiv.org/abs/2411.02820v1),  [pdf](http://arxiv.org/pdf/2411.02820v1)

**Tags**: cs.MA cs.AI cs.CL cs.LG 



### Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation   With Fluidic Heating
**Authors**: Di Ni, Ved Gund, Landon Ivy, Amit Lal

**Updated**: 2024-11-04T17:21:58Z

**Summary**: Integrated micro power generators are crucial components for micro robotic platforms to demonstrate untethered operation and to achieve autonomy. Current micro robotic electrostatic actuators typically require hundreds to thousands of voltages to output sufficient work. Pyroelectricity is one such source of high voltages that can be scaled to small form factors. This paper demonstrates a distributed pyroelectric high voltage generation mechanism to power kV actuators using alternating exposure of crystals to hot and cold water (300C to 900C water temperature). Using this fluidic temperature control, a pyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage capacitor yielding a 6.10 {\mu}J stored energy. A maximum energy of 17.46 {\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can be used to heat a distributed array of converters to generate electricity in distant robotic actuator sections. The development of this distributed system would enable untethered micro-robot to be operated with a flexible body and free of battery recharging, which advances its applications in the real world.

**Link**: [arxiv](http://arxiv.org/abs/2411.02295v1),  [pdf](http://arxiv.org/pdf/2411.02295v1)

**Tags**: cs.RO cs.SY eess.SY 



### TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory   Encryption
**Authors**: Martin Unterguggenberger, Lukas Lamster, David Schrammel, Martin Schwarzl, Stefan Mangard

**Updated**: 2024-11-04T12:14:07Z

**Summary**: Efficient cloud computing relies on in-process isolation to optimize performance by running workloads within a single process. Without heavy-weight process isolation, memory safety errors pose a significant security threat by allowing an adversary to extract or corrupt the private data of other co-located tenants. Existing in-process isolation mechanisms are not suitable for modern cloud requirements, e.g., MPK's 16 protection domains are insufficient to isolate thousands of cloud workers per process. Consequently, cloud service providers have a strong need for lightweight in-process isolation on commodity x86 machines.   This paper presents TME-Box, a novel isolation technique that enables fine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing Intel TME-MK, which is intended for the encryption of virtual machines, TME-Box offers lightweight and efficient in-process isolation. TME-Box enforces that sandboxes use their designated encryption keys for memory interactions through compiler instrumentation. This cryptographic isolation enables fine-grained access control, from single cache lines to full pages, and supports flexible data relocation. In addition, the design of TME-Box allows the efficient isolation of up to 32K concurrent sandboxes. We present a performance-optimized TME-Box prototype, utilizing x86 segment-based addressing, that showcases geomean performance overheads of 5.2 % for data isolation and 9.7 % for code and data isolation, evaluated with the SPEC CPU2017 benchmark suite.

**Link**: [arxiv](http://arxiv.org/abs/2407.10740v2),  [pdf](http://arxiv.org/pdf/2407.10740v2)

**Tags**: cs.CR 



### Diversity in Network-Friendly Recommendations
**Authors**: Evangelia Tzimpimpaki, Thrasyvoulos Spyropoulos

**Updated**: 2024-11-04T09:40:27Z

**Summary**: In recent years, the Internet has been dominated by content-rich platforms, employing recommendation systems to provide users with more appealing content (e.g., videos in YouTube, movies in Netflix). While traditional content recommendations are oblivious to network conditions, the paradigm of Network-Friendly Recommendations (NFR) has recently emerged, favoring content that improves network performance (e.g. cached near the user), while still being appealing to the user. However, NFR algorithms sometimes achieve their goal by shrinking the pool of content recommended to users. The undesirable side-effect is reduced content diversity, a phenomenon known as ``content/filter bubble''. This reduced diversity is problematic for both users, who are prevented from exploring a broader range of content, and content creators (e.g. YouTubers) whose content may be recommended less frequently, leading to perceived unfairness. In this paper, we first investigate - using real data and state-of-the-art NFR schemes - the extent of this phenomenon. We then formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly recommendations with - sufficient - content diversity), and through a series of transformation steps, we manage to reduce it to a linear program that can be solved fast and optimally. Our findings show that Diverse-NFR can achieve high network gains (comparable to non-diverse NFR) while maintaining diversity constraints. To our best knowledge, this is the first work that incorporates diversity issues into network-friendly recommendation algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2411.00601v2),  [pdf](http://arxiv.org/pdf/2411.00601v2)

**Tags**: cs.PF 



### Context Parallelism for Scalable Million-Token Inference
**Authors**: Amy Yang, Jingyi Yang, Aya Ibrahim, Xinfeng Xie, Bangsheng Tang, Grigory Sizov, Jongsoo Park, Jianyu Huang

**Updated**: 2024-11-04T04:15:36Z

**Summary**: We present context parallelism for long-context large language model inference, which achieves near-linear scaling for long-context prefill latency with up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M context prefill with Llama3 405B model in 77s (93% parallelization efficiency, 63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two lossless exact ring attention variants: pass-KV and pass-Q to cover a wide range of use cases with the state-of-the-art performance: full prefill, persistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected with RDMA and TCP both show similar scalability for long-context prefill, demonstrating that our method scales well using common commercial data center with medium-to-low inter-host bandwidth.

**Link**: [arxiv](http://arxiv.org/abs/2411.01783v1),  [pdf](http://arxiv.org/pdf/2411.01783v1)

**Tags**: cs.DC cs.AI cs.LG 



### Experimental demonstration of dark current mitigation by an   over-inserted plug in a normal conducting VHF gun
**Authors**: X. -H. Wang, G. Shu, H. Qian, X. Li, Z. Liu, Z. Jiang, H. Meng, C. Xing, Q. Zhou, H. Deng

**Updated**: 2024-11-04T02:35:03Z

**Summary**: The room temperature continuous wave (CW) very-high-frequency (VHF) gun is one of the candidates for the electron gun of the high-repetition-rate free-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~ 20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission leads to beam loss along the FEL machine, therefore is a critical parameter for the performance of the CW gun. In this paper, we presents a systematic study of the dark current reduction of the VHF gun, including cathode region optimizations, dark current tracking simulations and measurements. Over-inserted cathode plugs were tested in two VHF guns of different acceleration gap sizes, and both demonstrated significant dark current reduction ratios of more than two orders of magnitude.

**Link**: [arxiv](http://arxiv.org/abs/2411.01754v1),  [pdf](http://arxiv.org/pdf/2411.01754v1)

**Tags**: physics.acc-ph 



### Palu: Compressing KV-Cache with Low-Rank Projection
**Authors**: Chi-Chih Chang, Wei-Cheng Lin, Chien-Yu Lin, Chong-Yan Chen, Yu-Fang Hu, Pei-Shuo Wang, Ning-Chi Huang, Luis Ceze, Mohamed S. Abdelfattah, Kai-Chiang Wu

**Updated**: 2024-11-04T02:08:55Z

**Summary**: Post-training KV-Cache compression methods typically either sample a subset of effectual tokens or quantize the data into lower numerical bit width. However, these methods cannot exploit redundancy in the hidden dimension of the KV tensors. This paper presents a hidden dimension compression approach called Palu, a KV-Cache compression framework that utilizes low-rank projection to reduce inference-time LLM memory usage. Palu decomposes the linear layers into low-rank matrices, caches compressed intermediate states, and reconstructs the full keys and values on the fly. To improve accuracy, compression rate, and efficiency, Palu further encompasses (1) a medium-grained low-rank decomposition scheme, (2) an efficient rank search algorithm, (3) low-rank-aware quantization compatibility enhancements, and (4) optimized GPU kernels with operators fusion. Extensive experiments with popular LLMs show that Palu compresses KV-Cache by 50% while maintaining strong accuracy and delivering up to 1.89x on the RoPE-based attention module. When combined with quantization, Palu's inherent quantization-friendly design yields small to negligible extra accuracy degradation while saving additional memory than quantization-only methods and achieving up to 2.91x speedup for the RoPE-based attention. Moreover, it maintains comparable or even better accuracy (up to 1.19 lower perplexity) compared to quantization-only methods. These results demonstrate Palu's superior capability to effectively address the efficiency and memory challenges of LLM inference posed by KV-Cache. Our code is publicly available at: https://github.com/shadowpa0327/Palu

**Link**: [arxiv](http://arxiv.org/abs/2407.21118v2),  [pdf](http://arxiv.org/pdf/2407.21118v2)

**Tags**: cs.AI cs.LG 



### A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache   Compression
**Authors**: Alessio Devoto, Yu Zhao, Simone Scardapane, Pasquale Minervini

**Updated**: 2024-11-03T09:42:35Z

**Summary**: The deployment of large language models (LLMs) is often hindered by the extensive memory requirements of the Key-Value (KV) cache, especially as context lengths increase. Existing approaches to reduce the KV cache size involve either fine-tuning the model to learn a compression strategy or leveraging attention scores to reduce the sequence length. We analyse the attention distributions in decoder-only Transformers-based models and observe that attention allocation patterns stay consistent across most layers. Surprisingly, we find a clear correlation between the $L_2$ and the attention scores over cached KV pairs, where a low $L_2$ of a key embedding usually leads to a high attention score during decoding. This finding indicates that the influence of a KV pair is potentially determined by the key embedding itself before being queried. Based on this observation, we compress the KV cache based on the $L_2$ of key embeddings. Our experimental results show that this simple strategy can reduce the KV cache size by 50% on language modelling and needle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing accuracy. Moreover, without relying on the attention scores, this approach remains compatible with FlashAttention, enabling broader applicability.

**Link**: [arxiv](http://arxiv.org/abs/2406.11430v4),  [pdf](http://arxiv.org/pdf/2406.11430v4)

**Tags**: cs.CL cs.AI 



### Two-Timescale Model Caching and Resource Allocation for Edge-Enabled   AI-Generated Content Services
**Authors**: Zhang Liu, Hongyang Du, Xiangwang Hou, Lianfen Huang, Seyyedali Hosseinalipour, Dusit Niyato, Khaled Ben Letaief

**Updated**: 2024-11-03T07:01:13Z

**Summary**: Generative AI (GenAI) has emerged as a transformative technology, enabling customized and personalized AI-generated content (AIGC) services. In this paper, we address challenges of edge-enabled AIGC service provisioning, which remain underexplored in the literature. These services require executing GenAI models with billions of parameters, posing significant obstacles to resource-limited wireless edge. We subsequently introduce the formulation of joint model caching and resource allocation for AIGC services to balance a trade-off between AIGC quality and latency metrics. We obtain mathematical relationships of these metrics with the computational resources required by GenAI models via experimentation. Afterward, we decompose the formulation into a model caching subproblem on a long-timescale and a resource allocation subproblem on a short-timescale. Since the variables to be solved are discrete and continuous, respectively, we leverage a double deep Q-network (DDQN) algorithm to solve the former subproblem and propose a diffusion-based deep deterministic policy gradient (D3PG) algorithm to solve the latter. The proposed D3PG algorithm makes an innovative use of diffusion models as the actor network to determine optimal resource allocation decisions. Consequently, we integrate these two learning methods within the overarching two-timescale deep reinforcement learning (T2DRL) algorithm, the performance of which is studied through comparative numerical simulations.

**Link**: [arxiv](http://arxiv.org/abs/2411.01458v1),  [pdf](http://arxiv.org/pdf/2411.01458v1)

**Tags**: cs.LG cs.AI cs.DC 



### Disaggregated Database Management Systems
**Authors**: Shahram Ghandeharizadeh, Philip A. Bernstein, Dhruba Borthakur, Haoyu Huang, Jai Menon, Sumit Puri

**Updated**: 2024-11-02T14:40:36Z

**Summary**: Modern applications demand high performance and cost efficient database management systems (DBMSs). Their workloads may be diverse, ranging from online transaction processing to analytics and decision support. The cloud infrastructure enables disaggregation of monolithic DBMSs into components that facilitate software-hardware co-design. This is realized using pools of hardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using high-speed networks. This disaggregation trend is being adopted by cloud DBMSs because hardware re-provisioning can be achieved by simply invoking software APIs. Disaggregated DBMSs separate processing from storage, enabling each to scale elastically and independently. They may disaggregate compute usage based on functionality, e.g., compute needed for writes from compute needed for queries and compute needed for compaction. They may also use disaggregated memory, e.g., for intermediate results in a shuffle or for remote caching. The DBMS monitors the characteristics of a workload and dynamically assembles its components that are most efficient and cost effective for the workload. This paper is a summary of a panel session that discussed the capability, challenges, and opportunities of these emerging DBMSs and disaggregated hardware systems.

**Link**: [arxiv](http://arxiv.org/abs/2411.01269v1),  [pdf](http://arxiv.org/pdf/2411.01269v1)

**Tags**: cs.DB 



### CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores
**Authors**: Shahram Ghandeharizadeh, Sandy Irani, Jenny Lam, Jason Yap

**Updated**: 2024-11-02T13:52:49Z

**Summary**: Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a general purpose key-value store (KVS) that manages key-value pairs computed by applications with different access patterns, key-value sizes, and varying costs for each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS) algorithm that can be implemented as efficiently as LRU. In particular, CAMP's eviction policies are as effective as those of GDS but require only a small fraction of the updates to an internal data structure in order to make those decisions. Similar to an implementation of LRU using queues, it adapts to changing workload patterns based on the history of requests for different key-value pairs. It is superior to LRU because it considers both the size and cost of key-value pairs to maximize the utility of the available memory across competing applications. We compare CAMP with both LRU and an alternative that requires human intervention to partition memory into pools and assign grouping of key-value pairs to different pools. The results demonstrate CAMP is as fast as LRU while outperforming both LRU and the pooled alternative. We also present results from an implementation of CAMP using Twitter's version of memcached.

**Link**: [arxiv](http://arxiv.org/abs/2411.01246v1),  [pdf](http://arxiv.org/pdf/2411.01246v1)

**Tags**: cs.DB cs.DS cs.PF 



### NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM   Inference
**Authors**: Xuanlin Jiang, Yang Zhou, Shiyi Cao, Ion Stoica, Minlan Yu

**Updated**: 2024-11-02T05:15:44Z

**Summary**: Online LLM inference powers many exciting applications such as intelligent chatbots and autonomous agents. Modern LLM inference engines widely rely on request batching to improve inference throughput, aiming to make it cost-efficient when running on expensive GPU accelerators. However, the limited GPU memory has largely limited the batch size achieved in practice, leaving significant GPU compute resources wasted.   We present NEO, an online LLM inference system that offloads part of attention compute and KV cache states from the GPU to the local host CPU, effectively increasing the GPU batch size and thus inference throughput. To this end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling to balance GPU and CPU loads and fully utilize their compute and memory resources. We evaluate NEO on a wide range of workloads (i.e., code generation, text summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B, 70B). NEO achieves up to 7.5$\times$, 26%, and 14% higher throughput compared to GPU-only approach on T4, A10G, and H100 GPUs, respectively, while maintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3% throughput gain on A10G GPU.

**Link**: [arxiv](http://arxiv.org/abs/2411.01142v1),  [pdf](http://arxiv.org/pdf/2411.01142v1)

**Tags**: cs.DC cs.AI cs.LG 



### XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference
**Authors**: Joo Monteiro, tienne Marcotte, Pierre-Andr Nol, Valentina Zantedeschi, David Vzquez, Nicolas Chapados, Christopher Pal, Perouz Taslakian

**Updated**: 2024-11-01T14:56:52Z

**Summary**: In-context learning (ICL) approaches typically leverage prompting to condition decoder-only language model generation on reference information. Just-in-time processing of a context is inefficient due to the quadratic cost of self-attention operations, and caching is desirable. However, caching transformer states can easily require almost as much space as the model parameters. When the right context isn't known in advance, caching ICL can be challenging. This work addresses these limitations by introducing models that, inspired by the encoder-decoder architecture, use cross-attention to condition generation on reference text without the prompt. More precisely, we leverage pre-trained decoder-only models and only train a small number of added layers. We use Question-Answering (QA) as a testbed to evaluate the ability of our models to perform conditional generation and observe that they outperform ICL, are comparable to fine-tuned prompted LLMs, and drastically reduce the space footprint relative to standard KV caching by two orders of magnitude.

**Link**: [arxiv](http://arxiv.org/abs/2404.15420v3),  [pdf](http://arxiv.org/pdf/2404.15420v3)

**Tags**: cs.CL cs.AI 



### Block Transformer: Global-to-Local Language Modeling for Fast Inference
**Authors**: Namgyu Ho, Sangmin Bae, Taehyeon Kim, Hyunjik Jo, Yireun Kim, Tal Schuster, Adam Fisch, James Thorne, Se-Young Yun

**Updated**: 2024-11-01T08:52:18Z

**Summary**: We introduce the Block Transformer which adopts hierarchical global-to-local modeling to autoregressive transformers to mitigate the inference bottlenecks associated with self-attention. Self-attention requires the key-value (KV) cache of all previous sequences to be retrieved from memory at every decoding step to retrieve context information, leading to two primary bottlenecks during batch inference. First, there is a significant delay in obtaining the first token, as the information of the entire prompt must first be processed to prefill the KV cache. Second, computation of subsequent tokens is bottlenecked by the high memory I/O demand of fetching the entire KV cache, which grows linearly with sequence length, incurring quadratic memory reads overall. We design the Block Transformer to strategically mitigate these costs, by incorporating coarsity and locality into an integrated global-to-local architecture. At the lower layers, we aggregate tokens into fixed size blocks to apply attention across the entire sequence at coarse-grained detail, to capture the global context while minimizing KV cache overhead. At upper layers, we apply attention within each block to decode individual tokens, to model fine-grained details with a lightweight local KV cache. We pretrain vanilla and Block Transformers from scratch and demonstrate that Block Transformers reach 10--20x inference throughput compared to vanilla transformers with equivalent perplexity and zero-shot task performance. Code is available at https://github.com/itsnamgyu/block-transformer.

**Link**: [arxiv](http://arxiv.org/abs/2406.02657v2),  [pdf](http://arxiv.org/pdf/2406.02657v2)

**Tags**: cs.CL cs.AI cs.LG 



### Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence
**Authors**: John Whitington

**Updated**: 2024-10-31T18:31:13Z

**Summary**: We describe a hidden surface removal algorithm for two-dimensional layered scenes built from arbitrary primitives, particularly suited to interaction and animation in rich scenes (for example, in illustration). The method makes use of a set-based raster representation to implement a front-to-back rendering model which analyses and dramatically reduces the amount of rasterization and composition required to render a scene. The method is extended to add frame-to-frame coherence analysis and caching for interactive or animated scenes. A powerful system of primitive-combiners called filters is described, which preserves the efficiencies of the algorithm in highly complicated scenes. The set representation is extended to solve the problem of correlated mattes, leading to an efficient solution for high quality antialiasing. A prototype implementation has been prepared.

**Link**: [arxiv](http://arxiv.org/abs/2411.00131v1),  [pdf](http://arxiv.org/pdf/2411.00131v1)

**Tags**: cs.GR 



### Novel Architecture for Distributed Travel Data Integration and Service   Provision Using Microservices
**Authors**: Biman Barua, M. Shamim Kaiser

**Updated**: 2024-10-31T17:41:14Z

**Summary**: This paper introduces a microservices architecture for the purpose of enhancing the flexibility and performance of an airline reservation system. The architectural design incorporates Redis cache technologies, two different messaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and PostgreSQL). It also introduces authorization techniques, including secure communication through OAuth2 and JWT which is essential with the management of high-demand travel services. According to selected indicators, the architecture provides an impressive level of data consistency at 99.5% and a latency of data propagation of less than 75 ms allowing rapid and reliable intercommunication between microservices. A system throughput of 1050 events per second was achieved so that the acceptability level was maintained even during peak time. Redis caching reduced a 92% cache hit ratio on the database thereby lowering the burden on the database and increasing the speed of response. Further improvement of the systems scalability was done through the use of Docker and Kubernetes which enabled services to be expanded horizontally to cope with the changes in demand. The error rates were very low, at 0.2% further enhancing the efficiency of the system in handling real-time data integration. This approach is suggested to meet the specific needs of the airline reservation system. It is secure, fast, scalable, all serving to improve the user experience as well as the efficiency of operations. The low latency and high data integration levels and prevaiing efficient usage of the resources demonstrates the architecture ability to offer continued support in the ever growing high demand situations.

**Link**: [arxiv](http://arxiv.org/abs/2410.24174v1),  [pdf](http://arxiv.org/pdf/2410.24174v1)

**Tags**: cs.CE cs.CL cs.DC 



### MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM   Hardware
**Authors**: Sitian Chen, Amelie Chi Zhou, Yucheng Shi, Yusen Li, Xin Yao

**Updated**: 2024-10-31T10:45:02Z

**Summary**: In numerous production environments, Approximate Nearest Neighbor Search (ANNS) plays an indispensable role, particularly when dealing with massive datasets that can contain billions of entries. The necessity for rapid response times in these applications makes the efficiency of ANNS algorithms crucial. However, traditional ANNS approaches encounter substantial challenges at the billion-scale level. CPU-based methods are hindered by the limitations of memory bandwidth, while GPU-based methods struggle with memory capacity and resource utilization efficiency. This paper introduces MemANNS, an innovative framework that utilizes UPMEM PIM architecture to address the memory bottlenecks in ANNS algorithms at scale. We concentrate on optimizing a well-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques. First, we introduce an architecture-aware strategy for data placement and query scheduling that ensures an even distribution of workload across PIM chips, thereby maximizing the use of aggregated memory bandwidth. Additionally, we have developed an efficient thread scheduling mechanism that capitalizes on PIM's multi-threading capabilities and enhances memory management to boost cache efficiency. Moreover, we have recognized that real-world datasets often feature vectors with frequently co-occurring items. To address this, we propose a novel encoding method for IVFPQ that minimizes memory accesses during query processing. Our comprehensive evaluation using actual PIM hardware and real-world datasets at the billion-scale, show that MemANNS offers a significant 4.3x increase in QPS over CPU-based Faiss, and it matches the performance of GPU-based Faiss implementations. Additionally, MemANNS improves energy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU solutions.

**Link**: [arxiv](http://arxiv.org/abs/2410.23805v1),  [pdf](http://arxiv.org/pdf/2410.23805v1)

**Tags**: cs.AR 



### ALISE: Accelerating Large Language Model Serving with Speculative   Scheduling
**Authors**: Youpeng Zhao, Jun Wang

**Updated**: 2024-10-31T00:58:11Z

**Summary**: Large Language Models (LLMs) represent a revolutionary advancement in the contemporary landscape of artificial general intelligence (AGI). As exemplified by ChatGPT, LLM-based applications necessitate minimal response latency and maximal throughput for inference serving. However, due to the unpredictability of LLM execution, the first-come-first-serve (FCFS) scheduling policy employed by current LLM serving systems suffers from head-of-line (HoL) blocking issues and long job response times.   In this paper, we propose a new efficient LLM inference serving framework, named ALISE. The key design paradigm of ALISE is to leverage a novel speculative scheduler by estimating the execution time for each job and exploiting such prior knowledge to assign appropriate job priority orders, thus minimizing potential queuing delays for heterogeneous workloads. Furthermore, to mitigate the memory overhead of the intermediate key-value (KV) cache, we employ a priority-based adaptive memory management protocol and quantization-based compression techniques. Evaluations demonstrate that in comparison to the state-of-the-art solution vLLM, ALISE improves the throughput of inference serving by up to 1.8x and 2.1x under the same latency constraint on the Alpaca and ShareGPT datasets, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2410.23537v1),  [pdf](http://arxiv.org/pdf/2410.23537v1)

**Tags**: cs.PF cs.AI 



### Superposed Decoding: Multiple Generations from a Single Autoregressive   Inference Pass
**Authors**: Ethan Shen, Alan Fan, Sarah M. Pratt, Jae Sung Park, Matthew Wallingford, Sham M. Kakade, Ari Holtzman, Ranjay Krishna, Ali Farhadi, Aditya Kusupati

**Updated**: 2024-10-30T21:22:54Z

**Summary**: Many applications today provide users with multiple auto-complete drafts as they type, including GitHub's code completion, Gmail's smart compose, and Apple's messaging auto-suggestions. Under the hood, language models support this by running an autoregressive inference pass to provide a draft. Consequently, providing $k$ drafts to the user requires running an expensive language model $k$ times. To alleviate the computation cost of running $k$ inference passes, we propose Superposed Decoding, a new decoding algorithm that generates $k$ drafts at the computation cost of one autoregressive inference pass. We achieve this by feeding a superposition of the most recent token embeddings from the $k$ drafts as input to the next decoding step of the language model. At every inference step we combine the $k$ drafts with the top-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options, using an n-gram interpolation with minimal compute overhead to filter out incoherent generations. Our experiments show that $k$ drafts from Superposed Decoding are at least as coherent and factual as Nucleus Sampling and Greedy Decoding respectively, while being at least $2.44\times$ faster for $k\ge3$. In a compute-normalized setting, user evaluations demonstrably favor text generated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can also be combined with other decoding strategies, resulting in universal coverage gains when scaling inference time compute. Code and more examples open-sourced at https://github.com/RAIVNLab/SuperposedDecoding.

**Link**: [arxiv](http://arxiv.org/abs/2405.18400v6),  [pdf](http://arxiv.org/pdf/2405.18400v6)

**Tags**: cs.CL cs.LG 



### Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive   Edge Caching
**Authors**: Farnaz Niknia, Ping Wang, Zixu Wang, Aakash Agarwal, Adib S. Rezaei

**Updated**: 2024-10-30T16:06:21Z

**Summary**: This paper tackles the growing issue of excessive data transmission in networks. With increasing traffic, backhaul links and core networks are under significant traffic, leading to the investigation of caching solutions at edge routers. Many existing studies utilize Markov Decision Processes (MDP) to tackle caching problems, often assuming decision points at fixed intervals; however, real-world environments are characterized by random request arrivals. Additionally, critical file attributes such as lifetime, size, and priority significantly impact the effectiveness of caching policies, yet existing research fails to integrate all these attributes in policy design. In this work, we model the caching problem using a Semi-Markov Decision Process (SMDP) to better capture the continuous-time nature of real-world applications, enabling caching decisions to be triggered by random file requests. We then introduce a Proximal Policy Optimization (PPO)--based caching strategy that fully considers file attributes like lifetime, size, and priority. Simulations show that our method outperforms a recent Deep Reinforcement Learning-based technique. To further advance our research, we improved the convergence rate of PPO by prioritizing transitions within the replay buffer through an attention mechanism. This mechanism evaluates the similarity between the current state and all stored transitions, assigning higher priorities to transitions that exhibit greater similarity.

**Link**: [arxiv](http://arxiv.org/abs/2402.14576v3),  [pdf](http://arxiv.org/pdf/2402.14576v3)

**Tags**: cs.NI cs.LG cs.SY eess.SY 



### BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters   for Efficient LLM Inference
**Authors**: Junqi Zhao, Zhijin Fang, Shu Li, Shaohui Yang, Shichao He

**Updated**: 2024-10-30T14:53:37Z

**Summary**: Large language models (LLMs) are essential in natural language processing but often struggle with inference speed and computational efficiency, limiting real-time deployment. The key-value (KV) cache mechanism reduces computational overhead in transformer models, but challenges in maintaining contextual understanding remain. In this paper, we propose BUZZ, a novel KV caching algorithm that leverages structured contextual information to minimize cache memory usage while enhancing inference speed. BUZZ employs a beehive-structured sparse cache, incorporating a sliding window to capture recent information and dynamically segmenting historical tokens into chunks to prioritize important tokens in local neighborhoods. We evaluate BUZZ on four real-world datasets: CNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ (1) reduces cache memory usage by $\textbf{2.5}\times$ in LLM inference while maintaining over 99% accuracy in long-text summarization, and (2) surpasses state-of-the-art performance in multi-document question answering by $\textbf{7.69%}$ under the same memory limit, where full cache methods encounter out-of-memory issues. Additionally, BUZZ achieves significant inference speedup with a $\log{n}$ time complexity. The code is available at https://github.com/JunqiZhao888/buzz-llm.

**Link**: [arxiv](http://arxiv.org/abs/2410.23079v1),  [pdf](http://arxiv.org/pdf/2410.23079v1)

**Tags**: cs.CL cs.AI 



### Training-Free Exponential Context Extension via Cascading KV Cache
**Authors**: Jeffrey Willette, Heejun Lee, Youngwan Lee, Myeongjae Jeon, Sung Ju Hwang

**Updated**: 2024-10-30T03:31:09Z

**Summary**: The transformer's context window is vital for tasks such as few-shot learning and conditional generation as it preserves previous tokens for active memory. However, as the context lengths increase, the computational costs grow quadratically, hindering the deployment of large language models (LLMs) in real-world, long sequence scenarios. Although some recent key-value caching (KV Cache) methods offer linear inference complexity, they naively manage the stored context, prematurely evicting tokens and losing valuable information. Moreover, they lack an optimized prefill/prompt stage strategy, resulting in higher latency than even quadratic attention for realistic context sizes. In response, we introduce a novel mechanism that leverages cascading sub-cache buffers to selectively retain the most relevant tokens, enabling the model to maintain longer context histories without increasing the cache size. Our approach outperforms linear caching baselines across key benchmarks, including streaming perplexity, question answering, book summarization, and passkey retrieval, where it retains better retrieval accuracy at 1M tokens after four doublings of the cache size of 65K. Additionally, our method reduces prefill stage latency by a factor of 6.8 when compared to flash attention on 1M tokens. These innovations not only enhance the computational efficiency of LLMs but also pave the way for their effective deployment in resource-constrained environments, enabling large-scale, real-time applications with significantly reduced latency.

**Link**: [arxiv](http://arxiv.org/abs/2406.17808v2),  [pdf](http://arxiv.org/pdf/2406.17808v2)

**Tags**: cs.CL cs.AI cs.LG 



### WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series   Forecasting
**Authors**: Aobo Liang, Yan Sun

**Updated**: 2024-10-30T02:36:55Z

**Summary**: In recent years, Transformer-based models (Transformers) have achieved significant success in multivariate time series forecasting (MTSF). However, previous works focus on extracting features either from the time domain or the frequency domain, which inadequately captures the trends and periodic characteristics. To address this issue, we propose a wavelet learning framework to model complex temporal dependencies of the time series data. The wavelet domain integrates both time and frequency information, allowing for the analysis of local characteristics of signals at different scales. Additionally, the Softmax self-attention mechanism used by Transformers has quadratic complexity, which leads to excessive computational costs when capturing long-term dependencies. Therefore, we propose a novel attention mechanism: Rotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary position embeddings to inject relative positional information to sequence tokens and introduces a small number of routing tokens $r$ to aggregate information from the $KV$ matrices and redistribute it to the $Q$ matrix, offering linear complexity. We further propose WaveRoRA, which leverages RoRA to capture inter-series dependencies in the wavelet domain. We conduct extensive experiments on eight real-world datasets. The results indicate that WaveRoRA outperforms existing state-of-the-art models while maintaining lower computational costs.

**Link**: [arxiv](http://arxiv.org/abs/2410.22649v1),  [pdf](http://arxiv.org/pdf/2410.22649v1)

**Tags**: cs.LG 



### VL-Cache: Sparsity and Modality-Aware KV Cache Compression for   Vision-Language Model Inference Acceleration
**Authors**: Dezhan Tu, Danylo Vashchilenko, Yuzhe Lu, Panpan Xu

**Updated**: 2024-10-29T20:04:34Z

**Summary**: Vision-Language Models (VLMs) have demonstrated impressive performance across a versatile set of tasks. A key challenge in accelerating VLMs is storing and accessing the large Key-Value (KV) cache that encodes long visual contexts, such as images or videos. While existing KV cache compression methods are effective for Large Language Models (LLMs), directly migrating them to VLMs yields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache, a novel KV cache compression recipe tailored for accelerating VLM inference. In this paper, we first investigate the unique sparsity pattern of VLM attention by distinguishing visual and text tokens in prefill and decoding phases. Based on these observations, we introduce a layer-adaptive sparsity-aware cache budget allocation method that effectively distributes the limited cache budget across different layers, further reducing KV cache size without compromising accuracy. Additionally, we develop a modality-aware token scoring policy to better evaluate the token importance. Empirical results on multiple benchmark datasets demonstrate that retaining only 10% of KV cache achieves accuracy comparable to that with full cache. In a speed benchmark, our method accelerates end-to-end latency of generating 100 tokens by up to 2.33x and speeds up decoding by up to 7.08x, while reducing the memory footprint of KV cache in GPU by 90%.

**Link**: [arxiv](http://arxiv.org/abs/2410.23317v1),  [pdf](http://arxiv.org/pdf/2410.23317v1)

**Tags**: cs.CV cs.AI cs.CL cs.DC cs.PF 



### Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs
**Authors**: Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, Jianfeng Gao

**Updated**: 2024-10-29T18:26:09Z

**Summary**: In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss. We will release our code and the compatible CUDA kernel for reproducibility.

**Link**: [arxiv](http://arxiv.org/abs/2310.01801v4),  [pdf](http://arxiv.org/pdf/2310.01801v4)

**Tags**: cs.CL 



### Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware   Neuron Management
**Authors**: Tuowei Wang, Ruwen Fan, Minxing Huang, Zixu Hao, Kun Li, Ting Cao, Youyou Lu, Yaoxue Zhang, Ju Ren

**Updated**: 2024-10-29T17:33:19Z

**Summary**: Large Language Models (LLMs) have achieved remarkable success across various domains, yet deploying them on mobile devices remains an arduous challenge due to their extensive computational and memory demands. While lightweight LLMs have been developed to fit mobile environments, they suffer from degraded model accuracy. In contrast, sparsity-based techniques minimize DRAM usage by selectively transferring only relevant neurons to DRAM while retaining the full model in external storage, such as flash. However, such approaches are critically limited by numerous I/O operations, particularly on smartphones with severe IOPS constraints.   In this paper, we propose Ripple, a novel approach that accelerates LLM inference on smartphones by optimizing neuron placement in flash memory. Ripple leverages the concept of Neuron Co-Activation, where neurons frequently activated together are linked to facilitate continuous read access and optimize data transfer efficiency. Our approach incorporates a two-stage solution: an offline stage that reorganizes neuron placement based on co-activation patterns, and an online stage that employs tailored data access and caching strategies to align well with hardware characteristics. Evaluations conducted on a variety of smartphones and LLMs demonstrate that Ripple achieves up to 5.93x improvements in I/O latency compared to the state-of-the-art. As the first solution to optimize storage placement under sparsity, Ripple explores a new optimization space at the intersection of sparsity-driven algorithm and storage-level system co-design in LLM inference.

**Link**: [arxiv](http://arxiv.org/abs/2410.19274v2),  [pdf](http://arxiv.org/pdf/2410.19274v2)

**Tags**: cs.LG cs.AI cs.OS cs.PF 



### Modeling and Monitoring of Indoor Populations using Sparse Positioning   Data (Extension)
**Authors**: Xiao Li, Huan Li, Hua Lu, Christian S. Jensen

**Updated**: 2024-10-29T16:55:23Z

**Summary**: In large venues like shopping malls and airports, knowledge on the indoor populations fuels applications such as business analytics, venue management, and safety control. In this work, we provide means of modeling populations in partitions of indoor space offline and of monitoring indoor populations continuously, by using indoor positioning data. However, the low-sampling rates of indoor positioning render the data temporally and spatially sparse, which in turn renders the offline capture of indoor populations challenging. It is even more challenging to continuously monitor indoor populations, as positioning data may be missing or not ready yet at the current moment. To address these challenges, we first enable probabilistic modeling of populations in indoor space partitions as Normal distributions. Based on that, we propose two learning-based estimators for on-the-fly prediction of population distributions. Leveraging the prediction-based schemes, we provide a unified continuous query processing framework for a type of query that enables continuous monitoring of populated partitions. The framework encompasses caching and result validity mechanisms to reduce cost and maintain monitoring effectiveness. Extensive experiments on two real data sets show that the proposed estimators are able to outperform the state-of-the-art alternatives and that the query processing framework is effective and efficient.

**Link**: [arxiv](http://arxiv.org/abs/2410.21142v2),  [pdf](http://arxiv.org/pdf/2410.21142v2)

**Tags**: cs.DB 



### ProMoE: Fast MoE-based LLM Serving using Proactive Caching
**Authors**: Xiaoniu Song, Zihang Zhong, Rong Chen

**Updated**: 2024-10-29T15:31:27Z

**Summary**: The promising applications of large language models are often constrained by the limited GPU memory capacity available on edge devices. Mixture-of-Experts (MoE) models help mitigate this issue by activating only a subset of the model's parameters during computation, allowing the unused parameters to be offloaded to host memory and reducing overall GPU memory demand. However, existing cache-based offloading solutions handle cache misses reactively and significantly impact system performance. In this paper, we propose ProMoE, a novel proactive caching system that leverages intermediate model results to predict subsequent parameter usage. By proactively fetching experts in advance, ProMoE removes the loading time from the critical path and diminishes the performance overhead of offloading. Our evaluations demonstrate that ProMoE achieves an average speedup of 2.13x and 2.84x in the prefill and decode stages respectively, compared to existing offloading solutions.

**Link**: [arxiv](http://arxiv.org/abs/2410.22134v1),  [pdf](http://arxiv.org/pdf/2410.22134v1)

**Tags**: cs.DC cs.AI 



### The Impact of Inference Acceleration Strategies on Bias of LLMs
**Authors**: Elisabeth Kirsten, Ivan Habernal, Vedant Nanda, Muhammad Bilal Zafar

**Updated**: 2024-10-29T15:19:13Z

**Summary**: Last few years have seen unprecedented advances in capabilities of Large Language Models (LLMs). These advancements promise to deeply benefit a vast array of application domains. However, due to their immense size, performing inference with LLMs is both costly and slow. Consequently, a plethora of recent work has proposed strategies to enhance inference efficiency, e.g., quantization, pruning, and caching. These acceleration strategies reduce the inference cost and latency, often by several factors, while maintaining much of the predictive performance measured via common benchmarks. In this work, we explore another critical aspect of LLM performance: demographic bias in model generations due to inference acceleration optimizations. Using a wide range of metrics, we probe bias in model outputs from a number of angles. Analysis of outputs before and after inference acceleration shows significant change in bias. Worryingly, these bias effects are complex and unpredictable. A combination of an acceleration strategy and bias type may show little bias change in one model but may lead to a large effect in another. Our results highlight a need for in-depth and case-by-case evaluation of model bias after it has been modified to accelerate inference.

**Link**: [arxiv](http://arxiv.org/abs/2410.22118v1),  [pdf](http://arxiv.org/pdf/2410.22118v1)

**Tags**: cs.CL cs.AI cs.LG 



### LoongServe: Efficiently Serving Long-Context Large Language Models with   Elastic Sequence Parallelism
**Authors**: Bingyang Wu, Shengyu Liu, Yinmin Zhong, Peng Sun, Xuanzhe Liu, Xin Jin

**Updated**: 2024-10-29T13:04:42Z

**Summary**: The context window of large language models (LLMs) is rapidly increasing, leading to a huge variance in resource usage between different requests as well as between different phases of the same request. Restricted by static parallelism strategies, existing LLM serving systems cannot efficiently utilize the underlying resources to serve variable-length requests in different phases. To address this problem, we propose a new parallelism paradigm, elastic sequence parallelism (ESP), to elastically adapt to the variance between different requests and phases. Based on ESP, we design and build LoongServe, an LLM serving system that (1) improves computation efficiency by elastically adjusting the degree of parallelism in real-time, (2) improves communication efficiency by reducing key-value cache migration overhead and overlapping partial decoding communication with computation, and (3) improves GPU memory efficiency by reducing key-value cache fragmentation across instances. Our evaluation under diverse real-world datasets shows that LoongServe improves the maximum throughput by up to 3.85$\times$ compared to the chunked prefill and 5.81$\times$ compared to the prefill-decoding disaggregation.

**Link**: [arxiv](http://arxiv.org/abs/2404.09526v2),  [pdf](http://arxiv.org/pdf/2404.09526v2)

**Tags**: cs.DC cs.LG 



### ASVD: Activation-aware Singular Value Decomposition for Compressing   Large Language Models
**Authors**: Zhihang Yuan, Yuzhang Shang, Yue Song, Qiang Wu, Yan Yan, Guangyu Sun

**Updated**: 2024-10-29T12:28:58Z

**Summary**: In this paper, we introduce a new post-training compression paradigm for Large Language Models (LLMs) to facilitate their wider adoption. We delve into LLM weight low-rank decomposition, and find that the challenges of this task stem from the distribution variance in the LLM activations and the sensitivity difference among various kinds of layers. To address these issues, we propose a training-free approach called Activation-aware Singular Value Decomposition (ASVD). Specifically, ASVD manages activation outliers by transforming the weight matrix based on the activation distribution. This transformation allows the outliers in the activation matrix to be absorbed into the transformed weight matrix, thereby enhancing decomposition accuracy. Additionally, we propose an efficient iterative calibration process to optimize layer-specific decomposition by addressing the varying sensitivity of different LLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the success of the low-rank decomposition of projection matrices in the self-attention module, we further introduce ASVD to compress the KV cache. By reducing the channel dimension of KV activations, memory requirements for KV cache can be largely reduced. ASVD can further achieve 50% KV cache reductions without performance drop in a training-free manner. Code is anonymously available in supplementary materials.

**Link**: [arxiv](http://arxiv.org/abs/2312.05821v4),  [pdf](http://arxiv.org/pdf/2312.05821v4)

**Tags**: cs.CL 



### Dynamic Content Caching with Waiting Costs via Restless Multi-Armed   Bandits
**Authors**: Ankita Koley, Chandramani Singh

**Updated**: 2024-10-29T12:03:14Z

**Summary**: We consider a system with a local cache connected to a backend server and an end user population. A set of contents are stored at the the server where they continuously get updated. The local cache keeps copies, potentially stale, of a subset of the contents. The users make content requests to the local cache which either can serve the local version if available or can fetch a fresh version or can wait for additional requests before fetching and serving a fresh version. Serving a stale version of a content incurs an age-of-version(AoV) dependent ageing cost, fetching it from the server incurs a fetching cost, and making a request wait incurs a per unit time waiting cost. We focus on the optimal actions subject to the cache capacity constraint at each decision epoch, aiming at minimizing the long term average cost. We pose the problem as a Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based policy which is known to be asymptotically optimal. We explicitly characterize the Whittle indices. We numerically evaluate the proposed policy and also compare it to a greedy policy. We show that it is close to the optimal policy and substantially outperforms the greedy policy.

**Link**: [arxiv](http://arxiv.org/abs/2410.18627v2),  [pdf](http://arxiv.org/pdf/2410.18627v2)

**Tags**: cs.NI 



### QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs
**Authors**: Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li, Pashmina Cameron, Martin Jaggi, Dan Alistarh, Torsten Hoefler, James Hensman

**Updated**: 2024-10-29T11:09:12Z

**Summary**: We introduce QuaRot, a new Quantization scheme based on Rotations, which is able to quantize LLMs end-to-end, including all weights, activations, and KV cache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the hidden state without changing the output, making quantization easier. This computational invariance is applied to the hidden state (residual) of the LLM, as well as to the activations of the feed-forward components, aspects of the attention mechanism, and to the KV cache. The result is a quantized model where all matrix multiplications are performed in 4 bits, without any channels identified for retention in higher precision. Our 4-bit quantized LLaMa2-70B model has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the zero-shot performance. We also show that QuaRot can provide lossless 6 and 8 bit LLaMa2 models without any calibration data using round-to-nearest quantization. Code is available at: https://github.com/spcl/QuaRot.

**Link**: [arxiv](http://arxiv.org/abs/2404.00456v2),  [pdf](http://arxiv.org/pdf/2404.00456v2)

**Tags**: cs.LG 



### Unleashing the Potential of the Diffusion Model in Few-shot Semantic   Segmentation
**Authors**: Muzhi Zhu, Yang Liu, Zekai Luo, Chenchen Jing, Hao Chen, Guangkai Xu, Xinlong Wang, Chunhua Shen

**Updated**: 2024-10-29T04:21:30Z

**Summary**: The Diffusion Model has not only garnered noteworthy achievements in the realm of image generation but has also demonstrated its potential as an effective pretraining method utilizing unlabeled data. Drawing from the extensive potential unveiled by the Diffusion Model in both semantic correspondence and open vocabulary segmentation, our work initiates an investigation into employing the Latent Diffusion Model for Few-shot Semantic Segmentation. Recently, inspired by the in-context learning ability of large language models, Few-shot Semantic Segmentation has evolved into In-context Segmentation tasks, morphing into a crucial element in assessing generalist segmentation models. In this context, we concentrate on Few-shot Semantic Segmentation, establishing a solid foundation for the future development of a Diffusion-based generalist model for segmentation. Our initial focus lies in understanding how to facilitate interaction between the query image and the support image, resulting in the proposal of a KV fusion method within the self-attention framework. Subsequently, we delve deeper into optimizing the infusion of information from the support mask and simultaneously re-evaluating how to provide reasonable supervision from the query mask. Based on our analysis, we establish a simple and effective framework named DiffewS, maximally retaining the original Latent Diffusion Model's generative framework and effectively utilizing the pre-training prior. Experimental results demonstrate that our method significantly outperforms the previous SOTA models in multiple settings.

**Link**: [arxiv](http://arxiv.org/abs/2410.02369v3),  [pdf](http://arxiv.org/pdf/2410.02369v3)

**Tags**: cs.CV 



### Symmetric Locality: Definition and Initial Results
**Authors**: Giordan Escalona, Dylan McKellips, Chen Ding

**Updated**: 2024-10-29T02:52:24Z

**Summary**: In this short paper, we characterize symmetric locality. In designing algorithms, compilers, and systems, data movement is a common bottleneck in high-performance computation, in which we improve cache and memory performance. We study a special type of data reuse in the form of repeated traversals, or re-traversals, which are based on the symmetric group. The cyclic and sawtooth traces are previously known results in symmetric locality, and in this work, we would like to generalize this result for any re-traversal. Then, we also provide an abstract framework for applications in compiler design and machine learning models to improve the memory performance of certain programs.

**Link**: [arxiv](http://arxiv.org/abs/2407.19291v3),  [pdf](http://arxiv.org/pdf/2407.19291v3)

**Tags**: eess.SY cs.SY 



### Not All Heads Matter: A Head-Level KV Cache Compression Method with   Integrated Retrieval and Reasoning
**Authors**: Yu Fu, Zefan Cai, Abedelkadir Asi, Wayne Xiong, Yue Dong, Wen Xiao

**Updated**: 2024-10-28T19:32:23Z

**Summary**: Key-Value (KV) caching is a common technique to enhance the computational efficiency of Large Language Models (LLMs), but its memory overhead grows rapidly with input length. Prior work has shown that not all tokens are equally important for text generation, proposing layer-level KV cache compression to selectively retain key information. Recognizing the distinct roles of attention heads in generation, we propose HeadKV, a head-level KV cache compression method, and HeadKV-R2, which leverages a novel contextual reasoning ability estimation for compression. Our approach operates at the level of individual heads, estimating their importance for contextual QA tasks that require both retrieval and reasoning capabilities. Extensive experiments across diverse benchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct, Mistral-7B-Instruct), and long-context abilities tests demonstrate that our head-level KV cache compression significantly outperforms strong baselines, particularly in low-resource settings (KV size = 64 & 128). Notably, our method retains just 1.5% of the KV cache while achieving 97% of the performance of the full KV cache on the contextual question answering benchmark.

**Link**: [arxiv](http://arxiv.org/abs/2410.19258v2),  [pdf](http://arxiv.org/pdf/2410.19258v2)

**Tags**: cs.CL cs.AI 



### ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM   Inference
**Authors**: Hanshi Sun, Li-Wen Chang, Wenlei Bao, Size Zheng, Ningxin Zheng, Xin Liu, Harry Dong, Yuejie Chi, Beidi Chen

**Updated**: 2024-10-28T19:08:12Z

**Summary**: With the widespread deployment of long-context large language models (LLMs), there has been a growing demand for efficient support of high-throughput inference. However, as the key-value (KV) cache expands with the sequence length, the increasing memory footprint and the need to access it for each token generation both result in low throughput when serving long-context LLMs. While various dynamic sparse attention methods have been proposed to speed up inference while maintaining generation quality, they either fail to sufficiently reduce GPU memory consumption or introduce significant decoding latency by offloading the KV cache to the CPU. We present ShadowKV, a high-throughput long-context LLM inference system that stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences. To minimize decoding latency, ShadowKV employs an accurate KV selection strategy that reconstructs minimal sparse KV pairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks, including RULER, LongBench, and Needle In A Haystack, and models like Llama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K, we demonstrate that it can support up to 6$\times$ larger batch sizes and boost throughput by up to 3.04$\times$ on an A100 GPU without sacrificing accuracy, even surpassing the performance achievable with infinite batch size under the assumption of infinite GPU memory. The code is available at https://github.com/bytedance/ShadowKV.

**Link**: [arxiv](http://arxiv.org/abs/2410.21465v1),  [pdf](http://arxiv.org/pdf/2410.21465v1)

**Tags**: cs.LG cs.CL 



### Online Weighted Paging with Unknown Weights
**Authors**: Orin Levy, Noam Touitou, Aviv Rosenberg

**Updated**: 2024-10-28T17:57:40Z

**Summary**: Online paging is a fundamental problem in the field of online algorithms, in which one maintains a cache of $k$ slots as requests for fetching pages arrive online. In the weighted variant of this problem, each page has its own fetching cost; a substantial line of work on this problem culminated in an (optimal) $O(\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and Naor (FOCS'07).   Existing work for weighted paging assumes that page weights are known in advance, which is not always the case in practice. For example, in multi-level caching architectures, the expected cost of fetching a memory block is a function of its probability of being in a mid-level cache rather than the main memory. This complex property cannot be predicted in advance; over time, however, one may glean information about page weights through sampling their fetching cost multiple times.   We present the first algorithm for online weighted paging that does not know page weights in advance, but rather learns from weight samples. In terms of techniques, this requires providing (integral) samples to a fractional solver, requiring a delicate interface between this solver and the randomized rounding scheme; we believe that our work can inspire online algorithms to other problems that involve cost sampling.

**Link**: [arxiv](http://arxiv.org/abs/2410.21266v1),  [pdf](http://arxiv.org/pdf/2410.21266v1)

**Tags**: cs.LG cs.DS 



### Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent   Interconnects
**Authors**: Anastasiia Ruzhanskaia, Pengcheng Xu, David Cock, Timothy Roscoe

**Updated**: 2024-10-28T16:42:11Z

**Summary**: Conventional wisdom holds that an efficient interface between an OS running on a CPU and a high-bandwidth I/O device should be based on Direct Memory Access (DMA), descriptor rings, and interrupts: DMA offloads transfers from the CPU, descriptor rings provide buffering and queuing, and interrupts facilitate asynchronous interaction between cores and device with a lightweight notification mechanism. In this paper we question this wisdom in the light of modern hardware and workloads, particularly in cloud servers. Like others before us, we argue that the assumptions that led to this model are obsolete, and in many use-cases use of Programmed I/O (PIO), where the CPU explicitly transfers data and control information to and from a device via loads and stores, actually results in a more efficient system. However, unlike others to date, we push this idea further and show, in a real implementation, the gains in average and tail latency for fine-grained communication achievable using an open cache-coherence protocol which exposes cache transitions to a smart device. We show this using three use-cases: fine-grained RPC-style invocation of functions on an accelerator, offloading of operators in a streaming dataflow engine, and a network interface targeting for serverless functions, comparing our use of coherence with both traditional DMA-style interaction and a highly-optimized implementation using PIO over PCI Express (PCIe).

**Link**: [arxiv](http://arxiv.org/abs/2409.08141v2),  [pdf](http://arxiv.org/pdf/2409.08141v2)

**Tags**: cs.AR cs.OS 



### MagicPIG: LSH Sampling for Efficient LLM Generation
**Authors**: Zhuoming Chen, Ranajoy Sadhukhan, Zihao Ye, Yang Zhou, Jianyu Zhang, Niklas Nolte, Yuandong Tian, Matthijs Douze, Leon Bottou, Zhihao Jia, Beidi Chen

**Updated**: 2024-10-28T14:44:22Z

**Summary**: Large language models (LLMs) with long context windows have gained significant attention. However, the KV cache, stored to avoid re-computation, becomes a bottleneck. Various dynamic sparse or TopK-based attention approximation methods have been proposed to leverage the common insight that attention is sparse. In this paper, we first show that TopK attention itself suffers from quality degradation in certain downstream tasks because attention is not always as sparse as expected. Rather than selecting the keys and values with the highest attention scores, sampling with theoretical guarantees can provide a better estimation for attention output. To make the sampling-based approximation practical in LLM generation, we propose MagicPIG, a heterogeneous system based on Locality Sensitive Hashing (LSH). MagicPIG significantly reduces the workload of attention computation while preserving high accuracy for diverse tasks. MagicPIG stores the LSH hash tables and runs the attention computation on the CPU, which allows it to serve longer contexts and larger batch sizes with high approximation accuracy. MagicPIG can improve decoding throughput by $1.9\sim3.9\times$ across various GPU hardware and achieve 110ms decoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a context of 96k tokens. The code is available at \url{https://github.com/Infini-AI-Lab/MagicPIG}.

**Link**: [arxiv](http://arxiv.org/abs/2410.16179v2),  [pdf](http://arxiv.org/pdf/2410.16179v2)

**Tags**: cs.CL cs.LG 



### Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost   Edge Devices
**Authors**: Hiroki Matsutani, Masaaki Kondo, Kazuki Sunaga, Radu Marculescu

**Updated**: 2024-10-28T14:35:12Z

**Summary**: This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep neural networks to address the gap between pre-trained and deployed models. In our approach, trainable LoRA (low-rank adaptation) adapters are inserted between the last layer and every other layer to enhance the network expressive power while keeping the backward computation cost low. This architecture is well-suited to cache intermediate computation results of the forward pass and then can skip the forward computation of seen samples as training epochs progress. We implemented the combination of the proposed architecture and cache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our results show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average compared to the counterpart that has the same number of trainable parameters while preserving the accuracy, while taking only a few seconds on the microcontroller board.

**Link**: [arxiv](http://arxiv.org/abs/2410.21073v1),  [pdf](http://arxiv.org/pdf/2410.21073v1)

**Tags**: cs.LG cs.AI 



### Beyond Autoregression: Fast LLMs via Self-Distillation Through Time
**Authors**: Justin Deschenaux, Caglar Gulcehre

**Updated**: 2024-10-28T13:56:30Z

**Summary**: Autoregressive (AR) Large Language Models (LLMs) have demonstrated significant success across numerous tasks. However, the AR modeling paradigm presents certain limitations; for instance, contemporary autoregressive LLMs are trained to generate one token at a time, which can result in noticeable latency. Recent advances have indicated that search and repeated sampling can enhance performance in various applications, such as theorem proving, code generation, and alignment, by utilizing greater computational resources during inference. In this study, we demonstrate that diffusion language models are capable of generating at least 32 tokens simultaneously, while exceeding the performance of AR models in text quality and on the LAMBADA natural language understanding benchmark. This outcome is achieved through a novel distillation method for discrete diffusion models, which reduces the number of inference steps by a factor of 32-64. Practically, our models, even without caching, can generate tokens at a rate that is up to 8 times faster than AR models employing KV caching, and we anticipate further improvements with the inclusion of caching. Moreover, we demonstrate the efficacy of our approach for diffusion language models with up to 860M parameters.

**Link**: [arxiv](http://arxiv.org/abs/2410.21035v1),  [pdf](http://arxiv.org/pdf/2410.21035v1)

**Tags**: cs.LG cs.CL 



### SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by   Exploiting Temporal Continuity
**Authors**: Kunyun Wang, Jieru Zhao, Shuo Yang, Wenchao Ding, Minyi Guo

**Updated**: 2024-10-28T07:13:25Z

**Summary**: Deep learning models have become pivotal in the field of video processing and is increasingly critical in practical applications such as autonomous driving and object detection. Although Vision Transformers (ViTs) have demonstrated their power, Convolutional Neural Networks (CNNs) remain a highly efficient and high-performance choice for feature extraction and encoding. However, the intensive computational demands of convolution operations hinder its broader adoption as a video encoder. Given the inherent temporal continuity in video frames, changes between consecutive frames are minimal, allowing for the skipping of redundant computations. This technique, which we term as Diff Computation, presents two primary challenges. First, Diff Computation requires to cache intermediate feature maps to ensure the correctness of non-linear computations, leading to significant memory consumption. Second, the imbalance of sparsity among layers, introduced by Diff Computation, incurs accuracy degradation. To address these issues, we propose a memory-efficient scheduling method to eliminate memory overhead and an online adjustment mechanism to minimize accuracy degradation. We integrate these techniques into our framework, SparseTem, to seamlessly support various CNN-based video encoders. SparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with minimal accuracy drop and no additional memory overhead. Extensive experimental results demonstrate that SparseTem sets a new state-of-the-art by effectively utilizing temporal continuity to accelerate CNN-based video encoders.

**Link**: [arxiv](http://arxiv.org/abs/2410.20790v1),  [pdf](http://arxiv.org/pdf/2410.20790v1)

**Tags**: cs.CV 



### Accelerating Transformer Pre-training with 2:4 Sparsity
**Authors**: Yuezhou Hu, Kang Zhao, Weiyu Huang, Jianfei Chen, Jun Zhu

**Updated**: 2024-10-27T14:40:08Z

**Summary**: Training large transformers is slow, but recent innovations on GPU architecture give us an advantage. NVIDIA Ampere GPUs can execute a fine-grained 2:4 sparse matrix multiplication twice as fast as its dense equivalent. In the light of this property, we comprehensively investigate the feasibility of accelerating feed-forward networks (FFNs) of transformers in pre-training. First, we define a ``flip rate'' to monitor the stability of a 2:4 training process. Utilizing this metric, we propose three techniques to preserve accuracy: to modify the sparse-refined straight-through estimator by applying the masked decay term on gradients, to determine a feasible decay factor in warm-up stage, and to enhance the model's quality by a dense fine-tuning procedure near the end of pre-training. Besides, we devise two techniques to practically accelerate training: to calculate transposable 2:4 masks by convolution, and to accelerate gated activation functions by reducing GPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm achieves similar convergence to dense training algorithms on several transformer pre-training tasks, while actual acceleration can be observed on different shapes of transformer block apparently. Our toolkit is available at https://github.com/huyz2023/2by4-pretrain.

**Link**: [arxiv](http://arxiv.org/abs/2404.01847v3),  [pdf](http://arxiv.org/pdf/2404.01847v3)

**Tags**: cs.LG 



### On the I/O Complexity of the CYK Algorithm and of a Family of Related DP   Algorithms
**Authors**: Lorenzo De Stefani, Vedant Gupta

**Updated**: 2024-10-27T04:31:35Z

**Summary**: Asymptotically tight lower bounds are derived for the Input/Output (I/O) complexity of a class of dynamic programming algorithms including matrix chain multiplication, optimal polygon triangulation, and the construction of optimal binary search trees. Assuming no recomputation of intermediate values, we establish an $\Omega\left(\frac{n^3}{\sqrt{M}B}\right)$ I/O lower bound, where $n$ denotes the size of the input and $M$ denotes the size of the available fast memory (cache). When recomputation is allowed, we show the same bound holds for $M < cn$, where $c$ is a positive constant. In the case where $M \ge 2n$, we show an $\Omega\left(n/B\right)$ I/O lower bound. We also discuss algorithms for which the number of executed I/O operations matches asymptotically each of the presented lower bounds, which are thus asymptotically tight.   Additionally, we refine our general method to obtain a lower bound for the I/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the grammar impacts the I/O complexity. An upper bound with asymptotically matching performance in many cases is also provided.

**Link**: [arxiv](http://arxiv.org/abs/2410.20337v1),  [pdf](http://arxiv.org/pdf/2410.20337v1)

**Tags**: cs.DS F.2.0 



### Resource-Aware Hierarchical Federated Learning in Wireless Video Caching   Networks
**Authors**: Md Ferdous Pervej, Andreas F. Molisch

**Updated**: 2024-10-26T22:19:04Z

**Summary**: Backhaul traffic congestion caused by the video traffic of a few popular files can be alleviated by storing the to-be-requested content at various levels in wireless video caching networks. Typically, content service providers (CSPs) own the content, and the users request their preferred content from the CSPs using their (wireless) internet service providers (ISPs). As these parties do not reveal their private information and business secrets, traditional techniques may not be readily used to predict the dynamic changes in users' future demands. Motivated by this, we propose a novel resource-aware hierarchical federated learning (RawHFL) solution for predicting user's future content requests. A practical data acquisition technique is used that allows the user to update its local training dataset based on its requested content. Besides, since networking and other computational resources are limited, considering that only a subset of the users participate in the model training, we derive the convergence bound of the proposed algorithm. Based on this bound, we minimize a weighted utility function for jointly configuring the controllable parameters to train the RawHFL energy efficiently under practical resource constraints. Our extensive simulation results validate the proposed algorithm's superiority, in terms of test accuracy and energy cost, over existing baselines.

**Link**: [arxiv](http://arxiv.org/abs/2402.04216v3),  [pdf](http://arxiv.org/pdf/2402.04216v3)

**Tags**: cs.NI cs.LG cs.SY eess.SY 



### AdaNeg: Adaptive Negative Proxy Guided OOD Detection with   Vision-Language Models
**Authors**: Yabin Zhang, Lei Zhang

**Updated**: 2024-10-26T11:20:02Z

**Summary**: Recent research has shown that pre-trained vision-language models are effective at identifying out-of-distribution (OOD) samples by using negative labels as guidance. However, employing consistent negative labels across different OOD datasets often results in semantic misalignments, as these text labels may not accurately reflect the actual space of OOD images. To overcome this issue, we introduce \textit{adaptive negative proxies}, which are dynamically generated during testing by exploring actual OOD images, to align more closely with the underlying OOD label space and enhance the efficacy of negative proxy guidance. Specifically, our approach utilizes a feature memory bank to selectively cache discriminative features from test images, representing the targeted OOD distribution. This facilitates the creation of proxies that can better align with specific OOD datasets. While task-adaptive proxies average features to reflect the unique characteristics of each dataset, the sample-adaptive proxies weight features based on their similarity to individual test samples, exploring detailed sample-level nuances. The final score for identifying OOD samples integrates static negative labels with our proposed adaptive proxies, effectively combining textual and visual knowledge for enhanced performance. Our method is training-free and annotation-free, and it maintains fast testing speed. Extensive experiments across various benchmarks demonstrate the effectiveness of our approach, abbreviated as AdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg significantly outperforms existing methods, with a 2.45\% increase in AUROC and a 6.48\% reduction in FPR95. Codes are available at \url{https://github.com/YBZh/OpenOOD-VLM}.

**Link**: [arxiv](http://arxiv.org/abs/2410.20149v1),  [pdf](http://arxiv.org/pdf/2410.20149v1)

**Tags**: cs.CV cs.AI cs.LG 



### Lightweight, Secure and Stateful Serverless Computing with PSL
**Authors**: Alexander Thomas, Shubham Mishra, Kaiyuan Chen, John Kubiatowicz

**Updated**: 2024-10-25T23:17:56Z

**Summary**: We present PSL, a lightweight, secure and stateful Function-as-a-Serivce (FaaS) framework for Trusted Execution Environments (TEEs). The framework provides rich programming language support on heterogeneous TEE hardware for statically compiled binaries and/or WebAssembly (WASM) bytecodes, with a familiar Key-Value Store (KVS) interface to secure, performant, network-embedded storage. It achieves near-native execution speeds by utilizing the dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave WASM runtime with Just-In-Time (JIT) compilation. PSL is designed to efficiently operate within an asynchronous environment with a distributed tamper-proof confidential storage system, assuming minority failures. The system exchanges eventually-consistent state updates across nodes while utilizing release-consistent locking mechanisms to enhance transactional capabilities. The execution of PSL is up to 3.7x faster than the state-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read workload and 89k ops/s with 50% read/write workload. We demonstrate the scalability and adaptivity of PSL through a case study of secure and distributed training of deep neural networks.

**Link**: [arxiv](http://arxiv.org/abs/2410.20004v1),  [pdf](http://arxiv.org/pdf/2410.20004v1)

**Tags**: cs.CR cs.DC 



### LoCoCo: Dropping In Convolutions for Long Context Compression
**Authors**: Ruisi Cai, Yuandong Tian, Zhangyang Wang, Beidi Chen

**Updated**: 2024-10-25T21:09:59Z

**Summary**: This paper tackles the memory hurdle of processing long context sequences in Large Language Models (LLMs), by presenting a novel approach, Dropping In Convolutions for Long Context Compression (LoCoCo). LoCoCo employs only a fixed-size Key-Value (KV) cache, and can enhance efficiency in both inference and fine-tuning stages. Diverging from prior methods that selectively drop KV pairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion technique, blending previous KV pairs with incoming tokens to minimize the loss of contextual information and ensure accurate attention modeling. This token integration is achieved through injecting one-dimensional convolutional kernels that dynamically calculate mixing weights for each KV cache slot. Designed for broad compatibility with existing LLM frameworks, LoCoCo allows for straightforward "drop-in" integration without needing architectural modifications, while incurring minimal tuning overhead. Experiments demonstrate that LoCoCo maintains consistently outstanding performance across various context lengths and can achieve a high context compression rate during both inference and fine-tuning phases. During inference, we successfully compressed up to 3482 tokens into a 128-size KV cache, while retaining comparable performance to the full sequence - an accuracy improvement of up to 0.2791 compared to baselines at the same cache size. During post-training tuning, we also effectively extended the context length from 4K to 32K using a KV cache of fixed size 512, achieving performance similar to fine-tuning with entire sequences.

**Link**: [arxiv](http://arxiv.org/abs/2406.05317v2),  [pdf](http://arxiv.org/pdf/2406.05317v2)

**Tags**: cs.LG cs.CL 



### FutureFill: Fast Generation from Convolutional Sequence Models
**Authors**: Naman Agarwal, Xinyi Chen, Evan Dogariu, Vlad Feinberg, Daniel Suo, Peter Bartlett, Elad Hazan

**Updated**: 2024-10-25T19:45:33Z

**Summary**: We address the challenge of efficient auto-regressive generation in sequence prediction models by introducing FutureFill - a method for fast generation that applies to any sequence prediction algorithm based on convolutional operators. Our approach reduces the generation time requirement from quadratic to quasilinear relative to the context length. Additionally, FutureFill requires a prefill cache sized only by the number of tokens generated, which is smaller than the cache requirements for standard convolutional and attention-based models. We validate our theoretical findings with experimental evidence demonstrating correctness and efficiency gains in a synthetic generation task.

**Link**: [arxiv](http://arxiv.org/abs/2410.03766v2),  [pdf](http://arxiv.org/pdf/2410.03766v2)

**Tags**: cs.LG cs.AI cs.CL 



### RobustKV: Defending Large Language Models against Jailbreak Attacks via   KV Eviction
**Authors**: Tanqiu Jiang, Zian Wang, Jiacheng Liang, Changjiang Li, Yuhui Wang, Ting Wang

**Updated**: 2024-10-25T19:18:22Z

**Summary**: Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful queries within jailbreak prompts. While existing defenses primarily focus on mitigating the effects of jailbreak prompts, they often prove inadequate as jailbreak prompts can take arbitrary, adaptive forms. This paper presents RobustKV, a novel defense that adopts a fundamentally different approach by selectively removing critical tokens of harmful queries from key-value (KV) caches. Intuitively, for a jailbreak prompt to be effective, its tokens must achieve sufficient `importance' (as measured by attention scores), which inevitably lowers the importance of tokens in the concealed harmful query. Thus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV diminishes the presence of the harmful query in the KV cache, thus preventing the LLM from generating malicious responses. Extensive evaluation using benchmark datasets and models demonstrates that RobustKV effectively counters state-of-the-art jailbreak attacks while maintaining the LLM's general performance on benign queries. Moreover, RobustKV creates an intriguing evasiveness dilemma for adversaries, forcing them to balance between evading RobustKV and bypassing the LLM's built-in safeguards. This trade-off contributes to RobustKV's robustness against adaptive attacks. (warning: this paper contains potentially harmful content generated by LLMs.)

**Link**: [arxiv](http://arxiv.org/abs/2410.19937v1),  [pdf](http://arxiv.org/pdf/2410.19937v1)

**Tags**: cs.CR cs.AI cs.CL 



### Fast Inference for Augmented Large Language Models
**Authors**: Rana Shahout, Cong Liang, Shiji Xin, Qianru Lao, Yong Cui, Minlan Yu, Michael Mitzenmacher

**Updated**: 2024-10-25T19:18:00Z

**Summary**: Augmented Large Language Models (LLMs) enhance the capabilities of standalone LLMs by integrating external data sources through API calls. In interactive LLM applications, efficient scheduling is crucial for maintaining low request completion times, directly impacting user engagement. However, these augmentations introduce scheduling challenges due to the need to manage limited memory for cached information (KV caches). As a result, traditional size-based scheduling algorithms, such as Shortest Job First (SJF), become less effective at minimizing completion times. Existing work focuses only on handling requests during API calls by preserving, discarding, or swapping memory without considering how to schedule requests with API calls. In this paper, we propose LAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes request completion time through a unified scheduling approach that considers the total length of requests and their handling strategies during API calls. Recognizing that LLM inference is memory-bound, our approach ranks requests based on their consumption of memory over time, which depends on both the output sizes and how a request is managed during its API calls. To implement our scheduling, LAMPS predicts the strategy that minimizes memory waste of a request during its API calls, aligning with but improving upon existing approaches. We also propose starvation prevention techniques and optimizations to mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM and evaluate its performance against baseline LLM inference systems, demonstrating improvements in end-to-end latency by 27%-85% and reductions in TTFT by 4%-96% compared to the existing augmented-LLM system, with even greater gains over vLLM.

**Link**: [arxiv](http://arxiv.org/abs/2410.18248v2),  [pdf](http://arxiv.org/pdf/2410.18248v2)

**Tags**: cs.LG cs.AI 



### KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache   Quantization
**Authors**: Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney, Yakun Sophia Shao, Kurt Keutzer, Amir Gholami

**Updated**: 2024-10-25T18:29:43Z

**Summary**: LLMs are seeing growing use for applications which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in sub-4-bit precision. Our work, KVQuant, facilitates low precision KV cache quantization by incorporating several novel methods: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better represent the distributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we isolate outliers separately for each vector to minimize skews in quantization ranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral models, we achieve < 0.1 perplexity degradation with 3-bit quantization on both Wikitext-2 and C4, outperforming existing approaches. Our method enables serving LLaMA-7B with a context length of up to 1 million on a single A100-80GB GPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for KVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline fp16 matrix-vector multiplications, for the LLaMA-7B model.

**Link**: [arxiv](http://arxiv.org/abs/2401.18079v5),  [pdf](http://arxiv.org/pdf/2401.18079v5)

**Tags**: cs.LG 



### FasterCache: Training-Free Video Diffusion Model Acceleration with High   Quality
**Authors**: Zhengyao Lv, Chenyang Si, Junhao Song, Zhenyu Yang, Yu Qiao, Ziwei Liu, Kwan-Yee K. Wong

**Updated**: 2024-10-25T07:24:38Z

**Summary**: In this paper, we present \textbf{\textit{FasterCache}}, a novel training-free strategy designed to accelerate the inference of video diffusion models with high-quality generation. By analyzing existing cache-based methods, we observe that \textit{directly reusing adjacent-step features degrades video quality due to the loss of subtle variations}. We further perform a pioneering investigation of the acceleration potential of classifier-free guidance (CFG) and reveal significant redundancy between conditional and unconditional features within the same timestep. Capitalizing on these observations, we introduce FasterCache to substantially accelerate diffusion-based video generation. Our key contributions include a dynamic feature reuse strategy that preserves both feature distinction and temporal continuity, and CFG-Cache which optimizes the reuse of conditional and unconditional outputs to further enhance inference speed without compromising video quality. We empirically evaluate FasterCache on recent video diffusion models. Experimental results show that FasterCache can significantly accelerate video generation (\eg 1.67$\times$ speedup on Vchitect-2.0) while keeping video quality comparable to the baseline, and consistently outperform existing methods in both inference speed and video quality.

**Link**: [arxiv](http://arxiv.org/abs/2410.19355v1),  [pdf](http://arxiv.org/pdf/2410.19355v1)

**Tags**: cs.CV 



### Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with   System Co-Design
**Authors**: Ruisi Cai, Yeonju Ro, Geon-Woo Kim, Peihao Wang, Babak Ehteshami Bejnordi, Aditya Akella, Zhangyang Wang

**Updated**: 2024-10-24T19:48:51Z

**Summary**: The proliferation of large language models (LLMs) has led to the adoption of Mixture-of-Experts (MoE) architectures that dynamically leverage specialized subnetworks for improved efficiency and performance. Despite their benefits, MoE models face significant challenges during inference, including inefficient memory management and suboptimal batching, due to misaligned design choices between the model architecture and the system policies. Furthermore, the conventional approach of training MoEs from scratch is increasingly prohibitive in terms of cost. In this paper, we propose a novel framework Read-ME that transforms pre-trained dense LLMs into smaller MoE models (in contrast to "upcycling" generalist MoEs), avoiding the high costs of ground-up training. Our approach employs activation sparsity to extract experts. To compose experts, we examine the widely-adopted layer-wise router design and show its redundancy, and thus we introduce the pre-gating router decoupled from the MoE backbone that facilitates system-friendly pre-computing and lookahead scheduling, enhancing expert-aware batching and caching. Our codesign therefore addresses critical gaps on both the algorithmic and system fronts, establishing a scalable and efficient alternative for LLM inference in resource-constrained settings. Read-ME outperforms other popular open-source dense models of similar scales, achieving improvements of up to 10.1% on MMLU, and improving mean end-to-end latency up to 6.1%. Codes are available at: https://github.com/VITA-Group/READ-ME.

**Link**: [arxiv](http://arxiv.org/abs/2410.19123v1),  [pdf](http://arxiv.org/pdf/2410.19123v1)

**Tags**: cs.CL cs.LG 



### KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing
**Authors**: Yifei Yang, Zouying Cao, Qiguang Chen, Libo Qin, Dongjie Yang, Hai Zhao, Zhi Chen

**Updated**: 2024-10-24T08:06:41Z

**Summary**: The development of large language models (LLMs) has significantly expanded model sizes, resulting in substantial GPU memory requirements during inference. The key and value storage of the attention map in the KV (key-value) cache accounts for more than 80\% of this memory consumption. Nowadays, most existing KV cache compression methods focus on intra-layer compression within a single Transformer layer but few works consider layer-wise compression. In this paper, we propose a plug-and-play method called \textit{KVSharer}, which shares the KV cache between layers to achieve layer-wise compression. Rather than intuitively sharing based on higher similarity, we discover a counterintuitive phenomenon: sharing dissimilar KV caches better preserves the model performance. Experiments show that \textit{KVSharer} can reduce KV cache computation by 30\%, thereby lowering memory consumption without significantly impacting model performance and it can also achieve at least 1.3 times generation acceleration. Additionally, we verify that \textit{KVSharer} is compatible with existing intra-layer KV cache compression methods, and combining both can further save memory.

**Link**: [arxiv](http://arxiv.org/abs/2410.18517v1),  [pdf](http://arxiv.org/pdf/2410.18517v1)

**Tags**: cs.LG cs.AI cs.CL 



### The Nature of Mathematical Modeling and Probabilistic Optimization   Engineering in Generative AI
**Authors**: Fulu Li

**Updated**: 2024-10-24T05:29:20Z

**Summary**: In this paper, we give an in-depth analysis on the mathematical problem formulations and the probabilistic optimization explorations for some of the key components in Transformer model [33] in the field of generative AI. We explore and discuss some potential further enhancement for current state of the art methods for some key underlying technologies of generative AI models from algorithmic and probabilistic optimization perspective. In particular, we present an optimal solution for sub-word encoding (SWE) based on similar initial settings as that of byte-pair encoding (BPE) algorithm in [9] with similar objectives as that of WordPiece approach in [28, 31] to maximize the likelihood of the training data. We also present cross entropy optimization method to optimize hyperparameters for word2vec model [17]. In addition, we propose a factored combination of rotary positional encoding (RoPE) [32] and attention with linear biases (ALiBi) [23] with a harmonic series. We also present a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a probability distribution over block distances in the matrix to decide which block is likely to participate in a given round of attention computation while maintaining the lower triangle shape of the tensor for autoregressive language models by re-shaping the tensors. Finally, we present staircase adaptive quantization (SAQ) of key-value (KV) cache for multi-query attention (MQA) based on the framework presented in [16] to have gradual quantization degradation while achieving reasonable model quality and cost savings.

**Link**: [arxiv](http://arxiv.org/abs/2410.18441v1),  [pdf](http://arxiv.org/pdf/2410.18441v1)

**Tags**: cs.LG cs.AI 



### Digital Network Twins for Next-generation Wireless: Creation,   Optimization, and Challenges
**Authors**: Yuchen Liu, Zhiyuan Peng, Zifan Zhang, Hanzhi Yu, Mingzhe Chen

**Updated**: 2024-10-23T16:25:22Z

**Summary**: Digital network twins (DNTs), by representing a physical network using a virtual model, offer significant benefits such as streamlined network development, enhanced productivity, and cost reduction for next-generation (nextG) communication infrastructure. Existing works mainly describe the deployment of DNT technologies in various service sections.The full life cycle of DNTs for telecommunication has not yet been comprehensively studied, particularly in the aspects of fine-grained creation, real-time adaptation, resource-efficient deployment, and security protection. This article presents an in-depth overview of DNTs, exploring their concrete integration into networks and communication, covering the fundamental designs, the emergent applications, and critical challenges in multiple dimensions. We also include two detailed case studies to illustrate how DNTs can be applied in real-world scenarios such as wireless traffic forecasting and edge caching. Additionally, a forward-looking vision of the research opportunities in tackling the challenges of DNTs is provided, aiming to fully maximize the benefits of DNTs in nextG networks.

**Link**: [arxiv](http://arxiv.org/abs/2410.18002v1),  [pdf](http://arxiv.org/pdf/2410.18002v1)

**Tags**: cs.NI 



### Cold Start Latency in Serverless Computing: A Systematic Review,   Taxonomy, and Future Directions
**Authors**: Muhammed Golec, Guneet Kaur Walia, Mohit Kumar, Felix Cuadrado, Sukhpal Singh Gill, Steve Uhlig

**Updated**: 2024-10-23T15:44:09Z

**Summary**: Recently, academics and the corporate sector have paid attention to serverless computing, which enables dynamic scalability and an economic model. In serverless computing, users only pay for the time they actually use resources, enabling zero scaling to optimise cost and resource utilisation. However, this approach also introduces the serverless cold start problem. Researchers have developed various solutions to address the cold start problem, yet it remains an unresolved research area. In this article, we propose a systematic literature review on clod start latency in serverless computing. Furthermore, we create a detailed taxonomy of approaches to cold start latency, which we use to investigate existing techniques for reducing the cold start time and frequency. We have classified the current studies on cold start latency into several categories such as caching and application-level optimisation-based solutions, as well as Artificial Intelligence (AI)/Machine Learning (ML)-based solutions. Moreover, we have analyzed the impact of cold start latency on quality of service, explored current cold start latency mitigation methods, datasets, and implementation platforms, and classified them into categories based on their common characteristics and features. Finally, we outline the open challenges and highlight the possible future directions.

**Link**: [arxiv](http://arxiv.org/abs/2310.08437v2),  [pdf](http://arxiv.org/pdf/2310.08437v2)

**Tags**: cs.DC 



### ExpertFlow: Optimized Expert Activation and Token Allocation for   Efficient Mixture-of-Experts Inference
**Authors**: Xin He, Shunkang Zhang, Yuxin Wang, Haiyan Yin, Zihao Zeng, Shaohuai Shi, Zhenheng Tang, Xiaowen Chu, Ivor Tsang, Ong Yew Soon

**Updated**: 2024-10-23T15:24:54Z

**Summary**: Sparse Mixture of Experts (MoE) models, while outperforming dense Large Language Models (LLMs) in terms of performance, face significant deployment challenges during inference due to their high memory demands. Existing offloading techniques, which involve swapping activated and idle experts between the GPU and CPU, often suffer from rigid expert caching mechanisms. These mechanisms fail to adapt to dynamic routing, leading to inefficient cache utilization, or incur prohibitive costs for prediction training. To tackle these inference-specific challenges, we introduce ExpertFlow, a comprehensive system specifically designed to enhance inference efficiency by accommodating flexible routing and enabling efficient expert scheduling between CPU and GPU. This reduces overhead and boosts system performance. Central to our approach is a predictive routing path-based offloading mechanism that utilizes a lightweight predictor to accurately forecast routing paths before computation begins. This proactive strategy allows for real-time error correction in expert caching, significantly increasing cache hit ratios and reducing the frequency of expert transfers, thereby minimizing I/O overhead. Additionally, we implement a dynamic token scheduling strategy that optimizes MoE inference by rearranging input tokens across different batches. This method not only reduces the number of activated experts per batch but also improves computational efficiency. Our extensive experiments demonstrate that ExpertFlow achieves up to 93.72\% GPU memory savings and enhances inference speed by 2 to 10 times compared to baseline methods, highlighting its effectiveness and utility as a robust solution for resource-constrained inference scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2410.17954v1),  [pdf](http://arxiv.org/pdf/2410.17954v1)

**Tags**: cs.AI cs.CL 



### Value Residual Learning For Alleviating Attention Concentration In   Transformers
**Authors**: Zhanchao Zhou, Tianyi Wu, Zhiyun Jiang, Zhenzhong Lan

**Updated**: 2024-10-23T14:15:07Z

**Summary**: Transformers can capture long-range dependencies using self-attention, allowing tokens to attend to all others directly. However, stacking multiple attention layers leads to attention concentration. One natural way to address this issue is to use cross-layer attention, allowing information from earlier layers to be directly accessible to later layers. However, this approach is computationally expensive. To address this problem, we propose Transformer with residual value (ResFormer) which approximates cross-layer attention through adding a residual connection from the values of the the first layer to all subsequent layers. Based on this method, one variant is the Transformer with single layer value (SVFormer), where all layers share the same value embedding from first layer, reducing the KV cache by nearly 50%. Comprehensive empirical evidence demonstrates that ResFormer mitigates attention concentration problem in deeper layers and enhances representation across most layers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as downstream tasks. SVFormer trains significantly faster than the vanilla Transformer and performs better than other methods like GQA and CLA, with performance influenced by sequence length and cumulative learning rate.

**Link**: [arxiv](http://arxiv.org/abs/2410.17897v1),  [pdf](http://arxiv.org/pdf/2410.17897v1)

**Tags**: cs.CL 



### Full Version: (De/Re)-Composition of Data-Parallel Computations via   Multi-Dimensional Homomorphisms
**Authors**: Ari Rasch

**Updated**: 2024-10-23T10:39:15Z

**Summary**: We formally introduce a systematic (de/re)-composition approach, based on the algebraic formalism of "Multi-Dimensional Homomorphisms (MDHs)". Our approach is designed as general enough to be applicable to a wide range of data-parallel computations and for various kinds of target parallel architectures. To efficiently target the deep and complex memory and core hierarchies of contemporary architectures, we exploit our introduced (de/re)-composition approach for a correct-by-construction, parametrized cache blocking and parallelization strategy. We show that our approach is powerful enough to express, in the same formalism, the (de/re)-composition strategies of different classes of state-of-the-art approaches (scheduling-based, polyhedral, etc), and we demonstrate that the parameters of our strategies enable systematically generating code that can be fully automatically optimized (auto-tuned) for the particular target architecture and characteristics of the input and output data (e.g., their sizes and memory layouts). Particularly, our experiments confirm that via auto-tuning, we achieve higher performance than state-of-the-art approaches, including hand-optimized solutions provided by vendors (such as NVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a variety of data-parallel computations, including: linear algebra routines, stencil and quantum chemistry computations, data mining algorithms, and computations that recently gained high attention due to their relevance for deep learning.

**Link**: [arxiv](http://arxiv.org/abs/2405.05118v3),  [pdf](http://arxiv.org/pdf/2405.05118v3)

**Tags**: cs.PL 



### Markov Chain of Thought for Efficient Mathematical Reasoning
**Authors**: Wen Yang, Kai Fan, Minpeng Liao

**Updated**: 2024-10-23T07:53:29Z

**Summary**: Chain of Thought (CoT) of multi-step benefits from the logical structure of the reasoning steps and task-specific actions, significantly enhancing the mathematical reasoning capabilities of large language models. As the prevalence of long CoT, the number of reasoning steps exceeds manageable token limits and leads to higher computational demands. Inspired by the fundamental logic of human cognition, ``derive, then reduce'', we conceptualize the standard multi-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we consider the mathematical reasoning task, defining each reasoning step as text accompanied by a Python code snippet. To facilitate a longer reasoning path, self-correction is enabled through interactions with the code interpreter. Our MCoT aims to compress previous reasoning steps into a simplified question, enabling efficient next-step inference without relying on a lengthy KV cache. In our experiments, we curate the \texttt{MCoTInstruct} dataset, and the empirical results indicate that MCoT not only significantly enhances efficiency but also maintains comparable accuracy. While much remains to be explored, this work paves the way for exploring the long CoT reasoning abilities of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2410.17635v1),  [pdf](http://arxiv.org/pdf/2410.17635v1)

**Tags**: cs.AI cs.CL 



### ConfusedPilot: Confused Deputy Risks in RAG-based LLMs
**Authors**: Ayush RoyChowdhury, Mulong Luo, Prateek Sahu, Sarbartha Banerjee, Mohit Tiwari

**Updated**: 2024-10-23T05:55:31Z

**Summary**: Retrieval augmented generation (RAG) is a process where a large language model (LLM) retrieves useful information from a database and then generates the responses. It is becoming popular in enterprise settings for daily business operations. For example, Copilot for Microsoft 365 has accumulated millions of businesses. However, the security implications of adopting such RAG-based systems are unclear.   In this paper, we introduce ConfusedPilot, a class of security vulnerabilities of RAG systems that confuse Copilot and cause integrity and confidentiality violations in its responses. First, we investigate a vulnerability that embeds malicious text in the modified prompt in RAG, corrupting the responses generated by the LLM. Second, we demonstrate a vulnerability that leaks secret data, which leverages the caching mechanism during retrieval. Third, we investigate how both vulnerabilities can be exploited to propagate misinformation within the enterprise and ultimately impact its operations, such as sales and manufacturing. We also discuss the root cause of these attacks by investigating the architecture of a RAG-based system. This study highlights the security vulnerabilities in today's RAG-based systems and proposes design guidelines to secure future RAG-based systems.

**Link**: [arxiv](http://arxiv.org/abs/2408.04870v5),  [pdf](http://arxiv.org/pdf/2408.04870v5)

**Tags**: cs.CR cs.AI 



### Harnessing Your DRAM and SSD for Sustainable and Accessible LLM   Inference with Mixed-Precision and Multi-level Caching
**Authors**: Jie Peng, Zhang Cao, Huaizhi Qu, Zhengyu Zhang, Chang Guo, Yanyong Zhang, Zhichao Cao, Tianlong Chen

**Updated**: 2024-10-23T01:08:59Z

**Summary**: Although Large Language Models (LLMs) have demonstrated remarkable capabilities, their massive parameter counts and associated extensive computing make LLMs' deployment the main part of carbon emission from nowadays AI applications. Compared to modern GPUs like H$100$, it would be significantly carbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as shown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for LLM servings. However, the limited High Bandwidth Memory (HBM) available on such GPU often cannot support the loading of LLMs due to the gigantic model size and intermediate activation data, making their serving challenging. For instance, a LLaMA2 model with $70$B parameters typically requires $128$GB for inference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains infeasible even considering the additional $64$GB DRAM. To address this challenge, this paper proposes a mixed-precision with a model modularization algorithm to enable LLM inference on outdated hardware with resource constraints. (The precision denotes the numerical precision like FP16, INT8, INT4) and multi-level caching (M2Cache).)   Specifically, our M2Cache first modulizes neurons in LLM and creates their importance ranking. Then, it adopts a dynamic sparse mixed-precision quantization mechanism in weight space to reduce computational demands and communication overhead at each decoding step. It collectively lowers the operational carbon emissions associated with LLM inference. Moreover, M2Cache introduces a three-level cache management system with HBM, DRAM, and SSDs that complements the dynamic sparse mixed-precision inference. To enhance communication efficiency, M2Cache maintains a neuron-level mixed-precision LRU cache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD.

**Link**: [arxiv](http://arxiv.org/abs/2410.14740v2),  [pdf](http://arxiv.org/pdf/2410.14740v2)

**Tags**: cs.LG cs.DC 



### Token-wise Influential Training Data Retrieval for Large Language Models
**Authors**: Huawei Lin, Jikai Long, Zhaozhuo Xu, Weijie Zhao

**Updated**: 2024-10-22T19:07:08Z

**Summary**: Given a Large Language Model (LLM) generation, how can we identify which training data led to this generation? In this paper, we proposed RapidIn, a scalable framework adapting to LLMs for estimating the influence of each training data. The proposed framework consists of two stages: caching and retrieval. First, we compress the gradient vectors by over 200,000x, allowing them to be cached on disk or in GPU/CPU memory. Then, given a generation, RapidIn efficiently traverses the cached gradients to estimate the influence within minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports multi-GPU parallelization to substantially accelerate caching and retrieval. Our empirical result confirms the efficiency and effectiveness of RapidIn.

**Link**: [arxiv](http://arxiv.org/abs/2405.11724v2),  [pdf](http://arxiv.org/pdf/2405.11724v2)

**Tags**: cs.CL cs.AI cs.CR cs.IR 



### 3 kV Monolithic Bidirectional GaN HEMT on Sapphire
**Authors**: Md Tahmidul Alam, Swarnav Mukhopadhyay, Md Mobinul Haque, Shubhra S. Pasayat, Chirag Gupta

**Updated**: 2024-10-21T17:23:03Z

**Summary**: More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional GaN HEMTs for the first time having potential applications in 1200V or 1700V-class novel power converters. The on resistance of the fabricated transistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was optimized by utilizing two field plates in either side of the transistor and optimizing their geometry. Shorter first field plate lengths (less than 2 micron) resulted in higher breakdown voltage and the possible reason for this was discussed. The transistors had a steep subthreshold swing of 92 mV / dec. The on/off ratio was greater than 10^5 and it was limited by the tool capacity. The fabricated 3 kV transistor was benchmarked against the state-of-the-art monolithic bidirectional GaN HEMTs in the performance matrices of breakdown voltage and on resistance, that showed crucial progress.

**Link**: [arxiv](http://arxiv.org/abs/2410.16218v1),  [pdf](http://arxiv.org/pdf/2410.16218v1)

**Tags**: physics.app-ph 



### Do Large Language Models Need a Content Delivery Network?
**Authors**: Yihua Cheng, Kuntai Du, Jiayi Yao, Junchen Jiang

**Updated**: 2024-10-21T15:59:18Z

**Summary**: As the use of large language models (LLMs) expands rapidly, so does the range of knowledge needed to supplement various LLM queries. Thus, enabling flexible and efficient injection of new knowledge in LLM inference is critical. Three high-level options exist: (i) embedding the knowledge in LLM's weights (i.e., fine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e., in-context learning), or (iii) injecting the KV caches of the new knowledge to LLM during prefill. This paper argues that, although fine-tuning and in-context learning are popular, using KV caches as the medium of knowledge could simultaneously enable more modular management of knowledge injection and more efficient LLM serving with low cost and fast response. To realize these benefits, we envision a Knowledge Delivery Network (KDN), a new system component in LLM services that dynamically optimizes the storage, transfer, and composition of KV cache across LLM engines and other compute and storage resources. We believe that, just like content delivery networks (CDNs), such as Akamai, enabled the success of the Internet ecosystem through their efficient data delivery, KDNs will be critical to the success of LLM applications through their efficient knowledge delivery. We have open-sourced a KDN prototype at https://github.com/LMCache/LMCache.

**Link**: [arxiv](http://arxiv.org/abs/2409.13761v2),  [pdf](http://arxiv.org/pdf/2409.13761v2)

**Tags**: cs.CL cs.AI 



### Formalising CXL Cache Coherence
**Authors**: Chengsong Tan, Alastair F. Donaldson, John Wickerson

**Updated**: 2024-10-21T11:29:49Z

**Summary**: We report our experience formally modelling and verifying CXL.cache, the inter-device cache coherence protocol of the Compute Express Link standard. We have used the Isabelle proof assistant to create a formal model for CXL.cache based on the prose English specification. This led to us identifying and proposing fixes to several problems we identified as unclear, ambiguous or inaccurate, some of which could lead to incoherence if left unfixed. Nearly all our issues and proposed fixes have been confirmed and tentatively accepted by the CXL consortium for adoption, save for one which is still under discussion. To validate the faithfulness of our model we performed scenario verification of essential restrictions such as "Snoop-pushes-GO", and produced a fully mechanised proof of a coherence property of the model. The considerable size of this proof, comprising tens of thousands of lemmas, prompted us to develop new proof automation tools, which we have made available for other Isabelle users working with similarly cumbersome proofs.

**Link**: [arxiv](http://arxiv.org/abs/2410.15908v1),  [pdf](http://arxiv.org/pdf/2410.15908v1)

**Tags**: cs.AR cs.PL 



### Secure Collaborative Computation Offloading and Resource Allocation in   Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels
**Authors**: Tianqing Zhou, Bobo Wang, Dong Qin, Xuefang Nie, Nan Jiang, Chunguo Li

**Updated**: 2024-10-21T07:24:53Z

**Summary**: Cache-assisted ultra-dense mobile edge computing (MEC) networks are a promising solution for meeting the increasing demands of numerous Internet-of-Things mobile devices (IMDs). To address the complex interferences caused by small base stations (SBSs) deployed densely in such networks, this paper explores the combination of orthogonal frequency division multiple access (OFDMA), non-orthogonal multiple access (NOMA), and base station (BS) clustering. Additionally, security measures are introduced to protect IMDs' tasks offloaded to BSs from potential eavesdropping and malicious attacks. As for such a network framework, a computation offloading scheme is proposed to minimize IMDs' energy consumption while considering constraints such as delay, power, computing resources, and security costs, optimizing channel selections, task execution decisions, device associations, power controls, security service assignments, and computing resource allocations. To solve the formulated problem efficiently, we develop a further improved hierarchical adaptive search (FIHAS) algorithm, giving some insights into its parallel implementation, computation complexity, and convergence. Simulation results demonstrate that the proposed algorithms can achieve lower total energy consumption and delay compared to other algorithms when strict latency and cost constraints are imposed.

**Link**: [arxiv](http://arxiv.org/abs/2410.14142v2),  [pdf](http://arxiv.org/pdf/2410.14142v2)

**Tags**: cs.IT math.IT 



### Residual vector quantization for KV cache compression in large language   model
**Authors**: Ankur Kumar

**Updated**: 2024-10-21T07:20:41Z

**Summary**: KV cache compression methods have mainly relied on scalar quantization techniques to reduce the memory requirements during decoding. In this work, we apply residual vector quantization, which has been widely used for high fidelity audio compression, to compress KV cache in large language models (LLM). We adapt the standard recipe with minimal changes to compress the output of any key or value projection matrix in a pretrained LLM: we scale the vector by its standard deviation, divide channels into groups and then quantize each group with the same residual vector quantizer. We learn the codebook using exponential moving average and there are no other learnable parameters including the input and output projections normally used in a vector quantization set up. We find that a residual depth of 8 recovers most of the performance of the unquantized model. We also find that grouping non-contiguous channels together works better than grouping contiguous channels for compressing key matrix and the method further benefits from a light weight finetuning of LLM together with the quantization. Overall, the proposed technique is competitive with existing quantization methods while being much simpler and results in 5.5x compression compared to half precision.

**Link**: [arxiv](http://arxiv.org/abs/2410.15704v1),  [pdf](http://arxiv.org/pdf/2410.15704v1)

**Tags**: cs.LG 



### AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned   Quantization
**Authors**: Yifan Tan, Haoze Wang, Chao Yan, Yangdong Deng

**Updated**: 2024-10-21T05:06:01Z

**Summary**: Model quantization has become a crucial technique to address the issues of large memory consumption and long inference times associated with LLMs. Mixed-precision quantization, which distinguishes between important and unimportant parameters, stands out among numerous quantization schemes as it achieves a balance between precision and compression rate. However, existing approaches can only identify important parameters through qualitative analysis and manual experiments without quantitatively analyzing how their importance is determined. We propose a new criterion, so-called 'precision alignment', to build a quantitative framework to holistically evaluate the importance of parameters in mixed-precision quantization. Our observations on floating point addition under various real-world scenarios suggest that two addends should have identical precision, otherwise the information in the higher-precision number will be wasted. Such an observation offers an essential principle to determine the precision of each parameter in matrix multiplication operation. As the first step towards applying the above discovery to large model inference, we develop a dynamic KV-Cache quantization technique to effectively reduce memory access latency. Different from existing quantization approaches that focus on memory saving, this work directly aims to accelerate LLM inference through quantifying floating numbers. The proposed technique attains a 25% saving of memory access and delivers up to 1.3x speedup in the computation of attention in the decoding phase of LLM, with almost no loss of precision.

**Link**: [arxiv](http://arxiv.org/abs/2409.16546v2),  [pdf](http://arxiv.org/pdf/2409.16546v2)

**Tags**: cs.LG 



### WarmSwap: Sharing Dependencies for Accelerating Cold Starts in   Serverless Functions
**Authors**: Rui Li, Devesh Tiwari, Gene Cooperman

**Updated**: 2024-10-21T02:35:08Z

**Summary**: This work presents WarmSwap, a novel provider-side cold-start optimization for serverless computing. This optimization reduces cold-start time when booting and loading dependencies at runtime inside a function container. Previous approaches to the optimization of cold starts tend to fall into two categories: optimizing the infrastructure of serverless computing to benefit all serverless functions; or function-specific tuning for individual serverless functions. In contrast, WarmSwap offers a broad middle ground, which optimizes entire categories of serverless functions. WarmSwap eliminates the need to initialize middleware or software dependencies when launching a new serverless container, by migrating a pre-initialized live dependency image to the new function instance. WarmSwap respects the provider's cache constraints, as a single pre-warmed dependency image in the cache is shared among all serverless functions requiring that software dependency image. WarmSwap has been tested on seven representative functions from FunctionBench. In those tests, WarmSwap accelerates dependency loading for serverless functions with large dependency requirements by a factor ranging from 2.2 to 3.2. Simulation experiments using Azure traces indicate that WarmSwap can save 88\% of optimization space when sharing a dependency image among ten different functions.

**Link**: [arxiv](http://arxiv.org/abs/2409.09202v2),  [pdf](http://arxiv.org/pdf/2409.09202v2)

**Tags**: cs.DC 



### Edge AI: A Taxonomy, Systematic Review and Future Directions
**Authors**: Sukhpal Singh Gill, Muhammed Golec, Jianmin Hu, Minxian Xu, Junhui Du, Huaming Wu, Guneet Kaur Walia, Subramaniam Subramanian Murugesan, Babar Ali, Mohit Kumar, Kejiang Ye, Prabal Verma, Surendra Kumar, Felix Cuadrado, Steve Uhlig

**Updated**: 2024-10-20T13:37:46Z

**Summary**: Edge Artificial Intelligence (AI) incorporates a network of interconnected systems and devices that receive, cache, process, and analyze data in close communication with the location where the data is captured with AI technology. Recent advancements in AI efficiency, the widespread use of Internet of Things (IoT) devices, and the emergence of edge computing have unlocked the enormous scope of Edge AI. Edge AI aims to optimize data processing efficiency and velocity while ensuring data confidentiality and integrity. Despite being a relatively new field of research from 2014 to the present, it has shown significant and rapid development over the last five years. This article presents a systematic literature review for Edge AI to discuss the existing research, recent advancements, and future research directions. We created a collaborative edge AI learning system for cloud and edge computing analysis, including an in-depth study of the architectures that facilitate this mechanism. The taxonomy for Edge AI facilitates the classification and configuration of Edge AI systems while examining its potential influence across many fields through compassing infrastructure, cloud computing, fog computing, services, use cases, ML and deep learning, and resource management. This study highlights the significance of Edge AI in processing real-time data at the edge of the network. Additionally, it emphasizes the research challenges encountered by Edge AI systems, including constraints on resources, vulnerabilities to security threats, and problems with scalability. Finally, this study highlights the potential future research directions that aim to address the current limitations of Edge AI by providing innovative solutions.

**Link**: [arxiv](http://arxiv.org/abs/2407.04053v2),  [pdf](http://arxiv.org/pdf/2407.04053v2)

**Tags**: cs.DC 



### LLC Intra-set Write Balancing
**Authors**: Keshav Krishna, Ayush Verma

**Updated**: 2024-10-20T09:37:07Z

**Summary**: The increasing use of Non-Volatile Memory (NVM) in computer architecture has brought about new challenges, one of which is the write endurance problem. Frequent writes to a particular cache cell in NVM can lead to degradation of the memory cell and reduce its lifespan. To solve this problem, we propose a sample-based blocking technique for the Last Level Cache (LLC). Our approach involves defining a threshold value and sampling a subset of cache sets. If the number of writes to a way in a sampled set exceeds the threshold, the way is blocked, and writes are redirected to other ways. We also maintain a history structure to record the number of writes in a set and a PC-Table to use for blocking in unsampled sets. Based on blocking on sampled sets, variance of values stored in history is used to determine whether blocking had a positive impact or not, and on this basis, value corresponding to instruction pointer is incremented or decremented. This value is later used for blocking in unsampled sets. Our results show that our approach significantly balances write traffic to the cache and improves the overall lifespan of the memory cells while having better performance to the base-line system. Our approach can also be applied to other cache hierarchies and NVM technologies to mitigate the problem of write endurance.

**Link**: [arxiv](http://arxiv.org/abs/2410.15344v1),  [pdf](http://arxiv.org/pdf/2410.15344v1)

**Tags**: cs.AR 



### EPIC: Efficient Position-Independent Context Caching for Serving Large   Language Models
**Authors**: Junhao Hu, Wenrui Huang, Haoyi Wang, Weidong Wang, Tiancheng Hu, Qin Zhang, Hao Feng, Xusheng Chen, Yizhou Shan, Tao Xie

**Updated**: 2024-10-20T08:42:29Z

**Summary**: Large Language Models (LLMs) are critical for a wide range of applications, but serving them efficiently becomes increasingly challenging as inputs become more complex. Context caching improves serving performance by exploiting inter-request dependency and reusing key-value (KV) cache across requests, thus improving time-to-first-token (TTFT). However, existing prefix-based context caching requires exact token prefix matches, limiting cache reuse in few-shot learning, multi-document QA, or retrieval-augmented generation, where prefixes may vary. In this paper, we present EPIC, an LLM serving system that introduces position-independent context caching (PIC), enabling modular KV cache reuse regardless of token chunk position (or prefix). EPIC features two key designs: AttnLink, which leverages static attention sparsity to minimize recomputation for accuracy recovery, and KVSplit, a customizable chunking method that preserves semantic coherence. Our experiments demonstrate that Epic delivers up to 8x improvements in TTFT and 7x throughput over existing systems, with negligible or no accuracy loss. By addressing the limitations of traditional caching approaches, Epic enables more scalable and efficient LLM inference.

**Link**: [arxiv](http://arxiv.org/abs/2410.15332v1),  [pdf](http://arxiv.org/pdf/2410.15332v1)

**Tags**: cs.LG cs.CL cs.DC cs.PF 



### Lossless KV Cache Compression to 2%
**Authors**: Zhen Yang, J. N. Han, Kan Wu, Ruobing Xie, An Wang, Xingwu Sun, Zhanhui Kang

**Updated**: 2024-10-20T02:17:35Z

**Summary**: Large language models have revolutionized data processing in numerous domains, with their ability to handle extended context reasoning receiving notable recognition. To speed up inference, maintaining a key-value (KV) cache memory is essential. Nonetheless, the growing demands for KV cache memory create significant hurdles for efficient implementation. This work introduces a novel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing the KV cache to less than 2% of its original size while maintaining comparable performance levels. CLLA integrates multiple aspects of KV cache compression, including attention head/dimension reduction, layer sharing, and quantization techniques, into a cohesive framework. Our extensive experiments demonstrate that CLLA achieves lossless performance on most tasks while utilizing minimal KV cache, marking a significant advancement in practical KV cache compression.

**Link**: [arxiv](http://arxiv.org/abs/2410.15252v1),  [pdf](http://arxiv.org/pdf/2410.15252v1)

**Tags**: cs.CL cs.AI 



## Keyword: LLM Inference 
 ### UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts
**Authors**: Bo Yang, Qingping Yang, Runtao Liu

**Updated**: 2024-11-11T18:59:02Z

**Summary**: The evaluation of mathematical reasoning capabilities is essential for advancing Artificial General Intelligence (AGI). While Large Language Models (LLMs) have shown impressive performance in solving mathematical problems, existing benchmarks such as GSM8K and MATH present limitations, including narrow problem definitions with specific numbers and reliance on predetermined rules that hinder accurate assessments of reasoning and adaptability. This paper introduces the UTMath Benchmark, which robustly evaluates the models through extensive unit tests. It consists of 1,053 problems across 9 mathematical domains, with over 68 test cases per problem.We propose an innovative evaluation framework inspired by unit testing in software development, focusing on both accuracy and reliability of results. Furthermore, we introduce the Reasoning-to-Coding of Thoughts (RCoT) approach, which encourages LLMs to perform explicit reasoning before generating code, leading to generating more advanced solution and improved performance. Furthermore, we are releasing not only the UTMath benchmark but also the UTMath-Train training dataset (more than 70k samples), to support the community in further exploring mathematical reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2411.07240v1),  [pdf](http://arxiv.org/pdf/2411.07240v1)

**Tags**: cs.CL cs.AI 



### Tooling or Not Tooling? The Impact of Tools on Language Agents for   Chemistry Problem Solving
**Authors**: Botao Yu, Frazier N. Baker, Ziru Chen, Garrett Herb, Boyu Gou, Daniel Adu-Ampratwum, Xia Ning, Huan Sun

**Updated**: 2024-11-11T18:46:37Z

**Summary**: To enhance large language models (LLMs) for chemistry problem solving, several LLM-based agents augmented with tools have been proposed, such as ChemCrow and Coscientist. However, their evaluations are narrow in scope, leaving a large gap in understanding the benefits of tools across diverse chemistry tasks. To bridge this gap, we develop ChemAgent, an enhanced chemistry agent over ChemCrow, and conduct a comprehensive evaluation of its performance on both specialized chemistry tasks and general chemistry questions. Surprisingly, ChemAgent does not consistently outperform its base LLMs without tools. Our error analysis with a chemistry expert suggests that: For specialized chemistry tasks, such as synthesis prediction, we should augment agents with specialized tools; however, for general chemistry questions like those in exams, agents' ability to reason correctly with chemistry knowledge matters more, and tool augmentation does not always help.

**Link**: [arxiv](http://arxiv.org/abs/2411.07228v1),  [pdf](http://arxiv.org/pdf/2411.07228v1)

**Tags**: cs.AI cs.CE 



### An information field theory approach to Bayesian state and parameter   estimation in dynamical systems
**Authors**: Kairui Hao, Ilias Bilionis

**Updated**: 2024-11-11T18:38:32Z

**Summary**: Dynamical system state estimation and parameter calibration problems are ubiquitous across science and engineering. Bayesian approaches to the problem are the gold standard as they allow for the quantification of uncertainties and enable the seamless fusion of different experimental modalities. When the dynamics are discrete and stochastic, one may employ powerful techniques such as Kalman, particle, or variational filters. Practitioners commonly apply these methods to continuous-time, deterministic dynamical systems after discretizing the dynamics and introducing fictitious transition probabilities. However, approaches based on time-discretization suffer from the curse of dimensionality since the number of random variables grows linearly with the number of time-steps. Furthermore, the introduction of fictitious transition probabilities is an unsatisfactory solution because it increases the number of model parameters and may lead to inference bias. To address these drawbacks, the objective of this paper is to develop a scalable Bayesian approach to state and parameter estimation suitable for continuous-time, deterministic dynamical systems. Our methodology builds upon information field theory. Specifically, we construct a physics-informed prior probability measure on the function space of system responses so that functions that satisfy the physics are more likely. This prior allows us to quantify model form errors. We connect the system's response to observations through a probabilistic model of the measurement process. The joint posterior over the system responses and all parameters is given by Bayes' rule. To approximate the intractable posterior, we develop a stochastic variational inference algorithm. In summary, the developed methodology offers a powerful framework for Bayesian estimation in dynamical systems.

**Link**: [arxiv](http://arxiv.org/abs/2306.02150v2),  [pdf](http://arxiv.org/pdf/2306.02150v2)

**Tags**: physics.data-an cs.LG 



### Comparing Bottom-Up and Top-Down Steering Approaches on In-Context   Learning Tasks
**Authors**: Madeline Brumley, Joe Kwon, David Krueger, Dmitrii Krasheninnikov, Usman Anwar

**Updated**: 2024-11-11T18:36:17Z

**Summary**: A key objective of interpretability research on large language models (LLMs) is to develop methods for robustly steering models toward desired behaviors. To this end, two distinct approaches to interpretability -- ``bottom-up" and ``top-down" -- have been presented, but there has been little quantitative comparison between them. We present a case study comparing the effectiveness of representative vector steering methods from each branch: function vectors (FV; arXiv:2310.15213), as a bottom-up method, and in-context vectors (ICV; arXiv:2311.06668) as a top-down method. While both aim to capture compact representations of broad in-context learning tasks, we find they are effective only on specific types of tasks: ICVs outperform FVs in behavioral shifting, whereas FVs excel in tasks requiring more precision. We discuss the implications for future evaluations of steering methods and for further research into top-down and bottom-up steering given these findings.

**Link**: [arxiv](http://arxiv.org/abs/2411.07213v1),  [pdf](http://arxiv.org/pdf/2411.07213v1)

**Tags**: cs.LG 



### General Geospatial Inference with a Population Dynamics Foundation Model
**Authors**: Mohit Agarwal, Mimi Sun, Chaitanya Kamath, Arbaaz Muslim, Prithul Sarker, Joydeep Paul, Hector Yee, Marcin Sieniek, Kim Jablonski, Yael Mayer, David Fork, Sheila de Guia, Jamie McPike, Adam Boulanger, Tomer Shekel, David Schottlander, Yao Xiao, Manjit Chakravarthy Manukonda, Yun Liu, Neslihan Bulut, Sami Abu-el-haija, Arno Eigenwillig, Parth Kothari, Bryan Perozzi, Monica Bharel, Von Nguyen, Luke Barrington, Niv Efron, Yossi Matias, Greg Corrado, Krish Eswaran, Shruthi Prabhakara, Shravya Shetty, Gautam Prasad

**Updated**: 2024-11-11T18:32:44Z

**Summary**: Supporting the health and well-being of dynamic populations around the world requires governmental agencies, organizations and researchers to understand and reason over complex relationships between human behavior and local contexts in order to identify high-risk groups and strategically allocate limited resources. Traditional approaches to these classes of problems often entail developing manually curated, task-specific features and models to represent human behavior and the natural and built environment, which can be challenging to adapt to new, or even, related tasks. To address this, we introduce a Population Dynamics Foundation Model (PDFM) that aims to capture the relationships between diverse data modalities and is applicable to a broad range of geospatial tasks. We first construct a geo-indexed dataset for postal codes and counties across the United States, capturing rich aggregated information on human behavior from maps, busyness, and aggregated search trends, and environmental factors such as weather and air quality. We then model this data and the complex relationships between locations using a graph neural network, producing embeddings that can be adapted to a wide range of downstream tasks using relatively simple models. We evaluate the effectiveness of our approach by benchmarking it on 27 downstream tasks spanning three distinct domains: health indicators, socioeconomic factors, and environmental measurements. The approach achieves state-of-the-art performance on all 27 geospatial interpolation tasks, and on 25 out of the 27 extrapolation and super-resolution tasks. We combined the PDFM with a state-of-the-art forecasting foundation model, TimesFM, to predict unemployment and poverty, achieving performance that surpasses fully supervised forecasting. The full set of embeddings and sample code are publicly available for researchers.

**Link**: [arxiv](http://arxiv.org/abs/2411.07207v1),  [pdf](http://arxiv.org/pdf/2411.07207v1)

**Tags**: cs.LG cs.CY 



### DLCR: A Generative Data Expansion Framework via Diffusion for   Clothes-Changing Person Re-ID
**Authors**: Nyle Siddiqui, Florinel Alin Croitoru, Gaurav Kumar Nayak, Radu Tudor Ionescu, Mubarak Shah

**Updated**: 2024-11-11T18:28:33Z

**Summary**: With the recent exhibited strength of generative diffusion models, an open research question is \textit{if images generated by these models can be used to learn better visual representations}. While this generative data expansion may suffice for easier visual tasks, we explore its efficacy on a more difficult discriminative task: clothes-changing person re-identification (CC-ReID). CC-ReID aims to match people appearing in non-overlapping cameras, even when they change their clothes across cameras. Not only are current CC-ReID models constrained by the limited diversity of clothing in current CC-ReID datasets, but generating additional data that retains important personal features for accurate identification is a current challenge. To address this issue we propose DLCR, a novel data expansion framework that leverages pre-trained diffusion and large language models (LLMs) to accurately generate diverse images of individuals in varied attire. We generate additional data for five benchmark CC-ReID datasets (PRCC, CCVID, LaST, VC-Clothes, and LTCC) and \textbf{increase their clothing diversity by \boldmath{$10$}x, totaling over \boldmath{$2.1$}M images generated}. DLCR employs diffusion-based text-guided inpainting, conditioned on clothing prompts constructed using LLMs, to generate synthetic data that only modifies a subject's clothes while preserving their personally identifiable features. With this massive increase in data, we introduce two novel strategies - progressive learning and test-time prediction refinement - that respectively reduce training time and further boosts CC-ReID performance. On the PRCC dataset, we obtain a large top-1 accuracy improvement of $11.3\%$ by training CAL, a previous state of the art (SOTA) method, with DLCR-generated data. We publicly release our code and generated data for each dataset here: \url{https://github.com/CroitoruAlin/dlcr}.

**Link**: [arxiv](http://arxiv.org/abs/2411.07205v1),  [pdf](http://arxiv.org/pdf/2411.07205v1)

**Tags**: cs.CV 



### LLM-based Continuous Intrusion Detection Framework for Next-Gen Networks
**Authors**: Frederic Adjewa, Moez Esseghir, Leila Merghem-Boulahia

**Updated**: 2024-11-11T18:19:22Z

**Summary**: In this paper, we present an adaptive framework designed for the continuous detection, identification and classification of emerging attacks in network traffic. The framework employs a transformer encoder architecture, which captures hidden patterns in a bidirectional manner to differentiate between malicious and legitimate traffic. Initially, the framework focuses on the accurate detection of malicious activities, achieving a perfect recall of 100\% in distinguishing between attack and benign traffic. Subsequently, the system incrementally identifies unknown attack types by leveraging a Gaussian Mixture Model (GMM) to cluster features derived from high-dimensional BERT embeddings. This approach allows the framework to dynamically adjust its identification capabilities as new attack clusters are discovered, maintaining high detection accuracy. Even after integrating additional unknown attack clusters, the framework continues to perform at a high level, achieving 95.6\% in both classification accuracy and recall.The results demonstrate the effectiveness of the proposed framework in adapting to evolving threats while maintaining high accuracy in both detection and identification tasks. Our ultimate goal is to develop a scalable, real-time intrusion detection system that can continuously evolve with the ever-changing network threat landscape.

**Link**: [arxiv](http://arxiv.org/abs/2411.03354v2),  [pdf](http://arxiv.org/pdf/2411.03354v2)

**Tags**: cs.CR cs.NI 



### Data-Driven Predictive Control of Nonholonomic Robots Based on a   Bilinear Koopman Realization: Data Does Not Replace Geometry
**Authors**: Mario Rosenfelder, Lea Bold, Hannes Eschmann, Peter Eberhard, Karl Worthmann, Henrik Ebel

**Updated**: 2024-11-11T18:08:17Z

**Summary**: Advances in machine learning and the growing trend towards effortless data generation in real-world systems has led to an increasing interest for data-inferred models and data-based control in robotics. It seems appealing to govern robots solely based on data, bypassing the traditional, more elaborate pipeline of system modeling through first-principles and subsequent controller design. One promising data-driven approach is the Extended Dynamic Mode Decomposition (EDMD) for control-affine systems, a system class which contains many vehicles and machines of immense practical importance including, e.g., typical wheeled mobile robots. EDMD can be highly data-efficient, computationally inexpensive, can deal with nonlinear dynamics as prevalent in robotics and mechanics, and has a sound theoretical foundation rooted in Koopman theory. On this background, this present paper examines how EDMD models can be integrated into predictive controllers for nonholonomic mobile robots. In addition to the conventional kinematic mobile robot, we also cover the complete data-driven control pipeline - from data acquisition to control design - when the robot is not treated in terms of first-order kinematics but in a second-order manner, allowing to account for actuator dynamics. Using only real-world measurement data, it is shown in both simulations and hardware experiments that the surrogate models enable high-precision predictive controllers in the studied cases. However, the findings raise significant concerns about purely data-centric approaches that overlook the underlying geometry of nonholonomic systems, showing that, for nonholonomic systems, some geometric insight seems necessary and cannot be easily compensated for with large amounts of data.

**Link**: [arxiv](http://arxiv.org/abs/2411.07192v1),  [pdf](http://arxiv.org/pdf/2411.07192v1)

**Tags**: eess.SY cs.LG cs.RO cs.SY 



### The Super Weight in Large Language Models
**Authors**: Mengxia Yu, De Wang, Qi Shan, Colorado Reed, Alvin Wan

**Updated**: 2024-11-11T18:05:48Z

**Summary**: Recent works have shown a surprising result: a small fraction of Large Language Model (LLM) parameter outliers are disproportionately important to the quality of the model. LLMs contain billions of parameters, so these small fractions, such as 0.01%, translate to hundreds of thousands of parameters. In this work, we present an even more surprising finding: Pruning as few as a single parameter can destroy an LLM's ability to generate text -- increasing perplexity by 3 orders of magnitude and reducing zero-shot accuracy to guessing. We propose a data-free method for identifying such parameters, termed super weights, using a single forward pass through the model. We additionally find that these super weights induce correspondingly rare and large activation outliers, termed super activations. When preserved with high precision, super activations can improve simple round-to-nearest quantization to become competitive with state-of-the-art methods. For weight quantization, we similarly find that by preserving the super weight and clipping other weight outliers, round-to-nearest quantization can scale to much larger block sizes than previously considered. To facilitate further research into super weights, we provide an index of super weight coordinates for common, openly available LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2411.07191v1),  [pdf](http://arxiv.org/pdf/2411.07191v1)

**Tags**: cs.CL cs.AI 



### NatureLM-audio: an Audio-Language Foundation Model for Bioacoustics
**Authors**: David Robinson, Marius Miron, Masato Hagiwara, Olivier Pietquin

**Updated**: 2024-11-11T18:01:45Z

**Summary**: Large language models (LLMs) prompted with text and audio represent the state of the art in various auditory tasks, including speech, music, and general audio, showing emergent abilities on unseen tasks. However, these capabilities have yet to be fully demonstrated in bioacoustics tasks, such as detecting animal vocalizations in large recordings, classifying rare and endangered species, and labeling context and behavior - tasks that are crucial for conservation, biodiversity monitoring, and the study of animal behavior. In this work, we present NatureLM-audio, the first audio-language foundation model specifically designed for bioacoustics. Our carefully curated training dataset comprises text-audio pairs spanning a diverse range of bioacoustics, speech, and music data, designed to address the challenges posed by limited annotated datasets in the field. We demonstrate successful transfer of learned representations from music and speech to bioacoustics, and our model shows promising generalization to unseen taxa and tasks. Importantly, we test NatureLM-audio on a novel benchmark (BEANS-Zero) and it sets the new state of the art (SotA) on several bioacoustics tasks, including zero-shot classification of unseen species. To advance bioacoustics research, we also open-source the code for generating training and benchmark data, as well as for training the model.

**Link**: [arxiv](http://arxiv.org/abs/2411.07186v1),  [pdf](http://arxiv.org/pdf/2411.07186v1)

**Tags**: cs.SD cs.AI cs.LG eess.AS 



### Gradual Fine-Tuning with Graph Routing for Multi-Source Unsupervised   Domain Adaptation
**Authors**: Yao Ma, Samuel Louvan, Zhunxuan Wang

**Updated**: 2024-11-11T17:59:21Z

**Summary**: Multi-source unsupervised domain adaptation aims to leverage labeled data from multiple source domains for training a machine learning model to generalize well on a target domain without labels. Source domain selection plays a crucial role in determining the model's performance. It relies on the similarities amongst source and target domains. Nonetheless, existing work for source domain selection often involves heavyweight computational procedures, especially when dealing with numerous source domains and the need to identify the best ones from them. In this paper, we introduce a framework for gradual fine tuning (GFT) of machine learning models on multiple source domains. We represent multiple source domains as an undirected weighted graph. We then give a new generalization error bound for GFT along any path within the graph, which is used to determine the optimal path corresponding to the optimal training order. With this formulation, we introduce three lightweight graph-routing strategies which tend to minimize the error bound. Our best strategy improves $2.3\%$ of accuracy over the state-of-the-art on Natural Language Inference (NLI) task and achieves competitive performance on Sentiment Analysis (SA) task, especially a $3.9\%$ improvement on a more diverse subset of data we use for SA.

**Link**: [arxiv](http://arxiv.org/abs/2411.07185v1),  [pdf](http://arxiv.org/pdf/2411.07185v1)

**Tags**: cs.LG cs.AI 



### Counterfactual Generation from Language Models
**Authors**: Shauli Ravfogel, Anej Svete, Vsteinn Snbjarnarson, Ryan Cotterell

**Updated**: 2024-11-11T17:57:30Z

**Summary**: Understanding and manipulating the causal generation mechanisms in language models is essential for controlling their behavior. Previous work has primarily relied on techniques such as representation surgery -- e.g., model ablations or manipulation of linear subspaces tied to specific concepts -- to intervene on these models. To understand the impact of interventions precisely, it is useful to examine counterfactuals -- e.g., how a given sentence would have appeared had it been generated by the model following a specific intervention. We highlight that counterfactual reasoning is conceptually distinct from interventions, as articulated in Pearl's causal hierarchy. Based on this observation, we propose a framework for generating true string counterfactuals by reformulating language models as Generalized Structural-equation. Models using the Gumbel-max trick. This allows us to model the joint distribution over original strings and their counterfactuals resulting from the same instantiation of the sampling noise. We develop an algorithm based on hindsight Gumbel sampling that allows us to infer the latent noise variables and generate counterfactuals of observed strings. Our experiments demonstrate that the approach produces meaningful counterfactuals while at the same time showing that commonly used intervention techniques have considerable undesired side effects.

**Link**: [arxiv](http://arxiv.org/abs/2411.07180v1),  [pdf](http://arxiv.org/pdf/2411.07180v1)

**Tags**: cs.CL cs.AI cs.LG 



### Understanding Long Videos with Multimodal Language Models
**Authors**: Kanchana Ranasinghe, Xiang Li, Kumara Kahatapitiya, Michael S. Ryoo

**Updated**: 2024-11-11T17:56:29Z

**Summary**: Large Language Models (LLMs) have allowed recent LLM-based approaches to achieve excellent performance on long-video understanding benchmarks. We investigate how extensive world knowledge and strong reasoning skills of underlying LLMs influence this strong performance. Surprisingly, we discover that LLM-based approaches can yield surprisingly good accuracy on long-video tasks with limited video information, sometimes even with no video specific information. Building on this, we exploring injecting video-specific information into an LLM-based framework. We utilize off-the-shelf vision tools to extract three object-centric information modalities from videos and then leverage natural language as a medium for fusing this information. Our resulting Multimodal Video Understanding (MVU) framework demonstrates state-of-the-art performance across multiple video understanding benchmarks. Strong performance also on robotics domain tasks establish its strong generality. Our code will be released publicly.

**Link**: [arxiv](http://arxiv.org/abs/2403.16998v2),  [pdf](http://arxiv.org/pdf/2403.16998v2)

**Tags**: cs.CV 



### Continual Memorization of Factoids in Large Language Models
**Authors**: Howard Chen, Jiayi Geng, Adithya Bhaskar, Dan Friedman, Danqi Chen

**Updated**: 2024-11-11T17:56:15Z

**Summary**: Large language models can absorb a massive amount of knowledge through pretraining, but pretraining is inefficient for acquiring long-tailed or specialized facts. Therefore, fine-tuning on specialized or new knowledge that reflects changes in the world has become popular, though it risks disrupting the model's original capabilities. We study this fragility in the context of continual memorization, where the model is trained on a small set of long-tail factoids (factual associations) and must retain these factoids after multiple stages of subsequent training on other datasets. Through extensive experiments, we show that LLMs suffer from forgetting across a wide range of subsequent tasks, and simple replay techniques do not fully prevent forgetting, especially when the factoid datasets are trained in the later stages. We posit that there are two ways to alleviate forgetting: 1) protect the memorization process as the model learns the factoids, or 2) reduce interference from training in later stages. With this insight, we develop an effective mitigation strategy: REMIX (Random and Generic Data Mixing). REMIX prevents forgetting by mixing generic data sampled from pretraining corpora or even randomly generated word sequences during each stage, despite being unrelated to the memorized factoids in the first stage. REMIX can recover performance from severe forgetting, often outperforming replay-based methods that have access to the factoids from the first stage. We then analyze how REMIX alters the learning process and find that successful forgetting prevention is associated with a pattern: the model stores factoids in earlier layers than usual and diversifies the set of layers that store these factoids. The efficacy of REMIX invites further investigation into the underlying dynamics of memorization and forgetting, opening exciting possibilities for future research.

**Link**: [arxiv](http://arxiv.org/abs/2411.07175v1),  [pdf](http://arxiv.org/pdf/2411.07175v1)

**Tags**: cs.CL 



### Enhancing Predictive Maintenance in Mining Mobile Machinery through a   TinyML-enabled Hierarchical Inference Network
**Authors**: Ral de la Fuente, Luciano Radrigan, Anibal S Morales

**Updated**: 2024-11-11T17:48:04Z

**Summary**: Mining machinery operating in variable environments faces high wear and unpredictable stress, challenging Predictive Maintenance (PdM). This paper introduces the Edge Sensor Network for Predictive Maintenance (ESN-PdM), a hierarchical inference framework across edge devices, gateways, and cloud services for real-time condition monitoring. The system dynamically adjusts inference locations--on-device, on-gateway, or on-cloud--based on trade-offs among accuracy, latency, and battery life, leveraging Tiny Machine Learning (TinyML) techniques for model optimization on resource-constrained devices. Performance evaluations showed that on-sensor and on-gateway inference modes achieved over 90\% classification accuracy, while cloud-based inference reached 99\%. On-sensor inference reduced power consumption by approximately 44\%, enabling up to 104 hours of operation. Latency was lowest for on-device inference (3.33 ms), increasing when offloading to the gateway (146.67 ms) or cloud (641.71 ms). The ESN-PdM framework provides a scalable, adaptive solution for reliable anomaly detection and PdM, crucial for maintaining machinery uptime in remote environments. By balancing accuracy, latency, and energy consumption, this approach advances PdM frameworks for industrial applications.

**Link**: [arxiv](http://arxiv.org/abs/2411.07168v1),  [pdf](http://arxiv.org/pdf/2411.07168v1)

**Tags**: cs.LG cs.DC cs.MA cs.NI eess.SP 



### Deep Compression Autoencoder for Efficient High-Resolution Diffusion   Models
**Authors**: Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, Song Han

**Updated**: 2024-11-11T17:42:37Z

**Summary**: We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder models for accelerating high-resolution diffusion models. Existing autoencoder models have demonstrated impressive results at a moderate spatial compression ratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for high spatial compression ratios (e.g., 64x). We address this challenge by introducing two key techniques: (1) Residual Autoencoding, where we design our models to learn residuals based on the space-to-channel transformed features to alleviate the optimization difficulty of high spatial-compression autoencoders; (2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases training strategy for mitigating the generalization penalty of high spatial-compression autoencoders. With these designs, we improve the autoencoder's spatial compression ratio up to 128 while maintaining the reconstruction quality. Applying our DC-AE to latent diffusion models, we achieve significant speedup without accuracy drop. For example, on ImageNet 512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup on H100 GPU for UViT-H while achieving a better FID, compared with the widely used SD-VAE-f8 autoencoder. Our code is available at https://github.com/mit-han-lab/efficientvit.

**Link**: [arxiv](http://arxiv.org/abs/2410.10733v2),  [pdf](http://arxiv.org/pdf/2410.10733v2)

**Tags**: cs.CV cs.AI 



### A Domain-Agnostic Neurosymbolic Approach for Big Social Data Analysis:   Evaluating Mental Health Sentiment on Social Media during COVID-19
**Authors**: Vedant Khandelwal, Manas Gaur, Ugur Kursuncu, Valerie Shalin, Amit Sheth

**Updated**: 2024-11-11T17:41:54Z

**Summary**: Monitoring public sentiment via social media is potentially helpful during health crises such as the COVID-19 pandemic. However, traditional frequency-based, data-driven neural network-based approaches can miss newly relevant content due to the evolving nature of language in a dynamically evolving environment. Human-curated symbolic knowledge sources, such as lexicons for standard language and slang terms, can potentially elevate social media signals in evolving language. We introduce a neurosymbolic method that integrates neural networks with symbolic knowledge sources, enhancing the detection and interpretation of mental health-related tweets relevant to COVID-19. Our method was evaluated using a corpus of large datasets (approximately 12 billion tweets, 2.5 million subreddit data, and 700k news articles) and multiple knowledge graphs. This method dynamically adapts to evolving language, outperforming purely data-driven models with an F1 score exceeding 92\%. This approach also showed faster adaptation to new data and lower computational demands than fine-tuning pre-trained large language models (LLMs). This study demonstrates the benefit of neurosymbolic methods in interpreting text in a dynamic environment for tasks such as health surveillance.

**Link**: [arxiv](http://arxiv.org/abs/2411.07163v1),  [pdf](http://arxiv.org/pdf/2411.07163v1)

**Tags**: cs.AI I.2.4; I.2.6; I.2.7; I.2.0 



### CDR: Customizable Density Ratios of Strong-over-weak LLMs for Preference   Annotation
**Authors**: Guangxuan Xu, Kai Xu, Shivchander Sudalairaj, Hao Wang, Akash Srivastava

**Updated**: 2024-11-11T17:34:00Z

**Summary**: Preference tuning of large language models (LLMs) relies on high-quality human preference data, which is often expensive and time-consuming to gather. While existing methods can use trained reward models or proprietary model as judges for preference annotation, they have notable drawbacks: training reward models remain dependent on initial human data, and using proprietary model imposes license restrictions that inhibits commercial usage. In this paper, we introduce customized density ratio (CDR), a training-free and highly effective method that leverages off-the-shelf LLMs for preference data annotation. Our approach uses the log-density ratio between a better-aligned LLM and a less aligned LLM as a reward signal. We explores 221 different LLMs pairs and empirically demonstrate that increasing the performance gap between paired LLMs correlates with better reward generalization. Furthermore, we show that tailoring the density ratio reward function with specific criteria and preference exemplars enhances performance across domains and within target areas.   In our experiment using density ratio from a pair of Mistral-7B models, CDR achieves a RewardBench score of 82.6, outperforming the best trained reward functions from same model class and demonstrating competitive performance against SoTA models in Safety (91.0) and Reasoning (88.0) domains. We use CDR to annotate an on-policy preference dataset with which we preference tune Llama-3-8B-Instruct with SimPO. Using reward signals from two relatively weak models, our approach pushes Llama-3-8B to achieve a 37.4% (+15.1%) win rate on ArenaHard and a 40.7% (+17.8%) win rate on Length-Controlled AlpacaEval 2.0, along with a score of 8.0 on MT-Bench.

**Link**: [arxiv](http://arxiv.org/abs/2411.02481v2),  [pdf](http://arxiv.org/pdf/2411.02481v2)

**Tags**: cs.CL cs.AI 



### Conditional simulation via entropic optimal transport: Toward   non-parametric estimation of conditional Brenier maps
**Authors**: Ricardo Baptista, Aram-Alexandre Pooladian, Michael Brennan, Youssef Marzouk, Jonathan Niles-Weed

**Updated**: 2024-11-11T17:32:47Z

**Summary**: Conditional simulation is a fundamental task in statistical modeling: Generate samples from the conditionals given finitely many data points from a joint distribution. One promising approach is to construct conditional Brenier maps, where the components of the map pushforward a reference distribution to conditionals of the target. While many estimators exist, few, if any, come with statistical or algorithmic guarantees. To this end, we propose a non-parametric estimator for conditional Brenier maps based on the computational scalability of \emph{entropic} optimal transport. Our estimator leverages a result of Carlier et al. (2010), which shows that optimal transport maps under a rescaled quadratic cost asymptotically converge to conditional Brenier maps; our estimator is precisely the entropic analogues of these converging maps. We provide heuristic justifications for choosing the scaling parameter in the cost as a function of the number of samples by fully characterizing the Gaussian setting. We conclude by comparing the performance of the estimator to other machine learning and non-parametric approaches on benchmark datasets and Bayesian inference problems.

**Link**: [arxiv](http://arxiv.org/abs/2411.07154v1),  [pdf](http://arxiv.org/pdf/2411.07154v1)

**Tags**: stat.ML cs.LG math.OC 



### Using Large Language Models for Hyperparameter Optimization
**Authors**: Michael R. Zhang, Nishkrit Desai, Juhan Bae, Jonathan Lorraine, Jimmy Ba

**Updated**: 2024-11-11T17:30:55Z

**Summary**: This paper explores the use of foundational large language models (LLMs) in hyperparameter optimization (HPO). Hyperparameters are critical in determining the effectiveness of machine learning models, yet their optimization often relies on manual approaches in limited-budget settings. By prompting LLMs with dataset and model descriptions, we develop a methodology where LLMs suggest hyperparameter configurations, which are iteratively refined based on model performance. Our empirical evaluations on standard benchmarks reveal that within constrained search budgets, LLMs can match or outperform traditional HPO methods like Bayesian optimization across different models on standard benchmarks. Furthermore, we propose to treat the code specifying our model as a hyperparameter, which the LLM outputs and affords greater flexibility than existing HPO approaches.

**Link**: [arxiv](http://arxiv.org/abs/2312.04528v2),  [pdf](http://arxiv.org/pdf/2312.04528v2)

**Tags**: cs.LG cs.AI 



### Transformers for Charged Particle Track Reconstruction in High Energy   Physics
**Authors**: Samuel Van Stroud, Philippa Duckett, Max Hart, Nikita Pond, Sbastien Rettie, Gabriel Facini, Tim Scanlon

**Updated**: 2024-11-11T17:19:20Z

**Summary**: Reconstructing charged particle tracks is a fundamental task in modern collider experiments. The unprecedented particle multiplicities expected at the High-Luminosity Large Hadron Collider (HL-LHC) pose significant challenges for track reconstruction, where traditional algorithms become computationally infeasible. To address this challenge, we present a novel learned approach to track reconstruction that adapts recent advances in computer vision and object detection. Our architecture combines a Transformer hit filtering network with a MaskFormer reconstruction model that jointly optimises hit assignments and the estimation of the charged particles' properties. Evaluated on the TrackML dataset, our best performing model achieves state-of-the-art tracking performance with 97% efficiency for a fake rate of 0.6%, and inference times of 100ms. Our tunable approach enables specialisation for specific applications like triggering systems, while its underlying principles can be extended to other reconstruction challenges in high energy physics. This work demonstrates the potential of modern deep learning architectures to address emerging computational challenges in particle physics while maintaining the precision required for groundbreaking physics analysis.

**Link**: [arxiv](http://arxiv.org/abs/2411.07149v1),  [pdf](http://arxiv.org/pdf/2411.07149v1)

**Tags**: hep-ex 



### Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language   Models
**Authors**: Yancheng He, Shilong Li, Jiaheng Liu, Yingshui Tan, Hui Huang, Weixun Wang, Xingyuan Bu, Hangyu Guo, Chengwei Hu, Boren Zheng, Xuepeng Liu, Dekai Sun, Wenbo Su, Bo Zheng

**Updated**: 2024-11-11T17:10:56Z

**Summary**: New LLM evaluation benchmarks are important to align with the rapid development of Large Language Models (LLMs). In this work, we present Chinese SimpleQA, the first comprehensive Chinese benchmark to evaluate the factuality ability of language models to answer short questions, and Chinese SimpleQA mainly has five properties (i.e., Chinese, Diverse, High-quality, Static, Easy-to-evaluate). Specifically, first, we focus on the Chinese language over 6 major topics with 99 diverse subtopics. Second, we conduct a comprehensive quality control process to achieve high-quality questions and answers, where the reference answers are static and cannot be changed over time. Third, following SimpleQA, the questions and answers are very short, and the grading process is easy-to-evaluate based on OpenAI API. Based on Chinese SimpleQA, we perform a comprehensive evaluation on the factuality abilities of existing LLMs. Finally, we hope that Chinese SimpleQA could guide the developers to better understand the Chinese factuality abilities of their models and facilitate the growth of foundation models.

**Link**: [arxiv](http://arxiv.org/abs/2411.07140v1),  [pdf](http://arxiv.org/pdf/2411.07140v1)

**Tags**: cs.CL 



### Stronger Models are NOT Stronger Teachers for Instruction Tuning
**Authors**: Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Radha Poovendran

**Updated**: 2024-11-11T17:06:48Z

**Summary**: Instruction tuning has been widely adopted to ensure large language models (LLMs) follow user instructions effectively. The resulting instruction-following capabilities of LLMs heavily rely on the instruction datasets used for tuning. Recently, synthetic instruction datasets have emerged as an economically viable solution to provide LLMs diverse and high-quality instructions. However, existing approaches typically assume that larger or stronger models are stronger teachers for instruction tuning, and hence simply adopt these models as response generators to the synthetic instructions. In this paper, we challenge this commonly-adopted assumption. Our extensive experiments across five base models and twenty response generators reveal that larger and stronger models are not necessarily stronger teachers of smaller models. We refer to this phenomenon as the Larger Models' Paradox. We observe that existing metrics cannot precisely predict the effectiveness of response generators since they ignore the compatibility between teachers and base models being fine-tuned. We thus develop a novel metric, named as Compatibility-Adjusted Reward (CAR) to measure the effectiveness of response generators. Our experiments across five base models demonstrate that CAR outperforms almost all baselines.

**Link**: [arxiv](http://arxiv.org/abs/2411.07133v1),  [pdf](http://arxiv.org/pdf/2411.07133v1)

**Tags**: cs.AI cs.CL 



### ZT-RIC:A Zero Trust RIC Framework for ensuring data Privacy and   Confidentiality in Open RAN
**Authors**: Diana Lin, Samarth Bhargav, Azuka Chiejina, Mohamed I. Ibrahem, Vijay K. Shah

**Updated**: 2024-11-11T16:59:22Z

**Summary**: The advancement of 5G and NextG networks through Open Radio Access Network (O-RAN) architecture enables a shift toward virtualized, modular, and disaggregated configurations. A core component of O-RAN is the RAN Intelligent Controller (RIC), which manages RAN using machine learning-driven xApps that access sensitive data from RAN and User Equipment (UE), stored in the near Real-Time RIC (Near-RT RIC) database. This shared, open environment increases the risk of unauthorized data exposure. To address these concerns, this paper proposes a zero-trust RIC (ZT-RIC) framework that preserves data privacy across the RIC platform, including the RIC database, xApps, and E2 interface. ZT-RIC employs Inner Product Functional Encryption (IPFE) to encrypt RAN/UE data at the base station, preventing leaks through the E2 interface and shared database. Additionally, ZT-RIC enables xApps to perform inference on encrypted data without exposing sensitive information. For evaluation, a state-of-the-art InterClass xApp, which detects jamming signals using RAN key performance metrics (KPMs), is implemented. Testing on an LTE/5G O-RAN testbed shows that ZT-RIC preserves data confidentiality while achieving 97.9% accuracy in jamming detection and meeting sub-second latency requirements, with a round-trip time (RTT) of 0.527 seconds.

**Link**: [arxiv](http://arxiv.org/abs/2411.07128v1),  [pdf](http://arxiv.org/pdf/2411.07128v1)

**Tags**: cs.CR 



### Benchmarking LLMs' Judgments with No Gold Standard
**Authors**: Shengwei Xu, Yuxuan Lu, Grant Schoenebeck, Yuqing Kong

**Updated**: 2024-11-11T16:58:36Z

**Summary**: We introduce the GEM (Generative Estimator for Mutual Information), an evaluation metric for assessing language generation by Large Language Models (LLMs), particularly in generating informative judgments, without the need for a gold standard reference. GEM broadens the scenarios where we can benchmark LLM generation performance-from traditional ones, like machine translation and summarization, where gold standard references are readily available, to subjective tasks without clear gold standards, such as academic peer review.   GEM uses a generative model to estimate mutual information between candidate and reference responses, without requiring the reference to be a gold standard. In experiments on a human-annotated dataset, GEM demonstrates competitive correlations with human scores compared to the state-of-the-art GPT-4o Examiner, and outperforms all other baselines. Additionally, GEM is more robust against strategic manipulations, such as rephrasing or elongation, which can artificially inflate scores under a GPT-4o Examiner.   We also present GRE-bench (Generating Review Evaluation Benchmark) which evaluates LLMs based on how well they can generate high-quality peer reviews for academic research papers. Because GRE-bench is based upon GEM, it inherits its robustness properties. Additionally, GRE-bench circumvents data contamination problems (or data leakage) by using the continuous influx of new open-access research papers and peer reviews each year. We show GRE-bench results of various popular LLMs on their peer review capabilities using the ICLR2023 dataset.

**Link**: [arxiv](http://arxiv.org/abs/2411.07127v1),  [pdf](http://arxiv.org/pdf/2411.07127v1)

**Tags**: cs.CL cs.LG 



### Hire Me or Not? Examining Language Model's Behavior with Occupation   Attributes
**Authors**: Damin Zhang, Yi Zhang, Geetanjali Bihani, Julia Rayz

**Updated**: 2024-11-11T16:53:58Z

**Summary**: With the impressive performance in various downstream tasks, large language models (LLMs) have been widely integrated into production pipelines, like recruitment and recommendation systems. A known issue of models trained on natural language data is the presence of human biases, which can impact the fairness of the system. This paper investigates LLMs' behavior with respect to gender stereotypes, in the context of occupation decision making. Our framework is designed to investigate and quantify the presence of gender stereotypes in LLMs' behavior via multi-round question answering. Inspired by prior works, we construct a dataset by leveraging a standard occupation classification knowledge base released by authoritative agencies. We tested three LLMs (RoBERTa-large, GPT-3.5-turbo, and Llama2-70b-chat) and found that all models exhibit gender stereotypes analogous to human biases, but with different preferences. The distinct preferences of GPT-3.5-turbo and Llama2-70b-chat may imply the current alignment methods are insufficient for debiasing and could introduce new biases contradicting the traditional gender stereotypes.

**Link**: [arxiv](http://arxiv.org/abs/2405.06687v2),  [pdf](http://arxiv.org/pdf/2405.06687v2)

**Tags**: cs.CL 



### SCAR: Sparse Conditioned Autoencoders for Concept Detection and Steering   in LLMs
**Authors**: Ruben Hrle, Felix Friedrich, Manuel Brack, Bjrn Deiseroth, Patrick Schramowski, Kristian Kersting

**Updated**: 2024-11-11T16:51:39Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable capabilities in generating human-like text, but their output may not be aligned with the user or even produce harmful content. This paper presents a novel approach to detect and steer concepts such as toxicity before generation. We introduce the Sparse Conditioned Autoencoder (SCAR), a single trained module that extends the otherwise untouched LLM. SCAR ensures full steerability, towards and away from concepts (e.g., toxic content), without compromising the quality of the model's text generation on standard evaluation benchmarks. We demonstrate the effective application of our approach through a variety of concepts, including toxicity, safety, and writing style alignment. As such, this work establishes a robust framework for controlling LLM generations, ensuring their ethical and safe deployment in real-world applications.

**Link**: [arxiv](http://arxiv.org/abs/2411.07122v1),  [pdf](http://arxiv.org/pdf/2411.07122v1)

**Tags**: cs.CL 



### Privacy-Preserving Graph-Based Machine Learning with Fully Homomorphic   Encryption for Collaborative Anti-Money Laundering
**Authors**: Fabrianne Effendi, Anupam Chattopadhyay

**Updated**: 2024-11-11T16:47:58Z

**Summary**: Combating money laundering has become increasingly complex with the rise of cybercrime and digitalization of financial transactions. Graph-based machine learning techniques have emerged as promising tools for Anti-Money Laundering (AML) detection, capturing intricate relationships within money laundering networks. However, the effectiveness of AML solutions is hindered by data silos within financial institutions, limiting collaboration and overall efficacy. This research presents a novel privacy-preserving approach for collaborative AML machine learning, facilitating secure data sharing across institutions and borders while preserving privacy and regulatory compliance. Leveraging Fully Homomorphic Encryption (FHE), computations are directly performed on encrypted data, ensuring the confidentiality of financial data. Notably, FHE over the Torus (TFHE) was integrated with graph-based machine learning using Zama Concrete ML. The research contributes two key privacy-preserving pipelines. First, the development of a privacy-preserving Graph Neural Network (GNN) pipeline was explored. Optimization techniques like quantization and pruning were used to render the GNN FHE-compatible. Second, a privacy-preserving graph-based XGBoost pipeline leveraging Graph Feature Preprocessor (GFP) was successfully developed. Experiments demonstrated strong predictive performance, with the XGBoost model consistently achieving over 99% accuracy, F1-score, precision, and recall on the balanced AML dataset in both unencrypted and FHE-encrypted inference settings. On the imbalanced dataset, the incorporation of graph-based features improved the F1-score by 8%. The research highlights the need to balance the trade-off between privacy and computational efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2411.02926v2),  [pdf](http://arxiv.org/pdf/2411.02926v2)

**Tags**: cs.CR cs.LG 



### Stochastic gradient descent in continuous time for drift identification   in multiscale diffusions
**Authors**: Max Hirsch, Andrea Zanoni

**Updated**: 2024-11-11T16:42:45Z

**Summary**: We consider the setting of multiscale overdamped Langevin stochastic differential equations, and study the problem of learning the drift function of the homogenized dynamics from continuous-time observations of the multiscale system. We decompose the drift term in a truncated series of basis functions, and employ the stochastic gradient descent in continuous time to infer the coefficients of the expansion. Due to the incompatibility between the multiscale data and the homogenized model, the estimator alone is not able to reconstruct the exact drift. We therefore propose to filter the original trajectory through appropriate kernels and include filtered data in the stochastic differential equation for the estimator, which indeed solves the misspecification issue. Several numerical experiments highlight the accuracy of our approach. Moreover, we show theoretically in a simplified framework the asymptotic unbiasedness of our estimator in the limit of infinite data and when the multiscale parameter describing the fastest scale vanishes.

**Link**: [arxiv](http://arxiv.org/abs/2409.12935v2),  [pdf](http://arxiv.org/pdf/2409.12935v2)

**Tags**: math.NA cs.NA 



### TinyML Security: Exploring Vulnerabilities in Resource-Constrained   Machine Learning Systems
**Authors**: Jacob Huckelberry, Yuke Zhang, Allison Sansone, James Mickens, Peter A. Beerel, Vijay Janapa Reddi

**Updated**: 2024-11-11T16:41:22Z

**Summary**: Tiny Machine Learning (TinyML) systems, which enable machine learning inference on highly resource-constrained devices, are transforming edge computing but encounter unique security challenges. These devices, restricted by RAM and CPU capabilities two to three orders of magnitude smaller than conventional systems, make traditional software and hardware security solutions impractical. The physical accessibility of these devices exacerbates their susceptibility to side-channel attacks and information leakage. Additionally, TinyML models pose security risks, with weights potentially encoding sensitive data and query interfaces that can be exploited. This paper offers the first thorough survey of TinyML security threats. We present a device taxonomy that differentiates between IoT, EdgeML, and TinyML, highlighting vulnerabilities unique to TinyML. We list various attack vectors, assess their threat levels using the Common Vulnerability Scoring System, and evaluate both existing and possible defenses. Our analysis identifies where traditional security measures are adequate and where solutions tailored to TinyML are essential. Our results underscore the pressing need for specialized security solutions in TinyML to ensure robust and secure edge computing applications. We aim to inform the research community and inspire innovative approaches to protecting this rapidly evolving and critical field.

**Link**: [arxiv](http://arxiv.org/abs/2411.07114v1),  [pdf](http://arxiv.org/pdf/2411.07114v1)

**Tags**: cs.CR cs.LG 



### Learning Dynamics from Multicellular Graphs with Deep Neural Networks
**Authors**: Haiqian Yang, Florian Meyer, Shaoxun Huang, Liu Yang, Cristiana Lungu, Monilola A. Olayioye, Markus J. Buehler, Ming Guo

**Updated**: 2024-11-11T16:40:18Z

**Summary**: Multicellular self-assembly into functional structures is a dynamic process that is critical in the development and diseases, including embryo development, organ formation, tumor invasion, and others. Being able to infer collective cell migratory dynamics from their static configuration is valuable for both understanding and predicting these complex processes. However, the identification of structural features that can indicate multicellular motion has been difficult, and existing metrics largely rely on physical instincts. Here we show that using a graph neural network (GNN), the motion of multicellular collectives can be inferred from a static snapshot of cell positions, in both experimental and synthetic datasets.

**Link**: [arxiv](http://arxiv.org/abs/2401.12196v3),  [pdf](http://arxiv.org/pdf/2401.12196v3)

**Tags**: physics.bio-ph cond-mat.soft cs.LG 



### ROCODE: Integrating Backtracking Mechanism and Program Analysis in Large   Language Models for Code Generation
**Authors**: Xue Jiang, Yihong Dong, Yongding Tao, Huanyu Liu, Zhi Jin, Wenpin Jiao, Ge Li

**Updated**: 2024-11-11T16:39:13Z

**Summary**: Large language models (LLMs) have achieved impressive performance in code generation recently, offering programmers revolutionary assistance in software development. However, due to the auto-regressive nature of LLMs, they are susceptible to error accumulation during code generation. Once an error is produced, LLMs can merely continue to generate the subsequent code conditioned on it, given their inability to adjust previous outputs. Existing LLM-based approaches typically consider post-revising after code generation, leading to the challenging resolution of accumulated errors and the significant wastage of resources. Ideally, LLMs should rollback and resolve the occurred error in time during code generation, rather than proceed on the basis of the error and wait for post-revising after generation. In this paper, we propose ROCODE, which integrates the backtracking mechanism and program analysis into LLMs for code generation. Specifically, we employ program analysis to perform incremental error detection during the generation process. When an error is detected, the backtracking mechanism is triggered to priming rollback strategies and constraint regeneration, thereby eliminating the error early and ensuring continued generation on the correct basis. Experiments on multiple code generation benchmarks show that ROCODE can significantly reduce the errors generated by LLMs, with a compilation pass rate of 99.1%. The test pass rate is improved by up to 23.8% compared to the best baseline approach. Compared to the post-revising baseline, the token cost is reduced by 19.3%. Moreover, our approach is model-agnostic and achieves consistent improvements across nine representative LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2411.07112v1),  [pdf](http://arxiv.org/pdf/2411.07112v1)

**Tags**: cs.SE 



### Building a Taiwanese Mandarin Spoken Language Model: A First Attempt
**Authors**: Chih-Kai Yang, Yu-Kuan Fu, Chen-An Li, Yi-Cheng Lin, Yu-Xiang Lin, Wei-Chih Chen, Ho Lam Chung, Chun-Yi Kuan, Wei-Ping Huang, Ke-Han Lu, Tzu-Quan Lin, Hsiu-Hsuan Wang, En-Pei Hu, Chan-Jan Hsu, Liang-Hsuan Tseng, I-Hsiang Chiu, Ulin Sanga, Xuanjun Chen, Po-chun Hsu, Shu-wen Yang, Hung-yi Lee

**Updated**: 2024-11-11T16:37:40Z

**Summary**: This technical report presents our initial attempt to build a spoken large language model (LLM) for Taiwanese Mandarin, specifically tailored to enable real-time, speech-to-speech interaction in multi-turn conversations. Our end-to-end model incorporates a decoder-only transformer architecture and aims to achieve seamless interaction while preserving the conversational flow, including full-duplex capabilities allowing simultaneous speaking and listening. The paper also details the training process, including data preparation with synthesized dialogues and adjustments for real-time interaction. We also developed a platform to evaluate conversational fluency and response coherence in multi-turn dialogues. We hope the release of the report can contribute to the future development of spoken LLMs in Taiwanese Mandarin.

**Link**: [arxiv](http://arxiv.org/abs/2411.07111v1),  [pdf](http://arxiv.org/pdf/2411.07111v1)

**Tags**: cs.CL cs.SD eess.AS 



### Not Eliminate but Aggregate: Post-Hoc Control over Mixture-of-Experts to   Address Shortcut Shifts in Natural Language Understanding
**Authors**: Ukyo Honda, Tatsushi Oka, Peinan Zhang, Masato Mita

**Updated**: 2024-11-11T16:33:25Z

**Summary**: Recent models for natural language understanding are inclined to exploit simple patterns in datasets, commonly known as shortcuts. These shortcuts hinge on spurious correlations between labels and latent features existing in the training data. At inference time, shortcut-dependent models are likely to generate erroneous predictions under distribution shifts, particularly when some latent features are no longer correlated with the labels. To avoid this, previous studies have trained models to eliminate the reliance on shortcuts. In this study, we explore a different direction: pessimistically aggregating the predictions of a mixture-of-experts, assuming each expert captures relatively different latent features. The experimental results demonstrate that our post-hoc control over the experts significantly enhances the model's robustness to the distribution shift in shortcuts. Besides, we show that our approach has some practical advantages. We also analyze our model and provide results to support the assumption.

**Link**: [arxiv](http://arxiv.org/abs/2406.12060v2),  [pdf](http://arxiv.org/pdf/2406.12060v2)

**Tags**: cs.CL cs.LG 



### Advancing Object Goal Navigation Through LLM-enhanced Object Affinities   Transfer
**Authors**: Mengying Lin, Shugao Liu, Dingxi Zhang, Yaran Chen, Haoran Liu, Dongbin Zhao

**Updated**: 2024-11-11T16:20:28Z

**Summary**: In object goal navigation, agents navigate towards objects identified by category labels using visual and spatial information. Previously, solely network-based methods typically rely on historical data for object affinities estimation, lacking adaptability to new environments and unseen targets. Simultaneously, employing Large Language Models (LLMs) for navigation as either planners or agents, though offering a broad knowledge base, is cost-inefficient and lacks targeted historical experience. Addressing these challenges, we present the LLM-enhanced Object Affinities Transfer (LOAT) framework, integrating LLM-derived object semantics with network-based approaches to leverage experiential object affinities, thus improving adaptability in unfamiliar settings. LOAT employs a dual-module strategy: a generalized affinities module for accessing LLMs' vast knowledge and an experiential affinities module for applying learned object semantic relationships, complemented by a dynamic fusion module harmonizing these information sources based on temporal context. The resulting scores activate semantic maps before feeding into downstream policies, enhancing navigation systems with context-aware inputs. Our evaluations conducted in the AI2-THOR and Habitat simulators indicate significant improvements in both navigation success rates and overall efficiency. Furthermore, the system performs effectively when deployed on a real robot without requiring additional training, thereby validating the efficacy of LOAT in integrating LLM insights for enhanced object-goal navigation.

**Link**: [arxiv](http://arxiv.org/abs/2403.09971v2),  [pdf](http://arxiv.org/pdf/2403.09971v2)

**Tags**: cs.RO 



### A Multi-Agent Approach for REST API Testing with Semantic Graphs and   LLM-Driven Inputs
**Authors**: Myeongsoo Kim, Tyler Stennett, Saurabh Sinha, Alessandro Orso

**Updated**: 2024-11-11T16:20:27Z

**Summary**: As modern web services increasingly rely on REST APIs, their thorough testing has become crucial. Furthermore, the advent of REST API specifications such as the OpenAPI Specification has led to the emergence of many black-box REST API testing tools. However, these tools often focus on individual test elements in isolation (e.g., APIs, parameters, values), resulting in lower coverage and less effectiveness in detecting faults (i.e., 500 response codes). To address these limitations, we present AutoRestTest, the first black-box framework to adopt a dependency-embedded multi-agent approach for REST API testing, integrating Multi-Agent Reinforcement Learning (MARL) with a Semantic Property Dependency Graph (SPDG) and Large Language Models (LLMs). Our approach treats REST API testing as a separable problem, where four agents -- API, dependency, parameter, and value -- collaborate to optimize API exploration. LLMs handle domain-specific value restrictions, the SPDG model simplifies the search space for dependencies using a similarity score between API operations, and MARL dynamically optimizes the agents' behavior. Evaluated on 12 real-world REST services, AutoRestTest outperforms the four leading black-box REST API testing tools, including those assisted by RESTGPT (which augments realistic test inputs using LLMs), in terms of code coverage, operation coverage, and fault detection. Notably, AutoRestTest is the only tool able to identify an internal server error in Spotify. Our ablation study underscores the significant contributions of the agent learning, SPDG, and LLM components.

**Link**: [arxiv](http://arxiv.org/abs/2411.07098v1),  [pdf](http://arxiv.org/pdf/2411.07098v1)

**Tags**: cs.SE cs.AI 



### Impact of LLM-based Review Comment Generation in Practice: A Mixed   Open-/Closed-source User Study
**Authors**: Doriane Olewicki, Leuson Da Silva, Suhaib Mujahid, Arezou Amini, Benjamin Mah, Marco Castelluccio, Sarra Habchi, Foutse Khomh, Bram Adams

**Updated**: 2024-11-11T16:12:11Z

**Summary**: We conduct a large-scale empirical user study in a live setup to evaluate the acceptance of LLM-generated comments and their impact on the review process. This user study was performed in two organizations, Mozilla (which has its codebase available as open source) and Ubisoft (fully closed-source). Inside their usual review environment, participants were given access to RevMate, an LLM-based assistive tool suggesting generated review comments using an off-the-shelf LLM with Retrieval Augmented Generation to provide extra code and review context, combined with LLM-as-a-Judge, to auto-evaluate the generated comments and discard irrelevant cases. Based on more than 587 patch reviews provided by RevMate, we observed that 8.1% and 7.2%, respectively, of LLM-generated comments were accepted by reviewers in each organization, while 14.6% and 20.5% other comments were still marked as valuable as review or development tips. Refactoring-related comments are more likely to be accepted than Functional comments (18.2% and 18.6% compared to 4.8% and 5.2%). The extra time spent by reviewers to inspect generated comments or edit accepted ones (36/119), yielding an overall median of 43s per patch, is reasonable. The accepted generated comments are as likely to yield future revisions of the revised patch as human-written comments (74% vs 73% at chunk-level).

**Link**: [arxiv](http://arxiv.org/abs/2411.07091v1),  [pdf](http://arxiv.org/pdf/2411.07091v1)

**Tags**: cs.SE 



### An Interpretable X-ray Style Transfer via Trainable Local Laplacian   Filter
**Authors**: Dominik Eckert, Ludwig Ritschl, Christopher Syben, Christian Hmmer, Julia Wicklein, Marcel Beister, Steffen Kappler, Sebastian Stober

**Updated**: 2024-11-11T15:47:25Z

**Summary**: Radiologists have preferred visual impressions or 'styles' of X-ray images that are manually adjusted to their needs to support their diagnostic performance. In this work, we propose an automatic and interpretable X-ray style transfer by introducing a trainable version of the Local Laplacian Filter (LLF). From the shape of the LLF's optimized remap function, the characteristics of the style transfer can be inferred and reliability of the algorithm can be ensured. Moreover, we enable the LLF to capture complex X-ray style features by replacing the remap function with a Multi-Layer Perceptron (MLP) and adding a trainable normalization layer. We demonstrate the effectiveness of the proposed method by transforming unprocessed mammographic X-ray images into images that match the style of target mammograms and achieve a Structural Similarity Index (SSIM) of 0.94 compared to 0.82 of the baseline LLF style transfer method from Aubry et al.

**Link**: [arxiv](http://arxiv.org/abs/2411.07072v1),  [pdf](http://arxiv.org/pdf/2411.07072v1)

**Tags**: cs.CV cs.AI cs.LG 



### Universal Response and Emergence of Induction in LLMs
**Authors**: Niclas Luick

**Updated**: 2024-11-11T15:47:15Z

**Summary**: While induction is considered a key mechanism for in-context learning in LLMs, understanding its precise circuit decomposition beyond toy models remains elusive. Here, we study the emergence of induction behavior within LLMs by probing their response to weak single-token perturbations of the residual stream. We find that LLMs exhibit a robust, universal regime in which their response remains scale-invariant under changes in perturbation strength, thereby allowing us to quantify the build-up of token correlations throughout the model. By applying our method, we observe signatures of induction behavior within the residual stream of Gemma-2-2B, Llama-3.2-3B, and GPT-2-XL. Across all models, we find that these induction signatures gradually emerge within intermediate layers and identify the relevant model sections composing this behavior. Our results provide insights into the collective interplay of components within LLMs and serve as a benchmark for large-scale circuit analysis.

**Link**: [arxiv](http://arxiv.org/abs/2411.07071v1),  [pdf](http://arxiv.org/pdf/2411.07071v1)

**Tags**: cs.LG cs.AI cs.CL 



### On Active Privacy Auditing in Supervised Fine-tuning for White-Box   Language Models
**Authors**: Qian Sun, Hanpeng Wu, Xi Sheryl Zhang

**Updated**: 2024-11-11T15:46:07Z

**Summary**: The pretraining and fine-tuning approach has become the leading technique for various NLP applications. However, recent studies reveal that fine-tuning data, due to their sensitive nature, domain-specific characteristics, and identifiability, pose significant privacy concerns. To help develop more privacy-resilient fine-tuning models, we introduce a novel active privacy auditing framework, dubbed Parsing, designed to identify and quantify privacy leakage risks during the supervised fine-tuning (SFT) of language models (LMs). The framework leverages improved white-box membership inference attacks (MIAs) as the core technology, utilizing novel learning objectives and a two-stage pipeline to monitor the privacy of the LMs' fine-tuning process, maximizing the exposure of privacy risks. Additionally, we have improved the effectiveness of MIAs on large LMs including GPT-2, Llama2, and certain variants of them. Our research aims to provide the SFT community of LMs with a reliable, ready-to-use privacy auditing tool, and to offer valuable insights into safeguarding privacy during the fine-tuning process. Experimental results confirm the framework's efficiency across various models and tasks, emphasizing notable privacy concerns in the fine-tuning process. Project code available for https://github.com/mapleleavesss/PARSING.

**Link**: [arxiv](http://arxiv.org/abs/2411.07070v1),  [pdf](http://arxiv.org/pdf/2411.07070v1)

**Tags**: cs.CL cs.AI 



### Recent Advances in Named Entity Recognition: A Comprehensive Survey and   Comparative Study
**Authors**: Imed Keraghel, Stanislas Morbieu, Mohamed Nadif

**Updated**: 2024-11-11T15:45:02Z

**Summary**: Named Entity Recognition seeks to extract substrings within a text that name real-world objects and to determine their type (for example, whether they refer to persons or organizations). In this survey, we first present an overview of recent popular approaches, including advancements in Transformer-based methods and Large Language Models (LLMs) that have not had much coverage in other surveys. In addition, we discuss reinforcement learning and graph-based approaches, highlighting their role in enhancing NER performance. Second, we focus on methods designed for datasets with scarce annotations. Third, we evaluate the performance of the main NER implementations on a variety of datasets with differing characteristics (as regards their domain, their size, and their number of classes). We thus provide a deep comparison of algorithms that have never been considered together. Our experiments shed some light on how the characteristics of datasets affect the behavior of the methods we compare.

**Link**: [arxiv](http://arxiv.org/abs/2401.10825v2),  [pdf](http://arxiv.org/pdf/2401.10825v2)

**Tags**: cs.CL cs.LG 68T50, 68Q32 



### Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training
**Authors**: Elia Cunegatti, Leonardo Lucio Custode, Giovanni Iacca

**Updated**: 2024-11-11T15:30:16Z

**Summary**: Network pruning is a set of computational techniques that aim to reduce a given model's computational cost by removing a subset of its parameters while having minimal impact on performance. Throughout the last decade, the most widely used pruning paradigm has focused on pruning and re-training, which nowadays is inconvenient due to the vast amount of pre-trained models, which are in any case too expensive to re-train. In this paper, we exploit functional information from dense pre-trained models, i.e., their activations, to obtain sparse models that maximize the activations' alignment w.r.t. their corresponding dense models. Hence, we propose \textsc{NeuroAl}, a \emph{top-up} algorithm that can be used on top of any given pruning algorithm for LLMs, that modifies the block-wise and row-wise sparsity ratios to maximize the \emph{neuron alignment} among activations. Moreover, differently from existing methods, our approach adaptively selects the best parameters for the block-wise and row-wise sparsity ratios w.r.t. to the model and the desired sparsity (given as input), and requires \emph{no re-training}. We test our method on 4 different LLM families and 3 different sparsity ratios, showing how it consistently outperforms the latest state-of-the-art techniques. The code is available at https://github.com/eliacunegatti/NeuroAL.

**Link**: [arxiv](http://arxiv.org/abs/2411.07066v1),  [pdf](http://arxiv.org/pdf/2411.07066v1)

**Tags**: cs.LG cs.AI cs.CL 



### Statistical Inference with Limited Memory: A Survey
**Authors**: Tomer Berg, Or Ordentlich, Ofer Shayevitz

**Updated**: 2024-11-11T15:27:46Z

**Summary**: The problem of statistical inference in its various forms has been the subject of decades-long extensive research. Most of the effort has been focused on characterizing the behavior as a function of the number of available samples, with far less attention given to the effect of memory limitations on performance. Recently, this latter topic has drawn much interest in the engineering and computer science literature. In this survey paper, we attempt to review the state-of-the-art of statistical inference under memory constraints in several canonical problems, including hypothesis testing, parameter estimation, and distribution property testing/estimation. We discuss the main results in this developing field, and by identifying recurrent themes, we extract some fundamental building blocks for algorithmic construction, as well as useful techniques for lower bound derivations.

**Link**: [arxiv](http://arxiv.org/abs/2312.15225v3),  [pdf](http://arxiv.org/pdf/2312.15225v3)

**Tags**: cs.LG cs.IT math.IT stat.ML 



### LIFBench: Evaluating the Instruction Following Performance and Stability   of Large Language Models in Long-Context Scenarios
**Authors**: Xiaodong Wu, Minhao Wang, Yichen Liu, Xiaoming Shi, He Yan, Xiangju Lu, Junmin Zhu, Wei Zhang

**Updated**: 2024-11-11T14:43:51Z

**Summary**: As Large Language Models (LLMs) continue to advance in natural language processing (NLP), their ability to stably follow instructions in long-context inputs has become crucial for real-world applications. While existing benchmarks assess various LLM capabilities, they rarely focus on instruction-following in long-context scenarios or stability on different inputs. In response, we introduce the Long-context Instruction-Following Benchmark (LIFBench), a scalable dataset designed to evaluate LLMs' instruction-following capabilities and stability across long contexts. LIFBench comprises three long-context scenarios and eleven diverse tasks, supported by 2,766 instructions generated through an automated expansion method across three dimensions: length, expression, and variables. For evaluation, we propose LIFEval, a rubric-based assessment framework that provides precise, automated scoring of complex LLM responses without relying on LLM-assisted evaluations or human judgments. This approach facilitates a comprehensive analysis of model performance and stability across various perspectives. We conduct extensive experiments on 20 notable LLMs across six length intervals, analyzing their instruction-following capabilities and stability. Our work contributes LIFBench and LIFEval as robust tools for assessing LLM performance in complex, long-context settings, providing insights that can inform future LLM development.

**Link**: [arxiv](http://arxiv.org/abs/2411.07037v1),  [pdf](http://arxiv.org/pdf/2411.07037v1)

**Tags**: cs.CL 



### Inferring jet physics from neutron star - black hole mergers with   gravitational waves
**Authors**: Teagan A. Clarke, Paul D. Lasky, Eric Thrane

**Updated**: 2024-11-11T14:43:25Z

**Summary**: Neutron star - black hole (NSBH) mergers that undergo tidal disruption may launch jets that could power a gamma-ray burst. We use a population of simulated NSBH systems to measure jet parameters from the gravitational waves emitted by these systems. The conditions during the tidal disruption and merger phase required to power a gamma-ray burst are uncertain. It is likely that the system must achieve some minimum remnant baryonic mass after the merger before a jet can be launched to power a gamma-ray burst. Assuming a fiducial neutron star equation of state, we show how Bayesian hierarchical inference can be used to infer the minimum remnant mass required to launch a gamma-ray burst jet as well as the maximum gamma-ray burst viewing angle to detect a gamma-ray burst. We find that with 200 NSBH observations, we can measure the minimum disk mass to within 0.01 solar masses at 90% credibility. We simultaneously infer the maximum gamma-ray burst viewing angle to within 13 degrees at 90% credibility. We conclude that upcoming upgrades to the LIGO observatories may provide important new insights into the physics of NSBH jets.

**Link**: [arxiv](http://arxiv.org/abs/2411.07035v1),  [pdf](http://arxiv.org/pdf/2411.07035v1)

**Tags**: astro-ph.HE gr-qc 



### Dynamic and Textual Graph Generation Via Large-Scale LLM-based Agent   Simulation
**Authors**: Jiarui Ji, Runlin Lei, Jialing Bi, Zhewei Wei, Yankai Lin, Xuchen Pan, Yaliang Li, Bolin Ding

**Updated**: 2024-11-11T14:41:53Z

**Summary**: Graph generation is a fundamental task that has been extensively studied in social, technological, and scientific analysis. For modeling the dynamic graph evolution process, traditional rule-based methods struggle to capture community structures within graphs, while deep learning methods only focus on fitting training graphs. This limits existing graph generators to producing graphs that adhere to predefined rules or closely resemble training datasets, achieving poor performance in dynamic graph generation. Given that graphs are abstract representations arising from pairwise interactions in human activities, a realistic simulation of human-wise interaction could provide deeper insights into the graph evolution mechanism. With the increasing recognition of large language models (LLMs) in simulating human behavior, we introduce GraphAgent-Generator (GAG), a novel simulation-based framework for dynamic graph generation. Without training or fine-tuning process of LLM, our framework effectively replicates seven macro-level structural characteristics in established network science theories while surpassing existing baselines in graph expansion tasks by 31\% on specific evaluation metrics. Through node classification task, we validate GAG effectively preserves characteristics of real-world network for node-wise textual features in generated text-rich graph. Furthermore, by incorporating parallel acceleration, GAG supports generating graphs with up to nearly 100,000 nodes or 10 million edges through large-scale LLM-based agent simulation, with a minimum speed-up of 90.4\%. The source code is available at https://anonymous.4open.science/r/GraphAgent-2206.

**Link**: [arxiv](http://arxiv.org/abs/2410.09824v4),  [pdf](http://arxiv.org/pdf/2410.09824v4)

**Tags**: cs.CL 



### Scaling Long-Horizon Online POMDP Planning via Rapid State Space   Sampling
**Authors**: Yuanchu Liang, Edward Kim, Wil Thomason, Zachary Kingston, Hanna Kurniawati, Lydia E. Kavraki

**Updated**: 2024-11-11T14:38:40Z

**Summary**: Partially Observable Markov Decision Processes (POMDPs) are a general and principled framework for motion planning under uncertainty. Despite tremendous improvement in the scalability of POMDP solvers, long-horizon POMDPs (e.g., $\geq15$ steps) remain difficult to solve. This paper proposes a new approximate online POMDP solver, called Reference-Based Online POMDP Planning via Rapid State Space Sampling (ROP-RaS3). ROP-RaS3 uses novel extremely fast sampling-based motion planning techniques to sample the state space and generate a diverse set of macro actions online which are then used to bias belief-space sampling and infer high-quality policies without requiring exhaustive enumeration of the action space -- a fundamental constraint for modern online POMDP solvers. ROP-RaS3 is evaluated on various long-horizon POMDPs, including on a problem with a planning horizon of more than 100 steps and a problem with a 15-dimensional state space that requires more than 20 look ahead steps. In all of these problems, ROP-RaS3 substantially outperforms other state-of-the-art methods by up to multiple folds.

**Link**: [arxiv](http://arxiv.org/abs/2411.07032v1),  [pdf](http://arxiv.org/pdf/2411.07032v1)

**Tags**: cs.RO I.2.9; I.2.8 



### PediatricsGPT: Large Language Models as Chinese Medical Assistants for   Pediatric Applications
**Authors**: Dingkang Yang, Jinjie Wei, Dongling Xiao, Shunli Wang, Tong Wu, Gang Li, Mingcheng Li, Shuaibing Wang, Jiawei Chen, Yue Jiang, Qingyao Xu, Ke Li, Peng Zhai, Lihua Zhang

**Updated**: 2024-11-11T14:36:35Z

**Summary**: Developing intelligent pediatric consultation systems offers promising prospects for improving diagnostic efficiency, especially in China, where healthcare resources are scarce. Despite recent advances in Large Language Models (LLMs) for Chinese medicine, their performance is sub-optimal in pediatric applications due to inadequate instruction data and vulnerable training procedures. To address the above issues, this paper builds PedCorpus, a high-quality dataset of over 300,000 multi-task instructions from pediatric textbooks, guidelines, and knowledge graph resources to fulfil diverse diagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the first Chinese pediatric LLM assistant built on a systematic and robust training pipeline. In the continuous pre-training phase, we introduce a hybrid instruction pre-training mechanism to mitigate the internal-injected knowledge inconsistency of LLMs for medical domain adaptation. Immediately, the full-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the general medical knowledge schema into the models. After that, we devise a direct following preference optimization to enhance the generation of pediatrician-like humanistic responses. In the parameter-efficient secondary SFT phase, a mixture of universal-specific experts strategy is presented to resolve the competency conflict between medical generalist and pediatric expertise mastery. Extensive results based on the metrics, GPT-4, and doctor evaluations on distinct doctor downstream tasks show that PediatricsGPT consistently outperforms previous Chinese medical LLMs. Our model and dataset will be open-source for community development.

**Link**: [arxiv](http://arxiv.org/abs/2405.19266v4),  [pdf](http://arxiv.org/pdf/2405.19266v4)

**Tags**: cs.CL 



### Aligning LLMs for FL-free Program Repair
**Authors**: Junjielong Xu, Ying Fu, Shin Hwei Tan, Pinjia He

**Updated**: 2024-11-11T14:35:45Z

**Summary**: Large language models (LLMs) have achieved decent results on automated program repair (APR). However, the next token prediction training objective of decoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction objective of current infilling-style methods, which impedes LLMs from fully leveraging pre-trained knowledge for program repair. In addition, while some LLMs can locate and repair bugs in certain functions using the related artifacts (e.g., test cases), existing methods still depend on statement-level fault localization methods to provide a list of buggy hunks for repair. This restriction hinders LLMs from exploring potential patches beyond the given locations.   In this paper, we investigate a new approach to adapt LLMs to program repair. Our core insight is that LLM's APR capability can be greatly improved by simply aligning the output to their training objective and allowing them to refine the whole program without first identifying faulty statements. Based on this insight, we designed D4C, a straightforward prompting framework for APR. D4C can repair 180 bugs correctly in Defects4J, with each patch being sampled only 10 times. This surpasses the SOTA APR methods with perfect fault localization by 10% and reduces the patch sampling number by 90%. Our findings reveal that (1) objective alignment is crucial for fully exploiting LLM's pre-trained capability, and (2) replacing the traditional localize-buggy-hunks-then-repair workflow with direct debugging is more effective for LLM-based APR methods. Thus, we believe this paper introduces a new mindset for harnessing LLMs in APR.

**Link**: [arxiv](http://arxiv.org/abs/2404.08877v2),  [pdf](http://arxiv.org/pdf/2404.08877v2)

**Tags**: cs.SE cs.CL cs.LG 



### The Inherent Adversarial Robustness of Analog In-Memory Computing
**Authors**: Corey Lammie, Julian Bchel, Athanasios Vasilopoulos, Manuel Le Gallo, Abu Sebastian

**Updated**: 2024-11-11T14:29:59Z

**Summary**: A key challenge for Deep Neural Network (DNN) algorithms is their vulnerability to adversarial attacks. Inherently non-deterministic compute substrates, such as those based on Analog In-Memory Computing (AIMC), have been speculated to provide significant adversarial robustness when performing DNN inference. In this paper, we experimentally validate this conjecture for the first time on an AIMC chip based on Phase Change Memory (PCM) devices. We demonstrate higher adversarial robustness against different types of adversarial attacks when implementing an image classification network. Additional robustness is also observed when performing hardware-in-the-loop attacks, for which the attacker is assumed to have full access to the hardware. A careful study of the various noise sources indicate that a combination of stochastic noise sources (both recurrent and non-recurrent) are responsible for the adversarial robustness and that their type and magnitude disproportionately effects this property. Finally, it is demonstrated, via simulations, that when a much larger transformer network is used to implement a Natural Language Processing (NLP) task, additional robustness is still observed.

**Link**: [arxiv](http://arxiv.org/abs/2411.07023v1),  [pdf](http://arxiv.org/pdf/2411.07023v1)

**Tags**: cs.ET cs.CR 



### Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation
**Authors**: Ziwei Liu, Liang Zhang, Qian Li, Jianghua Wu, Guangxu Zhu

**Updated**: 2024-11-11T14:25:37Z

**Summary**: Retrieval-augmented generation (RAG) has shown impressive capability in providing reliable answer predictions and addressing hallucination problems. A typical RAG implementation uses powerful retrieval models to extract external information and large language models (LLMs) to generate answers. In contrast, recent LLM-based retrieval has gained attention for its substantial improvements in information retrieval (IR) due to the LLMs' semantic understanding capability. However, directly applying LLM to RAG systems presents challenges. This may cause feature locality problems as massive parametric knowledge can hinder effective usage of global information across the corpus; for example, an LLM-based retriever often inputs document summaries instead of full documents. Moreover, various pre-trained tasks in LLMs introduce variance, further weakening performance as a retriever.   To address these issues, we propose a novel two-stage fine-tuning architecture called Invar-RAG. In the retrieval stage, an LLM-based retriever is constructed by integrating LoRA-based representation learning to tackle feature locality issues. To enhance retrieval performance, we develop two patterns (invariant and variant patterns) and an invariance loss to reduce LLM variance. In the generation stage, a refined fine-tuning method is employed to improve LLM accuracy in generating answers based on retrieved information. Experimental results show that Invar-RAG significantly outperforms existing baselines across three open-domain question answering (ODQA) datasets. Code is available in the Supplementary Material for reproducibility.

**Link**: [arxiv](http://arxiv.org/abs/2411.07021v1),  [pdf](http://arxiv.org/pdf/2411.07021v1)

**Tags**: cs.IR 



### Estimating Causal Effects in Partially Directed Parametric Causal Factor   Graphs
**Authors**: Malte Luttermann, Tanya Braun, Ralf Mller, Marcel Gehrke

**Updated**: 2024-11-11T14:05:39Z

**Summary**: Lifting uses a representative of indistinguishable individuals to exploit symmetries in probabilistic relational models, denoted as parametric factor graphs, to speed up inference while maintaining exact answers. In this paper, we show how lifting can be applied to causal inference in partially directed graphs, i.e., graphs that contain both directed and undirected edges to represent causal relationships between random variables. We present partially directed parametric causal factor graphs (PPCFGs) as a generalisation of previously introduced parametric causal factor graphs, which require a fully directed graph. We further show how causal inference can be performed on a lifted level in PPCFGs, thereby extending the applicability of lifted causal inference to a broader range of models requiring less prior knowledge about causal relationships.

**Link**: [arxiv](http://arxiv.org/abs/2411.07006v1),  [pdf](http://arxiv.org/pdf/2411.07006v1)

**Tags**: cs.AI cs.DS cs.LG 



### Enhancing Robot Assistive Behaviour with Reinforcement Learning and   Theory of Mind
**Authors**: Antonio Andriella, Giovanni Falcone, Silvia Rossi

**Updated**: 2024-11-11T14:01:15Z

**Summary**: The adaptation to users' preferences and the ability to infer and interpret humans' beliefs and intents, which is known as the Theory of Mind (ToM), are two crucial aspects for achieving effective human-robot collaboration. Despite its importance, very few studies have investigated the impact of adaptive robots with ToM abilities. In this work, we present an exploratory comparative study to investigate how social robots equipped with ToM abilities impact users' performance and perception. We design a two-layer architecture. The Q-learning agent on the first layer learns the robot's higher-level behaviour. On the second layer, a heuristic-based ToM infers the user's intended strategy and is responsible for implementing the robot's assistance, as well as providing the motivation behind its choice. We conducted a user study in a real-world setting, involving 56 participants who interacted with either an adaptive robot capable of ToM, or with a robot lacking such abilities. Our findings suggest that participants in the ToM condition performed better, accepted the robot's assistance more often, and perceived its ability to adapt, predict and recognise their intents to a higher degree. Our preliminary insights could inform future research and pave the way for designing more complex computation architectures for adaptive behaviour with ToM capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2411.07003v1),  [pdf](http://arxiv.org/pdf/2411.07003v1)

**Tags**: cs.RO cs.AI cs.HC 



### LLMCount: Enhancing Stationary mmWave Detection with Multimodal-LLM
**Authors**: Boyan Li, Shengyi Ding, Deen Ma, Yixuan Wu, Hongjie Liao, Kaiyuan Hu

**Updated**: 2024-11-11T13:56:30Z

**Summary**: Millimeter wave sensing provides people with the capability of sensing the surrounding crowds in a non-invasive and privacy-preserving manner, which holds huge application potential. However, detecting stationary crowds remains challenging due to several factors such as minimal movements (like breathing or casual fidgets), which can be easily treated as noise clusters during data collection and consequently filtered in the following processing procedures. Additionally, the uneven distribution of signal power due to signal power attenuation and interferences resulting from external reflectors or absorbers further complicates accurate detection. To address these challenges and enable stationary crowd detection across various application scenarios requiring specialized domain adaption, we introduce LLMCount, the first system to harness the capabilities of large-language models (LLMs) to enhance crowd detection performance. By exploiting the decision-making capability of LLM, we can successfully compensate the signal power to acquire a uniform distribution and thereby achieve a detection with higher accuracy. To assess the system's performance, comprehensive evaluations are conducted under diversified scenarios like hall, meeting room, and cinema. The evaluation results show that our proposed approach reaches high detection accuracy with lower overall latency compared with previous methods.

**Link**: [arxiv](http://arxiv.org/abs/2409.16209v2),  [pdf](http://arxiv.org/pdf/2409.16209v2)

**Tags**: cs.CV 



### ENAT: Rethinking Spatial-temporal Interactions in Token-based Image   Synthesis
**Authors**: Zanlin Ni, Yulin Wang, Renping Zhou, Yizeng Han, Jiayi Guo, Zhiyuan Liu, Yuan Yao, Gao Huang

**Updated**: 2024-11-11T13:05:39Z

**Summary**: Recently, token-based generation have demonstrated their effectiveness in image synthesis. As a representative example, non-autoregressive Transformers (NATs) can generate decent-quality images in a few steps. NATs perform generation in a progressive manner, where the latent tokens of a resulting image are incrementally revealed. At each step, the unrevealed image regions are padded with mask tokens and inferred by NAT. In this paper, we delve into the mechanisms behind the effectiveness of NATs and uncover two important patterns that naturally emerge from NATs: Spatially (within a step), although mask and visible tokens are processed uniformly by NATs, the interactions between them are highly asymmetric. In specific, mask tokens mainly gather information for decoding, while visible tokens tend to primarily provide information, and their deep representations can be built only upon themselves. Temporally (across steps), the interactions between adjacent generation steps mostly concentrate on updating the representations of a few critical tokens, while the computation for the majority of tokens is generally repetitive. Driven by these findings, we propose EfficientNAT (ENAT), a NAT model that explicitly encourages these critical interactions inherent in NATs. At the spatial level, we disentangle the computations of visible and mask tokens by encoding visible tokens independently, while decoding mask tokens conditioned on the fully encoded visible tokens. At the temporal level, we prioritize the computation of the critical tokens at each step, while maximally reusing previously computed token representations to supplement necessary information. ENAT improves the performance of NATs notably with significantly reduced computational cost. Experiments on ImageNet-256, ImageNet-512 and MS-COCO validate the effectiveness of ENAT. Code is available at https://github.com/LeapLabTHU/ENAT.

**Link**: [arxiv](http://arxiv.org/abs/2411.06959v1),  [pdf](http://arxiv.org/pdf/2411.06959v1)

**Tags**: cs.CV cs.AI 



### Sniff AI: Is My 'Spicy' Your 'Spicy'? Exploring LLM's Perceptual   Alignment with Human Smell Experiences
**Authors**: Shu Zhong, Zetao Zhou, Christopher Dawes, Giada Brianz, Marianna Obrist

**Updated**: 2024-11-11T12:56:52Z

**Summary**: Aligning AI with human intent is important, yet perceptual alignment-how AI interprets what we see, hear, or smell-remains underexplored. This work focuses on olfaction, human smell experiences. We conducted a user study with 40 participants to investigate how well AI can interpret human descriptions of scents. Participants performed "sniff and describe" interactive tasks, with our designed AI system attempting to guess what scent the participants were experiencing based on their descriptions. These tasks evaluated the Large Language Model's (LLMs) contextual understanding and representation of scent relationships within its internal states - high-dimensional embedding space. Both quantitative and qualitative methods were used to evaluate the AI system's performance. Results indicated limited perceptual alignment, with biases towards certain scents, like lemon and peppermint, and continued failing to identify others, like rosemary. We discuss these findings in light of human-AI alignment advancements, highlighting the limitations and opportunities for enhancing HCI systems with multisensory experience integration.

**Link**: [arxiv](http://arxiv.org/abs/2411.06950v1),  [pdf](http://arxiv.org/pdf/2411.06950v1)

**Tags**: cs.CL cs.HC 



### Cancer-Answer: Empowering Cancer Care with Advanced Large Language   Models
**Authors**: Aniket Deroy, Subhankar Maity

**Updated**: 2024-11-11T12:54:22Z

**Summary**: Gastrointestinal (GI) tract cancers account for a substantial portion of the global cancer burden, where early diagnosis is critical for improved management and patient outcomes. The complex aetiologies and overlapping symptoms across GI cancers often delay diagnosis, leading to suboptimal treatment strategies. Cancer-related queries are crucial for timely diagnosis, treatment, and patient education, as access to accurate, comprehensive information can significantly influence outcomes. However, the complexity of cancer as a disease, combined with the vast amount of available data, makes it difficult for clinicians and patients to quickly find precise answers. To address these challenges, we leverage large language models (LLMs) such as GPT-3.5 Turbo to generate accurate, contextually relevant responses to cancer-related queries. Pre-trained with medical data, these models provide timely, actionable insights that support informed decision-making in cancer diagnosis and care, ultimately improving patient outcomes. We calculate two metrics: A1 (which represents the fraction of entities present in the model-generated answer compared to the gold standard) and A2 (which represents the linguistic correctness and meaningfulness of the model-generated answer with respect to the gold standard), achieving maximum values of 0.546 and 0.881, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2411.06946v1),  [pdf](http://arxiv.org/pdf/2411.06946v1)

**Tags**: cs.CL 



### Inferring reionization and galaxy properties from the patchy kinetic   Sunyaev-Zel'dovich signal
**Authors**: Ivan Nikoli, Andrei Mesinger, Yuxiang Qin, Adlie Gorce

**Updated**: 2024-11-11T12:51:41Z

**Summary**: The patchy kinetic Sunyaev-Zel'dovich (kSZ) signal is an integral probe of the timing and morphology of the epoch of reionization (EoR). Recent observations have claimed a low signal-to-noise (S/N) measurement, with a dramatic increase in S/N expected in the near future. In this work, we quantify what we can learn about the EoR from the kSZ signal. We perform Bayesian inference by sampling galaxy properties and using forward-models of the kSZ as well as other EoR and galaxy observations in the likelihood. Including the recent kSZ measurement obtained by the South Pole Telescope ($\mathcal{D}_{3000}^{\rm{pkSZ}} = 1.1_{-0.7}^{+1.1} \mu$K$^2$) shifts the posterior distribution in favor of faster and later reionization models, resulting in lower values of the optical depth to the CMB: $\tau_e = 0.052_{-0.008}^{+0.009}$ with a 68$\%$ confidence interval (C.I.). The combined EoR and UV luminosity function observations also imply a typical ionizing escape fraction of $0.04_{-0.03}^{+0.05}$ (95$\%$ C.I.), without a strong dependence on halo mass. We show how the patchy kSZ power from our posterior depends on the commonly-used parameters of reionization. For a given midpoint and duration, the EoR morphology only has a few percent impact on the patchy kSZ power in our posterior. However, a physical model is needed to obtain tight constraints from the current low S/N patchy kSZ measurement, as it allows us to take advantage of complimentary high-$z$ observations. Future high S/N detections of the patchy kSZ should decrease the current uncertainties on the timing of the EoR by factors of $\sim$2 - 3.

**Link**: [arxiv](http://arxiv.org/abs/2307.01265v2),  [pdf](http://arxiv.org/pdf/2307.01265v2)

**Tags**: astro-ph.CO astro-ph.GA 



### Hierarchical Narrative Analysis: Unraveling Perceptions of Generative AI
**Authors**: Riona Matsuoka, Hiroki Matsumoto, Takahiro Yoshida, Tomohiro Watanabe, Ryoma Kondo, Ryohei Hisano

**Updated**: 2024-11-11T12:50:44Z

**Summary**: Written texts reflect an author's perspective, making the thorough analysis of literature a key research method in fields such as the humanities and social sciences. However, conventional text mining techniques like sentiment analysis and topic modeling are limited in their ability to capture the hierarchical narrative structures that reveal deeper argumentative patterns. To address this gap, we propose a method that leverages large language models (LLMs) to extract and organize these structures into a hierarchical framework. We validate this approach by analyzing public opinions on generative AI collected by Japan's Agency for Cultural Affairs, comparing the narratives of supporters and critics. Our analysis provides clearer visualization of the factors influencing divergent opinions on generative AI, offering deeper insights into the structures of agreement and disagreement.

**Link**: [arxiv](http://arxiv.org/abs/2409.11032v3),  [pdf](http://arxiv.org/pdf/2409.11032v3)

**Tags**: cs.CL 



### Rotated Runtime Smooth: Training-Free Activation Smoother for accurate   INT4 inference
**Authors**: Ke Yi, Zengke Liu, Jianwei Zhang, Chengyuan Li, Tong Zhang, Junyang Lin, Jingren Zhou

**Updated**: 2024-11-11T12:45:51Z

**Summary**: Large language models have demonstrated promising capabilities upon scaling up parameters. However, serving large language models incurs substantial computation and memory movement costs due to their large scale. Quantization methods have been employed to reduce service costs and latency. Nevertheless, outliers in activations hinder the development of INT4 weight-activation quantization. Existing approaches separate outliers and normal values into two matrices or migrate outliers from activations to weights, suffering from high latency or accuracy degradation. Based on observing activations from large language models, outliers can be classified into channel-wise and spike outliers. In this work, we propose Rotated Runtime Smooth (RRS), a plug-and-play activation smoother for quantization, consisting of Runtime Smooth and the Rotation operation. Runtime Smooth (RS) is introduced to eliminate channel-wise outliers by smoothing activations with channel-wise maximums during runtime. The rotation operation can narrow the gap between spike outliers and normal values, alleviating the effect of victims caused by channel-wise smoothing. The proposed method outperforms the state-of-the-art method in the LLaMA and Qwen families and improves WikiText-2 perplexity from 57.33 to 6.66 for INT4 inference.

**Link**: [arxiv](http://arxiv.org/abs/2409.20361v2),  [pdf](http://arxiv.org/pdf/2409.20361v2)

**Tags**: cs.LG cs.AI 



### Multi-modal Iterative and Deep Fusion Frameworks for Enhanced Passive   DOA Sensing via a Green Massive H2AD MIMO Receiver
**Authors**: Jiatong Bai, Minghao Chen, Wankai Tang, Yifan Li, Cunhua Pan, Yongpeng Wu, Feng Shu

**Updated**: 2024-11-11T12:32:18Z

**Summary**: Most existing DOA estimation methods assume ideal source incident angles with minimal noise. Moreover, directly using pre-estimated angles to calculate weighted coefficients can lead to performance loss. Thus, a green multi-modal (MM) fusion DOA framework is proposed to realize a more practical, low-cost and high time-efficiency DOA estimation for a H$^2$AD array. Firstly, two more efficient clustering methods, global maximum cos\_similarity clustering (GMaxCS) and global minimum distance clustering (GMinD), are presented to infer more precise true solutions from the candidate solution sets. Based on this, an iteration weighted fusion (IWF)-based method is introduced to iteratively update weighted fusion coefficients and the clustering center of the true solution classes by using the estimated values. Particularly, the coarse DOA calculated by fully digital (FD) subarray, serves as the initial cluster center. The above process yields two methods called MM-IWF-GMaxCS and MM-IWF-GMinD. To further provide a higher-accuracy DOA estimation, a fusion network (fusionNet) is proposed to aggregate the inferred two-part true angles and thus generates two effective approaches called MM-fusionNet-GMaxCS and MM-fusionNet-GMinD. The simulation outcomes show the proposed four approaches can achieve the ideal DOA performance and the CRLB. Meanwhile, proposed MM-fusionNet-GMaxCS and MM-fusionNet-GMinD exhibit superior DOA performance compared to MM-IWF-GMaxCS and MM-IWF-GMinD, especially in extremely-low SNR range.

**Link**: [arxiv](http://arxiv.org/abs/2411.06927v1),  [pdf](http://arxiv.org/pdf/2411.06927v1)

**Tags**: cs.AI 



### Detecting Filamentarity in Climate and Galactic Spatial Point Processes
**Authors**: Aida Gjoka, Robin Henderson, Paul Oman

**Updated**: 2024-11-11T12:28:25Z

**Summary**: Evidence of excess filamentarity is considered for two spatial point process applications: local minima in whole earth precipitation modelling and locations of cold clumps in the Milky Way. A diagnostic test using the number of aligned triads and tetrads is developed. A Poisson filament process is proposed based on a parent Poisson process with correlated random walk offspring locations. Filaments are initially identified using an arc search method, with ABC for subsequent inference. Simulations indicate good performance. In both applications there is strong evidence of filamentarity. The method successfully identifies two outlying precipitation data sets.

**Link**: [arxiv](http://arxiv.org/abs/2411.06923v1),  [pdf](http://arxiv.org/pdf/2411.06923v1)

**Tags**: stat.AP 62P12 



### Gaussian Process Emulators for Few-Shot Segmentation in Cardiac MRI
**Authors**: Bruno Viti, Franz Thaler, Kathrin Lisa Kapper, Martin Urschler, Martin Holler, Elias Karabelas

**Updated**: 2024-11-11T12:13:58Z

**Summary**: Segmentation of cardiac magnetic resonance images (MRI) is crucial for the analysis and assessment of cardiac function, helping to diagnose and treat various cardiovascular diseases. Most recent techniques rely on deep learning and usually require an extensive amount of labeled data. To overcome this problem, few-shot learning has the capability of reducing data dependency on labeled data. In this work, we introduce a new method that merges few-shot learning with a U-Net architecture and Gaussian Process Emulators (GPEs), enhancing data integration from a support set for improved performance. GPEs are trained to learn the relation between the support images and the corresponding masks in latent space, facilitating the segmentation of unseen query images given only a small labeled support set at inference. We test our model with the M&Ms-2 public dataset to assess its ability to segment the heart in cardiac magnetic resonance imaging from different orientations, and compare it with state-of-the-art unsupervised and few-shot methods. Our architecture shows higher DICE coefficients compared to these methods, especially in the more challenging setups where the size of the support set is considerably small.

**Link**: [arxiv](http://arxiv.org/abs/2411.06911v1),  [pdf](http://arxiv.org/pdf/2411.06911v1)

**Tags**: cs.CV cs.AI cs.LG 



### Deep Riemannian Networks for End-to-End EEG Decoding
**Authors**: Daniel Wilson, Robin Tibor Schirrmeister, Lukas Alexander Wilhelm Gemein, Tonio Ball

**Updated**: 2024-11-11T12:05:10Z

**Summary**: State-of-the-art performance in electroencephalography (EEG) decoding tasks is currently often achieved with either Deep-Learning (DL) or Riemannian-Geometry-based decoders (RBDs). Recently, there is growing interest in Deep Riemannian Networks (DRNs) possibly combining the advantages of both previous classes of methods. However, there are still a range of topics where additional insight is needed to pave the way for a more widespread application of DRNs in EEG. These include architecture design questions such as network size and end-to-end ability. How these factors affect model performance has not been explored. Additionally, it is not clear how the data within these networks is transformed, and whether this would correlate with traditional EEG decoding. Our study aims to lay the groundwork in the area of these topics through the analysis of DRNs for EEG with a wide range of hyperparameters. Networks were tested on five public EEG datasets and compared with state-of-the-art ConvNets.   Here we propose EE(G)-SPDNet, and we show that this wide, end-to-end DRN can outperform the ConvNets, and in doing so use physiologically plausible frequency regions. We also show that the end-to-end approach learns more complex filters than traditional band-pass filters targeting the classical alpha, beta, and gamma frequency bands of the EEG, and that performance can benefit from channel specific filtering approaches. Additionally, architectural analysis revealed areas for further improvement due to the possible under utilisation of Riemannian specific information throughout the network. Our study thus shows how to design and train DRNs to infer task-related information from the raw EEG without the need of handcrafted filterbanks and highlights the potential of end-to-end DRNs such as EE(G)-SPDNet for high-performance EEG decoding.

**Link**: [arxiv](http://arxiv.org/abs/2212.10426v7),  [pdf](http://arxiv.org/pdf/2212.10426v7)

**Tags**: cs.LG eess.SP stat.ML 



### CTIBench: A Benchmark for Evaluating LLMs in Cyber Threat Intelligence
**Authors**: Md Tanvirul Alam, Dipkamal Bhusal, Le Nguyen, Nidhi Rastogi

**Updated**: 2024-11-11T12:00:35Z

**Summary**: Cyber threat intelligence (CTI) is crucial in today's cybersecurity landscape, providing essential insights to understand and mitigate the ever-evolving cyber threats. The recent rise of Large Language Models (LLMs) have shown potential in this domain, but concerns about their reliability, accuracy, and hallucinations persist. While existing benchmarks provide general evaluations of LLMs, there are no benchmarks that address the practical and applied aspects of CTI-specific tasks. To bridge this gap, we introduce CTIBench, a benchmark designed to assess LLMs' performance in CTI applications. CTIBench includes multiple datasets focused on evaluating knowledge acquired by LLMs in the cyber-threat landscape. Our evaluation of several state-of-the-art models on these tasks provides insights into their strengths and weaknesses in CTI contexts, contributing to a better understanding of LLM capabilities in CTI.

**Link**: [arxiv](http://arxiv.org/abs/2406.07599v3),  [pdf](http://arxiv.org/pdf/2406.07599v3)

**Tags**: cs.CR cs.AI 



### LongSafetyBench: Long-Context LLMs Struggle with Safety Issues
**Authors**: Mianqiu Huang, Xiaoran Liu, Shaojun Zhou, Mozhi Zhang, Chenkun Tan, Pengyu Wang, Qipeng Guo, Zhe Xu, Linyang Li, Zhikai Lei, Linlin Li, Qun Liu, Yaqian Zhou, Xipeng Qiu, Xuanjing Huang

**Updated**: 2024-11-11T11:57:37Z

**Summary**: With the development of large language models (LLMs), the sequence length of these models continues to increase, drawing significant attention to long-context language models. However, the evaluation of these models has been primarily limited to their capabilities, with a lack of research focusing on their safety. Existing work, such as ManyShotJailbreak, has to some extent demonstrated that long-context language models can exhibit safety concerns. However, the methods used are limited and lack comprehensiveness. In response, we introduce \textbf{LongSafetyBench}, the first benchmark designed to objectively and comprehensively evaluate the safety of long-context models. LongSafetyBench consists of 10 task categories, with an average length of 41,889 words. After testing eight long-context language models on LongSafetyBench, we found that existing models generally exhibit insufficient safety capabilities. The proportion of safe responses from most mainstream long-context LLMs is below 50\%. Moreover, models' safety performance in long-context scenarios does not always align with that in short-context scenarios. Further investigation revealed that long-context models tend to overlook harmful content within lengthy texts. We also proposed a simple yet effective solution, allowing open-source models to achieve performance comparable to that of top-tier closed-source models. We believe that LongSafetyBench can serve as a valuable benchmark for evaluating the safety capabilities of long-context language models. We hope that our work will encourage the broader community to pay attention to the safety of long-context models and contribute to the development of solutions to improve the safety of long-context LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2411.06899v1),  [pdf](http://arxiv.org/pdf/2411.06899v1)

**Tags**: cs.CL cs.AI cs.LG 



### Meaningful Learning: Enhancing Abstract Reasoning in Large Language   Models via Generic Fact Guidance
**Authors**: Kai Xiong, Xiao Ding, Ting Liu, Bing Qin, Dongliang Xu, Qing Yang, Hongtao Liu, Yixin Cao

**Updated**: 2024-11-11T11:35:28Z

**Summary**: Large language models (LLMs) have developed impressive performance and strong explainability across various reasoning scenarios, marking a significant stride towards mimicking human-like intelligence. Despite this, when tasked with several simple questions supported by a generic fact, LLMs often struggle to abstract and apply the generic fact to provide consistent and precise answers, revealing a deficiency in abstract reasoning abilities. This has sparked a vigorous debate about whether LLMs are genuinely reasoning or merely memorizing. In light of this, we design a preliminary study to quantify and delve into the abstract reasoning abilities of existing LLMs. Our findings reveal a substantial discrepancy between their general reasoning and abstract reasoning performances. To relieve this problem, we tailor an abstract reasoning dataset (AbsR) together with a meaningful learning paradigm to teach LLMs how to leverage generic facts for reasoning purposes. The results show that our approach not only boosts the general reasoning performance of LLMs but also makes considerable strides towards their capacity for abstract reasoning, moving beyond simple memorization or imitation to a more nuanced understanding and application of generic facts. The code is available at https://github.com/Waste-Wood/MeanLearn.

**Link**: [arxiv](http://arxiv.org/abs/2403.09085v2),  [pdf](http://arxiv.org/pdf/2403.09085v2)

**Tags**: cs.CL cs.AI 



### LMLPA: Language Model Linguistic Personality Assessment
**Authors**: Jingyao Zheng, Xian Wang, Simo Hosio, Xiaoxian Xu, Lik-Hang Lee

**Updated**: 2024-11-11T11:32:21Z

**Summary**: Large Language Models (LLMs) are increasingly used in everyday life and research. One of the most common use cases is conversational interactions, enabled by the language generation capabilities of LLMs. Just as between two humans, a conversation between an LLM-powered entity and a human depends on the personality of the conversants. However, measuring the personality of a given LLM is currently a challenge. This paper introduces the Language Model Linguistic Personality Assessment (LMLPA), a system designed to evaluate the linguistic personalities of LLMs. Our system helps to understand LLMs' language generation capabilities by quantitatively assessing the distinct personality traits reflected in their linguistic outputs. Unlike traditional human-centric psychometrics, the LMLPA adapts a personality assessment questionnaire, specifically the Big Five Inventory, to align with the operational capabilities of LLMs, and also incorporates the findings from previous language-based personality measurement literature. To mitigate sensitivity to the order of options, our questionnaire is designed to be open-ended, resulting in textual answers. Thus, the AI rater is needed to transform ambiguous personality information from text responses into clear numerical indicators of personality traits. Utilising Principal Component Analysis and reliability validations, our findings demonstrate that LLMs possess distinct personality traits that can be effectively quantified by the LMLPA. This research contributes to Human-Computer Interaction and Human-Centered AI, providing a robust framework for future studies to refine AI personality assessments and expand their applications in multiple areas, including education and manufacturing.

**Link**: [arxiv](http://arxiv.org/abs/2410.17632v2),  [pdf](http://arxiv.org/pdf/2410.17632v2)

**Tags**: cs.CL cs.AI I.2 



### LLM-Assisted Relevance Assessments: When Should We Ask LLMs for Help?
**Authors**: Rikiya Takehi, Ellen M. Voorhees, Tetsuya Sakai

**Updated**: 2024-11-11T11:17:35Z

**Summary**: Test collections are information retrieval tools that allow researchers to quickly and easily evaluate ranking algorithms. While test collections have become an integral part of IR research, the process of data creation involves significant efforts in manual annotations, which often makes it very expensive and time-consuming. Thus, the test collections could become small when the budget is limited, which may lead to unstable evaluations. As an alternative, recent studies have proposed the use of large language models (LLMs) to completely replace human assessors. However, while LLMs seem to somewhat correlate with human judgments, they are not perfect and often show bias. Moreover, even if a well-performing LLM or prompt is found on one dataset, there is no guarantee that it will perform similarly in practice, due to difference in tasks and data. Thus a complete replacement with LLMs is argued to be too risky and not fully trustable.   Thus, in this paper, we propose \textbf{L}LM-\textbf{A}ssisted \textbf{R}elevance \textbf{A}ssessments (\textbf{LARA}), an effective method to balance manual annotations with LLM annotations, which helps to make a rich and reliable test collection. We use the LLM's predicted relevance probabilities in order to select the most profitable documents to manually annotate under a budget constraint. While solely relying on LLM's predicted probabilities to manually annotate performs fairly well, with theoretical reasoning, LARA guides the human annotation process even more effectively via online calibration learning. Then, using the calibration model learned from the limited manual annotations, LARA debiases the LLM predictions to annotate the remaining non-assessed data. Empirical evaluations on TREC-COVID and TREC-8 Ad Hoc datasets show that LARA outperforms the alternative solutions under almost any budget constraint.

**Link**: [arxiv](http://arxiv.org/abs/2411.06877v1),  [pdf](http://arxiv.org/pdf/2411.06877v1)

**Tags**: cs.IR 



### CapeLLM: Support-Free Category-Agnostic Pose Estimation with Multimodal   Large Language Models
**Authors**: Junho Kim, Hyungjin Chung, Byung-Hoon Kim

**Updated**: 2024-11-11T11:08:26Z

**Summary**: Category-agnostic pose estimation (CAPE) has traditionally relied on support images with annotated keypoints, a process that is often cumbersome and may fail to fully capture the necessary correspondences across diverse object categories. Recent efforts have begun exploring the use of text-based queries, where the need for support keypoints is eliminated. However, the optimal use of textual descriptions for keypoints remains an underexplored area. In this work, we introduce CapeLLM, a novel approach that leverages a text-based multimodal large language model (MLLM) for CAPE. Our method only employs query image and detailed text descriptions as an input to estimate category-agnostic keypoints. We conduct extensive experiments to systematically explore the design space of LLM-based CAPE, investigating factors such as choosing the optimal description for keypoints, neural network architectures, and training strategies. Thanks to the advanced reasoning capabilities of the pre-trained MLLM, CapeLLM demonstrates superior generalization and robust performance. Our approach sets a new state-of-the-art on the MP-100 benchmark in the challenging 1-shot setting, marking a significant advancement in the field of category-agnostic pose estimation.

**Link**: [arxiv](http://arxiv.org/abs/2411.06869v1),  [pdf](http://arxiv.org/pdf/2411.06869v1)

**Tags**: cs.CV cs.LG 



### Capturing research literature attitude towards Sustainable Development   Goals: an LLM-based topic modeling approach
**Authors**: Francesco Invernici, Francesca Curati, Jelena Jakimov, Amirhossein Samavi, Anna Bernasconi

**Updated**: 2024-11-11T10:51:31Z

**Summary**: The world is facing a multitude of challenges that hinder the development of human civilization and the well-being of humanity on the planet. The Sustainable Development Goals (SDGs) were formulated by the United Nations in 2015 to address these global challenges by 2030. Natural language processing techniques can help uncover discussions on SDGs within research literature. We propose a completely automated pipeline to 1) fetch content from the Scopus database and prepare datasets dedicated to five groups of SDGs; 2) perform topic modeling, a statistical technique used to identify topics in large collections of textual data; and 3) enable topic exploration through keywords-based search and topic frequency time series extraction. For topic modeling, we leverage the stack of BERTopic scaled up to be applied on large corpora of textual documents (we find hundreds of topics on hundreds of thousands of documents), introducing i) a novel LLM-based embeddings computation for representing scientific abstracts in the continuous space and ii) a hyperparameter optimizer to efficiently find the best configuration for any new big datasets. We additionally produce the visualization of results on interactive dashboards reporting topics' temporal evolution. Results are made inspectable and explorable, contributing to the interpretability of the topic modeling process. Our proposed LLM-based topic modeling pipeline for big-text datasets allows users to capture insights on the evolution of the attitude toward SDGs within scientific abstracts in the 2006-2023 time span. All the results are reproducible by using our system; the workflow can be generalized to be applied at any point in time to any big corpus of textual documents.

**Link**: [arxiv](http://arxiv.org/abs/2411.02943v2),  [pdf](http://arxiv.org/pdf/2411.02943v2)

**Tags**: cs.CL 



### EHRNoteQA: An LLM Benchmark for Real-World Clinical Practice Using   Discharge Summaries
**Authors**: Sunjun Kweon, Jiyoun Kim, Heeyoung Kwak, Dongchul Cha, Hangyul Yoon, Kwanghyun Kim, Jeewon Yang, Seunghyun Won, Edward Choi

**Updated**: 2024-11-11T10:40:50Z

**Summary**: Discharge summaries in Electronic Health Records (EHRs) are crucial for clinical decision-making, but their length and complexity make information extraction challenging, especially when dealing with accumulated summaries across multiple patient admissions. Large Language Models (LLMs) show promise in addressing this challenge by efficiently analyzing vast and complex data. Existing benchmarks, however, fall short in properly evaluating LLMs' capabilities in this context, as they typically focus on single-note information or limited topics, failing to reflect the real-world inquiries required by clinicians. To bridge this gap, we introduce EHRNoteQA, a novel benchmark built on the MIMIC-IV EHR, comprising 962 different QA pairs each linked to distinct patients' discharge summaries. Every QA pair is initially generated using GPT-4 and then manually reviewed and refined by three clinicians to ensure clinical relevance. EHRNoteQA includes questions that require information across multiple discharge summaries and covers eight diverse topics, mirroring the complexity and diversity of real clinical inquiries. We offer EHRNoteQA in two formats: open-ended and multi-choice question answering, and propose a reliable evaluation method for each. We evaluate 27 LLMs using EHRNoteQA and examine various factors affecting the model performance (e.g., the length and number of discharge summaries). Furthermore, to validate EHRNoteQA as a reliable proxy for expert evaluations in clinical practice, we measure the correlation between the LLM performance on EHRNoteQA, and the LLM performance manually evaluated by clinicians. Results show that LLM performance on EHRNoteQA have higher correlation with clinician-evaluated performance (Spearman: 0.78, Kendall: 0.62) compared to other benchmarks, demonstrating its practical relevance in evaluating LLMs in clinical settings.

**Link**: [arxiv](http://arxiv.org/abs/2402.16040v5),  [pdf](http://arxiv.org/pdf/2402.16040v5)

**Tags**: cs.CL 



### Evaluating Large Language Models on Financial Report Summarization: An   Empirical Study
**Authors**: Xinqi Yang, Scott Zang, Yong Ren, Dingjie Peng, Zheng Wen

**Updated**: 2024-11-11T10:36:04Z

**Summary**: In recent years, Large Language Models (LLMs) have demonstrated remarkable versatility across various applications, including natural language understanding, domain-specific knowledge tasks, etc. However, applying LLMs to complex, high-stakes domains like finance requires rigorous evaluation to ensure reliability, accuracy, and compliance with industry standards. To address this need, we conduct a comprehensive and comparative study on three state-of-the-art LLMs, GLM-4, Mistral-NeMo, and LLaMA3.1, focusing on their effectiveness in generating automated financial reports. Our primary motivation is to explore how these models can be harnessed within finance, a field demanding precision, contextual relevance, and robustness against erroneous or misleading information. By examining each model's capabilities, we aim to provide an insightful assessment of their strengths and limitations. Our paper offers benchmarks for financial report analysis, encompassing proposed metrics such as ROUGE-1, BERT Score, and LLM Score. We introduce an innovative evaluation framework that integrates both quantitative metrics (e.g., precision, recall) and qualitative analyses (e.g., contextual fit, consistency) to provide a holistic view of each model's output quality. Additionally, we make our financial dataset publicly available, inviting researchers and practitioners to leverage, scrutinize, and enhance our findings through broader community engagement and collaborative improvement. Our dataset is available on huggingface.

**Link**: [arxiv](http://arxiv.org/abs/2411.06852v1),  [pdf](http://arxiv.org/pdf/2411.06852v1)

**Tags**: cs.CL cs.AI 



### Fast and Efficient Transformer-based Method for Bird's Eye View Instance   Prediction
**Authors**: Miguel Antunes-Garca, Luis M. Bergasa, Santiago Montiel-Marn, Rafael Barea, Fabio Snchez-Garca, ngel Llamazares

**Updated**: 2024-11-11T10:35:23Z

**Summary**: Accurate object detection and prediction are critical to ensure the safety and efficiency of self-driving architectures. Predicting object trajectories and occupancy enables autonomous vehicles to anticipate movements and make decisions with future information, increasing their adaptability and reducing the risk of accidents. Current State-Of-The-Art (SOTA) approaches often isolate the detection, tracking, and prediction stages, which can lead to significant prediction errors due to accumulated inaccuracies between stages. Recent advances have improved the feature representation of multi-camera perception systems through Bird's-Eye View (BEV) transformations, boosting the development of end-to-end systems capable of predicting environmental elements directly from vehicle sensor data. These systems, however, often suffer from high processing times and number of parameters, creating challenges for real-world deployment. To address these issues, this paper introduces a novel BEV instance prediction architecture based on a simplified paradigm that relies only on instance segmentation and flow prediction. The proposed system prioritizes speed, aiming at reduced parameter counts and inference times compared to existing SOTA architectures, thanks to the incorporation of an efficient transformer-based architecture. Furthermore, the implementation of the proposed architecture is optimized for performance improvements in PyTorch version 2.1. Code and trained models are available at https://github.com/miguelag99/Efficient-Instance-Prediction

**Link**: [arxiv](http://arxiv.org/abs/2411.06851v1),  [pdf](http://arxiv.org/pdf/2411.06851v1)

**Tags**: cs.CV cs.LG 



### 1-800-SHARED-TASKS @ NLU of Devanagari Script Languages: Detection of   Language, Hate Speech, and Targets using LLMs
**Authors**: Jebish Purbey, Siddartha Pullakhandam, Kanwal Mehreen, Muhammad Arham, Drishti Sharma, Ashay Srivastava, Ram Mohan Rao Kadiyala

**Updated**: 2024-11-11T10:34:36Z

**Summary**: This paper presents a detailed system description of our entry for the CHiPSAL 2025 shared task, focusing on language detection, hate speech identification, and target detection in Devanagari script languages. We experimented with a combination of large language models and their ensembles, including MuRIL, IndicBERT, and Gemma-2, and leveraged unique techniques like focal loss to address challenges in the natural understanding of Devanagari languages, such as multilingual processing and class imbalance. Our approach achieved competitive results across all tasks: F1 of 0.9980, 0.7652, and 0.6804 for Sub-tasks A, B, and C respectively. This work provides insights into the effectiveness of transformer models in tasks with domain-specific and linguistic challenges, as well as areas for potential improvement in future iterations.

**Link**: [arxiv](http://arxiv.org/abs/2411.06850v1),  [pdf](http://arxiv.org/pdf/2411.06850v1)

**Tags**: cs.CL cs.AI cs.LG 



### LLM-Neo: Parameter Efficient Knowledge Distillation for Large Language   Models
**Authors**: Runming Yang, Taiqiang Wu, Jiahao Wang, Pengfei Hu, Ngai Wong, Yujiu Yang

**Updated**: 2024-11-11T10:07:51Z

**Summary**: In this paper, we propose a novel LLM-Neo framework that efficiently transfers knowledge from a large language model (LLM) teacher to a compact student. Initially, we revisit the knowledge distillation (KD) and low-rank adaption (LoRA), and argue that they share the same paradigm. Inspired by this observation, we explore the strategy that combines LoRA and KD to enhance the efficiency of knowledge transfer. We first summarize some guidelines for this design and further develop the LLM-Neo. Experimental results on compressing Llama 2 and Llama 3 show that LLM-Neo outperforms various baselines. Further analysis demonstrates the robustness of the proposed LLM-Neo on variants of LoRA. The trained models have been available at \href{https://huggingface.co/collections/yang31210999/llm-neo-66e3c882f5579b829ff57eba}{this repository}.

**Link**: [arxiv](http://arxiv.org/abs/2411.06839v1),  [pdf](http://arxiv.org/pdf/2411.06839v1)

**Tags**: cs.CL cs.AI cs.LG 



### Persuasion with Large Language Models: a Survey
**Authors**: Alexander Rogiers, Sander Noels, Maarten Buyl, Tijl De Bie

**Updated**: 2024-11-11T10:05:52Z

**Summary**: The rapid rise of Large Language Models (LLMs) has created new disruptive possibilities for persuasive communication, by enabling fully-automated personalized and interactive content generation at an unprecedented scale. In this paper, we survey the research field of LLM-based persuasion that has emerged as a result. We begin by exploring the different modes in which LLM Systems are used to influence human attitudes and behaviors. In areas such as politics, marketing, public health, e-commerce, and charitable giving, such LLM Systems have already achieved human-level or even super-human persuasiveness. We identify key factors influencing their effectiveness, such as the manner of personalization and whether the content is labelled as AI-generated. We also summarize the experimental designs that have been used to evaluate progress. Our survey suggests that the current and future potential of LLM-based persuasion poses profound ethical and societal risks, including the spread of misinformation, the magnification of biases, and the invasion of privacy. These risks underscore the urgent need for ethical guidelines and updated regulatory frameworks to avoid the widespread deployment of irresponsible and harmful LLM Systems.

**Link**: [arxiv](http://arxiv.org/abs/2411.06837v1),  [pdf](http://arxiv.org/pdf/2411.06837v1)

**Tags**: cs.CL 



### Multimodal Structure-Aware Quantum Data Processing
**Authors**: Hala Hawashin, Mehrnoosh Sadrzadeh

**Updated**: 2024-11-11T10:03:47Z

**Summary**: While large language models (LLMs) have advanced the field of natural language processing (NLP), their "black box" nature obscures their decision-making processes. To address this, researchers developed structured approaches using higher order tensors. These are able to model linguistic relations, but stall when training on classical computers due to their excessive size. Tensors are natural inhabitants of quantum systems and training on quantum computers provides a solution by translating text to variational quantum circuits. In this paper, we develop MultiQ-NLP: a framework for structure-aware data processing with multimodal text+image data. Here, "structure" refers to syntactic and grammatical relationships in language, as well as the hierarchical organization of visual elements in images. We enrich the translation with new types and type homomorphisms and develop novel architectures to represent structure. When tested on a main stream image classification task (SVO Probes), our best model showed a par performance with the state of the art classical models; moreover the best model was fully structured.

**Link**: [arxiv](http://arxiv.org/abs/2411.04242v3),  [pdf](http://arxiv.org/pdf/2411.04242v3)

**Tags**: cs.LG 68T45, 68T50, 68Q12, 68U15, 68U10, 81P45, 81P68 I.2.7; I.2.10; H.5.1 



### HarmLevelBench: Evaluating Harm-Level Compliance and the Impact of   Quantization on Model Alignment
**Authors**: Yannis Belkhiter, Giulio Zizzo, Sergio Maffeis

**Updated**: 2024-11-11T10:02:49Z

**Summary**: With the introduction of the transformers architecture, LLMs have revolutionized the NLP field with ever more powerful models. Nevertheless, their development came up with several challenges. The exponential growth in computational power and reasoning capabilities of language models has heightened concerns about their security. As models become more powerful, ensuring their safety has become a crucial focus in research. This paper aims to address gaps in the current literature on jailbreaking techniques and the evaluation of LLM vulnerabilities. Our contributions include the creation of a novel dataset designed to assess the harmfulness of model outputs across multiple harm levels, as well as a focus on fine-grained harm-level analysis. Using this framework, we provide a comprehensive benchmark of state-of-the-art jailbreaking attacks, specifically targeting the Vicuna 13B v1.5 model. Additionally, we examine how quantization techniques, such as AWQ and GPTQ, influence the alignment and robustness of models, revealing trade-offs between enhanced robustness with regards to transfer attacks and potential increases in vulnerability on direct ones. This study aims to demonstrate the influence of harmful input queries on the complexity of jailbreaking techniques, as well as to deepen our understanding of LLM vulnerabilities and improve methods for assessing model robustness when confronted with harmful content, particularly in the context of compression strategies.

**Link**: [arxiv](http://arxiv.org/abs/2411.06835v1),  [pdf](http://arxiv.org/pdf/2411.06835v1)

**Tags**: cs.CL cs.CR 



### Entity Extraction from High-Level Corruption Schemes via Large Language   Models
**Authors**: Panagiotis Koletsis, Panagiotis-Konstantinos Gemos, Christos Chronis, Iraklis Varlamis, Vasilis Efthymiou, Georgios Th. Papadopoulos

**Updated**: 2024-11-11T10:02:24Z

**Summary**: The rise of financial crime that has been observed in recent years has created an increasing concern around the topic and many people, organizations and governments are more and more frequently trying to combat it. Despite the increase of interest in this area, there is a lack of specialized datasets that can be used to train and evaluate works that try to tackle those problems. This article proposes a new micro-benchmark dataset for algorithms and models that identify individuals and organizations, and their multiple writings, in news articles, and presents an approach that assists in its creation. Experimental efforts are also reported, using this dataset, to identify individuals and organizations in financial-crime-related articles using various low-billion parameter Large Language Models (LLMs). For these experiments, standard metrics (Accuracy, Precision, Recall, F1 Score) are reported and various prompt variants comprising the best practices of prompt engineering are tested. In addition, to address the problem of ambiguous entity mentions, a simple, yet effective LLM-based disambiguation method is proposed, ensuring that the evaluation aligns with reality. Finally, the proposed approach is compared against a widely used state-of-the-art open-source baseline, showing the superiority of the proposed method.

**Link**: [arxiv](http://arxiv.org/abs/2409.13704v2),  [pdf](http://arxiv.org/pdf/2409.13704v2)

**Tags**: cs.CL cs.IR 



### Self-Organizing State-Space Models with Artificial Dynamics
**Authors**: Yuan Chen, Mathieu Gerber, Christophe Andrieu, Randal Douc

**Updated**: 2024-11-11T09:56:59Z

**Summary**: We consider a state-space model (SSM) parametrized by some parameter $\theta$ and aim at performing joint parameter and state inference. A popular idea to carry out this task is to replace $\theta$ by a Markov chain $(\theta_t)_{t\geq 0}$ and then to apply a filtering algorithm to the extended, or self-organizing SSM (SO-SSM). However, the practical implementation of this idea in a theoretically justified way has remained an open problem. In this paper we fill this gap by introducing constructions of $(\theta_t)_{t\geq 0}$ that ensure the validity of the SO-SSM for joint parameter and state inference. Notably, we show that such SO-SSMs can be defined even if $\|\mathrm{Var}(\theta_{t}|\theta_{t-1})\|\rightarrow 0$ slowly as $t\rightarrow\infty$. This result is important since these models can be efficiently approximated using a particle filter. While SO-SSMs have been introduced for online inference, the development of iterated filtering (IF) has shown that they can also serve for computing the maximum likelihood estimator of an SSM. We also derive constructions of $(\theta_t)_{t\geq 0}$ and theoretical guarantees tailored to these specific applications of SO-SSMs and introduce new IF algorithms. From a practical point of view, the algorithms we develop are simple to implement and only require minimal tuning to perform well.

**Link**: [arxiv](http://arxiv.org/abs/2409.08928v4),  [pdf](http://arxiv.org/pdf/2409.08928v4)

**Tags**: math.ST stat.CO stat.ME stat.TH 



### Learning Interpretable Network Dynamics via Universal Neural Symbolic   Regression
**Authors**: Jiao Hu, Jiaxu Cui, Bo Yang

**Updated**: 2024-11-11T09:51:22Z

**Summary**: Discovering governing equations of complex network dynamics is a fundamental challenge in contemporary science with rich data, which can uncover the mysterious patterns and mechanisms of the formation and evolution of complex phenomena in various fields and assist in decision-making. In this work, we develop a universal computational tool that can automatically, efficiently, and accurately learn the symbolic changing patterns of complex system states by combining the excellent fitting ability from deep learning and the equation inference ability from pre-trained symbolic regression. We conduct intensive experimental verifications on more than ten representative scenarios from physics, biochemistry, ecology, epidemiology, etc. Results demonstrate the outstanding effectiveness and efficiency of our tool by comparing with the state-of-the-art symbolic regression techniques for network dynamics. The application to real-world systems including global epidemic transmission and pedestrian movements has verified its practical applicability. We believe that our tool can serve as a universal solution to dispel the fog of hidden mechanisms of changes in complex phenomena, advance toward interpretability, and inspire more scientific discoveries.

**Link**: [arxiv](http://arxiv.org/abs/2411.06833v1),  [pdf](http://arxiv.org/pdf/2411.06833v1)

**Tags**: cs.AI cs.LG cs.MA cs.SC 



### Test-Time Training with Quantum Auto-Encoder: From Distribution Shift to   Noisy Quantum Circuits
**Authors**: Damien Jian, Yu-Chao Huang, Hsi-Sheng Goan

**Updated**: 2024-11-11T09:40:36Z

**Summary**: In this paper, we propose test-time training with the quantum auto-encoder (QTTT). QTTT adapts to (1) data distribution shifts between training and testing data and (2) quantum circuit error by minimizing the self-supervised loss of the quantum auto-encoder. Empirically, we show that QTTT is robust against data distribution shifts and effective in mitigating random unitary noise in the quantum circuits during the inference. Additionally, we establish the theoretical performance guarantee of the QTTT architecture. Our novel framework presents a significant advancement in developing quantum neural networks for future real-world applications and functions as a plug-and-play extension for quantum machine learning models.

**Link**: [arxiv](http://arxiv.org/abs/2411.06828v1),  [pdf](http://arxiv.org/pdf/2411.06828v1)

**Tags**: quant-ph 



### Combining Domain and Alignment Vectors to Achieve Better   Knowledge-Safety Trade-offs in LLMs
**Authors**: Megh Thakkar, Yash More, Quentin Fournier, Matthew Riemer, Pin-Yu Chen, Amal Zouaq, Payel Das, Sarath Chandar

**Updated**: 2024-11-11T09:32:20Z

**Summary**: There is a growing interest in training domain-expert LLMs that excel in specific technical fields compared to their general-purpose instruction-tuned counterparts. However, these expert models often experience a loss in their safety abilities in the process, making them capable of generating harmful content. As a solution, we introduce an efficient and effective merging-based alignment method called \textsc{MergeAlign} that interpolates the domain and alignment vectors, creating safer domain-specific models while preserving their utility. We apply \textsc{MergeAlign} on Llama3 variants that are experts in medicine and finance, obtaining substantial alignment improvements with minimal to no degradation on domain-specific benchmarks. We study the impact of model merging through model similarity metrics and contributions of individual models being merged. We hope our findings open new research avenues and inspire more efficient development of safe expert LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2411.06824v1),  [pdf](http://arxiv.org/pdf/2411.06824v1)

**Tags**: cs.AI 



### Large Language Model in Medical Informatics: Direct Classification and   Enhanced Text Representations for Automatic ICD Coding
**Authors**: Zeyd Boukhers, AmeerAli Khan, Qusai Ramadan, Cong Yang

**Updated**: 2024-11-11T09:31:46Z

**Summary**: Addressing the complexity of accurately classifying International Classification of Diseases (ICD) codes from medical discharge summaries is challenging due to the intricate nature of medical documentation. This paper explores the use of Large Language Models (LLM), specifically the LLAMA architecture, to enhance ICD code classification through two methodologies: direct application as a classifier and as a generator of enriched text representations within a Multi-Filter Residual Convolutional Neural Network (MultiResCNN) framework. We evaluate these methods by comparing them against state-of-the-art approaches, revealing LLAMA's potential to significantly improve classification outcomes by providing deep contextual insights into medical texts.

**Link**: [arxiv](http://arxiv.org/abs/2411.06823v1),  [pdf](http://arxiv.org/pdf/2411.06823v1)

**Tags**: cs.LG cs.IR 



### Instruction Tuning for Large Language Models: A Survey
**Authors**: Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang

**Updated**: 2024-11-11T09:25:48Z

**Summary**: This paper surveys research works in the quickly advancing field of instruction tuning (IT), which can also be referred to as supervised fine-tuning (SFT)\footnote{In this paper, unless specified otherwise, instruction tuning (IT) will be equivalent to supervised fine-tuning (SFT).}, a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and application, along with analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research.Project page: github.com/xiaoya-li/Instruction-Tuning-Survey

**Link**: [arxiv](http://arxiv.org/abs/2308.10792v7),  [pdf](http://arxiv.org/pdf/2308.10792v7)

**Tags**: cs.CL cs.AI cs.LG 



### Federated Graph Condensation with Information Bottleneck Principles
**Authors**: Bo Yan, Sihao He, Cheng Yang, Shang Liu, Yang Cao, Chuan Shi

**Updated**: 2024-11-11T09:23:00Z

**Summary**: Graph condensation, which reduces the size of a large-scale graph by synthesizing a small-scale condensed graph as its substitution, has immediately benefited various graph learning tasks. However, existing graph condensation methods rely on centralized data storage, which is unfeasible for real-world decentralized data distribution, and overlook data holders' privacy-preserving requirements. To bridge the gap, we propose and study the novel problem of federated graph condensation for graph neural networks (GNNs). Specifically, we first propose a general framework for federated graph condensation, in which we decouple the typical gradient matching process for graph condensation into client-side gradient calculation and server-side gradient matching. In this way, the burdensome computation cost in client-side is largely alleviated. Besides, our empirical studies show that under the federated setting, the condensed graph will consistently leak data membership privacy, i.e., the condensed graph during the federated training can be utilized to steal the training data under the membership inference attacks (MIA). To tackle this issue, we innovatively incorporate information bottleneck principles into the federated graph condensation, which only needs to extract partial node features in one local pre-training step and utilize the features during federated training. Extensive experiments on real-world datasets demonstrate that our framework can consistently protect membership privacy during training. Meanwhile, it also achieves comparable and even superior performance against existing centralized graph condensation and federated graph learning methods.

**Link**: [arxiv](http://arxiv.org/abs/2405.03911v2),  [pdf](http://arxiv.org/pdf/2405.03911v2)

**Tags**: cs.LG cs.AI cs.CR cs.DC 



### CUDRT: Benchmarking the Detection Models of Human vs. Large Language   Models Generated Texts
**Authors**: Zhen Tao, Yanfang Chen, Dinghao Xi, Zhiyu Li, Wei Xu

**Updated**: 2024-11-11T09:19:46Z

**Summary**: While large language models (LLMs) have greatly enhanced text generation across industries, their human-like outputs make distinguishing between human and AI authorship challenging. Although many LLM-generated text detectors exist, current benchmarks mainly rely on static datasets, limiting their effectiveness in assessing model-based detectors requiring prior training. Furthermore, these benchmarks focus on specific scenarios like question answering and text refinement and are primarily limited to English, overlooking broader linguistic applications and LLM subtleties. To address these gaps, we construct a comprehensive bilingual benchmark in Chinese and English to rigorously evaluate mainstream LLM-generated text detection methods. We categorize LLM text generation into five key operations-Create, Update, Delete, Rewrite, and Translate (CUDRT)-covering the full range of LLM activities. For each CUDRT category, we developed extensive datasets enabling thorough assessment of detection performance, incorporating the latest mainstream LLMs for each language. We also establish a robust evaluation framework to support scalable, reproducible experiments, facilitating an in-depth analysis of how LLM operations, different LLMs, datasets, and multilingual training sets impact detector performance, particularly for model-based methods. Our extensive experiments provide critical insights for optimizing LLM-generated text detectors and suggest future directions to improve detection accuracy and generalization across diverse scenarios.Source code and dataset are available at GitHub.

**Link**: [arxiv](http://arxiv.org/abs/2406.09056v2),  [pdf](http://arxiv.org/pdf/2406.09056v2)

**Tags**: cs.CL cs.AI 



### ALPINE: Unveiling the Planning Capability of Autoregressive Learning in   Language Models
**Authors**: Siwei Wang, Yifei Shen, Shi Feng, Haoran Sun, Shang-Hua Teng, Wei Chen

**Updated**: 2024-11-11T09:16:56Z

**Summary**: Planning is a crucial element of both human intelligence and contemporary large language models (LLMs). In this paper, we initiate a theoretical investigation into the emergence of planning capabilities in Transformer-based LLMs via their next-word prediction mechanisms. We model planning as a network path-finding task, where the objective is to generate a valid path from a specified source node to a designated target node. Our mathematical characterization shows that Transformer architectures can execute path-finding by embedding the adjacency and reachability matrices within their weights. Furthermore, our theoretical analysis of gradient-based learning dynamics reveals that LLMs can learn both the adjacency and a limited form of the reachability matrices. These theoretical insights are then validated through experiments, which demonstrate that Transformer architectures indeed learn the adjacency and an incomplete reachability matrices, consistent with our theoretical predictions. When applying our methodology to the real-world planning benchmark Blocksworld, our observations remain consistent. Additionally, our analyses uncover a fundamental limitation of current Transformer architectures in path-finding: these architectures cannot identify reachability relationships through transitivity, which leads to failures in generating paths when concatenation is required. These findings provide new insights into how the internal mechanisms of autoregressive learning facilitate intelligent planning and deepen our understanding of how future LLMs might achieve more advanced and general planning-and-reasoning capabilities across diverse applications.

**Link**: [arxiv](http://arxiv.org/abs/2405.09220v3),  [pdf](http://arxiv.org/pdf/2405.09220v3)

**Tags**: cs.LG cs.AI cs.CL 



### Wavehax: Aliasing-Free Neural Waveform Synthesis Based on 2D Convolution   and Harmonic Prior for Reliable Complex Spectrogram Estimation
**Authors**: Reo Yoneyama, Atsushi Miyashita, Ryuichi Yamamoto, Tomoki Toda

**Updated**: 2024-11-11T09:03:58Z

**Summary**: Neural vocoders often struggle with aliasing in latent feature spaces, caused by time-domain nonlinear operations and resampling layers. Aliasing folds high-frequency components into the low-frequency range, making aliased and original frequency components indistinguishable and introducing two practical issues. First, aliasing complicates the waveform generation process, as the subsequent layers must address these aliasing effects, increasing the computational complexity. Second, it limits extrapolation performance, particularly in handling high fundamental frequencies, which degrades the perceptual quality of generated speech waveforms. This paper demonstrates that 1) time-domain nonlinear operations inevitably introduce aliasing but provide a strong inductive bias for harmonic generation, and 2) time-frequency-domain processing can achieve aliasing-free waveform synthesis but lacks the inductive bias for effective harmonic generation. Building on this insight, we propose Wavehax, an aliasing-free neural WAVEform generator that integrates 2D convolution and a HArmonic prior for reliable Complex Spectrogram estimation. Experimental results show that Wavehax achieves speech quality comparable to existing high-fidelity neural vocoders and exhibits exceptional robustness in scenarios requiring high fundamental frequency extrapolation, where aliasing effects become typically severe. Moreover, Wavehax requires less than 5% of the multiply-accumulate operations and model parameters compared to HiFi-GAN V1, while achieving over four times faster CPU inference speed.

**Link**: [arxiv](http://arxiv.org/abs/2411.06807v1),  [pdf](http://arxiv.org/pdf/2411.06807v1)

**Tags**: cs.SD eess.AS 



### AssistRAG: Boosting the Potential of Large Language Models with an   Intelligent Information Assistant
**Authors**: Yujia Zhou, Zheng Liu, Zhicheng Dou

**Updated**: 2024-11-11T09:03:52Z

**Summary**: The emergence of Large Language Models (LLMs) has significantly advanced natural language processing, but these models often generate factually incorrect information, known as "hallucination". Initial retrieval-augmented generation (RAG) methods like the "Retrieve-Read" framework was inadequate for complex reasoning tasks. Subsequent prompt-based RAG strategies and Supervised Fine-Tuning (SFT) methods improved performance but required frequent retraining and risked altering foundational LLM capabilities. To cope with these challenges, we propose Assistant-based Retrieval-Augmented Generation (AssistRAG), integrating an intelligent information assistant within LLMs. This assistant manages memory and knowledge through tool usage, action execution, memory building, and plan specification. Using a two-phase training approach, Curriculum Assistant Learning and Reinforced Preference Optimization. AssistRAG enhances information retrieval and decision-making. Experiments show AssistRAG significantly outperforms benchmarks, especially benefiting less advanced LLMs, by providing superior reasoning capabilities and accurate responses.

**Link**: [arxiv](http://arxiv.org/abs/2411.06805v1),  [pdf](http://arxiv.org/pdf/2411.06805v1)

**Tags**: cs.CL cs.AI cs.IR 



### Predicting ionic conductivity in solids from the machine-learned   potential energy landscape
**Authors**: Artem Maevskiy, Alexandra Carvalho, Emil Sataev, Volha Turchyna, Keian Noori, Aleksandr Rodin, A. H. Castro Neto, Andrey Ustyuzhanin

**Updated**: 2024-11-11T09:01:36Z

**Summary**: Discovering new superionic materials is essential for advancing solid-state batteries, which offer improved energy density and safety compared to the traditional lithium-ion batteries with liquid electrolytes. Conventional computational methods for identifying such materials are resource-intensive and not easily scalable. Recently, universal interatomic potential models have been developed using equivariant graph neural networks. These models are trained on extensive datasets of first-principles force and energy calculations. One can achieve significant computational advantages by leveraging them as the foundation for traditional methods of assessing the ionic conductivity, such as molecular dynamics or nudged elastic band techniques. However, the generalization error from model inference on diverse atomic structures arising in such calculations can compromise the reliability of the results. In this work, we propose an approach for the quick and reliable evaluation of ionic conductivity through the analysis of a universal interatomic potential. Our method incorporates a set of heuristic structure descriptors that effectively employ the rich knowledge of the underlying model while requiring minimal generalization capabilities. Using our descriptors, we rank lithium-containing materials in the Materials Project database according to their expected ionic conductivity. Eight out of the ten highest-ranked materials are confirmed to be superionic at room temperature in first-principles calculations. Notably, our method achieves a speed-up factor of approximately 50 compared to molecular dynamics driven by a machine-learning potential, and is at least 3,000 times faster compared to first-principles molecular dynamics.

**Link**: [arxiv](http://arxiv.org/abs/2411.06804v1),  [pdf](http://arxiv.org/pdf/2411.06804v1)

**Tags**: cond-mat.mtrl-sci cs.LG 



### Identifying the impact of local connectivity patterns on dynamics in   excitatory-inhibitory networks
**Authors**: Yuxiu Shao, David Dahmen, Stefano Recanatesi, Eric Shea-Brown, Srdjan Ostojic

**Updated**: 2024-11-11T08:57:44Z

**Summary**: Networks of excitatory and inhibitory (EI) neurons form a canonical circuit in the brain. Seminal theoretical results on dynamics of such networks are based on the assumption that synaptic strengths depend on the type of neurons they connect, but are otherwise statistically independent. Recent synaptic physiology datasets however highlight the prominence of specific connectivity patterns that go well beyond what is expected from independent connections. While decades of influential research have demonstrated the strong role of the basic EI cell type structure, to which extent additional connectivity features influence dynamics remains to be fully determined. Here we examine the effects of pairwise connectivity motifs on the linear dynamics in EI networks using an analytical framework that approximates the connectivity in terms of low-rank structures. This low-rank approximation is based on a mathematical derivation of the dominant eigenvalues of the connectivity matrix and predicts the impact on responses to external inputs of connectivity motifs and their interactions with cell-type structure. Our results reveal that a particular pattern of connectivity, chain motifs, have a much stronger impact on dominant eigenmodes than other pairwise motifs. An overrepresentation of chain motifs induces a strong positive eigenvalue in inhibition-dominated networks and generates a potential instability that requires revisiting the classical excitation-inhibition balance criteria. Examining effects of external inputs, we show that chain motifs can on their own induce paradoxical responses where an increased input to inhibitory neurons leads to a decrease in their activity due to the recurrent feedback. These findings have direct implications for the interpretation of experiments in which responses to optogenetic perturbations are measured and used to infer the dynamical regime of cortical circuits.

**Link**: [arxiv](http://arxiv.org/abs/2411.06802v1),  [pdf](http://arxiv.org/pdf/2411.06802v1)

**Tags**: q-bio.NC cond-mat.dis-nn cs.NE 



### Automatically Write Code Checker: An LLM-based Approach with   Logic-guided API Retrieval and Case by Case Iteration
**Authors**: Yuanyuan Xie, Jun Liu, Jiwei Yan, Jinhao Huang, Jun Yan, Jian Zhang

**Updated**: 2024-11-11T08:50:24Z

**Summary**: With the rising demand for code quality assurance, developers are not only utilizing existing static code checkers but also seeking custom checkers to satisfy their specific needs. Nowadays, various code-checking frameworks provide extensive checker customization interfaces to meet this need. However, both the abstract checking logic as well as the complex API usage of large-scale frameworks make this task challenging. To this end, automated code checker generation is anticipated to ease the burden of checker development. In this paper, we explore the feasibility of automated checker generation and propose AutoChecker, an innovative LLM-powered approach that can write code checkers automatically based on only a rule description and a test suite. Instead of generating the checker at once, AutoChecker incrementally updates the checker with the rule and one single test case each time, i.e., it iteratively generates the checker case by case. During each iteration, AutoChecker first decomposes the whole logic into a series of sub-operations and then uses the logic-guided API-context retrieval strategy to search related API-contexts from all the framework APIs. To evaluate the effectiveness of AutoChecker, we apply AutoChecker and two LLM-based baseline approaches to automatically generate checkers for 20 built-in PMD rules, including easy rules and hard rules. Experimental results demonstrate that AutoChecker significantly outperforms baseline approaches across all effectiveness metrics, where its average test pass rate improved over 4.2 times. Moreover, the checkers generated by AutoChecker are successfully applied to real-world projects, matching the performance of official checkers.

**Link**: [arxiv](http://arxiv.org/abs/2411.06796v1),  [pdf](http://arxiv.org/pdf/2411.06796v1)

**Tags**: cs.SE 



### Multi-spectral Sirens: Gravitational-wave Cosmology with (Multi-)   Sub-populations of Binary Black Holes
**Authors**: Yin-Jie Li, Shao-Peng Tang, Yuan-Zhu Wang, Yi-Zhong Fan

**Updated**: 2024-11-11T08:44:33Z

**Summary**: The cosmic expansion rate can be directly measured with gravitational-wave (GW) data of the compact binary mergers by jointly constraining the mass function of the population and the cosmological model via the so-called spectral sirens. Such a method relies on the features in the mass functions, which may originate from some individual subpopulations, and hence become blurred/indistinct due to the superposition of different subpopulations. In this work we propose a novel approach to constrain the cosmic expansion rate with subpopulations of GW events, named multi-spectral sirens. The advantage of the multi-spectral sirens compared to the traditional spectral sirens is demonstrated by the simulation with the mock data. The application of this approach to the GWTC-3 data yields $H_0=73.3^{+29.9}_{-25.6}~{\rm Mpc}^{-1}~{\rm km}~{\rm s}^{-1}$ (median and symmetric 68.3\% credible interval), which is about 19\% tighter than the result inferred with the traditional spectral sirens utilizing a PowerLaw+Peak mass function. The incorporation of the bright standard siren GW170817 with a uniform prior in [10,200] (log-uniform prior in [20,140]) ${\rm Mpc}^{-1}~{\rm km}~{\rm s}^{-1}$ gives $H_0=71.1^{+15.0}_{-7.5}~(70.3^{+12.9}_{-7.1})~{\rm Mpc}^{-1}~{\rm km}~{\rm s}^{-1}$ (68.3\% confidence level), corresponding to an improvement of $\sim26\%$ (23\%) with respect to the measurement from sole GW170817.

**Link**: [arxiv](http://arxiv.org/abs/2406.11607v2),  [pdf](http://arxiv.org/pdf/2406.11607v2)

**Tags**: astro-ph.CO astro-ph.HE gr-qc 



### Large-scale moral machine experiment on large language models
**Authors**: Muhammad Shahrul Zaim bin Ahmad, Kazuhiro Takemoto

**Updated**: 2024-11-11T08:36:49Z

**Summary**: The rapid advancement of Large Language Models (LLMs) and their potential integration into autonomous driving systems necessitates understanding their moral decision-making capabilities. While our previous study examined four prominent LLMs using the Moral Machine experimental framework, the dynamic landscape of LLM development demands a more comprehensive analysis. Here, we evaluate moral judgments across 51 different LLMs, including multiple versions of proprietary models (GPT, Claude, Gemini) and open-source alternatives (Llama, Gemma), to assess their alignment with human moral preferences in autonomous driving scenarios. Using a conjoint analysis framework, we evaluated how closely LLM responses aligned with human preferences in ethical dilemmas and examined the effects of model size, updates, and architecture. Results showed that proprietary models and open-source models exceeding 10 billion parameters demonstrated relatively close alignment with human judgments, with a significant negative correlation between model size and distance from human judgments in open-source models. However, model updates did not consistently improve alignment with human preferences, and many LLMs showed excessive emphasis on specific ethical principles. These findings suggest that while increasing model size may naturally lead to more human-like moral judgments, practical implementation in autonomous driving systems requires careful consideration of the trade-off between judgment quality and computational efficiency. Our comprehensive analysis provides crucial insights for the ethical design of autonomous systems and highlights the importance of considering cultural contexts in AI moral decision-making.

**Link**: [arxiv](http://arxiv.org/abs/2411.06790v1),  [pdf](http://arxiv.org/pdf/2411.06790v1)

**Tags**: cs.CY cs.CL cs.HC 



### The First Prompt Counts the Most! An Evaluation of Large Language Models   on Iterative Example-based Code Generation
**Authors**: Yingjie Fu, Bozhou Li, Linyi Li, Wentao Zhang, Tao Xie

**Updated**: 2024-11-11T08:05:37Z

**Summary**: The capabilities of Large Language Models (LLMs) in code generation, particularly for implementing target functionalities from natural language descriptions, have been extensively studied. As an alternative form of natural language, input-output examples (I/O examples) provide an accessible, unambiguous, and flexible way to describe functionalities, but the diversity, sparseness, and incompleteness of I/O examples also place challenges on understanding and implementing requirements. Therefore, generating code from input-output examples (i.e., example-based code generation) provides a new perspective, allowing us to evaluate LLMs' capability to infer target functionalities from limited information and to process new-form requirements. However, related research about LLMs in example-based code generation remains largely unexplored. To fill this gap, this paper presents the first comprehensive study on example-based code generation using LLMs. To address the incorrectness caused by the incompleteness of I/O examples, we adopt an iterative evaluation framework and formalize the objective of example-based code generation as two sequential sub-objectives: generating code conforming to given examples and generating code that successfully implements the target functionalities from (iteratively) given examples. We assess six state-of-the-art LLMs using a new benchmark of 168 diverse target functionalities. The results demonstrate that when requirements were described using iterative I/O examples rather than natural language, the LLMs' score decreased by over 60%, indicating that example-based code generation remains challenging for the evaluated LLMs. More interestingly, the vast majority (even over 95%) of successfully implemented functionalities are achieved in the first round of iterations, suggesting that the LLMs struggle to effectively utilize the iteratively supplemented requirements.

**Link**: [arxiv](http://arxiv.org/abs/2411.06774v1),  [pdf](http://arxiv.org/pdf/2411.06774v1)

**Tags**: cs.SE 



### PDC & DM-SFT: A Road for LLM SQL Bug-Fix Enhancing
**Authors**: Yiwen Duan, Yonghong Yu, Xiaoming Zhao, Yichang Wu, Wenbo Liu

**Updated**: 2024-11-11T07:47:20Z

**Summary**: Code Large Language Models (Code LLMs), such as Code llama and DeepSeek-Coder, have demonstrated exceptional performance in the code generation tasks. However, most existing models focus on the abilities of generating correct code, but often struggle with bug repair. We introduce a suit of methods to enhance LLM's SQL bug-fixing abilities. The methods are mainly consisted of two parts: A Progressive Dataset Construction (PDC) from scratch and Dynamic Mask Supervised Fine-tuning (DM-SFT). PDC proposes two data expansion methods from the perspectives of breadth first and depth first respectively. DM-SFT introduces an efficient bug-fixing supervised learning approach, which effectively reduce the total training steps and mitigate the "disorientation" in SQL code bug-fixing training. In our evaluation, the code LLM models trained with two methods have exceeds all current best performing model which size is much larger.

**Link**: [arxiv](http://arxiv.org/abs/2411.06767v1),  [pdf](http://arxiv.org/pdf/2411.06767v1)

**Tags**: cs.CL cs.AI cs.LG 



### Towards Fast Multilingual LLM Inference: Speculative Decoding and   Specialized Drafters
**Authors**: Euiin Yi, Taehyeon Kim, Hongseok Jeung, Du-Seong Chang, Se-Young Yun

**Updated**: 2024-11-11T07:34:25Z

**Summary**: Large language models (LLMs) have revolutionized natural language processing and broadened their applicability across diverse commercial applications. However, the deployment of these models is constrained by high inference time in multilingual settings. To mitigate this challenge, this paper explores a training recipe of an assistant model in speculative decoding, which is leveraged to draft and-then its future tokens are verified by the target LLM. We show that language-specific draft models, optimized through a targeted pretrain-and-finetune strategy, substantially brings a speedup in inference time compared to the previous methods. We validate these models across various languages in inference time, out-of-domain speedup, and GPT-4o evaluation.

**Link**: [arxiv](http://arxiv.org/abs/2406.16758v2),  [pdf](http://arxiv.org/pdf/2406.16758v2)

**Tags**: cs.CL 



### RoCar: A Relationship Network-based Evaluation Method for Large Language   Models
**Authors**: Ming Wang, Wenfang Wu, Chongyun Gao, Daling Wang, Shi Feng, Yifei Zhang

**Updated**: 2024-11-11T07:27:03Z

**Summary**: Large language models (LLMs) have received increasing attention. However, due to the complexity of its capabilities, how to rationally evaluate the capabilities of LLMs is still a task to be solved. We propose the RoCar method, which utilizes the defined basic schemas to randomly construct a task graph and generates natural language evaluation tasks based on the task graph to evaluate the reasoning and memory abilities of LLMs respectively. Due to the very large randomness of the task construction process, it is possible to ensure that none of the LLMs to be tested has directly learned the evaluation tasks, guaranteeing the fairness of the evaluation method.

**Link**: [arxiv](http://arxiv.org/abs/2307.15997v2),  [pdf](http://arxiv.org/pdf/2307.15997v2)

**Tags**: cs.CL cs.AI 



## Keyword: LLM Deployment 
 ### UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts
**Authors**: Bo Yang, Qingping Yang, Runtao Liu

**Updated**: 2024-11-11T18:59:02Z

**Summary**: The evaluation of mathematical reasoning capabilities is essential for advancing Artificial General Intelligence (AGI). While Large Language Models (LLMs) have shown impressive performance in solving mathematical problems, existing benchmarks such as GSM8K and MATH present limitations, including narrow problem definitions with specific numbers and reliance on predetermined rules that hinder accurate assessments of reasoning and adaptability. This paper introduces the UTMath Benchmark, which robustly evaluates the models through extensive unit tests. It consists of 1,053 problems across 9 mathematical domains, with over 68 test cases per problem.We propose an innovative evaluation framework inspired by unit testing in software development, focusing on both accuracy and reliability of results. Furthermore, we introduce the Reasoning-to-Coding of Thoughts (RCoT) approach, which encourages LLMs to perform explicit reasoning before generating code, leading to generating more advanced solution and improved performance. Furthermore, we are releasing not only the UTMath benchmark but also the UTMath-Train training dataset (more than 70k samples), to support the community in further exploring mathematical reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2411.07240v1),  [pdf](http://arxiv.org/pdf/2411.07240v1)

**Tags**: cs.CL cs.AI 



### OpenThaiGPT 1.5: A Thai-Centric Open Source Large Language Model
**Authors**: Sumeth Yuenyong, Kobkrit Viriyayudhakorn, Apivadee Piyatumrong, Jillaphat Jaroenkantasima

**Updated**: 2024-11-11T18:58:46Z

**Summary**: OpenThaiGPT 1.5 is an advanced Thai language chat model based on Qwen v2.5, finetuned on over 2,000,000 Thai instruction pairs. This report provides an engineering perspective on the model's development, capabilities, and performance. We discuss the model's architecture, training process, and key features, including multi-turn conversation support, Retrieval Augmented Generation (RAG) compatibility, and tool-calling functionality. Benchmark results demonstrate OpenThaiGPT 1.5's state-of-the-art performance on various Thai language tasks, outperforming other open-source Thai language models. We also address practical considerations such as GPU memory requirements and deployment strategies.

**Link**: [arxiv](http://arxiv.org/abs/2411.07238v1),  [pdf](http://arxiv.org/pdf/2411.07238v1)

**Tags**: cs.CL 



### Tooling or Not Tooling? The Impact of Tools on Language Agents for   Chemistry Problem Solving
**Authors**: Botao Yu, Frazier N. Baker, Ziru Chen, Garrett Herb, Boyu Gou, Daniel Adu-Ampratwum, Xia Ning, Huan Sun

**Updated**: 2024-11-11T18:46:37Z

**Summary**: To enhance large language models (LLMs) for chemistry problem solving, several LLM-based agents augmented with tools have been proposed, such as ChemCrow and Coscientist. However, their evaluations are narrow in scope, leaving a large gap in understanding the benefits of tools across diverse chemistry tasks. To bridge this gap, we develop ChemAgent, an enhanced chemistry agent over ChemCrow, and conduct a comprehensive evaluation of its performance on both specialized chemistry tasks and general chemistry questions. Surprisingly, ChemAgent does not consistently outperform its base LLMs without tools. Our error analysis with a chemistry expert suggests that: For specialized chemistry tasks, such as synthesis prediction, we should augment agents with specialized tools; however, for general chemistry questions like those in exams, agents' ability to reason correctly with chemistry knowledge matters more, and tool augmentation does not always help.

**Link**: [arxiv](http://arxiv.org/abs/2411.07228v1),  [pdf](http://arxiv.org/pdf/2411.07228v1)

**Tags**: cs.AI cs.CE 



### Comparing Bottom-Up and Top-Down Steering Approaches on In-Context   Learning Tasks
**Authors**: Madeline Brumley, Joe Kwon, David Krueger, Dmitrii Krasheninnikov, Usman Anwar

**Updated**: 2024-11-11T18:36:17Z

**Summary**: A key objective of interpretability research on large language models (LLMs) is to develop methods for robustly steering models toward desired behaviors. To this end, two distinct approaches to interpretability -- ``bottom-up" and ``top-down" -- have been presented, but there has been little quantitative comparison between them. We present a case study comparing the effectiveness of representative vector steering methods from each branch: function vectors (FV; arXiv:2310.15213), as a bottom-up method, and in-context vectors (ICV; arXiv:2311.06668) as a top-down method. While both aim to capture compact representations of broad in-context learning tasks, we find they are effective only on specific types of tasks: ICVs outperform FVs in behavioral shifting, whereas FVs excel in tasks requiring more precision. We discuss the implications for future evaluations of steering methods and for further research into top-down and bottom-up steering given these findings.

**Link**: [arxiv](http://arxiv.org/abs/2411.07213v1),  [pdf](http://arxiv.org/pdf/2411.07213v1)

**Tags**: cs.LG 



### Scaling Law Hypothesis for Multimodal Model
**Authors**: Qingyun Sun, Zhen Guo, PIN AI Team

**Updated**: 2024-11-11T18:32:16Z

**Summary**: We propose a scaling law hypothesis for multimodal models processing text, audio, images, and video within a shared token and embedding space. Our framework predicts model performance based on modality-specific compression and tokenization efficiency, extending established scaling laws from text-based decoder models to mixed-modality systems. We explore whether leveraging more training data in multiple modalities can reduce the size of the multimodal model, enabling efficient deployment on resource-constrained devices.

**Link**: [arxiv](http://arxiv.org/abs/2409.06754v4),  [pdf](http://arxiv.org/pdf/2409.06754v4)

**Tags**: cs.LG cs.AI 



### DLCR: A Generative Data Expansion Framework via Diffusion for   Clothes-Changing Person Re-ID
**Authors**: Nyle Siddiqui, Florinel Alin Croitoru, Gaurav Kumar Nayak, Radu Tudor Ionescu, Mubarak Shah

**Updated**: 2024-11-11T18:28:33Z

**Summary**: With the recent exhibited strength of generative diffusion models, an open research question is \textit{if images generated by these models can be used to learn better visual representations}. While this generative data expansion may suffice for easier visual tasks, we explore its efficacy on a more difficult discriminative task: clothes-changing person re-identification (CC-ReID). CC-ReID aims to match people appearing in non-overlapping cameras, even when they change their clothes across cameras. Not only are current CC-ReID models constrained by the limited diversity of clothing in current CC-ReID datasets, but generating additional data that retains important personal features for accurate identification is a current challenge. To address this issue we propose DLCR, a novel data expansion framework that leverages pre-trained diffusion and large language models (LLMs) to accurately generate diverse images of individuals in varied attire. We generate additional data for five benchmark CC-ReID datasets (PRCC, CCVID, LaST, VC-Clothes, and LTCC) and \textbf{increase their clothing diversity by \boldmath{$10$}x, totaling over \boldmath{$2.1$}M images generated}. DLCR employs diffusion-based text-guided inpainting, conditioned on clothing prompts constructed using LLMs, to generate synthetic data that only modifies a subject's clothes while preserving their personally identifiable features. With this massive increase in data, we introduce two novel strategies - progressive learning and test-time prediction refinement - that respectively reduce training time and further boosts CC-ReID performance. On the PRCC dataset, we obtain a large top-1 accuracy improvement of $11.3\%$ by training CAL, a previous state of the art (SOTA) method, with DLCR-generated data. We publicly release our code and generated data for each dataset here: \url{https://github.com/CroitoruAlin/dlcr}.

**Link**: [arxiv](http://arxiv.org/abs/2411.07205v1),  [pdf](http://arxiv.org/pdf/2411.07205v1)

**Tags**: cs.CV 



### Programming Distributed Collective Processes in the eXchange Calculus
**Authors**: Giorgio Audrito, Roberto Casadei, Ferruccio Damiani, Gianluca Torta, Mirko Viroli

**Updated**: 2024-11-11T18:26:31Z

**Summary**: Recent trends like the Internet of Things (IoT) suggest a vision of dense and multi-scale deployments of computing devices in nearly all kinds of environments. A prominent engineering challenge revolves around programming the collective adaptive behaviour of such computational ecosystems. This requires abstractions able to capture concepts like ensembles (dynamic groups of cooperating devices) and collective tasks (joint activities carried out by ensembles). In this work, we consider collections of devices interacting with neighbours and that execute in nearly-synchronised sense-compute-interact rounds, where the computation is given by a single program mapping sensing values and incoming messages to output and outcoming messages. To support programming whole computational collectives, we propose the abstraction of a distributed collective process, which can be used to define at once the ensemble formation logic and its collective task. We formalise the abstraction in the eXchange Calculus (XC), a core functional language based on neighbouring values (maps from neighbours to values) where state and interaction is handled through a single primitive, exchange, and provide a corresponding implementation in the FCPP language. Then, we exercise distributed collective processes using two case studies: multi-hop message propagation and distributed monitoring of spatial properties. Finally, we discuss the features of the abstraction and its suitability for different kinds of distributed computing applications.

**Link**: [arxiv](http://arxiv.org/abs/2401.11212v2),  [pdf](http://arxiv.org/pdf/2401.11212v2)

**Tags**: cs.DC cs.AI cs.MA cs.PL D.1.3; F.1.1; F.4.3; I.2.11; J.7 



### LLM-based Continuous Intrusion Detection Framework for Next-Gen Networks
**Authors**: Frederic Adjewa, Moez Esseghir, Leila Merghem-Boulahia

**Updated**: 2024-11-11T18:19:22Z

**Summary**: In this paper, we present an adaptive framework designed for the continuous detection, identification and classification of emerging attacks in network traffic. The framework employs a transformer encoder architecture, which captures hidden patterns in a bidirectional manner to differentiate between malicious and legitimate traffic. Initially, the framework focuses on the accurate detection of malicious activities, achieving a perfect recall of 100\% in distinguishing between attack and benign traffic. Subsequently, the system incrementally identifies unknown attack types by leveraging a Gaussian Mixture Model (GMM) to cluster features derived from high-dimensional BERT embeddings. This approach allows the framework to dynamically adjust its identification capabilities as new attack clusters are discovered, maintaining high detection accuracy. Even after integrating additional unknown attack clusters, the framework continues to perform at a high level, achieving 95.6\% in both classification accuracy and recall.The results demonstrate the effectiveness of the proposed framework in adapting to evolving threats while maintaining high accuracy in both detection and identification tasks. Our ultimate goal is to develop a scalable, real-time intrusion detection system that can continuously evolve with the ever-changing network threat landscape.

**Link**: [arxiv](http://arxiv.org/abs/2411.03354v2),  [pdf](http://arxiv.org/pdf/2411.03354v2)

**Tags**: cs.CR cs.NI 



### The Super Weight in Large Language Models
**Authors**: Mengxia Yu, De Wang, Qi Shan, Colorado Reed, Alvin Wan

**Updated**: 2024-11-11T18:05:48Z

**Summary**: Recent works have shown a surprising result: a small fraction of Large Language Model (LLM) parameter outliers are disproportionately important to the quality of the model. LLMs contain billions of parameters, so these small fractions, such as 0.01%, translate to hundreds of thousands of parameters. In this work, we present an even more surprising finding: Pruning as few as a single parameter can destroy an LLM's ability to generate text -- increasing perplexity by 3 orders of magnitude and reducing zero-shot accuracy to guessing. We propose a data-free method for identifying such parameters, termed super weights, using a single forward pass through the model. We additionally find that these super weights induce correspondingly rare and large activation outliers, termed super activations. When preserved with high precision, super activations can improve simple round-to-nearest quantization to become competitive with state-of-the-art methods. For weight quantization, we similarly find that by preserving the super weight and clipping other weight outliers, round-to-nearest quantization can scale to much larger block sizes than previously considered. To facilitate further research into super weights, we provide an index of super weight coordinates for common, openly available LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2411.07191v1),  [pdf](http://arxiv.org/pdf/2411.07191v1)

**Tags**: cs.CL cs.AI 



### NatureLM-audio: an Audio-Language Foundation Model for Bioacoustics
**Authors**: David Robinson, Marius Miron, Masato Hagiwara, Olivier Pietquin

**Updated**: 2024-11-11T18:01:45Z

**Summary**: Large language models (LLMs) prompted with text and audio represent the state of the art in various auditory tasks, including speech, music, and general audio, showing emergent abilities on unseen tasks. However, these capabilities have yet to be fully demonstrated in bioacoustics tasks, such as detecting animal vocalizations in large recordings, classifying rare and endangered species, and labeling context and behavior - tasks that are crucial for conservation, biodiversity monitoring, and the study of animal behavior. In this work, we present NatureLM-audio, the first audio-language foundation model specifically designed for bioacoustics. Our carefully curated training dataset comprises text-audio pairs spanning a diverse range of bioacoustics, speech, and music data, designed to address the challenges posed by limited annotated datasets in the field. We demonstrate successful transfer of learned representations from music and speech to bioacoustics, and our model shows promising generalization to unseen taxa and tasks. Importantly, we test NatureLM-audio on a novel benchmark (BEANS-Zero) and it sets the new state of the art (SotA) on several bioacoustics tasks, including zero-shot classification of unseen species. To advance bioacoustics research, we also open-source the code for generating training and benchmark data, as well as for training the model.

**Link**: [arxiv](http://arxiv.org/abs/2411.07186v1),  [pdf](http://arxiv.org/pdf/2411.07186v1)

**Tags**: cs.SD cs.AI cs.LG eess.AS 



### Understanding Long Videos with Multimodal Language Models
**Authors**: Kanchana Ranasinghe, Xiang Li, Kumara Kahatapitiya, Michael S. Ryoo

**Updated**: 2024-11-11T17:56:29Z

**Summary**: Large Language Models (LLMs) have allowed recent LLM-based approaches to achieve excellent performance on long-video understanding benchmarks. We investigate how extensive world knowledge and strong reasoning skills of underlying LLMs influence this strong performance. Surprisingly, we discover that LLM-based approaches can yield surprisingly good accuracy on long-video tasks with limited video information, sometimes even with no video specific information. Building on this, we exploring injecting video-specific information into an LLM-based framework. We utilize off-the-shelf vision tools to extract three object-centric information modalities from videos and then leverage natural language as a medium for fusing this information. Our resulting Multimodal Video Understanding (MVU) framework demonstrates state-of-the-art performance across multiple video understanding benchmarks. Strong performance also on robotics domain tasks establish its strong generality. Our code will be released publicly.

**Link**: [arxiv](http://arxiv.org/abs/2403.16998v2),  [pdf](http://arxiv.org/pdf/2403.16998v2)

**Tags**: cs.CV 



### Continual Memorization of Factoids in Large Language Models
**Authors**: Howard Chen, Jiayi Geng, Adithya Bhaskar, Dan Friedman, Danqi Chen

**Updated**: 2024-11-11T17:56:15Z

**Summary**: Large language models can absorb a massive amount of knowledge through pretraining, but pretraining is inefficient for acquiring long-tailed or specialized facts. Therefore, fine-tuning on specialized or new knowledge that reflects changes in the world has become popular, though it risks disrupting the model's original capabilities. We study this fragility in the context of continual memorization, where the model is trained on a small set of long-tail factoids (factual associations) and must retain these factoids after multiple stages of subsequent training on other datasets. Through extensive experiments, we show that LLMs suffer from forgetting across a wide range of subsequent tasks, and simple replay techniques do not fully prevent forgetting, especially when the factoid datasets are trained in the later stages. We posit that there are two ways to alleviate forgetting: 1) protect the memorization process as the model learns the factoids, or 2) reduce interference from training in later stages. With this insight, we develop an effective mitigation strategy: REMIX (Random and Generic Data Mixing). REMIX prevents forgetting by mixing generic data sampled from pretraining corpora or even randomly generated word sequences during each stage, despite being unrelated to the memorized factoids in the first stage. REMIX can recover performance from severe forgetting, often outperforming replay-based methods that have access to the factoids from the first stage. We then analyze how REMIX alters the learning process and find that successful forgetting prevention is associated with a pattern: the model stores factoids in earlier layers than usual and diversifies the set of layers that store these factoids. The efficacy of REMIX invites further investigation into the underlying dynamics of memorization and forgetting, opening exciting possibilities for future research.

**Link**: [arxiv](http://arxiv.org/abs/2411.07175v1),  [pdf](http://arxiv.org/pdf/2411.07175v1)

**Tags**: cs.CL 



### A Domain-Agnostic Neurosymbolic Approach for Big Social Data Analysis:   Evaluating Mental Health Sentiment on Social Media during COVID-19
**Authors**: Vedant Khandelwal, Manas Gaur, Ugur Kursuncu, Valerie Shalin, Amit Sheth

**Updated**: 2024-11-11T17:41:54Z

**Summary**: Monitoring public sentiment via social media is potentially helpful during health crises such as the COVID-19 pandemic. However, traditional frequency-based, data-driven neural network-based approaches can miss newly relevant content due to the evolving nature of language in a dynamically evolving environment. Human-curated symbolic knowledge sources, such as lexicons for standard language and slang terms, can potentially elevate social media signals in evolving language. We introduce a neurosymbolic method that integrates neural networks with symbolic knowledge sources, enhancing the detection and interpretation of mental health-related tweets relevant to COVID-19. Our method was evaluated using a corpus of large datasets (approximately 12 billion tweets, 2.5 million subreddit data, and 700k news articles) and multiple knowledge graphs. This method dynamically adapts to evolving language, outperforming purely data-driven models with an F1 score exceeding 92\%. This approach also showed faster adaptation to new data and lower computational demands than fine-tuning pre-trained large language models (LLMs). This study demonstrates the benefit of neurosymbolic methods in interpreting text in a dynamic environment for tasks such as health surveillance.

**Link**: [arxiv](http://arxiv.org/abs/2411.07163v1),  [pdf](http://arxiv.org/pdf/2411.07163v1)

**Tags**: cs.AI I.2.4; I.2.6; I.2.7; I.2.0 



### CDR: Customizable Density Ratios of Strong-over-weak LLMs for Preference   Annotation
**Authors**: Guangxuan Xu, Kai Xu, Shivchander Sudalairaj, Hao Wang, Akash Srivastava

**Updated**: 2024-11-11T17:34:00Z

**Summary**: Preference tuning of large language models (LLMs) relies on high-quality human preference data, which is often expensive and time-consuming to gather. While existing methods can use trained reward models or proprietary model as judges for preference annotation, they have notable drawbacks: training reward models remain dependent on initial human data, and using proprietary model imposes license restrictions that inhibits commercial usage. In this paper, we introduce customized density ratio (CDR), a training-free and highly effective method that leverages off-the-shelf LLMs for preference data annotation. Our approach uses the log-density ratio between a better-aligned LLM and a less aligned LLM as a reward signal. We explores 221 different LLMs pairs and empirically demonstrate that increasing the performance gap between paired LLMs correlates with better reward generalization. Furthermore, we show that tailoring the density ratio reward function with specific criteria and preference exemplars enhances performance across domains and within target areas.   In our experiment using density ratio from a pair of Mistral-7B models, CDR achieves a RewardBench score of 82.6, outperforming the best trained reward functions from same model class and demonstrating competitive performance against SoTA models in Safety (91.0) and Reasoning (88.0) domains. We use CDR to annotate an on-policy preference dataset with which we preference tune Llama-3-8B-Instruct with SimPO. Using reward signals from two relatively weak models, our approach pushes Llama-3-8B to achieve a 37.4% (+15.1%) win rate on ArenaHard and a 40.7% (+17.8%) win rate on Length-Controlled AlpacaEval 2.0, along with a score of 8.0 on MT-Bench.

**Link**: [arxiv](http://arxiv.org/abs/2411.02481v2),  [pdf](http://arxiv.org/pdf/2411.02481v2)

**Tags**: cs.CL cs.AI 



### Using Large Language Models for Hyperparameter Optimization
**Authors**: Michael R. Zhang, Nishkrit Desai, Juhan Bae, Jonathan Lorraine, Jimmy Ba

**Updated**: 2024-11-11T17:30:55Z

**Summary**: This paper explores the use of foundational large language models (LLMs) in hyperparameter optimization (HPO). Hyperparameters are critical in determining the effectiveness of machine learning models, yet their optimization often relies on manual approaches in limited-budget settings. By prompting LLMs with dataset and model descriptions, we develop a methodology where LLMs suggest hyperparameter configurations, which are iteratively refined based on model performance. Our empirical evaluations on standard benchmarks reveal that within constrained search budgets, LLMs can match or outperform traditional HPO methods like Bayesian optimization across different models on standard benchmarks. Furthermore, we propose to treat the code specifying our model as a hyperparameter, which the LLM outputs and affords greater flexibility than existing HPO approaches.

**Link**: [arxiv](http://arxiv.org/abs/2312.04528v2),  [pdf](http://arxiv.org/pdf/2312.04528v2)

**Tags**: cs.LG cs.AI 



### Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language   Models
**Authors**: Yancheng He, Shilong Li, Jiaheng Liu, Yingshui Tan, Hui Huang, Weixun Wang, Xingyuan Bu, Hangyu Guo, Chengwei Hu, Boren Zheng, Xuepeng Liu, Dekai Sun, Wenbo Su, Bo Zheng

**Updated**: 2024-11-11T17:10:56Z

**Summary**: New LLM evaluation benchmarks are important to align with the rapid development of Large Language Models (LLMs). In this work, we present Chinese SimpleQA, the first comprehensive Chinese benchmark to evaluate the factuality ability of language models to answer short questions, and Chinese SimpleQA mainly has five properties (i.e., Chinese, Diverse, High-quality, Static, Easy-to-evaluate). Specifically, first, we focus on the Chinese language over 6 major topics with 99 diverse subtopics. Second, we conduct a comprehensive quality control process to achieve high-quality questions and answers, where the reference answers are static and cannot be changed over time. Third, following SimpleQA, the questions and answers are very short, and the grading process is easy-to-evaluate based on OpenAI API. Based on Chinese SimpleQA, we perform a comprehensive evaluation on the factuality abilities of existing LLMs. Finally, we hope that Chinese SimpleQA could guide the developers to better understand the Chinese factuality abilities of their models and facilitate the growth of foundation models.

**Link**: [arxiv](http://arxiv.org/abs/2411.07140v1),  [pdf](http://arxiv.org/pdf/2411.07140v1)

**Tags**: cs.CL 



### Stronger Models are NOT Stronger Teachers for Instruction Tuning
**Authors**: Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Radha Poovendran

**Updated**: 2024-11-11T17:06:48Z

**Summary**: Instruction tuning has been widely adopted to ensure large language models (LLMs) follow user instructions effectively. The resulting instruction-following capabilities of LLMs heavily rely on the instruction datasets used for tuning. Recently, synthetic instruction datasets have emerged as an economically viable solution to provide LLMs diverse and high-quality instructions. However, existing approaches typically assume that larger or stronger models are stronger teachers for instruction tuning, and hence simply adopt these models as response generators to the synthetic instructions. In this paper, we challenge this commonly-adopted assumption. Our extensive experiments across five base models and twenty response generators reveal that larger and stronger models are not necessarily stronger teachers of smaller models. We refer to this phenomenon as the Larger Models' Paradox. We observe that existing metrics cannot precisely predict the effectiveness of response generators since they ignore the compatibility between teachers and base models being fine-tuned. We thus develop a novel metric, named as Compatibility-Adjusted Reward (CAR) to measure the effectiveness of response generators. Our experiments across five base models demonstrate that CAR outperforms almost all baselines.

**Link**: [arxiv](http://arxiv.org/abs/2411.07133v1),  [pdf](http://arxiv.org/pdf/2411.07133v1)

**Tags**: cs.AI cs.CL 



### Benchmarking LLMs' Judgments with No Gold Standard
**Authors**: Shengwei Xu, Yuxuan Lu, Grant Schoenebeck, Yuqing Kong

**Updated**: 2024-11-11T16:58:36Z

**Summary**: We introduce the GEM (Generative Estimator for Mutual Information), an evaluation metric for assessing language generation by Large Language Models (LLMs), particularly in generating informative judgments, without the need for a gold standard reference. GEM broadens the scenarios where we can benchmark LLM generation performance-from traditional ones, like machine translation and summarization, where gold standard references are readily available, to subjective tasks without clear gold standards, such as academic peer review.   GEM uses a generative model to estimate mutual information between candidate and reference responses, without requiring the reference to be a gold standard. In experiments on a human-annotated dataset, GEM demonstrates competitive correlations with human scores compared to the state-of-the-art GPT-4o Examiner, and outperforms all other baselines. Additionally, GEM is more robust against strategic manipulations, such as rephrasing or elongation, which can artificially inflate scores under a GPT-4o Examiner.   We also present GRE-bench (Generating Review Evaluation Benchmark) which evaluates LLMs based on how well they can generate high-quality peer reviews for academic research papers. Because GRE-bench is based upon GEM, it inherits its robustness properties. Additionally, GRE-bench circumvents data contamination problems (or data leakage) by using the continuous influx of new open-access research papers and peer reviews each year. We show GRE-bench results of various popular LLMs on their peer review capabilities using the ICLR2023 dataset.

**Link**: [arxiv](http://arxiv.org/abs/2411.07127v1),  [pdf](http://arxiv.org/pdf/2411.07127v1)

**Tags**: cs.CL cs.LG 



### Hire Me or Not? Examining Language Model's Behavior with Occupation   Attributes
**Authors**: Damin Zhang, Yi Zhang, Geetanjali Bihani, Julia Rayz

**Updated**: 2024-11-11T16:53:58Z

**Summary**: With the impressive performance in various downstream tasks, large language models (LLMs) have been widely integrated into production pipelines, like recruitment and recommendation systems. A known issue of models trained on natural language data is the presence of human biases, which can impact the fairness of the system. This paper investigates LLMs' behavior with respect to gender stereotypes, in the context of occupation decision making. Our framework is designed to investigate and quantify the presence of gender stereotypes in LLMs' behavior via multi-round question answering. Inspired by prior works, we construct a dataset by leveraging a standard occupation classification knowledge base released by authoritative agencies. We tested three LLMs (RoBERTa-large, GPT-3.5-turbo, and Llama2-70b-chat) and found that all models exhibit gender stereotypes analogous to human biases, but with different preferences. The distinct preferences of GPT-3.5-turbo and Llama2-70b-chat may imply the current alignment methods are insufficient for debiasing and could introduce new biases contradicting the traditional gender stereotypes.

**Link**: [arxiv](http://arxiv.org/abs/2405.06687v2),  [pdf](http://arxiv.org/pdf/2405.06687v2)

**Tags**: cs.CL 



### SCAR: Sparse Conditioned Autoencoders for Concept Detection and Steering   in LLMs
**Authors**: Ruben Hrle, Felix Friedrich, Manuel Brack, Bjrn Deiseroth, Patrick Schramowski, Kristian Kersting

**Updated**: 2024-11-11T16:51:39Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable capabilities in generating human-like text, but their output may not be aligned with the user or even produce harmful content. This paper presents a novel approach to detect and steer concepts such as toxicity before generation. We introduce the Sparse Conditioned Autoencoder (SCAR), a single trained module that extends the otherwise untouched LLM. SCAR ensures full steerability, towards and away from concepts (e.g., toxic content), without compromising the quality of the model's text generation on standard evaluation benchmarks. We demonstrate the effective application of our approach through a variety of concepts, including toxicity, safety, and writing style alignment. As such, this work establishes a robust framework for controlling LLM generations, ensuring their ethical and safe deployment in real-world applications.

**Link**: [arxiv](http://arxiv.org/abs/2411.07122v1),  [pdf](http://arxiv.org/pdf/2411.07122v1)

**Tags**: cs.CL 



### ROCODE: Integrating Backtracking Mechanism and Program Analysis in Large   Language Models for Code Generation
**Authors**: Xue Jiang, Yihong Dong, Yongding Tao, Huanyu Liu, Zhi Jin, Wenpin Jiao, Ge Li

**Updated**: 2024-11-11T16:39:13Z

**Summary**: Large language models (LLMs) have achieved impressive performance in code generation recently, offering programmers revolutionary assistance in software development. However, due to the auto-regressive nature of LLMs, they are susceptible to error accumulation during code generation. Once an error is produced, LLMs can merely continue to generate the subsequent code conditioned on it, given their inability to adjust previous outputs. Existing LLM-based approaches typically consider post-revising after code generation, leading to the challenging resolution of accumulated errors and the significant wastage of resources. Ideally, LLMs should rollback and resolve the occurred error in time during code generation, rather than proceed on the basis of the error and wait for post-revising after generation. In this paper, we propose ROCODE, which integrates the backtracking mechanism and program analysis into LLMs for code generation. Specifically, we employ program analysis to perform incremental error detection during the generation process. When an error is detected, the backtracking mechanism is triggered to priming rollback strategies and constraint regeneration, thereby eliminating the error early and ensuring continued generation on the correct basis. Experiments on multiple code generation benchmarks show that ROCODE can significantly reduce the errors generated by LLMs, with a compilation pass rate of 99.1%. The test pass rate is improved by up to 23.8% compared to the best baseline approach. Compared to the post-revising baseline, the token cost is reduced by 19.3%. Moreover, our approach is model-agnostic and achieves consistent improvements across nine representative LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2411.07112v1),  [pdf](http://arxiv.org/pdf/2411.07112v1)

**Tags**: cs.SE 



### Building a Taiwanese Mandarin Spoken Language Model: A First Attempt
**Authors**: Chih-Kai Yang, Yu-Kuan Fu, Chen-An Li, Yi-Cheng Lin, Yu-Xiang Lin, Wei-Chih Chen, Ho Lam Chung, Chun-Yi Kuan, Wei-Ping Huang, Ke-Han Lu, Tzu-Quan Lin, Hsiu-Hsuan Wang, En-Pei Hu, Chan-Jan Hsu, Liang-Hsuan Tseng, I-Hsiang Chiu, Ulin Sanga, Xuanjun Chen, Po-chun Hsu, Shu-wen Yang, Hung-yi Lee

**Updated**: 2024-11-11T16:37:40Z

**Summary**: This technical report presents our initial attempt to build a spoken large language model (LLM) for Taiwanese Mandarin, specifically tailored to enable real-time, speech-to-speech interaction in multi-turn conversations. Our end-to-end model incorporates a decoder-only transformer architecture and aims to achieve seamless interaction while preserving the conversational flow, including full-duplex capabilities allowing simultaneous speaking and listening. The paper also details the training process, including data preparation with synthesized dialogues and adjustments for real-time interaction. We also developed a platform to evaluate conversational fluency and response coherence in multi-turn dialogues. We hope the release of the report can contribute to the future development of spoken LLMs in Taiwanese Mandarin.

**Link**: [arxiv](http://arxiv.org/abs/2411.07111v1),  [pdf](http://arxiv.org/pdf/2411.07111v1)

**Tags**: cs.CL cs.SD eess.AS 



### Advancing Object Goal Navigation Through LLM-enhanced Object Affinities   Transfer
**Authors**: Mengying Lin, Shugao Liu, Dingxi Zhang, Yaran Chen, Haoran Liu, Dongbin Zhao

**Updated**: 2024-11-11T16:20:28Z

**Summary**: In object goal navigation, agents navigate towards objects identified by category labels using visual and spatial information. Previously, solely network-based methods typically rely on historical data for object affinities estimation, lacking adaptability to new environments and unseen targets. Simultaneously, employing Large Language Models (LLMs) for navigation as either planners or agents, though offering a broad knowledge base, is cost-inefficient and lacks targeted historical experience. Addressing these challenges, we present the LLM-enhanced Object Affinities Transfer (LOAT) framework, integrating LLM-derived object semantics with network-based approaches to leverage experiential object affinities, thus improving adaptability in unfamiliar settings. LOAT employs a dual-module strategy: a generalized affinities module for accessing LLMs' vast knowledge and an experiential affinities module for applying learned object semantic relationships, complemented by a dynamic fusion module harmonizing these information sources based on temporal context. The resulting scores activate semantic maps before feeding into downstream policies, enhancing navigation systems with context-aware inputs. Our evaluations conducted in the AI2-THOR and Habitat simulators indicate significant improvements in both navigation success rates and overall efficiency. Furthermore, the system performs effectively when deployed on a real robot without requiring additional training, thereby validating the efficacy of LOAT in integrating LLM insights for enhanced object-goal navigation.

**Link**: [arxiv](http://arxiv.org/abs/2403.09971v2),  [pdf](http://arxiv.org/pdf/2403.09971v2)

**Tags**: cs.RO 



### A Multi-Agent Approach for REST API Testing with Semantic Graphs and   LLM-Driven Inputs
**Authors**: Myeongsoo Kim, Tyler Stennett, Saurabh Sinha, Alessandro Orso

**Updated**: 2024-11-11T16:20:27Z

**Summary**: As modern web services increasingly rely on REST APIs, their thorough testing has become crucial. Furthermore, the advent of REST API specifications such as the OpenAPI Specification has led to the emergence of many black-box REST API testing tools. However, these tools often focus on individual test elements in isolation (e.g., APIs, parameters, values), resulting in lower coverage and less effectiveness in detecting faults (i.e., 500 response codes). To address these limitations, we present AutoRestTest, the first black-box framework to adopt a dependency-embedded multi-agent approach for REST API testing, integrating Multi-Agent Reinforcement Learning (MARL) with a Semantic Property Dependency Graph (SPDG) and Large Language Models (LLMs). Our approach treats REST API testing as a separable problem, where four agents -- API, dependency, parameter, and value -- collaborate to optimize API exploration. LLMs handle domain-specific value restrictions, the SPDG model simplifies the search space for dependencies using a similarity score between API operations, and MARL dynamically optimizes the agents' behavior. Evaluated on 12 real-world REST services, AutoRestTest outperforms the four leading black-box REST API testing tools, including those assisted by RESTGPT (which augments realistic test inputs using LLMs), in terms of code coverage, operation coverage, and fault detection. Notably, AutoRestTest is the only tool able to identify an internal server error in Spotify. Our ablation study underscores the significant contributions of the agent learning, SPDG, and LLM components.

**Link**: [arxiv](http://arxiv.org/abs/2411.07098v1),  [pdf](http://arxiv.org/pdf/2411.07098v1)

**Tags**: cs.SE cs.AI 



### Impact of LLM-based Review Comment Generation in Practice: A Mixed   Open-/Closed-source User Study
**Authors**: Doriane Olewicki, Leuson Da Silva, Suhaib Mujahid, Arezou Amini, Benjamin Mah, Marco Castelluccio, Sarra Habchi, Foutse Khomh, Bram Adams

**Updated**: 2024-11-11T16:12:11Z

**Summary**: We conduct a large-scale empirical user study in a live setup to evaluate the acceptance of LLM-generated comments and their impact on the review process. This user study was performed in two organizations, Mozilla (which has its codebase available as open source) and Ubisoft (fully closed-source). Inside their usual review environment, participants were given access to RevMate, an LLM-based assistive tool suggesting generated review comments using an off-the-shelf LLM with Retrieval Augmented Generation to provide extra code and review context, combined with LLM-as-a-Judge, to auto-evaluate the generated comments and discard irrelevant cases. Based on more than 587 patch reviews provided by RevMate, we observed that 8.1% and 7.2%, respectively, of LLM-generated comments were accepted by reviewers in each organization, while 14.6% and 20.5% other comments were still marked as valuable as review or development tips. Refactoring-related comments are more likely to be accepted than Functional comments (18.2% and 18.6% compared to 4.8% and 5.2%). The extra time spent by reviewers to inspect generated comments or edit accepted ones (36/119), yielding an overall median of 43s per patch, is reasonable. The accepted generated comments are as likely to yield future revisions of the revised patch as human-written comments (74% vs 73% at chunk-level).

**Link**: [arxiv](http://arxiv.org/abs/2411.07091v1),  [pdf](http://arxiv.org/pdf/2411.07091v1)

**Tags**: cs.SE 



### Orchestration Framework for Open System Models with Autonomous RISs and   Oblivious Base Stations
**Authors**: Victor Croisfelt, Francesco Devoti, Fabio Saggese, Vincenzo Sciancalepore, Xavier Costa-Prez, Petar Popovski

**Updated**: 2024-11-11T15:48:28Z

**Summary**: Autonomous reconfigurable intelligent surface (RIS) offers the potential to simplify deployment by reducing the need for real-time remote control between a base station (BS) and an RIS. However, we highlight two major challenges posed by autonomy. The first is implementation complexity, as autonomy requires hybrid RISs (HRISs) equipped with additional on-board hardware to monitor the propagation environment and conduct local channel estimation (CHEST), a process known as probing. The second challenge, termed probe distortion, reflects a form of the observer effect: during probing, an HRIS can inadvertently alter the propagation environment, potentially disrupting the operations of other communicating devices. While implementation complexity has been extensively studied, probe distortion remains largely unexplored. To further assess the potential of autonomous RISs, this paper comprehensively and pragmatically studies fundamental trade-offs posed by these challenges. We examine the robustness of an HRIS-assisted massive multiple-input multiple-output (mMIMO) system under minimal design choices that reflect the essential elements and stringent conditions, including (a) two extremes of implementation complexity realized through minimalist operational designs of two HRIS hardware architectures, and (b) an oblivious BS that fully embraces probe distortion. To make our analysis possible, we propose a physical-layer orchestration framework that aligns HRIS and mMIMO operations. We provide empirical evidence showing that autonomous RIS holds promise even under these strict conditions and propose new research directions, particularly for advancing the understanding of probe distortion.

**Link**: [arxiv](http://arxiv.org/abs/2304.10858v4),  [pdf](http://arxiv.org/pdf/2304.10858v4)

**Tags**: cs.IT cs.NI cs.SY eess.SP eess.SY math.IT 



### Universal Response and Emergence of Induction in LLMs
**Authors**: Niclas Luick

**Updated**: 2024-11-11T15:47:15Z

**Summary**: While induction is considered a key mechanism for in-context learning in LLMs, understanding its precise circuit decomposition beyond toy models remains elusive. Here, we study the emergence of induction behavior within LLMs by probing their response to weak single-token perturbations of the residual stream. We find that LLMs exhibit a robust, universal regime in which their response remains scale-invariant under changes in perturbation strength, thereby allowing us to quantify the build-up of token correlations throughout the model. By applying our method, we observe signatures of induction behavior within the residual stream of Gemma-2-2B, Llama-3.2-3B, and GPT-2-XL. Across all models, we find that these induction signatures gradually emerge within intermediate layers and identify the relevant model sections composing this behavior. Our results provide insights into the collective interplay of components within LLMs and serve as a benchmark for large-scale circuit analysis.

**Link**: [arxiv](http://arxiv.org/abs/2411.07071v1),  [pdf](http://arxiv.org/pdf/2411.07071v1)

**Tags**: cs.LG cs.AI cs.CL 



### Recent Advances in Named Entity Recognition: A Comprehensive Survey and   Comparative Study
**Authors**: Imed Keraghel, Stanislas Morbieu, Mohamed Nadif

**Updated**: 2024-11-11T15:45:02Z

**Summary**: Named Entity Recognition seeks to extract substrings within a text that name real-world objects and to determine their type (for example, whether they refer to persons or organizations). In this survey, we first present an overview of recent popular approaches, including advancements in Transformer-based methods and Large Language Models (LLMs) that have not had much coverage in other surveys. In addition, we discuss reinforcement learning and graph-based approaches, highlighting their role in enhancing NER performance. Second, we focus on methods designed for datasets with scarce annotations. Third, we evaluate the performance of the main NER implementations on a variety of datasets with differing characteristics (as regards their domain, their size, and their number of classes). We thus provide a deep comparison of algorithms that have never been considered together. Our experiments shed some light on how the characteristics of datasets affect the behavior of the methods we compare.

**Link**: [arxiv](http://arxiv.org/abs/2401.10825v2),  [pdf](http://arxiv.org/pdf/2401.10825v2)

**Tags**: cs.CL cs.LG 68T50, 68Q32 



### Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training
**Authors**: Elia Cunegatti, Leonardo Lucio Custode, Giovanni Iacca

**Updated**: 2024-11-11T15:30:16Z

**Summary**: Network pruning is a set of computational techniques that aim to reduce a given model's computational cost by removing a subset of its parameters while having minimal impact on performance. Throughout the last decade, the most widely used pruning paradigm has focused on pruning and re-training, which nowadays is inconvenient due to the vast amount of pre-trained models, which are in any case too expensive to re-train. In this paper, we exploit functional information from dense pre-trained models, i.e., their activations, to obtain sparse models that maximize the activations' alignment w.r.t. their corresponding dense models. Hence, we propose \textsc{NeuroAl}, a \emph{top-up} algorithm that can be used on top of any given pruning algorithm for LLMs, that modifies the block-wise and row-wise sparsity ratios to maximize the \emph{neuron alignment} among activations. Moreover, differently from existing methods, our approach adaptively selects the best parameters for the block-wise and row-wise sparsity ratios w.r.t. to the model and the desired sparsity (given as input), and requires \emph{no re-training}. We test our method on 4 different LLM families and 3 different sparsity ratios, showing how it consistently outperforms the latest state-of-the-art techniques. The code is available at https://github.com/eliacunegatti/NeuroAL.

**Link**: [arxiv](http://arxiv.org/abs/2411.07066v1),  [pdf](http://arxiv.org/pdf/2411.07066v1)

**Tags**: cs.LG cs.AI cs.CL 



### A Novel RFID Authentication Protocol Based on A Block-Order-Modulus   Variable Matrix Encryption Algorithm
**Authors**: Yan Wang, Ruiqi Liu, Tong Gao, Feng Shu, Xuemei Lei, Yongpeng Wu, Guan Gui, Jiangzhou Wang

**Updated**: 2024-11-11T15:16:27Z

**Summary**: In this paper, authentication for mobile radio frequency identification (RFID) systems with low-cost RFID sensor tags is studied. Firstly, an adaptive modulus (AM) encryption algorithm is proposed. Subsequently, in order to enhance the security without additional storage of new key matrices, a self-updating encryption order (SUEO) algorithm is designed. Furthermore, a diagonal block local transpose key matrix (DBLTKM) encryption algorithm is presented, which effectively expands the feasible domain of the key space. Based on the above three algorithms, a novel joint AM-SUEO-DBLTKM encryption algorithm is constructed. Making full use of the advantages of the proposed joint algorithm, a two-way RFID authentication protocol, named AM-SUEO-DBLTKM-RFID, is proposed for mobile RFID systems. In addition, the Burrows-Abadi-Needham (BAN) logic and security analysis indicate that the proposed AM-SUEO-DBLTKM-RFID protocol can effectively combat various typical attacks. Numerical results demonstrate that the proposed AM-SUEO-DBLTKM algorithm can save 99.59% of tag storage over traditional algorithms. Finally, the low computational complexity as well as the low storage cost of the proposed AM-SUEO-DBLTKM-RFID protocol facilitates deployment within low-cost RFID sensor tags.

**Link**: [arxiv](http://arxiv.org/abs/2312.10593v5),  [pdf](http://arxiv.org/pdf/2312.10593v5)

**Tags**: cs.CR eess.SP 



### LIFBench: Evaluating the Instruction Following Performance and Stability   of Large Language Models in Long-Context Scenarios
**Authors**: Xiaodong Wu, Minhao Wang, Yichen Liu, Xiaoming Shi, He Yan, Xiangju Lu, Junmin Zhu, Wei Zhang

**Updated**: 2024-11-11T14:43:51Z

**Summary**: As Large Language Models (LLMs) continue to advance in natural language processing (NLP), their ability to stably follow instructions in long-context inputs has become crucial for real-world applications. While existing benchmarks assess various LLM capabilities, they rarely focus on instruction-following in long-context scenarios or stability on different inputs. In response, we introduce the Long-context Instruction-Following Benchmark (LIFBench), a scalable dataset designed to evaluate LLMs' instruction-following capabilities and stability across long contexts. LIFBench comprises three long-context scenarios and eleven diverse tasks, supported by 2,766 instructions generated through an automated expansion method across three dimensions: length, expression, and variables. For evaluation, we propose LIFEval, a rubric-based assessment framework that provides precise, automated scoring of complex LLM responses without relying on LLM-assisted evaluations or human judgments. This approach facilitates a comprehensive analysis of model performance and stability across various perspectives. We conduct extensive experiments on 20 notable LLMs across six length intervals, analyzing their instruction-following capabilities and stability. Our work contributes LIFBench and LIFEval as robust tools for assessing LLM performance in complex, long-context settings, providing insights that can inform future LLM development.

**Link**: [arxiv](http://arxiv.org/abs/2411.07037v1),  [pdf](http://arxiv.org/pdf/2411.07037v1)

**Tags**: cs.CL 



### Dynamic and Textual Graph Generation Via Large-Scale LLM-based Agent   Simulation
**Authors**: Jiarui Ji, Runlin Lei, Jialing Bi, Zhewei Wei, Yankai Lin, Xuchen Pan, Yaliang Li, Bolin Ding

**Updated**: 2024-11-11T14:41:53Z

**Summary**: Graph generation is a fundamental task that has been extensively studied in social, technological, and scientific analysis. For modeling the dynamic graph evolution process, traditional rule-based methods struggle to capture community structures within graphs, while deep learning methods only focus on fitting training graphs. This limits existing graph generators to producing graphs that adhere to predefined rules or closely resemble training datasets, achieving poor performance in dynamic graph generation. Given that graphs are abstract representations arising from pairwise interactions in human activities, a realistic simulation of human-wise interaction could provide deeper insights into the graph evolution mechanism. With the increasing recognition of large language models (LLMs) in simulating human behavior, we introduce GraphAgent-Generator (GAG), a novel simulation-based framework for dynamic graph generation. Without training or fine-tuning process of LLM, our framework effectively replicates seven macro-level structural characteristics in established network science theories while surpassing existing baselines in graph expansion tasks by 31\% on specific evaluation metrics. Through node classification task, we validate GAG effectively preserves characteristics of real-world network for node-wise textual features in generated text-rich graph. Furthermore, by incorporating parallel acceleration, GAG supports generating graphs with up to nearly 100,000 nodes or 10 million edges through large-scale LLM-based agent simulation, with a minimum speed-up of 90.4\%. The source code is available at https://anonymous.4open.science/r/GraphAgent-2206.

**Link**: [arxiv](http://arxiv.org/abs/2410.09824v4),  [pdf](http://arxiv.org/pdf/2410.09824v4)

**Tags**: cs.CL 



### PediatricsGPT: Large Language Models as Chinese Medical Assistants for   Pediatric Applications
**Authors**: Dingkang Yang, Jinjie Wei, Dongling Xiao, Shunli Wang, Tong Wu, Gang Li, Mingcheng Li, Shuaibing Wang, Jiawei Chen, Yue Jiang, Qingyao Xu, Ke Li, Peng Zhai, Lihua Zhang

**Updated**: 2024-11-11T14:36:35Z

**Summary**: Developing intelligent pediatric consultation systems offers promising prospects for improving diagnostic efficiency, especially in China, where healthcare resources are scarce. Despite recent advances in Large Language Models (LLMs) for Chinese medicine, their performance is sub-optimal in pediatric applications due to inadequate instruction data and vulnerable training procedures. To address the above issues, this paper builds PedCorpus, a high-quality dataset of over 300,000 multi-task instructions from pediatric textbooks, guidelines, and knowledge graph resources to fulfil diverse diagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the first Chinese pediatric LLM assistant built on a systematic and robust training pipeline. In the continuous pre-training phase, we introduce a hybrid instruction pre-training mechanism to mitigate the internal-injected knowledge inconsistency of LLMs for medical domain adaptation. Immediately, the full-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the general medical knowledge schema into the models. After that, we devise a direct following preference optimization to enhance the generation of pediatrician-like humanistic responses. In the parameter-efficient secondary SFT phase, a mixture of universal-specific experts strategy is presented to resolve the competency conflict between medical generalist and pediatric expertise mastery. Extensive results based on the metrics, GPT-4, and doctor evaluations on distinct doctor downstream tasks show that PediatricsGPT consistently outperforms previous Chinese medical LLMs. Our model and dataset will be open-source for community development.

**Link**: [arxiv](http://arxiv.org/abs/2405.19266v4),  [pdf](http://arxiv.org/pdf/2405.19266v4)

**Tags**: cs.CL 



### Aligning LLMs for FL-free Program Repair
**Authors**: Junjielong Xu, Ying Fu, Shin Hwei Tan, Pinjia He

**Updated**: 2024-11-11T14:35:45Z

**Summary**: Large language models (LLMs) have achieved decent results on automated program repair (APR). However, the next token prediction training objective of decoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction objective of current infilling-style methods, which impedes LLMs from fully leveraging pre-trained knowledge for program repair. In addition, while some LLMs can locate and repair bugs in certain functions using the related artifacts (e.g., test cases), existing methods still depend on statement-level fault localization methods to provide a list of buggy hunks for repair. This restriction hinders LLMs from exploring potential patches beyond the given locations.   In this paper, we investigate a new approach to adapt LLMs to program repair. Our core insight is that LLM's APR capability can be greatly improved by simply aligning the output to their training objective and allowing them to refine the whole program without first identifying faulty statements. Based on this insight, we designed D4C, a straightforward prompting framework for APR. D4C can repair 180 bugs correctly in Defects4J, with each patch being sampled only 10 times. This surpasses the SOTA APR methods with perfect fault localization by 10% and reduces the patch sampling number by 90%. Our findings reveal that (1) objective alignment is crucial for fully exploiting LLM's pre-trained capability, and (2) replacing the traditional localize-buggy-hunks-then-repair workflow with direct debugging is more effective for LLM-based APR methods. Thus, we believe this paper introduces a new mindset for harnessing LLMs in APR.

**Link**: [arxiv](http://arxiv.org/abs/2404.08877v2),  [pdf](http://arxiv.org/pdf/2404.08877v2)

**Tags**: cs.SE cs.CL cs.LG 



### Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation
**Authors**: Ziwei Liu, Liang Zhang, Qian Li, Jianghua Wu, Guangxu Zhu

**Updated**: 2024-11-11T14:25:37Z

**Summary**: Retrieval-augmented generation (RAG) has shown impressive capability in providing reliable answer predictions and addressing hallucination problems. A typical RAG implementation uses powerful retrieval models to extract external information and large language models (LLMs) to generate answers. In contrast, recent LLM-based retrieval has gained attention for its substantial improvements in information retrieval (IR) due to the LLMs' semantic understanding capability. However, directly applying LLM to RAG systems presents challenges. This may cause feature locality problems as massive parametric knowledge can hinder effective usage of global information across the corpus; for example, an LLM-based retriever often inputs document summaries instead of full documents. Moreover, various pre-trained tasks in LLMs introduce variance, further weakening performance as a retriever.   To address these issues, we propose a novel two-stage fine-tuning architecture called Invar-RAG. In the retrieval stage, an LLM-based retriever is constructed by integrating LoRA-based representation learning to tackle feature locality issues. To enhance retrieval performance, we develop two patterns (invariant and variant patterns) and an invariance loss to reduce LLM variance. In the generation stage, a refined fine-tuning method is employed to improve LLM accuracy in generating answers based on retrieved information. Experimental results show that Invar-RAG significantly outperforms existing baselines across three open-domain question answering (ODQA) datasets. Code is available in the Supplementary Material for reproducibility.

**Link**: [arxiv](http://arxiv.org/abs/2411.07021v1),  [pdf](http://arxiv.org/pdf/2411.07021v1)

**Tags**: cs.IR 



### LLMCount: Enhancing Stationary mmWave Detection with Multimodal-LLM
**Authors**: Boyan Li, Shengyi Ding, Deen Ma, Yixuan Wu, Hongjie Liao, Kaiyuan Hu

**Updated**: 2024-11-11T13:56:30Z

**Summary**: Millimeter wave sensing provides people with the capability of sensing the surrounding crowds in a non-invasive and privacy-preserving manner, which holds huge application potential. However, detecting stationary crowds remains challenging due to several factors such as minimal movements (like breathing or casual fidgets), which can be easily treated as noise clusters during data collection and consequently filtered in the following processing procedures. Additionally, the uneven distribution of signal power due to signal power attenuation and interferences resulting from external reflectors or absorbers further complicates accurate detection. To address these challenges and enable stationary crowd detection across various application scenarios requiring specialized domain adaption, we introduce LLMCount, the first system to harness the capabilities of large-language models (LLMs) to enhance crowd detection performance. By exploiting the decision-making capability of LLM, we can successfully compensate the signal power to acquire a uniform distribution and thereby achieve a detection with higher accuracy. To assess the system's performance, comprehensive evaluations are conducted under diversified scenarios like hall, meeting room, and cinema. The evaluation results show that our proposed approach reaches high detection accuracy with lower overall latency compared with previous methods.

**Link**: [arxiv](http://arxiv.org/abs/2409.16209v2),  [pdf](http://arxiv.org/pdf/2409.16209v2)

**Tags**: cs.CV 



### Sensing Capacity for Integrated Sensing and Communication Systems in   Low-Altitude Economy
**Authors**: Jiahua Wan, Hong Ren, Cunhua Pan, Zhenkun Zhang, Songtao Gao, Yiming Yu, Chengzhong Wang

**Updated**: 2024-11-11T13:41:05Z

**Summary**: The burgeoning significance of the low-altitude economy (LAE) has garnered considerable interest, largely fuelled by the widespread deployment of unmanned aerial vehicles (UAVs). To tackle the challenges associated with the detection of unauthorized UAVs and the efficient scheduling of authorized UAVs, this letter introduces a novel performance metric, termed sensing capacity, for integrated sensing and communication (ISAC) systems. This metric, which quantifies the capability of a base station (BS) to detect multiple UAVs simultaneously, leverages signal-to-noise ratio (SNR) and probability of detection (PD) as key intermediate variables. Through mathematical derivations, we can derive a closed-form solution for the maximum number of UAVs that can be detected by the BS while adhering to a specific SNR constraint. Furthermore, an approximate solution based on PD constraints is proposed to facilitate the efficient determination of the threshold for the maximum number of detectable UAVs. The accuracy of this analytical approach is verified through extensive simulation results.

**Link**: [arxiv](http://arxiv.org/abs/2411.06983v1),  [pdf](http://arxiv.org/pdf/2411.06983v1)

**Tags**: eess.SP 



### xNVMe: Unleashing Storage Hardware-Software Co-design
**Authors**: Simon A. F. Lund, Vivek Shah

**Updated**: 2024-11-11T13:38:49Z

**Summary**: NVMe SSD hardware has witnessed widespread deployment as commodity and enterprise hardware due to its high performance and rich feature set. Despite the open specifications of various NVMe protocols by the NVMe Express group and NVMe being of software abstractions to program the underlying hardware. The myriad storage I/O paths such as POSIX storage API, ad-hoc OS mechanisms, and userspace I/O libraries have different syntax and semantics that complicate software development and stand in the way of mass adoption and evolution of the NVMe ecosystem. To unify the diverse I/O storage paths, we built xNVMe that exposes a single message-passing API to support both asynchronous and synchronous communication with NVMe devices. xNVMe provides various command sets to support diverse storage I/O paths in different OS (e.g., Linux, FreeBSD, Windows, and MacOS) and userspace libraries (e.g., SPDK) with minimal overhead. xNVMe is an Open Source project and has gained traction amongst various industry stakeholders. In this paper, we elaborate on the lessons that we have learned in the project during its evolution. We also provide some ongoing and future work planned for the project. We hope the database and storage systems community can join in the effort to both extend xNVMe and leverage it as a building block for innovative co-design of storage systems on modern NVMe hardware.

**Link**: [arxiv](http://arxiv.org/abs/2411.06980v1),  [pdf](http://arxiv.org/pdf/2411.06980v1)

**Tags**: cs.OS cs.DB cs.DC 



### Examining Attacks on Consensus and Incentive Systems in Proof-of-Work   Blockchains: A Systematic Literature Review
**Authors**: Dinitha Wijewardhana, Sugandima Vidanagamachchi, Nalin Arachchilage

**Updated**: 2024-11-11T13:30:26Z

**Summary**: Cryptocurrencies have gained popularity due to their transparency, security, and accessibility compared to traditional financial systems, with Bitcoin, introduced in 2009, leading the market. Bitcoin's security relies on blockchain technology - a decentralized ledger consisting of a consensus and an incentive mechanism. The consensus mechanism, Proof of Work (PoW), requires miners to solve difficult cryptographic puzzles to add new blocks, while the incentive mechanism rewards them with newly minted bitcoins. However, as Bitcoin's acceptance grows, it faces increasing threats from attacks targeting these mechanisms, such as selfish mining, double-spending, and block withholding. These attacks compromise security, efficiency, and reward distribution. Recent research shows that these attacks can be combined with each other or with either malicious strategies, such as network-layer attacks, or non-malicious strategies, like honest mining. These combinations lead to more sophisticated attacks, increasing the attacker's success rates and profitability. Therefore, understanding and evaluating these attacks is essential for developing effective countermeasures and ensuring long-term security. This paper begins by examining individual attacks executed in isolation and their profitability. It then explores how combining these attacks with each other or with other malicious and non-malicious strategies can enhance their overall effectiveness and profitability. The analysis further explores how the deployment of attacks such as selfish mining and block withholding by multiple competing mining pools against each other impacts their economic returns. Lastly, a set of design guidelines is provided, outlining areas future work should focus on to prevent or mitigate the identified threats.

**Link**: [arxiv](http://arxiv.org/abs/2411.00349v2),  [pdf](http://arxiv.org/pdf/2411.00349v2)

**Tags**: cs.CR cs.AI 



### Sniff AI: Is My 'Spicy' Your 'Spicy'? Exploring LLM's Perceptual   Alignment with Human Smell Experiences
**Authors**: Shu Zhong, Zetao Zhou, Christopher Dawes, Giada Brianz, Marianna Obrist

**Updated**: 2024-11-11T12:56:52Z

**Summary**: Aligning AI with human intent is important, yet perceptual alignment-how AI interprets what we see, hear, or smell-remains underexplored. This work focuses on olfaction, human smell experiences. We conducted a user study with 40 participants to investigate how well AI can interpret human descriptions of scents. Participants performed "sniff and describe" interactive tasks, with our designed AI system attempting to guess what scent the participants were experiencing based on their descriptions. These tasks evaluated the Large Language Model's (LLMs) contextual understanding and representation of scent relationships within its internal states - high-dimensional embedding space. Both quantitative and qualitative methods were used to evaluate the AI system's performance. Results indicated limited perceptual alignment, with biases towards certain scents, like lemon and peppermint, and continued failing to identify others, like rosemary. We discuss these findings in light of human-AI alignment advancements, highlighting the limitations and opportunities for enhancing HCI systems with multisensory experience integration.

**Link**: [arxiv](http://arxiv.org/abs/2411.06950v1),  [pdf](http://arxiv.org/pdf/2411.06950v1)

**Tags**: cs.CL cs.HC 



### Cancer-Answer: Empowering Cancer Care with Advanced Large Language   Models
**Authors**: Aniket Deroy, Subhankar Maity

**Updated**: 2024-11-11T12:54:22Z

**Summary**: Gastrointestinal (GI) tract cancers account for a substantial portion of the global cancer burden, where early diagnosis is critical for improved management and patient outcomes. The complex aetiologies and overlapping symptoms across GI cancers often delay diagnosis, leading to suboptimal treatment strategies. Cancer-related queries are crucial for timely diagnosis, treatment, and patient education, as access to accurate, comprehensive information can significantly influence outcomes. However, the complexity of cancer as a disease, combined with the vast amount of available data, makes it difficult for clinicians and patients to quickly find precise answers. To address these challenges, we leverage large language models (LLMs) such as GPT-3.5 Turbo to generate accurate, contextually relevant responses to cancer-related queries. Pre-trained with medical data, these models provide timely, actionable insights that support informed decision-making in cancer diagnosis and care, ultimately improving patient outcomes. We calculate two metrics: A1 (which represents the fraction of entities present in the model-generated answer compared to the gold standard) and A2 (which represents the linguistic correctness and meaningfulness of the model-generated answer with respect to the gold standard), achieving maximum values of 0.546 and 0.881, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2411.06946v1),  [pdf](http://arxiv.org/pdf/2411.06946v1)

**Tags**: cs.CL 



### Hierarchical Narrative Analysis: Unraveling Perceptions of Generative AI
**Authors**: Riona Matsuoka, Hiroki Matsumoto, Takahiro Yoshida, Tomohiro Watanabe, Ryoma Kondo, Ryohei Hisano

**Updated**: 2024-11-11T12:50:44Z

**Summary**: Written texts reflect an author's perspective, making the thorough analysis of literature a key research method in fields such as the humanities and social sciences. However, conventional text mining techniques like sentiment analysis and topic modeling are limited in their ability to capture the hierarchical narrative structures that reveal deeper argumentative patterns. To address this gap, we propose a method that leverages large language models (LLMs) to extract and organize these structures into a hierarchical framework. We validate this approach by analyzing public opinions on generative AI collected by Japan's Agency for Cultural Affairs, comparing the narratives of supporters and critics. Our analysis provides clearer visualization of the factors influencing divergent opinions on generative AI, offering deeper insights into the structures of agreement and disagreement.

**Link**: [arxiv](http://arxiv.org/abs/2409.11032v3),  [pdf](http://arxiv.org/pdf/2409.11032v3)

**Tags**: cs.CL 



### Efficient Unsupervised Domain Adaptation Regression for Spatial-Temporal   Air Quality Sensor Fusion
**Authors**: Keivan Faghih Niresi, Ismail Nejjar, Olga Fink

**Updated**: 2024-11-11T12:20:57Z

**Summary**: The deployment of affordable Internet of Things (IoT) sensors for air pollution monitoring has increased in recent years due to their scalability and cost-effectiveness. However, accurately calibrating these sensors in uncontrolled environments remains a significant challenge. While expensive reference sensors can provide accurate ground truth data, they are often deployed on a limited scale due to high costs, leading to a scarcity of labeled data. In diverse urban environments, data distributions constantly shift due to varying factors such as traffic patterns, industrial activities, and weather conditions, which impact sensor readings. Consequently, traditional machine learning models -- despite their increasing deployment for environmental sensor calibration -- often struggle to provide reliable pollutant measurements across different locations due to domain shifts. To address these challenges, we propose a novel unsupervised domain adaptation (UDA) method specifically tailored for regression tasks on graph-structured data. Our approach leverages Graph Neural Networks (GNNs) to model the relationships between sensors. To effectively capture critical spatial-temporal interactions, we incorporate spatial-temporal graph neural networks (STGNNs), which extend GNNs by incorporating temporal dynamics. To handle the resulting larger embeddings, we propose a domain adaptation method using a closed-form solution inspired by the Tikhonov-regularized least-squares problem. This method leverages Cholesky decomposition and power iteration to align the subspaces between source and target domains. By aligning these subspaces, our approach allows low-cost IoT sensors to learn calibration parameters from expensive reference sensors. This facilitates reliable pollutant measurements in new locations without the need for additional costly equipment.

**Link**: [arxiv](http://arxiv.org/abs/2411.06917v1),  [pdf](http://arxiv.org/pdf/2411.06917v1)

**Tags**: cs.LG eess.SP 



### CTIBench: A Benchmark for Evaluating LLMs in Cyber Threat Intelligence
**Authors**: Md Tanvirul Alam, Dipkamal Bhusal, Le Nguyen, Nidhi Rastogi

**Updated**: 2024-11-11T12:00:35Z

**Summary**: Cyber threat intelligence (CTI) is crucial in today's cybersecurity landscape, providing essential insights to understand and mitigate the ever-evolving cyber threats. The recent rise of Large Language Models (LLMs) have shown potential in this domain, but concerns about their reliability, accuracy, and hallucinations persist. While existing benchmarks provide general evaluations of LLMs, there are no benchmarks that address the practical and applied aspects of CTI-specific tasks. To bridge this gap, we introduce CTIBench, a benchmark designed to assess LLMs' performance in CTI applications. CTIBench includes multiple datasets focused on evaluating knowledge acquired by LLMs in the cyber-threat landscape. Our evaluation of several state-of-the-art models on these tasks provides insights into their strengths and weaknesses in CTI contexts, contributing to a better understanding of LLM capabilities in CTI.

**Link**: [arxiv](http://arxiv.org/abs/2406.07599v3),  [pdf](http://arxiv.org/pdf/2406.07599v3)

**Tags**: cs.CR cs.AI 



### LongSafetyBench: Long-Context LLMs Struggle with Safety Issues
**Authors**: Mianqiu Huang, Xiaoran Liu, Shaojun Zhou, Mozhi Zhang, Chenkun Tan, Pengyu Wang, Qipeng Guo, Zhe Xu, Linyang Li, Zhikai Lei, Linlin Li, Qun Liu, Yaqian Zhou, Xipeng Qiu, Xuanjing Huang

**Updated**: 2024-11-11T11:57:37Z

**Summary**: With the development of large language models (LLMs), the sequence length of these models continues to increase, drawing significant attention to long-context language models. However, the evaluation of these models has been primarily limited to their capabilities, with a lack of research focusing on their safety. Existing work, such as ManyShotJailbreak, has to some extent demonstrated that long-context language models can exhibit safety concerns. However, the methods used are limited and lack comprehensiveness. In response, we introduce \textbf{LongSafetyBench}, the first benchmark designed to objectively and comprehensively evaluate the safety of long-context models. LongSafetyBench consists of 10 task categories, with an average length of 41,889 words. After testing eight long-context language models on LongSafetyBench, we found that existing models generally exhibit insufficient safety capabilities. The proportion of safe responses from most mainstream long-context LLMs is below 50\%. Moreover, models' safety performance in long-context scenarios does not always align with that in short-context scenarios. Further investigation revealed that long-context models tend to overlook harmful content within lengthy texts. We also proposed a simple yet effective solution, allowing open-source models to achieve performance comparable to that of top-tier closed-source models. We believe that LongSafetyBench can serve as a valuable benchmark for evaluating the safety capabilities of long-context language models. We hope that our work will encourage the broader community to pay attention to the safety of long-context models and contribute to the development of solutions to improve the safety of long-context LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2411.06899v1),  [pdf](http://arxiv.org/pdf/2411.06899v1)

**Tags**: cs.CL cs.AI cs.LG 



### Meaningful Learning: Enhancing Abstract Reasoning in Large Language   Models via Generic Fact Guidance
**Authors**: Kai Xiong, Xiao Ding, Ting Liu, Bing Qin, Dongliang Xu, Qing Yang, Hongtao Liu, Yixin Cao

**Updated**: 2024-11-11T11:35:28Z

**Summary**: Large language models (LLMs) have developed impressive performance and strong explainability across various reasoning scenarios, marking a significant stride towards mimicking human-like intelligence. Despite this, when tasked with several simple questions supported by a generic fact, LLMs often struggle to abstract and apply the generic fact to provide consistent and precise answers, revealing a deficiency in abstract reasoning abilities. This has sparked a vigorous debate about whether LLMs are genuinely reasoning or merely memorizing. In light of this, we design a preliminary study to quantify and delve into the abstract reasoning abilities of existing LLMs. Our findings reveal a substantial discrepancy between their general reasoning and abstract reasoning performances. To relieve this problem, we tailor an abstract reasoning dataset (AbsR) together with a meaningful learning paradigm to teach LLMs how to leverage generic facts for reasoning purposes. The results show that our approach not only boosts the general reasoning performance of LLMs but also makes considerable strides towards their capacity for abstract reasoning, moving beyond simple memorization or imitation to a more nuanced understanding and application of generic facts. The code is available at https://github.com/Waste-Wood/MeanLearn.

**Link**: [arxiv](http://arxiv.org/abs/2403.09085v2),  [pdf](http://arxiv.org/pdf/2403.09085v2)

**Tags**: cs.CL cs.AI 



### LMLPA: Language Model Linguistic Personality Assessment
**Authors**: Jingyao Zheng, Xian Wang, Simo Hosio, Xiaoxian Xu, Lik-Hang Lee

**Updated**: 2024-11-11T11:32:21Z

**Summary**: Large Language Models (LLMs) are increasingly used in everyday life and research. One of the most common use cases is conversational interactions, enabled by the language generation capabilities of LLMs. Just as between two humans, a conversation between an LLM-powered entity and a human depends on the personality of the conversants. However, measuring the personality of a given LLM is currently a challenge. This paper introduces the Language Model Linguistic Personality Assessment (LMLPA), a system designed to evaluate the linguistic personalities of LLMs. Our system helps to understand LLMs' language generation capabilities by quantitatively assessing the distinct personality traits reflected in their linguistic outputs. Unlike traditional human-centric psychometrics, the LMLPA adapts a personality assessment questionnaire, specifically the Big Five Inventory, to align with the operational capabilities of LLMs, and also incorporates the findings from previous language-based personality measurement literature. To mitigate sensitivity to the order of options, our questionnaire is designed to be open-ended, resulting in textual answers. Thus, the AI rater is needed to transform ambiguous personality information from text responses into clear numerical indicators of personality traits. Utilising Principal Component Analysis and reliability validations, our findings demonstrate that LLMs possess distinct personality traits that can be effectively quantified by the LMLPA. This research contributes to Human-Computer Interaction and Human-Centered AI, providing a robust framework for future studies to refine AI personality assessments and expand their applications in multiple areas, including education and manufacturing.

**Link**: [arxiv](http://arxiv.org/abs/2410.17632v2),  [pdf](http://arxiv.org/pdf/2410.17632v2)

**Tags**: cs.CL cs.AI I.2 



### GraphRPM: Risk Pattern Mining on Industrial Large Attributed Graphs
**Authors**: Sheng Tian, Xintan Zeng, Yifei Hu, Baokun Wang, Yongchao Liu, Yue Jin, Changhua Meng, Chuntao Hong, Tianyi Zhang, Weiqiang Wang

**Updated**: 2024-11-11T11:20:30Z

**Summary**: Graph-based patterns are extensively employed and favored by practitioners within industrial companies due to their capacity to represent the behavioral attributes and topological relationships among users, thereby offering enhanced interpretability in comparison to black-box models commonly utilized for classification and recognition tasks. For instance, within the scenario of transaction risk management, a graph pattern that is characteristic of a particular risk category can be readily employed to discern transactions fraught with risk, delineate networks of criminal activity, or investigate the methodologies employed by fraudsters. Nonetheless, graph data in industrial settings is often characterized by its massive scale, encompassing data sets with millions or even billions of nodes, making the manual extraction of graph patterns not only labor-intensive but also necessitating specialized knowledge in particular domains of risk. Moreover, existing methodologies for mining graph patterns encounter significant obstacles when tasked with analyzing large-scale attributed graphs. In this work, we introduce GraphRPM, an industry-purpose parallel and distributed risk pattern mining framework on large attributed graphs. The framework incorporates a novel edge-involved graph isomorphism network alongside optimized operations for parallel graph computation, which collectively contribute to a considerable reduction in computational complexity and resource expenditure. Moreover, the intelligent filtration of efficacious risky graph patterns is facilitated by the proposed evaluation metrics. Comprehensive experimental evaluations conducted on real-world datasets of varying sizes substantiate the capability of GraphRPM to adeptly address the challenges inherent in mining patterns from large-scale industrial attributed graphs, thereby underscoring its substantial value for industrial deployment.

**Link**: [arxiv](http://arxiv.org/abs/2411.06878v1),  [pdf](http://arxiv.org/pdf/2411.06878v1)

**Tags**: cs.LG cs.AI cs.DC cs.SI 



### LLM-Assisted Relevance Assessments: When Should We Ask LLMs for Help?
**Authors**: Rikiya Takehi, Ellen M. Voorhees, Tetsuya Sakai

**Updated**: 2024-11-11T11:17:35Z

**Summary**: Test collections are information retrieval tools that allow researchers to quickly and easily evaluate ranking algorithms. While test collections have become an integral part of IR research, the process of data creation involves significant efforts in manual annotations, which often makes it very expensive and time-consuming. Thus, the test collections could become small when the budget is limited, which may lead to unstable evaluations. As an alternative, recent studies have proposed the use of large language models (LLMs) to completely replace human assessors. However, while LLMs seem to somewhat correlate with human judgments, they are not perfect and often show bias. Moreover, even if a well-performing LLM or prompt is found on one dataset, there is no guarantee that it will perform similarly in practice, due to difference in tasks and data. Thus a complete replacement with LLMs is argued to be too risky and not fully trustable.   Thus, in this paper, we propose \textbf{L}LM-\textbf{A}ssisted \textbf{R}elevance \textbf{A}ssessments (\textbf{LARA}), an effective method to balance manual annotations with LLM annotations, which helps to make a rich and reliable test collection. We use the LLM's predicted relevance probabilities in order to select the most profitable documents to manually annotate under a budget constraint. While solely relying on LLM's predicted probabilities to manually annotate performs fairly well, with theoretical reasoning, LARA guides the human annotation process even more effectively via online calibration learning. Then, using the calibration model learned from the limited manual annotations, LARA debiases the LLM predictions to annotate the remaining non-assessed data. Empirical evaluations on TREC-COVID and TREC-8 Ad Hoc datasets show that LARA outperforms the alternative solutions under almost any budget constraint.

**Link**: [arxiv](http://arxiv.org/abs/2411.06877v1),  [pdf](http://arxiv.org/pdf/2411.06877v1)

**Tags**: cs.IR 



### AI-Native Multi-Access Future Networks -- The REASON Architecture
**Authors**: Konstantinos Katsaros, Ioannis Mavromatis, Kostantinos Antonakoglou, Saptarshi Ghosh, Dritan Kaleshi, Toktam Mahmoodi, Hamid Asgari, Anastasios Karousos, Iman Tavakkolnia, Hossein Safi, Harald Hass, Constantinos Vrontos, Amin Emami, Juan Parra Ullauri, Shadi Moazzeni, Dimitra Simeonidou

**Updated**: 2024-11-11T11:10:39Z

**Summary**: The development of the sixth generation of communication networks (6G) has been gaining momentum over the past years, with a target of being introduced by 2030. Several initiatives worldwide are developing innovative solutions and setting the direction for the key features of these networks. Some common emerging themes are the tight integration of AI, the convergence of multiple access technologies and sustainable operation, aiming to meet stringent performance and societal requirements. To that end, we are introducing REASON - Realising Enabling Architectures and Solutions for Open Networks. The REASON project aims to address technical challenges in future network deployments, such as E2E service orchestration, sustainability, security and trust management, and policy management, utilising AI-native principles, considering multiple access technologies and cloud-native solutions.   This paper presents REASON's architecture and the identified requirements for future networks. The architecture is meticulously designed for modularity, interoperability, scalability, simplified troubleshooting, flexibility, and enhanced security, taking into consideration current and future standardisation efforts, and the ease of implementation and training. It is structured into four horizontal layers: Physical Infrastructure, Network Service, Knowledge, and End-User Application, complemented by two vertical layers: Management and Orchestration, and E2E Security. This layered approach ensures a robust, adaptable framework to support the diverse and evolving requirements of 6G networks, fostering innovation and facilitating seamless integration of advanced technologies.

**Link**: [arxiv](http://arxiv.org/abs/2411.06870v1),  [pdf](http://arxiv.org/pdf/2411.06870v1)

**Tags**: cs.NI cs.AI cs.SY eess.SY 



### CapeLLM: Support-Free Category-Agnostic Pose Estimation with Multimodal   Large Language Models
**Authors**: Junho Kim, Hyungjin Chung, Byung-Hoon Kim

**Updated**: 2024-11-11T11:08:26Z

**Summary**: Category-agnostic pose estimation (CAPE) has traditionally relied on support images with annotated keypoints, a process that is often cumbersome and may fail to fully capture the necessary correspondences across diverse object categories. Recent efforts have begun exploring the use of text-based queries, where the need for support keypoints is eliminated. However, the optimal use of textual descriptions for keypoints remains an underexplored area. In this work, we introduce CapeLLM, a novel approach that leverages a text-based multimodal large language model (MLLM) for CAPE. Our method only employs query image and detailed text descriptions as an input to estimate category-agnostic keypoints. We conduct extensive experiments to systematically explore the design space of LLM-based CAPE, investigating factors such as choosing the optimal description for keypoints, neural network architectures, and training strategies. Thanks to the advanced reasoning capabilities of the pre-trained MLLM, CapeLLM demonstrates superior generalization and robust performance. Our approach sets a new state-of-the-art on the MP-100 benchmark in the challenging 1-shot setting, marking a significant advancement in the field of category-agnostic pose estimation.

**Link**: [arxiv](http://arxiv.org/abs/2411.06869v1),  [pdf](http://arxiv.org/pdf/2411.06869v1)

**Tags**: cs.CV cs.LG 



### Capturing research literature attitude towards Sustainable Development   Goals: an LLM-based topic modeling approach
**Authors**: Francesco Invernici, Francesca Curati, Jelena Jakimov, Amirhossein Samavi, Anna Bernasconi

**Updated**: 2024-11-11T10:51:31Z

**Summary**: The world is facing a multitude of challenges that hinder the development of human civilization and the well-being of humanity on the planet. The Sustainable Development Goals (SDGs) were formulated by the United Nations in 2015 to address these global challenges by 2030. Natural language processing techniques can help uncover discussions on SDGs within research literature. We propose a completely automated pipeline to 1) fetch content from the Scopus database and prepare datasets dedicated to five groups of SDGs; 2) perform topic modeling, a statistical technique used to identify topics in large collections of textual data; and 3) enable topic exploration through keywords-based search and topic frequency time series extraction. For topic modeling, we leverage the stack of BERTopic scaled up to be applied on large corpora of textual documents (we find hundreds of topics on hundreds of thousands of documents), introducing i) a novel LLM-based embeddings computation for representing scientific abstracts in the continuous space and ii) a hyperparameter optimizer to efficiently find the best configuration for any new big datasets. We additionally produce the visualization of results on interactive dashboards reporting topics' temporal evolution. Results are made inspectable and explorable, contributing to the interpretability of the topic modeling process. Our proposed LLM-based topic modeling pipeline for big-text datasets allows users to capture insights on the evolution of the attitude toward SDGs within scientific abstracts in the 2006-2023 time span. All the results are reproducible by using our system; the workflow can be generalized to be applied at any point in time to any big corpus of textual documents.

**Link**: [arxiv](http://arxiv.org/abs/2411.02943v2),  [pdf](http://arxiv.org/pdf/2411.02943v2)

**Tags**: cs.CL 



### EHRNoteQA: An LLM Benchmark for Real-World Clinical Practice Using   Discharge Summaries
**Authors**: Sunjun Kweon, Jiyoun Kim, Heeyoung Kwak, Dongchul Cha, Hangyul Yoon, Kwanghyun Kim, Jeewon Yang, Seunghyun Won, Edward Choi

**Updated**: 2024-11-11T10:40:50Z

**Summary**: Discharge summaries in Electronic Health Records (EHRs) are crucial for clinical decision-making, but their length and complexity make information extraction challenging, especially when dealing with accumulated summaries across multiple patient admissions. Large Language Models (LLMs) show promise in addressing this challenge by efficiently analyzing vast and complex data. Existing benchmarks, however, fall short in properly evaluating LLMs' capabilities in this context, as they typically focus on single-note information or limited topics, failing to reflect the real-world inquiries required by clinicians. To bridge this gap, we introduce EHRNoteQA, a novel benchmark built on the MIMIC-IV EHR, comprising 962 different QA pairs each linked to distinct patients' discharge summaries. Every QA pair is initially generated using GPT-4 and then manually reviewed and refined by three clinicians to ensure clinical relevance. EHRNoteQA includes questions that require information across multiple discharge summaries and covers eight diverse topics, mirroring the complexity and diversity of real clinical inquiries. We offer EHRNoteQA in two formats: open-ended and multi-choice question answering, and propose a reliable evaluation method for each. We evaluate 27 LLMs using EHRNoteQA and examine various factors affecting the model performance (e.g., the length and number of discharge summaries). Furthermore, to validate EHRNoteQA as a reliable proxy for expert evaluations in clinical practice, we measure the correlation between the LLM performance on EHRNoteQA, and the LLM performance manually evaluated by clinicians. Results show that LLM performance on EHRNoteQA have higher correlation with clinician-evaluated performance (Spearman: 0.78, Kendall: 0.62) compared to other benchmarks, demonstrating its practical relevance in evaluating LLMs in clinical settings.

**Link**: [arxiv](http://arxiv.org/abs/2402.16040v5),  [pdf](http://arxiv.org/pdf/2402.16040v5)

**Tags**: cs.CL 



### Evaluating Large Language Models on Financial Report Summarization: An   Empirical Study
**Authors**: Xinqi Yang, Scott Zang, Yong Ren, Dingjie Peng, Zheng Wen

**Updated**: 2024-11-11T10:36:04Z

**Summary**: In recent years, Large Language Models (LLMs) have demonstrated remarkable versatility across various applications, including natural language understanding, domain-specific knowledge tasks, etc. However, applying LLMs to complex, high-stakes domains like finance requires rigorous evaluation to ensure reliability, accuracy, and compliance with industry standards. To address this need, we conduct a comprehensive and comparative study on three state-of-the-art LLMs, GLM-4, Mistral-NeMo, and LLaMA3.1, focusing on their effectiveness in generating automated financial reports. Our primary motivation is to explore how these models can be harnessed within finance, a field demanding precision, contextual relevance, and robustness against erroneous or misleading information. By examining each model's capabilities, we aim to provide an insightful assessment of their strengths and limitations. Our paper offers benchmarks for financial report analysis, encompassing proposed metrics such as ROUGE-1, BERT Score, and LLM Score. We introduce an innovative evaluation framework that integrates both quantitative metrics (e.g., precision, recall) and qualitative analyses (e.g., contextual fit, consistency) to provide a holistic view of each model's output quality. Additionally, we make our financial dataset publicly available, inviting researchers and practitioners to leverage, scrutinize, and enhance our findings through broader community engagement and collaborative improvement. Our dataset is available on huggingface.

**Link**: [arxiv](http://arxiv.org/abs/2411.06852v1),  [pdf](http://arxiv.org/pdf/2411.06852v1)

**Tags**: cs.CL cs.AI 



### Fast and Efficient Transformer-based Method for Bird's Eye View Instance   Prediction
**Authors**: Miguel Antunes-Garca, Luis M. Bergasa, Santiago Montiel-Marn, Rafael Barea, Fabio Snchez-Garca, ngel Llamazares

**Updated**: 2024-11-11T10:35:23Z

**Summary**: Accurate object detection and prediction are critical to ensure the safety and efficiency of self-driving architectures. Predicting object trajectories and occupancy enables autonomous vehicles to anticipate movements and make decisions with future information, increasing their adaptability and reducing the risk of accidents. Current State-Of-The-Art (SOTA) approaches often isolate the detection, tracking, and prediction stages, which can lead to significant prediction errors due to accumulated inaccuracies between stages. Recent advances have improved the feature representation of multi-camera perception systems through Bird's-Eye View (BEV) transformations, boosting the development of end-to-end systems capable of predicting environmental elements directly from vehicle sensor data. These systems, however, often suffer from high processing times and number of parameters, creating challenges for real-world deployment. To address these issues, this paper introduces a novel BEV instance prediction architecture based on a simplified paradigm that relies only on instance segmentation and flow prediction. The proposed system prioritizes speed, aiming at reduced parameter counts and inference times compared to existing SOTA architectures, thanks to the incorporation of an efficient transformer-based architecture. Furthermore, the implementation of the proposed architecture is optimized for performance improvements in PyTorch version 2.1. Code and trained models are available at https://github.com/miguelag99/Efficient-Instance-Prediction

**Link**: [arxiv](http://arxiv.org/abs/2411.06851v1),  [pdf](http://arxiv.org/pdf/2411.06851v1)

**Tags**: cs.CV cs.LG 



### 1-800-SHARED-TASKS @ NLU of Devanagari Script Languages: Detection of   Language, Hate Speech, and Targets using LLMs
**Authors**: Jebish Purbey, Siddartha Pullakhandam, Kanwal Mehreen, Muhammad Arham, Drishti Sharma, Ashay Srivastava, Ram Mohan Rao Kadiyala

**Updated**: 2024-11-11T10:34:36Z

**Summary**: This paper presents a detailed system description of our entry for the CHiPSAL 2025 shared task, focusing on language detection, hate speech identification, and target detection in Devanagari script languages. We experimented with a combination of large language models and their ensembles, including MuRIL, IndicBERT, and Gemma-2, and leveraged unique techniques like focal loss to address challenges in the natural understanding of Devanagari languages, such as multilingual processing and class imbalance. Our approach achieved competitive results across all tasks: F1 of 0.9980, 0.7652, and 0.6804 for Sub-tasks A, B, and C respectively. This work provides insights into the effectiveness of transformer models in tasks with domain-specific and linguistic challenges, as well as areas for potential improvement in future iterations.

**Link**: [arxiv](http://arxiv.org/abs/2411.06850v1),  [pdf](http://arxiv.org/pdf/2411.06850v1)

**Tags**: cs.CL cs.AI cs.LG 



### LLM-Neo: Parameter Efficient Knowledge Distillation for Large Language   Models
**Authors**: Runming Yang, Taiqiang Wu, Jiahao Wang, Pengfei Hu, Ngai Wong, Yujiu Yang

**Updated**: 2024-11-11T10:07:51Z

**Summary**: In this paper, we propose a novel LLM-Neo framework that efficiently transfers knowledge from a large language model (LLM) teacher to a compact student. Initially, we revisit the knowledge distillation (KD) and low-rank adaption (LoRA), and argue that they share the same paradigm. Inspired by this observation, we explore the strategy that combines LoRA and KD to enhance the efficiency of knowledge transfer. We first summarize some guidelines for this design and further develop the LLM-Neo. Experimental results on compressing Llama 2 and Llama 3 show that LLM-Neo outperforms various baselines. Further analysis demonstrates the robustness of the proposed LLM-Neo on variants of LoRA. The trained models have been available at \href{https://huggingface.co/collections/yang31210999/llm-neo-66e3c882f5579b829ff57eba}{this repository}.

**Link**: [arxiv](http://arxiv.org/abs/2411.06839v1),  [pdf](http://arxiv.org/pdf/2411.06839v1)

**Tags**: cs.CL cs.AI cs.LG 



### Persuasion with Large Language Models: a Survey
**Authors**: Alexander Rogiers, Sander Noels, Maarten Buyl, Tijl De Bie

**Updated**: 2024-11-11T10:05:52Z

**Summary**: The rapid rise of Large Language Models (LLMs) has created new disruptive possibilities for persuasive communication, by enabling fully-automated personalized and interactive content generation at an unprecedented scale. In this paper, we survey the research field of LLM-based persuasion that has emerged as a result. We begin by exploring the different modes in which LLM Systems are used to influence human attitudes and behaviors. In areas such as politics, marketing, public health, e-commerce, and charitable giving, such LLM Systems have already achieved human-level or even super-human persuasiveness. We identify key factors influencing their effectiveness, such as the manner of personalization and whether the content is labelled as AI-generated. We also summarize the experimental designs that have been used to evaluate progress. Our survey suggests that the current and future potential of LLM-based persuasion poses profound ethical and societal risks, including the spread of misinformation, the magnification of biases, and the invasion of privacy. These risks underscore the urgent need for ethical guidelines and updated regulatory frameworks to avoid the widespread deployment of irresponsible and harmful LLM Systems.

**Link**: [arxiv](http://arxiv.org/abs/2411.06837v1),  [pdf](http://arxiv.org/pdf/2411.06837v1)

**Tags**: cs.CL 



### Multimodal Structure-Aware Quantum Data Processing
**Authors**: Hala Hawashin, Mehrnoosh Sadrzadeh

**Updated**: 2024-11-11T10:03:47Z

**Summary**: While large language models (LLMs) have advanced the field of natural language processing (NLP), their "black box" nature obscures their decision-making processes. To address this, researchers developed structured approaches using higher order tensors. These are able to model linguistic relations, but stall when training on classical computers due to their excessive size. Tensors are natural inhabitants of quantum systems and training on quantum computers provides a solution by translating text to variational quantum circuits. In this paper, we develop MultiQ-NLP: a framework for structure-aware data processing with multimodal text+image data. Here, "structure" refers to syntactic and grammatical relationships in language, as well as the hierarchical organization of visual elements in images. We enrich the translation with new types and type homomorphisms and develop novel architectures to represent structure. When tested on a main stream image classification task (SVO Probes), our best model showed a par performance with the state of the art classical models; moreover the best model was fully structured.

**Link**: [arxiv](http://arxiv.org/abs/2411.04242v3),  [pdf](http://arxiv.org/pdf/2411.04242v3)

**Tags**: cs.LG 68T45, 68T50, 68Q12, 68U15, 68U10, 81P45, 81P68 I.2.7; I.2.10; H.5.1 



### HarmLevelBench: Evaluating Harm-Level Compliance and the Impact of   Quantization on Model Alignment
**Authors**: Yannis Belkhiter, Giulio Zizzo, Sergio Maffeis

**Updated**: 2024-11-11T10:02:49Z

**Summary**: With the introduction of the transformers architecture, LLMs have revolutionized the NLP field with ever more powerful models. Nevertheless, their development came up with several challenges. The exponential growth in computational power and reasoning capabilities of language models has heightened concerns about their security. As models become more powerful, ensuring their safety has become a crucial focus in research. This paper aims to address gaps in the current literature on jailbreaking techniques and the evaluation of LLM vulnerabilities. Our contributions include the creation of a novel dataset designed to assess the harmfulness of model outputs across multiple harm levels, as well as a focus on fine-grained harm-level analysis. Using this framework, we provide a comprehensive benchmark of state-of-the-art jailbreaking attacks, specifically targeting the Vicuna 13B v1.5 model. Additionally, we examine how quantization techniques, such as AWQ and GPTQ, influence the alignment and robustness of models, revealing trade-offs between enhanced robustness with regards to transfer attacks and potential increases in vulnerability on direct ones. This study aims to demonstrate the influence of harmful input queries on the complexity of jailbreaking techniques, as well as to deepen our understanding of LLM vulnerabilities and improve methods for assessing model robustness when confronted with harmful content, particularly in the context of compression strategies.

**Link**: [arxiv](http://arxiv.org/abs/2411.06835v1),  [pdf](http://arxiv.org/pdf/2411.06835v1)

**Tags**: cs.CL cs.CR 



### Entity Extraction from High-Level Corruption Schemes via Large Language   Models
**Authors**: Panagiotis Koletsis, Panagiotis-Konstantinos Gemos, Christos Chronis, Iraklis Varlamis, Vasilis Efthymiou, Georgios Th. Papadopoulos

**Updated**: 2024-11-11T10:02:24Z

**Summary**: The rise of financial crime that has been observed in recent years has created an increasing concern around the topic and many people, organizations and governments are more and more frequently trying to combat it. Despite the increase of interest in this area, there is a lack of specialized datasets that can be used to train and evaluate works that try to tackle those problems. This article proposes a new micro-benchmark dataset for algorithms and models that identify individuals and organizations, and their multiple writings, in news articles, and presents an approach that assists in its creation. Experimental efforts are also reported, using this dataset, to identify individuals and organizations in financial-crime-related articles using various low-billion parameter Large Language Models (LLMs). For these experiments, standard metrics (Accuracy, Precision, Recall, F1 Score) are reported and various prompt variants comprising the best practices of prompt engineering are tested. In addition, to address the problem of ambiguous entity mentions, a simple, yet effective LLM-based disambiguation method is proposed, ensuring that the evaluation aligns with reality. Finally, the proposed approach is compared against a widely used state-of-the-art open-source baseline, showing the superiority of the proposed method.

**Link**: [arxiv](http://arxiv.org/abs/2409.13704v2),  [pdf](http://arxiv.org/pdf/2409.13704v2)

**Tags**: cs.CL cs.IR 



### Combining Domain and Alignment Vectors to Achieve Better   Knowledge-Safety Trade-offs in LLMs
**Authors**: Megh Thakkar, Yash More, Quentin Fournier, Matthew Riemer, Pin-Yu Chen, Amal Zouaq, Payel Das, Sarath Chandar

**Updated**: 2024-11-11T09:32:20Z

**Summary**: There is a growing interest in training domain-expert LLMs that excel in specific technical fields compared to their general-purpose instruction-tuned counterparts. However, these expert models often experience a loss in their safety abilities in the process, making them capable of generating harmful content. As a solution, we introduce an efficient and effective merging-based alignment method called \textsc{MergeAlign} that interpolates the domain and alignment vectors, creating safer domain-specific models while preserving their utility. We apply \textsc{MergeAlign} on Llama3 variants that are experts in medicine and finance, obtaining substantial alignment improvements with minimal to no degradation on domain-specific benchmarks. We study the impact of model merging through model similarity metrics and contributions of individual models being merged. We hope our findings open new research avenues and inspire more efficient development of safe expert LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2411.06824v1),  [pdf](http://arxiv.org/pdf/2411.06824v1)

**Tags**: cs.AI 



### Large Language Model in Medical Informatics: Direct Classification and   Enhanced Text Representations for Automatic ICD Coding
**Authors**: Zeyd Boukhers, AmeerAli Khan, Qusai Ramadan, Cong Yang

**Updated**: 2024-11-11T09:31:46Z

**Summary**: Addressing the complexity of accurately classifying International Classification of Diseases (ICD) codes from medical discharge summaries is challenging due to the intricate nature of medical documentation. This paper explores the use of Large Language Models (LLM), specifically the LLAMA architecture, to enhance ICD code classification through two methodologies: direct application as a classifier and as a generator of enriched text representations within a Multi-Filter Residual Convolutional Neural Network (MultiResCNN) framework. We evaluate these methods by comparing them against state-of-the-art approaches, revealing LLAMA's potential to significantly improve classification outcomes by providing deep contextual insights into medical texts.

**Link**: [arxiv](http://arxiv.org/abs/2411.06823v1),  [pdf](http://arxiv.org/pdf/2411.06823v1)

**Tags**: cs.LG cs.IR 



### Instruction Tuning for Large Language Models: A Survey
**Authors**: Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang

**Updated**: 2024-11-11T09:25:48Z

**Summary**: This paper surveys research works in the quickly advancing field of instruction tuning (IT), which can also be referred to as supervised fine-tuning (SFT)\footnote{In this paper, unless specified otherwise, instruction tuning (IT) will be equivalent to supervised fine-tuning (SFT).}, a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and application, along with analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research.Project page: github.com/xiaoya-li/Instruction-Tuning-Survey

**Link**: [arxiv](http://arxiv.org/abs/2308.10792v7),  [pdf](http://arxiv.org/pdf/2308.10792v7)

**Tags**: cs.CL cs.AI cs.LG 



### Streetwise Agents: Empowering Offline RL Policies to Outsmart Exogenous   Stochastic Disturbances in RTC
**Authors**: Aditya Soni, Mayukh Das, Anjaly Parayil, Supriyo Ghosh, Shivam Shandilya, Ching-An Cheng, Vishak Gopal, Sami Khairy, Gabriel Mittag, Yasaman Hosseinkashi, Chetan Bansal

**Updated**: 2024-11-11T09:22:09Z

**Summary**: The difficulty of exploring and training online on real production systems limits the scope of real-time online data/feedback-driven decision making. The most feasible approach is to adopt offline reinforcement learning from limited trajectory samples. However, after deployment, such policies fail due to exogenous factors that temporarily or permanently disturb/alter the transition distribution of the assumed decision process structure induced by offline samples. This results in critical policy failures and generalization errors in sensitive domains like Real-Time Communication (RTC). We solve this crucial problem of identifying robust actions in presence of domain shifts due to unseen exogenous stochastic factors in the wild. As it is impossible to learn generalized offline policies within the support of offline data that are robust to these unseen exogenous disturbances, we propose a novel post-deployment shaping of policies (Streetwise), conditioned on real-time characterization of out-of-distribution sub-spaces. This leads to robust actions in bandwidth estimation (BWE) of network bottlenecks in RTC and in standard benchmarks. Our extensive experimental results on BWE and other standard offline RL benchmark environments demonstrate a significant improvement ($\approx$ 18% on some scenarios) in final returns wrt. end-user metrics over state-of-the-art baselines.

**Link**: [arxiv](http://arxiv.org/abs/2411.06815v1),  [pdf](http://arxiv.org/pdf/2411.06815v1)

**Tags**: cs.LG 



### CUDRT: Benchmarking the Detection Models of Human vs. Large Language   Models Generated Texts
**Authors**: Zhen Tao, Yanfang Chen, Dinghao Xi, Zhiyu Li, Wei Xu

**Updated**: 2024-11-11T09:19:46Z

**Summary**: While large language models (LLMs) have greatly enhanced text generation across industries, their human-like outputs make distinguishing between human and AI authorship challenging. Although many LLM-generated text detectors exist, current benchmarks mainly rely on static datasets, limiting their effectiveness in assessing model-based detectors requiring prior training. Furthermore, these benchmarks focus on specific scenarios like question answering and text refinement and are primarily limited to English, overlooking broader linguistic applications and LLM subtleties. To address these gaps, we construct a comprehensive bilingual benchmark in Chinese and English to rigorously evaluate mainstream LLM-generated text detection methods. We categorize LLM text generation into five key operations-Create, Update, Delete, Rewrite, and Translate (CUDRT)-covering the full range of LLM activities. For each CUDRT category, we developed extensive datasets enabling thorough assessment of detection performance, incorporating the latest mainstream LLMs for each language. We also establish a robust evaluation framework to support scalable, reproducible experiments, facilitating an in-depth analysis of how LLM operations, different LLMs, datasets, and multilingual training sets impact detector performance, particularly for model-based methods. Our extensive experiments provide critical insights for optimizing LLM-generated text detectors and suggest future directions to improve detection accuracy and generalization across diverse scenarios.Source code and dataset are available at GitHub.

**Link**: [arxiv](http://arxiv.org/abs/2406.09056v2),  [pdf](http://arxiv.org/pdf/2406.09056v2)

**Tags**: cs.CL cs.AI 



### ALPINE: Unveiling the Planning Capability of Autoregressive Learning in   Language Models
**Authors**: Siwei Wang, Yifei Shen, Shi Feng, Haoran Sun, Shang-Hua Teng, Wei Chen

**Updated**: 2024-11-11T09:16:56Z

**Summary**: Planning is a crucial element of both human intelligence and contemporary large language models (LLMs). In this paper, we initiate a theoretical investigation into the emergence of planning capabilities in Transformer-based LLMs via their next-word prediction mechanisms. We model planning as a network path-finding task, where the objective is to generate a valid path from a specified source node to a designated target node. Our mathematical characterization shows that Transformer architectures can execute path-finding by embedding the adjacency and reachability matrices within their weights. Furthermore, our theoretical analysis of gradient-based learning dynamics reveals that LLMs can learn both the adjacency and a limited form of the reachability matrices. These theoretical insights are then validated through experiments, which demonstrate that Transformer architectures indeed learn the adjacency and an incomplete reachability matrices, consistent with our theoretical predictions. When applying our methodology to the real-world planning benchmark Blocksworld, our observations remain consistent. Additionally, our analyses uncover a fundamental limitation of current Transformer architectures in path-finding: these architectures cannot identify reachability relationships through transitivity, which leads to failures in generating paths when concatenation is required. These findings provide new insights into how the internal mechanisms of autoregressive learning facilitate intelligent planning and deepen our understanding of how future LLMs might achieve more advanced and general planning-and-reasoning capabilities across diverse applications.

**Link**: [arxiv](http://arxiv.org/abs/2405.09220v3),  [pdf](http://arxiv.org/pdf/2405.09220v3)

**Tags**: cs.LG cs.AI cs.CL 



### Learning-to-Defer for Extractive Question Answering
**Authors**: Yannis Montreuil, Axel Carlier, Lai Xing Ng, Wei Tsang Ooi

**Updated**: 2024-11-11T09:06:51Z

**Summary**: Pre-trained language models have profoundly impacted the field of extractive question-answering, leveraging large-scale textual corpora to enhance contextual language understanding. Despite their success, these models struggle in complex scenarios that demand nuanced interpretation or inferential reasoning beyond immediate textual cues. Furthermore, their size poses deployment challenges on resource-constrained devices. Addressing these limitations, we introduce an adapted two-stage Learning-to-Defer mechanism that enhances decision-making by enabling selective deference to human experts or larger models without retraining language models in the context of question-answering. This approach not only maintains computational efficiency but also significantly improves model reliability and accuracy in ambiguous contexts. We establish the theoretical soundness of our methodology by proving Bayes and $(\mathcal{H}, \mathcal{R})$--consistency of our surrogate loss function, guaranteeing the optimality of the final solution. Empirical evaluations on the SQuADv2 dataset illustrate performance gains from integrating human expertise and leveraging larger models. Our results further demonstrate that deferring a minimal number of queries allows the smaller model to achieve performance comparable to their larger counterparts while preserving computing efficiency, thus broadening the applicability of pre-trained language models in diverse operational environments.

**Link**: [arxiv](http://arxiv.org/abs/2410.15761v2),  [pdf](http://arxiv.org/pdf/2410.15761v2)

**Tags**: cs.CL cs.LG stat.ML 



### AssistRAG: Boosting the Potential of Large Language Models with an   Intelligent Information Assistant
**Authors**: Yujia Zhou, Zheng Liu, Zhicheng Dou

**Updated**: 2024-11-11T09:03:52Z

**Summary**: The emergence of Large Language Models (LLMs) has significantly advanced natural language processing, but these models often generate factually incorrect information, known as "hallucination". Initial retrieval-augmented generation (RAG) methods like the "Retrieve-Read" framework was inadequate for complex reasoning tasks. Subsequent prompt-based RAG strategies and Supervised Fine-Tuning (SFT) methods improved performance but required frequent retraining and risked altering foundational LLM capabilities. To cope with these challenges, we propose Assistant-based Retrieval-Augmented Generation (AssistRAG), integrating an intelligent information assistant within LLMs. This assistant manages memory and knowledge through tool usage, action execution, memory building, and plan specification. Using a two-phase training approach, Curriculum Assistant Learning and Reinforced Preference Optimization. AssistRAG enhances information retrieval and decision-making. Experiments show AssistRAG significantly outperforms benchmarks, especially benefiting less advanced LLMs, by providing superior reasoning capabilities and accurate responses.

**Link**: [arxiv](http://arxiv.org/abs/2411.06805v1),  [pdf](http://arxiv.org/pdf/2411.06805v1)

**Tags**: cs.CL cs.AI cs.IR 



### Automatically Write Code Checker: An LLM-based Approach with   Logic-guided API Retrieval and Case by Case Iteration
**Authors**: Yuanyuan Xie, Jun Liu, Jiwei Yan, Jinhao Huang, Jun Yan, Jian Zhang

**Updated**: 2024-11-11T08:50:24Z

**Summary**: With the rising demand for code quality assurance, developers are not only utilizing existing static code checkers but also seeking custom checkers to satisfy their specific needs. Nowadays, various code-checking frameworks provide extensive checker customization interfaces to meet this need. However, both the abstract checking logic as well as the complex API usage of large-scale frameworks make this task challenging. To this end, automated code checker generation is anticipated to ease the burden of checker development. In this paper, we explore the feasibility of automated checker generation and propose AutoChecker, an innovative LLM-powered approach that can write code checkers automatically based on only a rule description and a test suite. Instead of generating the checker at once, AutoChecker incrementally updates the checker with the rule and one single test case each time, i.e., it iteratively generates the checker case by case. During each iteration, AutoChecker first decomposes the whole logic into a series of sub-operations and then uses the logic-guided API-context retrieval strategy to search related API-contexts from all the framework APIs. To evaluate the effectiveness of AutoChecker, we apply AutoChecker and two LLM-based baseline approaches to automatically generate checkers for 20 built-in PMD rules, including easy rules and hard rules. Experimental results demonstrate that AutoChecker significantly outperforms baseline approaches across all effectiveness metrics, where its average test pass rate improved over 4.2 times. Moreover, the checkers generated by AutoChecker are successfully applied to real-world projects, matching the performance of official checkers.

**Link**: [arxiv](http://arxiv.org/abs/2411.06796v1),  [pdf](http://arxiv.org/pdf/2411.06796v1)

**Tags**: cs.SE 



### Service Deployment in the On-Demand Economy: Employees, Contractors, or   Both?
**Authors**: Lijian Lu, Xin Weng, Li Xiao

**Updated**: 2024-11-11T08:42:41Z

**Summary**: The recent advancements in mobile/data technology have fostered a widespread adoption of on-demand or gig service platforms. The increasingly available data and independent contractors have enabled these platforms to design customized services and a cost-efficient workforce to effectively match demand and supply. In practice, a diverse landscape of the workforce has been observed: some rely solely on either employees or contractors, others use a blended workforce with both types of workers. In this paper, we consider a profit-maximizing service provider (SP) that decides to offer a single service or two differentiated services, along with the pricing and staffing of the workforce with employees and/or contractors, to price- and waiting-sensitive customers. Contractors independently determine whether or not to participate in the marketplace based on private reservation rates and per-service wage offered by the SP, while it controls the number of employees who receive per-hour wage. Under a single service, we show that the SP relies on either employees or contractors and identify sufficient and necessary conditions in which one workforce is better than the other. Under the optimal service deployment, we show that the SP offers either a single service relying solely on employees or contractors, or two differentiated services with a hybrid workforce depending on the service value and cost efficiencies of employees and contractors. Our analysis suggests that proliferating services with a blended workforce could improve the SP's profit significantly, and identifies conditions in which this value is significant. Our results provide an in-depth understanding and insightful guidance to on-demand platforms on the design of service differentiation and workforce models.

**Link**: [arxiv](http://arxiv.org/abs/2411.06793v1),  [pdf](http://arxiv.org/pdf/2411.06793v1)

**Tags**: math.OC 90B22 



### Large-scale moral machine experiment on large language models
**Authors**: Muhammad Shahrul Zaim bin Ahmad, Kazuhiro Takemoto

**Updated**: 2024-11-11T08:36:49Z

**Summary**: The rapid advancement of Large Language Models (LLMs) and their potential integration into autonomous driving systems necessitates understanding their moral decision-making capabilities. While our previous study examined four prominent LLMs using the Moral Machine experimental framework, the dynamic landscape of LLM development demands a more comprehensive analysis. Here, we evaluate moral judgments across 51 different LLMs, including multiple versions of proprietary models (GPT, Claude, Gemini) and open-source alternatives (Llama, Gemma), to assess their alignment with human moral preferences in autonomous driving scenarios. Using a conjoint analysis framework, we evaluated how closely LLM responses aligned with human preferences in ethical dilemmas and examined the effects of model size, updates, and architecture. Results showed that proprietary models and open-source models exceeding 10 billion parameters demonstrated relatively close alignment with human judgments, with a significant negative correlation between model size and distance from human judgments in open-source models. However, model updates did not consistently improve alignment with human preferences, and many LLMs showed excessive emphasis on specific ethical principles. These findings suggest that while increasing model size may naturally lead to more human-like moral judgments, practical implementation in autonomous driving systems requires careful consideration of the trade-off between judgment quality and computational efficiency. Our comprehensive analysis provides crucial insights for the ethical design of autonomous systems and highlights the importance of considering cultural contexts in AI moral decision-making.

**Link**: [arxiv](http://arxiv.org/abs/2411.06790v1),  [pdf](http://arxiv.org/pdf/2411.06790v1)

**Tags**: cs.CY cs.CL cs.HC 



### The First Prompt Counts the Most! An Evaluation of Large Language Models   on Iterative Example-based Code Generation
**Authors**: Yingjie Fu, Bozhou Li, Linyi Li, Wentao Zhang, Tao Xie

**Updated**: 2024-11-11T08:05:37Z

**Summary**: The capabilities of Large Language Models (LLMs) in code generation, particularly for implementing target functionalities from natural language descriptions, have been extensively studied. As an alternative form of natural language, input-output examples (I/O examples) provide an accessible, unambiguous, and flexible way to describe functionalities, but the diversity, sparseness, and incompleteness of I/O examples also place challenges on understanding and implementing requirements. Therefore, generating code from input-output examples (i.e., example-based code generation) provides a new perspective, allowing us to evaluate LLMs' capability to infer target functionalities from limited information and to process new-form requirements. However, related research about LLMs in example-based code generation remains largely unexplored. To fill this gap, this paper presents the first comprehensive study on example-based code generation using LLMs. To address the incorrectness caused by the incompleteness of I/O examples, we adopt an iterative evaluation framework and formalize the objective of example-based code generation as two sequential sub-objectives: generating code conforming to given examples and generating code that successfully implements the target functionalities from (iteratively) given examples. We assess six state-of-the-art LLMs using a new benchmark of 168 diverse target functionalities. The results demonstrate that when requirements were described using iterative I/O examples rather than natural language, the LLMs' score decreased by over 60%, indicating that example-based code generation remains challenging for the evaluated LLMs. More interestingly, the vast majority (even over 95%) of successfully implemented functionalities are achieved in the first round of iterations, suggesting that the LLMs struggle to effectively utilize the iteratively supplemented requirements.

**Link**: [arxiv](http://arxiv.org/abs/2411.06774v1),  [pdf](http://arxiv.org/pdf/2411.06774v1)

**Tags**: cs.SE 



### PDC & DM-SFT: A Road for LLM SQL Bug-Fix Enhancing
**Authors**: Yiwen Duan, Yonghong Yu, Xiaoming Zhao, Yichang Wu, Wenbo Liu

**Updated**: 2024-11-11T07:47:20Z

**Summary**: Code Large Language Models (Code LLMs), such as Code llama and DeepSeek-Coder, have demonstrated exceptional performance in the code generation tasks. However, most existing models focus on the abilities of generating correct code, but often struggle with bug repair. We introduce a suit of methods to enhance LLM's SQL bug-fixing abilities. The methods are mainly consisted of two parts: A Progressive Dataset Construction (PDC) from scratch and Dynamic Mask Supervised Fine-tuning (DM-SFT). PDC proposes two data expansion methods from the perspectives of breadth first and depth first respectively. DM-SFT introduces an efficient bug-fixing supervised learning approach, which effectively reduce the total training steps and mitigate the "disorientation" in SQL code bug-fixing training. In our evaluation, the code LLM models trained with two methods have exceeds all current best performing model which size is much larger.

**Link**: [arxiv](http://arxiv.org/abs/2411.06767v1),  [pdf](http://arxiv.org/pdf/2411.06767v1)

**Tags**: cs.CL cs.AI cs.LG 



### Towards Fast Multilingual LLM Inference: Speculative Decoding and   Specialized Drafters
**Authors**: Euiin Yi, Taehyeon Kim, Hongseok Jeung, Du-Seong Chang, Se-Young Yun

**Updated**: 2024-11-11T07:34:25Z

**Summary**: Large language models (LLMs) have revolutionized natural language processing and broadened their applicability across diverse commercial applications. However, the deployment of these models is constrained by high inference time in multilingual settings. To mitigate this challenge, this paper explores a training recipe of an assistant model in speculative decoding, which is leveraged to draft and-then its future tokens are verified by the target LLM. We show that language-specific draft models, optimized through a targeted pretrain-and-finetune strategy, substantially brings a speedup in inference time compared to the previous methods. We validate these models across various languages in inference time, out-of-domain speedup, and GPT-4o evaluation.

**Link**: [arxiv](http://arxiv.org/abs/2406.16758v2),  [pdf](http://arxiv.org/pdf/2406.16758v2)

**Tags**: cs.CL 



### RoCar: A Relationship Network-based Evaluation Method for Large Language   Models
**Authors**: Ming Wang, Wenfang Wu, Chongyun Gao, Daling Wang, Shi Feng, Yifei Zhang

**Updated**: 2024-11-11T07:27:03Z

**Summary**: Large language models (LLMs) have received increasing attention. However, due to the complexity of its capabilities, how to rationally evaluate the capabilities of LLMs is still a task to be solved. We propose the RoCar method, which utilizes the defined basic schemas to randomly construct a task graph and generates natural language evaluation tasks based on the task graph to evaluate the reasoning and memory abilities of LLMs respectively. Due to the very large randomness of the task construction process, it is possible to ensure that none of the LLMs to be tested has directly learned the evaluation tasks, guaranteeing the fairness of the evaluation method.

**Link**: [arxiv](http://arxiv.org/abs/2307.15997v2),  [pdf](http://arxiv.org/pdf/2307.15997v2)

**Tags**: cs.CL cs.AI 



### FiSTECH: Financial Style Transfer to Enhance Creativity without   Hallucinations in LLMs
**Authors**: Sohini Roychowdhury, Marko Krema, Brian Moore, Xingjian Lai, Dike Effedua, Bharat Jethwani

**Updated**: 2024-11-11T07:18:34Z

**Summary**: Recent trends in Generative AI have emerged towards fine-tuning foundational large language models (LLMs) to create domain-specific LLMs for automation and chatbot-like applications. Specialized applications for analytics-heavy domains such as Financial report generation require specific writing styles that comprise compound and creative sentences with minimized hallucinations. In this work, we explore the self-corrective auto-regressive qualities of LLMs to learn creativity in writing styles with minimal prompting. We propose a novel two-stage fine-tuning (FT) strategy wherein in the first stage public domain financial reports are used to train for writing styles while allowing the LLM to hallucinate. In the second stage the examples of hallucinations are manually corrected and further used to fine-tune the LLM. The finally trained LLM learns to generate specific financial report sections using minimal instructions and tabular data inputs while ensuring low fine-tuning costs. Our proposed two-stage fine-tuning boosts the accuracy of financial questions answering by two-folds while reducing hallucinations by over 50%. Also, the fine-tuned model has lower perplexity, improved ROUGE, TER and BLEU scores, higher creativity and knowledge density with lower uncertainty and cross entropy than base LLMs. Thus, the proposed framework can be generalized to train creativity in LLMs by first allowing them to hallucinate.

**Link**: [arxiv](http://arxiv.org/abs/2408.05365v3),  [pdf](http://arxiv.org/pdf/2408.05365v3)

**Tags**: cs.CL cs.AI cs.CE 



### OAEI-LLM: A Benchmark Dataset for Understanding Large Language Model   Hallucinations in Ontology Matching
**Authors**: Zhangcheng Qiang, Kerry Taylor, Weiqing Wang, Jing Jiang

**Updated**: 2024-11-11T06:26:39Z

**Summary**: Hallucinations of large language models (LLMs) commonly occur in domain-specific downstream tasks, with no exception in ontology matching (OM). The prevalence of using LLMs for OM raises the need for benchmarks to better understand LLM hallucinations. The OAEI-LLM dataset is an extended version of the Ontology Alignment Evaluation Initiative (OAEI) datasets that evaluate LLM-specific hallucinations in OM tasks. We outline the methodology used in dataset construction and schema extension, and provide examples of potential use cases.

**Link**: [arxiv](http://arxiv.org/abs/2409.14038v4),  [pdf](http://arxiv.org/pdf/2409.14038v4)

**Tags**: cs.AI cs.CL cs.IR 



### TF-DCon: Leveraging Large Language Models (LLMs) to Empower   Training-Free Dataset Condensation for Content-Based Recommendation
**Authors**: Jiahao Wu, Qijiong Liu, Hengchang Hu, Wenqi Fan, Shengcai Liu, Qing Li, Xiao-Ming Wu, Ke Tang

**Updated**: 2024-11-11T06:16:24Z

**Summary**: Modern techniques in Content-based Recommendation (CBR) leverage item content information to provide personalized services to users, but suffer from resource-intensive training on large datasets. To address this issue, we explore the dataset condensation for textual CBR in this paper. The goal of dataset condensation is to synthesize a small yet informative dataset, upon which models can achieve performance comparable to those trained on large datasets. While existing condensation approaches are tailored to classification tasks for continuous data like images or embeddings, direct application of them to CBR has limitations. To bridge this gap, we investigate efficient dataset condensation for content-based recommendation. Inspired by the remarkable abilities of large language models (LLMs) in text comprehension and generation, we leverage LLMs to empower the generation of textual content during condensation. To handle the interaction data involving both users and items, we devise a dual-level condensation method: content-level and user-level. At content-level, we utilize LLMs to condense all contents of an item into a new informative title. At user-level, we design a clustering-based synthesis module, where we first utilize LLMs to extract user interests. Then, the user interests and user embeddings are incorporated to condense users and generate interactions for condensed users. Notably, the condensation paradigm of this method is forward and free from iterative optimization on the synthesized dataset. Extensive empirical findings from our study, conducted on three authentic datasets, substantiate the efficacy of the proposed method. Particularly, we are able to approximate up to 97% of the original performance while reducing the dataset size by 95% (i.e., on dataset MIND).

**Link**: [arxiv](http://arxiv.org/abs/2310.09874v4),  [pdf](http://arxiv.org/pdf/2310.09874v4)

**Tags**: cs.IR 



### Mr.Steve: Instruction-Following Agents in Minecraft with What-Where-When   Memory
**Authors**: Junyeong Park, Junmo Cho, Sungjin Ahn

**Updated**: 2024-11-11T06:04:53Z

**Summary**: Significant advances have been made in developing general-purpose embodied AI in environments like Minecraft through the adoption of LLM-augmented hierarchical approaches. While these approaches, which combine high-level planners with low-level controllers, show promise, low-level controllers frequently become performance bottlenecks due to repeated failures. In this paper, we argue that the primary cause of failure in many low-level controllers is the absence of an episodic memory system. To address this, we introduce Mr.Steve (Memory Recall Steve-1), a novel low-level controller equipped with Place Event Memory (PEM), a form of episodic memory that captures what, where, and when information from episodes. This directly addresses the main limitation of the popular low-level controller, Steve-1. Unlike previous models that rely on short-term memory, PEM organizes spatial and event-based data, enabling efficient recall and navigation in long-horizon tasks. Additionally, we propose an Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing agents to alternate between exploration and task-solving based on recalled events. Our approach significantly improves task-solving and exploration efficiency compared to existing methods. We will release our code and demos on the project page: https://sites.google.com/view/mr-steve.

**Link**: [arxiv](http://arxiv.org/abs/2411.06736v1),  [pdf](http://arxiv.org/pdf/2411.06736v1)

**Tags**: cs.LG 



### Multi-Modal Forecaster: Jointly Predicting Time Series and Textual Data
**Authors**: Kai Kim, Howard Tsai, Rajat Sen, Abhimanyu Das, Zihao Zhou, Abhishek Tanpure, Mathew Luo, Rose Yu

**Updated**: 2024-11-11T06:04:15Z

**Summary**: Current forecasting approaches are largely unimodal and ignore the rich textual data that often accompany the time series due to lack of well-curated multimodal benchmark dataset. In this work, we develop TimeText Corpus (TTC), a carefully curated, time-aligned text and time dataset for multimodal forecasting. Our dataset is composed of sequences of numbers and text aligned to timestamps, and includes data from two different domains: climate science and healthcare. Our data is a significant contribution to the rare selection of available multimodal datasets. We also propose the Hybrid Multi-Modal Forecaster (Hybrid-MMF), a multimodal LLM that jointly forecasts both text and time series data using shared embeddings. However, contrary to our expectations, our Hybrid-MMF model does not outperform existing baselines in our experiments. This negative result highlights the challenges inherent in multimodal forecasting. Our code and data are available at https://github.com/Rose-STL-Lab/Multimodal_ Forecasting.

**Link**: [arxiv](http://arxiv.org/abs/2411.06735v1),  [pdf](http://arxiv.org/pdf/2411.06735v1)

**Tags**: cs.AI 



### DetectBench: Can Large Language Model Detect and Piece Together Implicit   Evidence?
**Authors**: Zhouhong Gu, Lin Zhang, Xiaoxuan Zhu, Jiangjie Chen, Wenhao Huang, Yikai Zhang, Shusen Wang, Zheyu Ye, Yan Gao, Hongwei Feng, Yanghua Xiao

**Updated**: 2024-11-11T05:48:35Z

**Summary**: Detecting evidence within the context is a key step in the process of reasoning task. Evaluating and enhancing the capabilities of LLMs in evidence detection will strengthen context-based reasoning performance. This paper proposes a benchmark called DetectBench for verifying the ability to detect and piece together implicit evidence within a long context. DetectBench contains 3,928 multiple-choice questions, with an average of 994 tokens per question. Each question contains an average of 4.55 pieces of implicit evidence, and solving the problem typically requires 7.62 logical jumps to find the correct answer. To enhance the performance of LLMs in evidence detection, this paper proposes Detective Reasoning Prompt and Finetune. Experiments demonstrate that the existing LLMs' abilities to detect evidence in long contexts are far inferior to humans. However, the Detective Reasoning Prompt effectively enhances the capability of powerful LLMs in evidence detection, while the Finetuning method shows significant effects in enhancing the performance of weaker LLMs. Moreover, when the abilities of LLMs in evidence detection are improved, their final reasoning performance is also enhanced accordingly.

**Link**: [arxiv](http://arxiv.org/abs/2406.12641v2),  [pdf](http://arxiv.org/pdf/2406.12641v2)

**Tags**: cs.CL 



### Script-Strategy Aligned Generation: Aligning LLMs with Expert-Crafted   Dialogue Scripts and Therapeutic Strategies for Psychotherapy
**Authors**: Xin Sun, Jan de Wit, Zhuying Li, Jiahuan Pei, Abdallah El Ali, Jos A. Bosch

**Updated**: 2024-11-11T05:14:14Z

**Summary**: Chatbots or conversational agents (CAs) are increasingly used to improve access to digital psychotherapy. Many current systems rely on rigid, rule-based designs, heavily dependent on expert-crafted dialogue scripts for guiding therapeutic conversations. Although recent advances in large language models (LLMs) offer the potential for more flexible interactions, their lack of controllability and transparency poses significant challenges in sensitive areas like psychotherapy. In this work, we explored how aligning LLMs with expert-crafted scripts can enhance psychotherapeutic chatbot performance. Our comparative study showed that LLMs aligned with expert-crafted scripts through prompting and fine-tuning significantly outperformed both pure LLMs and rule-based chatbots, achieving a more effective balance between dialogue flexibility and adherence to therapeutic principles. Building on findings, we proposed ``Script-Strategy Aligned Generation (SSAG)'', a flexible alignment approach that reduces reliance on fully scripted content while enhancing LLMs' therapeutic adherence and controllability. In a 10-day field study, SSAG demonstrated performance comparable to full script alignment and outperformed rule-based chatbots, empirically supporting SSAG as an efficient approach for aligning LLMs with domain expertise. Our work advances LLM applications in psychotherapy by providing a controllable, adaptable, and scalable solution for digital interventions, reducing reliance on expert effort. It also provides a collaborative framework for domain experts and developers to efficiently build expertise-aligned chatbots, broadening access to psychotherapy and behavioral interventions.

**Link**: [arxiv](http://arxiv.org/abs/2411.06723v1),  [pdf](http://arxiv.org/pdf/2411.06723v1)

**Tags**: cs.HC cs.AI 



### Ambient AI Scribing Support: Comparing the Performance of Specialized AI   Agentic Architecture to Leading Foundational Models
**Authors**: Chanseo Lee, Sonu Kumar, Kimon A. Vogt, Sam Meraj

**Updated**: 2024-11-11T04:45:48Z

**Summary**: This study compares Sporo Health's AI Scribe, a proprietary model fine-tuned for medical scribing, with various LLMs (GPT-4o, GPT-3.5, Gemma-9B, and Llama-3.2-3B) in clinical documentation. We analyzed de-identified patient transcripts from partner clinics, using clinician-provided SOAP notes as the ground truth. Each model generated SOAP summaries using zero-shot prompting, with performance assessed via recall, precision, and F1 scores. Sporo outperformed all models, achieving the highest recall (73.3%), precision (78.6%), and F1 score (75.3%) with the lowest performance variance. Statistically significant differences (p < 0.05) were found between Sporo and the other models, with post-hoc tests showing significant improvements over GPT-3.5, Gemma-9B, and Llama 3.2-3B. While Sporo outperformed GPT-4o by up to 10%, the difference was not statistically significant (p = 0.25). Clinical user satisfaction, measured with a modified PDQI-9 inventory, favored Sporo. Evaluations indicated Sporo's outputs were more accurate and relevant. This highlights the potential of Sporo's multi-agentic architecture to improve clinical workflows.

**Link**: [arxiv](http://arxiv.org/abs/2411.06713v1),  [pdf](http://arxiv.org/pdf/2411.06713v1)

**Tags**: cs.AI 



### Mining Causality: AI-Assisted Search for Instrumental Variables
**Authors**: Sukjin Han

**Updated**: 2024-11-11T04:41:32Z

**Summary**: The instrumental variables (IVs) method is a leading empirical strategy for causal inference. Finding IVs is a heuristic and creative process, and justifying its validity--especially exclusion restrictions--is largely rhetorical. We propose using large language models (LLMs) to search for new IVs through narratives and counterfactual reasoning, similar to how a human researcher would. The stark difference, however, is that LLMs can dramatically accelerate this process and explore an extremely large search space. We demonstrate how to construct prompts to search for potentially valid IVs. We contend that multi-step and role-playing prompting strategies are effective for simulating the endogenous decision-making processes of economic agents and for navigating language models through the realm of real-world scenarios. We apply our method to three well-known examples in economics: returns to schooling, supply and demand, and peer effects. We then extend our strategy to finding (i) control variables in regression and difference-in-differences and (ii) running variables in regression discontinuity designs.

**Link**: [arxiv](http://arxiv.org/abs/2409.14202v2),  [pdf](http://arxiv.org/pdf/2409.14202v2)

**Tags**: econ.EM stat.AP stat.ME stat.ML 



### Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters
**Authors**: Yujin Potter, Shiyang Lai, Junsol Kim, James Evans, Dawn Song

**Updated**: 2024-11-11T04:35:11Z

**Summary**: How could LLMs influence our democracy? We investigate LLMs' political leanings and the potential influence of LLMs on voters by conducting multiple experiments in a U.S. presidential election context. Through a voting simulation, we first demonstrate 18 open- and closed-weight LLMs' political preference for a Democratic nominee over a Republican nominee. We show how this leaning towards the Democratic nominee becomes more pronounced in instruction-tuned models compared to their base versions by analyzing their responses to candidate-policy related questions. We further explore the potential impact of LLMs on voter choice by conducting an experiment with 935 U.S. registered voters. During the experiments, participants interacted with LLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment results show a shift in voter choices towards the Democratic nominee following LLM interaction, widening the voting margin from 0.7% to 4.6%, even though LLMs were not asked to persuade users to support the Democratic nominee during the discourse. This effect is larger than many previous studies on the persuasiveness of political campaigns, which have shown minimal effects in presidential elections. Many users also expressed a desire for further political interaction with LLMs. Which aspects of LLM interactions drove these shifts in voter choice requires further study. Lastly, we explore how a safety method can make LLMs more politically neutral, while raising the question of whether such neutrality is truly the path forward.

**Link**: [arxiv](http://arxiv.org/abs/2410.24190v3),  [pdf](http://arxiv.org/pdf/2410.24190v3)

**Tags**: cs.CL cs.CY 



### NutriBench: A Dataset for Evaluating Large Language Models on Nutrition   Estimation from Meal Descriptions
**Authors**: Andong Hua, Mehak Preet Dhaliwal, Ryan Burke, Laya Pullela, Yao Qin

**Updated**: 2024-11-11T04:17:30Z

**Summary**: Accurate nutrition estimation helps people make informed dietary choices and is essential in the prevention of serious health complications. We present NutriBench, the first publicly available natural language meal description nutrition benchmark. NutriBench consists of 11,857 meal descriptions generated from real-world global dietary intake data. The data is human-verified and annotated with macro-nutrient labels, including carbohydrates, proteins, fats, and calories. We conduct an extensive evaluation of NutriBench on the task of carbohydrate estimation, testing twelve leading Large Language Models (LLMs), including GPT-4o, Llama3.1, Qwen2, Gemma2, and OpenBioLLM models, using standard, Chain-of-Thought and Retrieval-Augmented Generation strategies. Additionally, we present a study involving professional nutritionists, finding that LLMs can provide more accurate and faster estimates. Finally, we perform a real-world risk assessment by simulating the effect of carbohydrate predictions on the blood glucose levels of individuals with diabetes. Our work highlights the opportunities and challenges of using LLMs for nutrition estimation, demonstrating their potential to aid professionals and laypersons and improve health outcomes. Our benchmark is publicly available at: https://mehak126.github.io/nutribench.html

**Link**: [arxiv](http://arxiv.org/abs/2407.12843v4),  [pdf](http://arxiv.org/pdf/2407.12843v4)

**Tags**: cs.CL cs.AI 



### Beyond Text: Utilizing Vocal Cues to Improve Decision Making in LLMs for   Robot Navigation Tasks
**Authors**: Xingpeng Sun, Haoming Meng, Souradip Chakraborty, Amrit Singh Bedi, Aniket Bera

**Updated**: 2024-11-11T04:03:28Z

**Summary**: While LLMs excel in processing text in these human conversations, they struggle with the nuances of verbal instructions in scenarios like social navigation, where ambiguity and uncertainty can erode trust in robotic and other AI systems. We can address this shortcoming by moving beyond text and additionally focusing on the paralinguistic features of these audio responses. These features are the aspects of spoken communication that do not involve the literal wording (lexical content) but convey meaning and nuance through how something is said. We present Beyond Text: an approach that improves LLM decision-making by integrating audio transcription along with a subsection of these features, which focus on the affect and more relevant in human-robot conversations.This approach not only achieves a 70.26% winning rate, outperforming existing LLMs by 22.16% to 48.30% (gemini-1.5-pro and gpt-3.5 respectively), but also enhances robustness against token manipulation adversarial attacks, highlighted by a 22.44% less decrease ratio than the text-only language model in winning rate. Beyond Text' marks an advancement in social robot navigation and broader Human-Robot interactions, seamlessly integrating text-based guidance with human-audio-informed language models.

**Link**: [arxiv](http://arxiv.org/abs/2402.03494v3),  [pdf](http://arxiv.org/pdf/2402.03494v3)

**Tags**: cs.AI cs.RO 



### Autonomous Droplet Microfluidic Design Framework with Large Language   Models
**Authors**: Dinh-Nguyen Nguyen, Raymond Kai-Yu Tong, Ngoc-Duy Dinh

**Updated**: 2024-11-11T03:20:53Z

**Summary**: Droplet-based microfluidic devices have substantial promise as cost-effective alternatives to current assessment tools in biological research. Moreover, machine learning models that leverage tabular data, including input design parameters and their corresponding efficiency outputs, are increasingly utilised to automate the design process of these devices and to predict their performance. However, these models fail to fully leverage the data presented in the tables, neglecting crucial contextual information, including column headings and their associated descriptions. This study presents MicroFluidic-LLMs, a framework designed for processing and feature extraction, which effectively captures contextual information from tabular data formats. MicroFluidic-LLMs overcomes processing challenges by transforming the content into a linguistic format and leveraging pre-trained large language models (LLMs) for analysis. We evaluate our MicroFluidic-LLMs framework on 11 prediction tasks, covering aspects such as geometry, flow conditions, regimes, and performance, utilising a publicly available dataset on flow-focusing droplet microfluidics. We demonstrate that our MicroFluidic-LLMs framework can empower deep neural network models to be highly effective and straightforward while minimising the need for extensive data preprocessing. Moreover, the exceptional performance of deep neural network models, particularly when combined with advanced natural language processing models such as DistilBERT and GPT-2, reduces the mean absolute error in the droplet diameter and generation rate by nearly 5- and 7-fold, respectively, and enhances the regime classification accuracy by over 4%, compared with the performance reported in a previous study. This study lays the foundation for the huge potential applications of LLMs and machine learning in a wider spectrum of microfluidic applications.

**Link**: [arxiv](http://arxiv.org/abs/2411.06691v1),  [pdf](http://arxiv.org/pdf/2411.06691v1)

**Tags**: cs.AI 



### Quantum-Powered Optimization for Electric Vehicle Charging   Infrastructure Deployment
**Authors**: Nazmush Sakib, Xin Chen

**Updated**: 2024-11-11T03:03:20Z

**Summary**: The infrastructure development of electric vehicle charging stations (EVCS) is critical to the integration of electrical vehicles (EVs) into transportation systems, which requires significant investment and has long-term impact on the adoption of EVs. In this paper, a mathematical model is developed to identify the optimal placement of EVCS by utilizing a novel quantum annealing (QA) algorithm and quantum computation (QC). The objective of the optimization model is to determine the locations of EVCS that maximize their service quality for EV users. The model is validated using a real-world case study and solved using commercially available quantum computers from D-Wave. The case study shows that the QA algorithm can find the optimal placement of EVCS within seconds. The quality of the solutions obtained using QC is not sensitive to the shape or size of the area where EVCS are to be deployed.

**Link**: [arxiv](http://arxiv.org/abs/2411.06684v1),  [pdf](http://arxiv.org/pdf/2411.06684v1)

**Tags**: quant-ph cs.ET 



### WDMoE: Wireless Distributed Mixture of Experts for Large Language Models
**Authors**: Nan Xue, Yaping Sun, Zhiyong Chen, Meixia Tao, Xiaodong Xu, Liang Qian, Shuguang Cui, Wenjun Zhang, Ping Zhang

**Updated**: 2024-11-11T02:48:00Z

**Summary**: Large Language Models (LLMs) have achieved significant success in various natural language processing tasks, but the role of wireless networks in supporting LLMs has not been thoroughly explored. In this paper, we propose a wireless distributed Mixture of Experts (WDMoE) architecture to enable collaborative deployment of LLMs across edge servers at the base station (BS) and mobile devices in wireless networks. Specifically, we decompose the MoE layer in LLMs by placing the gating network and the preceding neural network layer at BS, while distributing the expert networks among the devices. This deployment leverages the parallel inference capabilities of expert networks on mobile devices, effectively utilizing the limited computing and caching resources of these devices. Accordingly, we develop a performance metric for WDMoE-based LLMs, which accounts for both model capability and latency. To minimize the latency while maintaining accuracy, we jointly optimize expert selection and bandwidth allocation based on the performance metric. Moreover, we build a hardware testbed using NVIDIA Jetson kits to validate the effectiveness of WDMoE. Both theoretical simulations and practical hardware experiments demonstrate that the proposed method can significantly reduce the latency without compromising LLM performance.

**Link**: [arxiv](http://arxiv.org/abs/2411.06681v1),  [pdf](http://arxiv.org/pdf/2411.06681v1)

**Tags**: cs.LG cs.AI cs.DC cs.IT math.IT 



### Anchor Attention, Small Cache: Code Generation with Large Language   Models
**Authors**: Xiangyu Zhang, Yu Zhou, Guang Yang, Harald C. Gall, Taolue Chen

**Updated**: 2024-11-11T02:47:05Z

**Summary**: The development of large language models (LLMs) has revolutionized automated code generation. However, their high demand of computation resources has hindered a broader deployment and raised environmental concerns. A common strategy for diminishing computational demands is to cache Key-Value (KV) states from the attention mechanism which is adopted predominately by mainstream LLMs. It can mitigate the need of repeated attention computations, but brings significant memory overhead. Current practices in NLP often use sparse attention which may, unfortunately, lead to substantial inaccuracies, or hallucinations, in code generation tasks. In this paper, we analyze the attention weights distribution within code generation models via an empirical study, uncovering a sparsity pattern, i.e., the aggregation of information at specific anchor points. Based on this observation, we propose a novel approach, AnchorCoder, which features token-wise anchor attention designed to extract and compress the contextual information, and layer-wise anchor attention enabling cross-layer communication to mitigate the issue of excessive superposition caused by the compression. The extensive experiments across multiple benchmark datasets confirm the effectiveness of AnchorCoder, which can consistently achieve a significant (at least 70%) reduction in KV cache requirements, while preserving the majority of model's performance.

**Link**: [arxiv](http://arxiv.org/abs/2411.06680v1),  [pdf](http://arxiv.org/pdf/2411.06680v1)

**Tags**: cs.SE 68N19 D.2.3 



### Changing Answer Order Can Decrease MMLU Accuracy
**Authors**: Vipul Gupta, David Pantoja, Candace Ross, Adina Williams, Megan Ung

**Updated**: 2024-11-11T02:27:54Z

**Summary**: As large language models (LLMs) have grown in prevalence, particular benchmarks have become essential for the evaluation of these models and for understanding model capabilities. Most commonly, we use test accuracy averaged across multiple subtasks in order to rank models on leaderboards, to determine which model is best for our purposes. In this paper, we investigate the robustness of the accuracy measurement on a widely used multiple choice question answering dataset, MMLU. When shuffling the answer label contents, we find that all explored models decrease in accuracy on MMLU, but not every model is equally sensitive. These findings suggest a possible adjustment to the standard practice of leaderboard testing, where we additionally consider the percentage of examples each model answers correctly by random chance.

**Link**: [arxiv](http://arxiv.org/abs/2406.19470v2),  [pdf](http://arxiv.org/pdf/2406.19470v2)

**Tags**: cs.CL 



### Enhancing Visual Classification using Comparative Descriptors
**Authors**: Hankyeol Lee, Gawon Seo, Wonseok Choi, Geunyoung Jung, Kyungwoo Song, Jiyoung Jung

**Updated**: 2024-11-11T02:24:36Z

**Summary**: The performance of vision-language models (VLMs), such as CLIP, in visual classification tasks, has been enhanced by leveraging semantic knowledge from large language models (LLMs), including GPT. Recent studies have shown that in zero-shot classification tasks, descriptors incorporating additional cues, high-level concepts, or even random characters often outperform those using only the category name. In many classification tasks, while the top-1 accuracy may be relatively low, the top-5 accuracy is often significantly higher. This gap implies that most misclassifications occur among a few similar classes, highlighting the model's difficulty in distinguishing between classes with subtle differences. To address this challenge, we introduce a novel concept of comparative descriptors. These descriptors emphasize the unique features of a target class against its most similar classes, enhancing differentiation. By generating and integrating these comparative descriptors into the classification framework, we refine the semantic focus and improve classification accuracy. An additional filtering process ensures that these descriptors are closer to the image embeddings in the CLIP space, further enhancing performance. Our approach demonstrates improved accuracy and robustness in visual classification tasks by addressing the specific challenge of subtle inter-class differences.

**Link**: [arxiv](http://arxiv.org/abs/2411.05357v2),  [pdf](http://arxiv.org/pdf/2411.05357v2)

**Tags**: cs.CV 



### Explore the Reasoning Capability of LLMs in the Chess Testbed
**Authors**: Shu Wang, Lei Ji, Renxi Wang, Wenxiao Zhao, Haokun Liu, Yifan Hou, Ying Nian Wu

**Updated**: 2024-11-11T01:42:56Z

**Summary**: Reasoning is a central capability of human intelligence. In recent years, with the advent of large-scale datasets, pretrained large language models have emerged with new capabilities, including reasoning. However, these models still struggle with long-term, complex reasoning tasks, such as playing chess. Based on the observation that expert chess players employ a dual approach combining long-term strategic play with short-term tactical play along with language explanation, we propose improving the reasoning capability of large language models in chess by integrating annotated strategy and tactic. Specifically, we collect a dataset named MATE, which consists of 1 million chess positions with candidate moves annotated by chess experts for strategy and tactics. We finetune the LLaMA-3-8B model and compare it against state-of-the-art commercial language models in the task of selecting better chess moves. Our experiments show that our models perform better than GPT, Claude, and Gemini models. We find that language explanations can enhance the reasoning capability of large language models.

**Link**: [arxiv](http://arxiv.org/abs/2411.06655v1),  [pdf](http://arxiv.org/pdf/2411.06655v1)

**Tags**: cs.CL cs.AI 



### Seq1F1B: Efficient Sequence-Level Pipeline Parallelism for Large   Language Model Training
**Authors**: Ao Sun, Weilin Zhao, Xu Han, Cheng Yang, Xinrong Zhang, Zhiyuan Liu, Chuan Shi, Maosong Sun

**Updated**: 2024-11-11T01:33:50Z

**Summary**: The emergence of large language models (LLMs) relies heavily on distributed training strategies, among which pipeline parallelism plays a crucial role. As LLMs' training sequence length extends to 32k or even 128k, the current pipeline parallel methods face severe bottlenecks, including high memory footprints and substantial pipeline bubbles, greatly hindering model scalability and training throughput. To enhance memory efficiency and training throughput, in this work, we introduce an efficient sequence-level one-forward-one-backward (1F1B) pipeline scheduling method tailored for training LLMs on long sequences named Seq1F1B. Seq1F1B decomposes batch-level schedulable units into finer sequence-level units, reducing bubble size and memory footprint. Considering that Seq1F1B may produce slight extra bubbles if sequences are split evenly, we design a computation-wise strategy to partition input sequences and mitigate this side effect. Compared to competitive pipeline baseline methods such as Megatron 1F1B pipeline parallelism, our method achieves higher training throughput with less memory footprint. Notably, Seq1F1B efficiently trains a LLM with 30B parameters on sequences up to 64k using 64 NVIDIA A100 GPUs without recomputation strategies, a feat unachievable with existing methods. Our source code is based on Megatron-LM, and now is avaiable at: https://github.com/MayDomine/Seq1F1B.git.

**Link**: [arxiv](http://arxiv.org/abs/2406.03488v5),  [pdf](http://arxiv.org/pdf/2406.03488v5)

**Tags**: cs.DC 



### Understanding Scaling Laws with Statistical and Approximation Theory for   Transformer Neural Networks on Intrinsically Low-dimensional Data
**Authors**: Alex Havrilla, Wenjing Liao

**Updated**: 2024-11-11T01:05:28Z

**Summary**: When training deep neural networks, a model's generalization error is often observed to follow a power scaling law dependent both on the model size and the data size. Perhaps the best known example of such scaling laws are for transformer-based large language models, where networks with billions of parameters are trained on trillions of tokens of text. Yet, despite sustained widespread interest, a rigorous understanding of why transformer scaling laws exist is still missing. To answer this question, we establish novel statistical estimation and mathematical approximation theories for transformers when the input data are concentrated on a low-dimensional manifold. Our theory predicts a power law between the generalization error and both the training data size and the network size for transformers, where the power depends on the intrinsic dimension $d$ of the training data. Notably, the constructed model architecture is shallow, requiring only logarithmic depth in $d$. By leveraging low-dimensional data structures under a manifold hypothesis, we are able to explain transformer scaling laws in a way which respects the data geometry. Moreover, we test our theory with empirical observation by training LLMs on natural language datasets. We find the observed empirical data scaling laws closely agree with our theoretical predictions. Taken together, these results rigorously show the intrinsic dimension of data to be a crucial quantity affecting transformer scaling laws in both theory and practice.

**Link**: [arxiv](http://arxiv.org/abs/2411.06646v1),  [pdf](http://arxiv.org/pdf/2411.06646v1)

**Tags**: cs.LG cs.AI cs.CL stat.ML 



### Model Editing for LLMs4Code: How Far are We?
**Authors**: Xiaopeng Li, Shangwen Wang, Shasha Li, Jun Ma, Jie Yu, Xiaodong Liu, Jing Wang, Bin Ji, Weimin Zhang

**Updated**: 2024-11-11T00:18:54Z

**Summary**: Large Language Models for Code (LLMs4Code) have been found to exhibit outstanding performance in the software engineering domain, especially the remarkable performance in coding tasks. However, even the most advanced LLMs4Code can inevitably contain incorrect or outdated code knowledge. Due to the high cost of training LLMs4Code, it is impractical to re-train the models for fixing these problematic code knowledge. Model editing is a new technical field for effectively and efficiently correcting erroneous knowledge in LLMs, where various model editing techniques and benchmarks have been proposed recently. Despite that, a comprehensive study that thoroughly compares and analyzes the performance of the state-of-the-art model editing techniques for adapting the knowledge within LLMs4Code across various code-related tasks is notably absent. To bridge this gap, we perform the first systematic study on applying state-of-the-art model editing approaches to repair the inaccuracy of LLMs4Code. To that end, we introduce a benchmark named CLMEEval, which consists of two datasets, i.e., CoNaLa-Edit (CNLE) with 21K+ code generation samples and CodeSearchNet-Edit (CSNE) with 16K+ code summarization samples. With the help of CLMEEval, we evaluate six advanced model editing techniques on three LLMs4Code: CodeLlama (7B), CodeQwen1.5 (7B), and Stable-Code (3B). Our findings include that the external memorization-based GRACE approach achieves the best knowledge editing effectiveness and specificity (the editing does not influence untargeted knowledge), while generalization (whether the editing can generalize to other semantically-identical inputs) is a universal challenge for existing techniques. Furthermore, building on in-depth case analysis, we introduce an enhanced version of GRACE called A-GRACE, which incorporates contrastive learning to better capture the semantics of the inputs.

**Link**: [arxiv](http://arxiv.org/abs/2411.06638v1),  [pdf](http://arxiv.org/pdf/2411.06638v1)

**Tags**: cs.SE cs.CL 



### "Knowing When You Don't Know": A Multilingual Relevance Assessment   Dataset for Robust Retrieval-Augmented Generation
**Authors**: Nandan Thakur, Luiz Bonifacio, Xinyu Zhang, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Boxing Chen, Mehdi Rezagholizadeh, Jimmy Lin

**Updated**: 2024-11-10T23:58:53Z

**Summary**: Retrieval-Augmented Generation (RAG) grounds Large Language Model (LLM) output by leveraging external knowledge sources to reduce factual hallucinations. However, prior work lacks a comprehensive evaluation of different language families, making it challenging to evaluate LLM robustness against errors in external retrieved knowledge. To overcome this, we establish NoMIRACL, a human-annotated dataset for evaluating LLM robustness in RAG across 18 typologically diverse languages. NoMIRACL includes both a non-relevant and a relevant subset. Queries in the non-relevant subset contain passages judged as non-relevant, whereas queries in the relevant subset include at least a single judged relevant passage. We measure relevance assessment using: (i) hallucination rate, measuring model tendency to hallucinate, when the answer is not present in passages in the non-relevant subset, and (ii) error rate, measuring model inaccuracy to recognize relevant passages in the relevant subset.In our work, we observe that most models struggle to balance the two capacities. Models such as LLAMA-2 and Orca-2 achieve over 88% hallucination rate on the non-relevant subset. Mistral and LLAMA-3 hallucinate less but can achieve up to a 74.9% error rate on the relevant subset. Overall, GPT-4 is observed to provide the best tradeoff on both subsets, highlighting future work necessary to improve LLM robustness. NoMIRACL dataset and evaluation code are available at: https://github.com/project-miracl/nomiracl.

**Link**: [arxiv](http://arxiv.org/abs/2312.11361v3),  [pdf](http://arxiv.org/pdf/2312.11361v3)

**Tags**: cs.CL cs.IR 



### vTune: Verifiable Fine-Tuning for LLMs Through Backdooring
**Authors**: Eva Zhang, Arka Pal, Akilesh Potti, Micah Goldblum

**Updated**: 2024-11-10T22:08:37Z

**Summary**: As fine-tuning large language models (LLMs) becomes increasingly prevalent, users often rely on third-party services with limited visibility into their fine-tuning processes. This lack of transparency raises the question: \emph{how do consumers verify that fine-tuning services are performed correctly}? For instance, a service provider could claim to fine-tune a model for each user, yet simply send all users back the same base model. To address this issue, we propose vTune, a simple method that uses a small number of \textit{backdoor} data points added to the training data to provide a statistical test for verifying that a provider fine-tuned a custom model on a particular user's dataset. Unlike existing works, vTune is able to scale to verification of fine-tuning on state-of-the-art LLMs, and can be used both with open-source and closed-source models. We test our approach across several model families and sizes as well as across multiple instruction-tuning datasets, and find that the statistical test is satisfied with p-values on the order of $\sim 10^{-40}$, with no negative impact on downstream task performance. Further, we explore several attacks that attempt to subvert vTune and demonstrate the method's robustness to these attacks.

**Link**: [arxiv](http://arxiv.org/abs/2411.06611v1),  [pdf](http://arxiv.org/pdf/2411.06611v1)

**Tags**: cs.LG cs.AI cs.CY 



