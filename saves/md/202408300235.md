# Arxiv Results
## Keyword: kv cache 
 ### RIP Linked List
**Authors**: Beno√Æt Sonntag, Dominique Colnet

**Updated**: 2024-08-28T08:41:45Z

**Summary**: Linked lists have long served as a valuable teaching tool in programming. However, the question arises: Are they truly practical for everyday program use? In most cases, it appears that array-based data structures offer distinct advantages, particularly in terms of memory efficiency and,more importantly, execution speed. While it's relatively straightforward to calculate the complexity of operations, gauging actual execution efficiency remains a challenge. This paper addresses this question by introducing a new benchmark. Our study compares various linked list implementations with several array-based alternatives. We also demonstrate the ease of incorporating memory caching for linked lists, enhancing their performance. Additionally, we introduce a new array-based data structure designed to excel in a wide range of operations.

**Link**: [arxiv](http://arxiv.org/abs/2306.06942v3),  [pdf](http://arxiv.org/pdf/2306.06942v3)

**Tags**: cs.DS 



### Efficient LLM Training and Serving with Heterogeneous Context Sharding   among Attention Heads
**Authors**: Xihui Lin, Yunan Zhang, Suyu Ge, Barun Patra, Vishrav Chaudhary, Hao Peng, Xia Song

**Updated**: 2024-08-27T22:06:20Z

**Summary**: Existing LLM training and inference frameworks struggle in boosting efficiency with sparsity while maintaining the integrity of context and model architecture. Inspired by the sharding concept in database and the fact that attention parallelizes over heads on accelerators, we propose Sparsely-Sharded (S2) Attention, an attention algorithm that allocates heterogeneous context partitions for different attention heads to divide and conquer. S2-Attention enforces each attention head to only attend to a partition of contexts following a strided sparsity pattern, while the full context is preserved as the union of all the shards. As attention heads are processed in separate thread blocks, the context reduction for each head can thus produce end-to-end speed-up and memory reduction. At inference, LLMs trained with S2-Attention can then take the KV cache reduction as free meals with guaranteed model quality preserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X wall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction in end-to-end training time and 10X inference latency, (2) on-par model training quality compared to default attention, (3)perfect needle retrieval accuracy over 32K context window. On top of the algorithm, we build DKernel, an LLM training and inference kernel library that allows users to customize sparsity patterns for their own models. We open-sourced DKerneland make it compatible with Megatron, Pytorch, and vLLM.

**Link**: [arxiv](http://arxiv.org/abs/2407.17678v2),  [pdf](http://arxiv.org/pdf/2407.17678v2)

**Tags**: cs.CL 



### Styx: Transactional Stateful Functions on Streaming Dataflows
**Authors**: Kyriakos Psarakis, George Siachamis, George Christodoulou, Marios Fragkoulis, Asterios Katsifodimos

**Updated**: 2024-08-27T17:30:41Z

**Summary**: Developing stateful cloud applications, such as low-latency workflows and microservices with strict consistency requirements, remains arduous for programmers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve these use cases. However, existing approaches either provide serializable transactional guarantees at the level of individual functions, or separate application logic from the state and use inefficient transactional protocols. These design choices increase the execution latency, limiting the adoption of SFaaS systems.   In this paper, we present Styx, a novel SFaaS runtime that executes serializable transactions across functions with exactly-once guarantees. Styx extends a deterministic transactional protocol to support an arbitrary call graph of stateful functions. It introduces a transaction-execution acknowledgment scheme that allows tracking a transactional workflow's SFaaS calls, guaranteeing atomicity and exactly-once processing. Finally, Styx features a function-execution caching mechanism and early transactional commit replies for optimized performance. Experiments with the YCSB-T, TPC-C, and Deathstar benchmarks show that Styx outperforms state-of-the-art approaches by achieving at least one order of magnitude higher throughput while exhibiting near-linear scalability and low latency.

**Link**: [arxiv](http://arxiv.org/abs/2312.06893v3),  [pdf](http://arxiv.org/pdf/2312.06893v3)

**Tags**: cs.DC cs.DB 



### Writing in the Margins: Better Inference Pattern for Long Context   Retrieval
**Authors**: Melisa Russak, Umar Jamil, Christopher Bryant, Kiran Kamble, Axel Magnuson, Mateusz Russak, Waseem AlShikh

**Updated**: 2024-08-27T09:34:38Z

**Summary**: In this paper, we introduce Writing in the Margins (WiM), a new inference pattern for Large Language Models designed to optimize the handling of long input sequences in retrieval-oriented tasks. This approach leverages the chunked prefill of the key-value cache to perform segment-wise inference, which enables efficient processing of extensive contexts along with the generation and classification of intermediate information ("margins") that guide the model towards specific tasks. This method increases computational overhead marginally while significantly enhancing the performance of off-the-shelf models without the need for fine-tuning. Specifically, we observe that WiM provides an average enhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG) and more than a 30.0% increase in the F1-score for aggregation tasks (CWE). Additionally, we show how the proposed pattern fits into an interactive retrieval design that provides end-users with ongoing updates about the progress of context processing, and pinpoints the integration of relevant information into the final response. We release our implementation of WiM using Hugging Face Transformers library at https://github.com/writer/writing-in-the-margins.

**Link**: [arxiv](http://arxiv.org/abs/2408.14906v1),  [pdf](http://arxiv.org/pdf/2408.14906v1)

**Tags**: cs.CL cs.IR 



### PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework   with Correlated Differential Privacy
**Authors**: Xianzhi Zhang, Yipeng Zhou, Di Wu, Quan Z. Sheng, Miao Hu, Linchang Xiao

**Updated**: 2024-08-27T02:03:36Z

**Summary**: Online video streaming has evolved into an integral component of the contemporary Internet landscape. Yet, the disclosure of user requests presents formidable privacy challenges. As users stream their preferred online videos, their requests are automatically seized by video content providers, potentially leaking users' privacy.   Unfortunately, current protection methods are not well-suited to preserving user request privacy from content providers while maintaining high-quality online video services. To tackle this challenge, we introduce a novel Privacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge devices to pre-fetch and cache videos, ensuring the privacy of users' requests while optimizing the efficiency of edge caching. More specifically, we design PPVF with three core components: (1) \textit{Online privacy budget scheduler}, which employs a theoretically guaranteed online algorithm to select non-requested videos as candidates with assigned privacy budgets. Alternative videos are chosen by an online algorithm that is theoretically guaranteed to consider both video utilities and available privacy budgets. (2) \textit{Noisy video request generator}, which generates redundant video requests (in addition to original ones) utilizing correlated differential privacy to obfuscate request privacy. (3) \textit{Online video utility predictor}, which leverages federated learning to collaboratively evaluate video utility in an online fashion, aiding in video selection in (1) and noise generation in (2). Finally, we conduct extensive experiments using real-world video request traces from Tencent Video. The results demonstrate that PPVF effectively safeguards user request privacy while upholding high video caching performance.

**Link**: [arxiv](http://arxiv.org/abs/2408.14735v1),  [pdf](http://arxiv.org/pdf/2408.14735v1)

**Tags**: cs.MM cs.CR cs.DC 



### Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference
**Authors**: Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, Song Han

**Updated**: 2024-08-26T21:01:02Z

**Summary**: As the demand for long-context large language models (LLMs) increases, models with context windows of up to 128K or 1M tokens are becoming increasingly prevalent. However, long-context LLM inference is challenging since the inference speed decreases significantly as the sequence length grows. This slowdown is primarily caused by loading a large KV cache during self-attention. Previous works have shown that a small portion of critical tokens will dominate the attention outcomes. However, we observe the criticality of a token highly depends on the query. To this end, we propose Quest, a query-aware KV cache selection algorithm. Quest keeps track of the minimal and maximal Key values in KV cache pages and estimates the criticality of a given page using Query vectors. By only loading the Top-K critical KV cache pages for attention, Quest significantly speeds up self-attention without sacrificing accuracy. We show that Quest can achieve up to 2.23x self-attention speedup, which reduces inference latency by 7.03x while performing well on tasks with long dependencies with negligible accuracy loss. Code is available at http://github.com/mit-han-lab/Quest .

**Link**: [arxiv](http://arxiv.org/abs/2406.10774v2),  [pdf](http://arxiv.org/pdf/2406.10774v2)

**Tags**: cs.CL cs.LG 



### Employing Artificial Intelligence to Steer Exascale Workflows with   Colmena
**Authors**: Logan Ward, J. Gregory Pauloski, Valerie Hayot-Sasson, Yadu Babuji, Alexander Brace, Ryan Chard, Kyle Chard, Rajeev Thakur, Ian Foster

**Updated**: 2024-08-26T17:21:19Z

**Summary**: Computational workflows are a common class of application on supercomputers, yet the loosely coupled and heterogeneous nature of workflows often fails to take full advantage of their capabilities. We created Colmena to leverage the massive parallelism of a supercomputer by using Artificial Intelligence (AI) to learn from and adapt a workflow as it executes. Colmena allows scientists to define how their application should respond to events (e.g., task completion) as a series of cooperative agents. In this paper, we describe the design of Colmena, the challenges we overcame while deploying applications on exascale systems, and the science workflows we have enhanced through interweaving AI. The scaling challenges we discuss include developing steering strategies that maximize node utilization, introducing data fabrics that reduce communication overhead of data-intensive tasks, and implementing workflow tasks that cache costly operations between invocations. These innovations coupled with a variety of application patterns accessible through our agent-based steering model have enabled science advances in chemistry, biophysics, and materials science using different types of AI. Our vision is that Colmena will spur creative solutions that harness AI across many domains of scientific computing.

**Link**: [arxiv](http://arxiv.org/abs/2408.14434v1),  [pdf](http://arxiv.org/pdf/2408.14434v1)

**Tags**: cs.DC cs.LG 



### Decision-Focused Learning to Predict Action Costs for Planning
**Authors**: Jayanta Mandi, Marco Foschini, Daniel Holler, Sylvie Thiebaux, Jorg Hoffmann, Tias Guns

**Updated**: 2024-08-26T11:29:07Z

**Summary**: In many automated planning applications, action costs can be hard to specify. An example is the time needed to travel through a certain road segment, which depends on many factors, such as the current weather conditions. A natural way to address this issue is to learn to predict these parameters based on input features (e.g., weather forecasts) and use the predicted action costs in automated planning afterward. Decision-Focused Learning (DFL) has been successful in learning to predict the parameters of combinatorial optimization problems in a way that optimizes solution quality rather than prediction quality. This approach yields better results than treating prediction and optimization as separate tasks. In this paper, we investigate for the first time the challenges of implementing DFL for automated planning in order to learn to predict the action costs. There are two main challenges to overcome: (1) planning systems are called during gradient descent learning, to solve planning problems with negative action costs, which are not supported in planning. We propose novel methods for gradient computation to avoid this issue. (2) DFL requires repeated planner calls during training, which can limit the scalability of the method. We experiment with different methods approximating the optimal plan as well as an easy-to-implement caching mechanism to speed up the learning process. As the first work that addresses DFL for automated planning, we demonstrate that the proposed gradient computation consistently yields significantly better plans than predictions aimed at minimizing prediction error; and that caching can temper the computation requirements.

**Link**: [arxiv](http://arxiv.org/abs/2408.06876v2),  [pdf](http://arxiv.org/pdf/2408.06876v2)

**Tags**: cs.AI cs.RO 



### Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems
**Authors**: Yiwei Li, Boyu Tian, Mingyu Gao

**Updated**: 2024-08-26T07:26:27Z

**Summary**: Hybrid main memory systems combine both performance and capacity advantages from heterogeneous memory technologies. With larger capacities, higher associativities, and finer granularities, hybrid memory systems currently exhibit significant metadata storage and lookup overheads for flexibly remapping data blocks between the two memory tiers. To alleviate the inefficiencies of existing designs, we propose Trimma, the combination of a multi-level metadata structure and an efficient metadata cache design. Trimma uses a multi-level metadata table to only track truly necessary address remap entries. The saved memory space is effectively utilized as extra DRAM cache capacity to improve performance. Trimma also uses separate formats to store the entries with non-identity and identity address mappings. This improves the overall remap cache hit rate, further boosting the performance. Trimma is transparent to software and compatible with various types of hybrid memory systems. When evaluated on a representative hybrid memory system with HBM3 and DDR5, Trimma achieves up to 1.68$\times$ and on average 1.33$\times$ speedup benefits, compared to state-of-the-art hybrid memory designs. These results show that Trimma effectively addresses metadata management overheads, especially for future scalable large-scale hybrid memory architectures.

**Link**: [arxiv](http://arxiv.org/abs/2402.16343v2),  [pdf](http://arxiv.org/pdf/2402.16343v2)

**Tags**: cs.AR 



### RollingCache: Using Runtime Behavior to Defend Against Cache Side   Channel Attacks
**Authors**: Divya Ojha, Sandhya Dwarkadas

**Updated**: 2024-08-26T04:32:56Z

**Summary**: Shared caches are vulnerable to side channel attacks through contention in cache sets. Besides being a simple source of information leak, these side channels form useful gadgets for more sophisticated attacks that compromise the security of shared systems.   The fundamental design aspect that contention attacks exploit is the deterministic nature of the set of addresses contending for a cache set. In this paper, we present RollingCache, a cache design that defends against contention attacks by dynamically changing the set of addresses contending for cache sets. Unlike prior defenses, RollingCache does not rely on address encryption/decryption, data relocation, or cache partitioning. We use one level of indirection to implement dynamic mapping controlled by the whole-cache runtime behavior. Our solution does not depend on having defined security domains, and can defend against an attacker running on the same or another core.   We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our security evaluation shows that our dynamic mapping removes the deterministic ability to identify the source of contention. The performance evaluation shows an impact of 1.67\% over a mix of workloads, with a corresponding

**Link**: [arxiv](http://arxiv.org/abs/2408.08795v2),  [pdf](http://arxiv.org/pdf/2408.08795v2)

**Tags**: cs.CR cs.AR 



### Decentralized Federated Learning with Model Caching on Mobile Agents
**Authors**: Xiaoyu Wang, Guojun Xiong, Houwei Cao, Jian Li, Yong Liu

**Updated**: 2024-08-26T03:58:20Z

**Summary**: Federated Learning (FL) aims to train a shared model using data and computation power on distributed agents coordinated by a central server. Decentralized FL (DFL) utilizes local model exchange and aggregation between agents to reduce the communication and computation overheads on the central server. However, when agents are mobile, the communication opportunity between agents can be sporadic, largely hindering the convergence and accuracy of DFL. In this paper, we study delay-tolerant model spreading and aggregation enabled by model caching on mobile agents. Each agent stores not only its own model, but also models of agents encountered in the recent past. When two agents meet, they exchange their own models as well as the cached models. Local model aggregation works on all models in the cache. We theoretically analyze the convergence of DFL with cached models, explicitly taking into account the model staleness introduced by caching. We design and compare different model caching algorithms for different DFL and mobility scenarios. We conduct detailed case studies in a vehicular network to systematically investigate the interplay between agent mobility, cache staleness, and model convergence. In our experiments, cached DFL converges quickly, and significantly outperforms DFL without caching.

**Link**: [arxiv](http://arxiv.org/abs/2408.14001v1),  [pdf](http://arxiv.org/pdf/2408.14001v1)

**Tags**: cs.LG cs.DC 



### Mobile Edge Computing Networks: Online Low-Latency and Fresh Service   Provisioning
**Authors**: Yuhan Yi, Guanglin Zhang, Hai Jiang

**Updated**: 2024-08-24T15:23:32Z

**Summary**: Edge service caching can significantly mitigate latency and reduce communication and computing overhead by fetching and initializing services (applications) from clouds. The freshness of cached service data is critical when providing satisfactory services to users, but has been overlooked in existing research efforts. In this paper, we study the online low-latency and fresh service provisioning in mobile edge computing (MEC) networks. Specifically, we jointly optimize the service caching, task offloading, and resource allocation without knowledge of future system information, which is formulated as a joint online long-term optimization problem. This problem is NP-hard. To solve the problem, we design a Lyapunov-based online framework that decouples the problem at temporal level into a series of per-time-slot subproblems. For each subproblem, we propose an online integrated optimization-deep reinforcement learning (OIODRL) method, which contains an optimization stage including a quadratically constrained quadratic program (QCQP) transformation and a semidefinite relaxation (SDR) method, and a learning stage including a deep reinforcement learning (DRL) algorithm. Extensive simulations show that the proposed OIODRL method achieves a near-optimal solution and outperforms other benchmark methods.

**Link**: [arxiv](http://arxiv.org/abs/2408.13605v1),  [pdf](http://arxiv.org/pdf/2408.13605v1)

**Tags**: cs.IT math.IT 



### MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context   Generation with Speculative Decoding
**Authors**: Jian Chen, Vashisth Tiwari, Ranajoy Sadhukhan, Zhuoming Chen, Jinyuan Shi, Ian En-Hsu Yen, Beidi Chen

**Updated**: 2024-08-23T17:54:34Z

**Summary**: Large Language Models (LLMs) have become more prevalent in long-context applications such as interactive chatbots, document analysis, and agent workflows, but it is challenging to serve long-context requests with low latency and high throughput. Speculative decoding (SD) is a widely used technique to reduce latency without sacrificing performance but the conventional wisdom suggests that its efficacy is limited to small batch sizes. In MagicDec, we show that surprisingly SD can achieve speedup even for a high throughput inference regime for moderate to long sequences. More interestingly, an intelligent drafting strategy can achieve better speedup with increasing batch size based on our rigorous analysis. MagicDec first identifies the bottleneck shifts with increasing batch size and sequence length, and uses these insights to deploy speculative decoding more effectively for high throughput inference. Then, it leverages draft models with sparse KV cache to address the KV bottleneck that scales with both sequence length and batch size. This finding underscores the broad applicability of speculative decoding in long-context serving, as it can enhance throughput and reduce latency without compromising accuracy. For moderate to long sequences, we demonstrate up to 2x speedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving batch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available at https://github.com/Infini-AI-Lab/MagicDec/.

**Link**: [arxiv](http://arxiv.org/abs/2408.11049v3),  [pdf](http://arxiv.org/pdf/2408.11049v3)

**Tags**: cs.CL 



### Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches
**Authors**: Dhruv Pratap Singh, Anjana A. Mahesh, B. Sundar Rajan

**Updated**: 2024-08-23T15:39:20Z

**Summary**: We consider a variant of the coded caching problem where users connect to two types of caches, called private caches and access caches. The problem setting consists of a server having a library of files and a set of access caches. Every user, equipped with a private cache, connects to $L$ neighboring access caches in a cyclic wrap-around fashion. The server populates the private and access caches with file contents in either coded or uncoded format. For this setting, we derive a lower bound on the optimal worst-case transmission rate using cut-set arguments. This lower bound applies to both coded and uncoded placements. We then provide an achievable scheme with uncoded placement and show that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for the dedicated cache network in the absence of access caches. Finally, we show that the proposed scheme achieves optimality in large memory regimes and provide numerical plots comparing the rate of the proposed scheme with the derived lower bound, demonstrating the optimality of our scheme.

**Link**: [arxiv](http://arxiv.org/abs/2408.13165v1),  [pdf](http://arxiv.org/pdf/2408.13165v1)

**Tags**: cs.IT math.IT 



### Fundamental Limits of Multi-Message Private Computation
**Authors**: Ali Gholami, Kai Wan, Tayyebeh Jahani-Nezhad, Hua Sun, Mingyue Ji, Giuseppe Caire

**Updated**: 2024-08-23T13:25:07Z

**Summary**: In a typical formulation of the private information retrieval (PIR) problem, a single user wishes to retrieve one out of $ K$ files from $N$ servers without revealing the demanded file index to any server. This paper formulates an extended model of PIR, referred to as multi-message private computation (MM-PC), where instead of retrieving a single file, the user wishes to retrieve $P>1$ linear combinations of files while preserving the privacy of the demand information. The MM-PC problem is a generalization of the private computation (PC) problem (where the user requests one linear combination of the files), and the multi-message private information retrieval (MM-PIR) problem (where the user requests $P>1$ files). A baseline achievable scheme repeats the optimal PC scheme by Sun and Jafar $P$ times, or treats each possible demanded linear combination as an independent file and then uses the near optimal MM-PIR scheme by Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that significantly improves upon the baseline schemes. In doing so, we design the queries inspired by the structure in the cache-aided scalar linear function retrieval scheme by Wan {\it et al.}, which leverages the dependency between linear functions to reduce the amount of communications. To ensure the decodability of our scheme, we propose a new method to benefit from the existing dependency, referred to as the sign assignment step. In the end, we use Maximum Distance Separable matrices to code the queries, which allows the reduction of download from the servers, while preserving privacy. By the proposed schemes, we characterize the capacity within a multiplicative factor of $2$.

**Link**: [arxiv](http://arxiv.org/abs/2305.05332v5),  [pdf](http://arxiv.org/pdf/2305.05332v5)

**Tags**: cs.IT math.IT 



### Which Part of the Heap is Useful? Improving Heap Liveness Analysis
**Authors**: Vini Kanvar, Uday P. Khedker

**Updated**: 2024-08-23T09:54:22Z

**Summary**: With the growing sizes of data structures allocated in heap, understanding the actual use of heap memory is critically important for minimizing cache misses and reclaiming unused memory. A static analysis aimed at this is difficult because the heap locations are unnamed. Using allocation sites to name them creates very few distinctions making it difficult to identify allocated heap locations that are not used. Heap liveness analysis using access graphs solves this problem by (a) using a storeless model of heap memory by naming the locations with access paths, and (b) representing the unbounded sets of access paths (which are regular languages) as finite automata.   We improve the scalability and efficiency of heap liveness analysis, and reduce the amount of computed heap liveness information by using deterministic automata and by minimizing the inclusion of aliased access paths in the language. Practically, our field-, flow-, context-sensitive liveness analysis on SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5 kLoC) and improves efficiency even up to 99%. For some of the benchmarks, our technique shows multifold reduction in the computed liveness information, ranging from 2 to 100 times (in terms of the number of live access paths), without compromising on soundness.

**Link**: [arxiv](http://arxiv.org/abs/2408.12947v1),  [pdf](http://arxiv.org/pdf/2408.12947v1)

**Tags**: cs.PL 



### Exposing Shadow Branches
**Authors**: Chrysanthos Pepi, Bhargav Reddy Godala, Krishnam Tibrewala, Gino Chacon, Paul V. Gratz, Daniel A. Jim√©nez, Gilles A. Pokam, David I. August

**Updated**: 2024-08-22T17:56:29Z

**Summary**: Modern processors implement a decoupled front-end in the form of Fetch Directed Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is driven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and branch target tracking structures to speculatively fetch instructions into the Instruction Cache (L1I). As data center applications become more complex, their code footprints also grow, resulting in an increase in Branch Target Buffer (BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB miss, the BPU may not identify the current instruction as a branch to FDIP. This can prevent FDIP from prefetching or cause it to speculate down the wrong path, further polluting the L1I cache. We observe that the vast majority, 75%, of BTB-missing, unidentified branches are actually present in instruction cache lines that FDIP has previously fetched but, these missing branches have not yet been decoded and inserted into the BTB. This is because the instruction line is decoded from an entry point (which is the target of the previous taken branch) till an exit point (the taken branch). Branch instructions present in the ignored portion of the cache line we call them "Shadow Branches". Here we present Skeia, a novel shadow branch decoding technique that identifies and decodes unused bytes in cache lines fetched by FDIP, inserting them into a Shadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB, allowing FDIP to speculate despite a BTB miss. With a minimal storage state of 12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB) and ~2% versus adding an equal amount of state to the BTB across 16 front-end bound applications. Since many branches stored in the SBB are unique compared to those in a similarly sized BTB, we consistently observe greater performance gains with Skeia across all examined sizes until saturation.

**Link**: [arxiv](http://arxiv.org/abs/2408.12592v1),  [pdf](http://arxiv.org/pdf/2408.12592v1)

**Tags**: cs.AR 



### Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties
**Authors**: Simon Hettler, Kankona Singha Roy, Raul Arenal, Leela S. Panchakarla

**Updated**: 2024-08-22T17:47:49Z

**Summary**: Layered CoO$_2$ is of great interest for its promising properties but is meta-stable in its bulk form. CoO$_2$ was synthesized by converting the quasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a hydrothermal treatment. The resulting nanostructures were predominantly nanoscrolls with very thin walls, which exhibit long-term stability. A detailed structural investigation reveals that the CoO$_2$ is found to crystallize in monoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure. Individual nanoscrolls are characterized electrically and show a p-type semiconducting nature with a high current-carrying capacity of 4$\cdot$10$^5$ A cm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The results demonstrate the possibility to stabilize meta-stable materials in low-dimensional forms and a promising application of the nanoscrolls as interconnect in high-voltage electronic circuitry.

**Link**: [arxiv](http://arxiv.org/abs/2309.14533v2),  [pdf](http://arxiv.org/pdf/2309.14533v2)

**Tags**: cond-mat.mtrl-sci 



### Rheological behavior of molybdenum disulfide (MoS2) inks under electric   fields: influence of concentration and voltage
**Authors**: Pedro C Rijo, Francisco J. Galindo-Rosales

**Updated**: 2024-08-21T10:26:26Z

**Summary**: This work provides a complete rheological characterization of molybdenum disulfide (MoS2) inks in the presence of electric fields. Several concentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The lubrication effects are present in the ink when the MoS2 concentration is higher than 0.10% w/w. The dielectric properties show the impossibility of a positive electrorheological effect for all MoS2-inks studied. The formation of vortices and electromigration of MoS2 particles occur under the influence of an external electric field. These two phenomena affect the rheological behavior of MoS2-inks under shear flow condition. Relatively to the extensional rheology experiments, the particle migration and the vortex formation promote anisotropy on the rheological properties of the inks which affects the relaxation time, the formation of beads-on-a-string and the uniaxial elongational flow condition is no longer valid. When the electric field strength is 1.5 kV/mm, the formation of Taylor's cone is observed and independent of MoS2 concentration.

**Link**: [arxiv](http://arxiv.org/abs/2408.11506v1),  [pdf](http://arxiv.org/pdf/2408.11506v1)

**Tags**: physics.flu-dyn cond-mat.soft 



### Towards End-to-End GPS Localization with Neural Pseudorange Correction
**Authors**: Xu Weng, KV Ling, Haochen Liu, Kun Cao

**Updated**: 2024-08-21T06:10:02Z

**Summary**: The pseudorange error is one of the root causes of localization inaccuracy in GPS. Previous data-driven methods regress and eliminate pseudorange errors using handcrafted intermediate labels. Unlike them, we propose an end-to-end GPS localization framework, E2E-PrNet, to train a neural network for pseudorange correction (PrNet) directly using the final task loss calculated with the ground truth of GPS receiver states. The gradients of the loss with respect to learnable parameters are backpropagated through a Differentiable Nonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing the data-driven neural network and the model-based DNLS module is verified with GPS data collected by Android phones, showing that E2E-PrNet outperforms the baseline weighted least squares method and the state-of-the-art end-to-end data-driven approach. Finally, we discuss the explainability of E2E-PrNet.

**Link**: [arxiv](http://arxiv.org/abs/2401.10685v2),  [pdf](http://arxiv.org/pdf/2401.10685v2)

**Tags**: cs.LG cs.AI eess.SP 



### Telepathic Datacenters: Fast RPCs using Shared CXL Memory
**Authors**: Suyash Mahar, Ehsan Hajyjasini, Seungjin Lee, Zifeng Zhang, Mingyao Shen, Steven Swanson

**Updated**: 2024-08-21T04:16:49Z

**Summary**: Datacenter applications often rely on remote procedure calls (RPCs) for fast, efficient, and secure communication. However, RPCs are slow, inefficient, and hard to use as they require expensive serialization and compression to communicate over a packetized serial network link. Compute Express Link 3.0 (CXL) offers an alternative solution, allowing applications to share data using a cache-coherent, shared-memory interface across clusters of machines.   RPCool is a new framework that exploits CXL's shared memory capabilities. RPCool avoids serialization by passing pointers to data structures in shared memory. While avoiding serialization is useful, directly sharing pointer-rich data eliminates the isolation that copying data over traditional networks provides, leaving the receiver vulnerable to invalid pointers and concurrent updates to shared data by the sender. RPCool restores this safety with careful and efficient management of memory permissions. Another significant challenge with CXL shared memory capabilities is that they are unlikely to scale to an entire datacenter. RPCool addresses this by falling back to RDMA-based communication.   Overall, RPCool reduces the round-trip latency by 1.93$\times$ and 7.2$\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms, respectively. Moreover, RPCool performs either comparably or better than other RPC mechanisms across a range of workloads.

**Link**: [arxiv](http://arxiv.org/abs/2408.11325v1),  [pdf](http://arxiv.org/pdf/2408.11325v1)

**Tags**: cs.DC cs.OS 



### QET: Enhancing Quantized LLM Parameters and KV cache Compression through   Element Substitution and Residual Clustering
**Authors**: Yanshu Wang, Wang Li, Tong Yang

**Updated**: 2024-08-21T02:32:43Z

**Summary**: Matrix quantization compresses matrix elements into a more compact form to reduce storage requirements, with dequantization enabling reconstruction for use. We define the Quantization Error Minimization (QEM) problem as minimizing the difference between the original and quantized matrices while ensuring the quantized matrix remains within fixed memory constraints. This technique is crucial in applications like Large Language Model (LLM) weight compression and KV cache compression, where large matrix sizes demand efficient storage solutions.   As modern LLMs like GPT-4 and BERT continue to grow, effective matrix compression is increasingly important. These models contain billions of parameters in matrix form, making efficient weight quantization essential for both storage and computational efficiency. Similarly, KV caches, storing intermediate inference results, are matrix-based and benefit significantly from optimized compression techniques.   To address the QEM problem in the context of LLM weight and KV cache compression, we propose Quantum Entanglement Trees (QET). QET leverages the local structure of matrix elements by iteratively swapping elements to create a locally ordered matrix, which is then grouped and quantized column by column. To enhance QET, we introduce two optimizations: residual quantization to further reduce Mean Squared Error (MSE) and masking with batch processing to accelerate the algorithm.   Our experiments demonstrate that QET can reduce MSE to 12.3% of its original value at the same compression ratio, outperforming leading baseline methods. Our contributions include framing the QEM problem specifically for LLM and KV cache compression, developing the QET algorithm, and implementing optimizations that improve accuracy and processing speed.

**Link**: [arxiv](http://arxiv.org/abs/2407.03637v3),  [pdf](http://arxiv.org/pdf/2407.03637v3)

**Tags**: cs.LG cs.CL 



### Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical   Planning and Control
**Authors**: Poppy Collis, Ryan Singh, Paul F Kinghorn, Christopher L Buckley

**Updated**: 2024-08-20T16:02:54Z

**Summary**: An open problem in artificial intelligence is how systems can flexibly learn discrete abstractions that are useful for solving inherently continuous problems. Previous work has demonstrated that a class of hybrid state-space model known as recurrent switching linear dynamical systems (rSLDS) discover meaningful behavioural units via the piecewise linear decomposition of complex continuous dynamics (Linderman et al., 2016). Furthermore, they model how the underlying continuous states drive these discrete mode switches. We propose that the rich representations formed by an rSLDS can provide useful abstractions for planning and control. We present a novel hierarchical model-based algorithm inspired by Active Inference in which a discrete MDP sits above a low-level linear-quadratic controller. The recurrent transition dynamics learned by the rSLDS allow us to (1) specify temporally-abstracted sub-goals in a method reminiscent of the options framework, (2) lift the exploration into discrete space allowing us to exploit information-theoretic exploration bonuses and (3) `cache' the approximate solutions to low-level problems in the discrete planner. We successfully apply our model to the sparse Continuous Mountain Car task, demonstrating fast system identification via enhanced exploration and non-trivial planning through the delineation of abstract sub-goals.

**Link**: [arxiv](http://arxiv.org/abs/2408.10970v1),  [pdf](http://arxiv.org/pdf/2408.10970v1)

**Tags**: cs.AI cs.SY eess.SY 



### Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI   Framework for Personal LLMs Fine-Tuning
**Authors**: Bei Ouyang, Shengyuan Ye, Liekang Zeng, Tianyi Qian, Jingyi Li, Xu Chen

**Updated**: 2024-08-20T11:30:12Z

**Summary**: Large language models (LLMs) have unlocked a plethora of powerful applications at the network edge, such as intelligent personal assistants. Data privacy and security concerns have prompted a shift towards edge-based fine-tuning of personal LLMs, away from cloud reliance. However, this raises issues of computational intensity and resource scarcity, hindering training efficiency and feasibility. While current studies investigate parameter-efficient fine-tuning (PEFT) techniques to mitigate resource constraints, our analysis indicates that these techniques are not sufficiently resource-efficient for edge devices. To tackle these challenges, we propose Pluto and Charon (PAC), a time and memory efficient collaborative edge AI framework for personal LLMs fine-tuning. PAC breaks the resource wall of personal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1) Algorithmically, PAC implements a personal LLMs fine-tuning technique that is efficient in terms of parameters, time, and memory. It utilizes Parallel Adapters to circumvent the need for a full backward pass through the LLM backbone. Additionally, an activation cache mechanism further streamlining the process by negating the necessity for repeated forward passes across multiple epochs. (2) Systematically, PAC leverages edge devices in close proximity, pooling them as a collective resource for in-situ personal LLMs fine-tuning, utilizing a hybrid data and pipeline parallelism to orchestrate distributed training. The use of the activation cache eliminates the need for forward pass through the LLM backbone,enabling exclusive fine-tuning of the Parallel Adapters using data parallelism. Extensive evaluation based on prototype implementation demonstrates that PAC remarkably outperforms state-of-the-art approaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction in memory footprint.

**Link**: [arxiv](http://arxiv.org/abs/2408.10746v1),  [pdf](http://arxiv.org/pdf/2408.10746v1)

**Tags**: cs.DC cs.AI cs.LG cs.NI 



### Heta: Distributed Training of Heterogeneous Graph Neural Networks
**Authors**: Yuchen Zhong, Junwei Su, Chuan Wu, Minjie Wang

**Updated**: 2024-08-20T04:46:18Z

**Summary**: Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic relationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable learning performance in various applications. However, current distributed GNN training systems often overlook unique characteristics of HetGs, such as varying feature dimensions and the prevalence of missing features among nodes, leading to suboptimal performance or even incompatibility with distributed HGNN training. We introduce Heta, a framework designed to address the communication bottleneck in distributed HGNN training. Heta leverages the inherent structure of HGNNs - independent relation-specific aggregations for each relation, followed by a cross-relation aggregation - and advocates for a novel Relation-Aggregation-First computation paradigm. It performs relation-specific aggregations within graph partitions and then exchanges partial aggregations. This design, coupled with a new graph partitioning method that divides a HetG based on its graph schema and HGNN computation dependency, substantially reduces communication overhead. Heta further incorporates an innovative GPU feature caching strategy that accounts for the different cache miss-penalties associated with diverse node types. Comprehensive evaluations of various HGNN models and large heterogeneous graph datasets demonstrate that Heta outperforms state-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in end-to-end epoch time, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2408.09697v2),  [pdf](http://arxiv.org/pdf/2408.09697v2)

**Tags**: cs.DC 



### Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory
**Authors**: Olena Tkach, Gerd Schoenhense

**Updated**: 2024-08-19T15:47:17Z

**Summary**: The strong electric field between the sample and the extractor is the core of cathode lenses and a pivotal determinant of high resolution. Nevertheless, fields in the range of 3-8 kV/mm can be a source of complications. Local field enhancement at sharp edges or microscopic protrusions of cleaved samples may result in field emission or flashovers. Moreover, slow background electrons are drawn into the microscope column, where they contribute to space charge effects. A novel front lens configuration, optimized through ray-tracing simulations, significantly reduces the field at the sample and allows even for zero field or retarding field, which serves to suppress space charge effects. One or several annular electrodes, situated in a concentric position relative to the extractor, serve to form an additional lens within the gap between the sample and the extractor. The refractory power of this lens, and consequently the field at the sample surface, can be modified by adjusting the potentials of the annular electrodes. The imaging properties and aberrations of this gap lens have been investigated with regard to momentum imaging and XPEEM. The study encompasses the energy range from the few-eV level for laser-ARPES to 6 keV, for hard X-ray ARPES. The additional converging lens situated in close proximity to the sample exhibits a reduced field curvature of the k-image in the backfocal plane. This allows for the acquisition of larger fields of view in both momentum and real-space imaging.

**Link**: [arxiv](http://arxiv.org/abs/2408.10104v1),  [pdf](http://arxiv.org/pdf/2408.10104v1)

**Tags**: physics.app-ph cond-mat.mtrl-sci physics.ins-det 



### Abstract Environment Trimming
**Authors**: Daniel Jurjo-Rivas, Jose F. Morales, Pedro L√≥pez-Garc√≠a, Manuel V. Hermenegildo

**Updated**: 2024-08-19T09:50:35Z

**Summary**: Variable sharing is a fundamental property in the static analysis of logic programs, since it is instrumental for ensuring correctness and increasing precision while inferring many useful program properties. Such properties include modes, determinacy, non-failure, cost, etc. This has motivated significant work on developing abstract domains to improve the precision and performance of sharing analyses. Much of this work has centered around the family of set-sharing domains, because of the high precision they offer. However, this comes at a price: their scalability to a wide set of realistic programs remains challenging and this hinders their wider adoption. In this work, rather than defining new sharing abstract domains, we focus instead on developing techniques which can be incorporated in the analyzers to address aspects that are known to affect the efficiency of these domains, such as the number of variables, without affecting precision. These techniques are inspired in others used in the context of compiler optimizations, such as expression reassociation and variable trimming. We present several such techniques and provide an extensive experimental evaluation of over 1100 program modules taken from both production code and classical benchmarks. This includes the Spectector cache analyzer, the s(CASP) system, the libraries of the Ciao system, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental results are quite encouraging: we have obtained significant speed-ups, and, more importantly, the number of modules that require a timeout was cut in half. As a result, many more programs can be analyzed precisely in reasonable times.

**Link**: [arxiv](http://arxiv.org/abs/2408.09848v1),  [pdf](http://arxiv.org/pdf/2408.09848v1)

**Tags**: cs.PL 



### AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for   Efficient MoE Inference
**Authors**: Shuzhang Zhong, Ling Liang, Yuan Wang, Runsheng Wang, Ru Huang, Meng Li

**Updated**: 2024-08-19T03:27:15Z

**Summary**: Mixture-of-Experts (MoE) models are designed to enhance the efficiency of large language models (LLMs) without proportionally increasing the computational demands. However, their deployment on edge devices still faces significant challenges due to high on-demand loading overheads from managing sparsely activated experts. This paper introduces AdapMoE, an algorithm-system co-design framework for efficient MoE inference. AdapMoE features adaptive expert gating and management to reduce the on-demand loading overheads. We observe the heterogeneity of experts loading across layers and tokens, based on which we propose a sensitivity-based strategy to adjust the number of activated experts dynamically. Meanwhile, we also integrate advanced prefetching and cache management techniques to further reduce the loading latency. Through comprehensive evaluations on various platforms, we demonstrate AdapMoE consistently outperforms existing techniques, reducing the average number of activated experts by 25% and achieving a 1.35x speedup without accuracy degradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE.

**Link**: [arxiv](http://arxiv.org/abs/2408.10284v1),  [pdf](http://arxiv.org/pdf/2408.10284v1)

**Tags**: cs.LG 



### Post-Training Sparse Attention with Double Sparsity
**Authors**: Shuo Yang, Ying Sheng, Joseph E. Gonzalez, Ion Stoica, Lianmin Zheng

**Updated**: 2024-08-18T17:27:17Z

**Summary**: The inference process for large language models is slow and memory-intensive, with one of the most critical bottlenecks being excessive Key-Value (KV) cache accesses. This paper introduces "Double Sparsity," a novel post-training sparse attention technique designed to alleviate this bottleneck by reducing KV cache access. Double Sparsity combines token sparsity, which focuses on utilizing only the important tokens for computing self-attention, with channel sparsity, an approach that uses important feature channels for identifying important tokens. Our key insight is that the pattern of channel sparsity is relatively static, allowing us to use offline calibration to make it efficient at runtime, thereby enabling accurate and efficient identification of important tokens. Moreover, this method can be combined with offloading to achieve significant memory usage reduction. Experimental results demonstrate that Double Sparsity can achieve $\frac{1}{16}$ token and channel sparsity with minimal impact on accuracy across various tasks, including wiki-2 perplexity, key-value retrieval, and long context benchmarks with models including Llama-2-7B, Llama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\times$ acceleration in attention operations and a 1.9$\times$ improvement in end-to-end inference on GPUs. With offloading, it achieves a decoding speed acceleration of 16.3$\times$ compared to state-of-the-art solutions at a sequence length of 256K. Our code is publicly available at https://github.com/andy-yang-1/DoubleSparse.

**Link**: [arxiv](http://arxiv.org/abs/2408.07092v2),  [pdf](http://arxiv.org/pdf/2408.07092v2)

**Tags**: cs.LG cs.AI cs.CL 



### CMD: A Cache-assisted GPU Memory Deduplication Architecture
**Authors**: Wei Zhao, Dan Feng, Wei Tong, Xueliang Wei, Bing Wu

**Updated**: 2024-08-18T13:54:46Z

**Summary**: Massive off-chip accesses in GPUs are the main performance bottleneck, and we divided these accesses into three types: (1) Write, (2) Data-Read, and (3) Read-Only. Besides, We find that many writes are duplicate, and the duplication can be inter-dup and intra-dup. While inter-dup means different memory blocks are identical, and intra-dup means all the 4B elements in a line are the same. In this work, we propose a cache-assisted GPU memory deduplication architecture named CMD to reduce the off-chip accesses via utilizing the data duplication in GPU applications. CMD includes three key design contributions which aim to reduce the three kinds of accesses: (1) A novel GPU memory deduplication architecture that removes the inter-dup and inter-dup lines. As for the inter-dup detection, we reduce the extra read requests caused by the traditional read-verify hash process. Besides, we design several techniques to manage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce the reads to duplicate data. When an L2 cache miss wants to read the duplicate block, if the reference block has been fetched to L2 and it is clean, we can copy it to the L2 missed block without accessing off-chip DRAM. As for the reads to intra-dup data, CMD uses the on-chip metadata cache to get the data. (3) When a cache line is evicted, the clean sectors in the line are invalidated while the dirty sectors are written back. However, most read-only victims are re-referenced from DRAM more than twice. Therefore, we add a full-associate FIFO to accommodate the read-only (it is also clean) victims to reduce the re-reference counts. Experiments show that CMD can decrease the off-chip accesses by 31.01%, reduce the energy by 32.78% and improve performance by 37.79%. Besides, CMD can improve the performance of memory-intensive workloads by 50.18%.

**Link**: [arxiv](http://arxiv.org/abs/2408.09483v1),  [pdf](http://arxiv.org/pdf/2408.09483v1)

**Tags**: cs.AR 



### Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for   Efficient LLM Inference
**Authors**: Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, S. Kevin Zhou

**Updated**: 2024-08-16T08:46:33Z

**Summary**: Large Language Models have excelled in various fields but encounter challenges in memory and time efficiency due to the expanding Key-Value (KV) cache required for long-sequence inference. Recent efforts try to reduce KV cache size to a given memory budget by evicting vast non-critical cache elements during runtime, while preserving generation quality. Our revisiting of current eviction methods reveals that they fundamentally minimize an upper bound of the $L_1$ eviction loss between the pre- and post-eviction outputs of multi-head self-attention mechanisms. Moreover, our analysis indicates that the common practices of uniformly assigning budgets across attention heads harm their post-eviction generation quality. In light of these findings, we propose a simple yet effective adaptive budget allocation algorithm. This algorithm not only optimizes the theoretical loss upper bound but also reduces the $L_1$ eviction loss in practice by aligning with the varied characteristics across different heads. By integrating this algorithm into two state-of-the-art methods, we demonstrate the effectiveness of using adaptive budget allocation to optimize KV cache eviction. Extensive evaluations on 16 datasets and the Needle-in-a-Haystack test confirm significant performance improvements across various tasks.

**Link**: [arxiv](http://arxiv.org/abs/2407.11550v3),  [pdf](http://arxiv.org/pdf/2407.11550v3)

**Tags**: cs.CL cs.AI 



### SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language   Models
**Authors**: Kaushal Kumar Maurya, KV Aditya Srivatsa, Ekaterina Kochmar

**Updated**: 2024-08-16T06:11:21Z

**Summary**: Large language models (LLMs) have gained increased popularity due to their remarkable success across various tasks, which has led to the active development of a large set of diverse LLMs. However, individual LLMs have limitations when applied to complex tasks because of such factors as training biases, model sizes, and the datasets used. A promising approach is to efficiently harness the diverse capabilities of LLMs to overcome these individual limitations. Towards this goal, we introduce a novel LLM selection algorithm called SelectLLM. This algorithm directs input queries to the most suitable subset of LLMs from a large pool, ensuring they collectively provide the correct response efficiently. SelectLLM uses a multi-label classifier, utilizing the classifier's predictions and confidence scores to design optimal policies for selecting an optimal, query-aware, and lightweight subset of LLMs. Our findings show that the proposed model outperforms individual LLMs and achieves competitive performance compared to similarly sized, computationally expensive top-performing LLM subsets. Specifically, with a similarly sized top-performing LLM subset, we achieve a significant reduction in latency on two standard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower latency for MMLU. Additionally, we conduct comprehensive analyses and ablation studies, which validate the robustness of the proposed model.

**Link**: [arxiv](http://arxiv.org/abs/2408.08545v1),  [pdf](http://arxiv.org/pdf/2408.08545v1)

**Tags**: cs.CL 



### Symmetric Locality: Definition and Initial Results
**Authors**: Giordan Escalona, Dylan McKellips, Chen Ding

**Updated**: 2024-08-16T04:12:25Z

**Summary**: In this short paper, we characterize symmetric locality. In designing algorithms, compilers, and systems, data movement is a common bottleneck in high-performance computation, in which we improve cache and memory performance. We study a special type of data reuse in the form of repeated traversals, or re-traversals, which are based on the symmetric group. The cyclic and sawtooth traces are previously known results in symmetric locality, and in this work, we would like to generalize this result for any re-traversal. Then, we also provide an abstract framework for applications in compiler design and machine learning models to improve the memory performance of certain programs.

**Link**: [arxiv](http://arxiv.org/abs/2407.19291v2),  [pdf](http://arxiv.org/pdf/2407.19291v2)

**Tags**: eess.SY cs.SY 



### ConfusedPilot: Confused Deputy Risks in RAG-based LLMs
**Authors**: Ayush RoyChowdhury, Mulong Luo, Prateek Sahu, Sarbartha Banerjee, Mohit Tiwari

**Updated**: 2024-08-15T05:24:19Z

**Summary**: Retrieval augmented generation (RAG) is a process where a large language model (LLM) retrieves useful information from a database and then generates the responses. It is becoming popular in enterprise settings for daily business operations. For example, Copilot for Microsoft 365 has accumulated millions of businesses. However, the security implications of adopting such RAG-based systems are unclear.   In this paper, we introduce ConfusedPilot, a class of security vulnerabilities of RAG systems that confuse Copilot and cause integrity and confidentiality violations in its responses. First, we investigate a vulnerability that embeds malicious text in the modified prompt in RAG, corrupting the responses generated by the LLM. Second, we demonstrate a vulnerability that leaks secret data, which leverages the caching mechanism during retrieval. Third, we investigate how both vulnerabilities can be exploited to propagate misinformation within the enterprise and ultimately impact its operations, such as sales and manufacturing. We also discuss the root cause of these attacks by investigating the architecture of a RAG-based system. This study highlights the security vulnerabilities in today's RAG-based systems and proposes design guidelines to secure future RAG-based systems.

**Link**: [arxiv](http://arxiv.org/abs/2408.04870v3),  [pdf](http://arxiv.org/pdf/2408.04870v3)

**Tags**: cs.CR cs.AI 



### A Case for Enabling Delegation of 5G Core Decisions to the RAN
**Authors**: Lucas Vancina, Geoffrey Xie

**Updated**: 2024-08-14T23:42:46Z

**Summary**: Under conventional 5G system design, the authentication and continuous monitoring of user equipment (UE) demands a reliable backhaul connection between the radio access network (RAN) and the core network functions (AMF, AUSF, UDM, etc.). This is not a given, especially in disaster response and military operations. We propose that, in these scenarios, decisions made by core functions can be effectively delegated to the RAN by leveraging the RAN's computing resources and the micro-service programmability of the O-RAN system architecture. This paper presents several concrete designs of core-RAN decision delegation, including caching of core decisions and replicating some of the core decision logic. Each design has revealed interesting performance and security trade-offs that warrant further investigation.

**Link**: [arxiv](http://arxiv.org/abs/2408.07853v1),  [pdf](http://arxiv.org/pdf/2408.07853v1)

**Tags**: cs.NI 



### The Bicameral Cache: a split cache for vector architectures
**Authors**: Susana Rebolledo, Borja Perez, Jose Luis Bosque, Peter Hsu

**Updated**: 2024-08-14T09:18:02Z

**Summary**: The Bicameral Cache is a cache organization proposal for a vector architecture that segregates data according to their access type, distinguishing scalar from vector references. Its aim is to avoid both types of references from interfering in each other's data locality, with a special focus on prioritizing the performance on vector references. The proposed system incorporates an additional, non-polluting prefetching mechanism to help populate the long vector cache lines in advance to increase the hit rate by further exploiting the spatial locality on vector data. Its evaluation was conducted on the Cavatools simulator, comparing the performance to a standard conventional cache, over different typical vector benchmarks for several vector lengths. The results proved the proposed cache speeds up performance on stride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition, the prefetching feature consistently provided an additional value.

**Link**: [arxiv](http://arxiv.org/abs/2407.15440v2),  [pdf](http://arxiv.org/pdf/2407.15440v2)

**Tags**: cs.AR cs.PF 



### At Least Factor-of-Two Optimization for RWLE-Based Homomorphic   Encryption
**Authors**: Jonathan Ly

**Updated**: 2024-08-14T05:42:35Z

**Summary**: Many modern applications that deal with sensitive data, such as healthcare and government services, outsource computation to cloud platforms. In such untrusted environments, privacy is of vital importance. One solution to this problem is homomorphic encryption (HE), a family of cryptographic schemes that support certain algebraic operations on encrypted data without the need for decryption. However, despite major advancements, encryption in modern HE schemes still comes with a non-trivial computational overhead that can hamper data-intensive workloads. To resolve this, recent research has shown that leveraging caching techniques, such as Rache, can significantly enhance the performance of HE schemes while maintaining security. Rache unfortunately displays a key limitation in the time complexity of its caching procedure, which scales with the size of the plaintext space. Smuche is another caching scheme that simultaneously improves the scalability of the caching procedure and turns the encryption process into a constant-time operation, utilizing only a single scalar multiplication. Even still, more can be done. In this paper, we present an encryption method we call ``Zinc" which entirely forgoes the multiple caching process, replacing it with a single scalar addition, and then injecting randomness that takes constant time with respect to the plaintext space. This injection of randomness is similar to Smuche, and a great improvement from Rache, allowing Zinc to achieve efficiency without compromising security. We implement the scheme using Microsoft SEAL and compare its performance to vanilla CKKS.

**Link**: [arxiv](http://arxiv.org/abs/2408.07304v1),  [pdf](http://arxiv.org/pdf/2408.07304v1)

**Tags**: cs.CR 



### Cache-Aided MIMO Communications: DoF Analysis and Transmitter   Optimization
**Authors**: Mohammad NaseriTehrani, MohammadJavad Salehi, Antti T√∂lli

**Updated**: 2024-08-13T13:56:14Z

**Summary**: Cache-aided MIMO communications aims to jointly exploit both coded caching~(CC) and spatial multiplexing gains to enhance communication efficiency. In this paper, we first analyze the achievable degrees of freedom~(DoF) in a MIMO-CC system with CC gain \(t\), where a server with \(L\) transmit antennas communicates with \(K\) users, each equipped with \(G\) receive antennas. We demonstrate that the enhanced achievable DoF is \(\max_{\beta, \Omega} \Omega \beta\), where the number of users \(\Omega\) served in each transmission is fine-tuned to maximize DoF, and \(\beta \le \min\big(G, \nicefrac{L \binom{\Omega-1}{t}}{1 + (\Omega - t - 1)\binom{\Omega-1}{t}}\big)\) represents the number of parallel streams decoded by each user. Second, we introduce an effective transmit covariance matrix design aimed at maximizing the symmetric rate, solved iteratively via successive convex approximation. Third, we propose a new class of MIMO-CC schemes using a novel scheduling mechanism leveraging maximal multicasting opportunities to maximize delivery rates at given SNR levels while adhering to linear processing constraints. Lastly, we devise linear multicast beamforming strategies tailored for the flexible scheduling schemes in MIMO-CC systems and present an iterative solution for the efficient design of beamformers. Extensive numerical simulations are used to verify the results of the paper.

**Link**: [arxiv](http://arxiv.org/abs/2407.15743v2),  [pdf](http://arxiv.org/pdf/2407.15743v2)

**Tags**: cs.IT eess.SP math.IT 



### Ownership in low-level intermediate representation
**Authors**: Siddharth Priya, Arie Gurfinkel

**Updated**: 2024-08-13T13:31:34Z

**Summary**: The concept of ownership in high level languages can aid both the programmer and the compiler to reason about the validity of memory operations. Previously, ownership semantics has been used successfully in high level automatic program verification to model a reference to data by a first order logic (FOL) representation of data instead of maintaining an address map. However, ownership semantics is not used in low level program verification. We have identified two challenges. First, ownership information is lost when a program is compiled to a low level intermediate representation (e.g., in LLVM IR). Second, pointers in low level programs point to bytes using an address map (e.g., in unsafe Rust) and thus the verification condition (VC) cannot always replace a pointer by its FOL abstraction. To remedy the situation, we develop ownership semantics for an LLVM like low level intermediate representation. Using these semantics, the VC can opportunistically model some memory accesses by a direct access of a pointer cache that stores byte representation of data. This scheme reduces instances where an address map must be maintained, especially for mostly safe programs that follow ownership semantics. For unsafe functionality, memory accesses are modelled by operations on an address map and we provide mechanisms to keep the address map and pointer cache in sync. We implement these semantics in SEABMC, a bit precise bounded model checker for LLVM. For evaluation, the source programs are assumed to be written in C. Since C does not have ownership built in, suitable macros are added that introduce and preserve ownership during translation to LLVM like IR for verification. This approach is evaluated on mature open source C code. For both handcrafted benchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT solving.

**Link**: [arxiv](http://arxiv.org/abs/2408.04043v3),  [pdf](http://arxiv.org/pdf/2408.04043v3)

**Tags**: cs.PL cs.SE D.2.4 



### Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache   Consumption
**Authors**: Luohe Shi, Hongyi Zhang, Yao Yao, Zuchao Li, Hai Zhao

**Updated**: 2024-08-13T09:55:43Z

**Summary**: Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022, have revolutionized various industries with their advanced language comprehension. However, their efficiency is challenged by the Transformer architecture' s struggle with handling long texts. KV-Cache has emerged as a pivotal solution to this issue, converting the time complexity of token generation from quadratic to linear, albeit with increased GPU memory overhead proportional to conversation length. With the development of the LLM community and academia, various KV-Cache compression methods have been proposed. In this review, we dissect the various properties of KV-Cache and elaborate on various methods currently used to optimize the KV-Cache space usage of LLMs. These methods span the pre-training phase, deployment phase, and inference phase, and we summarize the commonalities and differences among these methods. Additionally, we list some metrics for evaluating the long-text capabilities of large language models, from both efficiency and capability perspectives. Our review thus sheds light on the evolving landscape of LLM optimization, offering insights into future advancements in this dynamic field.

**Link**: [arxiv](http://arxiv.org/abs/2407.18003v3),  [pdf](http://arxiv.org/pdf/2407.18003v3)

**Tags**: cs.CL 



### Finch: Prompt-guided Key-Value Cache Compression
**Authors**: Giulio Corallo, Paolo Papotti

**Updated**: 2024-08-13T09:08:55Z

**Summary**: Recent large language model applications, such as Retrieval-Augmented Generation and chatbots, have led to an increased need to process longer input contexts. However, this requirement is hampered by inherent limitations. Architecturally, models are constrained by a context window defined during training. Additionally, processing extensive texts requires substantial GPU memory. We propose a novel approach, Finch, to compress the input context by leveraging the pre-trained model weights of the self-attention. Given a prompt and a long text, Finch iteratively identifies the most relevant Key (K) and Value (V) pairs over chunks of the text conditioned on the prompt. Only such pairs are stored in the KV cache, which, within the space constrained by the context window, ultimately contains a compressed version of the long text. Our proposal enables models to consume large inputs even with high compression (up to 93x) while preserving semantic integrity without the need for fine-tuning.

**Link**: [arxiv](http://arxiv.org/abs/2408.00167v2),  [pdf](http://arxiv.org/pdf/2408.00167v2)

**Tags**: cs.AI 



### Value-based Proactive Caching for Sensing Data in Internet of Vehicles
**Authors**: Yantong Wang, Ke Liu, Hui Ji, Jiande Sun

**Updated**: 2024-08-12T08:46:30Z

**Summary**: Sensing data (SD) plays an important role in safe-related applications for Internet of Vehicles. Proactively caching required sensing data (SD) is a pivotal strategy for alleviating network congestion and improving data accessibility. Despite merits, existing studies predominantly address SD caching within a single time slot, which may not be scalable to scenarios involving multi-slots. Furthermore, the oversight of service capacity at caching nodes could lead to significant queuing delays in SD reception. To tackle these limitations, we jointly consider the problem of anchoring caching placement and requests allocation for SD. A value model incorporating both temporal and spacial characteristics is first proposed to estimate the significance of different caching decisions. Subsequently, a stochastic integer nonlinear programming model is provided to optimize the long-term system performance, which is converted into a series of online optimization problem by leveraging the Lyapunov method and linearized via introducing auxiliary variables. To expedite the solution, we provide a binary quantum particle swarm optimization based algorithm with quadratic time complexity. Numerical investigations demonstrate the superiority of proposed algorithms compared with other schemes in terms of energy consumption, response latency, and cache-hit ratio.

**Link**: [arxiv](http://arxiv.org/abs/2408.05996v1),  [pdf](http://arxiv.org/pdf/2408.05996v1)

**Tags**: cs.NI 



### Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open   Source RISC-V application processor
**Authors**: Riccardo Tedeschi, Luca Valente, Gianmarco Ottavi, Enrico Zelioli, Nils Wistoff, Massimiliano Giacometti, Abdul Basit Sajjad, Luca Benini, Davide Rossi

**Updated**: 2024-08-12T07:47:28Z

**Summary**: Symmetric Multi-Processing (SMP) based on cache coherency is crucial for high-end embedded systems like automotive applications. RISC-V is gaining traction, and open-source hardware (OSH) platforms offer solutions to issues such as IP costs and vendor dependency. Existing multi-core cache-coherent RISC-V platforms are complex and not efficient for small embedded core clusters. We propose an open-source SystemVerilog implementation of a lightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our design uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with Splash-3 benchmarks, our solution shows up to 32.87% faster performance in a dual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized using GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of the system area.

**Link**: [arxiv](http://arxiv.org/abs/2407.19895v2),  [pdf](http://arxiv.org/pdf/2407.19895v2)

**Tags**: eess.SY cs.SY 



### Correct Wrong Path
**Authors**: Bhargav Reddy Godala, Sankara Prasad Ramesh, Krishnam Tibrewala, Chrysanthos Pepi, Gino Chacon, Svilen Kanev, Gilles A. Pokam, Daniel A. Jim√©nez, Paul V. Gratz, David I. August

**Updated**: 2024-08-12T03:53:51Z

**Summary**: Modern OOO CPUs have very deep pipelines with large branch misprediction recovery penalties. Speculatively executed instructions on the wrong path can significantly change cache state, depending on speculation levels. Architects often employ trace-driven simulation models in the design exploration stage, which sacrifice precision for speed. Trace-driven simulators are orders of magnitude faster than execution-driven models, reducing the often hundreds of thousands of simulation hours needed to explore new micro-architectural ideas. Despite this strong benefit of trace-driven simulation, these often fail to adequately model the consequences of wrong path because obtaining them is nontrivial. Prior works consider either a positive or negative impact of wrong path but not both. Here, we examine wrong path execution in simulation results and design a set of infrastructure for enabling wrong-path execution in a trace driven simulator. Our analysis shows the wrong path affects structures on both the instruction and data sides extensively, resulting in performance variations ranging from $-3.05$\% to $20.9$\% when ignoring wrong path. To benefit the research community and enhance the accuracy of simulators, we opened our traces and tracing utility in the hopes that industry can provide wrong-path traces generated by their internal simulators, enabling academic simulation without exposing industry IP.

**Link**: [arxiv](http://arxiv.org/abs/2408.05912v1),  [pdf](http://arxiv.org/pdf/2408.05912v1)

**Tags**: cs.AR 



### Hierarchical Coded Caching with Low Subpacketization and Coding Delay
**Authors**: Rashid Ummer N. T., B. Sundar Rajan

**Updated**: 2024-08-11T16:35:10Z

**Summary**: Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN) considered a broadcast network consisting of a single server connected to a set of users each having a cache memory. Motivated by practical scenarios, Karamchandani \textit{et al.} in [16] proposed a coded caching scheme for a two-layer hierarchical network consisting of a single server connected to multiple mirror sites and each mirror site connected to a distinct set of users, in which both mirror sites and users having cache memories. Low subpacketization level coded caching schemes are desirable for practical implementations. Placement delivery array (PDA) was proposed as a tool to design coded caching schemes with reduced subpacketization level by Yan \textit{et al.} in [4]. Schemes with reduced subpacketization levels are studied extensively in the literature for single-layer networks. Kong \textit{et al.} in [17] proposed a structure called hierarchical placement delivery arrays (HPDA), which characterizes a hierarchical coded caching system and also proposed a class of HPDAs that gives low subpacketization level schemes by using two PDAs. Low subpacketization level hierarchical schemes using combinatorial $t$-designs is proposed in [20]. Apart from that there is no other existing work that discusses the subpacketization problem in a hierarchical network. This paper proposes a class of HPDA construction that gives low subpacketization level hierarchical coded caching schemes, by first constructing a new class of PDAs. Compared with the existing schemes, in cases where the system parameters and subpacketization level are the same, the proposed hierarchical scheme has a better coding delay. Further, the new class of PDAs constructed either subsumes several known PDA constructions or achieves better transmission load for the same system parameters.

**Link**: [arxiv](http://arxiv.org/abs/2405.12747v2),  [pdf](http://arxiv.org/pdf/2405.12747v2)

**Tags**: cs.IT math.IT 



### Genie: Smart ROS-based Caching for Connected Autonomous Robots
**Authors**: Zexin Li, Soroush Bateni, Cong Liu

**Updated**: 2024-08-11T08:07:28Z

**Summary**: Despite the promising future of autonomous robots, several key issues currently remain that can lead to compromised performance and safety. One such issue is latency, where we find that even the latest embedded platforms from NVIDIA fail to execute intelligence tasks (e.g., object detection) of autonomous vehicles in a real-time fashion. One remedy to this problem is the promising paradigm of edge computing. Through collaboration with our industry partner, we identify key prohibitive limitations of the current edge mindset: (1) servers are not distributed enough and thus, are not close enough to vehicles, (2) current proposed edge solutions do not provide substantially better performance and extra information specific to autonomous vehicles to warrant their cost to the user, and (3) the state-of-the-art solutions are not compatible with popular frameworks used in autonomous systems, particularly the Robot Operating System (ROS).   To remedy these issues, we provide Genie, an encapsulation technique that can enable transparent caching in ROS in a non-intrusive way (i.e., without modifying the source code), can build the cache in a distributed manner (in contrast to traditional central caching methods), and can construct a collective three-dimensional object map to provide substantially better latency (even on low-power edge servers) and higher quality data to all vehicles in a certain locality. We fully implement our design on state-of-the-art industry-adopted embedded and edge platforms, using the prominent autonomous driving software Autoware, and find that Genie can enhance the latency of Autoware Vision Detector by 82% on average, enable object reusability 31% of the time on average and as much as 67% for the incoming requests, and boost the confidence in its object map considerably over time.

**Link**: [arxiv](http://arxiv.org/abs/2402.19410v2),  [pdf](http://arxiv.org/pdf/2402.19410v2)

**Tags**: cs.RO cs.SY eess.SY 



### Eigen Attention: Attention in Low-Rank Space for KV Cache Compression
**Authors**: Utkarsh Saxena, Gobinda Saha, Sakshi Choudhary, Kaushik Roy

**Updated**: 2024-08-10T22:47:12Z

**Summary**: Large language models (LLMs) represent a groundbreaking advancement in the domain of natural language processing due to their impressive reasoning abilities. Recently, there has been considerable interest in increasing the context lengths for these models to enhance their applicability to complex tasks. However, at long context lengths and large batch sizes, the key-value (KV) cache, which stores the attention keys and values, emerges as the new bottleneck in memory usage during inference. To address this, we propose Eigen Attention, which performs the attention operation in a low-rank space, thereby reducing the KV cache memory overhead. Our proposed approach is orthogonal to existing KV cache compression techniques and can be used synergistically with them. Through extensive experiments over OPT, MPT, and Llama model families, we demonstrate that Eigen Attention results in up to 40% reduction in KV cache sizes and up to 60% reduction in attention operation latency with minimal drop in performance.

**Link**: [arxiv](http://arxiv.org/abs/2408.05646v1),  [pdf](http://arxiv.org/pdf/2408.05646v1)

**Tags**: cs.LG cs.AI cs.CL 



### ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using   Gaussian Mixture Model
**Authors**: Hanqiu Chen, Yitu Wang, Luis Vitorio Cargnini, Mohammadreza Soltaniyeh, Dongyang Li, Gongjin Sun, Pradeep Subedi, Andrew Chang, Yiran Chen, Cong Hao

**Updated**: 2024-08-10T19:17:46Z

**Summary**: Compute Express Link (CXL) emerges as a solution for wide gap between computational speed and data communication rates among host and multiple devices. It fosters a unified and coherent memory space between host and CXL storage devices such as such as Solid-state drive (SSD) for memory expansion, with a corresponding DRAM implemented as the device cache. However, this introduces challenges such as substantial cache miss penalties, sub-optimal caching due to data access granularity mismatch between the DRAM "cache" and SSD "memory", and inefficient hardware cache management. To address these issues, we propose a novel solution, named ICGMM, which optimizes caching and eviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based approach. We prototype our solution on an FPGA board, which demonstrates a noteworthy improvement compared to the classic Least Recently Used (LRU) cache strategy. We observe a decrease in the cache miss rate ranging from 0.32% to 6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD access latency. Furthermore, when compared to the state-of-the-art Long Short-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA showcases an impressive latency reduction of over 10,000 times. Remarkably, this is achieved while demanding much fewer hardware resources.

**Link**: [arxiv](http://arxiv.org/abs/2408.05614v1),  [pdf](http://arxiv.org/pdf/2408.05614v1)

**Tags**: cs.AR cs.ET cs.SY eess.SY 



### Time-resolved measurement of neutron energy isotropy in a   sheared-flow-stabilized Z pinch
**Authors**: R. A. Ryan, P. E. Tsai, A. R. Johansen, A. Youmans, D. P. Higginson, J. M. Mitrani, C. S. Adams, D. A. Sutherland, B. Levitt, U. Shumlak

**Updated**: 2024-08-09T16:48:01Z

**Summary**: Previous measurements of neutron energy using fast plastic scintillators while operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of any yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been operated at increasingly higher input power, resulting in increased plasma current and larger fusion neutron yields. A detailed experimental study of the neutron energy isotropy in these regimes applies more stringent limits to possible contributions from beam-target fusion. The FuZE device operated at $-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and D-D fusion neutron yields of $4\times10^7$ neutrons per discharge. Measurements of the neutron energy isotropy under these operating conditions demonstrates the energy of deuteron beams is less than $7.4 \pm 5.6^\mathrm{(stat)} \pm 3.7^\mathrm{(syst)}~keV$. Characterization of the detector response has reduced the number of free parameters in the fit of the neutron energy distribution, improving the confidence in the forward-fit method. Gamma backgrounds have been measured and the impact of these contributions on the isotropy results have been studied. Additionally, a time dependent measurement of the isotropy has been resolved for the first time, indicating increases to possible deuteron beam energies at late times. This suggests the possible growth of $m$=0 instabilities at the end of the main radiation event but confirms that the majority of the neutron production exhibits isotropy consistent with thermonuclear origin.

**Link**: [arxiv](http://arxiv.org/abs/2408.05171v1),  [pdf](http://arxiv.org/pdf/2408.05171v1)

**Tags**: physics.plasm-ph nucl-ex 



### NACL: A General and Effective KV Cache Eviction Framework for LLMs at   Inference Time
**Authors**: Yilong Chen, Guoxia Wang, Junyuan Shang, Shiyao Cui, Zhenyu Zhang, Tingwen Liu, Shuohuan Wang, Yu Sun, Dianhai Yu, Hua Wu

**Updated**: 2024-08-08T01:20:13Z

**Summary**: Large Language Models (LLMs) have ignited an innovative surge of AI applications, marking a new era of exciting possibilities equipped with extended context windows. However, hosting these models is cost-prohibitive mainly due to the extensive memory consumption of KV Cache involving long-context modeling. Despite several works proposing to evict unnecessary tokens from the KV Cache, most of them rely on the biased local statistics of accumulated attention scores and report performance using unconvincing metric like perplexity on inadequate short-text evaluation. In this paper, we propose NACL, a general framework for long-context KV cache eviction that achieves more optimal and efficient eviction in a single operation during the encoding phase. Due to NACL's efficiency, we combine more accurate attention score statistics in PROXY TOKENS EVICTION with the diversified random eviction strategy of RANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance the robustness in maintaining pivotal tokens for long-context modeling tasks. Notably, our method significantly improves the performance on short- and long-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50% with over 95% performance maintenance. The code is available at https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.

**Link**: [arxiv](http://arxiv.org/abs/2408.03675v2),  [pdf](http://arxiv.org/pdf/2408.03675v2)

**Tags**: cs.CL 



### A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals   and Future Trends
**Authors**: Yao Zhao, Youyang Qu, Yong Xiang, Md Palash Uddin, Dezhong Peng, Longxiang Gao

**Updated**: 2024-08-07T23:48:59Z

**Summary**: Recent advances in edge computing~(EC) have pushed cloud-based data caching services to edge, however, such emerging edge storage comes with numerous challenging and unique security issues. One of them is the problem of edge data integrity verification (EDIV) which coordinates multiple participants (e.g., data owners and edge nodes) to inspect whether data cached on edge is authentic. To date, various solutions have been proposed to address the EDIV problem, while there is no systematic review. Thus, we offer a comprehensive survey for the first time, aiming to show current research status, open problems, and potentially promising insights for readers to further investigate this under-explored field. Specifically, we begin by stating the significance of the EDIV problem, the integrity verification difference between data cached on cloud and edge, and three typical system models with corresponding inspection processes. To thoroughly assess prior research efforts, we synthesize a universal criteria framework that an effective verification approach should satisfy. On top of it, a schematic development timeline is developed to reveal the research advance on EDIV in a sequential manner, followed by a detailed review of the existing EDIV solutions. Finally, we highlight intriguing research challenges and possible directions for future work, along with a discussion on how forthcoming technology, e.g., machine learning and context-aware security, can augment security in EC. Given our findings, some major observations are: there is a noticeable trend to equip EDIV solutions with various functions and diversify study scenarios; completing EDIV within two types of participants (i.e., data owner and edge nodes) is garnering escalating interest among researchers; although the majority of existing methods rely on cryptography, emerging technology is being explored to handle the EDIV problem.

**Link**: [arxiv](http://arxiv.org/abs/2210.10978v2),  [pdf](http://arxiv.org/pdf/2210.10978v2)

**Tags**: cs.CR 



### Zero-Delay QKV Compression for Mitigating KV Cache and Network   Bottlenecks in LLM Inference
**Authors**: Zeyu Zhang, Haiying Shen

**Updated**: 2024-08-07T22:10:26Z

**Summary**: In large-language models, memory constraints in the key-value cache (KVC) pose a challenge during inference, especially with long prompts. In this work, we observed that compressing KV values is more effective than compressing the model regarding accuracy and job completion time (JCT). However, quantizing KV values and dropping less-important tokens incur significant runtime computational time overhead, delaying JCT. These methods also cannot reduce computation time or high network communication time overhead in sequence-parallelism (SP) frameworks for long prompts. To tackle these issues, based on our insightful observations from experimental analysis, we propose ZeroC, a Zero-delay QKV Compression system that eliminates time overhead and even reduces computation and communication time of the model operations. ZeroC innovatively embeds compression and decompression operations within model operations and adaptively determines compression ratios at a hybrid layer-token level. Further, it enables a communication-efficient SP inference framework. Trace-driven experiments demonstrate that ZeroC achieves up to 80% lower average JCT, 35% lower average perplexity, and 2.8x higher throughput with the same latency compared to state-of-the-art compression methods. ZeroC also reduces the average JCT of current LLM serving systems by up to 91% with the constraint of 0.1 perplexity increase. We open-sourced the code.

**Link**: [arxiv](http://arxiv.org/abs/2408.04107v1),  [pdf](http://arxiv.org/pdf/2408.04107v1)

**Tags**: cs.LG cs.DC 



### Temporal Feature Matters: A Framework for Diffusion Model Quantization
**Authors**: Yushi Huang, Ruihao Gong, Xianglong Liu, Jing Liu, Yuhang Li, Jiwen Lu, Dacheng Tao

**Updated**: 2024-08-07T20:43:10Z

**Summary**: The Diffusion models, widely used for image generation, face significant challenges related to their broad applicability due to prolonged inference times and high memory demands. Efficient Post-Training Quantization (PTQ) is crucial to address these issues. However, unlike traditional models, diffusion models critically rely on the time-step for the multi-round denoising. Typically, each time-step is encoded into a hypersensitive temporal feature by several modules. Despite this, existing PTQ methods do not optimize these modules individually. Instead, they employ unsuitable reconstruction objectives and complex calibration methods, leading to significant disturbances in the temporal feature and denoising trajectory, as well as reduced compression efficiency. To address these challenges, we introduce a novel quantization framework that includes three strategies: 1) TIB-based Maintenance: Based on our innovative Temporal Information Block (TIB) definition, Temporal Information-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are developed to efficiently align original temporal features. 2) Cache-based Maintenance: Instead of indirect and complex optimization for the related modules, pre-computing and caching quantized counterparts of temporal features are developed to minimize errors. 3) Disturbance-aware Selection: Employ temporal feature errors to guide a fine-grained selection between the two maintenance strategies for further disturbance reduction. This framework preserves most of the temporal information and ensures high-quality end-to-end generation. Extensive testing on various datasets, diffusion models and hardware confirms our superior performance and acceleration..

**Link**: [arxiv](http://arxiv.org/abs/2407.19547v2),  [pdf](http://arxiv.org/pdf/2407.19547v2)

**Tags**: cs.CV 



### mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest   Neighbor Search
**Authors**: Ahmed Abdou, Tasneem Mohsen

**Updated**: 2024-08-07T09:34:55Z

**Summary**: Named Entity Recognition (NER) is a task in Natural Language Processing (NLP) that aims to identify and classify entities in text into predefined categories. However, when applied to Arabic data, NER encounters unique challenges stemming from the language's rich morphological inflections, absence of capitalization cues, and spelling variants, where a single word can comprise multiple morphemes. In this paper, we introduce Arabic KNN-NER, our submission to the Wojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the shared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained flat-entity recognition for Arabic text, where we identify a single main entity and possibly zero or multiple sub-entities for each word. Arabic KNN-NER augments the probability distribution of a fine-tuned model with another label probability distribution derived from performing a KNN search over the cached training data. Our submission achieved 91% on the test set on the WojoodFine dataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.

**Link**: [arxiv](http://arxiv.org/abs/2408.03652v1),  [pdf](http://arxiv.org/pdf/2408.03652v1)

**Tags**: cs.CL cs.LG 



### Potential and Limitation of High-Frequency Cores and Caches
**Authors**: Kunal Pai, Anusheel Nand, Jason Lowe-Power

**Updated**: 2024-08-06T17:16:19Z

**Summary**: This paper explores the potential of cryogenic computing and superconducting electronics as promising alternatives to traditional semiconductor devices. As semiconductor devices face challenges such as increased leakage currents and reduced performance at higher temperatures, these novel technologies offer high performance and low power computation. Cryogenic computing operates at ultra-low temperatures near 77 K, leading to lower leakage currents and improved electron mobility. On the other hand, superconducting electronics, operating near 0 K, allow electrons to flow without resistance, offering the potential for ultra-low-power, high-speed computation. This study presents a comprehensive performance modeling and analysis of these technologies and provides insights into their potential benefits and limitations. We implement models of in-order and out-of-order cores operating at high clock frequencies associated with superconducting electronics and cryogenic computing in gem5. We evaluate the performance of these components using workloads representative of real-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the potential speedups achievable by these components and the limitations posed by cache bandwidth. This work provides valuable insights into the performance implications and design trade-offs associated with cryogenic and superconducting technologies, laying the foundation for future research in this field using gem5.

**Link**: [arxiv](http://arxiv.org/abs/2408.03308v1),  [pdf](http://arxiv.org/pdf/2408.03308v1)

**Tags**: cs.AR 



### LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning
**Authors**: Lekai Chen, Ashutosh Trivedi, Alvaro Velasquez

**Updated**: 2024-08-06T07:12:09Z

**Summary**: The emergence of intelligence in large language models (LLMs) has inspired investigations into their integration into automata learning. This paper introduces the probabilistic Minimally Adequate Teacher (pMAT) formulation, which leverages a probabilistic oracle that could give persistent errors randomly during answering the membership queries for deterministic finite automata (DFA) learning. Given the tendency of LLMs to produce hallucinatory content, we have developed techniques to improve answer accuracy and ensure the correctness of the learned automata. We propose the $\mathtt{Discrimination}$ prompt as well as the $\mathtt{Verification}$ prompt and explore their advantages over common prompts. Additionally, we compare DFA learning performance between the TTT algorithm and common active learning algorithms. To address the exponential number of persistent errors, we implement a dynamic query cache refinement algorithm that identifies and corrects conflicting queries by combining the active and passive learning algorithms. The empirical results demonstrate the robustness and efficiency of our approach, providing a theoretical foundation for automata learning with LLMs in the loop.

**Link**: [arxiv](http://arxiv.org/abs/2408.02999v1),  [pdf](http://arxiv.org/pdf/2408.02999v1)

**Tags**: cs.FL cs.AI 



### NVPC: A Transparent NVM Page Cache
**Authors**: Guoyu Wang, Xilong Che, Haoyang Wei, Shuo Chen, Puyi He, Juncheng Hu

**Updated**: 2024-08-06T02:51:22Z

**Summary**: Towards a compatible utilization of NVM, NVM-specialized kernel file systems and NVM-based disk file system accelerators have been proposed. However, these studies only focus on one or several characteristics of NVM, while failing to exploit its best practice by putting NVM in the proper position of the whole storage stack. In this paper, we present NVPC, a transparent acceleration to existing kernel file systems with an NVM-enhanced page cache. The acceleration lies in two aspects, respectively matching the desperate needs of existing disk file systems: sync writes and cache-missed operations. Besides, the fast DRAM page cache is preserved for cache-hit operations. For sync writes, a high-performance log-based sync absorbing area is provided to redirect data destination from the slow disk to the fast NVM. Meanwhile, the byte-addressable feature of NVM is used to prevent write amplification. For cache-missed operations, NVPC makes use of the idle space on NVM to extend the DRAM page cache, so that more and larger workloads can fit into the cache. NVPC is entirely implemented as a page cache, thus can provide efficient speed-up to disk file systems with full transparency to users and full compatibility to lower file systems.   In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x faster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger than DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and SPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in 62.5% of the tested cases in our read/write/sync mixed evaluation, demonstrating that NVPC is more balanced and adaptive to complex real-world workloads. Experimental results also show that NVPC is the only method that accelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to any other use cases.

**Link**: [arxiv](http://arxiv.org/abs/2408.02911v1),  [pdf](http://arxiv.org/pdf/2408.02911v1)

**Tags**: cs.OS 



### Electron-beam-induced modification of gold microparticles in an SEM
**Authors**: Kristina Weinel, Marc Benjamin Hahn, Axel Lubk, Wen Feng, Ignacio Gonzalez Martinez, Bernd B√ºchner, Leonardo Agudo J√°come

**Updated**: 2024-08-05T12:09:50Z

**Summary**: Electron-beam-induced conversion of materials in a transmission electron microscope uses the high power density of a localized electron beam of acceleration voltages above 100 kV as an energy source to transform matter at the sub-micron scale. Here, the e-beam-induced transformation of precursor microparticles employing a low-energy e-beam with an acceleration voltage of 30 kV in a scanning electron microscope is developed to increase the versatility and efficiency of the technique. Under these conditions, the technique can be classified between e-beam lithography, where the e-beam is used to mill holes in or grow some different material onto a substrate, and e-beam welding, where matter can be welded together when overcoming the melting phase. Modifying gold microparticles on an amorphous SiOx substrate reveals the dominant role of inelastic electron-matter interaction and subsequent localized heating for the observed melting and vaporization of the precursor microparticles under the electron beam. Monte-Carlo scattering simulations and thermodynamic modeling further support the findings.

**Link**: [arxiv](http://arxiv.org/abs/2408.02409v1),  [pdf](http://arxiv.org/pdf/2408.02409v1)

**Tags**: cond-mat.mtrl-sci 



### SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference   Serving
**Authors**: Andreas Kosmas Kakolyris, Dimosthenis Masouros, Petros Vavaroutsos, Sotirios Xydis, Dimitrios Soudris

**Updated**: 2024-08-05T09:07:06Z

**Summary**: As Large Language Models (LLMs) gain traction, their reliance on power-hungry GPUs places ever-increasing energy demands, raising environmental and monetary concerns. Inference dominates LLM workloads, presenting a critical challenge for providers: minimizing energy costs under Service-Level Objectives (SLOs) that ensure optimal user experience. In this paper, we present \textit{throttLL'eM}, a framework that reduces energy consumption while meeting SLOs through the use of instance and GPU frequency scaling. \textit{throttLL'eM} features mechanisms that project future KV cache usage and batch size. Leveraging a Machine-Learning (ML) model that receives these projections as inputs, \textit{throttLL'eM} manages performance at the iteration level to satisfy SLOs with reduced frequencies and instance sizes. We show that the proposed ML model achieves $R^2$ scores greater than 0.97 and miss-predicts performance by less than 1 iteration per second on average. Experimental results on LLM inference traces show that \textit{throttLL'eM} achieves up to 43.8\% lower energy consumption and an energy efficiency improvement of at least $1.71\times$ under SLOs, when compared to NVIDIA's Triton server.

**Link**: [arxiv](http://arxiv.org/abs/2408.05235v1),  [pdf](http://arxiv.org/pdf/2408.05235v1)

**Tags**: cs.DC cs.AI cs.AR cs.LG 



### TriForce: Lossless Acceleration of Long Sequence Generation with   Hierarchical Speculative Decoding
**Authors**: Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, Beidi Chen

**Updated**: 2024-08-04T00:58:04Z

**Summary**: With large language models (LLMs) widely deployed in long content generation recently, there has emerged an increasing demand for efficient long-sequence inference support. However, key-value (KV) cache, which is stored to avoid re-computation, has emerged as a critical bottleneck by growing linearly in size with the sequence length. Due to the auto-regressive nature of LLMs, the entire KV cache will be loaded for every generated token, resulting in low utilization of computational cores and high latency. While various compression methods for KV cache have been proposed to alleviate this issue, they suffer from degradation in generation quality. We introduce TriForce, a hierarchical speculative decoding system that is scalable for long sequence generation. This approach leverages the original model weights and dynamic sparse KV cache via retrieval as a draft model, which serves as an intermediate layer in the hierarchy and is further speculated by a smaller model to reduce its drafting latency. TriForce not only facilitates impressive speedups for Llama2-7B-128K, achieving up to 2.31$\times$ on an A100 GPU but also showcases scalability in handling even longer contexts. For the offloading setting on two RTX 4090 GPUs, TriForce achieves 0.108s/token$\unicode{x2014}$only half as slow as the auto-regressive baseline on an A100, which attains 7.78$\times$ on our optimized offloading system. Additionally, TriForce performs 4.86$\times$ than DeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is highlighted by its consistently outstanding performance across various temperatures. The code is available at https://github.com/Infini-AI-Lab/TriForce.

**Link**: [arxiv](http://arxiv.org/abs/2404.11912v3),  [pdf](http://arxiv.org/pdf/2404.11912v3)

**Tags**: cs.CL cs.LG 



### Cross-layer Attention Sharing for Large Language Models
**Authors**: Yongyu Mu, Yuzhang Wu, Yuchun Fan, Chenglong Wang, Hengyu Li, Qiaozhi He, Murun Yang, Tong Xiao, Jingbo Zhu

**Updated**: 2024-08-04T00:38:34Z

**Summary**: As large language models (LLMs) evolve, the increase in model depth and parameter number leads to substantial redundancy. To enhance the efficiency of the attention mechanism, previous works primarily compress the KV cache or group attention heads, while largely overlooking redundancy between layers. Our comprehensive analyses across various LLMs show that highly similar attention patterns persist within most layers. It's intuitive to save the computation by sharing attention weights across layers. However, further analysis reveals two challenges: (1) Directly sharing the weight matrix without carefully rearranging the attention heads proves to be ineffective; (2) Shallow layers are vulnerable to small deviations in attention weights. Driven by these insights, we introduce LiSA, a lightweight substitute for self-attention in well-trained LLMs. LiSA employs tiny feed-forward networks to align attention heads between adjacent layers and low-rank matrices to approximate differences in layer-wise attention weights. Evaluations encompassing 13 typical benchmarks demonstrate that LiSA maintains high response quality in terms of accuracy and perplexity while reducing redundant attention calculations within 53-84% of the total layers. Our implementations of LiSA achieve a 6X compression of Q and K, with maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for LLaMA2-7B.

**Link**: [arxiv](http://arxiv.org/abs/2408.01890v1),  [pdf](http://arxiv.org/pdf/2408.01890v1)

**Tags**: cs.CL 



### Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling
**Authors**: Xiao Jiang, Grace J. Gang, J. Webster Stayman

**Updated**: 2024-08-02T18:25:57Z

**Summary**: Many spectral CT applications require accurate material decomposition. Existing material decomposition algorithms are often susceptible to significant noise magnification or, in the case of one-step model-based approaches, hampered by slow convergence rates and large computational requirements. In this work, we proposed a novel framework - spectral diffusion posterior sampling (spectral DPS) - for one-step reconstruction and multi-material decomposition, which combines sophisticated prior information captured by one-time unsupervised learning and an arbitrary analytic physical system model. Spectral DPS is built upon a general DPS framework for nonlinear inverse problems. Several strategies developed in previous work, including jumpstart sampling, Jacobian approximation, and multi-step likelihood updates are applied facilitate stable and accurate decompositions. The effectiveness of spectral DPS was evaluated on a simulated dual-layer and a kV-switching spectral system as well as on a physical cone-beam CT (CBCT) test bench. In simulation studies, spectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53% to 57.30% over MBMD, depending on the the region of interest. In physical phantom study, spectral DPS achieved a <1% error in estimating the mean density in a homogeneous region. Compared with baseline DPS, spectral DPS effectively avoided generating false structures in the homogeneous phantom and reduced the variability around edges. Both simulation and physical phantom studies demonstrated the superior performance of spectral DPS for stable and accurate material decomposition.

**Link**: [arxiv](http://arxiv.org/abs/2408.01519v1),  [pdf](http://arxiv.org/pdf/2408.01519v1)

**Tags**: physics.med-ph 



### Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching   in SSD's NAND Flash Memory Chip for Data Indexing Acceleration
**Authors**: Yun-Chih Chen, Yuan-Hao Chang, Tei-Wei Kuo

**Updated**: 2024-08-02T07:37:51Z

**Summary**: To index the increasing volume of data, modern data indexes are typically stored on SSDs and cached in DRAM. However, searching such an index has resulted in significant I/O traffic due to limited access locality and inefficient cache utilization. At the heart of index searching is the operation of filtering through vast data spans to isolate a small, relevant subset, which involves basic equality tests rather than the complex arithmetic provided by modern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which demonstrates the feasibility of performing data filtering directly within a NAND flash memory chip, transmitting only relevant search results rather than complete pages. Instead of adding complex circuits, we propose repurposing existing circuitry for efficient and accurate bitwise parallel matching. We demonstrate how different data structures can use our flexible SIMD command interface to offload index searches. This strategy not only frees up the CPU for more computationally demanding tasks, but it also optimizes DRAM usage for write buffering, significantly lowering energy consumption associated with I/O transmission between the CPU and DRAM. Extensive testing across a wide range of workloads reveals up to a 9X speedup in write-heavy workloads and up to 45% energy savings due to reduced read and write I/O. Furthermore, we achieve significant reductions in median and tail read latencies of up to 89% and 85% respectively.

**Link**: [arxiv](http://arxiv.org/abs/2408.00327v2),  [pdf](http://arxiv.org/pdf/2408.00327v2)

**Tags**: cs.AR 



### Caching Aided Multi-Tenant Serverless Computing
**Authors**: Chu Qiao, Cong Wang, Zhenkai Zhang, Yuede Ji, Xing Gao

**Updated**: 2024-08-01T23:52:43Z

**Summary**: One key to enabling high-performance serverless computing is to mitigate cold-starts. Current solutions utilize a warm pool to keep function alive: a warm-start can be analogous to a CPU cache-hit. However, modern cache has multiple hierarchies and the last-level cache is shared among cores, whereas the warm pool is limited to a single tenant for security concerns. Also, the warm pool keep-alive policy can be further optimized using cache replacement algorithms. In this paper, we borrow practical optimizations from caching, and design FaasCamp, a caching-aided multi-tenant serverless computing framework. FaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim pool introduced enabling secure function instance sharing among tenants. Also, FaasCamp leverages machine learning to approximate the optimal cache replacement policy to improve the warm rate. We have implemented a prototype and conducted extensive experiments under multiple scenarios. The results show that FaasCamp can outperform existing platforms with minimal overhead.

**Link**: [arxiv](http://arxiv.org/abs/2408.00957v1),  [pdf](http://arxiv.org/pdf/2408.00957v1)

**Tags**: cs.DC 



### Do language models plan ahead for future tokens?
**Authors**: Wilson Wu, John X. Morris, Lionel Levine

**Updated**: 2024-08-01T21:21:28Z

**Summary**: Do transformers "think ahead" during inference at a given position? It is known transformers prepare information in the hidden states of the forward pass at time step $t$ that is then used in future forward passes $t+\tau$. We posit two explanations for this phenomenon: pre-caching, in which off-diagonal gradient terms present during training result in the model computing features at $t$ irrelevant to the present inference task but useful for the future, and breadcrumbs, in which features most relevant to time step $t$ are already the same as those that would most benefit inference at time $t+\tau$. We test these hypotheses by training language models without propagating gradients to past timesteps, a scheme we formalize as myopic training. In a constructed synthetic data setting, we find clear evidence for pre-caching. In the autoregressive language modeling setting, our experiments are more suggestive of the breadcrumbs hypothesis, though pre-caching increases with model scale.

**Link**: [arxiv](http://arxiv.org/abs/2404.00859v2),  [pdf](http://arxiv.org/pdf/2404.00859v2)

**Tags**: cs.LG cs.CL 



### Intermittent Semi-working Mask: A New Masking Paradigm for LLMs
**Authors**: Mingcong Lu, Jiangcai Zhu, Wang Hao, Zheng Li, Shusheng Zhang, Kailai Shao, Chao Chen, Nan Li, Feng Wang, Xin Lu

**Updated**: 2024-08-01T13:22:01Z

**Summary**: Multi-turn dialogues are a key interaction method between humans and Large Language Models (LLMs), as conversations extend over multiple rounds, keeping LLMs' high generation quality and low latency is a challenge. Mainstream LLMs can be grouped into two categories based on masking strategy: causal LLM and prefix LLM. Several works have demonstrated that prefix LLMs tend to outperform causal ones in scenarios that heavily depend on historical context such as multi-turn dialogues or in-context learning, thanks to their bidirectional attention on prefix sequences. However, prefix LLMs have an inherent inefficient training problem in multi-turn dialogue datasets. In addition, the attention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV Cache) across dialogue rounds to reduce generation latency. In this paper, we propose a novel masking scheme called Intermittent Semi-working Mask (ISM) to address these problems. Specifically, we apply alternate bidirectional and unidirectional attention on queries and answers in the dialogue history. In this way, ISM is able to maintain the high quality of prefix LLM and low generation latency of causal LLM, simultaneously. Extensive experiments illustrate that our ISM achieves significant performance.

**Link**: [arxiv](http://arxiv.org/abs/2408.00539v1),  [pdf](http://arxiv.org/pdf/2408.00539v1)

**Tags**: cs.CL cs.AI 



### MoE-Infinity: Offloading-Efficient MoE Model Serving
**Authors**: Leyang Xue, Yao Fu, Zhan Lu, Luo Mai, Mahesh Marina

**Updated**: 2024-08-01T13:21:24Z

**Summary**: This paper presents MoE-Infinity, an offloading-efficient serving system for sparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity achieves novel request-level tracing for expert activation, capturing MoE's sparse execution patterns such as selective activation, group activation, and skewed reuse. Leveraging the request-level trace, MoE-Infinity performs effective expert prefetching and expert caching, achieving high efficiency in transferring model parameters from host memory to GPU memory. Experimental results demonstrate that MoE-Infinity achieves low latency comparable to expensive full-GPU deployments, which require up to 4X more GPU resources than MoE-Infinity. Compared to offloading-supporting LLM serving systems such as DeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm, MoE-Infinity exhibits superior latency performance, providing 2-20X improvements when serving various MoE models for a large collection of LLM tasks. MoE-Infinity's source code is publicly available a https://github.com/TorchMoE/MoE-Infinity

**Link**: [arxiv](http://arxiv.org/abs/2401.14361v2),  [pdf](http://arxiv.org/pdf/2401.14361v2)

**Tags**: cs.LG cs.PF 



### ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and   Two-Phase Partition
**Authors**: Lu Ye, Ze Tao, Yong Huang, Yang Li

**Updated**: 2024-08-01T07:51:25Z

**Summary**: Self-attention is an essential component of large language models (LLM) but a significant source of inference latency for long sequences. In multi-tenant LLM serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the presence of shared system prompts. Experiments show that ChunkAttention can speed up the self-attention kernel by 3.2-4.8$\times$ compared to the state-of-the-art implementation, with the length of the system prompt ranging from 1024 to 4096.

**Link**: [arxiv](http://arxiv.org/abs/2402.15220v4),  [pdf](http://arxiv.org/pdf/2402.15220v4)

**Tags**: cs.LG cs.CL 



### CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph   Neural Network Training with Communication Reduction
**Authors**: Shuai Zhang, Zite Jiang, Haihang You

**Updated**: 2024-08-01T01:57:09Z

**Summary**: Graph neural network training is mainly categorized into mini-batch and full-batch training methods. The mini-batch training method samples subgraphs from the original graph in each iteration. This sampling operation introduces extra computation overhead and reduces the training accuracy. Meanwhile, the full-batch training method calculates the features and corresponding gradients of all vertices in each iteration, and therefore has higher convergence accuracy. However, in the distributed cluster, frequent remote accesses of vertex features and gradients lead to huge communication overhead, thus restricting the overall training efficiency.   In this paper, we introduce the cached-based distributed full-batch graph neural network training framework (CDFGNN). We propose the adaptive cache mechanism to reduce the remote vertex access by caching the historical features and gradients of neighbor vertices. Besides, we further optimize the communication overhead by quantifying the messages and designing the graph partition algorithm for the hierarchical communication architecture. Experiments show that the adaptive cache mechanism reduces remote vertex accesses by 63.14% on average. Combined with communication quantization and hierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed full-batch training frameworks by 30.39% in our experiments. Our results indicate that CDFGNN has great potential in accelerating distributed full-batch GNN training tasks.

**Link**: [arxiv](http://arxiv.org/abs/2408.00232v1),  [pdf](http://arxiv.org/pdf/2408.00232v1)

**Tags**: cs.DC cs.LG 



### Towards Variable-Length In-Network Caching
**Authors**: Gyuyeong Kim

**Updated**: 2024-08-01T00:41:52Z

**Summary**: We present StarCache, a new in-network caching architecture that can cache variable-length items to balance a wide range of key-value workloads. Unlike existing works, StarCache does not cache hot items in the switch memory. Instead, we make hot items revisit the switch data plane continuously by exploiting packet recirculation. Our approach keeps cached key-value pairs in the switch data plane while freeing them from item size limitations caused by hardware constraints. We implement a StarCache prototype on an Intel Tofino switch. Our experimental results show that StarCache can balance highly skewed workloads with various key and value sizes.

**Link**: [arxiv](http://arxiv.org/abs/2407.21324v2),  [pdf](http://arxiv.org/pdf/2407.21324v2)

**Tags**: cs.NI 



### A2SF: Accumulative Attention Scoring with Forgetting Factor for Token   Pruning in Transformer Decoder
**Authors**: Hyun-rae Jo, Dongkun Shin

**Updated**: 2024-07-31T02:02:40Z

**Summary**: Recently, large language models (LLM) based on transformers are facing memory bottleneck issues due to KV cache, especially in long sequence handling. Previous researches proposed KV cache compression techniques that identify insignificant tokens based on Accumulative Attention Scores and removes their items from KV cache, noting that only few tokens play an important role in attention operations. However, we have observed that the existing Accumulative Attention Score is not suitable for the transformer decoder structure. In the decoder model, the number of times the Attention Score accumulates varies depending on the order of token appearance due to the effect of masking, causing an uneven comparison between tokens. To solve this, we propose Accumulative Attention Score with Forgetting Factor (A2SF) technique, which introduces a Forgetting Factor in the Attention Score accumulation process. A2SF applies a penalty to the past Attention Score generated from old tokens by repeatedly multiplying the Forgetting Factor to the Attention Score over time. Therefore, older tokens receive a larger penalty, providing fairness among different ages of tokens. Through the fair comparison among tokens, we can more effectively select important tokens. We have verified the accuracy improvement through A2SF in the OPT and LLaMA models and A2SF improves the accuracy of LLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.

**Link**: [arxiv](http://arxiv.org/abs/2407.20485v2),  [pdf](http://arxiv.org/pdf/2407.20485v2)

**Tags**: cs.CL cs.LG 



### Electric field control of magnetocaloric effect in cylindrical MnAs/PZT   magnetoelectric composite
**Authors**: Abdulkarim A. Amirov, Maksim A. Koliushenkov, Abdula A. Mukhuchev, Dibir M. Yusupov, Valeriya V. Govorina, Dmitriy S. Neznakhin, Gennady A. Govor, Akhmed M. Aliev

**Updated**: 2024-07-30T21:27:00Z

**Summary**: The possibility of electric field control of magnetocaloric effect through quasi-isostatic compression as a result of the converse piezoelectric effect was demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was shown that an electric voltage of 100 V corresponding to an electric field of E ~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the MnAs/PZT composite contributes to an increase in the maximum adiabatic temperature change by 0.2 K in the temperature range of the magnetostructural phase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations using the finite element method have shown that an electric field voltage of 100 V is capable of creating a quasi-isostatic mechanical stress in the region inside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak pressures up to 10 MPa, the contribution to the MCE from piezo compression linearly depends on the electrical voltage that can be used for control the MCE

**Link**: [arxiv](http://arxiv.org/abs/2407.21201v1),  [pdf](http://arxiv.org/pdf/2407.21201v1)

**Tags**: cond-mat.mtrl-sci 



### Palu: Compressing KV-Cache with Low-Rank Projection
**Authors**: Chi-Chih Chang, Wei-Cheng Lin, Chien-Yu Lin, Chong-Yan Chen, Yu-Fang Hu, Pei-Shuo Wang, Ning-Chi Huang, Luis Ceze, Kai-Chiang Wu

**Updated**: 2024-07-30T18:19:38Z

**Summary**: KV-Cache compression methods generally sample a KV-Cache of effectual tokens or quantize it into lower bits. However, these methods cannot exploit the redundancy of the hidden dimension of KV tensors. This paper investigates a unique hidden dimension approach called Palu, a novel KV-Cache compression framework that utilizes low-rank projection. Palu decomposes the linear layers into low-rank matrices, caches the smaller intermediate states, and reconstructs the full keys and values on the fly. To improve accuracy, compression rate, and efficiency, Palu further encompasses (1) a medium-grained low-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a low-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU kernels. Our extensive experiments with popular LLMs show that Palu can compress KV-Cache by more than 91.25% while maintaining a significantly better accuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache quantization methods at a similar or even higher memory usage. When compressing KV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the attention module. Our code is publicly available at https://github.com/shadowpa0327/Palu.

**Link**: [arxiv](http://arxiv.org/abs/2407.21118v1),  [pdf](http://arxiv.org/pdf/2407.21118v1)

**Tags**: cs.AI cs.LG 



### ThinK: Thinner Key Cache by Query-Driven Pruning
**Authors**: Yuhui Xu, Zhanming Jie, Hanze Dong, Lei Wang, Xudong Lu, Aojun Zhou, Amrita Saha, Caiming Xiong, Doyen Sahoo

**Updated**: 2024-07-30T17:59:08Z

**Summary**: Large Language Models (LLMs) have revolutionized the field of natural language processing, achieving unprecedented performance across a variety of applications by leveraging increased model sizes and sequence lengths. However, the associated rise in computational and memory costs poses significant challenges, particularly in managing long sequences due to the quadratic complexity of the transformer attention mechanism. This paper focuses on the long-context scenario, addressing the inefficiencies in KV cache memory consumption during inference. Unlike existing approaches that optimize the memory based on the sequence lengths, we uncover that the channel dimension of the KV cache exhibits significant redundancy, characterized by unbalanced magnitude distribution and low-rank structure in attention weights. Based on these observations, we propose ThinK, a novel query-dependent KV cache pruning method designed to minimize attention weight loss while selectively pruning the least significant channels. Our approach not only maintains or enhances model accuracy but also achieves a reduction in memory costs by over 20% compared with vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and Mistral models across various long-sequence datasets confirm the efficacy of ThinK, setting a new precedent for efficient LLM deployment without compromising performance. We also outline the potential of extending our method to value cache pruning, demonstrating ThinK's versatility and broad applicability in reducing both memory and computational overheads.

**Link**: [arxiv](http://arxiv.org/abs/2407.21018v1),  [pdf](http://arxiv.org/pdf/2407.21018v1)

**Tags**: cs.CL cs.AI 



### SpChar: Characterizing the Sparse Puzzle via Decision Trees
**Authors**: Francesco Sgherzi, Marco Siracusa, Ivan Fernandez, Adri√† Armejach, Miquel Moret√≥

**Updated**: 2024-07-30T13:06:36Z

**Summary**: Sparse matrix computation is crucial in various modern applications, including large-scale graph analytics, deep learning, and recommender systems. The performance of sparse kernels varies greatly depending on the structure of the input matrix, making it difficult to gain a comprehensive understanding of sparse computation and its relationship to inputs, algorithms, and target machine architecture. Despite extensive research on certain sparse kernels, such as Sparse Matrix-Vector Multiplication (SpMV), the overall family of sparse algorithms has yet to be investigated as a whole. This paper introduces SpChar, a workload characterization methodology for general sparse computation. SpChar employs tree-based models to identify the most relevant hardware and input characteristics, starting from hardware and input-related metrics gathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis enables the creation of a characterization loop that facilitates the optimization of sparse computation by mapping the impact of architectural features to inputs and algorithmic choices. We apply SpChar to more than 600 matrices from the SuiteSparse Matrix collection and three state-of-the-art Arm CPUs to determine the critical hardware and software characteristics that affect sparse computation. In our analysis, we determine that the biggest limiting factors for high-performance sparse computation are (1) the latency of the memory system, (2) the pipeline flush overhead resulting from branch misprediction, and (3) the poor reuse of cached elements. Additionally, we propose software and hardware optimizations that designers can implement to create a platform suitable for sparse computation. We then investigate these optimizations using the gem5 simulator to achieve a significant speedup of up to 2.63x compared to a CPU where the optimizations are not applied.

**Link**: [arxiv](http://arxiv.org/abs/2304.06944v2),  [pdf](http://arxiv.org/pdf/2304.06944v2)

**Tags**: cs.AR B.8.2 



### UpDown: Programmable fine-grained Events for Scalable Performance on   Irregular Applications
**Authors**: Andronicus Rajasukumar, Jiya Su, Yuqing, Wang, Tianshuo Su, Marziyeh Nourian, Jose M Monsalve Diaz, Tianchi Zhang, Jianru Ding, Wenyi Wang, Ziyi Zhang, Moubarak Jeje, Henry Hoffmann, Yanjing Li, Andrew A. Chien

**Updated**: 2024-07-30T12:16:39Z

**Summary**: Applications with irregular data structures, data-dependent control flows and fine-grained data transfers (e.g., real-world graph computations) perform poorly on cache-based systems. We propose the UpDown accelerator that supports fine-grained execution with novel architecture mechanisms - lightweight threading, event-driven scheduling, efficient ultra-short threads, and split-transaction DRAM access with software-controlled synchronization. These hardware primitives support software programmable events, enabling high performance on diverse data structures and algorithms. UpDown also supports scalable performance; hardware replication enables programs to scale up performance. Evaluation results show UpDown's flexibility and scalability enable it to outperform CPUs on graph mining and analytics computations by up to 116-195x geomean speedup and more than 4x speedup over prior accelerators. We show that UpDown generates high memory parallelism (~4.6x over CPU) required for memory intensive graph computations. We present measurements that attribute the performance of UpDown (23x architectural advantage) to its individual architectural mechanisms. Finally, we also analyze the area and power cost of UpDown's mechanisms for software programmability.

**Link**: [arxiv](http://arxiv.org/abs/2407.20773v1),  [pdf](http://arxiv.org/pdf/2407.20773v1)

**Tags**: cs.AR 



### Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models
**Authors**: Eman Ali, Muhammad Haris Khan

**Updated**: 2024-07-30T08:39:52Z

**Summary**: Recent advances in large-scale vision-language models have achieved impressive performance in various zero-shot image classification tasks. While prior studies have demonstrated significant improvements by introducing few-shot labelled target samples, they still require labelling of target samples, which greatly degrades their scalability and generalizability while handling various visual recognition tasks. We design NtUA, a Noise-tolerant Unsupervised Adapter that allows the learning of effective target models with few unlabelled target samples. NtUA works as a key-value cache that formulates visual features and predicted pseudo-labels of the few unlabelled target samples as key-value pairs. It consists of two complementary designs. The first is adaptive cache formation that combats pseudo-label noises by weighting the key-value pairs according to their prediction confidence. The second is knowledge-guided cache refinement, which refines pair values (i.e., pseudo-labels) and cache weights by leveraging knowledge distillation from large-scale vision language models. Extensive experiments show that NtUA achieves superior performance consistently across multiple widely adopted benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2309.14928v3),  [pdf](http://arxiv.org/pdf/2309.14928v3)

**Tags**: cs.CV cs.LG 



### Robust Federated Learning for Wireless Networks: A Demonstration with   Channel Estimation
**Authors**: Zexin Fang, Bin Han, Hans D. Schotten

**Updated**: 2024-07-30T08:19:53Z

**Summary**: Federated learning (FL) offers a privacy-preserving collaborative approach for training models in wireless networks, with channel estimation emerging as a promising application. Despite extensive studies on FL-empowered channel estimation, the security concerns associated with FL require meticulous attention. In a scenario where small base stations (SBSs) serve as local models trained on cached data, and a macro base station (MBS) functions as the global model setting, an attacker can exploit the vulnerability of FL, launching attacks with various adversarial attacks or deployment tactics. In this paper, we analyze such vulnerabilities, corresponding solutions were brought forth, and validated through simulation.

**Link**: [arxiv](http://arxiv.org/abs/2404.03088v2),  [pdf](http://arxiv.org/pdf/2404.03088v2)

**Tags**: cs.LG cs.AI cs.NI eess.SP 



### Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)
**Authors**: Ziyue Qiu, Juncheng Yang, Mor Harchol-Balter

**Updated**: 2024-07-30T04:01:25Z

**Summary**: Software caches are an intrinsic component of almost every computer system. Consequently, caching algorithms, particularly eviction policies, are the topic of many papers. Almost all these prior papers evaluate the caching algorithm based on its hit ratio, namely the fraction of requests that are found in the cache, as opposed to disk. The hit ratio is viewed as a proxy for traditional performance metrics like system throughput or response time. Intuitively it makes sense that higher hit ratio should lead to higher throughput (and lower response time), since more requests are found in the cache (low access time) as opposed to the disk (high access time).   This paper challenges this intuition. We show that increasing the hit ratio can actually hurt the throughput (and response time) for many caching algorithms. Our investigation follows a three-pronged approach involving (i) queueing modeling and analysis, (ii) implementation and measurement, and (iii) simulation to validate the accuracy of the queueing model. We also show that the phenomenon of throughput decreasing at higher hit ratios is likely to be more pronounced in future systems, where the trend is towards faster disks and higher numbers of cores per CPU.

**Link**: [arxiv](http://arxiv.org/abs/2404.16219v3),  [pdf](http://arxiv.org/pdf/2404.16219v3)

**Tags**: cs.PF 



### STT-RAM-based Hierarchical In-Memory Computing
**Authors**: Dhruv Gajaria, Kevin Antony Gomez, Tosiron Adegbija

**Updated**: 2024-07-29T01:43:26Z

**Summary**: In-memory computing promises to overcome the von Neumann bottleneck in computer systems by performing computations directly within the memory. Previous research has suggested using Spin-Transfer Torque RAM (STT-RAM) for in-memory computing due to its non-volatility, low leakage power, high density, endurance, and commercial viability. This paper explores hierarchical in-memory computing, where different levels of the memory hierarchy are augmented with processing elements to optimize workload execution. The paper investigates processing in memory (PiM) using non-volatile STT-RAM and processing in cache (PiC) using volatile STT-RAM with relaxed retention, which helps mitigate STT-RAM's write latency and energy overheads. We analyze tradeoffs and overheads associated with data movement for PiC versus write overheads for PiM using STT-RAMs for various workloads. We examine workload characteristics, such as computational intensity and CPU-dependent workloads with limited instruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using these workloads, we evaluate computing in STT-RAM versus SRAM at different cache hierarchy levels and explore the potential of heterogeneous STT-RAM cache architectures with various retention times for PiC and CPU-based computing. Our experiments reveal significant advantages of STT-RAM-based PiC over PiM for specific workloads. Finally, we describe open research problems in hierarchical in-memory computing architectures to further enhance this paradigm.

**Link**: [arxiv](http://arxiv.org/abs/2407.19637v1),  [pdf](http://arxiv.org/pdf/2407.19637v1)

**Tags**: cs.CY cs.AR 



### CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory   Processing
**Authors**: Dhruv Gajaria, Tosiron Adegbija, Kevin Gomez

**Updated**: 2024-07-29T01:17:54Z

**Summary**: Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures, especially those utilizing bit-line computing, offer promising solutions to mitigate data movement bottlenecks within the memory hierarchy. While previous studies have explored the integration of compute units within individual memory levels, the complexity and potential overheads associated with these designs have often limited their capabilities. This paper introduces a novel PiC/PiM architecture, Concurrent Hierarchical In-Memory Processing (CHIME), which strategically incorporates heterogeneous compute units across multiple levels of the memory hierarchy. This design targets the efficient execution of diverse, domain-specific workloads by placing computations closest to the data where it optimizes performance, energy consumption, data movement costs, and area. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing, such as high density, low leakage, and better resiliency to data corruption from activating multiple word lines. We demonstrate that CHIME enhances concurrency and improves compute unit utilization at each level of the memory hierarchy. We present strategies for exploring the design space, grouping, and placing the compute units across the memory hierarchy. Experiments reveal that, compared to the state-of-the-art bit-line computing approaches, CHIME achieves significant speedup and energy savings of 57.95% and 78.23% for various domain-specific workloads, while reducing the overheads associated with single-level compute designs.

**Link**: [arxiv](http://arxiv.org/abs/2407.19627v1),  [pdf](http://arxiv.org/pdf/2407.19627v1)

**Tags**: cs.CY 



### ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient   Multicore Processors
**Authors**: Dhruv Gajaria, Tosiron Adegbija

**Updated**: 2024-07-28T23:43:59Z

**Summary**: Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been widely studied as a way to reduce STT-RAM's write energy and latency overheads. Given a relaxed retention time STT-RAM level one (L1) cache, we analyze the impacts of dynamic voltage and frequency scaling (DVFS) -- a common optimization in modern processors -- on STT-RAM L1 cache design. Our analysis reveals that, apart from the fact that different applications may require different retention times, the clock frequency, which is typically ignored in most STT-RAM studies, may also significantly impact applications' retention time needs. Based on our findings, we propose an asymmetric-retention core (ARC) design for multicore architectures. ARC features retention time heterogeneity to specialize STT-RAM retention times to applications' needs. We also propose a runtime prediction model to determine the best core on which to run an application, based on the applications' characteristics, their retention time requirements, and available DVFS settings. Results reveal that the proposed approach can reduce the average cache energy by 20.19% and overall processor energy by 7.66%, compared to a homogeneous STT-RAM cache design.

**Link**: [arxiv](http://arxiv.org/abs/2407.19612v1),  [pdf](http://arxiv.org/pdf/2407.19612v1)

**Tags**: cs.CY cs.AR 



### SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning
**Authors**: Dhruv Gajaria, Kyle Kuan, Tosiron Adegbija

**Updated**: 2024-07-28T22:34:20Z

**Summary**: Prior studies have shown that the retention time of the non-volatile spin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's write energy and latency. However, since different applications may require different retention times, STT-RAM retention times must be critically explored to satisfy various applications' needs. This process can be challenging due to exploration overhead, and exacerbated by the fact that STT-RAM caches are emerging and are not readily available for design time exploration. This paper explores using known and easily obtainable statistics (e.g., SRAM statistics) to predict the appropriate STT-RAM retention times, in order to minimize exploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model, which utilizes machine learning to enable design time or runtime prediction of right-provisioned STT-RAM retention times for latency or energy optimization. Experimental results show that, on average, SCART can reduce the latency and energy by 20.34% and 29.12%, respectively, compared to a homogeneous retention time while reducing the exploration overheads by 52.58% compared to prior work.

**Link**: [arxiv](http://arxiv.org/abs/2407.19604v1),  [pdf](http://arxiv.org/pdf/2407.19604v1)

**Tags**: cs.CY cs.AR 



### Application State Management (ASM) in the Modern Web and Mobile   Applications: A Comprehensive Review
**Authors**: Anujkumarsinh Donvir, Apeksha Jain, Pradeep Kumar Saraswathi

**Updated**: 2024-07-27T18:26:32Z

**Summary**: The rapid evolution of web and mobile applications has necessitated robust mechanisms for managing application state to ensure consistency, performance, and user-friendliness. This comprehensive review examines the most effective Application State Management (ASM) techniques, categorized into Local State Management, State Management Libraries, and Server-Side State Management. By analyzing popular front end frameworks the study delves into local state management mechanisms. It also evaluates the state of front end management libraries, highlighting their implementations, benefits, and limitations. Server-side state management techniques, particularly caching, are discussed for their roles in enhancing data retrieval efficiency. This paper offers actionable insights for developers to build scalable, responsive applications, aiming to bridge the gap between theoretical knowledge and practical application. This study's critical analysis and recommendations aim to guide future research and development in ASM, contributing to the advancement of modern application architecture.

**Link**: [arxiv](http://arxiv.org/abs/2407.19318v1),  [pdf](http://arxiv.org/pdf/2407.19318v1)

**Tags**: cs.SE 



### Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for   Multi-Tenant DNN Inference
**Authors**: Yongkang Zhang, Haoxuan Yu, Chenxia Han, Cheng Wang, Baotong Lu, Yang Li, Xiaowen Chu, Huaicheng Li

**Updated**: 2024-07-27T08:52:39Z

**Summary**: Colocating high-priority, latency-sensitive (LS) and low-priority, best-effort (BE) DNN inference services reduces the total cost of ownership (TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts and PCIe bus contentions, existing GPU sharing solutions are unable to avoid resource conflicts among concurrently executing tasks, failing to achieve both low latency for LS tasks and high throughput for BE tasks. To bridge this gap, this paper presents Missile, a general GPU sharing solution for multi-tenant DNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware resource isolation between multiple LS and BE DNN tasks at software level. Through comprehensive reverse engineering, Missile first reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel conflicts using software-level cache coloring. It also isolates the PCIe bus and fairly allocates PCIe bandwidth using completely fair scheduler. We evaluate 12 mainstream DNNs with synthetic and real-world workloads on four GPUs. The results show that compared to the state-of-the-art GPU sharing solutions, Missile reduces tail latency for LS services by up to ~50%, achieves up to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants on-demand for optimal performance.

**Link**: [arxiv](http://arxiv.org/abs/2407.13996v2),  [pdf](http://arxiv.org/pdf/2407.13996v2)

**Tags**: cs.DC cs.AR cs.PF D.4.9; I.2.5 



### Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's   Impact on Spatio-Temporal Cross-Attentions
**Authors**: Ashkan Taghipour, Morteza Ghahremani, Mohammed Bennamoun, Aref Miri Rekavandi, Zinuo Li, Hamid Laga, Farid Boussaid

**Updated**: 2024-07-27T08:21:14Z

**Summary**: This paper investigates the role of CLIP image embeddings within the Stable Video Diffusion (SVD) framework, focusing on their impact on video generation quality and computational efficiency. Our findings indicate that CLIP embeddings, while crucial for aesthetic quality, do not significantly contribute towards the subject and background consistency of video outputs. Moreover, the computationally expensive cross-attention mechanism can be effectively replaced by a simpler linear layer. This layer is computed only once at the first diffusion inference step, and its output is then cached and reused throughout the inference process, thereby enhancing efficiency while maintaining high-quality outputs. Building on these insights, we introduce the VCUT, a training-free approach optimized for efficiency within the SVD architecture. VCUT eliminates temporal cross-attention and replaces spatial cross-attention with a one-time computed linear layer, significantly reducing computational load. The implementation of VCUT leads to a reduction of up to 322T Multiple-Accumulate Operations (MACs) per video and a decrease in model parameters by up to 50M, achieving a 20% reduction in latency compared to the baseline. Our approach demonstrates that conditioning during the Semantic Binding stage is sufficient, eliminating the need for continuous computation across all inference steps and setting a new standard for efficient video generation.

**Link**: [arxiv](http://arxiv.org/abs/2407.19205v1),  [pdf](http://arxiv.org/pdf/2407.19205v1)

**Tags**: cs.CV cs.AI 



### MetaHive: A Cache-Optimized Metadata Management for Heterogeneous   Key-Value Stores
**Authors**: Alireza Heidari, Amirhossein Ahmadi, Zefeng Zhi, Wei Zhang

**Updated**: 2024-07-26T21:11:58Z

**Summary**: Cloud key-value (KV) stores provide businesses with a cost-effective and adaptive alternative to traditional on-premise data management solutions. KV stores frequently consist of heterogeneous clusters, characterized by varying hardware specifications of the deployment nodes, with each node potentially running a distinct version of the KV store software. This heterogeneity is accompanied by the diverse metadata that they need to manage. In this study, we introduce MetaHive, a cache-optimized approach to managing metadata in heterogeneous KV store clusters. MetaHive disaggregates the original data from its associated metadata to promote independence between them, while maintaining their interconnection during usage. This makes the metadata opaque from the downstream processes and the other KV stores in the cluster. MetaHive also ensures that the KV and metadata entries are stored in the vicinity of each other in memory and storage. This allows MetaHive to optimally utilize the caching mechanism without extra storage read overhead for metadata retrieval. We deploy MetaHive to ensure data integrity in RocksDB and demonstrate its rapid data validation with minimal effect on performance.

**Link**: [arxiv](http://arxiv.org/abs/2407.19090v1),  [pdf](http://arxiv.org/pdf/2407.19090v1)

**Tags**: cs.DB cs.IR 



### Efficient Inference of Vision Instruction-Following Models with Elastic   Cache
**Authors**: Zuyan Liu, Benlin Liu, Jiahui Wang, Yuhao Dong, Guangyi Chen, Yongming Rao, Ranjay Krishna, Jiwen Lu

**Updated**: 2024-07-25T15:29:05Z

**Summary**: In the field of instruction-following large vision-language models (LVLMs), the efficient deployment of these models faces challenges, notably due to the high memory demands of their key-value (KV) caches. Conventional cache management strategies for LLMs focus on cache eviction, which often fails to address the specific needs of multimodal instruction-following models. Recognizing this gap, in this paper, we introduce Elastic Cache, a novel approach that benefits from applying distinct acceleration methods for instruction encoding and output generation stages. We investigate the metrics of importance in different stages and propose an importance-driven cache merging strategy to prune redundancy caches. Instead of discarding less important caches, our strategy identifies important key/value vectors as anchor points. Surrounding less important caches are then merged with these anchors, enhancing the preservation of contextual information in the KV caches while yielding an arbitrary acceleration ratio. For instruction encoding, we utilize the frequency to evaluate the importance of caches. Regarding output generation, we prioritize tokens based on their distance with an offset, by which both the initial and most recent tokens are retained. Results on a range of LVLMs demonstrate that Elastic Cache not only boosts efficiency but also notably outperforms existing pruning methods in language generation across various tasks. Code is available at https://github.com/liuzuyan/ElasticCache

**Link**: [arxiv](http://arxiv.org/abs/2407.18121v1),  [pdf](http://arxiv.org/pdf/2407.18121v1)

**Tags**: cs.CV 



### KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache
**Authors**: Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, Xia Hu

**Updated**: 2024-07-25T09:16:05Z

**Summary**: Efficiently serving large language models (LLMs) requires batching of many requests to reduce the cost per request. Yet, with larger batch sizes and longer context lengths, the key-value (KV) cache, which stores attention keys and values to avoid re-computations, significantly increases memory demands and becomes the new bottleneck in speed and memory usage. Additionally, the loading of the KV cache causes the computational core to be idle, which limits the inference speed. A straightforward and effective solution to reduce KV cache size is quantization, which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element distribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we conducted a comprehensive study on the element distribution in KV cache of popular LLMs. Our findings indicate that the key cache should be quantized per-channel, i.e., group elements along the channel dimension and quantize them together. In contrast, the value cache should be quantized per-token. From this analysis, we developed a tuning-free 2bit KV cache quantization algorithm named KIVI. With hardware-friendly implementation, KIVI can enable Llama, Falcon, and Mistral models to maintain almost the same quality while using $\mathbf{2.6\times}$ less peak memory (including model weight). This reduction in memory usage enables up to $\mathbf{4\times}$ larger batch size, bringing $\mathbf{2.35\times \sim 3.47\times}$ throughput on real LLM inference workload. The source code is available at https://github.com/jy-yuan/KIVI.

**Link**: [arxiv](http://arxiv.org/abs/2402.02750v2),  [pdf](http://arxiv.org/pdf/2402.02750v2)

**Tags**: cs.CL cs.LG cs.PF 



### An Efficient Inference Framework for Early-exit Large Language Models
**Authors**: Ruijie Miao, Yihan Yan, Xinshuo Yao, Tong Yang

**Updated**: 2024-07-25T07:50:17Z

**Summary**: Building efficient inference framework has gained increasing interests for research community. Early-exit models, a variant of LLMs, improves the inference efficiency of LLMs by skipping rest layers and directly generate output tokens when they are confident enough. However, there is no work of LLM inference framework that takes early-exit models into consideration. This is non-trivial as prior art on LLM inference cannot be directly applied to early-exit models. In this work, we solves two key challenges in building efficient inference framework for early-exit models: (1) batch inference at iteration-level granularity; and (2) KV cache management. For the former, we propose to process the batch until all sequences surpass the early-exit confidence threshold. For the latter, we propose to fill the KV cache of rest layers before the iteration terminates. Our evaluation shows that, compared with the original vLLM operating at full layers, our solution achieves up to 1.25x speed up.

**Link**: [arxiv](http://arxiv.org/abs/2407.20272v1),  [pdf](http://arxiv.org/pdf/2407.20272v1)

**Tags**: cs.CL cs.AI cs.LG 



### Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval   from Distributed System with Blind and Adversarial Servers
**Authors**: Qifa Yan, Xiaohu Tang, Zhengchun Zhou

**Updated**: 2024-07-24T13:36:03Z

**Summary**: In this work, a distributed server system composed of multiple servers that holds some coded files and multiple users that are interested in retrieving the linear functions of the files is investigated, where the servers are robust, blind and adversarial in the sense that any $J$ servers can together recover all files, while any $I$ colluding servers cannot obtain any information about the files, and at most $A$ servers maliciously provides erroneous information. In addition, the file library must be secure from a wiretapper who obtains all the signals, and the demands of any subset of users must kept private from the other users and servers, even if they collude. A coding scheme is proposed by incorporating the ideas of Shamir's secret sharing and key superposition into the framework of Placement Delivery Array (PDA), originally proposed to characterize the single-server coded caching system without any security or privacy constraints. It is shown that PDAs associated to Maddah-Ali and Niesen's coded caching scheme results in an achievable memory-storage-communication region, such that the storage size and communication load were optimal to within a multiplicative gap, except for the small memory regime when the number of files was smaller than the number of users.

**Link**: [arxiv](http://arxiv.org/abs/2301.08711v3),  [pdf](http://arxiv.org/pdf/2301.08711v3)

**Tags**: cs.IT math.IT 



### Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic   Violations
**Authors**: Craig Innes, Subramanian Ramamoorthy

**Updated**: 2024-07-24T12:56:41Z

**Summary**: Autonomous Vehicles (AVs) are often tested in simulation to estimate the probability they will violate safety specifications. Two common issues arise when using existing techniques to produce this estimation: If violations occur rarely, simple Monte-Carlo sampling techniques can fail to produce efficient estimates; if simulation horizons are too long, importance sampling techniques (which learn proposal distributions from past simulations) can fail to converge. This paper addresses both issues by interleaving rare-event sampling techniques with online specification monitoring algorithms. We use adaptive multi-level splitting to decompose simulations into partial trajectories, then calculate the distance of those partial trajectories to failure by leveraging robustness metrics from Signal Temporal Logic (STL). By caching those partial robustness metric values, we can efficiently re-use computations across multiple sampling stages. Our experiments on an interstate lane-change scenario show our method is viable for testing simulated AV-pipelines, efficiently estimating failure probabilities for STL specifications based on real traffic rules. We produce better estimates than Monte-Carlo and importance sampling in fewer simulations.

**Link**: [arxiv](http://arxiv.org/abs/2405.15771v2),  [pdf](http://arxiv.org/pdf/2405.15771v2)

**Tags**: cs.RO cs.AI cs.LG 



### Efficient Tuning and Inference for Large Language Models on Textual   Graphs
**Authors**: Yun Zhu, Yaoke Wang, Haizhou Shi, Siliang Tang

**Updated**: 2024-07-24T08:56:11Z

**Summary**: Rich textual and topological information of textual graphs need to be modeled in real-world applications such as webpages, e-commerce, and academic articles. Practitioners have been long following the path of adopting a shallow text encoder and a subsequent graph neural network (GNN) to solve this problem. In light of recent advancements in large language models (LLMs), it is apparent that integrating LLMs for enhanced textual encoding can substantially improve the performance of textual graphs. Nevertheless, the efficiency of these methods poses a significant challenge. In this paper, we propose ENGINE, a parameter- and memory-efficient fine-tuning method for textual graphs with an LLM encoder. The key insight is to combine the LLMs and GNNs through a tunable side structure, which significantly reduces the training complexity without impairing the joint model's capacity. Extensive experiments on textual graphs demonstrate our method's effectiveness by achieving the best model performance, meanwhile having the lowest training cost compared to previous methods. Moreover, we introduce two variants with caching and dynamic early exit to further enhance training and inference speed. Specifically, caching accelerates ENGINE's training by 12x, and dynamic early exit achieves up to 5x faster inference with a negligible performance drop (at maximum 1.17% relevant drop across 7 datasets). Our codes are available at: https://github.com/ZhuYun97/ENGINE

**Link**: [arxiv](http://arxiv.org/abs/2401.15569v2),  [pdf](http://arxiv.org/pdf/2401.15569v2)

**Tags**: cs.CL 



### Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference
**Authors**: Piotr Nawrot, Adrian ≈Åa≈Ñcucki, Marcin Chochowski, David Tarjan, Edoardo M. Ponti

**Updated**: 2024-07-23T17:55:30Z

**Summary**: Transformers have emerged as the backbone of large language models (LLMs). However, generation remains inefficient due to the need to store in memory a cache of key-value representations for past tokens, whose size scales linearly with the input sequence length and batch size. As a solution, we propose Dynamic Memory Compression (DMC), a method for online key-value cache compression at inference time. Most importantly, the model learns to apply different compression ratios in different heads and layers. We retrofit pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers, achieving up to 7x throughput increase during auto-regressive inference on an NVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible percentage of the original data without adding any extra parameters. DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query attention (GQA) and key-value eviction policies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded gains. Hence, DMC can serve as a drop-in replacement for KV caching in existing LLMs to fit longer contexts and larger batches within any given memory budget.

**Link**: [arxiv](http://arxiv.org/abs/2403.09636v2),  [pdf](http://arxiv.org/pdf/2403.09636v2)

**Tags**: cs.CL 



### 6G at $\frac{1}{6}g$: The Future of Cislunar Communications
**Authors**: Sahan Liyanaarachchi, Stavros Mitrolaris, Purbesh Mitra, Sennur Ulukus

**Updated**: 2024-07-23T17:42:57Z

**Summary**: What will the future of cislunar communications be? The ever-expanding horizons of the space exploration missions, and the need for establishing sustainable space communication and navigation infrastructure necessitate to think this question thoroughly. In this article, we examine how some of the concepts of 6G technologies developed for terrestrial networks can be relevant in the context of cislunar networks. We discuss how 6G concepts, such as reconfigurable intelligent surfaces, quantum-resistant physical layer security, private information read/write/cache networks, semantic and goal-oriented communications, information freshness based quality of communication metrics, multi-relay and cooperative networks, hold the potential to shape the future of cislunar communications.

**Link**: [arxiv](http://arxiv.org/abs/2407.16672v1),  [pdf](http://arxiv.org/pdf/2407.16672v1)

**Tags**: cs.IT cs.ET cs.NI math.IT 



### Hidden Web Caches Discovery
**Authors**: Matteo Golinelli, Bruno Crispo

**Updated**: 2024-07-23T08:58:06Z

**Summary**: Web caches play a crucial role in web performance and scalability. However, detecting cached responses is challenging when web servers do not reliably communicate the cache status through standardized headers. This paper presents a novel methodology for cache detection using timing analysis. Our approach eliminates the dependency on cache status headers, making it applicable to any web server. The methodology relies on sending paired requests using HTTP multiplexing functionality and makes heavy use of cache-busting to control the origin of the responses. By measuring the time it takes to receive responses from paired requests, we can determine if a response is cached or not. In each pair, one request is cache-busted to force retrieval from the origin server, while the other request is not and might be served from the cache, if present. A faster response time for the non-cache-busted request compared to the cache-busted one suggests the first one is coming from the cache. We implemented this approach in a tool and achieved an estimated accuracy of 89.6% compared to state-of-the-art methods based on cache status headers. Leveraging our cache detection approach, we conducted a large-scale experiment on the Tranco Top 50k websites. We identified a significant presence of hidden caches (5.8%) that do not advertise themselves through headers. Additionally, we employed our methodology to detect Web Cache Deception (WCD) vulnerabilities in these hidden caches. We discovered that 1.020 of them are susceptible to WCD vulnerabilities, potentially leaking sensitive data. Our findings demonstrate the effectiveness of our timing analysis methodology for cache discovery and highlight the importance of a tool that does not rely on cache-communicated cache status headers.

**Link**: [arxiv](http://arxiv.org/abs/2407.16303v1),  [pdf](http://arxiv.org/pdf/2407.16303v1)

**Tags**: cs.CR 



### A Programming Model for Disaggregated Memory over CXL
**Authors**: Gal Assa, Michal Friedman, Ori Lahav

**Updated**: 2024-07-23T08:55:10Z

**Summary**: CXL (Compute Express Link) is an emerging open industry-standard interconnect between processing and memory devices that is expected to revolutionize the way systems are designed in the near future. It enables cache-coherent shared memory pools in a disaggregated fashion at unprecedented scales, allowing algorithms to interact with a variety of storage devices using simple loads and stores in a cacheline granularity. Alongside with unleashing unique opportunities for a wide range of applications, CXL introduces new challenges of data management and crash consistency. Alas, CXL lacks an adequate programming model, which makes reasoning about the correctness and expected behaviors of algorithms and systems on top of it nearly impossible.   In this work, we present CXL0, the first programming model for concurrent programs running on top of CXL. We propose a high-level abstraction for CXL memory accesses and formally define operational semantics on top of that abstraction. We provide a set of general transformations that adapt concurrent algorithms to the new disruptive technology. Using these transformations, every linearizable algorithm can be easily transformed into its provably correct version in the face of a full-system or sub-system crash. We believe that this work will serve as the stepping stone for systems design and modelling on top of CXL, and support the development of future models as software and hardware evolve.

**Link**: [arxiv](http://arxiv.org/abs/2407.16300v1),  [pdf](http://arxiv.org/pdf/2407.16300v1)

**Tags**: cs.DC cs.ET 



### A deeper look at depth pruning of LLMs
**Authors**: Shoaib Ahmed Siddiqui, Xin Dong, Greg Heinrich, Thomas Breuel, Jan Kautz, David Krueger, Pavlo Molchanov

**Updated**: 2024-07-23T08:40:27Z

**Summary**: Large Language Models (LLMs) are not only resource-intensive to train but even more costly to deploy in production. Therefore, recent work has attempted to prune blocks of LLMs based on cheap proxies for estimating block importance, effectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b models without any significant degradation of downstream metrics. In this paper, we explore different block importance metrics by considering adaptive metrics such as Shapley value in addition to static ones explored in prior work. We show that adaptive metrics exhibit a trade-off in performance between tasks i.e., improvement on one task may degrade performance on the other due to differences in the computed block influences. Furthermore, we extend this analysis from a complete block to individual self-attention and feed-forward layers, highlighting the propensity of the self-attention layers to be more amendable to pruning, even allowing removal of upto 33% of the self-attention layers without incurring any performance degradation on MMLU for Mistral 7b (significant reduction in costly maintenance of KV-cache). Finally, we look at simple performance recovery techniques to emulate the pruned layers by training lightweight additive bias or low-rank linear adapters. Performance recovery using emulated updates avoids performance degradation for the initial blocks (up to 5% absolute improvement on MMLU), which is either competitive or superior to the learning-based technique.

**Link**: [arxiv](http://arxiv.org/abs/2407.16286v1),  [pdf](http://arxiv.org/pdf/2407.16286v1)

**Tags**: cs.LG cs.AI 



### vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving
**Authors**: Jiale Xu, Rui Zhang, Cong Guo, Weiming Hu, Zihan Liu, Feiyang Wu, Yu Feng, Shixuan Sun, Changxu Shao, Yuhong Guo, Junping Zhao, Ke Zhang, Minyi Guo, Jingwen Leng

**Updated**: 2024-07-22T14:37:58Z

**Summary**: Large Language Models (LLMs) are widely used across various domains, processing millions of daily requests. This surge in demand poses significant challenges in optimizing throughput and latency while keeping costs manageable. The Key-Value (KV) cache, a standard method for retaining previous computations, makes LLM inference highly bounded by memory. While batching strategies can enhance performance, they frequently lead to significant memory fragmentation. Even though cutting-edge systems like vLLM mitigate KV cache fragmentation using paged Attention mechanisms, they still suffer from inefficient memory and computational operations due to the tightly coupled page management and computation kernels.   This study introduces the vTensor, an innovative tensor structure for LLM inference based on GPU virtual memory management (VMM). vTensor addresses existing limitations by decoupling computation from memory defragmentation and offering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous approach, ensuring efficient, fragmentation-free memory management while accommodating various computation kernels across different LLM architectures. Experimental results indicate that vTensor achieves an average speedup of 1.86x across different models, with up to 2.42x in multi-turn chat scenarios. Additionally, vTensor provides average speedups of 2.12x and 3.15x in kernel evaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton prefix-prefilling kernels and vLLM paged Attention kernel, respectively. Furthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100 GPU compared to vLLM, enabling more memory-intensive workloads.

**Link**: [arxiv](http://arxiv.org/abs/2407.15309v1),  [pdf](http://arxiv.org/pdf/2407.15309v1)

**Tags**: cs.DC cs.LG 



### vLSM: Low tail latency and I/O amplification in LSM-based KV stores
**Authors**: Giorgos Xanthakis, Antonios Katsarakis, Giorgos Saloustros, Angelos Bilas

**Updated**: 2024-07-22T12:17:01Z

**Summary**: LSM-based key-value (KV) stores are an important component in modern data infrastructures. However, they suffer from high tail latency, in the order of several seconds, making them less attractive for user-facing applications. In this paper, we introduce the notion of compaction chains and we analyse how they affect tail latency. Then, we show that modern designs reduce tail latency, by trading I/O amplification or require large amounts of memory. Based on our analysis, we present vLSM, a new KV store design that improves tail latency significantly without compromising on memory or I/O amplification. vLSM reduces (a) compaction chain width by using small SSTs and eliminating the tiering compaction required in L0 by modern systems and (b) compaction chain length by using a larger than typical growth factor between L1 and L2 and introducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate it using db_bench and YCSB. Our evaluation highlights the underlying trade-off among memory requirements, I/O amplification, and tail latency, as well as the advantage of vLSM over current approaches. vLSM improves P99 tail latency by up to 4.8x for writes and by up to 12.5x for reads, reduces cumulative write stalls by up to 60% while also slightly improves I/O amplification at the same memory budget.

**Link**: [arxiv](http://arxiv.org/abs/2407.15581v1),  [pdf](http://arxiv.org/pdf/2407.15581v1)

**Tags**: cs.DB 



## Keyword: LLM Inference 
 ### Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of   Encoders
**Authors**: Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, Bryan Catanzaro, Andrew Tao, Jan Kautz, Zhiding Yu, Guilin Liu

**Updated**: 2024-08-28T17:59:31Z

**Summary**: The ability to accurately interpret complex visual information is a crucial topic of multimodal large language models (MLLMs). Recent work indicates that enhanced visual perception significantly reduces hallucinations and improves performance on resolution-sensitive tasks, such as optical character recognition and document analysis. A number of recent MLLMs achieve this goal using a mixture of vision encoders. Despite their success, there is a lack of systematic comparisons and detailed ablation studies addressing critical aspects, such as expert selection and the integration of multiple vision experts. This study provides an extensive exploration of the design space for MLLMs using a mixture of vision encoders and resolutions. Our findings reveal several underlying principles common to various existing strategies, leading to a streamlined yet effective design approach. We discover that simply concatenating visual tokens from a set of complementary vision encoders is as effective as more complex mixing architectures or strategies. We additionally introduce Pre-Alignment to bridge the gap between vision-focused encoders and language tokens, enhancing model coherence. The resulting family of MLLMs, Eagle, surpasses other leading open-source models on major MLLM benchmarks. Models and code: https://github.com/NVlabs/Eagle

**Link**: [arxiv](http://arxiv.org/abs/2408.15998v1),  [pdf](http://arxiv.org/pdf/2408.15998v1)

**Tags**: cs.CV cs.AI cs.LG cs.RO 



### CoGen: Learning from Feedback with Coupled Comprehension and Generation
**Authors**: Mustafa Omer Gul, Yoav Artzi

**Updated**: 2024-08-28T17:58:39Z

**Summary**: Systems with both language comprehension and generation capabilities can benefit from the tight connection between the two. This work studies coupling comprehension and generation with focus on continually learning from interaction with users. We propose techniques to tightly integrate the two capabilities for both learning and inference. We situate our studies in two-player reference games, and deploy various models for thousands of interactions with human users, while learning from interaction feedback signals. We show dramatic improvements in performance over time, with comprehension-generation coupling leading to performance improvements up to 26% in absolute terms and up to 17% higher accuracies compared to a non-coupled system. Our analysis also shows coupling has substantial qualitative impact on the system's language, making it significantly more human-like.

**Link**: [arxiv](http://arxiv.org/abs/2408.15992v1),  [pdf](http://arxiv.org/pdf/2408.15992v1)

**Tags**: cs.CL cs.AI cs.CV cs.LG 



### Thoughtseeds: Evolutionary Priors, Nested Markov Blankets, and the   Emergence of Embodied Cognition
**Authors**: Prakash Chandra Kavi, Gorka Zamora Lopez, Daniel Ari Friedman

**Updated**: 2024-08-28T17:51:16Z

**Summary**: The emergence of cognition requires a framework that bridges evolutionary principles with neurocomputational mechanisms. This paper introduces the "thoughtseed" framework, proposing that cognition arises from the dynamic interaction of self-organizing units of embodied knowledge called "thoughtseeds." We leverage evolutionary theory, "neuronal packets," and the "Inner Screen" hypothesis within Free Energy Principle, and propose a four-level hierarchical model of the cognitive agent's internal states: Neuronal Packet Domains (NPDs), Knowledge Domains (KDs), thoughtseeds network, and meta-cognition. The dynamic interplay within this hierarchy, mediated by nested Markov blankets and reciprocal message passing, facilitates the emergence of thoughtseeds as coherent patterns of activity that guide perception, action, and learning. The framework further explores the role of the organism's Umwelt and the principles of active inference, especially the generative model at each nested level, in shaping the selection and activation of thoughtseeds, leading to adaptive behavior through surprise minimization. The "Inner Screen" is posited as the locus of conscious experience, where the content of the dominant thoughtseed is projected, maintaining a unitary conscious experience. Active thoughtseeds are proposed as the fundamental units of thought that contribute to the "content of consciousness." We present a mathematical framework grounded in active inference and dynamical systems theory. The thoughtseed framework represents an initial but promising step towards a novel, biologically-grounded model for understanding the organizing principles and emergence of embodied cognition, offering a unified account of cognitive phenomena, from basic physiological regulation to higher-order thought processes, and potentially bridge neuroscience and contemplative traditions.

**Link**: [arxiv](http://arxiv.org/abs/2408.15982v1),  [pdf](http://arxiv.org/pdf/2408.15982v1)

**Tags**: q-bio.NC 



### WebPilot: A Versatile and Autonomous Multi-Agent System for Web Task   Execution with Strategic Exploration
**Authors**: Yao Zhang, Zijian Ma, Yunpu Ma, Zhen Han, Yu Wu, Volker Tresp

**Updated**: 2024-08-28T17:49:29Z

**Summary**: LLM-based autonomous agents often fail to execute complex web tasks that require dynamic interaction due to the inherent uncertainty and complexity of these environments. Existing LLM-based web agents typically rely on rigid, expert-designed policies specific to certain states and actions, which lack the flexibility and generalizability needed to adapt to unseen tasks. In contrast, humans excel by exploring unknowns, continuously adapting strategies, and resolving ambiguities through exploration. To emulate human-like adaptability, web agents need strategic exploration and complex decision-making. Monte Carlo Tree Search (MCTS) is well-suited for this, but classical MCTS struggles with vast action spaces, unpredictable state transitions, and incomplete information in web tasks. In light of this, we develop WebPilot, a multi-agent system with a dual optimization strategy that improves MCTS to better handle complex web environments. Specifically, the Global Optimization phase involves generating a high-level plan by breaking down tasks into manageable subtasks and continuously refining this plan, thereby focusing the search process and mitigating the challenges posed by vast action spaces in classical MCTS. Subsequently, the Local Optimization phase executes each subtask using a tailored MCTS designed for complex environments, effectively addressing uncertainties and managing incomplete information. Experimental results on WebArena and MiniWoB++ demonstrate the effectiveness of WebPilot. Notably, on WebArena, WebPilot achieves SOTA performance with GPT-4, achieving a 93% relative increase in success rate over the concurrent tree search-based method. WebPilot marks a significant advancement in general autonomous agent capabilities, paving the way for more advanced and reliable decision-making in practical environments.

**Link**: [arxiv](http://arxiv.org/abs/2408.15978v1),  [pdf](http://arxiv.org/pdf/2408.15978v1)

**Tags**: cs.AI 



### BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition   Capabilities of Language Models in Multi-Agent Systems
**Authors**: Wei Wang, Dan Zhang, Tao Feng, Boyan Wang, Jie Tang

**Updated**: 2024-08-28T17:43:55Z

**Summary**: Large Language Models (LLMs) are becoming increasingly powerful and capable of handling complex tasks, e.g., building single agents and multi-agent systems. Compared to single agents, multi-agent systems have higher requirements for the collaboration capabilities of language models. Many benchmarks are proposed to evaluate their collaborative abilities. However, these benchmarks lack fine-grained evaluations of LLM collaborative capabilities. Additionally, multi-agent collaborative and competitive scenarios are ignored in existing works. To address these two problems, we propose a benchmark, called BattleAgentBench, which defines seven sub-stages of three varying difficulty levels and conducts a fine-grained evaluation of language models in terms of single-agent scenario navigation capabilities, paired-agent task execution abilities, and multi-agent collaboration and competition capabilities. We conducted extensive evaluations on leading four closed-source and seven open-source models. Experimental results indicate that API-based models perform excellently on simple tasks but open-source small models struggle with simple tasks. Regarding difficult tasks that require collaborative and competitive abilities, although API-based models have demonstrated some collaborative capabilities, there is still enormous room for improvement.

**Link**: [arxiv](http://arxiv.org/abs/2408.15971v1),  [pdf](http://arxiv.org/pdf/2408.15971v1)

**Tags**: cs.CL 



### More Text, Less Point: Towards 3D Data-Efficient Point-Language   Understanding
**Authors**: Yuan Tang, Xu Han, Xianzhi Li, Qiao Yu, Jinfeng Xu, Yixue Hao, Long Hu, Min Chen

**Updated**: 2024-08-28T17:38:44Z

**Summary**: Enabling Large Language Models (LLMs) to comprehend the 3D physical world remains a significant challenge. Due to the lack of large-scale 3D-text pair datasets, the success of LLMs has yet to be replicated in 3D understanding. In this paper, we rethink this issue and propose a new task: 3D Data-Efficient Point-Language Understanding. The goal is to enable LLMs to achieve robust 3D object understanding with minimal 3D point cloud and text data pairs. To address this task, we introduce GreenPLM, which leverages more text data to compensate for the lack of 3D data. First, inspired by using CLIP to align images and text, we utilize a pre-trained point cloud-text encoder to map the 3D point cloud space to the text space. This mapping leaves us to seamlessly connect the text space with LLMs. Once the point-text-LLM connection is established, we further enhance text-LLM alignment by expanding the intermediate text space, thereby reducing the reliance on 3D point cloud data. Specifically, we generate 6M free-text descriptions of 3D objects, and design a three-stage training strategy to help LLMs better explore the intrinsic connections between different modalities. To achieve efficient modality alignment, we design a zero-parameter cross-attention module for token pooling. Extensive experimental results show that GreenPLM requires only 12% of the 3D training data used by existing state-of-the-art models to achieve superior 3D understanding. Remarkably, GreenPLM also achieves competitive performance using text-only data. The code and weights are available at: https://github.com/TangYuan96/GreenPLM.

**Link**: [arxiv](http://arxiv.org/abs/2408.15966v1),  [pdf](http://arxiv.org/pdf/2408.15966v1)

**Tags**: cs.CV cs.AI cs.CL 



### On harmonic oscillator hazard functions
**Authors**: J. A. Christen, F. J. Rubio

**Updated**: 2024-08-28T17:33:41Z

**Summary**: We propose a parametric hazard model obtained by enforcing positivity in the damped harmonic oscillator. The resulting model has closed-form hazard and cumulative hazard functions, facilitating likelihood and Bayesian inference on the parameters. We show that this model can capture a range of hazard shapes, such as increasing, decreasing, unimodal, bathtub, and oscillatory patterns, and characterize the tails of the corresponding survival function. We illustrate the use of this model in survival analysis using real data.

**Link**: [arxiv](http://arxiv.org/abs/2408.15964v1),  [pdf](http://arxiv.org/pdf/2408.15964v1)

**Tags**: stat.ME 



### Flextron: Many-in-One Flexible Large Language Model
**Authors**: Ruisi Cai, Saurav Muralidharan, Greg Heinrich, Hongxu Yin, Zhangyang Wang, Jan Kautz, Pavlo Molchanov

**Updated**: 2024-08-28T17:26:03Z

**Summary**: Training modern LLMs is extremely resource intensive, and customizing them for various deployment scenarios characterized by limited compute and memory resources through repeated training is impractical. In this paper, we introduce Flextron, a network architecture and post-training model optimization framework supporting flexible model deployment. The Flextron architecture utilizes a nested elastic structure to rapidly adapt to specific user-defined latency and accuracy targets during inference with no additional fine-tuning required. It is also input-adaptive, and can automatically route tokens through its sub-networks for improved performance and efficiency. We present a sample-efficient training method and associated routing algorithms for systematically transforming an existing trained LLM into a Flextron model. We evaluate Flextron on the GPT-3 and LLama-2 family of LLMs, and demonstrate superior performance over multiple end-to-end trained variants and other state-of-the-art elastic networks, all with a single pretraining run that consumes a mere 7.63% tokens compared to original pretraining.

**Link**: [arxiv](http://arxiv.org/abs/2406.10260v2),  [pdf](http://arxiv.org/pdf/2406.10260v2)

**Tags**: cs.CL cs.LG 



### Halfway Escape Optimization: A Quantum-Inspired Solution for General   Optimization Problems
**Authors**: Jiawen Li, Anwar PP Abdul Majeed, Pascal Lefevre

**Updated**: 2024-08-28T17:19:27Z

**Summary**: This paper first proposes the Halfway Escape Optimization (HEO) algorithm, a quantum-inspired metaheuristic designed to address general optimization problems characterized by rugged landscapes and high-dimensionality with an efficient convergence rate. The study presents a comprehensive comparative evaluation of HEO's performance against established optimization algorithms, including Particle Swarm Optimization (PSO), Genetic Algorithm (GA), Artificial Fish Swarm Algorithm (AFSA), Grey Wolf Optimizer (GWO), and Quantum behaved Particle Swarm Optimization (QPSO). The primary analysis encompasses 14 benchmark functions with dimension 30, demonstrating HEO's effectiveness and adaptability in navigating general optimization problems and providing valuable insights into its performance. The test of HEO in Pressure Vessel Design and Tubular Column Design infers its feasibility and potential in real-time applications. Further validation in Osmancik-97 and Cammeo Rice Classification proves the effectiveness of HEO and achieves a higher accuracy record.

**Link**: [arxiv](http://arxiv.org/abs/2405.02850v5),  [pdf](http://arxiv.org/pdf/2405.02850v5)

**Tags**: cs.NE cs.AI math.OC 



### InstanSeg: an embedding-based instance segmentation algorithm optimized   for accurate, efficient and portable cell segmentation
**Authors**: Thibaut Goldsborough, Ben Philps, Alan O'Callaghan, Fiona Inglis, Leo Leplat, Andrew Filby, Hakan Bilen, Peter Bankhead

**Updated**: 2024-08-28T17:14:21Z

**Summary**: Cell and nucleus segmentation are fundamental tasks for quantitative bioimage analysis. Despite progress in recent years, biologists and other domain experts still require novel algorithms to handle increasingly large and complex real-world datasets. These algorithms must not only achieve state-of-the-art accuracy, but also be optimized for efficiency, portability and user-friendliness. Here, we introduce InstanSeg: a novel embedding-based instance segmentation pipeline designed to identify cells and nuclei in microscopy images. Using six public cell segmentation datasets, we demonstrate that InstanSeg can significantly improve accuracy when compared to the most widely used alternative methods, while reducing the processing time by at least 60%. Furthermore, InstanSeg is designed to be fully serializable as TorchScript and supports GPU acceleration on a range of hardware. We provide an open-source implementation of InstanSeg in Python, in addition to a user-friendly, interactive QuPath extension for inference written in Java. Our code and pre-trained models are available at https://github.com/instanseg/instanseg .

**Link**: [arxiv](http://arxiv.org/abs/2408.15954v1),  [pdf](http://arxiv.org/pdf/2408.15954v1)

**Tags**: cs.CV 



### Atari-GPT: Investigating the Capabilities of Multimodal Large Language   Models as Low-Level Policies for Atari Games
**Authors**: Nicholas R. Waytowich, Devin White, MD Sunbeam, Vinicius G. Goecks

**Updated**: 2024-08-28T17:08:56Z

**Summary**: Recent advancements in large language models (LLMs) have expanded their capabilities beyond traditional text-based tasks to multimodal domains, integrating visual, auditory, and textual data. While multimodal LLMs have been extensively explored for high-level planning in domains like robotics and games, their potential as low-level controllers remains largely untapped. This paper explores the application of multimodal LLMs as low-level controllers in the domain of Atari video games, introducing Atari game performance as a new benchmark for evaluating the ability of multimodal LLMs to perform low-level control tasks. Unlike traditional reinforcement learning (RL) and imitation learning (IL) methods that require extensive computational resources as well as reward function specification, these LLMs utilize pre-existing multimodal knowledge to directly engage with game environments. Our study assesses multiple multimodal LLMs performance against traditional RL agents, human players, and random agents, focusing on their ability to understand and interact with complex visual scenes and formulate strategic responses. Additionally, we examine the impact of In-Context Learning (ICL) by incorporating human-demonstrated game-play trajectories to enhance the models contextual understanding. Through this investigation, we aim to determine the extent to which multimodal LLMs can leverage their extensive training to effectively function as low-level controllers, thereby redefining potential applications in dynamic and visually complex environments. Additional results and videos are available at our project webpage: https://sites.google.com/view/atari-gpt/.

**Link**: [arxiv](http://arxiv.org/abs/2408.15950v1),  [pdf](http://arxiv.org/pdf/2408.15950v1)

**Tags**: cs.AI 



### Probing Lorentz invariance with a high-energy neutrino flare
**Authors**: Mauricio Bustamante, John Ellis, Rostislav Konoplich, Alexander S. Sakharov

**Updated**: 2024-08-28T17:08:18Z

**Summary**: Time-of-flight measurements of high-energy astrophysical neutrinos can be used to probe Lorentz invariance, a pillar of modern physics. If Lorentz-invariance violation (LIV) occurs, it could cause neutrinos to slow down, with the delay scaling linearly or quadratically with their energy. We introduce non-parametric statistical methods designed to detect LIV-induced distortions in the temporal structure of a high-energy neutrino flare as it travels to Earth from a distant astrophysical source, independently of the intrinsic timing properties of the source. Our approach, illustrated using the 2014/2015 TeV-PeV neutrino flare from the blazar TXS 0506+056 detected by IceCube, finds that the LIV energy scale must exceed 10^{14} GeV (linear) or 10^9 GeV (quadratic). Our methods provide a robust means to investigate LIV by focusing solely on a neutrino flare, without relying on electromagnetic counterparts, and account for realistic energy and directional uncertainties. For completeness, we compare our limits inferred from TXS 0506+056 to the sensitivity inferred from multi-messenger detection of tentative coincidences between neutrinos and electromagnetic emission from active galactic nuclei and tidal disruption events.

**Link**: [arxiv](http://arxiv.org/abs/2408.15949v1),  [pdf](http://arxiv.org/pdf/2408.15949v1)

**Tags**: astro-ph.HE gr-qc hep-ex hep-ph hep-th 



### Halo bias in the peak model. A first-principles non-parametric approach
**Authors**: Eduard Salvador-Sol√©, Alberto Manrique

**Updated**: 2024-08-28T16:31:34Z

**Summary**: The Press-Schechter (PS) and excursion set (ES) models of structure formation fail in reproducing the halo bias found in simulations, while the excursion set-peaks (ESP) formalism built in the peak model reproduces it only at high masses and does not address in a fully satisfactory manner peak nesting and the mass and time of ellipsoidal collapse of triaxial peaks in the Gaussian-smoothed density field. Here we apply the CUSP formalism fixing all these issues from first principles and with no free parameters to infer the Lagrangian local peak bias parameters, which adopt very simple analytic expressions similar to those found in the PS and ES models. The predicted Eulerian linear halo bias recovers the results of simulations. More specifically, we show that the only small departure observed at intermediate and low masses can be due to the spurious halo splitting and grouping caused by the Spherical Overdensity halo-finding algorithm used in simulations.

**Link**: [arxiv](http://arxiv.org/abs/2408.15918v1),  [pdf](http://arxiv.org/pdf/2408.15918v1)

**Tags**: astro-ph.CO 



### Leveraging Open Knowledge for Advancing Task Expertise in Large Language   Models
**Authors**: Yuncheng Yang, Yulei Qin, Tong Wu, Zihan Xu, Gang Li, Pengcheng Guo, Hang Shao, Yucheng Shi, Ke Li, Xing Sun, Jie Yang, Yun Gu

**Updated**: 2024-08-28T16:28:07Z

**Summary**: The cultivation of expertise for large language models (LLMs) to solve tasks of specific areas often requires special-purpose tuning with calibrated behaviors on the expected stable outputs. To avoid huge cost brought by manual preparation of instruction datasets and training resources up to hundreds of hours, the exploitation of open knowledge including a wealth of low rank adaptation (LoRA) models and instruction datasets serves as a good starting point. However, existing methods on model and data selection focus on the performance of general-purpose capabilities while neglecting the knowledge gap exposed in domain-specific deployment. In the present study, we propose to bridge such gap by introducing few human-annotated samples (i.e., K-shot) for advancing task expertise of LLMs with open knowledge. Specifically, we develop an efficient and scalable pipeline to cost-efficiently produce task experts where K-shot data intervene in selecting the most promising expert candidates and the task-relevant instructions. A mixture-of-expert (MoE) system is built to make the best use of individual-yet-complementary knowledge between multiple experts. We unveil the two keys to the success of a MoE system, 1) the abidance by K-shot, and 2) the insistence on diversity. For the former, we ensure that models that truly possess problem-solving abilities on K-shot are selected rather than those blind guessers. Besides, during data selection, instructions that share task-relevant contexts with K-shot are prioritized. For the latter, we highlight the diversity of constituting experts and that of the fine-tuning instructions throughout the model and data selection process. Extensive experimental results confirm the superiority of our approach over existing methods on utilization of open knowledge across various tasks. Codes and models will be released later.

**Link**: [arxiv](http://arxiv.org/abs/2408.15915v1),  [pdf](http://arxiv.org/pdf/2408.15915v1)

**Tags**: cs.CV cs.AI cs.CL 



### Towards Human-Level Text Coding with LLMs: The Case of Fatherhood Roles   in Public Policy Documents
**Authors**: Lorenzo Lupo, Oscar Magnusson, Dirk Hovy, Elin Naurin, Lena W√§ngnerud

**Updated**: 2024-08-28T16:26:16Z

**Summary**: Recent advances in large language models (LLMs) like GPT-3.5 and GPT-4 promise automation with better results and less programming, opening up new opportunities for text analysis in political science. In this study, we evaluate LLMs on three original coding tasks involving typical complexities encountered in political science settings: a non-English language, legal and political jargon, and complex labels based on abstract constructs. Along the paper, we propose a practical workflow to optimize the choice of the model and the prompt. We find that the best prompting strategy consists of providing the LLMs with a detailed codebook, as the one provided to human coders. In this setting, an LLM can be as good as or possibly better than a human annotator while being much faster, considerably cheaper, and much easier to scale to large amounts of text. We also provide a comparison of GPT and popular open-source LLMs, discussing the trade-offs in the model's choice. Our software allows LLMs to be easily used as annotators and is publicly available: https://github.com/lorelupo/pappa.

**Link**: [arxiv](http://arxiv.org/abs/2311.11844v3),  [pdf](http://arxiv.org/pdf/2311.11844v3)

**Tags**: cs.CL cs.CY J.4; I.2 



### Accelerating Image-based Pest Detection on a Heterogeneous Multi-core   Microcontroller
**Authors**: Luca Bompani, Luca Crupi, Daniele Palossi, Olmo Baldoni, Davide Brunelli, Francesco Conti, Manuele Rusci, Luca Benini

**Updated**: 2024-08-29T11:51:34Z

**Summary**: The codling moth pest poses a significant threat to global crop production, with potential losses of up to 80% in apple orchards. Special camera-based sensor nodes are deployed in the field to record and transmit images of trapped insects to monitor the presence of the pest. This paper investigates the embedding of computer vision algorithms in the sensor node using a novel State-of-the-Art Microcontroller Unit (MCU), the GreenWaves Technologies' GAP9 System-on-Chip, which combines 10 RISC-V general purposes cores with a convolution hardware accelerator. We compare the performance of a lightweight Viola-Jones detector algorithm with a Convolutional Neural Network (CNN), MobileNetV3-SSDLite, trained for the pest detection task. On two datasets that differentiate for the distance between the camera sensor and the pest targets, the CNN generalizes better than the other method and achieves a detection accuracy between 83% and 72%. Thanks to the GAP9's CNN accelerator, the CNN inference task takes only 147 ms to process a 320$\times$240 image. Compared to the GAP8 MCU, which only relies on general-purpose cores for processing, we achieved 9.5$\times$ faster inference speed. When running on a 1000 mAh battery at 3.7 V, the estimated lifetime is approximately 199 days, processing an image every 30 seconds. Our study demonstrates that the novel heterogeneous MCU can perform end-to-end CNN inference with an energy consumption of just 4.85 mJ, matching the efficiency of the simpler Viola-Jones algorithm and offering power consumption up to 15$\times$ lower than previous methods. Code at: https://github.com/Bomps4/TAFE_Pest_Detection

**Link**: [arxiv](http://arxiv.org/abs/2408.15911v2),  [pdf](http://arxiv.org/pdf/2408.15911v2)

**Tags**: eess.IV J.3; I.4.8 



### Infusion: internal diffusion for inpainting of dynamic textures and   complex motion
**Authors**: Nicolas Cherel, Andr√©s Almansa, Yann Gousseau, Alasdair Newson

**Updated**: 2024-08-28T16:23:19Z

**Summary**: Video inpainting is the task of filling a region in a video in a visually convincing manner. It is very challenging due to the high dimensionality of the data and the temporal consistency required for obtaining convincing results. Recently, diffusion models have shown impressive results in modeling complex data distributions, including images and videos. Such models remain nonetheless very expensive to train and to perform inference with, which strongly reduce their applicability to videos, and yields unreasonable computational loads. We show that in the case of video inpainting, thanks to the highly auto-similar nature of videos, the training data of a diffusion model can be restricted to the input video and still produce very satisfying results. This leads us to adopt an internal learning approach, which also allows us to greatly reduce the neural network size by about three orders of magnitude less than current diffusion models used for image inpainting. We also introduce a new method for efficient training and inference of diffusion models in the context of internal learning, by splitting the diffusion process into different learning intervals corresponding to different noise levels of the diffusion process. To the best of our knowledge, this is the first video inpainting method based purely on diffusion. Other methods require additional components such as optical flow estimation, which limits their performance in the case of dynamic textures and complex motions. We show qualitative and quantitative results, demonstrating that our method reaches state of the art performance in the case of dynamic textures and complex dynamic backgrounds.

**Link**: [arxiv](http://arxiv.org/abs/2311.01090v3),  [pdf](http://arxiv.org/pdf/2311.01090v3)

**Tags**: cs.CV 



### Measuring $œÉ_8$ using DESI Legacy Imaging Surveys Emission-Line   Galaxies and Planck CMB Lensing and the Impact of Dust on Parameter Inferenc
**Authors**: Tanveer Karim, Sukhdeep Singh, Mehdi Rezaie, Daniel Eisenstein, Boryana Hadzhiyska, Joshua S. Speagle, Jessica Nicole Aguilar, Steven Ahlen, David Brooks, Todd Claybaugh, Axel de la Macorra, Simone Ferraro, Jaime E. Forero-Romero, Enrique Gazta√±aga, Satya Gontcho A Gontcho, Gaston Gutierrez, Julien Guy, Klaus Honscheid, Stephanie Juneau, David Kirkby, Alex Krolewski, Andrew Lambert, Martin Landriau, Michael Levi, Aaron Meisner, Ramon Miquel, John Moustakas, Andrea Mu√±oz-Guti√©rrez, Adam Myers, Gustavo Niz, Nathalie Palanque Delabrouille, Will Percival, Francisco Prada, Graziano Rossi, Eusebio Sanchez, Edward Schlafly, David Schlegel, Michael Schubnell, David Sprayberry, Gregory Tarl√©, Benjamin Alan Weaver, Hu Zou

**Updated**: 2024-08-28T16:22:53Z

**Summary**: Measuring the growth of structure is a powerful probe for studying the dark sector, especially in light of the $\sigma_8$ tension between primary CMB anisotropy and low-redshift surveys. This paper provides a new measurement of the amplitude of the matter power spectrum, $\sigma_8$, using galaxy-galaxy and galaxy-CMB lensing power spectra of Dark Energy Spectroscopic Instrument Legacy Imaging Surveys Emission-Line Galaxies and the $\textit{Planck}$ 2018 CMB lensing map. We create an ELG catalog composed of $27$ million galaxies and with a purity of $85\%$, covering a redshift range $0 < z < 3$, with $z_{\rm mean} = 1.09$. We implement several novel systematic corrections, such as jointly modeling the contribution of imaging systematics and photometric redshift uncertainties to the covariance matrix. We also study the impacts of various dust maps on cosmological parameter inference. We measure the cross-power spectra over $f_{\rm sky} = 0.25$ with a signal-to-background ratio of up to $ 30\sigma$. We find that the choice of dust maps to account for imaging systematics in estimating the ELG overdensity field has a significant impact on the final estimated values of $\sigma_8$ and $\Omega_{\rm M}$, with far-infrared emission-based dust maps preferring $\sigma_8$ to be as low as $0.702 \pm 0.030$, and stellar-reddening-based dust maps preferring as high as $0.719 \pm 0.030$. The highest preferred value is at $\sim 3 \sigma$ tension with the $\textit{Planck}$ primary anisotropy results. These findings indicate a need for tomographic analyses at high redshifts and joint modeling of systematics.

**Link**: [arxiv](http://arxiv.org/abs/2408.15909v1),  [pdf](http://arxiv.org/pdf/2408.15909v1)

**Tags**: astro-ph.CO 



### Decentralized LLM Inference over Edge Networks with Energy Harvesting
**Authors**: Aria Khoshsirat, Giovanni Perin, Michele Rossi

**Updated**: 2024-08-28T16:20:45Z

**Summary**: Large language models have significantly transformed multiple fields with their exceptional performance in natural language tasks, but their deployment in resource-constrained environments like edge networks presents an ongoing challenge. Decentralized techniques for inference have emerged, distributing the model blocks among multiple devices to improve flexibility and cost effectiveness. However, energy limitations remain a significant concern for edge devices. We propose a sustainable model for collaborative inference on interconnected, battery-powered edge devices with energy harvesting. A semi-Markov model is developed to describe the states of the devices, considering processing parameters and average green energy arrivals. This informs the design of scheduling algorithms that aim to minimize device downtimes and maximize network throughput. Through empirical evaluations and simulated runs, we validate the effectiveness of our approach, paving the way for energy-efficient decentralized inference over edge networks.

**Link**: [arxiv](http://arxiv.org/abs/2408.15907v1),  [pdf](http://arxiv.org/pdf/2408.15907v1)

**Tags**: cs.DC 



### Bayesian Inference analysis of jet quenching using inclusive jet and   hadron suppression measurements
**Authors**: R. Ehlers, Y. Chen, J. Mulligan, Y. Ji, A. Kumar, S. Mak, P. M. Jacobs, A. Majumder, A. Angerami, R. Arora, S. A. Bass, R. Datta, L. Du, H. Elfner, R. J. Fries, C. Gale, Y. He, B. V. Jacak, S. Jeon, F. Jonas, L. Kasper, M. Kordell II, R. Kunnawalkam-Elayavalli, J. Latessa, Y. -J. Lee, R. Lemmon, M. Luzum, A. Mankolli, C. Martin, H. Mehryar, T. Mengel, C. Nattrass, J. Norman, C. Parker, J. -F. Paquet, J. H. Putschke, H. Roch, G. Roland, B. Schenke, L. Schwiebert, A. Sengupta, C. Shen, M. Singh, C. Sirimanna, D. Soeder, R. A. Soltz, I. Soudi, Y. Tachibana, J. Velkovska, G. Vujanovic, X. -N. Wang, X. Wu, W. Zhao

**Updated**: 2024-08-28T16:20:44Z

**Summary**: The JETSCAPE Collaboration reports a new determination of the jet transport parameter $\hat{q}$ in the Quark-Gluon Plasma (QGP) using Bayesian Inference, incorporating all available inclusive hadron and jet yield suppression data measured in heavy-ion collisions at RHIC and the LHC. This multi-observable analysis extends the previously published JETSCAPE Bayesian Inference determination of $\hat{q}$, which was based solely on a selection of inclusive hadron suppression data. JETSCAPE is a modular framework incorporating detailed dynamical models of QGP formation and evolution, and jet propagation and interaction in the QGP. Virtuality-dependent partonic energy loss in the QGP is modeled as a thermalized weakly-coupled plasma, with parameters determined from Bayesian calibration using soft-sector observables. This Bayesian calibration of $\hat{q}$ utilizes Active Learning, a machine--learning approach, for efficient exploitation of computing resources. The experimental data included in this analysis span a broad range in collision energy and centrality, and in transverse momentum. In order to explore the systematic dependence of the extracted parameter posterior distributions, several different calibrations are reported, based on combined jet and hadron data; on jet or hadron data separately; and on restricted kinematic or centrality ranges of the jet and hadron data. Tension is observed in comparison of these variations, providing new insights into the physics of jet transport in the QGP and its theoretical formulation.

**Link**: [arxiv](http://arxiv.org/abs/2408.08247v2),  [pdf](http://arxiv.org/pdf/2408.08247v2)

**Tags**: hep-ph nucl-ex nucl-th 



### LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration   in Evolving Environments
**Authors**: Ruirui Chen, Weifeng Jiang, Chengwei Qin, Ishaan Singh Rawal, Cheston Tan, Dongkyu Choi, Bo Xiong, Bo Ai

**Updated**: 2024-08-28T16:15:45Z

**Summary**: The rapid obsolescence of information in Large Language Models (LLMs) has driven the development of various techniques to incorporate new facts. However, existing methods for knowledge editing still face difficulties with multi-hop questions that require accurate fact identification and sequential logical reasoning, particularly among numerous fact updates. To tackle these challenges, this paper introduces Graph Memory-based Editing for Large Language Models (GMeLLo), a straitforward and effective method that merges the explicit knowledge representation of Knowledge Graphs (KGs) with the linguistic flexibility of LLMs. Beyond merely leveraging LLMs for question answering, GMeLLo employs these models to convert free-form language into structured queries and fact triples, facilitating seamless interaction with KGs for rapid updates and precise multi-hop reasoning. Our results show that GMeLLo significantly surpasses current state-of-the-art knowledge editing methods in the multi-hop question answering benchmark, MQuAKE, especially in scenarios with extensive knowledge edits.

**Link**: [arxiv](http://arxiv.org/abs/2408.15903v1),  [pdf](http://arxiv.org/pdf/2408.15903v1)

**Tags**: cs.CL 



### Bias in LLMs as Annotators: The Effect of Party Cues on Labelling   Decision by Large Language Models
**Authors**: Sebastian Vallejo Vera, Hunter Driggers

**Updated**: 2024-08-28T16:05:20Z

**Summary**: Human coders are biased. We test similar biases in Large Language Models (LLMs) as annotators. By replicating an experiment run by Ennser-Jedenastik and Meyer (2018), we find evidence that LLMs use political information, and specifically party cues, to judge political statements. Not only do LLMs use relevant information to contextualize whether a statement is positive, negative, or neutral based on the party cue, they also reflect the biases of the human-generated data upon which they have been trained. We also find that unlike humans, who are only biased when faced with statements from extreme parties, LLMs exhibit significant bias even when prompted with statements from center-left and center-right parties. The implications of our findings are discussed in the conclusion.

**Link**: [arxiv](http://arxiv.org/abs/2408.15895v1),  [pdf](http://arxiv.org/pdf/2408.15895v1)

**Tags**: cs.CL cs.LG 



### The Role of Fibration Symmetries in Geometric Deep Learning
**Authors**: Osvaldo Velarde, Lucas Parra, Paolo Boldi, Hernan Makse

**Updated**: 2024-08-28T16:04:40Z

**Summary**: Geometric Deep Learning (GDL) unifies a broad class of machine learning techniques from the perspectives of symmetries, offering a framework for introducing problem-specific inductive biases like Graph Neural Networks (GNNs). However, the current formulation of GDL is limited to global symmetries that are not often found in real-world problems. We propose to relax GDL to allow for local symmetries, specifically fibration symmetries in graphs, to leverage regularities of realistic instances. We show that GNNs apply the inductive bias of fibration symmetries and derive a tighter upper bound for their expressive power. Additionally, by identifying symmetries in networks, we collapse network nodes, thereby increasing their computational efficiency during both inference and training of deep neural networks. The mathematical extension introduced here applies beyond graphs to manifolds, bundles, and grids for the development of models with inductive biases induced by local symmetries that can lead to better generalization.

**Link**: [arxiv](http://arxiv.org/abs/2408.15894v1),  [pdf](http://arxiv.org/pdf/2408.15894v1)

**Tags**: cs.LG 



### Strongly Interacting Quark Matter in Massive Quark Stars
**Authors**: Adamu Issifu, Franciele M. da Silva, Luis C. N. Santos, D√©bora P. Menezes, Tobias Frederico

**Updated**: 2024-08-28T16:02:00Z

**Summary**: This paper investigates the properties of strongly coupled matter at high densities in a quark star (QS). The QS is built from the density-dependent quark mass model (DDQM), modified to obtain higher maximum gravitational mass ($\rm M_{max}$) of the QS using the data from observed pulsars: XMMU J173203.3-344518, PSR J0030+0451, PSR J0740+6620, and PSR J0952-0607 as constraints in Bayesian inference. We observed that the quark matter (QM) that composes QSs with $\rm M_{max} > 2M_\odot$ violates the conformality criteria determined through conformal field theory. This behavior is interpreted as a consequence of the increase in quark population with $\rho_B$ and the concomitant formation of colored quark and gluon condensates, which are influenced by the pressure build-up in the stellar core as $\rho_B$ increases. This is reflected in the enhanced DDQM model employed, which introduces an additional term relevant at high densities. On the other hand, for $\rm M_{max} < 2M_\odot$ we observed the desired behavior of the QM as predicted by quantum chromodynamics (QCD) at higher densities, where the interaction decreases with increasing $\rho_B$ and eventually the quarks become deconfined due to the depletion of the DDQM through an additional attractive contribution in this case.

**Link**: [arxiv](http://arxiv.org/abs/2408.15889v1),  [pdf](http://arxiv.org/pdf/2408.15889v1)

**Tags**: nucl-th hep-ph 



### The ESO UVES/FEROS Large Programs of TESS OB pulsators. II. On the   physical origin of macroturbulence
**Authors**: Nadya Serebriakova, Andrew Tkachenko, Conny Aerts

**Updated**: 2024-08-28T16:01:21Z

**Summary**: Spectral lines of hot massive stars are known to exhibit large excess broadening in addition to rotational broadening. This excess broadening is often attributed to macroturbulence whose physical origin is a matter of active debate in the stellar astrophysics community. By looking into the statistical properties of a large sample of O- and B-type stars, both in the Galaxy and LMC, we aim to shed light on the physical origin of macroturbulent line broadening. We deliver newly measured macroturbulent velocities for 86 stars from the Galaxy in a consistent manner with 126 stars from the LMC. A total sample of 594 O- and B-type stars with measured macroturbulent velocities was composed by complementing our sample with archival data. Furthermore, we compute an extensive grid of MESA models to compare, in a statistical manner, the predicted interior properties of stars (such as convection and wave propagation) with the inference of macroturbulent velocities from high-resolution spectroscopic observations. We find the presence of two principally different regimes where, depending on the initial stellar mass, different mechanisms may be responsible for the observed excess line broadening. Stars with initial masses above some 30$M_{\odot}$ are found to have macroturbulent velocities fully determined by subsurface convective zones formed in the iron opacity bump (FeCZ), while some other mechanism is required to explain observations for masses below 12$M_{\odot}$. The latter finding leaves the potential for waves generated at the interface of the convective core and radiative envelope of the star to be responsible for the observed macroturbulent broadening. Both mechanisms may co-exist in the intermediate regime of stellar masses, between some 12 and 30$M_{\odot}$.

**Link**: [arxiv](http://arxiv.org/abs/2408.15888v1),  [pdf](http://arxiv.org/pdf/2408.15888v1)

**Tags**: astro-ph.SR 



### Secret Collusion among Generative AI Agents
**Authors**: Sumeet Ramesh Motwani, Mikhail Baranchuk, Martin Strohmeier, Vijay Bolina, Philip H. S. Torr, Lewis Hammond, Christian Schroeder de Witt

**Updated**: 2024-08-28T15:53:04Z

**Summary**: Recent capability increases in large language models (LLMs) open up applications in which groups of communicating generative AI agents solve joint tasks. This poses privacy and security challenges concerning the unauthorised sharing of information, or other unwanted forms of agent coordination. Modern steganographic techniques could render such dynamics hard to detect. In this paper, we comprehensively formalise the problem of secret collusion in systems of generative AI agents by drawing on relevant concepts from both AI and security literature. We study incentives for the use of steganography, and propose a variety of mitigation measures. Our investigations result in a model evaluation framework that systematically tests capabilities required for various forms of secret collusion. We provide extensive empirical results across a range of contemporary LLMs. While the steganographic capabilities of current models remain limited, GPT-4 displays a capability jump suggesting the need for continuous monitoring of steganographic frontier model capabilities. We conclude by laying out a comprehensive research program to mitigate future risks of collusion between generative AI models.

**Link**: [arxiv](http://arxiv.org/abs/2402.07510v2),  [pdf](http://arxiv.org/pdf/2402.07510v2)

**Tags**: cs.AI cs.CR 



### Persuasion Games using Large Language Models
**Authors**: Ganesh Prasath Ramani, Shirish Karande, Santhosh V, Yash Bhatia

**Updated**: 2024-08-28T15:50:41Z

**Summary**: Large Language Models (LLMs) have emerged as formidable instruments capable of comprehending and producing human-like text. This paper explores the potential of LLMs, to shape human perspectives and subsequently influence their decisions on particular tasks. This capability finds applications in diverse domains such as Investment, Credit cards and Insurance, wherein they assist users in selecting appropriate insurance policies, investment plans, Credit cards, Retail, as well as in Behavioral Change Support Systems (BCSS).   We present a sophisticated multi-agent framework wherein a consortium of agents operate in collaborative manner. The primary agent engages directly with users through persuasive dialogue, while the auxiliary agents perform tasks such as information retrieval, response analysis, development of persuasion strategies, and validation of facts. Empirical evidence from our experiments demonstrates that this collaborative methodology significantly enhances the persuasive efficacy of the LLM. We analyze user resistance to persuasive efforts continuously and counteract it by employing a combination of rule-based and LLM-based resistance-persuasion mapping techniques.   We employ simulated personas and generate conversations in insurance, banking, and retail domains to evaluate the proficiency of large language models (LLMs) in recognizing, adjusting to, and influencing various personality types. Concurrently, we examine the resistance mechanisms employed by LLM simulated personas. Persuasion is quantified via measurable surveys before and after interaction, LLM-generated scores on conversation, and user decisions (purchase or non-purchase).

**Link**: [arxiv](http://arxiv.org/abs/2408.15879v1),  [pdf](http://arxiv.org/pdf/2408.15879v1)

**Tags**: cs.AI cs.CL 



### Reproducibility-Oriented and Privacy-Preserving Genomic Dataset Sharing
**Authors**: Yuzhou Jiang, Tianxi Ji, Pan Li, Erman Ayday

**Updated**: 2024-08-28T15:24:07Z

**Summary**: As genomic research has become increasingly widespread in recent years, few studies have shared datasets due to the privacy concerns about the genomic records. This hinders the reproduction and validation of research outcomes, which are crucial for catching errors, e.g., miscalculations, during the research process. To address the reproducibility issue of genome-wide association studies (GWAS) outcomes, we propose an innovative method that involves a differential privacy-based scheme for sharing genomic datasets. The proposed scheme involves two stages. In the first stage, we generate a noisy copy of the target dataset by applying an optimized version of a previously proposed XOR mechanism on the binarized (encoded) dataset, where the binary noise generation considers biological features. However, the initial step introduces significant noise, making the dataset less suitable for direct GWAS outcome validation. Thus, in the second stage, we implement a post-processing technique that adjusts the Minor Allele Frequency values (MAFs) in the noisy dataset to align more closely with public MAF information using optimal transport, and then decode it back to genomic space. We evaluate the proposed scheme on three real-life genomic datasets and compare it with a baseline approach (local differential privacy) and two synthesis-based solutions with regard to GWAS outcome validation, data utility, and resistance against membership inference attacks (MIAs). We show that our proposed scheme outperforms all other methods in detecting GWAS outcome errors, achieves better utility, and provides higher privacy protection against membership inference attacks (MIAs). By utilizing our method, genomic researchers will be inclined to share a differentially private, yet of high quality version of their datasets.

**Link**: [arxiv](http://arxiv.org/abs/2209.06327v5),  [pdf](http://arxiv.org/pdf/2209.06327v5)

**Tags**: cs.CR 



### RecurrentGemma: Moving Past Transformers for Efficient Open Language   Models
**Authors**: Aleksandar Botev, Soham De, Samuel L Smith, Anushan Fernando, George-Cristian Muraru, Ruba Haroun, Leonard Berrada, Razvan Pascanu, Pier Giuseppe Sessa, Robert Dadashi, L√©onard Hussenot, Johan Ferret, Sertan Girgin, Olivier Bachem, Alek Andreev, Kathleen Kenealy, Thomas Mesnard, Cassidy Hardin, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi√®re, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Armand Joulin, Noah Fiedel, Evan Senter, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, David Budden, Arnaud Doucet, Sharad Vikram, Adam Paszke, Trevor Gale, Sebastian Borgeaud, Charlie Chen, Andy Brock, Antonia Paterson, Jenny Brennan, Meg Risdal, Raj Gundluru, Nesh Devanathan, Paul Mooney, Nilay Chauhan, Phil Culliton, Luiz Gustavo Martins, Elisa Bandy, David Huntsperger, Glenn Cameron, Arthur Zucker, Tris Warkentin, Ludovic Peran, Minh Giang, Zoubin Ghahramani, Cl√©ment Farabet, Koray Kavukcuoglu, Demis Hassabis, Raia Hadsell, Yee Whye Teh, Nando de Frietas

**Updated**: 2024-08-28T15:05:42Z

**Summary**: We introduce RecurrentGemma, a family of open language models which uses Google's novel Griffin architecture. Griffin combines linear recurrences with local attention to achieve excellent performance on language. It has a fixed-sized state, which reduces memory use and enables efficient inference on long sequences. We provide two sizes of models, containing 2B and 9B parameters, and provide pre-trained and instruction tuned variants for both. Our models achieve comparable performance to similarly-sized Gemma baselines despite being trained on fewer tokens.

**Link**: [arxiv](http://arxiv.org/abs/2404.07839v2),  [pdf](http://arxiv.org/pdf/2404.07839v2)

**Tags**: cs.LG cs.AI cs.CL 



### A Statistical Framework of Watermarks for Large Language Models: Pivot,   Detection Efficiency and Optimal Rules
**Authors**: Xiang Li, Feng Ruan, Huiyuan Wang, Qi Long, Weijie J. Su

**Updated**: 2024-08-28T15:01:04Z

**Summary**: Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of incorrectly classifying LLM-generated text as human-written). Our framework further reduces the problem of determining the optimal detection rule to solving a minimax optimization program. We apply this framework to two representative watermarks -- one of which has been internally implemented at OpenAI -- and obtain several findings that can be instrumental in guiding the practice of implementing watermarks. In particular, we derive optimal detection rules for these watermarks under our framework. These theoretically derived detection rules are demonstrated to be competitive and sometimes enjoy a higher power than existing detection approaches through numerical experiments.

**Link**: [arxiv](http://arxiv.org/abs/2404.01245v2),  [pdf](http://arxiv.org/pdf/2404.01245v2)

**Tags**: math.ST cs.CL cs.CR cs.LG stat.ML stat.TH 



### Downstream bias mitigation is all you need
**Authors**: Arkadeep Baksi, Rahul Singh, Tarun Joshi

**Updated**: 2024-08-28T14:59:31Z

**Summary**: The advent of transformer-based architectures and large language models (LLMs) have significantly advanced the performance of natural language processing (NLP) models. Since these LLMs are trained on huge corpuses of data from the web and other sources, there has been a major concern about harmful prejudices that may potentially be transferred from the data. In many applications, these pre-trained LLMs are fine-tuned on task specific datasets, which can further contribute to biases. This paper studies the extent of biases absorbed by LLMs during pre-training as well as task-specific behaviour after fine-tuning. We found that controlled interventions on pre-trained LLMs, prior to fine-tuning, have minimal effect on lowering biases in classifiers. However, the biases present in domain-specific datasets play a much bigger role, and hence mitigating them at this stage has a bigger impact. While pre-training does matter, but after the model has been pre-trained, even slight changes to co-occurrence rates in the fine-tuning dataset has a significant effect on the bias of the model.

**Link**: [arxiv](http://arxiv.org/abs/2408.00612v2),  [pdf](http://arxiv.org/pdf/2408.00612v2)

**Tags**: cs.CL 



### Look Before You Leap: Towards Decision-Aware and Generalizable   Tool-Usage for Large Language Models
**Authors**: Anchun Gui, Jian Li, Yong Dai, Nan Du, Han Xiao

**Updated**: 2024-08-28T14:54:11Z

**Summary**: Tool-augmented large language models (LLMs) are attracting widespread attention when accessing up-to-date knowledge and alleviating hallucination issues. Nowadays, advanced closed-source LLMs (e.g., ChatGPT) have demonstrated surprising tool-usage capabilities through prompting and in-context learning techniques. To empower the capabilities of open-source LLMs (e.g., LLaMA) in manipulating tools, current efforts focus on either template-driven or token-triggered tool-usage. However, the former hampers LLMs' flexibility to address diverse user's queries due to constrained tool interactions, while the latter limits the generalizability when engaging with new tools, since tool-usage learning is based on task- and tool-specific datasets. To alleviate these concerns, in this paper, we propose a decision-aware and generalizable tool-usage framework (DEER). Specifically, we first construct the tool-usage samples with multiple decision branches via an automatic generation pipeline, thereby inspiring the decision-making awareness of LLMs under diverse scenarios. Meanwhile, we propose a novel tool sampling strategy to enhance the generalizability of LLMs over unseen tools. Extensive experiments demonstrate that our proposed DEER is effective and significantly outperforms baselines across various datasets.

**Link**: [arxiv](http://arxiv.org/abs/2402.16696v3),  [pdf](http://arxiv.org/pdf/2402.16696v3)

**Tags**: cs.CL 



### Knowledge Navigator: LLM-guided Browsing Framework for Exploratory   Search in Scientific Literature
**Authors**: Uri Katz, Mosh Levy, Yoav Goldberg

**Updated**: 2024-08-28T14:48:37Z

**Summary**: The exponential growth of scientific literature necessitates advanced tools for effective knowledge exploration. We present Knowledge Navigator, a system designed to enhance exploratory search abilities by organizing and structuring the retrieved documents from broad topical queries into a navigable, two-level hierarchy of named and descriptive scientific topics and subtopics. This structured organization provides an overall view of the research themes in a domain, while also enabling iterative search and deeper knowledge discovery within specific subtopics by allowing users to refine their focus and retrieve additional relevant documents. Knowledge Navigator combines LLM capabilities with cluster-based methods to enable an effective browsing method. We demonstrate our approach's effectiveness through automatic and manual evaluations on two novel benchmarks, CLUSTREC-COVID and SCITOC. Our code, prompts, and benchmarks are made publicly available.

**Link**: [arxiv](http://arxiv.org/abs/2408.15836v1),  [pdf](http://arxiv.org/pdf/2408.15836v1)

**Tags**: cs.IR cs.AI cs.CL 



### EFTofLSS meets simulation-based inference: $œÉ_8$ from biased   tracers
**Authors**: Beatriz Tucci, Fabian Schmidt

**Updated**: 2024-08-28T14:41:44Z

**Summary**: Cosmological inferences typically rely on explicit expressions for the likelihood and covariance of the data vector, which normally consists of a set of summary statistics. However, in the case of nonlinear large-scale structure, exact expressions for either likelihood or covariance are unknown, and even approximate expressions can become very cumbersome, depending on the scales and summary statistics considered. Simulation-based inference (SBI), in contrast, does not require an explicit form for the likelihood but only a prior and a simulator, thereby naturally circumventing these issues. In this paper, we explore how this technique can be used to infer $\sigma_8$ from a Lagrangian effective field theory (EFT) based forward model for biased tracers. The power spectrum and bispectrum are used as summary statistics to obtain the posterior of the cosmological, bias and noise parameters via neural density estimation. We compare full simulation-based inference with cases where the data vector is drawn from a Gaussian likelihood with sample and analytical covariances. We conclude that, for $k_{\text{max}}=0.1h\text{Mpc}^{-1}$ and $0.2h\text{Mpc}^{-1}$, the form of the covariance is more important than the non-Gaussianity of the likelihood, although this conclusion is expected to depend on the cosmological parameter inferred, the summary statistics considered and range of scales probed.

**Link**: [arxiv](http://arxiv.org/abs/2310.03741v2),  [pdf](http://arxiv.org/pdf/2310.03741v2)

**Tags**: astro-ph.CO astro-ph.GA 



### The Fault in our Stars: Quality Assessment of Code Generation Benchmarks
**Authors**: Mohammed Latif Siddiq, Simantika Dristi, Joy Saha, Joanna C. S. Santos

**Updated**: 2024-08-28T14:38:51Z

**Summary**: Large Language Models (LLMs) are gaining popularity among software engineers. A crucial aspect of developing effective code generation LLMs is to evaluate these models using a robust benchmark. Evaluation benchmarks with quality issues can provide a false sense of performance. In this work, we conduct the first-of-its-kind study of the quality of prompts within benchmarks used to compare the performance of different code generation models. To conduct this study, we analyzed 3,566 prompts from 9 code generation benchmarks to identify quality issues in them. We also investigated whether fixing the identified quality issues in the benchmarks' prompts affects a model's performance. We also studied memorization issues of the evaluation dataset, which can put into question a benchmark's trustworthiness. We found that code generation evaluation benchmarks mainly focused on Python and coding exercises and had very limited contextual dependencies to challenge the model. These datasets and the developers' prompts suffer from quality issues like spelling and grammatical errors, unclear sentences to express developers' intent, and not using proper documentation style. Fixing all these issues in the benchmarks can lead to a better performance for Python code generation, but not a significant improvement was observed for Java code generation. We also found evidence that GPT-3.5-Turbo and CodeGen-2.5 models may have data contamination issues.

**Link**: [arxiv](http://arxiv.org/abs/2404.10155v2),  [pdf](http://arxiv.org/pdf/2404.10155v2)

**Tags**: cs.SE cs.LG 



### MR-Adopt: Automatic Deduction of Input Transformation Function for   Metamorphic Testing
**Authors**: Congying Xu, Songqiang Chen, Jiarong Wu, Shing-Chi Cheung, Valerio Terragni, Hengcheng Zhu, Jialun Cao

**Updated**: 2024-08-28T14:24:48Z

**Summary**: While a recent study reveals that many developer-written test cases can encode a reusable Metamorphic Relation (MR), over 70% of them directly hard-code the source input and follow-up input in the encoded relation. Such encoded MRs, which do not contain an explicit input transformation to transform the source inputs to corresponding follow-up inputs, cannot be reused with new source inputs to enhance test adequacy.   In this paper, we propose MR-Adopt (Automatic Deduction Of inPut Transformation) to automatically deduce the input transformation from the hard-coded source and follow-up inputs, aiming to enable the encoded MRs to be reused with new source inputs. With typically only one pair of source and follow-up inputs available in an MR-encoded test case as the example, we leveraged LLMs to understand the intention of the test case and generate additional examples of source-followup input pairs. This helps to guide the generation of input transformations generalizable to multiple source inputs. Besides, to mitigate the issue that LLMs generate erroneous code, we refine LLM-generated transformations by removing MR- irrelevant code elements with data-flow analysis. Finally, we assess candidate transformations based on encoded output relations and select the best transformation as the result. Evaluation results show that MR-Adopt can generate input transformations applicable to all experimental source inputs for 72.00% of encoded MRs, which is 33.33% more than using vanilla GPT-3.5. By incorporating MR- Adopt-generated input transformations, encoded MR-based test cases can effectively enhance the test adequacy, increasing the line coverage and mutation score by 10.62% and 18.91%, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2408.15815v1),  [pdf](http://arxiv.org/pdf/2408.15815v1)

**Tags**: cs.SE 



### Unveiling the Statistical Foundations of Chain-of-Thought Prompting   Methods
**Authors**: Xinyang Hu, Fengzhuo Zhang, Siyu Chen, Zhuoran Yang

**Updated**: 2024-08-28T14:13:41Z

**Summary**: Chain-of-Thought (CoT) prompting and its variants have gained popularity as effective methods for solving multi-step reasoning problems using pretrained large language models (LLMs). In this work, we analyze CoT prompting from a statistical estimation perspective, providing a comprehensive characterization of its sample complexity. To this end, we introduce a multi-step latent variable model that encapsulates the reasoning process, where the latent variable encodes the task information. Under this framework, we demonstrate that when the pretraining dataset is sufficiently large, the estimator formed by CoT prompting is equivalent to a Bayesian estimator. This estimator effectively solves the multi-step reasoning problem by aggregating a posterior distribution inferred from the demonstration examples in the prompt. Moreover, we prove that the statistical error of the CoT estimator can be decomposed into two main components: (i) a prompting error, which arises from inferring the true task using CoT prompts, and (ii) the statistical error of the pretrained LLM. We establish that, under appropriate assumptions, the prompting error decays exponentially to zero as the number of demonstrations increases. Additionally, we explicitly characterize the approximation and generalization errors of the pretrained LLM. Notably, we construct a transformer model that approximates the target distribution of the multi-step reasoning problem with an error that decreases exponentially in the number of transformer blocks. Our analysis extends to other variants of CoT, including Self-Consistent CoT, Tree-of-Thought, and Selection-Inference, offering a broad perspective on the efficacy of these methods. We also provide numerical experiments to validate the theoretical findings.

**Link**: [arxiv](http://arxiv.org/abs/2408.14511v2),  [pdf](http://arxiv.org/pdf/2408.14511v2)

**Tags**: cs.AI cs.CL cs.LG math.ST stat.ML stat.TH 



### Identifying Influential and Vulnerable Nodes in Interaction Networks   through Estimation of Transfer Entropy Between Univariate and Multivariate   Time Series
**Authors**: Julian Lee

**Updated**: 2024-08-28T14:11:59Z

**Summary**: Transfer entropy (TE) is a powerful tool for measuring causal relationships within interaction networks. Traditionally, TE and its conditional variants are applied pairwise between dynamic variables to infer these causal relationships. However, identifying the most influential or vulnerable node in a system requires measuring the causal influence of each component on the entire system and vice versa. In this paper, I propose using outgoing and incoming transfer entropy-where outgoing TE quantifies the influence of a node on the rest of the system, and incoming TE measures the influence of the rest of the system on the node. The node with the highest outgoing TE is identified as the most influential, or "hub", while the node with the highest incoming TE is the most vulnerable, or "anti-hub". Since these measures involve transfer entropy between univariate and multivariate time series, naive estimation methods can result in significant errors, particularly when the number of variables is comparable to or exceeds the number of samples. To address this, I introduce a novel estimation scheme that computes outgoing and incoming TE only between significantly interacting partners. The feasibility of this approach is demonstrated by using synthetic data, and by applying it to a real data of oral microbiota. The method successfully identifies the bacterial species known to be key players in the bacterial community, demonstrating the power of the new method.

**Link**: [arxiv](http://arxiv.org/abs/2408.15811v1),  [pdf](http://arxiv.org/pdf/2408.15811v1)

**Tags**: cond-mat.stat-mech physics.bio-ph 



### Stick to your Role! Stability of Personal Values Expressed in Large   Language Models
**Authors**: Grgur Kovaƒç, R√©my Portelas, Masataka Sawayama, Peter Ford Dominey, Pierre-Yves Oudeyer

**Updated**: 2024-08-28T14:04:05Z

**Summary**: The standard way to study Large Language Models (LLMs) with benchmarks or psychology questionnaires is to provide many different queries from similar minimal contexts (e.g. multiple choice questions). However, due to LLMs' highly context-dependent nature, conclusions from such minimal-context evaluations may be little informative about the model's behavior in deployment (where it will be exposed to many new contexts). We argue that context-dependence (specifically, value stability) should be studied as a specific property of LLMs and used as another dimension of LLM comparison (alongside others such as cognitive abilities, knowledge, or model size). We present a case-study on the stability of value expression over different contexts (simulated conversations on different topics) as measured using a standard psychology questionnaire (PVQ) and on behavioral downstream tasks. Reusing methods from psychology, we study Rank-order stability on the population (interpersonal) level, and Ipsative stability on the individual (intrapersonal) level. We consider two settings (with and without instructing LLMs to simulate particular personas), two simulated populations, and three downstream tasks. We observe consistent trends in the stability of models and model families - Mixtral, Mistral, GPT-3.5 and Qwen families are more stable than LLaMa-2 and Phi. The consistency of these trends implies that some models exhibit higher value stability than others, and that stability can be estimated with the set of introduced methodological tools. When instructed to simulate particular personas, LLMs exhibit low Rank-order stability, which further diminishes with conversation length. This highlights the need for future research on LLMs that coherently simulate different personas. This paper provides a foundational step in that direction, and, to our knowledge, it is the first study of value stability in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2402.14846v4),  [pdf](http://arxiv.org/pdf/2402.14846v4)

**Tags**: cs.CL cs.AI cs.LG 68T07 I.2.7 



### Investigating Complex HPV Dynamics Using Emulation and History Matching
**Authors**: Andrew Iskauskas, Jamie A. Cohen, Danny Scarponi, Ian Vernon, Michael Goldstein, Daniel Klein, Richard G. White, Nicky McCreesh

**Updated**: 2024-08-28T14:02:10Z

**Summary**: The study of transmission and progression of human papillomavirus (HPV) is crucial for understanding the incidence of cervical cancers, and has been identified as a priority worldwide. The complexity of the disease necessitates a detailed model of HPV transmission and its progression to cancer; to infer properties of the above we require a careful process that can match to imperfect or incomplete observational data. In this paper, we describe the HPVsim simulator to satisfy the former requirement; to satisfy the latter we couple this stochastic simulator to a process of emulation and history matching using the R package hmer. With these tools, we are able to obtain a comprehensive collection of parameter combinations that could give rise to observed cancer data, and explore the implications of the variability of these parameter sets as it relates to future health interventions.

**Link**: [arxiv](http://arxiv.org/abs/2408.15805v1),  [pdf](http://arxiv.org/pdf/2408.15805v1)

**Tags**: stat.AP stat.CO 



### Feedback-Driven Automated Whole Bug Report Reproduction for Android Apps
**Authors**: Dingbang Wang, Yu Zhao, Sidong Feng, Zhaoxu Zhang, William G. J. Halfond, Chunyang Chen, Xiaoxia Sun, Jiangfan Shi, Tingting Yu

**Updated**: 2024-08-28T13:52:57Z

**Summary**: In software development, bug report reproduction is a challenging task. This paper introduces ReBL, a novel feedback-driven approach that leverages GPT-4, a large-scale language model (LLM), to automatically reproduce Android bug reports. Unlike traditional methods, ReBL bypasses the use of Step to Reproduce (S2R) entities. Instead, it leverages the entire textual bug report and employs innovative prompts to enhance GPT's contextual reasoning. This approach is more flexible and context-aware than the traditional step-by-step entity matching approach, resulting in improved accuracy and effectiveness. In addition to handling crash reports, ReBL has the capability of handling non-crash functional bug reports. Our evaluation of 96 Android bug reports (73 crash and 23 non-crash) demonstrates that ReBL successfully reproduced 90.63% of these reports, averaging only 74.98 seconds per bug report. Additionally, ReBL outperformed three existing tools in both success rate and speed.

**Link**: [arxiv](http://arxiv.org/abs/2407.05165v3),  [pdf](http://arxiv.org/pdf/2407.05165v3)

**Tags**: cs.SE 



### Scaling Up Summarization: Leveraging Large Language Models for Long Text   Extractive Summarization
**Authors**: L√©o Hemamou, Mehdi Debiane

**Updated**: 2024-08-28T13:52:19Z

**Summary**: In an era where digital text is proliferating at an unprecedented rate, efficient summarization tools are becoming indispensable. While Large Language Models (LLMs) have been successfully applied in various NLP tasks, their role in extractive text summarization remains underexplored. This paper introduces EYEGLAXS (Easy Yet Efficient larGe LAnguage model for eXtractive Summarization), a framework that leverages LLMs, specifically LLAMA2-7B and ChatGLM2-6B, for extractive summarization of lengthy text documents. Instead of abstractive methods, which often suffer from issues like factual inaccuracies and hallucinations, EYEGLAXS focuses on extractive summarization to ensure factual and grammatical integrity. Utilizing state-of-the-art techniques such as Flash Attention and Parameter-Efficient Fine-Tuning (PEFT), EYEGLAXS addresses the computational and resource challenges typically associated with LLMs. The system sets new performance benchmarks on well-known datasets like PubMed and ArXiv. Furthermore, we extend our research through additional analyses that explore the adaptability of LLMs in handling different sequence lengths and their efficiency in training on smaller datasets. These contributions not only set a new standard in the field but also open up promising avenues for future research in extractive text summarization.

**Link**: [arxiv](http://arxiv.org/abs/2408.15801v1),  [pdf](http://arxiv.org/pdf/2408.15801v1)

**Tags**: cs.CL 



### Evaluating Named Entity Recognition Using Few-Shot Prompting with Large   Language Models
**Authors**: H√©di Zhegidi, Ludovic Moncla

**Updated**: 2024-08-28T13:42:28Z

**Summary**: This paper evaluates Few-Shot Prompting with Large Language Models for Named Entity Recognition (NER). Traditional NER systems rely on extensive labeled datasets, which are costly and time-consuming to obtain. Few-Shot Prompting or in-context learning enables models to recognize entities with minimal examples. We assess state-of-the-art models like GPT-4 in NER tasks, comparing their few-shot performance to fully supervised benchmarks. Results show that while there is a performance gap, large models excel in adapting to new entity types and domains with very limited data. We also explore the effects of prompt engineering, guided output format and context length on performance. This study underscores Few-Shot Learning's potential to reduce the need for large labeled datasets, enhancing NER scalability and accessibility.

**Link**: [arxiv](http://arxiv.org/abs/2408.15796v1),  [pdf](http://arxiv.org/pdf/2408.15796v1)

**Tags**: cs.IR cs.AI 



### Language Adaptation on a Tight Academic Compute Budget: Tokenizer   Swapping Works and Pure bfloat16 Is Enough
**Authors**: Konstantin Dobler, Gerard de Melo

**Updated**: 2024-08-28T13:37:07Z

**Summary**: We investigate continued pretraining of LLMs for language adaptation on a tight academic budget: a setting in which only a few GPUs can be used in parallel, for a heavily constrained duration. We focus on adapting Mistral-7B to German or Arabic and evaluate several techniques to improve efficiency and effectiveness in this setting. Our German models adapted on this tight compute budget underperform compared to the base Mistral-7B, while our Arabic models outperform several baselines, showing that for sufficiently well-represented languages, continued pretraining for specialization is not always helpful. Our main findings focus on training precision and tokenizer swapping. Our results show that pure bfloat16 training is a viable alternative to mixed-precision training, while being much faster when only using a few GPUs. Swapping the tokenizer for a specialized one yields more efficient tokenization and is competitive with the original tokenizer, which already contains some German tokens, but did not significantly increase performance for German. Code and model weights are available at on GitHub.

**Link**: [arxiv](http://arxiv.org/abs/2408.15793v1),  [pdf](http://arxiv.org/pdf/2408.15793v1)

**Tags**: cs.CL cs.LG 



### Efficient LLM Scheduling by Learning to Rank
**Authors**: Yichao Fu, Siqi Zhu, Runlong Su, Aurick Qiao, Ion Stoica, Hao Zhang

**Updated**: 2024-08-28T13:35:54Z

**Summary**: In Large Language Model (LLM) inference, the output length of an LLM request is typically regarded as not known a priori. Consequently, most LLM serving systems employ a simple First-come-first-serve (FCFS) scheduling strategy, leading to Head-Of-Line (HOL) blocking and reduced throughput and service quality. In this paper, we reexamine this assumption -- we show that, although predicting the exact generation length of each request is infeasible, it is possible to predict the relative ranks of output lengths in a batch of requests, using learning to rank. The ranking information offers valuable guidance for scheduling requests. Building on this insight, we develop a novel scheduler for LLM inference and serving that can approximate the shortest-job-first (SJF) schedule better than existing approaches. We integrate this scheduler with the state-of-the-art LLM serving system and show significant performance improvement in several important applications: 2.8x lower latency in chatbot serving and 6.5x higher throughput in synthetic data generation. Our code is available at https://github.com/hao-ai-lab/vllm-ltr.git

**Link**: [arxiv](http://arxiv.org/abs/2408.15792v1),  [pdf](http://arxiv.org/pdf/2408.15792v1)

**Tags**: cs.LG 



### A Stochastic Robust Adaptive Systems Level Approach to Stabilizing   Large-Scale Uncertain Markovian Jump Linear Systems
**Authors**: SooJean Han, Minwoo M. Kim, Ieun Choo

**Updated**: 2024-08-28T13:32:40Z

**Summary**: We propose a unified framework for robustly and adaptively stabilizing large-scale networked uncertain Markovian jump linear systems (MJLS) under external disturbances and mode switches that can change the network's topology. Adaptation is achieved by using minimal information on the disturbance to identify modes that are consistent with observable data. Robust control is achieved by extending the system level synthesis (SLS) approach, which allows us to pose the problem of simultaneously stabilizing multiple plants as a two-step convex optimization procedure. Our control pipeline computes a likelihood distribution of the system's current mode, uses them as probabilistic weights during simultaneous stabilization, then updates the likelihood via Bayesian inference. Because of this "softer" probabilistic approach to robust stabilization, our control pipeline does not suffer from abrupt destabilization issues due to changes in the system's true mode, which were observed in a previous method. Separability of SLS also lets us compute localized robust controllers for each subsystem, allowing for network scalability; we use several information consensus methods so that mode estimation can also be done locally. We apply our algorithms to disturbance-rejection on two sample dynamic power grid networks, a small-scale system with 7 nodes and a large-scale grid of 25 nodes.

**Link**: [arxiv](http://arxiv.org/abs/2408.15789v1),  [pdf](http://arxiv.org/pdf/2408.15789v1)

**Tags**: eess.SY cs.SY 



### When Multi-Task Learning Meets Partial Supervision: A Computer Vision   Review
**Authors**: Maxime Fontana, Michael Spratling, Miaojing Shi

**Updated**: 2024-08-28T13:30:36Z

**Summary**: Multi-Task Learning (MTL) aims to learn multiple tasks simultaneously while exploiting their mutual relationships. By using shared resources to simultaneously calculate multiple outputs, this learning paradigm has the potential to have lower memory requirements and inference times compared to the traditional approach of using separate methods for each task. Previous work in MTL has mainly focused on fully-supervised methods, as task relationships can not only be leveraged to lower the level of data-dependency of those methods but they can also improve performance. However, MTL introduces a set of challenges due to a complex optimisation scheme and a higher labeling requirement. This review focuses on how MTL could be utilised under different partial supervision settings to address these challenges. First, this review analyses how MTL traditionally uses different parameter sharing techniques to transfer knowledge in between tasks. Second, it presents the different challenges arising from such a multi-objective optimisation scheme. Third, it introduces how task groupings can be achieved by analysing task relationships. Fourth, it focuses on how partially supervised methods applied to MTL can tackle the aforementioned challenges. Lastly, this review presents the available datasets, tools and benchmarking results of such methods.

**Link**: [arxiv](http://arxiv.org/abs/2307.14382v2),  [pdf](http://arxiv.org/pdf/2307.14382v2)

**Tags**: cs.LG cs.AI cs.CV 



### Interactive Agents: Simulating Counselor-Client Psychological Counseling   via Role-Playing LLM-to-LLM Interactions
**Authors**: Huachuan Qiu, Zhenzhong Lan

**Updated**: 2024-08-28T13:29:59Z

**Summary**: Virtual counselors powered by large language models (LLMs) aim to create interactive support systems that effectively assist clients struggling with mental health challenges. To replicate counselor-client conversations, researchers have built an online mental health platform that allows professional counselors to provide clients with text-based counseling services for about an hour per session. Notwithstanding its effectiveness, challenges exist as human annotation is time-consuming, cost-intensive, privacy-protected, and not scalable. To address this issue and investigate the applicability of LLMs in psychological counseling conversation simulation, we propose a framework that employs two LLMs via role-playing for simulating counselor-client interactions. Our framework involves two LLMs, one acting as a client equipped with a specific and real-life user profile and the other playing the role of an experienced counselor, generating professional responses using integrative therapy techniques. We implement both the counselor and the client by zero-shot prompting the GPT-4 model. In order to assess the effectiveness of LLMs in simulating counselor-client interactions and understand the disparities between LLM- and human-generated conversations, we evaluate the synthetic data from various perspectives. We begin by assessing the client's performance through automatic evaluations. Next, we analyze and compare the disparities between dialogues generated by the LLM and those generated by professional counselors. Furthermore, we conduct extensive experiments to thoroughly examine the performance of our LLM-based counselor trained with synthetic interactive dialogues by benchmarking against state-of-the-art models for mental health.

**Link**: [arxiv](http://arxiv.org/abs/2408.15787v1),  [pdf](http://arxiv.org/pdf/2408.15787v1)

**Tags**: cs.CL cs.IR 



### LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language   Models
**Authors**: Jiayi Gui, Yiming Liu, Jiale Cheng, Xiaotao Gu, Xiao Liu, Hongning Wang, Yuxiao Dong, Jie Tang, Minlie Huang

**Updated**: 2024-08-28T13:16:41Z

**Summary**: Large Language Models (LLMs) have demonstrated notable capabilities across various tasks, showcasing complex problem-solving abilities. Understanding and executing complex rules, along with multi-step planning, are fundamental to logical reasoning and critical for practical LLM agents and decision-making systems. However, evaluating LLMs as effective rule-based executors and planners remains underexplored. In this paper, we introduce LogicGame, a novel benchmark designed to evaluate the comprehensive rule understanding, execution, and planning capabilities of LLMs. Unlike traditional benchmarks, LogicGame provides diverse games that contain a series of rules with an initial state, requiring models to comprehend and apply predefined regulations to solve problems. We create simulated scenarios in which models execute or plan operations to achieve specific outcomes. These game scenarios are specifically designed to distinguish logical reasoning from mere knowledge by relying exclusively on predefined rules. This separation allows for a pure assessment of rule-based reasoning capabilities. The evaluation considers not only final outcomes but also intermediate steps, providing a comprehensive assessment of model performance. Moreover, these intermediate steps are deterministic and can be automatically verified. LogicGame defines game scenarios with varying difficulty levels, from simple rule applications to complex reasoning chains, in order to offer a precise evaluation of model performance on rule understanding and multi-step execution. Utilizing LogicGame, we test various LLMs and identify notable shortcomings in their rule-based logical reasoning abilities.

**Link**: [arxiv](http://arxiv.org/abs/2408.15778v1),  [pdf](http://arxiv.org/pdf/2408.15778v1)

**Tags**: cs.AI cs.CL 



### A Survey on Evaluation of Multimodal Large Language Models
**Authors**: Jiaxing Huang, Jingyi Zhang

**Updated**: 2024-08-28T13:05:55Z

**Summary**: Multimodal Large Language Models (MLLMs) mimic human perception and reasoning system by integrating powerful Large Language Models (LLMs) with various modality encoders (e.g., vision, audio), positioning LLMs as the "brain" and various modality encoders as sensory organs. This framework endows MLLMs with human-like capabilities, and suggests a potential pathway towards achieving artificial general intelligence (AGI). With the emergence of all-round MLLMs like GPT-4V and Gemini, a multitude of evaluation methods have been developed to assess their capabilities across different dimensions. This paper presents a systematic and comprehensive review of MLLM evaluation methods, covering the following key aspects: (1) the background of MLLMs and their evaluation; (2) "what to evaluate" that reviews and categorizes existing MLLM evaluation tasks based on the capabilities assessed, including general multimodal recognition, perception, reasoning and trustworthiness, and domain-specific applications such as socioeconomic, natural sciences and engineering, medical usage, AI agent, remote sensing, video and audio processing, 3D point cloud analysis, and others; (3) "where to evaluate" that summarizes MLLM evaluation benchmarks into general and specific benchmarks; (4) "how to evaluate" that reviews and illustrates MLLM evaluation steps and metrics; Our overarching goal is to provide valuable insights for researchers in the field of MLLM evaluation, thereby facilitating the development of more capable and reliable MLLMs. We emphasize that evaluation should be regarded as a critical discipline, essential for advancing the field of MLLMs.

**Link**: [arxiv](http://arxiv.org/abs/2408.15769v1),  [pdf](http://arxiv.org/pdf/2408.15769v1)

**Tags**: cs.CV cs.AI cs.CL 



### Harmonized Speculative Sampling
**Authors**: Lefan Zhang, Xiaodan Wang, Yanhua Huang, Ruiwen Xu

**Updated**: 2024-08-28T12:59:12Z

**Summary**: Speculative sampling has proven to be an effective solution to accelerate decoding from large language models, where the acceptance rate significantly determines the performance. Most previous works on improving the acceptance rate focus on aligned training and efficient decoding, implicitly paying less attention to the linkage of training and decoding. In this work, we first investigate the linkage of training and decoding for speculative sampling and then propose a solution named HArmonized Speculative Sampling (HASS). HASS improves the acceptance rate without extra inference overhead by harmonizing training and decoding on their objectives and contexts. Experiments on three LLaMA models demonstrate that HASS achieves 2.81x-3.65x wall-clock time speedup ratio averaging across three datasets, which is 8%-15% faster than EAGLE-2.

**Link**: [arxiv](http://arxiv.org/abs/2408.15766v1),  [pdf](http://arxiv.org/pdf/2408.15766v1)

**Tags**: cs.LG cs.CL 



### The impacts of solar wind on the Martian upper atmosphere
**Authors**: Kamsali Nagaraja, S. C. Chakravarty

**Updated**: 2024-08-28T12:57:00Z

**Summary**: Since the first in-situ measurements of the altitude profile of upper atmospheric density and composition were carried out by the Viking lander missions in 1976, similar data are continuously gathered by MAVEN and MOM spacecraft orbiting Mars since their launch in September 2014 with mass spectrometers and other related payloads. Using near-simultaneous observations by the two orbiters, it is seen that both data sets indicate significant day-to-day variations of Argon density profiles in the thermosphere-exosphere, 150-300 km region, during the period 1-15, June 2018, when the solar EUV radiation did not show any appreciable change but the solar wind energetic particle fluxes did so. Extending this study to include the other parent atmospheric constituents carbon dioxide, helium, nitrogen and their photochemical products atomic oxygen, and carbon monoxide during the same period it is found that the density profiles of carbon dioxide and atomic oxygen also show similar variations with carbon dioxide densities showing an increasing trend similar to Argon, but a reversal of this trend for atomic oxygen densities. Using insitu and near simultaneous measurements of solar EUV fluxes and the solar wind plasma velocities and densities near MAVEN periapsis it is noted that, unlike the solar EUV radiation, solar wind parameters showed a decrease by a factor of 2-3. Hence, it is inferred that the energetic and penetrating solar wind charged particle impact-driven dissociation, ionisation and ion-chemical processes could decrease the carbon dioxide densities leading to an increase in atomic oxygen densities. This result is also discussed from the considerations of the proton gyro radius effect, pickup ions, sputtering, energetic neutral atoms driven ionisation and ion losses. Further data and modelling efforts would be necessary to confirm this finding.

**Link**: [arxiv](http://arxiv.org/abs/2210.01417v2),  [pdf](http://arxiv.org/pdf/2210.01417v2)

**Tags**: physics.space-ph astro-ph.EP 



### Sensitivity-Aware Amortized Bayesian Inference
**Authors**: Lasse Elsem√ºller, Hans Olischl√§ger, Marvin Schmitt, Paul-Christian B√ºrkner, Ullrich K√∂the, Stefan T. Radev

**Updated**: 2024-08-28T12:33:52Z

**Summary**: Sensitivity analyses reveal the influence of various modeling choices on the outcomes of statistical analyses. While theoretically appealing, they are overwhelmingly inefficient for complex Bayesian models. In this work, we propose sensitivity-aware amortized Bayesian inference (SA-ABI), a multifaceted approach to efficiently integrate sensitivity analyses into simulation-based inference with neural networks. First, we utilize weight sharing to encode the structural similarities between alternative likelihood and prior specifications in the training process with minimal computational overhead. Second, we leverage the rapid inference of neural networks to assess sensitivity to data perturbations and preprocessing steps. In contrast to most other Bayesian approaches, both steps circumvent the costly bottleneck of refitting the model for each choice of likelihood, prior, or data set. Finally, we propose to use deep ensembles to detect sensitivity arising from unreliable approximation (e.g., due to model misspecification). We demonstrate the effectiveness of our method in applied modeling problems, ranging from disease outbreak dynamics and global warming thresholds to human decision-making. Our results support sensitivity-aware inference as a default choice for amortized Bayesian workflows, automatically providing modelers with insights into otherwise hidden dimensions.

**Link**: [arxiv](http://arxiv.org/abs/2310.11122v6),  [pdf](http://arxiv.org/pdf/2310.11122v6)

**Tags**: stat.ML cs.LG stat.ME 



### Language-specific Calibration for Pruning Multilingual Language Models
**Authors**: Simon Kurz, Jian-Jia Chen, Lucie Flek, Zhixue Zhao

**Updated**: 2024-08-28T12:03:54Z

**Summary**: Recent advances in large language model (LLM) pruning have shown state-of-the-art compression results in post-training and retraining-free settings while maintaining high predictive performance. However, such research mainly considers calibrating pruning using English text, despite the multilingual nature of modern LLMs and their frequent uses in non-English languages. In this paper, we set out to explore effective strategies for calibrating the pruning of multilingual language models. We present the first comprehensive empirical study, comparing different calibration languages for pruning multilingual models across diverse tasks, models, and state-of-the-art pruning techniques. Our results present practical suggestions, for example, calibrating in the target language can efficiently yield lower perplexity, but does not necessarily benefit downstream tasks. Our further analysis experiments unveil that calibration in the target language mainly contributes to preserving language-specific features related to fluency and coherence, but might not contribute to capturing language-agnostic features such as language understanding and reasoning. Last, we provide practical recommendations for future practitioners.

**Link**: [arxiv](http://arxiv.org/abs/2408.14398v2),  [pdf](http://arxiv.org/pdf/2408.14398v2)

**Tags**: cs.CL cs.AI cs.LG 



### A review of sequential Monte Carlo methods for real-time disease   modeling
**Authors**: Dhorasso Temfack, Jason Wyse

**Updated**: 2024-08-28T12:03:25Z

**Summary**: Sequential Monte Carlo methods are a powerful framework for approximating the posterior distribution of a state variable in a sequential manner. They provide an attractive way of analyzing dynamic systems in real-time, taking into account the limitations of traditional approaches such as Markov Chain Monte Carlo methods, which are not well suited to data that arrives incrementally. This paper reviews and explores the application of Sequential Monte Carlo in dynamic disease modeling, highlighting its capacity for online inference and real-time adaptation to evolving disease dynamics. The integration of kernel density approximation techniques within the stochastic Susceptible-Exposed-Infectious-Recovered (SEIR) compartment model is examined, demonstrating the algorithm's effectiveness in monitoring time-varying parameters such as the effective reproduction number. Case studies, including simulations with synthetic data and analysis of real-world COVID-19 data from Ireland, demonstrate the practical applicability of this approach for informing timely public health interventions.

**Link**: [arxiv](http://arxiv.org/abs/2408.15739v1),  [pdf](http://arxiv.org/pdf/2408.15739v1)

**Tags**: q-bio.PE math.DS stat.CO 



### Inferring Individual Direct Causal Effects Under Heterogeneous Peer   Influence
**Authors**: Shishir Adhikari, Elena Zheleva

**Updated**: 2024-08-28T11:27:25Z

**Summary**: Causal inference in networks should account for interference, which occurs when a unit's outcome is influenced by treatments or outcomes of peers. Heterogeneous peer influence (HPI) occurs when a unit's outcome is influenced differently by different peers based on their attributes and relationships, or when each unit has a different susceptibility to peer influence. Existing solutions to estimating direct causal effects under interference consider either homogeneous influence from peers or specific heterogeneous influence mechanisms (e.g., based on local neighborhood structure). This paper presents a methodology for estimating individual direct causal effects in the presence of HPI where the mechanism of influence is not known a priori. We propose a structural causal model for networks that can capture different possible assumptions about network structure, interference conditions, and causal dependence and enables reasoning about identifiability in the presence of HPI. We find potential heterogeneous contexts using the causal model and propose a novel graph neural network-based estimator to estimate individual direct causal effects. We show that state-of-the-art methods for individual direct effect estimation produce biased results in the presence of HPI, and that our proposed estimator is robust.

**Link**: [arxiv](http://arxiv.org/abs/2305.17479v3),  [pdf](http://arxiv.org/pdf/2305.17479v3)

**Tags**: cs.SI cs.LG 



### Benchmarking ML Approaches to UWB-Based Range-Only Posture Recognition   for Human Robot-Interaction
**Authors**: Salma Salimi, Sahar Salimpour, Jorge Pe√±a Queralta, Wallace Moreira Bessa, Tomi Westerlund

**Updated**: 2024-08-28T11:24:17Z

**Summary**: Human pose estimation involves detecting and tracking the positions of various body parts using input data from sources such as images, videos, or motion and inertial sensors. This paper presents a novel approach to human pose estimation using machine learning algorithms to predict human posture and translate them into robot motion commands using ultra-wideband (UWB) nodes, as an alternative to motion sensors. The study utilizes five UWB sensors implemented on the human body to enable the classification of still poses and more robust posture recognition. This approach ensures effective posture recognition across a variety of subjects. These range measurements serve as input features for posture prediction models, which are implemented and compared for accuracy. For this purpose, machine learning algorithms including K-Nearest Neighbors (KNN), Support Vector Machine (SVM), and deep Multi-Layer Perceptron (MLP) neural network are employed and compared in predicting corresponding postures. We demonstrate the proposed approach for real-time control of different mobile/aerial robots with inference implemented in a ROS 2 node. Experimental results demonstrate the efficacy of the approach, showcasing successful prediction of human posture and corresponding robot movements with high accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2408.15717v1),  [pdf](http://arxiv.org/pdf/2408.15717v1)

**Tags**: cs.RO 



### Conan-embedding: General Text Embedding with More and Better Negative   Samples
**Authors**: Shiyu Li, Yang Tang, Shizhe Chen, Xi Chen

**Updated**: 2024-08-29T14:47:37Z

**Summary**: With the growing popularity of RAG, the capabilities of embedding models are gaining increasing attention. Embedding models are primarily trained through contrastive loss learning, with negative examples being a key component. Previous work has proposed various hard negative mining strategies, but these strategies are typically employed as preprocessing steps. In this paper, we propose the conan-embedding model, which maximizes the utilization of more and higher-quality negative examples. Specifically, since the model's ability to handle preprocessed negative examples evolves during training, we propose dynamic hard negative mining method to expose the model to more challenging negative examples throughout the training process. Secondly, contrastive learning requires as many negative examples as possible but is limited by GPU memory constraints. Therefore, we use a Cross-GPU balancing Loss to provide more negative examples for embedding training and balance the batch size across multiple tasks. Moreover, we also discovered that the prompt-response pairs from LLMs can be used for embedding training. Our approach effectively enhances the capabilities of embedding models, currently ranking first on the Chinese leaderboard of Massive text embedding benchmark

**Link**: [arxiv](http://arxiv.org/abs/2408.15710v2),  [pdf](http://arxiv.org/pdf/2408.15710v2)

**Tags**: cs.CL 



### Evading AI-Generated Content Detectors using Homoglyphs
**Authors**: Aldan Creo, Shushanta Pudasaini

**Updated**: 2024-08-28T11:10:59Z

**Summary**: The advent of large language models (LLMs) has enabled the generation of text that increasingly exhibits human-like characteristics. As the detection of such content is of significant importance, numerous studies have been conducted with the aim of developing reliable AI-generated text detectors. These detectors have demonstrated promising results on test data, but recent research has revealed that they can be circumvented by employing different techniques. In this paper, we present homoglyph-based attacks ($a \rightarrow {\alpha}$) as a means of circumventing existing detectors. A comprehensive evaluation was conducted to assess the effectiveness of these attacks on seven detectors, including ArguGPT, Binoculars, DetectGPT, Fast-DetectGPT, Ghostbuster, OpenAI's detector, and watermarking techniques, on five different datasets. Our findings demonstrate that homoglyph-based attacks can effectively circumvent state-of-the-art detectors, leading them to classify all texts as either AI-generated or human-written (decreasing the average Matthews Correlation Coefficient from 0.64 to -0.01). We then examine the effectiveness of these attacks by analyzing how homoglyphs impact different families of detectors. Finally, we discuss the implications of these findings and potential defenses against such attacks.

**Link**: [arxiv](http://arxiv.org/abs/2406.11239v2),  [pdf](http://arxiv.org/pdf/2406.11239v2)

**Tags**: cs.CL cs.AI 



### Comparing diversity, negativity, and stereotypes in Chinese-language AI   technologies: a case study on Baidu, Ernie and Qwen
**Authors**: Geng Liu, Carlo Alberto Bono, Francesco Pierri

**Updated**: 2024-08-28T10:51:18Z

**Summary**: Large Language Models (LLMs) and search engines have the potential to perpetuate biases and stereotypes by amplifying existing prejudices in their training data and algorithmic processes, thereby influencing public perception and decision-making. While most work has focused on Western-centric AI technologies, we study Chinese-based tools by investigating social biases embedded in the major Chinese search engine, Baidu, and two leading LLMs, Ernie and Qwen. Leveraging a dataset of 240 social groups across 13 categories describing Chinese society, we collect over 30k views encoded in the aforementioned tools by prompting them for candidate words describing such groups. We find that language models exhibit a larger variety of embedded views compared to the search engine, although Baidu and Qwen generate negative content more often than Ernie. We also find a moderate prevalence of stereotypes embedded in the language models, many of which potentially promote offensive and derogatory views. Our work highlights the importance of promoting fairness and inclusivity in AI technologies with a global perspective.

**Link**: [arxiv](http://arxiv.org/abs/2408.15696v1),  [pdf](http://arxiv.org/pdf/2408.15696v1)

**Tags**: cs.CY 



### Deciphering the Impact of Pretraining Data on Large Language Models   through Machine Unlearning
**Authors**: Yang Zhao, Li Du, Xiao Ding, Kai Xiong, Zhouhao Sun, Jun Shi, Ting Liu, Bing Qin

**Updated**: 2024-08-28T10:39:11Z

**Summary**: Through pretraining on a corpus with various sources, Large Language Models (LLMs) have gained impressive performance. However, the impact of each component of the pretraining corpus remains opaque. As a result, the organization of the pretraining corpus is still empirical and may deviate from the optimal. To address this issue, we systematically analyze the impact of 48 datasets from 5 major categories of pretraining data of LLMs and measure their impacts on LLMs using benchmarks about nine major categories of model capabilities. Our analyses provide empirical results about the contribution of multiple corpora on the performances of LLMs, along with their joint impact patterns, including complementary, orthogonal, and correlational relationships. We also identify a set of ``high-impact data'' such as Books that is significantly related to a set of model capabilities. These findings provide insights into the organization of data to support more efficient pretraining of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2402.11537v3),  [pdf](http://arxiv.org/pdf/2402.11537v3)

**Tags**: cs.CL cs.AI 



### Training-Free Action Recognition and Goal Inference with Dynamic Frame   Selection
**Authors**: Ee Yeo Keat, Zhang Hao, Alexander Matyasko, Basura Fernando

**Updated**: 2024-08-28T09:48:24Z

**Summary**: We introduce VidTFS, a Training-free, open-vocabulary video goal and action inference framework that combines the frozen vision foundational model (VFM) and large language model (LLM) with a novel dynamic Frame Selection module. Our experiments demonstrate that the proposed frame selection module improves the performance of the framework significantly. We validate the performance of the proposed VidTFS on four widely used video datasets, including CrossTask, COIN, UCF101, and ActivityNet, covering goal inference and action recognition tasks under open-vocabulary settings without requiring any training or fine-tuning. The results show that VidTFS outperforms pretrained and instruction-tuned multimodal language models that directly stack LLM and VFM for downstream video inference tasks. Our VidTFS with its adaptability shows the future potential for generalizing to new training-free video inference tasks.

**Link**: [arxiv](http://arxiv.org/abs/2401.12471v2),  [pdf](http://arxiv.org/pdf/2401.12471v2)

**Tags**: cs.CV 



### Adaptive Weighted Random Isolation (AWRI): a simple design to estimate   causal effects under network interference
**Authors**: Changhao Shi, Haoyu Yang, Yichen Qin, Yang Li

**Updated**: 2024-08-28T09:46:46Z

**Summary**: Recently, causal inference under interference has gained increasing attention in the literature. In this paper, we focus on randomized designs for estimating the total treatment effect (TTE), defined as the average difference in potential outcomes between fully treated and fully controlled groups. We propose a simple design called weighted random isolation (WRI) along with a restricted difference-in-means estimator (RDIM) for TTE estimation. Additionally, we derive a novel mean squared error surrogate for the RDIM estimator, supported by a network-adaptive weight selection algorithm. This can help us determine a fair weight for the WRI design, thereby effectively reducing the bias. Our method accommodates directed networks, extending previous frameworks. Extensive simulations demonstrate that the proposed method outperforms nine established methods across a wide range of scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2408.15670v1),  [pdf](http://arxiv.org/pdf/2408.15670v1)

**Tags**: stat.ME 



### StyleRemix: Interpretable Authorship Obfuscation via Distillation and   Perturbation of Style Elements
**Authors**: Jillian Fisher, Skyler Hallinan, Ximing Lu, Mitchell Gordon, Zaid Harchaoui, Yejin Choi

**Updated**: 2024-08-28T09:35:15Z

**Summary**: Authorship obfuscation, rewriting a text to intentionally obscure the identity of the author, is an important but challenging task. Current methods using large language models (LLMs) lack interpretability and controllability, often ignoring author-specific stylistic features, resulting in less robust performance overall.   To address this, we develop StyleRemix, an adaptive and interpretable obfuscation method that perturbs specific, fine-grained style elements of the original input text. StyleRemix uses pre-trained Low Rank Adaptation (LoRA) modules to rewrite an input specifically along various stylistic axes (e.g., formality and length) while maintaining low computational cost. StyleRemix outperforms state-of-the-art baselines and much larger LLMs in a variety of domains as assessed by both automatic and human evaluation.   Additionally, we release AuthorMix, a large set of 30K high-quality, long-form texts from a diverse set of 14 authors and 4 domains, and DiSC, a parallel corpus of 1,500 texts spanning seven style axes in 16 unique directions

**Link**: [arxiv](http://arxiv.org/abs/2408.15666v1),  [pdf](http://arxiv.org/pdf/2408.15666v1)

**Tags**: cs.CL 



### EffectiveASR: A Single-Step Non-Autoregressive Mandarin Speech   Recognition Architecture with High Accuracy and Inference Speed
**Authors**: Ziyang Zhuang, Chenfeng Miao, Kun Zou, Ming Fang, Tao Wei, Zijian Li, Ning Cheng, Wei Hu, Shaojun Wang, Jing Xiao

**Updated**: 2024-08-28T09:30:26Z

**Summary**: Non-autoregressive (NAR) automatic speech recognition (ASR) models predict tokens independently and simultaneously, bringing high inference speed. However, there is still a gap in the accuracy of the NAR models compared to the autoregressive (AR) models. In this paper, we propose a single-step NAR ASR architecture with high accuracy and inference speed, called EffectiveASR. It uses an Index Mapping Vector (IMV) based alignment generator to generate alignments during training, and an alignment predictor to learn the alignments for inference. It can be trained end-to-end (E2E) with cross-entropy loss combined with alignment loss. The proposed EffectiveASR achieves competitive results on the AISHELL-1 and AISHELL-2 Mandarin benchmarks compared to the leading models. Specifically, it achieves character error rates (CER) of 4.26%/4.62% on the AISHELL-1 dev/test dataset, which outperforms the AR Conformer with about 30x inference speedup.

**Link**: [arxiv](http://arxiv.org/abs/2406.08835v3),  [pdf](http://arxiv.org/pdf/2406.08835v3)

**Tags**: cs.SD eess.AS 



### Merging and Splitting Diffusion Paths for Semantically Coherent   Panoramas
**Authors**: Fabio Quattrini, Vittorio Pippi, Silvia Cascianelli, Rita Cucchiara

**Updated**: 2024-08-28T09:22:32Z

**Summary**: Diffusion models have become the State-of-the-Art for text-to-image generation, and increasing research effort has been dedicated to adapting the inference process of pretrained diffusion models to achieve zero-shot capabilities. An example is the generation of panorama images, which has been tackled in recent works by combining independent diffusion paths over overlapping latent features, which is referred to as joint diffusion, obtaining perceptually aligned panoramas. However, these methods often yield semantically incoherent outputs and trade-off diversity for uniformity. To overcome this limitation, we propose the Merge-Attend-Diffuse operator, which can be plugged into different types of pretrained diffusion models used in a joint diffusion setting to improve the perceptual and semantical coherence of the generated panorama images. Specifically, we merge the diffusion paths, reprogramming self- and cross-attention to operate on the aggregated latent space. Extensive quantitative and qualitative experimental analysis, together with a user study, demonstrate that our method maintains compatibility with the input prompt and visual quality of the generated images while increasing their semantic coherence. We release the code at https://github.com/aimagelab/MAD.

**Link**: [arxiv](http://arxiv.org/abs/2408.15660v1),  [pdf](http://arxiv.org/pdf/2408.15660v1)

**Tags**: cs.CV 



### An Empirical Study on Self-correcting Large Language Models for Data   Science Code Generation
**Authors**: Thai Tang Quoc, Duc Ha Minh, Tho Quan Thanh, Anh Nguyen-Duc

**Updated**: 2024-08-28T09:19:09Z

**Summary**: Large Language Models (LLMs) have recently advanced many applications on software engineering tasks, particularly the potential for code generation. Among contemporary challenges, code generated by LLMs often suffers from inaccuracies and hallucinations, requiring external inputs to correct. One recent strategy to fix these issues is to refine the code generated from LLMs using the input from the model itself (self-augmented). In this work, we proposed a novel method, namely CoT-SelfEvolve. CoT-SelfEvolve iteratively and automatically refines code through a self-correcting process, guided by a chain of thought constructed from real-world programming problem feedback. Focusing on data science code, including Python libraries such as NumPy and Pandas, our evaluations on the DS-1000 dataset demonstrate that CoT-SelfEvolve significantly outperforms existing models in solving complex problems. The framework shows substantial improvements in both initial code generation and subsequent iterations, with the model's accuracy increasing significantly with each additional iteration. This highlights the effectiveness of using chain-of-thought prompting to address complexities revealed by program executor traceback error messages. We also discuss how CoT-SelfEvolve can be integrated into continuous software engineering environments, providing a practical solution for improving LLM-based code generation.

**Link**: [arxiv](http://arxiv.org/abs/2408.15658v1),  [pdf](http://arxiv.org/pdf/2408.15658v1)

**Tags**: cs.SE cs.AI 



### Evidential Deep Partial Multi-View Classification With Discount Fusion
**Authors**: Haojian Huang, Zhe Liu, Sukumar Letchmunan, Muhammet Deveci, Mingwei Lin, Weizhong Wang

**Updated**: 2024-08-28T09:18:00Z

**Summary**: Incomplete multi-view data classification poses significant challenges due to the common issue of missing views in real-world scenarios. Despite advancements, existing methods often fail to provide reliable predictions, largely due to the uncertainty of missing views and the inconsistent quality of imputed data. To tackle these problems, we propose a novel framework called Evidential Deep Partial Multi-View Classification (EDP-MVC). Initially, we use K-means imputation to address missing views, creating a complete set of multi-view data. However, the potential conflicts and uncertainties within this imputed data can affect the reliability of downstream inferences. To manage this, we introduce a Conflict-Aware Evidential Fusion Network (CAEFN), which dynamically adjusts based on the reliability of the evidence, ensuring trustworthy discount fusion and producing reliable inference outcomes. Comprehensive experiments on various benchmark datasets reveal EDP-MVC not only matches but often surpasses the performance of state-of-the-art methods.

**Link**: [arxiv](http://arxiv.org/abs/2408.13123v2),  [pdf](http://arxiv.org/pdf/2408.13123v2)

**Tags**: cs.CV 



### Urdu Digital Text Word Optical Character Recognition Using Permuted Auto   Regressive Sequence Modeling
**Authors**: Ahmed Mustafa, Muhammad Tahir Rafique, Muhammad Ijlal Baig, Hasan Sajid, Muhammad Jawad Khan, Karam Dad Kallu

**Updated**: 2024-08-28T09:11:55Z

**Summary**: This research paper presents a novel word-level Optical Character Recognition (OCR) model developed specifically for digital Urdu text. The model utilizes transformer-based architectures and attention mechanisms to address the unique challenges of recognizing Urdu script, which includes handling a diverse range of text styles, fonts, and variations. Trained on a comprehensive dataset of approximately 160,000 Urdu text images, the model incorporates a permuted autoregressive sequence (PARSeq) architecture. This design enables context-aware inference and iterative refinement by leveraging bidirectional context information, significantly enhancing its ability to accurately recognize Urdu characters. The model achieves a character error rate (CER) of 0.178, highlighting its effectiveness and precision in real-world applications. However, the model has some limitations, such as difficulties with blurred images, non-horizontal orientations, and the presence of trailing punctuation marks, which can introduce noise into the recognition process. Addressing these challenges will be a key focus of future work. Future research will aim to further refine the model through advanced data augmentation techniques, optimization of hyperparameters, and the integration of context-aware language models, ultimately enhancing the model's performance and robustness in Urdu text recognition.

**Link**: [arxiv](http://arxiv.org/abs/2408.15119v2),  [pdf](http://arxiv.org/pdf/2408.15119v2)

**Tags**: cs.CV cs.AI 



### Hierarchical Blockmodelling for Knowledge Graphs
**Authors**: Marcin Pietrasik, Marek Reformat, Anna Wilbik

**Updated**: 2024-08-28T09:04:15Z

**Summary**: In this paper, we investigate the use of probabilistic graphical models, specifically stochastic blockmodels, for the purpose of hierarchical entity clustering on knowledge graphs. These models, seldom used in the Semantic Web community, decompose a graph into a set of probability distributions. The parameters of these distributions are then inferred allowing for their subsequent sampling to generate a random graph. In a non-parametric setting, this allows for the induction of hierarchical clusterings without prior constraints on the hierarchy's structure. Specifically, this is achieved by the integration of the Nested Chinese Restaurant Process and the Stick Breaking Process into the generative model. In this regard, we propose a model leveraging such integration and derive a collapsed Gibbs sampling scheme for its inference. To aid in understanding, we describe the steps in this derivation and provide an implementation for the sampler. We evaluate our model on synthetic and real-world datasets and quantitatively compare against benchmark models. We further evaluate our results qualitatively and find that our model is capable of inducing coherent cluster hierarchies in small scale settings. The work presented in this paper provides the first step for the further application of stochastic blockmodels for knowledge graphs on a larger scale. We conclude the paper with potential avenues for future work on more scalable inference schemes.

**Link**: [arxiv](http://arxiv.org/abs/2408.15649v1),  [pdf](http://arxiv.org/pdf/2408.15649v1)

**Tags**: cs.AI 



### TokenPacker: Efficient Visual Projector for Multimodal LLM
**Authors**: Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jie Qin, Jianke Zhu, Lei Zhang

**Updated**: 2024-08-28T08:49:57Z

**Summary**: The visual projector serves as an essential bridge between the visual encoder and the Large Language Model (LLM) in a Multimodal LLM (MLLM). Typically, MLLMs adopt a simple MLP to preserve all visual contexts via one-to-one transformation. However, the visual tokens are redundant and can be considerably increased when dealing with high-resolution images, impairing the efficiency of MLLMs significantly. Some recent works have introduced resampler or abstractor to reduce the number of resulting visual tokens. Unfortunately, they fail to capture finer details and undermine the visual reasoning capabilities of MLLMs. In this work, we propose a novel visual projector, which adopts a coarse-to-fine scheme to inject the enriched characteristics to generate the condensed visual tokens. In specific, we first interpolate the visual features as a low-resolution point query, providing the overall visual representation as the foundation. Then, we introduce a region-to-point injection module that utilizes high-resolution, multi-level region-based cues as fine-grained reference keys and values, allowing them to be fully absorbed within the corresponding local context region. This step effectively updates the coarse point query, transforming it into an enriched one for the subsequent LLM reasoning. Extensive experiments demonstrate that our approach compresses the visual tokens by 75%~89%, while achieves comparable or even better performance across diverse benchmarks with significantly higher efficiency. The source codes can be found at https://github.com/CircleRadon/TokenPacker.

**Link**: [arxiv](http://arxiv.org/abs/2407.02392v4),  [pdf](http://arxiv.org/pdf/2407.02392v4)

**Tags**: cs.CV 



### Large Language Model Sentinel: LLM Agent for Adversarial Purification
**Authors**: Guang Lin, Qibin Zhao

**Updated**: 2024-08-28T08:46:17Z

**Summary**: Over the past two years, the use of large language models (LLMs) has advanced rapidly. While these LLMs offer considerable convenience, they also raise security concerns, as LLMs are vulnerable to adversarial attacks by some well-designed textual perturbations. In this paper, we introduce a novel defense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is designed to enhance the adversarial robustness of LLMs by purifying the adversarial textual examples before feeding them into the target LLM. Our method comprises two main components: a) Agent instruction, which can simulate a new agent for adversarial defense, altering minimal characters to maintain the original meaning of the sentence while defending against attacks; b) Defense guidance, which provides strategies for modifying clean or adversarial examples to ensure effective defense and accurate outputs from the target LLMs. Remarkably, the defense agent demonstrates robust defensive capabilities even without learning from adversarial examples. Additionally, we conduct an intriguing adversarial experiment where we develop two agents, one for defense and one for attack, and engage them in mutual confrontation. During the adversarial interactions, neither agent completely beat the other. Extensive experiments on both open-source and closed-source LLMs demonstrate that our method effectively defends against adversarial attacks, thereby enhancing adversarial robustness.

**Link**: [arxiv](http://arxiv.org/abs/2405.20770v3),  [pdf](http://arxiv.org/pdf/2405.20770v3)

**Tags**: cs.CL cs.AI cs.CR 



### DocLayLLM: An Efficient and Effective Multi-modal Extension of Large   Language Models for Text-rich Document Understanding
**Authors**: Wenhui Liao, Jiapeng Wang, Hongliang Li, Chengyu Wang, Jun Huang, Lianwen Jin

**Updated**: 2024-08-28T08:32:44Z

**Summary**: Text-rich document understanding (TDU) refers to analyzing and comprehending documents containing substantial textual content. With the rapid evolution of large language models (LLMs), they have been widely leveraged for TDU due to their remarkable versatility and generalization. In this paper, we introduce DocLayLLM, an efficient and effective multi-modal extension of LLMs specifically designed for TDU. By integrating visual patch tokens and 2D positional tokens into LLMs and encoding the document content using the LLMs themselves, we fully take advantage of the document comprehension capability of LLMs and enhance their perception of OCR information. We have also deeply considered the role of the chain-of-thought (CoT) and innovatively proposed the techniques of CoT Pre-training and CoT Annealing. Our DocLayLLM can achieve remarkable performances with lightweight training settings, showcasing its efficiency and effectiveness. Experimental results demonstrate that our DocLayLLM surpasses existing OCR-dependent methods and also outperforms OCR-free competitors.

**Link**: [arxiv](http://arxiv.org/abs/2408.15045v2),  [pdf](http://arxiv.org/pdf/2408.15045v2)

**Tags**: cs.CV 



### CodeSift: An LLM-Based Reference-Less Framework for Automatic Code   Validation
**Authors**: Pooja Aggarwal, Oishik Chatterjee, Ting Dai, Prateeti Mohapatra, Brent Paulovicks, Brad Blancett, Arthur De Magalhaes

**Updated**: 2024-08-28T08:32:21Z

**Summary**: The advent of large language models (LLMs) has greatly facilitated code generation, but ensuring the functional correctness of generated code remains a challenge. Traditional validation methods are often time-consuming, error-prone, and impractical for large volumes of code. We introduce CodeSift, a novel framework that leverages LLMs as the first-line filter of code validation without the need for execution, reference code, or human feedback, thereby reducing the validation effort. We assess the effectiveness of our method across three diverse datasets encompassing two programming languages. Our results indicate that CodeSift outperforms state-of-the-art code evaluation methods. Internal testing conducted with subject matter experts reveals that the output generated by CodeSift is in line with human preference, reinforcing its effectiveness as a dependable automated code validation tool.

**Link**: [arxiv](http://arxiv.org/abs/2408.15630v1),  [pdf](http://arxiv.org/pdf/2408.15630v1)

**Tags**: cs.SE cs.AI 



### CBF-LLM: Safe Control for LLM Alignment
**Authors**: Yuya Miyaoka, Masaki Inoue

**Updated**: 2024-08-28T08:25:22Z

**Summary**: This paper proposes a control-based framework for aligning large language models (LLMs) by leveraging a control barrier function (CBF) to ensure user-desirable text generation. The presented framework applies the safety filter, designed based on the CBF, to the output generation of the baseline LLM, i.e., the sequence of the token, with the aim of intervening in the generated text. The overall text-generation system is implemented with Llama 3 and a RoBERTa model, and the source code is available at https://github.com/Mya-Mya/CBF-LLM. The experiment demonstrates its control ability and effectiveness in reducing the number of interventions needed for user-specified alignment tasks.

**Link**: [arxiv](http://arxiv.org/abs/2408.15625v1),  [pdf](http://arxiv.org/pdf/2408.15625v1)

**Tags**: eess.SY cs.AI cs.CL cs.SY 



### AI-native Memory: A Pathway from LLMs Towards AGI
**Authors**: Jingbo Shang, Zai Zheng, Jiale Wei, Xiang Ying, Felix Tao, Mindverse Team

**Updated**: 2024-08-28T08:07:49Z

**Summary**: Large language models (LLMs) have demonstrated the world with the sparks of artificial general intelligence (AGI). One opinion, especially from some startups working on LLMs, argues that an LLM with nearly unlimited context length can realize AGI. However, they might be too optimistic about the long-context capability of (existing) LLMs -- (1) Recent literature has shown that their effective context length is significantly smaller than their claimed context length; and (2) Our reasoning-in-a-haystack experiments further demonstrate that simultaneously finding the relevant information from a long context and conducting (simple) reasoning is nearly impossible. In this paper, we envision a pathway from LLMs to AGI through the integration of \emph{memory}. We believe that AGI should be a system where LLMs serve as core processors. In addition to raw data, the memory in this system would store a large number of important conclusions derived from reasoning processes. Compared with retrieval-augmented generation (RAG) that merely processing raw data, this approach not only connects semantically related information closer, but also simplifies complex inferences at the time of querying. As an intermediate stage, the memory will likely be in the form of natural language descriptions, which can be directly consumed by users too. Ultimately, every agent/person should have its own large personal model, a deep neural network model (thus \emph{AI-native}) that parameterizes and compresses all types of memory, even the ones cannot be described by natural languages. Finally, we discuss the significant potential of AI-native memory as the transformative infrastructure for (proactive) engagement, personalization, distribution, and social in the AGI era, as well as the incurred privacy and security challenges with preliminary solutions.

**Link**: [arxiv](http://arxiv.org/abs/2406.18312v4),  [pdf](http://arxiv.org/pdf/2406.18312v4)

**Tags**: cs.CL cs.AI 



### Boost Your NeRF: A Model-Agnostic Mixture of Experts Framework for High   Quality and Efficient Rendering
**Authors**: Francesco Di Sario, Riccardo Renzulli, Enzo Tartaglione, Marco Grangetto

**Updated**: 2024-08-28T07:49:14Z

**Summary**: Since the introduction of NeRFs, considerable attention has been focused on improving their training and inference times, leading to the development of Fast-NeRFs models. Despite demonstrating impressive rendering speed and quality, the rapid convergence of such models poses challenges for further improving reconstruction quality. Common strategies to improve rendering quality involves augmenting model parameters or increasing the number of sampled points. However, these computationally intensive approaches encounter limitations in achieving significant quality enhancements. This study introduces a model-agnostic framework inspired by Sparsely-Gated Mixture of Experts to enhance rendering quality without escalating computational complexity. Our approach enables specialization in rendering different scene components by employing a mixture of experts with varying resolutions. We present a novel gate formulation designed to maximize expert capabilities and propose a resolution-based routing technique to effectively induce sparsity and decompose scenes. Our work significantly improves reconstruction quality while maintaining competitive performance.

**Link**: [arxiv](http://arxiv.org/abs/2407.10389v2),  [pdf](http://arxiv.org/pdf/2407.10389v2)

**Tags**: cs.CV 



### Heterogeneous Clinical Trial Outcomes via Multi-Output Gaussian   Processes
**Authors**: Owen Thomas, Leiv R√∏nneberg

**Updated**: 2024-08-28T07:36:28Z

**Summary**: We make use of Kronecker structure for scaling Gaussian Process models to large-scale, heterogeneous, clinical data sets. Repeated measures, commonly performed in clinical research, facilitate computational acceleration for nonlinear Bayesian nonparametric models and enable exact sampling for non-conjugate inference, when combinations of continuous and discrete endpoints are observed. Model inference is performed in Stan, and comparisons are made with brms on simulated data and two real clinical data sets, following a radiological image quality theme. Scalable Gaussian Process models compare favourably with parametric models on real data sets with 17,460 observations. Different GP model specifications are explored, with components analogous to random effects, and their theoretical properties are described.

**Link**: [arxiv](http://arxiv.org/abs/2407.13283v2),  [pdf](http://arxiv.org/pdf/2407.13283v2)

**Tags**: stat.ME stat.AP 



### VFLIP: A Backdoor Defense for Vertical Federated Learning via   Identification and Purification
**Authors**: Yungi Cho, Woorim Han, Miseon Yu, Younghan Lee, Ho Bae, Yunheung Paek

**Updated**: 2024-08-29T02:01:56Z

**Summary**: Vertical Federated Learning (VFL) focuses on handling vertically partitioned data over FL participants. Recent studies have discovered a significant vulnerability in VFL to backdoor attacks which specifically target the distinct characteristics of VFL. Therefore, these attacks may neutralize existing defense mechanisms designed primarily for Horizontal Federated Learning (HFL) and deep neural networks. In this paper, we present the first backdoor defense, called VFLIP, specialized for VFL. VFLIP employs the identification and purification techniques that operate at the inference stage, consequently improving the robustness against backdoor attacks to a great extent. VFLIP first identifies backdoor-triggered embeddings by adopting a participant-wise anomaly detection approach. Subsequently, VFLIP conducts purification which removes the embeddings identified as malicious and reconstructs all the embeddings based on the remaining embeddings. We conduct extensive experiments on CIFAR10, CINIC10, Imagenette, NUS-WIDE, and BankMarketing to demonstrate that VFLIP can effectively mitigate backdoor attacks in VFL. https://github.com/blingcho/VFLIP-esorics24

**Link**: [arxiv](http://arxiv.org/abs/2408.15591v2),  [pdf](http://arxiv.org/pdf/2408.15591v2)

**Tags**: cs.LG 



### NOVUM: Neural Object Volumes for Robust Object Classification
**Authors**: Artur Jesslen, Guofeng Zhang, Angtian Wang, Wufei Ma, Alan Yuille, Adam Kortylewski

**Updated**: 2024-08-28T07:28:15Z

**Summary**: Discriminative models for object classification typically learn image-based representations that do not capture the compositional and 3D nature of objects. In this work, we show that explicitly integrating 3D compositional object representations into deep networks for image classification leads to a largely enhanced generalization in out-of-distribution scenarios. In particular, we introduce a novel architecture, referred to as NOVUM, that consists of a feature extractor and a neural object volume for every target object class. Each neural object volume is a composition of 3D Gaussians that emit feature vectors. This compositional object representation allows for a highly robust and fast estimation of the object class by independently matching the features of the 3D Gaussians of each category to features extracted from an input image. Additionally, the object pose can be estimated via inverse rendering of the corresponding neural object volume. To enable the classification of objects, the neural features at each 3D Gaussian are trained discriminatively to be distinct from (i) the features of 3D Gaussians in other categories, (ii) features of other 3D Gaussians of the same object, and (iii) the background features. Our experiments show that NOVUM offers intriguing advantages over standard architectures due to the 3D compositional structure of the object representation, namely: (1) An exceptional robustness across a spectrum of real-world and synthetic out-of-distribution shifts and (2) an enhanced human interpretability compared to standard models, all while maintaining real-time inference and a competitive accuracy on in-distribution data.

**Link**: [arxiv](http://arxiv.org/abs/2305.14668v4),  [pdf](http://arxiv.org/pdf/2305.14668v4)

**Tags**: cs.CV 



### SimpleSpeech 2: Towards Simple and Efficient Text-to-Speech with   Flow-based Scalar Latent Transformer Diffusion Models
**Authors**: Dongchao Yang, Rongjie Huang, Yuanyuan Wang, Haohan Guo, Dading Chong, Songxiang Liu, Xixin Wu, Helen Meng

**Updated**: 2024-08-28T07:16:37Z

**Summary**: Scaling Text-to-speech (TTS) to large-scale datasets has been demonstrated as an effective method for improving the diversity and naturalness of synthesized speech. At the high level, previous large-scale TTS models can be categorized into either Auto-regressive (AR) based (\textit{e.g.}, VALL-E) or Non-auto-regressive (NAR) based models (\textit{e.g.}, NaturalSpeech 2/3). Although these works demonstrate good performance, they still have potential weaknesses. For instance, AR-based models are plagued by unstable generation quality and slow generation speed; meanwhile, some NAR-based models need phoneme-level duration alignment information, thereby increasing the complexity of data pre-processing, model design, and loss design. In this work, we build upon our previous publication by implementing a simple and efficient non-autoregressive (NAR) TTS framework, termed SimpleSpeech 2. SimpleSpeech 2 effectively combines the strengths of both autoregressive (AR) and non-autoregressive (NAR) methods, offering the following key advantages: (1) simplified data preparation; (2) straightforward model and loss design; and (3) stable, high-quality generation performance with fast inference speed. Compared to our previous publication, we present ({\romannumeral1}) a detailed analysis of the influence of speech tokenizer and noisy label for TTS performance; ({\romannumeral2}) four distinct types of sentence duration predictors; ({\romannumeral3}) a novel flow-based scalar latent transformer diffusion model. With these improvement, we show a significant improvement in generation performance and generation speed compared to our previous work and other state-of-the-art (SOTA) large-scale TTS models. Furthermore, we show that SimpleSpeech 2 can be seamlessly extended to multilingual TTS by training it on multilingual speech datasets. Demos are available on: {https://dongchaoyang.top/SimpleSpeech2\_demo/}.

**Link**: [arxiv](http://arxiv.org/abs/2408.13893v2),  [pdf](http://arxiv.org/pdf/2408.13893v2)

**Tags**: cs.SD cs.CL eess.AS 



### Spectral Masking with Explicit Time-Context Windowing for Neural   Network-Based Monaural Speech Enhancement
**Authors**: Luan Vin√≠cius Fiorio, Boris Karanov, Bruno Defraene, Johan David, Wim van Houtum, Frans Widdershoven, Ronald M. Aarts

**Updated**: 2024-08-28T07:08:09Z

**Summary**: We propose and analyze the use of an explicit time-context window for neural network-based spectral masking speech enhancement to leverage signal context dependencies between neighboring frames. In particular, we concentrate on soft masking and loss computed on the time-frequency representation of the reconstructed speech. We show that the application of a time-context windowing function at both input and output of the neural network model improves the soft mask estimation process by combining multiple estimates taken from different contexts. The proposed approach is only applied as post-optimization in inference mode, not requiring additional layers or special training for the neural network model. Our results show that the method consistently increases both intelligibility and signal quality of the denoised speech, as demonstrated for two classes of convolutional-based speech enhancement models. Importantly, the proposed method requires only a negligible ($\leq1\%$) increase in the number of model parameters, making it suitable for hardware-constrained applications.

**Link**: [arxiv](http://arxiv.org/abs/2408.15582v1),  [pdf](http://arxiv.org/pdf/2408.15582v1)

**Tags**: eess.AS cs.SD 



### SIaM: Self-Improving Code-Assisted Mathematical Reasoning of Large   Language Models
**Authors**: Dian Yu, Baolin Peng, Ye Tian, Linfeng Song, Haitao Mi, Dong Yu

**Updated**: 2024-08-28T06:33:03Z

**Summary**: There is a growing trend of teaching large language models (LLMs) to solve mathematical problems through coding. Existing studies primarily focus on prompting powerful, closed-source models to generate seed training data followed by in-domain data augmentation, equipping LLMs with considerable capabilities for code-aided mathematical reasoning. However, continually training these models on augmented data derived from a few datasets such as GSM8K may impair their generalization abilities and restrict their effectiveness to a narrow range of question types. Conversely, the potential of improving such LLMs by leveraging large-scale, expert-written, diverse math question-answer pairs remains unexplored. To utilize these resources and tackle unique challenges such as code response assessment, we propose a novel paradigm that uses a code-based critic model to guide steps including question-code data construction, quality control, and complementary evaluation. We also explore different alignment algorithms with self-generated instruction/preference data to foster continuous improvement. Experiments across both in-domain (up to +5.7%) and out-of-domain (+4.4%) benchmarks in English and Chinese demonstrate the effectiveness of the proposed paradigm.

**Link**: [arxiv](http://arxiv.org/abs/2408.15565v1),  [pdf](http://arxiv.org/pdf/2408.15565v1)

**Tags**: cs.CL 



### Boosting Lossless Speculative Decoding via Feature Sampling and Partial   Alignment Distillation
**Authors**: Lujun Gui, Bin Xiao, Lei Su, Weipeng Chen

**Updated**: 2024-08-28T06:28:01Z

**Summary**: Lossless speculative decoding accelerates target large language model (LLM) inference by employing a lightweight draft model for generating tree-structured candidates, which are subsequently verified in parallel by the target LLM. Currently, effective approaches leverage feature-level rather than token-level autoregression within the draft model to facilitate more straightforward predictions and enhanced knowledge distillation. In this paper, we reassess these approaches and propose FSPAD (Feature Sampling and Partial Alignment Distillation for Lossless Speculative Decoding), which introduces two straightforward and effective components within the existing framework to boost lossless speculative decoding. Firstly, FSPAD utilizes token embeddings to sample features of the target LLM in high-dimensional space before feeding them into the draft model, due to the inherent uncertainty of the features preventing the draft model from obtaining the specific token output by the target LLM. Secondly, FSPAD introduces partial alignment distillation to weaken the draft model's connection between features and logits, aiming to reduce the conflict between feature alignment and logit confidence during training. Our experiments include both greedy and non-greedy decoding on the largest and smallest models from the Vicuna and LLaMA3-Instruct series, as well as tasks in multi-turn conversation, translation, summarization, question answering, mathematical reasoning, and retrieval-augmented generation. The results show that FSPAD outperforms the state-of-the-art method across all the aforementioned tasks and target LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2408.15562v1),  [pdf](http://arxiv.org/pdf/2408.15562v1)

**Tags**: cs.CL cs.LG 



### Regularity for a class of degenerate fully nonlinear nonlocal elliptic   equations
**Authors**: Yuzhou Fang, Vicentiu D. Radulescu, Chao Zhang

**Updated**: 2024-08-28T06:20:06Z

**Summary**: We consider a wide class of fully nonlinear integro-differential equations that degenerate when the gradient of the solution vanishes. By using compactness and perturbation arguments, we give a complete characterization of the regularity of viscosity solutions according to different diffusion orders. More precisely, when the order of the fractional diffusion is sufficiently close to 2, we obtain H\"{o}lder continuity for the gradient of any viscosity solutions and further derive an improved gradient regularity estimate at the origin. For the order of the fractional diffusion in the interval $(1, 2)$, we prove that there is at least one solution of class $C^{1, \alpha}_{\rm loc}$. Additionally, if the order of the fractional diffusion is in the interval $(0,1]$, the local H\"{o}lder continuity of solutions is inferred.

**Link**: [arxiv](http://arxiv.org/abs/2408.15559v1),  [pdf](http://arxiv.org/pdf/2408.15559v1)

**Tags**: math.AP 



### AnomalyLLM: Few-shot Anomaly Edge Detection for Dynamic Graphs using   Large Language Models
**Authors**: Shuo Liu, Di Yao, Lanting Fang, Zhetao Li, Wenbin Li, Kaiyu Feng, XiaoWen Ji, Jingping Bi

**Updated**: 2024-08-28T06:18:28Z

**Summary**: Detecting anomaly edges for dynamic graphs aims to identify edges significantly deviating from the normal pattern and can be applied in various domains, such as cybersecurity, financial transactions and AIOps. With the evolving of time, the types of anomaly edges are emerging and the labeled anomaly samples are few for each type. Current methods are either designed to detect randomly inserted edges or require sufficient labeled data for model training, which harms their applicability for real-world applications. In this paper, we study this problem by cooperating with the rich knowledge encoded in large language models(LLMs) and propose a method, namely AnomalyLLM. To align the dynamic graph with LLMs, AnomalyLLM pre-trains a dynamic-aware encoder to generate the representations of edges and reprograms the edges using the prototypes of word embeddings. Along with the encoder, we design an in-context learning framework that integrates the information of a few labeled samples to achieve few-shot anomaly detection. Experiments on four datasets reveal that AnomalyLLM can not only significantly improve the performance of few-shot anomaly detection, but also achieve superior results on new anomalies without any update of model parameters.

**Link**: [arxiv](http://arxiv.org/abs/2405.07626v2),  [pdf](http://arxiv.org/pdf/2405.07626v2)

**Tags**: cs.LG cs.AI 



### Geo-Llama: Leveraging LLMs for Human Mobility Trajectory Generation with   Spatiotemporal Constraints
**Authors**: Siyu Li, Toan Tran, Haowen Lin, John Krumm, Cyrus Shahabi, Li Xiong

**Updated**: 2024-08-28T06:16:42Z

**Summary**: Simulating human mobility data is essential for various application domains, including transportation, urban planning, and epidemic control, since real data are often inaccessible to researchers due to expensive costs and privacy issues. Several existing deep generative solutions propose learning from real trajectories to generate synthetic ones. Despite the progress, most of them suffer from training stability issues and scale poorly with growing data size. More importantly, they generally lack control mechanisms to steer the generated trajectories based on spatiotemporal constraints such as fixing specific visits. To address such limitations, we formally define the controlled trajectory generation problem with spatiotemporal constraints and propose Geo-Llama. This novel LLM-inspired framework enforces explicit visit constraints in a contextually coherent way. It fine-tunes pre-trained LLMs on trajectories with a visit-wise permutation strategy where each visit corresponds to a time and location. This enables the model to capture the spatiotemporal patterns regardless of visit orders and allows flexible and in-context constraint integration through prompts during generation. Extensive experiments on real-world and synthetic datasets validate the effectiveness of Geo-Llama, demonstrating its versatility and robustness in handling a broad range of constraints to generate more realistic trajectories compared to existing methods.

**Link**: [arxiv](http://arxiv.org/abs/2408.13918v2),  [pdf](http://arxiv.org/pdf/2408.13918v2)

**Tags**: cs.AI 



### WildFeedback: Aligning LLMs With In-situ User Interactions And Feedback
**Authors**: Taiwei Shi, Zhuoer Wang, Longqi Yang, Ying-Chun Lin, Zexue He, Mengting Wan, Pei Zhou, Sujay Jauhar, Xiaofeng Xu, Xia Song, Jennifer Neville

**Updated**: 2024-08-28T05:53:46Z

**Summary**: As large language models (LLMs) continue to advance, aligning these models with human preferences has emerged as a critical challenge. Traditional alignment methods, relying on human or LLM annotated datasets, are limited by their resource-intensive nature, inherent subjectivity, and the risk of feedback loops that amplify model biases. To overcome these limitations, we introduce WildFeedback, a novel framework that leverages real-time, in-situ user interactions to create preference datasets that more accurately reflect authentic human values. WildFeedback operates through a three-step process: feedback signal identification, preference data construction, and user-guided evaluation. We applied this framework to a large corpus of user-LLM conversations, resulting in a rich preference dataset that reflects genuine user preferences. This dataset captures the nuances of user preferences by identifying and classifying feedback signals within natural conversations, thereby enabling the construction of more representative and context-sensitive alignment data. Our extensive experiments demonstrate that LLMs fine-tuned on WildFeedback exhibit significantly improved alignment with user preferences, as evidenced by both traditional benchmarks and our proposed user-guided evaluation. By incorporating real-time feedback from actual users, WildFeedback addresses the scalability, subjectivity, and bias challenges that plague existing approaches, marking a significant step toward developing LLMs that are more responsive to the diverse and evolving needs of their users. In summary, WildFeedback offers a robust, scalable solution for aligning LLMs with true human values, setting a new standard for the development and evaluation of user-centric language models.

**Link**: [arxiv](http://arxiv.org/abs/2408.15549v1),  [pdf](http://arxiv.org/pdf/2408.15549v1)

**Tags**: cs.CL 



### ConsistencyTrack: A Robust Multi-Object Tracker with a Generation   Strategy of Consistency Model
**Authors**: Lifan Jiang, Zhihui Wang, Siqi Yin, Guangxiao Ma, Peng Zhang, Boxi Wu

**Updated**: 2024-08-28T05:53:30Z

**Summary**: Multi-object tracking (MOT) is a critical technology in computer vision, designed to detect multiple targets in video sequences and assign each target a unique ID per frame. Existed MOT methods excel at accurately tracking multiple objects in real-time across various scenarios. However, these methods still face challenges such as poor noise resistance and frequent ID switches. In this research, we propose a novel ConsistencyTrack, joint detection and tracking(JDT) framework that formulates detection and association as a denoising diffusion process on perturbed bounding boxes. This progressive denoising strategy significantly improves the model's noise resistance. During the training phase, paired object boxes within two adjacent frames are diffused from ground-truth boxes to a random distribution, and then the model learns to detect and track by reversing this process. In inference, the model refines randomly generated boxes into detection and tracking results through minimal denoising steps. ConsistencyTrack also introduces an innovative target association strategy to address target occlusion. Experiments on the MOT17 and DanceTrack datasets demonstrate that ConsistencyTrack outperforms other compared methods, especially better than DiffusionTrack in inference speed and other performance metrics. Our code is available at https://github.com/Tankowa/ConsistencyTrack.

**Link**: [arxiv](http://arxiv.org/abs/2408.15548v1),  [pdf](http://arxiv.org/pdf/2408.15548v1)

**Tags**: cs.CV 



### Redshift evolution of the X-ray and UV luminosity relation of quasars:   calibrated results from SNe Ia
**Authors**: Xiaolei Li, Ryan E. Keeley, Arman Shafieloo

**Updated**: 2024-08-28T05:48:35Z

**Summary**: Quasars could serve as standard candles if the relation between their ultraviolet and X-ray luminosities can be accurately calibrated. Previously, we developed a model-independent method to calibrate quasar standard candles using the distances-redshift relation reconstructed from Type Ia supernova at z<2 using Gaussian process regression. Interestingly, we found that the calibrated quasar standard candle dataset preferred a deviation from $\Lambda$CDM at redshifts above z>2. One interpretation of these findings is that the calibration parameters of the quasar UV-X-ray luminosity relationship evolves with redshift. In order to test the redshift dependence of the quasar calibration in a model-independent manner, we divided the quasar sample whose redshift overlap with the redshift coverage of Pantheon+ Type Ia supernova compilation into two sub-samples: a low-redshift quasar sub-sample and a high-redshift quasar sub-sample. Our present results show that there is about a 4$\sigma$ inconsistency between the quasar parameters inferred from the high-redshift quasar sub-sample and from the low-redshift sub-sample if no evolution of the quasar relation is considered. This inconsistency suggests the necessity of considering redshift evolution for the relationship between the quasars$'$ ultraviolet and X-ray luminosities. We then test an explicit parametrization of the redshift evolution of the quasar calibration parameters via $\gamma(z) = \gamma_0+\gamma_1(1+z)$ and $\beta(z)=\beta_0+\beta_1(1+z)$. Combining this redshift-dependent calibration relationship with the distance-redshift relationship reconstructed from Pantheon+ supernova compilation, we find the high-redshift sub-sample and low-redshift sub-sample become consistent at the 1$\sigma$ level, which means that the parameterized form of $\gamma(z)$ and $\beta(z)$ works well at describing the evolution of the quasar calibration parameters.

**Link**: [arxiv](http://arxiv.org/abs/2408.15547v1),  [pdf](http://arxiv.org/pdf/2408.15547v1)

**Tags**: astro-ph.CO 



### SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding
**Authors**: Sihang Li, Jian Huang, Jiaxi Zhuang, Yaorui Shi, Xiaochen Cai, Mingjun Xu, Xiang Wang, Linfeng Zhang, Guolin Ke, Hengxing Cai

**Updated**: 2024-08-28T05:41:52Z

**Summary**: Scientific literature understanding is crucial for extracting targeted information and garnering insights, thereby significantly advancing scientific discovery. Despite the remarkable success of Large Language Models (LLMs), they face challenges in scientific literature understanding, primarily due to (1) a lack of scientific knowledge and (2) unfamiliarity with specialized scientific tasks.   To develop an LLM specialized in scientific literature understanding, we propose a hybrid strategy that integrates continual pre-training (CPT) and supervised fine-tuning (SFT), to simultaneously infuse scientific domain knowledge and enhance instruction-following capabilities for domain-specific tasks.cIn this process, we identify two key challenges: (1) constructing high-quality CPT corpora, and (2) generating diverse SFT instructions. We address these challenges through a meticulous pipeline, including PDF text extraction, parsing content error correction, quality filtering, and synthetic instruction creation. Applying this strategy, we present a suite of LLMs: SciLitLLM, specialized in scientific literature understanding. These models demonstrate promising performance on scientific literature understanding benchmarks.   Our contributions are threefold: (1) We present an effective framework that integrates CPT and SFT to adapt LLMs to scientific literature understanding, which can also be easily adapted to other domains. (2) We propose an LLM-based synthesis method to generate diverse and high-quality scientific instructions, resulting in a new instruction set -- SciLitIns -- for supervised fine-tuning in less-represented scientific domains. (3) SciLitLLM achieves promising performance improvements on scientific literature understanding benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2408.15545v1),  [pdf](http://arxiv.org/pdf/2408.15545v1)

**Tags**: cs.LG cs.CL 



### Kangaroo: A Powerful Video-Language Model Supporting Long-context Video   Input
**Authors**: Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, Xiaoming Wei, Jianbin Jiao, Enhua Wu, Jie Hu

**Updated**: 2024-08-28T05:34:14Z

**Summary**: Rapid advancements have been made in extending Large Language Models (LLMs) to Large Multi-modal Models (LMMs). However, extending input modality of LLMs to video data remains a challenging endeavor, especially for long videos. Due to insufficient access to large-scale high-quality video data and the excessive compression of visual features, current methods exhibit limitations in effectively processing long videos. In this paper, we introduce Kangaroo, a powerful Video LMM aimed at addressing these challenges. Confronted with issue of inadequate training data, we develop a data curation system to build a large-scale dataset with high-quality annotations for vision-language pre-training and instruction tuning. In addition, we design a curriculum training pipeline with gradually increasing resolution and number of input frames to accommodate long videos. Evaluation results demonstrate that, with 8B parameters, Kangaroo achieves state-of-the-art performance across a variety of video understanding benchmarks while exhibiting competitive results on others. Particularly, on benchmarks specialized for long videos, Kangaroo excels some larger models with over 10B parameters and proprietary models.

**Link**: [arxiv](http://arxiv.org/abs/2408.15542v1),  [pdf](http://arxiv.org/pdf/2408.15542v1)

**Tags**: cs.CV cs.AI cs.MM 



### LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via   Layer-wise Relevance Propagation
**Authors**: Haichuan Hu, Yuhan Sun, Quanjun Zhang

**Updated**: 2024-08-29T08:45:30Z

**Summary**: Retrieval-Augmented Generation (RAG) has become a primary technique for mitigating hallucinations in large language models (LLMs). However, incomplete knowledge extraction and insufficient understanding can still mislead LLMs to produce irrelevant or even contradictory responses, which means hallucinations persist in RAG. In this paper, we propose LRP4RAG, a method based on the Layer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations in RAG. Specifically, we first utilize LRP to compute the relevance between the input and output of the RAG generator. We then apply further extraction and resampling to the relevance matrix. The processed relevance data are input into multiple classifiers to determine whether the output contains hallucinations. To the best of our knowledge, this is the first time that LRP has been used for detecting RAG hallucinations, and extensive experiments demonstrate that LRP4RAG outperforms existing baselines.

**Link**: [arxiv](http://arxiv.org/abs/2408.15533v2),  [pdf](http://arxiv.org/pdf/2408.15533v2)

**Tags**: cs.CL cs.AI 



### Drone Referring Localization: An Efficient Heterogeneous Spatial Feature   Interaction Method For UAV Self-Localization
**Authors**: Ming Dai, Enhui Zheng, Jiahao Chen, Lei Qi, Zhenhua Feng, Wankou Yang

**Updated**: 2024-08-28T04:36:23Z

**Summary**: Image retrieval (IR) has emerged as a promising approach for self-localization in unmanned aerial vehicles (UAVs). However, IR-based methods face several challenges: 1) Pre- and post-processing incur significant computational and storage overhead; 2) The lack of interaction between dual-source features impairs precise spatial perception. In this paper, we propose an efficient heterogeneous spatial feature interaction method, termed Drone Referring Localization (DRL), which aims to localize UAV-view images within satellite imagery. Unlike conventional methods that treat different data sources in isolation, followed by cosine similarity computations, DRL facilitates the learnable interaction of heterogeneous features. To implement the proposed DRL, we design two transformer-based frameworks, Post-Fusion and Mix-Fusion, enabling end-to-end training and inference. Furthermore, we introduce random scale cropping and weight balance loss techniques to augment paired data and optimize the balance between positive and negative sample weights. Additionally, we construct a new dataset, UL14, and establish a benchmark tailored to the DRL framework. Compared to traditional IR methods, DRL achieves superior localization accuracy (MA@20 +9.4\%) while significantly reducing computational time (1/7) and storage overhead (1/3). The dataset and code will be made publicly available. The dataset and code are available at \url{https://github.com/Dmmm1997/DRL} .

**Link**: [arxiv](http://arxiv.org/abs/2208.06561v3),  [pdf](http://arxiv.org/pdf/2208.06561v3)

**Tags**: cs.CV 



### Gravitational-Wave and Gravitational-Wave Memory Signatures of   Core-Collapse Supernovae
**Authors**: Lyla Choi, Adam Burrows, David Vartanyan

**Updated**: 2024-08-28T04:32:18Z

**Summary**: In this paper, we calculate the energy, signal-to-noise ratio, detection range, and angular anisotropy of the matter, matter memory, and neutrino memory gravitational wave (GW) signatures of 21 three-dimensional initially non-rotating core-collapse supernova (CCSN) models carried to late times. We find that inferred energy, signal-to-noise ratio, and detection range are angle-dependent quantities, and that the spread of possible energy, signal-to-noise, and detection ranges across all viewing angles generally increases with progenitor mass. When examining the low-frequency matter memory and neutrino memory components of the signal, we find that the neutrino memory is the most detectable component of a CCSN GW signal, and that DECIGO is best-equipped to detect both matter memory and neutrino memory. Moreover, we find that the polarization angle between the $h_+$ and $h_{\times}$ strains serves as a unique identifier of matter and neutrino memory. Finally, we develop a galactic density- and stellar mass-weighted formalism to calculate the rate at which we can expect to detect CCSN GW signals with Advanced LIGO. When considering only the matter component of the signal, the aLIGO detection rate is around 65$\%$ of the total galactic supernova rate, but increases to 90$\%$ when incorporating the neutrino memory component. We find that all future detectors (ET, CE, DECIGO) will be able to detect CCSN GW signals from the entire galaxy, and for the higher-mass progenitors even into the local group of galaxies.

**Link**: [arxiv](http://arxiv.org/abs/2408.01525v2),  [pdf](http://arxiv.org/pdf/2408.01525v2)

**Tags**: astro-ph.HE astro-ph.SR gr-qc 



### Affordable Generative Agents
**Authors**: Yangbin Yu, Qin Zhang, Junyou Li, Qiang Fu, Deheng Ye

**Updated**: 2024-08-28T04:04:45Z

**Summary**: The emergence of large language models (LLMs) has significantly advanced the simulation of believable interactive agents. However, the substantial cost on maintaining the prolonged agent interactions poses challenge over the deployment of believable LLM-based agents. Therefore, in this paper, we develop Affordable Generative Agents (AGA), a framework for enabling the generation of believable and low-cost interactions on both agent-environment and inter-agents levels. Specifically, for agent-environment interactions, we substitute repetitive LLM inferences with learned policies; while for inter-agent interactions, we model the social relationships between agents and compress auxiliary dialogue information. Extensive experiments on multiple environments show the effectiveness and efficiency of our proposed framework. Also, we delve into the mechanisms of emergent believable behaviors lying in LLM agents, demonstrating that agents can only generate finite behaviors in fixed environments, based upon which, we understand ways to facilitate emergent interaction behaviors. Our code is publicly available at: https://github.com/AffordableGenerativeAgents/Affordable-Generative-Agents.

**Link**: [arxiv](http://arxiv.org/abs/2402.02053v2),  [pdf](http://arxiv.org/pdf/2402.02053v2)

**Tags**: cs.AI cs.HC 



### A Survey of Large Language Models for European Languages
**Authors**: Wazir Ali, Sampo Pyysalo

**Updated**: 2024-08-28T03:56:37Z

**Summary**: Large Language Models (LLMs) have gained significant attention due to their high performance on a wide range of natural language tasks since the release of ChatGPT. The LLMs learn to understand and generate language by training billions of model parameters on vast volumes of text data. Despite being a relatively new field, LLM research is rapidly advancing in various directions. In this paper, we present an overview of LLM families, including LLaMA, PaLM, GPT, and MoE, and the methods developed to create and enhance LLMs for official European Union (EU) languages. We provide a comprehensive summary of common monolingual and multilingual datasets used for pretraining large language models.

**Link**: [arxiv](http://arxiv.org/abs/2408.15040v2),  [pdf](http://arxiv.org/pdf/2408.15040v2)

**Tags**: cs.CL 



### Towards Fully Autonomous Research Powered by LLMs: Case Study on   Simulations
**Authors**: Zhihan Liu, Yubo Chai, Jianfeng Li

**Updated**: 2024-08-28T03:48:05Z

**Summary**: The advent of Large Language Models (LLMs) has created new opportunities for the automation of scientific research, spanning both experimental processes and computational simulations. This study explores the feasibility of constructing an autonomous simulation agent (ASA) powered by LLM, through sophisticated API integration, to automate the entire research process, from experimental design, remote upload and simulation execution, data analysis, to report compilation. Using a simulation problem of polymer chain conformations as a case study, we assessed the performance of ASAs powered by different LLMs including GPT-4-Turbo. Our findings revealed that ASA-GPT-4o achieved near-flawless execution on designated research missions, underscoring the potential of LLMs to manage complete scientific investigations autonomously. The outlined automation can be iteratively performed up to twenty cycles without human intervention, illustrating the potential of LLMs for large-scale autonomous research endeavors. Additionally, we discussed the intrinsic traits of ASAs in managing extensive tasks, focusing on self-validation mechanisms and the balance between local attention and global oversight.

**Link**: [arxiv](http://arxiv.org/abs/2408.15512v1),  [pdf](http://arxiv.org/pdf/2408.15512v1)

**Tags**: cs.AI cs.CL physics.chem-ph 



### WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation   Integrating Web Search and Knowledge Graphs
**Authors**: Weijian Xie, Xuefeng Liang, Yuhui Liu, Kaihua Ni, Hong Cheng, Zetian Hu

**Updated**: 2024-08-28T03:47:28Z

**Summary**: Large Language Models (LLMs) have greatly contributed to the development of adaptive intelligent agents and are positioned as an important way to achieve Artificial General Intelligence (AGI). However, LLMs are prone to produce factually incorrect information and often produce "phantom" content that undermines their reliability, which poses a serious challenge for their deployment in real-world scenarios. Enhancing LLMs by combining external databases and information retrieval mechanisms is an effective path. To address the above challenges, we propose a new approach called WeKnow-RAG, which integrates Web search and Knowledge Graphs into a "Retrieval-Augmented Generation (RAG)" system. First, the accuracy and reliability of LLM responses are improved by combining the structured representation of Knowledge Graphs with the flexibility of dense vector retrieval. WeKnow-RAG then utilizes domain-specific knowledge graphs to satisfy a variety of queries and domains, thereby improving performance on factual information and complex reasoning tasks by employing multi-stage web page retrieval techniques using both sparse and dense retrieval methods. Our approach effectively balances the efficiency and accuracy of information retrieval, thus improving the overall retrieval process. Finally, we also integrate a self-assessment mechanism for the LLM to evaluate the trustworthiness of the answers it generates. Our approach proves its outstanding effectiveness in a wide range of offline experiments and online submissions.

**Link**: [arxiv](http://arxiv.org/abs/2408.07611v2),  [pdf](http://arxiv.org/pdf/2408.07611v2)

**Tags**: cs.CL cs.IR 



### IICPilot: An Intelligent Integrated Circuit Backend Design Framework   Using Open EDA
**Authors**: Zesong Jiang, Qing Zhang, Cheng Liu, Long Cheng, Huawei Li, Xiaowei Li

**Updated**: 2024-08-28T03:15:10Z

**Summary**: Open-source EDA tools are rapidly advancing, fostering collaboration, innovation, and knowledge sharing within the EDA community. However, the growing complexity of these tools, characterized by numerous design parameters and heuristics, poses a significant barrier to their widespread adoption. This complexity is particularly pronounced in integrated circuit (IC) backend designs, which place substantial demands on engineers' expertise in EDA tools. To tackle this challenge, we introduce IICPilot, an intelligent IC backend design system based on LLM technology. IICPilot automates various backend design procedures, including script generation, EDA tool invocation, design space exploration of EDA parameters, container-based computing resource allocation, and exception management. By automating these tasks, IICPilot significantly lowers the barrier to entry for open-source EDA tools. Specifically, IICPilot utilizes LangChain's multi-agent framework to efficiently handle distinct design tasks, enabling flexible enhancements independently. Moreover, IICPilot separates the backend design workflow from specific open-source EDA tools through a unified EDA calling interface. This approach allows seamless integration with different open-source EDA tools like OpenROAD and iEDA, streamlining the backend design and optimization across the EDA tools.

**Link**: [arxiv](http://arxiv.org/abs/2407.12576v2),  [pdf](http://arxiv.org/pdf/2407.12576v2)

**Tags**: cs.AR cs.AI 



## Keyword: LLM Deployment 
 ### Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of   Encoders
**Authors**: Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, Bryan Catanzaro, Andrew Tao, Jan Kautz, Zhiding Yu, Guilin Liu

**Updated**: 2024-08-28T17:59:31Z

**Summary**: The ability to accurately interpret complex visual information is a crucial topic of multimodal large language models (MLLMs). Recent work indicates that enhanced visual perception significantly reduces hallucinations and improves performance on resolution-sensitive tasks, such as optical character recognition and document analysis. A number of recent MLLMs achieve this goal using a mixture of vision encoders. Despite their success, there is a lack of systematic comparisons and detailed ablation studies addressing critical aspects, such as expert selection and the integration of multiple vision experts. This study provides an extensive exploration of the design space for MLLMs using a mixture of vision encoders and resolutions. Our findings reveal several underlying principles common to various existing strategies, leading to a streamlined yet effective design approach. We discover that simply concatenating visual tokens from a set of complementary vision encoders is as effective as more complex mixing architectures or strategies. We additionally introduce Pre-Alignment to bridge the gap between vision-focused encoders and language tokens, enhancing model coherence. The resulting family of MLLMs, Eagle, surpasses other leading open-source models on major MLLM benchmarks. Models and code: https://github.com/NVlabs/Eagle

**Link**: [arxiv](http://arxiv.org/abs/2408.15998v1),  [pdf](http://arxiv.org/pdf/2408.15998v1)

**Tags**: cs.CV cs.AI cs.LG cs.RO 



### WebPilot: A Versatile and Autonomous Multi-Agent System for Web Task   Execution with Strategic Exploration
**Authors**: Yao Zhang, Zijian Ma, Yunpu Ma, Zhen Han, Yu Wu, Volker Tresp

**Updated**: 2024-08-28T17:49:29Z

**Summary**: LLM-based autonomous agents often fail to execute complex web tasks that require dynamic interaction due to the inherent uncertainty and complexity of these environments. Existing LLM-based web agents typically rely on rigid, expert-designed policies specific to certain states and actions, which lack the flexibility and generalizability needed to adapt to unseen tasks. In contrast, humans excel by exploring unknowns, continuously adapting strategies, and resolving ambiguities through exploration. To emulate human-like adaptability, web agents need strategic exploration and complex decision-making. Monte Carlo Tree Search (MCTS) is well-suited for this, but classical MCTS struggles with vast action spaces, unpredictable state transitions, and incomplete information in web tasks. In light of this, we develop WebPilot, a multi-agent system with a dual optimization strategy that improves MCTS to better handle complex web environments. Specifically, the Global Optimization phase involves generating a high-level plan by breaking down tasks into manageable subtasks and continuously refining this plan, thereby focusing the search process and mitigating the challenges posed by vast action spaces in classical MCTS. Subsequently, the Local Optimization phase executes each subtask using a tailored MCTS designed for complex environments, effectively addressing uncertainties and managing incomplete information. Experimental results on WebArena and MiniWoB++ demonstrate the effectiveness of WebPilot. Notably, on WebArena, WebPilot achieves SOTA performance with GPT-4, achieving a 93% relative increase in success rate over the concurrent tree search-based method. WebPilot marks a significant advancement in general autonomous agent capabilities, paving the way for more advanced and reliable decision-making in practical environments.

**Link**: [arxiv](http://arxiv.org/abs/2408.15978v1),  [pdf](http://arxiv.org/pdf/2408.15978v1)

**Tags**: cs.AI 



### BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition   Capabilities of Language Models in Multi-Agent Systems
**Authors**: Wei Wang, Dan Zhang, Tao Feng, Boyan Wang, Jie Tang

**Updated**: 2024-08-28T17:43:55Z

**Summary**: Large Language Models (LLMs) are becoming increasingly powerful and capable of handling complex tasks, e.g., building single agents and multi-agent systems. Compared to single agents, multi-agent systems have higher requirements for the collaboration capabilities of language models. Many benchmarks are proposed to evaluate their collaborative abilities. However, these benchmarks lack fine-grained evaluations of LLM collaborative capabilities. Additionally, multi-agent collaborative and competitive scenarios are ignored in existing works. To address these two problems, we propose a benchmark, called BattleAgentBench, which defines seven sub-stages of three varying difficulty levels and conducts a fine-grained evaluation of language models in terms of single-agent scenario navigation capabilities, paired-agent task execution abilities, and multi-agent collaboration and competition capabilities. We conducted extensive evaluations on leading four closed-source and seven open-source models. Experimental results indicate that API-based models perform excellently on simple tasks but open-source small models struggle with simple tasks. Regarding difficult tasks that require collaborative and competitive abilities, although API-based models have demonstrated some collaborative capabilities, there is still enormous room for improvement.

**Link**: [arxiv](http://arxiv.org/abs/2408.15971v1),  [pdf](http://arxiv.org/pdf/2408.15971v1)

**Tags**: cs.CL 



### Ain't How You Deploy: An Analysis of BGP Security Policies Performance   Against Various Attack Scenarios with Differing Deployment Strategies
**Authors**: Seth Barrett, Calvin Idom, German Zavala Villafuerte, Andrew Byers, Berk Gulmezoglu

**Updated**: 2024-08-28T17:43:21Z

**Summary**: This paper investigates the performance of various Border Gateway Protocol (BGP) security policies against multiple attack scenarios using different deployment strategies. Through extensive simulations, we evaluate the effectiveness of defensive mechanisms such as Root Origin Validation (ROV), Autonomous System Provider Authorization (ASPA), and PeerROV across distinct AS deployment types. Our findings reveal critical insights into the strengths and limitations of current BGP security measures, providing guidance for future policy development and implementation.

**Link**: [arxiv](http://arxiv.org/abs/2408.15970v1),  [pdf](http://arxiv.org/pdf/2408.15970v1)

**Tags**: cs.CR 



### More Text, Less Point: Towards 3D Data-Efficient Point-Language   Understanding
**Authors**: Yuan Tang, Xu Han, Xianzhi Li, Qiao Yu, Jinfeng Xu, Yixue Hao, Long Hu, Min Chen

**Updated**: 2024-08-28T17:38:44Z

**Summary**: Enabling Large Language Models (LLMs) to comprehend the 3D physical world remains a significant challenge. Due to the lack of large-scale 3D-text pair datasets, the success of LLMs has yet to be replicated in 3D understanding. In this paper, we rethink this issue and propose a new task: 3D Data-Efficient Point-Language Understanding. The goal is to enable LLMs to achieve robust 3D object understanding with minimal 3D point cloud and text data pairs. To address this task, we introduce GreenPLM, which leverages more text data to compensate for the lack of 3D data. First, inspired by using CLIP to align images and text, we utilize a pre-trained point cloud-text encoder to map the 3D point cloud space to the text space. This mapping leaves us to seamlessly connect the text space with LLMs. Once the point-text-LLM connection is established, we further enhance text-LLM alignment by expanding the intermediate text space, thereby reducing the reliance on 3D point cloud data. Specifically, we generate 6M free-text descriptions of 3D objects, and design a three-stage training strategy to help LLMs better explore the intrinsic connections between different modalities. To achieve efficient modality alignment, we design a zero-parameter cross-attention module for token pooling. Extensive experimental results show that GreenPLM requires only 12% of the 3D training data used by existing state-of-the-art models to achieve superior 3D understanding. Remarkably, GreenPLM also achieves competitive performance using text-only data. The code and weights are available at: https://github.com/TangYuan96/GreenPLM.

**Link**: [arxiv](http://arxiv.org/abs/2408.15966v1),  [pdf](http://arxiv.org/pdf/2408.15966v1)

**Tags**: cs.CV cs.AI cs.CL 



### Flextron: Many-in-One Flexible Large Language Model
**Authors**: Ruisi Cai, Saurav Muralidharan, Greg Heinrich, Hongxu Yin, Zhangyang Wang, Jan Kautz, Pavlo Molchanov

**Updated**: 2024-08-28T17:26:03Z

**Summary**: Training modern LLMs is extremely resource intensive, and customizing them for various deployment scenarios characterized by limited compute and memory resources through repeated training is impractical. In this paper, we introduce Flextron, a network architecture and post-training model optimization framework supporting flexible model deployment. The Flextron architecture utilizes a nested elastic structure to rapidly adapt to specific user-defined latency and accuracy targets during inference with no additional fine-tuning required. It is also input-adaptive, and can automatically route tokens through its sub-networks for improved performance and efficiency. We present a sample-efficient training method and associated routing algorithms for systematically transforming an existing trained LLM into a Flextron model. We evaluate Flextron on the GPT-3 and LLama-2 family of LLMs, and demonstrate superior performance over multiple end-to-end trained variants and other state-of-the-art elastic networks, all with a single pretraining run that consumes a mere 7.63% tokens compared to original pretraining.

**Link**: [arxiv](http://arxiv.org/abs/2406.10260v2),  [pdf](http://arxiv.org/pdf/2406.10260v2)

**Tags**: cs.CL cs.LG 



### Atari-GPT: Investigating the Capabilities of Multimodal Large Language   Models as Low-Level Policies for Atari Games
**Authors**: Nicholas R. Waytowich, Devin White, MD Sunbeam, Vinicius G. Goecks

**Updated**: 2024-08-28T17:08:56Z

**Summary**: Recent advancements in large language models (LLMs) have expanded their capabilities beyond traditional text-based tasks to multimodal domains, integrating visual, auditory, and textual data. While multimodal LLMs have been extensively explored for high-level planning in domains like robotics and games, their potential as low-level controllers remains largely untapped. This paper explores the application of multimodal LLMs as low-level controllers in the domain of Atari video games, introducing Atari game performance as a new benchmark for evaluating the ability of multimodal LLMs to perform low-level control tasks. Unlike traditional reinforcement learning (RL) and imitation learning (IL) methods that require extensive computational resources as well as reward function specification, these LLMs utilize pre-existing multimodal knowledge to directly engage with game environments. Our study assesses multiple multimodal LLMs performance against traditional RL agents, human players, and random agents, focusing on their ability to understand and interact with complex visual scenes and formulate strategic responses. Additionally, we examine the impact of In-Context Learning (ICL) by incorporating human-demonstrated game-play trajectories to enhance the models contextual understanding. Through this investigation, we aim to determine the extent to which multimodal LLMs can leverage their extensive training to effectively function as low-level controllers, thereby redefining potential applications in dynamic and visually complex environments. Additional results and videos are available at our project webpage: https://sites.google.com/view/atari-gpt/.

**Link**: [arxiv](http://arxiv.org/abs/2408.15950v1),  [pdf](http://arxiv.org/pdf/2408.15950v1)

**Tags**: cs.AI 



### Leveraging Open Knowledge for Advancing Task Expertise in Large Language   Models
**Authors**: Yuncheng Yang, Yulei Qin, Tong Wu, Zihan Xu, Gang Li, Pengcheng Guo, Hang Shao, Yucheng Shi, Ke Li, Xing Sun, Jie Yang, Yun Gu

**Updated**: 2024-08-28T16:28:07Z

**Summary**: The cultivation of expertise for large language models (LLMs) to solve tasks of specific areas often requires special-purpose tuning with calibrated behaviors on the expected stable outputs. To avoid huge cost brought by manual preparation of instruction datasets and training resources up to hundreds of hours, the exploitation of open knowledge including a wealth of low rank adaptation (LoRA) models and instruction datasets serves as a good starting point. However, existing methods on model and data selection focus on the performance of general-purpose capabilities while neglecting the knowledge gap exposed in domain-specific deployment. In the present study, we propose to bridge such gap by introducing few human-annotated samples (i.e., K-shot) for advancing task expertise of LLMs with open knowledge. Specifically, we develop an efficient and scalable pipeline to cost-efficiently produce task experts where K-shot data intervene in selecting the most promising expert candidates and the task-relevant instructions. A mixture-of-expert (MoE) system is built to make the best use of individual-yet-complementary knowledge between multiple experts. We unveil the two keys to the success of a MoE system, 1) the abidance by K-shot, and 2) the insistence on diversity. For the former, we ensure that models that truly possess problem-solving abilities on K-shot are selected rather than those blind guessers. Besides, during data selection, instructions that share task-relevant contexts with K-shot are prioritized. For the latter, we highlight the diversity of constituting experts and that of the fine-tuning instructions throughout the model and data selection process. Extensive experimental results confirm the superiority of our approach over existing methods on utilization of open knowledge across various tasks. Codes and models will be released later.

**Link**: [arxiv](http://arxiv.org/abs/2408.15915v1),  [pdf](http://arxiv.org/pdf/2408.15915v1)

**Tags**: cs.CV cs.AI cs.CL 



### Towards Human-Level Text Coding with LLMs: The Case of Fatherhood Roles   in Public Policy Documents
**Authors**: Lorenzo Lupo, Oscar Magnusson, Dirk Hovy, Elin Naurin, Lena W√§ngnerud

**Updated**: 2024-08-28T16:26:16Z

**Summary**: Recent advances in large language models (LLMs) like GPT-3.5 and GPT-4 promise automation with better results and less programming, opening up new opportunities for text analysis in political science. In this study, we evaluate LLMs on three original coding tasks involving typical complexities encountered in political science settings: a non-English language, legal and political jargon, and complex labels based on abstract constructs. Along the paper, we propose a practical workflow to optimize the choice of the model and the prompt. We find that the best prompting strategy consists of providing the LLMs with a detailed codebook, as the one provided to human coders. In this setting, an LLM can be as good as or possibly better than a human annotator while being much faster, considerably cheaper, and much easier to scale to large amounts of text. We also provide a comparison of GPT and popular open-source LLMs, discussing the trade-offs in the model's choice. Our software allows LLMs to be easily used as annotators and is publicly available: https://github.com/lorelupo/pappa.

**Link**: [arxiv](http://arxiv.org/abs/2311.11844v3),  [pdf](http://arxiv.org/pdf/2311.11844v3)

**Tags**: cs.CL cs.CY J.4; I.2 



### Decentralized LLM Inference over Edge Networks with Energy Harvesting
**Authors**: Aria Khoshsirat, Giovanni Perin, Michele Rossi

**Updated**: 2024-08-28T16:20:45Z

**Summary**: Large language models have significantly transformed multiple fields with their exceptional performance in natural language tasks, but their deployment in resource-constrained environments like edge networks presents an ongoing challenge. Decentralized techniques for inference have emerged, distributing the model blocks among multiple devices to improve flexibility and cost effectiveness. However, energy limitations remain a significant concern for edge devices. We propose a sustainable model for collaborative inference on interconnected, battery-powered edge devices with energy harvesting. A semi-Markov model is developed to describe the states of the devices, considering processing parameters and average green energy arrivals. This informs the design of scheduling algorithms that aim to minimize device downtimes and maximize network throughput. Through empirical evaluations and simulated runs, we validate the effectiveness of our approach, paving the way for energy-efficient decentralized inference over edge networks.

**Link**: [arxiv](http://arxiv.org/abs/2408.15907v1),  [pdf](http://arxiv.org/pdf/2408.15907v1)

**Tags**: cs.DC 



### LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration   in Evolving Environments
**Authors**: Ruirui Chen, Weifeng Jiang, Chengwei Qin, Ishaan Singh Rawal, Cheston Tan, Dongkyu Choi, Bo Xiong, Bo Ai

**Updated**: 2024-08-28T16:15:45Z

**Summary**: The rapid obsolescence of information in Large Language Models (LLMs) has driven the development of various techniques to incorporate new facts. However, existing methods for knowledge editing still face difficulties with multi-hop questions that require accurate fact identification and sequential logical reasoning, particularly among numerous fact updates. To tackle these challenges, this paper introduces Graph Memory-based Editing for Large Language Models (GMeLLo), a straitforward and effective method that merges the explicit knowledge representation of Knowledge Graphs (KGs) with the linguistic flexibility of LLMs. Beyond merely leveraging LLMs for question answering, GMeLLo employs these models to convert free-form language into structured queries and fact triples, facilitating seamless interaction with KGs for rapid updates and precise multi-hop reasoning. Our results show that GMeLLo significantly surpasses current state-of-the-art knowledge editing methods in the multi-hop question answering benchmark, MQuAKE, especially in scenarios with extensive knowledge edits.

**Link**: [arxiv](http://arxiv.org/abs/2408.15903v1),  [pdf](http://arxiv.org/pdf/2408.15903v1)

**Tags**: cs.CL 



### Bias in LLMs as Annotators: The Effect of Party Cues on Labelling   Decision by Large Language Models
**Authors**: Sebastian Vallejo Vera, Hunter Driggers

**Updated**: 2024-08-28T16:05:20Z

**Summary**: Human coders are biased. We test similar biases in Large Language Models (LLMs) as annotators. By replicating an experiment run by Ennser-Jedenastik and Meyer (2018), we find evidence that LLMs use political information, and specifically party cues, to judge political statements. Not only do LLMs use relevant information to contextualize whether a statement is positive, negative, or neutral based on the party cue, they also reflect the biases of the human-generated data upon which they have been trained. We also find that unlike humans, who are only biased when faced with statements from extreme parties, LLMs exhibit significant bias even when prompted with statements from center-left and center-right parties. The implications of our findings are discussed in the conclusion.

**Link**: [arxiv](http://arxiv.org/abs/2408.15895v1),  [pdf](http://arxiv.org/pdf/2408.15895v1)

**Tags**: cs.CL cs.LG 



### Secret Collusion among Generative AI Agents
**Authors**: Sumeet Ramesh Motwani, Mikhail Baranchuk, Martin Strohmeier, Vijay Bolina, Philip H. S. Torr, Lewis Hammond, Christian Schroeder de Witt

**Updated**: 2024-08-28T15:53:04Z

**Summary**: Recent capability increases in large language models (LLMs) open up applications in which groups of communicating generative AI agents solve joint tasks. This poses privacy and security challenges concerning the unauthorised sharing of information, or other unwanted forms of agent coordination. Modern steganographic techniques could render such dynamics hard to detect. In this paper, we comprehensively formalise the problem of secret collusion in systems of generative AI agents by drawing on relevant concepts from both AI and security literature. We study incentives for the use of steganography, and propose a variety of mitigation measures. Our investigations result in a model evaluation framework that systematically tests capabilities required for various forms of secret collusion. We provide extensive empirical results across a range of contemporary LLMs. While the steganographic capabilities of current models remain limited, GPT-4 displays a capability jump suggesting the need for continuous monitoring of steganographic frontier model capabilities. We conclude by laying out a comprehensive research program to mitigate future risks of collusion between generative AI models.

**Link**: [arxiv](http://arxiv.org/abs/2402.07510v2),  [pdf](http://arxiv.org/pdf/2402.07510v2)

**Tags**: cs.AI cs.CR 



### Persuasion Games using Large Language Models
**Authors**: Ganesh Prasath Ramani, Shirish Karande, Santhosh V, Yash Bhatia

**Updated**: 2024-08-28T15:50:41Z

**Summary**: Large Language Models (LLMs) have emerged as formidable instruments capable of comprehending and producing human-like text. This paper explores the potential of LLMs, to shape human perspectives and subsequently influence their decisions on particular tasks. This capability finds applications in diverse domains such as Investment, Credit cards and Insurance, wherein they assist users in selecting appropriate insurance policies, investment plans, Credit cards, Retail, as well as in Behavioral Change Support Systems (BCSS).   We present a sophisticated multi-agent framework wherein a consortium of agents operate in collaborative manner. The primary agent engages directly with users through persuasive dialogue, while the auxiliary agents perform tasks such as information retrieval, response analysis, development of persuasion strategies, and validation of facts. Empirical evidence from our experiments demonstrates that this collaborative methodology significantly enhances the persuasive efficacy of the LLM. We analyze user resistance to persuasive efforts continuously and counteract it by employing a combination of rule-based and LLM-based resistance-persuasion mapping techniques.   We employ simulated personas and generate conversations in insurance, banking, and retail domains to evaluate the proficiency of large language models (LLMs) in recognizing, adjusting to, and influencing various personality types. Concurrently, we examine the resistance mechanisms employed by LLM simulated personas. Persuasion is quantified via measurable surveys before and after interaction, LLM-generated scores on conversation, and user decisions (purchase or non-purchase).

**Link**: [arxiv](http://arxiv.org/abs/2408.15879v1),  [pdf](http://arxiv.org/pdf/2408.15879v1)

**Tags**: cs.AI cs.CL 



### Practical Challenges for Reliable RIS Deployment in Heterogeneous   Multi-Operator Multi-Band Networks
**Authors**: Mehdi Monemi, Mehdi Rasti, Arthur S. de Sena, Mohammad Amir Fallah, Matti Latva-Aho, Marco Di Renzo

**Updated**: 2024-08-28T15:35:05Z

**Summary**: Reconfigurable intelligent surfaces (RISs) have been introduced as arrays of nearly passive elements with software-tunable electromagnetic properties to dynamically manipulate the reflection/transmission of radio signals. Research works in this area are focused on two applications, namely {\it user-assist} RIS aiming at tuning the RIS to enhance the quality-of-service (QoS) of target users, and the {\it malicious} RIS aiming for an attacker to degrade the QoS at victim receivers through generating {\it intended} destructive interference. While both user-assist and malicious RIS applications have been explored extensively, the impact of RIS deployments on imposing {\it unintended} interference on various wireless user-equipments (EUs) remains underexplored. This paper investigates the challenges of integrating RISs into multi-carrier, multi-user, and multi-operator networks. We discuss how RIS deployments intended to benefit specific users can negatively impact other users served at various carrier frequencies through different network operators. While not an ideal solution, we discuss how ultra-narrowband metasurfaces can be incorporated into the manufacturing of RISs to mitigate some challenges of RIS deployment in wireless networks. We also present a simulation scenario to illuminate some practical challenges associated with the deployment of RISs in shared public environments.

**Link**: [arxiv](http://arxiv.org/abs/2408.15867v1),  [pdf](http://arxiv.org/pdf/2408.15867v1)

**Tags**: eess.SY cs.SY 



### What is YOLOv8: An In-Depth Exploration of the Internal Features of the   Next-Generation Object Detector
**Authors**: Muhammad Yaseen

**Updated**: 2024-08-28T15:18:46Z

**Summary**: This study presents a detailed analysis of the YOLOv8 object detection model, focusing on its architecture, training techniques, and performance improvements over previous iterations like YOLOv5. Key innovations, including the CSPNet backbone for enhanced feature extraction, the FPN+PAN neck for superior multi-scale object detection, and the transition to an anchor-free approach, are thoroughly examined. The paper reviews YOLOv8's performance across benchmarks like Microsoft COCO and Roboflow 100, highlighting its high accuracy and real-time capabilities across diverse hardware platforms. Additionally, the study explores YOLOv8's developer-friendly enhancements, such as its unified Python package and CLI, which streamline model training and deployment. Overall, this research positions YOLOv8 as a state-of-the-art solution in the evolving object detection field.

**Link**: [arxiv](http://arxiv.org/abs/2408.15857v1),  [pdf](http://arxiv.org/pdf/2408.15857v1)

**Tags**: cs.CV 



### A Statistical Framework of Watermarks for Large Language Models: Pivot,   Detection Efficiency and Optimal Rules
**Authors**: Xiang Li, Feng Ruan, Huiyuan Wang, Qi Long, Weijie J. Su

**Updated**: 2024-08-28T15:01:04Z

**Summary**: Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of incorrectly classifying LLM-generated text as human-written). Our framework further reduces the problem of determining the optimal detection rule to solving a minimax optimization program. We apply this framework to two representative watermarks -- one of which has been internally implemented at OpenAI -- and obtain several findings that can be instrumental in guiding the practice of implementing watermarks. In particular, we derive optimal detection rules for these watermarks under our framework. These theoretically derived detection rules are demonstrated to be competitive and sometimes enjoy a higher power than existing detection approaches through numerical experiments.

**Link**: [arxiv](http://arxiv.org/abs/2404.01245v2),  [pdf](http://arxiv.org/pdf/2404.01245v2)

**Tags**: math.ST cs.CL cs.CR cs.LG stat.ML stat.TH 



### Downstream bias mitigation is all you need
**Authors**: Arkadeep Baksi, Rahul Singh, Tarun Joshi

**Updated**: 2024-08-28T14:59:31Z

**Summary**: The advent of transformer-based architectures and large language models (LLMs) have significantly advanced the performance of natural language processing (NLP) models. Since these LLMs are trained on huge corpuses of data from the web and other sources, there has been a major concern about harmful prejudices that may potentially be transferred from the data. In many applications, these pre-trained LLMs are fine-tuned on task specific datasets, which can further contribute to biases. This paper studies the extent of biases absorbed by LLMs during pre-training as well as task-specific behaviour after fine-tuning. We found that controlled interventions on pre-trained LLMs, prior to fine-tuning, have minimal effect on lowering biases in classifiers. However, the biases present in domain-specific datasets play a much bigger role, and hence mitigating them at this stage has a bigger impact. While pre-training does matter, but after the model has been pre-trained, even slight changes to co-occurrence rates in the fine-tuning dataset has a significant effect on the bias of the model.

**Link**: [arxiv](http://arxiv.org/abs/2408.00612v2),  [pdf](http://arxiv.org/pdf/2408.00612v2)

**Tags**: cs.CL 



### Look Before You Leap: Towards Decision-Aware and Generalizable   Tool-Usage for Large Language Models
**Authors**: Anchun Gui, Jian Li, Yong Dai, Nan Du, Han Xiao

**Updated**: 2024-08-28T14:54:11Z

**Summary**: Tool-augmented large language models (LLMs) are attracting widespread attention when accessing up-to-date knowledge and alleviating hallucination issues. Nowadays, advanced closed-source LLMs (e.g., ChatGPT) have demonstrated surprising tool-usage capabilities through prompting and in-context learning techniques. To empower the capabilities of open-source LLMs (e.g., LLaMA) in manipulating tools, current efforts focus on either template-driven or token-triggered tool-usage. However, the former hampers LLMs' flexibility to address diverse user's queries due to constrained tool interactions, while the latter limits the generalizability when engaging with new tools, since tool-usage learning is based on task- and tool-specific datasets. To alleviate these concerns, in this paper, we propose a decision-aware and generalizable tool-usage framework (DEER). Specifically, we first construct the tool-usage samples with multiple decision branches via an automatic generation pipeline, thereby inspiring the decision-making awareness of LLMs under diverse scenarios. Meanwhile, we propose a novel tool sampling strategy to enhance the generalizability of LLMs over unseen tools. Extensive experiments demonstrate that our proposed DEER is effective and significantly outperforms baselines across various datasets.

**Link**: [arxiv](http://arxiv.org/abs/2402.16696v3),  [pdf](http://arxiv.org/pdf/2402.16696v3)

**Tags**: cs.CL 



### Knowledge Navigator: LLM-guided Browsing Framework for Exploratory   Search in Scientific Literature
**Authors**: Uri Katz, Mosh Levy, Yoav Goldberg

**Updated**: 2024-08-28T14:48:37Z

**Summary**: The exponential growth of scientific literature necessitates advanced tools for effective knowledge exploration. We present Knowledge Navigator, a system designed to enhance exploratory search abilities by organizing and structuring the retrieved documents from broad topical queries into a navigable, two-level hierarchy of named and descriptive scientific topics and subtopics. This structured organization provides an overall view of the research themes in a domain, while also enabling iterative search and deeper knowledge discovery within specific subtopics by allowing users to refine their focus and retrieve additional relevant documents. Knowledge Navigator combines LLM capabilities with cluster-based methods to enable an effective browsing method. We demonstrate our approach's effectiveness through automatic and manual evaluations on two novel benchmarks, CLUSTREC-COVID and SCITOC. Our code, prompts, and benchmarks are made publicly available.

**Link**: [arxiv](http://arxiv.org/abs/2408.15836v1),  [pdf](http://arxiv.org/pdf/2408.15836v1)

**Tags**: cs.IR cs.AI cs.CL 



### The Fault in our Stars: Quality Assessment of Code Generation Benchmarks
**Authors**: Mohammed Latif Siddiq, Simantika Dristi, Joy Saha, Joanna C. S. Santos

**Updated**: 2024-08-28T14:38:51Z

**Summary**: Large Language Models (LLMs) are gaining popularity among software engineers. A crucial aspect of developing effective code generation LLMs is to evaluate these models using a robust benchmark. Evaluation benchmarks with quality issues can provide a false sense of performance. In this work, we conduct the first-of-its-kind study of the quality of prompts within benchmarks used to compare the performance of different code generation models. To conduct this study, we analyzed 3,566 prompts from 9 code generation benchmarks to identify quality issues in them. We also investigated whether fixing the identified quality issues in the benchmarks' prompts affects a model's performance. We also studied memorization issues of the evaluation dataset, which can put into question a benchmark's trustworthiness. We found that code generation evaluation benchmarks mainly focused on Python and coding exercises and had very limited contextual dependencies to challenge the model. These datasets and the developers' prompts suffer from quality issues like spelling and grammatical errors, unclear sentences to express developers' intent, and not using proper documentation style. Fixing all these issues in the benchmarks can lead to a better performance for Python code generation, but not a significant improvement was observed for Java code generation. We also found evidence that GPT-3.5-Turbo and CodeGen-2.5 models may have data contamination issues.

**Link**: [arxiv](http://arxiv.org/abs/2404.10155v2),  [pdf](http://arxiv.org/pdf/2404.10155v2)

**Tags**: cs.SE cs.LG 



### MR-Adopt: Automatic Deduction of Input Transformation Function for   Metamorphic Testing
**Authors**: Congying Xu, Songqiang Chen, Jiarong Wu, Shing-Chi Cheung, Valerio Terragni, Hengcheng Zhu, Jialun Cao

**Updated**: 2024-08-28T14:24:48Z

**Summary**: While a recent study reveals that many developer-written test cases can encode a reusable Metamorphic Relation (MR), over 70% of them directly hard-code the source input and follow-up input in the encoded relation. Such encoded MRs, which do not contain an explicit input transformation to transform the source inputs to corresponding follow-up inputs, cannot be reused with new source inputs to enhance test adequacy.   In this paper, we propose MR-Adopt (Automatic Deduction Of inPut Transformation) to automatically deduce the input transformation from the hard-coded source and follow-up inputs, aiming to enable the encoded MRs to be reused with new source inputs. With typically only one pair of source and follow-up inputs available in an MR-encoded test case as the example, we leveraged LLMs to understand the intention of the test case and generate additional examples of source-followup input pairs. This helps to guide the generation of input transformations generalizable to multiple source inputs. Besides, to mitigate the issue that LLMs generate erroneous code, we refine LLM-generated transformations by removing MR- irrelevant code elements with data-flow analysis. Finally, we assess candidate transformations based on encoded output relations and select the best transformation as the result. Evaluation results show that MR-Adopt can generate input transformations applicable to all experimental source inputs for 72.00% of encoded MRs, which is 33.33% more than using vanilla GPT-3.5. By incorporating MR- Adopt-generated input transformations, encoded MR-based test cases can effectively enhance the test adequacy, increasing the line coverage and mutation score by 10.62% and 18.91%, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2408.15815v1),  [pdf](http://arxiv.org/pdf/2408.15815v1)

**Tags**: cs.SE 



### Unveiling the Statistical Foundations of Chain-of-Thought Prompting   Methods
**Authors**: Xinyang Hu, Fengzhuo Zhang, Siyu Chen, Zhuoran Yang

**Updated**: 2024-08-28T14:13:41Z

**Summary**: Chain-of-Thought (CoT) prompting and its variants have gained popularity as effective methods for solving multi-step reasoning problems using pretrained large language models (LLMs). In this work, we analyze CoT prompting from a statistical estimation perspective, providing a comprehensive characterization of its sample complexity. To this end, we introduce a multi-step latent variable model that encapsulates the reasoning process, where the latent variable encodes the task information. Under this framework, we demonstrate that when the pretraining dataset is sufficiently large, the estimator formed by CoT prompting is equivalent to a Bayesian estimator. This estimator effectively solves the multi-step reasoning problem by aggregating a posterior distribution inferred from the demonstration examples in the prompt. Moreover, we prove that the statistical error of the CoT estimator can be decomposed into two main components: (i) a prompting error, which arises from inferring the true task using CoT prompts, and (ii) the statistical error of the pretrained LLM. We establish that, under appropriate assumptions, the prompting error decays exponentially to zero as the number of demonstrations increases. Additionally, we explicitly characterize the approximation and generalization errors of the pretrained LLM. Notably, we construct a transformer model that approximates the target distribution of the multi-step reasoning problem with an error that decreases exponentially in the number of transformer blocks. Our analysis extends to other variants of CoT, including Self-Consistent CoT, Tree-of-Thought, and Selection-Inference, offering a broad perspective on the efficacy of these methods. We also provide numerical experiments to validate the theoretical findings.

**Link**: [arxiv](http://arxiv.org/abs/2408.14511v2),  [pdf](http://arxiv.org/pdf/2408.14511v2)

**Tags**: cs.AI cs.CL cs.LG math.ST stat.ML stat.TH 



### Chop Chop: Byzantine Atomic Broadcast to the Network Limit
**Authors**: Martina Camaioni, Rachid Guerraoui, Matteo Monti, Pierre-Louis Roman, Manuel Vidigueira, Gauthier Voron

**Updated**: 2024-08-28T14:05:46Z

**Summary**: At the heart of state machine replication, the celebrated technique enabling decentralized and secure universal computation, lies Atomic Broadcast, a fundamental communication primitive that orders, authenticates, and deduplicates messages. This paper presents Chop Chop, a Byzantine Atomic Broadcast system that uses a novel authenticated memory pool to amortize the cost of ordering, authenticating and deduplicating messages, achieving "line rate" (i.e., closely matching the complexity of a protocol that does not ensure any ordering, authentication or Byzantine resilience) even when processing messages as small as 8 bytes. Chop Chop attains this performance by means of a new form of batching we call distillation. A distilled batch is a set of messages that are fast to authenticate, deduplicate, and order. Batches are distilled using a novel interactive protocol involving brokers, an untrusted layer of facilitating processes between clients and servers. In a geo-distributed deployment of 64 medium-sized servers, Chop Chop processes 43,600,000 messages per second with an average latency of 3.6 seconds. Under the same conditions, state-of-the-art alternatives offer two orders of magnitude less throughput for the same latency. We showcase three simple Chop Chop applications: a Payment system, an Auction house and a "Pixel war" game, respectively achieving 32, 2.3 and 35 million operations per second.

**Link**: [arxiv](http://arxiv.org/abs/2304.07081v2),  [pdf](http://arxiv.org/pdf/2304.07081v2)

**Tags**: cs.DC cs.CR 



### Stick to your Role! Stability of Personal Values Expressed in Large   Language Models
**Authors**: Grgur Kovaƒç, R√©my Portelas, Masataka Sawayama, Peter Ford Dominey, Pierre-Yves Oudeyer

**Updated**: 2024-08-28T14:04:05Z

**Summary**: The standard way to study Large Language Models (LLMs) with benchmarks or psychology questionnaires is to provide many different queries from similar minimal contexts (e.g. multiple choice questions). However, due to LLMs' highly context-dependent nature, conclusions from such minimal-context evaluations may be little informative about the model's behavior in deployment (where it will be exposed to many new contexts). We argue that context-dependence (specifically, value stability) should be studied as a specific property of LLMs and used as another dimension of LLM comparison (alongside others such as cognitive abilities, knowledge, or model size). We present a case-study on the stability of value expression over different contexts (simulated conversations on different topics) as measured using a standard psychology questionnaire (PVQ) and on behavioral downstream tasks. Reusing methods from psychology, we study Rank-order stability on the population (interpersonal) level, and Ipsative stability on the individual (intrapersonal) level. We consider two settings (with and without instructing LLMs to simulate particular personas), two simulated populations, and three downstream tasks. We observe consistent trends in the stability of models and model families - Mixtral, Mistral, GPT-3.5 and Qwen families are more stable than LLaMa-2 and Phi. The consistency of these trends implies that some models exhibit higher value stability than others, and that stability can be estimated with the set of introduced methodological tools. When instructed to simulate particular personas, LLMs exhibit low Rank-order stability, which further diminishes with conversation length. This highlights the need for future research on LLMs that coherently simulate different personas. This paper provides a foundational step in that direction, and, to our knowledge, it is the first study of value stability in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2402.14846v4),  [pdf](http://arxiv.org/pdf/2402.14846v4)

**Tags**: cs.CL cs.AI cs.LG 68T07 I.2.7 



### Feedback-Driven Automated Whole Bug Report Reproduction for Android Apps
**Authors**: Dingbang Wang, Yu Zhao, Sidong Feng, Zhaoxu Zhang, William G. J. Halfond, Chunyang Chen, Xiaoxia Sun, Jiangfan Shi, Tingting Yu

**Updated**: 2024-08-28T13:52:57Z

**Summary**: In software development, bug report reproduction is a challenging task. This paper introduces ReBL, a novel feedback-driven approach that leverages GPT-4, a large-scale language model (LLM), to automatically reproduce Android bug reports. Unlike traditional methods, ReBL bypasses the use of Step to Reproduce (S2R) entities. Instead, it leverages the entire textual bug report and employs innovative prompts to enhance GPT's contextual reasoning. This approach is more flexible and context-aware than the traditional step-by-step entity matching approach, resulting in improved accuracy and effectiveness. In addition to handling crash reports, ReBL has the capability of handling non-crash functional bug reports. Our evaluation of 96 Android bug reports (73 crash and 23 non-crash) demonstrates that ReBL successfully reproduced 90.63% of these reports, averaging only 74.98 seconds per bug report. Additionally, ReBL outperformed three existing tools in both success rate and speed.

**Link**: [arxiv](http://arxiv.org/abs/2407.05165v3),  [pdf](http://arxiv.org/pdf/2407.05165v3)

**Tags**: cs.SE 



### Scaling Up Summarization: Leveraging Large Language Models for Long Text   Extractive Summarization
**Authors**: L√©o Hemamou, Mehdi Debiane

**Updated**: 2024-08-28T13:52:19Z

**Summary**: In an era where digital text is proliferating at an unprecedented rate, efficient summarization tools are becoming indispensable. While Large Language Models (LLMs) have been successfully applied in various NLP tasks, their role in extractive text summarization remains underexplored. This paper introduces EYEGLAXS (Easy Yet Efficient larGe LAnguage model for eXtractive Summarization), a framework that leverages LLMs, specifically LLAMA2-7B and ChatGLM2-6B, for extractive summarization of lengthy text documents. Instead of abstractive methods, which often suffer from issues like factual inaccuracies and hallucinations, EYEGLAXS focuses on extractive summarization to ensure factual and grammatical integrity. Utilizing state-of-the-art techniques such as Flash Attention and Parameter-Efficient Fine-Tuning (PEFT), EYEGLAXS addresses the computational and resource challenges typically associated with LLMs. The system sets new performance benchmarks on well-known datasets like PubMed and ArXiv. Furthermore, we extend our research through additional analyses that explore the adaptability of LLMs in handling different sequence lengths and their efficiency in training on smaller datasets. These contributions not only set a new standard in the field but also open up promising avenues for future research in extractive text summarization.

**Link**: [arxiv](http://arxiv.org/abs/2408.15801v1),  [pdf](http://arxiv.org/pdf/2408.15801v1)

**Tags**: cs.CL 



### Emulating Brain-like Rapid Learning in Neuromorphic Edge Computing
**Authors**: Kenneth Stewart, Michael Neumeier, Sumit Bam Shrestha, Garrick Orchard, Emre Neftci

**Updated**: 2024-08-28T13:51:52Z

**Summary**: Achieving personalized intelligence at the edge with real-time learning capabilities holds enormous promise in enhancing our daily experiences and helping decision making, planning, and sensing. However, efficient and reliable edge learning remains difficult with current technology due to the lack of personalized data, insufficient hardware capabilities, and inherent challenges posed by online learning.   Over time and across multiple developmental stages, the brain has evolved to efficiently incorporate new knowledge by gradually building on previous knowledge. In this work, we emulate the multiple stages of learning with digital neuromorphic technology that simulates the neural and synaptic processes of the brain using two stages of learning. First, a meta-training stage trains the hyperparameters of synaptic plasticity for one-shot learning using a differentiable simulation of the neuromorphic hardware. This meta-training process refines a hardware local three-factor synaptic plasticity rule and its associated hyperparameters to align with the trained task domain. In a subsequent deployment stage, these optimized hyperparameters enable fast, data-efficient, and accurate learning of new classes. We demonstrate our approach using event-driven vision sensor data and the Intel Loihi neuromorphic processor with its plasticity dynamics, achieving real-time one-shot learning of new classes that is vastly improved over transfer learning. Our methodology can be deployed with arbitrary plasticity models and can be applied to situations demanding quick learning and adaptation at the edge, such as navigating unfamiliar environments or learning unexpected categories of data through user engagement.

**Link**: [arxiv](http://arxiv.org/abs/2408.15800v1),  [pdf](http://arxiv.org/pdf/2408.15800v1)

**Tags**: cs.NE cs.AI 



### Evaluating Named Entity Recognition Using Few-Shot Prompting with Large   Language Models
**Authors**: H√©di Zhegidi, Ludovic Moncla

**Updated**: 2024-08-28T13:42:28Z

**Summary**: This paper evaluates Few-Shot Prompting with Large Language Models for Named Entity Recognition (NER). Traditional NER systems rely on extensive labeled datasets, which are costly and time-consuming to obtain. Few-Shot Prompting or in-context learning enables models to recognize entities with minimal examples. We assess state-of-the-art models like GPT-4 in NER tasks, comparing their few-shot performance to fully supervised benchmarks. Results show that while there is a performance gap, large models excel in adapting to new entity types and domains with very limited data. We also explore the effects of prompt engineering, guided output format and context length on performance. This study underscores Few-Shot Learning's potential to reduce the need for large labeled datasets, enhancing NER scalability and accessibility.

**Link**: [arxiv](http://arxiv.org/abs/2408.15796v1),  [pdf](http://arxiv.org/pdf/2408.15796v1)

**Tags**: cs.IR cs.AI 



### Language Adaptation on a Tight Academic Compute Budget: Tokenizer   Swapping Works and Pure bfloat16 Is Enough
**Authors**: Konstantin Dobler, Gerard de Melo

**Updated**: 2024-08-28T13:37:07Z

**Summary**: We investigate continued pretraining of LLMs for language adaptation on a tight academic budget: a setting in which only a few GPUs can be used in parallel, for a heavily constrained duration. We focus on adapting Mistral-7B to German or Arabic and evaluate several techniques to improve efficiency and effectiveness in this setting. Our German models adapted on this tight compute budget underperform compared to the base Mistral-7B, while our Arabic models outperform several baselines, showing that for sufficiently well-represented languages, continued pretraining for specialization is not always helpful. Our main findings focus on training precision and tokenizer swapping. Our results show that pure bfloat16 training is a viable alternative to mixed-precision training, while being much faster when only using a few GPUs. Swapping the tokenizer for a specialized one yields more efficient tokenization and is competitive with the original tokenizer, which already contains some German tokens, but did not significantly increase performance for German. Code and model weights are available at on GitHub.

**Link**: [arxiv](http://arxiv.org/abs/2408.15793v1),  [pdf](http://arxiv.org/pdf/2408.15793v1)

**Tags**: cs.CL cs.LG 



### Efficient LLM Scheduling by Learning to Rank
**Authors**: Yichao Fu, Siqi Zhu, Runlong Su, Aurick Qiao, Ion Stoica, Hao Zhang

**Updated**: 2024-08-28T13:35:54Z

**Summary**: In Large Language Model (LLM) inference, the output length of an LLM request is typically regarded as not known a priori. Consequently, most LLM serving systems employ a simple First-come-first-serve (FCFS) scheduling strategy, leading to Head-Of-Line (HOL) blocking and reduced throughput and service quality. In this paper, we reexamine this assumption -- we show that, although predicting the exact generation length of each request is infeasible, it is possible to predict the relative ranks of output lengths in a batch of requests, using learning to rank. The ranking information offers valuable guidance for scheduling requests. Building on this insight, we develop a novel scheduler for LLM inference and serving that can approximate the shortest-job-first (SJF) schedule better than existing approaches. We integrate this scheduler with the state-of-the-art LLM serving system and show significant performance improvement in several important applications: 2.8x lower latency in chatbot serving and 6.5x higher throughput in synthetic data generation. Our code is available at https://github.com/hao-ai-lab/vllm-ltr.git

**Link**: [arxiv](http://arxiv.org/abs/2408.15792v1),  [pdf](http://arxiv.org/pdf/2408.15792v1)

**Tags**: cs.LG 



### Interactive Agents: Simulating Counselor-Client Psychological Counseling   via Role-Playing LLM-to-LLM Interactions
**Authors**: Huachuan Qiu, Zhenzhong Lan

**Updated**: 2024-08-28T13:29:59Z

**Summary**: Virtual counselors powered by large language models (LLMs) aim to create interactive support systems that effectively assist clients struggling with mental health challenges. To replicate counselor-client conversations, researchers have built an online mental health platform that allows professional counselors to provide clients with text-based counseling services for about an hour per session. Notwithstanding its effectiveness, challenges exist as human annotation is time-consuming, cost-intensive, privacy-protected, and not scalable. To address this issue and investigate the applicability of LLMs in psychological counseling conversation simulation, we propose a framework that employs two LLMs via role-playing for simulating counselor-client interactions. Our framework involves two LLMs, one acting as a client equipped with a specific and real-life user profile and the other playing the role of an experienced counselor, generating professional responses using integrative therapy techniques. We implement both the counselor and the client by zero-shot prompting the GPT-4 model. In order to assess the effectiveness of LLMs in simulating counselor-client interactions and understand the disparities between LLM- and human-generated conversations, we evaluate the synthetic data from various perspectives. We begin by assessing the client's performance through automatic evaluations. Next, we analyze and compare the disparities between dialogues generated by the LLM and those generated by professional counselors. Furthermore, we conduct extensive experiments to thoroughly examine the performance of our LLM-based counselor trained with synthetic interactive dialogues by benchmarking against state-of-the-art models for mental health.

**Link**: [arxiv](http://arxiv.org/abs/2408.15787v1),  [pdf](http://arxiv.org/pdf/2408.15787v1)

**Tags**: cs.CL cs.IR 



### LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language   Models
**Authors**: Jiayi Gui, Yiming Liu, Jiale Cheng, Xiaotao Gu, Xiao Liu, Hongning Wang, Yuxiao Dong, Jie Tang, Minlie Huang

**Updated**: 2024-08-28T13:16:41Z

**Summary**: Large Language Models (LLMs) have demonstrated notable capabilities across various tasks, showcasing complex problem-solving abilities. Understanding and executing complex rules, along with multi-step planning, are fundamental to logical reasoning and critical for practical LLM agents and decision-making systems. However, evaluating LLMs as effective rule-based executors and planners remains underexplored. In this paper, we introduce LogicGame, a novel benchmark designed to evaluate the comprehensive rule understanding, execution, and planning capabilities of LLMs. Unlike traditional benchmarks, LogicGame provides diverse games that contain a series of rules with an initial state, requiring models to comprehend and apply predefined regulations to solve problems. We create simulated scenarios in which models execute or plan operations to achieve specific outcomes. These game scenarios are specifically designed to distinguish logical reasoning from mere knowledge by relying exclusively on predefined rules. This separation allows for a pure assessment of rule-based reasoning capabilities. The evaluation considers not only final outcomes but also intermediate steps, providing a comprehensive assessment of model performance. Moreover, these intermediate steps are deterministic and can be automatically verified. LogicGame defines game scenarios with varying difficulty levels, from simple rule applications to complex reasoning chains, in order to offer a precise evaluation of model performance on rule understanding and multi-step execution. Utilizing LogicGame, we test various LLMs and identify notable shortcomings in their rule-based logical reasoning abilities.

**Link**: [arxiv](http://arxiv.org/abs/2408.15778v1),  [pdf](http://arxiv.org/pdf/2408.15778v1)

**Tags**: cs.AI cs.CL 



### A Survey on Evaluation of Multimodal Large Language Models
**Authors**: Jiaxing Huang, Jingyi Zhang

**Updated**: 2024-08-28T13:05:55Z

**Summary**: Multimodal Large Language Models (MLLMs) mimic human perception and reasoning system by integrating powerful Large Language Models (LLMs) with various modality encoders (e.g., vision, audio), positioning LLMs as the "brain" and various modality encoders as sensory organs. This framework endows MLLMs with human-like capabilities, and suggests a potential pathway towards achieving artificial general intelligence (AGI). With the emergence of all-round MLLMs like GPT-4V and Gemini, a multitude of evaluation methods have been developed to assess their capabilities across different dimensions. This paper presents a systematic and comprehensive review of MLLM evaluation methods, covering the following key aspects: (1) the background of MLLMs and their evaluation; (2) "what to evaluate" that reviews and categorizes existing MLLM evaluation tasks based on the capabilities assessed, including general multimodal recognition, perception, reasoning and trustworthiness, and domain-specific applications such as socioeconomic, natural sciences and engineering, medical usage, AI agent, remote sensing, video and audio processing, 3D point cloud analysis, and others; (3) "where to evaluate" that summarizes MLLM evaluation benchmarks into general and specific benchmarks; (4) "how to evaluate" that reviews and illustrates MLLM evaluation steps and metrics; Our overarching goal is to provide valuable insights for researchers in the field of MLLM evaluation, thereby facilitating the development of more capable and reliable MLLMs. We emphasize that evaluation should be regarded as a critical discipline, essential for advancing the field of MLLMs.

**Link**: [arxiv](http://arxiv.org/abs/2408.15769v1),  [pdf](http://arxiv.org/pdf/2408.15769v1)

**Tags**: cs.CV cs.AI cs.CL 



### Adaptive Traffic Signal Control Using Reinforcement Learning
**Authors**: Muhammad Tahir Rafique, Ahmed Mustafa, Hasan Sajid

**Updated**: 2024-08-28T12:35:56Z

**Summary**: Traffic demand is continuously increasing, leading to significant congestion issues in major urban areas. Constructing new infrastructure is a potential solution but presents a substantial financial burden on national economies. An alternative approach involves optimizing existing traffic networks through the dynamic control of traffic signals at intersections. Recent advancements in Reinforcement Learning (RL) techniques have demonstrated their capability to address the complexities associated with traffic congestion. In this paper, we propose a solution to traffic congestion using reinforcement learning. We define the state as a scalar representing the queue length, demonstrating that the algorithm can effectively learn from this simplified state representation. This approach can potentially reduce deployment costs by minimizing the number of sensors required at intersections. We have developed two RL algorithms: a turn-based agent, which prioritizes traffic signals for the intersection side with higher traffic, and a time-based agent, which adheres to a fixed phase cycle, adjusting the phase duration based on traffic conditions. To assess the performance of these algorithms, we designed four distinct traffic scenarios and computed seven evaluation metrics for each. Simulation results indicate that both algorithms outperform conventional traffic signal control systems.

**Link**: [arxiv](http://arxiv.org/abs/2408.15751v1),  [pdf](http://arxiv.org/pdf/2408.15751v1)

**Tags**: cs.AI 



### A Hybrid Approach for Low-Complexity Joint Acoustic Echo and Noise   Reduction
**Authors**: Shrishti Saha Shetu, Naveen Kumar Desiraju, Jose Miguel Martinez Aponte, Emanu√´l A. P. Habets, Edwin Mabande

**Updated**: 2024-08-28T12:24:54Z

**Summary**: Deep learning-based methods that jointly perform the task of acoustic echo and noise reduction (AENR) often require high memory and computational resources, making them unsuitable for real-time deployment on low-resource platforms such as embedded devices. We propose a low-complexity hybrid approach for joint AENR by employing a single model to suppress both residual echo and noise components. Specifically, we integrate the state-of-the-art (SOTA) ULCNet model, which was originally proposed to achieve ultra-low complexity noise suppression, in a hybrid system and train it for joint AENR. We show that the proposed approach achieves better echo reduction and comparable noise reduction performance with much lower computational complexity and memory requirements than all considered SOTA methods, at the cost of slight degradation in speech quality.

**Link**: [arxiv](http://arxiv.org/abs/2408.15746v1),  [pdf](http://arxiv.org/pdf/2408.15746v1)

**Tags**: eess.AS cs.SD 



### Language-specific Calibration for Pruning Multilingual Language Models
**Authors**: Simon Kurz, Jian-Jia Chen, Lucie Flek, Zhixue Zhao

**Updated**: 2024-08-28T12:03:54Z

**Summary**: Recent advances in large language model (LLM) pruning have shown state-of-the-art compression results in post-training and retraining-free settings while maintaining high predictive performance. However, such research mainly considers calibrating pruning using English text, despite the multilingual nature of modern LLMs and their frequent uses in non-English languages. In this paper, we set out to explore effective strategies for calibrating the pruning of multilingual language models. We present the first comprehensive empirical study, comparing different calibration languages for pruning multilingual models across diverse tasks, models, and state-of-the-art pruning techniques. Our results present practical suggestions, for example, calibrating in the target language can efficiently yield lower perplexity, but does not necessarily benefit downstream tasks. Our further analysis experiments unveil that calibration in the target language mainly contributes to preserving language-specific features related to fluency and coherence, but might not contribute to capturing language-agnostic features such as language understanding and reasoning. Last, we provide practical recommendations for future practitioners.

**Link**: [arxiv](http://arxiv.org/abs/2408.14398v2),  [pdf](http://arxiv.org/pdf/2408.14398v2)

**Tags**: cs.CL cs.AI cs.LG 



### Conan-embedding: General Text Embedding with More and Better Negative   Samples
**Authors**: Shiyu Li, Yang Tang, Shizhe Chen, Xi Chen

**Updated**: 2024-08-29T14:47:37Z

**Summary**: With the growing popularity of RAG, the capabilities of embedding models are gaining increasing attention. Embedding models are primarily trained through contrastive loss learning, with negative examples being a key component. Previous work has proposed various hard negative mining strategies, but these strategies are typically employed as preprocessing steps. In this paper, we propose the conan-embedding model, which maximizes the utilization of more and higher-quality negative examples. Specifically, since the model's ability to handle preprocessed negative examples evolves during training, we propose dynamic hard negative mining method to expose the model to more challenging negative examples throughout the training process. Secondly, contrastive learning requires as many negative examples as possible but is limited by GPU memory constraints. Therefore, we use a Cross-GPU balancing Loss to provide more negative examples for embedding training and balance the batch size across multiple tasks. Moreover, we also discovered that the prompt-response pairs from LLMs can be used for embedding training. Our approach effectively enhances the capabilities of embedding models, currently ranking first on the Chinese leaderboard of Massive text embedding benchmark

**Link**: [arxiv](http://arxiv.org/abs/2408.15710v2),  [pdf](http://arxiv.org/pdf/2408.15710v2)

**Tags**: cs.CL 



### Evading AI-Generated Content Detectors using Homoglyphs
**Authors**: Aldan Creo, Shushanta Pudasaini

**Updated**: 2024-08-28T11:10:59Z

**Summary**: The advent of large language models (LLMs) has enabled the generation of text that increasingly exhibits human-like characteristics. As the detection of such content is of significant importance, numerous studies have been conducted with the aim of developing reliable AI-generated text detectors. These detectors have demonstrated promising results on test data, but recent research has revealed that they can be circumvented by employing different techniques. In this paper, we present homoglyph-based attacks ($a \rightarrow {\alpha}$) as a means of circumventing existing detectors. A comprehensive evaluation was conducted to assess the effectiveness of these attacks on seven detectors, including ArguGPT, Binoculars, DetectGPT, Fast-DetectGPT, Ghostbuster, OpenAI's detector, and watermarking techniques, on five different datasets. Our findings demonstrate that homoglyph-based attacks can effectively circumvent state-of-the-art detectors, leading them to classify all texts as either AI-generated or human-written (decreasing the average Matthews Correlation Coefficient from 0.64 to -0.01). We then examine the effectiveness of these attacks by analyzing how homoglyphs impact different families of detectors. Finally, we discuss the implications of these findings and potential defenses against such attacks.

**Link**: [arxiv](http://arxiv.org/abs/2406.11239v2),  [pdf](http://arxiv.org/pdf/2406.11239v2)

**Tags**: cs.CL cs.AI 



### Comparing diversity, negativity, and stereotypes in Chinese-language AI   technologies: a case study on Baidu, Ernie and Qwen
**Authors**: Geng Liu, Carlo Alberto Bono, Francesco Pierri

**Updated**: 2024-08-28T10:51:18Z

**Summary**: Large Language Models (LLMs) and search engines have the potential to perpetuate biases and stereotypes by amplifying existing prejudices in their training data and algorithmic processes, thereby influencing public perception and decision-making. While most work has focused on Western-centric AI technologies, we study Chinese-based tools by investigating social biases embedded in the major Chinese search engine, Baidu, and two leading LLMs, Ernie and Qwen. Leveraging a dataset of 240 social groups across 13 categories describing Chinese society, we collect over 30k views encoded in the aforementioned tools by prompting them for candidate words describing such groups. We find that language models exhibit a larger variety of embedded views compared to the search engine, although Baidu and Qwen generate negative content more often than Ernie. We also find a moderate prevalence of stereotypes embedded in the language models, many of which potentially promote offensive and derogatory views. Our work highlights the importance of promoting fairness and inclusivity in AI technologies with a global perspective.

**Link**: [arxiv](http://arxiv.org/abs/2408.15696v1),  [pdf](http://arxiv.org/pdf/2408.15696v1)

**Tags**: cs.CY 



### Deciphering the Impact of Pretraining Data on Large Language Models   through Machine Unlearning
**Authors**: Yang Zhao, Li Du, Xiao Ding, Kai Xiong, Zhouhao Sun, Jun Shi, Ting Liu, Bing Qin

**Updated**: 2024-08-28T10:39:11Z

**Summary**: Through pretraining on a corpus with various sources, Large Language Models (LLMs) have gained impressive performance. However, the impact of each component of the pretraining corpus remains opaque. As a result, the organization of the pretraining corpus is still empirical and may deviate from the optimal. To address this issue, we systematically analyze the impact of 48 datasets from 5 major categories of pretraining data of LLMs and measure their impacts on LLMs using benchmarks about nine major categories of model capabilities. Our analyses provide empirical results about the contribution of multiple corpora on the performances of LLMs, along with their joint impact patterns, including complementary, orthogonal, and correlational relationships. We also identify a set of ``high-impact data'' such as Books that is significantly related to a set of model capabilities. These findings provide insights into the organization of data to support more efficient pretraining of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2402.11537v3),  [pdf](http://arxiv.org/pdf/2402.11537v3)

**Tags**: cs.CL cs.AI 



### A quasi-ohmic back contact achieved by inserting single-crystal graphene   in flexible Kesterite solar cells
**Authors**: Yixiong Ji, Wentong Yang, Di Yan, Wei Luo, Jialu Li, Shi Tang, Jintao Fu, James Bullock, Mei Gao, Xin Li, Zhancheng Li, Jun Yang, Xingzhan Wei, Haofei Shi, Fangyang Liu, Paul Mulvaney

**Updated**: 2024-08-28T10:18:29Z

**Summary**: Flexible photovoltaics with a lightweight and adaptable nature that allows for deployment on curved surfaces and in building facades have always been a goal vigorously pursued by researchers in thin-film solar cell technology. The recent strides made in improving the sunlight-to-electricity conversion efficiency of kesterite Cu$_{2}$ZnSn(S, Se)$_{4}$ (CZTSSe) suggest it to be a perfect candidate. However, making use of rare Mo foil in CZTSSe solar cells causes severe problems in thermal expansion matching, uneven grain growth, and severe problems at the back contact of the devices. Herein, a strategy utilizing single-crystal graphene to modify the back interface of flexible CZTSSe solar cells is proposed. It will be shown that the insertion of graphene at the Mo foil/CZTSSe interface provides strong physical support for the subsequent deposition of the CZTSSe absorber layer, improving the adhesion between the absorber layer and the Mo foil substrate. Additionally, the graphene passivates the rough sites on the surface of the Mo foil, enhancing the chemical homogeneity of the substrate, and resulting in a more crystalline and homogeneous CZTSSe absorber layer on the Mo foil substrate. The detrimental reaction between Mo and CZTSSe has also been eliminated. Through an analysis of the electrical properties, it is found that the introduction of graphene at the back interface promotes the formation of a quasi-ohmic contact at the back contact, decreasing the back contact barrier of the solar cell, and leading to efficient collection of charges at the back interface. This investigation demonstrates that solution-based CZTSSe photovoltaic devices could form the basis of cheap and flexible solar cells.

**Link**: [arxiv](http://arxiv.org/abs/2408.15684v1),  [pdf](http://arxiv.org/pdf/2408.15684v1)

**Tags**: cond-mat.mtrl-sci cond-mat.mes-hall 



### Training-Free Action Recognition and Goal Inference with Dynamic Frame   Selection
**Authors**: Ee Yeo Keat, Zhang Hao, Alexander Matyasko, Basura Fernando

**Updated**: 2024-08-28T09:48:24Z

**Summary**: We introduce VidTFS, a Training-free, open-vocabulary video goal and action inference framework that combines the frozen vision foundational model (VFM) and large language model (LLM) with a novel dynamic Frame Selection module. Our experiments demonstrate that the proposed frame selection module improves the performance of the framework significantly. We validate the performance of the proposed VidTFS on four widely used video datasets, including CrossTask, COIN, UCF101, and ActivityNet, covering goal inference and action recognition tasks under open-vocabulary settings without requiring any training or fine-tuning. The results show that VidTFS outperforms pretrained and instruction-tuned multimodal language models that directly stack LLM and VFM for downstream video inference tasks. Our VidTFS with its adaptability shows the future potential for generalizing to new training-free video inference tasks.

**Link**: [arxiv](http://arxiv.org/abs/2401.12471v2),  [pdf](http://arxiv.org/pdf/2401.12471v2)

**Tags**: cs.CV 



### StyleRemix: Interpretable Authorship Obfuscation via Distillation and   Perturbation of Style Elements
**Authors**: Jillian Fisher, Skyler Hallinan, Ximing Lu, Mitchell Gordon, Zaid Harchaoui, Yejin Choi

**Updated**: 2024-08-28T09:35:15Z

**Summary**: Authorship obfuscation, rewriting a text to intentionally obscure the identity of the author, is an important but challenging task. Current methods using large language models (LLMs) lack interpretability and controllability, often ignoring author-specific stylistic features, resulting in less robust performance overall.   To address this, we develop StyleRemix, an adaptive and interpretable obfuscation method that perturbs specific, fine-grained style elements of the original input text. StyleRemix uses pre-trained Low Rank Adaptation (LoRA) modules to rewrite an input specifically along various stylistic axes (e.g., formality and length) while maintaining low computational cost. StyleRemix outperforms state-of-the-art baselines and much larger LLMs in a variety of domains as assessed by both automatic and human evaluation.   Additionally, we release AuthorMix, a large set of 30K high-quality, long-form texts from a diverse set of 14 authors and 4 domains, and DiSC, a parallel corpus of 1,500 texts spanning seven style axes in 16 unique directions

**Link**: [arxiv](http://arxiv.org/abs/2408.15666v1),  [pdf](http://arxiv.org/pdf/2408.15666v1)

**Tags**: cs.CL 



### An Empirical Study on Self-correcting Large Language Models for Data   Science Code Generation
**Authors**: Thai Tang Quoc, Duc Ha Minh, Tho Quan Thanh, Anh Nguyen-Duc

**Updated**: 2024-08-28T09:19:09Z

**Summary**: Large Language Models (LLMs) have recently advanced many applications on software engineering tasks, particularly the potential for code generation. Among contemporary challenges, code generated by LLMs often suffers from inaccuracies and hallucinations, requiring external inputs to correct. One recent strategy to fix these issues is to refine the code generated from LLMs using the input from the model itself (self-augmented). In this work, we proposed a novel method, namely CoT-SelfEvolve. CoT-SelfEvolve iteratively and automatically refines code through a self-correcting process, guided by a chain of thought constructed from real-world programming problem feedback. Focusing on data science code, including Python libraries such as NumPy and Pandas, our evaluations on the DS-1000 dataset demonstrate that CoT-SelfEvolve significantly outperforms existing models in solving complex problems. The framework shows substantial improvements in both initial code generation and subsequent iterations, with the model's accuracy increasing significantly with each additional iteration. This highlights the effectiveness of using chain-of-thought prompting to address complexities revealed by program executor traceback error messages. We also discuss how CoT-SelfEvolve can be integrated into continuous software engineering environments, providing a practical solution for improving LLM-based code generation.

**Link**: [arxiv](http://arxiv.org/abs/2408.15658v1),  [pdf](http://arxiv.org/pdf/2408.15658v1)

**Tags**: cs.SE cs.AI 



### TokenPacker: Efficient Visual Projector for Multimodal LLM
**Authors**: Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jie Qin, Jianke Zhu, Lei Zhang

**Updated**: 2024-08-28T08:49:57Z

**Summary**: The visual projector serves as an essential bridge between the visual encoder and the Large Language Model (LLM) in a Multimodal LLM (MLLM). Typically, MLLMs adopt a simple MLP to preserve all visual contexts via one-to-one transformation. However, the visual tokens are redundant and can be considerably increased when dealing with high-resolution images, impairing the efficiency of MLLMs significantly. Some recent works have introduced resampler or abstractor to reduce the number of resulting visual tokens. Unfortunately, they fail to capture finer details and undermine the visual reasoning capabilities of MLLMs. In this work, we propose a novel visual projector, which adopts a coarse-to-fine scheme to inject the enriched characteristics to generate the condensed visual tokens. In specific, we first interpolate the visual features as a low-resolution point query, providing the overall visual representation as the foundation. Then, we introduce a region-to-point injection module that utilizes high-resolution, multi-level region-based cues as fine-grained reference keys and values, allowing them to be fully absorbed within the corresponding local context region. This step effectively updates the coarse point query, transforming it into an enriched one for the subsequent LLM reasoning. Extensive experiments demonstrate that our approach compresses the visual tokens by 75%~89%, while achieves comparable or even better performance across diverse benchmarks with significantly higher efficiency. The source codes can be found at https://github.com/CircleRadon/TokenPacker.

**Link**: [arxiv](http://arxiv.org/abs/2407.02392v4),  [pdf](http://arxiv.org/pdf/2407.02392v4)

**Tags**: cs.CV 



### Large Language Model Sentinel: LLM Agent for Adversarial Purification
**Authors**: Guang Lin, Qibin Zhao

**Updated**: 2024-08-28T08:46:17Z

**Summary**: Over the past two years, the use of large language models (LLMs) has advanced rapidly. While these LLMs offer considerable convenience, they also raise security concerns, as LLMs are vulnerable to adversarial attacks by some well-designed textual perturbations. In this paper, we introduce a novel defense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is designed to enhance the adversarial robustness of LLMs by purifying the adversarial textual examples before feeding them into the target LLM. Our method comprises two main components: a) Agent instruction, which can simulate a new agent for adversarial defense, altering minimal characters to maintain the original meaning of the sentence while defending against attacks; b) Defense guidance, which provides strategies for modifying clean or adversarial examples to ensure effective defense and accurate outputs from the target LLMs. Remarkably, the defense agent demonstrates robust defensive capabilities even without learning from adversarial examples. Additionally, we conduct an intriguing adversarial experiment where we develop two agents, one for defense and one for attack, and engage them in mutual confrontation. During the adversarial interactions, neither agent completely beat the other. Extensive experiments on both open-source and closed-source LLMs demonstrate that our method effectively defends against adversarial attacks, thereby enhancing adversarial robustness.

**Link**: [arxiv](http://arxiv.org/abs/2405.20770v3),  [pdf](http://arxiv.org/pdf/2405.20770v3)

**Tags**: cs.CL cs.AI cs.CR 



### DocLayLLM: An Efficient and Effective Multi-modal Extension of Large   Language Models for Text-rich Document Understanding
**Authors**: Wenhui Liao, Jiapeng Wang, Hongliang Li, Chengyu Wang, Jun Huang, Lianwen Jin

**Updated**: 2024-08-28T08:32:44Z

**Summary**: Text-rich document understanding (TDU) refers to analyzing and comprehending documents containing substantial textual content. With the rapid evolution of large language models (LLMs), they have been widely leveraged for TDU due to their remarkable versatility and generalization. In this paper, we introduce DocLayLLM, an efficient and effective multi-modal extension of LLMs specifically designed for TDU. By integrating visual patch tokens and 2D positional tokens into LLMs and encoding the document content using the LLMs themselves, we fully take advantage of the document comprehension capability of LLMs and enhance their perception of OCR information. We have also deeply considered the role of the chain-of-thought (CoT) and innovatively proposed the techniques of CoT Pre-training and CoT Annealing. Our DocLayLLM can achieve remarkable performances with lightweight training settings, showcasing its efficiency and effectiveness. Experimental results demonstrate that our DocLayLLM surpasses existing OCR-dependent methods and also outperforms OCR-free competitors.

**Link**: [arxiv](http://arxiv.org/abs/2408.15045v2),  [pdf](http://arxiv.org/pdf/2408.15045v2)

**Tags**: cs.CV 



### CodeSift: An LLM-Based Reference-Less Framework for Automatic Code   Validation
**Authors**: Pooja Aggarwal, Oishik Chatterjee, Ting Dai, Prateeti Mohapatra, Brent Paulovicks, Brad Blancett, Arthur De Magalhaes

**Updated**: 2024-08-28T08:32:21Z

**Summary**: The advent of large language models (LLMs) has greatly facilitated code generation, but ensuring the functional correctness of generated code remains a challenge. Traditional validation methods are often time-consuming, error-prone, and impractical for large volumes of code. We introduce CodeSift, a novel framework that leverages LLMs as the first-line filter of code validation without the need for execution, reference code, or human feedback, thereby reducing the validation effort. We assess the effectiveness of our method across three diverse datasets encompassing two programming languages. Our results indicate that CodeSift outperforms state-of-the-art code evaluation methods. Internal testing conducted with subject matter experts reveals that the output generated by CodeSift is in line with human preference, reinforcing its effectiveness as a dependable automated code validation tool.

**Link**: [arxiv](http://arxiv.org/abs/2408.15630v1),  [pdf](http://arxiv.org/pdf/2408.15630v1)

**Tags**: cs.SE cs.AI 



### Can Visual Language Models Replace OCR-Based Visual Question Answering   Pipelines in Production? A Case Study in Retail
**Authors**: Bianca Lamm, Janis Keuper

**Updated**: 2024-08-28T08:25:41Z

**Summary**: Most production-level deployments for Visual Question Answering (VQA) tasks are still build as processing pipelines of independent steps including image pre-processing, object- and text detection, Optical Character Recognition (OCR) and (mostly supervised) object classification. However, the recent advances in vision Foundation Models [25] and Vision Language Models (VLMs) [23] raise the question if these custom trained, multi-step approaches can be replaced with pre-trained, single-step VLMs. This paper analyzes the performance and limits of various VLMs in the context of VQA and OCR [5, 9, 12] tasks in a production-level scenario. Using data from the Retail-786k [10] dataset, we investigate the capabilities of pre-trained VLMs to answer detailed questions about advertised products in images. Our study includes two commercial models, GPT-4V [16] and GPT-4o [17], as well as four open-source models: InternVL [5], LLaVA 1.5 [12], LLaVA-NeXT [13], and CogAgent [9]. Our initial results show, that there is in general no big performance gap between open-source and commercial models. However, we observe a strong task dependent variance in VLM performance: while most models are able to answer questions regarding the product brand and price with high accuracy, they completely fail at the same time to correctly identity the specific product name or discount. This indicates the problem of VLMs to solve fine-grained classification tasks as well to model the more abstract concept of discounts.

**Link**: [arxiv](http://arxiv.org/abs/2408.15626v1),  [pdf](http://arxiv.org/pdf/2408.15626v1)

**Tags**: cs.CV 



### CBF-LLM: Safe Control for LLM Alignment
**Authors**: Yuya Miyaoka, Masaki Inoue

**Updated**: 2024-08-28T08:25:22Z

**Summary**: This paper proposes a control-based framework for aligning large language models (LLMs) by leveraging a control barrier function (CBF) to ensure user-desirable text generation. The presented framework applies the safety filter, designed based on the CBF, to the output generation of the baseline LLM, i.e., the sequence of the token, with the aim of intervening in the generated text. The overall text-generation system is implemented with Llama 3 and a RoBERTa model, and the source code is available at https://github.com/Mya-Mya/CBF-LLM. The experiment demonstrates its control ability and effectiveness in reducing the number of interventions needed for user-specified alignment tasks.

**Link**: [arxiv](http://arxiv.org/abs/2408.15625v1),  [pdf](http://arxiv.org/pdf/2408.15625v1)

**Tags**: eess.SY cs.AI cs.CL cs.SY 



### AI-native Memory: A Pathway from LLMs Towards AGI
**Authors**: Jingbo Shang, Zai Zheng, Jiale Wei, Xiang Ying, Felix Tao, Mindverse Team

**Updated**: 2024-08-28T08:07:49Z

**Summary**: Large language models (LLMs) have demonstrated the world with the sparks of artificial general intelligence (AGI). One opinion, especially from some startups working on LLMs, argues that an LLM with nearly unlimited context length can realize AGI. However, they might be too optimistic about the long-context capability of (existing) LLMs -- (1) Recent literature has shown that their effective context length is significantly smaller than their claimed context length; and (2) Our reasoning-in-a-haystack experiments further demonstrate that simultaneously finding the relevant information from a long context and conducting (simple) reasoning is nearly impossible. In this paper, we envision a pathway from LLMs to AGI through the integration of \emph{memory}. We believe that AGI should be a system where LLMs serve as core processors. In addition to raw data, the memory in this system would store a large number of important conclusions derived from reasoning processes. Compared with retrieval-augmented generation (RAG) that merely processing raw data, this approach not only connects semantically related information closer, but also simplifies complex inferences at the time of querying. As an intermediate stage, the memory will likely be in the form of natural language descriptions, which can be directly consumed by users too. Ultimately, every agent/person should have its own large personal model, a deep neural network model (thus \emph{AI-native}) that parameterizes and compresses all types of memory, even the ones cannot be described by natural languages. Finally, we discuss the significant potential of AI-native memory as the transformative infrastructure for (proactive) engagement, personalization, distribution, and social in the AGI era, as well as the incurred privacy and security challenges with preliminary solutions.

**Link**: [arxiv](http://arxiv.org/abs/2406.18312v4),  [pdf](http://arxiv.org/pdf/2406.18312v4)

**Tags**: cs.CL cs.AI 



### SIaM: Self-Improving Code-Assisted Mathematical Reasoning of Large   Language Models
**Authors**: Dian Yu, Baolin Peng, Ye Tian, Linfeng Song, Haitao Mi, Dong Yu

**Updated**: 2024-08-28T06:33:03Z

**Summary**: There is a growing trend of teaching large language models (LLMs) to solve mathematical problems through coding. Existing studies primarily focus on prompting powerful, closed-source models to generate seed training data followed by in-domain data augmentation, equipping LLMs with considerable capabilities for code-aided mathematical reasoning. However, continually training these models on augmented data derived from a few datasets such as GSM8K may impair their generalization abilities and restrict their effectiveness to a narrow range of question types. Conversely, the potential of improving such LLMs by leveraging large-scale, expert-written, diverse math question-answer pairs remains unexplored. To utilize these resources and tackle unique challenges such as code response assessment, we propose a novel paradigm that uses a code-based critic model to guide steps including question-code data construction, quality control, and complementary evaluation. We also explore different alignment algorithms with self-generated instruction/preference data to foster continuous improvement. Experiments across both in-domain (up to +5.7%) and out-of-domain (+4.4%) benchmarks in English and Chinese demonstrate the effectiveness of the proposed paradigm.

**Link**: [arxiv](http://arxiv.org/abs/2408.15565v1),  [pdf](http://arxiv.org/pdf/2408.15565v1)

**Tags**: cs.CL 



### Boosting Lossless Speculative Decoding via Feature Sampling and Partial   Alignment Distillation
**Authors**: Lujun Gui, Bin Xiao, Lei Su, Weipeng Chen

**Updated**: 2024-08-28T06:28:01Z

**Summary**: Lossless speculative decoding accelerates target large language model (LLM) inference by employing a lightweight draft model for generating tree-structured candidates, which are subsequently verified in parallel by the target LLM. Currently, effective approaches leverage feature-level rather than token-level autoregression within the draft model to facilitate more straightforward predictions and enhanced knowledge distillation. In this paper, we reassess these approaches and propose FSPAD (Feature Sampling and Partial Alignment Distillation for Lossless Speculative Decoding), which introduces two straightforward and effective components within the existing framework to boost lossless speculative decoding. Firstly, FSPAD utilizes token embeddings to sample features of the target LLM in high-dimensional space before feeding them into the draft model, due to the inherent uncertainty of the features preventing the draft model from obtaining the specific token output by the target LLM. Secondly, FSPAD introduces partial alignment distillation to weaken the draft model's connection between features and logits, aiming to reduce the conflict between feature alignment and logit confidence during training. Our experiments include both greedy and non-greedy decoding on the largest and smallest models from the Vicuna and LLaMA3-Instruct series, as well as tasks in multi-turn conversation, translation, summarization, question answering, mathematical reasoning, and retrieval-augmented generation. The results show that FSPAD outperforms the state-of-the-art method across all the aforementioned tasks and target LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2408.15562v1),  [pdf](http://arxiv.org/pdf/2408.15562v1)

**Tags**: cs.CL cs.LG 



### AnomalyLLM: Few-shot Anomaly Edge Detection for Dynamic Graphs using   Large Language Models
**Authors**: Shuo Liu, Di Yao, Lanting Fang, Zhetao Li, Wenbin Li, Kaiyu Feng, XiaoWen Ji, Jingping Bi

**Updated**: 2024-08-28T06:18:28Z

**Summary**: Detecting anomaly edges for dynamic graphs aims to identify edges significantly deviating from the normal pattern and can be applied in various domains, such as cybersecurity, financial transactions and AIOps. With the evolving of time, the types of anomaly edges are emerging and the labeled anomaly samples are few for each type. Current methods are either designed to detect randomly inserted edges or require sufficient labeled data for model training, which harms their applicability for real-world applications. In this paper, we study this problem by cooperating with the rich knowledge encoded in large language models(LLMs) and propose a method, namely AnomalyLLM. To align the dynamic graph with LLMs, AnomalyLLM pre-trains a dynamic-aware encoder to generate the representations of edges and reprograms the edges using the prototypes of word embeddings. Along with the encoder, we design an in-context learning framework that integrates the information of a few labeled samples to achieve few-shot anomaly detection. Experiments on four datasets reveal that AnomalyLLM can not only significantly improve the performance of few-shot anomaly detection, but also achieve superior results on new anomalies without any update of model parameters.

**Link**: [arxiv](http://arxiv.org/abs/2405.07626v2),  [pdf](http://arxiv.org/pdf/2405.07626v2)

**Tags**: cs.LG cs.AI 



### Geo-Llama: Leveraging LLMs for Human Mobility Trajectory Generation with   Spatiotemporal Constraints
**Authors**: Siyu Li, Toan Tran, Haowen Lin, John Krumm, Cyrus Shahabi, Li Xiong

**Updated**: 2024-08-28T06:16:42Z

**Summary**: Simulating human mobility data is essential for various application domains, including transportation, urban planning, and epidemic control, since real data are often inaccessible to researchers due to expensive costs and privacy issues. Several existing deep generative solutions propose learning from real trajectories to generate synthetic ones. Despite the progress, most of them suffer from training stability issues and scale poorly with growing data size. More importantly, they generally lack control mechanisms to steer the generated trajectories based on spatiotemporal constraints such as fixing specific visits. To address such limitations, we formally define the controlled trajectory generation problem with spatiotemporal constraints and propose Geo-Llama. This novel LLM-inspired framework enforces explicit visit constraints in a contextually coherent way. It fine-tunes pre-trained LLMs on trajectories with a visit-wise permutation strategy where each visit corresponds to a time and location. This enables the model to capture the spatiotemporal patterns regardless of visit orders and allows flexible and in-context constraint integration through prompts during generation. Extensive experiments on real-world and synthetic datasets validate the effectiveness of Geo-Llama, demonstrating its versatility and robustness in handling a broad range of constraints to generate more realistic trajectories compared to existing methods.

**Link**: [arxiv](http://arxiv.org/abs/2408.13918v2),  [pdf](http://arxiv.org/pdf/2408.13918v2)

**Tags**: cs.AI 



### WildFeedback: Aligning LLMs With In-situ User Interactions And Feedback
**Authors**: Taiwei Shi, Zhuoer Wang, Longqi Yang, Ying-Chun Lin, Zexue He, Mengting Wan, Pei Zhou, Sujay Jauhar, Xiaofeng Xu, Xia Song, Jennifer Neville

**Updated**: 2024-08-28T05:53:46Z

**Summary**: As large language models (LLMs) continue to advance, aligning these models with human preferences has emerged as a critical challenge. Traditional alignment methods, relying on human or LLM annotated datasets, are limited by their resource-intensive nature, inherent subjectivity, and the risk of feedback loops that amplify model biases. To overcome these limitations, we introduce WildFeedback, a novel framework that leverages real-time, in-situ user interactions to create preference datasets that more accurately reflect authentic human values. WildFeedback operates through a three-step process: feedback signal identification, preference data construction, and user-guided evaluation. We applied this framework to a large corpus of user-LLM conversations, resulting in a rich preference dataset that reflects genuine user preferences. This dataset captures the nuances of user preferences by identifying and classifying feedback signals within natural conversations, thereby enabling the construction of more representative and context-sensitive alignment data. Our extensive experiments demonstrate that LLMs fine-tuned on WildFeedback exhibit significantly improved alignment with user preferences, as evidenced by both traditional benchmarks and our proposed user-guided evaluation. By incorporating real-time feedback from actual users, WildFeedback addresses the scalability, subjectivity, and bias challenges that plague existing approaches, marking a significant step toward developing LLMs that are more responsive to the diverse and evolving needs of their users. In summary, WildFeedback offers a robust, scalable solution for aligning LLMs with true human values, setting a new standard for the development and evaluation of user-centric language models.

**Link**: [arxiv](http://arxiv.org/abs/2408.15549v1),  [pdf](http://arxiv.org/pdf/2408.15549v1)

**Tags**: cs.CL 



### SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding
**Authors**: Sihang Li, Jian Huang, Jiaxi Zhuang, Yaorui Shi, Xiaochen Cai, Mingjun Xu, Xiang Wang, Linfeng Zhang, Guolin Ke, Hengxing Cai

**Updated**: 2024-08-28T05:41:52Z

**Summary**: Scientific literature understanding is crucial for extracting targeted information and garnering insights, thereby significantly advancing scientific discovery. Despite the remarkable success of Large Language Models (LLMs), they face challenges in scientific literature understanding, primarily due to (1) a lack of scientific knowledge and (2) unfamiliarity with specialized scientific tasks.   To develop an LLM specialized in scientific literature understanding, we propose a hybrid strategy that integrates continual pre-training (CPT) and supervised fine-tuning (SFT), to simultaneously infuse scientific domain knowledge and enhance instruction-following capabilities for domain-specific tasks.cIn this process, we identify two key challenges: (1) constructing high-quality CPT corpora, and (2) generating diverse SFT instructions. We address these challenges through a meticulous pipeline, including PDF text extraction, parsing content error correction, quality filtering, and synthetic instruction creation. Applying this strategy, we present a suite of LLMs: SciLitLLM, specialized in scientific literature understanding. These models demonstrate promising performance on scientific literature understanding benchmarks.   Our contributions are threefold: (1) We present an effective framework that integrates CPT and SFT to adapt LLMs to scientific literature understanding, which can also be easily adapted to other domains. (2) We propose an LLM-based synthesis method to generate diverse and high-quality scientific instructions, resulting in a new instruction set -- SciLitIns -- for supervised fine-tuning in less-represented scientific domains. (3) SciLitLLM achieves promising performance improvements on scientific literature understanding benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2408.15545v1),  [pdf](http://arxiv.org/pdf/2408.15545v1)

**Tags**: cs.LG cs.CL 



### Kangaroo: A Powerful Video-Language Model Supporting Long-context Video   Input
**Authors**: Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, Xiaoming Wei, Jianbin Jiao, Enhua Wu, Jie Hu

**Updated**: 2024-08-28T05:34:14Z

**Summary**: Rapid advancements have been made in extending Large Language Models (LLMs) to Large Multi-modal Models (LMMs). However, extending input modality of LLMs to video data remains a challenging endeavor, especially for long videos. Due to insufficient access to large-scale high-quality video data and the excessive compression of visual features, current methods exhibit limitations in effectively processing long videos. In this paper, we introduce Kangaroo, a powerful Video LMM aimed at addressing these challenges. Confronted with issue of inadequate training data, we develop a data curation system to build a large-scale dataset with high-quality annotations for vision-language pre-training and instruction tuning. In addition, we design a curriculum training pipeline with gradually increasing resolution and number of input frames to accommodate long videos. Evaluation results demonstrate that, with 8B parameters, Kangaroo achieves state-of-the-art performance across a variety of video understanding benchmarks while exhibiting competitive results on others. Particularly, on benchmarks specialized for long videos, Kangaroo excels some larger models with over 10B parameters and proprietary models.

**Link**: [arxiv](http://arxiv.org/abs/2408.15542v1),  [pdf](http://arxiv.org/pdf/2408.15542v1)

**Tags**: cs.CV cs.AI cs.MM 



### LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via   Layer-wise Relevance Propagation
**Authors**: Haichuan Hu, Yuhan Sun, Quanjun Zhang

**Updated**: 2024-08-29T08:45:30Z

**Summary**: Retrieval-Augmented Generation (RAG) has become a primary technique for mitigating hallucinations in large language models (LLMs). However, incomplete knowledge extraction and insufficient understanding can still mislead LLMs to produce irrelevant or even contradictory responses, which means hallucinations persist in RAG. In this paper, we propose LRP4RAG, a method based on the Layer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations in RAG. Specifically, we first utilize LRP to compute the relevance between the input and output of the RAG generator. We then apply further extraction and resampling to the relevance matrix. The processed relevance data are input into multiple classifiers to determine whether the output contains hallucinations. To the best of our knowledge, this is the first time that LRP has been used for detecting RAG hallucinations, and extensive experiments demonstrate that LRP4RAG outperforms existing baselines.

**Link**: [arxiv](http://arxiv.org/abs/2408.15533v2),  [pdf](http://arxiv.org/pdf/2408.15533v2)

**Tags**: cs.CL cs.AI 



### Depth-Weighted Detection of Behaviours of Risk in People with Dementia   using Cameras
**Authors**: Pratik K. Mishra, Irene Ballester, Andrea Iaboni, Bing Ye, Kristine Newman, Alex Mihailidis, Shehroz S. Khan

**Updated**: 2024-08-28T04:12:07Z

**Summary**: The behavioural and psychological symptoms of dementia, such as agitation and aggression, present a significant health and safety risk in residential care settings. Many care facilities have video cameras in place for digital monitoring of public spaces, which can be leveraged to develop an automated behaviours of risk detection system that can alert the staff to enable timely intervention and prevent the situation from escalating. However, one of the challenges in our previous study was the presence of false alarms due to obstruction of view by activities happening close to the camera. To address this issue, we proposed a novel depth-weighted loss function to train a customized convolutional autoencoder to enforce equivalent importance to the events happening both near and far from the cameras; thus, helping to reduce false alarms and making the method more suitable for real-world deployment. The proposed method was trained using data from nine participants with dementia across three cameras situated in a specialized dementia unit and achieved an area under the curve of receiver operating characteristic of $0.852$, $0.81$ and $0.768$ for the three cameras. Ablation analysis was conducted for the individual components of the proposed method and the performance of the proposed method was investigated for participant-specific and sex-specific behaviours of risk detection. The proposed method performed reasonably well in detecting behaviours of risk in people with dementia motivating further research toward the development of a behaviours of risk detection system suitable for deployment in video surveillance systems in care facilities.

**Link**: [arxiv](http://arxiv.org/abs/2408.15519v1),  [pdf](http://arxiv.org/pdf/2408.15519v1)

**Tags**: cs.CV 



### Affordable Generative Agents
**Authors**: Yangbin Yu, Qin Zhang, Junyou Li, Qiang Fu, Deheng Ye

**Updated**: 2024-08-28T04:04:45Z

**Summary**: The emergence of large language models (LLMs) has significantly advanced the simulation of believable interactive agents. However, the substantial cost on maintaining the prolonged agent interactions poses challenge over the deployment of believable LLM-based agents. Therefore, in this paper, we develop Affordable Generative Agents (AGA), a framework for enabling the generation of believable and low-cost interactions on both agent-environment and inter-agents levels. Specifically, for agent-environment interactions, we substitute repetitive LLM inferences with learned policies; while for inter-agent interactions, we model the social relationships between agents and compress auxiliary dialogue information. Extensive experiments on multiple environments show the effectiveness and efficiency of our proposed framework. Also, we delve into the mechanisms of emergent believable behaviors lying in LLM agents, demonstrating that agents can only generate finite behaviors in fixed environments, based upon which, we understand ways to facilitate emergent interaction behaviors. Our code is publicly available at: https://github.com/AffordableGenerativeAgents/Affordable-Generative-Agents.

**Link**: [arxiv](http://arxiv.org/abs/2402.02053v2),  [pdf](http://arxiv.org/pdf/2402.02053v2)

**Tags**: cs.AI cs.HC 



### A Survey of Large Language Models for European Languages
**Authors**: Wazir Ali, Sampo Pyysalo

**Updated**: 2024-08-28T03:56:37Z

**Summary**: Large Language Models (LLMs) have gained significant attention due to their high performance on a wide range of natural language tasks since the release of ChatGPT. The LLMs learn to understand and generate language by training billions of model parameters on vast volumes of text data. Despite being a relatively new field, LLM research is rapidly advancing in various directions. In this paper, we present an overview of LLM families, including LLaMA, PaLM, GPT, and MoE, and the methods developed to create and enhance LLMs for official European Union (EU) languages. We provide a comprehensive summary of common monolingual and multilingual datasets used for pretraining large language models.

**Link**: [arxiv](http://arxiv.org/abs/2408.15040v2),  [pdf](http://arxiv.org/pdf/2408.15040v2)

**Tags**: cs.CL 



### Towards Fully Autonomous Research Powered by LLMs: Case Study on   Simulations
**Authors**: Zhihan Liu, Yubo Chai, Jianfeng Li

**Updated**: 2024-08-28T03:48:05Z

**Summary**: The advent of Large Language Models (LLMs) has created new opportunities for the automation of scientific research, spanning both experimental processes and computational simulations. This study explores the feasibility of constructing an autonomous simulation agent (ASA) powered by LLM, through sophisticated API integration, to automate the entire research process, from experimental design, remote upload and simulation execution, data analysis, to report compilation. Using a simulation problem of polymer chain conformations as a case study, we assessed the performance of ASAs powered by different LLMs including GPT-4-Turbo. Our findings revealed that ASA-GPT-4o achieved near-flawless execution on designated research missions, underscoring the potential of LLMs to manage complete scientific investigations autonomously. The outlined automation can be iteratively performed up to twenty cycles without human intervention, illustrating the potential of LLMs for large-scale autonomous research endeavors. Additionally, we discussed the intrinsic traits of ASAs in managing extensive tasks, focusing on self-validation mechanisms and the balance between local attention and global oversight.

**Link**: [arxiv](http://arxiv.org/abs/2408.15512v1),  [pdf](http://arxiv.org/pdf/2408.15512v1)

**Tags**: cs.AI cs.CL physics.chem-ph 



### WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation   Integrating Web Search and Knowledge Graphs
**Authors**: Weijian Xie, Xuefeng Liang, Yuhui Liu, Kaihua Ni, Hong Cheng, Zetian Hu

**Updated**: 2024-08-28T03:47:28Z

**Summary**: Large Language Models (LLMs) have greatly contributed to the development of adaptive intelligent agents and are positioned as an important way to achieve Artificial General Intelligence (AGI). However, LLMs are prone to produce factually incorrect information and often produce "phantom" content that undermines their reliability, which poses a serious challenge for their deployment in real-world scenarios. Enhancing LLMs by combining external databases and information retrieval mechanisms is an effective path. To address the above challenges, we propose a new approach called WeKnow-RAG, which integrates Web search and Knowledge Graphs into a "Retrieval-Augmented Generation (RAG)" system. First, the accuracy and reliability of LLM responses are improved by combining the structured representation of Knowledge Graphs with the flexibility of dense vector retrieval. WeKnow-RAG then utilizes domain-specific knowledge graphs to satisfy a variety of queries and domains, thereby improving performance on factual information and complex reasoning tasks by employing multi-stage web page retrieval techniques using both sparse and dense retrieval methods. Our approach effectively balances the efficiency and accuracy of information retrieval, thus improving the overall retrieval process. Finally, we also integrate a self-assessment mechanism for the LLM to evaluate the trustworthiness of the answers it generates. Our approach proves its outstanding effectiveness in a wide range of offline experiments and online submissions.

**Link**: [arxiv](http://arxiv.org/abs/2408.07611v2),  [pdf](http://arxiv.org/pdf/2408.07611v2)

**Tags**: cs.CL cs.IR 



### IICPilot: An Intelligent Integrated Circuit Backend Design Framework   Using Open EDA
**Authors**: Zesong Jiang, Qing Zhang, Cheng Liu, Long Cheng, Huawei Li, Xiaowei Li

**Updated**: 2024-08-28T03:15:10Z

**Summary**: Open-source EDA tools are rapidly advancing, fostering collaboration, innovation, and knowledge sharing within the EDA community. However, the growing complexity of these tools, characterized by numerous design parameters and heuristics, poses a significant barrier to their widespread adoption. This complexity is particularly pronounced in integrated circuit (IC) backend designs, which place substantial demands on engineers' expertise in EDA tools. To tackle this challenge, we introduce IICPilot, an intelligent IC backend design system based on LLM technology. IICPilot automates various backend design procedures, including script generation, EDA tool invocation, design space exploration of EDA parameters, container-based computing resource allocation, and exception management. By automating these tasks, IICPilot significantly lowers the barrier to entry for open-source EDA tools. Specifically, IICPilot utilizes LangChain's multi-agent framework to efficiently handle distinct design tasks, enabling flexible enhancements independently. Moreover, IICPilot separates the backend design workflow from specific open-source EDA tools through a unified EDA calling interface. This approach allows seamless integration with different open-source EDA tools like OpenROAD and iEDA, streamlining the backend design and optimization across the EDA tools.

**Link**: [arxiv](http://arxiv.org/abs/2407.12576v2),  [pdf](http://arxiv.org/pdf/2407.12576v2)

**Tags**: cs.AR cs.AI 



### MODULI: Unlocking Preference Generalization via Diffusion Models for   Offline Multi-Objective Reinforcement Learning
**Authors**: Yifu Yuan, Zhenrui Zheng, Zibin Dong, Jianye Hao

**Updated**: 2024-08-28T03:10:45Z

**Summary**: Multi-objective Reinforcement Learning (MORL) seeks to develop policies that simultaneously optimize multiple conflicting objectives, but it requires extensive online interactions. Offline MORL provides a promising solution by training on pre-collected datasets to generalize to any preference upon deployment. However, real-world offline datasets are often conservatively and narrowly distributed, failing to comprehensively cover preferences, leading to the emergence of out-of-distribution (OOD) preference areas. Existing offline MORL algorithms exhibit poor generalization to OOD preferences, resulting in policies that do not align with preferences. Leveraging the excellent expressive and generalization capabilities of diffusion models, we propose MODULI (Multi-objective Diffusion Planner with Sliding Guidance), which employs a preference-conditioned diffusion model as a planner to generate trajectories that align with various preferences and derive action for decision-making. To achieve accurate generation, MODULI introduces two return normalization methods under diverse preferences for refining guidance. To further enhance generalization to OOD preferences, MODULI proposes a novel sliding guidance mechanism, which involves training an additional slider adapter to capture the direction of preference changes. Incorporating the slider, it transitions from in-distribution (ID) preferences to generating OOD preferences, patching, and extending the incomplete Pareto front. Extensive experiments on the D4MORL benchmark demonstrate that our algorithm outperforms state-of-the-art Offline MORL baselines, exhibiting excellent generalization to OOD preferences.

**Link**: [arxiv](http://arxiv.org/abs/2408.15501v1),  [pdf](http://arxiv.org/pdf/2408.15501v1)

**Tags**: cs.LG cs.AI 



### Procedural Adherence and Interpretability Through Neuro-Symbolic   Generative Agents
**Authors**: Raven Rothkopf, Hannah Tongxin Zeng, Mark Santolucito

**Updated**: 2024-08-28T02:37:08Z

**Summary**: The surge in popularity of large language models (LLMs) has opened doors for new approaches to the creation of interactive agents. However, managing and interpreting the temporal behavior of such agents over the course of a potentially infinite interaction remain challenging. The stateful, long-term horizon reasoning required for coherent agent behavior does not fit well into the LLM paradigm. We propose a combination of formal logic-based program synthesis and LLM content generation to bring guarantees of procedural adherence and interpretability to generative agent behavior. To illustrate the benefit of procedural adherence and interpretability, we use Temporal Stream Logic (TSL) to generate an automaton that enforces an interpretable, high-level temporal structure on an agent. With the automaton tracking the context of the interaction and making decisions to guide the conversation accordingly, we can drive content generation in a way that allows the LLM to focus on a shorter context window. We evaluated our approach on different tasks involved in creating an interactive agent specialized for generating choose-your-own-adventure games. We found that over all of the tasks, an automaton-enhanced agent with procedural guarantees achieves at least 96% adherence to its temporal constraints, whereas a purely LLM-based agent demonstrates as low as 14.67% adherence.

**Link**: [arxiv](http://arxiv.org/abs/2402.16905v2),  [pdf](http://arxiv.org/pdf/2402.16905v2)

**Tags**: cs.AI cs.LG cs.LO 



### Enhancing and Accelerating Large Language Models via Instruction-Aware   Contextual Compression
**Authors**: Haowen Hou, Fei Ma, Binwen Bai, Xinxin Zhu, Fei Yu

**Updated**: 2024-08-28T02:31:15Z

**Summary**: Large Language Models (LLMs) have garnered widespread attention due to their remarkable performance across various tasks. However, to mitigate the issue of hallucinations, LLMs often incorporate retrieval-augmented pipeline to provide them with rich external knowledge and context. Nevertheless, challenges stem from inaccurate and coarse-grained context retrieved from the retriever. Supplying irrelevant context to the LLMs can result in poorer responses, increased inference latency, and higher costs. This paper introduces a method called Instruction-Aware Contextual Compression, which filters out less informative content, thereby accelerating and enhancing the use of LLMs. The experimental results demonstrate that Instruction-Aware Contextual Compression notably reduces memory consumption and minimizes generation latency while maintaining performance levels comparable to those achieved with the use of the full context. Specifically, we achieved a 50% reduction in context-related costs, resulting in a 5% reduction in inference memory usage and a 2.2-fold increase in inference speed, with only a minor drop of 0.047 in Rouge-1. These findings suggest that our method strikes an effective balance between efficiency and performance.

**Link**: [arxiv](http://arxiv.org/abs/2408.15491v1),  [pdf](http://arxiv.org/pdf/2408.15491v1)

**Tags**: cs.CL 



### Legilimens: Practical and Unified Content Moderation for Large Language   Model Services
**Authors**: Jialin Wu, Jiangyi Deng, Shengyuan Pang, Yanjiao Chen, Jiayang Xu, Xinfeng Li, Wenyuan Xu

**Updated**: 2024-08-28T02:27:07Z

**Summary**: Given the societal impact of unsafe content generated by large language models (LLMs), ensuring that LLM services comply with safety standards is a crucial concern for LLM service providers. Common content moderation methods are limited by an effectiveness-and-efficiency dilemma, where simple models are fragile while sophisticated models consume excessive computational resources. In this paper, we reveal for the first time that effective and efficient content moderation can be achieved by extracting conceptual features from chat-oriented LLMs, despite their initial fine-tuning for conversation rather than content moderation. We propose a practical and unified content moderation framework for LLM services, named Legilimens, which features both effectiveness and efficiency. Our red-team model-based data augmentation enhances the robustness of Legilimens against state-of-the-art jailbreaking. Additionally, we develop a framework to theoretically analyze the cost-effectiveness of Legilimens compared to other methods. We have conducted extensive experiments on five host LLMs, seventeen datasets, and nine jailbreaking methods to verify the effectiveness, efficiency, and robustness of Legilimens against normal and adaptive adversaries. A comparison of Legilimens with both commercial and academic baselines demonstrates the superior performance of Legilimens. Furthermore, we confirm that Legilimens can be applied to few-shot scenarios and extended to multi-label classification tasks.

**Link**: [arxiv](http://arxiv.org/abs/2408.15488v1),  [pdf](http://arxiv.org/pdf/2408.15488v1)

**Tags**: cs.CL 



### Large Language Models Understand Layout
**Authors**: Weiming Li, Manni Duan, Dong An, Yan Shao

**Updated**: 2024-08-28T02:04:24Z

**Summary**: Large language models (LLMs) demonstrate extraordinary abilities in a wide range of natural language processing (NLP) tasks. In this paper, we show that, beyond text understanding capability, LLMs are capable of processing text layouts that are denoted by spatial markers. They are able to answer questions that require explicit spatial perceiving and reasoning, while a drastic performance drop is observed when the spatial markers from the original data are excluded. We perform a series of experiments with the GPT-3.5, Baichuan2, Llama2 and ChatGLM3 models on various types of layout-sensitive datasets for further analysis. The experimental results reveal that the layout understanding ability of LLMs is mainly introduced by the coding data for pretraining, which is further enhanced at the instruction-tuning stage. In addition, layout understanding can be enhanced by integrating low-cost, auto-generated data approached by a novel text game. Finally, we show that layout understanding ability is beneficial for building efficient visual question-answering (VQA) systems.

**Link**: [arxiv](http://arxiv.org/abs/2407.05750v3),  [pdf](http://arxiv.org/pdf/2407.05750v3)

**Tags**: cs.CL 



### Lagrangian approach to origami vertex analysis: Kinematics
**Authors**: Matthew Grasinger, Andrew Gillman, Philip Buskohl

**Updated**: 2024-08-28T00:52:39Z

**Summary**: The use of origami in engineering has significantly expanded in recent years, spanning deployable structures across scales, folding robotics, and mechanical metamaterials. However, finding foldable paths can be a formidable task as the kinematics are determined by a nonlinear system of equations, often with several degrees of freedom. In this work, we leverage a Lagrangian approach to derive reduced-order compatibility conditions for rigid-facet origami vertices with reflection and rotational symmetries. Then, using the reduced-order conditions, we derive exact, multi-degree of freedom solutions for degree 6 and degree 8 vertices with prescribed symmetries. The exact kinematic solutions allow us to efficiently investigate the topology of allowable kinematics, including the consideration of a self-contact constraint, and then visually interpret the role of geometric design parameters on these admissible fold paths by monitoring the change in the kinematic topology. We then introduce a procedure to construct lower symmetry kinematic solutions by breaking symmetry of higher order kinematic solutions in a systematic way that preserves compatibility. The multi-degree of freedom solutions discovered here should assist with building intuition of the kinematic feasibility of higher degree origami vertices and also facilitate the development of new algorithmic procedures for origami-engineering design.

**Link**: [arxiv](http://arxiv.org/abs/2408.15460v1),  [pdf](http://arxiv.org/pdf/2408.15460v1)

**Tags**: cond-mat.soft math-ph math.MP 



### Algorithm-Assisted Decision Making and Racial Disparities in Housing: A   Study of the Allegheny Housing Assessment Tool
**Authors**: Lingwei Cheng, Cameron Drayton, Alexandra Chouldechova, Rhema Vaithianathan

**Updated**: 2024-08-27T23:13:18Z

**Summary**: The demand for housing assistance across the United States far exceeds the supply, leaving housing providers the task of prioritizing clients for receipt of this limited resource. To be eligible for federal funding, local homelessness systems are required to implement assessment tools as part of their prioritization processes. The Vulnerability Index Service Prioritization Decision Assistance Tool (VI-SPDAT) is the most commonly used assessment tool nationwide. Recent studies have criticized the VI-SPDAT as exhibiting racial bias, which may lead to unwarranted racial disparities in housing provision. In response to these criticisms, some jurisdictions have developed alternative tools, such as the Allegheny Housing Assessment (AHA), which uses algorithms to assess clients' risk levels. Drawing on data from its deployment, we conduct descriptive and quantitative analyses to evaluate whether replacing the VI-SPDAT with the AHA affects racial disparities in housing allocation. We find that the VI-SPDAT tended to assign higher risk scores to white clients and lower risk scores to Black clients, and that white clients were served at a higher rates pre-AHA deployment. While post-deployment service decisions became better aligned with the AHA score, and the distribution of AHA scores is similar across racial groups, we do not find evidence of a corresponding decrease in disparities in service rates. We attribute the persistent disparity to the use of Alt-AHA, a survey-based tool that is used in cases of low data quality, as well as group differences in eligibility-related factors, such as chronic homelessness and veteran status. We discuss the implications for housing service systems seeking to reduce racial disparities in their service delivery.

**Link**: [arxiv](http://arxiv.org/abs/2407.21209v2),  [pdf](http://arxiv.org/pdf/2407.21209v2)

**Tags**: cs.HC econ.GN q-fin.EC 



### Efficient LLM Training and Serving with Heterogeneous Context Sharding   among Attention Heads
**Authors**: Xihui Lin, Yunan Zhang, Suyu Ge, Barun Patra, Vishrav Chaudhary, Hao Peng, Xia Song

**Updated**: 2024-08-27T22:06:20Z

**Summary**: Existing LLM training and inference frameworks struggle in boosting efficiency with sparsity while maintaining the integrity of context and model architecture. Inspired by the sharding concept in database and the fact that attention parallelizes over heads on accelerators, we propose Sparsely-Sharded (S2) Attention, an attention algorithm that allocates heterogeneous context partitions for different attention heads to divide and conquer. S2-Attention enforces each attention head to only attend to a partition of contexts following a strided sparsity pattern, while the full context is preserved as the union of all the shards. As attention heads are processed in separate thread blocks, the context reduction for each head can thus produce end-to-end speed-up and memory reduction. At inference, LLMs trained with S2-Attention can then take the KV cache reduction as free meals with guaranteed model quality preserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X wall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction in end-to-end training time and 10X inference latency, (2) on-par model training quality compared to default attention, (3)perfect needle retrieval accuracy over 32K context window. On top of the algorithm, we build DKernel, an LLM training and inference kernel library that allows users to customize sparsity patterns for their own models. We open-sourced DKerneland make it compatible with Megatron, Pytorch, and vLLM.

**Link**: [arxiv](http://arxiv.org/abs/2407.17678v2),  [pdf](http://arxiv.org/pdf/2407.17678v2)

**Tags**: cs.CL 



### Fast and Modular Autonomy Software for Autonomous Racing Vehicles
**Authors**: Andrew Saba, Aderotimi Adetunji, Adam Johnson, Aadi Kothari, Matthew Sivaprakasam, Joshua Spisak, Prem Bharatia, Arjun Chauhan, Brendan Duff Jr., Noah Gasparro, Charles King, Ryan Larkin, Brian Mao, Micah Nye, Anjali Parashar, Joseph Attias, Aurimas Balciunas, Austin Brown, Chris Chang, Ming Gao, Cindy Heredia, Andrew Keats, Jose Lavariega, William Muckelroy III, Andre Slavescu, Nickolas Stathas, Nayana Suvarna, Chuan Tian Zhang, Sebastian Scherer, Deva Ramanan

**Updated**: 2024-08-27T21:57:16Z

**Summary**: Autonomous motorsports aim to replicate the human racecar driver with software and sensors. As in traditional motorsports, Autonomous Racing Vehicles (ARVs) are pushed to their handling limits in multi-agent scenarios at extremely high ($\geq 150mph$) speeds. This Operational Design Domain (ODD) presents unique challenges across the autonomy stack. The Indy Autonomous Challenge (IAC) is an international competition aiming to advance autonomous vehicle development through ARV competitions. While far from challenging what a human racecar driver can do, the IAC is pushing the state of the art by facilitating full-sized ARV competitions. This paper details the MIT-Pitt-RW Team's approach to autonomous racing in the IAC. In this work, we present our modular and fast approach to agent detection, motion planning and controls to create an autonomy stack. We also provide analysis of the performance of the software stack in single and multi-agent scenarios for rapid deployment in a fast-paced competition environment. We also cover what did and did not work when deployed on a physical system the Dallara AV-21 platform and potential improvements to address these shortcomings. Finally, we convey lessons learned and discuss limitations and future directions for improvement.

**Link**: [arxiv](http://arxiv.org/abs/2408.15425v1),  [pdf](http://arxiv.org/pdf/2408.15425v1)

**Tags**: cs.RO cs.AI cs.SE 



### AUTOGENICS: Automated Generation of Context-Aware Inline Comments for   Code Snippets on Programming Q&A Sites Using LLM
**Authors**: Suborno Deb Bappon, Saikat Mondal, Banani Roy

**Updated**: 2024-08-27T21:21:13Z

**Summary**: Inline comments in the source code facilitate easy comprehension, reusability, and enhanced readability. However, code snippets in answers on Q&A sites like Stack Overflow (SO) often lack comments because answerers volunteer their time and often skip comments or explanations due to time constraints. Existing studies show that these online code examples are difficult to read and understand, making it difficult for developers (especially novices) to use them correctly and leading to misuse. Given these challenges, we introduced AUTOGENICS, a tool designed to integrate with SO to generate effective inline comments for code snippets in SO answers exploiting large language models (LLMs). Our contributions are threefold. First, we randomly select 400 answer code snippets from SO and generate inline comments for them using LLMs. We then manually evaluate these comments' effectiveness using four key metrics: accuracy, adequacy, conciseness, and usefulness. Overall, LLMs demonstrate promising effectiveness in generating inline comments for SO answer code snippets. Second, we surveyed 14 active SO users to perceive the effectiveness of these inline comments. The survey results are consistent with our previous manual evaluation. However, according to our evaluation, LLMs-generated comments are less effective for shorter code snippets and sometimes produce noisy comments. Third, to address the gaps, we introduced AUTOGENICS, which extracts additional context from question texts and generates context-aware inline comments. It also optimizes comments by removing noise (e.g., comments in import statements and variable declarations). We evaluate the effectiveness of AUTOGENICS-generated comments using the same four metrics that outperform those of standard LLMs. AUTOGENICS might (a) enhance code comprehension, (b) save time, and improve developers' ability to learn and reuse code more accurately.

**Link**: [arxiv](http://arxiv.org/abs/2408.15411v1),  [pdf](http://arxiv.org/pdf/2408.15411v1)

**Tags**: cs.SE 



### Awes, Laws, and Flaws From Today's LLM Research
**Authors**: Adrian de Wynter

**Updated**: 2024-08-29T17:00:24Z

**Summary**: We perform a critical examination of the scientific methodology behind contemporary large language model (LLM) research. For this we assess over 2,000 research works based on criteria typical of what is considered good research (e.g. presence of statistical tests and reproducibility) and cross-validate it with arguments that are at the centre of controversy (e.g., claims of emergent behaviour, the use of LLMs as evaluators). We find multiple trends, such as declines in claims of emergent behaviour and ethics disclaimers; the rise of LLMs as evaluators in spite of a lack of consensus from the community about their useability; and an increase of claims of LLM reasoning abilities, typically without leveraging human evaluation. This paper underscores the need for more scrutiny and rigour by and from this field to live up to the fundamentals of a responsible scientific method that is ethical, reproducible, systematic, and open to criticism.

**Link**: [arxiv](http://arxiv.org/abs/2408.15409v2),  [pdf](http://arxiv.org/pdf/2408.15409v2)

**Tags**: cs.CL 



### The AI-Native Software Development Lifecycle: A Theoretical and   Practical New Methodology
**Authors**: Cory Hymel

**Updated**: 2024-08-27T19:10:23Z

**Summary**: As AI continues to advance and impact every phase of the software development lifecycle (SDLC), a need for a new way of building software will emerge. By analyzing the factors that influence the current state of the SDLC and how those will change with AI we propose a new model of development. This white paper proposes the emergence of a fully AI-native SDLC, where AI is integrated seamlessly into every phase of development, from planning to deployment. We introduce the V-Bounce model, an adaptation of the traditional V-model that incorporates AI from end to end. The V-Bounce model leverages AI to dramatically reduce time spent in implementation phases, shifting emphasis towards requirements gathering, architecture design, and continuous validation. This model redefines the role of humans from primary implementers to primarily validators and verifiers with AI acting as an implementation engine.

**Link**: [arxiv](http://arxiv.org/abs/2408.03416v3),  [pdf](http://arxiv.org/pdf/2408.03416v3)

**Tags**: cs.SE 



### UNA: Unifying Alignments of RLHF/PPO, DPO and KTO by a Generalized   Implicit Reward Function
**Authors**: Zhichao Wang, Bin Bi, Can Huang, Shiva Kumar Pentyala, Zixu James Zhu, Sitaram Asur, Na Claire Cheng

**Updated**: 2024-08-27T18:04:07Z

**Summary**: An LLM is pretrained on trillions of tokens, but the pretrained LLM may still generate undesired responses. To solve this problem, alignment techniques such as RLHF, DPO and KTO are proposed. However, these alignment techniques have limitations. For example, RLHF requires training the reward model and policy separately, which is complex, time-consuming, memory intensive and unstable during training processes. DPO proposes a mapping between an optimal policy and a reward, greatly simplifying the training process of RLHF. However, it can not take full advantages of a reward model and it is limited to pairwise preference data.   In this paper, we propose \textbf{UN}ified \textbf{A}lignment (UNA) which unifies RLHF/PPO, DPO and KTO. Firstly, we mathematically prove that given the classical RLHF objective, the optimal policy is induced by a generalize implicit reward function. With this novel mapping between a reward model and an optimal policy, UNA can 1. unify RLHF/PPO, DPO and KTO into a supervised learning of minimizing the difference between an implicit reward and an explicit reward; 2. outperform RLHF/PPO while simplify, stabilize, speed up and reduce memory burden of RL fine-tuning process; 3. accommodate different feedback types including pairwise, binary and scalar feedback. Downstream experiments show UNA outperforms DPO, KTO and RLHF.

**Link**: [arxiv](http://arxiv.org/abs/2408.15339v1),  [pdf](http://arxiv.org/pdf/2408.15339v1)

**Tags**: cs.LG cs.CL 



### A Multi-Agent Reinforcement Learning Scheme for SFC Placement in Edge   Computing Networks
**Authors**: Congzhou Li, Zhouxiang Wu, Divya Khanure, Jason P. Jue

**Updated**: 2024-08-27T18:01:22Z

**Summary**: In the 5G era and beyond, it is favorable to deploy latency-sensitive and reliability-aware services on edge computing networks in which the computing and network resources are more limited compared to cloud and core networks but can respond more promptly. These services can be composed as Service Function Chains (SFCs) which consist of a sequence of ordered Virtual Network Functions (VNFs). To achieve efficient edge resources allocation for SFC requests and optimal profit for edge service providers, we formulate the SFC placement problem in an edge environment and propose a multi-agent Reinforcement Learning (RL) scheme to address the problem. The proposed scheme employs a set of RL agents to collaboratively make SFC placement decisions, such as path selection, VNF configuration, and VNF deployment. Simulation results show our model can improve the profit of edge service providers by 12\% compared with a heuristic solution.

**Link**: [arxiv](http://arxiv.org/abs/2408.15337v1),  [pdf](http://arxiv.org/pdf/2408.15337v1)

**Tags**: cs.NI 



### Generative Verifiers: Reward Modeling as Next-Token Prediction
**Authors**: Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, Rishabh Agarwal

**Updated**: 2024-08-27T17:57:45Z

**Summary**: Verifiers or reward models are often used to enhance the reasoning performance of large language models (LLMs). A common approach is the Best-of-N method, where N candidate solutions generated by the LLM are ranked by a verifier, and the best one is selected. While LLM-based verifiers are typically trained as discriminative classifiers to score solutions, they do not utilize the text generation capabilities of pretrained LLMs. To overcome this limitation, we instead propose training verifiers using the ubiquitous next-token prediction objective, jointly on verification and solution generation. Compared to standard verifiers, such generative verifiers (GenRM) can benefit from several advantages of LLMs: they integrate seamlessly with instruction tuning, enable chain-of-thought reasoning, and can utilize additional inference-time compute via majority voting for better verification. We demonstrate that when using Gemma-based verifiers on algorithmic and grade-school math reasoning tasks, GenRM outperforms discriminative verifiers and LLM-as-a-Judge, showing a 16-64% improvement in the percentage of problems solved with Best-of-N. Furthermore, we show that GenRM scales favorably across dataset size, model capacity, and inference-time compute.

**Link**: [arxiv](http://arxiv.org/abs/2408.15240v1),  [pdf](http://arxiv.org/pdf/2408.15240v1)

**Tags**: cs.LG 



### SelectLLM: Can LLMs Select Important Instructions to Annotate?
**Authors**: Ritik Sachin Parkar, Jaehyung Kim, Jong Inn Park, Dongyeop Kang

**Updated**: 2024-08-27T17:57:07Z

**Summary**: Instruction tuning benefits from large and diverse datasets; however, creating such datasets involves a high cost of human labeling. While synthetic datasets generated by large language models (LLMs) have partly solved this issue, they often contain low-quality data. One effective solution is selectively annotating unlabelled instructions, especially given the relative ease of acquiring unlabeled instructions or texts from various sources. However, how to select unlabelled instructions is not well-explored, especially in the context of LLMs. Therefore, we introduce SelectLLM, an alternative framework that leverages the capabilities of LLMs to select unlabeled instructions more effectively. Specifically, SelectLLM consists of two key steps: Coreset-based clustering of unlabelled instructions for enlarging diversity and prompting of LLM to identify the most beneficial instructions within each cluster. We evaluate SelectLLM on AlpacaEval2 and MT-Bench, demonstrating its ability to outperform state-of-the-art methods like Alpagasus. In addition, we compare the performance and compatibility of SelectLLM with various LLMs, such as ChatGPT, LLaMA-3.1-70B, and Gemma-2-27b. SelectLLM's adaptability and robustness are further evidenced by its ability to maintain high performance across both human and synthetic datasets. All code and data are publicly available (https://github.com/minnesotanlp/select-llm).

**Link**: [arxiv](http://arxiv.org/abs/2401.16553v7),  [pdf](http://arxiv.org/pdf/2401.16553v7)

**Tags**: cs.CL cs.AI 



### The Mamba in the Llama: Distilling and Accelerating Hybrid Models
**Authors**: Junxiong Wang, Daniele Paliotta, Avner May, Alexander M. Rush, Tri Dao

**Updated**: 2024-08-27T17:56:11Z

**Summary**: Linear RNN architectures, like Mamba, can be competitive with Transformer models in language modeling while having advantageous deployment characteristics. Given the focus on training large-scale Transformer models, we consider the challenge of converting these pretrained models for deployment. We demonstrate that it is feasible to distill large Transformers into linear RNNs by reusing the linear projection weights from attention layers with academic GPU resources. The resulting hybrid model, which incorporates a quarter of the attention layers, achieves performance comparable to the original Transformer in chat benchmarks and outperforms open-source hybrid Mamba models trained from scratch with trillions of tokens in both chat benchmarks and general benchmarks. Moreover, we introduce a hardware-aware speculative decoding algorithm that accelerates the inference speed of Mamba and hybrid models. Overall we show how, with limited computation resources, we can remove many of the original attention layers and generate from the resulting model more efficiently. Our top-performing model, distilled from Llama3-8B-Instruct, achieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and 7.35 on MT-Bench, surpassing the best instruction-tuned linear RNN model.

**Link**: [arxiv](http://arxiv.org/abs/2408.15237v1),  [pdf](http://arxiv.org/pdf/2408.15237v1)

**Tags**: cs.LG cs.AI 



### DCT-CryptoNets: Scaling Private Inference in the Frequency Domain
**Authors**: Arjun Roy, Kaushik Roy

**Updated**: 2024-08-27T17:48:29Z

**Summary**: The convergence of fully homomorphic encryption (FHE) and machine learning offers unprecedented opportunities for private inference of sensitive data. FHE enables computation directly on encrypted data, safeguarding the entire machine learning pipeline, including data and model confidentiality. However, existing FHE-based implementations for deep neural networks face significant challenges in computational cost, latency, and scalability, limiting their practical deployment. This paper introduces DCT-CryptoNets, a novel approach that leverages frequency-domain learning to tackle these issues. Our method operates directly in the frequency domain, utilizing the discrete cosine transform (DCT) commonly employed in JPEG compression. This approach is inherently compatible with remote computing services, where images are usually transmitted and stored in compressed formats. DCT-CryptoNets reduces the computational burden of homomorphic operations by focusing on perceptually relevant low-frequency components. This is demonstrated by substantial latency reduction of up to 5.3$\times$ compared to prior work on image classification tasks, including a novel demonstration of ImageNet inference within 2.5 hours, down from 12.5 hours compared to prior work on equivalent compute resources. Moreover, DCT-CryptoNets improves the reliability of encrypted accuracy by reducing variability (e.g., from $\pm$2.5\% to $\pm$1.0\% on ImageNet). This study demonstrates a promising avenue for achieving efficient and practical privacy-preserving deep learning on high resolution images seen in real-world applications.

**Link**: [arxiv](http://arxiv.org/abs/2408.15231v1),  [pdf](http://arxiv.org/pdf/2408.15231v1)

**Tags**: cs.CR cs.CV cs.LG 



### C3DM: Constrained-Context Conditional Diffusion Models for Imitation   Learning
**Authors**: Vaibhav Saxena, Yotto Koga, Danfei Xu

**Updated**: 2024-08-27T17:45:27Z

**Summary**: Behavior Cloning (BC) methods are effective at learning complex manipulation tasks. However, they are prone to spurious correlation - expressive models may focus on distractors that are irrelevant to action prediction - and are thus fragile in real-world deployment. Prior methods have addressed this challenge by exploring different model architectures and action representations. However, none were able to balance between sample efficiency and robustness against distractors for solving manipulation tasks with a complex action space. We present \textbf{C}onstrained-\textbf{C}ontext \textbf{C}onditional \textbf{D}iffusion \textbf{M}odel (C3DM), a diffusion model policy for solving 6-DoF robotic manipulation tasks with robustness to distractions that can learn deployable robot policies from as little as five demonstrations. A key component of C3DM is a fixation step that helps the action denoiser to focus on task-relevant regions around a predicted fixation point while ignoring distractors in the context. We empirically show that C3DM is robust to out-of-distribution distractors, and consistently achieves high success rates on a wide array of tasks, ranging from table-top manipulation to industrial kitting that require varying levels of precision and robustness to distractors.

**Link**: [arxiv](http://arxiv.org/abs/2311.01419v2),  [pdf](http://arxiv.org/pdf/2311.01419v2)

**Tags**: cs.RO 



### LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet
**Authors**: Nathaniel Li, Ziwen Han, Ian Steneker, Willow Primack, Riley Goodside, Hugh Zhang, Zifan Wang, Cristina Menghini, Summer Yue

**Updated**: 2024-08-27T17:33:30Z

**Summary**: Recent large language model (LLM) defenses have greatly improved models' ability to refuse harmful queries, even when adversarially attacked. However, LLM defenses are primarily evaluated against automated adversarial attacks in a single turn of conversation, an insufficient threat model for real-world malicious use. We demonstrate that multi-turn human jailbreaks uncover significant vulnerabilities, exceeding 70% attack success rate (ASR) on HarmBench against defenses that report single-digit ASRs with automated single-turn attacks. Human jailbreaks also reveal vulnerabilities in machine unlearning defenses, successfully recovering dual-use biosecurity knowledge from unlearned models. We compile these results into Multi-Turn Human Jailbreaks (MHJ), a dataset of 2,912 prompts across 537 multi-turn jailbreaks. We publicly release MHJ alongside a compendium of jailbreak tactics developed across dozens of commercial red teaming engagements, supporting research towards stronger LLM defenses.

**Link**: [arxiv](http://arxiv.org/abs/2408.15221v1),  [pdf](http://arxiv.org/pdf/2408.15221v1)

**Tags**: cs.LG cs.CL cs.CR cs.CY 



### FRAMER/Miu: Tagged Pointer-based Capability and Fundamental Cost of   Memory Safety & Coherence (Position Paper)
**Authors**: Myoung Jin Nam

**Updated**: 2024-08-27T17:31:26Z

**Summary**: Ensuring system correctness, such as memory safety, can eliminate security vulnerabilities that attackers could exploit in the first place. However, high and unpredictable performance degradation remains a primary challenge.   Recognizing that it is extremely difficult to achieve complete system correctness for production deployment, researchers make trade-offs between performance, detection coverage, interoperability, precision, and detection timing.   This research strikes a balance between comprehensive system protection and the costs required to obtain it, identifies the desirable roles of software and hardware, and presents a tagged pointer-based capability system as a stand-alone software solution and a prototype for future hardware design. This paper presents follow-up plans for the FRAMER/Miu generic framework to achieve these goals.

**Link**: [arxiv](http://arxiv.org/abs/2408.15219v1),  [pdf](http://arxiv.org/pdf/2408.15219v1)

**Tags**: cs.CR 



### Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in   Language Models
**Authors**: Wenxuan Zhang, Philip H. S. Torr, Mohamed Elhoseiny, Adel Bibi

**Updated**: 2024-08-27T17:31:21Z

**Summary**: Fine-tuning large language models (LLMs) on human preferences, typically through reinforcement learning from human feedback (RLHF), has proven successful in enhancing their capabilities. However, ensuring the safety of LLMs during the fine-tuning remains a critical concern, and mitigating the potential conflicts in safety and helpfulness is costly in RLHF. To address this issue, we propose a supervised learning framework called Bi-Factorial Preference Optimization (BFPO), which re-parameterizes a joint RLHF objective of both safety and helpfulness into a single supervised learning objective. In the supervised optimization, a labeling function is used to capture global preferences ranking to balance both safety and helpfulness. To evaluate BFPO, we develop a benchmark including comprehensive discriminative and generative tasks for helpfulness and harmlessness. The results indicate that our method significantly outperforms existing approaches in both safety and helpfulness. Moreover, BFPO eliminates the need for human prompting and annotation in LLM fine-tuning while achieving the same level of safety as methods that heavily rely on human labor, with less than 10% of the computational resources. The training recipes and models will be released.

**Link**: [arxiv](http://arxiv.org/abs/2408.15313v1),  [pdf](http://arxiv.org/pdf/2408.15313v1)

**Tags**: cs.AI cs.CL cs.LG 



### Investigating Coverage Criteria in Large Language Models: An In-Depth   Study Through Jailbreak Attacks
**Authors**: Shide Zhou, Tianlin Li, Kailong Wang, Yihao Huang, Ling Shi, Yang Liu, Haoyu Wang

**Updated**: 2024-08-27T17:14:21Z

**Summary**: The swift advancement of large language models (LLMs) has profoundly shaped the landscape of artificial intelligence; however, their deployment in sensitive domains raises grave concerns, particularly due to their susceptibility to malicious exploitation. This situation underscores the insufficiencies in pre-deployment testing, highlighting the urgent need for more rigorous and comprehensive evaluation methods. This study presents a comprehensive empirical analysis assessing the efficacy of conventional coverage criteria in identifying these vulnerabilities, with a particular emphasis on the pressing issue of jailbreak attacks. Our investigation begins with a clustering analysis of the hidden states in LLMs, demonstrating that intrinsic characteristics of these states can distinctly differentiate between various types of queries. Subsequently, we assess the performance of these criteria across three critical dimensions: criterion level, layer level, and token level. Our findings uncover significant disparities in neuron activation patterns between the processing of normal and jailbreak queries, thereby corroborating the clustering results. Leveraging these findings, we propose an innovative approach for the real-time detection of jailbreak attacks by utilizing neural activation features. Our classifier demonstrates remarkable accuracy, averaging 96.33% in identifying jailbreak queries, including those that could lead to adversarial attacks. The importance of our research lies in its comprehensive approach to addressing the intricate challenges of LLM security. By enabling instantaneous detection from the model's first token output, our method holds promise for future systems integrating LLMs, offering robust real-time detection capabilities. This study advances our understanding of LLM security testing, and lays a critical foundation for the development of more resilient AI systems.

**Link**: [arxiv](http://arxiv.org/abs/2408.15207v1),  [pdf](http://arxiv.org/pdf/2408.15207v1)

**Tags**: cs.SE 



### Can Unconfident LLM Annotations Be Used for Confident Conclusions?
**Authors**: Kristina Gligoriƒá, Tijana Zrnic, Cinoo Lee, Emmanuel J. Cand√®s, Dan Jurafsky

**Updated**: 2024-08-27T17:03:18Z

**Summary**: Large language models (LLMs) have shown high agreement with human raters across a variety of tasks, demonstrating potential to ease the challenges of human data collection. In computational social science (CSS), researchers are increasingly leveraging LLM annotations to complement slow and expensive human annotations. Still, guidelines for collecting and using LLM annotations, without compromising the validity of downstream conclusions, remain limited. We introduce Confidence-Driven Inference: a method that combines LLM annotations and LLM confidence indicators to strategically select which human annotations should be collected, with the goal of producing accurate statistical estimates and provably valid confidence intervals while reducing the number of human annotations needed. Our approach comes with safeguards against LLM annotations of poor quality, guaranteeing that the conclusions will be both valid and no less accurate than if we only relied on human annotations. We demonstrate the effectiveness of Confidence-Driven Inference over baselines in statistical estimation tasks across three CSS settings--text politeness, stance, and bias--reducing the needed number of human annotations by over 25% in each. Although we use CSS settings for demonstration, Confidence-Driven Inference can be used to estimate most standard quantities across a broad range of NLP problems.

**Link**: [arxiv](http://arxiv.org/abs/2408.15204v1),  [pdf](http://arxiv.org/pdf/2408.15204v1)

**Tags**: cs.CL cs.AI cs.HC 



### Quantum teleportation coexisting with classical communications in   optical fiber
**Authors**: Jordan M. Thomas, Fei I. Yeh, Jim Hao Chen, Joe J. Mambretti, Scott J. Kohlert, Gregory S. Kanter, Prem Kumar

**Updated**: 2024-08-27T16:38:50Z

**Summary**: The ability for quantum and conventional networks to operate in the same optical fibers would aid the deployment of quantum network technology on a large scale. Quantum teleportation is a fundamental operation in quantum networking, but has yet to be demonstrated in fibers populated with high-power conventional optical signals. Here we report to the best of our knowledge the first demonstration of quantum teleportation over fibers carrying conventional telecommunications traffic. Quantum state transfer is achieved over a 30.2-km fiber carrying 400-Gbps C-band classical traffic with a Bell state measurement performed at the fiber midpoint. To protect quantum fidelity from spontaneous Raman scattering noise, we use optimal O-band quantum channels, narrow spectro-temporal filtering, and multi-photon coincidence detection. Fidelity is shown to be well maintained with an elevated C-band classical power of 18.7 dBm, which could support multiple classical channels totaling many terabits/s aggregate data rates. These results show the feasibility of advanced quantum and classical network applications operating within a unified fiber infrastructure.

**Link**: [arxiv](http://arxiv.org/abs/2404.10738v3),  [pdf](http://arxiv.org/pdf/2404.10738v3)

**Tags**: quant-ph 



### X-Reflect: Cross-Reflection Prompting for Multimodal Recommendation
**Authors**: Hanjia Lyu, Ryan Rossi, Xiang Chen, Md Mehrab Tanjim, Stefano Petrangeli, Somdeb Sarkhel, Jiebo Luo

**Updated**: 2024-08-27T16:10:21Z

**Summary**: Large Language Models (LLMs) and Large Multimodal Models (LMMs) have been shown to enhance the effectiveness of enriching item descriptions, thereby improving the accuracy of recommendation systems. However, most existing approaches either rely on text-only prompting or employ basic multimodal strategies that do not fully exploit the complementary information available from both textual and visual modalities. This paper introduces a novel framework, Cross-Reflection Prompting, termed X-Reflect, designed to address these limitations by prompting LMMs to explicitly identify and reconcile supportive and conflicting information between text and images. By capturing nuanced insights from both modalities, this approach generates more comprehensive and contextually richer item representations. Extensive experiments conducted on two widely used benchmarks demonstrate that our method outperforms existing prompting baselines in downstream recommendation accuracy. Additionally, we evaluate the generalizability of our framework across different LMM backbones and the robustness of the prompting strategies, offering insights for optimization. This work underscores the importance of integrating multimodal information and presents a novel solution for improving item understanding in multimodal recommendation systems.

**Link**: [arxiv](http://arxiv.org/abs/2408.15172v1),  [pdf](http://arxiv.org/pdf/2408.15172v1)

**Tags**: cs.IR cs.CL cs.CV 



### Measuring text summarization factuality using atomic facts entailment   metrics in the context of retrieval augmented generation
**Authors**: N. E. Kriman

**Updated**: 2024-08-27T16:09:56Z

**Summary**: The use of large language models (LLMs) has significantly increased since the introduction of ChatGPT in 2022, demonstrating their value across various applications. However, a major challenge for enterprise and commercial adoption of LLMs is their tendency to generate inaccurate information, a phenomenon known as "hallucination." This project proposes a method for estimating the factuality of a summary generated by LLMs when compared to a source text. Our approach utilizes Naive Bayes classification to assess the accuracy of the content produced.

**Link**: [arxiv](http://arxiv.org/abs/2408.15171v1),  [pdf](http://arxiv.org/pdf/2408.15171v1)

**Tags**: cs.CL 68T50 I.2.7 



### Zero-Shot Character Identification and Speaker Prediction in Comics via   Iterative Multimodal Fusion
**Authors**: Yingxuan Li, Ryota Hinami, Kiyoharu Aizawa, Yusuke Matsui

**Updated**: 2024-08-27T15:56:33Z

**Summary**: Recognizing characters and predicting speakers of dialogue are critical for comic processing tasks, such as voice generation or translation. However, because characters vary by comic title, supervised learning approaches like training character classifiers which require specific annotations for each comic title are infeasible. This motivates us to propose a novel zero-shot approach, allowing machines to identify characters and predict speaker names based solely on unannotated comic images. In spite of their importance in real-world applications, these task have largely remained unexplored due to challenges in story comprehension and multimodal integration. Recent large language models (LLMs) have shown great capability for text understanding and reasoning, while their application to multimodal content analysis is still an open problem. To address this problem, we propose an iterative multimodal framework, the first to employ multimodal information for both character identification and speaker prediction tasks. Our experiments demonstrate the effectiveness of the proposed framework, establishing a robust baseline for these tasks. Furthermore, since our method requires no training data or annotations, it can be used as-is on any comic series.

**Link**: [arxiv](http://arxiv.org/abs/2404.13993v3),  [pdf](http://arxiv.org/pdf/2404.13993v3)

**Tags**: cs.MM cs.CV 



### Blackbox optimization for origami-inspired bistable structures
**Authors**: Luca Boisneault, Charles Audet, David Melancon

**Updated**: 2024-08-27T15:40:11Z

**Summary**: Bistable mechanical systems exhibit two stable configurations where the elastic energy is locally minimized. To realize such systems, origami techniques have been proposed as a versatile platform to design deployable structures with both compact and functional stable states. Conceptually, a bistable origami motif is composed of two-dimensional surfaces connected by one-dimensional fold lines. This leads to stable configurations exhibiting zero-energy local minima. Physically, origami-inspired structures are three-dimensional, comprising facets and hinges fabricated in a distinct stable state where residual stresses are minimized. This leads to the dominance of one stable state over the other. To improve mechanical performance, one can solve the constrained optimization problem of maximizing the bistability of origami structures, defined as the amount of elastic energy required to switch between stable states, while ensuring materials used for the facets and hinges remain within their elastic regime. In this study, the Mesh Adaptive Direct Search (MADS) algorithm, a blackbox optimization technique, is used to solve the constrained optimization problem. The bistable waterbomb-base origami motif is selected as a case-study to present the methodology. The elastic energy of this origami pattern under deployment is calculated via Finite Element simulations which serve as the blackbox in the MADS optimization loop. To validate the results, optimized waterbomb-base geometries are built via Fused Filament Fabrication and their response under loading is characterized experimentally on a Uniaxial Test Machine. Ultimately, our method offers a general framework for optimizing bistability in mechanical systems, presenting opportunities for advancement across various engineering applications.

**Link**: [arxiv](http://arxiv.org/abs/2408.15147v1),  [pdf](http://arxiv.org/pdf/2408.15147v1)

**Tags**: math.OC 



### Development of a Large Language Model-based Multi-Agent Clinical   Decision Support System for Korean Triage and Acuity Scale (KTAS)-Based   Triage and Treatment Planning in Emergency Departments
**Authors**: Seungjun Han, Wongyung Choi

**Updated**: 2024-08-27T15:16:06Z

**Summary**: Emergency department (ED) overcrowding and the complexity of rapid decision-making in critical care settings pose significant challenges to healthcare systems worldwide. While clinical decision support systems (CDSS) have shown promise, the integration of large language models (LLMs) offers new possibilities for enhancing triage accuracy and clinical decision-making. This study presents an LLM-driven CDSS designed to assist ED physicians and nurses in patient triage, treatment planning, and overall emergency care management.   We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM, orchestrated by CrewAI and Langchain. The system comprises four AI agents emulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED Coordinator. It incorporates the Korean Triage and Acuity Scale (KTAS) for triage assessment and integrates with the RxNorm API for medication management.   The model was evaluated using the Asclepius dataset, with performance assessed by a clinical emergency medicine specialist. The CDSS demonstrated high accuracy in triage decision-making compared to the baseline of a single-agent system. Furthermore, the system exhibited strong performance in critical areas, including primary diagnosis, critical findings identification, disposition decision-making, treatment planning, and resource allocation.   Our multi-agent CDSS demonstrates significant potential for supporting comprehensive emergency care management. By leveraging state-of-the-art AI technologies, this system offers a scalable and adaptable tool that could enhance emergency medical care delivery, potentially alleviating ED overcrowding and improving patient outcomes. This work contributes to the growing field of AI applications in emergency medicine and offers a promising direction for future research and clinical implementation.

**Link**: [arxiv](http://arxiv.org/abs/2408.07531v2),  [pdf](http://arxiv.org/pdf/2408.07531v2)

**Tags**: cs.AI cs.CL cs.LG 



### Using LLMs for Explaining Sets of Counterfactual Examples to Final Users
**Authors**: Arturo Fredes, Jordi Vitria

**Updated**: 2024-08-27T15:13:06Z

**Summary**: Causality is vital for understanding true cause-and-effect relationships between variables within predictive models, rather than relying on mere correlations, making it highly relevant in the field of Explainable AI. In an automated decision-making scenario, causal inference methods can analyze the underlying data-generation process, enabling explanations of a model's decision by manipulating features and creating counterfactual examples. These counterfactuals explore hypothetical scenarios where a minimal number of factors are altered, providing end-users with valuable information on how to change their situation. However, interpreting a set of multiple counterfactuals can be challenging for end-users who are not used to analyzing raw data records. In our work, we propose a novel multi-step pipeline that uses counterfactuals to generate natural language explanations of actions that will lead to a change in outcome in classifiers of tabular data using LLMs. This pipeline is designed to guide the LLM through smaller tasks that mimic human reasoning when explaining a decision based on counterfactual cases. We conducted various experiments using a public dataset and proposed a method of closed-loop evaluation to assess the coherence of the final explanation with the counterfactuals, as well as the quality of the content. Results are promising, although further experiments with other datasets and human evaluations should be carried out.

**Link**: [arxiv](http://arxiv.org/abs/2408.15133v1),  [pdf](http://arxiv.org/pdf/2408.15133v1)

**Tags**: cs.LG 



### Resource Placement for Rate and Fidelity Maximization in Quantum   Networks
**Authors**: Shahrooz Pouryousef, Hassan Shapourian, Alireza Shabani, Ramana Kompella, Don Towsley

**Updated**: 2024-08-27T15:09:40Z

**Summary**: Existing classical optical network infrastructure cannot be immediately used for quantum network applications due to photon loss. The first step towards enabling quantum networks is the integration of quantum repeaters into optical networks. However, the expenses and intrinsic noise inherent in quantum hardware underscore the need for an efficient deployment strategy that optimizes the allocation of quantum repeaters and memories. In this paper, we present a comprehensive framework for network planning, aiming to efficiently distributing quantum repeaters across existing infrastructure, with the objective of maximizing quantum network utility within an entanglement distribution network. We apply our framework to several cases including a preliminary illustration of a dumbbell network topology and real-world cases of the SURFnet and ESnet. We explore the effect of quantum memory multiplexing within quantum repeaters, as well as the influence of memory coherence time on quantum network utility. We further examine the effects of different fairness assumptions on network planning, uncovering their impacts on real-time network performance.

**Link**: [arxiv](http://arxiv.org/abs/2308.16264v3),  [pdf](http://arxiv.org/pdf/2308.16264v3)

**Tags**: quant-ph cs.NI 



### Time Series Analysis for Education: Methods, Applications, and Future   Directions
**Authors**: Shengzhong Mao, Chaoli Zhang, Yichi Song, Jindong Wang, Xiao-Jun Zeng, Zenglin Xu, Qingsong Wen

**Updated**: 2024-08-27T15:06:17Z

**Summary**: Recent advancements in the collection and analysis of sequential educational data have brought time series analysis to a pivotal position in educational research, highlighting its essential role in facilitating data-driven decision-making. However, there is a lack of comprehensive summaries that consolidate these advancements. To the best of our knowledge, this paper is the first to provide a comprehensive review of time series analysis techniques specifically within the educational context. We begin by exploring the landscape of educational data analytics, categorizing various data sources and types relevant to education. We then review four prominent time series methods-forecasting, classification, clustering, and anomaly detection-illustrating their specific application points in educational settings. Subsequently, we present a range of educational scenarios and applications, focusing on how these methods are employed to address diverse educational tasks, which highlights the practical integration of multiple time series methods to solve complex educational problems. Finally, we conclude with a discussion on future directions, including personalized learning analytics, multimodal data fusion, and the role of large language models (LLMs) in educational time series. The contributions of this paper include a detailed taxonomy of educational data, a synthesis of time series techniques with specific educational applications, and a forward-looking perspective on emerging trends and future research opportunities in educational analysis. The related papers and resources are available and regularly updated at the project page.

**Link**: [arxiv](http://arxiv.org/abs/2408.13960v2),  [pdf](http://arxiv.org/pdf/2408.13960v2)

**Tags**: cs.LG cs.AI cs.CY 



### The Uniqueness of LLaMA3-70B with Per-Channel Quantization: An Empirical   Study
**Authors**: Minghai Qin

**Updated**: 2024-08-27T15:03:01Z

**Summary**: We have observed a distinctive quantization-related behavior in the LLaMA3/3.1-70B models that is absent in both the LLaMA2-70B and LLaMA3/3.1-8B/405B models. Quantization is a crucial technique for deploying large language models (LLMs) efficiently. Among various bit widths and representations for weights and activations, the 8-bit integer weight and 8-bit integer activation (W8A8) configuration is particularly popular due to its widespread hardware support. However, the impact of W8A8 post-training quantization on model accuracy remains contentious. While several studies have suggested calibrating either weights or activations to mitigate accuracy degradation, a comprehensive solution has yet to be identified. In this paper, we empirically investigate multiple LLMs featured on an open LLM leaderboard, discovering that the LLaMA3-70B model series have a unique accuracy degradation behavior with W8A8 per-channel post-training quantization. In contrast, other model series such as LLaMA2, LLaMA3-8B, Qwen, Mixtral, Mistral, Phi-3, and Falcon demonstrate robust performance with W8A8, sometimes surpassing their FP16 counterparts. Contrary to previous assertions attributing degradation to the large dynamic range of activations, our findings indicate that the weight distribution of the LLaMA3-70B is the primary factor behind the vulnerability. By meticulously analyzing the distinct characteristics of weight distributions across Transformer blocks, we propose a mixed strategy with less than 3% of the layers enabling finer W8A8 quantization granularity, while the remaining 97% of layers retain the per-channel configuration. As a result, the average accuracy of LLaMA3-70B-W8A8 is increased from 45.5% to 73.4% (just 0.7% shy of LLaMA3-70B-FP16) across eight reasoning tasks. Notably, our method requires neither calibration nor fine-tuning.

**Link**: [arxiv](http://arxiv.org/abs/2408.15301v1),  [pdf](http://arxiv.org/pdf/2408.15301v1)

**Tags**: cs.LG cs.AI 



