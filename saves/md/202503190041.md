# Arxiv Results
## Keyword: kv cache 
 ### Knowledge-Aware Iterative Retrieval for Multi-Agent Systems
**Authors**: Seyoung Song

**Updated**: 2025-03-17T15:27:02Z

**Summary**: We introduce a novel large language model (LLM)-driven agent framework, which iteratively refines queries and filters contextual evidence by leveraging dynamically evolving knowledge. A defining feature of the system is its decoupling of external sources from an internal knowledge cache that is progressively updated to guide both query generation and evidence selection. This design mitigates bias-reinforcement loops and enables dynamic, trackable search exploration paths, thereby optimizing the trade-off between exploring diverse information and maintaining accuracy through autonomous agent decision-making. Our approach is evaluated on a broad range of open-domain question answering benchmarks, including multi-step tasks that mirror real-world scenarios where integrating information from multiple sources is critical, especially given the vulnerabilities of LLMs that lack explicit reasoning or planning capabilities. The results show that the proposed system not only outperforms single-step baselines regardless of task difficulty but also, compared to conventional iterative retrieval methods, demonstrates pronounced advantages in complex tasks through precise evidence-based reasoning and enhanced efficiency. The proposed system supports both competitive and collaborative sharing of updated context, enabling multi-agent extension. The benefits of multi-agent configurations become especially prominent as task difficulty increases. The number of convergence steps scales with task difficulty, suggesting cost-effective scalability.

**Link**: [arxiv](http://arxiv.org/abs/2503.13275v1),  [pdf](http://arxiv.org/pdf/2503.13275v1)

**Tags**: cs.AI cs.IR I.2.0; I.2.7; I.2.11; H.3.3 



### HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads
**Authors**: Pranav Suryadevara

**Updated**: 2025-03-17T11:10:49Z

**Summary**: The growth of machine learning (ML) workloads has underscored the importance of efficient memory hierarchies to address bandwidth, latency, and scalability challenges. HERMES focuses on optimizing memory subsystems for RISC-V architectures to meet the computational needs of ML models such as CNNs, RNNs, and Transformers. This project explores state-of-the-art techniques such as advanced prefetching, tensor-aware caching, and hybrid memory models. The cornerstone of HERMES is the integration of shared L3 caches with fine-grained coherence protocols and specialized pathways to deep learning accelerators like Gemmini. Simulation tools like Gem5 and DRAMSim2 are used to evaluate baseline performance and scalability under representative ML workloads. The findings of this study highlight the design choices and anticipated challenges, paving the way for low-latency scalable memory operations for ML applications.

**Link**: [arxiv](http://arxiv.org/abs/2503.13064v1),  [pdf](http://arxiv.org/pdf/2503.13064v1)

**Tags**: cs.AR cs.PF B.3.2; C.1.3; C.3 



### Tuning the CMS Coffea-casa facility for 200 Gbps Challenge
**Authors**: Sam Albin, Garhan Attebury, Kenneth Bloom, Brian Paul Bockelman, Benjamin Tovar Lopez, Carl Lundstedt, Oksana Shadura, John Thiltges, Derek Weitzel, Andrew Wightman

**Updated**: 2025-03-17T09:46:35Z

**Summary**: As a part of the IRIS-HEP "Analysis Grand Challenge" activities, the Coffea-casa AF team executed a "200 Gbps Challenge". One of the goals of this challenge was to provide a setup for execution of a test notebook-style analysis on the facility that could process a 200 TB CMS NanoAOD dataset in 20 minutes.   We describe the solutions we deployed at the facility to execute the challenge tasks. The facility was configured to provide 2000+ cores for quick turn-around, low-latency analysis. To reach the highest event processing rates we tested different scaling backends, both scaling over HTCondor and Kubernetes resources and using Dask and Taskvine schedulers. This configuration also allowed us to compare two different services for managing Dask clusters, Dask labextention, and Dask Gateway server, under extreme conditions.   A robust set of XCache servers with a redirector were deployed in Kubernetes to cache the dataset to minimize wide-area network traffic. The XCache servers were backed with solid-state NVME drives deployed within the Kubernetes cluster nodes. All data access was authenticated using scitokens and was transparent to the user. To ensure we could track and measure data throughput precisely, we used our existing Prometheus monitoring stack to monitor the XCache pod throughput on the Kubernetes network layer. Using the rate query across all of the 8 XCache pods we were able to view a stacked cumulative graph of the total throughput for each XCache. This monitoring setup allowed us to ensure uniform data rates across all nodes while verifying we had reached the 200 Gbps benchmark.

**Link**: [arxiv](http://arxiv.org/abs/2503.12991v1),  [pdf](http://arxiv.org/pdf/2503.12991v1)

**Tags**: hep-ex 



### ROMA: a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM
**Authors**: Wenqiang Wang, Yijia Zhang, Zikai Zhang, Guanting Huo, Hao Liang, Shijie Cao, Ningyi Xu

**Updated**: 2025-03-17T09:44:17Z

**Summary**: As large language models (LLMs) demonstrate powerful capabilities, deploying them on edge devices has become increasingly crucial, offering advantages in privacy and real-time interaction. QLoRA has emerged as the standard approach for on-device LLMs, leveraging quantized models to reduce memory and computational costs while utilizing LoRA for task-specific adaptability. In this work, we propose ROMA, a QLoRA accelerator with a hybrid storage architecture that uses ROM for quantized base models and SRAM for LoRA weights and KV cache. Our insight is that the quantized base model is stable and converged, making it well-suited for ROM storage. Meanwhile, LoRA modules offer the flexibility to adapt to new data without requiring updates to the base model. To further reduce the area cost of ROM, we introduce a novel B-ROM design and integrate it with the compute unit to form a fused cell for efficient use of chip resources. ROMA can effectively store both a 4-bit 3B and a 2-bit 8B LLaMA model entirely on-chip, achieving a notable generation speed exceeding 20,000 tokens/s without requiring external memory.

**Link**: [arxiv](http://arxiv.org/abs/2503.12988v1),  [pdf](http://arxiv.org/pdf/2503.12988v1)

**Tags**: cs.AR cs.AI 



### WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images
**Authors**: Yansong Guo, Jie Hu, Yansong Qu, Liujuan Cao

**Updated**: 2025-03-17T03:30:29Z

**Summary**: Recent advances in interactive 3D segmentation from 2D images have demonstrated impressive performance. However, current models typically require extensive scene-specific training to accurately reconstruct and segment objects, which limits their applicability in real-time scenarios. In this paper, we introduce WildSeg3D, an efficient approach that enables the segmentation of arbitrary 3D objects across diverse environments using a feed-forward mechanism. A key challenge of this feed-forward approach lies in the accumulation of 3D alignment errors across multiple 2D views, which can lead to inaccurate 3D segmentation results. To address this issue, we propose Dynamic Global Aligning (DGA), a technique that improves the accuracy of global multi-view alignment by focusing on difficult-to-match 3D points across images, using a dynamic adjustment function. Additionally, for real-time interactive segmentation, we introduce Multi-view Group Mapping (MGM), a method that utilizes an object mask cache to integrate multi-view segmentations and respond rapidly to user prompts. WildSeg3D demonstrates robust generalization across arbitrary scenes, thereby eliminating the need for scene-specific training. Specifically, WildSeg3D not only attains the accuracy of state-of-the-art (SOTA) methods but also achieves a $40\times$ speedup compared to existing SOTA models. Our code will be publicly available.

**Link**: [arxiv](http://arxiv.org/abs/2503.08407v2),  [pdf](http://arxiv.org/pdf/2503.08407v2)

**Tags**: cs.CV 



### ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video   Understanding
**Authors**: Xiao Wang, Qingyi Si, Jianlong Wu, Shiyu Zhu, Li Cao, Liqiang Nie

**Updated**: 2025-03-16T16:25:31Z

**Summary**: Video Large Language Models (VideoLLMs) have made significant strides in video understanding but struggle with long videos due to the limitations of their backbone LLMs. Existing solutions rely on length extrapolation, which is memory-constrained, or visual token compression, which primarily leverages low-level temporal redundancy while overlooking the more effective high-level knowledge redundancy. To address this, we propose $\textbf{ReTaKe}$, a training-free method with two novel modules DPSelect and PivotKV, to jointly reduce both temporal visual redundancy and knowledge redundancy for video compression. To align with the way of human temporal perception, DPSelect identifies keyframes based on inter-frame distance peaks. To leverage LLMs' learned prior knowledge, PivotKV marks the keyframes as pivots and compress non-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe enables VideoLLMs to process 8 times longer frames (up to 2048), outperforming similar-sized models by 3-5% and even rivaling much larger ones on VideoMME, MLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression operations with prefilling, ReTaKe introduces only ~10% prefilling latency overhead while reducing decoding latency by ~20%. Our code is available at https://github.com/SCZwangxiao/video-ReTaKe.

**Link**: [arxiv](http://arxiv.org/abs/2412.20504v4),  [pdf](http://arxiv.org/pdf/2412.20504v4)

**Tags**: cs.CV cs.CL cs.MM 



### CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences
**Authors**: Ziran Qin, Yuchen Cao, Mingbao Lin, Wen Hu, Shixuan Fan, Ke Cheng, Weiyao Lin, Jianguo Li

**Updated**: 2025-03-16T12:49:44Z

**Summary**: Large language models (LLMs) excel at processing long sequences, boosting demand for key-value (KV) caching. While recent efforts to evict KV cache have alleviated the inference burden, they often fail to allocate resources rationally across layers with different attention patterns. In this paper, we introduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach that frames KV cache eviction as a "cake-slicing problem." CAKE assesses layer-specific preferences by considering attention dynamics in both spatial and temporal dimensions, allocates rational cache size for layers accordingly, and manages memory constraints in a cascading manner. This approach enables a global view of cache allocation, adaptively distributing resources across diverse attention mechanisms while maintaining memory budgets. CAKE also employs a new eviction indicator that considers the shifting importance of tokens over time, addressing limitations in existing methods that overlook temporal dynamics. Comprehensive experiments on LongBench and NeedleBench show that CAKE maintains model performance with only 3.2% of the KV cache and consistently outperforms current baselines across various models and memory constraints, particularly in low-memory settings. Additionally, CAKE achieves over 10x speedup in decoding latency compared to full cache when processing contexts of 128K tokens with FlashAttention-2. Our code is available at https://github.com/antgroup/cakekv.

**Link**: [arxiv](http://arxiv.org/abs/2503.12491v1),  [pdf](http://arxiv.org/pdf/2503.12491v1)

**Tags**: cs.CL 



### LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching
**Authors**: Feihong Yan, Qingyan Wei, Jiayi Tang, Jiajun Li, Yulin Wang, Xuming Hu, Huiqi Li, Linfeng Zhang

**Updated**: 2025-03-16T10:54:59Z

**Summary**: Masked Autoregressive (MAR) models have emerged as a promising approach in image generation, expected to surpass traditional autoregressive models in computational efficiency by leveraging the capability of parallel decoding. However, their dependence on bidirectional self-attention inherently conflicts with conventional KV caching mechanisms, creating unexpected computational bottlenecks that undermine their expected efficiency. To address this problem, this paper studies the caching mechanism for MAR by leveraging two types of redundancy: Token Redundancy indicates that a large portion of tokens have very similar representations in the adjacent decoding steps, which allows us to first cache them in previous steps and then reuse them in the later steps. Condition Redundancy indicates that the difference between conditional and unconditional output in classifier-free guidance exhibits very similar values in adjacent steps. Based on these two redundancies, we propose LazyMAR, which introduces two caching mechanisms to handle them one by one. LazyMAR is training-free and plug-and-play for all MAR models. Experimental results demonstrate that our method achieves 2.83 times acceleration with almost no drop in generation quality. Our codes will be released in https://github.com/feihongyan1/LazyMAR.

**Link**: [arxiv](http://arxiv.org/abs/2503.12450v1),  [pdf](http://arxiv.org/pdf/2503.12450v1)

**Tags**: cs.CV 



### Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and   Generalizable Point Cloud Analysis
**Authors**: Hongyu Sun, Qiuhong Ke, Ming Cheng, Yongcai Wang, Deying Li, Chenhui Gou, Jianfei Cai

**Updated**: 2025-03-15T14:13:23Z

**Summary**: This paper proposes a general solution to enable point cloud recognition models to handle distribution shifts at test time. Unlike prior methods, which rely heavily on training data-often inaccessible during online inference-and are limited to recognizing a fixed set of point cloud classes predefined during training, we explore a more practical and challenging scenario: adapting the model solely based on online test data to recognize both previously seen classes and novel, unseen classes at test time. To this end, we develop Point-Cache, a hierarchical cache model that captures essential clues of online test samples, particularly focusing on the global structure of point clouds and their local-part details. Point-Cache, which serves as a rich 3D knowledge base, is dynamically managed to prioritize the inclusion of high-quality samples. Designed as a plug-and-play module, our method can be flexibly integrated into large multimodal 3D models to support open-vocabulary point cloud recognition. Notably, our solution operates with efficiency comparable to zero-shot inference, as it is entirely training-free. Point-Cache demonstrates substantial gains across 8 challenging benchmarks and 4 representative large 3D models, highlighting its effectiveness. Code is available at https://github.com/auniquesun/Point-Cache.

**Link**: [arxiv](http://arxiv.org/abs/2503.12150v1),  [pdf](http://arxiv.org/pdf/2503.12150v1)

**Tags**: cs.CV 



### MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion   Models
**Authors**: Yuchen Xia, Divyam Sharma, Yichao Yuan, Souvik Kundu, Nishil Talati

**Updated**: 2025-03-15T02:48:27Z

**Summary**: Diffusion-based text-to-image generation models trade latency for quality: small models are fast but generate lower-quality images, while large models produce better images but are slow.   We present MoDM, a novel caching-based serving system for diffusion models that dynamically balances latency and quality through a mixture of diffusion models. Unlike prior approaches that rely on model-specific internal features, MoDM caches final images, allowing seamless retrieval and reuse across multiple diffusion model families.   This design enables adaptive serving by dynamically balancing latency and image quality: using smaller models for cache-hit requests to reduce latency while reserving larger models for cache-miss requests to maintain quality. Small model image quality is preserved using retrieved cached images.   We design a global monitor that optimally allocates GPU resources and balances inference workload, ensuring high throughput while meeting service-level objectives under varying request rates. Our evaluations show that MoDM significantly reduces average serving time by 2.5x while retaining image quality, making it a practical solution for scalable and resource-efficient model deployment.

**Link**: [arxiv](http://arxiv.org/abs/2503.11972v1),  [pdf](http://arxiv.org/pdf/2503.11972v1)

**Tags**: cs.DC 



### CCRSat: A Collaborative Computation Reuse Framework for Satellite Edge   Computing Networks
**Authors**: Ye Zhang, Zhishu Shen, Dawen Jiang, Xiangrui Liu, Qiushi Zheng, Jiong Jin

**Updated**: 2025-03-15T01:35:53Z

**Summary**: In satellite computing applications, such as remote sensing, tasks often involve similar or identical input data, leading to the same processing results. Computation reuse is an emerging paradigm that leverages the execution results of previous tasks to enhance the utilization of computational resources. While this paradigm has been extensively studied in terrestrial networks with abundant computing and caching resources, such as named data networking (NDN), it is essential to develop a framework appropriate for resource-constrained satellite networks, which are expected to have longer task completion times. In this paper, we propose CCRSat, a collaborative computation reuse framework for satellite edge computing networks. CCRSat initially implements local computation reuse on an independent satellite, utilizing a satellite reuse state (SRS) to assess the efficiency of computation reuse. Additionally, an inter-satellite computation reuse algorithm is introduced, which utilizes the collaborative sharing of similarity in previously processed data among multiple satellites. The evaluation results tested on real-world datasets demonstrate that, compared to comparative scenarios, our proposed CCRSat can significantly reduce task completion time by up to 62.1% and computational resource consumption by up to 28.8%.

**Link**: [arxiv](http://arxiv.org/abs/2503.11946v1),  [pdf](http://arxiv.org/pdf/2503.11946v1)

**Tags**: cs.DC 



### Accelerating Sparse Tensor Decomposition Using Adaptive Linearized   Representation
**Authors**: Jan Laukemann, Ahmed E. Helal, S. Isaac Geronimo Anderson, Fabio Checconi, Yongseok Soh, Jesmin Jahan Tithi, Teresa Ranadive, Brian J Gravelle, Fabrizio Petrini, Jee Choi

**Updated**: 2025-03-15T00:49:55Z

**Summary**: High-dimensional sparse data emerge in many critical application domains such as healthcare and cybersecurity. To extract meaningful insights from massive volumes of these multi-dimensional data, scientists employ unsupervised analysis tools based on tensor decomposition (TD) methods. However, real-world sparse tensors exhibit highly irregular shapes and data distributions, which pose significant challenges for making efficient use of modern parallel processors. This study breaks the prevailing assumption that compressing sparse tensors into coarse-grained structures or along a particular dimension/mode is more efficient than keeping them in a fine-grained, mode-agnostic form. Our novel sparse tensor representation, Adaptive Linearized Tensor Order (ALTO), encodes tensors in a compact format that can be easily streamed from memory and is amenable to both caching and parallel execution. In contrast to existing compressed tensor formats, ALTO constructs one tensor copy that is agnostic to both the mode orientation and the irregular distribution of nonzero elements. To demonstrate the efficacy of ALTO, we propose a set of parallel TD algorithms that exploit the inherent data reuse of tensor computations to substantially reduce synchronization overhead, decrease memory footprint, and improve parallel performance. Additionally, we characterize the major execution bottlenecks of TD methods on the latest Intel Xeon Scalable processors and introduce dynamic adaptation heuristics to automatically select the best algorithm based on the sparse tensor characteristics. Across a diverse set of real-world data sets, ALTO outperforms the state-of-the-art approaches, achieving more than an order-of-magnitude speedup over the best mode-agnostic formats. Compared to the best mode-specific formats, ALTO achieves 5.1X geometric mean speedup at a fraction (25%) of their storage costs.

**Link**: [arxiv](http://arxiv.org/abs/2403.06348v2),  [pdf](http://arxiv.org/pdf/2403.06348v2)

**Tags**: cs.DC cs.DS cs.PF 



### Key, Value, Compress: A Systematic Exploration of KV Cache Compression   Techniques
**Authors**: Neusha Javidnia, Bita Darvish Rouhani, Farinaz Koushanfar

**Updated**: 2025-03-14T19:02:16Z

**Summary**: Large language models (LLMs) have demonstrated exceptional capabilities in generating text, images, and video content. However, as context length grows, the computational cost of attention increases quadratically with the number of tokens, presenting significant efficiency challenges. This paper presents an analysis of various Key-Value (KV) cache compression strategies, offering a comprehensive taxonomy that categorizes these methods by their underlying principles and implementation techniques. Furthermore, we evaluate their impact on performance and inference latency, providing critical insights into their effectiveness. Our findings highlight the trade-offs involved in KV cache compression and its influence on handling long-context scenarios, paving the way for more efficient LLM implementations.

**Link**: [arxiv](http://arxiv.org/abs/2503.11816v1),  [pdf](http://arxiv.org/pdf/2503.11816v1)

**Tags**: cs.CL 



### Making Every Step Effective: Jailbreaking Large Vision-Language Models   Through Hierarchical KV Equalization
**Authors**: Shuyang Hao, Yiwei Wang, Bryan Hooi, Jun Liu, Muhao Chen, Zi Huang, Yujun Cai

**Updated**: 2025-03-14T17:57:42Z

**Summary**: In the realm of large vision-language models (LVLMs), adversarial jailbreak attacks serve as a red-teaming approach to identify safety vulnerabilities of these models and their associated defense mechanisms. However, we identify a critical limitation: not every adversarial optimization step leads to a positive outcome, and indiscriminately accepting optimization results at each step may reduce the overall attack success rate. To address this challenge, we introduce HKVE (Hierarchical Key-Value Equalization), an innovative jailbreaking framework that selectively accepts gradient optimization results based on the distribution of attention scores across different layers, ensuring that every optimization step positively contributes to the attack. Extensive experiments demonstrate HKVE's significant effectiveness, achieving attack success rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL, substantially outperforming existing methods by margins of 20.43\%, 21.01\% and 26.43\% respectively. Furthermore, making every step effective not only leads to an increase in attack success rate but also allows for a reduction in the number of iterations, thereby lowering computational costs. Warning: This paper contains potentially harmful example data.

**Link**: [arxiv](http://arxiv.org/abs/2503.11750v1),  [pdf](http://arxiv.org/pdf/2503.11750v1)

**Tags**: cs.CV cs.CR 



### Alchemist: Towards the Design of Efficient Online Continual Learning   System
**Authors**: Yuyang Huang, Yuhan Liu, Haryadi S. Gunawi, Beibin Li, Changho Hwang

**Updated**: 2025-03-14T16:57:12Z

**Summary**: Continual learning has become a promising solution to refine large language models incrementally by leveraging user feedback. In particular, online continual learning - iteratively training the model with small batches of user feedback - has demonstrated notable performance improvements. However, the existing practice of separating training and serving processes forces the online trainer to recompute the intermediate results already done during serving. Such redundant computations can account for 30%-42% of total training time.   In this paper, we propose Alchemist, to the best of our knowledge, the first online continual learning system that efficiently reuses serving activations to increase training throughput. Alchemist introduces two key techniques: (1) recording and storing activations and KV cache only during the prefill phase to minimize latency and memory overhead; and (2) smart activation offloading and hedging. Evaluations with inputs of varied token length sampled from ShareGPT dataset show that compared with a separate training cluster, Alchemist significantly increases training throughput by up to 1.72x, reduces up to 47% memory usage during training, and supports up to 2x more training tokens - all while maintaining negligible impact on serving latency.

**Link**: [arxiv](http://arxiv.org/abs/2503.01066v2),  [pdf](http://arxiv.org/pdf/2503.01066v2)

**Tags**: cs.LG cs.CL cs.DC 



### ARCAS: Adaptive Runtime System for Chiplet-Aware Scheduling
**Authors**: Alessandro Fogli, Bo Zhao, Peter Pietzuch, Jana Giceva

**Updated**: 2025-03-14T14:47:55Z

**Summary**: The growing disparity between CPU core counts and available memory bandwidth has intensified memory contention in servers. This particularly affects highly parallelizable applications, which must achieve efficient cache utilization to maintain performance as CPU core counts grow. Optimizing cache utilization, however, is complex for recent chiplet-based CPUs, whose partitioned L3 caches lead to varying latencies and bandwidths, even within a single NUMA domain. Classical NUMA optimizations and task scheduling approaches unfortunately fail to address the performance issues of chiplet-based CPUs.   We describe Adaptive Runtime system for Chiplet-Aware Scheduling (ARCAS), a new runtime system designed for chiplet-based CPUs. ARCAS combines chiplet-aware task scheduling heuristics, hardware-aware memory allocation, and fine-grained performance monitoring to optimize workload execution. It implements a lightweight concurrency model that combines user-level thread features-such as individual stacks, per-task scheduling, and state management-with coroutine-like behavior, allowing tasks to suspend and resume execution at defined points while efficiently managing task migration across chiplets. Our evaluation across diverse scenarios shows ARCAS's effectiveness for optimizing the performance of memory-intensive parallel applications.

**Link**: [arxiv](http://arxiv.org/abs/2503.11460v1),  [pdf](http://arxiv.org/pdf/2503.11460v1)

**Tags**: cs.AR cs.DC cs.PF cs.SY eess.SY 



### Text Compression for Efficient Language Generation
**Authors**: David Gu, Peter Belcak, Roger Wattenhofer

**Updated**: 2025-03-14T14:14:05Z

**Summary**: We challenge the prevailing assumption that LLMs must rely fully on sub-word tokens for high-quality text generation. To this end, we propose the "Generative Pretrained Thoughtformer" (GPTHF), a hierarchical transformer language model capable of text generation by compressing text into sentence embeddings and employing a sentence attention mechanism. GPTHF retains GPT's architecture, modifying only token interactions via dynamic sparse attention masks.   Our experiments show that GPTHF achieves an up to an order of magnitude improvement in FLOPs efficiency and a threefold increase in runtime speed compared to equally-sized GPT models in the low-size regime. This is achieved through a unique generation method that caches and reuses sentence embeddings, allowing significant portions of the input to bypass large parts of the network.

**Link**: [arxiv](http://arxiv.org/abs/2503.11426v1),  [pdf](http://arxiv.org/pdf/2503.11426v1)

**Tags**: cs.CL 



### X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and   Extreme KV Compression
**Authors**: Guihong Li, Mehdi Rezagholizadeh, Mingyu Yang, Vikram Appia, Emad Barsoum

**Updated**: 2025-03-14T06:49:37Z

**Summary**: Multi-head latent attention (MLA) is designed to optimize KV cache memory through low-rank key-value joint compression. Rather than caching keys and values separately, MLA stores their compressed latent representations, reducing memory overhead while maintaining the performance. While MLA improves memory efficiency without compromising language model accuracy, its major limitation lies in its integration during the pre-training phase, requiring models to be trained from scratch. This raises a key question: can we use MLA's benefits fully or partially in models that have already been pre-trained with different attention mechanisms? In this paper, we propose X-EcoMLA to deploy post training distillation to enable the upcycling of Transformer-based attention into an efficient hybrid (i.e., combination of regular attention and MLA layers) or full MLA variant through lightweight post-training adaptation, bypassing the need for extensive pre-training. We demonstrate that leveraging the dark knowledge of a well-trained model can enhance training accuracy and enable extreme KV cache compression in MLA without compromising model performance. Our results show that using an 8B teacher model allows us to compress the KV cache size of the Llama3.2-1B-Inst baseline by 6.4x while preserving 100% of its average score across multiple tasks on the LM Harness Evaluation benchmark. This is achieved with only 3.6B training tokens and about 70 GPU hours on AMD MI300 GPUs, compared to the 370K GPU hours required for pre-training the Llama3.2-1B model.

**Link**: [arxiv](http://arxiv.org/abs/2503.11132v1),  [pdf](http://arxiv.org/pdf/2503.11132v1)

**Tags**: cs.CL 



### Limits of KV Cache Compression for Tensor Attention based Autoregressive   Transformers
**Authors**: Yifang Chen, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Yu Tian

**Updated**: 2025-03-14T06:01:42Z

**Summary**: The key-value (KV) cache in autoregressive transformers presents a significant bottleneck during inference, which restricts the context length capabilities of large language models (LLMs). While previous work analyzes the fundamental space complexity barriers in standard attention mechanism [Haris and Onak, 2025], our work generalizes the space complexity barriers result to tensor attention version. Our theoretical contributions rely on a novel reduction from communication complexity and deduce the memory lower bound for tensor-structured attention mechanisms when $d = \Omega(\log n)$. In the low dimensional regime where $d = o(\log n)$, we analyze the theoretical bounds of the space complexity as well. Overall, our work provides a theoretical foundation for us to understand the compression-expressivity tradeoff in tensor attention mechanisms and offers more perspectives in developing more memory-efficient transformer architectures.

**Link**: [arxiv](http://arxiv.org/abs/2503.11108v1),  [pdf](http://arxiv.org/pdf/2503.11108v1)

**Tags**: cs.LG cs.AI cs.CC cs.CL 



### Long Context Tuning for Video Generation
**Authors**: Yuwei Guo, Ceyuan Yang, Ziyan Yang, Zhibei Ma, Zhijie Lin, Zhenheng Yang, Dahua Lin, Lu Jiang

**Updated**: 2025-03-13T17:40:07Z

**Summary**: Recent advances in video generation can produce realistic, minute-long single-shot videos with scalable diffusion transformers. However, real-world narrative videos require multi-shot scenes with visual and dynamic consistency across shots. In this work, we introduce Long Context Tuning (LCT), a training paradigm that expands the context window of pre-trained single-shot video diffusion models to learn scene-level consistency directly from data. Our method expands full attention mechanisms from individual shots to encompass all shots within a scene, incorporating interleaved 3D position embedding and an asynchronous noise strategy, enabling both joint and auto-regressive shot generation without additional parameters. Models with bidirectional attention after LCT can further be fine-tuned with context-causal attention, facilitating auto-regressive generation with efficient KV-cache. Experiments demonstrate single-shot models after LCT can produce coherent multi-shot scenes and exhibit emerging capabilities, including compositional generation and interactive shot extension, paving the way for more practical visual content creation. See https://guoyww.github.io/projects/long-context-video/ for more details.

**Link**: [arxiv](http://arxiv.org/abs/2503.10589v1),  [pdf](http://arxiv.org/pdf/2503.10589v1)

**Tags**: cs.CV 



### Autoregressive Image Generation with Randomized Parallel Decoding
**Authors**: Haopeng Li, Jinyue Yang, Guoqi Li, Huan Wang

**Updated**: 2025-03-13T17:19:51Z

**Summary**: We introduce ARPG, a novel visual autoregressive model that enables randomized parallel generation, addressing the inherent limitations of conventional raster-order approaches, which hinder inference efficiency and zero-shot generalization due to their sequential, predefined token generation order. Our key insight is that effective random-order modeling necessitates explicit guidance for determining the position of the next predicted token. To this end, we propose a novel guided decoding framework that decouples positional guidance from content representation, encoding them separately as queries and key-value pairs. By directly incorporating this guidance into the causal attention mechanism, our approach enables fully random-order training and generation, eliminating the need for bidirectional attention. Consequently, ARPG readily generalizes to zero-shot tasks such as image inpainting, outpainting, and resolution expansion. Furthermore, it supports parallel inference by concurrently processing multiple queries using a shared KV cache. On the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only 64 sampling steps, achieving over a 20-fold increase in throughput while reducing memory consumption by over 75% compared to representative recent autoregressive models at a similar scale.

**Link**: [arxiv](http://arxiv.org/abs/2503.10568v1),  [pdf](http://arxiv.org/pdf/2503.10568v1)

**Tags**: cs.CV 



### ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion   Transformer
**Authors**: Jinyi Hu, Shengding Hu, Yuxuan Song, Yufei Huang, Mingxuan Wang, Hao Zhou, Zhiyuan Liu, Wei-Ying Ma, Maosong Sun

**Updated**: 2025-03-13T16:29:17Z

**Summary**: We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion Transformer, that innovatively combines autoregressive and diffusion paradigms for modeling continuous visual information. By introducing a block-wise autoregressive unit, ACDiT offers a flexible interpolation between token-wise autoregression and full-sequence diffusion, bypassing the limitations of discrete tokenization. The generation of each block is formulated as a conditional diffusion process, conditioned on prior blocks. ACDiT is easy to implement, as simple as creating a Skip-Causal Attention Mask (SCAM) on standard diffusion transformer during training. During inference, the process iterates between diffusion denoising and autoregressive decoding that can make full use of KV-Cache. We show that ACDiT performs best among all autoregressive baselines under similar model scales on image and video generation tasks. We also demonstrate that benefiting from autoregressive modeling, pretrained ACDiT can be transferred in visual understanding tasks despite being trained with the diffusion objective. The analysis of the trade-off between autoregressive modeling and diffusion demonstrates the potential of ACDiT to be used in long-horizon visual generation tasks. We hope that ACDiT offers a novel perspective on visual autoregressive generation and unlocks new avenues for unified models.

**Link**: [arxiv](http://arxiv.org/abs/2412.07720v2),  [pdf](http://arxiv.org/pdf/2412.07720v2)

**Tags**: cs.CV 



### TokenCarve: Information-Preserving Visual Token Compression in   Multimodal Large Language Models
**Authors**: Xudong Tan, Peng Ye, Chongjun Tu, Jianjian Cao, Yaoxin Yang, Lin Zhang, Dongzhan Zhou, Tao Chen

**Updated**: 2025-03-13T16:04:31Z

**Summary**: Multimodal Large Language Models (MLLMs) are becoming increasingly popular, while the high computational cost associated with multimodal data input, particularly from visual tokens, poses a significant challenge. Existing training-based token compression methods improve inference efficiency but require costly retraining, while training-free methods struggle to maintain performance when aggressively reducing token counts. In this study, we reveal that the performance degradation of MLLM closely correlates with the accelerated loss of information in the attention output matrix. This insight introduces a novel information-preserving perspective, making it possible to maintain performance even under extreme token compression. Based on this finding, we propose TokenCarve, a training-free, plug-and-play, two-stage token compression framework. The first stage employs an Information-Preservation-Guided Selection (IPGS) strategy to prune low-information tokens, while the second stage further leverages IPGS to guide token merging, minimizing information loss. Extensive experiments on 11 datasets and 2 model variants demonstrate the effectiveness of TokenCarve. It can even reduce the number of visual tokens to 22.2% of the original count, achieving a 1.23x speedup in inference, a 64% reduction in KV cache storage, and only a 1.54% drop in accuracy. Our code is available at https://github.com/ShawnTan86/TokenCarve.

**Link**: [arxiv](http://arxiv.org/abs/2503.10501v1),  [pdf](http://arxiv.org/pdf/2503.10501v1)

**Tags**: cs.CV 



### Source-primed Multi-turn Conversation Helps Large Language Models   Translate Documents
**Authors**: Hanxu Hu, Jannis Vamvas, Rico Sennrich

**Updated**: 2025-03-13T15:57:50Z

**Summary**: LLMs have paved the way for truly simple document-level machine translation, but challenges such as omission errors remain. In this paper, we study a simple method for handling document-level machine translation, by leveraging previous contexts in a multi-turn conversational manner. Specifically, by decomposing documents into segments and iteratively translating them while maintaining previous turns, this method ensures coherent translations without additional training, and can fully re-use the KV cache of previous turns thus minimizing computational overhead. We further propose a `source-primed' method that first provides the whole source document before multi-turn translation. We empirically show this multi-turn method outperforms both translating entire documents in a single turn and translating each segment independently according to multiple automatic metrics in representative LLMs, establishing a strong baseline for document-level translation using LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2503.10494v1),  [pdf](http://arxiv.org/pdf/2503.10494v1)

**Tags**: cs.CL 



### KV-Distill: Nearly Lossless Learnable Context Compression for LLMs
**Authors**: Vivek Chari, Guanghui Qin, Benjamin Van Durme

**Updated**: 2025-03-13T13:15:28Z

**Summary**: Sequence-to-sequence tasks often benefit from long contexts, but the quadratic complexity of self-attention in standard Transformers renders this non-trivial. During generation, temporary representations -stored in the so-called KV cache-account for a large portion of GPU memory usage and scale linearly with context length. We introduce KV-Distill, a Transformer compression framework that distills long context KV caches into significantly shorter representations in a question-independent fashion. KV-Distill can be trained as a parameter-efficient adaptor for pretrained models, and enables the compression of arbitrary spans of a context while preserving pre-trained model capabilities. We treat a compressed-uncompressed cache as a student-teacher pairing and apply a KL-type divergence to match the generated outputs. KV-Distill outperforms other compression techniques in worst-case extractive tasks and approaches uncompressed performance in long context question answering and summarization, and it can be fine-tuned on domain-specific contexts to reduce lengths by up to 99% while preserving downstream performance. We demonstrate the generalizability of KV-Distill across various model sizes and architectures.

**Link**: [arxiv](http://arxiv.org/abs/2503.10337v1),  [pdf](http://arxiv.org/pdf/2503.10337v1)

**Tags**: cs.CL cs.AI 



### EEdit : Rethinking the Spatial and Temporal Redundancy for Efficient   Image Editing
**Authors**: Zexuan Yan, Yue Ma, Chang Zou, Wenteng Chen, Qifeng Chen, Linfeng Zhang

**Updated**: 2025-03-13T11:26:45Z

**Summary**: Inversion-based image editing is rapidly gaining momentum while suffering from significant computation overhead, hindering its application in real-time interactive scenarios. In this paper, we rethink that the redundancy in inversion-based image editing exists in both the spatial and temporal dimensions, such as the unnecessary computation in unedited regions and the redundancy in the inversion progress. To tackle these challenges, we propose a practical framework, named EEdit, to achieve efficient image editing. Specifically, we introduce three techniques to solve them one by one. For spatial redundancy, spatial locality caching is introduced to compute the edited region and its neighboring regions while skipping the unedited regions, and token indexing preprocessing is designed to further accelerate the caching. For temporal redundancy, inversion step skipping is proposed to reuse the latent for efficient editing. Our experiments demonstrate an average of 2.46 $\times$ acceleration without performance drop in a wide range of editing tasks including prompt-guided image editing, dragging and image composition. Our codes are available at https://github.com/yuriYanZeXuan/EEdit

**Link**: [arxiv](http://arxiv.org/abs/2503.10270v1),  [pdf](http://arxiv.org/pdf/2503.10270v1)

**Tags**: cs.CV 



### FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware
**Authors**: Korbinian Pöppel, Maximilian Beck, Sepp Hochreiter

**Updated**: 2025-03-13T11:14:49Z

**Summary**: While Transformers and other sequence-parallelizable neural network architectures seem like the current state of the art in sequence modeling, they specifically lack state-tracking capabilities. These are important for time-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs, as well as modern variants like sLSTM do have these capabilities at the cost of strictly sequential processing. While this is often seen as a strong limitation, we show how fast these networks can get with our hardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the register level on modern GPUs. We extend traditional RNNs with a parallelization variant that processes multiple RNNs of smaller hidden state in parallel, similar to the head-wise processing in Transformers. To enable flexibility on different GPU variants, we introduce a new optimization framework for hardware-internal cache sizes, memory and compute handling. It models the hardware in a setting using polyhedral-like constraints, including the notion of divisibility. This speeds up the solution process in our ConstrINT library for general integer constraint satisfaction problems (integer CSPs). We show that our kernels can achieve 50x speed-ups over a vanilla PyTorch implementation and allow 40x larger hidden sizes compared to our Triton implementation. Our open-source kernels and the optimization library are released here to boost research in the direction of state-tracking enabled RNNs and sequence modeling: https://github.com/NX-AI/flashrnn

**Link**: [arxiv](http://arxiv.org/abs/2412.07752v3),  [pdf](http://arxiv.org/pdf/2412.07752v3)

**Tags**: cs.LG cs.AI 



### Demoting Security via Exploitation of Cache Demote Operation in Intel's   Latest ISA Extension
**Authors**: Taehun Kim, Hyerean Jang, Youngjoo Shin

**Updated**: 2025-03-13T05:43:14Z

**Summary**: ISA extensions are increasingly adopted to boost the performance of specialized workloads without requiring an entire architectural redesign. However, these enhancements can inadvertently expose new attack surfaces in the microarchitecture. In this paper, we investigate Intel's recently introduced cldemote extension, which promotes efficient data sharing by transferring cache lines from upper-level caches to the Last Level Cache (LLC). Despite its performance benefits, we uncover critical properties-unprivileged access, inter-cache state transition, and fault suppression-that render cldemote exploitable for microarchitectural attacks. We propose two new attack primitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote constructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate of 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on Linux. Furthermore, we show that leveraging cldemote accelerates eviction set construction in non-inclusive LLC designs by obviating the need for helper threads or extensive cache conflicts, thereby reducing construction time by 36% yet retaining comparable success rates. Finally, we examine how ISA extensions contribute to broader microarchitectural attacks, identifying five key exploitable characteristics and categorizing four distinct attack types. We also discuss potential countermeasures, highlighting the far-reaching security implications of emerging ISA extensions.

**Link**: [arxiv](http://arxiv.org/abs/2503.10074v1),  [pdf](http://arxiv.org/pdf/2503.10074v1)

**Tags**: cs.CR 



### MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context   Inference
**Authors**: Zhongwei Wan, Hui Shen, Xin Wang, Che Liu, Zheda Mai, Mi Zhang

**Updated**: 2025-03-13T04:04:08Z

**Summary**: Long-context Multimodal Large Language Models (MLLMs) that incorporate long text-image and text-video modalities, demand substantial resources as their multimodal Key-Value (KV) caches grow with increasing input lengths, challenging inference efficiency. Existing methods for KV cache compression, in both text-only and multimodal LLMs, have neglected attention density variations across layers, thus often adopting uniform or progressive reduction strategies for layer-wise cache allocation. In this work, we propose MEDA, a dynamic layer-wise KV cache allocation method for efficient multimodal long-context inference. As its core, MEDA utilizes cross-modal attention entropy to determine the KV cache size at each MLLMs layer. Given the dynamically allocated KV cache size at each layer, MEDA also employs a KV pair selection scheme to identify which KV pairs to select and a KV pair merging strategy that merges the selected and non-selected ones to preserve information from the entire context. MEDA achieves up to 72% KV cache memory reduction and 2.82 times faster decoding speed, while maintaining or enhancing performance on various multimodal tasks in long-context settings, including multi-images and long-video scenarios. Our code is released at https://github.com/AIoT-MLSys-Lab/MEDA.

**Link**: [arxiv](http://arxiv.org/abs/2502.17599v2),  [pdf](http://arxiv.org/pdf/2502.17599v2)

**Tags**: cs.CL 



### ZeroMerge: Parameter-Free KV Cache Compression for Memory-Efficient   Long-Context LLMs
**Authors**: Xin Liu, Pei Liu, Guoming Tang

**Updated**: 2025-03-13T03:36:03Z

**Summary**: The linear growth of key-value (KV) cache memory and quadratic computational complexity pose significant bottlenecks for large language models (LLMs) in long-context processing. While existing KV cache optimization methods address these challenges through token pruning or feature merging, they often suffer from irreversible information loss or require costly parameter retraining. We propose ZeroMerge, a dynamic zero-shot compression framework that achieves efficient cache management through three key innovations: (1) Fine-grained memory allocation guided by multi-dimensional token importance metrics at head-level granularity, (2) A residual merging mechanism that preserves critical context through compensated attention scoring, and (3) Parameter-free adaptation compatible with diverse LLM architectures without retraining. Comprehensive evaluations across LLaMA-2 model demonstrate that ZeroMerge maintains full-cache performance at 5\% compression ratios while doubling inference throughput at 40K token lengths. The method effectively balances memory efficiency, generation quality, and deployment flexibility, advancing practical long-context LLM applications. The code is available at https://github.com/SusCom-Lab/ZeroMerge.

**Link**: [arxiv](http://arxiv.org/abs/2503.10714v1),  [pdf](http://arxiv.org/pdf/2503.10714v1)

**Tags**: cs.CL cs.AI 



### D2O: Dynamic Discriminative Operations for Efficient Long-Context   Inference of Large Language Models
**Authors**: Zhongwei Wan, Xinjian Wu, Yu Zhang, Yi Xin, Chaofan Tao, Zhihong Zhu, Xin Wang, Siqi Luo, Jing Xiong, Longyue Wang, Mi Zhang

**Updated**: 2025-03-13T03:16:43Z

**Summary**: Generative inference in Large Language Models (LLMs) is impeded by the growing memory demands of Key-Value (KV) cache, especially for longer sequences. Traditional KV cache eviction strategies, which discard less critical KV pairs based on attention scores, often degrade generation quality, leading to issues such as context loss or hallucinations. In this work, we introduce Dynamic Discriminative Operations (D2O), a KV cache compression method that optimizes KV cache size dynamically and discriminatively at two levels without fine-tuning, while preserving essential context. At layer level, D2O leverages the varying densities of attention weights between shallow and deep layers to dynamically determine which layers should avoid excessive eviction via a novel dynamic allocation strategy to minimize information loss. At token level, D2O incorporates a compensation mechanism that maintains a similarity threshold to re-discriminate the importance of currently discarded tokens, determining whether they should be recalled and merged with similar tokens. We conduct experiments on various benchmarks and LLM architectures. Our results show that D2O not only achieves significant memory savings and enhances inference throughput by more than 3$\times$ but also maintains high-quality long-text generation.

**Link**: [arxiv](http://arxiv.org/abs/2406.13035v3),  [pdf](http://arxiv.org/pdf/2406.13035v3)

**Tags**: cs.CL 



### MoE-Infinity: Efficient MoE Inference on Personal Machines with   Sparsity-Aware Expert Cache
**Authors**: Leyang Xue, Yao Fu, Zhan Lu, Luo Mai, Mahesh Marina

**Updated**: 2025-03-12T18:14:21Z

**Summary**: This paper presents MoE-Infinity, an efficient MoE inference system designed for personal machines with limited GPU memory capacity. The key idea for MoE-Infinity is that on personal machines, which are often single-user environments, MoE-based LLMs typically operate with a batch size of one. In this setting, MoE models exhibit a high degree of activation sparsity, meaning a small number of experts are frequently reused in generating tokens during the decode phase. Leveraging this idea, we design a sparsity-aware expert cache, which can trace the sparse activation of experts during inference and carefully select the trace that represents the sparsity pattern. By analyzing these selected traces, MoE-Infinity guides the replacement and prefetching of the expert cache, providing 3.1-16.7x per-token latency improvements over numerous state-of-the-art systems, including vLLM, Ollama, DeepSpeed and BrainStorm across various MoE models (DeepSeek and Mixtral) when handling different LLM tasks. MoE-Infinity's source code is publicly available at https://github.com/EfficientMoE/MoE-Infinity

**Link**: [arxiv](http://arxiv.org/abs/2401.14361v3),  [pdf](http://arxiv.org/pdf/2401.14361v3)

**Tags**: cs.LG cs.PF 



### PRISM: Efficient Long-Range Reasoning With Short-Context LLMs
**Authors**: Dulhan Jayalath, James Bradley Wendt, Nicholas Monath, Sandeep Tata, Beliz Gunel

**Updated**: 2025-03-12T17:59:18Z

**Summary**: Long-range tasks demand reasoning over long inputs. Current solutions require large compute budgets, training data, model weight access, or complex task-specific designs. We introduce PRISM, which processes information as a stream of chunks while maintaining a structured in-context memory specified with a typed hierarchical schema. PRISM outperforms baselines on diverse tasks while using at least 4x shorter contexts than long-context models. This approach is token-efficient, producing concise outputs and efficiently leveraging key-value (KV) caches to reduce costs by up to 54% compared to alternative short-context methods. PRISM scales down to tiny chunks (<500 tokens) without increasing encoding costs or sacrificing quality, and generalizes to new tasks with minimal effort by automatically generating schemas from task descriptions.

**Link**: [arxiv](http://arxiv.org/abs/2412.18914v2),  [pdf](http://arxiv.org/pdf/2412.18914v2)

**Tags**: cs.AI 



### Block Diffusion: Interpolating Between Autoregressive and Diffusion   Language Models
**Authors**: Marianne Arriola, Aaron Gokaslan, Justin T Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, Volodymyr Kuleshov

**Updated**: 2025-03-12T17:43:40Z

**Summary**: Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page: https://m-arriola.com/bd3lms/

**Link**: [arxiv](http://arxiv.org/abs/2503.09573v1),  [pdf](http://arxiv.org/pdf/2503.09573v1)

**Tags**: cs.LG cs.AI 



### N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual   In-Context Learning
**Authors**: Jie He, Simon Yu, Deyi Xiong, Víctor Gutiérrez-Basulto, Jeff Z. Pan

**Updated**: 2025-03-12T10:05:05Z

**Summary**: Recent advancements of in-context learning (ICL) show language models can significantly improve their performance when demonstrations are provided. However, little attention has been paid to model calibration and prediction confidence of ICL in cross-lingual scenarios. To bridge this gap, we conduct a thorough analysis of ICL for cross-lingual sentiment classification. Our findings suggest that ICL performs poorly in cross-lingual scenarios, exhibiting low accuracy and presenting high calibration errors. In response, we propose a novel approach, N2C2, which employs a -nearest neighbors augmented classifier for prediction confidence calibration. N2C2 narrows the prediction gap by leveraging a datastore of cached few-shot instances. Specifically, N2C2 integrates the predictions from the datastore and incorporates confidence-aware distribution, semantically consistent retrieval representation, and adaptive neighbor combination modules to effectively utilize the limited number of supporting instances. Evaluation on two multilingual sentiment classification datasets demonstrates that N2C2 outperforms traditional ICL. It surpasses fine tuning, prompt tuning and recent state-of-the-art methods in terms of accuracy and calibration errors.

**Link**: [arxiv](http://arxiv.org/abs/2503.09218v1),  [pdf](http://arxiv.org/pdf/2503.09218v1)

**Tags**: cs.CL 



### KV-Edit: Training-Free Image Editing for Precise Background Preservation
**Authors**: Tianrui Zhu, Shiyi Zhang, Jiawei Shao, Yansong Tang

**Updated**: 2025-03-12T07:23:32Z

**Summary**: Background consistency remains a significant challenge in image editing tasks. Despite extensive developments, existing works still face a trade-off between maintaining similarity to the original image and generating content that aligns with the target. Here, we propose KV-Edit, a training-free approach that uses KV cache in DiTs to maintain background consistency, where background tokens are preserved rather than regenerated, eliminating the need for complex mechanisms or expensive training, ultimately generating new content that seamlessly integrates with the background within user-provided regions. We further explore the memory consumption of the KV cache during editing and optimize the space complexity to $O(1)$ using an inversion-free method. Our approach is compatible with any DiT-based generative model without additional training. Experiments demonstrate that KV-Edit significantly outperforms existing approaches in terms of both background and image quality, even surpassing training-based methods. Project webpage is available at https://xilluill.github.io/projectpages/KV-Edit

**Link**: [arxiv](http://arxiv.org/abs/2502.17363v3),  [pdf](http://arxiv.org/pdf/2502.17363v3)

**Tags**: cs.CV 



### FasterCache: Training-Free Video Diffusion Model Acceleration with High   Quality
**Authors**: Zhengyao Lv, Chenyang Si, Junhao Song, Zhenyu Yang, Yu Qiao, Ziwei Liu, Kwan-Yee K. Wong

**Updated**: 2025-03-12T03:40:38Z

**Summary**: In this paper, we present \textbf{\textit{FasterCache}}, a novel training-free strategy designed to accelerate the inference of video diffusion models with high-quality generation. By analyzing existing cache-based methods, we observe that \textit{directly reusing adjacent-step features degrades video quality due to the loss of subtle variations}. We further perform a pioneering investigation of the acceleration potential of classifier-free guidance (CFG) and reveal significant redundancy between conditional and unconditional features within the same timestep. Capitalizing on these observations, we introduce FasterCache to substantially accelerate diffusion-based video generation. Our key contributions include a dynamic feature reuse strategy that preserves both feature distinction and temporal continuity, and CFG-Cache which optimizes the reuse of conditional and unconditional outputs to further enhance inference speed without compromising video quality. We empirically evaluate FasterCache on recent video diffusion models. Experimental results show that FasterCache can significantly accelerate video generation (\eg 1.67$\times$ speedup on Vchitect-2.0) while keeping video quality comparable to the baseline, and consistently outperform existing methods in both inference speed and video quality.

**Link**: [arxiv](http://arxiv.org/abs/2410.19355v2),  [pdf](http://arxiv.org/pdf/2410.19355v2)

**Tags**: cs.CV 



### Performance Models for a Two-tiered Storage System
**Authors**: Aparna Sasidharan, Xian-He, Jay Lofstead, Scott Klasky

**Updated**: 2025-03-12T00:12:39Z

**Summary**: This work describes the design, implementation and performance analysis of a distributed two-tiered storage software. The first tier functions as a distributed software cache implemented using solid-state devices~(NVMes) and the second tier consists of multiple hard disks~(HDDs). We describe an online learning algorithm that manages data movement between the tiers. The software is hybrid, i.e. both distributed and multi-threaded. The end-to-end performance model of the two-tier system was developed using queuing networks and behavioral models of storage devices. We identified significant parameters that affect the performance of storage devices and created behavioral models for each device. The performance of the software was evaluated on a many-core cluster using non-trivial read/write workloads. The paper provides examples to illustrate the use of these models.

**Link**: [arxiv](http://arxiv.org/abs/2503.08966v1),  [pdf](http://arxiv.org/pdf/2503.08966v1)

**Tags**: cs.DC 



### BCZT/LSMO/BCZT multilayer films for high temperature energy storage   capacitors
**Authors**: Afaak Lakouader, Abdelilah Lahmar, Spela Kunej, Daoud Mezzane, Jamal Belhadi, El Hassan Choukri, Lahoucine Hajji, Mbarek Amjoud, Zdravko Kutnjak, Igor A. Lukyanchuk, Mimoun El Marssi

**Updated**: 2025-03-11T22:44:38Z

**Summary**: Ba0.85Ca0.15Zr0.1Ti0.9O3/La0.8Sr0.2MnO3/Ba0.85Ca0.15Zr0.1Ti0.9O3 (BCZT/LSMO/BCZT) sandwich films were elaborated using the sol-gel spin coating process. The dielectric properties displayed excellent thermal stability with the temperature coefficient of capacitance, TCC, remaining within 10% between -50 C and 300 C. The high energy storage density, Wrec, of 11.8 J/cm3 observed in this sandwich films, is nearly twice as high as that of the BCZT films, with an efficiency, n, of 77% under a weak electric field of 800 kV/cm. Furthermore, the stability of Wrec and n was observed along the studied temperature interval making them promising candidates for high-temperature energy storage capacitors.

**Link**: [arxiv](http://arxiv.org/abs/2503.08941v1),  [pdf](http://arxiv.org/pdf/2503.08941v1)

**Tags**: cond-mat.mtrl-sci physics.app-ph 



### LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for   Efficient Long-Context Inference
**Authors**: Guangtao Wang, Shubhangi Upasani, Chen Wu, Darshan Gandhi, Jonathan Li, Changran Hu, Bo Li, Urmish Thakker

**Updated**: 2025-03-11T20:45:02Z

**Summary**: Efficient long-context inference is critical as large language models (LLMs) adopt context windows of ranging from 128K to 1M tokens. However, the growing key-value (KV) cache and the high computational complexity of attention create significant bottlenecks in memory usage and latency. In this paper, we find that attention in diverse long-context tasks exhibits sparsity, and LLMs implicitly "know" which tokens can be dropped or evicted at the head level after the pre-filling stage. Based on this insight, we propose Self-Attention Guided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for long-context inference. After prefilling, our method performs a one-time top-k selection at both the token and head levels to compress the KV cache, enabling efficient inference with the reduced cache. Evaluations on LongBench and three long-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct, and Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable to full attention while significantly improving efficiency. Specifically, SAGE-KV achieves 4x higher memory efficiency with improved accuracy over the static KV cache selection method StreamLLM, and 2x higher memory efficiency with better accuracy than the dynamic KV cache selection method Quest.

**Link**: [arxiv](http://arxiv.org/abs/2503.08879v1),  [pdf](http://arxiv.org/pdf/2503.08879v1)

**Tags**: cs.CL cs.AI cs.LG 



### Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse   Attention
**Authors**: Emily Xiao, Chin-Jou Li, Yilin Zhang, Graham Neubig, Amanda Bertsch

**Updated**: 2025-03-11T17:30:58Z

**Summary**: Many-shot in-context learning has recently shown promise as an alternative to finetuning, with the major advantage that the same model can be served for multiple tasks. However, this shifts the computational burden from training-time to inference-time, making deployment of many-shot ICL challenging to justify in-practice. This cost is further increased if a custom demonstration set is retrieved for each inference example. We present Dynamic Block-Sparse Attention, a training-free framework for retrieval-based many-shot in-context learning. By combining carefully designed block-sparse attention and retrieval of cached groups of demonstrations, we achieve comparable per-example latency to finetuning while maintaining on average >95% of the best method's accuracy across strong ICL and finetuning baselines. We hope that this will further enable the deployment of many-shot ICL at scale.

**Link**: [arxiv](http://arxiv.org/abs/2503.08640v1),  [pdf](http://arxiv.org/pdf/2503.08640v1)

**Tags**: cs.CL 



### FastCache: Optimizing Multimodal LLM Serving through Lightweight   KV-Cache Compression Framework
**Authors**: Jianian Zhu, Hang Wu, Haojie Wang, Yinghui Li, Biao Hou, Ruixuan Li, Jidong Zhai

**Updated**: 2025-03-11T14:10:58Z

**Summary**: Multi-modal Large Language Models (MLLMs) serving systems commonly employ KV-cache compression to reduce memory footprint. However, existing compression methods introduce significant processing overhead and queuing delays, particularly in concurrent serving scenarios. We present \texttt{FastCache}, a novel serving framework that effectively addresses these challenges through two key innovations: (1) a dynamic batching strategy that optimizes request scheduling across prefill, compression, and decode stages, and (2) an efficient KV-cache memory pool mechanism that eliminates memory fragmentation while maintaining high GPU utilization. Our comprehensive experiments on the GQA and MileBench datasets demonstrate that \texttt{FastCache} achieves up to 19.3$\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\times$ improvement in throughput compared to state-of-the-art baselines. The system maintains stable performance under high-concurrency scenarios (up to 40 req/s) while reducing average memory consumption by 20\%. These results establish \texttt{FastCache} as an efficient solution for real-world LLM serving systems with KV-cache compression.

**Link**: [arxiv](http://arxiv.org/abs/2503.08461v1),  [pdf](http://arxiv.org/pdf/2503.08461v1)

**Tags**: cs.MM cs.DC 



### SCBench: A KV Cache-Centric Analysis of Long-Context Methods
**Authors**: Yucheng Li, Huiqiang Jiang, Qianhui Wu, Xufang Luo, Surin Ahn, Chengruidong Zhang, Amir H. Abdi, Dongsheng Li, Jianfeng Gao, Yuqing Yang, Lili Qiu

**Updated**: 2025-03-11T14:02:04Z

**Summary**: Long-context LLMs have enabled numerous downstream applications but also introduced significant challenges related to computational and memory efficiency. To address these challenges, optimizations for long-context inference have been developed, centered around the KV cache. However, existing benchmarks often evaluate in single-request, neglecting the full lifecycle of the KV cache in real-world use. This oversight is particularly critical, as KV cache reuse has become widely adopted in LLMs inference frameworks, such as vLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft, Google, and Anthropic. To address this gap, we introduce SCBench(SharedContextBench), a comprehensive benchmark for evaluating long-context methods from a KV cachecentric perspective: 1) KV cache generation, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache loading. Specifically, SCBench uses test examples with shared context, ranging 12 tasks with two shared context modes, covering four categories of long-context capabilities: string retrieval, semantic retrieval, global information, and multi-task. With it, we provide an extensive KV cache-centric analysis of eight categories long-context solutions, including Gated Linear RNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention, KV cache dropping, quantization, retrieval, loading, and prompt compression. The evaluation is conducted on 8 long-context LLMs. Our findings show that sub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding with O(n) memory and sub-O(n^2) pre-filling computation perform robustly. Dynamic sparsity yields more expressive KV caches than static patterns, and layer-level sparsity in hybrid architectures reduces memory usage with strong performance. Additionally, we identify attention distribution shift issues in long-generation scenarios. https://aka.ms/SCBench.

**Link**: [arxiv](http://arxiv.org/abs/2412.10319v2),  [pdf](http://arxiv.org/pdf/2412.10319v2)

**Tags**: cs.CL cs.LG 



### Coherent Video Inpainting Using Optical Flow-Guided Efficient Diffusion
**Authors**: Bohai Gu, Hao Luo, Song Guo, Peiran Dong, Qihua Zhou

**Updated**: 2025-03-11T13:13:11Z

**Summary**: The text-guided video inpainting technique has significantly improved the performance of content generation applications. A recent family for these improvements uses diffusion models, which have become essential for achieving high-quality video inpainting results, yet they still face performance bottlenecks in temporal consistency and computational efficiency. This motivates us to propose a new video inpainting framework using optical Flow-guided Efficient Diffusion (FloED) for higher video coherence. Specifically, FloED employs a dual-branch architecture, where the time-agnostic flow branch restores corrupted flow first, and the multi-scale flow adapters provide motion guidance to the main inpainting branch. Besides, a training-free latent interpolation method is proposed to accelerate the multi-step denoising process using flow warping. With the flow attention cache mechanism, FLoED efficiently reduces the computational cost of incorporating optical flow. Extensive experiments on background restoration and object removal tasks show that FloED outperforms state-of-the-art diffusion-based methods in both quality and efficiency. Our codes and models will be made publicly available.

**Link**: [arxiv](http://arxiv.org/abs/2412.00857v3),  [pdf](http://arxiv.org/pdf/2412.00857v3)

**Tags**: cs.CV 



### Breaking the Low-Rank Dilemma of Linear Attention
**Authors**: Qihang Fan, Huaibo Huang, Ran He

**Updated**: 2025-03-11T09:17:02Z

**Summary**: The Softmax attention mechanism in Transformer models is notoriously computationally expensive, particularly due to its quadratic complexity, posing significant challenges in vision applications. In contrast, linear attention provides a far more efficient solution by reducing the complexity to linear levels. However, compared to Softmax attention, linear attention often experiences significant performance degradation. Our experiments indicate that this performance drop is due to the low-rank nature of linear attention's feature map, which hinders its ability to adequately model complex spatial information. In this paper, to break the low-rank dilemma of linear attention, we conduct rank analysis from two perspectives: the KV buffer and the output features. Consequently, we introduce Rank-Augmented Linear Attention (RALA), which rivals the performance of Softmax attention while maintaining linear complexity and high efficiency. Based on RALA, we construct the Rank-Augmented Vision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT achieves excellent performance across various vision tasks. Specifically, without using any additional labels, data, or supervision during training, RAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters and 4.6G FLOPs. This result significantly surpasses previous linear attention mechanisms, fully illustrating the potential of RALA. Code will be available at https://github.com/qhfan/RALA.

**Link**: [arxiv](http://arxiv.org/abs/2411.07635v5),  [pdf](http://arxiv.org/pdf/2411.07635v5)

**Tags**: cs.CV 



### Optimization and Benchmarking of Monolithically Stackable Gain Cell   Memory for Last-Level Cache
**Authors**: Faaiq Waqar, Jungyoun Kwak, Junmo Lee, Minji Shon, Mohammadhosein Gholamrezaei, Kevin Skadron, Shimeng Yu

**Updated**: 2025-03-11T03:26:20Z

**Summary**: The Last Level Cache (LLC) is the processor's critical bridge between on-chip and off-chip memory levels - optimized for high density, high bandwidth, and low operation energy. To date, high-density (HD) SRAM has been the conventional device of choice; however, with the slowing of transistor scaling, as reflected in the industry's almost identical HD SRAM cell size from 5 nm to 3 nm, alternative solutions such as 3D stacking with advanced packaging like hybrid bonding are pursued (as demonstrated in AMD's V-cache). Escalating data demands necessitate ultra-large on-chip caches to decrease costly off-chip memory movement, pushing the exploration of device technology toward monolithic 3D (M3D) integration where transistors can be stacked in the back-end-of-line (BEOL) at the interconnect level. M3D integration requires fabrication techniques compatible with a low thermal budget (<400 degC). Among promising BEOL device candidates are amorphous oxide semiconductor (AOS) transistors, particularly desirable for their ultra-low leakage (<fA/um), enabling persistent data retention (>seconds) when used in a gain-cell configuration. This paper examines device, circuit, and system-level tradeoffs when optimizing BEOL-compatible AOS-based 2-transistor gain cell (2T-GC) for LLC. A cache early-exploration tool, NS-Cache, is developed to model caches in advanced 7 and 3 nm nodes and is integrated with the Gem5 simulator to systematically benchmark the impact of the newfound density/performance when compared to HD-SRAM, MRAM, and 1T1C eDRAM alternatives for LLC.

**Link**: [arxiv](http://arxiv.org/abs/2503.06304v2),  [pdf](http://arxiv.org/pdf/2503.06304v2)

**Tags**: cs.ET B.8.2; B.3.1 



### Queueing, Predictions, and LLMs: Challenges and Open Problems
**Authors**: Michael Mitzenmacher, Rana Shahout

**Updated**: 2025-03-10T17:12:47Z

**Summary**: Queueing systems present many opportunities for applying machine-learning predictions, such as estimated service times, to improve system performance. This integration raises numerous open questions about how predictions can be effectively leveraged to improve scheduling decisions. Recent studies explore queues with predicted service times, typically aiming to minimize job time in the system. We review these works, highlight the effectiveness of predictions, and present open questions on queue performance. We then move to consider an important practical example of using predictions in scheduling, namely Large Language Model (LLM) systems, which presents novel scheduling challenges and highlights the potential for predictions to improve performance. In particular, we consider LLMs performing inference. Inference requests (jobs) in LLM systems are inherently complex; they have variable inference times, dynamic memory footprints that are constrained by key-value (KV) store memory limitations, and multiple possible preemption approaches that affect performance differently. We provide background on the important aspects of scheduling in LLM systems, and introduce new models and open problems that arise from them. We argue that there are significant opportunities for applying insights and analysis from queueing theory to scheduling in LLM systems.

**Link**: [arxiv](http://arxiv.org/abs/2503.07545v1),  [pdf](http://arxiv.org/pdf/2503.07545v1)

**Tags**: cs.AI cs.DS 



### TokenButler: Token Importance is Predictable
**Authors**: Yash Akhauri, Ahmed F AbouElhamayed, Yifei Gao, Chi-Chih Chang, Nilesh Jain, Mohamed S. Abdelfattah

**Updated**: 2025-03-10T16:41:14Z

**Summary**: Large Language Models (LLMs) rely on the Key-Value (KV) Cache to store token history, enabling efficient decoding of tokens. As the KV-Cache grows, it becomes a major memory and computation bottleneck, however, there is an opportunity to alleviate this bottleneck, especially because prior research has shown that only a small subset of tokens contribute meaningfully to each decoding step. A key challenge in finding these critical tokens is that they are dynamic, and heavily input query-dependent. Existing methods either risk quality by evicting tokens permanently, or retain the full KV-Cache but rely on retrieving chunks (pages) of tokens at generation, failing at dense, context-rich tasks. Additionally, many existing KV-Cache sparsity methods rely on inaccurate proxies for token importance. To address these limitations, we introduce TokenButler, a high-granularity, query-aware predictor that learns to identify these critical tokens. By training a light-weight predictor with less than 1.2% parameter overhead, TokenButler prioritizes tokens based on their contextual, predicted importance. This improves perplexity & downstream accuracy by over 8% relative to SoTA methods for estimating token importance. We evaluate TokenButler on a novel synthetic small-context co-referential retrieval task, demonstrating near-oracle accuracy. Code, models and benchmarks: https://github.com/abdelfattah-lab/TokenButler

**Link**: [arxiv](http://arxiv.org/abs/2503.07518v1),  [pdf](http://arxiv.org/pdf/2503.07518v1)

**Tags**: cs.CL cs.AI cs.LG 



### Revealing Rotational Symmetry Breaking Charge-density Wave Order in   Kagome Superconductor (Rb, K)V$_3$Sb$_5$ by Ultrafast Pump-probe Experiments
**Authors**: Qinwen Deng, Hengxin Tan, Brenden R. Ortiz, Stephen D. Wilson, Binghai Yan, Liang Wu

**Updated**: 2025-03-10T15:49:20Z

**Summary**: The recently discovered Kagome superconductor AV$_3$Sb$_5$ (where A refers to K, Rb, Cs) has stimulated widespread research interest due to its interplay of non-trivial topology and unconventional correlated physics including charge-density waves (CDW) and superconductivity. The essential prerequisite to understanding the microscopic mechanisms of this complex electronic landscape is to unveil the configuration and symmetry of the charge-density wave order. As to now, little consensus has been made on what symmetry is broken. Herein, we clarify the microscopic structure and symmetry breaking of the CDW phase in RbV$_3$Sb$_5$ and KV$_3$Sb$_5$ by ultrafast time-resolved reflectivity. Our approach is based on extracting coherent phonon spectra induced by three-dimensional CDW and comparing them to calculated phonon frequencies via density-functional theory. The combination of these experimental results and calculations provides compelling evidence that the CDW structure of both compounds prevailing up to T$_{\text{CDW}}$ is the 2 $\times$ 2 $\times$ 2 staggered inverse Star-of-David pattern with interlayer $\pi$ phase shift, in which the six-fold rotational symmetry is broken. These observations thus corroborate six-fold rotational symmetry breaking throughout the CDW phase of RbV$_3$Sb$_5$ and KV$_3$Sb$_5$.

**Link**: [arxiv](http://arxiv.org/abs/2503.07474v1),  [pdf](http://arxiv.org/pdf/2503.07474v1)

**Tags**: cond-mat.str-el cond-mat.mtrl-sci 



### Modeling and Simulating Emerging Memory Technologies: A Tutorial
**Authors**: Yun-Chih Chen, Tristan Seidl, Nils Hölscher, Christian Hakert, Minh Duy Truong, Jian-Jia Chen, João Paulo C. de Lima, Asif Ali Khan, Jeronimo Castrillon, Ali Nezhadi, Lokesh Siddhu, Hassan Nassar, Mahta Mayahinia, Mehdi Baradaran Tahoori, Jörg Henkel, Nils Wilbert, Stefan Wildermann, Jürgen Teich

**Updated**: 2025-03-10T12:10:30Z

**Summary**: Non-volatile Memory (NVM) technologies present a promising alternative to traditional volatile memories such as SRAM and DRAM. Due to the limited availability of real NVM devices, simulators play a crucial role in architectural exploration and hardware-software co-design. This tutorial presents a simulation toolchain through four detailed case studies, showcasing its applicability to various domains of system design, including hybrid main-memory and cache, compute-in-memory, and wear-leveling design. These case studies provide the reader with practical insights on customizing the toolchain for their specific research needs. The source code is open-sourced.

**Link**: [arxiv](http://arxiv.org/abs/2502.10167v2),  [pdf](http://arxiv.org/pdf/2502.10167v2)

**Tags**: cs.AR 



### Exposure Bias Reduction for Enhancing Diffusion Transformer Feature   Caching
**Authors**: Zhen Zou, Hu Yu, Jie Xiao, Feng Zhao

**Updated**: 2025-03-10T09:49:18Z

**Summary**: Diffusion Transformer (DiT) has exhibited impressive generation capabilities but faces great challenges due to its high computational complexity. To address this problem, various methods, notably feature caching, have been introduced. However, these approaches focus on aligning non-cache diffusion without analyzing the impact of caching on the generation of intermediate processes. So the lack of exploration provides us with room for analysis and improvement. In this paper, we analyze the impact of caching on the SNR of the diffusion process and discern that feature caching intensifies the denoising procedure, and we further identify this as a more severe exposure bias issue. Drawing on this insight, we introduce EB-Cache, a joint cache strategy that aligns the Non-exposure bias (which gives us a higher performance ceiling) diffusion process. Our approach incorporates a comprehensive understanding of caching mechanisms and offers a novel perspective on leveraging caches to expedite diffusion processes. Empirical results indicate that EB-Cache optimizes model performance while concurrently facilitating acceleration. Specifically, in the 50-step generation process, EB-Cache achieves 1.49$\times$ acceleration with 0.63 FID reduction from 3.69, surpassing prior acceleration methods. Code will be available at \href{https://github.com/aSleepyTree/EB-Cache}{https://github.com/aSleepyTree/EB-Cache}.

**Link**: [arxiv](http://arxiv.org/abs/2503.07120v1),  [pdf](http://arxiv.org/pdf/2503.07120v1)

**Tags**: cs.CV cs.LG 



### EasyControl: Adding Efficient and Flexible Control for Diffusion   Transformer
**Authors**: Yuxuan Zhang, Yirui Yuan, Yiren Song, Haofan Wang, Jiaming Liu

**Updated**: 2025-03-10T08:07:17Z

**Summary**: Recent advancements in Unet-based diffusion models, such as ControlNet and IP-Adapter, have introduced effective spatial and subject control mechanisms. However, the DiT (Diffusion Transformer) architecture still struggles with efficient and flexible control. To tackle this issue, we propose EasyControl, a novel framework designed to unify condition-guided diffusion transformers with high efficiency and flexibility. Our framework is built on three key innovations. First, we introduce a lightweight Condition Injection LoRA Module. This module processes conditional signals in isolation, acting as a plug-and-play solution. It avoids modifying the base model weights, ensuring compatibility with customized models and enabling the flexible injection of diverse conditions. Notably, this module also supports harmonious and robust zero-shot multi-condition generalization, even when trained only on single-condition data. Second, we propose a Position-Aware Training Paradigm. This approach standardizes input conditions to fixed resolutions, allowing the generation of images with arbitrary aspect ratios and flexible resolutions. At the same time, it optimizes computational efficiency, making the framework more practical for real-world applications. Third, we develop a Causal Attention Mechanism combined with the KV Cache technique, adapted for conditional generation tasks. This innovation significantly reduces the latency of image synthesis, improving the overall efficiency of the framework. Through extensive experiments, we demonstrate that EasyControl achieves exceptional performance across various application scenarios. These innovations collectively make our framework highly efficient, flexible, and suitable for a wide range of tasks.

**Link**: [arxiv](http://arxiv.org/abs/2503.07027v1),  [pdf](http://arxiv.org/pdf/2503.07027v1)

**Tags**: cs.CV 



### From Reusing to Forecasting: Accelerating Diffusion Models with   TaylorSeers
**Authors**: Jiacheng Liu, Chang Zou, Yuanhuiyi Lyu, Junjie Chen, Linfeng Zhang

**Updated**: 2025-03-10T05:09:42Z

**Summary**: Diffusion Transformers (DiT) have revolutionized high-fidelity image and video synthesis, yet their computational demands remain prohibitive for real-time applications. To solve this problem, feature caching has been proposed to accelerate diffusion models by caching the features in the previous timesteps and then reusing them in the following timesteps. However, at timesteps with significant intervals, the feature similarity in diffusion models decreases substantially, leading to a pronounced increase in errors introduced by feature caching, significantly harming the generation quality. To solve this problem, we propose TaylorSeer, which firstly shows that features of diffusion models at future timesteps can be predicted based on their values at previous timesteps. Based on the fact that features change slowly and continuously across timesteps, TaylorSeer employs a differential method to approximate the higher-order derivatives of features and predict features in future timesteps with Taylor series expansion. Extensive experiments demonstrate its significant effectiveness in both image and video synthesis, especially in high acceleration ratios. For instance, it achieves an almost lossless acceleration of 4.99$\times$ on FLUX and 5.00$\times$ on HunyuanVideo without additional training. On DiT, it achieves $3.41$ lower FID compared with previous SOTA at $4.53$$\times$ acceleration. %Our code is provided in the supplementary materials and will be made publicly available on GitHub. Our codes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer

**Link**: [arxiv](http://arxiv.org/abs/2503.06923v1),  [pdf](http://arxiv.org/pdf/2503.06923v1)

**Tags**: cs.CV cs.AI 



### Piccolo: Large-Scale Graph Processing with Fine-Grained In-Memory   Scatter-Gather
**Authors**: Changmin Shin, Jaeyong Song, Hongsun Jang, Dogeun Kim, Jun Sung, Taehee Kwon, Jae Hyung Ju, Frank Liu, Yeonkyu Choi, Jinho Lee

**Updated**: 2025-03-10T02:41:21Z

**Summary**: Graph processing requires irregular, fine-grained random access patterns incompatible with contemporary off-chip memory architecture, leading to inefficient data access. This inefficiency makes graph processing an extremely memory-bound application. Because of this, existing graph processing accelerators typically employ a graph tiling-based or processing-in-memory (PIM) approach to relieve the memory bottleneck. In the tiling-based approach, a graph is split into chunks that fit within the on-chip cache to maximize data reuse. In the PIM approach, arithmetic units are placed within memory to perform operations such as reduction or atomic addition. However, both approaches have several limitations, especially when implemented on current memory standards (i.e., DDR). Because the access granularity provided by DDR is much larger than that of the graph vertex property data, much of the bandwidth and cache capacity are wasted. PIM is meant to alleviate such issues, but it is difficult to use in conjunction with the tiling-based approach, resulting in a significant disadvantage. Furthermore, placing arithmetic units inside a memory chip is expensive, thereby supporting multiple types of operation is thought to be impractical. To address the above limitations, we present Piccolo, an end-to-end efficient graph processing accelerator with fine-grained in-memory random scatter-gather. Instead of placing expensive arithmetic units in off-chip memory, Piccolo focuses on reducing the off-chip traffic with non-arithmetic function-in-memory of random scatter-gather. To fully benefit from in-memory scatter-gather, Piccolo redesigns the cache and MHA of the accelerator such that it can enjoy both the advantage of tiling and in-memory operations. Piccolo achieves a maximum speedup of 3.28$\times$ and a geometric mean speedup of 1.62$\times$ across various and extensive benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2503.05116v2),  [pdf](http://arxiv.org/pdf/2503.05116v2)

**Tags**: cs.AR 



### Temporal Feature Matters: A Framework for Diffusion Model Quantization
**Authors**: Yushi Huang, Ruihao Gong, Xianglong Liu, Jing Liu, Yuhang Li, Jiwen Lu, Dacheng Tao

**Updated**: 2025-03-09T17:43:28Z

**Summary**: The Diffusion models, widely used for image generation, face significant challenges related to their broad applicability due to prolonged inference times and high memory demands. Efficient Post-Training Quantization (PTQ) is crucial to address these issues. However, unlike traditional models, diffusion models critically rely on the time-step for the multi-round denoising. Typically, each time-step is encoded into a hypersensitive temporal feature by several modules. Despite this, existing PTQ methods do not optimize these modules individually. Instead, they employ unsuitable reconstruction objectives and complex calibration methods, leading to significant disturbances in the temporal feature and denoising trajectory, as well as reduced compression efficiency. To address these challenges, we introduce a novel quantization framework that includes three strategies: 1) TIB-based Maintenance: Based on our innovative Temporal Information Block (TIB) definition, Temporal Information-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are developed to efficiently align original temporal features. 2) Cache-based Maintenance: Instead of indirect and complex optimization for the related modules, pre-computing and caching quantized counterparts of temporal features are developed to minimize errors. 3) Disturbance-aware Selection: Employ temporal feature errors to guide a fine-grained selection between the two maintenance strategies for further disturbance reduction. This framework preserves most of the temporal information and ensures high-quality end-to-end generation. Extensive testing on various datasets, diffusion models and hardware confirms our superior performance and acceleration.

**Link**: [arxiv](http://arxiv.org/abs/2407.19547v3),  [pdf](http://arxiv.org/pdf/2407.19547v3)

**Tags**: cs.CV 



### AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric   Reduction and Restoration
**Authors**: Wenhao Sun, Rong-Cheng Tu, Jingyi Liao, Zhao Jin, Dacheng Tao

**Updated**: 2025-03-09T16:14:51Z

**Summary**: Diffusion Transformers (DiTs) have proven effective in generating high-quality videos but are hindered by high computational costs. Existing video DiT sampling acceleration methods often rely on costly fine-tuning or exhibit limited generalization capabilities. We propose Asymmetric Reduction and Restoration (AsymRnR), a training-free and model-agnostic method to accelerate video DiTs. It builds on the observation that redundancies of feature tokens in DiTs vary significantly across different model blocks, denoising steps, and feature types. Our AsymRnR asymmetrically reduces redundant tokens in the attention operation, achieving acceleration with negligible degradation in output quality and, in some cases, even improving it. We also tailored a reduction schedule to distribute the reduction across components adaptively. To further accelerate this process, we introduce a matching cache for more efficient reduction. Backed by theoretical foundations and extensive experimental validation, AsymRnR integrates into state-of-the-art video DiTs and offers substantial speedup.

**Link**: [arxiv](http://arxiv.org/abs/2412.11706v2),  [pdf](http://arxiv.org/pdf/2412.11706v2)

**Tags**: cs.CV 



### Beyond Decoder-only: Large Language Models Can be Good Encoders for   Machine Translation
**Authors**: Yingfeng Luo, Tong Zheng, Yongyu Mu, Bei Li, Qinghong Zhang, Yongqi Gao, Ziqiang Xu, Peinan Feng, Xiaoqian Liu, Tong Xiao, Jingbo Zhu

**Updated**: 2025-03-09T12:54:05Z

**Summary**: The field of neural machine translation (NMT) has changed with the advent of large language models (LLMs). Much of the recent emphasis in natural language processing (NLP) has been on modeling machine translation and many other problems using a single pre-trained Transformer decoder, while encoder-decoder architectures, which were the standard in earlier NMT models, have received relatively less attention. In this paper, we explore translation models that are universal, efficient, and easy to optimize, by marrying the world of LLMs with the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder unchanged. We also develop methods for adapting LLMs to work better with the NMT decoder. Furthermore, we construct a new dataset involving multiple tasks to assess how well the machine translation system generalizes across various tasks. Evaluations on the WMT and our datasets show that results using our method match or surpass a range of baselines in terms of translation quality, but achieve $2.4 \sim 6.5 \times$ inference speedups and a $75\%$ reduction in the memory footprint of the KV cache. It also demonstrates strong generalization across a variety of translation-related tasks.

**Link**: [arxiv](http://arxiv.org/abs/2503.06594v1),  [pdf](http://arxiv.org/pdf/2503.06594v1)

**Tags**: cs.CL 



### QuantCache: Adaptive Importance-Guided Quantization with Hierarchical   Latent and Layer Caching for Video Generation
**Authors**: Junyi Wu, Zhiteng Li, Zheng Hui, Yulun Zhang, Linghe Kong, Xiaokang Yang

**Updated**: 2025-03-09T10:31:51Z

**Summary**: Recently, Diffusion Transformers (DiTs) have emerged as a dominant architecture in video generation, surpassing U-Net-based models in terms of performance. However, the enhanced capabilities of DiTs come with significant drawbacks, including increased computational and memory costs, which hinder their deployment on resource-constrained devices. Current acceleration techniques, such as quantization and cache mechanism, offer limited speedup and are often applied in isolation, failing to fully address the complexities of DiT architectures. In this paper, we propose QuantCache, a novel training-free inference acceleration framework that jointly optimizes hierarchical latent caching, adaptive importance-guided quantization, and structural redundancy-aware pruning. QuantCache achieves an end-to-end latency speedup of 6.72$\times$ on Open-Sora with minimal loss in generation quality. Extensive experiments across multiple video generation benchmarks demonstrate the effectiveness of our method, setting a new standard for efficient DiT inference. The code and models will be available at https://github.com/JunyiWuCode/QuantCache.

**Link**: [arxiv](http://arxiv.org/abs/2503.06545v1),  [pdf](http://arxiv.org/pdf/2503.06545v1)

**Tags**: cs.CV 



### Seesaw: High-throughput LLM Inference via Model Re-sharding
**Authors**: Qidong Su, Wei Zhao, Xin Li, Muralidhar Andoorveedu, Chenhao Jiang, Zhanda Zhu, Kevin Song, Christina Giannoula, Gennady Pekhimenko

**Updated**: 2025-03-09T04:14:06Z

**Summary**: To improve the efficiency of distributed large language model (LLM) inference, various parallelization strategies, such as tensor and pipeline parallelism, have been proposed. However, the distinct computational characteristics inherent in the two stages of LLM inference-prefilling and decoding-render a single static parallelization strategy insufficient for the effective optimization of both stages. In this work, we present Seesaw, an LLM inference engine optimized for throughput-oriented tasks. The key idea behind Seesaw is dynamic model re-sharding, a technique that facilitates the dynamic reconfiguration of parallelization strategies across stages, thereby maximizing throughput at both phases. To mitigate re-sharding overhead and optimize computational efficiency, we employ tiered KV cache buffering and transition-minimizing scheduling. These approaches work synergistically to reduce the overhead caused by frequent stage transitions while ensuring maximum batching efficiency. Our evaluation demonstrates that Seesaw achieves a throughput increase of up to 1.78x (1.36x on average) compared to vLLM, the most widely used state-of-the-art LLM inference engine.

**Link**: [arxiv](http://arxiv.org/abs/2503.06433v1),  [pdf](http://arxiv.org/pdf/2503.06433v1)

**Tags**: cs.DC cs.AI 



### Learning Mamba as a Continual Learner: Meta-learning Selective State   Space Models for Efficient Continual Learning
**Authors**: Chongyang Zhao, Dong Gong

**Updated**: 2025-03-09T02:19:22Z

**Summary**: Continual learning (CL) aims to efficiently learn from a non-stationary data stream, without storing or recomputing all seen samples. CL enables prediction on new tasks by incorporating sequential training samples. Building on this connection between CL and sequential modeling, meta-continual learning (MCL) aims to meta-learn an efficient continual learner as a sequence prediction model, with advanced sequence models like Transformers being natural choices. However, despite decent performance, Transformers rely on a linearly growing cache to store all past representations, conflicting with CL's objective of not storing all seen samples and limiting efficiency. In this paper, we focus on meta-learning sequence-prediction-based continual learners without retaining all past representations. While attention-free models with fixed-size hidden states (e.g., Linear Transformers) align with CL's essential goal and efficiency needs, they have shown limited effectiveness in MCL in previous literature. Given Mamba's strong sequence modeling performance and attention-free nature, we explore a key question: Can attention-free models like Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks, we propose MambaCL, a meta-learned continual learner. To enhance MambaCL's training, we introduce selectivity regularization, leveraging the connection between Mamba and Transformers to guide its behavior over sequences. Furthermore, we study how Mamba and other models perform across various MCL scenarios through extensive and well-designed experiments. Our results highlight the promising performance and strong generalization of Mamba and attention-free models in MCL, demonstrating its potential for efficient continual learning and adaptation.

**Link**: [arxiv](http://arxiv.org/abs/2412.00776v3),  [pdf](http://arxiv.org/pdf/2412.00776v3)

**Tags**: cs.LG 



### Decentralized Learning Strategies for Estimation Error Minimization with   Graph Neural Networks
**Authors**: Xingran Chen, Navid NaderiAlizadeh, Alejandro Ribeiro, Shirin Saeedi Bidokhti

**Updated**: 2025-03-08T21:55:15Z

**Summary**: We address the challenge of sampling and remote estimation for autoregressive Markovian processes in a multi-hop wireless network with statistically-identical agents. Agents cache the most recent samples from others and communicate over wireless collision channels governed by an underlying graph topology. Our goal is to minimize time-average estimation error and/or age of information with decentralized scalable sampling and transmission policies, considering both oblivious (where decision-making is independent of the physical processes) and non-oblivious policies (where decision-making depends on physical processes). We prove that in oblivious policies, minimizing estimation error is equivalent to minimizing the age of information. The complexity of the problem, especially the multi-dimensional action spaces and arbitrary network topologies, makes theoretical methods for finding optimal transmission policies intractable. We optimize the policies using a graphical multi-agent reinforcement learning framework, where each agent employs a permutation-equivariant graph neural network architecture. Theoretically, we prove that our proposed framework exhibits desirable transferability properties, allowing transmission policies trained on small- or moderate-size networks to be executed effectively on large-scale topologies. Numerical experiments demonstrate that (i) Our proposed framework outperforms state-of-the-art baselines; (ii) The trained policies are transferable to larger networks, and their performance gains increase with the number of agents; (iii) The training procedure withstands non-stationarity even if we utilize independent learning techniques; and, (iv) Recurrence is pivotal in both independent learning and centralized training and decentralized execution, and improves the resilience to non-stationarity in independent learning.

**Link**: [arxiv](http://arxiv.org/abs/2404.03227v2),  [pdf](http://arxiv.org/pdf/2404.03227v2)

**Tags**: eess.SP cs.LG 



### Synergizing AI and Digital Twins for Next-Generation Network   Optimization, Forecasting, and Security
**Authors**: Zifan Zhang, Minghong Fang, Dianwei Chen, Xianfeng Yang, Yuchen Liu

**Updated**: 2025-03-08T18:30:54Z

**Summary**: Digital network twins (DNTs) are virtual representations of physical networks, designed to enable real-time monitoring, simulation, and optimization of network performance. When integrated with machine learning (ML) techniques, particularly federated learning (FL) and reinforcement learning (RL), DNTs emerge as powerful solutions for managing the complexities of network operations. This article presents a comprehensive analysis of the synergy of DNTs, FL, and RL techniques, showcasing their collective potential to address critical challenges in 6G networks. We highlight key technical challenges that need to be addressed, such as ensuring network reliability, achieving joint data-scenario forecasting, and maintaining security in high-risk environments. Additionally, we propose several pipelines that integrate DNT and ML within coherent frameworks to enhance network optimization and security. Case studies demonstrate the practical applications of our proposed pipelines in edge caching and vehicular networks. In edge caching, the pipeline achieves over 80% cache hit rates while balancing base station loads. In autonomous vehicular system, it ensure a 100% no-collision rate, showcasing its reliability in safety-critical scenarios. By exploring these synergies, we offer insights into the future of intelligent and adaptive network systems that automate decision-making and problem-solving.

**Link**: [arxiv](http://arxiv.org/abs/2503.06302v1),  [pdf](http://arxiv.org/pdf/2503.06302v1)

**Tags**: cs.NI cs.AI cs.LG 



### Rethinking Video Tokenization: A Conditioned Diffusion-based Approach
**Authors**: Nianzu Yang, Pandeng Li, Liming Zhao, Yang Li, Chen-Wei Xie, Yehui Tang, Xudong Lu, Zhihang Liu, Yun Zheng, Yu Liu, Junchi Yan

**Updated**: 2025-03-08T14:48:15Z

**Summary**: Existing video tokenizers typically use the traditional Variational Autoencoder (VAE) architecture for video compression and reconstruction. However, to achieve good performance, its training process often relies on complex multi-stage training tricks that go beyond basic reconstruction loss and KL regularization. Among these tricks, the most challenging is the precise tuning of adversarial training with additional Generative Adversarial Networks (GANs) in the final stage, which can hinder stable convergence. In contrast to GANs, diffusion models offer more stable training processes and can generate higher-quality results. Inspired by these advantages, we propose CDT, a novel Conditioned Diffusion-based video Tokenizer, that replaces the GAN-based decoder with a conditional causal diffusion model. The encoder compresses spatio-temporal information into compact latents, while the decoder reconstructs videos through a reverse diffusion process conditioned on these latents. During inference, we incorporate a feature cache mechanism to generate videos of arbitrary length while maintaining temporal continuity and adopt sampling acceleration technique to enhance efficiency. Trained using only a basic MSE diffusion loss for reconstruction, along with KL term and LPIPS perceptual loss from scratch, extensive experiments demonstrate that CDT achieves state-of-the-art performance in video reconstruction tasks with just a single-step sampling. Even a scaled-down version of CDT (3$\times$ inference speedup) still performs comparably with top baselines. Moreover, the latent video generation model trained with CDT also exhibits superior performance. The source code and pretrained weights will be released shortly, so please stay tuned for updates!

**Link**: [arxiv](http://arxiv.org/abs/2503.03708v2),  [pdf](http://arxiv.org/pdf/2503.03708v2)

**Tags**: cs.CV cs.AI 



### ML-based Adaptive Prefetching and Data Placement for US HEP Systems
**Authors**: Venkat Sai Suman Lamba Karanam, Sarat Sasank Barla, Byrav Ramamurthy, Derek Weitzel

**Updated**: 2025-03-08T02:35:16Z

**Summary**: Although benefits from caching in US HEP are well-known, current caching strategies are not adaptive i.e. they do not adapt to changing cache access patterns. Newer developments such as High Luminosity - Large Hadron Collider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move toward streaming readout based Data Acquisition systems (DAQs) will increase the data production exponentially and hence burden the storage, compute \& network infrastructures. Moreover, existing caching frameworks are optimized to reduce latency, but not optimized for storage. This in combination with limited cache capacities relative to total data makes it difficult to achieve data locality.   In this work, we present Machine Learning-aided (ML) caching strategies. Specifically, first we present a Long Short-Term Memory-based (LSTM) hourly cache usage prediction. Second, we present an hourly file-level access prediction model based on CatboostRegressor. To date, most ML-based cache prediction strategies in HEP have focused on daily cache usage and limited works tackled hourly cache usage and even less strategies addressed hourly file-level access prediction. File-level access prediction allows for the design of intelligent prefetching and data placement strategies with fine-grained control. We validated our cache prediction strategies using data collected from SoCal MINI caches in August 2024. We are currently extending WRENCH simulator to reflect the US HEP ecosystem at the storage, network and compute levels. We plan to deploy our cache prediction strategies into WRENCH and later perform extensive analysis with complex data access patterns and candidate infrastructure configurations.

**Link**: [arxiv](http://arxiv.org/abs/2503.06015v1),  [pdf](http://arxiv.org/pdf/2503.06015v1)

**Tags**: cs.DC 



### Choosing Augmentation Parameters in OSQP- A New Approach based on   Conjugate Directions
**Authors**: Avinash Kumar

**Updated**: 2025-03-07T21:16:41Z

**Summary**: This work proposes a new method to select the augmentation parameters in the operator splitting quadratic program (OSQP) algorithm so as to reduce the computation time of overall algorithm. The selection is based upon the information of conjugate directions of the coefficient matrix of a linear system of equations present in the algorithm. This selection makes it possible to cache these conjugate directions, instead of computing them at each iteration, resulting in faster computation of the solution of the linear system thus reducing the overall computation time. This reduction is demonstrated by a numerical example.

**Link**: [arxiv](http://arxiv.org/abs/2503.05941v1),  [pdf](http://arxiv.org/pdf/2503.05941v1)

**Tags**: math.OC 



### Simple linear attention language models balance the recall-throughput   tradeoff
**Authors**: Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, Christopher Ré

**Updated**: 2025-03-07T18:57:52Z

**Summary**: Recent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff curve, recovering the full quality of attention on one end and the small state size of attention-alternatives on the other. We train language models up to 1.3b parameters and show that BASED matches the strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points. Implementations of linear attention are often less efficient than optimized standard attention implementations. To make BASED competitive, we develop IO-aware algorithms that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameter models. Code for this work is provided at: https://github.com/HazyResearch/based.

**Link**: [arxiv](http://arxiv.org/abs/2402.18668v2),  [pdf](http://arxiv.org/pdf/2402.18668v2)

**Tags**: cs.CL cs.LG 



### DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured   LLM Inference
**Authors**: Jinwei Yao, Kaiqi Chen, Kexun Zhang, Jiaxuan You, Binhang Yuan, Zeke Wang, Tao Lin

**Updated**: 2025-03-07T17:47:42Z

**Summary**: Large language models (LLMs) are increasingly employed for complex tasks that process multiple generation calls in a tree structure with shared prefixes of tokens, including few-shot prompting, multi-step reasoning, speculative decoding, etc. However, existing inference systems for tree-based applications are inefficient due to improper partitioning of queries and KV cache during attention calculation. This leads to two main issues: (1) a lack of memory access (IO) reuse for KV cache of shared prefixes, and (2) poor load balancing.As a result, there is redundant KV cache IO between GPU global memory and shared memory, along with low GPU utilization. To address these challenges, we propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient attention algorithm with prefix-aware and load-balanced KV cache partitions. DeFT reduces the number of read/write operations of KV cache during attention calculation through KV-Guided Grouping, a method that avoids repeatedly loading KV cache of shared prefixes in attention computation. Additionally, we propose Flattened Tree KV Splitting, a mechanism that ensures even distribution of the KV cache across partitions with little computation redundancy, enhancing GPU utilization during attention computations. By reducing 73-99% KV cache IO and nearly 100% IO for partial results during attention calculation, DeFT achieves up to 2.23/3.59x speedup in the end-to-end/attention latency across three practical tree-based workloads compared to state-of-the-art attention algorithms. Our code is available at https://github.com/LINs-lab/DeFT.

**Link**: [arxiv](http://arxiv.org/abs/2404.00242v4),  [pdf](http://arxiv.org/pdf/2404.00242v4)

**Tags**: cs.CL cs.AI 



### Leveraging Approximate Caching for Faster Retrieval-Augmented Generation
**Authors**: Shai Bergman, Zhang Ji, Anne-Marie Kermarrec, Diana Petrescu, Rafael Pires, Mathis Randl, Martijn de Vos

**Updated**: 2025-03-07T15:54:04Z

**Summary**: Retrieval-augmented generation (RAG) enhances the reliability of large language model (LLM) answers by integrating external knowledge. However, RAG increases the end-to-end inference time since looking for relevant documents from large vector databases is computationally expensive. To address this, we introduce Proximity, an approximate key-value cache that optimizes the RAG workflow by leveraging similarities in user queries. Instead of treating each query independently, Proximity reuses previously retrieved documents when similar queries appear, reducing reliance on expensive vector database lookups. We evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it significantly improves retrieval efficiency while maintaining response accuracy. Proximity reduces retrieval latency by up to 59% while maintaining accuracy and lowers the computational burden on the vector database. We also experiment with different similarity thresholds and quantify the trade-off between speed and recall. Our work shows that approximate caching is a viable and effective strategy for optimizing RAG-based systems.

**Link**: [arxiv](http://arxiv.org/abs/2503.05530v1),  [pdf](http://arxiv.org/pdf/2503.05530v1)

**Tags**: cs.DB cs.LG cs.PF 



### MeanCache: User-Centric Semantic Caching for LLM Web Services
**Authors**: Waris Gill, Mohamed Elidrisi, Pallavi Kalapatapu, Ammar Ahmed, Ali Anwar, Muhammad Ali Gulzar

**Updated**: 2025-03-07T14:49:07Z

**Summary**: Large Language Models (LLMs) like ChatGPT and Llama have revolutionized natural language processing and search engine dynamics. However, these models incur exceptionally high computational costs. For instance, GPT-3 consists of 175 billion parameters, where inference demands billions of floating-point operations. Caching is a natural solution to reduce LLM inference costs on repeated queries, which constitute about 31% of the total queries. However, existing caching methods are incapable of finding semantic similarities among LLM queries nor do they operate on contextual queries, leading to unacceptable false hit-and-miss rates. This paper introduces MeanCache, a user-centric semantic cache for LLM-based services that identifies semantically similar queries to determine cache hit or miss. Using MeanCache, the response to a user's semantically similar query can be retrieved from a local cache rather than re-querying the LLM, thus reducing costs, service provider load, and environmental impact. MeanCache leverages Federated Learning (FL) to collaboratively train a query similarity model without violating user privacy. By placing a local cache in each user's device and using FL, MeanCache reduces the latency and costs and enhances model performance, resulting in lower false hit rates. MeanCache also encodes context chains for every cached query, offering a simple yet highly effective mechanism to discern contextual query responses from standalone. Our experiments benchmarked against the state-of-the-art caching method, reveal that MeanCache attains an approximately 17% higher F-score and a 20% increase in precision during semantic cache hit-and-miss decisions while performing even better on contextual queries. It also reduces the storage requirement by 83% and accelerates semantic cache hit-and-miss decisions by 11%.

**Link**: [arxiv](http://arxiv.org/abs/2403.02694v4),  [pdf](http://arxiv.org/pdf/2403.02694v4)

**Tags**: cs.LG cs.AI cs.CL cs.CR cs.DC I.2.7 



### Accelerating Diffusion Transformer via Gradient-Optimized Cache
**Authors**: Junxiang Qiu, Lin Liu, Shuo Wang, Jinda Lu, Kezhou Chen, Yanbin Hao

**Updated**: 2025-03-07T05:31:47Z

**Summary**: Feature caching has emerged as an effective strategy to accelerate diffusion transformer (DiT) sampling through temporal feature reuse. It is a challenging problem since (1) Progressive error accumulation from cached blocks significantly degrades generation quality, particularly when over 50\% of blocks are cached; (2) Current error compensation approaches neglect dynamic perturbation patterns during the caching process, leading to suboptimal error correction. To solve these problems, we propose the Gradient-Optimized Cache (GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient queue dynamically computes the gradient differences between cached and recomputed features. These gradients are weighted and propagated to subsequent steps, directly compensating for the approximation errors introduced by caching. (2) Inflection-Aware Optimization: Through statistical analysis of feature variation patterns, we identify critical inflection points where the denoising trajectory changes direction. By aligning gradient updates with these detected phases, we prevent conflicting gradient directions during error correction. Extensive evaluations on ImageNet demonstrate GOC's superior trade-off between efficiency and quality. With 50\% cached blocks, GOC achieves IS 216.28 (26.3\% higher) and FID 3.907 (43\% lower) compared to baseline DiT, while maintaining identical computational costs. These improvements persist across various cache ratios, demonstrating robust adaptability to different acceleration requirements.

**Link**: [arxiv](http://arxiv.org/abs/2503.05156v1),  [pdf](http://arxiv.org/pdf/2503.05156v1)

**Tags**: cs.CV 



### LVLM-Compress-Bench: Benchmarking the Broader Impact of Large   Vision-Language Model Compression
**Authors**: Souvik Kundu, Anahita Bhiwandiwalla, Sungduk Yu, Phillip Howard, Tiep Le, Sharath Nittur Sridhar, David Cobbley, Hao Kang, Vasudev Lal

**Updated**: 2025-03-06T21:21:18Z

**Summary**: Despite recent efforts in understanding the compression impact on large language models (LLMs) in terms of their downstream task performance and trustworthiness on relatively simpler uni-modal benchmarks (for example, question answering, common sense reasoning), their detailed study on multi-modal Large Vision-Language Models (LVLMs) is yet to be unveiled. Towards mitigating this gap, we present LVLM-Compress-Bench, a framework to first thoroughly study the broad impact of compression on the generative performance of LVLMs with multi-modal input driven tasks. In specific, we consider two major classes of compression for autoregressive models, namely KV cache and weight compression, for the dynamically growing intermediate cache and static weights, respectively.   We use four LVLM variants of the popular LLaVA framework to present our analysis via integrating various state-of-the-art KV and weight compression methods including uniform, outlier-reduced, and group quantization for the KV cache and weights. With this framework we demonstrate on ten different multi-modal datasets with different capabilities including recognition, knowledge, language generation, spatial awareness, visual reasoning, hallucination and visual illusion identification, toxicity, stereotypes and bias. In specific, our framework demonstrates the compression impact on both general and ethically critical metrics leveraging a combination of real world and synthetic datasets to encompass diverse societal intersectional attributes. Extensive experimental evaluations yield diverse and intriguing observations on the behavior of LVLMs at different quantization budget of KV and weights, in both maintaining and losing performance as compared to the baseline model with FP16 data format.   Code will be open-sourced at https://github.com/opengear-project/LVLM-compress-bench.

**Link**: [arxiv](http://arxiv.org/abs/2503.04982v1),  [pdf](http://arxiv.org/pdf/2503.04982v1)

**Tags**: cs.CV cs.AI cs.CL 



### Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge   Reasoning
**Authors**: Giulio Corallo, Orion Weller, Fabio Petroni, Paolo Papotti

**Updated**: 2025-03-06T21:07:41Z

**Summary**: Incorporating external knowledge in large language models (LLMs) enhances their utility across diverse applications, but existing methods have trade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via similarity search, but key information may fall outside top ranked results. Long-context models can process multiple documents but are computationally expensive and limited by context window size. Inspired by students condensing study material for open-book exams, we propose task-aware key-value (KV) cache compression, which compresses external knowledge in a zero- or few-shot setup. This enables LLMs to reason efficiently over a compacted representation of all relevant information. Experiments show our approach outperforms both RAG and task-agnostic compression methods. On LongBench v2, it improves accuracy by up to 7 absolute points over RAG with a 30x compression rate, while reducing inference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG performs well when sparse evidence suffices, whereas task-aware compression is superior for broad knowledge tasks.

**Link**: [arxiv](http://arxiv.org/abs/2503.04973v1),  [pdf](http://arxiv.org/pdf/2503.04973v1)

**Tags**: cs.CL cs.AI cs.IR cs.LG 



### Markov Chain of Thought for Efficient Mathematical Reasoning
**Authors**: Wen Yang, Minpeng Liao, Kai Fan

**Updated**: 2025-03-06T06:39:56Z

**Summary**: Chain of Thought (CoT) of multi-step benefits from the logical structure of the reasoning steps and task-specific actions, significantly enhancing the mathematical reasoning capabilities of large language models. As the prevalence of long CoT, the number of reasoning steps exceeds manageable token limits and leads to higher computational demands. Inspired by the fundamental logic of human cognition, "derive, then reduce", we conceptualize the standard multi-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we consider the mathematical reasoning task, defining each reasoning step as text accompanied by a Python code snippet. To facilitate a longer reasoning path, self-correction is enabled through interactions with the code interpreter. Our MCoT aims to compress previous reasoning steps into a simplified question, enabling efficient next-step inference without relying on a lengthy KV cache. In our experiments, we curate the $\texttt{MCoTInstruct}$ dataset, and the empirical results indicate that MCoT not only significantly enhances efficiency but also maintains comparable accuracy. While much remains to be explored, this work paves the way for exploring the long CoT reasoning abilities of LLMs. The code is available at https://github.com/james-yw/Markov-Chain-of-Thought

**Link**: [arxiv](http://arxiv.org/abs/2410.17635v2),  [pdf](http://arxiv.org/pdf/2410.17635v2)

**Tags**: cs.AI cs.CL 



### TUNA: Tuning Unstable and Noisy Cloud Applications
**Authors**: Johannes Freischuetz, Konstantinos Kanellis, Brian Kroth, Shivaram Venkataraman

**Updated**: 2025-03-05T20:36:51Z

**Summary**: Autotuning plays a pivotal role in optimizing the performance of systems, particularly in large-scale cloud deployments. One of the main challenges in performing autotuning in the cloud arises from performance variability. We first investigate the extent to which noise slows autotuning and find that as little as $5\%$ noise can lead to a $2.5$x slowdown in converging to the best-performing configuration. We measure the magnitude of noise in cloud computing settings and find that while some components (CPU, disk) have almost no performance variability, there are still sources of significant variability (caches, memory). Furthermore, variability leads to autotuning finding unstable configurations. As many as $63.3\%$ of the configurations selected as "best" during tuning can have their performance degrade by $30\%$ or more when deployed. Using this as motivation, we propose a novel approach to improve the efficiency of autotuning systems by (a) detecting and removing outlier configurations and (b) using ML-based approaches to provide a more stable true signal of de-noised experiment results to the optimizer. The resulting system, TUNA (Tuning Unstable and Noisy Cloud Applications) enables faster convergence and robust configurations. Tuning postgres running mssales, an enterprise production workload, we find that TUNA can lead to $1.88$x lower running time on average with $2.58x$ lower standard deviation compared to traditional sampling methodologies.

**Link**: [arxiv](http://arxiv.org/abs/2503.01801v2),  [pdf](http://arxiv.org/pdf/2503.01801v2)

**Tags**: cs.OS 



### GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera   Control
**Authors**: Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Müller, Alexander Keller, Sanja Fidler, Jun Gao

**Updated**: 2025-03-05T18:59:50Z

**Summary**: We present GEN3C, a generative video model with precise Camera Control and temporal 3D Consistency. Prior video models already generate realistic videos, but they tend to leverage little 3D information, leading to inconsistencies, such as objects popping in and out of existence. Camera control, if implemented at all, is imprecise, because camera parameters are mere inputs to the neural network which must then infer how the video depends on the camera. In contrast, GEN3C is guided by a 3D cache: point clouds obtained by predicting the pixel-wise depth of seed images or previously generated frames. When generating the next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with the new camera trajectory provided by the user. Crucially, this means that GEN3C neither has to remember what it previously generated nor does it have to infer the image structure from the camera pose. The model, instead, can focus all its generative power on previously unobserved regions, as well as advancing the scene state to the next frame. Our results demonstrate more precise camera control than prior work, as well as state-of-the-art results in sparse-view novel view synthesis, even in challenging settings such as driving scenes and monocular dynamic video. Results are best viewed in videos. Check out our webpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/

**Link**: [arxiv](http://arxiv.org/abs/2503.03751v1),  [pdf](http://arxiv.org/pdf/2503.03751v1)

**Tags**: cs.CV cs.GR 



### Online Scheduling for LLM Inference with KV Cache Constraints
**Authors**: Patrick Jaillet, Jiashuo Jiang, Chara Podimata, Zijie Zhou

**Updated**: 2025-03-05T14:43:01Z

**Summary**: Large Language Model (LLM) inference, where a trained model generates text one word at a time in response to user prompts, is a computationally intensive process requiring efficient scheduling to optimize latency and resource utilization. A key challenge in LLM inference is the management of the Key-Value (KV) cache, which reduces redundant computations but introduces memory constraints. In this work, we model LLM inference with KV cache constraints theoretically and propose novel batching and scheduling algorithms that minimize inference latency while effectively managing the KV cache's memory.   We analyze both semi-online and fully online scheduling models, and our results are threefold. First, we provide a polynomial-time algorithm that achieves exact optimality in terms of average latency in the semi-online prompt arrival model. Second, in the fully online case with a stochastic prompt arrival, we introduce an efficient online scheduling algorithm with constant regret. Third, we prove that no algorithm (deterministic or randomized) can achieve a constant competitive ratio in fully online adversarial settings. Our empirical evaluations on a public LLM inference dataset, using the Llama-70B model on A100 GPUs, show that our approach significantly outperforms benchmark algorithms used currently in practice, achieving lower latency while reducing energy consumption. Overall, our results offer a path toward more sustainable and cost-effective LLM deployment.

**Link**: [arxiv](http://arxiv.org/abs/2502.07115v3),  [pdf](http://arxiv.org/pdf/2502.07115v3)

**Tags**: cs.LG cs.AI math.OC 



### StableToolBench: Towards Stable Large-Scale Benchmarking on Tool   Learning of Large Language Models
**Authors**: Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, Yang Liu

**Updated**: 2025-03-05T07:39:03Z

**Summary**: Large Language Models (LLMs) have witnessed remarkable advancements in recent years, prompting the exploration of tool learning, which integrates LLMs with external tools to address diverse real-world challenges. Assessing the capability of LLMs to utilise tools necessitates large-scale and stable benchmarks. However, previous works relied on either hand-crafted online tools with limited scale, or large-scale real online APIs suffering from instability of API status. To address this problem, we introduce StableToolBench, a benchmark evolving from ToolBench, proposing a virtual API server and stable evaluation system. The virtual API server contains a caching system and API simulators which are complementary to alleviate the change in API status. Meanwhile, the stable evaluation system designs solvable pass and win rates using GPT-4 as the automatic evaluator to eliminate the randomness during evaluation. Experimental results demonstrate the stability of StableToolBench, and further discuss the effectiveness of API simulators, the caching system, and the evaluator system.

**Link**: [arxiv](http://arxiv.org/abs/2403.07714v5),  [pdf](http://arxiv.org/pdf/2403.07714v5)

**Tags**: cs.CL 



### Enhancing Memory Efficiency in Large Language Model Training Through   Chronos-aware Pipeline Parallelism
**Authors**: Xinyuan Lin, Chenlu Li, Zongle Huang, Chunyu Wang, Bo Xiao, Huazhong Yang, Shishi Duan, Yongpan Liu

**Updated**: 2025-03-05T04:54:50Z

**Summary**: Larger model sizes and longer sequence lengths have empowered the Large Language Model (LLM) to achieve outstanding performance across various domains. However, this progress brings significant storage capacity challenges for LLM pretraining. High Bandwidth Memory (HBM) is expensive and requires more advanced packaging technologies for capacity expansion, creating an urgent need for memory-efficient scheduling strategies. Yet, prior pipeline parallelism schedules have primarily focused on reducing bubble overhead, often neglecting memory efficiency and lacking compatibility with other memory-efficient strategies. Consequently, these methods struggle to meet the storage demands of storage capacity for next-generation LLM. This work presents ChronosPipe, a Chronos-aware pipeline parallelism for memory-efficient LLM pretraining. The core insight of ChronosPipe is to treat HBM as a fast but small 'cache,' optimizing and exploiting temporal locality within LLM pretraining to enhance HBM utilization. ChronosPipe introduces a pipeline scheduling strategy, Chronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal locality of activations. Additionally, it leverages Chronos-Recomp and Chronos-Offload to efficiently harness the intrinsic temporal locality of activations and weights in Deep Neural Networks. Experiment results show that ChronosPipe can expand the trainable model size by 2.4x while maintaining comparable throughput, achieving 1.5x better than the 1F1B strategy combined with recomputation.

**Link**: [arxiv](http://arxiv.org/abs/2503.03182v1),  [pdf](http://arxiv.org/pdf/2503.03182v1)

**Tags**: cs.DC 



### InfiniSST: Simultaneous Translation of Unbounded Speech with Large   Language Model
**Authors**: Siqi Ouyang, Xi Xu, Lei Li

**Updated**: 2025-03-04T19:51:29Z

**Summary**: Simultaneous translation of unbounded streaming speech remains a challenging problem due to the need for effectively processing the history speech context and past translations so that quality and latency, including computation overhead, can be balanced. Most prior works assume pre-segmented speech, limiting their real-world applicability. In this paper, we propose InfiniSST, a novel approach that formulates SST as a multi-turn dialogue task, enabling seamless translation of unbounded speech. We construct translation trajectories and robust segments from MuST-C with multi-latency augmentation during training and develop a key-value (KV) cache management strategy to facilitate efficient inference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that InfiniSST reduces computation-aware latency by 0.5 to 1 second while maintaining the same translation quality compared to baselines. Ablation studies further validate the contributions of our data construction and cache management strategy. We release the code at https://github.com/LeiLiLab/InfiniSST

**Link**: [arxiv](http://arxiv.org/abs/2503.02969v1),  [pdf](http://arxiv.org/pdf/2503.02969v1)

**Tags**: cs.CL cs.AI 



### Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression
**Authors**: Nathan Godey, Alessio Devoto, Yu Zhao, Simone Scardapane, Pasquale Minervini, Éric de la Clergerie, Benoît Sagot

**Updated**: 2025-03-04T17:37:49Z

**Summary**: Autoregressive language models rely on a Key-Value (KV) Cache, which avoids re-computing past hidden states during generation, making it faster. As model sizes and context lengths grow, the KV Cache becomes a significant memory bottleneck, which calls for compression methods that limit its size during generation. In this paper, we discover surprising properties of Query (Q) and Key (K) vectors that allow us to efficiently approximate attention scores without computing the attention maps. We propose Q-Filters, a training-free KV Cache compression method that filters out less crucial Key-Value pairs based on a single context-agnostic projection. Contrarily to many alternatives, Q-Filters is compatible with FlashAttention, as it does not require direct access to attention weights. Experimental results in long-context settings demonstrate that Q-Filters is competitive with attention-based compression methods such as SnapKV in retrieval tasks while consistently outperforming efficient compression schemes such as Streaming-LLM in generation setups. Notably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task with a x32 compression level while reducing the generation perplexity drop by up to 65% in text generation compared to Streaming-LLM.

**Link**: [arxiv](http://arxiv.org/abs/2503.02812v1),  [pdf](http://arxiv.org/pdf/2503.02812v1)

**Tags**: cs.CL cs.AI 



### Efficient and Optimal No-Regret Caching under Partial Observation
**Authors**: Younes Ben Mazziane, Francescomaria Faticanti, Sara Alouf, Giovanni Neglia

**Updated**: 2025-03-04T16:21:33Z

**Summary**: Online learning algorithms have been successfully used to design caching policies with sublinear regret in the total number of requests, with no statistical assumption about the request sequence. Most existing algorithms involve computationally expensive operations and require knowledge of all past requests. However, this may not be feasible in practical scenarios like caching at a cellular base station. Therefore, we study the caching problem in a more restrictive setting where only a fraction of past requests are observed, and we propose a randomized caching policy with sublinear regret based on the classic online learning algorithm Follow-the-Perturbed-Leader (FPL). Our caching policy is the first to attain the asymptotically optimal regret bound while ensuring asymptotically constant amortized time complexity in the partial observability setting of requests. The experimental evaluation compares the proposed solution against classic caching policies and validates the proposed approach under synthetic and real-world request traces.

**Link**: [arxiv](http://arxiv.org/abs/2503.02758v1),  [pdf](http://arxiv.org/pdf/2503.02758v1)

**Tags**: cs.LG cs.NI 



### Let the Code LLM Edit Itself When You Edit the Code
**Authors**: Zhenyu He, Jun Zhang, Shengjie Luo, Jingjing Xu, Zhi Zhang, Di He

**Updated**: 2025-03-04T13:01:07Z

**Summary**: In this work, we investigate a typical scenario in code generation where a developer edits existing code in real time and requests a code assistant, e.g., a large language model, to re-predict the next token or next line on the fly. Naively, the LLM needs to re-encode the entire KV cache to provide an accurate prediction. However, this process is computationally expensive, especially when the sequence length is long. Simply encoding the edited subsequence and integrating it to the original KV cache meets the temporal confusion problem, leading to significantly worse performance. We address this efficiency and accuracy trade-off by introducing \underline{\textbf{Positional \textbf{I}ntegrity \textbf{E}ncoding} (PIE). Building upon the rotary positional encoding, PIE first removes the rotary matrices in the Key cache that introduce temporal confusion and then reapplies the correct rotary matrices. This process ensures that positional relationships between tokens are correct and requires only a single round of matrix multiplication. We validate the effectiveness of PIE through extensive experiments on the RepoBench-C-8k dataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters. Our evaluation includes three real-world coding tasks: code insertion, code deletion, and multi-place code editing. Results demonstrate that PIE reduces computational overhead by over 85% compared to the standard full recomputation approach across all model sizes and tasks while well approximating the model performance.

**Link**: [arxiv](http://arxiv.org/abs/2407.03157v2),  [pdf](http://arxiv.org/pdf/2407.03157v2)

**Tags**: cs.CL cs.AI cs.LG cs.SE 



### Q&C: When Quantization Meets Cache in Efficient Image Generation
**Authors**: Xin Ding, Xin Li, Haotong Qin, Zhibo Chen

**Updated**: 2025-03-04T11:19:02Z

**Summary**: Quantization and cache mechanisms are typically applied individually for efficient Diffusion Transformers (DiTs), each demonstrating notable potential for acceleration. However, the promoting effect of combining the two mechanisms on efficient generation remains under-explored. Through empirical investigation, we find that the combination of quantization and cache mechanisms for DiT is not straightforward, and two key challenges lead to severe catastrophic performance degradation: (i) the sample efficacy of calibration datasets in post-training quantization (PTQ) is significantly eliminated by cache operation; (ii) the combination of the above mechanisms introduces more severe exposure bias within sampling distribution, resulting in amplified error accumulation in the image generation process. In this work, we take advantage of these two acceleration mechanisms and propose a hybrid acceleration method by tackling the above challenges, aiming to further improve the efficiency of DiTs while maintaining excellent generation capability. Concretely, a temporal-aware parallel clustering (TAP) is designed to dynamically improve the sample selection efficacy for the calibration within PTQ for different diffusion steps. A variance compensation (VC) strategy is derived to correct the sampling distribution. It mitigates exposure bias through an adaptive correction factor generation. Extensive experiments have shown that our method has accelerated DiTs by 12.7x while preserving competitive generation capability. The code will be available at https://github.com/xinding-sys/Quant-Cache.

**Link**: [arxiv](http://arxiv.org/abs/2503.02508v1),  [pdf](http://arxiv.org/pdf/2503.02508v1)

**Tags**: cs.CV eess.IV 



### Energy efficiency of cache eviction algorithms for Zipf distributed   objects
**Authors**: Emese Sziklay, Tamás Jursonovics

**Updated**: 2025-03-04T11:15:47Z

**Summary**: This paper presents a summary analysis of the Least Frequently Used (LFU) and Perfect Least Frequently Used (PLFU) cache eviction algorithms on real data, transferred on Content Delivery Nettworks (CDNs), as well as on Zipf distributed samples. In light of the growing emphasis on energy efficiency in CDNs in recent years due to rising energy costs, this paper considers and discusses the total CPU time required to run a cache algorithm. The total CPU time represents a novel metric for evaluating cache performance, and it is contrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a new algorithm with an admission policy and the eviction strategy that of PLFU is presented. The results demonstrate that it is a simple and straightforward algorithm to implement and offers high CHR and low CPU time.

**Link**: [arxiv](http://arxiv.org/abs/2503.02504v1),  [pdf](http://arxiv.org/pdf/2503.02504v1)

**Tags**: cs.PF 



### PersonaX: A Recommendation Agent Oriented User Modeling Framework for   Long Behavior Sequence
**Authors**: Yunxiao Shi, Wujiang Xu, Zeqi Zhang, Xing Zi, Qiang Wu, Min Xu

**Updated**: 2025-03-04T08:41:40Z

**Summary**: Recommendation agents leverage large language models for user modeling LLM UM to construct textual personas guiding alignment with real users. However existing LLM UM methods struggle with long user generated content UGC due to context limitations and performance degradation. To address this sampling strategies prioritize relevance or recency are often applied yet they inevitably neglect the diverse user interests embedded within the discarded behaviors resulting in incomplete modeling and degraded profiling quality. Furthermore relevance based sampling requires real time retrieval forcing the user modeling process to operate online which introduces significant latency overhead. In this paper we propose PersonaX an agent agnostic LLM UM framework that tackles these challenges through sub behavior sequence SBS selection and offline multi persona construction. PersonaX extracts compact SBS segments offline to capture diverse user interests generating fine grained textual personas that are cached for efficient online retrieval. This approach ensures that the user persona used for prompting remains highly relevant to the current context while eliminating the need for online user modeling. For SBS selection we ensure both efficiency length less than five and high representational quality by balancing prototypicality and diversity within the sampled data. Extensive experiments validate the effectiveness and versatility of PersonaX in high quality user profiling. Utilizing only 30 to 50 percent of the behavioral data with a sequence length of 480 integrating PersonaX with AgentCF yields an absolute performance improvement of 3 to 11 percent while integration with Agent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic framework sets a new benchmark for scalable user modeling paving the way for more accurate and efficient LLM driven recommendation agents.

**Link**: [arxiv](http://arxiv.org/abs/2503.02398v1),  [pdf](http://arxiv.org/pdf/2503.02398v1)

**Tags**: cs.IR cs.AI 



### VQ-LLM: High-performance Code Generation for Vector Quantization   Augmented LLM Inference
**Authors**: Zihan Liu, Xinhao Luo, Junxian Guo, Wentao Ni, Yangjie Zhou, Yue Guan, Cong Guo, Weihao Cui, Yu Feng, Minyi Guo, Yuhao Zhu, Minjia Zhang, Jingwen Leng, Chen Jin

**Updated**: 2025-03-04T03:18:56Z

**Summary**: In this work, we design and implement VQ-LLM, an efficient fused Vector Quantization (VQ) kernel generation framework. We first introduce a software abstraction called codebook cache to optimize codebook access efficiency and support the integration of VQ with various computations. The codebook cache adaptively stores different entries across the GPU's memory hierarchy, including off-chip global memory, on-chip shared memory, and registers. Centered around the codebook cache, we design an efficient computation engine that optimizes memory traffic during computations involving codebooks. This compute engine adopts the codebook-centric dataflow and fusion optimizations. Additionally, we provide adaptive heuristics to tailor parameter selection in our optimizations to diverse VQ configurations. Our optimizations achieve an average latency reduction of 46.13% compared to unoptimized versions. Compared to existing open-source implementations, our methods decrease latency by 64.36% to 99.1%. A final comparison with state-of-the-art element-wise quantization methods like AWQ and KVQuant shows that our VQ-LLM is practically viable, achieving latencies close or even better latencies to those at equivalent bit-widths, potentially offering greater accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2503.02236v1),  [pdf](http://arxiv.org/pdf/2503.02236v1)

**Tags**: cs.DC 



### RefreshKV: Updating Small KV Cache During Long-form Generation
**Authors**: Fangyuan Xu, Tanya Goyal, Eunsol Choi

**Updated**: 2025-03-03T18:23:47Z

**Summary**: Generating long sequences of tokens given a long-context input is a very compute-intensive inference scenario for large language models (LLMs). One prominent inference speed-up approach is to construct a smaller key-value (KV) cache, relieving LLMs from computing attention over a long sequence of tokens. While such methods work well to generate short sequences, their performance degrades rapidly for long-form generation. Most KV compression happens once, prematurely removing tokens that can be useful later in the generation. We propose a new inference method, RefreshKV, that flexibly alternates between full context attention and attention over a subset of input tokens during generation. After each full attention step, we update the smaller KV cache based on the attention pattern over the entire input. Applying our method to off-the-shelf LLMs achieves comparable speedup to eviction-based methods while improving performance for various long-form generation tasks. Lastly, we show that continued pretraining with our inference setting brings further gains in performance.

**Link**: [arxiv](http://arxiv.org/abs/2411.05787v2),  [pdf](http://arxiv.org/pdf/2411.05787v2)

**Tags**: cs.CL 



### EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and   Joint Low-Rank Projection
**Authors**: Yuhao Zhou, Sirui Song, Boyang Liu, Zhiheng Xi, Senjie Jin, Xiaoran Fan, Zhihao Zhang, Wei Li, Xuanjing Huang

**Updated**: 2025-03-03T14:26:51Z

**Summary**: Rotary Position Embedding (RoPE) enables each attention head to capture multi-frequency information along the sequence dimension and is widely applied in foundation models. However, the nonlinearity introduced by RoPE complicates optimization of the key state in the Key-Value (KV) cache for RoPE-based attention. Existing KV cache compression methods typically store key state before rotation and apply the transformation during decoding, introducing additional computational overhead. This paper introduces EliteKV, a flexible modification framework for RoPE-based models supporting variable KV cache compression ratios. EliteKV first identifies the intrinsic frequency preference of each head using RoPElite, selectively restoring linearity to certain dimensions of key within attention computation. Building on this, joint low-rank compression of key and value enables partial cache sharing. Experimental results show that with minimal uptraining on only $0.6\%$ of the original training data, RoPE-based models achieve a $75\%$ reduction in KV cache size while preserving performance within a negligible margin. Furthermore, EliteKV consistently performs well across models of different scales within the same family.

**Link**: [arxiv](http://arxiv.org/abs/2503.01586v1),  [pdf](http://arxiv.org/pdf/2503.01586v1)

**Tags**: cs.LG cs.AI cs.CL 



### KurTail : Kurtosis-based LLM Quantization
**Authors**: Mohammad Sadegh Akhondzadeh, Aleksandar Bojchevski, Evangelos Eleftheriou, Martino Dazzi

**Updated**: 2025-03-03T12:43:06Z

**Summary**: One of the challenges of quantizing a large language model (LLM) is the presence of outliers. Outliers often make uniform quantization schemes less effective, particularly in extreme cases such as 4-bit quantization. We introduce KurTail, a new post-training quantization (PTQ) scheme that leverages Kurtosis-based rotation to mitigate outliers in the activations of LLMs. Our method optimizes Kurtosis as a measure of tailedness. This approach enables the quantization of weights, activations, and the KV cache in 4 bits. We utilize layer-wise optimization, ensuring memory efficiency. KurTail outperforms existing quantization methods, offering a 13.3\% boost in MMLU accuracy and a 15.5\% drop in Wiki perplexity compared to QuaRot. It also outperforms SpinQuant with a 2.6\% MMLU gain and reduces perplexity by 2.9\%, all while reducing the training cost. For comparison, learning the rotation using SpinQuant for Llama3-70B requires at least four NVIDIA H100 80GB GPUs, whereas our method requires only a single GPU, making it a more accessible solution for consumer GPU.

**Link**: [arxiv](http://arxiv.org/abs/2503.01483v1),  [pdf](http://arxiv.org/pdf/2503.01483v1)

**Tags**: cs.LG 



### Performance Optimization of 3D Stencil Computation on ARM Scalable   Vector Extension
**Authors**: Hongguang Chen

**Updated**: 2025-03-03T09:38:20Z

**Summary**: Stencil computation is essential in high-performance computing, especially for large-scale tasks like liquid simulation and weather forecasting. Optimizing its performance can reduce both energy consumption and computation time, which is critical in disaster prediction. This paper explores optimization techniques for 7-point 3D stencil computation on ARM's Scalable Vector Extension (SVE), using the Roofline model and tools like Gem5 and cacti. We evaluate software optimizations such as vectorization and tiling, as well as hardware adjustments in ARM SVE vector lengths and cache configurations. The study also examines performance, power consumption, and chip area trade-offs to identify optimal configurations for ARM-based systems.

**Link**: [arxiv](http://arxiv.org/abs/2503.01348v1),  [pdf](http://arxiv.org/pdf/2503.01348v1)

**Tags**: cs.PF 



### WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large   Language Models
**Authors**: Jian Yuan, Ziwei He, Haoli Bai, Jingwen Leng, Bo Jiang

**Updated**: 2025-03-03T09:12:34Z

**Summary**: Large Language Models (LLMs) use key-value (KV) cache to reduce redundant computation in autoregressive generation. However, the KV cache size increases linearly during generation, leading to excessive memory usage, especially for long texts. Most KV cache compression methods evict the unimportant KV pairs to maintain a fixed cache size, which leads to the permanent loss of tokens during generation. However, singular value decomposition shows that \textit{values} do not exhibit a strong low-rank property as \textit{keys} do, suggesting that information is distributed more evenly across \textit{values}, in contrast to its more redundant distribution within \textit{keys}. Therefore, methods that evict both \textit{keys} and \textit{values} risk losing crucial information and compromise context integrity, ultimately degrading the output quality. To address this problem, we propose WeightedKV, a novel, training-free approach that discards the \textit{keys} of less important tokens, while merging their \textit{values} into neighboring tokens via a convex combination weighted by their average attention scores. In this way, the retained \textit{keys} serve as anchors that guide the generation process, while the merged \textit{values} provide a rich contextual backdrop. We assess our method on four widely used language modeling datasets, demonstrating superior performance compared to all baseline methods, particularly with a lower budget ratio.

**Link**: [arxiv](http://arxiv.org/abs/2503.01330v1),  [pdf](http://arxiv.org/pdf/2503.01330v1)

**Tags**: cs.CL 



### CacheQuant: Comprehensively Accelerated Diffusion Models
**Authors**: Xuewen Liu, Zhikai Li, Qingyi Gu

**Updated**: 2025-03-03T09:04:51Z

**Summary**: Diffusion models have gradually gained prominence in the field of image synthesis, showcasing remarkable generative capabilities. Nevertheless, the slow inference and complex networks, resulting from redundancy at both temporal and structural levels, hinder their low-latency applications in real-world scenarios. Current acceleration methods for diffusion models focus separately on temporal and structural levels. However, independent optimization at each level to further push the acceleration limits results in significant performance degradation. On the other hand, integrating optimizations at both levels can compound the acceleration effects. Unfortunately, we find that the optimizations at these two levels are not entirely orthogonal. Performing separate optimizations and then simply integrating them results in unsatisfactory performance. To tackle this issue, we propose CacheQuant, a novel training-free paradigm that comprehensively accelerates diffusion models by jointly optimizing model caching and quantization techniques. Specifically, we employ a dynamic programming approach to determine the optimal cache schedule, in which the properties of caching and quantization are carefully considered to minimize errors. Additionally, we propose decoupled error correction to further mitigate the coupled and accumulated errors step by step. Experimental results show that CacheQuant achieves a 5.18 speedup and 4 compression for Stable Diffusion on MS-COCO, with only a 0.02 loss in CLIP score. Our code are open-sourced: https://github.com/BienLuky/CacheQuant .

**Link**: [arxiv](http://arxiv.org/abs/2503.01323v1),  [pdf](http://arxiv.org/pdf/2503.01323v1)

**Tags**: cs.CV cs.AI 



### DCI: A Coordinated Allocation and Filling Workload-Aware Dual-Cache   Allocation GNN Inference Acceleration System
**Authors**: Yi Luo, Yaobin Wang, Qi Wang, Yingchen Song, Huan Wu, Qingfeng Wang, Jun Huang

**Updated**: 2025-03-03T08:06:55Z

**Summary**: Graph Neural Networks (GNNs) are powerful tools for processing graph-structured data, increasingly used for large-scale real-world graphs via sampling-based inference methods. However, inherent characteristics of neighbor sampling lead to redundant data loading during GNN inference, compounded by inefficient data transfers between host and GPU memory, resulting in slow inference and low resource utilization. Existing methods to accelerate GNN inference face several challenges: (1) low practical GPU memory utilization, (2) overlooking adjacency matrix locality, and (3) long preprocessing time. To address these challenges, we introduce DCI, an efficient workload-aware dual-cache allocation system for GNN inference acceleration. DCI allocates cache capacities for both node features and adjacency matrices based on workload patterns during the pre-sampling phase, leveraging a lightweight cache-filling algorithm to optimize data loading efficiency. Experimental results demonstrate that DCI accelerates sampling and node feature loading, achieving end-to-end inference speedups of 1.18$\times$ to 11.26$\times$ compared to DGL, and 1.14$\times$ to 13.68$\times$ over RAIN, while reducing preprocessing time by 52.8\% to 98.7\%. Additionally, DCI outperforms state-of-the-art single-cache inference systems by achieving speedup of 1.08$\times$ to 1.32$\times$. We also compared DCI with DUCATI's dual-cache population strategy. Our lightweight population algorithm allows DCI to achieve nearly the same inference speed while keeping preprocessing time to less than 20\% of that required by DUCATI.

**Link**: [arxiv](http://arxiv.org/abs/2503.01281v1),  [pdf](http://arxiv.org/pdf/2503.01281v1)

**Tags**: cs.AR 



### TokenSelect: Efficient Long-Context Inference and Length Extrapolation   for LLMs via Dynamic Token-Level KV Cache Selection
**Authors**: Wei Wu, Zhuoshi Pan, Chao Wang, Liyi Chen, Yunchu Bai, Tianfu Wang, Kun Fu, Zheng Wang, Hui Xiong

**Updated**: 2025-03-03T05:49:41Z

**Summary**: The rapid advancement of Large Language Models (LLMs) has driven growing demand for processing extended context sequences in contemporary applications. However, this progress faces two major challenges: performance degradation due to sequence lengths out-of-distribution, and excessively long inference times caused by the quadratic computational complexity of attention. These issues hinder the application of LLMs in long-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free method for efficient and accurate long-context inference. TokenSelect builds upon the observation of non-contiguous attention sparsity, using Query-Key dot products to measure per-head KV Cache criticality at token-level. By per-head soft voting mechanism, TokenSelect selectively involves a few critical KV cache tokens in attention calculation without sacrificing accuracy. To further accelerate TokenSelect, we design the Selection Cache based on observations of consecutive Query similarity and implemented efficient dot product kernel, significantly reducing the overhead. A comprehensive evaluation of TokenSelect demonstrates up to 23.84x speedup in attention computation and up to 2.28x acceleration in end-to-end latency, while providing superior performance compared to state-of-the-art long-context inference methods.

**Link**: [arxiv](http://arxiv.org/abs/2411.02886v2),  [pdf](http://arxiv.org/pdf/2411.02886v2)

**Tags**: cs.CL cs.AI cs.LG 



### Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses   in LLMs
**Authors**: Ravi Ghadia, Avinash Kumar, Gaurav Jain, Prashant Nair, Poulami Das

**Updated**: 2025-03-02T18:12:50Z

**Summary**: Autoregressive Transformers rely on Key-Value (KV) caching to accelerate inference. However, the linear growth of the KV cache with context length leads to excessive memory consumption and bandwidth constraints. This bottleneck is particularly problematic in real-time applications -- such as chatbots and interactive assistants -- where low latency and high memory efficiency are critical. Existing methods drop distant tokens or compress states in a lossy manner, sacrificing accuracy by discarding vital context or introducing bias.   We propose MorphKV, an inference-time technique that maintains a constant-sized KV cache while preserving accuracy. MorphKV balances long-range dependencies and local coherence during text generation. It eliminates early-token bias while retaining high-fidelity context by adaptively ranking tokens through correlation-aware selection. Unlike heuristic retention or lossy compression, MorphKV iteratively refines the KV cache via lightweight updates guided by attention patterns of recent tokens. This approach captures inter-token correlation with greater accuracy, crucial for tasks like content creation and code generation. Our studies on long-response tasks show 52.9$\%$ memory savings and 18.2$\%$ higher accuracy on average compared to state-of-the-art prior works, enabling efficient real-world deployment.

**Link**: [arxiv](http://arxiv.org/abs/2503.00979v1),  [pdf](http://arxiv.org/pdf/2503.00979v1)

**Tags**: cs.CL cs.AI cs.LG 



### When Attention Sink Emerges in Language Models: An Empirical View
**Authors**: Xiangming Gu, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du, Ye Wang, Min Lin

**Updated**: 2025-03-02T14:37:53Z

**Summary**: Language Models (LMs) assign significant attention to the first token, even if it is not semantically important, which is known as attention sink. This phenomenon has been widely adopted in applications such as streaming/long context generation, KV cache optimization, inference acceleration, model quantization, and others. Despite its widespread use, a deep understanding of attention sink in LMs is still lacking. In this work, we first demonstrate that attention sinks exist universally in LMs with various inputs, even in small models. Furthermore, attention sink is observed to emerge during the LM pre-training, motivating us to investigate how optimization, data distribution, loss function, and model architecture in LM pre-training influence its emergence. We highlight that attention sink emerges after effective optimization on sufficient training data. The sink position is highly correlated with the loss function and data distribution. Most importantly, we find that attention sink acts more like key biases, storing extra attention scores, which could be non-informative and not contribute to the value computation. We also observe that this phenomenon (at least partially) stems from tokens' inner dependence on attention scores as a result of softmax normalization. After relaxing such dependence by replacing softmax attention with other attention operations, such as sigmoid attention without normalization, attention sinks do not emerge in LMs up to 1B parameters. The code is available at https://github.com/sail-sg/Attention-Sink.

**Link**: [arxiv](http://arxiv.org/abs/2410.10781v2),  [pdf](http://arxiv.org/pdf/2410.10781v2)

**Tags**: cs.CL cs.AI cs.LG 



### MoSFormer: Augmenting Temporal Context with Memory of Surgery for   Surgical Phase Recognition
**Authors**: Hao Ding, Xu Lian, Mathias Unberath

**Updated**: 2025-03-02T02:26:21Z

**Summary**: Surgical phase recognition from video enables various downstream applications. Transformer-based sliding window approaches have set the state-of-the-art by capturing rich spatial-temporal features. However, while transformers can theoretically handle arbitrary-length sequences, in practice they are limited by memory and compute constraints, resulting in fixed context windows that struggle with maintaining temporal consistency across lengthy surgical procedures. This often leads to fragmented predictions and limited procedure-level understanding. To address these challenges, we propose Memory of Surgery (MoS), a framework that enriches temporal modeling by incorporating both semantic interpretable long-term surgical history and short-term impressions. MoSFormer, our enhanced transformer architecture, integrates MoS using a carefully designed encoding and fusion mechanism. We further introduce step filtering to refine history representation and develop a memory caching pipeline to improve training and inference stability, mitigating shortcut learning and overfitting. MoSFormer demonstrates state-of-the-art performance on multiple benchmarks. On the Challenging BernBypass70 benchmark, it attains 88.0 video-level accuracy and phase-level metrics of 70.7 precision, 68.7 recall, and 66.3 F1 score, outperforming its baseline with 2.1 video-level accuracy and phase-level metrics of 4.6 precision, 3.6 recall, and 3.8 F1 score. Further studies confirms the individual and combined benefits of long-term and short-term memory components through ablation and counterfactual inference. Qualitative results shows improved temporal consistency. The augmented temporal context enables procedure-level understanding, paving the way for more comprehensive surgical video analysis.

**Link**: [arxiv](http://arxiv.org/abs/2503.00695v1),  [pdf](http://arxiv.org/pdf/2503.00695v1)

**Tags**: cs.CV 



### IterGen: Iterative Semantic-aware Structured LLM Generation with   Backtracking
**Authors**: Shubham Ugare, Rohan Gumaste, Tarun Suresh, Gagandeep Singh, Sasa Misailovic

**Updated**: 2025-03-02T01:39:57Z

**Summary**: Large Language Models (LLMs) are widely used for tasks such as natural language and code generation, but their outputs often suffer from issues like hallucination, toxicity, and incorrect results. Current libraries for structured LLM generation rely on left-to-right decoding without support for backtracking, limiting the ability to correct or refine outputs mid-generation.   To address this, we introduce IterGen, a user-friendly library for iterative, grammar-guided LLM generation that enables users to move both forward and backward within the generated output based on grammar symbols. By leveraging a symbol-to-position mapping and maintaining the key-value (KV) cache state, IterGen ensures efficient and structured generation while allowing for corrections during the process. We demonstrate IterGen's effectiveness in two important applications: reducing privacy leakage in LLM outputs and improving the accuracy of LLM-generated SQL and Vega-Lite queries.   Our code and additional resources are available at https://structuredllm.com.

**Link**: [arxiv](http://arxiv.org/abs/2410.07295v2),  [pdf](http://arxiv.org/pdf/2410.07295v2)

**Tags**: cs.SE cs.LG cs.PL 



### Streaming Video Question-Answering with In-context Video KV-Cache   Retrieval
**Authors**: Shangzhe Di, Zhelun Yu, Guanghao Zhang, Haoyuan Li, Tao Zhong, Hao Cheng, Bolin Li, Wanggui He, Fangxun Shu, Hao Jiang

**Updated**: 2025-03-01T15:53:33Z

**Summary**: We propose ReKV, a novel training-free approach that enables efficient streaming video question-answering (StreamingVQA), by seamlessly integrating with existing Video Large Language Models (Video-LLMs). Traditional VideoQA systems struggle with long videos, as they must process entire videos before responding to queries, and repeat this process for each new question. In contrast, our approach analyzes long videos in a streaming manner, allowing for prompt responses as soon as user queries are received. Building on a common Video-LLM, we first incorporate a sliding-window attention mechanism, ensuring that input frames attend to a limited number of preceding frames, thereby reducing computational overhead. To prevent information loss, we store processed video key-value caches (KV-Caches) in RAM and disk, reloading them into GPU memory as needed. Additionally, we introduce a retrieval method that leverages an external retriever or the parameters within Video-LLMs to retrieve only query-relevant KV-Caches, ensuring both efficiency and accuracy in question answering. ReKV enables the separation of video encoding and question-answering across different processes and GPUs, significantly enhancing the efficiency of StreamingVQA. Through comprehensive experimentation, we validate the efficacy and practicality of our approach, which significantly boosts efficiency and enhances applicability over existing VideoQA models.

**Link**: [arxiv](http://arxiv.org/abs/2503.00540v1),  [pdf](http://arxiv.org/pdf/2503.00540v1)

**Tags**: cs.CV 



### Progressive Sparse Attention: Algorithm and System Co-design for   Efficient Attention in LLM Serving
**Authors**: Qihui Zhou, Peiqi Yin, Pengfei Zuo, James Cheng

**Updated**: 2025-03-01T07:56:42Z

**Summary**: Processing long contexts has become a critical capability for modern large language models (LLMs). However, serving long-context LLMs comes with significant inference costs due to the high memory overhead of the key-value (KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes) to mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV cache selection, which results in a trade-off between accuracy and efficiency. A larger $k$ improves accuracy but decreases efficiency, while a smaller $k$ boosts efficiency but compromises accuracy. To overcome this trade-off, this paper presents PSA, a $\underline{P}$rogressive $\underline{S}$parse $\underline{A}$ttention mechanism that integrates algorithmic innovations with system co-design to achieve both high inference accuracy and improved efficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache budget of different tokens and layers according to their real attention weight distributions, rather than relying on a fixed budget $k$. This enables high accuracy while minimizing KV cache usage. To further enhance execution efficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU interleaving and synchronization overhead during PSA computation. Additionally, we implement unified GPU memory management that optimizes PSA's memory utilization by accounting for uneven memory requirements across different model layers. Extensive experimental results demonstrate that PSA reduces KV cache usage for attention computation by up to 2.4$\times$ and 8.8$\times$, and increases end-to-end serving throughput by up to 1.4$\times$ and 2.0$\times$, compared to state-of-the-art DSAes and systems without sparse attention, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2503.00392v1),  [pdf](http://arxiv.org/pdf/2503.00392v1)

**Tags**: cs.LG cs.AI 



## Keyword: LLM Inference 
 ### MetaScale: Test-Time Scaling with Evolving Meta-Thoughts
**Authors**: Qin Liu, Wenxuan Zhou, Nan Xu, James Y. Huang, Fei Wang, Sheng Zhang, Hoifung Poon, Muhao Chen

**Updated**: 2025-03-17T17:59:54Z

**Summary**: One critical challenge for large language models (LLMs) for making complex reasoning is their reliance on matching reasoning patterns from training data, instead of proactively selecting the most appropriate cognitive strategy to solve a given task. Existing approaches impose fixed cognitive structures that enhance performance in specific tasks but lack adaptability across diverse scenarios. To address this limitation, we introduce METASCALE, a test-time scaling framework based on meta-thoughts -- adaptive thinking strategies tailored to each task. METASCALE initializes a pool of candidate meta-thoughts, then iteratively selects and evaluates them using a multi-armed bandit algorithm with upper confidence bound selection, guided by a reward model. To further enhance adaptability, a genetic algorithm evolves high-reward meta-thoughts, refining and extending the strategy pool over time. By dynamically proposing and optimizing meta-thoughts at inference time, METASCALE improves both accuracy and generalization across a wide range of tasks. Experimental results demonstrate that MetaScale consistently outperforms standard inference approaches, achieving an 11% performance gain in win rate on Arena-Hard for GPT-4o, surpassing o1-mini by 0.9% under style control. Notably, METASCALE scales more effectively with increasing sampling budgets and produces more structured, expert-level responses.

**Link**: [arxiv](http://arxiv.org/abs/2503.13447v1),  [pdf](http://arxiv.org/pdf/2503.13447v1)

**Tags**: cs.CL cs.AI cs.LG 



### Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is   Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance
**Authors**: Noah Y. Siegel, Nicolas Heess, Maria Perez-Ortiz, Oana-Maria Camburu

**Updated**: 2025-03-17T17:59:39Z

**Summary**: As large language models (LLMs) become increasingly capable, ensuring that their self-generated explanations are faithful to their internal decision-making process is critical for safety and oversight. In this work, we conduct a comprehensive counterfactual faithfulness analysis across 62 models from 8 families, encompassing both pretrained and instruction-tuned variants and significantly extending prior studies of counterfactual tests. We introduce phi-CCT, a simplified variant of the Correlational Counterfactual Test, which avoids the need for token probabilities while explaining most of the variance of the original test. Our findings reveal clear scaling trends: larger models are consistently more faithful on our metrics. However, when comparing instruction-tuned and human-imitated explanations, we find that observed differences in faithfulness can often be attributed to explanation verbosity, leading to shifts along the true-positive/false-positive Pareto frontier. While instruction-tuning and prompting can influence this trade-off, we find limited evidence that they fundamentally expand the frontier of explanatory faithfulness beyond what is achievable with pretrained models of comparable size. Our analysis highlights the nuanced relationship between instruction-tuning, verbosity, and the faithful representation of model decision processes.

**Link**: [arxiv](http://arxiv.org/abs/2503.13445v1),  [pdf](http://arxiv.org/pdf/2503.13445v1)

**Tags**: cs.CL cs.AI I.2.7 



### MaTVLM: Hybrid Mamba-Transformer for Efficient Vision-Language Modeling
**Authors**: Yingyue Li, Bencheng Liao, Wenyu Liu, Xinggang Wang

**Updated**: 2025-03-18T07:07:28Z

**Summary**: With the advancement of RNN models with linear complexity, the quadratic complexity challenge of transformers has the potential to be overcome. Notably, the emerging Mamba-2 has demonstrated competitive performance, bridging the gap between RNN models and transformers. However, due to sequential processing and vanishing gradients, RNN models struggle to capture long-range dependencies, limiting contextual understanding. This results in slow convergence, high resource demands, and poor performance on downstream understanding and complex reasoning tasks. In this work, we present a hybrid model MaTVLM by substituting a portion of the transformer decoder layers in a pre-trained VLM with Mamba-2 layers. Leveraging the inherent relationship between attention and Mamba-2, we initialize Mamba-2 with corresponding attention weights to accelerate convergence. Subsequently, we employ a single-stage distillation process, using the pre-trained VLM as the teacher model to transfer knowledge to the MaTVLM, further enhancing convergence speed and performance. Furthermore, we investigate the impact of differential distillation loss within our training framework. We evaluate the MaTVLM on multiple benchmarks, demonstrating competitive performance against the teacher model and existing VLMs while surpassing both Mamba-based VLMs and models of comparable parameter scales. Remarkably, the MaTVLM achieves up to 3.6x faster inference than the teacher model while reducing GPU memory consumption by 27.5%, all without compromising performance. Code and models are released at http://github.com/hustvl/MaTVLM.

**Link**: [arxiv](http://arxiv.org/abs/2503.13440v2),  [pdf](http://arxiv.org/pdf/2503.13440v2)

**Tags**: cs.CV 



### Deep Belief Markov Models for POMDP Inference
**Authors**: Giacomo Arcieri, Konstantinos G. Papakonstantinou, Daniel Straub, Eleni Chatzi

**Updated**: 2025-03-17T17:58:45Z

**Summary**: This work introduces a novel deep learning-based architecture, termed the Deep Belief Markov Model (DBMM), which provides efficient, model-formulation agnostic inference in Partially Observable Markov Decision Process (POMDP) problems. The POMDP framework allows for modeling and solving sequential decision-making problems under observation uncertainty. In complex, high-dimensional, partially observable environments, existing methods for inference based on exact computations (e.g., via Bayes' theorem) or sampling algorithms do not scale well. Furthermore, ground truth states may not be available for learning the exact transition dynamics. DBMMs extend deep Markov models into the partially observable decision-making framework and allow efficient belief inference entirely based on available observation data via variational inference methods. By leveraging the potency of neural networks, DBMMs can infer and simulate non-linear relationships in the system dynamics and naturally scale to problems with high dimensionality and discrete or continuous variables. In addition, neural network parameters can be dynamically updated efficiently based on data availability. DBMMs can thus be used to infer a belief variable, thus enabling the derivation of POMDP solutions over the belief space. We evaluate the efficacy of the proposed methodology by evaluating the capability of model-formulation agnostic inference of DBMMs in benchmark problems that include discrete and continuous variables.

**Link**: [arxiv](http://arxiv.org/abs/2503.13438v1),  [pdf](http://arxiv.org/pdf/2503.13438v1)

**Tags**: cs.LG cs.AI 



### Unified Autoregressive Visual Generation and Understanding with   Continuous Tokens
**Authors**: Lijie Fan, Luming Tang, Siyang Qin, Tianhong Li, Xuan Yang, Siyuan Qiao, Andreas Steiner, Chen Sun, Yuanzhen Li, Tao Zhu, Michael Rubinstein, Michalis Raptis, Deqing Sun, Radu Soricut

**Updated**: 2025-03-17T17:58:30Z

**Summary**: We present UniFluid, a unified autoregressive framework for joint visual generation and understanding leveraging continuous visual tokens. Our unified autoregressive architecture processes multimodal image and text inputs, generating discrete tokens for text and continuous tokens for image. We find though there is an inherent trade-off between the image generation and understanding task, a carefully tuned training recipe enables them to improve each other. By selecting an appropriate loss balance weight, the unified model achieves results comparable to or exceeding those of single-task baselines on both tasks. Furthermore, we demonstrate that employing stronger pre-trained LLMs and random-order generation during training is important to achieve high-fidelity image generation within this unified framework. Built upon the Gemma model series, UniFluid exhibits competitive performance across both image generation and understanding, demonstrating strong transferability to various downstream tasks, including image editing for generation, as well as visual captioning and question answering for understanding.

**Link**: [arxiv](http://arxiv.org/abs/2503.13436v1),  [pdf](http://arxiv.org/pdf/2503.13436v1)

**Tags**: cs.CV cs.LG 



### Top General Performance = Top Domain Performance? DomainCodeBench: A   Multi-domain Code Generation Benchmark
**Authors**: Dewu Zheng, Yanlin Wang, Ensheng Shi, Xilin Liu, Yuchi Ma, Hongyu Zhang, Zibin Zheng

**Updated**: 2025-03-17T17:58:13Z

**Summary**: With the rapid advancement of large language models (LLMs), extensive research has been conducted to investigate the code generation capabilities of LLMs. However, existing efforts primarily focus on general-domain tasks, leaving LLMs' code generation performance in real-world application domains underexplored. This raises a critical question: can a model's general-domain coding ability reliably represent its ability in specialized domains? In this paper, we introduce DomainCodeBench, a multi-domain code generation benchmark designed to systematically evaluate LLMs across 12 software application domains and 15 programming languages. DomainCodeBench contains 2,400 manually verified tasks with ground truth, human-annotated docstrings, and fine-grained dependency information to ensure more coverage of domain-specific challenges. Specifically, we first identify the most popular application domains by topic mining. Then, we curate coding tasks based on commonly used frameworks and platforms in each domain. We obtain several findings through extensive experiments on DomainCodeBench with ten mainstream LLMs. (1) Performance decoupling: experiments reveal that top general-domain models do not consistently excel in specific application domains; (2) Domain-specific weaknesses: LLMs often fail due to domain knowledge gaps and third-party library misusage; (3) Contextual enhancement: we show that augmenting prompts with domain-specific knowledge improves performance by around 38.17%, providing actionable insights for performance optimization. Our replication package, including the benchmark, source code, and experimental results, is available at https://github.com/DeepSoftwareAnalytics/DomainCodeBench.

**Link**: [arxiv](http://arxiv.org/abs/2412.18573v2),  [pdf](http://arxiv.org/pdf/2412.18573v2)

**Tags**: cs.SE cs.AI cs.CL 



### xLSTM 7B: A Recurrent LLM for Fast and Efficient Inference
**Authors**: Maximilian Beck, Korbinian Pöppel, Phillip Lippe, Richard Kurle, Patrick M. Blies, Günter Klambauer, Sebastian Böck, Sepp Hochreiter

**Updated**: 2025-03-17T17:54:55Z

**Summary**: Recent breakthroughs in solving reasoning, math and coding problems with Large Language Models (LLMs) have been enabled by investing substantial computation budgets at inference time. Therefore, inference speed is one of the most critical properties of LLM architectures, and there is a growing need for LLMs that are efficient and fast at inference. Recently, LLMs built on the xLSTM architecture have emerged as a powerful alternative to Transformers, offering linear compute scaling with sequence length and constant memory usage, both highly desirable properties for efficient inference. However, such xLSTM-based LLMs have yet to be scaled to larger models and assessed and compared with respect to inference speed and efficiency. In this work, we introduce xLSTM 7B, a 7-billion-parameter LLM that combines xLSTM's architectural benefits with targeted optimizations for fast and efficient inference. Our experiments demonstrate that xLSTM 7B achieves performance on downstream tasks comparable to other similar-sized LLMs, while providing significantly faster inference speeds and greater efficiency compared to Llama- and Mamba-based LLMs. These results establish xLSTM 7B as the fastest and most efficient 7B LLM, offering a solution for tasks that require large amounts of test-time computation. Our work highlights xLSTM's potential as a foundational architecture for methods building on heavy use of LLM inference. Our model weights, model code and training code are open-source.

**Link**: [arxiv](http://arxiv.org/abs/2503.13427v1),  [pdf](http://arxiv.org/pdf/2503.13427v1)

**Tags**: cs.LG cs.AI cs.CL 



### SuperBPE: Space Travel for Language Models
**Authors**: Alisa Liu, Jonathan Hayase, Valentin Hofmann, Sewoong Oh, Noah A. Smith, Yejin Choi

**Updated**: 2025-03-17T17:53:23Z

**Summary**: The assumption across nearly all language model (LM) tokenization schemes is that tokens should be subwords, i.e., contained within word boundaries. While providing a seemingly reasonable inductive bias, is this common practice limiting the potential of modern LMs? Whitespace is not a reliable delimiter of meaning, as evidenced by multi-word expressions (e.g., "by the way"), crosslingual variation in the number of words needed to express a concept (e.g., "spacesuit helmet" in German is "raumanzughelm"), and languages that do not use whitespace at all (e.g., Chinese). To explore the potential of tokenization beyond subwords, we introduce a "superword" tokenizer, SuperBPE, which incorporates a simple pretokenization curriculum into the byte-pair encoding (BPE) algorithm to first learn subwords, then superwords that bridge whitespace. This brings dramatic improvements in encoding efficiency: when fixing the vocabulary size to 200k, SuperBPE encodes a fixed piece of text with up to 33% fewer tokens than BPE on average. In experiments, we pretrain 8B transformer LMs from scratch while fixing the model size, vocabulary size, and train compute, varying *only* the algorithm for learning the vocabulary. Our model trained with SuperBPE achieves an average +4.0% absolute improvement over the BPE baseline across 30 downstream tasks (including +8.2% on MMLU), while simultaneously requiring 27% less compute at inference time. In analysis, we find that SuperBPE results in segmentations of text that are more uniform in per-token difficulty. Qualitatively, this may be because SuperBPE tokens often capture common multi-word expressions that function semantically as a single unit. SuperBPE is a straightforward, local modification to tokenization that improves both encoding efficiency and downstream performance, yielding better language models overall.

**Link**: [arxiv](http://arxiv.org/abs/2503.13423v1),  [pdf](http://arxiv.org/pdf/2503.13423v1)

**Tags**: cs.CL cs.LG 



### Optimal Expert Selection for Distributed Mixture-of-Experts at the   Wireless Edge
**Authors**: Shengling Qin, Hai Wu, Hongyang Du, Kaibin Huang

**Updated**: 2025-03-17T17:51:49Z

**Summary**: The emergence of distributed Mixture-of-Experts (DMoE) systems, which deploy expert models at edge nodes, offers a pathway to achieving connected intelligence in sixth-generation (6G) mobile networks and edge artificial intelligence (AI). However, current DMoE systems lack an effective expert selection algorithm to address the simultaneous task-expert relevance and channel diversity inherent in these systems. Traditional AI or communication systems focus on either performance or channel conditions, and direct application of these methods leads to high communication overhead or low performance. To address this, we propose the DMoE protocol to schedule the expert inference and inter-expert transmission. This protocol identifies expert selection and subcarrier allocation as key optimization problems. We formulate an expert selection problem by incorporating both AI performance and channel conditions, and further extend it to a Joint Expert and Subcarrier Allocation (JESA) problem for comprehensive AI and channel management within the DMoE framework. For the NP-hard expert selection problem, we introduce the Dynamic Expert Selection (DES) algorithm, which leverages a linear relaxation as a bounding criterion to significantly reduce search complexity. For the JESA problem, we discover a unique structural property that ensures asymptotic optimality in most scenarios. We propose an iterative algorithm that addresses subcarrier allocation as a subproblem and integrates it with the DES algorithm. The proposed framework effectively manages the tradeoff between task relevance and channel conditions through a tunable importance factor, enabling flexible adaptation to diverse scenarios. Numerical experiments validate the dual benefits of the proposed expert selection algorithm: high performance and significantly reduced cost.

**Link**: [arxiv](http://arxiv.org/abs/2503.13421v1),  [pdf](http://arxiv.org/pdf/2503.13421v1)

**Tags**: cs.DC eess.SP 



### A Comprehensive Survey on Multi-Agent Cooperative Decision-Making:   Scenarios, Approaches, Challenges and Perspectives
**Authors**: Weiqiang Jin, Hongyang Du, Biao Zhao, Xingwu Tian, Bohang Shi, Guang Yang

**Updated**: 2025-03-17T17:45:46Z

**Summary**: With the rapid development of artificial intelligence, intelligent decision-making techniques have gradually surpassed human levels in various human-machine competitions, especially in complex multi-agent cooperative task scenarios. Multi-agent cooperative decision-making involves multiple agents working together to complete established tasks and achieve specific objectives. These techniques are widely applicable in real-world scenarios such as autonomous driving, drone navigation, disaster rescue, and simulated military confrontations. This paper begins with a comprehensive survey of the leading simulation environments and platforms used for multi-agent cooperative decision-making. Specifically, we provide an in-depth analysis for these simulation environments from various perspectives, including task formats, reward allocation, and the underlying technologies employed. Subsequently, we provide a comprehensive overview of the mainstream intelligent decision-making approaches, algorithms and models for multi-agent systems (MAS). Theseapproaches can be broadly categorized into five types: rule-based (primarily fuzzy logic), game theory-based, evolutionary algorithms-based, deep multi-agent reinforcement learning (MARL)-based, and large language models(LLMs)reasoning-based. Given the significant advantages of MARL andLLMs-baseddecision-making methods over the traditional rule, game theory, and evolutionary algorithms, this paper focuses on these multi-agent methods utilizing MARL and LLMs-based techniques. We provide an in-depth discussion of these approaches, highlighting their methodology taxonomies, advantages, and drawbacks. Further, several prominent research directions in the future and potential challenges of multi-agent cooperative decision-making are also detailed.

**Link**: [arxiv](http://arxiv.org/abs/2503.13415v1),  [pdf](http://arxiv.org/pdf/2503.13415v1)

**Tags**: cs.MA cs.AI 



### DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization   Framework from a Deep-Learning Perspective
**Authors**: Dengyun Peng, Yuhang Zhou, Qiguang Chen, Jinhao Liu, Jingjing Chen, Libo Qin

**Updated**: 2025-03-18T04:41:37Z

**Summary**: Large Language Models (LLMs) have achieved remarkable success across diverse tasks, largely driven by well-designed prompts. However, crafting and selecting such prompts often requires considerable human effort, significantly limiting its scalability. To mitigate this, recent studies have explored automated prompt optimization as a promising solution. Despite these efforts, existing methods still face critical challenges in robustness, efficiency, and generalization. To systematically address these challenges, we first conduct an empirical analysis to identify the limitations of current reflection-based prompt optimization paradigm. Building on these insights, we propose 7 innovative approaches inspired by traditional deep learning paradigms for prompt optimization (DLPO), seamlessly integrating these concepts into text-based gradient optimization. Through these advancements, we progressively tackle the aforementioned challenges and validate our methods through extensive experimentation. We hope our study not only provides valuable guidance for future research but also offers a comprehensive understanding of the challenges and potential solutions in prompt optimization. Our code is available at https://github.com/sfasfaffa/DLPO.

**Link**: [arxiv](http://arxiv.org/abs/2503.13413v2),  [pdf](http://arxiv.org/pdf/2503.13413v2)

**Tags**: cs.CL cs.AI 



### Toward Generative 6G Simulation: An Experimental Multi-Agent LLM and   ns-3 Integration
**Authors**: Farhad Rezazadeh, Amir Ashtari Gargari, Sandra Lagen, Houbing Song, Dusit Niyato, Lingjia Liu

**Updated**: 2025-03-17T17:34:04Z

**Summary**: The move toward open Sixth-Generation (6G) networks necessitates a novel approach to full-stack simulation environments for evaluating complex technology developments before prototyping and real-world implementation. This paper introduces an innovative approach\footnote{A lightweight, mock version of the code is available on GitHub at that combines a multi-agent framework with the Network Simulator 3 (ns-3) to automate and optimize the generation, debugging, execution, and analysis of complex 5G network scenarios. Our framework orchestrates a suite of specialized agents -- namely, the Simulation Generation Agent, Test Designer Agent, Test Executor Agent, and Result Interpretation Agent -- using advanced LangChain coordination. The Simulation Generation Agent employs a structured chain-of-thought (CoT) reasoning process, leveraging LLMs and retrieval-augmented generation (RAG) to translate natural language simulation specifications into precise ns-3 scripts. Concurrently, the Test Designer Agent generates comprehensive automated test suites by integrating knowledge retrieval techniques with dynamic test case synthesis. The Test Executor Agent dynamically deploys and runs simulations, managing dependencies and parsing detailed performance metrics. At the same time, the Result Interpretation Agent utilizes LLM-driven analysis to extract actionable insights from the simulation outputs. By integrating external resources such as library documentation and ns-3 testing frameworks, our experimental approach can enhance simulation accuracy and adaptability, reducing reliance on extensive programming expertise. A detailed case study using the ns-3 5G-LENA module validates the effectiveness of the proposed approach. The code generation process converges in an average of 1.8 iterations, has a syntax error rate of 17.0%, a mean response time of 7.3 seconds, and receives a human evaluation score of 7.5.

**Link**: [arxiv](http://arxiv.org/abs/2503.13402v1),  [pdf](http://arxiv.org/pdf/2503.13402v1)

**Tags**: cs.NI 



### MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based   Scientific Research
**Authors**: James Burgess, Jeffrey J Nirschl, Laura Bravo-Sánchez, Alejandro Lozano, Sanket Rajan Gupte, Jesus G. Galaz-Montoya, Yuhui Zhang, Yuchang Su, Disha Bhowmik, Zachary Coman, Sarina M. Hasan, Alexandra Johannesson, William D. Leineweber, Malvika G Nair, Ridhi Yarlagadda, Connor Zuraski, Wah Chiu, Sarah Cohen, Jan N. Hansen, Manuel D Leonetti, Chad Liu, Emma Lundberg, Serena Yeung-Levy

**Updated**: 2025-03-17T17:33:10Z

**Summary**: Scientific research demands sophisticated reasoning over multimodal data, a challenge especially prevalent in biology. Despite recent advances in multimodal large language models (MLLMs) for AI-assisted research, existing multimodal reasoning benchmarks only target up to college-level difficulty, while research-level benchmarks emphasize lower-level perception, falling short of the complex multimodal reasoning needed for scientific discovery. To bridge this gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark designed to assess three reasoning capabilities vital in research workflows: expert image understanding, hypothesis generation, and experiment proposal. MicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology experts across diverse microscopy modalities, ensuring VQA samples represent real scientific practice. In constructing the benchmark, we find that standard MCQ generation methods induce language shortcuts, motivating a new two-stage pipeline: an optimized LLM prompt structures question-answer pairs into MCQs; then, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking on state-of-the-art MLLMs reveal a peak performance of 53\%; models with smaller LLMs only slightly underperform top models, suggesting that language-based reasoning is less challenging than multimodal reasoning; and tuning with scientific articles enhances performance. Expert analysis of chain-of-thought responses shows that perception errors are the most frequent, followed by knowledge errors and then overgeneralization errors. These insights highlight the challenges in multimodal scientific reasoning, showing MicroVQA is a valuable resource advancing AI-driven biomedical research. MicroVQA is available at https://huggingface.co/datasets/jmhb/microvqa, and project page at https://jmhb0.github.io/microvqa.

**Link**: [arxiv](http://arxiv.org/abs/2503.13399v1),  [pdf](http://arxiv.org/pdf/2503.13399v1)

**Tags**: cs.CV cs.AI cs.CL cs.LG q-bio.CB 



### Cream of the Crop: Harvesting Rich, Scalable and Transferable   Multi-Modal Data for Instruction Fine-Tuning
**Authors**: Mengyao Lyu, Yan Li, Huasong Zhong, Wenhao Yang, Hui Chen, Jungong Han, Guiguang Ding, Zhenheng Yang

**Updated**: 2025-03-17T17:11:22Z

**Summary**: The hypothesis that pretrained large language models (LLMs) necessitate only minimal supervision during the fine-tuning (SFT) stage (Zhou et al., 2024) has been substantiated by recent advancements in data curation and selection research. However, their stability and generalizability are compromised due to the vulnerability to experimental setups and validation protocols, falling short of surpassing random sampling (Diddee & Ippolito, 2024; Xia et al., 2024b). Built upon LLMs, multi-modal LLMs (MLLMs), combined with the sheer token volume and heightened heterogeneity of data sources, amplify both the significance and complexity of data selection.   To harvest multi-modal instructional data in a robust and efficient manner, we re-define the granularity of the quality metric by decomposing it into 14 vision-language-related capabilities, and introduce multi-modal rich scorers to evaluate the capabilities of each data candidate. To promote diversity, in light of the inherent objective of the alignment stage, we take interaction style as diversity indicator and use a multi-modal rich styler to identify data instruction patterns. In doing so, our multi-modal rich scorers and styler (mmSSR) guarantee that high-scoring information is conveyed to users in diversified forms. Free from embedding-based clustering or greedy sampling, mmSSR efficiently scales to millions of data with varying budget constraints, supports customization for general or specific capability acquisition, and facilitates training-free generalization to new domains for curation. Across 10+ experimental settings, validated by 14 multi-modal benchmarks, we demonstrate consistent improvements over random sampling, baseline strategies and state-of-the-art selection methods, achieving 99.1% of full performance with only 30% of the 2.6M data.

**Link**: [arxiv](http://arxiv.org/abs/2503.13383v1),  [pdf](http://arxiv.org/pdf/2503.13383v1)

**Tags**: cs.CV cs.AI cs.CL cs.LG 



### TimeZero: Temporal Video Grounding with Reasoning-Guided LVLM
**Authors**: Ye Wang, Boshen Xu, Zihao Yue, Zihan Xiao, Ziheng Wang, Liang Zhang, Dingyi Yang, Wenxuan Wang, Qin Jin

**Updated**: 2025-03-17T17:04:20Z

**Summary**: We introduce TimeZero, a reasoning-guided LVLM designed for the temporal video grounding (TVG) task. This task requires precisely localizing relevant video segments within long videos based on a given language query. TimeZero tackles this challenge by extending the inference process, enabling the model to reason about video-language relationships solely through reinforcement learning. To evaluate the effectiveness of TimeZero, we conduct experiments on two benchmarks, where TimeZero achieves state-of-the-art performance on Charades-STA. Code is available at https://github.com/www-Ye/TimeZero.

**Link**: [arxiv](http://arxiv.org/abs/2503.13377v1),  [pdf](http://arxiv.org/pdf/2503.13377v1)

**Tags**: cs.CV cs.AI cs.CL 



### Sightation Counts: Leveraging Sighted User Feedback in Building a   BLV-aligned Dataset of Diagram Descriptions
**Authors**: Wan Ju Kang, Eunki Kim, Na Min An, Sangryul Kim, Haemin Choi, Ki Hoon Kwak, James Thorne

**Updated**: 2025-03-17T16:52:46Z

**Summary**: Often, the needs and visual abilities differ between the annotator group and the end user group. Generating detailed diagram descriptions for blind and low-vision (BLV) users is one such challenging domain. Sighted annotators could describe visuals with ease, but existing studies have shown that direct generations by them are costly, bias-prone, and somewhat lacking by BLV standards. In this study, we ask sighted individuals to assess -- rather than produce -- diagram descriptions generated by vision-language models (VLM) that have been guided with latent supervision via a multi-pass inference. The sighted assessments prove effective and useful to professional educators who are themselves BLV and teach visually impaired learners. We release Sightation, a collection of diagram description datasets spanning 5k diagrams and 137k samples for completion, preference, retrieval, question answering, and reasoning training purposes and demonstrate their fine-tuning potential in various downstream tasks.

**Link**: [arxiv](http://arxiv.org/abs/2503.13369v1),  [pdf](http://arxiv.org/pdf/2503.13369v1)

**Tags**: cs.AI cs.CV cs.HC 



### MAC: A Benchmark for Multiple Attributes Compositional Zero-Shot   Learning
**Authors**: Shuo Xu, Sai Wang, Xinyue Hu, Yutian Lin, Sibei Yang, Yu Wu

**Updated**: 2025-03-18T06:49:14Z

**Summary**: Compositional Zero-Shot Learning (CZSL) aims to learn semantic primitives (attributes and objects) from seen compositions and recognize unseen attribute-object compositions. Existing CZSL datasets focus on single attributes, neglecting the fact that objects naturally exhibit multiple interrelated attributes. Their narrow attribute scope and single attribute labeling introduce annotation biases, misleading the learning of attributes and causing inaccurate evaluation. To address these issues, we introduce the Multi-Attribute Composition (MAC) dataset, encompassing 22,838 images and 17,627 compositions with comprehensive and representative attribute annotations. MAC shows complex relationship between attributes and objects, with each attribute type linked to an average of 82.2 object types, and each object type associated with 31.4 attribute types. Based on MAC, we propose multi-attribute compositional zero-shot learning that requires deeper semantic understanding and advanced attribute associations, establishing a more realistic and challenging benchmark for CZSL. We also propose Multi-attribute Visual-Primitive Integrator (MVP-Integrator), a robust baseline for multi-attribute CZSL, which disentangles semantic primitives and performs effective visual-primitive association. Experimental results demonstrate that MVP-Integrator significantly outperforms existing CZSL methods on MAC with improved inference efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2406.12757v3),  [pdf](http://arxiv.org/pdf/2406.12757v3)

**Tags**: cs.CV 



### Euclid preparation. BAO analysis of photometric galaxy clustering in   configuration space
**Authors**: Euclid Collaboration, V. Duret, S. Escoffier, W. Gillard, I. Tutusaus, S. Camera, N. Tessore, F. J. Castander, N. Aghanim, A. Amara, L. Amendola, S. Andreon, N. Auricchio, C. Baccigalupi, M. Baldi, S. Bardelli, P. Battaglia, A. Biviano, D. Bonino, E. Branchini, M. Brescia, J. Brinchmann, A. Caillat, G. Cañas-Herrera, V. Capobianco, C. Carbone, V. F. Cardone, J. Carretero, S. Casas, M. Castellano, G. Castignani, S. Cavuoti, K. C. Chambers, A. Cimatti, C. Colodro-Conde, G. Congedo, C. J. Conselice, L. Conversi, Y. Copin, F. Courbin, H. M. Courtois, M. Cropper, A. Da Silva, H. Degaudenzi, S. de la Torre, G. De Lucia, A. M. Di Giorgio, H. Dole, F. Dubath, X. Dupac, S. Dusini, A. Ealet, M. Farina, R. Farinelli, S. Farrens, F. Faustini, S. Ferriol, F. Finelli, S. Fotopoulou, N. Fourmanoit, M. Frailis, E. Franceschi, M. Fumana, S. Galeotta, B. Gillis, C. Giocoli, J. Gracia-Carpio, A. Grazian, F. Grupp, S. V. H. Haugan, W. Holmes, F. Hormuth, A. Hornstrup, P. Hudelot, K. Jahnke, M. Jhabvala, B. Joachimi, E. Keihänen, S. Kermiche, A. Kiessling, M. Kilbinger, B. Kubik, M. Kunz, H. Kurki-Suonio, O. Lahav, A. M. C. Le Brun, S. Ligori, P. B. Lilje, V. Lindholm, I. Lloro, G. Mainetti, D. Maino, E. Maiorano, O. Mansutti, S. Marcin, O. Marggraf, K. Markovic, M. Martinelli, N. Martinet, F. Marulli, R. Massey, S. Maurogordato, E. Medinaceli, S. Mei, M. Melchior, Y. Mellier, M. Meneghetti, E. Merlin, G. Meylan, A. Mora, M. Moresco, B. Morin, L. Moscardini, E. Munari, R. Nakajima, C. Neissner, R. C. Nichol, S. -M. Niemi, C. Padilla, S. Paltani, F. Pasian, K. Pedersen, W. J. Percival, V. Pettorino, S. Pires, G. Polenta, M. Poncet, L. A. Popa, L. Pozzetti, F. Raison, R. Rebolo, J. Rhodes, G. Riccio, E. Romelli, M. Roncarelli, R. Saglia, Z. Sakr, D. Sapone, B. Sartoris, J. A. Schewtschenko, P. Schneider, T. Schrabback, A. Secroun, E. Sefusatti, G. Seidel, S. Serrano, P. Simon, C. Sirignano, G. Sirri, A. Spurio Mancini, L. Stanco, J. -L. Starck, J. Steinwagner, P. Tallada-Crespí, D. Tavagnacco, A. N. Taylor, I. Tereno, S. Toft, R. Toledo-Moreo, F. Torradeflot, J. Valiviita, T. Vassallo, G. Verdoes Kleijn, A. Veropalumbo, Y. Wang, J. Weller, G. Zamorani, F. M. Zerbi, E. Zucca, M. Bolzonella, C. Burigana, M. Calabrese, D. Di Ferdinando, J. A. Escartin Vigo, L. Gabarra, S. Matthew, N. Mauri, A. Pezzotta, M. Pöntinen, C. Porciani, V. Scottez, M. Tenti, M. Viel, M. Wiesmann, Y. Akrami, V. Allevato, I. T. Andika, M. Archidiacono, F. Atrio-Barandela, A. Balaguera-Antolinez, M. Ballardini, D. Bertacca, M. Bethermin, A. Blanchard, L. Blot, H. Böhringer, S. Borgani, M. L. Brown, S. Bruton, R. Cabanac, A. Calabro, B. Camacho Quevedo, A. Cappi, F. Caro, C. S. Carvalho, T. Castro, F. Cogato, S. Contarini, T. Contini, A. R. Cooray, S. Davini, F. De Paolis, G. Desprez, A. Díaz-Sánchez, S. Di Domizio, J. M. Diego, A. G. Ferrari, P. G. Ferreira, A. Finoguenov, K. Ganga, J. García-Bellido, T. Gasparetto, E. Gaztanaga, F. Giacomini, F. Gianotti, G. Gozaliasl, A. Gregorio, M. Guidi, C. M. Gutierrez, A. Hall, S. Hemmati, H. Hildebrandt, J. Hjorth, J. J. E. Kajava, Y. Kang, V. Kansal, D. Karagiannis, C. C. Kirkpatrick, S. Kruk, M. Lattanzi, M. Lembo, G. Leroy, J. Lesgourgues, T. I. Liaudat, S. J. Liu, A. Loureiro, G. Maggio, M. Magliocchetti, F. Mannucci, R. Maoli, J. Martín-Fleitas, C. J. A. P. Martins, L. Maurin, R. B. Metcalf, M. Miluzio, P. Monaco, C. Moretti, C. Murray, S. Nadathur, K. Naidoo, A. Navarro-Alsina, S. Nesseris, K. Paterson, A. Pisani, D. Potter, I. Risso, P. -F. Rocci, M. Sahlén, E. Sarpa, A. Schneider, D. Sciotti, E. Sellentin, M. Sereno, A. Silvestri, L. C. Smith, K. Tanidis, C. Tao, G. Testera, R. Teyssier, S. Tosi, A. Troja, M. Tucci, C. Valieri, A. Venhola, D. Vergani, F. Vernizzi, G. Verza, P. Vielzeuf, N. A. Walton

**Updated**: 2025-03-17T16:47:07Z

**Summary**: With about 1.5 billion galaxies expected to be observed, the very large number of objects in the Euclid photometric survey will allow for precise studies of galaxy clustering from a single survey, over a large range of redshifts $0.2 < z < 2.5$. In this work, we use photometric redshifts to extract the baryon acoustic oscillation signal (BAO) from the Flagship galaxy mock catalogue with a tomographic approach to constrain the evolution of the Universe and infer its cosmological parameters. We measure the two-point angular correlation function in 13 redshift bins. A template-fitting approach is applied to the measurement to extract the shift of the BAO peak through the transverse Alcock--Paczynski parameter $\alpha$. A joint analysis of all redshift bins is performed to constrain $\alpha$ at the effective redshift $z_\mathrm{eff}=0.77$ with MCMC and profile likelihood techniques. We also extract one $\alpha_i$ parameter per redshift bin to quantify its evolution as a function of time. From these 13 $\alpha_i$, which are directly proportional to the ratio $D_\mathrm{A}/\,r_\mathrm{s,\,drag}$, we constrain $h$, $\Omega_\mathrm{b}$, and $\Omega_\mathrm{cdm}$. From the joint analysis, we constrain $\alpha(z_\mathrm{eff}=0.77)=1.0011^{+0.0078}_{-0.0079}$, which represents a three-fold improvement over current constraints from the Dark Energy Survey. As expected, the constraining power in the analysis of each redshift bin is lower, with an uncertainty ranging from $\pm\,0.13$ to $\pm\,0.024$. From these results, we constrain $h$ at 0.45 %, $\Omega_\mathrm{b}$ at 0.91 %, and $\Omega_\mathrm{cdm}$ at 7.7 %. We quantify the influence of analysis choices like the template, scale cuts, redshift bins, and systematic effects like redshift-space distortions over our constraints both at the level of the extracted $\alpha_i$ parameters and at the level of cosmological inference.

**Link**: [arxiv](http://arxiv.org/abs/2503.11621v2),  [pdf](http://arxiv.org/pdf/2503.11621v2)

**Tags**: astro-ph.CO 



### Mitigating Visual Forgetting via Take-along Visual Conditioning for   Multi-modal Long CoT Reasoning
**Authors**: Hai-Long Sun, Zhun Sun, Houwen Peng, Han-Jia Ye

**Updated**: 2025-03-17T16:45:12Z

**Summary**: Recent advancements in Large Language Models (LLMs) have demonstrated enhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting to advanced, product-oriented solutions like OpenAI o1. During our re-implementation of this model, we noticed that in multimodal tasks requiring visual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to maintain focus on the visual information, in other words, MLLMs suffer from a gradual decline in attention to visual information as reasoning progresses, causing text-over-relied outputs. To investigate this, we ablate image inputs during long-chain reasoning. Concretely, we truncate the reasoning process midway, then re-complete the reasoning process with the input image removed. We observe only a ~2% accuracy drop on MathVista's test-hard subset, revealing the model's textual outputs dominate the following reasoning process. Motivated by this, we propose Take-along Visual Conditioning (TVC), a strategy that shifts image input to critical reasoning stages and compresses redundant visual tokens via dynamic pruning. This methodology helps the model retain attention to the visual components throughout the reasoning. Our approach achieves state-of-the-art performance on average across five mathematical reasoning benchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in enhancing multimodal reasoning systems.

**Link**: [arxiv](http://arxiv.org/abs/2503.13360v1),  [pdf](http://arxiv.org/pdf/2503.13360v1)

**Tags**: cs.CV cs.AI cs.LG 



### Agents Play Thousands of 3D Video Games
**Authors**: Zhongwen Xu, Xianliang Wang, Siyi Li, Tao Yu, Liang Wang, Qiang Fu, Wei Yang

**Updated**: 2025-03-17T16:42:34Z

**Summary**: We present PORTAL, a novel framework for developing artificial intelligence agents capable of playing thousands of 3D video games through language-guided policy generation. By transforming decision-making problems into language modeling tasks, our approach leverages large language models (LLMs) to generate behavior trees represented in domain-specific language (DSL). This method eliminates the computational burden associated with traditional reinforcement learning approaches while preserving strategic depth and rapid adaptability. Our framework introduces a hybrid policy structure that combines rule-based nodes with neural network components, enabling both high-level strategic reasoning and precise low-level control. A dual-feedback mechanism incorporating quantitative game metrics and vision-language model analysis facilitates iterative policy improvement at both tactical and strategic levels. The resulting policies are instantaneously deployable, human-interpretable, and capable of generalizing across diverse gaming environments. Experimental results demonstrate PORTAL's effectiveness across thousands of first-person shooter (FPS) games, showcasing significant improvements in development efficiency, policy generalization, and behavior diversity compared to traditional approaches. PORTAL represents a significant advancement in game AI development, offering a practical solution for creating sophisticated agents that can operate across thousands of commercial video games with minimal development overhead. Experiment results on the 3D video games are best viewed on https://zhongwen.one/projects/portal .

**Link**: [arxiv](http://arxiv.org/abs/2503.13356v1),  [pdf](http://arxiv.org/pdf/2503.13356v1)

**Tags**: cs.LG 



### Spatio-Temporal Distortion Aware Omnidirectional Video Super-Resolution
**Authors**: Hongyu An, Xinfeng Zhang, Shijie Zhao, Li Zhang, Ruiqin Xiong

**Updated**: 2025-03-17T16:22:15Z

**Summary**: Omnidirectional video (ODV) provides an immersive visual experience and is widely utilized in virtual reality and augmented reality. However, restricted capturing devices and transmission bandwidth lead to low-resolution ODVs. Video super-resolution (SR) is proposed to enhance resolution, but practical ODV spatial projection distortions and temporal flickering are not well addressed directly applying existing methods. To achieve better ODV-SR reconstruction, we propose a Spatio-Temporal Distortion Aware Network (STDAN) oriented to ODV characteristics. Specifically, a spatially continuous distortion modulation module is introduced to improve discrete projection distortions. Next, we design an interlaced multi-frame reconstruction mechanism to refine temporal consistency across frames. Furthermore, we incorporate latitude-saliency adaptive weights during training to concentrate on regions with higher texture complexity and human-watching interest. In general, we explore inference-free and real-world viewing matched strategies to provide an application-friendly method on a novel ODV-SR dataset with practical scenarios. Extensive experimental results demonstrate the superior performance of the proposed STDAN over state-of-the-art methods.

**Link**: [arxiv](http://arxiv.org/abs/2410.11506v2),  [pdf](http://arxiv.org/pdf/2410.11506v2)

**Tags**: cs.CV 



### STEP: Simultaneous Tracking and Estimation of Pose for Animals and   Humans
**Authors**: Shashikant Verma, Harish Katti, Soumyaratna Debnath, Yamuna Swamy, Shanmuganathan Raman

**Updated**: 2025-03-17T16:22:00Z

**Summary**: We introduce STEP, a novel framework utilizing Transformer-based discriminative model prediction for simultaneous tracking and estimation of pose across diverse animal species and humans. We are inspired by the fact that the human brain exploits spatiotemporal continuity and performs concurrent localization and pose estimation despite the specialization of brain areas for form and motion processing. Traditional discriminative models typically require predefined target states for determining model weights, a challenge we address through Gaussian Map Soft Prediction (GMSP) and Offset Map Regression Adapter (OMRA) Modules. These modules remove the necessity of keypoint target states as input, streamlining the process. Our method starts with a known target state initialized through a pre-trained detector or manual initialization in the initial frame of a given video sequence. It then seamlessly tracks the target and estimates keypoints of anatomical importance as output for subsequent frames. Unlike prevalent top-down pose estimation methods, our approach doesn't rely on per-frame target detections due to its tracking capability. This facilitates a significant advancement in inference efficiency and potential applications. We train and validate our approach on datasets encompassing diverse species. Our experiments demonstrate superior results compared to existing methods, opening doors to various applications, including but not limited to action recognition and behavioral analysis.

**Link**: [arxiv](http://arxiv.org/abs/2503.13344v1),  [pdf](http://arxiv.org/pdf/2503.13344v1)

**Tags**: cs.CV 



### LearnMate: Enhancing Online Education with LLM-Powered Personalized   Learning Plans and Support
**Authors**: Xinyu Jessica Wang, Christine Lee, Bilge Mutlu

**Updated**: 2025-03-17T16:18:23Z

**Summary**: With the increasing prevalence of online learning, adapting education to diverse learner needs remains a persistent challenge. Recent advancements in artificial intelligence (AI), particularly large language models (LLMs), promise powerful tools and capabilities to enhance personalized learning in online educational environments. In this work, we explore how LLMs can improve personalized learning experiences by catering to individual user needs toward enhancing the overall quality of online education. We designed personalization guidelines based on the growing literature on personalized learning to ground LLMs in generating tailored learning plans. To operationalize these guidelines, we implemented LearnMate, an LLM-based system that generates personalized learning plans and provides users with real-time learning support. We discuss the implications and future directions of this work, aiming to move beyond the traditional one-size-fits-all approach by integrating LLM-based personalized support into online learning environments.

**Link**: [arxiv](http://arxiv.org/abs/2503.13340v1),  [pdf](http://arxiv.org/pdf/2503.13340v1)

**Tags**: cs.HC 



### Reliable and Efficient Amortized Model-based Evaluation
**Authors**: Sang Truong, Yuheng Tu, Percy Liang, Bo Li, Sanmi Koyejo

**Updated**: 2025-03-17T16:15:02Z

**Summary**: Comprehensive evaluations of language models (LM) during both development and deployment phases are necessary because these models possess numerous capabilities (e.g., mathematical reasoning, legal support, or medical diagnostic) as well as safety risks (e.g., racial bias, toxicity, or misinformation). The average score across a wide range of benchmarks provides a signal that helps guide the use of these LMs in practice. Currently, holistic evaluations are costly due to the large volume of benchmark questions, making frequent evaluations impractical. A popular attempt to lower the cost is to compute the average score on a subset of the benchmark. This approach, unfortunately, often renders an unreliable measure of LM performance because the average score is often confounded with the difficulty of the questions in the benchmark subset. Item response theory (IRT) was designed to address this challenge, providing a reliable measurement by careful controlling for question difficulty. Unfortunately, question difficulty is expensive to estimate. Facing this challenge, we train a model that predicts question difficulty from its content, enabling a reliable measurement at a fraction of the cost. In addition, we leverage this difficulty predictor to further improve the evaluation efficiency through training a question generator given a difficulty level. This question generator is essential in adaptive testing, where, instead of using a random subset of the benchmark questions, informative questions are adaptively chosen based on the current estimation of LLM performance. Experiments on 22 common natural language benchmarks and 172 LMs show that this approach is more reliable and efficient compared to current common practice.

**Link**: [arxiv](http://arxiv.org/abs/2503.13335v1),  [pdf](http://arxiv.org/pdf/2503.13335v1)

**Tags**: cs.CL cs.AI cs.LG stat.AP 



### LEAVS: An LLM-based Labeler for Abdominal CT Supervision
**Authors**: Ricardo Bigolin Lanfredi, Yan Zhuang, Mark Finkelstein, Praveen Thoppey Srinivasan Balamuralikrishna, Luke Krembs, Brandon Khoury, Arthi Reddy, Pritam Mukherjee, Neil M. Rofsky, Ronald M. Summers

**Updated**: 2025-03-17T16:09:22Z

**Summary**: Extracting structured labels from radiology reports has been employed to create vision models to simultaneously detect several types of abnormalities. However, existing works focus mainly on the chest region. Few works have been investigated on abdominal radiology reports due to more complex anatomy and a wider range of pathologies in the abdomen. We propose LEAVS (Large language model Extractor for Abdominal Vision Supervision). This labeler can annotate the certainty of presence and the urgency of seven types of abnormalities for nine abdominal organs on CT radiology reports. To ensure broad coverage, we chose abnormalities that encompass most of the finding types from CT reports. Our approach employs a specialized chain-of-thought prompting strategy for a locally-run LLM using sentence extraction and multiple-choice questions in a tree-based decision system. We demonstrate that the LLM can extract several abnormality types across abdominal organs with an average F1 score of 0.89, significantly outperforming competing labelers and humans. Additionally, we show that extraction of urgency labels achieved performance comparable to human annotations. Finally, we demonstrate that the abnormality labels contain valuable information for training a single vision model that classifies several organs as normal or abnormal. We release our code and structured annotations for a public CT dataset containing over 1,000 CT volumes.

**Link**: [arxiv](http://arxiv.org/abs/2503.13330v1),  [pdf](http://arxiv.org/pdf/2503.13330v1)

**Tags**: eess.IV cs.AI cs.CV 



### Nullu: Mitigating Object Hallucinations in Large Vision-Language Models   via HalluSpace Projection
**Authors**: Le Yang, Ziwei Zheng, Boxu Chen, Zhengyu Zhao, Chenhao Lin, Chao Shen

**Updated**: 2025-03-17T16:05:34Z

**Summary**: Recent studies have shown that large vision-language models (LVLMs) often suffer from the issue of object hallucinations (OH). To mitigate this issue, we introduce an efficient method that edits the model weights based on an unsafe subspace, which we call HalluSpace in this paper. With truthful and hallucinated text prompts accompanying the visual content as inputs, the HalluSpace can be identified by extracting the hallucinated embedding features and removing the truthful representations in LVLMs. By orthogonalizing the model weights, input features will be projected into the Null space of the HalluSpace to reduce OH, based on which we name our method Nullu. We reveal that HalluSpaces generally contain prior information in the large language models (LLMs) applied to build LVLMs, which have been shown as essential causes of OH in previous studies. Therefore, null space projection suppresses the LLMs' priors to filter out the hallucinated features, resulting in contextually accurate outputs. Experiments show that our method can effectively mitigate OH across different LVLM families without extra inference costs and also show strong performance in general LVLM benchmarks. Code is released at https://github.com/Ziwei-Zheng/Nullu.

**Link**: [arxiv](http://arxiv.org/abs/2412.13817v3),  [pdf](http://arxiv.org/pdf/2412.13817v3)

**Tags**: cs.CV 



### Detection of millimeter-wave coronal emission in a quasar at   cosmological distance using microlensing
**Authors**: M. Rybak, D. Sluse, K. Gupta, M. Millon, E. Behar, F. Courbin, J. P. McKean, H. R. Stacey

**Updated**: 2025-03-17T15:52:53Z

**Summary**: Determining the nature of emission processes at the heart of quasars is critical for understanding environments of supermassive black holes. One of the key open questions is the origin of long-wavelength emission from radio-quiet quasars. The proposed mechanisms span a broad range, from central star formation to dusty torus, low-power jets, or coronal emission from the innermost accretion disk. Distinguishing between these scenarios requires probing spatial scales $\leq$0.01 pc, beyond the reach of any current millimetre-wave telescope. Fortunately, in gravitationally lensed quasars, compact mm-wave emission might be microlensed by stars in the foreground galaxy, providing strong constraints on the source size. We report a striking change in rest-frame 1.3-mm flux-ratios in RXJ1131-1231, a quadruply-lensed quasar at z = 0.658, observed by the Atacama Large Millimeter/submillimeter Array (ALMA) in 2015 and 2020. The observed flux-ratio variability is consistent with microlensing of a very compact source with a half-light radius $\leq$50 astronomical units. The compactness of the source leaves coronal emission as the most likely scenario. Furthermore, the inferred mm-wave and X-ray luminosities follow the characteristic G\"udel-Benz relationship for coronal emission. These observations represent the first unambiguous evidence for coronae as the dominant mechanism for long-wavelength emission in radio-quiet quasars.

**Link**: [arxiv](http://arxiv.org/abs/2503.13313v1),  [pdf](http://arxiv.org/pdf/2503.13313v1)

**Tags**: astro-ph.GA 



### Leveraging Large Language Models for Collective Decision-Making
**Authors**: Marios Papachristou, Longqi Yang, Chin-Chia Hsu

**Updated**: 2025-03-17T15:50:13Z

**Summary**: In various work contexts, such as meeting scheduling, collaborating, and project planning, collective decision-making is essential but often challenging due to diverse individual preferences, varying work focuses, and power dynamics among members. To address this, we propose a system leveraging Large Language Models (LLMs) to facilitate group decision-making by managing conversations and balancing preferences among individuals. Our system aims to extract individual preferences from each member's conversation with the system and suggest options that satisfy the preferences of the members. We specifically apply this system to corporate meeting scheduling. We create synthetic employee profiles and simulate conversations at scale, leveraging LLMs to evaluate the system performance as a novel approach to conducting a user study. Our results indicate efficient coordination with reduced interactions between the members and the LLM-based system. The system refines and improves its proposed options over time, ensuring that many of the members' individual preferences are satisfied in an equitable way. Finally, we conduct a survey study involving human participants to assess our system's ability to aggregate preferences and reasoning about them. Our findings show that the system exhibits strong performance in both dimensions.

**Link**: [arxiv](http://arxiv.org/abs/2311.04928v3),  [pdf](http://arxiv.org/pdf/2311.04928v3)

**Tags**: cs.CL cs.AI cs.HC cs.SI 



### Computation Mechanism Behind LLM Position Generalization
**Authors**: Chi Han, Heng Ji

**Updated**: 2025-03-17T15:47:37Z

**Summary**: Most written natural languages are composed of sequences of words and sentences. Similar to humans, large language models (LLMs) exhibit flexibility in handling textual positions - a phenomenon we term position generalization. They can understand texts with position perturbations and generalize to longer texts than those encountered during training with the latest techniques. These phenomena suggest that LLMs handle positions tolerantly, but how LLMs computationally process positional relevance remains largely unexplored. This work connects the linguistic phenomenon with LLMs' computational mechanisms. We show how LLMs enforce certain computational mechanisms for the aforementioned tolerance in position perturbations. Despite the complex design of the self-attention mechanism, this work reveals that LLMs learn a counterintuitive disentanglement of attention logits. Their values show a 0.959 linear correlation with an approximation of the arithmetic sum of positional relevance and semantic importance. Furthermore, we identify a prevalent pattern in intermediate features, which we prove theoretically enables this effect. The pattern, which is different from how randomly initialized parameters would behave, suggests that it is a learned behavior rather than a natural result of the model architecture. Based on these findings, we provide computational explanations and criteria for LLMs' position flexibilities. This work takes a pioneering step in linking position generalization with modern LLMs' internal mechanisms.

**Link**: [arxiv](http://arxiv.org/abs/2503.13305v1),  [pdf](http://arxiv.org/pdf/2503.13305v1)

**Tags**: cs.CL cs.AI 



### LIMCA: LLM for Automating Analog In-Memory Computing Architecture Design   Exploration
**Authors**: Deepak Vungarala, Md Hasibul Amin, Pietro Mercati, Arnob Ghosh, Arman Roohi, Ramtin Zand, Shaahin Angizi

**Updated**: 2025-03-17T15:45:17Z

**Summary**: Resistive crossbars enabling analog In-Memory Computing (IMC) have emerged as a promising architecture for Deep Neural Network (DNN) acceleration, offering high memory bandwidth and in-situ computation. However, the manual, knowledge-intensive design process and the lack of high-quality circuit netlists have significantly constrained design space exploration and optimization to behavioral system-level tools. In this work, we introduce LIMCA, a novel fine-tune-free Large Language Model (LLM)-driven framework for automating the design and evaluation of IMC crossbar architectures. Unlike traditional approaches, LIMCA employs a No-Human-In-Loop (NHIL) automated pipeline to generate and validate circuit netlists for SPICE simulations, eliminating manual intervention. LIMCA systematically explores the IMC design space by leveraging a structured dataset and LLM-based performance evaluation. Our experimental results on MNIST classification demonstrate that LIMCA successfully generates crossbar designs achieving $\geq$96% accuracy while maintaining a power consumption $\leq$3W, making this the first work in LLM-assisted IMC design space exploration. Compared to existing frameworks, LIMCA provides an automated, scalable, and hardware-aware solution, reducing design exploration time while ensuring user-constrained performance trade-offs.

**Link**: [arxiv](http://arxiv.org/abs/2503.13301v1),  [pdf](http://arxiv.org/pdf/2503.13301v1)

**Tags**: cs.AR 



### A Survey on Transformer Context Extension: Approaches and Evaluation
**Authors**: Yijun Liu, Jinzheng Yu, Yang Xu, Zhongyang Li, Qingfu Zhu

**Updated**: 2025-03-17T15:44:09Z

**Summary**: Large language models (LLMs) based on Transformer have been widely applied in the filed of natural language processing (NLP), demonstrating strong performance, particularly in handling short text tasks. However, when it comes to long context scenarios, the performance of LLMs degrades due to some challenges. To alleviate this phenomenon, there is a number of work proposed recently. In this survey, we first list the challenges of applying pre-trained LLMs to process long contexts. Then systematically review the approaches related to long context and propose our taxonomy categorizing them into four main types: positional encoding, context compression, retrieval augmented, and attention pattern. In addition to the approaches, we focus on the evaluation of long context, organizing relevant data, tasks, and metrics based on existing long context benchmarks. Finally, we summarize unresolved issues in the long context domain and put forward our views on future developments.

**Link**: [arxiv](http://arxiv.org/abs/2503.13299v1),  [pdf](http://arxiv.org/pdf/2503.13299v1)

**Tags**: cs.CL cs.AI 



### Quantitative Image-Based Validation Framework for Assessing Global   Coronal Magnetic Field Models
**Authors**: Christopher E. Rura, Vadim M. Uritsky, Shaela I. Jones, Cooper Downs, Nathalia Alzate, Charles N. Arge

**Updated**: 2025-03-17T15:39:43Z

**Summary**: Coronagraph observations provide key information about the orientation of the Sun's magnetic field. Previous studies used quasi-radial features detected in coronagraph images to improve coronal magnetic field models by comparing the orientation of the features to the projected orientation of the model field. Various algorithms segment these coronal features to approximate the local plane-of-sky geometry of the coronal magnetic field, and their orientation can be used as input for optimizing and constraining coronal magnetic field models. We present a new framework that allows for further quantitative evaluations of image-based coronal segmentation methods against magnetic field models, and vice-versa. We compare quasi-radial features identified from QRaFT, a global coronal feature tracing algorithm, in white-light coronagraph images to outputs of MAS, an advanced magnetohydrodynamic model. We use the FORWARD toolset to produce synthetic polarized brightness images co-aligned to real coronagraph observations, segment features in these images, and quantify the difference between the inferred and model magnetic field. This approach allows us to geometrically compare features segmented in artificial images to those segmented in white-light coronagraph observations against the plane-of-sky projected MAS coronal magnetic field. We quantify QRaFT's performance in the artificial images and observational data, and perform statistical analyses that measure the similarity of these results to compute the accuracy and uncertainty of the model output to the observational data. The results demonstrate through a quantitative evaluation that a coronal segmentation method identifies the global large-scale orientation of the coronal magnetic field within $\sim\pm10^\circ$ of the plane-of-sky projected MAS magnetic field.

**Link**: [arxiv](http://arxiv.org/abs/2503.13292v1),  [pdf](http://arxiv.org/pdf/2503.13292v1)

**Tags**: astro-ph.SR 



### $φ$-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time   Exploration and Exploitation
**Authors**: Fangzhi Xu, Hang Yan, Chang Ma, Haiteng Zhao, Jun Liu, Qika Lin, Zhiyong Wu

**Updated**: 2025-03-17T15:38:33Z

**Summary**: Inference-time optimization scales computation to derive deliberate reasoning steps for effective performance. While previous search-based strategies address the short-sightedness of auto-regressive generation, the vast search space leads to excessive exploration and insufficient exploitation. To strike an efficient balance to derive the optimal step, we frame the decoding strategy as foresight sampling, leveraging simulated future steps to obtain globally optimal step estimation. Built on it, we propose a novel decoding strategy, named $\phi$-Decoding. To provide a precise and expressive estimation of step value, $\phi$-Decoding approximates two distributions via foresight and clustering. Sampling from the joint distribution, the optimal steps can be selected for exploitation. To support adaptive computation allocation, we propose in-width and in-depth pruning strategies, featuring a light-weight solution to achieve inference efficiency. Extensive experiments across seven benchmarks show $\phi$-Decoding outperforms strong baselines in both performance and efficiency. Additional analysis demonstrates its generalization across various LLMs and scalability across a wide range of computing budgets. The code will be released at https://github.com/xufangzhi/phi-Decoding, and the open-source PyPI package is coming soon.

**Link**: [arxiv](http://arxiv.org/abs/2503.13288v1),  [pdf](http://arxiv.org/pdf/2503.13288v1)

**Tags**: cs.LG cs.AI cs.CL 



### LLM-Match: An Open-Sourced Patient Matching Model Based on Large   Language Models and Retrieval-Augmented Generation
**Authors**: Xiaodi Li, Shaika Chowdhury, Chung Il Wi, Maria Vassilaki, Ken Liu, Terence T Sio, Owen Garrick, Young J Juhn, James R Cerhan, Cui Tao, Nansu Zong

**Updated**: 2025-03-17T15:31:55Z

**Summary**: Patient matching is the process of linking patients to appropriate clinical trials by accurately identifying and matching their medical records with trial eligibility criteria. We propose LLM-Match, a novel framework for patient matching leveraging fine-tuned open-source large language models. Our approach consists of four key components. First, a retrieval-augmented generation (RAG) module extracts relevant patient context from a vast pool of electronic health records (EHRs). Second, a prompt generation module constructs input prompts by integrating trial eligibility criteria (both inclusion and exclusion criteria), patient context, and system instructions. Third, a fine-tuning module with a classification head optimizes the model parameters using structured prompts and ground-truth labels. Fourth, an evaluation module assesses the fine-tuned model's performance on the testing datasets. We evaluated LLM-Match on four open datasets, n2c2, SIGIR, TREC 2021, and TREC 2022, using open-source models, comparing it against TrialGPT, Zero-Shot, and GPT-4-based closed models. LLM-Match outperformed all baselines.

**Link**: [arxiv](http://arxiv.org/abs/2503.13281v1),  [pdf](http://arxiv.org/pdf/2503.13281v1)

**Tags**: cs.CL cs.AI cs.LG 



### Goal2Story: A Multi-Agent Fleet based on Privately Enabled sLLMs for   Impacting Mapping on Requirements Elicitation
**Authors**: Xinkai Zou, Yan Liu, Xiongbo Shi, Chen Yang

**Updated**: 2025-03-17T15:31:20Z

**Summary**: As requirements drift with rapid iterations, agile development becomes the dominant paradigm. Goal-driven Requirements Elicitation (RE) is a pivotal yet challenging task in agile project development due to its heavy tangling with adaptive planning and efficient collaboration. Recently, AI agents have shown promising ability in supporting requirements analysis by saving significant time and effort for stakeholders. However, current research mainly focuses on functional RE, and research works have not been reported bridging the long journey from goal to user stories. Moreover, considering the cost of LLM facilities and the need for data and idea protection, privately hosted small-sized LLM should be further utilized in RE. To address these challenges, we propose Goal2Story, a multi-agent fleet that adopts the Impact Mapping (IM) framework while merely using cost-effective sLLMs for goal-driven RE. Moreover, we introduce a StorySeek dataset that contains over 1,000 user stories (USs) with corresponding goals and project context information, as well as the semi-automatic dataset construction method. For evaluation, we proposed two metrics: Factuality Hit Rate (FHR) to measure consistency between the generated USs with the dataset and Quality And Consistency Evaluation (QuACE) to evaluate the quality of the generated USs. Experimental results demonstrate that Goal2Story outperforms the baseline performance of the Super-Agent adopting powerful LLMs, while also showcasing the performance improvements in key metrics brought by CoT and Agent Profile to Goal2Story, as well as its exploration in identifying latent needs.

**Link**: [arxiv](http://arxiv.org/abs/2503.13279v1),  [pdf](http://arxiv.org/pdf/2503.13279v1)

**Tags**: cs.SE cs.AI 



### Knowledge-Aware Iterative Retrieval for Multi-Agent Systems
**Authors**: Seyoung Song

**Updated**: 2025-03-17T15:27:02Z

**Summary**: We introduce a novel large language model (LLM)-driven agent framework, which iteratively refines queries and filters contextual evidence by leveraging dynamically evolving knowledge. A defining feature of the system is its decoupling of external sources from an internal knowledge cache that is progressively updated to guide both query generation and evidence selection. This design mitigates bias-reinforcement loops and enables dynamic, trackable search exploration paths, thereby optimizing the trade-off between exploring diverse information and maintaining accuracy through autonomous agent decision-making. Our approach is evaluated on a broad range of open-domain question answering benchmarks, including multi-step tasks that mirror real-world scenarios where integrating information from multiple sources is critical, especially given the vulnerabilities of LLMs that lack explicit reasoning or planning capabilities. The results show that the proposed system not only outperforms single-step baselines regardless of task difficulty but also, compared to conventional iterative retrieval methods, demonstrates pronounced advantages in complex tasks through precise evidence-based reasoning and enhanced efficiency. The proposed system supports both competitive and collaborative sharing of updated context, enabling multi-agent extension. The benefits of multi-agent configurations become especially prominent as task difficulty increases. The number of convergence steps scales with task difficulty, suggesting cost-effective scalability.

**Link**: [arxiv](http://arxiv.org/abs/2503.13275v1),  [pdf](http://arxiv.org/pdf/2503.13275v1)

**Tags**: cs.AI cs.IR I.2.0; I.2.7; I.2.11; H.3.3 



### Masala-CHAI: A Large-Scale SPICE Netlist Dataset for Analog Circuits by   Harnessing AI
**Authors**: Jitendra Bhandari, Vineet Bhat, Yuheng He, Hamed Rahmani, Siddharth Garg, Ramesh Karri

**Updated**: 2025-03-17T15:22:28Z

**Summary**: Masala-CHAI is the first fully automated framework leveraging large language models (LLMs) to generate Simulation Programs with Integrated Circuit Emphasis (SPICE) netlists. It addresses a long-standing challenge in automating netlist generation for analog circuits within circuit design automation. Automating this workflow could accelerate the creation of finetuned LLMs for analog circuit design and verification. We identify key challenges in this automation and evaluate the multi-modal capabilities of state-of-the-art LLMs, particularly GPT-4, to address these issues. We propose a three-step workflow to overcome current limitations: labeling analog circuits, prompt tuning, and netlist verification. This approach aims to create an end-to-end SPICE netlist generator from circuit schematic images, tackling the long-standing hurdle of accurate netlist generation. Our framework demonstrates significant performance improvements, tested on approximately 2,100 schematics of varying complexity. We open-source this solution for community-driven development.

**Link**: [arxiv](http://arxiv.org/abs/2411.14299v4),  [pdf](http://arxiv.org/pdf/2411.14299v4)

**Tags**: cs.AR 



### DAgent: A Relational Database-Driven Data Analysis Report Generation   Agent
**Authors**: Wenyi Xu, Yuren Mao, Xiaolu Zhang, Chao Zhang, Xuemei Dong, Mengfei Zhang, Jun Zhou, Yunjun Gao

**Updated**: 2025-03-17T15:22:19Z

**Summary**: Relational database-driven data analysis (RDB-DA) report generation, which aims to generate data analysis reports after querying relational databases, has been widely applied in fields such as finance and healthcare. Typically, these tasks are manually completed by data scientists, making the process very labor-intensive and showing a clear need for automation. Although existing methods (e.g., Table QA or Text-to-SQL) have been proposed to reduce human dependency, they cannot handle complex analytical tasks that require multi-step reasoning, cross-table associations, and synthesizing insights into reports. Moreover, there is no dataset available for developing automatic RDB-DA report generation. To fill this gap, this paper proposes an LLM agent system for RDB-DA report generation tasks, dubbed DAgent; moreover, we construct a benchmark for automatic data analysis report generation, which includes a new dataset DA-Dataset and evaluation metrics. DAgent integrates planning, tools, and memory modules to decompose natural language questions into logically independent sub-queries, accurately retrieve key information from relational databases, and generate analytical reports that meet the requirements of completeness, correctness, and conciseness through multi-step reasoning and effective data integration. Experimental analysis on the DA-Dataset demonstrates that DAgent's superiority in retrieval performance and analysis report generation quality, showcasing its strong potential for tackling complex database analysis report generation tasks.

**Link**: [arxiv](http://arxiv.org/abs/2503.13269v1),  [pdf](http://arxiv.org/pdf/2503.13269v1)

**Tags**: cs.DB 



### On the accuracy of posterior recovery with neural network emulators
**Authors**: H. T. J. Bevins, T. Gessey-Jones, W. J. Handley

**Updated**: 2025-03-17T15:17:15Z

**Summary**: Neural network emulators are widely used in astrophysics and cosmology to approximate complex simulations inside Bayesian inference loops. Ad hoc rules of thumb are often used to justify the emulator accuracy required for reliable posterior recovery. We provide a theoretically motivated limit on the maximum amount of incorrect information inferred by using an emulator with a given accuracy. Under assumptions of linearity in the model, uncorrelated noise in the data and a Gaussian likelihood function, we demonstrate that the difference between the true underlying posterior and the recovered posterior can be quantified via a Kullback-Leibler divergence. We demonstrate how this limit can be used in the field of 21-cm cosmology by comparing the posteriors recovered when fitting mock data sets generated with the 1D radiative transfer code ARES directly with the simulation code and separately with an emulator. This paper is partly in response to and builds upon recent discussions in the literature which call into question the use of emulators in Bayesian inference pipelines. Upon repeating some aspects of these analyses, we find these concerns quantitatively unjustified, with accurate posterior recovery possible even when the mean RMSE error for the emulator is approximately 20% of the magnitude of the noise in the data. For the purposes of community reproducibility, we make our analysis code public at this link https://github.com/htjb/validating_posteriors.

**Link**: [arxiv](http://arxiv.org/abs/2503.13263v1),  [pdf](http://arxiv.org/pdf/2503.13263v1)

**Tags**: astro-ph.CO astro-ph.IM 



### GraphRouter: A Graph-based Router for LLM Selections
**Authors**: Tao Feng, Yanzhen Shen, Jiaxuan You

**Updated**: 2025-03-17T15:08:47Z

**Summary**: The rapidly growing number and variety of Large Language Models (LLMs) present significant challenges in efficiently selecting the appropriate LLM for a given query, especially considering the trade-offs between performance and computational cost. Current LLM selection methods often struggle to generalize across new LLMs and different tasks because of their limited ability to leverage contextual interactions among tasks, queries, and LLMs, as well as their dependence on a transductive learning framework. To address these shortcomings, we introduce a novel inductive graph framework, named as GraphRouter, which fully utilizes the contextual information among tasks, queries, and LLMs to enhance the LLM selection process. GraphRouter constructs a heterogeneous graph comprising task, query, and LLM nodes, with interactions represented as edges, which efficiently captures the contextual information between the query's requirements and the LLM's capabilities. Through an innovative edge prediction mechanism, GraphRouter is able to predict attributes (the effect and cost of LLM response) of potential edges, allowing for optimized recommendations that adapt to both existing and newly introduced LLMs without requiring retraining. Comprehensive experiments across three distinct effect-cost weight scenarios have shown that GraphRouter substantially surpasses existing routers, delivering a minimum performance improvement of 12.3%. In addition, it achieves enhanced generalization across new LLMs settings and supports diverse tasks with at least a 9.5% boost in effect and a significant reduction in computational demands. This work endeavors to apply a graph-based approach for the contextual and adaptive selection of LLMs, offering insights for real-world applications. Our codes for GraphRouter is released at https://github.com/ulab-uiuc/GraphRouter.

**Link**: [arxiv](http://arxiv.org/abs/2410.03834v2),  [pdf](http://arxiv.org/pdf/2410.03834v2)

**Tags**: cs.AI 



### MindEye-OmniAssist: A Gaze-Driven LLM-Enhanced Assistive Robot System   for Implicit Intention Recognition and Task Execution
**Authors**: Zejia Zhang, Bo Yang, Xinxing Chen, Weizhuang Shi, Haoyuan Wang, Wei Luo, Jian Huang

**Updated**: 2025-03-17T15:06:14Z

**Summary**: A promising effective human-robot interaction in assistive robotic systems is gaze-based control. However, current gaze-based assistive systems mainly help users with basic grasping actions, offering limited support. Moreover, the restricted intent recognition capability constrains the assistive system's ability to provide diverse assistance functions. In this paper, we propose an open implicit intention recognition framework powered by Large Language Model (LLM) and Vision Foundation Model (VFM), which can process gaze input and recognize user intents that are not confined to predefined or specific scenarios. Furthermore, we implement a gaze-driven LLM-enhanced assistive robot system (MindEye-OmniAssist) that recognizes user's intentions through gaze and assists in completing task. To achieve this, the system utilizes open vocabulary object detector, intention recognition network and LLM to infer their full intentions. By integrating eye movement feedback and LLM, it generates action sequences to assist the user in completing tasks. Real-world experiments have been conducted for assistive tasks, and the system achieved an overall success rate of 41/55 across various undefined tasks. Preliminary results show that the proposed method holds the potential to provide a more user-friendly human-computer interaction interface and significantly enhance the versatility and effectiveness of assistive systems by supporting more complex and diverse task.

**Link**: [arxiv](http://arxiv.org/abs/2503.13250v1),  [pdf](http://arxiv.org/pdf/2503.13250v1)

**Tags**: cs.RO cs.HC 



### Learning Program Behavioral Models from Synthesized Input-Output Pairs
**Authors**: Tural Mammadov, Dietrich Klakow, Alexander Koller, Andreas Zeller

**Updated**: 2025-03-17T15:04:06Z

**Summary**: We introduce Modelizer - a novel framework that, given a black-box program, learns a model from its input/output behavior using neural machine translation algorithms. The resulting model mocks the original program: Given an input, the model predicts the output that would have been produced by the program. However, the model is also reversible - that is, the model can predict the input that would have produced a given output. Finally, the model is differentiable and can be efficiently restricted to predict only a certain aspect of the program behavior. Modelizer uses grammars to synthesize and inputs and unsupervised tokenizers to decompose the resulting outputs, allowing it to learn sequence-to-sequence associations between token streams. Other than input grammars, Modelizer only requires the ability to execute the program. The resulting models are small, requiring fewer than 6.3 million parameters for languages such as Markdown or HTML; and they are accurate, achieving up to 95.4% accuracy and a BLEU score of 0.98 with standard error 0.04 in mocking real-world applications. As it learns from and predicts executions rather than code, Modelizer departs from the LLM-centric research trend, opening new opportunities for program-specific models that are fully tuned towards individual programs. Indeed, we foresee several applications of these models, especially as the output of the program can be any aspect of program behavior. Beyond mocking and predicting program behavior, the models can also synthesize inputs that are likely to produce a particular behavior, such as failures or coverage, thus assisting in program understanding and maintenance.

**Link**: [arxiv](http://arxiv.org/abs/2407.08597v2),  [pdf](http://arxiv.org/pdf/2407.08597v2)

**Tags**: cs.SE cs.LG 68T07 (Primary), 68N30 (Secondary), 68Q42 D.2.5; D.2.7; I.2.6; F.1.1; F.4.3 



### Pairwise vs Higher-order interactions: Can we identify the interaction   type in coupled oscillators from time series?
**Authors**: Weiwei Su, Shigefumi Hata, Hiroshi Kori, Hiroya Nakao, Ryota Kobayashi

**Updated**: 2025-03-17T14:57:59Z

**Summary**: Rhythmic phenomena, which are ubiquitous in biological systems, are typically modelled as systems of coupled limit cycle oscillators. Recently, there has been an increased interest in understanding the impact of higher-order interactions on the population dynamics of coupled oscillators. Meanwhile, estimating a mathematical model from experimental data is a vital step in understanding the dynamics of real-world complex systems. In coupled oscillator systems, identifying the type of interaction (e.g. pairwise or three-body) of a network is challenging, because different interactions can induce similar dynamical states and bifurcations. In this study, we have developed a method based on the adaptive LASSO (Least Absolute Shrinkage and Selection Operator) to infer the interactions between the oscillators from time series data. The proposed method can successfully classify the type of interaction and infer the probabilities of the existence of pairwise and three-body couplings. Through systematic analysis of synthetic datasets, we have demonstrated that our method outperforms two baseline methods, LASSO and OLS (Ordinary Least Squares), in accurately inferring the topology and strength of couplings between oscillators. Finally, we demonstrate the effectiveness of the proposed method by applying it to the synthetic data of 100 oscillators. These results imply that the proposed method is promising for identifying interactions from rhythmic activities in real-world systems.

**Link**: [arxiv](http://arxiv.org/abs/2503.13244v1),  [pdf](http://arxiv.org/pdf/2503.13244v1)

**Tags**: nlin.CD physics.data-an 



### ProDiF: Protecting Domain-Invariant Features to Secure Pre-Trained   Models Against Extraction
**Authors**: Tong Zhou, Shijin Duan, Gaowen Liu, Charles Fleming, Ramana Rao Kompella, Shaolei Ren, Xiaolin Xu

**Updated**: 2025-03-17T14:37:42Z

**Summary**: Pre-trained models are valuable intellectual property, capturing both domain-specific and domain-invariant features within their weight spaces. However, model extraction attacks threaten these assets by enabling unauthorized source-domain inference and facilitating cross-domain transfer via the exploitation of domain-invariant features. In this work, we introduce **ProDiF**, a novel framework that leverages targeted weight space manipulation to secure pre-trained models against extraction attacks. **ProDiF** quantifies the transferability of filters and perturbs the weights of critical filters in unsecured memory, while preserving actual critical weights in a Trusted Execution Environment (TEE) for authorized users. A bi-level optimization further ensures resilience against adaptive fine-tuning attacks. Experimental results show that **ProDiF** reduces source-domain accuracy to near-random levels and decreases cross-domain transferability by 74.65\%, providing robust protection for pre-trained models. This work offers comprehensive protection for pre-trained DNN models and highlights the potential of weight space manipulation as a novel approach to model security.

**Link**: [arxiv](http://arxiv.org/abs/2503.13224v1),  [pdf](http://arxiv.org/pdf/2503.13224v1)

**Tags**: cs.CR cs.LG 



### Regularization Using Synthetic Data in High-Dimensional Models
**Authors**: Weihao Li, Dongming Huang

**Updated**: 2025-03-17T14:33:10Z

**Summary**: To address the challenges of reliable statistical inference in high-dimensional models, we introduce the Synthetic-data Regularized Estimator (SRE). Unlike traditional regularization methods, the SRE regularizes the complex target model via a weighted likelihood based on synthetic data generated from a simpler, more stable model. This method provides a theoretically sound and practically effective alternative to parameter penalization. We establish key theoretical properties of the SRE in generalized linear models, including existence, stability, consistency, and minimax rate optimality. Applying the Convex Gaussian Min-Max Theorem, we derive a precise asymptotic characterization in the high-dimensional linear regime. To deal with the non-separable regularization, we introduce a novel decomposition in our analysis. Building upon these results, we develop practical methodologies for tuning parameter selection, confidence interval construction, and calibrated variable selection in high-dimensional inference. The effectiveness of the SRE is demonstrated through simulation studies and real-data applications.

**Link**: [arxiv](http://arxiv.org/abs/2407.04194v4),  [pdf](http://arxiv.org/pdf/2407.04194v4)

**Tags**: math.ST stat.TH 



### Causal Graphs Meet Thoughts: Enhancing Complex Reasoning in   Graph-Augmented LLMs
**Authors**: Hang Luo, Jian Zhang, Chujun Li

**Updated**: 2025-03-17T14:32:08Z

**Summary**: In knowledge-intensive tasks, especially in high-stakes domains like medicine and law, it is critical not only to retrieve relevant information but also to provide causal reasoning and explainability. Large language models (LLMs) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, integrating knowledge graphs with Graph Retrieval-Augmented Generation (Graph RAG) has emerged as an effective solution. Traditional Graph RAG methods often rely on simple graph traversal or semantic similarity, which do not capture causal relationships or align well with the model's internal reasoning steps. This paper proposes a novel pipeline that filters large knowledge graphs to emphasize cause-effect edges, aligns the retrieval process with the model's chain-of-thought (CoT), and enhances reasoning through multi-stage path improvements. Experiments on medical question-answering tasks show consistent gains, with up to a 10\% absolute improvement across multiple large language models (LLMs). This approach demonstrates the value of combining causal reasoning with stepwise retrieval, leading to more interpretable and logically grounded solutions for complex queries.

**Link**: [arxiv](http://arxiv.org/abs/2501.14892v2),  [pdf](http://arxiv.org/pdf/2501.14892v2)

**Tags**: cs.AI cs.CL 



### Can Language Models Follow Multiple Turns of Entangled Instructions?
**Authors**: Chi Han

**Updated**: 2025-03-17T14:31:37Z

**Summary**: Despite significant achievements in improving the instruction-following capabilities of large language models (LLMs), the ability to process multiple potentially entangled or conflicting instructions remains a considerable challenge. Real-world scenarios often require consistency across multiple instructions over time, such as secret privacy, personal preferences, and prioritization, which demand sophisticated abilities to integrate multiple turns and carefully balance competing objectives when instructions intersect or conflict. This work presents a systematic investigation of LLMs' capabilities in handling multiple turns of instructions, covering three levels of difficulty: (1) retrieving information from instructions, (2) tracking and reasoning across turns, and (3) resolving conflicts among instructions. We construct MultiTurnInstruct with around 1.1K high-quality multi-turn conversations through the human-in-the-loop approach and result in nine capability categories, including statics and dynamics, reasoning, and multitasking. Our finding reveals an intriguing trade-off between different capabilities. While GPT models demonstrate superior memorization, they show reduced effectiveness in privacy-protection tasks requiring selective information withholding. Larger models exhibit stronger reasoning capabilities but still struggle with resolving conflicting instructions. Importantly, these performance gaps cannot be attributed solely to information loss, as models demonstrate strong BLEU scores on memorization tasks but their attention mechanisms fail to integrate multiple related instructions effectively. These findings highlight critical areas for improvement in complex real-world tasks involving multi-turn instructions.

**Link**: [arxiv](http://arxiv.org/abs/2503.13222v1),  [pdf](http://arxiv.org/pdf/2503.13222v1)

**Tags**: cs.CL cs.AI 



### Measurement of the inhomogeneity of the KATRIN tritium source electric   potential by high-resolution spectroscopy of conversion electrons from   $^{83m}$Kr
**Authors**: H. Acharya, M. Aker, D. Batzler, A. Beglarian, J. Beisenkötter, M. Biassoni, B. Bieringer, Y. Biondi, F. Block, B. Bornschein, L. Bornschein, M. Böttcher, M. Carminati, A. Chatrabhuti, S. Chilingaryan, B. A. Daniel, M. Descher, D. Díaz Barrero, O. Dragoun, G. Drexlin, F. Edzards, K. Eitel, E. Ellinger, R. Engel, S. Enomoto, A. Felden, C. Fengler, C. Fiorini, J. A. Formaggio, C. Forstner, F. M. Fränkle, G. Gagliardi, K. Gauda, A. S. Gavin, W. Gil, F. Glück, R. Größle, V. Gupta, K. Habib, V. Hannen, L. Hasselmann, K. Helbing, S. Heyns, R. Hiller, D. Hillesheimer, D. Hinz, T. Höhn, A. Huber, A. Jansen, K. Khosonthongkee, M. Klein, J. Kohpeiß, C. Köhler, A. Kopmann, N. Kovac, L. La Cascio, L. Laschinger, T. Lasserre, J. Lauer, T. L. Le, O. Lebeda, B. Lehnert, A. Lokhov, M. Machatschek, M. Mark, A. Marsteller, E. L. Martin, K. McMichael, C. Melzer, S. Mertens, S. Mohanty, J. Mostafa, A. Nava, H. Neumann, S. Niemes, I. Nutini, A. Onillon, R. Ostertag, D. S. Parno, U. Pinsook, J. Plößner, A. W. P. Poon, J. M. L. Poyato, F. Priester, J. Ráliš, S. Ramachandran, R. G. H. Robertson, C. Rodenbeck, M. Röllig, R. Sack, A. Saenz, R. Salomon, P. Schäfer, M. Slezák, K. Schlösser, M. Schlösser, L. Schlüter, S. Schneidewind, U. Schnurr, J. Schürmann, A. K. Schütz, A. Schwemmer, A. Schwenck, J. Seeyangnok, M. Šefčík, D. Siegmann, F. Simon, J. Songwadhana, F. Spanier, D. Spreng, W. Sreethawong, M. Steidl, J. Štorek, X. Stribl, M. Sturm, N. Suwonjandee, N. Tan Jerome, H. H. Telle, L. A. Thorne, T. Thümmler, K. Trost, K. Valerius, D. Vénos, C. Weinheimer, S. Welte, J. Wendel, C. Wiesinger, J. F. Wilkerson, J. Wolf, S. Wüstling, J. Wydra, W. Xu, G. Zeller

**Updated**: 2025-03-17T14:31:29Z

**Summary**: Precision spectroscopy of the electron spectrum of the tritium $\beta$-decay near the kinematic endpoint is a direct method to determine the effective electron antineutrino mass. The KArlsruhe TRItium Neutrino (KATRIN) experiment aims to determine this quantity with a sensitivity of better than 0.3$\,$eV (90$\,$% C.L.). An inhomogeneous electric potential in the tritium source of KATRIN can lead to distortions of the $\beta$-spectrum, which directly impact the neutrino-mass observable. This effect can be quantified through precision spectroscopy of the conversion-electrons of co-circulated metastable $^{83m}$Kr. Therefore, dedicated, several-weeks long measurement campaigns have been performed within the KATRIN data taking schedule. In this work, we infer the tritium source potential observables from these measurements, and present their implications for the neutrino-mass determination.

**Link**: [arxiv](http://arxiv.org/abs/2503.13221v1),  [pdf](http://arxiv.org/pdf/2503.13221v1)

**Tags**: nucl-ex hep-ex 



### Dense Policy: Bidirectional Autoregressive Learning of Actions
**Authors**: Yue Su, Xinyu Zhan, Hongjie Fang, Han Xue, Hao-Shu Fang, Yong-Lu Li, Cewu Lu, Lixin Yang

**Updated**: 2025-03-17T14:28:08Z

**Summary**: Mainstream visuomotor policies predominantly rely on generative models for holistic action prediction, while current autoregressive policies, predicting the next token or chunk, have shown suboptimal results. This motivates a search for more effective learning methods to unleash the potential of autoregressive policies for robotic manipulation. This paper introduces a bidirectionally expanded learning approach, termed Dense Policy, to establish a new paradigm for autoregressive policies in action prediction. It employs a lightweight encoder-only architecture to iteratively unfold the action sequence from an initial single frame into the target sequence in a coarse-to-fine manner with logarithmic-time inference. Extensive experiments validate that our dense policy has superior autoregressive learning capabilities and can surpass existing holistic generative policies. Our policy, example data, and training code will be publicly available upon publication. Project page: https: //selen-suyue.github.io/DspNet/.

**Link**: [arxiv](http://arxiv.org/abs/2503.13217v1),  [pdf](http://arxiv.org/pdf/2503.13217v1)

**Tags**: cs.RO cs.CV cs.LG 



### Standardizing Structural Causal Models
**Authors**: Weronika Ormaniec, Scott Sussex, Lars Lorch, Bernhard Schölkopf, Andreas Krause

**Updated**: 2025-03-17T14:26:33Z

**Summary**: Synthetic datasets generated by structural causal models (SCMs) are commonly used for benchmarking causal structure learning algorithms. However, the variances and pairwise correlations in SCM data tend to increase along the causal ordering. Several popular algorithms exploit these artifacts, possibly leading to conclusions that do not generalize to real-world settings. Existing metrics like $\operatorname{Var}$-sortability and $\operatorname{R^2}$-sortability quantify these patterns, but they do not provide tools to remedy them. To address this, we propose internally-standardized structural causal models (iSCMs), a modification of SCMs that introduces a standardization operation at each variable during the generative process. By construction, iSCMs are not $\operatorname{Var}$-sortable. We also find empirical evidence that they are mostly not $\operatorname{R^2}$-sortable for commonly-used graph families. Moreover, contrary to the post-hoc standardization of data generated by standard SCMs, we prove that linear iSCMs are less identifiable from prior knowledge on the weights and do not collapse to deterministic relationships in large systems, which may make iSCMs a useful model in causal inference beyond the benchmarking problem studied here. Our code is publicly available at: https://github.com/werkaaa/iscm.

**Link**: [arxiv](http://arxiv.org/abs/2406.11601v3),  [pdf](http://arxiv.org/pdf/2406.11601v3)

**Tags**: cs.LG stat.ML 



### MAME: Multidimensional Adaptive Metamer Exploration with Human   Perceptual Feedback
**Authors**: Mina Kamao, Hayato Ono, Ayumu Yamashita, Kaoru Amano, Masataka Sawayama

**Updated**: 2025-03-17T14:23:04Z

**Summary**: Alignment between human brain networks and artificial models is actively studied in machine learning and neuroscience. A widely adopted approach to explore their functional alignment is to identify metamers for both humans and models. Metamers refer to input stimuli that are physically different but equivalent within a given system. If a model's metameric space completely matched the human metameric space, the model would achieve functional alignment with humans. However, conventional methods lack direct ways to search for human metamers. Instead, researchers first develop biologically inspired models and then infer about human metamers indirectly by testing whether model metamers also appear as metamers to humans. Here, we propose the Multidimensional Adaptive Metamer Exploration (MAME) framework, enabling direct high-dimensional exploration of human metameric space. MAME leverages online image generation guided by human perceptual feedback. Specifically, it modulates reference images across multiple dimensions by leveraging hierarchical responses from convolutional neural networks (CNNs). Generated images are presented to participants whose perceptual discriminability is assessed in a behavioral task. Based on participants' responses, subsequent image generation parameters are adaptively updated online. Using our MAME framework, we successfully measured a human metameric space of over fifty dimensions within a single experiment. Experimental results showed that human discrimination sensitivity was lower for metameric images based on low-level features compared to high-level features, which image contrast metrics could not explain. The finding suggests that the model computes low-level information not essential for human perception. Our framework has the potential to contribute to developing interpretable AI and understanding of brain function in neuroscience.

**Link**: [arxiv](http://arxiv.org/abs/2503.13212v1),  [pdf](http://arxiv.org/pdf/2503.13212v1)

**Tags**: cs.LG 



### Improving Complex Reasoning with Dynamic Prompt Corruption: A soft   prompt Optimization Approach
**Authors**: Sinan Fan, Liang Xie, Chen Shen, Ge Teng, Xiaosong Yuan, Xiaofeng Zhang, Chenxi Huang, Wenxiao Wang, Xiaofei He, Jieping Ye

**Updated**: 2025-03-17T14:20:48Z

**Summary**: Prompt-tuning (PT) for large language models (LLMs) can facilitate the performance on various conventional NLP tasks with significantly fewer trainable parameters. However, our investigation reveals that PT provides limited improvement and may even degrade the primitive performance of LLMs on complex reasoning tasks. Such a phenomenon suggests that soft prompts can positively impact certain instances while negatively affecting others, particularly during the later phases of reasoning. To address these challenges, We first identify an information accumulation within the soft prompts. Through detailed analysis, we demonstrate that this phenomenon is often accompanied by erroneous information flow patterns in the deeper layers of the model, which ultimately lead to incorrect reasoning outcomes. we propose a novel method called \textbf{D}ynamic \textbf{P}rompt \textbf{C}orruption (DPC) to take better advantage of soft prompts in complex reasoning tasks, which dynamically adjusts the influence of soft prompts based on their impact on the reasoning process. Specifically, DPC consists of two stages: Dynamic Trigger and Dynamic Corruption. First, Dynamic Trigger measures the impact of soft prompts, identifying whether beneficial or detrimental. Then, Dynamic Corruption mitigates the negative effects of soft prompts by selectively masking key tokens that interfere with the reasoning process. We validate the proposed approach through extensive experiments on various LLMs and reasoning tasks, including GSM8K, MATH, and AQuA. Experimental results demonstrate that DPC can consistently enhance the performance of PT, achieving 4\%-8\% accuracy gains compared to vanilla prompt tuning, highlighting the effectiveness of our approach and its potential to enhance complex reasoning in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2503.13208v1),  [pdf](http://arxiv.org/pdf/2503.13208v1)

**Tags**: cs.CL cs.AI 



### Towards Efficient Mixture of Experts: A Holistic Study of Compression   Techniques
**Authors**: Shwai He, Daize Dong, Liang Ding, Ang Li

**Updated**: 2025-03-17T14:18:42Z

**Summary**: Scaling large language models has driven remarkable advancements across various domains, yet the continual increase in model size presents significant challenges for real-world deployment. The Mixture of Experts (MoE) architecture offers a promising solution by dynamically selecting and activating only a subset of experts during inference, thus substantially reducing computational costs while preserving high performance. Despite these benefits, MoE introduces new inefficiencies, such as excessive parameters and communication overhead. In this work, we present a holistic study of compression techniques for Mixture of Experts to enhance both efficiency and scalability. While recent efforts have focused on Expert Trimming, which reduces the number of experts, these approaches still suffer from considerable communication and computational costs. To address this, we propose more aggressive strategies, such as Layer Drop, which removes entire MoE layers, and Block Drop, which eliminates transformer blocks. Surprisingly, these aggressive pruning techniques not only preserve model performance but also substantially improve computation and memory efficiency. Furthermore, beyond Expert Trimming, we also introduce Expert Slimming, which compresses individual experts to further boost performance and can be seamlessly integrated with Expert Trimming. Extensive experimental results demonstrate the effectiveness of our proposed methods-Layer Drop and Block Drop-along with the comprehensive recipe that integrates Expert Slimming and Expert Trimming, achieving a 6.05x speedup with 77.1% reduced memory usage while maintaining over 92% of performance on Mixtral-8x7B. Our code is released at https://github.com/CASE-Lab-UMD/Unified-MoE-Compression.

**Link**: [arxiv](http://arxiv.org/abs/2406.02500v3),  [pdf](http://arxiv.org/pdf/2406.02500v3)

**Tags**: cs.LG cs.AI 



### MAP: Evaluation and Multi-Agent Enhancement of Large Language Models for   Inpatient Pathways
**Authors**: Zhen Chen, Zhihao Peng, Xusheng Liang, Cheng Wang, Peigan Liang, Linsheng Zeng, Minjie Ju, Yixuan Yuan

**Updated**: 2025-03-17T14:14:28Z

**Summary**: Inpatient pathways demand complex clinical decision-making based on comprehensive patient information, posing critical challenges for clinicians. Despite advancements in large language models (LLMs) in medical applications, limited research focused on artificial intelligence (AI) inpatient pathways systems, due to the lack of large-scale inpatient datasets. Moreover, existing medical benchmarks typically concentrated on medical question-answering and examinations, ignoring the multifaceted nature of clinical decision-making in inpatient settings. To address these gaps, we first developed the Inpatient Pathway Decision Support (IPDS) benchmark from the MIMIC-IV database, encompassing 51,274 cases across nine triage departments and 17 major disease categories alongside 16 standardized treatment options. Then, we proposed the Multi-Agent Inpatient Pathways (MAP) framework to accomplish inpatient pathways with three clinical agents, including a triage agent managing the patient admission, a diagnosis agent serving as the primary decision maker at the department, and a treatment agent providing treatment plans. Additionally, our MAP framework includes a chief agent overseeing the inpatient pathways to guide and promote these three clinician agents. Extensive experiments showed our MAP improved the diagnosis accuracy by 25.10% compared to the state-of-the-art LLM HuatuoGPT2-13B. It is worth noting that our MAP demonstrated significant clinical compliance, outperforming three board-certified clinicians by 10%-12%, establishing a foundation for inpatient pathways systems.

**Link**: [arxiv](http://arxiv.org/abs/2503.13205v1),  [pdf](http://arxiv.org/pdf/2503.13205v1)

**Tags**: cs.AI cs.CL cs.CV cs.HC cs.MA 



### Scalable inference with Autoregressive Neural Ratio Estimation
**Authors**: Noemi Anau Montel, James Alvey, Christoph Weniger

**Updated**: 2025-03-17T14:12:19Z

**Summary**: In recent years, there has been a remarkable development of simulation-based inference (SBI) algorithms, and they have now been applied across a wide range of astrophysical and cosmological analyses. There are a number of key advantages to these methods, centred around the ability to perform scalable statistical inference without an explicit likelihood. In this work, we propose two technical building blocks to a specific sequential SBI algorithm, truncated marginal neural ratio estimation (TMNRE). In particular, first we develop autoregressive ratio estimation with the aim to robustly estimate correlated high-dimensional posteriors. Secondly, we propose a slice-based nested sampling algorithm to efficiently draw both posterior samples and constrained prior samples from ratio estimators, the latter being instrumental for sequential inference. To validate our implementation, we carry out inference tasks on three concrete examples: a toy model of a multi-dimensional Gaussian, the analysis of a stellar stream mock observation, and finally, a proof-of-concept application to substructure searches in strong gravitational lensing. In addition, we publicly release the code for both the autoregressive ratio estimator and the slice sampler.

**Link**: [arxiv](http://arxiv.org/abs/2308.08597v2),  [pdf](http://arxiv.org/pdf/2308.08597v2)

**Tags**: astro-ph.IM astro-ph.CO astro-ph.GA 



### Causes of evolutionary divergence in prostate cancer
**Authors**: Emre Esenturk, Atef Sahli, Valeriia Haberland, Aleksandra Ziuboniewicz, Christopher Wirth, G. Steven Bova, Robert G Bristow, Mark N Brook, Benedikt Brors, Adam Butler, Géraldine Cancel-Tassin, Kevin CL Cheng, Colin S Cooper, Niall M Corcoran, Olivier Cussenot, Ros A Eeles, Francesco Favero, Clarissa Gerhauser, Abraham Gihawi, Etsehiwot G Girma, Vincent J Gnanapragasam, Andreas J Gruber, Anis Hamid, Vanessa M Hayes, Housheng Hansen He, Christopher M Hovens, Eddie Luidy Imada, G. Maria Jakobsdottir, Chol-hee Jung, Francesca Khani, Zsofia Kote-Jarai, Philippe Lamy, Gregory Leeman, Massimo Loda, Pavlo Lutsik, Luigi Marchionni, Ramyar Molania, Anthony T Papenfuss, Diogo Pellegrina, Bernard Pope, Lucio R Queiroz, Tobias Rausch, Jüri Reimand, Brian Robinson, Thorsten Schlomm, Karina D Sørensen, Sebastian Uhrig, Joachim Weischenfeldt, Yaobo Xu, Takafumi N Yamaguchi, Claudio Zanettini, Andy G Lynch, David C Wedge, Daniel S Brewer, Dan J Woodcock

**Updated**: 2025-03-17T14:00:02Z

**Summary**: Cancer progression involves the sequential accumulation of genetic alterations that cumulatively shape the tumour phenotype. In prostate cancer, tumours can follow divergent evolutionary trajectories that lead to distinct subtypes, but the causes of this divergence remain unclear. While causal inference could elucidate the factors involved, conventional methods are unsuitable due to the possibility of unobserved confounders and ambiguity in the direction of causality. Here, we propose a method that circumvents these issues and apply it to genomic data from 829 prostate cancer patients. We identify several genetic alterations that drive divergence as well as others that prevent this transition, locking tumours into one trajectory. Further analysis reveals that these genetic alterations may cause each other, implying a positive-feedback loop that accelerates divergence. Our findings provide insights into how cancer subtypes emerge and offer a foundation for genomic surveillance strategies aimed at monitoring the progression of prostate cancer.

**Link**: [arxiv](http://arxiv.org/abs/2503.13189v1),  [pdf](http://arxiv.org/pdf/2503.13189v1)

**Tags**: q-bio.GN 



### OLÉ -- Online Learning Emulation in Cosmology
**Authors**: Sven Günther, Lennart Balkenhol, Christian Fidler, Ali Rida Khalife, Julien Lesgourgues, Markus R. Mosbech, Ravi Kumar Sharma

**Updated**: 2025-03-17T13:56:45Z

**Summary**: In this work, we present OL\'E, a new online learning emulator for use in cosmological inference. The emulator relies on Gaussian Processes and Principal Component Analysis for efficient data compression and fast evaluation. Moreover, OL\'E features an automatic error estimation for optimal active sampling and online learning. All training data is computed on-the-fly, making the emulator applicable to any cosmological model or dataset. We illustrate the emulator's performance on an array of cosmological models and data sets, showing significant improvements in efficiency over similar emulators without degrading accuracy compared to standard theory codes. We find that OL\'E is able to considerably speed up the inference process, increasing the efficiency by a factor of $30-350$, including data acquisition and training. Typically the runtime of the likelihood code becomes the computational bottleneck. Furthermore, OL\'E emulators are differentiable; we demonstrate that, together with the differentiable likelihoods available in the $\texttt{candl}$ library, we can construct a gradient-based sampling method which yields an additional improvement factor of 4. OL\'E can be easily interfaced with the popular samplers $\texttt{MontePython}$ and $\texttt{Cobaya}$, and the Einstein-Boltzmann solvers $\texttt{CLASS}$ and $\texttt{CAMB}$. OL\'E is publicly available at https://github.com/svenguenther/OLE .

**Link**: [arxiv](http://arxiv.org/abs/2503.13183v1),  [pdf](http://arxiv.org/pdf/2503.13183v1)

**Tags**: astro-ph.CO astro-ph.IM 



### A super-resolution reconstruction method for lightweight building images   based on an expanding feature modulation network
**Authors**: Yi Zhang, Wenye Zhou, Ruonan Lin

**Updated**: 2025-03-17T13:54:26Z

**Summary**: This study proposes a lightweight method for building image super-resolution using a Dilated Contextual Feature Modulation Network (DCFMN). The process includes obtaining high-resolution images, down-sampling them to low-resolution, enhancing the low-resolution images, constructing and training a lightweight network model, and generating super-resolution outputs. To address challenges such as regular textures and long-range dependencies in building images, the DCFMN integrates an expansion separable modulation unit and a local feature enhancement module. The former employs multiple expansion convolutions equivalent to a large kernel to efficiently aggregate multi-scale features while leveraging a simple attention mechanism for adaptivity. The latter encodes local features, mixes channel information, and ensures no additional computational burden during inference through reparameterization. This approach effectively resolves the limitations of existing lightweight super-resolution networks in modeling long-range dependencies, achieving accurate and efficient global feature modeling without increasing computational costs, and significantly improving both reconstruction quality and lightweight efficiency for building image super-resolution models.

**Link**: [arxiv](http://arxiv.org/abs/2503.13179v1),  [pdf](http://arxiv.org/pdf/2503.13179v1)

**Tags**: cs.CV 



### Valley: Video Assistant with Large Language model Enhanced abilitY
**Authors**: Ruipu Luo, Ziwang Zhao, Min Yang, Zheming Yang, Minghui Qiu, Tao Wang, Zhongyu Wei, Yanhao Wang, Cen Chen

**Updated**: 2025-03-17T13:51:51Z

**Summary**: Large Language Models (LLMs), with remarkable conversational capability, have emerged as AI assistants that can handle both visual and textual modalities. However, their effectiveness in joint video and language understanding has not been extensively explored. In the paper, we introduce Valley, a multi-modal foundation model that is designed to enable enhanced video comprehension and instruction-following capabilities. To this end, we construct two datasets, namely Valley-702k and Valley-instruct-73k, to cover a diverse range of video-text alignment and video-based instruction tasks, such as multi-shot captions, long video descriptions, action recognition, causal inference, etc. Then, we adopt ViT-L/14 as the vision encoder and explore three different temporal modeling modules to learn multifaceted features for enhanced video understanding. In addition, we implement a two-phase training approach for Valley: the first phase focuses solely on training the projection module to facilitate the LLM's capacity to understand visual input, and the second phase jointly trains the projection module and the LLM to improve their instruction following ability. Extensive experiments demonstrate that Valley has the potential to serve as an effective video assistant, simplifying complex video-understanding scenarios. Our code and data are published anonymously at https://github.com/valley-vl/Valley.

**Link**: [arxiv](http://arxiv.org/abs/2306.07207v3),  [pdf](http://arxiv.org/pdf/2306.07207v3)

**Tags**: cs.CV cs.AI cs.CL 



### Assessing the Robustness of LLM-based NLP Software via Automated Testing
**Authors**: Mingxuan Xiao, Yan Xiao, Shunhui Ji, Hanbo Cai, Lei Xue, Pengcheng Zhang

**Updated**: 2025-03-17T13:42:06Z

**Summary**: Benefiting from the advancements in LLMs, NLP software has undergone rapid development. Such software is widely employed in various safety-critical tasks, such as financial sentiment analysis, toxic content moderation, and log generation. Unlike traditional software, LLM-based NLP software relies on prompts and examples as inputs. Given the complexity of LLMs and the unpredictability of real-world inputs, quantitatively assessing the robustness of such software is crucial. However, to the best of our knowledge, no automated robustness testing methods have been specifically designed to evaluate the overall inputs of LLM-based NLP software. To this end, this paper introduces the first AutOmated Robustness Testing frAmework, AORTA, which reconceptualizes the testing process into a combinatorial optimization problem. Existing testing methods designed for DNN-based software can be applied to LLM-based software by AORTA, but their effectiveness is limited. To address this, we propose a novel testing method for LLM-based software within AORTA called Adaptive Beam Search. ABS is tailored for the expansive feature space of LLMs and improves testing effectiveness through an adaptive beam width and the capability for backtracking. We successfully embed 18 test methods in the designed framework AORTA and compared the test validity of ABS with three datasets and five threat models. ABS facilitates a more comprehensive and accurate robustness assessment before software deployment, with an average test success rate of 86.138%. Compared to the currently best-performing baseline PWWS, ABS significantly reduces the computational overhead by up to 3441.895 seconds per successful test case and decreases the number of queries by 218.762 times on average. Furthermore, test cases generated by ABS exhibit greater naturalness and transferability.

**Link**: [arxiv](http://arxiv.org/abs/2412.21016v2),  [pdf](http://arxiv.org/pdf/2412.21016v2)

**Tags**: cs.SE 



### Empowering Edge Intelligence: A Comprehensive Survey on On-Device AI   Models
**Authors**: Xubin Wang, Zhiqing Tang, Jianxiong Guo, Tianhui Meng, Chenhao Wang, Tian Wang, Weijia Jia

**Updated**: 2025-03-17T13:37:33Z

**Summary**: The rapid advancement of artificial intelligence (AI) technologies has led to an increasing deployment of AI models on edge and terminal devices, driven by the proliferation of the Internet of Things (IoT) and the need for real-time data processing. This survey comprehensively explores the current state, technical challenges, and future trends of on-device AI models. We define on-device AI models as those designed to perform local data processing and inference, emphasizing their characteristics such as real-time performance, resource constraints, and enhanced data privacy. The survey is structured around key themes, including the fundamental concepts of AI models, application scenarios across various domains, and the technical challenges faced in edge environments. We also discuss optimization and implementation strategies, such as data preprocessing, model compression, and hardware acceleration, which are essential for effective deployment. Furthermore, we examine the impact of emerging technologies, including edge computing and foundation models, on the evolution of on-device AI models. By providing a structured overview of the challenges, solutions, and future directions, this survey aims to facilitate further research and application of on-device AI, ultimately contributing to the advancement of intelligent systems in everyday life.

**Link**: [arxiv](http://arxiv.org/abs/2503.06027v2),  [pdf](http://arxiv.org/pdf/2503.06027v2)

**Tags**: cs.AI cs.LG cs.NI 



### LLM-based speaker diarization correction: A generalizable approach
**Authors**: Georgios Efstathiadis, Vijay Yadav, Anzar Abbas

**Updated**: 2025-03-17T13:34:07Z

**Summary**: Speaker diarization is necessary for interpreting conversations transcribed using automated speech recognition (ASR) tools. Despite significant developments in diarization methods, diarization accuracy remains an issue. Here, we investigate the use of large language models (LLMs) for diarization correction as a post-processing step. LLMs were fine-tuned using the Fisher corpus, a large dataset of transcribed conversations. The ability of the models to improve diarization accuracy in a holdout dataset from the Fisher corpus as well as an independent dataset was measured. We report that fine-tuned LLMs can markedly improve diarization accuracy. However, model performance is constrained to transcripts produced using the same ASR tool as the transcripts used for fine-tuning, limiting generalizability. To address this constraint, an ensemble model was developed by combining weights from three separate models, each fine-tuned using transcripts from a different ASR tool. The ensemble model demonstrated better overall performance than each of the ASR-specific models, suggesting that a generalizable and ASR-agnostic approach may be achievable. We have made the weights of these models publicly available on HuggingFace at https://huggingface.co/bklynhlth.

**Link**: [arxiv](http://arxiv.org/abs/2406.04927v3),  [pdf](http://arxiv.org/pdf/2406.04927v3)

**Tags**: eess.AS cs.CL 



### Language-guided Open-world Video Anomaly Detection
**Authors**: Zihao Liu, Xiaoyu Wu, Jianqin Wu, Xuxu Wang, Linlin Yang

**Updated**: 2025-03-17T13:31:19Z

**Summary**: Video anomaly detection models aim to detect anomalies that deviate from what is expected. In open-world scenarios, the expected events may change as requirements change. For example, not wearing a mask is considered abnormal during a flu outbreak but normal otherwise. However, existing methods assume that the definition of anomalies is invariable, and thus are not applicable to the open world. To address this, we propose a novel open-world VAD paradigm with variable definitions, allowing guided detection through user-provided natural language at inference time. This paradigm necessitates establishing a robust mapping from video and textual definition to anomaly score. Therefore, we propose LaGoVAD (Language-guided Open-world VAD), a model that dynamically adapts anomaly definitions through two regularization strategies: diversifying the relative durations of anomalies via dynamic video synthesis, and enhancing feature robustness through contrastive learning with negative mining. Training such adaptable models requires diverse anomaly definitions, but existing datasets typically provide given labels without semantic descriptions. To bridge this gap, we collect PreVAD (Pre-training Video Anomaly Dataset), the largest and most diverse video anomaly dataset to date, featuring 35,279 annotated videos with multi-level category labels and descriptions that explicitly define anomalies. Zero-shot experiments on seven datasets demonstrate SOTA performance. Data and code will be released.

**Link**: [arxiv](http://arxiv.org/abs/2503.13160v1),  [pdf](http://arxiv.org/pdf/2503.13160v1)

**Tags**: cs.CV 



### Noise-Aware Differentially Private Variational Inference
**Authors**: Talal Alrawajfeh, Joonas Jälkö, Antti Honkela

**Updated**: 2025-03-17T13:23:33Z

**Summary**: Differential privacy (DP) provides robust privacy guarantees for statistical inference, but this can lead to unreliable results and biases in downstream applications. While several noise-aware approaches have been proposed which integrate DP perturbation into the inference, they are limited to specific types of simple probabilistic models. In this work, we propose a novel method for noise-aware approximate Bayesian inference based on stochastic gradient variational inference which can also be applied to high-dimensional and non-conjugate models. We also propose a more accurate evaluation method for noise-aware posteriors. Empirically, our inference method has similar performance to existing methods in the domain where they are applicable. Outside this domain, we obtain accurate coverages on high-dimensional Bayesian linear regression and well-calibrated predictive probabilities on Bayesian logistic regression with the UCI Adult dataset.

**Link**: [arxiv](http://arxiv.org/abs/2410.19371v2),  [pdf](http://arxiv.org/pdf/2410.19371v2)

**Tags**: stat.ML cs.CR cs.LG 



### Are LLMs (Really) Ideological? An IRT-based Analysis and Alignment Tool   for Perceived Socio-Economic Bias in LLMs
**Authors**: Jasmin Wachter, Michael Radloff, Maja Smolej, Katharina Kinder-Kurlanda

**Updated**: 2025-03-17T13:20:09Z

**Summary**: We introduce an Item Response Theory (IRT)-based framework to detect and quantify socioeconomic bias in large language models (LLMs) without relying on subjective human judgments. Unlike traditional methods, IRT accounts for item difficulty, improving ideological bias estimation. We fine-tune two LLM families (Meta-LLaMa 3.2-1B-Instruct and Chat- GPT 3.5) to represent distinct ideological positions and introduce a two-stage approach: (1) modeling response avoidance and (2) estimating perceived bias in answered responses. Our results show that off-the-shelf LLMs often avoid ideological engagement rather than exhibit bias, challenging prior claims of partisanship. This empirically validated framework enhances AI alignment research and promotes fairer AI governance.

**Link**: [arxiv](http://arxiv.org/abs/2503.13149v1),  [pdf](http://arxiv.org/pdf/2503.13149v1)

**Tags**: cs.AI cs.CL cs.CY 



### Reangle-A-Video: 4D Video Generation as Video-to-Video Translation
**Authors**: Hyeonho Jeong, Suhyeon Lee, Jong Chul Ye

**Updated**: 2025-03-17T13:01:59Z

**Summary**: We introduce Reangle-A-Video, a unified framework for generating synchronized multi-view videos from a single input video. Unlike mainstream approaches that train multi-view video diffusion models on large-scale 4D datasets, our method reframes the multi-view video generation task as video-to-videos translation, leveraging publicly available image and video diffusion priors. In essence, Reangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An image-to-video diffusion transformer is synchronously fine-tuned in a self-supervised manner to distill view-invariant motion from a set of warped videos. (2) Multi-View Consistent Image-to-Images Translation: The first frame of the input video is warped and inpainted into various camera perspectives under an inference-time cross-view consistency guidance using DUSt3R, generating multi-view consistent starting images. Extensive experiments on static view transport and dynamic camera control show that Reangle-A-Video surpasses existing methods, establishing a new solution for multi-view video generation. We will publicly release our code and data. Project page: https://hyeonho99.github.io/reangle-a-video/

**Link**: [arxiv](http://arxiv.org/abs/2503.09151v2),  [pdf](http://arxiv.org/pdf/2503.09151v2)

**Tags**: cs.CV cs.AI 



### MIXPINN: Mixed-Material Simulations by Physics-Informed Neural Network
**Authors**: Xintian Yuan, Yunke Ao, Boqi Chen, Philipp Fuernstahl

**Updated**: 2025-03-17T12:48:29Z

**Summary**: Simulating the complex interactions between soft tissues and rigid anatomy is critical for applications in surgical training, planning, and robotic-assisted interventions. Traditional Finite Element Method (FEM)-based simulations, while accurate, are computationally expensive and impractical for real-time scenarios. Learning-based approaches have shown promise in accelerating predictions but have fallen short in modeling soft-rigid interactions effectively. We introduce MIXPINN, a physics-informed Graph Neural Network (GNN) framework for mixed-material simulations, explicitly capturing soft-rigid interactions using graph-based augmentations. Our approach integrates Virtual Nodes (VNs) and Virtual Edges (VEs) to enhance rigid body constraint satisfaction while preserving computational efficiency. By leveraging a graph-based representation of biomechanical structures, MIXPINN learns high-fidelity deformations from FEM-generated data and achieves real-time inference with sub-millimeter accuracy. We validate our method in a realistic clinical scenario, demonstrating superior performance compared to baseline GNN models and traditional FEM methods. Our results show that MIXPINN reduces computational cost by an order of magnitude while maintaining high physical accuracy, making it a viable solution for real-time surgical simulation and robotic-assisted procedures.

**Link**: [arxiv](http://arxiv.org/abs/2503.13123v1),  [pdf](http://arxiv.org/pdf/2503.13123v1)

**Tags**: cs.RO cs.AI 



### Video Depth without Video Models
**Authors**: Bingxin Ke, Dominik Narnhofer, Shengyu Huang, Lei Ke, Torben Peters, Katerina Fragkiadaki, Anton Obukhov, Konrad Schindler

**Updated**: 2025-03-17T12:43:52Z

**Summary**: Video depth estimation lifts monocular video clips to 3D by inferring dense depth at every frame. Recent advances in single-image depth estimation, brought about by the rise of large foundation models and the use of synthetic training data, have fueled a renewed interest in video depth. However, naively applying a single-image depth estimator to every frame of a video disregards temporal continuity, which not only leads to flickering but may also break when camera motion causes sudden changes in depth range. An obvious and principled solution would be to build on top of video foundation models, but these come with their own limitations; including expensive training and inference, imperfect 3D consistency, and stitching routines for the fixed-length (short) outputs. We take a step back and demonstrate how to turn a single-image latent diffusion model (LDM) into a state-of-the-art video depth estimator. Our model, which we call RollingDepth, has two main ingredients: (i) a multi-frame depth estimator that is derived from a single-image LDM and maps very short video snippets (typically frame triplets) to depth snippets. (ii) a robust, optimization-based registration algorithm that optimally assembles depth snippets sampled at various different frame rates back into a consistent video. RollingDepth is able to efficiently handle long videos with hundreds of frames and delivers more accurate depth videos than both dedicated video depth estimators and high-performing single-frame models. Project page: rollingdepth.github.io.

**Link**: [arxiv](http://arxiv.org/abs/2411.19189v2),  [pdf](http://arxiv.org/pdf/2411.19189v2)

**Tags**: cs.CV 



### Cornstarch: Distributed Multimodal Training Must Be Multimodality-Aware
**Authors**: Insu Jang, Runyu Lu, Nikhil Bansal, Ang Chen, Mosharaf Chowdhury

**Updated**: 2025-03-17T12:39:15Z

**Summary**: Multimodal large language models (MLLMs) extend the capabilities of large language models (LLMs) by combining heterogeneous model architectures to handle diverse modalities like images and audio. However, this inherent heterogeneity in MLLM model structure and data types makes makeshift extensions to existing LLM training frameworks unsuitable for efficient MLLM training.   In this paper, we present Cornstarch, the first general-purpose distributed MLLM training framework. Cornstarch facilitates modular MLLM construction, enables composable parallelization of constituent models, and introduces MLLM-specific optimizations to pipeline and context parallelism for efficient distributed MLLM training. Our evaluation shows that Cornstarch outperforms state-of-the-art solutions by up to $1.57\times$ in terms of training throughput.   Cornstarch is an open-source project available at https://github.com/cornstarch-org/Cornstarch.

**Link**: [arxiv](http://arxiv.org/abs/2503.11367v2),  [pdf](http://arxiv.org/pdf/2503.11367v2)

**Tags**: cs.DC 



### VeriLeaky: Navigating IP Protection vs Utility in Fine-Tuning for   LLM-Driven Verilog Coding
**Authors**: Zeng Wang, Minghao Shao, Mohammed Nabeel, Prithwish Basu Roy, Likhitha Mankali, Jitendra Bhandari, Ramesh Karri, Ozgur Sinanoglu, Muhammad Shafique, Johann Knechtel

**Updated**: 2025-03-17T12:38:03Z

**Summary**: Large language models (LLMs) offer significant potential for coding, yet fine-tuning (FT) with curated data is essential for niche languages like Verilog. Using proprietary intellectual property (IP) for FT presents a serious risk, as FT data can be leaked through LLM inference. This leads to a critical dilemma for design houses: seeking to build externally accessible LLMs offering competitive Verilog coding, how can they leverage in-house IP to enhance FT utility while ensuring IP protection?   For the first time in the literature, we study this dilemma. Using LLaMA 3.1-8B, we conduct in-house FT on a baseline Verilog dataset (RTLCoder) supplemented with our own in-house IP, which is validated through multiple tape-outs. To rigorously assess IP leakage, we quantify structural similarity (AST/Dolos) and functional equivalence (Synopsys Formality) between generated codes and our in-house IP. We show that our IP can indeed be leaked, confirming the threat. As defense, we evaluate logic locking of Verilog codes (ASSURE). This offers some level of protection, yet reduces the IP's utility for FT and degrades the LLM's performance. Our study shows the need for novel strategies that are both effective and minimally disruptive to FT, an essential effort for enabling design houses to fully utilize their proprietary IP toward LLM-driven Verilog coding.

**Link**: [arxiv](http://arxiv.org/abs/2503.13116v1),  [pdf](http://arxiv.org/pdf/2503.13116v1)

**Tags**: cs.CR cs.AR cs.LG 



### MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs
**Authors**: Erik Daxberger, Nina Wenzel, David Griffiths, Haiming Gang, Justin Lazarow, Gefen Kohavi, Kai Kang, Marcin Eichner, Yinfei Yang, Afshin Dehghan, Peter Grasch

**Updated**: 2025-03-17T12:34:22Z

**Summary**: Multimodal large language models (MLLMs) excel at 2D visual understanding but remain limited in their ability to reason about 3D space. In this work, we leverage large-scale high-quality 3D scene data with open-set annotations to introduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation benchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data covers diverse spatial tasks including spatial relationship prediction, metric size and distance estimation, and 3D grounding. We show that CA-VQA enables us to train MM-Spatial, a strong generalist MLLM that also achieves state-of-the-art performance on 3D spatial understanding benchmarks, including our own. We show how incorporating metric depth and multi-view inputs (provided in CA-VQA) can further improve 3D understanding, and demonstrate that data alone allows our model to achieve depth perception capabilities comparable to dedicated monocular depth estimation models. We will publish our SFT dataset and benchmark.

**Link**: [arxiv](http://arxiv.org/abs/2503.13111v1),  [pdf](http://arxiv.org/pdf/2503.13111v1)

**Tags**: cs.CV cs.CL cs.LG 



### Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways   to Faster Inference
**Authors**: Hao Yin, Guangzong Si, Zilei Wang

**Updated**: 2025-03-17T12:31:23Z

**Summary**: Multimodal large language models (MLLMs) improve performance on vision-language tasks by integrating visual features from pre-trained vision encoders into large language models (LLMs). However, how MLLMs process and utilize visual information remains unclear. In this paper, a shift in the dominant flow of visual information is uncovered: (1) in shallow layers, strong interactions are observed between image tokens and instruction tokens, where most visual information is injected into instruction tokens to form cross-modal semantic representations; (2) in deeper layers, image tokens primarily interact with each other, aggregating the remaining visual information to optimize semantic representations within visual modality. Based on these insights, we propose Hierarchical Modality-Aware Pruning (HiMAP), a plug-and-play inference acceleration method that dynamically prunes image tokens at specific layers, reducing computational costs by approximately 65% without sacrificing performance. Our findings offer a new understanding of visual information processing in MLLMs and provide a state-of-the-art solution for efficient inference.

**Link**: [arxiv](http://arxiv.org/abs/2503.13108v1),  [pdf](http://arxiv.org/pdf/2503.13108v1)

**Tags**: cs.CV cs.AI 



### ClearSight: Visual Signal Enhancement for Object Hallucination   Mitigation in Multimodal Large language Models
**Authors**: Hao Yin, Guangzong Si, Zilei Wang

**Updated**: 2025-03-17T12:30:40Z

**Summary**: Contrastive decoding strategies are widely used to mitigate object hallucinations in multimodal large language models (MLLMs). By reducing over-reliance on language priors, these strategies ensure that generated content remains closely grounded in visual inputs, producing contextually accurate outputs. Since contrastive decoding requires no additional training or external tools, it offers both computational efficiency and versatility, making it highly attractive. However, these methods present two main limitations: (1) bluntly suppressing language priors can compromise coherence and accuracy of generated content, and (2) processing contrastive inputs adds computational load, significantly slowing inference speed. To address these challenges, we propose Visual Amplification Fusion (VAF), a plug-and-play technique that enhances attention to visual signals within the model's middle layers, where modality fusion predominantly occurs. This approach enables more effective capture of visual features, reducing the model's bias toward language modality. Experimental results demonstrate that VAF significantly reduces hallucinations across various MLLMs without affecting inference speed, while maintaining coherence and accuracy in generated outputs.

**Link**: [arxiv](http://arxiv.org/abs/2503.13107v1),  [pdf](http://arxiv.org/pdf/2503.13107v1)

**Tags**: cs.CV cs.AI 



### Fine-tuning can Help Detect Pretraining Data from Large Language Models
**Authors**: Hengxiang Zhang, Songxin Zhang, Bingyi Jing, Hongxin Wei

**Updated**: 2025-03-17T12:29:05Z

**Summary**: In the era of large language models (LLMs), detecting pretraining data has been increasingly important due to concerns about fair evaluation and ethical risks. Current methods differentiate members and non-members by designing scoring functions, like Perplexity and Min-k%. However, the diversity and complexity of training data magnifies the difficulty of distinguishing, leading to suboptimal performance in detecting pretraining data. In this paper, we first explore the benefits of unseen data, which can be easily collected after the release of the LLM. We find that the perplexities of LLMs shift differently for members and non-members, after fine-tuning with a small amount of previously unseen data. In light of this, we introduce a novel and effective method termed Fine-tuned Score Deviation(FSD), which improves the performance of current scoring functions for pretraining data detection. In particular, we propose to measure the deviation distance of current scores after fine-tuning on a small amount of unseen data within the same domain. In effect, using a few unseen data can largely decrease the scores of all non-members, leading to a larger deviation distance than members. Extensive experiments demonstrate the effectiveness of our method, significantly improving the AUC score on common benchmark datasets across various models.

**Link**: [arxiv](http://arxiv.org/abs/2410.10880v2),  [pdf](http://arxiv.org/pdf/2410.10880v2)

**Tags**: cs.CL cs.AI cs.LG 



### Managing Hybrid Solid-State Drives Using Large Language Models
**Authors**: Qian Wei, Yi Li, Zehao Chen, Zhaoyan Shen, Dongxiao Yu, Bingzhe Li

**Updated**: 2025-03-17T12:25:42Z

**Summary**: Hybrid Solid-State Drives (SSDs), which integrate several types of flash cells (e.g., single-level cell (SLC) and multiple-level cell (MLC)) in a single drive and enable them to convert between each other, are designed to deliver both high performance and high storage capacity. However, compared to traditional SSDs, hybrid SSDs also introduce a much larger design space, resulting in higher optimization complexity due to more design factors involved, including flash conversion timing and data migration between different flash cells, etc. To address these challenges, large language models (LLMs) could be a promising technique, as they excel in handling complex, high-dimensional parameter space exploration by leveraging their advanced capability to identify patterns and optimize solutions. Recent works have started exploring the use of LLMs to optimize computer systems. However, to the best of our knowledge, no study has focused on optimizing SSDs with the assistance of LLMs.   In this work, we explore the potential of LLMs in understanding and efficiently managing hybrid SSD design space. Specifically, two important questions are exploited and analyzed: 1) Can LLMs offer optimization potential for Hybrid SSD management? 2) How to leverage LLMs for the performance and efficiency of hybrid SSD optimization? Based on the observations of exploration, we propose a comprehensive auto-tuning framework for hybrid SSDs, integrating LLMs to recommend customized configurations using calibration prompts derived from hardware, system, and workload information. Experimental results reveal a 62.35% improvement in throughput and a 57.99% decrease in write amplification compared to the default hybrid SSD configurations achieved with the incorporation of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2503.13105v1),  [pdf](http://arxiv.org/pdf/2503.13105v1)

**Tags**: cs.AR 



### REPA: Russian Error Types Annotation for Evaluating Text Generation and   Judgment Capabilities
**Authors**: Alexander Pugachev, Alena Fenogenova, Vladislav Mikhailov, Ekaterina Artemova

**Updated**: 2025-03-17T12:15:16Z

**Summary**: Recent advances in large language models (LLMs) have introduced the novel paradigm of using LLMs as judges, where an LLM evaluates and scores the outputs of another LLM, which often correlates highly with human preferences. However, the use of LLM-as-a-judge has been primarily studied in English. In this paper, we evaluate this framework in Russian by introducing the Russian Error tyPes Annotation dataset (REPA), a dataset of 1k user queries and 2k LLM-generated responses. Human annotators labeled each response pair expressing their preferences across ten specific error types, as well as selecting an overall preference. We rank six generative LLMs across the error types using three rating systems based on human preferences. We also evaluate responses using eight LLM judges in zero-shot and few-shot settings. We describe the results of analyzing the judges and position and length biases. Our findings reveal a notable gap between LLM judge performance in Russian and English. However, rankings based on human and LLM preferences show partial alignment, suggesting that while current LLM judges struggle with fine-grained evaluation in Russian, there is potential for improvement.

**Link**: [arxiv](http://arxiv.org/abs/2503.13102v1),  [pdf](http://arxiv.org/pdf/2503.13102v1)

**Tags**: cs.CL 



### Who Wrote This? Identifying Machine vs Human-Generated Text in Hausa
**Authors**: Babangida Sani, Aakansha Soy, Sukairaj Hafiz Imam, Ahmad Mustapha, Lukman Jibril Aliyu, Idris Abdulmumin, Ibrahim Said Ahmad, Shamsuddeen Hassan Muhammad

**Updated**: 2025-03-17T12:13:37Z

**Summary**: The advancement of large language models (LLMs) has allowed them to be proficient in various tasks, including content generation. However, their unregulated usage can lead to malicious activities such as plagiarism and generating and spreading fake news, especially for low-resource languages. Most existing machine-generated text detectors are trained on high-resource languages like English, French, etc. In this study, we developed the first large-scale detector that can distinguish between human- and machine-generated content in Hausa. We scrapped seven Hausa-language media outlets for the human-generated text and the Gemini-2.0 flash model to automatically generate the corresponding Hausa-language articles based on the human-generated article headlines. We fine-tuned four pre-trained Afri-centric models (AfriTeVa, AfriBERTa, AfroXLMR, and AfroXLMR-76L) on the resulting dataset and assessed their performance using accuracy and F1-score metrics. AfroXLMR achieved the highest performance with an accuracy of 99.23% and an F1 score of 99.21%, demonstrating its effectiveness for Hausa text detection. Our dataset is made publicly available to enable further research.

**Link**: [arxiv](http://arxiv.org/abs/2503.13101v1),  [pdf](http://arxiv.org/pdf/2503.13101v1)

**Tags**: cs.CL 



### Beemo: Benchmark of Expert-edited Machine-generated Outputs
**Authors**: Ekaterina Artemova, Jason Lucas, Saranya Venkatraman, Jooyoung Lee, Sergei Tilga, Adaku Uchendu, Vladislav Mikhailov

**Updated**: 2025-03-17T12:05:36Z

**Summary**: The rapid proliferation of large language models (LLMs) has increased the volume of machine-generated texts (MGTs) and blurred text authorship in various domains. However, most existing MGT benchmarks include single-author texts (human-written and machine-generated). This conventional design fails to capture more practical multi-author scenarios, where the user refines the LLM response for natural flow, coherence, and factual correctness. Our paper introduces the Benchmark of Expert-edited Machine-generated Outputs (Beemo), which includes 6.5k texts written by humans, generated by ten instruction-finetuned LLMs, and edited by experts for various use cases, ranging from creative writing to summarization. Beemo additionally comprises 13.1k machine-generated and LLM-edited texts, allowing for diverse MGT detection evaluation across various edit types. We document Beemo's creation protocol and present the results of benchmarking 33 configurations of MGT detectors in different experimental setups. We find that expert-based editing evades MGT detection, while LLM-edited texts are unlikely to be recognized as human-written. Beemo and all materials are publicly available.

**Link**: [arxiv](http://arxiv.org/abs/2411.04032v3),  [pdf](http://arxiv.org/pdf/2411.04032v3)

**Tags**: cs.CL 



### ClusComp: A Simple Paradigm for Model Compression and Efficient   Finetuning
**Authors**: Baohao Liao, Christian Herold, Seyyed Hadi Hashemi, Stefan Vasilev, Shahram Khadivi, Christof Monz

**Updated**: 2025-03-17T11:52:16Z

**Summary**: As large language models (LLMs) scale, model compression is crucial for edge deployment and accessibility. Weight-only quantization reduces model size but suffers from performance degradation at lower bit widths. Moreover, standard finetuning is incompatible with quantized models, and alternative methods often fall short of full finetuning. In this paper, we propose ClusComp, a simple yet effective compression paradigm that clusters weight matrices into codebooks and finetunes them block-by-block. ClusComp (1) achieves superior performance in 2-4 bit quantization, (2) pushes compression to 1-bit while outperforming ultra-low-bit methods with minimal finetuning, and (3) enables efficient finetuning, even surpassing existing quantization-based approaches and rivaling full FP16 finetuning. Notably, ClusComp supports compression and finetuning of 70B LLMs on a single A6000-48GB GPU.

**Link**: [arxiv](http://arxiv.org/abs/2503.13089v1),  [pdf](http://arxiv.org/pdf/2503.13089v1)

**Tags**: cs.CL cs.AI 



### A Framework to Assess Multilingual Vulnerabilities of LLMs
**Authors**: Likai Tang, Niruth Bogahawatta, Yasod Ginige, Jiarui Xu, Shixuan Sun, Surangika Ranathunga, Suranga Seneviratne

**Updated**: 2025-03-17T11:39:44Z

**Summary**: Large Language Models (LLMs) are acquiring a wider range of capabilities, including understanding and responding in multiple languages. While they undergo safety training to prevent them from answering illegal questions, imbalances in training data and human evaluation resources can make these models more susceptible to attacks in low-resource languages (LRL). This paper proposes a framework to automatically assess the multilingual vulnerabilities of commonly used LLMs. Using our framework, we evaluated six LLMs across eight languages representing varying levels of resource availability. We validated the assessments generated by our automated framework through human evaluation in two languages, demonstrating that the framework's results align with human judgments in most cases. Our findings reveal vulnerabilities in LRL; however, these may pose minimal risk as they often stem from the model's poor performance, resulting in incoherent responses.

**Link**: [arxiv](http://arxiv.org/abs/2503.13081v1),  [pdf](http://arxiv.org/pdf/2503.13081v1)

**Tags**: cs.CL cs.AI 



### A robust score test in g-computation for covariate adjustment in   randomized clinical trials leveraging different variance estimators via   influence functions
**Authors**: Xin Zhang, Haitao Chu, Lin Liu, Satrajit Roychoudhury

**Updated**: 2025-03-17T11:11:18Z

**Summary**: G-computation has become a widely used robust method for estimating unconditional (marginal) treatment effects with covariate adjustment in the analysis of randomized clinical trials. Statistical inference in this context typically relies on the Wald test or Wald interval, which can be easily implemented using a consistent variance estimator. However, existing literature suggests that when sample sizes are small or when parameters of interest are near boundary values, Wald-based methods may be less reliable due to type I error rate inflation and insufficient interval coverage. In this article, we propose a robust score test for g-computation estimators in the context of two-sample treatment comparisons. The proposed test is asymptotically valid under simple and stratified (biased-coin) randomization schemes, even when regression models are misspecified. These test statistics can be conveniently computed using existing variance estimators, and the corresponding confidence intervals have closed-form expressions, making them convenient to implement. Through extensive simulations, we demonstrate the superior finite-sample performance of the proposed method. Finally, we apply the proposed method to reanalyze a completed randomized clinical trial. The new analysis using our proposed score test achieves statistical significance, whilst reducing the issue of type I error inflation.

**Link**: [arxiv](http://arxiv.org/abs/2503.13066v1),  [pdf](http://arxiv.org/pdf/2503.13066v1)

**Tags**: stat.ME stat.AP 



### Open-Set Gait Recognition from Sparse mmWave Radar Point Clouds
**Authors**: Riccardo Mazzieri, Jacopo Pegoraro, Michele Rossi

**Updated**: 2025-03-17T11:06:08Z

**Summary**: The adoption of Millimeter-Wave (mmWave) radar devices for human sensing, particularly gait recognition, has recently gathered significant attention due to their efficiency, resilience to environmental conditions, and privacy-preserving nature. In this work, we tackle the challenging problem of Open-set Gait Recognition (OSGR) from sparse mmWave radar point clouds. Unlike most existing research, which assumes a closed-set scenario, our work considers the more realistic open-set case, where unknown subjects might be present at inference time, and should be correctly recognized by the system. Point clouds are well-suited for edge computing applications with resource constraints, but are more significantly affected by noise and random fluctuations than other representations, like the more common micro-Doppler signature. This is the first work addressing open-set gait recognition with sparse point cloud data. To do so, we propose a novel neural network architecture that combines supervised classification with unsupervised reconstruction of the point clouds, creating a robust, rich, and highly regularized latent space of gait features. To detect unknown subjects at inference time, we introduce a probabilistic novelty detection algorithm that leverages the structured latent space and offers a tunable trade-off between inference speed and prediction accuracy. Along with this paper, we release mmGait10, an original human gait dataset featuring over five hours of measurements from ten subjects, under varied walking modalities. Extensive experimental results show that our solution attains F1-Score improvements by 24% over state-of-the-art methods, on average, and across multiple openness levels.

**Link**: [arxiv](http://arxiv.org/abs/2503.07435v2),  [pdf](http://arxiv.org/pdf/2503.07435v2)

**Tags**: cs.CV eess.SP 



### MaskSDM with Shapley values to improve flexibility, robustness, and   explainability in species distribution modeling
**Authors**: Robin Zbinden, Nina van Tiel, Gencer Sumbul, Chiara Vanalli, Benjamin Kellenberger, Devis Tuia

**Updated**: 2025-03-17T11:02:28Z

**Summary**: Species Distribution Models (SDMs) play a vital role in biodiversity research, conservation planning, and ecological niche modeling by predicting species distributions based on environmental conditions. The selection of predictors is crucial, strongly impacting both model accuracy and how well the predictions reflect ecological patterns. To ensure meaningful insights, input variables must be carefully chosen to match the study objectives and the ecological requirements of the target species. However, existing SDMs, including both traditional and deep learning-based approaches, often lack key capabilities for variable selection: (i) flexibility to choose relevant predictors at inference without retraining; (ii) robustness to handle missing predictor values without compromising accuracy; and (iii) explainability to interpret and accurately quantify each predictor's contribution. To overcome these limitations, we introduce MaskSDM, a novel deep learning-based SDM that enables flexible predictor selection by employing a masked training strategy. This approach allows the model to make predictions with arbitrary subsets of input variables while remaining robust to missing data. It also provides a clearer understanding of how adding or removing a given predictor affects model performance and predictions. Additionally, MaskSDM leverages Shapley values for precise predictor contribution assessments, improving upon traditional approximations. We evaluate MaskSDM on the global sPlotOpen dataset, modeling the distributions of 12,738 plant species. Our results show that MaskSDM outperforms imputation-based methods and approximates models trained on specific subsets of variables. These findings underscore MaskSDM's potential to increase the applicability and adoption of SDMs, laying the groundwork for developing foundation models in SDMs that can be readily applied to diverse ecological applications.

**Link**: [arxiv](http://arxiv.org/abs/2503.13057v1),  [pdf](http://arxiv.org/pdf/2503.13057v1)

**Tags**: cs.LG cs.CV 



### Perturbing a quantum black hole
**Authors**: Casey Cartwright, Umut Gürsoy, Juan F. Pedraza, Guim Planella Planas

**Updated**: 2025-03-17T10:47:22Z

**Summary**: We analyze the analytic structure of correlators in the field theory dual to the quantum Ba\~{n}ados-Teitelboim-Zanelli (qBTZ) black hole, a braneworld model incorporating exact backreaction from quantum conformal matter. We first compute the quasi-normal mode (QNM) spectrum of operators with dimension $\Delta$ and spin $s=0,\pm 1/2$. The leading QNMs and their overtones display qualitatively different behavior depending on the branch of qBTZ solution, which corresponds to distinct CFT states: branch 1 is a conical singularity dressed with a horizon while branch 2 is a quantum-corrected BTZ black hole. Consequently, the relaxation of probe matter effectively differentiates the CFT states and identifies the corresponding bulk descriptions. We then turn to pole-skipping locations where Green's functions are not unique. At these points, frequency is proportional to temperature, but momentum exhibits complex temperature dependence due to quantum effects. Under the assumption that the pole-skipping point closest to the origin reflects quantum chaos, we infer the likely behavior of the quantum Lyapunov exponent and butterfly velocity in the dual theory. Finally, we examine pole collisions in complex momentum space, showing that quantum corrections imprint a unique signature on the analytic structure of the poles in retarded Green's functions, resulting in level-crossing phenomena that differ notably from the level-touching phenomena in the uncorrected BTZ geometry.

**Link**: [arxiv](http://arxiv.org/abs/2408.08010v2),  [pdf](http://arxiv.org/pdf/2408.08010v2)

**Tags**: hep-th gr-qc 



### Evaluating the Instruction-following Abilities of Language Models using   Knowledge Tasks
**Authors**: Rudra Murthy, Praveen Venkateswaran, Prince Kumar, Danish Contractor

**Updated**: 2025-03-17T10:45:15Z

**Summary**: LLM evaluation benchmarks have traditionally separated the testing of knowledge/reasoning capabilities from instruction following. In this work, we study the interaction between knowledge and instruction following, and observe that LLMs struggle to follow simple answer modifying instructions, and are also distracted by instructions that should have no bearing on the original knowledge task answer. We leverage existing multiple-choice answer based knowledge benchmarks and apply a set of simple instructions which include manipulating text (eg.: change case), numeric quantities (eg.: increase value, change formatting), operate on lists (eg.: sort answer candidates) and distractor instructions (eg.: change case of numeric answers).

**Link**: [arxiv](http://arxiv.org/abs/2410.12972v2),  [pdf](http://arxiv.org/pdf/2410.12972v2)

**Tags**: cs.CL 



### In-Context Learning Enables Robot Action Prediction in LLMs
**Authors**: Yida Yin, Zekai Wang, Yuvan Sharma, Dantong Niu, Trevor Darrell, Roei Herzig

**Updated**: 2025-03-17T10:43:54Z

**Summary**: Recently, Large Language Models (LLMs) have achieved remarkable success using in-context learning (ICL) in the language domain. However, leveraging the ICL capabilities within LLMs to directly predict robot actions remains largely unexplored. In this paper, we introduce RoboPrompt, a framework that enables off-the-shelf text-only LLMs to directly predict robot actions through ICL without training. Our approach first heuristically identifies keyframes that capture important moments from an episode. Next, we extract end-effector actions from these keyframes as well as the estimated initial object poses, and both are converted into textual descriptions. Finally, we construct a structured template to form ICL demonstrations from these textual descriptions and a task instruction. This enables an LLM to directly predict robot actions at test time. Through extensive experiments and analysis, RoboPrompt shows stronger performance over zero-shot and ICL baselines in simulated and real-world settings. Our project page is available at https://davidyyd.github.io/roboprompt.

**Link**: [arxiv](http://arxiv.org/abs/2410.12782v2),  [pdf](http://arxiv.org/pdf/2410.12782v2)

**Tags**: cs.RO cs.CL 



### Overview of the NTCIR-18 Automatic Evaluation of LLMs (AEOLLM) Task
**Authors**: Junjie Chen, Haitao Li, Zhumin Chu, Yiqun Liu, Qingyao Ai

**Updated**: 2025-03-17T10:42:34Z

**Summary**: In this paper, we provide an overview of the NTCIR-18 Automatic Evaluation of LLMs (AEOLLM) task. As large language models (LLMs) grow popular in both academia and industry, how to effectively evaluate the capacity of LLMs becomes an increasingly critical but still challenging issue. Existing methods can be divided into two types: manual evaluation, which is expensive, and automatic evaluation, which faces many limitations including task format (the majority belong to multiple-choice questions) and evaluation criteria (occupied by reference-based metrics). To advance the innovation of automatic evaluation, we propose the AEOLLM task which focuses on generative tasks and encourages reference-free methods. Besides, we set up diverse subtasks such as dialogue generation, text expansion, summary generation and non-factoid question answering to comprehensively test different methods. This year, we received 48 runs from 4 teams in total. This paper will describe the background of the task, the data set, the evaluation measures and the evaluation results, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2503.13038v1),  [pdf](http://arxiv.org/pdf/2503.13038v1)

**Tags**: cs.CL 



### H-AddiVortes: Heteroscedastic (Bayesian) Additive Voronoi Tessellations
**Authors**: Adam J. Stone, John Paul Gosling

**Updated**: 2025-03-17T10:41:31Z

**Summary**: This paper introduces the Heteroscedastic AddiVortes model, a Bayesian non-parametric regression framework that simultaneously models the conditional mean and variance of a response variable using adaptive Voronoi tessellations. By employing a sum-of-tessellations approach for the mean and a product-of-tessellations approach for the variance, the model provides a flexible and interpretable means to capture complex, predictor-dependent relationships and heteroscedastic patterns in data. This dual-layer representation enables precise inference, even in high-dimensional settings, while maintaining computational feasibility through efficient Markov Chain Monte Carlo (MCMC) sampling and conjugate prior structures. We illustrate the model's capability through both simulated and real-world datasets, demonstrating its ability to capture nuanced variance structures, provide reliable predictive uncertainty quantification, and highlight key predictors influencing both the mean response and its variability. Empirical results show that the Heteroscedastic AddiVortes model offers a substantial improvement in capturing distributional properties compared to both homoscedastic and heteroscedastic alternatives, making it a robust tool for complex regression problems in various applied settings.

**Link**: [arxiv](http://arxiv.org/abs/2503.13037v1),  [pdf](http://arxiv.org/pdf/2503.13037v1)

**Tags**: stat.ME stat.CO stat.ML 



### Learning Spatially Adaptive $\ell_1$-Norms Weights for Convolutional   Synthesis Regularization
**Authors**: Andreas Kofler, Luca Calatroni, Christoph Kolbitsch, Kostas Papafitsoros

**Updated**: 2025-03-17T10:38:39Z

**Summary**: We propose an unrolled algorithm approach for learning spatially adaptive parameter maps in the framework of convolutional synthesis-based $\ell_1$ regularization. More precisely, we consider a family of pre-trained convolutional filters and estimate deeply parametrized spatially varying parameters applied to the sparse feature maps by means of unrolling a FISTA algorithm to solve the underlying sparse estimation problem. The proposed approach is evaluated for image reconstruction of low-field MRI and compared to spatially adaptive and non-adaptive analysis-type procedures relying on Total Variation regularization and to a well-established model-based deep learning approach. We show that the proposed approach produces visually and quantitatively comparable results with the latter approaches and at the same time remains highly interpretable. In particular, the inferred parameter maps quantify   the local contribution of each filter in the reconstruction, which provides valuable insight into the algorithm mechanism and could potentially be used to discard unsuited filters.

**Link**: [arxiv](http://arxiv.org/abs/2503.09483v2),  [pdf](http://arxiv.org/pdf/2503.09483v2)

**Tags**: cs.LG cs.CV math.OC 



### TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation
**Authors**: Lin Sun, Guangxiang Zhao, Xiaoqi Jian, Yuhan Wu, Weihong Lin, Yongfu Zhu, Change Jia, Linglin Zhang, Jinzhu Wu, Junfeng Ran, Sai-er Hu, Zihan Jiang, Junting Zhou, Wenrui Liu, Bin Cui, Tong Yang, Xiangzheng Zhang

**Updated**: 2025-03-17T10:36:30Z

**Summary**: The challenge of reducing the size of Large Language Models (LLMs) while maintaining their performance has gained significant attention. However, existing methods, such as model distillation and transfer learning, often fail to achieve high accuracy. To address this limitation, we introduce the Branch-Merge distillation approach, which enhances model compression through two phases: (1) the Branch Phase, where knowledge from a large teacher model is \textit{selectively distilled} into specialized student models via domain-specific supervised fine-tuning (SFT); And (2) the Merge Phase, where these student models are merged to enable cross-domain knowledge transfer and improve generalization. We validate our distillation approach using DeepSeek-R1 as the teacher and DeepSeek-R1-Distill-Qwen-32B as the student. The resulting merged model, TinyR1-32B-Preview, outperforms its counterpart DeepSeek-R1-Distill-Qwen-32B across multiple benchmarks, including Mathematics (+5.5 points), Coding (+4.4 points) and Science (+2.9 points), while achieving near-equal performance to DeepSeek-R1 on AIME 2024. The Branch-Merge distillation approach provides a scalable solution for creating smaller, high-performing LLMs with reduced computational cost and time.

**Link**: [arxiv](http://arxiv.org/abs/2503.04872v2),  [pdf](http://arxiv.org/pdf/2503.04872v2)

**Tags**: cs.CL cs.AI 



### Chain of Evidences and Evidence to Generate: Prompting for Context   Grounded and Retrieval Augmented Reasoning
**Authors**: Md Rizwan Parvez

**Updated**: 2025-03-17T10:35:11Z

**Summary**: While chain-of-thoughts (CoT) prompting has revolutionized how LLMs perform reasoning tasks, its current methods and variations (e.g, Self-consistency, ReACT, Reflexion, Tree-of-Thoughts (ToT), Cumulative Reasoning (CR) etc.,) suffer from limitations like limited context grounding, hallucination/inconsistent output generation, and iterative sluggishness. To overcome these challenges, we introduce a novel mono/dual-step zero-shot prompting framework built upon two unique strategies Chain of Evidences (CoE)} and Evidence to Generate (E2G). Instead of unverified reasoning claims, our innovative approaches leverage the power of "evidence for decision making" by first focusing exclusively on the thought sequences explicitly mentioned in the context which then serve as extracted evidence, guiding the LLM's output generation process with greater precision and efficiency. This simple yet potent approach unlocks the full potential of chain-of-thoughts prompting, facilitating faster, more reliable, and contextually aware reasoning in LLMs. Our framework consistently achieves remarkable results across various knowledge-intensive reasoning and generation tasks, surpassing baseline approaches with state-of-the-art LLMs. For instance, (i) on the LogiQA benchmark using GPT-4, CoE achieves a new state-of-the-art accuracy of 53.8%, surpassing CoT by 18%, ToT by 11%, and CR by 9%; (ii) CoE with PaLM-2 outperforms the variable-shot performance of Gemini Ultra by 0.9 F1 points, achieving an F1 score of 83.3 on DROP. We release our prompts and outputs on these benchmarks as a new instruction tuning dataset for future research at https://huggingface.co/datasets/kagnlp/Chain-of-Evidences/.

**Link**: [arxiv](http://arxiv.org/abs/2401.05787v2),  [pdf](http://arxiv.org/pdf/2401.05787v2)

**Tags**: cs.CL 



### HiMTok: Learning Hierarchical Mask Tokens for Image Segmentation with   Large Multimodal Model
**Authors**: Tao Wang, Changxu Cheng, Lingfeng Wang, Senda Chen, Wuyue Zhao

**Updated**: 2025-03-17T10:29:08Z

**Summary**: The remarkable performance of large multimodal models (LMMs) has attracted significant interest from the image segmentation community. To align with the next-token-prediction paradigm, current LMM-driven segmentation methods either use object boundary points to represent masks or introduce special segmentation tokens, whose hidden states are decoded by a segmentation model requiring the original image as input. However, these approaches often suffer from inadequate mask representation and complex architectures, limiting the potential of LMMs. In this work, we propose the Hierarchical Mask Tokenizer (HiMTok), which represents segmentation masks with up to 32 tokens and eliminates the need for the original image during mask de-tokenization. HiMTok allows for compact and coarse-to-fine mask representations, aligning well with the LLM next-token-prediction paradigm and facilitating the direct acquisition of segmentation capabilities. We develop a 3-stage training recipe for progressive learning of segmentation and visual capabilities, featuring a hierarchical mask loss for effective coarse-to-fine learning. Additionally, we enable bidirectional information flow, allowing conversion between bounding boxes and mask tokens to fully leverage multi-task training potential. Extensive experiments demonstrate that our method achieves state-of-the-art performance across various segmentation tasks,while also enhancing visual grounding and maintaining overall visual understanding.

**Link**: [arxiv](http://arxiv.org/abs/2503.13026v1),  [pdf](http://arxiv.org/pdf/2503.13026v1)

**Tags**: cs.CV 



### Dynamic Relation Inference via Verb Embeddings
**Authors**: Omri Suissa, Muhiim Ali, Ariana Azarbal, Hui Shen, Shekhar Pradhan

**Updated**: 2025-03-17T10:24:27Z

**Summary**: CLIP has demonstrated exceptional image-text matching capabilities due to its training on contrastive learning tasks. Past research has suggested that whereas CLIP effectively matches text to images when the matching can be achieved just by matching the text with the objects in the image, CLIP struggles when the matching depends on representing the relationship among the objects in the images (i.e., inferring relations). Previous attempts to address this limitation by training CLIP on relation detection datasets with only linguistic supervision have met with limited success. In this paper, we offer insights and practical methods to advance the field of relation inference from images. This paper approaches the task of creating a model that effectively detects relations among the objects in images by producing text and image embeddings that capture relationships through linguistic supervision. To this end, we propose Dynamic Relation Inference via Verb Embeddings (DRIVE), which augments the COCO dataset, fine-tunes CLIP with hard negatives subject-relation-object triples and corresponding images, and introduces a novel loss function to improve relation detection. Evaluated on multiple CLIP-based models, our method significantly improves zero-shot relation inference accuracy in both frozen and fine-tuned settings, significantly outperforming CLIP and state-of-the-art models while generalizing well on unseen data.

**Link**: [arxiv](http://arxiv.org/abs/2503.13021v1),  [pdf](http://arxiv.org/pdf/2503.13021v1)

**Tags**: cs.CL cs.CV 



### The nebular spectra of SN 2023ixf: A lower mass, partially stripped   progenitor may be the result of binary interaction
**Authors**: Philip D. Michel, Paolo A. Mazzali, Daniel A. Perley, K-Ryan Hinds, Jacob L. Wise

**Updated**: 2025-03-17T10:21:43Z

**Summary**: SN 2023ixf is one of the brightest Core Collapse Supernovae of the 21st century and offers a rare opportunity to investigate the late stage of a Supernova through nebular phase spectroscopy. We present four nebular phase spectra from day +291 to +413 after explosion. This is supplemented with high cadence early phase spectroscopic observations and photometry covering the first 500 days to investigate explosion parameters. The narrow and blue-shifted nebular Oxygen emission lines are used to infer an ejected Oxygen mass of $<0.65M_\odot$, consistent with models of a relatively low mass ($M_{ZAMS}<15M_\odot$) progenitor. An energy of 0.3 to $1.4 \times10^{51}$ erg and a light curve powered by an initial $^{56}$Ni mass of $0.049 \pm 0.005 M_\odot$ appear consistent with a relatively standard Type II explosion, while an incomplete $\gamma$-ray trapping (with timescale of $240\pm4$ days) suggests a lower ejecta mass. Assuming a typical explosion, the broad Hydrogen and Calcium profiles suggest a common origin within a lower mass, partially stripped envelope. Hydrogen emission broadens with time, indicating contribution from an additional power source at an extended distance; while the emergence of high velocity ($\sim$6,000 km s$^{-1}$) Hydrogen emission features (beginning around day +200) may be explained by Shock Interaction with a dense Hydrogen-rich region located at $\sim1.5 \times 10^{16}$cm. Such envelope mass loss for a low mass progenitor may be explained through theoretical models of Binary interaction.

**Link**: [arxiv](http://arxiv.org/abs/2503.13017v1),  [pdf](http://arxiv.org/pdf/2503.13017v1)

**Tags**: astro-ph.HE astro-ph.GA astro-ph.SR 



### Efficient Motion-Aware Video MLLM
**Authors**: Zijia Zhao, Yuqi Huo, Tongtian Yue, Longteng Guo, Haoyu Lu, Bingning Wang, Weipeng Chen, Jing Liu

**Updated**: 2025-03-17T10:20:05Z

**Summary**: Most current video MLLMs rely on uniform frame sampling and image-level encoders, resulting in inefficient data processing and limited motion awareness. To address these challenges, we introduce EMA, an Efficient Motion-Aware video MLLM that utilizes compressed video structures as inputs. We propose a motion-aware GOP (Group of Pictures) encoder that fuses spatial and motion information within a GOP unit in the compressed video stream, generating compact, informative visual tokens. By integrating fewer but denser RGB frames with more but sparser motion vectors in this native slow-fast input architecture, our approach reduces redundancy and enhances motion representation. Additionally, we introduce MotionBench, a benchmark for evaluating motion understanding across four motion types: linear, curved, rotational, and contact-based. Experimental results show that EMA achieves state-of-the-art performance on both MotionBench and popular video question answering benchmarks, while reducing inference costs. Moreover, EMA demonstrates strong scalability, as evidenced by its competitive performance on long video understanding benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2503.13016v1),  [pdf](http://arxiv.org/pdf/2503.13016v1)

**Tags**: cs.CV 



### Gravitational form factors and mechanical properties of the nucleon in a   meson dominance approach
**Authors**: Wojciech Broniowski, Enrique Ruiz Arriola

**Updated**: 2025-03-17T10:16:46Z

**Summary**: We analyze the gravitational form factors and mechanical properties of the nucleon, focusing on both some general issues as well as on modeling with meson dominance. We show that the lattice QCD results for the nucleon gravitational form factors at $m_\pi=170$~MeV, available for space-like momentum transfer squared up to 2GeV, are explained in a natural way within the meson dominance approach. We carry out the proper Raman spin decomposition of the energy-momentum tensor and in each spin channel use a minimum number of resonances consistent with the perturbative QCD short-distance constraints. These constraints are related to the super-convergence sum rules, following from the asymptotic perturbative QCD fall-off of the form factors. The value of the nucleon $D$-term following from the fits is -3.0(4). Next, we obtain the two-dimensional transverse gravitational densities of the nucleon in the transverse coordinate $b$. With the super-convergence sum rules, we derive new sum rules for these densities at the origin and for their derivatives, involving logarithmic weighting in the corresponding spectral density integrals. From analysis of the threshold behavior in the time-like region and the properties of the $\pi\pi \to N\bar{N}$ reaction, we infer the behavior of the transverse densities at asymptotically large coordinates. We also carry out the meson dominance analysis of the two- and three-dimensional mechanical properties of the nucleon (the pressure and stress) and explore their connection to the spectral densities via dispersion relations.

**Link**: [arxiv](http://arxiv.org/abs/2503.09297v2),  [pdf](http://arxiv.org/pdf/2503.09297v2)

**Tags**: hep-ph hep-lat 



### TuBA: Cross-Lingual Transferability of Backdoor Attacks in LLMs with   Instruction Tuning
**Authors**: Xuanli He, Jun Wang, Qiongkai Xu, Pasquale Minervini, Pontus Stenetorp, Benjamin I. P. Rubinstein, Trevor Cohn

**Updated**: 2025-03-17T10:09:29Z

**Summary**: The implications of backdoor attacks on English-centric large language models (LLMs) have been widely examined - such attacks can be achieved by embedding malicious behaviors during training and activated under specific conditions that trigger malicious outputs. Despite the increasing support for multilingual capabilities in open-source and proprietary LLMs, the impact of backdoor attacks on these systems remains largely under-explored. Our research focuses on cross-lingual backdoor attacks against multilingual LLMs, particularly investigating how poisoning the instruction-tuning data for one or two languages can affect the outputs for languages whose instruction-tuning data were not poisoned. Despite its simplicity, our empirical analysis reveals that our method exhibits remarkable efficacy in models like mT5 and GPT-4o, with high attack success rates, surpassing 90% in more than 7 out of 12 languages across various scenarios. Our findings also indicate that more powerful models show increased susceptibility to transferable cross-lingual backdoor attacks, which also applies to LLMs predominantly pre-trained on English data, such as Llama2, Llama3, and Gemma. Moreover, our experiments demonstrate 1) High Transferability: the backdoor mechanism operates successfully in cross-lingual response scenarios across 26 languages, achieving an average attack success rate of 99%, and 2) Robustness: the proposed attack remains effective even after defenses are applied. These findings expose critical security vulnerabilities in multilingual LLMs and highlight the urgent need for more robust, targeted defense strategies to address the unique challenges posed by cross-lingual backdoor transfer.

**Link**: [arxiv](http://arxiv.org/abs/2404.19597v3),  [pdf](http://arxiv.org/pdf/2404.19597v3)

**Tags**: cs.CL cs.CR 



### Knowledge Distillation: Enhancing Neural Network Compression with   Integrated Gradients
**Authors**: David E. Hernandez, Jose Ramon Chang, Torbjörn E. M. Nordling

**Updated**: 2025-03-17T10:07:50Z

**Summary**: Efficient deployment of deep neural networks on resource-constrained devices demands advanced compression techniques that preserve accuracy and interoperability. This paper proposes a machine learning framework that augments Knowledge Distillation (KD) with Integrated Gradients (IG), an attribution method, to optimise the compression of convolutional neural networks. We introduce a novel data augmentation strategy where IG maps, precomputed from a teacher model, are overlaid onto training images to guide a compact student model toward critical feature representations. This approach leverages the teacher's decision-making insights, enhancing the student's ability to replicate complex patterns with reduced parameters. Experiments on CIFAR-10 demonstrate the efficacy of our method: a student model, compressed 4.1-fold from the MobileNet-V2 teacher, achieves 92.5% classification accuracy, surpassing the baseline student's 91.4% and traditional KD approaches, while reducing inference latency from 140 ms to 13 ms--a tenfold speedup. We perform hyperparameter optimisation for efficient learning. Comprehensive ablation studies dissect the contributions of KD and IG, revealing synergistic effects that boost both performance and model explainability. Our method's emphasis on feature-level guidance via IG distinguishes it from conventional KD, offering a data-driven solution for mining transferable knowledge in neural architectures. This work contributes to machine learning by providing a scalable, interpretable compression technique, ideal for edge computing applications where efficiency and transparency are paramount.

**Link**: [arxiv](http://arxiv.org/abs/2503.13008v1),  [pdf](http://arxiv.org/pdf/2503.13008v1)

**Tags**: cs.LG cs.CV 68T05, 68T07 I.2.6; I.4.2; I.4.9 



### Exploring ReAct Prompting for Task-Oriented Dialogue: Insights and   Shortcomings
**Authors**: Michelle Elizabeth, Morgan Veyret, Miguel Couceiro, Ondrej Dusek, Lina M. Rojas-Barahona

**Updated**: 2025-03-17T10:01:21Z

**Summary**: Large language models (LLMs) gained immense popularity due to their impressive capabilities in unstructured conversations. Empowering LLMs with advanced prompting strategies such as reasoning and acting (ReAct) (Yao et al., 2022) has shown promise in solving complex tasks traditionally requiring reinforcement learning. In this work, we apply the ReAct strategy to guide LLMs performing task-oriented dialogue (TOD). We evaluate ReAct-based LLMs (ReAct-LLMs) both in simulation and with real users. While ReAct-LLMs severely underperform state-of-the-art approaches on success rate in simulation, this difference becomes less pronounced in human evaluation. Moreover, compared to the baseline, humans report higher subjective satisfaction with ReAct-LLM despite its lower success rate, most likely thanks to its natural and confidently phrased responses.

**Link**: [arxiv](http://arxiv.org/abs/2412.01262v2),  [pdf](http://arxiv.org/pdf/2412.01262v2)

**Tags**: cs.CL cs.AI cs.HC 



### TFDM: Time-Variant Frequency-Based Point Cloud Diffusion with Mamba
**Authors**: Jiaxu Liu, Li Li, Hubert P. H. Shum, Toby P. Breckon

**Updated**: 2025-03-17T10:00:14Z

**Summary**: Diffusion models currently demonstrate impressive performance over various generative tasks. Recent work on image diffusion highlights the strong capabilities of Mamba (state space models) due to its efficient handling of long-range dependencies and sequential data modeling. Unfortunately, joint consideration of state space models with 3D point cloud generation remains limited. To harness the powerful capabilities of the Mamba model for 3D point cloud generation, we propose a novel diffusion framework containing dual latent Mamba block (DM-Block) and a time-variant frequency encoder (TF-Encoder). The DM-Block apply a space-filling curve to reorder points into sequences suitable for Mamba state-space modeling, while operating in a latent space to mitigate the computational overhead that arises from direct 3D data processing. Meanwhile, the TF-Encoder takes advantage of the ability of the diffusion model to refine fine details in later recovery stages by prioritizing key points within the U-Net architecture. This frequency-based mechanism ensures enhanced detail quality in the final stages of generation. Experimental results on the ShapeNet-v2 dataset demonstrate that our method achieves state-of-the-art performance (ShapeNet-v2: 0.14\% on 1-NNA-Abs50 EMD and 57.90\% on COV EMD) on certain metrics for specific categories while reducing computational parameters and inference time by up to 10$\times$ and 9$\times$, respectively. Source code is available in Supplementary Materials and will be released upon accpetance.

**Link**: [arxiv](http://arxiv.org/abs/2503.13004v1),  [pdf](http://arxiv.org/pdf/2503.13004v1)

**Tags**: cs.CV 



## Keyword: LLM Deployment 
 ### MetaScale: Test-Time Scaling with Evolving Meta-Thoughts
**Authors**: Qin Liu, Wenxuan Zhou, Nan Xu, James Y. Huang, Fei Wang, Sheng Zhang, Hoifung Poon, Muhao Chen

**Updated**: 2025-03-17T17:59:54Z

**Summary**: One critical challenge for large language models (LLMs) for making complex reasoning is their reliance on matching reasoning patterns from training data, instead of proactively selecting the most appropriate cognitive strategy to solve a given task. Existing approaches impose fixed cognitive structures that enhance performance in specific tasks but lack adaptability across diverse scenarios. To address this limitation, we introduce METASCALE, a test-time scaling framework based on meta-thoughts -- adaptive thinking strategies tailored to each task. METASCALE initializes a pool of candidate meta-thoughts, then iteratively selects and evaluates them using a multi-armed bandit algorithm with upper confidence bound selection, guided by a reward model. To further enhance adaptability, a genetic algorithm evolves high-reward meta-thoughts, refining and extending the strategy pool over time. By dynamically proposing and optimizing meta-thoughts at inference time, METASCALE improves both accuracy and generalization across a wide range of tasks. Experimental results demonstrate that MetaScale consistently outperforms standard inference approaches, achieving an 11% performance gain in win rate on Arena-Hard for GPT-4o, surpassing o1-mini by 0.9% under style control. Notably, METASCALE scales more effectively with increasing sampling budgets and produces more structured, expert-level responses.

**Link**: [arxiv](http://arxiv.org/abs/2503.13447v1),  [pdf](http://arxiv.org/pdf/2503.13447v1)

**Tags**: cs.CL cs.AI cs.LG 



### MoManipVLA: Transferring Vision-language-action Models for General   Mobile Manipulation
**Authors**: Zhenyu Wu, Yuheng Zhou, Xiuwei Xu, Ziwei Wang, Haibin Yan

**Updated**: 2025-03-17T17:59:52Z

**Summary**: Mobile manipulation is the fundamental challenge for robotics to assist humans with diverse tasks and environments in everyday life. However, conventional mobile manipulation approaches often struggle to generalize across different tasks and environments because of the lack of large-scale training. In contrast, recent advances in vision-language-action (VLA) models have shown impressive generalization capabilities, but these foundation models are developed for fixed-base manipulation tasks. Therefore, we propose an efficient policy adaptation framework named MoManipVLA to transfer pre-trained VLA models of fix-base manipulation to mobile manipulation, so that high generalization ability across tasks and environments can be achieved in mobile manipulation policy. Specifically, we utilize pre-trained VLA models to generate waypoints of the end-effector with high generalization ability. We design motion planning objectives for the mobile base and the robot arm, which aim at maximizing the physical feasibility of the trajectory. Finally, we present an efficient bi-level objective optimization framework for trajectory generation, where the upper-level optimization predicts waypoints for base movement to enhance the manipulator policy space, and the lower-level optimization selects the optimal end-effector trajectory to complete the manipulation task. In this way, MoManipVLA can adjust the position of the robot base in a zero-shot manner, thus making the waypoints predicted from the fixed-base VLA models feasible. Extensive experimental results on OVMM and the real world demonstrate that MoManipVLA achieves a 4.2% higher success rate than the state-of-the-art mobile manipulation, and only requires 50 training cost for real world deployment due to the strong generalization ability in the pre-trained VLA models.

**Link**: [arxiv](http://arxiv.org/abs/2503.13446v1),  [pdf](http://arxiv.org/pdf/2503.13446v1)

**Tags**: cs.RO cs.CV 



### Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is   Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance
**Authors**: Noah Y. Siegel, Nicolas Heess, Maria Perez-Ortiz, Oana-Maria Camburu

**Updated**: 2025-03-17T17:59:39Z

**Summary**: As large language models (LLMs) become increasingly capable, ensuring that their self-generated explanations are faithful to their internal decision-making process is critical for safety and oversight. In this work, we conduct a comprehensive counterfactual faithfulness analysis across 62 models from 8 families, encompassing both pretrained and instruction-tuned variants and significantly extending prior studies of counterfactual tests. We introduce phi-CCT, a simplified variant of the Correlational Counterfactual Test, which avoids the need for token probabilities while explaining most of the variance of the original test. Our findings reveal clear scaling trends: larger models are consistently more faithful on our metrics. However, when comparing instruction-tuned and human-imitated explanations, we find that observed differences in faithfulness can often be attributed to explanation verbosity, leading to shifts along the true-positive/false-positive Pareto frontier. While instruction-tuning and prompting can influence this trade-off, we find limited evidence that they fundamentally expand the frontier of explanatory faithfulness beyond what is achievable with pretrained models of comparable size. Our analysis highlights the nuanced relationship between instruction-tuning, verbosity, and the faithful representation of model decision processes.

**Link**: [arxiv](http://arxiv.org/abs/2503.13445v1),  [pdf](http://arxiv.org/pdf/2503.13445v1)

**Tags**: cs.CL cs.AI I.2.7 



### Unified Autoregressive Visual Generation and Understanding with   Continuous Tokens
**Authors**: Lijie Fan, Luming Tang, Siyang Qin, Tianhong Li, Xuan Yang, Siyuan Qiao, Andreas Steiner, Chen Sun, Yuanzhen Li, Tao Zhu, Michael Rubinstein, Michalis Raptis, Deqing Sun, Radu Soricut

**Updated**: 2025-03-17T17:58:30Z

**Summary**: We present UniFluid, a unified autoregressive framework for joint visual generation and understanding leveraging continuous visual tokens. Our unified autoregressive architecture processes multimodal image and text inputs, generating discrete tokens for text and continuous tokens for image. We find though there is an inherent trade-off between the image generation and understanding task, a carefully tuned training recipe enables them to improve each other. By selecting an appropriate loss balance weight, the unified model achieves results comparable to or exceeding those of single-task baselines on both tasks. Furthermore, we demonstrate that employing stronger pre-trained LLMs and random-order generation during training is important to achieve high-fidelity image generation within this unified framework. Built upon the Gemma model series, UniFluid exhibits competitive performance across both image generation and understanding, demonstrating strong transferability to various downstream tasks, including image editing for generation, as well as visual captioning and question answering for understanding.

**Link**: [arxiv](http://arxiv.org/abs/2503.13436v1),  [pdf](http://arxiv.org/pdf/2503.13436v1)

**Tags**: cs.CV cs.LG 



### Top General Performance = Top Domain Performance? DomainCodeBench: A   Multi-domain Code Generation Benchmark
**Authors**: Dewu Zheng, Yanlin Wang, Ensheng Shi, Xilin Liu, Yuchi Ma, Hongyu Zhang, Zibin Zheng

**Updated**: 2025-03-17T17:58:13Z

**Summary**: With the rapid advancement of large language models (LLMs), extensive research has been conducted to investigate the code generation capabilities of LLMs. However, existing efforts primarily focus on general-domain tasks, leaving LLMs' code generation performance in real-world application domains underexplored. This raises a critical question: can a model's general-domain coding ability reliably represent its ability in specialized domains? In this paper, we introduce DomainCodeBench, a multi-domain code generation benchmark designed to systematically evaluate LLMs across 12 software application domains and 15 programming languages. DomainCodeBench contains 2,400 manually verified tasks with ground truth, human-annotated docstrings, and fine-grained dependency information to ensure more coverage of domain-specific challenges. Specifically, we first identify the most popular application domains by topic mining. Then, we curate coding tasks based on commonly used frameworks and platforms in each domain. We obtain several findings through extensive experiments on DomainCodeBench with ten mainstream LLMs. (1) Performance decoupling: experiments reveal that top general-domain models do not consistently excel in specific application domains; (2) Domain-specific weaknesses: LLMs often fail due to domain knowledge gaps and third-party library misusage; (3) Contextual enhancement: we show that augmenting prompts with domain-specific knowledge improves performance by around 38.17%, providing actionable insights for performance optimization. Our replication package, including the benchmark, source code, and experimental results, is available at https://github.com/DeepSoftwareAnalytics/DomainCodeBench.

**Link**: [arxiv](http://arxiv.org/abs/2412.18573v2),  [pdf](http://arxiv.org/pdf/2412.18573v2)

**Tags**: cs.SE cs.AI cs.CL 



### xLSTM 7B: A Recurrent LLM for Fast and Efficient Inference
**Authors**: Maximilian Beck, Korbinian Pöppel, Phillip Lippe, Richard Kurle, Patrick M. Blies, Günter Klambauer, Sebastian Böck, Sepp Hochreiter

**Updated**: 2025-03-17T17:54:55Z

**Summary**: Recent breakthroughs in solving reasoning, math and coding problems with Large Language Models (LLMs) have been enabled by investing substantial computation budgets at inference time. Therefore, inference speed is one of the most critical properties of LLM architectures, and there is a growing need for LLMs that are efficient and fast at inference. Recently, LLMs built on the xLSTM architecture have emerged as a powerful alternative to Transformers, offering linear compute scaling with sequence length and constant memory usage, both highly desirable properties for efficient inference. However, such xLSTM-based LLMs have yet to be scaled to larger models and assessed and compared with respect to inference speed and efficiency. In this work, we introduce xLSTM 7B, a 7-billion-parameter LLM that combines xLSTM's architectural benefits with targeted optimizations for fast and efficient inference. Our experiments demonstrate that xLSTM 7B achieves performance on downstream tasks comparable to other similar-sized LLMs, while providing significantly faster inference speeds and greater efficiency compared to Llama- and Mamba-based LLMs. These results establish xLSTM 7B as the fastest and most efficient 7B LLM, offering a solution for tasks that require large amounts of test-time computation. Our work highlights xLSTM's potential as a foundational architecture for methods building on heavy use of LLM inference. Our model weights, model code and training code are open-source.

**Link**: [arxiv](http://arxiv.org/abs/2503.13427v1),  [pdf](http://arxiv.org/pdf/2503.13427v1)

**Tags**: cs.LG cs.AI cs.CL 



### A Comprehensive Survey on Multi-Agent Cooperative Decision-Making:   Scenarios, Approaches, Challenges and Perspectives
**Authors**: Weiqiang Jin, Hongyang Du, Biao Zhao, Xingwu Tian, Bohang Shi, Guang Yang

**Updated**: 2025-03-17T17:45:46Z

**Summary**: With the rapid development of artificial intelligence, intelligent decision-making techniques have gradually surpassed human levels in various human-machine competitions, especially in complex multi-agent cooperative task scenarios. Multi-agent cooperative decision-making involves multiple agents working together to complete established tasks and achieve specific objectives. These techniques are widely applicable in real-world scenarios such as autonomous driving, drone navigation, disaster rescue, and simulated military confrontations. This paper begins with a comprehensive survey of the leading simulation environments and platforms used for multi-agent cooperative decision-making. Specifically, we provide an in-depth analysis for these simulation environments from various perspectives, including task formats, reward allocation, and the underlying technologies employed. Subsequently, we provide a comprehensive overview of the mainstream intelligent decision-making approaches, algorithms and models for multi-agent systems (MAS). Theseapproaches can be broadly categorized into five types: rule-based (primarily fuzzy logic), game theory-based, evolutionary algorithms-based, deep multi-agent reinforcement learning (MARL)-based, and large language models(LLMs)reasoning-based. Given the significant advantages of MARL andLLMs-baseddecision-making methods over the traditional rule, game theory, and evolutionary algorithms, this paper focuses on these multi-agent methods utilizing MARL and LLMs-based techniques. We provide an in-depth discussion of these approaches, highlighting their methodology taxonomies, advantages, and drawbacks. Further, several prominent research directions in the future and potential challenges of multi-agent cooperative decision-making are also detailed.

**Link**: [arxiv](http://arxiv.org/abs/2503.13415v1),  [pdf](http://arxiv.org/pdf/2503.13415v1)

**Tags**: cs.MA cs.AI 



### DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization   Framework from a Deep-Learning Perspective
**Authors**: Dengyun Peng, Yuhang Zhou, Qiguang Chen, Jinhao Liu, Jingjing Chen, Libo Qin

**Updated**: 2025-03-18T04:41:37Z

**Summary**: Large Language Models (LLMs) have achieved remarkable success across diverse tasks, largely driven by well-designed prompts. However, crafting and selecting such prompts often requires considerable human effort, significantly limiting its scalability. To mitigate this, recent studies have explored automated prompt optimization as a promising solution. Despite these efforts, existing methods still face critical challenges in robustness, efficiency, and generalization. To systematically address these challenges, we first conduct an empirical analysis to identify the limitations of current reflection-based prompt optimization paradigm. Building on these insights, we propose 7 innovative approaches inspired by traditional deep learning paradigms for prompt optimization (DLPO), seamlessly integrating these concepts into text-based gradient optimization. Through these advancements, we progressively tackle the aforementioned challenges and validate our methods through extensive experimentation. We hope our study not only provides valuable guidance for future research but also offers a comprehensive understanding of the challenges and potential solutions in prompt optimization. Our code is available at https://github.com/sfasfaffa/DLPO.

**Link**: [arxiv](http://arxiv.org/abs/2503.13413v2),  [pdf](http://arxiv.org/pdf/2503.13413v2)

**Tags**: cs.CL cs.AI 



### Toward Generative 6G Simulation: An Experimental Multi-Agent LLM and   ns-3 Integration
**Authors**: Farhad Rezazadeh, Amir Ashtari Gargari, Sandra Lagen, Houbing Song, Dusit Niyato, Lingjia Liu

**Updated**: 2025-03-17T17:34:04Z

**Summary**: The move toward open Sixth-Generation (6G) networks necessitates a novel approach to full-stack simulation environments for evaluating complex technology developments before prototyping and real-world implementation. This paper introduces an innovative approach\footnote{A lightweight, mock version of the code is available on GitHub at that combines a multi-agent framework with the Network Simulator 3 (ns-3) to automate and optimize the generation, debugging, execution, and analysis of complex 5G network scenarios. Our framework orchestrates a suite of specialized agents -- namely, the Simulation Generation Agent, Test Designer Agent, Test Executor Agent, and Result Interpretation Agent -- using advanced LangChain coordination. The Simulation Generation Agent employs a structured chain-of-thought (CoT) reasoning process, leveraging LLMs and retrieval-augmented generation (RAG) to translate natural language simulation specifications into precise ns-3 scripts. Concurrently, the Test Designer Agent generates comprehensive automated test suites by integrating knowledge retrieval techniques with dynamic test case synthesis. The Test Executor Agent dynamically deploys and runs simulations, managing dependencies and parsing detailed performance metrics. At the same time, the Result Interpretation Agent utilizes LLM-driven analysis to extract actionable insights from the simulation outputs. By integrating external resources such as library documentation and ns-3 testing frameworks, our experimental approach can enhance simulation accuracy and adaptability, reducing reliance on extensive programming expertise. A detailed case study using the ns-3 5G-LENA module validates the effectiveness of the proposed approach. The code generation process converges in an average of 1.8 iterations, has a syntax error rate of 17.0%, a mean response time of 7.3 seconds, and receives a human evaluation score of 7.5.

**Link**: [arxiv](http://arxiv.org/abs/2503.13402v1),  [pdf](http://arxiv.org/pdf/2503.13402v1)

**Tags**: cs.NI 



### MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based   Scientific Research
**Authors**: James Burgess, Jeffrey J Nirschl, Laura Bravo-Sánchez, Alejandro Lozano, Sanket Rajan Gupte, Jesus G. Galaz-Montoya, Yuhui Zhang, Yuchang Su, Disha Bhowmik, Zachary Coman, Sarina M. Hasan, Alexandra Johannesson, William D. Leineweber, Malvika G Nair, Ridhi Yarlagadda, Connor Zuraski, Wah Chiu, Sarah Cohen, Jan N. Hansen, Manuel D Leonetti, Chad Liu, Emma Lundberg, Serena Yeung-Levy

**Updated**: 2025-03-17T17:33:10Z

**Summary**: Scientific research demands sophisticated reasoning over multimodal data, a challenge especially prevalent in biology. Despite recent advances in multimodal large language models (MLLMs) for AI-assisted research, existing multimodal reasoning benchmarks only target up to college-level difficulty, while research-level benchmarks emphasize lower-level perception, falling short of the complex multimodal reasoning needed for scientific discovery. To bridge this gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark designed to assess three reasoning capabilities vital in research workflows: expert image understanding, hypothesis generation, and experiment proposal. MicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology experts across diverse microscopy modalities, ensuring VQA samples represent real scientific practice. In constructing the benchmark, we find that standard MCQ generation methods induce language shortcuts, motivating a new two-stage pipeline: an optimized LLM prompt structures question-answer pairs into MCQs; then, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking on state-of-the-art MLLMs reveal a peak performance of 53\%; models with smaller LLMs only slightly underperform top models, suggesting that language-based reasoning is less challenging than multimodal reasoning; and tuning with scientific articles enhances performance. Expert analysis of chain-of-thought responses shows that perception errors are the most frequent, followed by knowledge errors and then overgeneralization errors. These insights highlight the challenges in multimodal scientific reasoning, showing MicroVQA is a valuable resource advancing AI-driven biomedical research. MicroVQA is available at https://huggingface.co/datasets/jmhb/microvqa, and project page at https://jmhb0.github.io/microvqa.

**Link**: [arxiv](http://arxiv.org/abs/2503.13399v1),  [pdf](http://arxiv.org/pdf/2503.13399v1)

**Tags**: cs.CV cs.AI cs.CL cs.LG q-bio.CB 



### Cream of the Crop: Harvesting Rich, Scalable and Transferable   Multi-Modal Data for Instruction Fine-Tuning
**Authors**: Mengyao Lyu, Yan Li, Huasong Zhong, Wenhao Yang, Hui Chen, Jungong Han, Guiguang Ding, Zhenheng Yang

**Updated**: 2025-03-17T17:11:22Z

**Summary**: The hypothesis that pretrained large language models (LLMs) necessitate only minimal supervision during the fine-tuning (SFT) stage (Zhou et al., 2024) has been substantiated by recent advancements in data curation and selection research. However, their stability and generalizability are compromised due to the vulnerability to experimental setups and validation protocols, falling short of surpassing random sampling (Diddee & Ippolito, 2024; Xia et al., 2024b). Built upon LLMs, multi-modal LLMs (MLLMs), combined with the sheer token volume and heightened heterogeneity of data sources, amplify both the significance and complexity of data selection.   To harvest multi-modal instructional data in a robust and efficient manner, we re-define the granularity of the quality metric by decomposing it into 14 vision-language-related capabilities, and introduce multi-modal rich scorers to evaluate the capabilities of each data candidate. To promote diversity, in light of the inherent objective of the alignment stage, we take interaction style as diversity indicator and use a multi-modal rich styler to identify data instruction patterns. In doing so, our multi-modal rich scorers and styler (mmSSR) guarantee that high-scoring information is conveyed to users in diversified forms. Free from embedding-based clustering or greedy sampling, mmSSR efficiently scales to millions of data with varying budget constraints, supports customization for general or specific capability acquisition, and facilitates training-free generalization to new domains for curation. Across 10+ experimental settings, validated by 14 multi-modal benchmarks, we demonstrate consistent improvements over random sampling, baseline strategies and state-of-the-art selection methods, achieving 99.1% of full performance with only 30% of the 2.6M data.

**Link**: [arxiv](http://arxiv.org/abs/2503.13383v1),  [pdf](http://arxiv.org/pdf/2503.13383v1)

**Tags**: cs.CV cs.AI cs.CL cs.LG 



### Structure-Activation Synergy: A Dual Efficiency Framework for   Parameter-Memory Optimized Transfer Learning
**Authors**: Tian Jin, Enjun Du, Changwei Wang, Wenhao Xu, Ding Luo

**Updated**: 2025-03-17T16:50:29Z

**Summary**: While parameter-efficient transfer learning (PETL) successfully reduces trainable parameters for adapting large pre-trained models, conventional methods exhibit limited effectiveness in decreasing activation memory consumption - a critical bottleneck for deployment on resource-constrained devices. We present Structure-Activation Synergy (S2A), an innovative framework achieving dual optimization of parameters and memory through two synergistic mechanisms: (1) Structural activation modules (bias/prompt/side adaptations) that strategically minimize both parametric complexity and intermediate feature storage requirements, and (2) Derivative-aware 4-bit quantization for non-parametric operators that maintains model fidelity through gradient-informed precision allocation. Extensive evaluations across multiple architectures (ViT, Swin, ResNet) and datasets (ImageNet-1K, CIFAR, DomainNet) demonstrate S2A's superior efficiency, reducing GPU memory consumption by 75\% (4.2 average reduction) while maintaining 98.7\% of full fine-tuning accuracy with only 0.9\% tunable parameters. This hardware-aware paradigm establishes new state-of-the-art in efficient model adaptation, offering practical deployment advantages through simultaneous parameter and memory optimization without compromising model capability

**Link**: [arxiv](http://arxiv.org/abs/2503.08154v2),  [pdf](http://arxiv.org/pdf/2503.08154v2)

**Tags**: cs.CV 



### Mitigating Visual Forgetting via Take-along Visual Conditioning for   Multi-modal Long CoT Reasoning
**Authors**: Hai-Long Sun, Zhun Sun, Houwen Peng, Han-Jia Ye

**Updated**: 2025-03-17T16:45:12Z

**Summary**: Recent advancements in Large Language Models (LLMs) have demonstrated enhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting to advanced, product-oriented solutions like OpenAI o1. During our re-implementation of this model, we noticed that in multimodal tasks requiring visual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to maintain focus on the visual information, in other words, MLLMs suffer from a gradual decline in attention to visual information as reasoning progresses, causing text-over-relied outputs. To investigate this, we ablate image inputs during long-chain reasoning. Concretely, we truncate the reasoning process midway, then re-complete the reasoning process with the input image removed. We observe only a ~2% accuracy drop on MathVista's test-hard subset, revealing the model's textual outputs dominate the following reasoning process. Motivated by this, we propose Take-along Visual Conditioning (TVC), a strategy that shifts image input to critical reasoning stages and compresses redundant visual tokens via dynamic pruning. This methodology helps the model retain attention to the visual components throughout the reasoning. Our approach achieves state-of-the-art performance on average across five mathematical reasoning benchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in enhancing multimodal reasoning systems.

**Link**: [arxiv](http://arxiv.org/abs/2503.13360v1),  [pdf](http://arxiv.org/pdf/2503.13360v1)

**Tags**: cs.CV cs.AI cs.LG 



### Agents Play Thousands of 3D Video Games
**Authors**: Zhongwen Xu, Xianliang Wang, Siyi Li, Tao Yu, Liang Wang, Qiang Fu, Wei Yang

**Updated**: 2025-03-17T16:42:34Z

**Summary**: We present PORTAL, a novel framework for developing artificial intelligence agents capable of playing thousands of 3D video games through language-guided policy generation. By transforming decision-making problems into language modeling tasks, our approach leverages large language models (LLMs) to generate behavior trees represented in domain-specific language (DSL). This method eliminates the computational burden associated with traditional reinforcement learning approaches while preserving strategic depth and rapid adaptability. Our framework introduces a hybrid policy structure that combines rule-based nodes with neural network components, enabling both high-level strategic reasoning and precise low-level control. A dual-feedback mechanism incorporating quantitative game metrics and vision-language model analysis facilitates iterative policy improvement at both tactical and strategic levels. The resulting policies are instantaneously deployable, human-interpretable, and capable of generalizing across diverse gaming environments. Experimental results demonstrate PORTAL's effectiveness across thousands of first-person shooter (FPS) games, showcasing significant improvements in development efficiency, policy generalization, and behavior diversity compared to traditional approaches. PORTAL represents a significant advancement in game AI development, offering a practical solution for creating sophisticated agents that can operate across thousands of commercial video games with minimal development overhead. Experiment results on the 3D video games are best viewed on https://zhongwen.one/projects/portal .

**Link**: [arxiv](http://arxiv.org/abs/2503.13356v1),  [pdf](http://arxiv.org/pdf/2503.13356v1)

**Tags**: cs.LG 



### CogSys: Efficient and Scalable Neurosymbolic Cognition System via   Algorithm-Hardware Co-Design
**Authors**: Zishen Wan, Hanchen Yang, Ritik Raj, Che-Kai Liu, Ananda Samajdar, Arijit Raychowdhury, Tushar Krishna

**Updated**: 2025-03-17T16:42:33Z

**Summary**: Neurosymbolic AI is an emerging compositional paradigm that fuses neural learning with symbolic reasoning to enhance the transparency, interpretability, and trustworthiness of AI. It also exhibits higher data efficiency making it promising for edge deployments. Despite the algorithmic promises and demonstrations, unfortunately executing neurosymbolic workloads on current hardware (CPU/GPU/TPU) is challenging due to higher memory intensity, greater compute heterogeneity and access pattern irregularity, leading to severe hardware underutilization.   This work proposes CogSys, a characterization and co-design framework dedicated to neurosymbolic AI system acceleration, aiming to win both reasoning efficiency and scalability. On the algorithm side, CogSys proposes an efficient factorization technique to alleviate compute and memory overhead. On the hardware side, CogSys proposes a scalable neurosymbolic architecture with reconfigurable neuro/symbolic processing elements (nsPE) and bubble streaming (BS) dataflow with spatial-temporal (ST) mapping for highly parallel and efficient neurosymbolic computation. On the system side, CogSys features an adaptive workload-aware scheduler (adSCH) to orchestrate heterogeneous kernels and enhance resource utilization. Evaluated across cognitive workloads, CogSys enables reconfigurable support for neural and symbolic kernels and exhibits >75x speedup over TPU-like systolic array with only <5% area overhead, as benchmarked under the TSMC 28nm technology node. CogSys achieves 4x-96x speedup compared to desktop and edge GPUs. For the first time, CogSys enables real-time abduction reasoning towards human fluid intelligence, requiring only 0.3 s per reasoning task with 4 mm2 area and 1.48 W power consumption.

**Link**: [arxiv](http://arxiv.org/abs/2503.01162v2),  [pdf](http://arxiv.org/pdf/2503.01162v2)

**Tags**: cs.AR 



### LearnMate: Enhancing Online Education with LLM-Powered Personalized   Learning Plans and Support
**Authors**: Xinyu Jessica Wang, Christine Lee, Bilge Mutlu

**Updated**: 2025-03-17T16:18:23Z

**Summary**: With the increasing prevalence of online learning, adapting education to diverse learner needs remains a persistent challenge. Recent advancements in artificial intelligence (AI), particularly large language models (LLMs), promise powerful tools and capabilities to enhance personalized learning in online educational environments. In this work, we explore how LLMs can improve personalized learning experiences by catering to individual user needs toward enhancing the overall quality of online education. We designed personalization guidelines based on the growing literature on personalized learning to ground LLMs in generating tailored learning plans. To operationalize these guidelines, we implemented LearnMate, an LLM-based system that generates personalized learning plans and provides users with real-time learning support. We discuss the implications and future directions of this work, aiming to move beyond the traditional one-size-fits-all approach by integrating LLM-based personalized support into online learning environments.

**Link**: [arxiv](http://arxiv.org/abs/2503.13340v1),  [pdf](http://arxiv.org/pdf/2503.13340v1)

**Tags**: cs.HC 



### Reliable and Efficient Amortized Model-based Evaluation
**Authors**: Sang Truong, Yuheng Tu, Percy Liang, Bo Li, Sanmi Koyejo

**Updated**: 2025-03-17T16:15:02Z

**Summary**: Comprehensive evaluations of language models (LM) during both development and deployment phases are necessary because these models possess numerous capabilities (e.g., mathematical reasoning, legal support, or medical diagnostic) as well as safety risks (e.g., racial bias, toxicity, or misinformation). The average score across a wide range of benchmarks provides a signal that helps guide the use of these LMs in practice. Currently, holistic evaluations are costly due to the large volume of benchmark questions, making frequent evaluations impractical. A popular attempt to lower the cost is to compute the average score on a subset of the benchmark. This approach, unfortunately, often renders an unreliable measure of LM performance because the average score is often confounded with the difficulty of the questions in the benchmark subset. Item response theory (IRT) was designed to address this challenge, providing a reliable measurement by careful controlling for question difficulty. Unfortunately, question difficulty is expensive to estimate. Facing this challenge, we train a model that predicts question difficulty from its content, enabling a reliable measurement at a fraction of the cost. In addition, we leverage this difficulty predictor to further improve the evaluation efficiency through training a question generator given a difficulty level. This question generator is essential in adaptive testing, where, instead of using a random subset of the benchmark questions, informative questions are adaptively chosen based on the current estimation of LLM performance. Experiments on 22 common natural language benchmarks and 172 LMs show that this approach is more reliable and efficient compared to current common practice.

**Link**: [arxiv](http://arxiv.org/abs/2503.13335v1),  [pdf](http://arxiv.org/pdf/2503.13335v1)

**Tags**: cs.CL cs.AI cs.LG stat.AP 



### LEAVS: An LLM-based Labeler for Abdominal CT Supervision
**Authors**: Ricardo Bigolin Lanfredi, Yan Zhuang, Mark Finkelstein, Praveen Thoppey Srinivasan Balamuralikrishna, Luke Krembs, Brandon Khoury, Arthi Reddy, Pritam Mukherjee, Neil M. Rofsky, Ronald M. Summers

**Updated**: 2025-03-17T16:09:22Z

**Summary**: Extracting structured labels from radiology reports has been employed to create vision models to simultaneously detect several types of abnormalities. However, existing works focus mainly on the chest region. Few works have been investigated on abdominal radiology reports due to more complex anatomy and a wider range of pathologies in the abdomen. We propose LEAVS (Large language model Extractor for Abdominal Vision Supervision). This labeler can annotate the certainty of presence and the urgency of seven types of abnormalities for nine abdominal organs on CT radiology reports. To ensure broad coverage, we chose abnormalities that encompass most of the finding types from CT reports. Our approach employs a specialized chain-of-thought prompting strategy for a locally-run LLM using sentence extraction and multiple-choice questions in a tree-based decision system. We demonstrate that the LLM can extract several abnormality types across abdominal organs with an average F1 score of 0.89, significantly outperforming competing labelers and humans. Additionally, we show that extraction of urgency labels achieved performance comparable to human annotations. Finally, we demonstrate that the abnormality labels contain valuable information for training a single vision model that classifies several organs as normal or abnormal. We release our code and structured annotations for a public CT dataset containing over 1,000 CT volumes.

**Link**: [arxiv](http://arxiv.org/abs/2503.13330v1),  [pdf](http://arxiv.org/pdf/2503.13330v1)

**Tags**: eess.IV cs.AI cs.CV 



### Nullu: Mitigating Object Hallucinations in Large Vision-Language Models   via HalluSpace Projection
**Authors**: Le Yang, Ziwei Zheng, Boxu Chen, Zhengyu Zhao, Chenhao Lin, Chao Shen

**Updated**: 2025-03-17T16:05:34Z

**Summary**: Recent studies have shown that large vision-language models (LVLMs) often suffer from the issue of object hallucinations (OH). To mitigate this issue, we introduce an efficient method that edits the model weights based on an unsafe subspace, which we call HalluSpace in this paper. With truthful and hallucinated text prompts accompanying the visual content as inputs, the HalluSpace can be identified by extracting the hallucinated embedding features and removing the truthful representations in LVLMs. By orthogonalizing the model weights, input features will be projected into the Null space of the HalluSpace to reduce OH, based on which we name our method Nullu. We reveal that HalluSpaces generally contain prior information in the large language models (LLMs) applied to build LVLMs, which have been shown as essential causes of OH in previous studies. Therefore, null space projection suppresses the LLMs' priors to filter out the hallucinated features, resulting in contextually accurate outputs. Experiments show that our method can effectively mitigate OH across different LVLM families without extra inference costs and also show strong performance in general LVLM benchmarks. Code is released at https://github.com/Ziwei-Zheng/Nullu.

**Link**: [arxiv](http://arxiv.org/abs/2412.13817v3),  [pdf](http://arxiv.org/pdf/2412.13817v3)

**Tags**: cs.CV 



### SMPR: A structure-enhanced multimodal drug-disease prediction model for   drug repositioning and cold start
**Authors**: Xin Dong, Rui Miao, Suyan Zhang, Shuaibing Jia, Leifeng Zhang, Yong Liang, Jianhua Zhang, Yi Zhun Zhu

**Updated**: 2025-03-17T15:59:20Z

**Summary**: Repositioning drug-disease relationships has always been a hot field of research. However, actual cases of biologically validated drug relocation remain very limited, and existing models have not yet fully utilized the structural information of the drug. Furthermore, most repositioning models are only used to complete the relationship matrix, and their practicality is poor when dealing with drug cold start problems. This paper proposes a structure-enhanced multimodal relationship prediction model (SMRP). SMPR is based on the SMILE structure of the drug, using the Mol2VEC method to generate drug embedded representations, and learn disease embedded representations through heterogeneous network graph neural networks. Ultimately, a drug-disease relationship matrix is constructed. In addition, to reduce the difficulty of users' use, SMPR also provides a cold start interface based on structural similarity based on reposition results to simply and quickly predict drug-related diseases. The repositioning ability and cold start capability of the model are verified from multiple perspectives. While the AUC and ACUPR scores of repositioning reach 99% and 61% respectively, the AUC of cold start achieve 80%. In particular, the cold start Recall indicator can reach more than 70%, which means that SMPR is more sensitive to positive samples. Finally, case analysis is used to verify the practical value of the model and visual analysis directly demonstrates the improvement of the structure to the model. For quick use, we also provide local deployment of the model and package it into an executable program.

**Link**: [arxiv](http://arxiv.org/abs/2503.13322v1),  [pdf](http://arxiv.org/pdf/2503.13322v1)

**Tags**: cs.LG 



### Leveraging Large Language Models for Collective Decision-Making
**Authors**: Marios Papachristou, Longqi Yang, Chin-Chia Hsu

**Updated**: 2025-03-17T15:50:13Z

**Summary**: In various work contexts, such as meeting scheduling, collaborating, and project planning, collective decision-making is essential but often challenging due to diverse individual preferences, varying work focuses, and power dynamics among members. To address this, we propose a system leveraging Large Language Models (LLMs) to facilitate group decision-making by managing conversations and balancing preferences among individuals. Our system aims to extract individual preferences from each member's conversation with the system and suggest options that satisfy the preferences of the members. We specifically apply this system to corporate meeting scheduling. We create synthetic employee profiles and simulate conversations at scale, leveraging LLMs to evaluate the system performance as a novel approach to conducting a user study. Our results indicate efficient coordination with reduced interactions between the members and the LLM-based system. The system refines and improves its proposed options over time, ensuring that many of the members' individual preferences are satisfied in an equitable way. Finally, we conduct a survey study involving human participants to assess our system's ability to aggregate preferences and reasoning about them. Our findings show that the system exhibits strong performance in both dimensions.

**Link**: [arxiv](http://arxiv.org/abs/2311.04928v3),  [pdf](http://arxiv.org/pdf/2311.04928v3)

**Tags**: cs.CL cs.AI cs.HC cs.SI 



### Computation Mechanism Behind LLM Position Generalization
**Authors**: Chi Han, Heng Ji

**Updated**: 2025-03-17T15:47:37Z

**Summary**: Most written natural languages are composed of sequences of words and sentences. Similar to humans, large language models (LLMs) exhibit flexibility in handling textual positions - a phenomenon we term position generalization. They can understand texts with position perturbations and generalize to longer texts than those encountered during training with the latest techniques. These phenomena suggest that LLMs handle positions tolerantly, but how LLMs computationally process positional relevance remains largely unexplored. This work connects the linguistic phenomenon with LLMs' computational mechanisms. We show how LLMs enforce certain computational mechanisms for the aforementioned tolerance in position perturbations. Despite the complex design of the self-attention mechanism, this work reveals that LLMs learn a counterintuitive disentanglement of attention logits. Their values show a 0.959 linear correlation with an approximation of the arithmetic sum of positional relevance and semantic importance. Furthermore, we identify a prevalent pattern in intermediate features, which we prove theoretically enables this effect. The pattern, which is different from how randomly initialized parameters would behave, suggests that it is a learned behavior rather than a natural result of the model architecture. Based on these findings, we provide computational explanations and criteria for LLMs' position flexibilities. This work takes a pioneering step in linking position generalization with modern LLMs' internal mechanisms.

**Link**: [arxiv](http://arxiv.org/abs/2503.13305v1),  [pdf](http://arxiv.org/pdf/2503.13305v1)

**Tags**: cs.CL cs.AI 



### LIMCA: LLM for Automating Analog In-Memory Computing Architecture Design   Exploration
**Authors**: Deepak Vungarala, Md Hasibul Amin, Pietro Mercati, Arnob Ghosh, Arman Roohi, Ramtin Zand, Shaahin Angizi

**Updated**: 2025-03-17T15:45:17Z

**Summary**: Resistive crossbars enabling analog In-Memory Computing (IMC) have emerged as a promising architecture for Deep Neural Network (DNN) acceleration, offering high memory bandwidth and in-situ computation. However, the manual, knowledge-intensive design process and the lack of high-quality circuit netlists have significantly constrained design space exploration and optimization to behavioral system-level tools. In this work, we introduce LIMCA, a novel fine-tune-free Large Language Model (LLM)-driven framework for automating the design and evaluation of IMC crossbar architectures. Unlike traditional approaches, LIMCA employs a No-Human-In-Loop (NHIL) automated pipeline to generate and validate circuit netlists for SPICE simulations, eliminating manual intervention. LIMCA systematically explores the IMC design space by leveraging a structured dataset and LLM-based performance evaluation. Our experimental results on MNIST classification demonstrate that LIMCA successfully generates crossbar designs achieving $\geq$96% accuracy while maintaining a power consumption $\leq$3W, making this the first work in LLM-assisted IMC design space exploration. Compared to existing frameworks, LIMCA provides an automated, scalable, and hardware-aware solution, reducing design exploration time while ensuring user-constrained performance trade-offs.

**Link**: [arxiv](http://arxiv.org/abs/2503.13301v1),  [pdf](http://arxiv.org/pdf/2503.13301v1)

**Tags**: cs.AR 



### A Survey on Transformer Context Extension: Approaches and Evaluation
**Authors**: Yijun Liu, Jinzheng Yu, Yang Xu, Zhongyang Li, Qingfu Zhu

**Updated**: 2025-03-17T15:44:09Z

**Summary**: Large language models (LLMs) based on Transformer have been widely applied in the filed of natural language processing (NLP), demonstrating strong performance, particularly in handling short text tasks. However, when it comes to long context scenarios, the performance of LLMs degrades due to some challenges. To alleviate this phenomenon, there is a number of work proposed recently. In this survey, we first list the challenges of applying pre-trained LLMs to process long contexts. Then systematically review the approaches related to long context and propose our taxonomy categorizing them into four main types: positional encoding, context compression, retrieval augmented, and attention pattern. In addition to the approaches, we focus on the evaluation of long context, organizing relevant data, tasks, and metrics based on existing long context benchmarks. Finally, we summarize unresolved issues in the long context domain and put forward our views on future developments.

**Link**: [arxiv](http://arxiv.org/abs/2503.13299v1),  [pdf](http://arxiv.org/pdf/2503.13299v1)

**Tags**: cs.CL cs.AI 



### $φ$-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time   Exploration and Exploitation
**Authors**: Fangzhi Xu, Hang Yan, Chang Ma, Haiteng Zhao, Jun Liu, Qika Lin, Zhiyong Wu

**Updated**: 2025-03-17T15:38:33Z

**Summary**: Inference-time optimization scales computation to derive deliberate reasoning steps for effective performance. While previous search-based strategies address the short-sightedness of auto-regressive generation, the vast search space leads to excessive exploration and insufficient exploitation. To strike an efficient balance to derive the optimal step, we frame the decoding strategy as foresight sampling, leveraging simulated future steps to obtain globally optimal step estimation. Built on it, we propose a novel decoding strategy, named $\phi$-Decoding. To provide a precise and expressive estimation of step value, $\phi$-Decoding approximates two distributions via foresight and clustering. Sampling from the joint distribution, the optimal steps can be selected for exploitation. To support adaptive computation allocation, we propose in-width and in-depth pruning strategies, featuring a light-weight solution to achieve inference efficiency. Extensive experiments across seven benchmarks show $\phi$-Decoding outperforms strong baselines in both performance and efficiency. Additional analysis demonstrates its generalization across various LLMs and scalability across a wide range of computing budgets. The code will be released at https://github.com/xufangzhi/phi-Decoding, and the open-source PyPI package is coming soon.

**Link**: [arxiv](http://arxiv.org/abs/2503.13288v1),  [pdf](http://arxiv.org/pdf/2503.13288v1)

**Tags**: cs.LG cs.AI cs.CL 



### LLM-Match: An Open-Sourced Patient Matching Model Based on Large   Language Models and Retrieval-Augmented Generation
**Authors**: Xiaodi Li, Shaika Chowdhury, Chung Il Wi, Maria Vassilaki, Ken Liu, Terence T Sio, Owen Garrick, Young J Juhn, James R Cerhan, Cui Tao, Nansu Zong

**Updated**: 2025-03-17T15:31:55Z

**Summary**: Patient matching is the process of linking patients to appropriate clinical trials by accurately identifying and matching their medical records with trial eligibility criteria. We propose LLM-Match, a novel framework for patient matching leveraging fine-tuned open-source large language models. Our approach consists of four key components. First, a retrieval-augmented generation (RAG) module extracts relevant patient context from a vast pool of electronic health records (EHRs). Second, a prompt generation module constructs input prompts by integrating trial eligibility criteria (both inclusion and exclusion criteria), patient context, and system instructions. Third, a fine-tuning module with a classification head optimizes the model parameters using structured prompts and ground-truth labels. Fourth, an evaluation module assesses the fine-tuned model's performance on the testing datasets. We evaluated LLM-Match on four open datasets, n2c2, SIGIR, TREC 2021, and TREC 2022, using open-source models, comparing it against TrialGPT, Zero-Shot, and GPT-4-based closed models. LLM-Match outperformed all baselines.

**Link**: [arxiv](http://arxiv.org/abs/2503.13281v1),  [pdf](http://arxiv.org/pdf/2503.13281v1)

**Tags**: cs.CL cs.AI cs.LG 



### Goal2Story: A Multi-Agent Fleet based on Privately Enabled sLLMs for   Impacting Mapping on Requirements Elicitation
**Authors**: Xinkai Zou, Yan Liu, Xiongbo Shi, Chen Yang

**Updated**: 2025-03-17T15:31:20Z

**Summary**: As requirements drift with rapid iterations, agile development becomes the dominant paradigm. Goal-driven Requirements Elicitation (RE) is a pivotal yet challenging task in agile project development due to its heavy tangling with adaptive planning and efficient collaboration. Recently, AI agents have shown promising ability in supporting requirements analysis by saving significant time and effort for stakeholders. However, current research mainly focuses on functional RE, and research works have not been reported bridging the long journey from goal to user stories. Moreover, considering the cost of LLM facilities and the need for data and idea protection, privately hosted small-sized LLM should be further utilized in RE. To address these challenges, we propose Goal2Story, a multi-agent fleet that adopts the Impact Mapping (IM) framework while merely using cost-effective sLLMs for goal-driven RE. Moreover, we introduce a StorySeek dataset that contains over 1,000 user stories (USs) with corresponding goals and project context information, as well as the semi-automatic dataset construction method. For evaluation, we proposed two metrics: Factuality Hit Rate (FHR) to measure consistency between the generated USs with the dataset and Quality And Consistency Evaluation (QuACE) to evaluate the quality of the generated USs. Experimental results demonstrate that Goal2Story outperforms the baseline performance of the Super-Agent adopting powerful LLMs, while also showcasing the performance improvements in key metrics brought by CoT and Agent Profile to Goal2Story, as well as its exploration in identifying latent needs.

**Link**: [arxiv](http://arxiv.org/abs/2503.13279v1),  [pdf](http://arxiv.org/pdf/2503.13279v1)

**Tags**: cs.SE cs.AI 



### Knowledge-Aware Iterative Retrieval for Multi-Agent Systems
**Authors**: Seyoung Song

**Updated**: 2025-03-17T15:27:02Z

**Summary**: We introduce a novel large language model (LLM)-driven agent framework, which iteratively refines queries and filters contextual evidence by leveraging dynamically evolving knowledge. A defining feature of the system is its decoupling of external sources from an internal knowledge cache that is progressively updated to guide both query generation and evidence selection. This design mitigates bias-reinforcement loops and enables dynamic, trackable search exploration paths, thereby optimizing the trade-off between exploring diverse information and maintaining accuracy through autonomous agent decision-making. Our approach is evaluated on a broad range of open-domain question answering benchmarks, including multi-step tasks that mirror real-world scenarios where integrating information from multiple sources is critical, especially given the vulnerabilities of LLMs that lack explicit reasoning or planning capabilities. The results show that the proposed system not only outperforms single-step baselines regardless of task difficulty but also, compared to conventional iterative retrieval methods, demonstrates pronounced advantages in complex tasks through precise evidence-based reasoning and enhanced efficiency. The proposed system supports both competitive and collaborative sharing of updated context, enabling multi-agent extension. The benefits of multi-agent configurations become especially prominent as task difficulty increases. The number of convergence steps scales with task difficulty, suggesting cost-effective scalability.

**Link**: [arxiv](http://arxiv.org/abs/2503.13275v1),  [pdf](http://arxiv.org/pdf/2503.13275v1)

**Tags**: cs.AI cs.IR I.2.0; I.2.7; I.2.11; H.3.3 



### Masala-CHAI: A Large-Scale SPICE Netlist Dataset for Analog Circuits by   Harnessing AI
**Authors**: Jitendra Bhandari, Vineet Bhat, Yuheng He, Hamed Rahmani, Siddharth Garg, Ramesh Karri

**Updated**: 2025-03-17T15:22:28Z

**Summary**: Masala-CHAI is the first fully automated framework leveraging large language models (LLMs) to generate Simulation Programs with Integrated Circuit Emphasis (SPICE) netlists. It addresses a long-standing challenge in automating netlist generation for analog circuits within circuit design automation. Automating this workflow could accelerate the creation of finetuned LLMs for analog circuit design and verification. We identify key challenges in this automation and evaluate the multi-modal capabilities of state-of-the-art LLMs, particularly GPT-4, to address these issues. We propose a three-step workflow to overcome current limitations: labeling analog circuits, prompt tuning, and netlist verification. This approach aims to create an end-to-end SPICE netlist generator from circuit schematic images, tackling the long-standing hurdle of accurate netlist generation. Our framework demonstrates significant performance improvements, tested on approximately 2,100 schematics of varying complexity. We open-source this solution for community-driven development.

**Link**: [arxiv](http://arxiv.org/abs/2411.14299v4),  [pdf](http://arxiv.org/pdf/2411.14299v4)

**Tags**: cs.AR 



### DAgent: A Relational Database-Driven Data Analysis Report Generation   Agent
**Authors**: Wenyi Xu, Yuren Mao, Xiaolu Zhang, Chao Zhang, Xuemei Dong, Mengfei Zhang, Jun Zhou, Yunjun Gao

**Updated**: 2025-03-17T15:22:19Z

**Summary**: Relational database-driven data analysis (RDB-DA) report generation, which aims to generate data analysis reports after querying relational databases, has been widely applied in fields such as finance and healthcare. Typically, these tasks are manually completed by data scientists, making the process very labor-intensive and showing a clear need for automation. Although existing methods (e.g., Table QA or Text-to-SQL) have been proposed to reduce human dependency, they cannot handle complex analytical tasks that require multi-step reasoning, cross-table associations, and synthesizing insights into reports. Moreover, there is no dataset available for developing automatic RDB-DA report generation. To fill this gap, this paper proposes an LLM agent system for RDB-DA report generation tasks, dubbed DAgent; moreover, we construct a benchmark for automatic data analysis report generation, which includes a new dataset DA-Dataset and evaluation metrics. DAgent integrates planning, tools, and memory modules to decompose natural language questions into logically independent sub-queries, accurately retrieve key information from relational databases, and generate analytical reports that meet the requirements of completeness, correctness, and conciseness through multi-step reasoning and effective data integration. Experimental analysis on the DA-Dataset demonstrates that DAgent's superiority in retrieval performance and analysis report generation quality, showcasing its strong potential for tackling complex database analysis report generation tasks.

**Link**: [arxiv](http://arxiv.org/abs/2503.13269v1),  [pdf](http://arxiv.org/pdf/2503.13269v1)

**Tags**: cs.DB 



### Channel Estimation for Pinching-Antenna Systems (PASS)
**Authors**: Jian Xiao, Ji Wang, Yuanwei Liu

**Updated**: 2025-03-17T15:20:49Z

**Summary**: Pinching Antennas (PAs) represent a revolutionary flexible antenna technology that leverages dielectric waveguides and electromagnetic coupling to mitigate large-scale path loss. This letter is the first to explore channel estimation for Pinching-Antenna SyStems (PASS), addressing their uniquely ill-conditioned and underdetermined channel characteristics. In particular, two efficient deep learning-based channel estimators are proposed. 1) PAMoE: This estimator incorporates dynamic padding, feature embedding, fusion, and mixture of experts (MoE) modules, which effectively leverage the positional information of PAs and exploit expert diversity. 2) PAformer: This Transformer-style estimator employs the self-attention mechanism to predict channel coefficients in a per-antenna manner, which offers more flexibility to adaptively deal with dynamic numbers of PAs in practical deployment. Numerical results demonstrate that 1) the proposed deep learning-based channel estimators outperform conventional methods and exhibit excellent zero-shot learning capabilities, and 2) PAMoE delivers higher channel estimation accuracy via MoE specialization, while PAformer natively handles an arbitrary number of PAs, trading self-attention complexity for superior scalability.

**Link**: [arxiv](http://arxiv.org/abs/2503.13268v1),  [pdf](http://arxiv.org/pdf/2503.13268v1)

**Tags**: cs.IT eess.SP math.IT 



### GraphRouter: A Graph-based Router for LLM Selections
**Authors**: Tao Feng, Yanzhen Shen, Jiaxuan You

**Updated**: 2025-03-17T15:08:47Z

**Summary**: The rapidly growing number and variety of Large Language Models (LLMs) present significant challenges in efficiently selecting the appropriate LLM for a given query, especially considering the trade-offs between performance and computational cost. Current LLM selection methods often struggle to generalize across new LLMs and different tasks because of their limited ability to leverage contextual interactions among tasks, queries, and LLMs, as well as their dependence on a transductive learning framework. To address these shortcomings, we introduce a novel inductive graph framework, named as GraphRouter, which fully utilizes the contextual information among tasks, queries, and LLMs to enhance the LLM selection process. GraphRouter constructs a heterogeneous graph comprising task, query, and LLM nodes, with interactions represented as edges, which efficiently captures the contextual information between the query's requirements and the LLM's capabilities. Through an innovative edge prediction mechanism, GraphRouter is able to predict attributes (the effect and cost of LLM response) of potential edges, allowing for optimized recommendations that adapt to both existing and newly introduced LLMs without requiring retraining. Comprehensive experiments across three distinct effect-cost weight scenarios have shown that GraphRouter substantially surpasses existing routers, delivering a minimum performance improvement of 12.3%. In addition, it achieves enhanced generalization across new LLMs settings and supports diverse tasks with at least a 9.5% boost in effect and a significant reduction in computational demands. This work endeavors to apply a graph-based approach for the contextual and adaptive selection of LLMs, offering insights for real-world applications. Our codes for GraphRouter is released at https://github.com/ulab-uiuc/GraphRouter.

**Link**: [arxiv](http://arxiv.org/abs/2410.03834v2),  [pdf](http://arxiv.org/pdf/2410.03834v2)

**Tags**: cs.AI 



### MindEye-OmniAssist: A Gaze-Driven LLM-Enhanced Assistive Robot System   for Implicit Intention Recognition and Task Execution
**Authors**: Zejia Zhang, Bo Yang, Xinxing Chen, Weizhuang Shi, Haoyuan Wang, Wei Luo, Jian Huang

**Updated**: 2025-03-17T15:06:14Z

**Summary**: A promising effective human-robot interaction in assistive robotic systems is gaze-based control. However, current gaze-based assistive systems mainly help users with basic grasping actions, offering limited support. Moreover, the restricted intent recognition capability constrains the assistive system's ability to provide diverse assistance functions. In this paper, we propose an open implicit intention recognition framework powered by Large Language Model (LLM) and Vision Foundation Model (VFM), which can process gaze input and recognize user intents that are not confined to predefined or specific scenarios. Furthermore, we implement a gaze-driven LLM-enhanced assistive robot system (MindEye-OmniAssist) that recognizes user's intentions through gaze and assists in completing task. To achieve this, the system utilizes open vocabulary object detector, intention recognition network and LLM to infer their full intentions. By integrating eye movement feedback and LLM, it generates action sequences to assist the user in completing tasks. Real-world experiments have been conducted for assistive tasks, and the system achieved an overall success rate of 41/55 across various undefined tasks. Preliminary results show that the proposed method holds the potential to provide a more user-friendly human-computer interaction interface and significantly enhance the versatility and effectiveness of assistive systems by supporting more complex and diverse task.

**Link**: [arxiv](http://arxiv.org/abs/2503.13250v1),  [pdf](http://arxiv.org/pdf/2503.13250v1)

**Tags**: cs.RO cs.HC 



### Learning Program Behavioral Models from Synthesized Input-Output Pairs
**Authors**: Tural Mammadov, Dietrich Klakow, Alexander Koller, Andreas Zeller

**Updated**: 2025-03-17T15:04:06Z

**Summary**: We introduce Modelizer - a novel framework that, given a black-box program, learns a model from its input/output behavior using neural machine translation algorithms. The resulting model mocks the original program: Given an input, the model predicts the output that would have been produced by the program. However, the model is also reversible - that is, the model can predict the input that would have produced a given output. Finally, the model is differentiable and can be efficiently restricted to predict only a certain aspect of the program behavior. Modelizer uses grammars to synthesize and inputs and unsupervised tokenizers to decompose the resulting outputs, allowing it to learn sequence-to-sequence associations between token streams. Other than input grammars, Modelizer only requires the ability to execute the program. The resulting models are small, requiring fewer than 6.3 million parameters for languages such as Markdown or HTML; and they are accurate, achieving up to 95.4% accuracy and a BLEU score of 0.98 with standard error 0.04 in mocking real-world applications. As it learns from and predicts executions rather than code, Modelizer departs from the LLM-centric research trend, opening new opportunities for program-specific models that are fully tuned towards individual programs. Indeed, we foresee several applications of these models, especially as the output of the program can be any aspect of program behavior. Beyond mocking and predicting program behavior, the models can also synthesize inputs that are likely to produce a particular behavior, such as failures or coverage, thus assisting in program understanding and maintenance.

**Link**: [arxiv](http://arxiv.org/abs/2407.08597v2),  [pdf](http://arxiv.org/pdf/2407.08597v2)

**Tags**: cs.SE cs.LG 68T07 (Primary), 68N30 (Secondary), 68Q42 D.2.5; D.2.7; I.2.6; F.1.1; F.4.3 



### Wireless charging and readout via textile coil for continuous full-body   wearable computing
**Authors**: Ryo Takahashi, Yoshihiro Kawahara

**Updated**: 2025-03-17T14:46:24Z

**Summary**: The growing use of wearable devices for activity tracking, healthcare, and haptics faces challenges due to the bulkiness and short lifespan of batteries. Integration of a textile-based wireless charging and readout system into everyday clothing can enable seamless power supply and data collection around the body. However, expanding such system to cover the entire body is challenging, as it increases electromagnetic interference with the body, degrading the performance of wireless system. This article introduces a meandered textile coil designed for body-scale, efficient wireless charging and readout. The meander coil can confine a strong inductive field near the body surface, ensuring W-class safe charging and sensitive readout with uW-class low power. Moreover, its zigzag design is simple enough for mass production on industrial knitting machines. Therefore, the body-scale meander coil can continuously operate battery-free wearable devices across the body, leading to ubiquitous deployment of continuous full-body wearable computing into everyday clothing.

**Link**: [arxiv](http://arxiv.org/abs/2502.11015v2),  [pdf](http://arxiv.org/pdf/2502.11015v2)

**Tags**: cs.HC 



### Robust Decision-Making Via Free Energy Minimization
**Authors**: Allahkaram Shafiei, Hozefa Jesawada, Karl Friston, Giovanni Russo

**Updated**: 2025-03-17T14:36:08Z

**Summary**: Despite their groundbreaking performance, state-of-the-art autonomous agents can misbehave when training and environmental conditions become inconsistent, with minor mismatches leading to undesirable behaviors or even catastrophic failures. Robustness towards these training/environment ambiguities is a core requirement for intelligent agents and its fulfillment is a long-standing challenge when deploying agents in the real world. Here, departing from mainstream views seeking robustness through training, we introduce DR-FREE, a free energy model that installs this core property by design. It directly wires robustness into the agent decision-making mechanisms via free energy minimization. By combining a robust extension of the free energy principle with a novel resolution engine, DR-FREE returns a policy that is optimal-yet-robust against ambiguity. Moreover, for the first time, it reveals the mechanistic role of ambiguity on optimal decisions and requisite Bayesian belief updating. We evaluate DR-FREE on an experimental testbed involving real rovers navigating an ambiguous environment filled with obstacles. Across all the experiments, DR-FREE enables robots to successfully navigate towards their goal even when, in contrast, standard free energy minimizing agents that do not use DR-FREE fail. In short, DR-FREE can tackle scenarios that elude previous methods: this milestone may inspire both deployment in multi-agent settings and, at a perhaps deeper level, the quest for a biologically plausible explanation of how natural agents - with little or no training - survive in capricious environments.

**Link**: [arxiv](http://arxiv.org/abs/2503.13223v1),  [pdf](http://arxiv.org/pdf/2503.13223v1)

**Tags**: cs.AI cs.SY eess.SY math.OC 



### Causal Graphs Meet Thoughts: Enhancing Complex Reasoning in   Graph-Augmented LLMs
**Authors**: Hang Luo, Jian Zhang, Chujun Li

**Updated**: 2025-03-17T14:32:08Z

**Summary**: In knowledge-intensive tasks, especially in high-stakes domains like medicine and law, it is critical not only to retrieve relevant information but also to provide causal reasoning and explainability. Large language models (LLMs) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, integrating knowledge graphs with Graph Retrieval-Augmented Generation (Graph RAG) has emerged as an effective solution. Traditional Graph RAG methods often rely on simple graph traversal or semantic similarity, which do not capture causal relationships or align well with the model's internal reasoning steps. This paper proposes a novel pipeline that filters large knowledge graphs to emphasize cause-effect edges, aligns the retrieval process with the model's chain-of-thought (CoT), and enhances reasoning through multi-stage path improvements. Experiments on medical question-answering tasks show consistent gains, with up to a 10\% absolute improvement across multiple large language models (LLMs). This approach demonstrates the value of combining causal reasoning with stepwise retrieval, leading to more interpretable and logically grounded solutions for complex queries.

**Link**: [arxiv](http://arxiv.org/abs/2501.14892v2),  [pdf](http://arxiv.org/pdf/2501.14892v2)

**Tags**: cs.AI cs.CL 



### Can Language Models Follow Multiple Turns of Entangled Instructions?
**Authors**: Chi Han

**Updated**: 2025-03-17T14:31:37Z

**Summary**: Despite significant achievements in improving the instruction-following capabilities of large language models (LLMs), the ability to process multiple potentially entangled or conflicting instructions remains a considerable challenge. Real-world scenarios often require consistency across multiple instructions over time, such as secret privacy, personal preferences, and prioritization, which demand sophisticated abilities to integrate multiple turns and carefully balance competing objectives when instructions intersect or conflict. This work presents a systematic investigation of LLMs' capabilities in handling multiple turns of instructions, covering three levels of difficulty: (1) retrieving information from instructions, (2) tracking and reasoning across turns, and (3) resolving conflicts among instructions. We construct MultiTurnInstruct with around 1.1K high-quality multi-turn conversations through the human-in-the-loop approach and result in nine capability categories, including statics and dynamics, reasoning, and multitasking. Our finding reveals an intriguing trade-off between different capabilities. While GPT models demonstrate superior memorization, they show reduced effectiveness in privacy-protection tasks requiring selective information withholding. Larger models exhibit stronger reasoning capabilities but still struggle with resolving conflicting instructions. Importantly, these performance gaps cannot be attributed solely to information loss, as models demonstrate strong BLEU scores on memorization tasks but their attention mechanisms fail to integrate multiple related instructions effectively. These findings highlight critical areas for improvement in complex real-world tasks involving multi-turn instructions.

**Link**: [arxiv](http://arxiv.org/abs/2503.13222v1),  [pdf](http://arxiv.org/pdf/2503.13222v1)

**Tags**: cs.CL cs.AI 



### Improving Complex Reasoning with Dynamic Prompt Corruption: A soft   prompt Optimization Approach
**Authors**: Sinan Fan, Liang Xie, Chen Shen, Ge Teng, Xiaosong Yuan, Xiaofeng Zhang, Chenxi Huang, Wenxiao Wang, Xiaofei He, Jieping Ye

**Updated**: 2025-03-17T14:20:48Z

**Summary**: Prompt-tuning (PT) for large language models (LLMs) can facilitate the performance on various conventional NLP tasks with significantly fewer trainable parameters. However, our investigation reveals that PT provides limited improvement and may even degrade the primitive performance of LLMs on complex reasoning tasks. Such a phenomenon suggests that soft prompts can positively impact certain instances while negatively affecting others, particularly during the later phases of reasoning. To address these challenges, We first identify an information accumulation within the soft prompts. Through detailed analysis, we demonstrate that this phenomenon is often accompanied by erroneous information flow patterns in the deeper layers of the model, which ultimately lead to incorrect reasoning outcomes. we propose a novel method called \textbf{D}ynamic \textbf{P}rompt \textbf{C}orruption (DPC) to take better advantage of soft prompts in complex reasoning tasks, which dynamically adjusts the influence of soft prompts based on their impact on the reasoning process. Specifically, DPC consists of two stages: Dynamic Trigger and Dynamic Corruption. First, Dynamic Trigger measures the impact of soft prompts, identifying whether beneficial or detrimental. Then, Dynamic Corruption mitigates the negative effects of soft prompts by selectively masking key tokens that interfere with the reasoning process. We validate the proposed approach through extensive experiments on various LLMs and reasoning tasks, including GSM8K, MATH, and AQuA. Experimental results demonstrate that DPC can consistently enhance the performance of PT, achieving 4\%-8\% accuracy gains compared to vanilla prompt tuning, highlighting the effectiveness of our approach and its potential to enhance complex reasoning in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2503.13208v1),  [pdf](http://arxiv.org/pdf/2503.13208v1)

**Tags**: cs.CL cs.AI 



### Towards Efficient Mixture of Experts: A Holistic Study of Compression   Techniques
**Authors**: Shwai He, Daize Dong, Liang Ding, Ang Li

**Updated**: 2025-03-17T14:18:42Z

**Summary**: Scaling large language models has driven remarkable advancements across various domains, yet the continual increase in model size presents significant challenges for real-world deployment. The Mixture of Experts (MoE) architecture offers a promising solution by dynamically selecting and activating only a subset of experts during inference, thus substantially reducing computational costs while preserving high performance. Despite these benefits, MoE introduces new inefficiencies, such as excessive parameters and communication overhead. In this work, we present a holistic study of compression techniques for Mixture of Experts to enhance both efficiency and scalability. While recent efforts have focused on Expert Trimming, which reduces the number of experts, these approaches still suffer from considerable communication and computational costs. To address this, we propose more aggressive strategies, such as Layer Drop, which removes entire MoE layers, and Block Drop, which eliminates transformer blocks. Surprisingly, these aggressive pruning techniques not only preserve model performance but also substantially improve computation and memory efficiency. Furthermore, beyond Expert Trimming, we also introduce Expert Slimming, which compresses individual experts to further boost performance and can be seamlessly integrated with Expert Trimming. Extensive experimental results demonstrate the effectiveness of our proposed methods-Layer Drop and Block Drop-along with the comprehensive recipe that integrates Expert Slimming and Expert Trimming, achieving a 6.05x speedup with 77.1% reduced memory usage while maintaining over 92% of performance on Mixtral-8x7B. Our code is released at https://github.com/CASE-Lab-UMD/Unified-MoE-Compression.

**Link**: [arxiv](http://arxiv.org/abs/2406.02500v3),  [pdf](http://arxiv.org/pdf/2406.02500v3)

**Tags**: cs.LG cs.AI 



### MAP: Evaluation and Multi-Agent Enhancement of Large Language Models for   Inpatient Pathways
**Authors**: Zhen Chen, Zhihao Peng, Xusheng Liang, Cheng Wang, Peigan Liang, Linsheng Zeng, Minjie Ju, Yixuan Yuan

**Updated**: 2025-03-17T14:14:28Z

**Summary**: Inpatient pathways demand complex clinical decision-making based on comprehensive patient information, posing critical challenges for clinicians. Despite advancements in large language models (LLMs) in medical applications, limited research focused on artificial intelligence (AI) inpatient pathways systems, due to the lack of large-scale inpatient datasets. Moreover, existing medical benchmarks typically concentrated on medical question-answering and examinations, ignoring the multifaceted nature of clinical decision-making in inpatient settings. To address these gaps, we first developed the Inpatient Pathway Decision Support (IPDS) benchmark from the MIMIC-IV database, encompassing 51,274 cases across nine triage departments and 17 major disease categories alongside 16 standardized treatment options. Then, we proposed the Multi-Agent Inpatient Pathways (MAP) framework to accomplish inpatient pathways with three clinical agents, including a triage agent managing the patient admission, a diagnosis agent serving as the primary decision maker at the department, and a treatment agent providing treatment plans. Additionally, our MAP framework includes a chief agent overseeing the inpatient pathways to guide and promote these three clinician agents. Extensive experiments showed our MAP improved the diagnosis accuracy by 25.10% compared to the state-of-the-art LLM HuatuoGPT2-13B. It is worth noting that our MAP demonstrated significant clinical compliance, outperforming three board-certified clinicians by 10%-12%, establishing a foundation for inpatient pathways systems.

**Link**: [arxiv](http://arxiv.org/abs/2503.13205v1),  [pdf](http://arxiv.org/pdf/2503.13205v1)

**Tags**: cs.AI cs.CL cs.CV cs.HC cs.MA 



### Valley: Video Assistant with Large Language model Enhanced abilitY
**Authors**: Ruipu Luo, Ziwang Zhao, Min Yang, Zheming Yang, Minghui Qiu, Tao Wang, Zhongyu Wei, Yanhao Wang, Cen Chen

**Updated**: 2025-03-17T13:51:51Z

**Summary**: Large Language Models (LLMs), with remarkable conversational capability, have emerged as AI assistants that can handle both visual and textual modalities. However, their effectiveness in joint video and language understanding has not been extensively explored. In the paper, we introduce Valley, a multi-modal foundation model that is designed to enable enhanced video comprehension and instruction-following capabilities. To this end, we construct two datasets, namely Valley-702k and Valley-instruct-73k, to cover a diverse range of video-text alignment and video-based instruction tasks, such as multi-shot captions, long video descriptions, action recognition, causal inference, etc. Then, we adopt ViT-L/14 as the vision encoder and explore three different temporal modeling modules to learn multifaceted features for enhanced video understanding. In addition, we implement a two-phase training approach for Valley: the first phase focuses solely on training the projection module to facilitate the LLM's capacity to understand visual input, and the second phase jointly trains the projection module and the LLM to improve their instruction following ability. Extensive experiments demonstrate that Valley has the potential to serve as an effective video assistant, simplifying complex video-understanding scenarios. Our code and data are published anonymously at https://github.com/valley-vl/Valley.

**Link**: [arxiv](http://arxiv.org/abs/2306.07207v3),  [pdf](http://arxiv.org/pdf/2306.07207v3)

**Tags**: cs.CV cs.AI cs.CL 



### Assessing the Robustness of LLM-based NLP Software via Automated Testing
**Authors**: Mingxuan Xiao, Yan Xiao, Shunhui Ji, Hanbo Cai, Lei Xue, Pengcheng Zhang

**Updated**: 2025-03-17T13:42:06Z

**Summary**: Benefiting from the advancements in LLMs, NLP software has undergone rapid development. Such software is widely employed in various safety-critical tasks, such as financial sentiment analysis, toxic content moderation, and log generation. Unlike traditional software, LLM-based NLP software relies on prompts and examples as inputs. Given the complexity of LLMs and the unpredictability of real-world inputs, quantitatively assessing the robustness of such software is crucial. However, to the best of our knowledge, no automated robustness testing methods have been specifically designed to evaluate the overall inputs of LLM-based NLP software. To this end, this paper introduces the first AutOmated Robustness Testing frAmework, AORTA, which reconceptualizes the testing process into a combinatorial optimization problem. Existing testing methods designed for DNN-based software can be applied to LLM-based software by AORTA, but their effectiveness is limited. To address this, we propose a novel testing method for LLM-based software within AORTA called Adaptive Beam Search. ABS is tailored for the expansive feature space of LLMs and improves testing effectiveness through an adaptive beam width and the capability for backtracking. We successfully embed 18 test methods in the designed framework AORTA and compared the test validity of ABS with three datasets and five threat models. ABS facilitates a more comprehensive and accurate robustness assessment before software deployment, with an average test success rate of 86.138%. Compared to the currently best-performing baseline PWWS, ABS significantly reduces the computational overhead by up to 3441.895 seconds per successful test case and decreases the number of queries by 218.762 times on average. Furthermore, test cases generated by ABS exhibit greater naturalness and transferability.

**Link**: [arxiv](http://arxiv.org/abs/2412.21016v2),  [pdf](http://arxiv.org/pdf/2412.21016v2)

**Tags**: cs.SE 



### Empowering Edge Intelligence: A Comprehensive Survey on On-Device AI   Models
**Authors**: Xubin Wang, Zhiqing Tang, Jianxiong Guo, Tianhui Meng, Chenhao Wang, Tian Wang, Weijia Jia

**Updated**: 2025-03-17T13:37:33Z

**Summary**: The rapid advancement of artificial intelligence (AI) technologies has led to an increasing deployment of AI models on edge and terminal devices, driven by the proliferation of the Internet of Things (IoT) and the need for real-time data processing. This survey comprehensively explores the current state, technical challenges, and future trends of on-device AI models. We define on-device AI models as those designed to perform local data processing and inference, emphasizing their characteristics such as real-time performance, resource constraints, and enhanced data privacy. The survey is structured around key themes, including the fundamental concepts of AI models, application scenarios across various domains, and the technical challenges faced in edge environments. We also discuss optimization and implementation strategies, such as data preprocessing, model compression, and hardware acceleration, which are essential for effective deployment. Furthermore, we examine the impact of emerging technologies, including edge computing and foundation models, on the evolution of on-device AI models. By providing a structured overview of the challenges, solutions, and future directions, this survey aims to facilitate further research and application of on-device AI, ultimately contributing to the advancement of intelligent systems in everyday life.

**Link**: [arxiv](http://arxiv.org/abs/2503.06027v2),  [pdf](http://arxiv.org/pdf/2503.06027v2)

**Tags**: cs.AI cs.LG cs.NI 



### LLM-based speaker diarization correction: A generalizable approach
**Authors**: Georgios Efstathiadis, Vijay Yadav, Anzar Abbas

**Updated**: 2025-03-17T13:34:07Z

**Summary**: Speaker diarization is necessary for interpreting conversations transcribed using automated speech recognition (ASR) tools. Despite significant developments in diarization methods, diarization accuracy remains an issue. Here, we investigate the use of large language models (LLMs) for diarization correction as a post-processing step. LLMs were fine-tuned using the Fisher corpus, a large dataset of transcribed conversations. The ability of the models to improve diarization accuracy in a holdout dataset from the Fisher corpus as well as an independent dataset was measured. We report that fine-tuned LLMs can markedly improve diarization accuracy. However, model performance is constrained to transcripts produced using the same ASR tool as the transcripts used for fine-tuning, limiting generalizability. To address this constraint, an ensemble model was developed by combining weights from three separate models, each fine-tuned using transcripts from a different ASR tool. The ensemble model demonstrated better overall performance than each of the ASR-specific models, suggesting that a generalizable and ASR-agnostic approach may be achievable. We have made the weights of these models publicly available on HuggingFace at https://huggingface.co/bklynhlth.

**Link**: [arxiv](http://arxiv.org/abs/2406.04927v3),  [pdf](http://arxiv.org/pdf/2406.04927v3)

**Tags**: eess.AS cs.CL 



### Are LLMs (Really) Ideological? An IRT-based Analysis and Alignment Tool   for Perceived Socio-Economic Bias in LLMs
**Authors**: Jasmin Wachter, Michael Radloff, Maja Smolej, Katharina Kinder-Kurlanda

**Updated**: 2025-03-17T13:20:09Z

**Summary**: We introduce an Item Response Theory (IRT)-based framework to detect and quantify socioeconomic bias in large language models (LLMs) without relying on subjective human judgments. Unlike traditional methods, IRT accounts for item difficulty, improving ideological bias estimation. We fine-tune two LLM families (Meta-LLaMa 3.2-1B-Instruct and Chat- GPT 3.5) to represent distinct ideological positions and introduce a two-stage approach: (1) modeling response avoidance and (2) estimating perceived bias in answered responses. Our results show that off-the-shelf LLMs often avoid ideological engagement rather than exhibit bias, challenging prior claims of partisanship. This empirically validated framework enhances AI alignment research and promotes fairer AI governance.

**Link**: [arxiv](http://arxiv.org/abs/2503.13149v1),  [pdf](http://arxiv.org/pdf/2503.13149v1)

**Tags**: cs.AI cs.CL cs.CY 



### Cornstarch: Distributed Multimodal Training Must Be Multimodality-Aware
**Authors**: Insu Jang, Runyu Lu, Nikhil Bansal, Ang Chen, Mosharaf Chowdhury

**Updated**: 2025-03-17T12:39:15Z

**Summary**: Multimodal large language models (MLLMs) extend the capabilities of large language models (LLMs) by combining heterogeneous model architectures to handle diverse modalities like images and audio. However, this inherent heterogeneity in MLLM model structure and data types makes makeshift extensions to existing LLM training frameworks unsuitable for efficient MLLM training.   In this paper, we present Cornstarch, the first general-purpose distributed MLLM training framework. Cornstarch facilitates modular MLLM construction, enables composable parallelization of constituent models, and introduces MLLM-specific optimizations to pipeline and context parallelism for efficient distributed MLLM training. Our evaluation shows that Cornstarch outperforms state-of-the-art solutions by up to $1.57\times$ in terms of training throughput.   Cornstarch is an open-source project available at https://github.com/cornstarch-org/Cornstarch.

**Link**: [arxiv](http://arxiv.org/abs/2503.11367v2),  [pdf](http://arxiv.org/pdf/2503.11367v2)

**Tags**: cs.DC 



### VeriLeaky: Navigating IP Protection vs Utility in Fine-Tuning for   LLM-Driven Verilog Coding
**Authors**: Zeng Wang, Minghao Shao, Mohammed Nabeel, Prithwish Basu Roy, Likhitha Mankali, Jitendra Bhandari, Ramesh Karri, Ozgur Sinanoglu, Muhammad Shafique, Johann Knechtel

**Updated**: 2025-03-17T12:38:03Z

**Summary**: Large language models (LLMs) offer significant potential for coding, yet fine-tuning (FT) with curated data is essential for niche languages like Verilog. Using proprietary intellectual property (IP) for FT presents a serious risk, as FT data can be leaked through LLM inference. This leads to a critical dilemma for design houses: seeking to build externally accessible LLMs offering competitive Verilog coding, how can they leverage in-house IP to enhance FT utility while ensuring IP protection?   For the first time in the literature, we study this dilemma. Using LLaMA 3.1-8B, we conduct in-house FT on a baseline Verilog dataset (RTLCoder) supplemented with our own in-house IP, which is validated through multiple tape-outs. To rigorously assess IP leakage, we quantify structural similarity (AST/Dolos) and functional equivalence (Synopsys Formality) between generated codes and our in-house IP. We show that our IP can indeed be leaked, confirming the threat. As defense, we evaluate logic locking of Verilog codes (ASSURE). This offers some level of protection, yet reduces the IP's utility for FT and degrades the LLM's performance. Our study shows the need for novel strategies that are both effective and minimally disruptive to FT, an essential effort for enabling design houses to fully utilize their proprietary IP toward LLM-driven Verilog coding.

**Link**: [arxiv](http://arxiv.org/abs/2503.13116v1),  [pdf](http://arxiv.org/pdf/2503.13116v1)

**Tags**: cs.CR cs.AR cs.LG 



### MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs
**Authors**: Erik Daxberger, Nina Wenzel, David Griffiths, Haiming Gang, Justin Lazarow, Gefen Kohavi, Kai Kang, Marcin Eichner, Yinfei Yang, Afshin Dehghan, Peter Grasch

**Updated**: 2025-03-17T12:34:22Z

**Summary**: Multimodal large language models (MLLMs) excel at 2D visual understanding but remain limited in their ability to reason about 3D space. In this work, we leverage large-scale high-quality 3D scene data with open-set annotations to introduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation benchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data covers diverse spatial tasks including spatial relationship prediction, metric size and distance estimation, and 3D grounding. We show that CA-VQA enables us to train MM-Spatial, a strong generalist MLLM that also achieves state-of-the-art performance on 3D spatial understanding benchmarks, including our own. We show how incorporating metric depth and multi-view inputs (provided in CA-VQA) can further improve 3D understanding, and demonstrate that data alone allows our model to achieve depth perception capabilities comparable to dedicated monocular depth estimation models. We will publish our SFT dataset and benchmark.

**Link**: [arxiv](http://arxiv.org/abs/2503.13111v1),  [pdf](http://arxiv.org/pdf/2503.13111v1)

**Tags**: cs.CV cs.CL cs.LG 



### Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways   to Faster Inference
**Authors**: Hao Yin, Guangzong Si, Zilei Wang

**Updated**: 2025-03-17T12:31:23Z

**Summary**: Multimodal large language models (MLLMs) improve performance on vision-language tasks by integrating visual features from pre-trained vision encoders into large language models (LLMs). However, how MLLMs process and utilize visual information remains unclear. In this paper, a shift in the dominant flow of visual information is uncovered: (1) in shallow layers, strong interactions are observed between image tokens and instruction tokens, where most visual information is injected into instruction tokens to form cross-modal semantic representations; (2) in deeper layers, image tokens primarily interact with each other, aggregating the remaining visual information to optimize semantic representations within visual modality. Based on these insights, we propose Hierarchical Modality-Aware Pruning (HiMAP), a plug-and-play inference acceleration method that dynamically prunes image tokens at specific layers, reducing computational costs by approximately 65% without sacrificing performance. Our findings offer a new understanding of visual information processing in MLLMs and provide a state-of-the-art solution for efficient inference.

**Link**: [arxiv](http://arxiv.org/abs/2503.13108v1),  [pdf](http://arxiv.org/pdf/2503.13108v1)

**Tags**: cs.CV cs.AI 



### Fine-tuning can Help Detect Pretraining Data from Large Language Models
**Authors**: Hengxiang Zhang, Songxin Zhang, Bingyi Jing, Hongxin Wei

**Updated**: 2025-03-17T12:29:05Z

**Summary**: In the era of large language models (LLMs), detecting pretraining data has been increasingly important due to concerns about fair evaluation and ethical risks. Current methods differentiate members and non-members by designing scoring functions, like Perplexity and Min-k%. However, the diversity and complexity of training data magnifies the difficulty of distinguishing, leading to suboptimal performance in detecting pretraining data. In this paper, we first explore the benefits of unseen data, which can be easily collected after the release of the LLM. We find that the perplexities of LLMs shift differently for members and non-members, after fine-tuning with a small amount of previously unseen data. In light of this, we introduce a novel and effective method termed Fine-tuned Score Deviation(FSD), which improves the performance of current scoring functions for pretraining data detection. In particular, we propose to measure the deviation distance of current scores after fine-tuning on a small amount of unseen data within the same domain. In effect, using a few unseen data can largely decrease the scores of all non-members, leading to a larger deviation distance than members. Extensive experiments demonstrate the effectiveness of our method, significantly improving the AUC score on common benchmark datasets across various models.

**Link**: [arxiv](http://arxiv.org/abs/2410.10880v2),  [pdf](http://arxiv.org/pdf/2410.10880v2)

**Tags**: cs.CL cs.AI cs.LG 



### Managing Hybrid Solid-State Drives Using Large Language Models
**Authors**: Qian Wei, Yi Li, Zehao Chen, Zhaoyan Shen, Dongxiao Yu, Bingzhe Li

**Updated**: 2025-03-17T12:25:42Z

**Summary**: Hybrid Solid-State Drives (SSDs), which integrate several types of flash cells (e.g., single-level cell (SLC) and multiple-level cell (MLC)) in a single drive and enable them to convert between each other, are designed to deliver both high performance and high storage capacity. However, compared to traditional SSDs, hybrid SSDs also introduce a much larger design space, resulting in higher optimization complexity due to more design factors involved, including flash conversion timing and data migration between different flash cells, etc. To address these challenges, large language models (LLMs) could be a promising technique, as they excel in handling complex, high-dimensional parameter space exploration by leveraging their advanced capability to identify patterns and optimize solutions. Recent works have started exploring the use of LLMs to optimize computer systems. However, to the best of our knowledge, no study has focused on optimizing SSDs with the assistance of LLMs.   In this work, we explore the potential of LLMs in understanding and efficiently managing hybrid SSD design space. Specifically, two important questions are exploited and analyzed: 1) Can LLMs offer optimization potential for Hybrid SSD management? 2) How to leverage LLMs for the performance and efficiency of hybrid SSD optimization? Based on the observations of exploration, we propose a comprehensive auto-tuning framework for hybrid SSDs, integrating LLMs to recommend customized configurations using calibration prompts derived from hardware, system, and workload information. Experimental results reveal a 62.35% improvement in throughput and a 57.99% decrease in write amplification compared to the default hybrid SSD configurations achieved with the incorporation of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2503.13105v1),  [pdf](http://arxiv.org/pdf/2503.13105v1)

**Tags**: cs.AR 



### REPA: Russian Error Types Annotation for Evaluating Text Generation and   Judgment Capabilities
**Authors**: Alexander Pugachev, Alena Fenogenova, Vladislav Mikhailov, Ekaterina Artemova

**Updated**: 2025-03-17T12:15:16Z

**Summary**: Recent advances in large language models (LLMs) have introduced the novel paradigm of using LLMs as judges, where an LLM evaluates and scores the outputs of another LLM, which often correlates highly with human preferences. However, the use of LLM-as-a-judge has been primarily studied in English. In this paper, we evaluate this framework in Russian by introducing the Russian Error tyPes Annotation dataset (REPA), a dataset of 1k user queries and 2k LLM-generated responses. Human annotators labeled each response pair expressing their preferences across ten specific error types, as well as selecting an overall preference. We rank six generative LLMs across the error types using three rating systems based on human preferences. We also evaluate responses using eight LLM judges in zero-shot and few-shot settings. We describe the results of analyzing the judges and position and length biases. Our findings reveal a notable gap between LLM judge performance in Russian and English. However, rankings based on human and LLM preferences show partial alignment, suggesting that while current LLM judges struggle with fine-grained evaluation in Russian, there is potential for improvement.

**Link**: [arxiv](http://arxiv.org/abs/2503.13102v1),  [pdf](http://arxiv.org/pdf/2503.13102v1)

**Tags**: cs.CL 



### Who Wrote This? Identifying Machine vs Human-Generated Text in Hausa
**Authors**: Babangida Sani, Aakansha Soy, Sukairaj Hafiz Imam, Ahmad Mustapha, Lukman Jibril Aliyu, Idris Abdulmumin, Ibrahim Said Ahmad, Shamsuddeen Hassan Muhammad

**Updated**: 2025-03-17T12:13:37Z

**Summary**: The advancement of large language models (LLMs) has allowed them to be proficient in various tasks, including content generation. However, their unregulated usage can lead to malicious activities such as plagiarism and generating and spreading fake news, especially for low-resource languages. Most existing machine-generated text detectors are trained on high-resource languages like English, French, etc. In this study, we developed the first large-scale detector that can distinguish between human- and machine-generated content in Hausa. We scrapped seven Hausa-language media outlets for the human-generated text and the Gemini-2.0 flash model to automatically generate the corresponding Hausa-language articles based on the human-generated article headlines. We fine-tuned four pre-trained Afri-centric models (AfriTeVa, AfriBERTa, AfroXLMR, and AfroXLMR-76L) on the resulting dataset and assessed their performance using accuracy and F1-score metrics. AfroXLMR achieved the highest performance with an accuracy of 99.23% and an F1 score of 99.21%, demonstrating its effectiveness for Hausa text detection. Our dataset is made publicly available to enable further research.

**Link**: [arxiv](http://arxiv.org/abs/2503.13101v1),  [pdf](http://arxiv.org/pdf/2503.13101v1)

**Tags**: cs.CL 



### Beemo: Benchmark of Expert-edited Machine-generated Outputs
**Authors**: Ekaterina Artemova, Jason Lucas, Saranya Venkatraman, Jooyoung Lee, Sergei Tilga, Adaku Uchendu, Vladislav Mikhailov

**Updated**: 2025-03-17T12:05:36Z

**Summary**: The rapid proliferation of large language models (LLMs) has increased the volume of machine-generated texts (MGTs) and blurred text authorship in various domains. However, most existing MGT benchmarks include single-author texts (human-written and machine-generated). This conventional design fails to capture more practical multi-author scenarios, where the user refines the LLM response for natural flow, coherence, and factual correctness. Our paper introduces the Benchmark of Expert-edited Machine-generated Outputs (Beemo), which includes 6.5k texts written by humans, generated by ten instruction-finetuned LLMs, and edited by experts for various use cases, ranging from creative writing to summarization. Beemo additionally comprises 13.1k machine-generated and LLM-edited texts, allowing for diverse MGT detection evaluation across various edit types. We document Beemo's creation protocol and present the results of benchmarking 33 configurations of MGT detectors in different experimental setups. We find that expert-based editing evades MGT detection, while LLM-edited texts are unlikely to be recognized as human-written. Beemo and all materials are publicly available.

**Link**: [arxiv](http://arxiv.org/abs/2411.04032v3),  [pdf](http://arxiv.org/pdf/2411.04032v3)

**Tags**: cs.CL 



### ClusComp: A Simple Paradigm for Model Compression and Efficient   Finetuning
**Authors**: Baohao Liao, Christian Herold, Seyyed Hadi Hashemi, Stefan Vasilev, Shahram Khadivi, Christof Monz

**Updated**: 2025-03-17T11:52:16Z

**Summary**: As large language models (LLMs) scale, model compression is crucial for edge deployment and accessibility. Weight-only quantization reduces model size but suffers from performance degradation at lower bit widths. Moreover, standard finetuning is incompatible with quantized models, and alternative methods often fall short of full finetuning. In this paper, we propose ClusComp, a simple yet effective compression paradigm that clusters weight matrices into codebooks and finetunes them block-by-block. ClusComp (1) achieves superior performance in 2-4 bit quantization, (2) pushes compression to 1-bit while outperforming ultra-low-bit methods with minimal finetuning, and (3) enables efficient finetuning, even surpassing existing quantization-based approaches and rivaling full FP16 finetuning. Notably, ClusComp supports compression and finetuning of 70B LLMs on a single A6000-48GB GPU.

**Link**: [arxiv](http://arxiv.org/abs/2503.13089v1),  [pdf](http://arxiv.org/pdf/2503.13089v1)

**Tags**: cs.CL cs.AI 



### A Framework to Assess Multilingual Vulnerabilities of LLMs
**Authors**: Likai Tang, Niruth Bogahawatta, Yasod Ginige, Jiarui Xu, Shixuan Sun, Surangika Ranathunga, Suranga Seneviratne

**Updated**: 2025-03-17T11:39:44Z

**Summary**: Large Language Models (LLMs) are acquiring a wider range of capabilities, including understanding and responding in multiple languages. While they undergo safety training to prevent them from answering illegal questions, imbalances in training data and human evaluation resources can make these models more susceptible to attacks in low-resource languages (LRL). This paper proposes a framework to automatically assess the multilingual vulnerabilities of commonly used LLMs. Using our framework, we evaluated six LLMs across eight languages representing varying levels of resource availability. We validated the assessments generated by our automated framework through human evaluation in two languages, demonstrating that the framework's results align with human judgments in most cases. Our findings reveal vulnerabilities in LRL; however, these may pose minimal risk as they often stem from the model's poor performance, resulting in incoherent responses.

**Link**: [arxiv](http://arxiv.org/abs/2503.13081v1),  [pdf](http://arxiv.org/pdf/2503.13081v1)

**Tags**: cs.CL cs.AI 



### Evaluating the Instruction-following Abilities of Language Models using   Knowledge Tasks
**Authors**: Rudra Murthy, Praveen Venkateswaran, Prince Kumar, Danish Contractor

**Updated**: 2025-03-17T10:45:15Z

**Summary**: LLM evaluation benchmarks have traditionally separated the testing of knowledge/reasoning capabilities from instruction following. In this work, we study the interaction between knowledge and instruction following, and observe that LLMs struggle to follow simple answer modifying instructions, and are also distracted by instructions that should have no bearing on the original knowledge task answer. We leverage existing multiple-choice answer based knowledge benchmarks and apply a set of simple instructions which include manipulating text (eg.: change case), numeric quantities (eg.: increase value, change formatting), operate on lists (eg.: sort answer candidates) and distractor instructions (eg.: change case of numeric answers).

**Link**: [arxiv](http://arxiv.org/abs/2410.12972v2),  [pdf](http://arxiv.org/pdf/2410.12972v2)

**Tags**: cs.CL 



### In-Context Learning Enables Robot Action Prediction in LLMs
**Authors**: Yida Yin, Zekai Wang, Yuvan Sharma, Dantong Niu, Trevor Darrell, Roei Herzig

**Updated**: 2025-03-17T10:43:54Z

**Summary**: Recently, Large Language Models (LLMs) have achieved remarkable success using in-context learning (ICL) in the language domain. However, leveraging the ICL capabilities within LLMs to directly predict robot actions remains largely unexplored. In this paper, we introduce RoboPrompt, a framework that enables off-the-shelf text-only LLMs to directly predict robot actions through ICL without training. Our approach first heuristically identifies keyframes that capture important moments from an episode. Next, we extract end-effector actions from these keyframes as well as the estimated initial object poses, and both are converted into textual descriptions. Finally, we construct a structured template to form ICL demonstrations from these textual descriptions and a task instruction. This enables an LLM to directly predict robot actions at test time. Through extensive experiments and analysis, RoboPrompt shows stronger performance over zero-shot and ICL baselines in simulated and real-world settings. Our project page is available at https://davidyyd.github.io/roboprompt.

**Link**: [arxiv](http://arxiv.org/abs/2410.12782v2),  [pdf](http://arxiv.org/pdf/2410.12782v2)

**Tags**: cs.RO cs.CL 



### Overview of the NTCIR-18 Automatic Evaluation of LLMs (AEOLLM) Task
**Authors**: Junjie Chen, Haitao Li, Zhumin Chu, Yiqun Liu, Qingyao Ai

**Updated**: 2025-03-17T10:42:34Z

**Summary**: In this paper, we provide an overview of the NTCIR-18 Automatic Evaluation of LLMs (AEOLLM) task. As large language models (LLMs) grow popular in both academia and industry, how to effectively evaluate the capacity of LLMs becomes an increasingly critical but still challenging issue. Existing methods can be divided into two types: manual evaluation, which is expensive, and automatic evaluation, which faces many limitations including task format (the majority belong to multiple-choice questions) and evaluation criteria (occupied by reference-based metrics). To advance the innovation of automatic evaluation, we propose the AEOLLM task which focuses on generative tasks and encourages reference-free methods. Besides, we set up diverse subtasks such as dialogue generation, text expansion, summary generation and non-factoid question answering to comprehensively test different methods. This year, we received 48 runs from 4 teams in total. This paper will describe the background of the task, the data set, the evaluation measures and the evaluation results, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2503.13038v1),  [pdf](http://arxiv.org/pdf/2503.13038v1)

**Tags**: cs.CL 



### TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation
**Authors**: Lin Sun, Guangxiang Zhao, Xiaoqi Jian, Yuhan Wu, Weihong Lin, Yongfu Zhu, Change Jia, Linglin Zhang, Jinzhu Wu, Junfeng Ran, Sai-er Hu, Zihan Jiang, Junting Zhou, Wenrui Liu, Bin Cui, Tong Yang, Xiangzheng Zhang

**Updated**: 2025-03-17T10:36:30Z

**Summary**: The challenge of reducing the size of Large Language Models (LLMs) while maintaining their performance has gained significant attention. However, existing methods, such as model distillation and transfer learning, often fail to achieve high accuracy. To address this limitation, we introduce the Branch-Merge distillation approach, which enhances model compression through two phases: (1) the Branch Phase, where knowledge from a large teacher model is \textit{selectively distilled} into specialized student models via domain-specific supervised fine-tuning (SFT); And (2) the Merge Phase, where these student models are merged to enable cross-domain knowledge transfer and improve generalization. We validate our distillation approach using DeepSeek-R1 as the teacher and DeepSeek-R1-Distill-Qwen-32B as the student. The resulting merged model, TinyR1-32B-Preview, outperforms its counterpart DeepSeek-R1-Distill-Qwen-32B across multiple benchmarks, including Mathematics (+5.5 points), Coding (+4.4 points) and Science (+2.9 points), while achieving near-equal performance to DeepSeek-R1 on AIME 2024. The Branch-Merge distillation approach provides a scalable solution for creating smaller, high-performing LLMs with reduced computational cost and time.

**Link**: [arxiv](http://arxiv.org/abs/2503.04872v2),  [pdf](http://arxiv.org/pdf/2503.04872v2)

**Tags**: cs.CL cs.AI 



### Chain of Evidences and Evidence to Generate: Prompting for Context   Grounded and Retrieval Augmented Reasoning
**Authors**: Md Rizwan Parvez

**Updated**: 2025-03-17T10:35:11Z

**Summary**: While chain-of-thoughts (CoT) prompting has revolutionized how LLMs perform reasoning tasks, its current methods and variations (e.g, Self-consistency, ReACT, Reflexion, Tree-of-Thoughts (ToT), Cumulative Reasoning (CR) etc.,) suffer from limitations like limited context grounding, hallucination/inconsistent output generation, and iterative sluggishness. To overcome these challenges, we introduce a novel mono/dual-step zero-shot prompting framework built upon two unique strategies Chain of Evidences (CoE)} and Evidence to Generate (E2G). Instead of unverified reasoning claims, our innovative approaches leverage the power of "evidence for decision making" by first focusing exclusively on the thought sequences explicitly mentioned in the context which then serve as extracted evidence, guiding the LLM's output generation process with greater precision and efficiency. This simple yet potent approach unlocks the full potential of chain-of-thoughts prompting, facilitating faster, more reliable, and contextually aware reasoning in LLMs. Our framework consistently achieves remarkable results across various knowledge-intensive reasoning and generation tasks, surpassing baseline approaches with state-of-the-art LLMs. For instance, (i) on the LogiQA benchmark using GPT-4, CoE achieves a new state-of-the-art accuracy of 53.8%, surpassing CoT by 18%, ToT by 11%, and CR by 9%; (ii) CoE with PaLM-2 outperforms the variable-shot performance of Gemini Ultra by 0.9 F1 points, achieving an F1 score of 83.3 on DROP. We release our prompts and outputs on these benchmarks as a new instruction tuning dataset for future research at https://huggingface.co/datasets/kagnlp/Chain-of-Evidences/.

**Link**: [arxiv](http://arxiv.org/abs/2401.05787v2),  [pdf](http://arxiv.org/pdf/2401.05787v2)

**Tags**: cs.CL 



### HiMTok: Learning Hierarchical Mask Tokens for Image Segmentation with   Large Multimodal Model
**Authors**: Tao Wang, Changxu Cheng, Lingfeng Wang, Senda Chen, Wuyue Zhao

**Updated**: 2025-03-17T10:29:08Z

**Summary**: The remarkable performance of large multimodal models (LMMs) has attracted significant interest from the image segmentation community. To align with the next-token-prediction paradigm, current LMM-driven segmentation methods either use object boundary points to represent masks or introduce special segmentation tokens, whose hidden states are decoded by a segmentation model requiring the original image as input. However, these approaches often suffer from inadequate mask representation and complex architectures, limiting the potential of LMMs. In this work, we propose the Hierarchical Mask Tokenizer (HiMTok), which represents segmentation masks with up to 32 tokens and eliminates the need for the original image during mask de-tokenization. HiMTok allows for compact and coarse-to-fine mask representations, aligning well with the LLM next-token-prediction paradigm and facilitating the direct acquisition of segmentation capabilities. We develop a 3-stage training recipe for progressive learning of segmentation and visual capabilities, featuring a hierarchical mask loss for effective coarse-to-fine learning. Additionally, we enable bidirectional information flow, allowing conversion between bounding boxes and mask tokens to fully leverage multi-task training potential. Extensive experiments demonstrate that our method achieves state-of-the-art performance across various segmentation tasks,while also enhancing visual grounding and maintaining overall visual understanding.

**Link**: [arxiv](http://arxiv.org/abs/2503.13026v1),  [pdf](http://arxiv.org/pdf/2503.13026v1)

**Tags**: cs.CV 



### Test-Time Domain Generalization via Universe Learning: A Multi-Graph   Matching Approach for Medical Image Segmentation
**Authors**: Xingguo Lv, Xingbo Dong, Liwen Wang, Jiewen Yang, Lei Zhao, Bin Pu, Zhe Jin, Xuejun Li

**Updated**: 2025-03-17T10:11:11Z

**Summary**: Despite domain generalization (DG) has significantly addressed the performance degradation of pre-trained models caused by domain shifts, it often falls short in real-world deployment. Test-time adaptation (TTA), which adjusts a learned model using unlabeled test data, presents a promising solution. However, most existing TTA methods struggle to deliver strong performance in medical image segmentation, primarily because they overlook the crucial prior knowledge inherent to medical images. To address this challenge, we incorporate morphological information and propose a framework based on multi-graph matching. Specifically, we introduce learnable universe embeddings that integrate morphological priors during multi-source training, along with novel unsupervised test-time paradigms for domain adaptation. This approach guarantees cycle-consistency in multi-matching while enabling the model to more effectively capture the invariant priors of unseen data, significantly mitigating the effects of domain shifts. Extensive experiments demonstrate that our method outperforms other state-of-the-art approaches on two medical image segmentation benchmarks for both multi-source and single-source domain generalization tasks. The source code is available at https://github.com/Yore0/TTDG-MGM.

**Link**: [arxiv](http://arxiv.org/abs/2503.13012v1),  [pdf](http://arxiv.org/pdf/2503.13012v1)

**Tags**: cs.CV cs.AI 



### TuBA: Cross-Lingual Transferability of Backdoor Attacks in LLMs with   Instruction Tuning
**Authors**: Xuanli He, Jun Wang, Qiongkai Xu, Pasquale Minervini, Pontus Stenetorp, Benjamin I. P. Rubinstein, Trevor Cohn

**Updated**: 2025-03-17T10:09:29Z

**Summary**: The implications of backdoor attacks on English-centric large language models (LLMs) have been widely examined - such attacks can be achieved by embedding malicious behaviors during training and activated under specific conditions that trigger malicious outputs. Despite the increasing support for multilingual capabilities in open-source and proprietary LLMs, the impact of backdoor attacks on these systems remains largely under-explored. Our research focuses on cross-lingual backdoor attacks against multilingual LLMs, particularly investigating how poisoning the instruction-tuning data for one or two languages can affect the outputs for languages whose instruction-tuning data were not poisoned. Despite its simplicity, our empirical analysis reveals that our method exhibits remarkable efficacy in models like mT5 and GPT-4o, with high attack success rates, surpassing 90% in more than 7 out of 12 languages across various scenarios. Our findings also indicate that more powerful models show increased susceptibility to transferable cross-lingual backdoor attacks, which also applies to LLMs predominantly pre-trained on English data, such as Llama2, Llama3, and Gemma. Moreover, our experiments demonstrate 1) High Transferability: the backdoor mechanism operates successfully in cross-lingual response scenarios across 26 languages, achieving an average attack success rate of 99%, and 2) Robustness: the proposed attack remains effective even after defenses are applied. These findings expose critical security vulnerabilities in multilingual LLMs and highlight the urgent need for more robust, targeted defense strategies to address the unique challenges posed by cross-lingual backdoor transfer.

**Link**: [arxiv](http://arxiv.org/abs/2404.19597v3),  [pdf](http://arxiv.org/pdf/2404.19597v3)

**Tags**: cs.CL cs.CR 



### Knowledge Distillation: Enhancing Neural Network Compression with   Integrated Gradients
**Authors**: David E. Hernandez, Jose Ramon Chang, Torbjörn E. M. Nordling

**Updated**: 2025-03-17T10:07:50Z

**Summary**: Efficient deployment of deep neural networks on resource-constrained devices demands advanced compression techniques that preserve accuracy and interoperability. This paper proposes a machine learning framework that augments Knowledge Distillation (KD) with Integrated Gradients (IG), an attribution method, to optimise the compression of convolutional neural networks. We introduce a novel data augmentation strategy where IG maps, precomputed from a teacher model, are overlaid onto training images to guide a compact student model toward critical feature representations. This approach leverages the teacher's decision-making insights, enhancing the student's ability to replicate complex patterns with reduced parameters. Experiments on CIFAR-10 demonstrate the efficacy of our method: a student model, compressed 4.1-fold from the MobileNet-V2 teacher, achieves 92.5% classification accuracy, surpassing the baseline student's 91.4% and traditional KD approaches, while reducing inference latency from 140 ms to 13 ms--a tenfold speedup. We perform hyperparameter optimisation for efficient learning. Comprehensive ablation studies dissect the contributions of KD and IG, revealing synergistic effects that boost both performance and model explainability. Our method's emphasis on feature-level guidance via IG distinguishes it from conventional KD, offering a data-driven solution for mining transferable knowledge in neural architectures. This work contributes to machine learning by providing a scalable, interpretable compression technique, ideal for edge computing applications where efficiency and transparency are paramount.

**Link**: [arxiv](http://arxiv.org/abs/2503.13008v1),  [pdf](http://arxiv.org/pdf/2503.13008v1)

**Tags**: cs.LG cs.CV 68T05, 68T07 I.2.6; I.4.2; I.4.9 



### Exploring ReAct Prompting for Task-Oriented Dialogue: Insights and   Shortcomings
**Authors**: Michelle Elizabeth, Morgan Veyret, Miguel Couceiro, Ondrej Dusek, Lina M. Rojas-Barahona

**Updated**: 2025-03-17T10:01:21Z

**Summary**: Large language models (LLMs) gained immense popularity due to their impressive capabilities in unstructured conversations. Empowering LLMs with advanced prompting strategies such as reasoning and acting (ReAct) (Yao et al., 2022) has shown promise in solving complex tasks traditionally requiring reinforcement learning. In this work, we apply the ReAct strategy to guide LLMs performing task-oriented dialogue (TOD). We evaluate ReAct-based LLMs (ReAct-LLMs) both in simulation and with real users. While ReAct-LLMs severely underperform state-of-the-art approaches on success rate in simulation, this difference becomes less pronounced in human evaluation. Moreover, compared to the baseline, humans report higher subjective satisfaction with ReAct-LLM despite its lower success rate, most likely thanks to its natural and confidently phrased responses.

**Link**: [arxiv](http://arxiv.org/abs/2412.01262v2),  [pdf](http://arxiv.org/pdf/2412.01262v2)

**Tags**: cs.CL cs.AI cs.HC 



### A Multi-Stage Framework with Taxonomy-Guided Reasoning for Occupation   Classification Using Large Language Models
**Authors**: Palakorn Achananuparp, Ee-Peng Lim

**Updated**: 2025-03-17T09:44:50Z

**Summary**: Automatically annotating job data with standardized occupations from taxonomies, known as occupation classification, is crucial for labor market analysis. However, this task is often hindered by data scarcity and the challenges of manual annotations. While large language models (LLMs) hold promise due to their extensive world knowledge and in-context learning capabilities, their effectiveness depends on their knowledge of occupational taxonomies, which remains unclear. In this study, we assess the ability of LLMs to generate precise taxonomic entities from taxonomy, highlighting their limitations. To address these challenges, we propose a multi-stage framework consisting of inference, retrieval, and reranking stages, which integrates taxonomy-guided reasoning examples to enhance performance by aligning outputs with taxonomic knowledge. Evaluations on a large-scale dataset show significant improvements in classification accuracy. Furthermore, we demonstrate the framework's adaptability for multi-label skill classification. Our results indicate that the framework outperforms existing LLM-based methods, offering a practical and scalable solution for occupation classification and related tasks across LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2503.12989v1),  [pdf](http://arxiv.org/pdf/2503.12989v1)

**Tags**: cs.CL cs.AI cs.SI 



### ROMA: a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM
**Authors**: Wenqiang Wang, Yijia Zhang, Zikai Zhang, Guanting Huo, Hao Liang, Shijie Cao, Ningyi Xu

**Updated**: 2025-03-17T09:44:17Z

**Summary**: As large language models (LLMs) demonstrate powerful capabilities, deploying them on edge devices has become increasingly crucial, offering advantages in privacy and real-time interaction. QLoRA has emerged as the standard approach for on-device LLMs, leveraging quantized models to reduce memory and computational costs while utilizing LoRA for task-specific adaptability. In this work, we propose ROMA, a QLoRA accelerator with a hybrid storage architecture that uses ROM for quantized base models and SRAM for LoRA weights and KV cache. Our insight is that the quantized base model is stable and converged, making it well-suited for ROM storage. Meanwhile, LoRA modules offer the flexibility to adapt to new data without requiring updates to the base model. To further reduce the area cost of ROM, we introduce a novel B-ROM design and integrate it with the compute unit to form a fused cell for efficient use of chip resources. ROMA can effectively store both a 4-bit 3B and a 2-bit 8B LLaMA model entirely on-chip, achieving a notable generation speed exceeding 20,000 tokens/s without requiring external memory.

**Link**: [arxiv](http://arxiv.org/abs/2503.12988v1),  [pdf](http://arxiv.org/pdf/2503.12988v1)

**Tags**: cs.AR cs.AI 



### Aligning Vision to Language: Text-Free Multimodal Knowledge Graph   Construction for Enhanced LLMs Reasoning
**Authors**: Junming Liu, Siyuan Meng, Yanting Gao, Song Mao, Pinlong Cai, Guohang Yan, Yirong Chen, Zilin Bian, Botian Shi, Ding Wang

**Updated**: 2025-03-17T09:31:14Z

**Summary**: Multimodal reasoning in Large Language Models (LLMs) struggles with incomplete knowledge and hallucination artifacts, challenges that textual Knowledge Graphs (KGs) only partially mitigate due to their modality isolation. While Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal understanding, their practical construction is impeded by semantic narrowness of manual text annotations and inherent noise in visual-semantic entity linkages. In this paper, we propose Vision-align-to-Language integrated Knowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances LLMs reasoning through cross-modal information supplementation. Specifically, we cascade pre-trained Vision-Language Models (VLMs) to align image features with text, transforming them into descriptions that encapsulate image-specific information. Furthermore, we developed a cross-modal similarity verification mechanism to quantify semantic consistency, effectively filtering out noise introduced during feature alignment. Even without manually annotated image captions, the refined descriptions alone suffice to construct the MMKG. Compared to conventional MMKGs construction paradigms, our approach achieves substantial storage efficiency gains while maintaining direct entity-to-image linkage capability. Experimental results on multimodal reasoning tasks demonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art models. Our code is published at https://github.com/Wings-Of-Disaster/VaLiK.

**Link**: [arxiv](http://arxiv.org/abs/2503.12972v1),  [pdf](http://arxiv.org/pdf/2503.12972v1)

**Tags**: cs.CV cs.AI 



### SensorLLM: Aligning Large Language Models with Motion Sensors for Human   Activity Recognition
**Authors**: Zechen Li, Shohreh Deldari, Linyao Chen, Hao Xue, Flora D. Salim

**Updated**: 2025-03-17T09:28:43Z

**Summary**: We introduce SensorLLM, a two-stage framework that enables Large Language Models (LLMs) to perform human activity recognition (HAR) from sensor data. Despite their strong reasoning and generalization capabilities, LLMs remain underutilized for motion sensor data due to the lack of semantic context in time-series, computational constraints, and challenges in processing numerical inputs. SensorLLM addresses these limitations through a Sensor-Language Alignment stage, where we introduce special tokens for each sensor channel and automatically generate textual trend descriptions. This alignment enables LLMs to capture numerical variations, channel-specific features, and data of varying duration--without requiring human annotations. In the subsequent Task-Aware Tuning stage, we refine the model for HAR classification, achieving performance that matches or surpasses state-of-the-art methods. Our results demonstrate that SensorLLM evolves into an effective sensor learner, reasoner, and classifier through Sensor-Language Alignment, generalizing across diverse HAR datasets. We believe this work establishes a foundation for future research on time-series and text alignment, paving the way for foundation models in sensor data analysis.

**Link**: [arxiv](http://arxiv.org/abs/2410.10624v2),  [pdf](http://arxiv.org/pdf/2410.10624v2)

**Tags**: cs.CL 



### Initial acquisition requirements for optical cavities in the space   gravitational wave antennae DECIGO and B-DECIGO
**Authors**: Yuta Michimura, Koji Nagano, Kentaro Komori, Kiwamu Izumi, Takahiro Ito, Satoshi Ikari, Tomotada Akutsu, Masaki Ando, Isao Kawano, Mitsuru Musha, Shuichi Sato

**Updated**: 2025-03-17T09:15:20Z

**Summary**: DECIGO (DECi-hertz Interferometer Gravitational Wave Observatory) is a space-based gravitational wave antenna concept targeting the 0.1-10 Hz band. It consists of three spacecraft arranged in an equilateral triangle with 1,000 km sides, forming Fabry-P\'erot cavities between them. A precursor mission, B-DECIGO, is also planned, featuring a smaller 100 km triangle. Operating these cavities requires ultra-precise formation flying, where inter-mirror distance and alignment must be precisely controlled. Achieving this necessitates a sequential improvement in precision using various sensors and actuators, from the deployment of the spacecraft to laser link acquisition and ultimately to the control of the Fabry-P\'erot cavities to maintain resonance. In this paper, we derive the precision requirements at each stage and discuss the feasibility of achieving them. We show that the relative speed between cavity mirrors must be controlled at the sub-micrometer-per-second level and that relative alignment must be maintained at the sub-microradian level to obtain control signals from the Fabry-P\'erot cavities of DECIGO and B-DECIGO.

**Link**: [arxiv](http://arxiv.org/abs/2503.12960v1),  [pdf](http://arxiv.org/pdf/2503.12960v1)

**Tags**: gr-qc astro-ph.IM physics.ins-det physics.optics 



### Performance Analysis and Industry Deployment of Post-Quantum   Cryptography Algorithms
**Authors**: Elif Dicle Demir, Buse Bilgin, Mehmet Cengiz Onbasli

**Updated**: 2025-03-17T09:06:03Z

**Summary**: As quantum computing advances, modern cryptographic standards face an existential threat, necessitating a transition to post-quantum cryptography (PQC). The National Institute of Standards and Technology (NIST) has selected CRYSTALS-Kyber and CRYSTALS-Dilithium as standardized PQC algorithms for secure key exchange and digital signatures, respectively. This study conducts a comprehensive performance analysis of these algorithms by benchmarking execution times across cryptographic operations such as key generation, encapsulation, decapsulation, signing, and verification. Additionally, the impact of AVX2 optimizations is evaluated to assess hardware acceleration benefits. Our findings demonstrate that Kyber and Dilithium achieve efficient execution times, outperforming classical cryptographic schemes such as RSA and ECDSA at equivalent security levels. Beyond technical performance, the real-world deployment of PQC introduces challenges in telecommunications networks, where large-scale infrastructure upgrades, interoperability with legacy systems, and regulatory constraints must be addressed. This paper examines the feasibility of PQC adoption in telecom environments, highlighting key transition challenges, security risks, and implementation strategies. Through industry case studies, we illustrate how telecom operators are integrating PQC into 5G authentication, subscriber identity protection, and secure communications. Our analysis provides insights into the computational trade-offs, deployment considerations, and standardization efforts shaping the future of quantum-safe cryptographic infrastructure.

**Link**: [arxiv](http://arxiv.org/abs/2503.12952v1),  [pdf](http://arxiv.org/pdf/2503.12952v1)

**Tags**: cs.CR 68 E.3 



### Interleaved-Modal Chain-of-Thought
**Authors**: Jun Gao, Yongqi Li, Ziqiang Cao, Wenjie Li

**Updated**: 2025-03-17T09:01:38Z

**Summary**: Chain-of-Thought (CoT) prompting elicits large language models (LLMs) to produce a series of intermediate reasoning steps before arriving at the final answer. However, when transitioning to vision-language models (VLMs), their text-only rationales struggle to express the fine-grained associations with the original image. In this paper, we propose an image-incorporated multimodal Chain-of-Thought, named \textbf{Interleaved-modal Chain-of-Thought (ICoT)}, which generates sequential reasoning steps consisting of paired visual and textual rationales to infer the final answer. Intuitively, the novel ICoT requires VLMs to enable the generation of fine-grained interleaved-modal content, which is hard for current VLMs to fulfill. Considering that the required visual information is usually part of the input image, we propose \textbf{Attention-driven Selection (ADS)} to realize ICoT over existing VLMs. ADS intelligently inserts regions of the input image to generate the interleaved-modal reasoning steps with ignorable additional latency. ADS relies solely on the attention map of VLMs without the need for parameterization, and therefore it is a plug-and-play strategy that can be generalized to a spectrum of VLMs. We apply ADS to realize ICoT on two popular VLMs of different architectures. Extensive evaluations of three benchmarks have shown that ICoT prompting achieves substantial performance (up to 14\%) and interpretability improvements compared to existing multimodal CoT prompting methods.

**Link**: [arxiv](http://arxiv.org/abs/2411.19488v2),  [pdf](http://arxiv.org/pdf/2411.19488v2)

**Tags**: cs.CV cs.AI cs.LG 



### MirrorGuard: Adaptive Defense Against Jailbreaks via Entropy-Guided   Mirror Crafting
**Authors**: Rui Pu, Chaozhuo Li, Rui Ha, Litian Zhang, Lirong Qiu, Xi Zhang

**Updated**: 2025-03-17T08:41:29Z

**Summary**: Defending large language models (LLMs) against jailbreak attacks is crucial for ensuring their safe deployment. Existing defense strategies generally rely on predefined static criteria to differentiate between harmful and benign prompts. However, such rigid rules are incapable of accommodating the inherent complexity and dynamic nature of real jailbreak attacks. In this paper, we propose a novel concept of ``mirror'' to enable dynamic and adaptive defense. A mirror refers to a dynamically generated prompt that mirrors the syntactic structure of the input while ensuring semantic safety. The personalized discrepancies between the input prompts and their corresponding mirrors serve as the guiding principles for defense. A new defense paradigm, MirrorGuard, is further proposed to detect and calibrate risky inputs based on such mirrors. An entropy-based detection metric, Relative Input Uncertainty (RIU), is integrated into MirrorGuard to quantify the discrepancies between input prompts and mirrors. MirrorGuard is evaluated on several popular datasets, demonstrating state-of-the-art defense performance while maintaining general effectiveness.

**Link**: [arxiv](http://arxiv.org/abs/2503.12931v1),  [pdf](http://arxiv.org/pdf/2503.12931v1)

**Tags**: cs.CR cs.AI 



### EMN: Brain-inspired Elastic Memory Network for Quick Domain Adaptive   Feature Mapping
**Authors**: Jianming Lv, Chengjun Wang, Depin Liang, Qianli Ma, Wei Chen, Xueqi Cheng

**Updated**: 2025-03-17T08:34:07Z

**Summary**: Utilizing unlabeled data in the target domain to perform continuous optimization is critical to enhance the generalization ability of neural networks. Most domain adaptation methods focus on time-consuming optimization of deep feature extractors, which limits the deployment on lightweight edge devices. Inspired by the memory mechanism and powerful generalization ability of biological neural networks in human brains, we propose a novel gradient-free Elastic Memory Network, namely EMN, to support quick fine-tuning of the mapping between features and prediction without heavy optimization of deep features. In particular, EMN adopts randomly connected neurons to memorize the association of features and labels, where the signals in the network are propagated as impulses, and the prediction is made by associating the memories stored on neurons based on their confidence. More importantly, EMN supports reinforced memorization of feature mapping based on unlabeled data to quickly adapt to a new domain. Experiments based on four cross-domain real-world datasets show that EMN can achieve up to 10% enhancement of performance while only needing less than 1% timing cost of traditional domain adaptation methods.

**Link**: [arxiv](http://arxiv.org/abs/2402.14598v2),  [pdf](http://arxiv.org/pdf/2402.14598v2)

**Tags**: cs.NE cs.LG 



### Sparrow: Data-Efficient Video-LLM with Text-to-Image Augmentation
**Authors**: Shukang Yin, Chaoyou Fu, Sirui Zhao, Yunhang Shen, Chunjiang Ge, Yan Yang, Zuwei Long, Yuhan Dai, Yongdong Luo, Haoyu Cao, Tong Xu, Xing Sun, Caifeng Shan, Ran He, Enhong Chen

**Updated**: 2025-03-17T08:33:00Z

**Summary**: Recent years have witnessed the success of Multimodal Large Language Models (MLLMs) in the vision understanding domain. The success of these models can largely be attributed to the dominant scaling law, which states that larger parameter sizes and data volumes contribute to better performance. Notably, data scaling has mainly been powered by automatic data pipelines, which center around the self-instruction of LLMs. The paradigm has been taken for granted for quite some time, but the study of the effectiveness of scaling with these data has been neglected for a long time. In this context, this work revisits scaling with synthetic data and focuses on developing video-LLMs from a data-centric perspective. Our main study approach is fine-tuning pre-trained image-LLMs with video data and investigating learning efficiency through data scaling. Results from our preliminary experiments reveal a low learning efficiency phenomenon when simply scaling up video data samples, which, through our probing, can be ascribed to a lack of instruction diversity. Aiming at this issue, we propose a data augmentation method called Sparrow, which synthesizes video-like samples from pure text instruction data. Mixing these synthetic samples with the video data enables a more efficient training scheme. Through comprehensive experiments, we demonstrate that our proposed method achieves performance comparable to or even superior to baselines trained with many more samples. Meanwhile, we find that incorporating these synthetic samples can boost the performance of long video understanding without training with long video data. The code and data examples are available at https://github.com/VITA-MLLM/Sparrow.

**Link**: [arxiv](http://arxiv.org/abs/2411.19951v4),  [pdf](http://arxiv.org/pdf/2411.19951v4)

**Tags**: cs.CV cs.CL cs.LG 



### ThinkPatterns-21k: A Systematic Study on the Impact of Thinking Patterns   in LLMs
**Authors**: Pengcheng Wen, Jiaming Ji, Chi-Min Chan, Juntao Dai, Donghai Hong, Yaodong Yang, Sirui Han, Yike Guo

**Updated**: 2025-03-17T08:29:04Z

**Summary**: Large language models (LLMs) have demonstrated enhanced performance through the \textit{Thinking then Responding} paradigm, where models generate internal thoughts before final responses (aka, System 2 thinking). However, existing research lacks a systematic understanding of the mechanisms underlying how thinking patterns affect performance across model sizes. In this work, we conduct a comprehensive analysis of the impact of various thinking types on model performance and introduce ThinkPatterns-21k, a curated dataset comprising 21k instruction-response pairs (QA) collected from existing instruction-following datasets with five thinking types. For each pair, we augment it with five distinct internal thinking patterns: one unstructured thinking (monologue) and four structured variants (decomposition, self-ask, self-debate and self-critic), while maintaining the same instruction and response. Through extensive evaluation across different model sizes (3B-32B parameters), we have two key findings: (1) smaller models (<30B parameters) can benefit from most of structured thinking patterns, while larger models (32B) with structured thinking like decomposition would degrade performance and (2) unstructured monologue demonstrates broad effectiveness across different model sizes. Finally, we released all of our datasets, checkpoints, training logs of diverse thinking patterns to reproducibility, aiming to facilitate further research in this direction.

**Link**: [arxiv](http://arxiv.org/abs/2503.12918v1),  [pdf](http://arxiv.org/pdf/2503.12918v1)

**Tags**: cs.CL 



### HICD: Hallucination-Inducing via Attention Dispersion for Contrastive   Decoding to Mitigate Hallucinations in Large Language Models
**Authors**: Xinyan Jiang, Hang Ye, Yongxin Zhu, Xiaoying Zheng, Zikang Chen, Jun Gong

**Updated**: 2025-03-17T08:17:28Z

**Summary**: Large Language Models (LLMs) often generate hallucinations, producing outputs that are contextually inaccurate or factually incorrect. We introduce HICD, a novel method designed to induce hallucinations for contrastive decoding to mitigate hallucinations. Unlike existing contrastive decoding methods, HICD selects attention heads crucial to the model's prediction as inducing heads, then induces hallucinations by dispersing attention of these inducing heads and compares the hallucinated outputs with the original outputs to obtain the final result. Our approach significantly improves performance on tasks requiring contextual faithfulness, such as context completion, reading comprehension, and question answering. It also improves factuality in tasks requiring accurate knowledge recall. We demonstrate that our inducing heads selection and attention dispersion method leads to more "contrast-effective" hallucinations for contrastive decoding, outperforming other hallucination-inducing methods. Our findings provide a promising strategy for reducing hallucinations by inducing hallucinations in a controlled manner, enhancing the performance of LLMs in a wide range of tasks.

**Link**: [arxiv](http://arxiv.org/abs/2503.12908v1),  [pdf](http://arxiv.org/pdf/2503.12908v1)

**Tags**: cs.CL cs.AI 



### AVHBench: A Cross-Modal Hallucination Benchmark for Audio-Visual Large   Language Models
**Authors**: Kim Sung-Bin, Oh Hyun-Bin, JungMok Lee, Arda Senocak, Joon Son Chung, Tae-Hyun Oh

**Updated**: 2025-03-17T08:14:35Z

**Summary**: Following the success of Large Language Models (LLMs), expanding their boundaries to new modalities represents a significant paradigm shift in multimodal understanding. Human perception is inherently multimodal, relying not only on text but also on auditory and visual cues for a complete understanding of the world. In recognition of this fact, audio-visual LLMs have recently emerged. Despite promising developments, the lack of dedicated benchmarks poses challenges for understanding and evaluating models. In this work, we show that audio-visual LLMs struggle to discern subtle relationships between audio and visual signals, leading to hallucinations and highlighting the need for reliable benchmarks. To address this, we introduce AVHBench, the first comprehensive benchmark specifically designed to evaluate the perception and comprehension capabilities of audio-visual LLMs. Our benchmark includes tests for assessing hallucinations, as well as the cross-modal matching and reasoning abilities of these models. Our results reveal that most existing audio-visual LLMs struggle with hallucinations caused by cross-interactions between modalities, due to their limited capacity to perceive complex multimodal signals and their relationships. Additionally, we demonstrate that simple training with our AVHBench improves robustness of audio-visual LLMs against hallucinations. Dataset: https://github.com/kaist-ami/AVHBench

**Link**: [arxiv](http://arxiv.org/abs/2410.18325v2),  [pdf](http://arxiv.org/pdf/2410.18325v2)

**Tags**: cs.CV 



### A Semantic-based Optimization Approach for Repairing LLMs: Case Study on   Code Generation
**Authors**: Jian Gu, Aldeida Aleti, Chunyang Chen, Hongyu Zhang

**Updated**: 2025-03-17T07:59:42Z

**Summary**: Language Models (LMs) are widely used in software engineering for code generation, but they may produce code with errors. Rather than repairing the generated code, an alternative way is to address the underlying failures of models. LM repair offers a lightweight solution to this challenge: it requires minimal data, reduces computational costs, and reduces the side effects. Unlike retraining, LM repair focuses on applying tailored updates to targeted neurons, making it ideal for scenarios with limited resources, high-performance demands, or strict safety requirements. In this paper, we propose \ul{S}emantic \ul{T}argeting for \ul{A}nalytical \ul{R}epair (\textsc{STAR}), a pioneering and novel semantic-based optimization approach for repairing LLMs. \textsc{STAR} realizes main operations in LM repair methods in an optimization process, including locating ``buggy neurons'', solving ``neuron patches'', and patching ``buggy neurons''. Correspondingly, it computes the deltas of weight matrix as the prior information to guide optimization; and attributes the targeted layers and neurons leveraging statistical insights. The neuron patches are computed with a solid semantic-based analytical formula, which directly bridges the changes to logits with the deltas of neurons, by steering latent representations. Compared to the prior work of LM repair (\textsc{MINT}) and optimization methods (\textsc{SGD}), \textsc{STAR} integrates their strengths while mitigating their limitations. \textsc{STAR} supports solving multiple failures together, significantly improving the usefulness. Evaluated on three code generation tasks using popular code LMs, \textsc{STAR} demonstrates superior effectiveness. Additionally, \textsc{STAR} exhibits better efficiency. In terms of side effects, namely the balance between generalization and specificity, \textsc{STAR} outperforms prior work by a significant margin.

**Link**: [arxiv](http://arxiv.org/abs/2503.12899v1),  [pdf](http://arxiv.org/pdf/2503.12899v1)

**Tags**: cs.SE cs.CL cs.LG 



### Safeguarding LLM Embeddings in End-Cloud Collaboration via   Entropy-Driven Perturbation
**Authors**: Shuaifan Jin, Xiaoyi Pang, Zhibo Wang, He Wang, Jiacheng Du, Jiahui Hu, Kui Ren

**Updated**: 2025-03-17T07:58:05Z

**Summary**: Recent studies improve on-device language model (LM) inference through end-cloud collaboration, where the end device retrieves useful information from cloud databases to enhance local processing, known as Retrieval-Augmented Generation (RAG). Typically, to retrieve information from the cloud while safeguarding privacy, the end device transforms original data into embeddings with a local embedding model. However, the recently emerging Embedding Inversion Attacks (EIAs) can still recover the original data from text embeddings (e.g., training a recovery model to map embeddings back to original texts), posing a significant threat to user privacy. To address this risk, we propose EntroGuard, an entropy-driven perturbation-based embedding privacy protection method, which can protect the privacy of text embeddings while maintaining retrieval accuracy during the end-cloud collaboration. Specifically, to defeat various EIAs, we perturb the embeddings to increase the entropy of the recovered text in the common structure of recovery models, thus steering the embeddings toward meaningless texts rather than original sensitive texts during the recovery process. To maintain retrieval performance in the cloud, we constrain the perturbations within a bound, applying the strategy of reducing them where redundant and increasing them where sparse. Moreover, EntroGuard can be directly integrated into end devices without requiring any modifications to the embedding model. Extensive experimental results demonstrate that EntroGuard can reduce the risk of privacy leakage by up to 8 times at most with negligible loss of retrieval performance compared to existing privacy-preserving methods.

**Link**: [arxiv](http://arxiv.org/abs/2503.12896v1),  [pdf](http://arxiv.org/pdf/2503.12896v1)

**Tags**: cs.CR 



### Unlearning or Obfuscating? Jogging the Memory of Unlearned LLMs via   Benign Relearning
**Authors**: Shengyuan Hu, Yiwei Fu, Zhiwei Steven Wu, Virginia Smith

**Updated**: 2025-03-17T07:46:49Z

**Summary**: Machine unlearning is a promising approach to mitigate undesirable memorization of training data in ML models. However, in this work we show that existing approaches for unlearning in LLMs are surprisingly susceptible to a simple set of $\textit{benign relearning attacks}$. With access to only a small and potentially loosely related set of data, we find that we can ''jog'' the memory of unlearned models to reverse the effects of unlearning. For example, we show that relearning on public medical articles can lead an unlearned LLM to output harmful knowledge about bioweapons, and relearning general wiki information about the book series Harry Potter can force the model to output verbatim memorized text. We formalize this unlearning-relearning pipeline, explore the attack across three popular unlearning benchmarks, and discuss future directions and guidelines that result from our study. Our work indicates that current approximate unlearning methods simply suppress the model outputs and fail to robustly forget target knowledge in the LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2406.13356v4),  [pdf](http://arxiv.org/pdf/2406.13356v4)

**Tags**: cs.LG 



### Neonpool: Reimagining cryptocurrency transaction pools for lightweight   clients and IoT devices
**Authors**: Hina Binte Haq, Syed Taha Ali, Asad Salman, Patrick McCorry, Siamak F. Shahandashti

**Updated**: 2025-03-17T07:37:50Z

**Summary**: The transaction pool plays a critical role in processing and disseminating transactions in cryptocurrency networks. However, increasing transaction loads strain the resources of full node deployments. We present Neonpool, an innovative transaction pool optimization using bloom filter variants, which reduces the memory footprint of the transaction pool to a fraction. Implemented in C++ and benchmarked using a unique Bitcoin and Ethereum dataset, our solution verifies and forwards transactions with over 99.99\% accuracy and does not necessitate a hard fork. Neonpool is ideally suited for lightweight cryptocurrency clients and for resource-constrained devices such as browsers, systems-on-a-chip, mobile or IoT devices.

**Link**: [arxiv](http://arxiv.org/abs/2412.16217v2),  [pdf](http://arxiv.org/pdf/2412.16217v2)

**Tags**: cs.CR 



### LR$^2$Bench: Evaluating Long-chain Reflective Reasoning Capabilities of   Large Language Models via Constraint Satisfaction Problems
**Authors**: Jianghao Chen, Zhenlin Wei, Zhenjiang Ren, Ziyong Li, Jiajun Zhang

**Updated**: 2025-03-17T07:36:01Z

**Summary**: Recent progress in o1-like models has significantly enhanced the reasoning abilities of Large Language Models (LLMs), empowering them to tackle increasingly complex tasks through reflection capabilities, such as making assumptions, backtracking, and self-refinement. However, effectively evaluating such reflection capabilities remains challenging due to the lack of appropriate benchmarks. To bridge this gap, we introduce LR$^2$Bench, a novel benchmark designed to evaluate the Long-chain Reflective Reasoning capabilities of LLMs. LR$^2$Bench comprises 850 samples across six Constraint Satisfaction Problems (CSPs) where reflective reasoning is crucial for deriving solutions that meet all given constraints. Each type of task focuses on distinct constraint patterns, such as knowledge-based, logical, and spatial constraints, providing a comprehensive evaluation of diverse problem-solving scenarios. We conduct extensive evaluation on both conventional models and o1-like models. Our experimental results reveal that even the most advanced reasoning-specific models, such as DeepSeek-R1 and OpenAI o1-preview, struggle with tasks in LR$^2$Bench, achieving an average Exact Match score of only 20.0% and 23.6%, respectively. These findings underscore the significant room for improvement in the reflective reasoning capabilities of current LLMs. The leaderboard of our benchmark is available at https://huggingface.co/spaces/UltraRonin/LR2Bench

**Link**: [arxiv](http://arxiv.org/abs/2502.17848v2),  [pdf](http://arxiv.org/pdf/2502.17848v2)

**Tags**: cs.CL 



### Precise Localization of Memories: A Fine-grained Neuron-level Knowledge   Editing Technique for LLMs
**Authors**: Haowen Pan, Xiaozhi Wang, Yixin Cao, Zenglin Shi, Xun Yang, Juanzi Li, Meng Wang

**Updated**: 2025-03-17T07:34:41Z

**Summary**: Knowledge editing aims to update outdated information in Large Language Models (LLMs). A representative line of study is locate-then-edit methods, which typically employ causal tracing to identify the modules responsible for recalling factual knowledge about entities. However, we find these methods are often sensitive only to changes in the subject entity, leaving them less effective at adapting to changes in relations. This limitation results in poor editing locality, which can lead to the persistence of irrelevant or inaccurate facts, ultimately compromising the reliability of LLMs. We believe this issue arises from the insufficient precision of knowledge localization. To address this, we propose a Fine-grained Neuron-level Knowledge Editing (FiNE) method that enhances editing locality without affecting overall success rates. By precisely identifying and modifying specific neurons within feed-forward networks, FiNE significantly improves knowledge localization and editing. Quantitative experiments demonstrate that FiNE efficiently achieves better overall performance compared to existing techniques, providing new insights into the localization and modification of knowledge within LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2503.01090v2),  [pdf](http://arxiv.org/pdf/2503.01090v2)

**Tags**: cs.CL 



### Optimizing Ansatz Design in Quantum Generative Adversarial Networks   Using Large Language Models
**Authors**: Kento Ueda, Atsushi Matsuo

**Updated**: 2025-03-17T07:29:05Z

**Summary**: We present a novel approach for improving the design of ansatzes in Quantum Generative Adversarial Networks (qGANs) by leveraging Large Language Models (LLMs). By combining the strengths of LLMs with qGANs, our approach iteratively refines ansatz structures to improve accuracy while reducing circuit depth and the number of parameters. This study paves the way for further exploration in AI-driven quantum algorithm design. The flexibility of our proposed workflow extends to other quantum variational algorithms, providing a general framework for optimizing quantum circuits in a variety of quantum computing tasks.

**Link**: [arxiv](http://arxiv.org/abs/2503.12884v1),  [pdf](http://arxiv.org/pdf/2503.12884v1)

**Tags**: quant-ph 



### nvBench 2.0: A Benchmark for Natural Language to Visualization under   Ambiguity
**Authors**: Tianqi Luo, Chuhan Huang, Leixian Shen, Boyan Li, Shuyu Shen, Wei Zeng, Nan Tang, Yuyu Luo

**Updated**: 2025-03-17T07:20:11Z

**Summary**: Natural Language to Visualization (NL2VIS) enables users to create visualizations from natural language queries, making data insights more accessible. However, NL2VIS faces challenges in interpreting ambiguous queries, as users often express their visualization needs in imprecise language. To address this challenge, we introduce nvBench 2.0, a new benchmark designed to evaluate NL2VIS systems in scenarios involving ambiguous queries. nvBench 2.0 includes 7,878 natural language queries and 24,076 corresponding visualizations, derived from 780 tables across 153 domains. It is built using a controlled ambiguity-injection pipeline that generates ambiguous queries through a reverse-generation workflow. By starting with unambiguous seed visualizations and selectively injecting ambiguities, the pipeline yields multiple valid interpretations for each query, with each ambiguous query traceable to its corresponding visualization through step-wise reasoning paths. We evaluate various Large Language Models (LLMs) on their ability to perform ambiguous NL2VIS tasks using nvBench 2.0. We also propose Step-NL2VIS, an LLM-based model trained on nvBench 2.0, which enhances performance in ambiguous scenarios through step-wise preference optimization. Our results show that Step-NL2VIS outperforms all baselines, setting a new state-of-the-art for ambiguous NL2VIS tasks.

**Link**: [arxiv](http://arxiv.org/abs/2503.12880v1),  [pdf](http://arxiv.org/pdf/2503.12880v1)

**Tags**: cs.CL cs.AI 



### Enable Time-Sensitive Applications in Kubernetes with Container Network   Interface Plugin Agnostic Metadata Proxy
**Authors**: Ferenc Orosi, Ferenc Fejes

**Updated**: 2025-03-17T07:16:31Z

**Summary**: Application deployment in cloud environment is dominated by Kubernetes-orchestrated microservices. Provides a secure environment, networking, storage, isolation, scheduling, and many other abstractions that can be easily extended to meet our needs. Time-Sensitive Applications (TSAs) have special requirements for compute and network. Deploying TSAs in Kubernetes is challenging because the networking implemented by Container Network Interface (CNI) plugins is not aware of the traffic characteristic required by Time-Sensitive Network. Even if a network interface supports TSN features (e.g.: Scheduled Traffic) and a modified CNI plugin is aware of this interface, the pod network isolation built on top of Linux deletes the metadata required for TSN protocols to work with.   We propose TSN metadata proxy, a simple architecture that allows any TSA microservice to use the TSN capabilities of the physical NIC, without any modification. This architecture is tightly integrated with the Kubernetes networking model, works with popular CNI plugins, and supports services such as ClusterIP, NodePort, or LoadBalancer without additional configuration. Unlike former proposals, this architecture does not require either bypassing the Linux kernel network stack, direct access to the physical NIC, escalated privileges for the TSA microservice, or even modification of the TSA.

**Link**: [arxiv](http://arxiv.org/abs/2503.12878v1),  [pdf](http://arxiv.org/pdf/2503.12878v1)

**Tags**: cs.NI 



### Efficient Transfer Learning for Video-language Foundation Models
**Authors**: Haoxing Chen, Zizheng Huang, Yan Hong, Yanshuo Wang, Zhongcai Lyu, Zhuoer Xu, Jun Lan, Zhangxuan Gu

**Updated**: 2025-03-18T02:51:43Z

**Summary**: Pre-trained vision-language models provide a robust foundation for efficient transfer learning across various downstream tasks. In the field of video action recognition, mainstream approaches often introduce additional modules to capture temporal information. Although the additional modules increase the capacity of model, enabling it to better capture video-specific inductive biases, existing methods typically introduce a substantial number of new parameters and are prone to catastrophic forgetting of previously acquired generalizable knowledge. In this paper, we propose a parameter-efficient Multi-modal Spatio-Temporal Adapter (MSTA) to enhance the alignment between textual and visual representations, achieving a balance between generalizable knowledge and task-specific adaptation. Furthermore, to mitigate over-fitting and enhance generalizability, we introduce a spatio-temporal description-guided consistency constraint.This constraint involves providing template inputs (e.g., "a video of \{\textbf{cls}\}") to the trainable language branch and LLM-generated spatio-temporal descriptions to the pre-trained language branch, enforcing output consistency between the branches. This approach reduces overfitting to downstream tasks and enhances the distinguishability of the trainable branch within the spatio-temporal semantic space. We evaluate the effectiveness of our approach across four tasks: zero-shot transfer, few-shot learning, base-to-novel generalization, and fully-supervised learning. Compared to many state-of-the-art methods, our MSTA achieves outstanding performance across all evaluations, while using only 2-7\% of the trainable parameters in the original model.

**Link**: [arxiv](http://arxiv.org/abs/2411.11223v4),  [pdf](http://arxiv.org/pdf/2411.11223v4)

**Tags**: cs.CV 



### Enhancing LLM Reasoning with Iterative DPO: A Comprehensive Empirical   Investigation
**Authors**: Songjun Tu, Jiahao Lin, Xiangyu Tian, Qichao Zhang, Linjing Li, Yuqian Fu, Nan Xu, Wei He, Xiangyuan Lan, Dongmei Jiang, Dongbin Zhao

**Updated**: 2025-03-17T06:28:25Z

**Summary**: Recent advancements in post-training methodologies for large language models (LLMs) have highlighted reinforcement learning (RL) as a critical component for enhancing reasoning. However, the substantial computational costs associated with RL-based approaches have led to growing interest in alternative paradigms, such as Direct Preference Optimization (DPO). In this study, we investigate the effectiveness of DPO in facilitating self-improvement for LLMs through iterative preference-based learning. We demonstrate that a single round of DPO with coarse filtering significantly enhances mathematical reasoning performance, particularly for strong base model. Furthermore, we design an iterative enhancement framework for both the generator and the reward model (RM), enabling their mutual improvement through online interaction across multiple rounds of DPO. Finally, with simple verifiable rewards, our model DPO-VP achieves RL-level performance with significantly lower computational overhead. These findings highlight DPO as a scalable and cost-effective alternative to RL, offering a practical solution for enhancing LLM reasoning in resource-constrained situations.

**Link**: [arxiv](http://arxiv.org/abs/2503.12854v1),  [pdf](http://arxiv.org/pdf/2503.12854v1)

**Tags**: cs.CL 



### ACT360: An Efficient 360-Degree Action Detection and Summarization   Framework for Mission-Critical Training and Debriefing
**Authors**: Aditi Tiwari, Klara Nahrstedt

**Updated**: 2025-03-17T06:12:36Z

**Summary**: Effective training and debriefing are critical in high-stakes, mission-critical environments such as disaster response, military simulations, and industrial safety, where precision and minimizing errors are paramount. The traditional post-training analysis relies on manually reviewing 2D videos, a time-consuming process that lacks comprehensive situational awareness. To address these limitations, we introduce ACT360, a system that leverages 360-degree videos and machine learning for automated action detection and structured debriefing. ACT360 integrates 360YOWO, an enhanced You Only Watch Once (YOWO) model with spatial attention and equirectangular-aware convolution (EAC) to mitigate panoramic video distortions. To enable deployment in resource-constrained environments, we apply quantization and model pruning, reducing the model size by 74% while maintaining robust accuracy (mAP drop of only 1.5%, from 0.865 to 0.850) and improving inference speed. We validate our approach on a publicly available dataset of 55 labeled 360-degree videos covering seven key operational actions, recorded across various real-world training sessions and environmental conditions. Additionally, ACT360 integrates 360AIE (Action Insight Explorer), a web-based interface for automatic action detection, retrieval, and textual summarization using large language models (LLMs), significantly enhancing post-incident analysis efficiency. ACT360 serves as a generalized framework for mission-critical debriefing, incorporating EAC, spatial attention, summarization, and model optimization. These innovations apply to any training environment requiring lightweight action detection and structured post-exercise analysis.

**Link**: [arxiv](http://arxiv.org/abs/2503.12852v1),  [pdf](http://arxiv.org/pdf/2503.12852v1)

**Tags**: cs.CV cs.MM 



### Chain-of-Probe: Examining the Necessity and Accuracy of CoT Step-by-Step
**Authors**: Zezhong Wang, Xingshan Zeng, Weiwen Liu, Yufei Wang, Liangyou Li, Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu, Kam-Fai Wong

**Updated**: 2025-03-17T06:04:48Z

**Summary**: Current research found the issue of Early Answering in large language models (LLMs), where the models already have an answer before generating the Chain-of-Thought (CoT). This phenomenon suggests a potential lack of necessary dependency between the predicted answer and the reasoning process. Consequently, two important questions arise: (1) Is CoT still necessary if the model already has an answer? (2) Can the correctness of the answer serve as valid evidence for the correctness of CoT? To address these questions, we propose a method, namely Chain-of-Probe (CoP), to probe changes in the mind during the model's reasoning. The probing results show that in a significant number of question-answer cases, CoT appears to be unnecessary, and this necessity correlates with the simplicity of the task, defined by reasoning steps required. Furthermore, by analyzing patterns in mind change, we examine the correctness of the model's reasoning. Our validation reveals that many responses, although correct in their final answer, contain errors in their reasoning process. To this end, we propose a strategic approach based on CoP to prioritize answers with correct reasoning among multiple candidates, thereby bolstering the reliability of the model's reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2406.16144v2),  [pdf](http://arxiv.org/pdf/2406.16144v2)

**Tags**: cs.CL 



### ToolFlow: Boosting LLM Tool-Calling Through Natural and Coherent   Dialogue Synthesis
**Authors**: Zezhong Wang, Xingshan Zeng, Weiwen Liu, Liangyou Li, Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu, Kam-Fai Wong

**Updated**: 2025-03-17T06:00:52Z

**Summary**: Supervised fine-tuning (SFT) is a common method to enhance the tool calling capabilities of Large Language Models (LLMs), with the training data often being synthesized. The current data synthesis process generally involves sampling a set of tools, formulating a requirement based on these tools, and generating the call statements. However, tools sampled randomly lack relevance, making them difficult to combine and thus reducing the diversity of the data. Additionally, current work overlooks the coherence between turns of dialogues, leading to a gap between the synthesized data and real-world scenarios. To address these issues, we propose a Graph-based Sampling strategy to sample more relevant tool combinations, and a Planned-generation strategy to create plans that guide the synthesis of coherent dialogues. We integrate these two strategies and enable multiple agents to synthesize the dialogue data interactively, resulting in our tool-calling data synthesis pipeline ToolFlow. Data quality assessments demonstrate improvements in the naturalness and coherence of our synthesized dialogues. Finally, we apply SFT on LLaMA-3.1-8B using 8,000 synthetic dialogues generated with ToolFlow. Results show that the model achieves tool-calling performance comparable to or even surpassing GPT-4, while maintaining strong general capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2410.18447v2),  [pdf](http://arxiv.org/pdf/2410.18447v2)

**Tags**: cs.CL 



### Policy Prototyping for LLMs: Pluralistic Alignment via Interactive and   Collaborative Policymaking
**Authors**: K. J. Kevin Feng, Inyoung Cheong, Quan Ze Chen, Amy X. Zhang

**Updated**: 2025-03-17T05:45:58Z

**Summary**: Emerging efforts in AI alignment seek to broaden participation in shaping model behavior by eliciting and integrating collective input into a policy for model finetuning. While pluralistic, these processes are often linear and do not allow participating stakeholders to confirm whether potential outcomes of their contributions are indeed consistent with their intentions. Design prototyping has long advocated for rapid iteration using tight feedback loops of ideation, experimentation, and evaluation to mitigate these issues. We thus propose policy prototyping for LLMs, a new process that draws inspiration from prototyping practices to enable stakeholders to collaboratively and interactively draft LLM policies. Through learnings from a real-world LLM policymaking initiative at an industrial AI lab, we motivate our approach and characterize policy prototyping with four guiding principles. Because policy prototyping emphasizes a contrasting set of priorities compared to previous approaches, we envision our approach to be a valuable addition to the methodological repertoire for collaborative, pluralistic alignment.

**Link**: [arxiv](http://arxiv.org/abs/2409.08622v2),  [pdf](http://arxiv.org/pdf/2409.08622v2)

**Tags**: cs.HC 



### Exploiting Edited Large Language Models as General Scientific Optimizers
**Authors**: Qitan Lv, Tianyu Liu, Hong Wang

**Updated**: 2025-03-17T05:40:49Z

**Summary**: Large language models (LLMs) have been widely adopted in mathematical optimization in scientific scenarios for their extensive knowledge and advanced reasoning capabilities. Existing methods mainly focus on utilizing LLMs to solve optimization problems in a prompt-based manner, which takes observational feedback as additional textual descriptions. However, due to LLM's \textbf{high sensitivity to the prompts} and \textbf{tendency to get lost in lengthy prompts}, these methods struggle to effectively utilize the {observational} feedback from each optimization step, which severely hinders the applications for real-world scenarios. To address these challenges, we propose a conceptually simple and general {bi-level} optimization method, namely \textbf{G}eneral \textbf{S}cientific \textbf{O}ptimizers (GSO). Specifically, GSO first utilizes inner-level simulators as experimental platforms to evaluate the current solution and provide observational feedback. Then, LLMs serve as knowledgeable and versatile scientists, generating new solutions by refining potential errors from the feedback as the outer-level optimization. Finally, simulations together with the expert knowledge in LLMs are jointly updated with bi-level interactions via model editing. Extensive experiments show that GSO consistently outperforms existing state-of-the-art methods using \textit{six} different LLM backbones on \textit{seven} different tasks, demonstrating the effectiveness and a wide range of applications.

**Link**: [arxiv](http://arxiv.org/abs/2503.09620v2),  [pdf](http://arxiv.org/pdf/2503.09620v2)

**Tags**: math.OC cs.AI 



### SparseLUT: Sparse Connectivity Optimization for Lookup Table-based Deep   Neural Networks
**Authors**: Binglei Lou, Ruilin Wu, Philip Leong

**Updated**: 2025-03-17T05:21:54Z

**Summary**: The deployment of deep neural networks (DNNs) on resource-constrained edge devices such as field-programmable gate arrays (FPGAs) requires a careful balance of latency, power, and resource usage while maintaining high accuracy. Existing Lookup Table (LUT)-based DNNs, including LogicNets, PolyLUT, PolyLUT-Add, and NeuraLUT, exploit native FPGA resources with random sparse connectivity. This paper introduces SparseLUT, a connectivity-centric training technique tailored for LUT-based DNNs. SparseLUT leverages a non-greedy training strategy that prioritizes the pruning of less significant connections and strategically regrows alternative ones, resulting in efficient convergence to the target sparsity. Experimental results show consistent accuracy improvements across benchmarks, including up to a 2.13\% increase on MNIST and a 0.94\% improvement for Jet Substructure Classification compared to random sparsity. This is done without any hardware overhead and achieves state-of-the-art results for LUT-based DNNs.

**Link**: [arxiv](http://arxiv.org/abs/2503.12829v1),  [pdf](http://arxiv.org/pdf/2503.12829v1)

**Tags**: cs.AR cs.AI 



### To Trust or Not to Trust? Enhancing Large Language Models' Situated   Faithfulness to External Contexts
**Authors**: Yukun Huang, Sanxing Chen, Hongyi Cai, Bhuwan Dhingra

**Updated**: 2025-03-17T04:47:58Z

**Summary**: Large Language Models (LLMs) are often augmented with external contexts, such as those used in retrieval-augmented generation (RAG). However, these contexts can be inaccurate or intentionally misleading, leading to conflicts with the model's internal knowledge. We argue that robust LLMs should demonstrate situated faithfulness, dynamically calibrating their trust in external information based on their confidence in the internal knowledge and the external context to resolve knowledge conflicts. To benchmark this capability, we evaluate LLMs across several QA datasets, including a newly created dataset featuring in-the-wild incorrect contexts sourced from Reddit posts. We show that when provided with both correct and incorrect contexts, both open-source and proprietary models tend to overly rely on external information, regardless of its factual accuracy. To enhance situated faithfulness, we propose two approaches: Self-Guided Confidence Reasoning (SCR) and Rule-Based Confidence Reasoning (RCR). SCR enables models to self-assess the confidence of external information relative to their own internal knowledge to produce the most accurate answer. RCR, in contrast, extracts explicit confidence signals from the LLM and determines the final answer using predefined rules. Our results show that for LLMs with strong reasoning capabilities, such as GPT-4o and GPT-4o mini, SCR outperforms RCR, achieving improvements of up to 24.2% over a direct input augmentation baseline. Conversely, for a smaller model like Llama-3-8B, RCR outperforms SCR. Fine-tuning SCR with our proposed Confidence Reasoning Direct Preference Optimization (CR-DPO) method improves performance on both seen and unseen datasets, yielding an average improvement of 8.9% on Llama-3-8B. In addition to quantitative results, we offer insights into the relative strengths of SCR and RCR.

**Link**: [arxiv](http://arxiv.org/abs/2410.14675v2),  [pdf](http://arxiv.org/pdf/2410.14675v2)

**Tags**: cs.CL cs.AI 



### Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study   on Audio Question Answering
**Authors**: Gang Li, Jizhong Liu, Heinrich Dinkel, Yadong Niu, Junbo Zhang, Jian Luan

**Updated**: 2025-03-17T04:20:29Z

**Summary**: Recently, reinforcement learning (RL) has been shown to greatly enhance the reasoning capabilities of large language models (LLMs), and RL-based approaches have been progressively applied to visual multimodal tasks. However, the audio modality has largely been overlooked in these developments. Thus, we conduct a series of RL explorations in audio understanding and reasoning, specifically focusing on the audio question answering (AQA) task. We leverage the group relative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and our experiments demonstrated state-of-the-art performance on the MMAU Test-mini benchmark, achieving an accuracy rate of 64.5%. The main findings in this technical report are as follows: 1) The GRPO algorithm can be effectively applied to large audio language models (LALMs), even when the model has only 8.2B parameters; 2) With only 38k post-training samples, RL significantly outperforms supervised fine-tuning (SFT), indicating that RL-based approaches can be effective without large datasets; 3) The explicit reasoning process has not shown significant benefits for AQA tasks, and how to efficiently utilize deep thinking remains an open question for further research; 4) LALMs still lag far behind humans auditory-language reasoning, suggesting that the RL-based approaches warrant further exploration. Our project is available at https://github.com/xiaomi-research/r1-aqa and https://huggingface.co/mispeech/r1-aqa.

**Link**: [arxiv](http://arxiv.org/abs/2503.11197v2),  [pdf](http://arxiv.org/pdf/2503.11197v2)

**Tags**: cs.SD cs.AI cs.CL eess.AS 



### Efficient Learning With Sine-Activated Low-rank Matrices
**Authors**: Yiping Ji, Hemanth Saratchandran, Cameron Gordon, Zeyu Zhang, Simon Lucey

**Updated**: 2025-03-17T04:11:01Z

**Summary**: Low-rank decomposition has emerged as a vital tool for enhancing parameter efficiency in neural network architectures, gaining traction across diverse applications in machine learning. These techniques significantly lower the number of parameters, striking a balance between compactness and performance. However, a common challenge has been the compromise between parameter efficiency and the accuracy of the model, where reduced parameters often lead to diminished accuracy compared to their full-rank counterparts. In this work, we propose a novel theoretical framework that integrates a sinusoidal function within the low-rank decomposition process. This approach not only preserves the benefits of the parameter efficiency characteristic of low-rank methods but also increases the decomposition's rank, thereby enhancing model performance. Our method proves to be a plug in enhancement for existing low-rank models, as evidenced by its successful application in Vision Transformers (ViT), Large Language Models (LLMs), Neural Radiance Fields (NeRF) and 3D shape modelling.

**Link**: [arxiv](http://arxiv.org/abs/2403.19243v5),  [pdf](http://arxiv.org/pdf/2403.19243v5)

**Tags**: cs.LG cs.CV cs.NE 



