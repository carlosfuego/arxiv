# Arxiv Results
## Keyword: kv cache 
 ### Over 3 kV and Ultra-Low leakage Vertical (011) \b{eta}-Ga2O3 Power   Diodes with Engineered Schottky Contact and High-permittivity Dielectric   Field Plate
**Authors**: Emerson J. Hollar, Esmat Farzana

**Updated**: 2025-10-29T17:00:16Z

**Summary**: We report over 3 kV breakdown voltage and ultra-low leakage (011) \b{eta}-Ga2O3 power devices utilizing Schottky barrier engineering and high-permittivity (\k{appa}) dielectric (ZrO2) field plate. The (011) orientation of \b{eta}-Ga2O3 enabled low background doping and thick drift layers which are promising to support kV-class vertical \b{eta}-Ga2O3 power switches. The Schottky barrier engineering was performed with a composite Pt cap/PtOx/Pt (1.5 nm) anode contact to take advantage of the enhanced reverse blocking capabilities enabled by PtOx while allowing low turn-on voltage by the interfacing thin Pt layer. We also performed a systematic study using a co-processed Pt/(011) \b{eta}-Ga2O3 Schottky barrier diodes (SBDs) on the same wafer. The bare SBDs revealed a breakdown voltage of ~1.5 kV, while the field-plate Pt/(011) \b{eta}-Ga2O3 SBDs achieved an increased breakdown voltage of 2.75 kV owing to the edge field management. Further enhancement of the breakdown voltage was achieved by tunneling leakage management using composite Pt cap/PtOx/Pt (1.5 nm) Schottky contacts that ultimately enabled breakdown voltage of 3.7 kV for the field-plate diodes. Remarkably, the Pt cap/PtOx/Pt (1.5 nm) Schottky contacts maintained similar turn-on voltage as the Pt/(011) \b{eta}-Ga2O3 SBDs. The combination of efficient tunneling leakage management by composite Pt cap/PtOx/Pt (1.5 nm) contacts with similar turn-on voltage, edge field reduction by high-\k{appa} dielectric ZrO2 field plate, as well as the advantageous material properties offered by (011) \b{eta}-Ga2O3 demonstrate a promising strategy for developing ultra-low leakage and multi-kV class vertical (011) \b{eta}-Ga2O3 power devices.

**Link**: [arxiv](http://arxiv.org/abs/2510.25695v1),  [pdf](http://arxiv.org/pdf/2510.25695v1)

**Tags**: eess.SY cond-mat.mtrl-sci cs.SY 



### Quickest Change Point Detection with Measurements over a Lossy Link
**Authors**: Krishna Chaythanya KV, Saqib Abbas Baba, Anurag Kumar, Arpan Chattopadhyay, Rajesh Sundaresan

**Updated**: 2025-10-29T15:12:35Z

**Summary**: Motivated by Industry 4.0 applications, we consider quickest change detection (QCD) of an abrupt change in a process when its measurements are transmitted by a sensor over a lossy wireless link to a decision maker (DM). The sensor node samples measurements using a Bernoulli sampling process, and places the measurement samples in the transmit queue of its transmitter. The transmitter uses a retransmit-until-success transmission strategy to deliver packets to the DM over the lossy link, in which the packet losses are modeled as a Bernoulli process, with different loss probabilities before and after the change. We pose the QCD problem in the non-Bayesian setting under Lorden's framework, and propose a CUSUM algorithm. By defining a suitable Markov process, involving the DM measurements and the queue length process, we show that the problem reduces to QCD in a Markov process. Characterizing the information measure per measurement sample at the DM, we establish the asymptotic optimality of our algorithm when the false alarm rate tends to zero. Further, when the DM receives incomplete data due to channel loss, we present asymptotically optimal QCD algorithms by suitably modifying the CUSUM algorithm. We then explore the last-come-first-served (LCFS) queuing discipline at the sensor transmit queue to lower detection delay in the non-asymptotic case. Next, we consider the case of multiple sensors, each with its own wireless transmitter queue, and show that our analysis extends to the case of multiple homogeneous sensors. When the sensors are heterogeneous, we present a sensor scheduling algorithm that minimizes detection delay by balancing the trade-off between the age of the observations and their information content. Numerical analysis demonstrate trade-offs that can be used to optimize system design parameters in the non-asymptotic regime.

**Link**: [arxiv](http://arxiv.org/abs/2510.25604v1),  [pdf](http://arxiv.org/pdf/2510.25604v1)

**Tags**: eess.SP 



### PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse   Attention for Vision-Language Large Models
**Authors**: Zhonghua Jiang, Kunxi Li, Yiyun Zhou, Sihao Liu, Zhaode Wang, Chengfei lv, Shengyu Zhang

**Updated**: 2025-10-30T03:43:02Z

**Summary**: Vision-Language Large Models (VLLMs) face significant efficiency challenges when processing high-resolution inputs. The quadratic complexity in attention and autoregressive generation, as well as the constantly growing key value (KV) cache size, severely hinder the prefilling and decoding stages. Recent efforts have attempted to compress KV cache by identifying and pruning KV cache of less important tokens, but these methods typically rely on attention scores to estimate token importance, making them incompatible with efficient attention mechanisms such as FlashAttention and Sparse Attention, which do not explicitly compute attention matrices. Moreover, existing methods overlook how sparse attention, while accelerating the prefilling stage, alters the information structure of the KV cache, thereby compromising the effectiveness of downstream KV cache compression strategies. To address this issue, we propose PureKV, a plug-and-play framework for joint optimization of sparse attention and KV cache compression. We first introduce a KV cache compression strategy that is fully compatible with efficient attention accelerators. Our method utilizes lower layer attention scores to estimate the importance of high layers' KV cache, enabling active pruning without compromising accuracy. In addition, we have designed a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically tailored for video KV cache compression algorithms. This module combines spatial and temporal attention sparsity to improve the compression efficiency of KV cache optimization algorithms by purifying spatial noise and temporal redundancy in KV cache. At the same time, ST-SpAttn also accelerated the prefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2, Qwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and 3.16 times prefill acceleration, with negligible quality degradation.

**Link**: [arxiv](http://arxiv.org/abs/2510.25600v2),  [pdf](http://arxiv.org/pdf/2510.25600v2)

**Tags**: cs.MM 



### RegionE: Adaptive Region-Aware Generation for Efficient Image Editing
**Authors**: Pengtao Chen, Xianfang Zeng, Maosen Zhao, Mingzhu Shen, Peng Ye, Bangyin Xiang, Zhibo Wang, Wei Cheng, Gang Yu, Tao Chen

**Updated**: 2025-10-29T14:58:37Z

**Summary**: Recently, instruction-based image editing (IIE) has received widespread attention. In practice, IIE often modifies only specific regions of an image, while the remaining areas largely remain unchanged. Although these two types of regions differ significantly in generation difficulty and computational redundancy, existing IIE models do not account for this distinction, instead applying a uniform generation process across the entire image. This motivates us to propose RegionE, an adaptive, region-aware generation framework that accelerates IIE tasks without additional training. Specifically, the RegionE framework consists of three main components: 1) Adaptive Region Partition. We observed that the trajectory of unedited regions is straight, allowing for multi-step denoised predictions to be inferred in a single step. Therefore, in the early denoising stages, we partition the image into edited and unedited regions based on the difference between the final estimated result and the reference image. 2) Region-Aware Generation. After distinguishing the regions, we replace multi-step denoising with one-step prediction for unedited areas. For edited regions, the trajectory is curved, requiring local iterative denoising. To improve the efficiency and quality of local iterative generation, we propose the Region-Instruction KV Cache, which reduces computational cost while incorporating global information. 3) Adaptive Velocity Decay Cache. Observing that adjacent timesteps in edited regions exhibit strong velocity similarity, we further propose an adaptive velocity decay cache to accelerate the local denoising process. We applied RegionE to state-of-the-art IIE base models, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE achieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o confirmed that semantic and perceptual fidelity were well preserved.

**Link**: [arxiv](http://arxiv.org/abs/2510.25590v1),  [pdf](http://arxiv.org/pdf/2510.25590v1)

**Tags**: cs.CV cs.AI 



### FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual   Question Answering
**Authors**: Liangyu Zhong, Fabio Rosenthal, Joachim Sicking, Fabian HÃ¼ger, Thorsten Bagdonat, Hanno Gottschalk, Leo Schwinn

**Updated**: 2025-10-29T14:46:17Z

**Summary**: While Multimodal Large Language Models (MLLMs) offer strong perception and reasoning capabilities for image-text input, Visual Question Answering (VQA) focusing on small image details still remains a challenge. Although visual cropping techniques seem promising, recent approaches have several limitations: the need for task-specific fine-tuning, low efficiency due to uninformed exhaustive search, or incompatibility with efficient attention implementations. We address these shortcomings by proposing a training-free visual cropping method, dubbed FOCUS, that leverages MLLM-internal representations to guide the search for the most relevant image region. This is accomplished in four steps: first, we identify the target object(s) in the VQA prompt; second, we compute an object relevance map using the key-value (KV) cache; third, we propose and rank relevant image regions based on the map; and finally, we perform the fine-grained VQA task using the top-ranked region. As a result of this informed search strategy, FOCUS achieves strong performance across four fine-grained VQA datasets and three types of MLLMs. It outperforms three popular visual cropping methods in both accuracy and efficiency, and matches the best-performing baseline, ZoomEye, while requiring 3 - 6.5 x less compute.

**Link**: [arxiv](http://arxiv.org/abs/2506.21710v2),  [pdf](http://arxiv.org/pdf/2506.21710v2)

**Tags**: cs.CV 



### Serve Programs, Not Prompts
**Authors**: In Gim, Lin Zhong

**Updated**: 2025-10-29T11:29:03Z

**Summary**: Current large language model (LLM) serving systems, primarily designed for text completion, are neither efficient nor adaptable for increasingly complex LLM applications due to their inflexible design. We propose a new LLM serving system architecture that serves programs instead of prompts to address this problem. These programs, called LLM Inference Programs (LIPs), allow users to customize token prediction and KV cache management at runtime and to offload parts of their application logic, such as tool execution, to the server. We describe an example of this architecture through a system named Symphony, which functions as an operating system for LIPs. Symphony exposes LLM model computations via system calls and virtualizes KV cache with a dedicated file system, while ensuring GPU efficiency with a two-level process scheduling scheme. Symphony has the potential to open the door to a more efficient and extensible ecosystem for LLM applications.

**Link**: [arxiv](http://arxiv.org/abs/2510.25412v1),  [pdf](http://arxiv.org/pdf/2510.25412v1)

**Tags**: cs.CL 



### Model-Document Protocol for AI Search
**Authors**: Hongjin Qian, Zheng Liu

**Updated**: 2025-10-30T08:52:17Z

**Summary**: AI search depends on linking large language models (LLMs) with vast external knowledge sources. Yet web pages, PDF files, and other raw documents are not inherently LLM-ready: they are long, noisy, and unstructured. Conventional retrieval methods treat these documents as verbatim text and return raw passages, leaving the burden of fragment assembly and contextual reasoning to the LLM. This gap underscores the need for a new retrieval paradigm that redefines how models interact with documents.   We introduce the Model-Document Protocol (MDP), a general framework that formalizes how raw text is bridged to LLMs through consumable knowledge representations. Rather than treating retrieval as passage fetching, MDP defines multiple pathways that transform unstructured documents into task-specific, LLM-ready inputs. These include agentic reasoning, which curates raw evidence into coherent context; memory grounding, which accumulates reusable notes to enrich reasoning; and structured leveraging, which encodes documents into formal representations such as graphs or key-value caches. All three pathways share the same goal: ensuring that what reaches the LLM is not raw fragments but compact, structured knowledge directly consumable for reasoning.   As an instantiation, we present MDP-Agent, which realizes the protocol through an agentic process: constructing document-level gist memories for global coverage, performing diffusion-based exploration with vertical exploitation to uncover layered dependencies, and applying map-reduce style synthesis to integrate large-scale evidence into compact yet sufficient context. Experiments on information-seeking benchmarks demonstrate that MDP-Agent outperforms baselines, validating both the soundness of the MDP framework and the effectiveness of its agentic instantiation.

**Link**: [arxiv](http://arxiv.org/abs/2510.25160v2),  [pdf](http://arxiv.org/pdf/2510.25160v2)

**Tags**: cs.CL cs.AI cs.IR 



### Off-Centered WoS-Type Solvers with Statistical Weighting
**Authors**: Anchang Bao, Jie Xu, Enya Shen, Jianmin Wang

**Updated**: 2025-10-29T04:09:50Z

**Summary**: Stochastic PDE solvers have emerged as a powerful alternative to traditional discretization-based methods for solving partial differential equations (PDEs), especially in geometry processing and graphics. While off-centered estimators enhance sample reuse in WoS-type Monte Carlo solvers, they introduce correlation artifacts and bias when Green's functions are approximated. In this paper, we propose a statistically weighted off-centered WoS-type estimator that leverages local similarity filtering to selectively combine samples across neighboring evaluation points. Our method balances bias and variance through a principled weighting strategy that suppresses unreliable estimators. We demonstrate our approach's effectiveness on various PDEs,including screened Poisson equations and boundary conditions, achieving consistent improvements over existing solvers such as vanilla Walk on Spheres, mean value caching, and boundary value caching. Our method also naturally extends to gradient field estimation and mixed boundary problems.

**Link**: [arxiv](http://arxiv.org/abs/2510.25152v1),  [pdf](http://arxiv.org/pdf/2510.25152v1)

**Tags**: cs.GR 



### NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized   Generalist Robotic Policies
**Authors**: Jiahong Chen, Jing Wang, Long Chen, Chuwei Cai, Jinghui Lu

**Updated**: 2025-10-29T03:00:36Z

**Summary**: Vision-language-action (VLA) models have significantly advanced robotic manipulation by integrating vision-language models (VLMs), and action decoders into a unified architecture. However, their deployment on resource-constrained edge devices, such as mobile robots or embedded systems (e.g., Jetson Orin Nano), remains challenging due to high computational demands, especially in real-world scenarios where power, latency, and computational resources are critical. To close this gap, we introduce Nano-scale Vision-Language Action (NanoVLA), a family of lightweight VLA architectures that achieve high performance with minimal resources. Our core innovations include: (1) vision-language decoupling that moves conventional early vision and language inputs fusion in VLM to late stage, achieving better performance while enabling caching and reduce inference overhead and latency; (2) long-short action chunking to ensure smooth, coherent multi-step planning without sacrificing real-time responsiveness; (3) dynamic routing that adaptively assigns lightweight or heavy backbones based on task complexity, further optimizing inference efficiency. Experimental results on several benchmarks, as well as real-world deployments, demonstrate that NanoVLA achieves up to 52x faster inference on edge devices compared to previous state-of-the-art VLA models, with 98% less parameters while maintaining or surpassing their task accuracy and generalization. Ablation studies confirm that our decoupling strategy preserves cross-task transferability, and the routing module enhances cost-performance trade-offs, enabling practical, high-precision robotic manipulation on resource-constrained hardware.

**Link**: [arxiv](http://arxiv.org/abs/2510.25122v1),  [pdf](http://arxiv.org/pdf/2510.25122v1)

**Tags**: cs.RO 



### Parallel Loop Transformer for Efficient Test-Time Computation Scaling
**Authors**: Bohong Wu, Mengzhao Chen, Xiang Luo, Shen Yan, Qifan Yu, Fan Xia, Tianqi Zhang, Hongrui Zhan, Zheng Zhong, Xun Zhou, Siyuan Qiao, Xingyan Bin

**Updated**: 2025-10-28T15:35:50Z

**Summary**: Large Language Models (LLMs) are powerful but often too slow and costly for real-world use during inference. Looped transformers save on parameters by reusing the same weights for multiple computational steps, or "loops." However, this approach has a major flaw: the loops run one after another, causing inference latency and memory requirements to increase with each added loop. This makes them impractical for fast applications. To solve this problem, we introduce the Parallel Loop Transformer (PLT). PLT is a new architecture that delivers the performance benefits of a deep, looped model but with the low latency of a standard, non-looped model. PLT works using two key techniques. First, Cross-Loop Parallelism (CLP) breaks the sequential dependency by computing different loops for different tokens at the same time, all within a single pass. Second, to prevent memory costs from growing, we use an Efficient Representation Enhancement strategy. This method shares the memory (KV cache) from the first loop with all other loops. It then uses a Gated Sliding-Window Attention (G-SWA) to combine this shared global information with local information, maintaining high accuracy. Our experiments show that PLT achieves the high accuracy of a traditional looped model but with almost no extra latency or memory cost compared to a standard transformer.

**Link**: [arxiv](http://arxiv.org/abs/2510.24824v1),  [pdf](http://arxiv.org/pdf/2510.24824v1)

**Tags**: cs.CL 



### An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine
**Authors**: Pedram Fard, Alaleh Azhir, Neguine Rezaii, Jiazi Tian, Hossein Estiri

**Updated**: 2025-10-28T12:28:02Z

**Summary**: Artificial intelligence in medicine is built to serve the average patient. By minimizing error across large datasets, most systems deliver strong aggregate accuracy yet falter at the margins: patients with rare variants, multimorbidity, or underrepresented demographics. This average patient fallacy erodes both equity and trust. We propose a different design: a multi-agent ecosystem for N-of-1 decision support. In this environment, agents clustered by organ systems, patient populations, and analytic modalities draw on a shared library of models and evidence synthesis tools. Their results converge in a coordination layer that weighs reliability, uncertainty, and data density before presenting the clinician with a decision-support packet: risk estimates bounded by confidence ranges, outlier flags, and linked evidence. Validation shifts from population averages to individual reliability, measured by error in low-density regions, calibration in the small, and risk--coverage trade-offs. Anticipated challenges include computational demands, automation bias, and regulatory fit, addressed through caching strategies, consensus checks, and adaptive trial frameworks. By moving from monolithic models to orchestrated intelligence, this approach seeks to align medical AI with the first principle of medicine: care that is transparent, equitable, and centered on the individual.

**Link**: [arxiv](http://arxiv.org/abs/2510.24359v1),  [pdf](http://arxiv.org/pdf/2510.24359v1)

**Tags**: cs.AI cs.SY eess.SY q-bio.QM stat.AP 



### SALS: Sparse Attention in Latent Space for KV cache Compression
**Authors**: Junlin Mu, Hantao Huang, Jihang Zhang, Minghui Yu, Tao Wang, Yidong Li

**Updated**: 2025-10-28T10:32:52Z

**Summary**: Large Language Models capable of handling extended contexts are in high demand, yet their inference remains challenging due to substantial Key-Value cache size and high memory bandwidth requirements. Previous research has demonstrated that KV cache exhibits low-rank characteristics within the hidden dimension, suggesting the potential for effective compression. However, due to the widely adopted Rotary Position Embedding mechanism in modern LLMs, naive low-rank compression suffers severe accuracy degradation or creates a new speed bottleneck, as the low-rank cache must first be reconstructed in order to apply RoPE. In this paper, we introduce two key insights: first, the application of RoPE to the key vectors increases their variance, which in turn results in a higher rank; second, after the key vectors are transformed into the latent space, they largely maintain their representation across most layers. Based on these insights, we propose the Sparse Attention in Latent Space framework. SALS projects the KV cache into a compact latent space via low-rank projection, and performs sparse token selection using RoPE-free query-key interactions in this space. By reconstructing only a small subset of important tokens, it avoids the overhead of full KV cache reconstruction. We comprehensively evaluate SALS on various tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and additionally verify its scalability on the RULER-128k benchmark with LLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA performance by maintaining competitive accuracy. Under different settings, SALS achieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention operator compared to FlashAttention2 on the 4K sequence. For the end-to-end throughput performance, we achieves 1.4-fold and 4.5-fold improvement compared to GPT-fast on 4k and 32K sequences, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2510.24273v1),  [pdf](http://arxiv.org/pdf/2510.24273v1)

**Tags**: cs.LG 



### Pie: A Programmable Serving System for Emerging LLM Applications
**Authors**: In Gim, Zhiyao Ma, Seung-seob Lee, Lin Zhong

**Updated**: 2025-10-28T04:17:55Z

**Summary**: Emerging large language model (LLM) applications involve diverse reasoning strategies and agentic workflows, straining the capabilities of existing serving systems built on a monolithic token generation loop. This paper introduces Pie, a programmable LLM serving system designed for flexibility and efficiency. Pie decomposes the traditional generation loop into fine-grained service handlers exposed via an API and delegates control of the generation process to user-provided programs, called inferlets. This enables applications to implement new KV cache strategies, bespoke generation logic, and seamlessly integrate computation and I/O-entirely within the application, without requiring modifications to the serving system. Pie executes inferlets using WebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows Pie matches state-of-the-art performance on standard tasks (3-12% latency overhead) while significantly improving latency and throughput (1.3x-3.4x higher) on agentic workflows by enabling application-specific optimizations.

**Link**: [arxiv](http://arxiv.org/abs/2510.24051v1),  [pdf](http://arxiv.org/pdf/2510.24051v1)

**Tags**: cs.CL 



### FastKV: KV Cache Compression for Fast Long-Context Processing with   Token-Selective Propagation
**Authors**: Dongwon Jo, Jiwon Song, Yulhwa Kim, Jae-Joon Kim

**Updated**: 2025-10-28T04:00:18Z

**Summary**: While large language models (LLMs) excel at handling long-context sequences, they require substantial prefill computation and key-value (KV) cache, which can heavily burden computational efficiency and memory usage in both prefill and decoding stages. Recent works that compress KV caches with prefill acceleration reduce this cost but inadvertently tie the prefill compute reduction to the decoding KV budget. This coupling arises from overlooking the layer-dependent variation of critical context, often leading to accuracy degradation. To address this issue, we introduce FastKV, a KV cache compression framework designed to reduce latency in both prefill and decoding by leveraging the stabilization of token importance in later layers. FastKV performs full-context computation until a Token-Selective Propagation (TSP) layer, which forwards only the most informative tokens to subsequent layers. From these propagated tokens, FastKV independently selects salient KV entries for caching, thereby decoupling KV budget from the prefill compute reduction based on the TSP decision. This independent control of the TSP rate and KV retention rate enables flexible optimization of efficiency and accuracy. Experimental results show that FastKV achieves speedups of up to 1.82$\times$ in prefill and 2.87$\times$ in decoding compared to the full-context baseline, while matching the accuracy of the baselines that only accelerate the decoding stage. Our code is available at https://github.com/dongwonjo/FastKV.

**Link**: [arxiv](http://arxiv.org/abs/2502.01068v4),  [pdf](http://arxiv.org/pdf/2502.01068v4)

**Tags**: cs.LG cs.CL 



### STree: Speculative Tree Decoding for Hybrid State-Space Models
**Authors**: Yangchao Wu, Zongyue Qin, Alex Wong, Stefano Soatto

**Updated**: 2025-10-27T21:48:48Z

**Summary**: Speculative decoding is a technique to leverage hardware concurrency in order to enable multiple steps of token generation in a single forward pass, thus improving the efficiency of large-scale autoregressive (AR) Transformer models. State-space models (SSMs) are already more efficient than AR Transformers, since their state summarizes all past data with no need to cache or re-process tokens in the sliding window context. However, their state can also comprise thousands of tokens; so, speculative decoding has recently been extended to SSMs. Existing approaches, however, do not leverage the tree-based verification methods, since current SSMs lack the means to compute a token tree efficiently. We propose the first scalable algorithm to perform tree-based speculative decoding in state-space models (SSMs) and hybrid architectures of SSMs and Transformer layers. We exploit the structure of accumulated state transition matrices to facilitate tree-based speculative decoding with minimal overhead relative to current SSM implementations. Along with the algorithm, we describe a hardware-aware implementation that improves naive application of AR Transformer tree-based speculative decoding methods to SSMs. Furthermore, we outperform vanilla speculative decoding with SSMs even with a baseline drafting model and tree structure on three different benchmarks, opening up opportunities for further speed up with SSM and hybrid model inference. Code can be found at: https://github.com/wyc1997/stree.

**Link**: [arxiv](http://arxiv.org/abs/2505.14969v2),  [pdf](http://arxiv.org/pdf/2505.14969v2)

**Tags**: cs.LG cs.AI 



### KV-weights are all you need for skipless transformers
**Authors**: Nils Graef

**Updated**: 2025-10-27T17:31:15Z

**Summary**: He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the V and P (post-attention projection) linear layers, which reduces the total number of weights. However, this scheme is only applicable to MHA (multi-head attention), but not for MQA (multi-query attention) and GQA (grouped-query attention). The latter schemes are used by many popular LLMs such as Llama 2, Mistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes mathematically equivalent versions that are suitable for MQA and GQA. For example, removing Q and P from a skipless version of Mistral-7B would remove 15% of its weights (and thus reduce its compute and memory complexity). Watch our explainer video https://youtu.be/Tx_lMpphd2g and see https://github.com/OpenMachine-ai/transformer-tricks for code and more transformer tricks.

**Link**: [arxiv](http://arxiv.org/abs/2404.12362v2),  [pdf](http://arxiv.org/pdf/2404.12362v2)

**Tags**: cs.LG 



### Leveraging Approximate Caching for Faster Retrieval-Augmented Generation
**Authors**: Shai Bergman, Anne-Marie Kermarrec, Diana Petrescu, Rafael Pires, Mathis Randl, Martijn de Vos, Ji Zhang

**Updated**: 2025-10-27T16:20:28Z

**Summary**: Retrieval-augmented generation (RAG) improves the reliability of large language model (LLM) answers by integrating external knowledge. However, RAG increases the end-to-end inference time since looking for relevant documents from large vector databases is computationally expensive. To address this, we introduce Proximity, an approximate key-value cache that optimizes the RAG workflow by leveraging similarities in user queries. Instead of treating each query independently, Proximity reuses previously retrieved documents when similar queries appear, substantially reducing the reliance on expensive vector database lookups. To efficiently scale, Proximity employs a locality-sensitive hashing (LSH) scheme that enables fast cache lookups while preserving retrieval accuracy. We evaluate Proximity using the MMLU and MedRAG question-answering benchmarks. Our experiments demonstrate that Proximity with our LSH scheme and a realistically-skewed MedRAG workload reduces database calls by 77.2% while maintaining database recall and test accuracy. We experiment with different similarity tolerances and cache capacities, and show that the time spent within the Proximity cache remains low and constant (4.8 microseconds) even as the cache grows substantially in size. Our results demonstrate that approximate caching is a practical and effective strategy for optimizing RAG-based systems.

**Link**: [arxiv](http://arxiv.org/abs/2503.05530v3),  [pdf](http://arxiv.org/pdf/2503.05530v3)

**Tags**: cs.DB cs.LG cs.PF 



### A Data-driven ML Approach for Maximizing Performance in LLM-Adapter   Serving
**Authors**: Ferran Agullo, Joan Oliveras, Chen Wang, Alberto Gutierrez-Torre, Olivier Tardieu, Alaa Youssef, Jordi Torres, Josep Ll. Berral

**Updated**: 2025-10-27T14:59:46Z

**Summary**: With the rapid adoption of Large Language Models (LLMs), LLM-adapters have become increasingly common, providing lightweight specialization of large-scale models. Serving hundreds or thousands of these adapters on a single GPU allows request aggregation, increasing throughput, but may also cause request starvation if GPU memory limits are exceeded. To address this issue, this study focuses on determining the joint configuration of concurrent and parallel adapters that maximizes GPU throughput without inducing starvation, given heterogeneous adapter and traffic properties. We propose a data-driven ML approach leveraging interpretable models to tackle this caching problem and introduce the first Digital Twin capable of reproducing an LLM-adapter serving system, enabling efficient training data generation. Experiments with the vLLM framework and LoRA adapters show that the Digital Twin reproduces throughput within 5.1% of real results, while the ML approach predicts optimal numbers of concurrent and parallel adapters with an error of at most 7.2% under heterogeneous, real-world workloads.

**Link**: [arxiv](http://arxiv.org/abs/2508.08343v2),  [pdf](http://arxiv.org/pdf/2508.08343v2)

**Tags**: cs.PF cs.AI cs.CL 



### PESTO: Real-Time Pitch Estimation with Self-supervised   Transposition-equivariant Objective
**Authors**: Alain Riou, Bernardo Torres, Ben Hayes, Stefan Lattner, GaÃ«tan Hadjeres, GaÃ«l Richard, Geoffroy Peeters

**Updated**: 2025-10-27T11:55:07Z

**Summary**: In this paper, we introduce PESTO, a self-supervised learning approach for single-pitch estimation using a Siamese architecture. Our model processes individual frames of a Variable-$Q$ Transform (VQT) and predicts pitch distributions. The neural network is designed to be equivariant to translations, notably thanks to a Toeplitz fully-connected layer. In addition, we construct pitch-shifted pairs by translating and cropping the VQT frames and train our model with a novel class-based transposition-equivariant objective, eliminating the need for annotated data. Thanks to this architecture and training objective, our model achieves remarkable performances while being very lightweight ($130$k parameters). Evaluations on music and speech datasets (MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms self-supervised baselines but also competes with supervised methods, exhibiting superior cross-dataset generalization. Finally, we enhance PESTO's practical utility by developing a streamable VQT implementation using cached convolutions. Combined with our model's low latency (less than 10 ms) and minimal parameter count, this makes PESTO particularly suitable for real-time applications.

**Link**: [arxiv](http://arxiv.org/abs/2508.01488v2),  [pdf](http://arxiv.org/pdf/2508.01488v2)

**Tags**: cs.SD cs.AI cs.LG eess.AS 



### Batch Speculative Decoding Done Right
**Authors**: Ranran Haoran Zhang, Soumik Dey, Ashirbad Mishra, Hansi Wu, Binbin Li, Rui Zhang

**Updated**: 2025-10-26T23:59:23Z

**Summary**: Speculative decoding speeds up LLM inference by using a small draft model to propose multiple tokens that a target model verifies in parallel. Extending this idea to batches is essential for production serving, but it introduces the ragged tensor problem: sequences in the same batch accept different numbers of draft tokens, breaking right-alignment and corrupting position IDs, attention masks, and KV-cache state. We show that several existing batch implementations violate output equivalence-the fundamental requirement that speculative decoding must produce identical token sequences to standard autoregressive generation. These violations occur precisely due to improper handling of the ragged tensor problem. In response, we (1) characterize the synchronization requirements that guarantee correctness, (2) present a correctness-first batch speculative decoding EQSPEC that exposes realignment as consuming 40% of overhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences and dynamically forms same-length groups, to reduce the realignment overhead while preserving per-sequence speculative speedups. On the SpecBench dataset, across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our approach achieves up to 3$\times$ throughput improvement at batch size 8 compared to batch size 1, with efficient scaling through batch size 8, while maintaining 95% output equivalence. Our method requires no custom kernels and integrates cleanly with existing inference stacks. Our code is available at https://github.com/eBay/spec_dec.

**Link**: [arxiv](http://arxiv.org/abs/2510.22876v1),  [pdf](http://arxiv.org/pdf/2510.22876v1)

**Tags**: cs.CL cs.AI 



### Jarvis: Towards Personalized AI Assistant via Personal KV-Cache   Retrieval
**Authors**: Binxiao Xu, Junyu Feng, Ruichuan An, Yulin Luo, Shilin Yan, Hao Liang, Ming Lu, Wentao Zhang

**Updated**: 2025-10-26T17:28:05Z

**Summary**: The rapid development of Vision-language models (VLMs) enables open-ended perception and reasoning. Recent works have started to investigate how to adapt general-purpose VLMs into personalized assistants. Even commercial models such as ChatGPT now support model personalization by incorporating user-specific information. However, existing methods either learn a set of concept tokens or train a VLM to utilize user-specific information. However, both pipelines struggle to generate accurate answers as personalized assistants. We introduce Jarvis, an innovative framework for a personalized AI assistant through personal KV-Cache retrieval, which stores user-specific information in the KV-Caches of both textual and visual tokens. The textual tokens are created by summarizing user information into metadata, while the visual tokens are produced by extracting distinct image patches from the user's images. When answering a question, Jarvis first retrieves related KV-Caches from personal storage and uses them to ensure accuracy in responses. We also introduce a fine-grained benchmark built with the same distinct image patch mining pipeline, emphasizing accurate question answering based on fine-grained user-specific information. Jarvis is capable of providing more accurate responses, particularly when they depend on specific local details. Jarvis achieves state-of-the-art results in both visual question answering and text-only tasks across multiple datasets, indicating a practical path toward personalized AI assistants. The code and dataset will be released.

**Link**: [arxiv](http://arxiv.org/abs/2510.22765v1),  [pdf](http://arxiv.org/pdf/2510.22765v1)

**Tags**: cs.AI 



### FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline
**Authors**: Jingwei Xu, Junbin Kang, Mingkai Dong, Mingyu Liu, Lu Zhang, Shaohong Guo, Ziyan Qiu, Mingzhen You, Ziyi Tian, Anqi Yu, Tianhong Ding, Xinwei Hu, Haibo Chen

**Updated**: 2025-10-26T13:31:41Z

**Summary**: Client-side metadata caching has long been considered an effective method for accelerating metadata operations in distributed file systems (DFSs). However, we have found that client-side state (e.g., caching) is not only ineffective but also consumes valuable memory resources in the deep learning pipelines. We thus propose FalconFS, a DFS optimized for deep learning pipelines with the stateless-client architecture. Specifically, instead of performing client-side path resolution and caching, FalconFS efficiently resolves paths on the server side using hybrid metadata indexing and lazy namespace replication. FalconFS also boosts server concurrency with concurrent request merging and provides easy deployment with VFS shortcut. Evaluations against CephFS and Lustre show that FalconFS achieves up to 5.72$\times$ throughput for small file read/write and up to 12.81$\times$ throughput for deep learning model training. FalconFS has been running in Huawei autonomous driving system's production environment with 10,000 NPUs for one year and has been open-sourced.

**Link**: [arxiv](http://arxiv.org/abs/2507.10367v3),  [pdf](http://arxiv.org/pdf/2507.10367v3)

**Tags**: cs.DC cs.PF 



### SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression   Block Size
**Authors**: Jinhan Chen, Jianchun Liu, Hongli Xu, Xianjun Gao, Shilong Wang

**Updated**: 2025-10-26T07:17:10Z

**Summary**: The growing memory footprint of the Key-Value (KV) cache poses a severe scalability bottleneck for long-context Large Language Model (LLM) inference. While KV cache eviction has emerged as an effective solution by discarding less critical tokens, existing token-, block-, and sentence-level compression methods struggle to balance semantic coherence and memory efficiency. To this end, we introduce SABlock, a \underline{s}emantic-aware KV cache eviction framework with \underline{a}daptive \underline{block} sizes. Specifically, SABlock first performs semantic segmentation to align compression boundaries with linguistic structures, then applies segment-guided token scoring to refine token importance estimation. Finally, for each segment, a budget-driven search strategy adaptively determines the optimal block size that preserves semantic integrity while improving compression efficiency under a given cache budget. Extensive experiments on long-context benchmarks demonstrate that SABlock consistently outperforms state-of-the-art baselines under the same memory budgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9% retrieval accuracy with only 96 KV entries, nearly matching the performance of the full-cache baseline that retains up to 8K entries. Under a fixed cache budget of 1,024, SABlock further reduces peak memory usage by 46.28% and achieves up to 9.5x faster decoding on a 128K context length.

**Link**: [arxiv](http://arxiv.org/abs/2510.22556v1),  [pdf](http://arxiv.org/pdf/2510.22556v1)

**Tags**: cs.CL 



### AttentionPredictor: Temporal Patterns Matter for KV Cache Compression
**Authors**: Qingyue Yang, Jie Wang, Xing Li, Zhihai Wang, Chen Chen, Lei Chen, Xianzhi Yu, Wulong Liu, Jianye Hao, Mingxuan Yuan, Bin Li

**Updated**: 2025-10-26T04:25:10Z

**Summary**: With the development of large language models (LLMs), efficient inference through Key-Value (KV) cache compression has attracted considerable attention, especially for long-context generation. To compress the KV cache, recent methods identify critical KV tokens through static modeling of attention scores. However, these methods often struggle to accurately determine critical tokens as they neglect the temporal patterns in attention scores, resulting in a noticeable degradation in LLM performance. To address this challenge, we propose AttentionPredictor, which is the first learning-based method to directly predict attention patterns for KV cache compression and critical token identification. Specifically, AttentionPredictor learns a lightweight, unified convolution model to dynamically capture spatiotemporal patterns and predict the next-token attention scores. An appealing feature of AttentionPredictor is that it accurately predicts the attention score and shares the unified prediction model, which consumes negligible memory, among all transformer layers. Moreover, we propose a cross-token critical cache prefetching framework that hides the token estimation time overhead to accelerate the decoding stage. By retaining most of the attention information, AttentionPredictor achieves 13$\times$ KV cache compression and 5.6$\times$ speedup in a cache offloading scenario with comparable LLM performance, significantly outperforming the state-of-the-arts. The code is available at https://github.com/MIRALab-USTC/LLM-AttentionPredictor.

**Link**: [arxiv](http://arxiv.org/abs/2502.04077v3),  [pdf](http://arxiv.org/pdf/2502.04077v3)

**Tags**: cs.CL cs.LG 



### A machine learning framework integrating seed traits and plasma   parameters for predicting germination uplift in crops
**Authors**: Saklain Niam, Tashfiqur Rahman, Md. Amjad Patwary, Mukarram Hossain

**Updated**: 2025-10-26T01:25:24Z

**Summary**: Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet outcomes remain difficult to predict due to complex seed--plasma--environment interactions. This study introduces the first machine learning framework to forecast germination uplift in soybean, barley, sunflower, radish, and tomato under dielectric barrier discharge (DBD) plasma. Among the models tested (GB, XGB, ET, and hybrids), Extra Trees (ET) performed best (R\textsuperscript{2} = 0.919; RMSE = 3.21; MAE = 2.62), improving to R\textsuperscript{2} = 0.925 after feature reduction. Engineering analysis revealed a hormetic response: negligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for 200--500 s, and reduced germination beyond 20 kV or prolonged exposures. Discharge power was also a dominant factor, with germination rate maximizing at $\geq$100 W with low exposure time. Species and cultivar-level predictions showed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high consistency, while sunflower remained slightly higher variable (MAE = 3.80). Among cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted, while Arian (2.86) and Ny\'{\i}rs\'{e}gi fekete (3.74) were comparatively poorly captured. This framework was also embedded into MLflow, providing a decision-support tool for optimizing CP seed germination in precision agriculture.

**Link**: [arxiv](http://arxiv.org/abs/2510.23657v1),  [pdf](http://arxiv.org/pdf/2510.23657v1)

**Tags**: cs.LG 



### Backward-Friendly Optimization: Training Large Language Models with   Approximate Gradients under Memory Constraints
**Authors**: Jing Yang, Kaitong Cai, Yijia Fan, Yufeng Yang, Keze Wang

**Updated**: 2025-10-26T00:50:12Z

**Summary**: Full fine-tuning of Large Language Models (LLMs) is notoriously memory-intensive, primarily because conventional optimizers such as SGD or Adam assume access to exact gradients derived from cached activations. Existing solutions either alter the model architecture (e.g., reversible networks) or trade memory for computation (e.g., activation checkpointing), but the optimizer itself remains untouched. In this work, we introduce GradLite, a backward-friendly optimizer that relaxes the requirement of exact gradients, enabling efficient training even when intermediate activations are aggressively discarded or approximated. GradLite leverages two key techniques: (i) low-rank Jacobian approximation, which reduces the dimensionality of backpropagated error signals, and (ii) error-feedback correction, which accumulates and compensates approximation errors across iterations to preserve convergence guarantees. We provide a theoretical analysis showing that GradLite maintains unbiased gradient estimates with bounded variance, ensuring convergence rates comparable to Adam. Empirically, GradLite reduces optimizer-state and activation memory consumption by up to 50\% without architectural changes, and achieves on-par or superior downstream performance on reasoning (MMLU, GSM8K), multilingual, and dialogue benchmarks compared to checkpointing and optimizer-centric baselines (LoMo, GaLore).

**Link**: [arxiv](http://arxiv.org/abs/2510.22467v1),  [pdf](http://arxiv.org/pdf/2510.22467v1)

**Tags**: cs.LG cs.AI 



### Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive   Token-Level Computation
**Authors**: Sangmin Bae, Yujin Kim, Reza Bayat, Sungnyun Kim, Jiyoun Ha, Tal Schuster, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Aaron Courville, Se-Young Yun

**Updated**: 2025-10-25T14:12:56Z

**Summary**: Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, we also propose a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to further decrease memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines. These gains demonstrate that MoR is an effective path towards large-model quality without incurring large-model cost.

**Link**: [arxiv](http://arxiv.org/abs/2507.10524v3),  [pdf](http://arxiv.org/pdf/2507.10524v3)

**Tags**: cs.CL cs.LG 



### Efficient Low Rank Attention for Long-Context Inference in Large   Language Models
**Authors**: Tenghui Li, Guoxu Zhou, Xuyang Zhao, Yuning Qiu, Qibin Zhao

**Updated**: 2025-10-25T11:43:27Z

**Summary**: As the length of input text grows, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long-context inference on resource constrained devices. Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or suboptimal retention of key-value pairs. We introduce Low Rank Query and Key attention (LRQK), a two-stage framework that jointly decomposes the full-precision query and key matrices into compact rank-\(r\) factors during the prefill stage, and then uses these low-dimensional projections to compute proxy attention scores in \(\mathcal{O}(lr)\) time at each decode step. By selecting only the top-\(k\) tokens and a small fixed set of recent tokens, LRQK employs a mixed GPU-CPU cache with a hit-and-miss mechanism that transfers only missing full-precision KV pairs, thereby preserving exact attention outputs while reducing CPU-GPU data movement. Extensive experiments on the RULER and LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK matches or surpasses leading sparse-attention methods in long context settings, while delivering significant memory savings with minimal loss in accuracy. Our code is available at https://github.com/tenghuilee/LRQK.

**Link**: [arxiv](http://arxiv.org/abs/2510.23649v1),  [pdf](http://arxiv.org/pdf/2510.23649v1)

**Tags**: cs.LG cs.AI 



### Fundamental Limits of Coded Caching with Fixed Subpacketization
**Authors**: Minquan Cheng, Yifei Huang, Youlong Wu, Jinyan Wang

**Updated**: 2025-10-25T03:34:34Z

**Summary**: Coded caching is a promising technique to create coded multicast opportunities for cache-aided networks. By splitting each file into $F$ equal packets (i.e., the subpacketization level $F$) and letting each user cache a set of packets, the transmission load can be significantly reduced via coded multicasting. It has been shown that a higher subpacketization level could potentially lead to a lower transmission load, as more packets can be combined for efficient transmission. On the other hand, a larger $F$ indicates a higher coding complexity and is problematic from a practical perspective when $F$ is extremely large. Despite many works attempting to design coded caching schemes with low subpacketization levels, a fundamental problem remains open: What is the minimum transmission load given any fixed subpacketization level? In this paper, we consider the classical cache-aided networks with identically uncoded placement and one-shot delivery strategy, and investigate the fundamental trade-off between the transmission load and the subpacketization level. We propose a \emph{general} lower bound on the transmission load for any fixed subpacketization by reformulating the centralized coded caching schemes via the combinatorial structure of the corresponding placement delivery array. The lower bound also recovers existing optimality results for the bipartite graph scheme (including the well-known Maddah-Ali and Niesen (MN) scheme and the conjugate MN scheme) as well as the grouping bipartite graph scheme. Furthermore, by carefully exploiting the combinatorial structure and computing the union size of sorted sets, we establish a new optimality result, i.e., the partition scheme can achieve the optimal rate-subpacketization trade-off.

**Link**: [arxiv](http://arxiv.org/abs/2510.22145v1),  [pdf](http://arxiv.org/pdf/2510.22145v1)

**Tags**: cs.IT math.IT 



### EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient   Image Editing
**Authors**: Zexuan Yan, Yue Ma, Chang Zou, Wenteng Chen, Qifeng Chen, Linfeng Zhang

**Updated**: 2025-10-25T02:29:47Z

**Summary**: Inversion-based image editing is rapidly gaining momentum while suffering from significant computation overhead, hindering its application in real-time interactive scenarios. In this paper, we rethink that the redundancy in inversion-based image editing exists in both the spatial and temporal dimensions, such as the unnecessary computation in unedited regions and the redundancy in the inversion progress. To tackle these challenges, we propose a practical framework, named EEdit, to achieve efficient image editing. Specifically, we introduce three techniques to solve them one by one. For spatial redundancy, spatial locality caching is introduced to compute the edited region and its neighboring regions while skipping the unedited regions, and token indexing preprocessing is designed to further accelerate the caching. For temporal redundancy, inversion step skipping is proposed to reuse the latent for efficient editing. Our experiments demonstrate an average of 2.46 $\times$ acceleration without performance drop in a wide range of editing tasks including prompt-guided image editing, dragging and image composition. Our codes are available at https://github.com/yuriYanZeXuan/EEdit

**Link**: [arxiv](http://arxiv.org/abs/2503.10270v3),  [pdf](http://arxiv.org/pdf/2503.10270v3)

**Tags**: cs.CV 



### Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning
**Authors**: Chaofan Lin, Jiaming Tang, Shuo Yang, Hanshuo Wang, Tian Tang, Boyu Tian, Ion Stoica, Mingyu Gao

**Updated**: 2025-10-25T00:33:14Z

**Summary**: Leveraging attention sparsity to accelerate long-context large language models (LLMs) has been a hot research topic. However, current algorithms such as sparse attention or key-value (KV) cache compression tend to use a fixed budget, which presents a significant challenge during deployment because it fails to account for the dynamic nature of real-world scenarios, where the optimal balance between accuracy and efficiency can vary greatly. In this paper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse attention can surprisingly achieve adaptive budgeting. Based on this, we propose Twilight, a framework to bring adaptive sparsity to any existing sparse attention algorithm without sacrificing their accuracy. Empirical results show that Twilight can adaptively prune at most 98% of redundant tokens, leading to $15.4\times$ acceleration in self-attention operations and $3.9\times$ acceleration in end-to-end per token latency in long context LLM decoding.

**Link**: [arxiv](http://arxiv.org/abs/2502.02770v4),  [pdf](http://arxiv.org/pdf/2502.02770v4)

**Tags**: cs.LG cs.CL 



### Massive Memorization with Hundreds of Trillions of Parameters for   Sequential Transducer Generative Recommenders
**Authors**: Zhimin Chen, Chenyu Zhao, Ka Chun Mo, Yunjiang Jiang, Jane H. Lee, Shouwei Chen, Khushhall Chandra Mahajan, Ning Jiang, Kai Ren, Jinhui Li, Wen-Yun Yang

**Updated**: 2025-10-24T22:17:49Z

**Summary**: Modern large-scale recommendation systems rely heavily on user interaction history sequences to enhance the model performance. The advent of large language models and sequential modeling techniques, particularly transformer-like architectures, has led to significant advancements recently (e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories (10k to 100k items) generally improves model performance, it also creates significant challenges on latency, queries per second (QPS) and GPU cost in industry-scale recommendation systems. Existing models do not adequately address these industrial scalability issues. In this paper, we propose a novel two-stage modeling framework, namely VIrtual Sequential Target Attention (VISTA), which decomposes traditional target attention from a candidate item to user history items into two distinct stages: (1) user history summarization into a few hundred tokens; followed by (2) candidate item attention to those tokens. These summarization token embeddings are then cached in storage system and then utilized as sequence features for downstream model training and inference. This novel design for scalability enables VISTA to scale to lifelong user histories (up to one million items) while keeping downstream training and inference costs fixed, which is essential in industry. Our approach achieves significant improvements in offline and online metrics and has been successfully deployed on an industry leading recommendation platform serving billions of users.

**Link**: [arxiv](http://arxiv.org/abs/2510.22049v1),  [pdf](http://arxiv.org/pdf/2510.22049v1)

**Tags**: cs.IR cs.LG 



### BachVid: Training-Free Video Generation with Consistent Background and   Character
**Authors**: Han Yan, Xibin Song, Yifu Wang, Hongdong Li, Pan Ji, Chao Ma

**Updated**: 2025-10-24T17:56:37Z

**Summary**: Diffusion Transformers (DiTs) have recently driven significant progress in text-to-video (T2V) generation. However, generating multiple videos with consistent characters and backgrounds remains a significant challenge. Existing methods typically rely on reference images or extensive training, and often only address character consistency, leaving background consistency to image-to-video models. We introduce BachVid, the first training-free method that achieves consistent video generation without needing any reference images. Our approach is based on a systematic analysis of DiT's attention mechanism and intermediate features, revealing its ability to extract foreground masks and identify matching points during the denoising process. Our method leverages this finding by first generating an identity video and caching the intermediate variables, and then inject these cached variables into corresponding positions in newly generated videos, ensuring both foreground and background consistency across multiple videos. Experimental results demonstrate that BachVid achieves robust consistency in generated videos without requiring additional training, offering a novel and efficient solution for consistent video generation without relying on reference images or additional training.

**Link**: [arxiv](http://arxiv.org/abs/2510.21696v1),  [pdf](http://arxiv.org/pdf/2510.21696v1)

**Tags**: cs.CV 



### Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention   and Contextualized Learnable Token Eviction
**Authors**: Mutian He, Philip N. Garner

**Updated**: 2025-10-24T16:56:22Z

**Summary**: Linear-attention models that compress the entire input sequence into a fixed-size recurrent state offer an efficient alternative to Transformers, but their finite memory induces forgetfulness that harms retrieval-intensive tasks. To mitigate the issue, we explore a series of hybrid models that restore direct access to past tokens. We interleave token mixers with intermediate time and space complexity between linear and full attention, including sparse attention with token eviction, and the query-aware native sparse attention. Particularly, we propose a novel learnable token eviction approach. Combined with sliding-window attention, an end-to-end trainable lightweight CNN aggregates information from both past and future adjacent tokens to adaptively retain a limited set of critical KV-pairs per head, maintaining linear attention's constant time and space complexity. Efficient Triton kernels for the sparse attention mechanisms are provided. Empirical evaluations on retrieval-intensive benchmarks support the effectiveness of our approaches.

**Link**: [arxiv](http://arxiv.org/abs/2510.20787v2),  [pdf](http://arxiv.org/pdf/2510.20787v2)

**Tags**: cs.CL cs.LG 



### Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory
**Authors**: Manel Lurbe, Miguel Avargues, Salvador Petit, Maria E. Gomez, Rui Yang, Guanhao Wang, Julio Sahuquillo

**Updated**: 2025-10-24T14:55:42Z

**Summary**: Emerging applications, such as big data analytics and machine learning, require increasingly large amounts of main memory, often exceeding the capacity of current commodity processors built on DRAM technology. To address this, recent research has focused on off-chip memory controllers that facilitate access to diverse memory media, each with unique density and latency characteristics. While these solutions improve memory system performance, they also exacerbate the already significant memory latency. As a result, multi-level prefetching techniques are essential to mitigate these extended latencies.   This paper investigates the advantages of prefetching across both sides of the memory system: the off-chip memory and the on-chip cache hierarchy. Our primary objective is to assess the impact of a multi-level prefetching engine on overall system performance. Additionally, we analyze the individual contribution of each prefetching level to system efficiency. To achieve this, the study evaluates two key prefetching approaches: HMC (Hybrid Memory Controller) and HMC+L1, both of which employ prefetching mechanisms commonly used by processor vendors. The HMC approach integrates a prefetcher within the off-chip hybrid memory controller, while the HMC+L1 approach combines this with additional L1 on-chip prefetchers.   Experimental results on an out-of-order execution processor show that on-chip cache prefetchers are crucial for maximizing the benefits of off-chip prefetching, which in turn further enhances performance. Specifically, the off-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and up to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher coverage to as much as 92%. Consequently, overall performance increases from 9% with the HMC approach to 12% when L1 prefetching is also employed.

**Link**: [arxiv](http://arxiv.org/abs/2509.17388v2),  [pdf](http://arxiv.org/pdf/2509.17388v2)

**Tags**: cs.DC 



### Global Predecessor Indexing: Avoiding Binary Search in Weighted Job   Scheduling
**Authors**: Amit Joshi

**Updated**: 2025-10-24T11:53:34Z

**Summary**: We present an improved solution to the Weighted Job Scheduling (WJS) problem. While the classical dynamic programming (DP) solution for $n$ jobs runs in $O(n \log(n))$ time due to comparison-based sorting and per-job binary search, we eliminate the binary search bottleneck. In its place, we introduce a novel multi-phase preprocessing technique called \emph{Global Predecessor Indexing (GPI)}, which computes the latest non-overlapping job (i.e., the predecessor) for all jobs via a two-pointer linear-time pass after sorting. This yields a time complexity of $O(S(n) + n)$ where $S(n)$ is the time to sort all jobs. GPI enables direct use in the classical DP recurrence. When combined with linear-time sorting, GPI yields a complete $O(n)$ solution. Even with comparison-based sorting, GPI significantly outperforms the classical solution in practice by avoiding repeated binary searches in favor of the more cache-efficient extra sort and two-pointer pass.

**Link**: [arxiv](http://arxiv.org/abs/2506.22922v2),  [pdf](http://arxiv.org/pdf/2506.22922v2)

**Tags**: cs.DS 



### Compositional Monte Carlo Tree Diffusion for Extendable Planning
**Authors**: Jaesik Yoon, Hyeonseo Cho, Sungjin Ahn

**Updated**: 2025-10-24T11:42:38Z

**Summary**: Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured tree search to enable effective trajectory exploration through stepwise reasoning. However, MCTD remains fundamentally limited by training trajectory lengths. While periodic replanning allows plan concatenation for longer plan generation, the planning process remains locally confined, as MCTD searches within individual trajectories without access to global context. We propose Compositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates planning from individual trajectory optimization to reasoning over complete plan compositions. C-MCTD introduces three complementary components: (1) Online Composer, which performs globally-aware planning by searching across entire plan compositions; (2) Distributed Composer, which reduces search complexity through parallel exploration from multiple starting points; and (3) Preplan Composer, which accelerates inference by leveraging cached plan graphs.

**Link**: [arxiv](http://arxiv.org/abs/2510.21361v1),  [pdf](http://arxiv.org/pdf/2510.21361v1)

**Tags**: cs.LG 



### Robustifying Learning-Augmented Caching Efficiently without Compromising   1-Consistency
**Authors**: Peng Chen, Hailiang Zhao, Jiaji Zhang, Xueyan Tang, Yixuan Wang, Shuiguang Deng

**Updated**: 2025-10-24T08:41:38Z

**Summary**: The online caching problem aims to minimize cache misses when serving a sequence of requests under a limited cache size. While naive learning-augmented caching algorithms achieve ideal $1$-consistency, they lack robustness guarantees. Existing robustification methods either sacrifice $1$-consistency or introduce excessive computational overhead. In this paper, we introduce Guard, a lightweight robustification framework that enhances the robustness of a broad class of learning-augmented caching algorithms to $2H_k + 2$, while preserving their $1$-consistency. Guard achieves the current best-known trade-off between consistency and robustness, with only $O(1)$ additional per-request overhead, thereby maintaining the original time complexity of the base algorithm. Extensive experiments across multiple real-world datasets and prediction models validate the effectiveness of Guard in practice.

**Link**: [arxiv](http://arxiv.org/abs/2507.16242v6),  [pdf](http://arxiv.org/pdf/2507.16242v6)

**Tags**: cs.DS cs.LG 



### A General Solution for the Implementation of CI/CD in Embedded Linux   Development
**Authors**: Behnam Agahi, Hamed Farbeh

**Updated**: 2025-10-24T08:35:21Z

**Summary**: With the growing use of embedded systems in various industries, the need for automated platforms for the development and deployment of customized Linux-based operating systems has become more important. This research was conducted with the aim of designing and implementing an integrated and reproducible infrastructure for the development, building, and testing of a Linux-based operating system using the Yocto Project. The proposed structure was implemented based on a three-layer architecture consisting of the main Yocto repositories, a custom layer (meta-custom), and a coordinating manifest layer to ensure version synchronization, scalability, and reproducibility. Three sample projects, including libhelloworld, helloworld, and the kernel module hello mod, were developed and integrated into the build process. Continuous Integration and Continuous Deployment pipelines were implemented with GitLab CI and combined with an isolated Docker environment to automate and streamline the build and testing workflows. Using a local cache server containing hashserv, downloads and sstate cache significantly reduced the build time. The functionality and stability of the system were verified through six boot test scenarios in the QEMU simulator. The results show that the proposed design not only ensures reproducibility but also can be extended to advanced applications such as continuous deployment of real-time Linux versions. Future recommendations include expanding automated tests, implementing system monitoring with Prometheus and Grafana, using distributed builds, optimizing with Docker multi-stage builds, and enabling continuous deployment of real-time Linux changes to provide a stable and scalable model for industrial and research projects in embedded systems with a rapid and reliable development cycle.

**Link**: [arxiv](http://arxiv.org/abs/2510.19240v2),  [pdf](http://arxiv.org/pdf/2510.19240v2)

**Tags**: cs.SE 



### InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video   Understanding
**Authors**: Minsoo Kim, Kyuhong Shim, Jungwook Choi, Simyung Chang

**Updated**: 2025-10-24T05:39:03Z

**Summary**: Modern multimodal large language models (MLLMs) can reason over hour-long video, yet their key-value (KV) cache grows linearly with time-quickly exceeding the fixed memory of phones, AR glasses, and edge robots. Prior compression schemes either assume the whole video and user query are available offline or must first build the full cache, so memory still scales with stream length. InfiniPot-V is the first training-free, query-agnostic framework that enforces a hard, length-independent memory cap for streaming video understanding. During video encoding it monitors the cache and, once a user-set threshold is reached, runs a lightweight compression pass that (i) removes temporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii) keeps semantically significant tokens via Value-Norm (VaN) ranking. Across four open-source MLLMs and four long-video and streaming-video benchmarks, InfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation, and matches or surpasses full-cache accuracy-even in multi-turn dialogues. By dissolving the KV cache bottleneck without retraining or query knowledge, InfiniPot-V closes the gap for on-device streaming video assistants.

**Link**: [arxiv](http://arxiv.org/abs/2506.15745v2),  [pdf](http://arxiv.org/pdf/2506.15745v2)

**Tags**: eess.IV cs.LG 



### Reasoning Path Compression: Compressing Generation Trajectories for   Efficient LLM Reasoning
**Authors**: Jiwon Song, Dongwon Jo, Yulhwa Kim, Jae-Joon Kim

**Updated**: 2025-10-24T04:48:06Z

**Summary**: Recent reasoning-focused language models achieve high accuracy by generating lengthy intermediate reasoning paths before producing final answers. While this approach is effective in solving problems that require logical thinking, long reasoning paths significantly increase memory usage and reduce throughput of token generation, limiting the practical deployment of such models. We propose Reasoning Path Compression (RPC), a training-free method that accelerates inference by leveraging the semantic sparsity of reasoning paths. RPC periodically compresses the KV cache by retaining cache entries that receive high importance score, which are computed using a selector window composed of recently generated queries. Experiments show that RPC improves generation throughput of QwQ-32B by up to 1.60$\times$ compared to the inference with full KV cache, with an accuracy drop of 1.2\% on the AIME 2024 benchmark. Our findings demonstrate that semantic sparsity in reasoning traces can be effectively exploited for compression, offering a practical path toward efficient deployment of reasoning LLMs. Our code is available at https://github.com/jiwonsong-dev/ReasoningPathCompression.

**Link**: [arxiv](http://arxiv.org/abs/2505.13866v2),  [pdf](http://arxiv.org/pdf/2505.13866v2)

**Tags**: cs.CL 



### Tensor Product Attention Is All You Need
**Authors**: Yifan Zhang, Yifeng Liu, Huizhuo Yuan, Zhen Qin, Yang Yuan, Quanquan Gu, Andrew Chi-Chih Yao

**Updated**: 2025-10-23T23:35:32Z

**Summary**: Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, substantially shrinking the KV cache size at inference time. By factorizing these representations into contextual low-rank components and seamlessly integrating with Rotary Position Embedding (RoPE), TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model architecture for sequence modeling. Through extensive empirical evaluation on language modeling tasks, we demonstrate that T6 surpasses or matches the performance of standard Transformer baselines including Multi-Head Attention (MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and Multi-Head Latent Attention (MLA) across various metrics, including perplexity and a range of established evaluation benchmarks. Notably, TPA's memory efficiency and computational efficiency at decoding stage enables processing longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. Project Page: https://github.com/tensorgi/TPA.

**Link**: [arxiv](http://arxiv.org/abs/2501.06425v5),  [pdf](http://arxiv.org/pdf/2501.06425v5)

**Tags**: cs.CL cs.AI cs.LG 



### T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic   Planning
**Authors**: Amartya Chakraborty, Paresh Dashore, Nadia Bathaee, Anmol Jain, Anirban Das, Shi-Xiong Zhang, Sambit Sahu, Milind Naphade, Genta Indra Winata

**Updated**: 2025-10-23T21:31:35Z

**Summary**: Large Language Models (LLMs) have demonstrated impressive capabilities as intelligent agents capable of solving complex problems. However, effective planning in scenarios involving dependencies between API or tool calls-particularly in multi-turn conversations-remains a significant challenge. To address this, we introduce T1, a tool-augmented, multi-domain, multi-turn conversational dataset specifically designed to capture and manage inter-tool dependencies across diverse domains. T1 enables rigorous evaluation of agents' ability to coordinate tool use across nine distinct domains (4 single domain and 5 multi-domain) with the help of an integrated caching mechanism for both short- and long-term memory, while supporting dynamic replanning-such as deciding whether to recompute or reuse cached results. Beyond facilitating research on tool use and planning, T1 also serves as a benchmark for evaluating the performance of open-weight and proprietary large language models. We present results powered by T1-Agent, highlighting their ability to plan and reason in complex, tool-dependent scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2505.16986v2),  [pdf](http://arxiv.org/pdf/2505.16986v2)

**Tags**: cs.CL cs.AI 



### Panorama: Fast-Track Nearest Neighbors
**Authors**: Vansh Ramani, Alexis Schlomer, Akash Nayar, Sayan Ranu, Jignesh M. Patel, Panagiotis Karras

**Updated**: 2025-10-23T19:45:39Z

**Summary**: Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose embeddings are close to that of a given query in a high-dimensional space, aiming to balance accuracy with speed. Used in recommendation systems, image and video retrieval, natural language processing, and retrieval-augmented generation (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT utilize graph, tree, clustering, and quantization techniques to navigate large vector spaces. Despite this progress, ANNS systems spend up to 99% of query time to compute distances in their final refinement phase. In this paper, we present PANORAMA, a machine learning-driven approach that tackles the ANNS verification bottleneck through data-adaptive learned orthogonal transforms that facilitate the accretive refinement of distance bounds. Such transforms compact over 90% of signal energy into the first half of dimensions, enabling early candidate pruning with partial distance computations. We integrate PANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and Annoy, without index modification, using level-major memory layouts, SIMD-vectorized partial distance computations, and cache-aware access patterns. Experiments across diverse datasets -- from image-based CIFAR-10 and GIST to modern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate that PANORAMA affords a 2--30$\times$ end-to-end speedup with no recall loss.

**Link**: [arxiv](http://arxiv.org/abs/2510.00566v3),  [pdf](http://arxiv.org/pdf/2510.00566v3)

**Tags**: cs.LG cs.AI cs.DB 



### A Stable Whitening Optimizer for Efficient Neural Network Training
**Authors**: Kevin Frans, Sergey Levine, Pieter Abbeel

**Updated**: 2025-10-23T18:52:25Z

**Summary**: In this work, we take an experimentally grounded look at neural network optimization. Building on the Shampoo family of algorithms, we identify and alleviate three key issues, resulting in the proposed SPlus method. First, we find that naive Shampoo is prone to divergence when matrix-inverses are cached for long periods. We introduce an alternate bounded update combining a historical eigenbasis with instantaneous normalization, resulting in across-the-board stability and significantly lower computational requirements. Second, we adapt a shape-aware scaling to enable learning rate transfer across network width. Third, we find that high learning rates result in large parameter noise, and propose a simple iterate-averaging scheme which unblocks faster learning. To properly confirm these findings, we introduce a pointed Transformer training benchmark, considering three objectives (language modelling, image classification, and diffusion modelling) across different stages of training. On average, SPlus is able to reach the validation performance of Adam within 44-58% of the gradient steps and 62-83% of the wallclock time.

**Link**: [arxiv](http://arxiv.org/abs/2506.07254v3),  [pdf](http://arxiv.org/pdf/2506.07254v3)

**Tags**: cs.LG 



### Mixing Importance with Diversity: Joint Optimization for KV Cache   Compression in Large Vision-Language Models
**Authors**: Xuyang Liu, Xiyan Gui, Yuchao Zhang, Linfeng Zhang

**Updated**: 2025-10-23T16:17:47Z

**Summary**: Recent large vision-language models (LVLMs) demonstrate remarkable capabilities in processing extended multi-modal sequences, yet the resulting key-value (KV) cache expansion creates a critical memory bottleneck that fundamentally limits deployment scalability. While existing KV cache compression methods focus on retaining high-importance KV pairs to minimize storage, they often overlook the modality-specific semantic redundancy patterns that emerge distinctively in multi-modal KV caches. In this work, we first analyze how, beyond simple importance, the KV cache in LVLMs exhibits varying levels of redundancy across attention heads. We show that relying solely on importance can only cover a subset of the full KV cache information distribution, leading to potential loss of semantic coverage. To address this, we propose \texttt{MixKV}, a novel method that mixes importance with diversity for optimized KV cache compression in LVLMs. \texttt{MixKV} adapts to head-wise semantic redundancy, selectively balancing diversity and importance when compressing KV pairs. Extensive experiments demonstrate that \texttt{MixKV} consistently enhances existing methods across multiple LVLMs. Under extreme compression (budget=64), \texttt{MixKV} improves baseline methods by an average of \textbf{5.1\%} across five multi-modal understanding benchmarks and achieves remarkable gains of \textbf{8.0\%} and \textbf{9.0\%} for SnapKV and AdaKV on GUI grounding tasks, all while maintaining comparable inference efficiency. Furthermore, \texttt{MixKV} extends seamlessly to LLMs with comparable performance gains. Our code is available at \href{https://github.com/xuyang-liu16/MixKV}{\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}.

**Link**: [arxiv](http://arxiv.org/abs/2510.20707v1),  [pdf](http://arxiv.org/pdf/2510.20707v1)

**Tags**: cs.CV 



### WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables
**Authors**: Hunter McCoy, Prashant Pandey

**Updated**: 2025-10-23T15:26:38Z

**Summary**: GPU hash tables are increasingly used to accelerate data processing, but their limited functionality restricts adoption in large-scale data processing applications. Current limitations include incomplete concurrency support and missing compound operations such as upserts.   This paper presents WarpSpeed, a library of high-performance concurrent GPU hash tables with a unified benchmarking framework for performance analysis. WarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and provides a rich API designed for modern GPU applications. Our evaluation uses diverse benchmarks to assess both correctness and scalability, and we demonstrate real-world impact by integrating these hash tables into three downstream applications.   We propose several optimization techniques to reduce concurrency overhead, including fingerprint-based metadata to minimize cache line probes and specialized Nvidia GPU instructions for lock-free queries. Our findings provide new insights into concurrent GPU hash table design and offer practical guidance for developing efficient, scalable data structures on modern GPUs.

**Link**: [arxiv](http://arxiv.org/abs/2509.16407v2),  [pdf](http://arxiv.org/pdf/2509.16407v2)

**Tags**: cs.DC cs.DS 



### Neural Attention Search
**Authors**: Difan Deng, Marius Lindauer

**Updated**: 2025-10-23T14:23:24Z

**Summary**: We present Neural Attention Search (NAtS), a framework that automatically evaluates the importance of each token within a sequence and determines if the corresponding token can be dropped after several steps. This approach can efficiently reduce the KV cache sizes required by transformer-based models during inference and thus reduce inference costs. In this paper, we design a search space that contains three token types: (i) Global Tokens will be preserved and queried by all the following tokens. (ii) Local Tokens survive until the next global token appears. (iii) Sliding Window Tokens have an impact on the inference of a fixed size of the next following tokens. Similar to the One-Shot Neural Architecture Search approach, this token-type information can be learned jointly with the architecture weights via a learnable attention mask. Experiments on both training a new transformer from scratch and fine-tuning existing large language models show that NAtS can efficiently reduce the KV cache size required for the models while maintaining the models' performance.

**Link**: [arxiv](http://arxiv.org/abs/2502.13251v4),  [pdf](http://arxiv.org/pdf/2502.13251v4)

**Tags**: cs.CL cs.AI 



### GPU-Accelerated Primal Heuristics for Mixed Integer Programming
**Authors**: Akif ÃÃ¶rdÃ¼k, Piotr Sielski, Alice Boucher, Kumar Aatish

**Updated**: 2025-10-23T12:39:59Z

**Summary**: We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer Programming. Leveraging GPU acceleration enables exploration of larger search regions and faster iterations. A GPU-accelerated PDLP serves as an approximate LP solver, while a new probing cache facilitates rapid roundings and early infeasibility detection. Several state-of-the-art heuristics, including Feasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further accelerated and enhanced. The combined approach of these GPU-driven algorithms yields significant improvements over existing methods, both in the number of feasible solutions and the quality of objectives by achieving 221 feasible solutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved dataset.

**Link**: [arxiv](http://arxiv.org/abs/2510.20499v1),  [pdf](http://arxiv.org/pdf/2510.20499v1)

**Tags**: math.OC cs.DC 



### HA-RAG: Hotness-Aware RAG Acceleration via Mixed Precision and Data   Placement
**Authors**: Danying Ge, Jianhua Gao, Yixue Yang, Weixing Ji

**Updated**: 2025-10-23T12:28:58Z

**Summary**: Retrieval-Augmented Generation (RAG) improves model output accuracy by leveraging external knowledge bases, serving as an effective solution to address hallucination issues and knowledge-update delays in Large Language Models (LLMs). However, the introduction of external knowledge bases presents RAG with challenges in long-context processing, significantly increasing memory consumption and inference latency. Existing research accelerates inference by precomputing Key and Value (KV) of the knowledge base and loading them on-demand during inference. Based on the access frequency of different KV chunks within the external knowledge base, this paper proposes a hotness-aware RAG (HA-RAG) inference optimization system. First, leveraging the numerical distribution of KV chunks, we introduce a hotness-aware mixed-precision compressing and loading method to reduce disk I/O and memory access overhead. Second, we design a hotness-aware data placement strategy that prioritizes storing frequently accessed KV chunks in high-speed memory to improve data access efficiency. Experimental results demonstrate that, compared with TurboRAG, the proposed HA-RAG achieves an average speedup of 2.10x and maximum speedup of 10.49x in Time-To-First-Token (TTFT) with negligible accuracy loss.

**Link**: [arxiv](http://arxiv.org/abs/2510.20878v1),  [pdf](http://arxiv.org/pdf/2510.20878v1)

**Tags**: cs.LG cs.AI C.4; E.4; I.2 



### Prefetching Cache Optimization Using Graph Neural Networks: A Modular   Framework and Conceptual Analysis
**Authors**: F. I. Qowy

**Updated**: 2025-10-23T10:35:35Z

**Summary**: Caching and prefetching techniques are fundamental to modern computing, serving to bridge the growing performance gap between processors and memory. Traditional prefetching strategies are often limited by their reliance on predefined heuristics or simplified statistical models, which fail to capture the complex, non-linear dependencies in modern data access patterns. This paper introduces a modular framework leveraging Graph Neural Networks (GNNs) to model and predict access patterns within graph-structured data, focusing on web navigation and hierarchical file systems. The toolchain consists of: a route mapper for extracting structural information, a graph constructor for creating graph representations, a walk session generator for simulating user behaviors, and a gnn prefetch module for training and inference. We provide a detailed conceptual analysis showing how GNN-based approaches can outperform conventional methods by learning intricate dependencies. This work offers both theoretical foundations and a practical, replicable pipeline for future research in graph-driven systems optimization.

**Link**: [arxiv](http://arxiv.org/abs/2510.21865v1),  [pdf](http://arxiv.org/pdf/2510.21865v1)

**Tags**: cs.PF cs.LG cs.SE 



### Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism   on Dependency-Bound Kernels
**Authors**: RubÃ©n Langarita, JesÃºs Alastruey-BenedÃ©, Pablo IbÃ¡Ã±ez-MarÃ­n, Santiago Marco-Sola, Miquel MoretÃ³, AdriÃ  Armejach

**Updated**: 2025-10-23T10:06:48Z

**Summary**: Multiple HPC applications are often bottlenecked by compute-intensive kernels implementing complex dependency patterns (data-dependency bound). Traditional general-purpose accelerators struggle to effectively exploit fine-grain parallelism due to limitations in implementing convoluted data-dependency patterns (like SIMD) and overheads due to synchronization and data transfers (like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved performance and energy efficiency at a high cost in hardware design and programming complexity and often lack the flexibility to process different workloads. We propose Squire, a general-purpose accelerator designed to exploit fine-grain parallelism effectively on dependency-bound kernels. Each Squire accelerator has a set of general-purpose low-power in-order cores that can rapidly communicate among themselves and directly access data from the L2 cache. Our proposal integrates one Squire accelerator per core in a typical multicore system, allowing the acceleration of dependency-bound kernels within parallel tasks with minimal software changes. As a case study, we evaluate Squire's effectiveness by accelerating five kernels that implement complex dependency patterns. We use three of these kernels to build an end-to-end read-mapping tool that will be used to evaluate Squire. Squire obtains speedups up to 7.64$\times$ in dynamic programming kernels. Overall, Squire provides an acceleration for an end-to-end application of 3.66$\times$. In addition, Squire reduces energy consumption by up to 56% with a minimal area overhead of 10.5% compared to a Neoverse-N1 baseline.

**Link**: [arxiv](http://arxiv.org/abs/2510.20400v1),  [pdf](http://arxiv.org/pdf/2510.20400v1)

**Tags**: cs.AR 



### Bi-Mamba: Towards Accurate 1-Bit State Space Models
**Authors**: Shengkun Tang, Liqun Ma, Haonan Li, Mingjie Sun, Zhiqiang Shen

**Updated**: 2025-10-23T09:55:50Z

**Summary**: The typical Selective State-Space Model (SSM) used in Mamba addresses several limitations of Transformers, such as the quadratic computational complexity with respect to sequence length and the significant memory requirements during inference due to the key-value (KV) cache. However, the increasing size of Mamba models continues to pose challenges for training and deployment, particularly due to their substantial computational demands during both training and inference. In this work, we introduce $\texttt{Bi-Mamba}$, a scalable and powerful 1-bit Mamba architecture designed to enable more efficient large language models (LLMs), with model sizes of 780M, 1.3B, and 2.7B parameters. $\texttt{Bi-Mamba}$ models are trained from scratch on a standard LLM-scale dataset using an autoregressive distillation loss. Extensive experiments on language modeling benchmarks demonstrate that $\texttt{Bi-Mamba}$ achieves performance comparable to its full-precision (FP16 or BF16) counterparts, while outperforming post-training binarization (PTB) Mamba and binarization-aware training (BAT) Transformer baselines. Moreover, $\texttt{Bi-Mamba}$ drastically reduces memory usage and computational cost compared to the original Mamba. Our work pioneers a new line of linear-complexity LLMs under low-bit representation and provides the way for the design of specialized hardware optimized for efficient 1-bit Mamba-based models. Code and the pre-trained weights are available at https://github.com/Tangshengku/Bi-Mamba.

**Link**: [arxiv](http://arxiv.org/abs/2411.11843v2),  [pdf](http://arxiv.org/pdf/2411.11843v2)

**Tags**: cs.CL cs.AI 



### A Survey on Cache Methods in Diffusion Models: Toward Efficient   Multi-Modal Generation
**Authors**: Jiacheng Liu, Xinyu Wang, Yuqi Lin, Zhikai Wang, Peiru Wang, Peiliang Cai, Qinming Zhou, Zhengan Yan, Zexuan Yan, Zhengyi Shi, Chang Zou, Yue Ma, Linfeng Zhang

**Updated**: 2025-10-23T09:09:15Z

**Summary**: Diffusion Models have become a cornerstone of modern generative AI for their exceptional generation quality and controllability. However, their inherent \textit{multi-step iterations} and \textit{complex backbone networks} lead to prohibitive computational overhead and generation latency, forming a major bottleneck for real-time applications. Although existing acceleration techniques have made progress, they still face challenges such as limited applicability, high training costs, or quality degradation.   Against this backdrop, \textbf{Diffusion Caching} offers a promising training-free, architecture-agnostic, and efficient inference paradigm. Its core mechanism identifies and reuses intrinsic computational redundancies in the diffusion process. By enabling feature-level cross-step reuse and inter-layer scheduling, it reduces computation without modifying model parameters. This paper systematically reviews the theoretical foundations and evolution of Diffusion Caching and proposes a unified framework for its classification and analysis.   Through comparative analysis of representative methods, we show that Diffusion Caching evolves from \textit{static reuse} to \textit{dynamic prediction}. This trend enhances caching flexibility across diverse tasks and enables integration with other acceleration techniques such as sampling optimization and model distillation, paving the way for a unified, efficient inference framework for future multimodal and interactive applications. We argue that this paradigm will become a key enabler of real-time and efficient generative AI, injecting new vitality into both theory and practice of \textit{Efficient Generative Intelligence}.

**Link**: [arxiv](http://arxiv.org/abs/2510.19755v2),  [pdf](http://arxiv.org/pdf/2510.19755v2)

**Tags**: cs.LG cs.AI cs.CV 



### Improving Model Representation and Reducing KV Cache via Skip   Connections with First Value Heads
**Authors**: Zhoutong Wu, Yuan Zhang, Yiming Dong, Chenheng Zhang, Cong Fang, Kun Yuan, Zhouchen Lin

**Updated**: 2025-10-23T08:29:11Z

**Summary**: Transformer models have driven breakthroughs across various language tasks by their strong capability to learn rich contextual representations. Scaling them to improve representation, however, often demands substantial memory and compute costs, such as the Key-Value (KV) cache used during auto-regressive decoding. Skip connections offer a promising way to improve representation without bloating resource usage, yet most prior works either improve expressivity while leaving KV costs unchanged, or reduce memory at the cost of weaker representation. In this work, we propose SkipV1Former, a Transformer variant that uses skip connections from the first layer's Value heads to strengthen model representation and reduce KV cache. Specifically, from the second block onward, each layer reuses half of its Value heads from the very first layer, while computing the other half as usual-cutting Value projections and V cache by nearly 50 \%. Theoretically, we show that routing uncompressed first-layer Values into deeper layers restores information lost to compression and accelerates the model's implicit mesa-optimization-a key pattern of Transformer in auto-regressive tasks. Empirically, across different model scales, SkipV1Former delivers consistent reductions of approximately 25 \% in KV cache while improving perplexity relative to standard Multi-Head Attention (MHA) Transformers and some advanced variants. Moreover, we propose a recipe for uptraining existing MHA Transformer checkpoints to SkipV1Former with only 10-15\% additional compute. Finally, SkipV1Former can seamlessly combine advanced methods like Group-Query Attention and Multi-Latent Attention to achieve further KV cache savings and performance improvement. When combined with YOCO, it cuts KV cache size by nearly 50 \% while still improving performance.

**Link**: [arxiv](http://arxiv.org/abs/2510.16807v2),  [pdf](http://arxiv.org/pdf/2510.16807v2)

**Tags**: cs.LG cs.AI 



### Soft Phonon Charge-Density Wave Formation in the Kagome Metal   KV$_3$Sb$_5$
**Authors**: Yifan Wang, Chenchao Xu, Zhimian Wu, Huachen Rao, Zhaoyang Shan, Yi Liu, Guanghan Cao, Michael Smidman, Ming Shi, Huiqiu Yuan, Tao Wu, Xianhui Chen, Chao Cao, Yu Song

**Updated**: 2025-10-23T05:22:09Z

**Summary**: A range of of unusual emergent behaviors have been reported in the charge-density wave (CDW) state of the $A$V$_3$Sb$_5$ ($A=~$K, Rb, Cs) kagome metals, including a CDW formation process without soft phonons, which points to an unconventional CDW mechanism. Here, we use inelastic x-ray scattering to show that the CDW in KV$_3$Sb$_5$ forms via phonons that soften to zero energy at the CDW ordering vector ($L$-point) around $T_{\rm CDW}=78$~K. These soft phonons exhibit a remarkable in-plane anisotropy, extending over a much larger momentum range along $L$-$A$ relative to $L$-$H$, which leads to diffuse scattering common among $A$V$_3$Sb$_5$. Using first-principles calculations, we find that the momentum-dependent electron-phonon coupling (EPC) is peaked at $L$ and exhibits the same in-plane anisotropy as the phonon softening. Conversely, the electronic susceptibility is not peaked at $L$ and shows the opposite in-plane anisotropy. Our findings favor momentum-dependent EPC as the driving mechanism of the CDW in KV$_3$Sb$_5$, with a CDW formation process similar to that of transition metal dichalcogenides.

**Link**: [arxiv](http://arxiv.org/abs/2510.20230v1),  [pdf](http://arxiv.org/pdf/2510.20230v1)

**Tags**: cond-mat.supr-con cond-mat.str-el 



### Not All Heads Matter: A Head-Level KV Cache Compression Method with   Integrated Retrieval and Reasoning
**Authors**: Yu Fu, Zefan Cai, Abedelkadir Asi, Wayne Xiong, Yue Dong, Wen Xiao

**Updated**: 2025-10-23T00:47:24Z

**Summary**: Key-Value (KV) caching is a common technique to enhance the computational efficiency of Large Language Models (LLMs), but its memory overhead grows rapidly with input length. Prior work has shown that not all tokens are equally important for text generation, proposing layer-level KV cache compression to selectively retain key information. Recognizing the distinct roles of attention heads in generation, we propose HeadKV, a head-level KV cache compression method, and HeadKV-R2, which leverages a novel contextual reasoning ability estimation for compression. Our approach operates at the level of individual heads, estimating their importance for contextual QA tasks that require both retrieval and reasoning capabilities. Extensive experiments across diverse benchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct, Mistral-7B-Instruct), and long-context abilities tests demonstrate that our head-level KV cache compression significantly outperforms strong baselines, particularly in low-resource settings (KV size = 64 & 128). Notably, our method retains just 1.5% of the KV cache while achieving 97% of the performance of the full KV cache on the contextual question answering benchmark. Codes are available at https://github.com/FYYFU/HeadKV

**Link**: [arxiv](http://arxiv.org/abs/2410.19258v4),  [pdf](http://arxiv.org/pdf/2410.19258v4)

**Tags**: cs.CL cs.AI 



### Blending Complementary Memory Systems in Hybrid Quadratic-Linear   Transformers
**Authors**: Kazuki Irie, Morris Yau, Samuel J. Gershman

**Updated**: 2025-10-23T00:40:38Z

**Summary**: We develop hybrid memory architectures for general-purpose sequence processing neural networks, that combine key-value memory using softmax attention (KV-memory) with fast weight memory through dynamic synaptic modulation (FW-memory) -- the core principles of quadratic and linear transformers, respectively. These two memory systems have complementary but individually limited properties: KV-memory offers precise retrieval but is constrained by quadratic complexity in sequence length, while FW-memory supports arbitrarily long sequences and enables more expressive computation but sacrifices precise recall. We propose and compare three methods to blend these two systems into a single memory system, differing in how and when input information is delivered to each system, to leverage the strengths of both. We conduct experiments on general language modeling and retrieval tasks by training 340M- and 1.3B-parameter models from scratch, as well as on synthetic algorithmic tasks designed to precisely illustrate the benefits of certain hybrid methods over others. We also evaluate our hybrid memory systems on reinforcement learning in partially observable environments. Overall, we demonstrate how a well-designed hybrid can overcome the limitations of its individual components, offering new insights into the design principle of neural memory systems.

**Link**: [arxiv](http://arxiv.org/abs/2506.00744v2),  [pdf](http://arxiv.org/pdf/2506.00744v2)

**Tags**: cs.LG 



### Detecting and Preventing Latent Risk Accumulation in High-Performance   Software Systems
**Authors**: Jahidul Arafat, Kh. M. Moniruzzaman, Shamim Hossain, Fariha Tasmin

**Updated**: 2025-10-22T23:56:45Z

**Summary**: Modern distributed systems employ aggressive optimization strategies that create latent risks - hidden vulnerabilities where exceptional performance masks catastrophic fragility when optimizations fail. Cache layers achieving 99% hit rates can obscure database bottlenecks until cache failures trigger 100x load amplification and cascading collapse. Current reliability engineering focuses on reactive incident response rather than proactive detection of optimization-induced vulnerabilities. This paper presents the first comprehensive framework for systematic latent risk detection, prevention, and optimization through integrated mathematical modeling, intelligent perturbation testing, and risk-aware performance optimization. We introduce the Latent Risk Index (LRI) that correlates strongly with incident severity (r=0.863, p<0.001), enabling predictive risk assessment. Our framework integrates three systems: HYDRA employing six optimization-aware perturbation strategies achieving 89.7% risk discovery rates, RAVEN providing continuous production monitoring with 92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling risk-aware optimization maintaining 96.6% baseline performance while reducing latent risks by 59.2%. Evaluation across three testbed environments demonstrates strong statistical validation with large effect sizes (Cohen d>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24 weeks shows 69.1% mean time to recovery reduction, 78.6% incident severity reduction, and 81 prevented incidents generating 1.44M USD average annual benefits with 3.2-month ROI. Our approach transforms reliability engineering from reactive incident management to proactive risk-aware optimization.

**Link**: [arxiv](http://arxiv.org/abs/2510.03712v2),  [pdf](http://arxiv.org/pdf/2510.03712v2)

**Tags**: cs.SE 68M15, 90B25, 68T05, 90C29 C.4; C.2.4; D.2.5; D.4.5 



### CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware   Cloud-Edge Cooperation
**Authors**: Hasan Akgul, Mari Eplik, Javier Rojas, Aina Binti Abdullah, Pieter van der Merwe

**Updated**: 2025-10-22T15:16:56Z

**Summary**: We present CoSense-LLM, an edge-first framework that turns continuous multimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and lightweight vision) into compact, verifiable semantic tokens and coordinates with large language models under explicit latency, energy, bandwidth, and privacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight encoder that aligns sensor embeddings with language and compresses them into short discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer that grounds generation in site specific policies and notes; (iii) PromptRouter, a cost and uncertainty aware policy that selects edge only generation, edge plus retrieval, or compact cloud escalation; and (iv) Secure Execution, an auditable redaction path that enforces data minimization so raw waveforms never leave the device. The system works with modern serving optimizations, including paged or streaming KV caches, FlashAttention style kernels, speculative decoding, and quantized LoRA adapters, and supports on device personalization and federated updates under non IID drift. Across home, office, and clinic deployments, CoSense-LLM delivers grounded explanations while meeting tight service level objectives: it sustains sub second (p95) end to end latency on edge dominant paths, reduces inter tier token and bandwidth costs by preferring local retrieval grounded responses, and preserves privacy by transmitting only discrete codes and redacted metadata. Ablations show that Edge-RAG improves factual consistency and reduces contradictions, calibrated uncertainty enables selective abstention and controlled escalations, and KV plus decoding accelerators lower energy per decision. The results support an edge first design that treats semantics, privacy, and predictable latency as co equal goals for large model deployments in interference prone environments.

**Link**: [arxiv](http://arxiv.org/abs/2510.19670v1),  [pdf](http://arxiv.org/pdf/2510.19670v1)

**Tags**: cs.CL I.2.6; C.2.4; C.3 



### dInfer: An Efficient Inference Framework for Diffusion Language Models
**Authors**: Yuxin Ma, Lun Du, Lanning Wei, Kun Chen, Qian Xu, Kangyu Wang, Guofeng Feng, Guoshan Lu, Lin Liu, Xiaojing Qi, Xinyuan Zhang, Zhen Tao, Haibo Feng, Ziyun Jiang, Ying Xu, Zenan Huang, Yihong Zhuang, Haokai Xu, Jiaqi Hu, Zhenzhong Lan, Junbo Zhao, Jianguo Li, Da Zheng

**Updated**: 2025-10-22T14:33:49Z

**Summary**: Diffusion-based large language models (dLLMs) have emerged as a promising alternative to autoregressive (AR) LLMs, leveraging denoising-based generation to enable inherent parallelism. Even more and more open-sourced dLLM models emerge, yet their widespread adoption remains constrained by the lack of a standardized and efficient inference framework. We present dInfer, an efficient and extensible framework for dLLM inference. dInfer decomposes the inference pipeline into four modular components--model, diffusion iteration manager, decoding strategy, and KV-cache manager--and integrates novel algorithms for each component alongside system-level optimizations. Through this combination of algorithmic innovations and system enhancements, dInfer achieves substantial efficiency gains without compromising output quality on LLaDA-MoE. At batch size 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800 tokens per second across six benchmarks on $8\times$ H800 GPUs. Compared to prior systems, dInfer delivers a $10\times$ speedup over Fast-dLLM while maintaining similar model performance. Even compared to the AR model (with a comparable number of activation parameters and performance) QWen2.5-3B, which is highly optimized with the latest vLLM inference engine, dInfer still delivers a $2$-$3\times$ speedup. The implementation of dInfer is open-sourced at https://github.com/inclusionAI/dInfer.

**Link**: [arxiv](http://arxiv.org/abs/2510.08666v3),  [pdf](http://arxiv.org/pdf/2510.08666v3)

**Tags**: cs.CL cs.AI 



### Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs   via Sparse Attention
**Authors**: J Rosser, JosÃ© Luis Redondo GarcÃ­a, Gustavo Penha, Konstantina Palla, Hugues Bouchard

**Updated**: 2025-10-22T09:42:29Z

**Summary**: As Large Language Models (LLMs) scale to million-token contexts, traditional Mechanistic Interpretability techniques for analyzing attention scale quadratically with context length, demanding terabytes of memory beyond 100,000 tokens. We introduce Sparse Tracing, a novel technique that leverages dynamic sparse attention to efficiently analyze long context attention patterns. We present Stream, a compilable hierarchical pruning algorithm that estimates per-head sparse attention masks in near-linear time $O(T \log T)$ and linear space $O(T)$, enabling one-pass interpretability at scale. Stream performs a binary-search-style refinement to retain only the top-$k$ key blocks per query while preserving the model's next-token behavior. We apply Stream to long chain-of-thought reasoning traces and identify thought anchors while pruning 97-99\% of token interactions. On the RULER benchmark, Stream preserves critical retrieval paths while discarding 90-96\% of interactions and exposes layer-wise routes from the needle to output. Our method offers a practical drop-in tool for analyzing attention patterns and tracing information flow without terabytes of caches. By making long context interpretability feasible on consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring. Code is available at https://anonymous.4open.science/r/stream-03B8/.

**Link**: [arxiv](http://arxiv.org/abs/2510.19875v1),  [pdf](http://arxiv.org/pdf/2510.19875v1)

**Tags**: cs.CL cs.AI 68T40 I.2.11 



### ODataX: A Progressive Evolution of the Open Data Protocol
**Authors**: Anirudh Ganesh, Nitin Sood

**Updated**: 2025-10-22T07:50:06Z

**Summary**: The Open Data Protocol (OData) provides a standardized approach for building and consuming RESTful APIs with rich query capabilities. Despite its power and maturity, OData adoption remains confined primarily to enterprise environments, particularly within Microsoft and SAP ecosystems. This paper analyzes the key barriers preventing wider OData adoption and introduces ODataX, an evolved version of the protocol designed to address these limitations. ODataX maintains backward compatibility with OData v4 while introducing progressive complexity disclosure through simplified query syntax, built-in performance guardrails via query cost estimation, and enhanced caching mechanisms. This work aims to bridge the gap between enterprise-grade query standardization and the simplicity demanded by modern web development practices.

**Link**: [arxiv](http://arxiv.org/abs/2510.24761v1),  [pdf](http://arxiv.org/pdf/2510.24761v1)

**Tags**: cs.DB cs.IR cs.SE 



### LAPRAD: LLM-Assisted PRotocol Attack Discovery
**Authors**: R. Can Aygun, Yehuda Afek, Anat Bremler-Barr, Leonard Kleinrock

**Updated**: 2025-10-22T05:47:41Z

**Summary**: With the goal of improving the security of Internet protocols, we seek faster, semi-automatic methods to discover new vulnerabilities in protocols such as DNS, BGP, and others. To this end, we introduce the LLM-Assisted Protocol Attack Discovery (LAPRAD) methodology, enabling security researchers with some DNS knowledge to efficiently uncover vulnerabilities that would otherwise be hard to detect.   LAPRAD follows a three-stage process. In the first, we consult an LLM (GPT-o1) that has been trained on a broad corpus of DNS-related sources and previous DDoS attacks to identify potential exploits. In the second stage, a different LLM automatically constructs the corresponding attack configurations using the ReACT approach implemented via LangChain (DNS zone file generation). Finally, in the third stage, we validate the attack's functionality and effectiveness.   Using LAPRAD, we uncovered three new DDoS attacks on the DNS protocol and rediscovered two recently reported ones that were not included in the LLM's training data. The first new attack employs a bait-and-switch technique to trick resolvers into caching large, bogus DNSSEC RRSIGs, reducing their serving capacity to as little as 6%. The second exploits large DNSSEC encryption algorithms (RSA-4096) with multiple keys, thereby bypassing a recently implemented default RRSet limit. The third leverages ANY-type responses to produce a similar effect.   These variations of a cache-flushing DDoS attack, called SigCacheFlush, circumvent existing patches, severely degrade resolver query capacity, and impact the latest versions of major DNS resolver implementations.

**Link**: [arxiv](http://arxiv.org/abs/2510.19264v1),  [pdf](http://arxiv.org/pdf/2510.19264v1)

**Tags**: cs.CR cs.AI cs.NI 



### PruneHal: Reducing Hallucinations in Multi-modal Large Language Models   through Adaptive KV Cache Pruning
**Authors**: Fengyuan Sun, Hui Chen, Xinhao Xu, Dandan Zheng, Jingdong Chen, Jun Zhou, Jungong Han, Guiguang Ding

**Updated**: 2025-10-22T02:41:07Z

**Summary**: While multi-modal large language models (MLLMs) have made significant progress in recent years, the issue of hallucinations remains a major challenge. To mitigate this phenomenon, existing solutions either introduce additional data for further training or incorporate external or internal information during inference. However, these approaches inevitably introduce extra computational costs. In this paper, we observe that hallucinations in MLLMs are strongly associated with insufficient attention allocated to visual tokens. In particular, the presence of redundant visual tokens disperses the model's attention, preventing it from focusing on the most informative ones. As a result, critical visual cues are often under-attended, which in turn exacerbates the occurrence of hallucinations. Building on this observation, we propose \textbf{PruneHal}, a training-free, simple yet effective method that leverages adaptive KV cache pruning to enhance the model's focus on critical visual information, thereby mitigating hallucinations. To the best of our knowledge, we are the first to apply token pruning for hallucination mitigation in MLLMs. Notably, our method don't require additional training and incurs nearly no extra inference cost. Moreover, PruneHal is model-agnostic and can be seamlessly integrated with different decoding strategies, including those specifically designed for hallucination mitigation. We evaluate PruneHal on several widely used hallucination evaluation benchmarks using four mainstream MLLMs, achieving robust and outstanding results that highlight the effectiveness and superiority of our method. Our code will be publicly available.

**Link**: [arxiv](http://arxiv.org/abs/2510.19183v1),  [pdf](http://arxiv.org/pdf/2510.19183v1)

**Tags**: cs.CV cs.AI 



### Think Straight, Stop Smart: Structured Reasoning for Efficient Multi-Hop   RAG
**Authors**: Jihwan Bang, Juntae Lee, Seunghan Yang, Sungha Choi

**Updated**: 2025-10-22T02:09:23Z

**Summary**: Multi-hop retrieval-augmented generation (RAG) is a promising strategy for complex reasoning, yet existing iterative prompting approaches remain inefficient. They often regenerate predictable token sequences at every step and rely on stochastic stopping, leading to excessive token usage and unstable termination. We propose TSSS (Think Straight, Stop Smart), a structured multi-hop RAG framework designed for efficiency. TSSS introduces (i) a template-based reasoning that caches recurring prefixes and anchors sub-queries to the main question, reducing token generation cost while promoting stable reasoning, and (ii) a retriever-based terminator, which deterministically halts reasoning once additional sub-queries collapse into repetition. This separation of structured reasoning and termination control enables both faster inference and more reliable answers. On HotpotQA, 2WikiMultiHop, and MuSiQue, TSSS achieves state-of-the-art accuracy and competitive efficiency among RAG-CoT approaches, highlighting its effectiveness in efficiency-constrained scenarios such as on-device inference.

**Link**: [arxiv](http://arxiv.org/abs/2510.19171v1),  [pdf](http://arxiv.org/pdf/2510.19171v1)

**Tags**: cs.CL 



### LLMBridge: Reducing Costs to Access LLMs in a Prompt-Centric Internet
**Authors**: Noah Martin, Abdullah Bin Faisal, Hiba Eltigani, Rukhshan Haroon, Swaminathan Lamelas, Fahad Dogar

**Updated**: 2025-10-21T22:37:11Z

**Summary**: Today's Internet infrastructure is centered around content retrieval over HTTP, with middleboxes (e.g., HTTP proxies) playing a crucial role in performance, security, and cost-effectiveness. We envision a future where Internet communication will be dominated by "prompts" sent to generative AI models. For this, we will need proxies that provide similar functions to HTTP proxies (e.g., caching, routing, compression) while dealing with unique challenges and opportunities of prompt-based communication. As a first step toward supporting prompt-based communication, we present LLMBridge, an LLM proxy designed for cost-conscious users, such as those in developing regions and education (e.g., students, instructors). LLMBridge supports three key optimizations: model selection (routing prompts to the most suitable model), context management (intelligently reducing the amount of context), and semantic caching (serving prompts using local models and vector databases). These optimizations introduce trade-offs between cost and quality, which applications navigate through a high-level, bidirectional interface. As case studies, we deploy LLMBridge in two cost-sensitive settings: a WhatsApp-based Q&A service and a university classroom environment. The WhatsApp service has been live for over twelve months, serving 100+ users and handling more than 14.7K requests. In parallel, we exposed LLMBridge to students across three computer science courses over a semester, where it supported diverse LLM-powered applications - such as reasoning agents and chatbots - and handled an average of 500 requests per day. We report on deployment experiences across both settings and use the collected workloads to benchmark the effectiveness of various cost-optimization strategies, analyzing their trade-offs in cost, latency, and response quality.

**Link**: [arxiv](http://arxiv.org/abs/2410.11857v3),  [pdf](http://arxiv.org/pdf/2410.11857v3)

**Tags**: cs.DC cs.LG 



### DFUSE: Strongly Consistent Write-Back Kernel Caching for Distributed   Userspace File Systems
**Authors**: Haoyu Li, Jingkai Fu, Qing Li, Windsor Hsu, Asaf Cidon

**Updated**: 2025-10-21T21:07:17Z

**Summary**: Cloud platforms host thousands of tenants that demand POSIX semantics, high throughput, and rapid evolution from their storage layer. Kernel-native distributed file systems supply raw speed, but their privileged code base couples every release to the kernel, widens the blast radius of crashes, and slows innovation. FUSE-based distributed file systems flip those trade-offs: they run in user space for fast deployment and strong fault isolation, yet the FUSE interface disables the kernel's write-back page cache whenever strong consistency is required. Practitioners must therefore choose between (i) weak consistency with fast write-back caching or (ii) strong consistency with slow write-through I/O, a limitation that has kept FUSE distributed file systems out of write-intensive cloud workloads.   To this end, we present DFUSE, the first distributed FUSE file system that delivers write-back kernel caching and strong consistency. DFUSE achieves this by offloading userspace consistency control to the kernel driver, allowing coordinated access to the kernel's page cache across nodes. This design eliminates blind local cache updates and ensures cluster-wide strong consistency without compromising performance. In our evaluation, DFUSE achieves up to 68.0% higher throughput and 40.4% lower latency than the existing write-through design of FUSE-based distributed file systems.

**Link**: [arxiv](http://arxiv.org/abs/2503.18191v3),  [pdf](http://arxiv.org/pdf/2503.18191v3)

**Tags**: cs.OS 



### Putting the Context back into Memory
**Authors**: David A. Roberts

**Updated**: 2025-10-21T16:32:50Z

**Summary**: Requests arriving at main memory are often different from what programmers can observe or estimate by using CPU-based monitoring. Hardware cache prefetching, memory request scheduling and interleaving cause a loss of observability that limits potential data movement and tiering optimizations. In response, memory-side telemetry hardware like page access heat map units (HMU) and page prefetchers were proposed to inform Operating Systems with accurate usage data. However, it is still hard to map memory activity to software program functions and objects because of the decoupled nature of host processors and memory devices. Valuable program context is stripped out from the memory bus, leaving only commands, addresses and data. Programmers have expert knowledge of future data accesses, priorities, and access to processor state, which could be useful hints for runtime memory device optimization. This paper makes context visible at memory devices by encoding any user-visible state as detectable packets in the memory read address stream, in a nondestructive manner without significant capacity overhead, drivers or special access privileges. We prototyped an end-to-end system with metadata injection that can be reliably detected and decoded from a memory address trace, either by a host processor, or a memory module. We illustrate a use case with precise code execution markers and object address range tracking. In the future, real time metadata decoding with near-memory computing (NMC) could provide customized telemetry and statistics to users, or act on application hints to perform functions like prioritizing requests, remapping data and reconfiguring devices.

**Link**: [arxiv](http://arxiv.org/abs/2510.15878v2),  [pdf](http://arxiv.org/pdf/2510.15878v2)

**Tags**: cs.AR cs.OS cs.PF 



### SSD: Spatial-Semantic Head Decoupling for Efficient Autoregressive Image   Generation
**Authors**: Siyong Jian, Huan Wang

**Updated**: 2025-10-21T15:17:37Z

**Summary**: Autoregressive image generation models like Janus-Pro produce high-quality images, but at the significant cost of high memory and ever-growing computational demands due to the large number of visual tokens. While KV cache compression has been extensively studied in language modeling, it still remains largely unexplored for the image generation domain. In this work, we begin by identifying a distinct and prominent attention phenomenon, which we term spatial locality and emergent semantic sink. To leverage this key insight, we introduce a novel KV cache compression framework. Specifically, we compress the KV cache for all visual tokens by adaptively decoupling attention heads into two separate types: for spatial-locality heads, our method maintains a short recent token window; for semantic-sink heads, it strategically preserves a compact set of highly-attended tokens. Our extensive experiments demonstrate that the proposed method achieves a 5$\times$ reduction in memory usage and a notable 6.6$\times$ speedup in overall throughput with only minimal visual quality loss, thereby enabling highly efficient native autoregressive image generation on resource-constrained hardware.

**Link**: [arxiv](http://arxiv.org/abs/2510.18716v1),  [pdf](http://arxiv.org/pdf/2510.18716v1)

**Tags**: cs.CV 



### ParaLog: Consistent Host-side Logging for Parallel Checkpoints
**Authors**: Steven W. D. Chien, Kento Sato, Artur Podobas, Niclas Jansson, Stefano Markidis, Michio Honda

**Updated**: 2025-10-21T15:13:47Z

**Summary**: Output-intensive scientific applications are highly sensitive to low storage throughput. While existing scientific application stacks are optimized for traditional High-Performance Computing (HPC) environments with high remote storage and network bandwidth, these assumptions often fail in modern settings like cloud deployment. This is because the existing scientific application I/O stack fails to leverage the available resources. At the same time, scientific applications exhibit special synchronization and data output requirements that are difficult to satisfy using traditional approaches such as block-level or filesystem-level caching. We introduce ParaLog, a distributed host-side logging approach designed to accelerate scientific applications transparently. ParaLog emphasizes deployability, enabling support for unmodified message passing interface (MPI) applications and implementations while preserving crash consistency semantics. We evaluate ParaLog across traditional HPC, cloud HPC, local clusters, and hybrid environments, demonstrating its capability to reduce end-to-end execution time by 13-26% for popular scientific applications in cloud settings.

**Link**: [arxiv](http://arxiv.org/abs/2401.14576v3),  [pdf](http://arxiv.org/pdf/2401.14576v3)

**Tags**: cs.DC cs.PF 



### Reasoning Language Model Inference Serving Unveiled: An Empirical Study
**Authors**: Qi Li, Junpan Wu, Xiang Liu, Yuxin Wang, Zeyu Li, Zhenheng Tang, Yuhan Chen, Shaohuai Shi, Xiaowen Chu

**Updated**: 2025-10-21T14:25:51Z

**Summary**: The reasoning large language model (RLLM) has been proven competitive in solving complex reasoning tasks such as mathematics, coding, compared to general LLM. However, the serving performance and behavior of RLLM remains unexplored, which may undermine the deployment and utilization of RLLM in real-world scenario. To close this gap, in this paper, we conduct a comprehensive study of RLLM service. We first perform a pilot study on comparing the serving performance between RLLM and traditional LLM and reveal that there are several distinct differences regarding serving behavior: (1) significant memory usage and fluctuations; (2) straggler requests; (3) adaptive running time; (4) domain preference. Then we further investigate whether existing inference optimization techniques are valid for RLLM. Our main takeaways are that model quantization methods and speculative decoding can improve service system efficiency with small compromise to RLLM accuracy, while prefix caching, KV cache quantization may even degrade accuracy or serving performance for small RLLM. Lastly, we conduct evaluation under real world workload modeled by Gamma distribution to verify our findings. Empirical results of real world workload evaluation across different dataset are aligned with our main findings regarding RLLM serving. We hope our work can provide the research community and industry with insights to advance RLLM inference serving.

**Link**: [arxiv](http://arxiv.org/abs/2510.18672v1),  [pdf](http://arxiv.org/pdf/2510.18672v1)

**Tags**: cs.LG cs.AI 



### Tokencake: A KV-Cache-centric Serving Framework for LLM-based   Multi-Agent Applications
**Authors**: Zhuohang Bian, Feiyang Wu, Teng Ma, Youwei Zhuo

**Updated**: 2025-10-21T12:39:32Z

**Summary**: Large Language Models (LLMs) are increasingly deployed in complex multi-agent applications that use external function calls. This workload creates severe performance challenges for the KV Cache: space contention leads to the eviction of critical agents' caches and time underutilization leaves the cache of agents stalled on long-running tool calls idling in GPU memory. We present Tokencake, a KV-Cache-centric serving framework that co-optimizes scheduling and memory management with an agent-aware design. Tokencake's Space Scheduler uses dynamic memory partitioning to shield critical agents from contention, while its Time Scheduler employs a proactive offload and predictive upload mechanism to repurpose GPU memory during function call stalls. Our evaluation on representative multi-agent benchmarks shows that Tokencake can reduce end-to-end latency by over 47.06%, improve effective GPU memory utilization by up to 16.9% compared to vLLM.

**Link**: [arxiv](http://arxiv.org/abs/2510.18586v1),  [pdf](http://arxiv.org/pdf/2510.18586v1)

**Tags**: cs.DC 



### EfficientNav: Towards On-Device Object-Goal Navigation with Navigation   Map Caching and Retrieval
**Authors**: Zebin Yang, Sunjian Zheng, Tong Xie, Tianshi Xu, Bo Yu, Fan Wang, Jie Tang, Shaoshan Liu, Meng Li

**Updated**: 2025-10-21T11:52:44Z

**Summary**: Object-goal navigation (ObjNav) tasks an agent with navigating to the location of a specific object in an unseen environment. Embodied agents equipped with large language models (LLMs) and online constructed navigation maps can perform ObjNav in a zero-shot manner. However, existing agents heavily rely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small LLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to limited model capacity for understanding complex navigation maps, which prevents deploying ObjNav on local devices. At the same time, the long prompt introduced by the navigation map description will cause high planning latency on local devices. In this paper, we propose EfficientNav to enable on-device efficient LLM-based zero-shot ObjNav. To help the smaller LLMs better understand the environment, we propose semantics-aware memory retrieval to prune redundant information in navigation maps. To reduce planning latency, we propose discrete memory caching and attention-based memory clustering to efficiently save and re-use the KV cache. Extensive experimental results demonstrate that EfficientNav achieves 11.1% improvement in success rate on HM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time latency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our code will be released soon.

**Link**: [arxiv](http://arxiv.org/abs/2510.18546v1),  [pdf](http://arxiv.org/pdf/2510.18546v1)

**Tags**: cs.RO cs.AI 



### VLA-Cache: Efficient Vision-Language-Action Manipulation via Adaptive   Token Caching
**Authors**: Siyu Xu, Yunke Wang, Chenghao Xia, Dihao Zhu, Tao Huang, Chang Xu

**Updated**: 2025-10-21T10:33:29Z

**Summary**: Vision-Language-Action (VLA) models have demonstrated strong multi-modal reasoning capabilities, enabling direct action generation from visual perception and language instructions in an end-to-end manner. However, their substantial computational cost poses a challenge for real-time robotic control, where rapid decision-making is essential. This paper introduces VLA-Cache, a training-free inference acceleration method that reduces computational overhead by adaptively caching and reusing static visual tokens across frames. Exploiting the temporal continuity in robotic manipulation, VLA-Cache identifies minimally changed tokens between adjacent frames and reuses their cached key-value representations, thereby circumventing redundant computations. Additionally, to maintain action precision, VLA-Cache selectively re-computes task-relevant tokens that are environmentally sensitive, ensuring the fidelity of critical visual information. To further optimize efficiency, we introduce a layer adaptive token reusing strategy that dynamically adjusts the reuse ratio based on attention concentration across decoder layers, prioritizing critical tokens for recomputation. Extensive experiments on two simulation platforms (LIBERO and SIMPLER) and a real-world robotic system demonstrate that VLA-Cache achieves up to 1.7x speedup in CUDA latency and a 15% increase in control frequency, with negligible loss on task success rate. The code and videos can be found at our project page: https://vla-cache.github.io.

**Link**: [arxiv](http://arxiv.org/abs/2502.02175v2),  [pdf](http://arxiv.org/pdf/2502.02175v2)

**Tags**: cs.RO cs.CV cs.LG 



### Tree of Agents: Improving Long-Context Capabilities of Large Language   Models through Multi-Perspective Reasoning
**Authors**: Song Yu, Xiaofei Xu, Ke Deng, Li Li, Lin Tian

**Updated**: 2025-10-21T10:08:33Z

**Summary**: Large language models (LLMs) face persistent challenges when handling long-context tasks, most notably the lost in the middle issue, where information located in the middle of a long input tends to be underutilized. Some existing methods that reduce input have the risk of discarding key information, while others that extend context windows often lead to attention dispersion. To address these limitations, we propose Tree of Agents (TOA), a multi-agent reasoning framework that segments the input into chunks processed by independent agents. Each agent generates its local cognition, then agents dynamically exchange information for collaborative reasoning along tree-structured paths. TOA enables agents to probe different reasoning orders for multi-perspective understanding, effectively mitigating position bias and reducing hallucinations. To improve processing efficiency, we incorporate prefix-hash caching and adaptive pruning strategies, achieving significant performance improvements with comparable API overhead. Experiments show that TOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple baselines and demonstrates comparable performance to the latest and much larger commercial models, such as Gemini1.5-pro, on various long-context tasks. Code is available at https://github.com/Aireduce952/Tree-of-Agents.

**Link**: [arxiv](http://arxiv.org/abs/2509.06436v2),  [pdf](http://arxiv.org/pdf/2509.06436v2)

**Tags**: cs.AI 



### How Efficient Are Diffusion Language Models? A Critical Examination of   Efficiency Evaluation Practices
**Authors**: Han Peng, Peiyu Liu, Zican Dong, Daixuan Cheng, Junyi Li, Yiru Tang, Shuo Wang, Wayne Xin Zhao

**Updated**: 2025-10-30T08:46:37Z

**Summary**: Diffusion language models (DLMs) have emerged as a promising alternative to the long-dominant autoregressive (AR) paradigm, offering a parallelable decoding process that could yield greater efficiency. Yet, in practice, current open-source DLMs often underperform their AR counterparts in speed, limiting their real-world utility. This work presents a systematic study of DLM efficiency, identifying key issues in prior evaluation methods. Through empirical benchmarking and a roofline-based theoretical analysis, we demonstrate that AR models generally achieve higher throughput, while DLMs consistently lag. We also investigate acceleration strategies, finding that techniques like dual cache and parallel decoding mainly offer gains at small batch sizes, with their benefits diminishing upon scaling. Our findings underscore the necessity of robust evaluation methods and improved acceleration strategies to advance research on DLMs.

**Link**: [arxiv](http://arxiv.org/abs/2510.18480v2),  [pdf](http://arxiv.org/pdf/2510.18480v2)

**Tags**: cs.CL 



### Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference
**Authors**: Siyuan Yan, Guo-Qing Jiang, Yuchen Zhang, Xiaoxing Ma, Ran Zhu, Chun Cao, Jingwei Xu

**Updated**: 2025-10-21T08:44:47Z

**Summary**: Large language models (LLMs) now support context windows of hundreds of thousands to millions of tokens, enabling applications such as long-document summarization, large-scale code synthesis, multi-document question answering and persistent multi-turn dialogue. However, such extended contexts exacerbate the quadratic cost of self-attention, leading to severe latency in autoregressive decoding. Existing sparse attention methods alleviate these costs but rely on heuristic patterns that struggle to recall critical key-value (KV) pairs for each query, resulting in accuracy degradation. We introduce Adamas, a lightweight yet highly accurate sparse attention mechanism designed for long-context inference. Adamas applies the Hadamard transform, bucketization and 2-bit compression to produce compact representations, and leverages Manhattan-distance estimation for efficient top-k selections. Experiments show that Adamas matches the accuracy of full attention with only a 64-token budget, achieves near-lossless performance at 128, and supports up to 8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering up to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences. Remarkably, Adamas attains comparable or even lower perplexity than full attention, underscoring its effectiveness in maintaining accuracy under aggressive sparsity.

**Link**: [arxiv](http://arxiv.org/abs/2510.18413v1),  [pdf](http://arxiv.org/pdf/2510.18413v1)

**Tags**: cs.CL 



### The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM   Serving Systems
**Authors**: Linke Song, Zixuan Pang, Wenhao Wang, Zihao Wang, XiaoFeng Wang, Hongbo Chen, Wei Song, Yier Jin, Dan Meng, Rui Hou

**Updated**: 2025-10-21T06:47:29Z

**Summary**: The wide deployment of Large Language Models (LLMs) has given rise to strong demands for optimizing their inference performance. Today's techniques serving this purpose primarily focus on reducing latency and improving throughput through algorithmic and hardware enhancements, while largely overlooking their privacy side effects, particularly in a multi-user environment. In our research, for the first time, we discovered a set of new timing side channels in LLM systems, arising from shared caches and GPU memory allocations, which can be exploited to infer both confidential system prompts and those issued by other users. These vulnerabilities echo security challenges observed in traditional computing systems, highlighting an urgent need to address potential information leakage in LLM serving infrastructures. In this paper, we report novel attack strategies designed to exploit such timing side channels inherent in LLM deployments, specifically targeting the Key-Value (KV) cache and semantic cache widely used to enhance LLM inference performance. Our approach leverages timing measurements and classification models to detect cache hits, allowing an adversary to infer private prompts with high accuracy. We also propose a token-by-token search algorithm to efficiently recover shared prompt prefixes in the caches, showing the feasibility of stealing system prompts and those produced by peer users. Our experimental studies on black-box testing of popular online LLM services demonstrate that such privacy risks are completely realistic, with significant consequences. Our findings underscore the need for robust mitigation to protect LLM systems against such emerging threats.

**Link**: [arxiv](http://arxiv.org/abs/2409.20002v5),  [pdf](http://arxiv.org/pdf/2409.20002v5)

**Tags**: cs.CR 



### A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated   in a coupled reactive transport HPC simulation
**Authors**: Max LÃ¼bke, Marco De Lucia, Steffen Christgau, Stefan Petri, Bettina Schnor

**Updated**: 2025-10-21T06:30:21Z

**Summary**: Surrogate models can play a pivotal role in enhancing performance in contemporary High-Performance Computing applications. Cache-based surrogates use already calculated simulation results to interpolate or extrapolate further simulation output values. But this approach only pays off if the access time to retrieve the needed values is much faster than the actual simulation. While the most existing key-value stores use a Client-Server architecture with dedicated storage nodes, this is not the most suitable architecture for HPC applications. Instead, we propose a distributed architecture where the parallel processes offer a part of their available memory to build a shared distributed hash table based on MPI. This paper presents three DHT approaches with the special requirements of HPC applications in mind. The presented lock-free design outperforms both DHT versions which use explicit synchronization by coarse-grained resp. fine-grained locking. The lock-free DHT shows very good scaling regarding read and write performance. The runtime of a coupled reactive transport simulation was improved between 14% and 42% using the lock-free DHT as a surrogate model.

**Link**: [arxiv](http://arxiv.org/abs/2504.14374v3),  [pdf](http://arxiv.org/pdf/2504.14374v3)

**Tags**: cs.DC 



### StreamingTOM: Streaming Token Compression for Efficient Video   Understanding
**Authors**: Xueyi Chen, Keda Tao, Kele Shao, Huan Wang

**Updated**: 2025-10-21T03:39:41Z

**Summary**: Unlike offline processing, streaming video vision-language models face two fundamental constraints: causality and accumulation. Causality prevents access to future frames that offline methods exploit, while accumulation causes tokens to grow unbounded, creating efficiency bottlenecks. However, existing approaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefill unchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stage framework that addresses both pre-LLM and post-LLM bottlenecks with predictable latency. Causal Temporal Reduction imposes a fixed per-frame budget and selects tokens based on adjacent-frame changes and token saliency, drastically reducing per-frame prefill cost by processing only a compact subset of visual tokens per frame instead of all visual tokens. Online Quantized Memory stores tokens in 4-bit format, retrieves relevant groups on demand, and dequantizes them, keeping the active kv-cache bounded regardless of stream length. Experiments demonstrate our method achieves $15.7\times$ kv-cache compression, $1.2\times$ lower peak memory and $2\times$ faster TTFT compared to prior SOTA. StreamingTOM maintains state-of-the-art accuracy among training-free methods with an average of $63.8\%$ on offline benchmarks and $55.8\%/3.7$ on RVS. These results highlight the practical benefits of our two-stage approach for efficient streaming video understanding with bounded growth.

**Link**: [arxiv](http://arxiv.org/abs/2510.18269v1),  [pdf](http://arxiv.org/pdf/2510.18269v1)

**Tags**: cs.CV cs.AI 



### SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference
**Authors**: Samir Khaki, Junxian Guo, Jiaming Tang, Shang Yang, Yukang Chen, Konstantinos N. Plataniotis, Yao Lu, Song Han, Zhijian Liu

**Updated**: 2025-10-20T17:35:47Z

**Summary**: Vision Language Models (VLMs) have rapidly advanced in integrating visual and textual reasoning, powering applications across high-resolution image understanding, long-video analysis, and multi-turn conversation. However, their scalability remains limited by the growing number of visual tokens that dominate inference latency. We present SparseVILA, a new paradigm for efficient VLM inference that decouples visual sparsity across the prefilling and decoding stages. SparseVILA distributes sparsity across stages by pruning redundant visual tokens during prefill and retrieving only query-relevant tokens during decoding. This decoupled design matches leading prefill pruning methods while preserving multi-turn fidelity by retaining most of the visual cache so that query-aware tokens can be retrieved at each conversation round. Built on an AWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster prefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end speedup on long-context video tasks -- while improving accuracy on document-understanding and reasoning tasks. By decoupling query-agnostic pruning and query-aware retrieval, SparseVILA establishes a new direction for efficient multimodal inference, offering a training-free, architecture-agnostic framework for accelerating large VLMs without sacrificing capability.

**Link**: [arxiv](http://arxiv.org/abs/2510.17777v1),  [pdf](http://arxiv.org/pdf/2510.17777v1)

**Tags**: cs.CV 



### StreamingThinker: Large Language Models Can Think While Reading
**Authors**: Junlong Tong, Yingqi Fan, Anhao Zhao, Yunpu Ma, Xiaoyu Shen

**Updated**: 2025-10-20T07:27:37Z

**Summary**: Large language models (LLMs) have demonstrated remarkable capabilities in chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm initiates thinking only after the entire input is available, which introduces unnecessary latency and weakens attention to earlier information in dynamic scenarios. Inspired by human cognition of thinking while reading, we first design a \textit{\textbf{streaming thinking}} paradigm for LLMs, where reasoning unfolds in the order of input and further adjusts its depth once reading is complete. We instantiate this paradigm with \textit{StreamingThinker}, a framework that enables LLMs to think while reading through the integration of streaming CoT generation, streaming-constraint training, and streaming parallel inference. Specifically, StreamingThinker employs streaming reasoning units with quality control for CoT generation, enforces order-preserving reasoning through streaming attention masks and position encoding, and leverages parallel KV caches that decouple input encoding from reasoning generation, thereby ensuring alignment and enabling true concurrency. We evaluate StreamingThinker on the Qwen3 model family across math reasoning, logical reasoning, and context-based QA reasoning tasks. Experimental results show that the StreamingThinker preserves performance comparable to batch thinking, while yielding an 80\% reduction in token waiting before the onset of reasoning and a more than 60\% reduction in time-level latency for producing the final answer, demonstrating the effectiveness of the streaming paradigm for LLM reasoning. Code will be released at \href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this repository.}

**Link**: [arxiv](http://arxiv.org/abs/2510.17238v1),  [pdf](http://arxiv.org/pdf/2510.17238v1)

**Tags**: cs.CL 



### Can Transformer Memory Be Corrupted? Investigating Cache-Side   Vulnerabilities in Large Language Models
**Authors**: Elias Hossain, Swayamjit Saha, Somshubhra Roy, Ravi Prasad

**Updated**: 2025-10-20T02:04:18Z

**Summary**: Even when prompts and parameters are secured, transformer language models remain vulnerable because their key-value (KV) cache during inference constitutes an overlooked attack surface. This paper introduces Malicious Token Injection (MTI), a modular framework that systematically perturbs cached key vectors at selected layers and timesteps through controlled magnitude and frequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A theoretical analysis quantifies how these perturbations propagate through attention, linking logit deviations to the Frobenius norm of corruption and softmax Lipschitz dynamics. Empirical results show that MTI significantly alters next-token distributions and downstream task performance across GPT-2 and LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic reasoning pipelines. These findings identify cache integrity as a critical yet underexplored vulnerability in current LLM deployments, positioning cache corruption as a reproducible and theoretically grounded threat model for future robustness and security research.

**Link**: [arxiv](http://arxiv.org/abs/2510.17098v1),  [pdf](http://arxiv.org/pdf/2510.17098v1)

**Tags**: cs.CR cs.AI 



### Video Reasoning without Training
**Authors**: Deepak Sridhar, Kartikeya Bhardwaj, Jeya Pradha Jeyaraj, Nuno Vasconcelos, Ankita Nayak, Harris Teague

**Updated**: 2025-10-19T23:17:13Z

**Summary**: Video reasoning using Large Multimodal Models (LMMs) relies on costly reinforcement learning (RL) and verbose chain-of-thought, resulting in substantial computational overhead during both training and inference. Moreover, the mechanisms that control the thinking process in these reasoning models are very limited. In this paper, using entropy of the model's output as a signal, we discover that the high-quality models go through a series of micro-explorations and micro-exploitations which keep the reasoning process grounded (i.e., avoid excessive randomness while the model is exploring or thinking through an answer). We further observe that once this "thinking" process is over, more accurate models demonstrate a better convergence by reducing the entropy significantly via a final exploitation phase (i.e., a more certain convergence towards a solution trajectory). We then use these novel, theoretically-grounded insights to tune the model's behavior directly at inference, without using any RL or supervised fine-tuning. Specifically, during inference, our proposed approach called V-Reason (Video-Reason) adapts the value cache of the LMM via a few optimization steps on a small, trainable controller using an entropy-based objective, i.e., no supervision from any dataset or RL is necessary. This tuning improves the model's micro-exploration and exploitation behavior during inference. Our experiments show that our proposed method achieves significant improvements over the base instruction-tuned models across several video reasoning datasets, narrowing the gap with RL-trained models to within 0.6% average accuracy without any training, while offering massive efficiency benefits: output tokens are reduced by 58.6% compared to the RL model.

**Link**: [arxiv](http://arxiv.org/abs/2510.17045v1),  [pdf](http://arxiv.org/pdf/2510.17045v1)

**Tags**: cs.CV cs.AI cs.LG 



### Addendum: Systematic Evaluation of Randomized Cache Designs against   Cache Occupancy
**Authors**: Anirban Chakraborty, Nimish Mishra, Sayandeep Saha, Sarani Bhattacharya, Debdeep Mukhopadhyay

**Updated**: 2025-10-19T15:13:25Z

**Summary**: In the main text published at USENIX Security 2025, we presented a systematic analysis of the role of cache occupancy in the design considerations for randomized caches (from the perspectives of performance and security). On the performance front, we presented a uniform benchmarking strategy that allows for a fair comparison among different randomized cache designs. Likewise, from the security perspective, we presented three threat assumptions: (1) covert channels; (2) process fingerprinting side-channel; and (3) AES key recovery. The main takeaway of our work is an open problem of designing a randomized cache of comparable efficiency with modern set-associative LLCs, while still resisting both contention-based and occupancy-based attacks. This note is meant as an addendum to the main text in light of the observations made in [2]. To summarize, the authors in [2] argue that (1) L1d cache size plays a role in adversarial success, and that (2) a patched version of MIRAGE with randomized initial seeding of global eviction map prevents leakage of AES key. We discuss the same in this addendum.

**Link**: [arxiv](http://arxiv.org/abs/2510.16871v1),  [pdf](http://arxiv.org/pdf/2510.16871v1)

**Tags**: cs.CR 



### Mixed-Precision Quantization for Language Models: Techniques and   Prospects
**Authors**: Mariam Rakka, Marios Fournarakis, Olga Krestinskaya, Jinane Bazzi, Khaled N. Salama, Fadi Kurdahi, Ahmed M. Eltawil, Mohammed E. Fouda

**Updated**: 2025-10-19T12:16:40Z

**Summary**: The rapid scaling of language models (LMs) has resulted in unprecedented computational, memory, and energy requirements, making their training and deployment increasingly unsustainable. Quantization has emerged as an essential compression technique to reduce model size, alleviate memory bottlenecks, and accelerate inference. However, while uniform low-bit quantization (e.g., INT8, INT4) provides significant efficiency gains, it can degrade accuracy in sensitive components of transformer-based LMs. Mixed-precision quantization offers a promising alternative by selectively allocating precision across layers or within tensors to balance efficiency and accuracy. This survey provides a comprehensive overview of Mixed-Precision quantization frameworks for LMs (MXPLMs). We first review quantization fundamentals, including uniform and non-uniform quantizers, quantization granularity, and methods widely used in post-training quantization. We then categorize and compare recent MXPLM frameworks according to their bit allocation strategies and precision configurations across weights, activations, and key-value caches. A comparative analysis highlights differences in perplexity, zero-shot task performance, and deployment trade-offs. Furthermore, we contrast MXPLMs with earlier mixed-precision quantization methods for deep neural networks, identifying strategies that transfer and those that face challenges in the LM setting. Finally, we summarize open issues and future directions, including hardware-aware design, activation quantization, and scalable optimization methods for billion-parameter models. By consolidating recent advances, this work serves as a reference for understanding the current landscape and research prospects of mixed-precision quantization for large-scale language models.

**Link**: [arxiv](http://arxiv.org/abs/2510.16805v1),  [pdf](http://arxiv.org/pdf/2510.16805v1)

**Tags**: cs.LG cs.AI 



### AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in   Large Language Models
**Authors**: Zeyu Li, Chuanfu Xiao, Yang Wang, Xiang Liu, Zhenheng Tang, Baotong Lu, Mao Yang, Xinyu Chen, Xiaowen Chu

**Updated**: 2025-10-18T11:29:52Z

**Summary**: Quantization has emerged as an effective and lightweight solution to reduce the memory footprint of the KV cache in Large Language Models. Nevertheless, minimizing the accuracy degradation caused by ultra-low-bit KV cache quantization remains a significant challenge. While scalar quantization is constrained by 1-bit bound, vector quantization exploits intra-vector correlations and enables sub-bit regimes, making it more suitable for ultra-low-bit quantization. To further mitigate quantization-induced degradation, we reveal that the degradation is highly uneven across tokens in attention quality. To investigate this unevenness, we introduce anchor score to measure each token's sensitivity to quantization. Our analysis and experiments show that preserving a small subset (1\%) of tokens with the highest Anchor Score significantly mitigates accuracy loss under aggressive quantization.   We propose AnTKV, a dual-stage framework that leverages anchor token-aware vector quantization to compress the KV cache. It combines offline token-aware centroids learning and online anchor token selection to balance compression and accuracy. To enable efficient deployment, we design an online anchor token selection kernel compatible with FlashAttention. It allows LLaMA3-8B to scale to 840K tokens on a single 80GB A100, while delivering up to $3.5\times$ higher decoding throughput over the FP16 baseline. Experiments demonstrate that AnTKV matches or surpasses prior methods at 4-bit, and significantly reduce perplexity under ultra-low-bit quantization, achieving 6.32 at 1-bit on Mistral-7B, compared to 7.25 for CQ and 15.36 for KVQuant.

**Link**: [arxiv](http://arxiv.org/abs/2506.19505v2),  [pdf](http://arxiv.org/pdf/2506.19505v2)

**Tags**: cs.CL 



### MPCache: MPC-Friendly KV Cache Eviction for Efficient Private LLM   Inference
**Authors**: Wenxuan Zeng, Ye Dong, Jinjin Zhou, Jin Tan, Lei Wang, Tao Wei, Runsheng Wang, Meng Li

**Updated**: 2025-10-18T06:04:53Z

**Summary**: Private large language model (LLM) inference based on secure multi-party computation (MPC) achieves formal data privacy protection but suffers from significant latency overhead, especially for long input sequences. While key-value (KV) cache eviction and sparse attention algorithms have been proposed for efficient LLM inference in plaintext, they are not designed for MPC and cannot benefit private LLM inference directly. In this paper, we propose an accurate and MPC-friendly KV cache eviction framework, dubbed MPCache, building on the observation that historical tokens in a long sequence may have different effects on the downstream decoding. Hence, MPCache combines a look-once static eviction algorithm to discard unimportant KV cache and a query-aware dynamic selection algorithm to activate only a small subset of KV cache for attention computation. MPCache further incorporates a series of optimizations for efficient dynamic KV cache selection, including MPC-friendly similarity approximation, hierarchical KV cache clustering, and cross-layer index-sharing strategy. Extensive experiments demonstrate that MPCache consistently outperforms prior-art KV cache eviction baselines across different generation tasks and achieves 1.8 ~ 2.01x and 3.39 ~ 8.37x decoding latency and communication reduction on different sequence lengths, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2501.06807v2),  [pdf](http://arxiv.org/pdf/2501.06807v2)

**Tags**: cs.CR 



### Autoencoding-Free Context Compression for LLMs via Contextual Semantic   Anchors
**Authors**: Xin Liu, Runsong Zhao, Pengcheng Huang, Xinyu Liu, Junyi Xiao, Chunyang Xiao, Tong Xiao, Shengxiang Gao, Zhengtao Yu, Jingbo Zhu

**Updated**: 2025-10-18T02:48:35Z

**Summary**: Context compression presents a promising approach for accelerating large language model (LLM) inference by compressing long contexts into compact representations. Current context compression methods predominantly rely on autoencoding tasks to train context-agnostic compression tokens to compress contextual semantics. While autoencoding tasks enable compression tokens to acquire compression capabilities, compression via autoencoding tasks creates a fundamental mismatch: the models are optimized for reconstruction that diverge from actual downstream tasks, thereby weakening the features more beneficial for real-world usage. We propose Semantic-Anchor Compression (SAC), a novel method that shifts from autoencoding task based compression to an architecture that is equipped with this compression capability \textit{a priori}. Instead of training models to compress contexts through autoencoding tasks, SAC directly selects so-called anchor tokens from the original context and aggregates contextual information into their key-value (KV) representations. By deriving representations directly from the contextual tokens, SAC eliminates the need for autoencoding training. To ensure compression performance while directly leveraging anchor tokens, SAC incorporates two key designs: (1) anchor embeddings that enable the compressor to identify critical tokens, and (2) bidirectional attention modification that allows anchor tokens to capture information from the entire context. Experimental results demonstrate that SAC consistently outperforms existing context compression methods across various compression ratios. On out-of-distribution evaluation using MRQA, SAC achieves 1 EM improvement at 5x compression over strong baselines, with increasing advantages at higher compression ratios.

**Link**: [arxiv](http://arxiv.org/abs/2510.08907v3),  [pdf](http://arxiv.org/pdf/2510.08907v3)

**Tags**: cs.CL 



### QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value   Weight Compression in Low-Precision Vision-Language Models
**Authors**: Yutong Wang, Haiyu Wang, Sai Qian Zhang

**Updated**: 2025-10-18T01:31:14Z

**Summary**: Vision-Language Models (VLMs) are integral to tasks such as image captioning and visual question answering, but their high computational cost, driven by large memory footprints and processing time, limits their scalability and real-time applicability. In this work, we propose leveraging Singular-Value Decomposition (SVD) over the joint query (Q), key (K), and value (V) weight matrices to reduce KV cache size and computational overhead. We in addition introduce an efficient rank allocation strategy that dynamically adjusts the SVD rank based on its impact on VLM accuracy, achieving a significant reduction in both memory usage and computational cost. Finally, we extend this approach by applying quantization to both VLM weights and activations, resulting in a highly efficient VLM. Our method outperforms previous approaches that rely solely on quantization or SVD by achieving more than $10\%$ accuracy improvement while consuming less hardware cost, making it better for real-time deployment on resource-constrained devices. We open source our code at \href{https://github.com/SAI-Lab-NYU/QSVD}{\texttt{https://github.com/SAI-Lab-NYU/QSVD}}.

**Link**: [arxiv](http://arxiv.org/abs/2510.16292v1),  [pdf](http://arxiv.org/pdf/2510.16292v1)

**Tags**: cs.LG 



### What Limits Agentic Systems Efficiency?
**Authors**: Song Bian, Minghao Yan, Anand Jayarajan, Gennady Pekhimenko, Shivaram Venkataraman

**Updated**: 2025-10-18T00:21:45Z

**Summary**: Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have demonstrated strong reasoning capabilities. To further enhance LLM capabilities, recent agentic systems, such as Deep Research, incorporate web interactions into LLM reasoning to mitigate uncertainties and reduce potential errors. However, existing research predominantly focuses on reasoning performance, often neglecting the efficiency of agentic systems. In this work, we present a comprehensive empirical study that identifies efficiency bottlenecks in web-interactive agentic systems. We decompose end-to-end latency into two primary components: LLM API latency and web environment latency. We conduct a comprehensive empirical study across 15 models and 5 providers to demonstrate high variability in API-based agentic systems. We observe that web environment latency can contribute as much as 53.7% to the overall latency in a web-based agentic system. To improve latency, we propose SpecCache, a caching framework augmented with speculative execution that can reduce web environment overhead. Extensive evaluations on two standard benchmarks show that our approach improves the cache hit rate by up to 58x compared to a random caching strategy, while reducing web environment overhead by up to 3.2x, without degrading agentic system performance.

**Link**: [arxiv](http://arxiv.org/abs/2510.16276v1),  [pdf](http://arxiv.org/pdf/2510.16276v1)

**Tags**: cs.AI cs.LG 



### A single optically detectable tumbling spin in silicon
**Authors**: FÃ©lix Cache, Yoann Baron, Baptiste Lefaucher, Jean-Baptiste Jager, FrÃ©dÃ©ric Mazen, FrÃ©dÃ©ric MilÃ©si, SÃ©bastien KerdilÃ¨s, Isabelle Robert-Philip, Jean-Michel GÃ©rard, Guillaume Cassabois, Vincent Jacques, AnaÃ¯s DrÃ©au

**Updated**: 2025-10-17T12:38:25Z

**Summary**: Electron spin resonance spectroscopy is a widely used technique for analyzing the microscopic structure, local environment and reorientation of atomic and molecular systems. Conventional inductive detection methods typically require to probe more than a billion of electron spins such that single atom motion is hidden through ensemble averaging. While several single spin spectroscopy methods are currently available, they have been so far limited to static systems. Here we demonstrate single spin spectroscopy of a fluorescent tumbling defect in silicon called the G center, behaving as a pseudo-molecule randomly reorienting itself in the crystalline matrix. Using high-resolution spin spectroscopy, we reveal a fine magnetic structure resulting from the spin principal axes jumping between discrete orientations in the crystal. By modeling the atomic reorientation of the defect, we demonstrate that spin tumbling induces variations in the coupling to the microwave magnetic field, enabling position-dependent Rabi frequencies to be detected in coherent spin control experiments. By virtue of its pseudo-molecule configuration, the G center in silicon is a unique quantum system to investigate the mutual interaction between optical, spin and rotation properties in a highly versatile material.

**Link**: [arxiv](http://arxiv.org/abs/2510.15590v1),  [pdf](http://arxiv.org/pdf/2510.15590v1)

**Tags**: quant-ph 



### FHE-SQL: Fully Homomorphic Encrypted SQL Database
**Authors**: Po-Yu Tseng, Po-Chu Hsu, Shih-Wei Liao

**Updated**: 2025-10-17T08:07:27Z

**Summary**: FHE-SQL is a privacy-preserving database system that enables secure query processing on encrypted data using Fully Homomorphic Encryption (FHE), providing privacy guaranties where an untrusted server can execute encrypted queries without learning either the query contents or the underlying data. Unlike property-preserving encryption-based systems such as CryptDB, which rely on deterministic or order-preserving encryption and are vulnerable to frequency, order, and equality-pattern inference attacks, FHE-SQL performs computations entirely under encryption, eliminating these leakage channels. Compared to trusted-hardware approaches such as TrustedDB, which depend on a hardware security module and thus inherit its trust and side-channel limitations, our design achieves end-to-end cryptographic protection without requiring trusted execution environments. In contrast to high-performance FHE-based engines-Hermes, which target specialized workloads such as vector search, FHE-SQL supports general SQL query semantics with schema-aware, type-safe definitions suitable for relational data management. FHE-SQL mitigates the high cost of ciphertext space by using an indirection architecture that separates metadata in RocksDB from large ciphertexts in blob storage. It supports oblivious selection via homomorphic boolean masks, multi-tier caching, and garbage collection, with security proven under the Universal Composability framework.

**Link**: [arxiv](http://arxiv.org/abs/2510.15413v1),  [pdf](http://arxiv.org/pdf/2510.15413v1)

**Tags**: cs.CR cs.DB 



### PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following   Models Need for Efficient Generation
**Authors**: Ao Wang, Hui Chen, Jiaxin Li, Jianchao Tan, Kefeng Zhang, Xunliang Cai, Zijia Lin, Jungong Han, Guiguang Ding

**Updated**: 2025-10-17T06:54:10Z

**Summary**: Recently, large vision-language models (LVLMs) have rapidly gained popularity for their strong generation and reasoning capabilities given diverse multimodal inputs. However, these models incur significant computational and memory overhead during inference, which greatly hinders the efficient deployment in practical scenarios. The extensive key-value (KV) cache, necessitated by the lengthy input and output sequences, notably contributes to the high inference cost. Based on this, recent works have investigated ways to reduce the KV cache size for higher efficiency. Although effective, they generally overlook the distinct importance distributions of KV vectors across layers and maintain the same cache size for each layer during the next token prediction. This results in the significant contextual information loss for certain layers, leading to notable performance decline. To address this, we present PrefixKV, where "Prefix" means the top-ranked KV based on importance rather than position in the original sequence. It reframes the challenge of determining KV cache sizes for all layers into the task of searching for the optimal global prefix configuration. With an adaptive layer-wise KV retention recipe based on binary search, the maximum contextual information can thus be preserved in each layer, facilitating the generation. Extensive experiments demonstrate that our method achieves the state-of-the-art performance compared with others. It exhibits superior inference efficiency and generation quality trade-offs, showing promising potential for practical applications. Code is available at https://github.com/THU-MIG/PrefixKV.

**Link**: [arxiv](http://arxiv.org/abs/2412.03409v4),  [pdf](http://arxiv.org/pdf/2412.03409v4)

**Tags**: cs.CV 



### Cross-layer Attention Sharing for Pre-trained Large Language Models
**Authors**: Yongyu Mu, Yuzhang Wu, Yuchun Fan, Chenglong Wang, Hengyu Li, Jiali Zeng, Qiaozhi He, Murun Yang, Fandong Meng, Jie Zhou, Tong Xiao, Jingbo Zhu

**Updated**: 2025-10-17T06:45:17Z

**Summary**: To enhance the efficiency of the attention mechanism within large language models (LLMs), previous works primarily compress the KV cache or group attention heads, while largely overlooking redundancy between layers. Our comprehensive analyses across various LLMs show that highly similar attention patterns persist within most layers. It's intuitive to reduce the redundancy by sharing attention weights across layers. However, further analysis reveals two challenges: (1) Directly sharing the weight matrix without carefully rearranging the attention heads proves to be ineffective; (2) Shallow layers are vulnerable to small deviations in attention weights.   Driven by these insights, we introduce LISA, a lightweight substitute for self-attention in well-trained LLMs. LISA employs tiny feed-forward networks to align attention heads between adjacent layers and low-rank matrices to approximate differences in layer-wise attention weights. Evaluations encompassing 13 typical benchmarks demonstrate that LISA maintains high response quality in terms of accuracy and perplexity while reducing redundant attention calculations within 53%-84% of the total layers. Our implementations of LISA achieve a 6x compression of Q and K matrices within the attention mechanism, with maximum throughput improvements 19.5%, 32.3%, and 40.1% for LLaMA3-8B, LLaMA2-7B, and LLaMA2-13B, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2408.01890v2),  [pdf](http://arxiv.org/pdf/2408.01890v2)

**Tags**: cs.CL 



### Tail-Optimized Caching for LLM Inference
**Authors**: Wenxin Zhang, Yueying Li, Ciamac C. Moallemi, Tianyi Peng

**Updated**: 2025-10-16T21:22:16Z

**Summary**: Prompt caching is critical for reducing latency and cost in LLM inference: OpenAI and Anthropic report up to 50-90% cost savings through prompt reuse. Despite its widespread success, little is known about what constitutes an optimal prompt caching policy, particularly when optimizing tail latency, a metric of central importance to practitioners. The widely used Least Recently Used (LRU) policy can perform arbitrarily poor on this metric, as it is oblivious to the heterogeneity of conversation lengths. To address this gap, we propose Tail-Optimized LRU, a simple two-line modification that reallocates KV cache capacity to prioritize high-latency conversations by evicting cache entries that are unlikely to affect future turns. Though the implementation is simple, we prove its optimality under a natural stochastic model of conversation dynamics, providing the first theoretical justification for LRU in this setting, a result that may be of independent interest to the caching community. Experimentally, on real conversation data WildChat, Tail-Optimized LRU achieves up to 27.5% reduction in P90 tail Time to First Token latency and 23.9% in P95 tail latency compared to LRU, along with up to 38.9% decrease in SLO violations of 200ms. We believe this provides a practical and theoretically grounded option for practitioners seeking to optimize tail latency in real-world LLM deployments.

**Link**: [arxiv](http://arxiv.org/abs/2510.15152v1),  [pdf](http://arxiv.org/pdf/2510.15152v1)

**Tags**: eess.SY cs.SY 



### Hive Hash Table: A Warp-Cooperative, Dynamically Resizable Hash Table   for GPUs
**Authors**: Md Sabbir Hossain Polak, David Troendle, Byunghyun Jang

**Updated**: 2025-10-16T19:28:30Z

**Summary**: Hash tables are essential building blocks in data-intensive applications, yet existing GPU implementations often struggle with concurrent updates, high load factors, and irregular memory access patterns. We present Hive hash table, a high-performance, warp-cooperative and dynamically resizable GPU hash table that adapts to varying workloads without global rehashing.   Hive hash table makes three key contributions. First, a cache-aligned packed bucket layout stores key-value pairs as 64-bit words, enabling coalesced memory access and atomic updates via single-CAS operations. Second, warp-synchronous concurrency protocols - Warp-Aggregated-Bitmask-Claim (WABC) and Warp-Cooperative Match-and-Elect (WCME) - reduce contention to one atomic operation per warp while ensuring lock-free progress. Third, a load-factor-aware dynamic resizing strategy expands or contracts capacity in warp-parallel K-bucket batches using linear hashing, maintaining balanced occupancy. To handle insertions under heavy contention, Hive hash table employs a four-step strategy: replace, claim-and-commit, bounded cuckoo eviction, and overflow-stash fallback. This design provides lock-free fast paths and bounded recovery cost under contention determined by a fixed eviction depth, while eliminating ABA hazards during concurrent updates.   Experimental evaluation on an NVIDIA RTX 4090 shows Hive hash table sustains load factors up to 95% while delivering 1.5-2x higher throughput than state-of-the-art GPU hash tables (Slab-Hash, DyCuckoo, WarpCore) under mixed insert-delete-lookup workloads. On balanced workload, Hive hash table reaches 3.5 billion updates/s and nearly 4 billion lookups/s, demonstrating scalability and efficiency for GPU-accelerated data processing.

**Link**: [arxiv](http://arxiv.org/abs/2510.15095v1),  [pdf](http://arxiv.org/pdf/2510.15095v1)

**Tags**: cs.DC 



### Attention Is All You Need for KV Cache in Diffusion LLMs
**Authors**: Quan Nguyen-Tri, Mukul Ranjan, Zhiqiang Shen

**Updated**: 2025-10-16T17:59:48Z

**Summary**: This work studies how to adaptively recompute key-value (KV) caches for diffusion large language models (DLMs) to maximize prediction accuracy while minimizing decoding latency. Prior methods' decoders recompute QKV for all tokens at every denoising step and layer, despite KV states changing little across most steps, especially in shallow layers, leading to substantial redundancy. We make three observations: (1) distant ${\bf MASK}$ tokens primarily act as a length-bias and can be cached block-wise beyond the active prediction window; (2) KV dynamics increase with depth, suggesting that selective refresh starting from deeper layers is sufficient; and (3) the most-attended token exhibits the smallest KV drift, providing a conservative lower bound on cache change for other tokens. Building on these, we propose ${\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that jointly decides ${when}$ to refresh (via an attention-aware drift test on the most-attended token) and ${where}$ to refresh (via a depth-aware schedule that recomputes from a chosen layer onward while reusing shallow-layer caches and off-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs adaptive, layer-aware cache updates for diffusion LLMs, reducing redundant computation and accelerating decoding with negligible loss in generation quality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across mathematical reasoning and code generation tasks demonstrate consistent speedups: $8.7\times$ on GSM8K (256 tokens), $45.1\times$ on longer sequences, and $4.8\times$ on HumanEval, while consistently maintaining higher accuracy than the baseline. Our method achieves significantly higher throughput ($6.8\times$ on GSM8K) than existing confidence-based approaches while preserving generation quality, enabling practical deployment of diffusion LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2510.14973v1),  [pdf](http://arxiv.org/pdf/2510.14973v1)

**Tags**: cs.CL cs.AI cs.LG 



### A Performance Portable Matrix Free Dense MTTKRP in GenTen
**Authors**: Gabriel Kosmacher, Eric T. Phipps, Sivasankaran Rajamanickam

**Updated**: 2025-10-16T17:10:03Z

**Summary**: We extend the GenTen tensor decomposition package by introducing an accelerated dense matricized tensor times Khatri-Rao product (MTTKRP), the workhorse kernel for canonical polyadic (CP) tensor decompositions, that is portable and performant on modern CPU and GPU architectures. In contrast to the state-of-the-art matrix multiply based MTTKRP kernels used by Tensor Toolbox, TensorLy, etc., that explicitly form Khatri-Rao matrices, we develop a matrix-free element-wise parallelization approach whose memory cost grows with the rank R like the sum of the tensor shape O(R(n+m+k)), compared to matrix-based methods whose memory cost grows like the product of the tensor shape O(R(mnk)). For the largest problem we study, a rank 2000 MTTKRP, the smaller growth rate yields a matrix-free memory cost of just 2% of the matrix-based methods, a 50x improvement. In practice, the reduced memory impact means our matrix-free MTTKRP can compute a rank 2000 tensor decomposition on a single NVIDIA H100 instead of six H100s using a matrix-based MTTKRP. We also compare our optimized matrix-free MTTKRP to baseline matrix-free implementations on different devices, showing a 3x single-device speedup on an Intel 8480+ CPU and an 11x speedup on a H100 GPU. In addition to numerical results, we provide fine grained performance models for an ideal multi-level cache machine, compare analytical performance predictions to empirical results, and provide a motivated heuristic selection for selecting an algorithmic hyperparameter.

**Link**: [arxiv](http://arxiv.org/abs/2510.14891v1),  [pdf](http://arxiv.org/pdf/2510.14891v1)

**Tags**: cs.MS G.2 



## Keyword: LLM Inference 
 ### E-Scores for (In)Correctness Assessment of Generative Model Outputs
**Authors**: Guneet S. Dhillon, Javier GonzÃ¡lez, Teodora Pandeva, Alicia Curth

**Updated**: 2025-10-29T17:59:16Z

**Summary**: While generative models, especially large language models (LLMs), are ubiquitous in today's world, principled mechanisms to assess their (in)correctness are limited. Using the conformal prediction framework, previous works construct sets of LLM responses where the probability of including an incorrect response, or error, is capped at a desired user-defined tolerance level. However, since these methods are based on p-values, they are susceptible to p-hacking, i.e., choosing the tolerance level post-hoc can invalidate the guarantees. We therefore leverage e-values to complement generative model outputs with e-scores as a measure of incorrectness. In addition to achieving the same statistical guarantees as before, e-scores provide users flexibility in adaptively choosing tolerance levels after observing the e-scores themselves, by upper bounding a post-hoc notion of error called size distortion. We experimentally demonstrate their efficacy in assessing LLM outputs for different correctness types: mathematical factuality and property constraints satisfaction.

**Link**: [arxiv](http://arxiv.org/abs/2510.25770v1),  [pdf](http://arxiv.org/pdf/2510.25770v1)

**Tags**: stat.ML cs.AI cs.LG 



### Neural Stochastic Flows: Solver-Free Modelling and Inference for SDE   Solutions
**Authors**: Naoki Kiyohara, Edward Johns, Yingzhen Li

**Updated**: 2025-10-29T17:59:06Z

**Summary**: Stochastic differential equations (SDEs) are well suited to modelling noisy and irregularly sampled time series found in finance, physics, and machine learning. Traditional approaches require costly numerical solvers to sample between arbitrary time points. We introduce Neural Stochastic Flows (NSFs) and their latent variants, which directly learn (latent) SDE transition laws using conditional normalising flows with architectural constraints that preserve properties inherited from stochastic flows. This enables one-shot sampling between arbitrary states and yields up to two orders of magnitude speed-ups at large time gaps. Experiments on synthetic SDE simulations and on real-world tracking and video data show that NSFs maintain distributional accuracy comparable to numerical approaches while dramatically reducing computation for arbitrary time-point sampling.

**Link**: [arxiv](http://arxiv.org/abs/2510.25769v1),  [pdf](http://arxiv.org/pdf/2510.25769v1)

**Tags**: cs.LG stat.ML 



### Decomposition-Enhanced Training for Post-Hoc Attributions In Language   Models
**Authors**: Sriram Balasubramaniam, Samyadeep Basu, Koustava Goswami, Ryan Rossi, Varun Manjunatha, Roshan Santhosh, Ruiyi Zhang, Soheil Feizi, Nedim Lipka

**Updated**: 2025-10-29T17:58:59Z

**Summary**: Large language models (LLMs) are increasingly used for long-document question answering, where reliable attribution to sources is critical for trust. Existing post-hoc attribution methods work well for extractive QA but struggle in multi-hop, abstractive, and semi-extractive settings, where answers synthesize information across passages. To address these challenges, we argue that post-hoc attribution can be reframed as a reasoning problem, where answers are decomposed into constituent units, each tied to specific context. We first show that prompting models to generate such decompositions alongside attributions improves performance. Building on this, we introduce DecompTune, a post-training method that teaches models to produce answer decompositions as intermediate reasoning steps. We curate a diverse dataset of complex QA tasks, annotated with decompositions by a strong LLM, and post-train Qwen-2.5 (7B and 14B) using a two-stage SFT + GRPO pipeline with task-specific curated rewards. Across extensive experiments and ablations, DecompTune substantially improves attribution quality, outperforming prior methods and matching or exceeding state-of-the-art frontier models.

**Link**: [arxiv](http://arxiv.org/abs/2510.25766v1),  [pdf](http://arxiv.org/pdf/2510.25766v1)

**Tags**: cs.CL 



### 3D Optimization for AI Inference Scaling: Balancing Accuracy, Cost, and   Latency
**Authors**: Minseok Jung, Abhas Ricky, Muhammad Rameez Chatni

**Updated**: 2025-10-29T17:57:23Z

**Summary**: AI inference scaling is often tuned through 1D heuristics (a fixed reasoning passes) or 2D bivariate trade-offs (e.g., performance vs. compute), which fail to consider cost and latency constraints. We introduce a 3D optimization framework that jointly calibrates accuracy, cost, and latency within a unified decision space, enabling constraints-aware inference scaling. Using Monte Carlo simulations across three representative scenarios and nine simulated large language models, we evaluate four optimization methods to address the 3D multi-objective optimization (MOO) problem. Framing inference scaling in MOO shapes a feasible space that 1D and 2D optimizations fail to capture, enabling environmentadaptive selection of the inference scaling k. Results show that knee-point optimization achieves the best balance, while accuracy-maximization remains favorable when precision is prioritized. The framework establishes a theoretical foundation for deployment-aware inference scaling across diverse operational contexts.

**Link**: [arxiv](http://arxiv.org/abs/2510.18905v2),  [pdf](http://arxiv.org/pdf/2510.18905v2)

**Tags**: cs.LG cs.AI 



### SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars
**Authors**: Xiaosheng Zhao, Yang Huang, Guirong Xue, Xiao Kong, Jifeng Liu, Xiaoyu Tang, Timothy C. Beers, Yuan-Sen Ting, A-Li Luo

**Updated**: 2025-10-29T17:57:03Z

**Summary**: In recent years, large language models (LLMs) have transformed natural language understanding through vast datasets and large-scale parameterization. Inspired by this success, we present SpecCLIP, a foundation model framework that extends LLM-inspired methodologies to stellar spectral analysis. Stellar spectra, akin to structured language, encode rich physical and chemical information about stars. By training foundation models on large-scale spectral datasets, our goal is to learn robust and informative embeddings that support diverse downstream applications. As a proof of concept, SpecCLIP involves pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed by contrastive alignment using the CLIP (Contrastive Language-Image Pre-training) framework, adapted to associate spectra from different instruments. This alignment is complemented by auxiliary decoders that preserve spectrum-specific information and enable translation (prediction) between spectral types, with the former achieved by maximizing mutual information between embeddings and input spectra. The result is a cross-spectrum framework enabling intrinsic calibration and flexible applications across instruments. We demonstrate that fine-tuning these models on moderate-sized labeled datasets improves adaptability to tasks such as stellar-parameter estimation and chemical-abundance determination. SpecCLIP also enhances the accuracy and precision of parameter estimates benchmarked against external survey data. Additionally, its similarity search and cross-spectrum prediction capabilities offer potential for anomaly detection. Our results suggest that contrastively trained foundation models enriched with spectrum-aware decoders can advance precision stellar spectroscopy.

**Link**: [arxiv](http://arxiv.org/abs/2507.01939v3),  [pdf](http://arxiv.org/pdf/2507.01939v3)

**Tags**: astro-ph.IM astro-ph.SR cs.AI cs.LG 



### DiagramEval: Evaluating LLM-Generated Diagrams via Graphs
**Authors**: Chumeng Liang, Jiaxuan You

**Updated**: 2025-10-29T17:56:17Z

**Summary**: Diagrams play a central role in research papers for conveying ideas, yet they are often notoriously complex and labor-intensive to create. Although diagrams are presented as images, standard image generative models struggle to produce clear diagrams with well-defined structure. We argue that a promising direction is to generate demonstration diagrams directly in textual form as SVGs, which can leverage recent advances in large language models (LLMs). However, due to the complexity of components and the multimodal nature of diagrams, sufficiently discriminative and explainable metrics for evaluating the quality of LLM-generated diagrams remain lacking. In this paper, we propose DiagramEval, a novel evaluation metric designed to assess demonstration diagrams generated by LLMs. Specifically, DiagramEval conceptualizes diagrams as graphs, treating text elements as nodes and their connections as directed edges, and evaluates diagram quality using two new groups of metrics: node alignment and path alignment. For the first time, we effectively evaluate diagrams produced by state-of-the-art LLMs on recent research literature, quantitatively demonstrating the validity of our metrics. Furthermore, we show how the enhanced explainability of our proposed metrics offers valuable insights into the characteristics of LLM-generated diagrams. Code: https://github.com/ulab-uiuc/diagram-eval.

**Link**: [arxiv](http://arxiv.org/abs/2510.25761v1),  [pdf](http://arxiv.org/pdf/2510.25761v1)

**Tags**: cs.CL 



### TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological   Counseling
**Authors**: He Hu, Yucheng Zhou, Chiyuan Ma, Qianning Wang, Zheng Zhang, Fei Ma, Laizhong Cui, Qi Tian

**Updated**: 2025-10-29T17:54:20Z

**Summary**: Large language models (LLMs) in psychological counseling have attracted increasing attention. However, existing approaches often lack emotional understanding, adaptive strategies, and the use of therapeutic methods across multiple sessions with long-term memory, leaving them far from real clinical practice. To address these critical gaps, we introduce TheraMind, a strategic and adaptive agent for longitudinal psychological counseling. The cornerstone of TheraMind is a novel dual-loop architecture that decouples the complex counseling process into an Intra-Session Loop for tactical dialogue management and a Cross-Session Loop for strategic therapeutic planning. The Intra-Session Loop perceives the patient's emotional state to dynamically select response strategies while leveraging cross-session memory to ensure continuity. Crucially, the Cross-Session Loop empowers the agent with long-term adaptability by evaluating the efficacy of the applied therapy after each session and adjusting the method for subsequent interactions. We validate our approach in a high-fidelity simulation environment grounded in real clinical cases. Extensive evaluations show that TheraMind outperforms other methods, especially on multi-session metrics like Coherence, Flexibility, and Therapeutic Attunement, validating the effectiveness of its dual-loop design in emulating strategic, adaptive, and longitudinal therapeutic behavior. The code is publicly available at https://0mwwm0.github.io/TheraMind/.

**Link**: [arxiv](http://arxiv.org/abs/2510.25758v1),  [pdf](http://arxiv.org/pdf/2510.25758v1)

**Tags**: cs.AI 



### Agentic Economic Modeling
**Authors**: Bohan Zhang, Jiaxuan Li, Ali HortaÃ§su, Xiaoyang Ye, Victor Chernozhukov, Angelo Ni, Edward Huang

**Updated**: 2025-10-29T17:46:07Z

**Summary**: We introduce Agentic Economic Modeling (AEM), a framework that aligns synthetic LLM choices with small-sample human evidence for reliable econometric inference. AEM first generates task-conditioned synthetic choices via LLMs, then learns a bias-correction mapping from task features and raw LLM choices to human-aligned choices, upon which standard econometric estimators perform inference to recover demand elasticities and treatment effects.We validate AEM in two experiments. In a large scale conjoint study with millions of observations, using only 10% of the original data to fit the correction model lowers the error of the demand-parameter estimates, while uncorrected LLM choices even increase the errors. In a regional field experiment, a mixture model calibrated on 10% of geographic regions estimates an out-of-domain treatment effect of -65\pm10 bps, closely matching the full human experiment (-60\pm8 bps).Under time-wise extrapolation, training with only day-one human data yields -24 bps (95% CI: [-26, -22], p<1e-5),improving over the human-only day-one baseline (-17 bps, 95% CI: [-43, +9], p=0.2049).These results demonstrate AEM's potential to improve RCT efficiency and establish a foundation method for LLM-based counterfactual generation.

**Link**: [arxiv](http://arxiv.org/abs/2510.25743v1),  [pdf](http://arxiv.org/pdf/2510.25743v1)

**Tags**: econ.EM 



### Scaling Latent Reasoning via Looped Language Models
**Authors**: Rui-Jie Zhu, Zixuan Wang, Kai Hua, Tianyu Zhang, Ziniu Li, Haoran Que, Boyi Wei, Zixin Wen, Fan Yin, He Xing, Lu Li, Jiajun Shi, Kaijing Ma, Shanda Li, Taylor Kergan, Andrew Smith, Xingwei Qu, Mude Hui, Bohong Wu, Qiyang Min, Hongzhi Huang, Xun Zhou, Wei Ye, Jiaheng Liu, Jian Yang, Yunfeng Shi, Chenghua Lin, Enduo Zhao, Tianle Cai, Ge Zhang, Wenhao Huang, Yoshua Bengio, Jason Eshraghian

**Updated**: 2025-10-29T17:45:42Z

**Summary**: Modern LLMs are trained to "think" primarily via explicit text generation, such as chain-of-thought (CoT), which defers reasoning to post-training and under-leverages pre-training data. We present and open-source Ouro, named after the recursive Ouroboros, a family of pre-trained Looped Language Models (LoopLM) that instead build reasoning into the pre-training phase through (i) iterative computation in latent space, (ii) an entropy-regularized objective for learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and 2.6B models enjoy superior performance that match the results of up to 12B SOTA LLMs across a wide range of benchmarks. Through controlled experiments, we show this advantage stems not from increased knowledge capacity, but from superior knowledge manipulation capabilities. We also show that LoopLM yields reasoning traces more aligned with final outputs than explicit CoT. We hope our results show the potential of LoopLM as a novel scaling direction in the reasoning era. Our model could be found in: http://ouro-llm.github.io.

**Link**: [arxiv](http://arxiv.org/abs/2510.25741v1),  [pdf](http://arxiv.org/pdf/2510.25741v1)

**Tags**: cs.CL 



### Hawk: Leveraging Spatial Context for Faster Autoregressive Text-to-Image   Generation
**Authors**: Zhi-Kai Chen, Jun-Peng Jiang, Han-Jia Ye, De-Chuan Zhan

**Updated**: 2025-10-29T17:43:31Z

**Summary**: Autoregressive (AR) image generation models are capable of producing high-fidelity images but often suffer from slow inference due to their inherently sequential, token-by-token decoding process. Speculative decoding, which employs a lightweight draft model to approximate the output of a larger AR model, has shown promise in accelerating text generation without compromising quality. However, its application to image generation remains largely underexplored. The challenges stem from a significantly larger sampling space, which complicates the alignment between the draft and target model outputs, coupled with the inadequate use of the two-dimensional spatial structure inherent in images, thereby limiting the modeling of local dependencies. To overcome these challenges, we introduce Hawk, a new approach that harnesses the spatial structure of images to guide the speculative model toward more accurate and efficient predictions. Experimental results on multiple text-to-image benchmarks demonstrate a 1.71x speedup over standard AR models, while preserving both image fidelity and diversity.

**Link**: [arxiv](http://arxiv.org/abs/2510.25739v1),  [pdf](http://arxiv.org/pdf/2510.25739v1)

**Tags**: cs.CV cs.LG 



### The Limits of Obliviate: Evaluating Unlearning in LLMs via   Stimulus-Knowledge Entanglement-Behavior Framework
**Authors**: Aakriti Shah, Thai Le

**Updated**: 2025-10-29T17:37:50Z

**Summary**: Unlearning in large language models (LLMs) is crucial for managing sensitive data and correcting misinformation, yet evaluating its effectiveness remains an open problem. We investigate whether persuasive prompting can recall factual knowledge from deliberately unlearned LLMs across models ranging from 2.7B to 13B parameters (OPT-2.7B, LLaMA-2-7B, LLaMA-3.1-8B, LLaMA-2-13B). Drawing from ACT-R and Hebbian theory (spreading activation theories), as well as communication principles, we introduce Stimulus-Knowledge Entanglement-Behavior Framework (SKeB), which models information entanglement via domain graphs and tests whether factual recall in unlearned models is correlated with persuasive framing. We develop entanglement metrics to quantify knowledge activation patterns and evaluate factuality, non-factuality, and hallucination in outputs. Our results show persuasive prompts substantially enhance factual knowledge recall (14.8% baseline vs. 24.5% with authority framing), with effectiveness inversely correlated to model size (128% recovery in 2.7B vs. 15% in 13B). SKeB provides a foundation for assessing unlearning completeness, robustness, and overall behavior in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2510.25732v1),  [pdf](http://arxiv.org/pdf/2510.25732v1)

**Tags**: cs.CL cs.AI I.2.7; I.2.6; I.2.4; G.2.2 



### BambooKG: A Neurobiologically-inspired Frequency-Weight Knowledge Graph
**Authors**: Vanya Arikutharam, Arkadiy Ukolov

**Updated**: 2025-10-29T17:31:27Z

**Summary**: Retrieval-Augmented Generation allows LLMs to access external knowledge, reducing hallucinations and ageing-data issues. However, it treats retrieved chunks independently and struggles with multi-hop or relational reasoning, especially across documents. Knowledge graphs enhance this by capturing the relationships between entities using triplets, enabling structured, multi-chunk reasoning. However, these tend to miss information that fails to conform to the triplet structure. We introduce BambooKG, a knowledge graph with frequency-based weights on non-triplet edges which reflect link strength, drawing on the Hebbian principle of "fire together, wire together". This decreases information loss and results in improved performance on single- and multi-hop reasoning, outperforming the existing solutions.

**Link**: [arxiv](http://arxiv.org/abs/2510.25724v1),  [pdf](http://arxiv.org/pdf/2510.25724v1)

**Tags**: cs.AI 



### gwsnr: A python package for efficient signal-to-noise calculation of   gravitational-waves
**Authors**: Hemantakumar Phurailatpam, Otto Akseli Hannuksela

**Updated**: 2025-10-29T17:29:51Z

**Summary**: Gravitational wave astrophysics requires accurate evaluation of the Signal-to-Noise Ratio (SNR) and the Probability of Detection (Pdet) for applications such as population simulations and hierarchical Bayesian inference with selection effects. Traditional approaches for computing SNR are often computationally demanding and inefficient for large-scale analyses. The gwsnr Python package addresses this challenge by providing efficient and flexible tools for calculating the optimal SNR. It features a user-friendly interface and employs techniques such as a partial-scaling interpolation method for non-precessing binaries, a multiprocessing inner-product routine for frequency-domain waveforms that include spin precession and subdominant modes, among others. High computational performance is achieved through NumPy vectorization and Just-in-Time compilation with Numba, with optional GPU acceleration using JAX and MLX. By combining efficiency, scalability, and ease of use, gwsnr enables large-scale simulations of detectable compact binary mergers and facilitates robust modeling of selection effects through Pdet.

**Link**: [arxiv](http://arxiv.org/abs/2412.09888v3),  [pdf](http://arxiv.org/pdf/2412.09888v3)

**Tags**: astro-ph.IM astro-ph.HE 



### LLMs are Better Than You Think: Label-Guided In-Context Learning for   Named Entity Recognition
**Authors**: Fan Bai, Hamid Hassanzadeh, Ardavan Saeedi, Mark Dredze

**Updated**: 2025-10-29T17:27:45Z

**Summary**: In-context learning (ICL) enables large language models (LLMs) to perform new tasks using only a few demonstrations. However, in Named Entity Recognition (NER), existing ICL methods typically rely on task-agnostic semantic similarity for demonstration retrieval, which often yields less relevant examples and leads to inferior results. We introduce DEER, a training-free ICL approach that enables LLMs to make more informed entity predictions through the use of label-grounded statistics. DEER leverages token-level statistics from training labels to identify tokens most informative for entity recognition, enabling entity-focused demonstrations. It further uses these statistics to detect and refine error-prone tokens through a targeted reflection step. Evaluated on five NER datasets across four LLMs, DEER consistently outperforms existing ICL methods and achieves performance comparable to supervised fine-tuning. Further analyses demonstrate that DEER improves example retrieval, remains effective on both seen and unseen entities, and exhibits strong robustness in low-resource settings.

**Link**: [arxiv](http://arxiv.org/abs/2505.23722v2),  [pdf](http://arxiv.org/pdf/2505.23722v2)

**Tags**: cs.CL 



### Cosmological Constraints from Dark Energy Survey Year 1 Cluster Lensing   and Abundances with Simulation-based Forward-Modeling
**Authors**: AndrÃ©s N. Salcedo, Eduardo Rozo, Hao-Yi Wu, David H. Weinberg, Pranav Chiploonkar, Chun-Hao To, Shulei Cao, Eli S. Rykoff, Nicole Marcelina Gountanis, Conghao Zhou

**Updated**: 2025-10-29T17:17:14Z

**Summary**: We present a simulation-based forward-modeling framework for cosmological inference from optical galaxy-cluster samples, and apply it to the abundance and weak-lensing signals of DES-Y1 redMaPPer clusters. The model embeds cosmology-dependent optical selection using a counts-in-cylinders approach, while also accounting for cluster miscentering and baryonic feedback in lensing. Applied to DES-Y1, and assuming a flat $\Lambda$CDM cosmology, we obtain $\Omega_m=0.254^{+0.026}_{-0.020}$ and $\sigma_8=0.826^{+0.030}_{-0.034}$, consistent with a broad suite of low-redshift structure measurements, including recent full-shape analyses, the DES/KiDS/HSC 3$\times$2 results, and most cluster-abundance studies. Our results are also consistent with \textit{Planck}, with the difference being significant at $2.58\sigma$. These results establish simulation-based forward-modeling of cluster abundances as a promising new tool for precision cosmology with Stage~IV survey data.

**Link**: [arxiv](http://arxiv.org/abs/2510.25706v1),  [pdf](http://arxiv.org/pdf/2510.25706v1)

**Tags**: astro-ph.CO 



### Spontaneous Giving and Calculated Greed in Language Models
**Authors**: Yuxuan Li, Hirokazu Shirado

**Updated**: 2025-10-29T17:15:43Z

**Summary**: Large language models demonstrate strong problem-solving abilities through reasoning techniques such as chain-of-thought prompting and reflection. However, it remains unclear whether these reasoning capabilities extend to a form of social intelligence: making effective decisions in cooperative contexts. We examine this question using economic games that simulate social dilemmas. First, we apply chain-of-thought and reflection prompting to GPT-4o in a Public Goods Game. We then evaluate multiple off-the-shelf models across six cooperation and punishment games, comparing those with and without explicit reasoning mechanisms. We find that reasoning models consistently reduce cooperation and norm enforcement, favoring individual rationality. In repeated interactions, groups with more reasoning agents exhibit lower collective gains. These behaviors mirror human patterns of "spontaneous giving and calculated greed." Our findings underscore the need for LLM architectures that incorporate social intelligence alongside reasoning, to help address--rather than reinforce--the challenges of collective action.

**Link**: [arxiv](http://arxiv.org/abs/2502.17720v4),  [pdf](http://arxiv.org/pdf/2502.17720v4)

**Tags**: cs.CL cs.AI 



### To Rely or Not to Rely? Evaluating Interventions for Appropriate   Reliance on Large Language Models
**Authors**: Jessica Y. Bo, Sophia Wan, Ashton Anderson

**Updated**: 2025-10-29T17:10:45Z

**Summary**: As Large Language Models become integral to decision-making, optimism about their power is tempered with concern over their errors. Users may over-rely on LLM advice that is confidently stated but wrong, or under-rely due to mistrust. Reliance interventions have been developed to help users of LLMs, but they lack rigorous evaluation for appropriate reliance. We benchmark the performance of three relevant interventions by conducting a randomized online experiment with 400 participants attempting two challenging tasks: LSAT logical reasoning and image-based numerical estimation. For each question, participants first answered independently, then received LLM advice modified by one of three reliance interventions and answered the question again. Our findings indicate that while interventions reduce over-reliance, they generally fail to improve appropriate reliance. Furthermore, people became more confident after making wrong reliance decisions in certain contexts, demonstrating poor calibration. Based on our findings, we discuss implications for designing effective reliance interventions in human-LLM collaboration.

**Link**: [arxiv](http://arxiv.org/abs/2412.15584v3),  [pdf](http://arxiv.org/pdf/2412.15584v3)

**Tags**: cs.HC 



### Precise In-Parameter Concept Erasure in Large Language Models
**Authors**: Yoav Gur-Arieh, Clara Suslik, Yihuai Hong, Fazl Barez, Mor Geva

**Updated**: 2025-10-29T17:09:56Z

**Summary**: Large language models (LLMs) often acquire knowledge during pretraining that is undesirable in downstream deployments, e.g., sensitive information or copyrighted content. Existing approaches for removing such knowledge rely on fine-tuning, training low-rank adapters or fact-level editing, but these are either too coarse, too shallow, or ineffective. In this work, we propose PISCES (Precise In-parameter Suppression for Concept EraSure), a novel framework for precisely erasing entire concepts from model parameters by directly editing directions that encode them in parameter space. PISCES uses a disentangler model to decompose MLP vectors into interpretable features, identifies those associated with a target concept using automated interpretability techniques, and removes them from model parameters. Experiments on Gemma 2 and Llama 3.1 over various concepts show that PISCES achieves modest gains in efficacy over leading erasure methods, reducing accuracy on the target concept to as low as 7.7%, while dramatically improving erasure specificity (by up to 31%) and robustness (by up to 38%). Overall, these results demonstrate that feature-based in-parameter editing enables a more precise and reliable approach for removing conceptual knowledge in language models.

**Link**: [arxiv](http://arxiv.org/abs/2505.22586v2),  [pdf](http://arxiv.org/pdf/2505.22586v2)

**Tags**: cs.CL 



### Interpreting LLMs as Credit Risk Classifiers: Do Their Feature   Explanations Align with Classical ML?
**Authors**: Saeed AlMarri, Kristof Juhasz, Mathieu Ravaut, Gautier Marti, Hamdan Al Ahbabi, Ibrahim Elfadel

**Updated**: 2025-10-29T17:05:00Z

**Summary**: Large Language Models (LLMs) are increasingly explored as flexible alternatives to classical machine learning models for classification tasks through zero-shot prompting. However, their suitability for structured tabular data remains underexplored, especially in high-stakes financial applications such as financial risk assessment. This study conducts a systematic comparison between zero-shot LLM-based classifiers and LightGBM, a state-of-the-art gradient-boosting model, on a real-world loan default prediction task. We evaluate their predictive performance, analyze feature attributions using SHAP, and assess the reliability of LLM-generated self-explanations. While LLMs are able to identify key financial risk indicators, their feature importance rankings diverge notably from LightGBM, and their self-explanations often fail to align with empirical SHAP attributions. These findings highlight the limitations of LLMs as standalone models for structured financial risk prediction and raise concerns about the trustworthiness of their self-generated explanations. Our results underscore the need for explainability audits, baseline comparisons with interpretable models, and human-in-the-loop oversight when deploying LLMs in risk-sensitive financial environments.

**Link**: [arxiv](http://arxiv.org/abs/2510.25701v1),  [pdf](http://arxiv.org/pdf/2510.25701v1)

**Tags**: cs.CL 



### SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous   Speech Translation
**Authors**: Chenyang Le, Bing Han, Jinshun Li, Songyong Chen, Yanmin Qian

**Updated**: 2025-10-29T17:02:41Z

**Summary**: Simultaneous Speech Translation (SimulST) enables real-time cross-lingual communication by jointly optimizing speech recognition and machine translation under strict latency constraints. Existing systems struggle to balance translation quality, latency, and semantic coherence, particularly in multilingual many-to-many scenarios where divergent read and write policies hinder unified strategy learning. In this paper, we present SimulMEGA (Simultaneous Generation by Mixture-of-Experts Gating), an unsupervised policy learning framework that combines prefix-based training with a Mixture-of-Experts refiner to learn effective read and write decisions in an implicit manner, without adding inference-time overhead. Our design requires only minimal modifications to standard transformer architectures and generalizes across both speech-to-text and text-to-speech streaming tasks. Through comprehensive evaluation on six language pairs, our 500M parameter speech-to-text model outperforms the Seamless baseline, achieving under 7 percent BLEU degradation at 1.5 seconds average lag and under 3 percent at 3 seconds. We further demonstrate the versatility of SimulMEGA by extending it to streaming TTS with a unidirectional backbone, yielding superior latency quality tradeoffs.

**Link**: [arxiv](http://arxiv.org/abs/2509.01200v2),  [pdf](http://arxiv.org/pdf/2509.01200v2)

**Tags**: cs.CL cs.SD eess.AS 



### Fourier Neural Operators for Two-Phase, 2D Mold-Filling Problems Related   to Metal Casting
**Authors**: Edgard Moreira Minete, Mathis Immertreu, Fabian Teichmann, Sebastian MÃ¼ller

**Updated**: 2025-10-30T12:22:50Z

**Summary**: We formulate mold filling in metal casting as a 2D neural operator learning problem that maps geometry and boundary data on an unstructured mesh to time resolved flow quantities, replacing expensive transient CFD. In the proposed method, a graph based encoder aggregates local neighborhood information on the input mesh and encodes geometry and boundary data, a Fourier spectral core operates on a regular latent grid to capture global interactions across the domain, and a graph based decoder projects the latent fields to a target mesh. The model is trained to jointly predict velocity components, pressure, and liquid volume fraction over a fixed rollout horizon and generalizes across different ingate locations and process settings. On held out geometries and inlet conditions, it reproduces large scale advection and the fluid-air interface evolution with localized errors near steep gradients. The mean relative L2 error is about 5% across all fields, and inference is two to three orders of magnitude faster than conventional CFD, enabling design in the loop exploration. Ablation studies show monotonic accuracy degradation under stronger spatial subsampling of input vertices and a smoother decline under temporal subsampling. Halving the training set yields only a small increase in error. These results establish neural operators as accurate and data efficient surrogates for 2D mold filling and enable rapid optimization of gating systems in casting workflows.

**Link**: [arxiv](http://arxiv.org/abs/2510.25697v2),  [pdf](http://arxiv.org/pdf/2510.25697v2)

**Tags**: cs.CE I.6.3, J.2, I.2.1 



### Towards Scaling Deep Neural Networks with Predictive Coding: Theory and   Practice
**Authors**: Francesco Innocenti

**Updated**: 2025-10-29T16:59:39Z

**Summary**: Backpropagation (BP) is the standard algorithm for training the deep neural networks that power modern artificial intelligence including large language models. However, BP is energy inefficient and unlikely to be implemented by the brain. This thesis studies an alternative, potentially more efficient brain-inspired algorithm called predictive coding (PC). Unlike BP, PC networks (PCNs) perform inference by iterative equilibration of neuron activities before learning or weight updates. Recent work has suggested that this iterative inference procedure provides a range of benefits over BP, such as faster training. However, these advantages have not been consistently observed, the inference and learning dynamics of PCNs are still poorly understood, and deep PCNs remain practically untrainable. Here, we make significant progress towards scaling PCNs by taking a theoretical approach grounded in optimisation theory. First, we show that the learning dynamics of PC can be understood as an approximate trust-region method using second-order information, despite explicitly using only first-order local updates. Second, going beyond this approximation, we show that PC can in principle make use of arbitrarily higher-order information, such that for feedforward networks the effective landscape on which PC learns is far more benign and robust to vanishing gradients than the (mean squared error) loss landscape. Third, motivated by a study of the inference dynamics of PCNs, we propose a new parameterisation called "$\mu$PC", which for the first time allows stable training of 100+ layer networks with little tuning and competitive performance on simple tasks. Overall, this thesis significantly advances our fundamental understanding of the inference and learning dynamics of PCNs, while highlighting the need for future research to focus on hardware co-design if PC is to compete with BP at scale.

**Link**: [arxiv](http://arxiv.org/abs/2510.23323v2),  [pdf](http://arxiv.org/pdf/2510.23323v2)

**Tags**: cs.LG cs.NE I.2.6 



### Process-Level Trajectory Evaluation for Environment Configuration in   Software Engineering Agents
**Authors**: Jiayi Kuang, Yinghui Li, Xin Zhang, Yangning Li, Di Yin, Xing Sun, Ying Shen, Philip S. Yu

**Updated**: 2025-10-29T16:59:07Z

**Summary**: Large language model-based agents show promise for software engineering, but environment configuration remains a bottleneck due to heavy manual effort and scarce large-scale, high-quality datasets. Existing benchmarks assess only end-to-end build/test success, obscuring where and why agents succeed or fail. We introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench, which provides process-level trajectory assessment of fine-grained agent capabilities during environment setup-planning, perception-driven error diagnosis, feedback-driven repair, and action to execute final environment configuration. Our task instances are automatically constructed by injecting realistic README errors and are validated in Docker for scalable, high-quality evaluation. Enconda-bench combines process-level analysis with end-to-end executability to enable capability assessments beyond aggregate success rates. Evaluations across state-of-the-art LLMs and agent frameworks show that while agents can localize errors, they struggle to translate feedback into effective corrections, limiting end-to-end performance. To our knowledge, Enconda-bench is the first framework to provide process-level internal capability assessment for environment configuration, offering actionable insights for improving software engineering agents.

**Link**: [arxiv](http://arxiv.org/abs/2510.25694v1),  [pdf](http://arxiv.org/pdf/2510.25694v1)

**Tags**: cs.SE cs.AI cs.CL 



### Squeezed Light Generation in Periodically Poled Thin-Film Lithium   Niobate Waveguides
**Authors**: Xiaodong Shi, Angela Anna Baiju, Xu Chen, Sakthi Sanjeev Mohanraj, Sihao Wang, Veerendra Dhyani, Biveen Shajilal, Mengyao Zhao, Ran Yang, Yue Li, Guangxing Wu, Hao Hao, Victor Leong, Ping Koy Lam, Di Zhu

**Updated**: 2025-10-29T16:51:36Z

**Summary**: Squeezed states of light play a key role in quantum-enhanced sensing and continuous-variable quantum information processing. Realizing integrated squeezed light sources is crucial for developing compact and scalable photonic quantum systems. In this work, we demonstrate on-chip broadband vacuum squeezing at telecommunication wavelengths on the thin-film lithium niobate (TFLN) platform. Our device integrates periodically poled lithium niobate (PPLN) nanophotonic waveguides with low-loss edge couplers, comprising bilayer inverse tapers and an SU-8 polymer waveguide. This configuration achieves a fiber-to-chip coupling loss of 1.4 dB and a total homodyne detection loss of 4 dB, enabling a measured squeezing level of 1.4 dB. Additional measurements in a more efficient PPLN waveguide (without low-loss couplers) infer an on-chip squeezing level of over 10 dB at a pump power of 62 mW. These results underscore the potential of TFLN platform for efficient and scalable squeezed light generation.

**Link**: [arxiv](http://arxiv.org/abs/2508.08599v2),  [pdf](http://arxiv.org/pdf/2508.08599v2)

**Tags**: physics.optics 



### Graph Network-based Structural Simulator: Graph Neural Networks for   Structural Dynamics
**Authors**: Alessandro Lucchetti, Francesco Cadini, Marco Giglio, Luca Lomazzi

**Updated**: 2025-10-29T16:47:24Z

**Summary**: Graph Neural Networks (GNNs) have recently been explored as surrogate models for numerical simulations. While their applications in computational fluid dynamics have been investigated, little attention has been given to structural problems, especially for dynamic cases. To address this gap, we introduce the Graph Network-based Structural Simulator (GNSS), a GNN framework for surrogate modeling of dynamic structural problems.   GNSS follows the encode-process-decode paradigm typical of GNN-based machine learning models, and its design makes it particularly suited for dynamic simulations thanks to three key features: (i) expressing node kinematics in node-fixed local frames, which avoids catastrophic cancellation in finite-difference velocities; (ii) employing a sign-aware regression loss, which reduces phase errors in long rollouts; and (iii) using a wavelength-informed connectivity radius, which optimizes graph construction.   We evaluate GNSS on a case study involving a beam excited by a 50kHz Hanning-modulated pulse. The results show that GNSS accurately reproduces the physics of the problem over hundreds of timesteps and generalizes to unseen loading conditions, where existing GNNs fail to converge or deliver meaningful predictions.   Compared with explicit finite element baselines, GNSS achieves substantial inference speedups while preserving spatial and temporal fidelity. These findings demonstrate that locality-preserving GNNs with physics-consistent update rules are a competitive alternative for dynamic, wave-dominated structural simulations.

**Link**: [arxiv](http://arxiv.org/abs/2510.25683v1),  [pdf](http://arxiv.org/pdf/2510.25683v1)

**Tags**: cs.LG cs.AI cs.CE physics.comp-ph 



### ZK-SenseLM: Verifiable Large-Model Wireless Sensing with Selective   Abstention and Zero-Knowledge Attestation
**Authors**: Hasan Akgul, Mari Eplik, Javier Rojas, Aina Binti Abdullah, Pieter van der Merwe

**Updated**: 2025-10-29T16:43:07Z

**Summary**: ZK-SenseLM is a secure and auditable wireless sensing framework that pairs a large-model encoder for Wi-Fi channel state information (and optionally mmWave radar or RFID) with a policy-grounded decision layer and end-to-end zero-knowledge proofs of inference. The encoder uses masked spectral pretraining with phase-consistency regularization, plus a light cross-modal alignment that ties RF features to compact, human-interpretable policy tokens. To reduce unsafe actions under distribution shift, we add a calibrated selective-abstention head; the chosen risk-coverage operating point is registered and bound into the proof. We implement a four-stage proving pipeline: (C1) feature sanity and commitment, (C2) threshold and version binding, (C3) time-window binding, and (C4) PLONK-style proofs that the quantized network, given the committed window, produced the logged action and confidence. Micro-batched proving amortizes cost across adjacent windows, and a gateway option offloads proofs from low-power devices. The system integrates with differentially private federated learning and on-device personalization without weakening verifiability: model hashes and the registered threshold are part of each public statement. Across activity, presence or intrusion, respiratory proxy, and RF fingerprinting tasks, ZK-SenseLM improves macro-F1 and calibration, yields favorable coverage-risk curves under perturbations, and rejects tamper and replay with compact proofs and fast verification.

**Link**: [arxiv](http://arxiv.org/abs/2510.25677v1),  [pdf](http://arxiv.org/pdf/2510.25677v1)

**Tags**: cs.CR cs.CL C.2.1; D.4.6; E.3; I.2.6; I.5.4 



### Mechanistic Interpretability of RNNs emulating Hidden Markov Models
**Authors**: Elia Torre, Michele Viscione, Lucas Pompe, Benjamin F Grewe, Valerio Mante

**Updated**: 2025-10-29T16:42:07Z

**Summary**: Recurrent neural networks (RNNs) provide a powerful approach in neuroscience to infer latent dynamics in neural populations and to generate hypotheses about the neural computations underlying behavior. However, past work has focused on relatively simple, input-driven, and largely deterministic behaviors - little is known about the mechanisms that would allow RNNs to generate the richer, spontaneous, and potentially stochastic behaviors observed in natural settings. Modeling with Hidden Markov Models (HMMs) has revealed a segmentation of natural behaviors into discrete latent states with stochastic transitions between them, a type of dynamics that may appear at odds with the continuous state spaces implemented by RNNs. Here we first show that RNNs can replicate HMM emission statistics and then reverse-engineer the trained networks to uncover the mechanisms they implement. In the absence of inputs, the activity of trained RNNs collapses towards a single fixed point. When driven by stochastic input, trajectories instead exhibit noise-sustained dynamics along closed orbits. Rotation along these orbits modulates the emission probabilities and is governed by transitions between regions of slow, noise-driven dynamics connected by fast, deterministic transitions. The trained RNNs develop highly structured connectivity, with a small set of "kick neurons" initiating transitions between these regions. This mechanism emerges during training as the network shifts into a regime of stochastic resonance, enabling it to perform probabilistic computations. Analyses across multiple HMM architectures - fully connected, cyclic, and linear-chain - reveal that this solution generalizes through the modular reuse of the same dynamical motif, suggesting a compositional principle by which RNNs can emulate complex discrete latent dynamics.

**Link**: [arxiv](http://arxiv.org/abs/2510.25674v1),  [pdf](http://arxiv.org/pdf/2510.25674v1)

**Tags**: cs.LG 



### User Misconceptions of LLM-Based Conversational Programming Assistants
**Authors**: Gabrielle O'Brien, Antonio Pedro Santos Alves, Sebastian Baltes, Grischa Liebel, Mircea Lungu, Marcos Kalinowski

**Updated**: 2025-10-29T16:23:46Z

**Summary**: Programming assistants powered by large language models (LLMs) have become widely available, with conversational assistants like ChatGPT proving particularly accessible to less experienced programmers. However, the varied capabilities of these tools across model versions and the mixed availability of extensions that enable web search, code execution, or retrieval-augmented generation create opportunities for user misconceptions about what systems can and cannot do. Such misconceptions may lead to over-reliance, unproductive practices, or insufficient quality control in LLM-assisted programming. Here, we aim to characterize misconceptions that users of conversational LLM-based assistants may have in programming contexts. Using a two-phase approach, we first brainstorm and catalog user misconceptions that may occur, and then conduct a qualitative analysis to examine whether these conceptual issues surface in naturalistic Python-programming conversations with an LLM-based chatbot drawn from an openly available dataset. Indeed, we see evidence that some users have misplaced expectations about the availability of LLM-based chatbot features like web access, code execution, or non-text output generation. We also see potential evidence for deeper conceptual issues around the scope of information required to debug, validate, and optimize programs. Our findings reinforce the need for designing LLM-based tools that more clearly communicate their programming capabilities to users.

**Link**: [arxiv](http://arxiv.org/abs/2510.25662v1),  [pdf](http://arxiv.org/pdf/2510.25662v1)

**Tags**: cs.HC cs.AI 



### Accurate Leakage Speculation for Quantum Error Correction
**Authors**: Chaithanya Naik Mude, Swamit Tannu

**Updated**: 2025-10-29T16:23:27Z

**Summary**: Quantum Error Correction (QEC) protects qubits against bit- and phase-flip errors in the |0> or |1> subspace, but physical qubits can also leak into higher energy levels (e.g., |2>). Leakage is especially harmful, as it corrupts all subsequent syndrome measurements and can spread to neighboring qubits. Detecting leakage on data qubits is particularly challenging, since they are never measured directly during QEC cycles. Prior work, such as eraser, addresses this by inferring leakage from syndrome patterns using a fixed heuristic. However, this approach often misclassifies benign syndromes, triggering excessive leakage-reduction circuits (LRCs). Because LRCs are themselves noisy and slow, these false triggers lengthen QEC cycles and inflate logical error rates.   We propose gladiator, a general and adaptable leakage speculation framework that works across surface code, color code, and qLDPC codes. Offline, gladiator builds a code-aware error-propagation graph calibrated to device data. Online, it classifies each syndrome in a few nanoseconds and schedules LRC only when the observed pattern is provably leakage-dominated. This precise speculation eliminates up to 3x (and on average 2x) unnecessary LRCs, shortens QEC cycles, and suppresses false positives at their source. Evaluated on standard fault-tolerant benchmarks, gladiator delivers 1.7x-3.9x speedups and 16% reduction in logical error rate, advancing the efficiency of fault-tolerant quantum computing.

**Link**: [arxiv](http://arxiv.org/abs/2510.25661v1),  [pdf](http://arxiv.org/pdf/2510.25661v1)

**Tags**: quant-ph cs.AR 



### Extreme equivalent width-selected low-mass starbursts at $z=4-9$:   insights into their role in cosmic reionization
**Authors**: M. Llerena, L. Pentericci, R. AmorÃ­n, A. Ferrara, M. Dickinson, F. Arevalo, A. CalabrÃ², L. Napolitano, S. Mascia, P. Arrabal Haro, R. Begley, N. J. Cleri, K. Davis, W. Hu, J. S. Kartaltepe, A. M. Koekemoer, R. A. Lucas, E. McGrath, D. J. McLeod, C. Papovich, T. M. Stanton, A. J. Taylor, R. Tripodi, X. Wang, L. Y. A. Yung

**Updated**: 2025-10-29T16:07:51Z

**Summary**: We investigate the properties of extreme emission line galaxies (EELGs) at $z=4-9$ and their role in reionization. Compact, low-mass galaxies with intense optical emission lines are linked to elevated specific star formation rates (sSFRs) and recent bursts of star formation. Feedback in these systems may enable the leakage of ionizing radiation into the intergalactic medium. Using JWST/NIRSpec spectroscopy from the CAPERS, CEERS, and RUBIES surveys, we compile 160 NIRCam-selected EELGs in the EGS field. These galaxies show extreme rest-frame equivalent widths (EWs), with a median EW([O III]+H${\beta}$)=1616\r{A} and EW(H${\alpha}$)=763\r{A}. They are low-mass (median log(M$_{\star}$/M$_{\odot}$)=8.26) with high sSFRs (median 43 Gyr$^{-1}$), above the $z\sim6$ main sequence. UV slopes are diverse, with a mean $\beta=-2.0$, and only 7% have extremely blue continua ($\beta<-2.6$). Emission-line diagnostics suggest stellar populations as the primary ionizing source, although an AGN fraction of 14% is found. These galaxies are efficient ionizing photon producers, with mean log($\xi_{\rm ion}$ [Hz erg$^{-1}$])=25.34, exceeding typical values at similar redshifts. Escape fractions, however, are heterogeneous: 9% of EELGs show escape fractions $>$10% for both Ly${\alpha}$ and LyC photons, while 82% lack detectable Ly${\alpha}$ emission. The median inferred LyC escape fraction is modest (4.4%) but enhanced in super-Eddington systems with sSFR >25 Gyr$^{-1}$. The galaxies are extremely compact, with a median effective radius of 0.49 kpc, and exhibit a recent star-formation burst. Our analysis indicates that sSFR and star-formation rate surface density are the primary drivers of their extreme emission line strengths.

**Link**: [arxiv](http://arxiv.org/abs/2510.25647v1),  [pdf](http://arxiv.org/pdf/2510.25647v1)

**Tags**: astro-ph.GA 



### OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement   Learning
**Authors**: Ziyou Hu, Zhengliang Shi, Minghang Zhu, Haitao Li, Teng Sun, Pengjie Ren, Suzan Verberne, Zhaochun Ren

**Updated**: 2025-10-29T16:06:18Z

**Summary**: Reward models (RMs) have become essential for aligning large language models (LLMs), serving as scalable proxies for human evaluation in both training and inference. However, existing RMs struggle on knowledge-intensive and long-form tasks, where evaluating correctness requires grounding beyond the model's internal knowledge. This limitation hinders them from reliably discriminating subtle quality differences, especially when external evidence is necessary. To address this, we introduce OpenRM, a tool-augmented long-form reward model that systematically judges open-ended responses by invoking external tools to gather relevant evidence. We train OpenRM with Group Relative Policy Optimization (GRPO) on over 27K synthesized pairwise examples generated through a controllable data synthesis framework. The training objective jointly supervises intermediate tool usage and final outcome accuracy, incentivizing our reward model to learn effective evidence-based judgment strategies. Extensive experiments on three newly-collected datasets and two widely-used benchmarks demonstrate that OpenRM substantially outperforms existing reward modeling approaches. As a further step, we integrate OpenRM into both inference-time response selection and training-time data selection. This yields consistent gains in downstream LLM alignment tasks, highlighting the potential of tool-augmented reward models for scaling reliable long-form evaluation.

**Link**: [arxiv](http://arxiv.org/abs/2510.24636v2),  [pdf](http://arxiv.org/pdf/2510.24636v2)

**Tags**: cs.CL 



### DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware   Regressive GRPO
**Authors**: Jinyoung Park, Jeehye Na, Jinyoung Kim, Hyunwoo J. Kim

**Updated**: 2025-10-29T15:59:41Z

**Summary**: Recent works have demonstrated the effectiveness of reinforcement learning (RL)-based post-training for enhancing the reasoning capabilities of large language models (LLMs). In particular, Group Relative Policy Optimization (GRPO) has shown impressive success using a PPO-style reinforcement algorithm with group-normalized rewards. However, the effectiveness of GRPO in Video Large Language Models (VideoLLMs) has still been less studyed. In this paper, we explore GRPO and identify two problems that deteriorate the effective learning: (1) reliance on safeguards, and (2) vanishing advantage. To mitigate these challenges, we propose DeepVideo-R1, a video large language model trained with Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation. Reg-GRPO reformulates the GRPO loss function into a regression task that directly predicts the advantage in GRPO, eliminating the need for safeguards such as the clipping and min functions. It directly aligns the model with advantages, providing guidance to prefer better ones. The difficulty-aware data augmentation strategy augments input prompts/videos to locate the difficulty of samples at solvable difficulty levels, enabling diverse reward signals. Our experimental results show that our approach significantly improves video reasoning performance across multiple benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2506.07464v3),  [pdf](http://arxiv.org/pdf/2506.07464v3)

**Tags**: cs.CV cs.AI 



### Exploring the In-Context Learning Capabilities of LLMs for Money   Laundering Detection in Financial Graphs
**Authors**: Erfan Pirmorad

**Updated**: 2025-10-29T15:56:28Z

**Summary**: The complexity and interconnectivity of entities involved in money laundering demand investigative reasoning over graph-structured data. This paper explores the use of large language models (LLMs) as reasoning engines over localized subgraphs extracted from a financial knowledge graph. We propose a lightweight pipeline that retrieves k-hop neighborhoods around entities of interest, serializes them into structured text, and prompts an LLM via few-shot in-context learning to assess suspiciousness and generate justifications. Using synthetic anti-money laundering (AML) scenarios that reflect common laundering behaviors, we show that LLMs can emulate analyst-style logic, highlight red flags, and provide coherent explanations. While this study is exploratory, it illustrates the potential of LLM-based graph reasoning in AML and lays groundwork for explainable, language-driven financial crime analytics.

**Link**: [arxiv](http://arxiv.org/abs/2507.14785v2),  [pdf](http://arxiv.org/pdf/2507.14785v2)

**Tags**: cs.LG cs.AI 



### The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity
**Authors**: Ali Aouad, Aymane El Gadarri, Vivek F. Farias

**Updated**: 2025-10-29T15:51:35Z

**Summary**: Traditional LLM alignment methods are vulnerable to heterogeneity in human preferences. Fitting a na\"ive probabilistic model to pairwise comparison data (say over prompt-completion pairs) yields an inconsistent estimate of the population-average utility -a canonical measure of social welfare. We propose a new method, dubbed the sign estimator, that provides a simple, provably consistent, and efficient estimator by replacing cross-entropy with binary classification loss in the aggregation step. This simple modification recovers consistent ordinal alignment under mild assumptions and achieves the first polynomial finite-sample error bounds in this setting. In realistic simulations of LLM alignment using digital twins, the sign estimator substantially reduces preference distortion over a panel of simulated personas, cutting (angular) estimation error by nearly 35% and decreasing disagreement with true population preferences from 12% to 8% compared to standard RLHF. Our method also compares favorably to panel data heuristics that explicitly model user heterogeneity and require tracking individual-level preference data-all while maintaining the implementation simplicity of existing LLM alignment pipelines.

**Link**: [arxiv](http://arxiv.org/abs/2510.23965v2),  [pdf](http://arxiv.org/pdf/2510.23965v2)

**Tags**: cs.AI cs.LG stat.ML 



### EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic   Health Record Analysis
**Authors**: Yusheng Liao, Chaoyi Wu, Junwei Liu, Shuyang Jiang, Pengcheng Qiu, Haowen Wang, Yun Yue, Shuai Zhen, Jian Wang, Qianrui Fan, Jinjie Gu, Ya Zhang, Yanfeng Wang, Yu Wang, Weidi Xie

**Updated**: 2025-10-29T15:32:47Z

**Summary**: Electronic Health Records (EHRs) contain rich yet complex information, and their automated analysis is critical for clinical decision-making. Despite recent advances of large language models (LLMs) in clinical workflows, their ability to analyze EHRs remains limited due to narrow task coverage and lack of EHR-oriented reasoning capabilities. This paper aims to bridge the gap, specifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning instruction dataset, comprising 300k high-quality reasoning cases and 4M non-reasoning cases across 42 distinct EHR tasks. Its core innovation is a thinking-graph-driven framework that enables to generate high-quality reasoning data at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced LLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage training paradigm, including domain adaptation, reasoning enhancement, and reinforcement learning, EHR-R1 systematically acquires domain knowledge and diverse reasoning capabilities, enabling accurate and robust EHR analysis. Lastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning 42 tasks, to comprehensively assess reasoning and prediction across EHR scenarios. In experiments, we show that the resulting EHR-R1 consistently outperforms state-of-the-art commercial and open-source LLMs (including DeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and achieving a 10\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins, EHR-R1, and EHR-Bench have significantly advanced the development for more reliable and clinically relevant EHR analysis.

**Link**: [arxiv](http://arxiv.org/abs/2510.25628v1),  [pdf](http://arxiv.org/pdf/2510.25628v1)

**Tags**: cs.CL 



### Are Language Models Efficient Reasoners? A Perspective from Logic   Programming
**Authors**: Andreas Opedal, Yanick Zengaffinen, Haruki Shirakami, Clemente Pasti, Mrinmaya Sachan, Abulhair Saparov, Ryan Cotterell, Bernhard SchÃ¶lkopf

**Updated**: 2025-10-29T15:30:31Z

**Summary**: Modern language models (LMs) exhibit strong deductive reasoning capabilities, yet standard evaluations emphasize correctness while overlooking a key aspect of human-like reasoning: efficiency. In real-world reasoning scenarios, much of the available information is irrelevant, and effective deductive inference requires identifying and ignoring such distractions. We propose a framework for assessing LM reasoning efficiency through the lens of logic programming, introducing a simple method to align proofs written in natural language -- as generated by an LM -- with shortest proofs found by executing the logic program. Efficiency is quantified by measuring how well a model avoids unnecessary inference. Empirically, we construct a dataset of math word problems injected with various number of irrelevant axioms that vary in semantic overlap with the goal theorem. We find that current LMs show marked accuracy declines under such conditions -- even with minimal, domain-consistent distractions -- and the proofs they generate frequently exhibit detours through irrelevant inferences.

**Link**: [arxiv](http://arxiv.org/abs/2510.25626v1),  [pdf](http://arxiv.org/pdf/2510.25626v1)

**Tags**: cs.CL cs.AI cs.LG cs.LO 



### Evaluating the Role of Verifiers in Test-Time Scaling for Legal   Reasoning Tasks
**Authors**: Davide Romano, Jonathan Schwarz, Daniele GiofrÃ©

**Updated**: 2025-10-29T15:27:47Z

**Summary**: Test-time scaling (TTS) techniques can improve the performance of large language models (LLMs) at the expense of additional computation and latency. While TTS has proven effective in formal domains such as mathematics and programming \citep{snell2024scaling, chen2024more}, its value in argumentative domains such as law remains underexplored. We present an empirical study of verifier-based TTS methods for legal multiple-choice QA (MCQA) across five benchmarks. Using a family of 7 reward models, we evaluate both outcome-level (Best-of-$N$) and process-level (tree search) verification under realistic low-$N$ budgets. Our analysis systematically investigates how verifier utility is affected by key properties such as domain specialization, model size, and supervision type (process-supervised PRMs vs. outcome-only ORMs), even when applied across different roles.

**Link**: [arxiv](http://arxiv.org/abs/2510.25623v1),  [pdf](http://arxiv.org/pdf/2510.25623v1)

**Tags**: cs.CL 



### Towards a Common Framework for Autoformalization
**Authors**: Agnieszka Mensfelt, David Tena Cucala, Santiago Franco, Angeliki Koutsoukou-Argyraki, Vince Trencsenyi, Kostas Stathis

**Updated**: 2025-10-29T15:27:12Z

**Summary**: Autoformalization has emerged as a term referring to the automation of formalization - specifically, the formalization of mathematics using interactive theorem provers (proof assistants). Its rapid development has been driven by progress in deep learning, especially large language models (LLMs). More recently, the term has expanded beyond mathematics to describe the broader task of translating informal input into formal logical representations. At the same time, a growing body of research explores using LLMs to translate informal language into formal representations for reasoning, planning, and knowledge representation - often without explicitly referring to this process as autoformalization. As a result, despite addressing similar tasks, the largely independent development of these research areas has limited opportunities for shared methodologies, benchmarks, and theoretical frameworks that could accelerate progress. The goal of this paper is to review - explicit or implicit - instances of what can be considered autoformalization and to propose a unified framework, encouraging cross-pollination between different fields to advance the development of next generation AI systems.

**Link**: [arxiv](http://arxiv.org/abs/2509.09810v2),  [pdf](http://arxiv.org/pdf/2509.09810v2)

**Tags**: cs.AI 



### FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering
**Authors**: Mohammad Aghajani Asl, Behrooz Minaei Bidgoli

**Updated**: 2025-10-29T15:25:34Z

**Summary**: The advent of Large Language Models (LLMs) has revolutionized Natural Language Processing, yet their application in high-stakes, specialized domains like religious question answering is hindered by challenges like hallucination and unfaithfulness to authoritative sources. This issue is particularly critical for the Persian-speaking Muslim community, where accuracy and trustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG) systems, relying on simplistic single-pass pipelines, fall short on complex, multi-hop queries requiring multi-step reasoning and evidence aggregation. To address this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful Advanced Question Answering in the Persian Islamic domain. FARSIQA is built upon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative Refinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting process: it adaptively decomposes complex queries, assesses evidence sufficiency, and enters an iterative loop to generate sub-queries, progressively filling information gaps. Operating on a curated knowledge base of over one million authoritative Islamic documents, FARSIQA demonstrates superior performance. Rigorous evaluation on the challenging IslamicPCQA benchmark shows state-of-the-art performance: the system achieves a remarkable 97.0% in Negative Rejection - a 40-point improvement over baselines - and a high Answer Correctness score of 74.3%. Our work establishes a new standard for Persian Islamic QA and validates that our iterative, adaptive architecture is crucial for building faithful, reliable AI systems in sensitive domains.

**Link**: [arxiv](http://arxiv.org/abs/2510.25621v1),  [pdf](http://arxiv.org/pdf/2510.25621v1)

**Tags**: cs.CL cs.AI cs.IR 68T50, 68T05, 68T30 I.2.7; H.3.3 



### RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness
**Authors**: Tianyu Yu, Haoye Zhang, Qiming Li, Qixin Xu, Yuan Yao, Da Chen, Xiaoman Lu, Ganqu Cui, Yunkai Dang, Taiwen He, Xiaocheng Feng, Jun Song, Bo Zheng, Zhiyuan Liu, Tat-Seng Chua, Maosong Sun

**Updated**: 2025-10-29T15:19:12Z

**Summary**: Traditional feedback learning for hallucination reduction relies on labor-intensive manual labeling or expensive proprietary models. This leaves the community without foundational knowledge about how to build high-quality feedback with open-source MLLMs. In this work, we introduce RLAIF-V, a novel framework that aligns MLLMs in a fully open-source paradigm. RLAIF-V maximally explores open-source MLLMs from two perspectives, including high-quality feedback data generation for preference learning and self-feedback guidance for inference-time scaling. Extensive experiments on six benchmarks in both automatic and human evaluation show that RLAIF-V substantially enhances the trustworthiness of models at both preference learning and inference time. RLAIF-V 7B reduces object hallucination by 80.7\% and overall hallucination by 33.7\%. Remarkably, RLAIF-V 12B further reveals the self-alignment potential of open-source MLLMs, where the model can learn from feedback of itself to achieve super GPT-4V trustworthiness.

**Link**: [arxiv](http://arxiv.org/abs/2405.17220v3),  [pdf](http://arxiv.org/pdf/2405.17220v3)

**Tags**: cs.CL 



### Counterfactual-based Agent Influence Ranker for Agentic AI Workflows
**Authors**: Amit Giloni, Chiara Picardi, Roy Betser, Shamik Bose, Aishvariya Priya Rathina Sabapathy, Roman Vainshtein

**Updated**: 2025-10-29T15:17:31Z

**Summary**: An Agentic AI Workflow (AAW), also known as an LLM-based multi-agent system, is an autonomous system that assembles several LLM-based agents to work collaboratively towards a shared goal. The high autonomy, widespread adoption, and growing interest in such AAWs highlight the need for a deeper understanding of their operations, from both quality and security aspects. To this day, there are no existing methods to assess the influence of each agent on the AAW's final output. Adopting techniques from related fields is not feasible since existing methods perform only static structural analysis, which is unsuitable for inference time execution. We present Counterfactual-based Agent Influence Ranker (CAIR) - the first method for assessing the influence level of each agent on the AAW's output and determining which agents are the most influential. By performing counterfactual analysis, CAIR provides a task-agnostic analysis that can be used both offline and at inference time. We evaluate CAIR using an AAWs dataset of our creation, containing 30 different use cases with 230 different functionalities. Our evaluation showed that CAIR produces consistent rankings, outperforms baseline methods, and can easily enhance the effectiveness and relevancy of downstream tasks.

**Link**: [arxiv](http://arxiv.org/abs/2510.25612v1),  [pdf](http://arxiv.org/pdf/2510.25612v1)

**Tags**: cs.AI cs.MA 



### Inference on Welfare and Value Functionals under Optimal Treatment   Assignment
**Authors**: Xiaohong Chen, Zhenxiao Chen, Wayne Yuan Gao

**Updated**: 2025-10-29T15:16:11Z

**Summary**: We provide theoretical results for the estimation and inference of a class of welfare and value functionals of the nonparametric conditional average treatment effect (CATE) function under optimal treatment assignment, i.e., treatment is assigned to an observed type if and only if its CATE is nonnegative. For the optimal welfare functional defined as the average value of CATE on the subpopulation with nonnegative CATE, we establish the $\sqrt{n}$ asymptotic normality of the semiparametric plug-in estimators and provide an analytical asymptotic variance formula. For more general value functionals, we show that the plug-in estimators are typically asymptotically normal at the 1-dimensional nonparametric estimation rate, and we provide a consistent variance estimator based on the sieve Riesz representer, as well as a proposed computational procedure for numerical integration on submanifolds. The key reason underlying the different convergence rates for the welfare functional versus the general value functional lies in that, on the boundary subpopulation for whom CATE is zero, the integrand vanishes for the welfare functional but does not for general value functionals. We demonstrate in Monte Carlo simulations the good finite-sample performance of our estimation and inference procedures, and conduct an empirical application of our methods on the effectiveness of job training programs on earnings using the JTPA data set.

**Link**: [arxiv](http://arxiv.org/abs/2510.25607v1),  [pdf](http://arxiv.org/pdf/2510.25607v1)

**Tags**: econ.EM 



### INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization   Formats
**Authors**: Mengzhao Chen, Meng Wu, Hui Jin, Zhihang Yuan, Jing Liu, Chaoyi Zhang, Yunshui Li, Jie Huang, Jin Ma, Zeyue Xue, Zhiheng Liu, Xingyan Bin, Ping Luo

**Updated**: 2025-10-29T15:11:53Z

**Summary**: Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly embracing low-precision floating-point (FP) formats to handle the pervasive activation outliers in Large Language Models (LLMs). Despite this industry trend, a unified comparison of FP and integer (INT) quantization across varying granularities has been missing, leaving algorithm and hardware co-design without clear guidance. This paper fills that gap by systematically investigating the trade-offs between FP and INT formats. We reveal a critical performance crossover: while FP excels in coarse-grained quantization, the comparison at fine-grained (block-wise) levels is more nuanced. Our comprehensive comparison demonstrates that for popular 8-bit fine-grained formats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart in both algorithmic accuracy and hardware efficiency. However, for 4-bit formats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we show that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like Hadamard rotation are applied. We also introduce a symmetric clipping method that resolves gradient bias in fine-grained low-bit INT training, enabling nearly lossless performance for MXINT8 training. These findings challenge the current hardware trajectory, demonstrating that a one-size-fits-all FP approach is suboptimal and advocating that fine-grained INT formats, particularly MXINT8, offer a better balance of accuracy, power, and efficiency for future AI accelerators.

**Link**: [arxiv](http://arxiv.org/abs/2510.25602v1),  [pdf](http://arxiv.org/pdf/2510.25602v1)

**Tags**: cs.LG cs.AI 



### Communication and Verification in LLM Agents towards Collaboration under   Information Asymmetry
**Authors**: Run Peng, Ziqiao Ma, Amy Pang, Sikai Li, Zhang Xi-Jia, Yingzhuo Yu, Cristian-Paul Bara, Joyce Chai

**Updated**: 2025-10-29T15:03:53Z

**Summary**: While Large Language Model (LLM) agents are often approached from the angle of action planning/generation to accomplish a goal (e.g., given by language descriptions), their abilities to collaborate with each other to achieve a joint goal are not well explored. To address this limitation, this paper studies LLM agents in task collaboration, particularly under the condition of information asymmetry, where agents have disparities in their knowledge and skills and need to work together to complete a shared task. We extend Einstein Puzzles, a classical symbolic puzzle, to a table-top game. In this game, two LLM agents must reason, communicate, and act to satisfy spatial and relational constraints required to solve the puzzle. We apply a fine-tuning-plus-verifier framework in which LLM agents are equipped with various communication strategies and verification signals from the environment. Empirical results highlight the critical importance of aligned communication, especially when agents possess both information-seeking and -providing capabilities. Interestingly, agents without communication can still achieve high task performance; however, further analysis reveals a lack of true rule understanding and lower trust from human evaluators. Instead, by integrating an environment-based verifier, we enhance agents' ability to comprehend task rules and complete tasks, promoting both safer and more interpretable collaboration in AI systems. https://github.com/Roihn/EinsteinPuzzles

**Link**: [arxiv](http://arxiv.org/abs/2510.25595v1),  [pdf](http://arxiv.org/pdf/2510.25595v1)

**Tags**: cs.CL cs.AI 



### HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based   Sequential Recommendation
**Authors**: Jingyi Zhou, Cheng Chen, Kai Zuo, Manjie Xu, Zhendong Fu, Yibo Chen, Xu Tang, Yao Hu

**Updated**: 2025-10-29T15:00:42Z

**Summary**: Large language models (LLMs) have recently demonstrated strong potential for sequential recommendation. However, current LLM-based approaches face critical limitations in modeling users' long-term and diverse interests. First, due to inference latency and feature fetching bandwidth constraints, existing methods typically truncate user behavior sequences to include only the most recent interactions, resulting in the loss of valuable long-range preference signals. Second, most current methods rely on next-item prediction with a single predicted embedding, overlooking the multifaceted nature of user interests and limiting recommendation diversity. To address these challenges, we propose HyMiRec, a hybrid multi-interest sequential recommendation framework, which leverages a lightweight recommender to extracts coarse interest embeddings from long user sequences and an LLM-based recommender to captures refined interest embeddings. To alleviate the overhead of fetching features, we introduce a residual codebook based on cosine similarity, enabling efficient compression and reuse of user history embeddings. To model the diverse preferences of users, we design a disentangled multi-interest learning module, which leverages multiple interest queries to learn disentangles multiple interest signals adaptively, allowing the model to capture different facets of user intent. Extensive experiments are conducted on both benchmark datasets and a collected industrial dataset, demonstrating our effectiveness over existing state-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec brings consistent improvements in real-world recommendation systems. Code is available at https://github.com/FireRedTeam/FireRedSeqRec.

**Link**: [arxiv](http://arxiv.org/abs/2510.13738v2),  [pdf](http://arxiv.org/pdf/2510.13738v2)

**Tags**: cs.IR 



### RegionE: Adaptive Region-Aware Generation for Efficient Image Editing
**Authors**: Pengtao Chen, Xianfang Zeng, Maosen Zhao, Mingzhu Shen, Peng Ye, Bangyin Xiang, Zhibo Wang, Wei Cheng, Gang Yu, Tao Chen

**Updated**: 2025-10-29T14:58:37Z

**Summary**: Recently, instruction-based image editing (IIE) has received widespread attention. In practice, IIE often modifies only specific regions of an image, while the remaining areas largely remain unchanged. Although these two types of regions differ significantly in generation difficulty and computational redundancy, existing IIE models do not account for this distinction, instead applying a uniform generation process across the entire image. This motivates us to propose RegionE, an adaptive, region-aware generation framework that accelerates IIE tasks without additional training. Specifically, the RegionE framework consists of three main components: 1) Adaptive Region Partition. We observed that the trajectory of unedited regions is straight, allowing for multi-step denoised predictions to be inferred in a single step. Therefore, in the early denoising stages, we partition the image into edited and unedited regions based on the difference between the final estimated result and the reference image. 2) Region-Aware Generation. After distinguishing the regions, we replace multi-step denoising with one-step prediction for unedited areas. For edited regions, the trajectory is curved, requiring local iterative denoising. To improve the efficiency and quality of local iterative generation, we propose the Region-Instruction KV Cache, which reduces computational cost while incorporating global information. 3) Adaptive Velocity Decay Cache. Observing that adjacent timesteps in edited regions exhibit strong velocity similarity, we further propose an adaptive velocity decay cache to accelerate the local denoising process. We applied RegionE to state-of-the-art IIE base models, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE achieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o confirmed that semantic and perceptual fidelity were well preserved.

**Link**: [arxiv](http://arxiv.org/abs/2510.25590v1),  [pdf](http://arxiv.org/pdf/2510.25590v1)

**Tags**: cs.CV cs.AI 



### Standardization of Psychiatric Diagnoses -- Role of Fine-tuned LLM   Consortium and OpenAI-gpt-oss Reasoning LLM Enabled Decision Support System
**Authors**: Eranga Bandara, Ross Gore, Atmaram Yarlagadda, Anita H. Clayton, Preston Samuel, Christopher K. Rhea, Sachin Shetty

**Updated**: 2025-10-29T14:54:22Z

**Summary**: The diagnosis of most mental disorders, including psychiatric evaluations, primarily depends on dialogues between psychiatrists and patients. This subjective process can lead to variability in diagnoses across clinicians and patients, resulting in inconsistencies and challenges in achieving reliable outcomes. To address these issues and standardize psychiatric diagnoses, we propose a Fine-Tuned Large Language Model (LLM) Consortium and OpenAI-gpt-oss Reasoning LLM-enabled Decision Support System for the clinical diagnosis of mental disorders. Our approach leverages fine-tuned LLMs trained on conversational datasets involving psychiatrist-patient interactions focused on mental health conditions (e.g., depression). The diagnostic predictions from individual models are aggregated through a consensus-based decision-making process, refined by the OpenAI-gpt-oss reasoning LLM. We propose a novel method for deploying LLM agents that orchestrate communication between the LLM consortium and the reasoning LLM, ensuring transparency, reliability, and responsible AI across the entire diagnostic workflow. Experimental results demonstrate the transformative potential of combining fine-tuned LLMs with a reasoning model to create a robust and highly accurate diagnostic system for mental health assessment. A prototype of the proposed platform, integrating three fine-tuned LLMs with the OpenAI-gpt-oss reasoning LLM, was developed in collaboration with the U.S. Army Medical Research Team in Norfolk, Virginia, USA. To the best of our knowledge, this work represents the first application of a fine-tuned LLM consortium integrated with a reasoning LLM for clinical mental health diagnosis paving the way for next-generation AI-powered eHealth systems aimed at standardizing psychiatric diagnoses.

**Link**: [arxiv](http://arxiv.org/abs/2510.25588v1),  [pdf](http://arxiv.org/pdf/2510.25588v1)

**Tags**: cs.AI 



### Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption   Masking And Normalization
**Authors**: Filip Sondej, Yushi Yang, MikoÅaj Kniejski, Marcel Windys

**Updated**: 2025-10-29T14:52:49Z

**Summary**: Language models can retain dangerous knowledge and skills even after extensive safety fine-tuning, posing both misuse and misalignment risks. Recent studies show that even specialized unlearning methods can be easily reversed. To address this, we systematically evaluate many existing and novel components of unlearning methods and identify ones crucial for irreversible unlearning.   We introduce Disruption Masking, a technique in which we only allow updating weights, where the signs of the unlearning gradient and the retaining gradient are the same. This ensures all updates are non-disruptive.   Additionally, we identify the need for normalizing the unlearning gradients, and also confirm the usefulness of meta-learning. We combine these insights into MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and validate its effectiveness at preventing the recovery of dangerous capabilities. MUDMAN outperforms the prior TAR method by 40%, setting a new state-of-the-art for robust unlearning.

**Link**: [arxiv](http://arxiv.org/abs/2506.12484v4),  [pdf](http://arxiv.org/pdf/2506.12484v4)

**Tags**: cs.LG cs.AI cs.CL 



### Quantum Transformer: Accelerating model inference via quantum linear   algebra
**Authors**: Naixu Guo, Zhan Yu, Matthew Choi, Yizhan Han, Aman Agrawal, Kouhei Nakaji, AlÃ¡n Aspuru-Guzik, Patrick Rebentrost

**Updated**: 2025-10-29T14:48:21Z

**Summary**: Powerful generative artificial intelligence from large language models (LLMs) harnesses extensive computational resources for inference. In this work, we investigate the transformer architecture, a key component of these models, under the lens of fault-tolerant quantum computing. We develop quantum subroutines to construct the building blocks in the transformer, including the self-attention, residual connection with layer normalization, and feed-forward network. As an important subroutine, we show how to efficiently implement the Hadamard product and element-wise functions of matrices on quantum computers. Our algorithm prepares an amplitude encoding of the transformer output, which can be measured for prediction or use in the next layer. We find that the matrix norm of the input sequence plays a dominant role in the quantum complexity. With numerical experiments on open-source LLMs, including for bio-informatics applications, we demonstrate the potential of a quantum speedup for transformer inference in practical regimes.

**Link**: [arxiv](http://arxiv.org/abs/2402.16714v3),  [pdf](http://arxiv.org/pdf/2402.16714v3)

**Tags**: quant-ph cs.AI cs.CL 



### Population of Binary Black Holes Inferred from One Hundred and Fifty   Gravitational Wave Signals
**Authors**: Vaibhav Tiwari

**Updated**: 2025-10-30T11:19:36Z

**Summary**: The LIGO-Virgo-KAGRA collaborations have reported gravitational wave signals from more than 150 binary black holes in the fourth catalog (GWTC-4). Here, we investigate the population properties of these binary black holes using the mixture-model framework Vamana. We present one-dimensional distributions of masses and spins, explore their correlations, and examine their evolution with redshift. These features may reflect astrophysical processes associated with binary black hole formation channels, although most remain poorly constrained. A notable feature is a peak near $10M_\odot$ in the primary mass and $8M_\odot$ in the chirp mass. Additionally, the primary and secondary masses correlate uniquely, producing pronounced peaks in the chirp mass around $14M_\odot$ and $27M_\odot$. The three peaks are roughly separated by a factor of two. A simple explanation for such well-placed peaks is a hierarchical merger scenario, in which the first peak arises from mergers of black holes of stellar origin, and higher-mass peaks arise from repeated mergers of black holes from lower-mass peaks. Although most binaries do not exhibit the high spins and characteristic mass ratios expected from hierarchical mergers, those that do are associated with the peaks observed in the chirp mass distribution.

**Link**: [arxiv](http://arxiv.org/abs/2510.25579v2),  [pdf](http://arxiv.org/pdf/2510.25579v2)

**Tags**: astro-ph.HE 



### Lost in Phonation: Voice Quality Variation as an Evaluation Dimension   for Speech Foundation Models
**Authors**: Harm Lameris, Shree Harsha Bokkahalli Satish, Joakim Gustafson, Ãva SzÃ©kely

**Updated**: 2025-10-29T14:44:44Z

**Summary**: Recent advances in speech foundation models (SFMs) have enabled the direct processing of spoken language from raw audio, bypassing intermediate textual representations. This capability allows SFMs to be exposed to, and potentially respond to, rich paralinguistic variations embedded in the input speech signal. One under-explored dimension of paralinguistic variation is voice quality, encompassing phonation types such as creaky and breathy voice. These phonation types are known to influence how listeners infer affective state, stance and social meaning in speech. Existing benchmarks for speech understanding largely rely on multiple-choice question answering (MCQA) formats, which are prone to failure and therefore unreliable in capturing the nuanced ways paralinguistic features influence model behaviour. In this paper, we probe SFMs through open-ended generation tasks and speech emotion recognition, evaluating whether model behaviours are consistent across different phonation inputs. We introduce a new parallel dataset featuring synthesized modifications to voice quality, designed to evaluate SFM responses to creaky and breathy voice. Our work provides the first examination of SFM sensitivity to these particular non-lexical aspects of speech perception.

**Link**: [arxiv](http://arxiv.org/abs/2510.25577v1),  [pdf](http://arxiv.org/pdf/2510.25577v1)

**Tags**: eess.AS cs.AI cs.CL 



### Who You Are Matters: Bridging Topics and Social Roles via LLM-Enhanced   Logical Recommendation
**Authors**: Qing Yu, Xiaobei Wang, Shuchang Liu, Yandong Bai, Xiaoyu Yang, Xueliang Wang, Chang Meng, Shanshan Wu, Hailan Yang, Huihui Xiao, Xiang Li, Fan Yang, Xiaoqiang Feng, Lantao Hu, Han Li, Kun Gai, Lixin Zou

**Updated**: 2025-10-29T14:42:46Z

**Summary**: Recommender systems filter contents/items valuable to users by inferring preferences from user features and historical behaviors. Mainstream approaches follow the learning-to-rank paradigm, which focus on discovering and modeling item topics (e.g., categories), and capturing user preferences on these topics based on historical interactions. However, this paradigm often neglects the modeling of user characteristics and their social roles, which are logical confounders influencing the correlated interest and user preference transition. To bridge this gap, we introduce the user role identification task and the behavioral logic modeling task that aim to explicitly model user roles and learn the logical relations between item topics and user social roles. We show that it is possible to explicitly solve these tasks through an efficient integration framework of Large Language Model (LLM) and recommendation systems, for which we propose TagCF. On the one hand, TagCF exploits the (Multi-modal) LLM's world knowledge and logic inference ability to extract realistic tag-based virtual logic graphs that reveal dynamic and expressive knowledge of users, refining our understanding of user behaviors. On the other hand, TagCF presents empirically effective integration modules that take advantage of the extracted tag-logic information, augmenting the recommendation performance. We conduct both online experiments and offline experiments with industrial and public datasets as verification of TagCF's effectiveness, and we empirically show that the user role modeling strategy is potentially a better choice than the modeling of item topics. Additionally, we provide evidence that the extracted logic graphs are empirically a general and transferable knowledge that can benefit a wide range of recommendation tasks. Our code is available in https://github.com/Code2Q/TagCF.

**Link**: [arxiv](http://arxiv.org/abs/2505.10940v3),  [pdf](http://arxiv.org/pdf/2505.10940v3)

**Tags**: cs.IR cs.AI 



### Pearl: A Foundation Model for Placing Every Atom in the Right Location
**Authors**: Genesis Research Team, Alejandro Dobles, Nina Jovic, Kenneth Leidal, Pranav Murugan, David C. Williams, Drausin Wulsin, Nate Gruver, Christina X. Ji, Korrawat Pruegsanusak, Gianluca Scarpellini, Ansh Sharma, Wojciech Swiderski, Andrea Bootsma, Richard Strong Bowen, Charlotte Chen, Jamin Chen, Marc AndrÃ© DÃ¤mgen, Benjamin DiFrancesco, J. D. Fishman, Alla Ivanova, Zach Kagin, David Li-Bland, Zuli Liu, Igor Morozov, Jeffrey Ouyang-Zhang, Frank C. Pickard IV, Kushal S. Shah, Ben Shor, Gabriel Monteiro da Silva, Roy Tal, Maxx Tessmer, Carl Tilbury, Cyr Vetcher, Daniel Zeng, Maruan Al-Shedivat, Aleksandra Faust, Evan N. Feinberg, Michael V. LeVine, Matteus Pan

**Updated**: 2025-10-29T14:41:45Z

**Summary**: Accurately predicting the three-dimensional structures of protein-ligand complexes remains a fundamental challenge in computational drug discovery that limits the pace and success of therapeutic design. Deep learning methods have recently shown strong potential as structural prediction tools, achieving promising accuracy across diverse biomolecular systems. However, their performance and utility are constrained by scarce experimental data, inefficient architectures, physically invalid poses, and the limited ability to exploit auxiliary information available at inference. To address these issues, we introduce Pearl (Placing Every Atom in the Right Location), a foundation model for protein-ligand cofolding at scale. Pearl addresses these challenges with three key innovations: (1) training recipes that include large-scale synthetic data to overcome data scarcity; (2) architectures that incorporate an SO(3)-equivariant diffusion module to inherently respect 3D rotational symmetries, improving generalization and sample efficiency, and (3) controllable inference, including a generalized multi-chain templating system supporting both protein and non-polymeric components as well as dual unconditional/conditional modes. Pearl establishes a new state-of-the-art performance in protein-ligand cofolding. On the key metric of generating accurate (RMSD < 2 \r{A}) and physically valid poses, Pearl surpasses AlphaFold 3 and other open source baselines on the public Runs N' Poses and PoseBusters benchmarks, delivering 14.5% and 14.2% improvements, respectively, over the next best model. In the pocket-conditional cofolding regime, Pearl delivers $3.6\times$ improvement on a proprietary set of challenging, real-world drug targets at the more rigorous RMSD < 1 \r{A} threshold. Finally, we demonstrate that model performance correlates directly with synthetic dataset size used in training.

**Link**: [arxiv](http://arxiv.org/abs/2510.24670v2),  [pdf](http://arxiv.org/pdf/2510.24670v2)

**Tags**: cs.LG q-bio.QM 



### Collab-REC: An LLM-based Agentic Framework for Balancing Recommendations   in Tourism
**Authors**: Ashmi Banerjee, Fitri Nur Aisyah, Adithi Satish, Wolfgang WÃ¶rndl, Yashar Deldjoo

**Updated**: 2025-10-29T14:31:38Z

**Summary**: We propose Collab-REC, a multi-agent framework designed to counteract popularity bias and enhance diversity in tourism recommendations. In our setting, three LLM-based agents -- Personalization, Popularity, and Sustainability generate city suggestions from complementary perspectives. A non-LLM moderator then merges and refines these proposals via multi-round negotiation, ensuring each agent's viewpoint is incorporated while penalizing spurious or repeated responses. Experiments on European city queries show that Collab-REC improves diversity and overall relevance compared to a single-agent baseline, surfacing lesser-visited locales that often remain overlooked. This balanced, context-aware approach addresses over-tourism and better aligns with constraints provided by the user, highlighting the promise of multi-stakeholder collaboration in LLM-driven recommender systems.

**Link**: [arxiv](http://arxiv.org/abs/2508.15030v2),  [pdf](http://arxiv.org/pdf/2508.15030v2)

**Tags**: cs.AI 



### GradeSQL: Test-Time Inference with Outcome Reward Models for Text-to-SQL   Generation from Large Language Models
**Authors**: Mattia Tritto, Giuseppe Farano, Dario Di Palma, Gaetano Rossiello, Fedelucio Narducci, Dharmashankar Subramanian, Tommaso Di Noia

**Updated**: 2025-10-29T14:09:33Z

**Summary**: Text-to-SQL, the task of translating natural language questions into SQL queries, has significantly advanced with the introduction of Large Language Models (LLMs), broadening database accessibility for a wide range of users. Despite substantial progress in generating valid SQL, current LLMs still struggle with complex queries. To address this limitation, test-time strategies such as Best-of-N (BoN) and Majority Voting (Maj) are often employed, based on the assumption that LLMs can produce correct answers after multiple attempts. However, these methods rely on surface-level heuristics, selecting the syntactically correct query through execution-based BoN (ex-BoN) or the most frequently generated one through Majority Voting. Recently, Outcome Reward Models (ORMs), which assign utility scores to generated outputs based on semantic correctness, have emerged as a promising reinforcement learning approach for improving model alignment. We argue that ORMs could serve as an effective new test-time heuristic, although their application in this context remains largely underexplored.   In this work, we propose a unified framework for training ORMs tailored to the Text-to-SQL task and assess their effectiveness as a test-time heuristic within the BoN strategy. We benchmark ORMs against ex-BoN and Maj across the BIRD and Spider datasets, fine-tuning diverse open-source LLMs from the Qwen2, Granite3, and Llama3 families. Results show that ORMs outperform ex-BoN and Maj, achieving execution accuracy gains of +4.33% (BIRD) and +2.10% (Spider) over ex-BoN, and +2.91% (BIRD) and +0.93% (Spider) over Maj. We further demonstrate that finetuning models already aligned with SQL generation, such as OmniSQL, yields superior ORM performance. Additionally, we observe that ORMs achieve competitive results on simple queries and benefit more from an increased number of candidates compared to ex-BoN and Maj.

**Link**: [arxiv](http://arxiv.org/abs/2509.01308v2),  [pdf](http://arxiv.org/pdf/2509.01308v2)

**Tags**: cs.AI cs.CL cs.DB 



### TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM   Persona Simulation
**Authors**: Bangde Du, Minghao Guo, Songming He, Ziyi Ye, Xi Zhu, Weihang Su, Shuqi Zhu, Yujia Zhou, Yongfeng Zhang, Qingyao Ai, Yiqun Liu

**Updated**: 2025-10-30T11:19:24Z

**Summary**: Large Language Models (LLMs) are exhibiting emergent human-like abilities and are increasingly envisioned as the foundation for simulating an individual's communication style, behavioral tendencies, and personality traits. However, current evaluations of LLM-based persona simulation remain limited: most rely on synthetic dialogues, lack systematic frameworks, and lack analysis of the capability requirement. To address these limitations, we introduce TwinVoice, a comprehensive benchmark for assessing persona simulation across diverse real-world contexts. TwinVoice encompasses three dimensions: Social Persona (public social interactions), Interpersonal Persona (private dialogues), and Narrative Persona (role-based expression). It further decomposes the evaluation of LLM performance into six fundamental capabilities, including opinion consistency, memory recall, logical reasoning, lexical fidelity, persona tone, and syntactic style. Experimental results reveal that while advanced models achieve moderate accuracy in persona simulation, they still fall short of capabilities such as syntactic style and memory recall. Consequently, the average performance achieved by LLMs remains considerably below the human baseline.

**Link**: [arxiv](http://arxiv.org/abs/2510.25536v2),  [pdf](http://arxiv.org/pdf/2510.25536v2)

**Tags**: cs.CL I.2.7; I.2.6; I.2.0 



### Using latent representations to link disjoint longitudinal data for   mixed-effects regression
**Authors**: Clemens SchÃ¤chter, Maren Hackenberg, Michelle Pfaffenlehner, FÃ©lix B. Tambe-Ndonfack, Thorsten Schmidt, Astrid Pechmann, Janbernd Kirschner, Jan Hasenauser, Harald Binder

**Updated**: 2025-10-29T13:56:44Z

**Summary**: Many rare diseases offer limited established treatment options, leading patients to switch therapies when new medications emerge. To analyze the impact of such treatment switches within the low sample size limitations of rare disease trials, it is important to use all available data sources. This, however, is complicated when usage of measurement instruments change during the observation period, for example when instruments are adapted to specific age ranges. The resulting disjoint longitudinal data trajectories, complicate the application of traditional modeling approaches like mixed-effects regression. We tackle this by mapping observations of each instrument to a aligned low-dimensional temporal trajectory, enabling longitudinal modeling across instruments. Specifically, we employ a set of variational autoencoder architectures to embed item values into a shared latent space for each time point. Temporal disease dynamics and treatment switch effects are then captured through a mixed-effects regression model applied to latent representations. To enable statistical inference, we present a novel statistical testing approach that accounts for the joint parameter estimation of mixed-effects regression and variational autoencoders. The methodology is applied to quantify the impact of treatment switches for patients with spinal muscular atrophy. Here, our approach aligns motor performance items from different measurement instruments for mixed-effects regression and maps estimated effects back to the observed item level to quantify the treatment switch effect. Our approach allows for model selection as well as for assessing effects of treatment switching. The results highlight the potential of modeling in joint latent representations for addressing small data challenges.

**Link**: [arxiv](http://arxiv.org/abs/2510.25531v1),  [pdf](http://arxiv.org/pdf/2510.25531v1)

**Tags**: stat.ML cs.AI cs.LG 68T07 G.3; I.2.6; J.3 



### Zero Reinforcement Learning Towards General Domains
**Authors**: Yuyuan Zeng, Yufei Huang, Can Xu, Qingfeng Sun, Jianfeng Yan, Guanghui Xu, Tao Yang, Fengzong Lian

**Updated**: 2025-10-29T13:52:44Z

**Summary**: Zero Reinforcement Learning (Zero-RL) has proven to be an effective approach for enhancing the reasoning capabilities of large language models (LLMs) by directly applying reinforcement learning with verifiable rewards on pretrained models, without the need for a supervised fine-tuning phase. However, current research on zero-RL primarily focuses on domains with easily verifiable reward signals, such as mathematics, programming, and other reasoning tasks. The challenge of eliciting reasoning abilities in more diverse scenarios, where verification is not straightforward, remains underexplored. To address this gap, we propose a novel zero-RL paradigm designed to improve a model's reasoning ability across both verifiable and non-verifiable domains. By combining verifiable rewards with a generative reward model, we conduct multi-task zero-RL training across both domains, facilitating the transfer of reasoning capabilities between them. Furthermore, to mitigate reward hacking in the generative reward model, we design a smooth length penalty that encourages the generation of more comprehensive thinking tokens in general domains. Experimental results on Qwen3-8B-Base and Qwen3-14B-Base demonstrate that our approach achieves superior reasoning performance, not only on tasks requiring extensive reasoning but also on more general tasks.

**Link**: [arxiv](http://arxiv.org/abs/2510.25528v1),  [pdf](http://arxiv.org/pdf/2510.25528v1)

**Tags**: cs.AI 



### Group Interventions on Deep Networks for Causal Discovery in Subsystems
**Authors**: Wasim Ahmad, Joachim Denzler, Maha Shadaydeh

**Updated**: 2025-10-29T13:42:56Z

**Summary**: Causal discovery uncovers complex relationships between variables, enhancing predictions, decision-making, and insights into real-world systems, especially in nonlinear multivariate time series. However, most existing methods primarily focus on pairwise cause-effect relationships, overlooking interactions among groups of variables, i.e., subsystems and their collective causal influence. In this study, we introduce gCDMI, a novel multi-group causal discovery method that leverages group-level interventions on trained deep neural networks and employs model invariance testing to infer causal relationships. Our approach involves three key steps. First, we use deep learning to jointly model the structural relationships among groups of all time series. Second, we apply group-wise interventions to the trained model. Finally, we conduct model invariance testing to determine the presence of causal links among variable groups. We evaluate our method on simulated datasets, demonstrating its superior performance in identifying group-level causal relationships compared to existing methods. Additionally, we validate our approach on real-world datasets, including brain networks and climate ecosystems. Our results highlight that applying group-level interventions to deep learning models, combined with invariance testing, can effectively reveal complex causal structures, offering valuable insights for domains such as neuroscience and climate science.

**Link**: [arxiv](http://arxiv.org/abs/2510.23906v2),  [pdf](http://arxiv.org/pdf/2510.23906v2)

**Tags**: cs.LG cs.AI 



### Predicate Renaming via Large Language Models
**Authors**: Elisabetta Gentili, Tony Ribeiro, Fabrizio Riguzzi, Katsumi Inoue

**Updated**: 2025-10-29T13:39:41Z

**Summary**: In this paper, we address the problem of giving names to predicates in logic rules using Large Language Models (LLMs). In the context of Inductive Logic Programming, various rule generation methods produce rules containing unnamed predicates, with Predicate Invention being a key example. This hinders the readability, interpretability, and reusability of the logic theory. Leveraging recent advancements in LLMs development, we explore their ability to process natural language and code to provide semantically meaningful suggestions for giving a name to unnamed predicates. The evaluation of our approach on some hand-crafted logic rules indicates that LLMs hold potential for this task.

**Link**: [arxiv](http://arxiv.org/abs/2510.25517v1),  [pdf](http://arxiv.org/pdf/2510.25517v1)

**Tags**: cs.AI 



### Solar photospheric velocities measured in space: a comparison between   SO/PHI-HRT and SDO/HMI
**Authors**: D. Calchetti, K. Albert, F. J. BailÃ©n, J. Blanco RodrÃ­guez, J. S. Castellanos DurÃ¡n, A. Feller, A. Gandorfer, J. Hirzberger, J. Sinjan, X. Li, T. Oba, D. Orozco SÃºarez, T. L. RiethmÃ¼ller, J. Schou, S. K. Solanki, H. Strecker, A. Ulyanov, G. Valori

**Updated**: 2025-10-29T13:38:27Z

**Summary**: The Polarimetric and Helioseismic Imager (SO/PHI) onboard Solar Orbiter is a spectropolarimeter scanning the Fe I line at 617.3 nm, providing data of the solar photosphere. The same line is sampled by the Helioseismic and Magnetic Imager (HMI) on board the Solar Dynamics Observatory (SDO) and many other on-ground instruments. In this paper, we aim at assessing the consistency between line-of-sight (LoS) velocity measurements from the two instruments. Reliable measurements of up and down flows from SO/PHI are crucial and unique when Solar Orbiter is facing the far side of the Sun. Also, a combination of measurements from two vantage points to study horizontal flows must rely on consistent observations. For this purpose, we compare the LoS velocity measured by SO/PHI's High Resolution Telescope (SO/PHI-HRT) and SDO/HMI on 29 March 2023, when Solar Orbiter was crossing the Sun-Earth line at a distance of 0.39 au from the Sun. Because such co-alignments are rare, this configuration offered an almost unique opportunity to directly compare data products from both telescopes. The data are aligned and remapped to allow a pixel-by-pixel comparison of the whole time series of 4 hours length. Temporal and spatial variations are considered for a direct combination of the measurements. The LoS velocity distributions are evaluated and a clear linear relation is found between the two instruments with a slope of 0.94 and a correlation of 90%. We find that the signals form at similar heights, with a separation of 10$\pm$12 km, which is larger than previous estimates. A close-up look at the penumbra of a sunspot and its Evershed flow is presented. We conclude that the signals inferred by SO/PHI-HRT and SDO/HMI show very good agreement and high correlation when instrumental effects and large-scale velocities on the Sun are properly accounted for.

**Link**: [arxiv](http://arxiv.org/abs/2510.25515v1),  [pdf](http://arxiv.org/pdf/2510.25515v1)

**Tags**: astro-ph.SR 



### Many LLMs Are More Utilitarian Than One
**Authors**: Anita Keshmirian, Razan Baltaji, Babak Hemmatian, Hadi Asghari, Lav R. Varshney

**Updated**: 2025-10-29T13:37:41Z

**Summary**: Moral judgment is integral to large language models' (LLMs) social reasoning. As multi-agent systems gain prominence, it becomes crucial to understand how LLMs function when collaborating compared to operating as individual agents. In human moral judgment, group deliberation leads to a Utilitarian Boost: a tendency to endorse norm violations that inflict harm but maximize benefits for the greatest number of people. We study whether a similar dynamic emerges in multi-agent LLM systems. We test six models on well-established sets of moral dilemmas across two conditions: (1) Solo, where models reason independently, and (2) Group, where they engage in multi-turn discussions in pairs or triads. In personal dilemmas, where agents decide whether to directly harm an individual for the benefit of others, all models rated moral violations as more acceptable when part of a group, demonstrating a Utilitarian Boost similar to that observed in humans. However, the mechanism for the Boost in LLMs differed: While humans in groups become more utilitarian due to heightened sensitivity to decision outcomes, LLM groups showed either reduced sensitivity to norms or enhanced impartiality. We report model differences in when and how strongly the Boost manifests. We also discuss prompt and agent compositions that enhance or mitigate the effect. We end with a discussion of the implications for AI alignment, multi-agent design, and artificial moral reasoning. Code available at: https://github.com/baltaci-r/MoralAgents

**Link**: [arxiv](http://arxiv.org/abs/2507.00814v2),  [pdf](http://arxiv.org/pdf/2507.00814v2)

**Tags**: cs.CL cs.AI cs.CY I.2.7; I.2.11 



### RoboOmni: Proactive Robot Manipulation in Omni-modal Context
**Authors**: Siyin Wang, Jinlan Fu, Feihong Liu, Xinzhe He, Huangxuan Wu, Junhao Shi, Kexin Huang, Zhaoye Fei, Jingjing Gong, Zuxuan Wu, Yugang Jiang, See-Kiong Ng, Tat-Seng Chua, Xipeng Qiu

**Updated**: 2025-10-29T13:37:19Z

**Summary**: Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid progress in Vision-Language-Action (VLA) models for robotic manipulation. Although effective in many scenarios, current approaches largely rely on explicit instructions, whereas in real-world interactions, humans rarely issue instructions directly. Effective collaboration requires robots to infer user intentions proactively. In this work, we introduce cross-modal contextual instructions, a new setting where intent is derived from spoken dialogue, environmental sounds, and visual cues rather than explicit commands. To address this new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor framework based on end-to-end omni-modal LLMs that unifies intention recognition, interaction confirmation, and action execution. RoboOmni fuses auditory and visual signals spatiotemporally for robust intention recognition, while supporting direct speech interaction. To address the absence of training data for proactive intention recognition in robotic manipulation, we build OmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640 backgrounds, and six contextual instruction types. Experiments in simulation and real-world settings show that RoboOmni surpasses text- and ASR-based baselines in success rate, inference speed, intention recognition, and proactive assistance.

**Link**: [arxiv](http://arxiv.org/abs/2510.23763v2),  [pdf](http://arxiv.org/pdf/2510.23763v2)

**Tags**: cs.RO cs.CL cs.CV 



### MTIR-SQL: Multi-turn Tool-Integrated Reasoning Reinforcement Learning   for Text-to-SQL
**Authors**: Zekun Xu, Siyu Xia, Chuhuai Yue, Jiajun Chai, Mingxue Tian, Xiaohan Wang, Wei Lin, Haoxuan Li, Guojun Yin

**Updated**: 2025-10-29T13:34:27Z

**Summary**: As large language models (LLMs) are increasingly used in Text-to-SQL tasks, Reinforcement Learning (RL) has become a common method for improving performance. Existing methods primarily rely on static execution feedback, which restricts real-time error correction. However, integrating multi-turn tool invocation along with dynamic feedback could significantly improve adaptability and robustness, ultimately enhancing model performance. To address these issues, we propose MTIR-SQL, an innovative Multi-turn Tool-Integrated Reasoning reinforcement learning framework for Text-to-SQL. Our approach introduces an execution-aware multi-turn reasoning paradigm that seamlessly incorporates database execution feedback at each reasoning step, enabling context-sensitive query generation and progressive refinement throughout the reasoning process. The framework extends the GRPO algorithm to accommodate complex multi-turn interaction scenarios. Considering the training instability characteristics of MTIR and the potential for significant Deviation of model distribution from the initial model, we enhance the GRPO algorithm by adding a trajectory filtering mechanism and removing KL loss constraints. Experimental results demonstrate that MTIR-SQL, with 4B parameters, achieves \textbf{64.4}\% accuracy in the BIRD Dev and 84.6% execution accuracy in the SPIDER Dev, significantly outperforming existing approaches.

**Link**: [arxiv](http://arxiv.org/abs/2510.25510v1),  [pdf](http://arxiv.org/pdf/2510.25510v1)

**Tags**: cs.AI 



### Reflections on the Reproducibility of Commercial LLM Performance in   Empirical Software Engineering Studies
**Authors**: Florian Angermeir, Maximilian Amougou, Mark Kreitz, Andreas Bauer, Matthias Linhuber, Davide Fucci, Fabiola MoyÃ³n C., Daniel Mendez, Tony Gorschek

**Updated**: 2025-10-29T13:31:32Z

**Summary**: Large Language Models have gained remarkable interest in industry and academia. The increasing interest in LLMs in academia is also reflected in the number of publications on this topic over the last years. For instance, alone 78 of the around 425 publications at ICSE 2024 performed experiments with LLMs. Conducting empirical studies with LLMs remains challenging and raises questions on how to achieve reproducible results, for both other researchers and practitioners. One important step towards excelling in empirical research on LLMs and their application is to first understand to what extent current research results are eventually reproducible and what factors may impede reproducibility. This investigation is within the scope of our work. We contribute an analysis of the reproducibility of LLM-centric studies, provide insights into the factors impeding reproducibility, and discuss suggestions on how to improve the current state. In particular, we studied the 86 articles describing LLM-centric studies, published at ICSE 2024 and ASE 2024. Of the 86 articles, 18 provided research artefacts and used OpenAI models. We attempted to replicate those 18 studies. Of the 18 studies, only five were fit for reproduction. For none of the five studies, we were able to fully reproduce the results. Two studies seemed to be partially reproducible, and three studies did not seem to be reproducible. Our results highlight not only the need for stricter research artefact evaluations but also for more robust study designs to ensure the reproducible value of future publications.

**Link**: [arxiv](http://arxiv.org/abs/2510.25506v1),  [pdf](http://arxiv.org/pdf/2510.25506v1)

**Tags**: cs.SE cs.AI 



### TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time   Series Forecasting
**Authors**: Vladyslav Moroshan, Julien Siems, Arber Zela, Timur Carstensen, Frank Hutter

**Updated**: 2025-10-29T13:27:18Z

**Summary**: Foundation models for zero-shot time series forecasting face challenges in efficient long-horizon prediction and reproducibility, with existing synthetic-only approaches underperforming on challenging benchmarks. This paper presents TempoPFN, a univariate time series foundation model based on linear Recurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The model uses a GatedDeltaProduct architecture with state-weaving for fully parallelizable training across sequence lengths, eliminating the need for windowing or summarization techniques while maintaining robust temporal state-tracking. Our comprehensive synthetic data pipeline unifies diverse generators, including stochastic differential equations, Gaussian processes, and audio synthesis, with novel augmentations. In zero-shot evaluations on the Gift-Eval benchmark, TempoPFN achieves top-tier competitive performance, outperforming all existing synthetic-only approaches and surpassing the vast majority of models trained on real-world data, while being more efficient than existing baselines by leveraging fully parallelizable training and inference. We open-source our complete data generation pipeline and training code, providing a reproducible foundation for future research.

**Link**: [arxiv](http://arxiv.org/abs/2510.25502v1),  [pdf](http://arxiv.org/pdf/2510.25502v1)

**Tags**: cs.LG cs.AI stat.ML 



### A Multilingual, Large-Scale Study of the Interplay between LLM   Safeguards, Personalisation, and Disinformation
**Authors**: JoÃ£o A. Leite, Arnav Arora, Silvia Gargova, JoÃ£o Luz, Gustavo Sampaio, Ian Roberts, Carolina Scarton, Kalina Bontcheva

**Updated**: 2025-10-29T13:26:49Z

**Summary**: Large Language Models (LLMs) can generate human-like disinformation, yet their ability to personalise such content across languages and demographics remains underexplored. This study presents the first large-scale, multilingual analysis of persona-targeted disinformation generation by LLMs. Employing a red teaming methodology, we prompt eight state-of-the-art LLMs with 324 false narratives and 150 demographic personas (combinations of country, generation, and political orientation) across four languages--English, Russian, Portuguese, and Hindi--resulting in AI-TRAITS, a comprehensive dataset of 1.6 million personalised disinformation texts. Results show that the use of even simple personalisation prompts significantly increases the likelihood of jailbreaks across all studied LLMs, up to 10 percentage points, and alters linguistic and rhetorical patterns that enhance narrative persuasiveness. Models such as Grok and GPT exhibited jailbreak rates and personalisation scores both exceeding 85%. These insights expose critical vulnerabilities in current state-of-the-art LLMs and offer a foundation for improving safety alignment and detection strategies in multilingual and cross-demographic contexts.

**Link**: [arxiv](http://arxiv.org/abs/2510.12993v2),  [pdf](http://arxiv.org/pdf/2510.12993v2)

**Tags**: cs.CL 



### Securing AI Agent Execution
**Authors**: Christoph BÃ¼hler, Matteo Biagiola, Luca Di Grazia, Guido Salvaneschi

**Updated**: 2025-10-29T13:11:21Z

**Summary**: Large Language Models (LLMs) have evolved into AI agents that interact with external tools and environments to perform complex tasks. The Model Context Protocol (MCP) has become the de facto standard for connecting agents with such resources, but security has lagged behind: thousands of MCP servers execute with unrestricted access to host systems, creating a broad attack surface. In this paper, we introduce AgentBound, the first access control framework for MCP servers. AgentBound combines a declarative policy mechanism, inspired by the Android permission model, with a policy enforcement engine that contains malicious behavior without requiring MCP server modifications. We build a dataset containing the 296 most popular MCP servers, and show that access control policies can be generated automatically from source code with 80.9% accuracy. We also show that AgentBound blocks the majority of security threats in several malicious MCP servers, and that policy enforcement engine introduces negligible overhead. Our contributions provide developers and project managers with a practical foundation for securing MCP servers while maintaining productivity, enabling researchers and tool builders to explore new directions for declarative access control and MCP security.

**Link**: [arxiv](http://arxiv.org/abs/2510.21236v2),  [pdf](http://arxiv.org/pdf/2510.21236v2)

**Tags**: cs.CR cs.AI cs.SE D.2.0 



### Generalized Pseudo-Relevance Feedback
**Authors**: Yiteng Tu, Weihang Su, Yujia Zhou, Yiqun Liu, Fen Lin, Qin Liu, Qingyao Ai

**Updated**: 2025-10-29T13:08:35Z

**Summary**: Query rewriting is a fundamental technique in information retrieval (IR). It typically employs the retrieval result as relevance feedback to refine the query and thereby addresses the vocabulary mismatch between user queries and relevant documents. Traditional pseudo-relevance feedback (PRF) and its vector-based extension (VPRF) improve retrieval performance by leveraging top-retrieved documents as relevance feedback. However, they are constructed based on two major hypotheses: the relevance assumption (top documents are relevant) and the model assumption (rewriting methods need to be designed specifically for particular model architectures). While recent large language models (LLMs)-based generative relevance feedback (GRF) enables model-free query reformulation, it either suffers from severe LLM hallucination or, again, relies on the relevance assumption to guarantee the effectiveness of rewriting quality. To overcome these limitations, we introduce an assumption-relaxed framework: \textit{Generalized Pseudo Relevance Feedback} (GPRF), which performs model-free, natural language rewriting based on retrieved documents, not only eliminating the model assumption but also reducing dependence on the relevance assumption. Specifically, we design a utility-oriented training pipeline with reinforcement learning to ensure robustness against noisy feedback. Extensive experiments across multiple benchmarks and retrievers demonstrate that GPRF consistently outperforms strong baselines, establishing it as an effective and generalizable framework for query rewriting.

**Link**: [arxiv](http://arxiv.org/abs/2510.25488v1),  [pdf](http://arxiv.org/pdf/2510.25488v1)

**Tags**: cs.IR 



### NetEcho: From Real-World Streaming Side-Channels to Full LLM   Conversation Recovery
**Authors**: Zheng Zhang, Guanlong Wu, Sen Deng, Shuai Wang, Yinqian Zhang

**Updated**: 2025-10-29T12:47:36Z

**Summary**: In the rapidly expanding landscape of Large Language Model (LLM) applications, real-time output streaming has become the dominant interaction paradigm. While this enhances user experience, recent research reveals that it exposes a non-trivial attack surface through network side-channels. Adversaries can exploit patterns in encrypted traffic to infer sensitive information and reconstruct private conversations. In response, LLM providers and third-party services are deploying defenses such as traffic padding and obfuscation to mitigate these vulnerabilities.   This paper starts by presenting a systematic analysis of contemporary side-channel defenses in mainstream LLM applications, with a focus on services from vendors like OpenAI and DeepSeek. We identify and examine seven representative deployment scenarios, each incorporating active/passive mitigation techniques. Despite these enhanced security measures, our investigation uncovers significant residual information that remains vulnerable to leakage within the network traffic.   Building on this discovery, we introduce NetEcho, a novel, LLM-based framework that comprehensively unleashes the network side-channel risks of today's LLM applications. NetEcho is designed to recover entire conversations -- including both user prompts and LLM responses -- directly from encrypted network traffic. It features a deliberate design that ensures high-fidelity text recovery, transferability across different deployment scenarios, and moderate operational cost. In our evaluations on medical and legal applications built upon leading models like DeepSeek-v3 and GPT-4o, NetEcho can recover avg $\sim$70\% information of each conversation, demonstrating a critical limitation in current defense mechanisms. We conclude by discussing the implications of our findings and proposing future directions for augmenting network traffic security.

**Link**: [arxiv](http://arxiv.org/abs/2510.25472v1),  [pdf](http://arxiv.org/pdf/2510.25472v1)

**Tags**: cs.CR 



### Fine-Tuned Language Models for Domain-Specific Summarization and Tagging
**Authors**: Jun Wang, Fuming Lin, Yuyu Chen

**Updated**: 2025-10-29T12:33:48Z

**Summary**: This paper presents a pipeline integrating fine-tuned large language models (LLMs) with named entity recognition (NER) for efficient domain-specific text summarization and tagging. The authors address the challenge posed by rapidly evolving sub-cultural languages and slang, which complicate automated information extraction and law enforcement monitoring. By leveraging the LLaMA Factory framework, the study fine-tunes LLMs on both generalpurpose and custom domain-specific datasets, particularly in the political and security domains. The models are evaluated using BLEU and ROUGE metrics, demonstrating that instruction fine-tuning significantly enhances summarization and tagging accuracy, especially for specialized corpora. Notably, the LLaMA3-8B-Instruct model, despite its initial limitations in Chinese comprehension, outperforms its Chinese-trained counterpart after domainspecific fine-tuning, suggesting that underlying reasoning capabilities can transfer across languages. The pipeline enables concise summaries and structured entity tagging, facilitating rapid document categorization and distribution. This approach proves scalable and adaptable for real-time applications, supporting efficient information management and the ongoing need to capture emerging language trends. The integration of LLMs and NER offers a robust solution for transforming unstructured text into actionable insights, crucial for modern knowledge management and security operations.

**Link**: [arxiv](http://arxiv.org/abs/2510.25460v1),  [pdf](http://arxiv.org/pdf/2510.25460v1)

**Tags**: cs.CL cs.AI 



### Grounded in Reality: Learning and Deploying Proactive LLM from Offline   Logs
**Authors**: Fei Wei, Daoyuan Chen, Ce Wang, Yilun Huang, Yushuo Chen, Xuchen Pan, Yaliang Li, Bolin Ding

**Updated**: 2025-10-29T12:08:07Z

**Summary**: Large Language Models (LLMs) excel as passive responders, but teaching them to be proactive, goal-oriented partners, a critical capability in high-stakes domains, remains a major challenge. Current paradigms either myopically optimize single-turn attributes or rely on brittle, high-cost user simulators, creating a persistent ``reality gap''. To bridge this gap, we introduce \texttt{Learn-to-Ask}, a general, simulator-free framework for learning and deploying proactive dialogue agents \textit{directly from offline expert data}, bypassing the need to model complex user dynamics. Our key insight is to reframe the offline policy learning problem by leveraging the \textbf{observed future} of each expert trajectory. This allows us to infer a dense, turn-by-turn reward signal grounded in the expert's revealed strategy, decomposing the intractable long-horizon problem into a series of supervised learning tasks, and training a policy to output a structured \texttt{(action, state_assessment)} tuple, governing both \textbf{what to ask} and, crucially, \textbf{when to stop}. To ensure reward fidelity, our Automated Grader Calibration pipeline systematically purges noise from the LLM-based reward model with minimal human supervision. Empirically, we demonstrate the efficacy of \texttt{Learn-to-Ask} in a real-world medical dataset, using LLMs of varying sizes up to 32B. Our approach culminates in the successful deployment of LLMs into a live, large-scale online AI service. In rigorous in-house evaluations, our model was launched and achieved performance even superior to human experts, proving our framework's ability to translate offline data into tangible, real-world impact. We hope this work provides a practical and economically viable blueprint for transforming passive LLMs into proactive, goal-oriented LLM applications.

**Link**: [arxiv](http://arxiv.org/abs/2510.25441v1),  [pdf](http://arxiv.org/pdf/2510.25441v1)

**Tags**: cs.CL cs.AI 



### SATURN: SAT-based Reinforcement Learning to Unleash Language Model   Reasoning
**Authors**: Huanyu Liu, Jia Li, Hao Zhu, Kechi Zhang, Yihong Dong, Ge Li

**Updated**: 2025-10-29T12:06:15Z

**Summary**: How to design reinforcement learning (RL) tasks that effectively unleash the reasoning capability of large language models (LLMs) remains an open question. Existing RL tasks (e.g., math, programming, and constructing reasoning tasks) suffer from three key limitations: (1) Scalability. They rely heavily on human annotation or expensive LLM synthesis to generate sufficient training data. (2) Verifiability. LLMs' outputs are hard to verify automatically and reliably. (3) Controllable Difficulty. Most tasks lack fine-grained difficulty control, making it hard to train LLMs to develop reasoning ability from easy to hard.   To address these limitations, we propose Saturn, a SAT-based RL framework that uses Boolean Satisfiability (SAT) problems to train and evaluate LLMs reasoning. Saturn enables scalable task construction, rule-based verification, and precise difficulty control. Saturn designs a curriculum learning pipeline that continuously improves LLMs' reasoning capability by constructing SAT tasks of increasing difficulty and training LLMs from easy to hard. To ensure stable training, we design a principled mechanism to control difficulty transitions.   We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying difficulty. It supports the evaluation of how LLM reasoning changes with problem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain Saturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT problems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of +14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B and Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g., AIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in constructing RL tasks, Saturn achieves further improvements of +8.8%. We release the source code, data, and models to support future research.

**Link**: [arxiv](http://arxiv.org/abs/2505.16368v2),  [pdf](http://arxiv.org/pdf/2505.16368v2)

**Tags**: cs.LG cs.AI 



### Brain-inspired Computational Intelligence via Predictive Coding
**Authors**: Tommaso Salvatori, Ankur Mali, Christopher L. Buckley, Thomas Lukasiewicz, Rajesh P. N. Rao, Karl Friston, Alexander Ororbia

**Updated**: 2025-10-29T12:00:29Z

**Summary**: Artificial intelligence (AI) is rapidly becoming one of the key technologies of this century. The majority of results in AI thus far have been achieved using deep neural networks trained with a learning algorithm called error backpropagation, always considered biologically implausible. To this end, recent works have studied learning algorithms for deep neural networks inspired by the neurosciences. One such theory, called predictive coding (PC), has shown promising properties that make it potentially valuable for the machine learning community: it can model information processing in different areas of the brain, can be used in control and robotics, has a solid mathematical foundation in variational inference, and performs its computations asynchronously. Inspired by such properties, works that propose novel PC-like algorithms are starting to be present in multiple sub-fields of machine learning and AI at large. Here, we survey such efforts by first providing a broad overview of the history of PC to provide common ground for the understanding of the recent developments, then by describing current efforts and results, and concluding with a large discussion of possible implications and ways forward.

**Link**: [arxiv](http://arxiv.org/abs/2308.07870v3),  [pdf](http://arxiv.org/pdf/2308.07870v3)

**Tags**: cs.AI cs.LG cs.NE 



### A Critical Study of Automatic Evaluation in Sign Language Translation
**Authors**: Shakib Yazdani, Yasser Hamidullah, Cristina EspaÃ±a-Bonet, Eleftherios Avramidis, Josef van Genabith

**Updated**: 2025-10-29T11:57:03Z

**Summary**: Automatic evaluation metrics are crucial for advancing sign language translation (SLT). Current SLT evaluation metrics, such as BLEU and ROUGE, are only text-based, and it remains unclear to what extent text-based metrics can reliably capture the quality of SLT outputs. To address this gap, we investigate the limitations of text-based SLT evaluation metrics by analyzing six metrics, including BLEU, chrF, and ROUGE, as well as BLEURT on the one hand, and large language model (LLM)-based evaluators such as G-Eval and GEMBA zero-shot direct assessment on the other hand. Specifically, we assess the consistency and robustness of these metrics under three controlled conditions: paraphrasing, hallucinations in model outputs, and variations in sentence length. Our analysis highlights the limitations of lexical overlap metrics and demonstrates that while LLM-based evaluators better capture semantic equivalence often missed by conventional metrics, they can also exhibit bias toward LLM-paraphrased translations. Moreover, although all metrics are able to detect hallucinations, BLEU tends to be overly sensitive, whereas BLEURT and LLM-based evaluators are comparatively lenient toward subtle cases. This motivates the need for multimodal evaluation frameworks that extend beyond text-based metrics to enable a more holistic assessment of SLT outputs.

**Link**: [arxiv](http://arxiv.org/abs/2510.25434v1),  [pdf](http://arxiv.org/pdf/2510.25434v1)

**Tags**: cs.CL 



### Depth and Autonomy: A Framework for Evaluating LLM Applications in   Social Science Research
**Authors**: Ali Sanaei, Ali Rajabzadeh

**Updated**: 2025-10-29T11:55:21Z

**Summary**: Large language models (LLMs) are increasingly utilized by researchers across a wide range of domains, and qualitative social science is no exception; however, this adoption faces persistent challenges, including interpretive bias, low reliability, and weak auditability. We introduce a framework that situates LLM usage along two dimensions, interpretive depth and autonomy, thereby offering a straightforward way to classify LLM applications in qualitative research and to derive practical design recommendations. We present the state of the literature with respect to these two dimensions, based on all published social science papers available on Web of Science that use LLMs as a tool and not strictly as the subject of study. Rather than granting models expansive freedom, our approach encourages researchers to decompose tasks into manageable segments, much as they would when delegating work to capable undergraduate research assistants. By maintaining low levels of autonomy and selectively increasing interpretive depth only where warranted and under supervision, one can plausibly reap the benefits of LLMs while preserving transparency and reliability.

**Link**: [arxiv](http://arxiv.org/abs/2510.25432v1),  [pdf](http://arxiv.org/pdf/2510.25432v1)

**Tags**: cs.CL 



### Alibaba International E-commerce Product Search Competition DcuRAGONs   Team Technical Report
**Authors**: Thang-Long Nguyen-Ho, Minh-Khoi Pham, Hoang-Bao Le

**Updated**: 2025-10-29T11:50:52Z

**Summary**: This report details our methodology and results developed for the Multilingual E-commerce Search Competition. The problem aims to recognize relevance between user queries versus product items in a multilingual context and improve recommendation performance on e-commerce platforms. Utilizing Large Language Models (LLMs) and their capabilities in other tasks, our data-centric method achieved the highest score compared to other solutions during the competition. Final leaderboard is publised at https://alibaba-international-cikm2025.github.io. The source code for our project is published at https://github.com/nhtlongcs/e-commerce-product-search.

**Link**: [arxiv](http://arxiv.org/abs/2510.25428v1),  [pdf](http://arxiv.org/pdf/2510.25428v1)

**Tags**: cs.IR cs.AI 



### RLMEval: Evaluating Research-Level Neural Theorem Proving
**Authors**: Auguste Poiroux, Antoine Bosselut, Viktor KunÄak

**Updated**: 2025-10-29T11:49:49Z

**Summary**: Despite impressive results on curated benchmarks, the practical impact of large language models (LLMs) on research-level neural theorem proving and proof autoformalization is still limited. We introduce RLMEval, an evaluation suite for these tasks, focusing on research-level mathematics from real-world Lean formalization projects. RLMEval targets the evaluation of neural theorem proving and proof autoformalization on challenging research-level theorems by leveraging real Lean Blueprint formalization projects. Our evaluation of state-of-the-art models on RLMEval, comprising 613 theorems from 6 Lean projects, reveals a significant gap: progress on existing benchmarks does not readily translate to these more realistic settings, with the best model achieving only a 10.3 % pass rate. RLMEval provides a new, challenging benchmark designed to guide and accelerate progress in automated reasoning for formal mathematics.

**Link**: [arxiv](http://arxiv.org/abs/2510.25427v1),  [pdf](http://arxiv.org/pdf/2510.25427v1)

**Tags**: cs.CL cs.AI 



### Implicature in Interaction: Understanding Implicature Improves Alignment   in Human-LLM Interaction
**Authors**: Asutosh Hota, Jussi P. P. Jokinen

**Updated**: 2025-10-29T11:49:42Z

**Summary**: The rapid advancement of Large Language Models (LLMs) is positioning language at the core of human-computer interaction (HCI). We argue that advancing HCI requires attention to the linguistic foundations of interaction, particularly implicature (meaning conveyed beyond explicit statements through shared context) which is essential for human-AI (HAI) alignment. This study examines LLMs' ability to infer user intent embedded in context-driven prompts and whether understanding implicature improves response generation. Results show that larger models approximate human interpretations more closely, while smaller models struggle with implicature inference. Furthermore, implicature-based prompts significantly enhance the perceived relevance and quality of responses across models, with notable gains in smaller models. Overall, 67.6% of participants preferred responses with implicature-embedded prompts to literal ones, highlighting a clear preference for contextually nuanced communication. Our work contributes to understanding how linguistic theory can be used to address the alignment problem by making HAI interaction more natural and contextually grounded.

**Link**: [arxiv](http://arxiv.org/abs/2510.25426v1),  [pdf](http://arxiv.org/pdf/2510.25426v1)

**Tags**: cs.CL cs.AI 



### Inferring Mobility Reductions from COVID-19 Disease Spread along the   Urban-Rural Gradient
**Authors**: Sydney Paltra, Jonas Dehning, Viola Priesemann, Kai Nagel

**Updated**: 2025-10-29T11:48:01Z

**Summary**: The COVID-19 pandemic reshaped human mobility through policy interventions and voluntary behavioral changes. Mobility adaptions helped mitigate pandemic spread, however our knowledge which environmental, social, and demographic factors helped mobility reduction and pandemic mitigation is patchy. We introduce a Bayesian hierarchical model to quantify heterogeneity in mobility responses across time and space in Germany's 400 districts using anonymized mobile phone data. Decomposing mobility into a disease-responsive component and disease-independent factors (temperature, school vacations, public holidays) allows us to quantify the impact of each factor. We find significant differences in reaction to disease spread along the urban-rural gradient, with large cities reducing mobility most strongly. Employment sectors further help explain variance in reaction strength during the first wave, while political variables gain significance during the second wave. However, reduced mobility only partially translates to lower peak incidence, indicating the influence of other hidden factors. Our results identify key drivers of mobility reductions and demonstrate that mobility behavior can serve as an operational proxy for population response.

**Link**: [arxiv](http://arxiv.org/abs/2510.25424v1),  [pdf](http://arxiv.org/pdf/2510.25424v1)

**Tags**: stat.AP physics.data-an 



### Small Talk, Big Impact? LLM-based Conversational Agents to Mitigate   Passive Fatigue in Conditional Automated Driving
**Authors**: Lewis Cockram, Yueteng Yu, Jorge Pardo, Xiaomeng Li, Andry Rakotonirainy, Jonny Kuo, Sebastien Demmel, Mike LennÃ©, Ronald Schroeter

**Updated**: 2025-10-29T11:40:38Z

**Summary**: Passive fatigue during conditional automated driving can compromise driver readiness and safety. This paper presents findings from a test-track study with 40 participants in a real-world rural automated driving scenario. In this scenario, a Large Language Model (LLM) based conversational agent (CA) was designed to check in with drivers and re-engage them with their surroundings. Drawing on in-car video recordings, sleepiness ratings and interviews, we analysed how drivers interacted with the agent and how these interactions shaped alertness. Users found the CA helpful for supporting vigilance during passive fatigue. Thematic analysis of acceptability further revealed three user preference profiles that implicate future intention to use CAs. Positioning empirically observed profiles within existing CA archetype frameworks highlights the need for adaptive design sensitive to diverse user groups. This work underscores the potential of CAs as proactive Human-Machine Interface (HMI) interventions, demonstrating how natural language can support context-aware interaction during automated driving.

**Link**: [arxiv](http://arxiv.org/abs/2510.25421v1),  [pdf](http://arxiv.org/pdf/2510.25421v1)

**Tags**: cs.HC 



### Improving Temporal Consistency and Fidelity at Inference-time in   Perceptual Video Restoration by Zero-shot Image-based Diffusion Models
**Authors**: Nasrin Rahimi, A. Murat Tekalp

**Updated**: 2025-10-29T11:40:06Z

**Summary**: Diffusion models have emerged as powerful priors for single-image restoration, but their application to zero-shot video restoration suffers from temporal inconsistencies due to the stochastic nature of sampling and complexity of incorporating explicit temporal modeling. In this work, we address the challenge of improving temporal coherence in video restoration using zero-shot image-based diffusion models without retraining or modifying their architecture. We propose two complementary inference-time strategies: (1) Perceptual Straightening Guidance (PSG) based on the neuroscience-inspired perceptual straightening hypothesis, which steers the diffusion denoising process towards smoother temporal evolution by incorporating a curvature penalty in a perceptual space to improve temporal perceptual scores, such as Fr\'echet Video Distance (FVD) and perceptual straightness; and (2) Multi-Path Ensemble Sampling (MPES), which aims at reducing stochastic variation by ensembling multiple diffusion trajectories to improve fidelity (distortion) scores, such as PSNR and SSIM, without sacrificing sharpness. Together, these training-free techniques provide a practical path toward temporally stable high-fidelity perceptual video restoration using large pretrained diffusion models. We performed extensive experiments over multiple datasets and degradation types, systematically evaluating each strategy to understand their strengths and limitations. Our results show that while PSG enhances temporal naturalness, particularly in case of temporal blur, MPES consistently improves fidelity and spatio-temporal perception--distortion trade-off across all tasks.

**Link**: [arxiv](http://arxiv.org/abs/2510.25420v1),  [pdf](http://arxiv.org/pdf/2510.25420v1)

**Tags**: eess.IV cs.AI 



### Serve Programs, Not Prompts
**Authors**: In Gim, Lin Zhong

**Updated**: 2025-10-29T11:29:03Z

**Summary**: Current large language model (LLM) serving systems, primarily designed for text completion, are neither efficient nor adaptable for increasingly complex LLM applications due to their inflexible design. We propose a new LLM serving system architecture that serves programs instead of prompts to address this problem. These programs, called LLM Inference Programs (LIPs), allow users to customize token prediction and KV cache management at runtime and to offload parts of their application logic, such as tool execution, to the server. We describe an example of this architecture through a system named Symphony, which functions as an operating system for LIPs. Symphony exposes LLM model computations via system calls and virtualizes KV cache with a dedicated file system, while ensuring GPU efficiency with a two-level process scheduling scheme. Symphony has the potential to open the door to a more efficient and extensible ecosystem for LLM applications.

**Link**: [arxiv](http://arxiv.org/abs/2510.25412v1),  [pdf](http://arxiv.org/pdf/2510.25412v1)

**Tags**: cs.CL 



### BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic   Domains
**Authors**: Vijay Devane, Mohd Nauman, Bhargav Patel, Aniket Mahendra Wakchoure, Yogeshkumar Sant, Shyam Pawar, Viraj Thakur, Ananya Godse, Sunil Patra, Neha Maurya, Suraj Racha, Nitish Kamal Singh, Ajay Nagpal, Piyush Sawarkar, Kundeshwar Vijayrao Pundalik, Rohit Saluja, Ganesh Ramakrishnan

**Updated**: 2025-10-30T10:48:05Z

**Summary**: The rapid advancement of large language models(LLMs) has intensified the need for domain and culture specific evaluation. Existing benchmarks are largely Anglocentric and domain-agnostic, limiting their applicability to India-centric contexts. To address this gap, we introduce BhashaBench V1, the first domain-specific, multi-task, bilingual benchmark focusing on critical Indic knowledge systems. BhashaBench V1 contains 74,166 meticulously curated question-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from authentic government and domain-specific exams. It spans four major domains: Agriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and covering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs reveals significant domain and language specific performance gaps, with especially large disparities in low-resource domains. For instance, GPT-4o achieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models consistently perform better on English content compared to Hindi across all domains. Subdomain-level analysis shows that areas such as Cyber Law, International Finance perform relatively well, while Panchakarma, Seed Science, and Human Rights remain notably weak. BhashaBench V1 provides a comprehensive dataset for evaluating large language models across India's diverse knowledge domains. It enables assessment of models' ability to integrate domain-specific knowledge with bilingual understanding. All code, benchmarks, and resources are publicly available to support open research.

**Link**: [arxiv](http://arxiv.org/abs/2510.25409v2),  [pdf](http://arxiv.org/pdf/2510.25409v2)

**Tags**: cs.CL cs.AI 



### GPTOpt: Towards Efficient LLM-Based Black-Box Optimization
**Authors**: Jamison Meindl, Yunsheng Tian, Tony Cui, Veronika Thost, Zhang-Wei Hong, Jie Chen, Wojciech Matusik, Mina KonakoviÄ LukoviÄ

**Updated**: 2025-10-29T11:21:55Z

**Summary**: Global optimization of expensive, derivative-free black-box functions demands extreme sample efficiency. Classical methods such as Bayesian Optimization (BO) can be effective, but they often require careful parameter tuning to each application domain. At the same time, Large Language Models (LLMs) have shown broad capabilities, yet state-of-the-art models remain limited in solving continuous black-box optimization tasks. We introduce GPTOpt, an LLM-based optimization method that equips LLMs with continuous black-box optimization capabilities. By fine-tuning large language models on extensive synthetic datasets derived from diverse BO parameterizations, GPTOpt leverages LLM pre-training to generalize across optimization tasks. On a variety of black-box optimization benchmarks, GPTOpt surpasses traditional optimizers, highlighting the capacity of LLMs for advanced numerical reasoning and introducing a flexible framework for global optimization without parameter tuning.

**Link**: [arxiv](http://arxiv.org/abs/2510.25404v1),  [pdf](http://arxiv.org/pdf/2510.25404v1)

**Tags**: cs.LG cs.AI 



### Towards Automated Quality Assurance of Patent Specifications: A   Multi-Dimensional LLM Framework
**Authors**: Yuqian Chai, Chaochao Wang, Weilei Wang

**Updated**: 2025-10-30T02:45:14Z

**Summary**: Although AI drafting tools have gained prominence in patent writing, the systematic evaluation of AI-generated patent content quality represents a significant research gap. To address this gap, We propose to evaluate patents using regulatory compliance, technical coherence, and figure-reference consistency detection modules, and then generate improvement suggestions via an integration module. The framework is validated on a comprehensive dataset comprising 80 human-authored and 80 AI-generated patents from two patent drafting tools. Evaluation is performed on 10,841 total sentences, 8,924 non-template sentences, and 554 patent figures for the three detection modules respectively, achieving balanced accuracies of 99.74%, 82.12%, and 91.2% against expert annotations. Additional analysis was conducted to examine defect distributions across patent sections, technical domains, and authoring sources. Section-based analysis indicates that figure-text consistency and technical detail precision require particular attention. Mechanical Engineering and Construction show more claim-specification inconsistencies due to complex technical documentation requirements. AI-generated patents show a significant gap compared to human-authored ones. While human-authored patents primarily contain surface-level errors like typos, AI-generated patents exhibit more structural defects in figure-text alignment and cross-references.

**Link**: [arxiv](http://arxiv.org/abs/2510.25402v2),  [pdf](http://arxiv.org/pdf/2510.25402v2)

**Tags**: cs.IR cs.CE 



### Robust Preference Optimization via Dynamic Target Margins
**Authors**: Jie Sun, Junkang Wu, Jiancan Wu, Zhibo Zhu, Xingyu Lu, Jun Zhou, Lintao Ma, Xiang Wang

**Updated**: 2025-10-29T11:04:12Z

**Summary**: The alignment of Large Language Models (LLMs) is crucial for ensuring their safety and reliability in practical applications. Direct Preference Optimization (DPO) has emerged as an efficient method that directly optimizes models using preference pairs, significantly reducing resource demands. However, the effectiveness of DPO heavily depends on the data quality, which is frequently compromised by noise. In this work, we propose $\gamma$-PO, a dynamic target margin preference optimization algorithm that adjust reward margins at the pairwise level. By introducing instance-specific margin calibration, $\gamma$-PO strategically prioritizes high-confidence pairs (those demonstrating higher reward margins) while suppressing potential noise from ambiguous pairs. Moreover, $\gamma$-PO is a plug-and-play method, compatible with variants of DPO that rely on reward margin between preference pairs. Across benchmarks such as AlpacaEval2 and Arena-Hard, $\gamma$-PO achieves an average 4.4\% improvement over other baselines, setting new benchmarks for state-of-the-art performance. Additionally, $\gamma$-PO requires minimal code changes and has a negligible impact on training efficiency, making it a robust solution for enhancing LLMs alignment. Our codes are available at \href{https://github.com/sunjie279/gammaPO}{https://github.com/sunjie279/gammaPO}.

**Link**: [arxiv](http://arxiv.org/abs/2506.03690v2),  [pdf](http://arxiv.org/pdf/2506.03690v2)

**Tags**: cs.CL 



### Grouping Nodes With Known Value Differences: A Lossless UCT-based   Abstraction Algorithm
**Authors**: Robin SchmÃ¶cker, Alexander Dockhorn, Bodo Rosenhahn

**Updated**: 2025-10-29T11:03:44Z

**Summary**: A core challenge of Monte Carlo Tree Search (MCTS) is its sample efficiency, which can be improved by grouping state-action pairs and using their aggregate statistics instead of single-node statistics. On the Go Abstractions in Upper Confidence bounds applied to Trees (OGA-UCT) is the state-of-the-art MCTS abstraction algorithm for deterministic environments that builds its abstraction using the Abstractions of State-Action Pairs (ASAP) framework, which aims to detect states and state-action pairs with the same value under optimal play by analysing the search graph. ASAP, however, requires two state-action pairs to have the same immediate reward, which is a rigid condition that limits the number of abstractions that can be found and thereby the sample efficiency. In this paper, we break with the paradigm of grouping value-equivalent states or state-action pairs and instead group states and state-action pairs with possibly different values as long as the difference between their values can be inferred. We call this abstraction framework Known Value Difference Abstractions (KVDA), which infers the value differences by analysis of the immediate rewards and modifies OGA-UCT to use this framework instead. The modification is called KVDA-UCT, which detects significantly more abstractions than OGA-UCT, introduces no additional parameter, and outperforms OGA-UCT on a variety of deterministic environments and parameter settings.

**Link**: [arxiv](http://arxiv.org/abs/2510.25388v1),  [pdf](http://arxiv.org/pdf/2510.25388v1)

**Tags**: cs.AI 



### GnnXemplar: Exemplars to Explanations -- Natural Language Rules for   Global GNN Interpretability
**Authors**: Burouj Armgaan, Eshan Jain, Harsh Pandey, Mahesh Chandran, Sayan Ranu

**Updated**: 2025-10-29T10:57:22Z

**Summary**: Graph Neural Networks (GNNs) are widely used for node classification, yet their opaque decision-making limits trust and adoption. While local explanations offer insights into individual predictions, global explanation methods, those that characterize an entire class, remain underdeveloped. Existing global explainers rely on motif discovery in small graphs, an approach that breaks down in large, real-world settings where subgraph repetition is rare, node attributes are high-dimensional, and predictions arise from complex structure-attribute interactions. We propose GnnXemplar, a novel global explainer inspired from Exemplar Theory from cognitive science. GnnXemplar identifies representative nodes in the GNN embedding space, exemplars, and explains predictions using natural language rules derived from their neighborhoods. Exemplar selection is framed as a coverage maximization problem over reverse k-nearest neighbors, for which we provide an efficient greedy approximation. To derive interpretable rules, we employ a self-refining prompt strategy using large language models (LLMs). Experiments across diverse benchmarks show that GnnXemplar significantly outperforms existing methods in fidelity, scalability, and human interpretability, as validated by a user study with 60 participants.

**Link**: [arxiv](http://arxiv.org/abs/2509.18376v2),  [pdf](http://arxiv.org/pdf/2509.18376v2)

**Tags**: cs.LG cs.SI 



### Roleplaying with Structure: Synthetic Therapist-Client Conversation   Generation from Questionnaires
**Authors**: Doan Nam Long Vu, Rui Tan, Lena Moench, Svenja Jule Francke, Daniel Woiwod, Florian Thomas-Odenthal, Sanna Stroth, Tilo Kircher, Christiane Hermann, Udo Dannlowski, Hamidreza Jamalabadi, Shaoxiong Ji

**Updated**: 2025-10-29T10:55:52Z

**Summary**: The development of AI for mental health is hindered by a lack of authentic therapy dialogues, due to strict privacy regulations and the fact that clinical sessions were historically rarely recorded. We present an LLM-driven pipeline that generates synthetic counseling dialogues based on structured client profiles and psychological questionnaires. Grounded on the principles of Cognitive Behavioral Therapy (CBT), our method creates synthetic therapeutic conversations for clinical disorders such as anxiety and depression. Our framework, SQPsych (Structured Questionnaire-based Psychotherapy), converts structured psychological input into natural language dialogues through therapist-client simulations. Due to data governance policies and privacy restrictions prohibiting the transmission of clinical questionnaire data to third-party services, previous methodologies relying on proprietary models are infeasible in our setting. We address this limitation by generating a high-quality corpus using open-weight LLMs, validated through human expert evaluation and LLM-based assessments. Our SQPsychLLM models fine-tuned on SQPsychConv achieve strong performance on counseling benchmarks, surpassing baselines in key therapeutic skills. Our findings highlight the potential of synthetic data to enable scalable, data-secure, and clinically informed AI for mental health support. We will release our code, models, and corpus at https://ai-mh.github.io/SQPsych

**Link**: [arxiv](http://arxiv.org/abs/2510.25384v1),  [pdf](http://arxiv.org/pdf/2510.25384v1)

**Tags**: cs.CL 



### CGM-Led Multimodal Tracking with Chatbot Support: An Autoethnography in   Sub-Health
**Authors**: Dongyijie Primo Pan, Lan Luo, Yike Wang, Pan Hui

**Updated**: 2025-10-29T10:53:32Z

**Summary**: Metabolic disorders present a pressing global health challenge, with China carrying the world's largest burden. While continuous glucose monitoring (CGM) has transformed diabetes care, its potential for supporting sub-health populations -- such as individuals who are overweight, prediabetic, or anxious -- remains underexplored. At the same time, large language models (LLMs) are increasingly used in health coaching, yet CGM is rarely incorporated as a first-class signal. To address this gap, we conducted a six-week autoethnography, combining CGM with multimodal indicators captured via common digital devices and a chatbot that offered personalized reflections and explanations of glucose fluctuations. Our findings show how CGM-led, data-first multimodal tracking, coupled with conversational support, shaped everyday practices of diet, activity, stress, and wellbeing. This work contributes to HCI by extending CGM research beyond clinical diabetes and demonstrating how LLM-driven agents can support preventive health and reflection in at-risk populations.

**Link**: [arxiv](http://arxiv.org/abs/2510.25381v1),  [pdf](http://arxiv.org/pdf/2510.25381v1)

**Tags**: cs.HC 



### Hallucinations in Bibliographic Recommendation: Citation Frequency as a   Proxy for Training Data Redundancy
**Authors**: Junichiro Niimi

**Updated**: 2025-10-29T10:51:35Z

**Summary**: Large language models (LLMs) have been increasingly applied to a wide range of tasks, from natural language understanding to code generation. While they have also been used to assist in bibliographic recommendation, the hallucination of non-existent papers remains a major issue. Building on prior studies, this study hypothesizes that an LLM's ability to correctly produce bibliographic information depends on whether the underlying knowledge is generated or memorized, with highly cited papers (i.e., more frequently appear in the training corpus) showing lower hallucination rates. We therefore assume citation count as a proxy for training data redundancy (i.e., the frequency with which a given bibliographic record is repeatedly represented in the pretraining corpus) and investigate how citation frequency affects hallucinated references in LLM outputs. Using GPT-4.1, we generated and manually verified 100 bibliographic records across twenty computer-science domains, and measured factual consistency via cosine similarity between generated and authentic metadata. The results revealed that (i) hallucination rates vary across research domains, (ii) citation count is strongly correlated with factual accuracy, and (iii) bibliographic information becomes almost verbatimly memorized beyond approximately 1,000 citations. These findings suggest that highly cited papers are nearly verbatimly retained in the model, indicating a threshold where generalization shifts into memorization.

**Link**: [arxiv](http://arxiv.org/abs/2510.25378v1),  [pdf](http://arxiv.org/pdf/2510.25378v1)

**Tags**: cs.CL cs.AI 



### LaMP-Val: Large Language Models Empower Personalized Valuation in   Auction
**Authors**: Jie Sun, Tianyu Zhang, Houcheng Jiang, Kexin Huang, Xiang Shu, Zhibo Zhu, Lintao Ma, Xingyu Lu, Jun Zhou, Junkang Wu, Chi Luo, An Zhang, Junkang Wu, Jiancan Wu, Xiang Wang

**Updated**: 2025-10-29T10:48:48Z

**Summary**: Auctions are a vital economic mechanism used to determine the market value of goods or services through competitive bidding within a specific framework. However, much of the current research primarily focuses on the bidding algorithms used within auction mechanisms. This often neglects the potential benefits of incorporating individual users' unique preferences into the valuation process. Our theoretical and empirical analysis demonstrates that valuation errors can significantly impact the overall utility. To bridge this gap, we propose a personalized valuation framework, namely Large \underline{La}nguage \underline{M}odels-powered \underline{P}ersonalized \underline{Val}uation (LaMP-Val), which integrates Large Language Models to incorporate personalized semantic preference into users valuation process. LaMP-Val integrating three components: data, learning, and evaluation. The data component tackles the challenge of building a novel dataset specifically for LLMs fine-tuning in personalized valuation modeling. The learning component introduces a diversity template to enhance LLMs' capacity for modeling fine-grained personal valuation patterns. The evaluation component establishes a closed-loop system where LLM-generated valuations interact with bidding strategies and auction. It proposes two novel metrics to quantify valuation precision and bidding intention accuracy in personalized scenarios. Extensive experiments show that LaMP-Val more accurately captures personalized values and achieves greater profits than baseline approaches.

**Link**: [arxiv](http://arxiv.org/abs/2410.15817v2),  [pdf](http://arxiv.org/pdf/2410.15817v2)

**Tags**: cs.CE 



### Latent variable estimation with composite Hilbert space Gaussian   processes
**Authors**: Soham Mukherjee, Javier Enrique Aguilar, Marcello Zago, Manfred Claassen, Paul-Christian BÃ¼rkner

**Updated**: 2025-10-29T10:42:56Z

**Summary**: We develop a scalable class of models for latent variable estimation using composite Gaussian processes, with a focus on derivative Gaussian processes. We jointly model multiple data sources as outputs to improve the accuracy of latent variable inference under a single probabilistic framework. Similarly specified exact Gaussian processes scale poorly with large datasets. To overcome this, we extend the recently developed Hilbert space approximation methods for Gaussian processes to obtain a reduced-rank representation of the composite covariance function through its spectral decomposition. Specifically, we derive and analyze the spectral decomposition of derivative covariance functions and further study their properties theoretically. Using these spectral decompositions, our methods easily scale up to data scenarios involving thousands of samples. We validate our methods in terms of latent variable estimation accuracy, uncertainty calibration, and inference speed across diverse simulation scenarios. Finally, using a real world case study from single-cell biology, we demonstrate the potential of our models in estimating latent cellular ordering given gene expression levels, thus enhancing our understanding of the underlying biological process.

**Link**: [arxiv](http://arxiv.org/abs/2510.25371v1),  [pdf](http://arxiv.org/pdf/2510.25371v1)

**Tags**: stat.ME 



### Monitoring Transformative Technological Convergence Through   LLM-Extracted Semantic Entity Triple Graphs
**Authors**: Alexander Sternfeld, Andrei Kucharavy, Dimitri Percia David, Alain Mermoud, Julian Jang-Jaccard, Nathan Monnet

**Updated**: 2025-10-29T10:41:03Z

**Summary**: Forecasting transformative technologies remains a critical but challenging task, particularly in fast-evolving domains such as Information and Communication Technologies (ICTs). Traditional expert-based methods struggle to keep pace with short innovation cycles and ambiguous early-stage terminology. In this work, we propose a novel, data-driven pipeline to monitor the emergence of transformative technologies by identifying patterns of technological convergence.   Our approach leverages advances in Large Language Models (LLMs) to extract semantic triples from unstructured text and construct a large-scale graph of technology-related entities and relations. We introduce a new method for grouping semantically similar technology terms (noun stapling) and develop graph-based metrics to detect convergence signals. The pipeline includes multi-stage filtering, domain-specific keyword clustering, and a temporal trend analysis of topic co-occurence.   We validate our methodology on two complementary datasets: 278,625 arXiv preprints (2017--2024) to capture early scientific signals, and 9,793 USPTO patent applications (2018-2024) to track downstream commercial developments. Our results demonstrate that the proposed pipeline can identify both established and emerging convergence patterns, offering a scalable and generalizable framework for technology forecasting grounded in full-text analysis.

**Link**: [arxiv](http://arxiv.org/abs/2510.25370v1),  [pdf](http://arxiv.org/pdf/2510.25370v1)

**Tags**: cs.CL 



### Have a thing? Reasoning around recursion with dynamic typing in grounded   arithmetic
**Authors**: Elliot Bobrow, Bryan Ford, Stefan MilenkoviÄ

**Updated**: 2025-10-29T10:40:12Z

**Summary**: Neither the classical nor intuitionistic logic traditions are perfectly-aligned with the purpose of reasoning about computation, in that neither logical tradition can normally permit the direct expression of arbitrary general-recursive functions without inconsistency. We introduce grounded arithmetic or GA, a minimalistic but nonetheless powerful foundation for formal reasoning that allows the direct expression of arbitrary recursive definitions. GA adjusts the traditional inference rules such that terms that express nonterminating computations harmlessly denote no semantic value (i.e., "bottom") instead of leading into logical paradox or inconsistency. Recursive functions may be proven terminating in GA essentially by "dynamically typing" terms, or equivalently, symbolically reverse-executing the computations they denote via GA's inference rules. Once recursive functions have been proven terminating, logical reasoning about their results reduce to the familiar classical rules. A mechanically-checked consistency proof in Isabelle/HOL exists for the basic quantifier-free fragment of GA. Quantifiers may be added atop this foundation as ordinary computations, whose inference rules are thus admissible and do not introduce new inconsistency risks. While GA is only a first step towards richly-typed grounded deduction practical for everyday use in manual or automated computational reasoning, it shows the promise that the expressive freedom of arbitrary recursive definition can in principle be incorporated into formal systems.

**Link**: [arxiv](http://arxiv.org/abs/2510.25369v1),  [pdf](http://arxiv.org/pdf/2510.25369v1)

**Tags**: cs.PL cs.FL cs.LO math.LO F.3.1; F.4.1 



### Position: Biology is the Challenge Physics-Informed ML Needs to Evolve
**Authors**: Julien Martinelli

**Updated**: 2025-10-29T10:39:29Z

**Summary**: Physics-Informed Machine Learning (PIML) has successfully integrated mechanistic understanding into machine learning, particularly in domains governed by well-known physical laws. This success has motivated efforts to apply PIML to biology, a field rich in dynamical systems but shaped by different constraints. Biological modeling, however, presents unique challenges: multi-faceted and uncertain prior knowledge, heterogeneous and noisy data, partial observability, and complex, high-dimensional networks. In this position paper, we argue that these challenges should not be seen as obstacles to PIML, but as catalysts for its evolution. We propose Biology-Informed Machine Learning (BIML): a principled extension of PIML that retains its structural grounding while adapting to the practical realities of biology. Rather than replacing PIML, BIML retools its methods to operate under softer, probabilistic forms of prior knowledge. We outline four foundational pillars as a roadmap for this transition: uncertainty quantification, contextualization, constrained latent structure inference, and scalability. Foundation Models and Large Language Models will be key enablers, bridging human expertise with computational modeling. We conclude with concrete recommendations to build the BIML ecosystem and channel PIML-inspired innovation toward challenges of high scientific and societal relevance.

**Link**: [arxiv](http://arxiv.org/abs/2510.25368v1),  [pdf](http://arxiv.org/pdf/2510.25368v1)

**Tags**: cs.LG cs.AI cs.NE 



### OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents
**Authors**: Thomas Kuntz, Agatha Duzan, Hao Zhao, Francesco Croce, Zico Kolter, Nicolas Flammarion, Maksym Andriushchenko

**Updated**: 2025-10-29T10:34:04Z

**Summary**: Computer use agents are LLM-based agents that can directly interact with a graphical user interface, by processing screenshots or accessibility trees. While these systems are gaining popularity, their safety has been largely overlooked, despite the fact that evaluating and understanding their potential for harmful behavior is essential for widespread adoption. To address this gap, we introduce OS-Harm, a new benchmark for measuring safety of computer use agents. OS-Harm is built on top of the OSWorld environment and aims to test models across three categories of harm: deliberate user misuse, prompt injection attacks, and model misbehavior. To cover these cases, we create 150 tasks that span several types of safety violations (harassment, copyright infringement, disinformation, data exfiltration, etc.) and require the agent to interact with a variety of OS applications (email client, code editor, browser, etc.). Moreover, we propose an automated judge to evaluate both accuracy and safety of agents that achieves high agreement with human annotations (0.76 and 0.79 F1 score). We evaluate computer use agents based on a range of frontier models - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide insights into their safety. In particular, all models tend to directly comply with many deliberate misuse queries, are relatively vulnerable to static prompt injections, and occasionally perform unsafe actions. The OS-Harm benchmark is available at https://github.com/tml-epfl/os-harm.

**Link**: [arxiv](http://arxiv.org/abs/2506.14866v2),  [pdf](http://arxiv.org/pdf/2506.14866v2)

**Tags**: cs.SE cs.LG 



### Not ready for the bench: LLM legal interpretation is unstable and out of   step with human judgments
**Authors**: Abhishek Purushothama, Junghyun Min, Brandon Waldon, Nathan Schneider

**Updated**: 2025-10-29T10:21:25Z

**Summary**: Legal interpretation frequently involves assessing how a legal text, as understood by an 'ordinary' speaker of the language, applies to the set of facts characterizing a legal dispute in the U.S. judicial system. Recent scholarship has proposed that legal practitioners add large language models (LLMs) to their interpretive toolkit. This work offers an empirical argument against LLM interpretation as recently practiced by legal scholars and federal judges. Our investigation in English shows that models do not provide stable interpretive judgments: varying the question format can lead the model to wildly different conclusions. Moreover, the models show weak to moderate correlation with human judgment, with large variance across model and question variant, suggesting that it is dangerous to give much credence to the conclusions produced by generative AI.

**Link**: [arxiv](http://arxiv.org/abs/2510.25356v1),  [pdf](http://arxiv.org/pdf/2510.25356v1)

**Tags**: cs.CL 



### Differential Mamba
**Authors**: Nadav Schneider, Itamar Zimerman, Eliya Nachmani

**Updated**: 2025-10-29T10:17:57Z

**Summary**: Sequence models like Transformers and RNNs often overallocate attention to irrelevant context, leading to noisy intermediate representations. This degrades LLM capabilities by promoting hallucinations, weakening long-range and retrieval abilities, and reducing robustness. Recent work has shown that differential design can mitigate this issue in Transformers, improving their effectiveness across various applications. In this paper, we explore whether these techniques, originally developed for Transformers, can be applied to Mamba, a recent architecture based on selective state-space layers that achieves Transformer-level performance with greater efficiency. We show that a naive adaptation of differential design to Mamba is insufficient and requires careful architectural modifications. To address this, we introduce a novel differential mechanism for Mamba, empirically validated on language modeling benchmarks, demonstrating improved retrieval capabilities and superior performance over vanilla Mamba. Finally, we conduct extensive ablation studies and empirical analyses to justify our design choices and provide evidence that our approach effectively mitigates the overallocation problem in Mamba-based models. Our code is publicly available: https://github.com/NadavSc/Diff-Mamba

**Link**: [arxiv](http://arxiv.org/abs/2507.06204v2),  [pdf](http://arxiv.org/pdf/2507.06204v2)

**Tags**: cs.LG cs.AI cs.CL 



## Keyword: LLM Deployment 
 ### E-Scores for (In)Correctness Assessment of Generative Model Outputs
**Authors**: Guneet S. Dhillon, Javier GonzÃ¡lez, Teodora Pandeva, Alicia Curth

**Updated**: 2025-10-29T17:59:16Z

**Summary**: While generative models, especially large language models (LLMs), are ubiquitous in today's world, principled mechanisms to assess their (in)correctness are limited. Using the conformal prediction framework, previous works construct sets of LLM responses where the probability of including an incorrect response, or error, is capped at a desired user-defined tolerance level. However, since these methods are based on p-values, they are susceptible to p-hacking, i.e., choosing the tolerance level post-hoc can invalidate the guarantees. We therefore leverage e-values to complement generative model outputs with e-scores as a measure of incorrectness. In addition to achieving the same statistical guarantees as before, e-scores provide users flexibility in adaptively choosing tolerance levels after observing the e-scores themselves, by upper bounding a post-hoc notion of error called size distortion. We experimentally demonstrate their efficacy in assessing LLM outputs for different correctness types: mathematical factuality and property constraints satisfaction.

**Link**: [arxiv](http://arxiv.org/abs/2510.25770v1),  [pdf](http://arxiv.org/pdf/2510.25770v1)

**Tags**: stat.ML cs.AI cs.LG 



### Decomposition-Enhanced Training for Post-Hoc Attributions In Language   Models
**Authors**: Sriram Balasubramaniam, Samyadeep Basu, Koustava Goswami, Ryan Rossi, Varun Manjunatha, Roshan Santhosh, Ruiyi Zhang, Soheil Feizi, Nedim Lipka

**Updated**: 2025-10-29T17:58:59Z

**Summary**: Large language models (LLMs) are increasingly used for long-document question answering, where reliable attribution to sources is critical for trust. Existing post-hoc attribution methods work well for extractive QA but struggle in multi-hop, abstractive, and semi-extractive settings, where answers synthesize information across passages. To address these challenges, we argue that post-hoc attribution can be reframed as a reasoning problem, where answers are decomposed into constituent units, each tied to specific context. We first show that prompting models to generate such decompositions alongside attributions improves performance. Building on this, we introduce DecompTune, a post-training method that teaches models to produce answer decompositions as intermediate reasoning steps. We curate a diverse dataset of complex QA tasks, annotated with decompositions by a strong LLM, and post-train Qwen-2.5 (7B and 14B) using a two-stage SFT + GRPO pipeline with task-specific curated rewards. Across extensive experiments and ablations, DecompTune substantially improves attribution quality, outperforming prior methods and matching or exceeding state-of-the-art frontier models.

**Link**: [arxiv](http://arxiv.org/abs/2510.25766v1),  [pdf](http://arxiv.org/pdf/2510.25766v1)

**Tags**: cs.CL 



### 3D Optimization for AI Inference Scaling: Balancing Accuracy, Cost, and   Latency
**Authors**: Minseok Jung, Abhas Ricky, Muhammad Rameez Chatni

**Updated**: 2025-10-29T17:57:23Z

**Summary**: AI inference scaling is often tuned through 1D heuristics (a fixed reasoning passes) or 2D bivariate trade-offs (e.g., performance vs. compute), which fail to consider cost and latency constraints. We introduce a 3D optimization framework that jointly calibrates accuracy, cost, and latency within a unified decision space, enabling constraints-aware inference scaling. Using Monte Carlo simulations across three representative scenarios and nine simulated large language models, we evaluate four optimization methods to address the 3D multi-objective optimization (MOO) problem. Framing inference scaling in MOO shapes a feasible space that 1D and 2D optimizations fail to capture, enabling environmentadaptive selection of the inference scaling k. Results show that knee-point optimization achieves the best balance, while accuracy-maximization remains favorable when precision is prioritized. The framework establishes a theoretical foundation for deployment-aware inference scaling across diverse operational contexts.

**Link**: [arxiv](http://arxiv.org/abs/2510.18905v2),  [pdf](http://arxiv.org/pdf/2510.18905v2)

**Tags**: cs.LG cs.AI 



### SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars
**Authors**: Xiaosheng Zhao, Yang Huang, Guirong Xue, Xiao Kong, Jifeng Liu, Xiaoyu Tang, Timothy C. Beers, Yuan-Sen Ting, A-Li Luo

**Updated**: 2025-10-29T17:57:03Z

**Summary**: In recent years, large language models (LLMs) have transformed natural language understanding through vast datasets and large-scale parameterization. Inspired by this success, we present SpecCLIP, a foundation model framework that extends LLM-inspired methodologies to stellar spectral analysis. Stellar spectra, akin to structured language, encode rich physical and chemical information about stars. By training foundation models on large-scale spectral datasets, our goal is to learn robust and informative embeddings that support diverse downstream applications. As a proof of concept, SpecCLIP involves pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed by contrastive alignment using the CLIP (Contrastive Language-Image Pre-training) framework, adapted to associate spectra from different instruments. This alignment is complemented by auxiliary decoders that preserve spectrum-specific information and enable translation (prediction) between spectral types, with the former achieved by maximizing mutual information between embeddings and input spectra. The result is a cross-spectrum framework enabling intrinsic calibration and flexible applications across instruments. We demonstrate that fine-tuning these models on moderate-sized labeled datasets improves adaptability to tasks such as stellar-parameter estimation and chemical-abundance determination. SpecCLIP also enhances the accuracy and precision of parameter estimates benchmarked against external survey data. Additionally, its similarity search and cross-spectrum prediction capabilities offer potential for anomaly detection. Our results suggest that contrastively trained foundation models enriched with spectrum-aware decoders can advance precision stellar spectroscopy.

**Link**: [arxiv](http://arxiv.org/abs/2507.01939v3),  [pdf](http://arxiv.org/pdf/2507.01939v3)

**Tags**: astro-ph.IM astro-ph.SR cs.AI cs.LG 



### DiagramEval: Evaluating LLM-Generated Diagrams via Graphs
**Authors**: Chumeng Liang, Jiaxuan You

**Updated**: 2025-10-29T17:56:17Z

**Summary**: Diagrams play a central role in research papers for conveying ideas, yet they are often notoriously complex and labor-intensive to create. Although diagrams are presented as images, standard image generative models struggle to produce clear diagrams with well-defined structure. We argue that a promising direction is to generate demonstration diagrams directly in textual form as SVGs, which can leverage recent advances in large language models (LLMs). However, due to the complexity of components and the multimodal nature of diagrams, sufficiently discriminative and explainable metrics for evaluating the quality of LLM-generated diagrams remain lacking. In this paper, we propose DiagramEval, a novel evaluation metric designed to assess demonstration diagrams generated by LLMs. Specifically, DiagramEval conceptualizes diagrams as graphs, treating text elements as nodes and their connections as directed edges, and evaluates diagram quality using two new groups of metrics: node alignment and path alignment. For the first time, we effectively evaluate diagrams produced by state-of-the-art LLMs on recent research literature, quantitatively demonstrating the validity of our metrics. Furthermore, we show how the enhanced explainability of our proposed metrics offers valuable insights into the characteristics of LLM-generated diagrams. Code: https://github.com/ulab-uiuc/diagram-eval.

**Link**: [arxiv](http://arxiv.org/abs/2510.25761v1),  [pdf](http://arxiv.org/pdf/2510.25761v1)

**Tags**: cs.CL 



### TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological   Counseling
**Authors**: He Hu, Yucheng Zhou, Chiyuan Ma, Qianning Wang, Zheng Zhang, Fei Ma, Laizhong Cui, Qi Tian

**Updated**: 2025-10-29T17:54:20Z

**Summary**: Large language models (LLMs) in psychological counseling have attracted increasing attention. However, existing approaches often lack emotional understanding, adaptive strategies, and the use of therapeutic methods across multiple sessions with long-term memory, leaving them far from real clinical practice. To address these critical gaps, we introduce TheraMind, a strategic and adaptive agent for longitudinal psychological counseling. The cornerstone of TheraMind is a novel dual-loop architecture that decouples the complex counseling process into an Intra-Session Loop for tactical dialogue management and a Cross-Session Loop for strategic therapeutic planning. The Intra-Session Loop perceives the patient's emotional state to dynamically select response strategies while leveraging cross-session memory to ensure continuity. Crucially, the Cross-Session Loop empowers the agent with long-term adaptability by evaluating the efficacy of the applied therapy after each session and adjusting the method for subsequent interactions. We validate our approach in a high-fidelity simulation environment grounded in real clinical cases. Extensive evaluations show that TheraMind outperforms other methods, especially on multi-session metrics like Coherence, Flexibility, and Therapeutic Attunement, validating the effectiveness of its dual-loop design in emulating strategic, adaptive, and longitudinal therapeutic behavior. The code is publicly available at https://0mwwm0.github.io/TheraMind/.

**Link**: [arxiv](http://arxiv.org/abs/2510.25758v1),  [pdf](http://arxiv.org/pdf/2510.25758v1)

**Tags**: cs.AI 



### Agentic Economic Modeling
**Authors**: Bohan Zhang, Jiaxuan Li, Ali HortaÃ§su, Xiaoyang Ye, Victor Chernozhukov, Angelo Ni, Edward Huang

**Updated**: 2025-10-29T17:46:07Z

**Summary**: We introduce Agentic Economic Modeling (AEM), a framework that aligns synthetic LLM choices with small-sample human evidence for reliable econometric inference. AEM first generates task-conditioned synthetic choices via LLMs, then learns a bias-correction mapping from task features and raw LLM choices to human-aligned choices, upon which standard econometric estimators perform inference to recover demand elasticities and treatment effects.We validate AEM in two experiments. In a large scale conjoint study with millions of observations, using only 10% of the original data to fit the correction model lowers the error of the demand-parameter estimates, while uncorrected LLM choices even increase the errors. In a regional field experiment, a mixture model calibrated on 10% of geographic regions estimates an out-of-domain treatment effect of -65\pm10 bps, closely matching the full human experiment (-60\pm8 bps).Under time-wise extrapolation, training with only day-one human data yields -24 bps (95% CI: [-26, -22], p<1e-5),improving over the human-only day-one baseline (-17 bps, 95% CI: [-43, +9], p=0.2049).These results demonstrate AEM's potential to improve RCT efficiency and establish a foundation method for LLM-based counterfactual generation.

**Link**: [arxiv](http://arxiv.org/abs/2510.25743v1),  [pdf](http://arxiv.org/pdf/2510.25743v1)

**Tags**: econ.EM 



### Scaling Latent Reasoning via Looped Language Models
**Authors**: Rui-Jie Zhu, Zixuan Wang, Kai Hua, Tianyu Zhang, Ziniu Li, Haoran Que, Boyi Wei, Zixin Wen, Fan Yin, He Xing, Lu Li, Jiajun Shi, Kaijing Ma, Shanda Li, Taylor Kergan, Andrew Smith, Xingwei Qu, Mude Hui, Bohong Wu, Qiyang Min, Hongzhi Huang, Xun Zhou, Wei Ye, Jiaheng Liu, Jian Yang, Yunfeng Shi, Chenghua Lin, Enduo Zhao, Tianle Cai, Ge Zhang, Wenhao Huang, Yoshua Bengio, Jason Eshraghian

**Updated**: 2025-10-29T17:45:42Z

**Summary**: Modern LLMs are trained to "think" primarily via explicit text generation, such as chain-of-thought (CoT), which defers reasoning to post-training and under-leverages pre-training data. We present and open-source Ouro, named after the recursive Ouroboros, a family of pre-trained Looped Language Models (LoopLM) that instead build reasoning into the pre-training phase through (i) iterative computation in latent space, (ii) an entropy-regularized objective for learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and 2.6B models enjoy superior performance that match the results of up to 12B SOTA LLMs across a wide range of benchmarks. Through controlled experiments, we show this advantage stems not from increased knowledge capacity, but from superior knowledge manipulation capabilities. We also show that LoopLM yields reasoning traces more aligned with final outputs than explicit CoT. We hope our results show the potential of LoopLM as a novel scaling direction in the reasoning era. Our model could be found in: http://ouro-llm.github.io.

**Link**: [arxiv](http://arxiv.org/abs/2510.25741v1),  [pdf](http://arxiv.org/pdf/2510.25741v1)

**Tags**: cs.CL 



### The Limits of Obliviate: Evaluating Unlearning in LLMs via   Stimulus-Knowledge Entanglement-Behavior Framework
**Authors**: Aakriti Shah, Thai Le

**Updated**: 2025-10-29T17:37:50Z

**Summary**: Unlearning in large language models (LLMs) is crucial for managing sensitive data and correcting misinformation, yet evaluating its effectiveness remains an open problem. We investigate whether persuasive prompting can recall factual knowledge from deliberately unlearned LLMs across models ranging from 2.7B to 13B parameters (OPT-2.7B, LLaMA-2-7B, LLaMA-3.1-8B, LLaMA-2-13B). Drawing from ACT-R and Hebbian theory (spreading activation theories), as well as communication principles, we introduce Stimulus-Knowledge Entanglement-Behavior Framework (SKeB), which models information entanglement via domain graphs and tests whether factual recall in unlearned models is correlated with persuasive framing. We develop entanglement metrics to quantify knowledge activation patterns and evaluate factuality, non-factuality, and hallucination in outputs. Our results show persuasive prompts substantially enhance factual knowledge recall (14.8% baseline vs. 24.5% with authority framing), with effectiveness inversely correlated to model size (128% recovery in 2.7B vs. 15% in 13B). SKeB provides a foundation for assessing unlearning completeness, robustness, and overall behavior in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2510.25732v1),  [pdf](http://arxiv.org/pdf/2510.25732v1)

**Tags**: cs.CL cs.AI I.2.7; I.2.6; I.2.4; G.2.2 



### GENRE-CMR: Generalizable Deep Learning for Diverse Multi-Domain Cardiac   MRI Reconstruction
**Authors**: Kian Anvari Hamedani, Narges Razizadeh, Shahabedin Nabavi, Mohsen Ebrahimi Moghaddam

**Updated**: 2025-10-29T17:35:15Z

**Summary**: Accelerated Cardiovascular Magnetic Resonance (CMR) image reconstruction remains a critical challenge due to the trade-off between scan time and image quality, particularly when generalizing across diverse acquisition settings. We propose GENRE-CMR, a generative adversarial network (GAN)-based architecture employing a residual deep unrolled reconstruction framework to enhance reconstruction fidelity and generalization. The architecture unrolls iterative optimization into a cascade of convolutional subnetworks, enriched with residual connections to enable progressive feature propagation from shallow to deeper stages. To further improve performance, we integrate two loss functions: (1) an Edge-Aware Region (EAR) loss, which guides the network to focus on structurally informative regions and helps prevent common reconstruction blurriness; and (2) a Statistical Distribution Alignment (SDA) loss, which regularizes the feature space across diverse data distributions via a symmetric KL divergence formulation. Extensive experiments confirm that GENRE-CMR surpasses state-of-the-art methods on training and unseen data, achieving 0.9552 SSIM and 38.90 dB PSNR on unseen distributions across various acceleration factors and sampling trajectories. Ablation studies confirm the contribution of each proposed component to reconstruction quality and generalization. Our framework presents a unified and robust solution for high-quality CMR reconstruction, paving the way for clinically adaptable deployment across heterogeneous acquisition protocols.

**Link**: [arxiv](http://arxiv.org/abs/2508.20600v2),  [pdf](http://arxiv.org/pdf/2508.20600v2)

**Tags**: eess.IV cs.CV 



### BambooKG: A Neurobiologically-inspired Frequency-Weight Knowledge Graph
**Authors**: Vanya Arikutharam, Arkadiy Ukolov

**Updated**: 2025-10-29T17:31:27Z

**Summary**: Retrieval-Augmented Generation allows LLMs to access external knowledge, reducing hallucinations and ageing-data issues. However, it treats retrieved chunks independently and struggles with multi-hop or relational reasoning, especially across documents. Knowledge graphs enhance this by capturing the relationships between entities using triplets, enabling structured, multi-chunk reasoning. However, these tend to miss information that fails to conform to the triplet structure. We introduce BambooKG, a knowledge graph with frequency-based weights on non-triplet edges which reflect link strength, drawing on the Hebbian principle of "fire together, wire together". This decreases information loss and results in improved performance on single- and multi-hop reasoning, outperforming the existing solutions.

**Link**: [arxiv](http://arxiv.org/abs/2510.25724v1),  [pdf](http://arxiv.org/pdf/2510.25724v1)

**Tags**: cs.AI 



### Bob's Confetti: Phonetic Memorization Attacks in Music and Video   Generation
**Authors**: Jaechul Roh, Zachary Novack, Yuefeng Peng, Niloofar Mireshghallah, Taylor Berg-Kirkpatrick, Amir Houmansadr

**Updated**: 2025-10-29T17:29:43Z

**Summary**: Generative AI systems for music and video commonly use text-based filters to prevent the regurgitation of copyrighted material. We expose a fundamental flaw in this approach by introducing Adversarial PhoneTic Prompting (APT), a novel attack that bypasses these safeguards by exploiting phonetic memorization. The APT attack replaces iconic lyrics with homophonic but semantically unrelated alternatives (e.g., "mom's spaghetti" becomes "Bob's confetti"), preserving acoustic structure while altering meaning; we identify high-fidelity phonetic matches using CMU pronouncing dictionary. We demonstrate that leading Lyrics-to-Song (L2S) models like SUNO and YuE regenerate songs with striking melodic and rhythmic similarity to their copyrighted originals when prompted with these altered lyrics. More surprisingly, this vulnerability extends across modalities. When prompted with phonetically modified lyrics from a song, a Text-to-Video (T2V) model like Veo 3 reconstructs visual scenes from the original music video-including specific settings and character archetypes-despite the absence of any visual cues in the prompt. Our findings reveal that models memorize deep, structural patterns tied to acoustics, not just verbatim text. This phonetic-to-visual leakage represents a critical vulnerability in transcript-conditioned generative models, rendering simple copyright filters ineffective and raising urgent concerns about the secure deployment of multimodal AI systems. Demo examples are available at our project page (https://jrohsc.github.io/music_attack/).

**Link**: [arxiv](http://arxiv.org/abs/2507.17937v3),  [pdf](http://arxiv.org/pdf/2507.17937v3)

**Tags**: cs.SD cs.AI cs.CL eess.AS 



### LLMs are Better Than You Think: Label-Guided In-Context Learning for   Named Entity Recognition
**Authors**: Fan Bai, Hamid Hassanzadeh, Ardavan Saeedi, Mark Dredze

**Updated**: 2025-10-29T17:27:45Z

**Summary**: In-context learning (ICL) enables large language models (LLMs) to perform new tasks using only a few demonstrations. However, in Named Entity Recognition (NER), existing ICL methods typically rely on task-agnostic semantic similarity for demonstration retrieval, which often yields less relevant examples and leads to inferior results. We introduce DEER, a training-free ICL approach that enables LLMs to make more informed entity predictions through the use of label-grounded statistics. DEER leverages token-level statistics from training labels to identify tokens most informative for entity recognition, enabling entity-focused demonstrations. It further uses these statistics to detect and refine error-prone tokens through a targeted reflection step. Evaluated on five NER datasets across four LLMs, DEER consistently outperforms existing ICL methods and achieves performance comparable to supervised fine-tuning. Further analyses demonstrate that DEER improves example retrieval, remains effective on both seen and unseen entities, and exhibits strong robustness in low-resource settings.

**Link**: [arxiv](http://arxiv.org/abs/2505.23722v2),  [pdf](http://arxiv.org/pdf/2505.23722v2)

**Tags**: cs.CL 



### Spontaneous Giving and Calculated Greed in Language Models
**Authors**: Yuxuan Li, Hirokazu Shirado

**Updated**: 2025-10-29T17:15:43Z

**Summary**: Large language models demonstrate strong problem-solving abilities through reasoning techniques such as chain-of-thought prompting and reflection. However, it remains unclear whether these reasoning capabilities extend to a form of social intelligence: making effective decisions in cooperative contexts. We examine this question using economic games that simulate social dilemmas. First, we apply chain-of-thought and reflection prompting to GPT-4o in a Public Goods Game. We then evaluate multiple off-the-shelf models across six cooperation and punishment games, comparing those with and without explicit reasoning mechanisms. We find that reasoning models consistently reduce cooperation and norm enforcement, favoring individual rationality. In repeated interactions, groups with more reasoning agents exhibit lower collective gains. These behaviors mirror human patterns of "spontaneous giving and calculated greed." Our findings underscore the need for LLM architectures that incorporate social intelligence alongside reasoning, to help address--rather than reinforce--the challenges of collective action.

**Link**: [arxiv](http://arxiv.org/abs/2502.17720v4),  [pdf](http://arxiv.org/pdf/2502.17720v4)

**Tags**: cs.CL cs.AI 



### To Rely or Not to Rely? Evaluating Interventions for Appropriate   Reliance on Large Language Models
**Authors**: Jessica Y. Bo, Sophia Wan, Ashton Anderson

**Updated**: 2025-10-29T17:10:45Z

**Summary**: As Large Language Models become integral to decision-making, optimism about their power is tempered with concern over their errors. Users may over-rely on LLM advice that is confidently stated but wrong, or under-rely due to mistrust. Reliance interventions have been developed to help users of LLMs, but they lack rigorous evaluation for appropriate reliance. We benchmark the performance of three relevant interventions by conducting a randomized online experiment with 400 participants attempting two challenging tasks: LSAT logical reasoning and image-based numerical estimation. For each question, participants first answered independently, then received LLM advice modified by one of three reliance interventions and answered the question again. Our findings indicate that while interventions reduce over-reliance, they generally fail to improve appropriate reliance. Furthermore, people became more confident after making wrong reliance decisions in certain contexts, demonstrating poor calibration. Based on our findings, we discuss implications for designing effective reliance interventions in human-LLM collaboration.

**Link**: [arxiv](http://arxiv.org/abs/2412.15584v3),  [pdf](http://arxiv.org/pdf/2412.15584v3)

**Tags**: cs.HC 



### Precise In-Parameter Concept Erasure in Large Language Models
**Authors**: Yoav Gur-Arieh, Clara Suslik, Yihuai Hong, Fazl Barez, Mor Geva

**Updated**: 2025-10-29T17:09:56Z

**Summary**: Large language models (LLMs) often acquire knowledge during pretraining that is undesirable in downstream deployments, e.g., sensitive information or copyrighted content. Existing approaches for removing such knowledge rely on fine-tuning, training low-rank adapters or fact-level editing, but these are either too coarse, too shallow, or ineffective. In this work, we propose PISCES (Precise In-parameter Suppression for Concept EraSure), a novel framework for precisely erasing entire concepts from model parameters by directly editing directions that encode them in parameter space. PISCES uses a disentangler model to decompose MLP vectors into interpretable features, identifies those associated with a target concept using automated interpretability techniques, and removes them from model parameters. Experiments on Gemma 2 and Llama 3.1 over various concepts show that PISCES achieves modest gains in efficacy over leading erasure methods, reducing accuracy on the target concept to as low as 7.7%, while dramatically improving erasure specificity (by up to 31%) and robustness (by up to 38%). Overall, these results demonstrate that feature-based in-parameter editing enables a more precise and reliable approach for removing conceptual knowledge in language models.

**Link**: [arxiv](http://arxiv.org/abs/2505.22586v2),  [pdf](http://arxiv.org/pdf/2505.22586v2)

**Tags**: cs.CL 



### Interpreting LLMs as Credit Risk Classifiers: Do Their Feature   Explanations Align with Classical ML?
**Authors**: Saeed AlMarri, Kristof Juhasz, Mathieu Ravaut, Gautier Marti, Hamdan Al Ahbabi, Ibrahim Elfadel

**Updated**: 2025-10-29T17:05:00Z

**Summary**: Large Language Models (LLMs) are increasingly explored as flexible alternatives to classical machine learning models for classification tasks through zero-shot prompting. However, their suitability for structured tabular data remains underexplored, especially in high-stakes financial applications such as financial risk assessment. This study conducts a systematic comparison between zero-shot LLM-based classifiers and LightGBM, a state-of-the-art gradient-boosting model, on a real-world loan default prediction task. We evaluate their predictive performance, analyze feature attributions using SHAP, and assess the reliability of LLM-generated self-explanations. While LLMs are able to identify key financial risk indicators, their feature importance rankings diverge notably from LightGBM, and their self-explanations often fail to align with empirical SHAP attributions. These findings highlight the limitations of LLMs as standalone models for structured financial risk prediction and raise concerns about the trustworthiness of their self-generated explanations. Our results underscore the need for explainability audits, baseline comparisons with interpretable models, and human-in-the-loop oversight when deploying LLMs in risk-sensitive financial environments.

**Link**: [arxiv](http://arxiv.org/abs/2510.25701v1),  [pdf](http://arxiv.org/pdf/2510.25701v1)

**Tags**: cs.CL 



### Process-Level Trajectory Evaluation for Environment Configuration in   Software Engineering Agents
**Authors**: Jiayi Kuang, Yinghui Li, Xin Zhang, Yangning Li, Di Yin, Xing Sun, Ying Shen, Philip S. Yu

**Updated**: 2025-10-29T16:59:07Z

**Summary**: Large language model-based agents show promise for software engineering, but environment configuration remains a bottleneck due to heavy manual effort and scarce large-scale, high-quality datasets. Existing benchmarks assess only end-to-end build/test success, obscuring where and why agents succeed or fail. We introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench, which provides process-level trajectory assessment of fine-grained agent capabilities during environment setup-planning, perception-driven error diagnosis, feedback-driven repair, and action to execute final environment configuration. Our task instances are automatically constructed by injecting realistic README errors and are validated in Docker for scalable, high-quality evaluation. Enconda-bench combines process-level analysis with end-to-end executability to enable capability assessments beyond aggregate success rates. Evaluations across state-of-the-art LLMs and agent frameworks show that while agents can localize errors, they struggle to translate feedback into effective corrections, limiting end-to-end performance. To our knowledge, Enconda-bench is the first framework to provide process-level internal capability assessment for environment configuration, offering actionable insights for improving software engineering agents.

**Link**: [arxiv](http://arxiv.org/abs/2510.25694v1),  [pdf](http://arxiv.org/pdf/2510.25694v1)

**Tags**: cs.SE cs.AI cs.CL 



### User Misconceptions of LLM-Based Conversational Programming Assistants
**Authors**: Gabrielle O'Brien, Antonio Pedro Santos Alves, Sebastian Baltes, Grischa Liebel, Mircea Lungu, Marcos Kalinowski

**Updated**: 2025-10-29T16:23:46Z

**Summary**: Programming assistants powered by large language models (LLMs) have become widely available, with conversational assistants like ChatGPT proving particularly accessible to less experienced programmers. However, the varied capabilities of these tools across model versions and the mixed availability of extensions that enable web search, code execution, or retrieval-augmented generation create opportunities for user misconceptions about what systems can and cannot do. Such misconceptions may lead to over-reliance, unproductive practices, or insufficient quality control in LLM-assisted programming. Here, we aim to characterize misconceptions that users of conversational LLM-based assistants may have in programming contexts. Using a two-phase approach, we first brainstorm and catalog user misconceptions that may occur, and then conduct a qualitative analysis to examine whether these conceptual issues surface in naturalistic Python-programming conversations with an LLM-based chatbot drawn from an openly available dataset. Indeed, we see evidence that some users have misplaced expectations about the availability of LLM-based chatbot features like web access, code execution, or non-text output generation. We also see potential evidence for deeper conceptual issues around the scope of information required to debug, validate, and optimize programs. Our findings reinforce the need for designing LLM-based tools that more clearly communicate their programming capabilities to users.

**Link**: [arxiv](http://arxiv.org/abs/2510.25662v1),  [pdf](http://arxiv.org/pdf/2510.25662v1)

**Tags**: cs.HC cs.AI 



### OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement   Learning
**Authors**: Ziyou Hu, Zhengliang Shi, Minghang Zhu, Haitao Li, Teng Sun, Pengjie Ren, Suzan Verberne, Zhaochun Ren

**Updated**: 2025-10-29T16:06:18Z

**Summary**: Reward models (RMs) have become essential for aligning large language models (LLMs), serving as scalable proxies for human evaluation in both training and inference. However, existing RMs struggle on knowledge-intensive and long-form tasks, where evaluating correctness requires grounding beyond the model's internal knowledge. This limitation hinders them from reliably discriminating subtle quality differences, especially when external evidence is necessary. To address this, we introduce OpenRM, a tool-augmented long-form reward model that systematically judges open-ended responses by invoking external tools to gather relevant evidence. We train OpenRM with Group Relative Policy Optimization (GRPO) on over 27K synthesized pairwise examples generated through a controllable data synthesis framework. The training objective jointly supervises intermediate tool usage and final outcome accuracy, incentivizing our reward model to learn effective evidence-based judgment strategies. Extensive experiments on three newly-collected datasets and two widely-used benchmarks demonstrate that OpenRM substantially outperforms existing reward modeling approaches. As a further step, we integrate OpenRM into both inference-time response selection and training-time data selection. This yields consistent gains in downstream LLM alignment tasks, highlighting the potential of tool-augmented reward models for scaling reliable long-form evaluation.

**Link**: [arxiv](http://arxiv.org/abs/2510.24636v2),  [pdf](http://arxiv.org/pdf/2510.24636v2)

**Tags**: cs.CL 



### DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware   Regressive GRPO
**Authors**: Jinyoung Park, Jeehye Na, Jinyoung Kim, Hyunwoo J. Kim

**Updated**: 2025-10-29T15:59:41Z

**Summary**: Recent works have demonstrated the effectiveness of reinforcement learning (RL)-based post-training for enhancing the reasoning capabilities of large language models (LLMs). In particular, Group Relative Policy Optimization (GRPO) has shown impressive success using a PPO-style reinforcement algorithm with group-normalized rewards. However, the effectiveness of GRPO in Video Large Language Models (VideoLLMs) has still been less studyed. In this paper, we explore GRPO and identify two problems that deteriorate the effective learning: (1) reliance on safeguards, and (2) vanishing advantage. To mitigate these challenges, we propose DeepVideo-R1, a video large language model trained with Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation. Reg-GRPO reformulates the GRPO loss function into a regression task that directly predicts the advantage in GRPO, eliminating the need for safeguards such as the clipping and min functions. It directly aligns the model with advantages, providing guidance to prefer better ones. The difficulty-aware data augmentation strategy augments input prompts/videos to locate the difficulty of samples at solvable difficulty levels, enabling diverse reward signals. Our experimental results show that our approach significantly improves video reasoning performance across multiple benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2506.07464v3),  [pdf](http://arxiv.org/pdf/2506.07464v3)

**Tags**: cs.CV cs.AI 



### Exploring the In-Context Learning Capabilities of LLMs for Money   Laundering Detection in Financial Graphs
**Authors**: Erfan Pirmorad

**Updated**: 2025-10-29T15:56:28Z

**Summary**: The complexity and interconnectivity of entities involved in money laundering demand investigative reasoning over graph-structured data. This paper explores the use of large language models (LLMs) as reasoning engines over localized subgraphs extracted from a financial knowledge graph. We propose a lightweight pipeline that retrieves k-hop neighborhoods around entities of interest, serializes them into structured text, and prompts an LLM via few-shot in-context learning to assess suspiciousness and generate justifications. Using synthetic anti-money laundering (AML) scenarios that reflect common laundering behaviors, we show that LLMs can emulate analyst-style logic, highlight red flags, and provide coherent explanations. While this study is exploratory, it illustrates the potential of LLM-based graph reasoning in AML and lays groundwork for explainable, language-driven financial crime analytics.

**Link**: [arxiv](http://arxiv.org/abs/2507.14785v2),  [pdf](http://arxiv.org/pdf/2507.14785v2)

**Tags**: cs.LG cs.AI 



### The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity
**Authors**: Ali Aouad, Aymane El Gadarri, Vivek F. Farias

**Updated**: 2025-10-29T15:51:35Z

**Summary**: Traditional LLM alignment methods are vulnerable to heterogeneity in human preferences. Fitting a na\"ive probabilistic model to pairwise comparison data (say over prompt-completion pairs) yields an inconsistent estimate of the population-average utility -a canonical measure of social welfare. We propose a new method, dubbed the sign estimator, that provides a simple, provably consistent, and efficient estimator by replacing cross-entropy with binary classification loss in the aggregation step. This simple modification recovers consistent ordinal alignment under mild assumptions and achieves the first polynomial finite-sample error bounds in this setting. In realistic simulations of LLM alignment using digital twins, the sign estimator substantially reduces preference distortion over a panel of simulated personas, cutting (angular) estimation error by nearly 35% and decreasing disagreement with true population preferences from 12% to 8% compared to standard RLHF. Our method also compares favorably to panel data heuristics that explicitly model user heterogeneity and require tracking individual-level preference data-all while maintaining the implementation simplicity of existing LLM alignment pipelines.

**Link**: [arxiv](http://arxiv.org/abs/2510.23965v2),  [pdf](http://arxiv.org/pdf/2510.23965v2)

**Tags**: cs.AI cs.LG stat.ML 



### Drone Beam Mapping of the TONE Radio Dish Array
**Authors**: Emily R. Kuhn, Will Tyndall, Benjamin R. B. Saliwanchik, Anna Rose Polish, Maile Harris, Laura B. Newburgh

**Updated**: 2025-10-29T15:50:30Z

**Summary**: Drone-based beam measurements are a promising avenue to tackle the critical challenge of calibration for 21 cm cosmology telescopes. In this paper, we introduce a new drone-based calibration system for 400-800 MHz radio observatories, describing its instrumentation and first deployment. We discuss measurements of the TONE array, a CHIME/FRB outrigger pathfinder, and present results, including full 2D high spatial resolution beam maps in both co- and cross-polarization, as well as comparisons to simulations. The polarized beam maps cover a 70 degree by 70 degree grid, capturing the first two sidelobes and measuring the TONE main beam and first sidelobe with 7-9% statistical errors. We investigate polarization angle alignment with frequency, finding significant polarization leakage in the TONE antennas at frequencies above 600 MHz, and a polarization axis rotation with frequency. We describe statistical and systematic errors, as well as measurements of radio frequency interference from the drone and equipment. Our drone system is the first to incorporate a broad-band switched calibration source in the drone payload, enabling background subtraction and direct measurements of the RFI emitted by the drone. The results presented are the first drone-based 2D measurements of cross-polar beam structure and of polarization alignment of an array. The high frequency and spatial resolution achieved with this system have revealed the rich structure of the beam of each antenna, and enabled comparisons between individual dishes and to electromagnetic simulations.

**Link**: [arxiv](http://arxiv.org/abs/2502.03759v2),  [pdf](http://arxiv.org/pdf/2502.03759v2)

**Tags**: astro-ph.IM astro-ph.CO 



### EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic   Health Record Analysis
**Authors**: Yusheng Liao, Chaoyi Wu, Junwei Liu, Shuyang Jiang, Pengcheng Qiu, Haowen Wang, Yun Yue, Shuai Zhen, Jian Wang, Qianrui Fan, Jinjie Gu, Ya Zhang, Yanfeng Wang, Yu Wang, Weidi Xie

**Updated**: 2025-10-29T15:32:47Z

**Summary**: Electronic Health Records (EHRs) contain rich yet complex information, and their automated analysis is critical for clinical decision-making. Despite recent advances of large language models (LLMs) in clinical workflows, their ability to analyze EHRs remains limited due to narrow task coverage and lack of EHR-oriented reasoning capabilities. This paper aims to bridge the gap, specifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning instruction dataset, comprising 300k high-quality reasoning cases and 4M non-reasoning cases across 42 distinct EHR tasks. Its core innovation is a thinking-graph-driven framework that enables to generate high-quality reasoning data at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced LLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage training paradigm, including domain adaptation, reasoning enhancement, and reinforcement learning, EHR-R1 systematically acquires domain knowledge and diverse reasoning capabilities, enabling accurate and robust EHR analysis. Lastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning 42 tasks, to comprehensively assess reasoning and prediction across EHR scenarios. In experiments, we show that the resulting EHR-R1 consistently outperforms state-of-the-art commercial and open-source LLMs (including DeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and achieving a 10\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins, EHR-R1, and EHR-Bench have significantly advanced the development for more reliable and clinically relevant EHR analysis.

**Link**: [arxiv](http://arxiv.org/abs/2510.25628v1),  [pdf](http://arxiv.org/pdf/2510.25628v1)

**Tags**: cs.CL 



### Evaluating the Role of Verifiers in Test-Time Scaling for Legal   Reasoning Tasks
**Authors**: Davide Romano, Jonathan Schwarz, Daniele GiofrÃ©

**Updated**: 2025-10-29T15:27:47Z

**Summary**: Test-time scaling (TTS) techniques can improve the performance of large language models (LLMs) at the expense of additional computation and latency. While TTS has proven effective in formal domains such as mathematics and programming \citep{snell2024scaling, chen2024more}, its value in argumentative domains such as law remains underexplored. We present an empirical study of verifier-based TTS methods for legal multiple-choice QA (MCQA) across five benchmarks. Using a family of 7 reward models, we evaluate both outcome-level (Best-of-$N$) and process-level (tree search) verification under realistic low-$N$ budgets. Our analysis systematically investigates how verifier utility is affected by key properties such as domain specialization, model size, and supervision type (process-supervised PRMs vs. outcome-only ORMs), even when applied across different roles.

**Link**: [arxiv](http://arxiv.org/abs/2510.25623v1),  [pdf](http://arxiv.org/pdf/2510.25623v1)

**Tags**: cs.CL 



### Towards a Common Framework for Autoformalization
**Authors**: Agnieszka Mensfelt, David Tena Cucala, Santiago Franco, Angeliki Koutsoukou-Argyraki, Vince Trencsenyi, Kostas Stathis

**Updated**: 2025-10-29T15:27:12Z

**Summary**: Autoformalization has emerged as a term referring to the automation of formalization - specifically, the formalization of mathematics using interactive theorem provers (proof assistants). Its rapid development has been driven by progress in deep learning, especially large language models (LLMs). More recently, the term has expanded beyond mathematics to describe the broader task of translating informal input into formal logical representations. At the same time, a growing body of research explores using LLMs to translate informal language into formal representations for reasoning, planning, and knowledge representation - often without explicitly referring to this process as autoformalization. As a result, despite addressing similar tasks, the largely independent development of these research areas has limited opportunities for shared methodologies, benchmarks, and theoretical frameworks that could accelerate progress. The goal of this paper is to review - explicit or implicit - instances of what can be considered autoformalization and to propose a unified framework, encouraging cross-pollination between different fields to advance the development of next generation AI systems.

**Link**: [arxiv](http://arxiv.org/abs/2509.09810v2),  [pdf](http://arxiv.org/pdf/2509.09810v2)

**Tags**: cs.AI 



### FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering
**Authors**: Mohammad Aghajani Asl, Behrooz Minaei Bidgoli

**Updated**: 2025-10-29T15:25:34Z

**Summary**: The advent of Large Language Models (LLMs) has revolutionized Natural Language Processing, yet their application in high-stakes, specialized domains like religious question answering is hindered by challenges like hallucination and unfaithfulness to authoritative sources. This issue is particularly critical for the Persian-speaking Muslim community, where accuracy and trustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG) systems, relying on simplistic single-pass pipelines, fall short on complex, multi-hop queries requiring multi-step reasoning and evidence aggregation. To address this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful Advanced Question Answering in the Persian Islamic domain. FARSIQA is built upon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative Refinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting process: it adaptively decomposes complex queries, assesses evidence sufficiency, and enters an iterative loop to generate sub-queries, progressively filling information gaps. Operating on a curated knowledge base of over one million authoritative Islamic documents, FARSIQA demonstrates superior performance. Rigorous evaluation on the challenging IslamicPCQA benchmark shows state-of-the-art performance: the system achieves a remarkable 97.0% in Negative Rejection - a 40-point improvement over baselines - and a high Answer Correctness score of 74.3%. Our work establishes a new standard for Persian Islamic QA and validates that our iterative, adaptive architecture is crucial for building faithful, reliable AI systems in sensitive domains.

**Link**: [arxiv](http://arxiv.org/abs/2510.25621v1),  [pdf](http://arxiv.org/pdf/2510.25621v1)

**Tags**: cs.CL cs.AI cs.IR 68T50, 68T05, 68T30 I.2.7; H.3.3 



### Counterfactual-based Agent Influence Ranker for Agentic AI Workflows
**Authors**: Amit Giloni, Chiara Picardi, Roy Betser, Shamik Bose, Aishvariya Priya Rathina Sabapathy, Roman Vainshtein

**Updated**: 2025-10-29T15:17:31Z

**Summary**: An Agentic AI Workflow (AAW), also known as an LLM-based multi-agent system, is an autonomous system that assembles several LLM-based agents to work collaboratively towards a shared goal. The high autonomy, widespread adoption, and growing interest in such AAWs highlight the need for a deeper understanding of their operations, from both quality and security aspects. To this day, there are no existing methods to assess the influence of each agent on the AAW's final output. Adopting techniques from related fields is not feasible since existing methods perform only static structural analysis, which is unsuitable for inference time execution. We present Counterfactual-based Agent Influence Ranker (CAIR) - the first method for assessing the influence level of each agent on the AAW's output and determining which agents are the most influential. By performing counterfactual analysis, CAIR provides a task-agnostic analysis that can be used both offline and at inference time. We evaluate CAIR using an AAWs dataset of our creation, containing 30 different use cases with 230 different functionalities. Our evaluation showed that CAIR produces consistent rankings, outperforms baseline methods, and can easily enhance the effectiveness and relevancy of downstream tasks.

**Link**: [arxiv](http://arxiv.org/abs/2510.25612v1),  [pdf](http://arxiv.org/pdf/2510.25612v1)

**Tags**: cs.AI cs.MA 



### Predicting Abandonment of Open Source Software Projects with An   Integrated Feature Framework
**Authors**: Yiming Xu, Runzhi He, Hengzhi Ye, Minghui Zhou, Huaimin Wang

**Updated**: 2025-10-29T15:15:46Z

**Summary**: Open Source Software (OSS) is a cornerstone of contemporary software development, yet the increasing prevalence of OSS project abandonment threatens global software supply chains. Although previous research has explored abandonment prediction methods, these methods often demonstrate unsatisfactory predictive performance, further plagued by imprecise abandonment discrimination, limited interpretability, and a lack of large, generalizable datasets. In this work, we address these challenges by reliably detecting OSS project abandonment through a dual approach: explicit archival status and rigorous semantic analysis of project documentation or description. Leveraging a precise and scalable labeling pipeline, we curate a comprehensive longitudinal dataset of 115,466 GitHub repositories, encompassing 57,733 confirmed abandonment repositories, enriched with detailed, timeline-based behavioral features. Building on this foundation, we introduce an integrated, multi-perspective feature framework for abandonment prediction, capturing user-centric, maintainer-centric, and project evolution features. Survival analysis using an AFT model yields a high C-index of 0.846, substantially outperforming models confined to surface features. Further, feature ablation and SHAP analyses confirm both the predictive power and interpretability of our approach. We further demonstrate practical deployment of a GBSA classifier for package risk in openEuler. By unifying precise labeling, multi-perspective features, and interpretable modeling, our work provides reproducible, scalable, and practitioner-oriented support for understanding and managing abandonment risk in large OSS ecosystems. Our tool not only predicts abandonment but also enhances program comprehension by providing actionable insights into the health and sustainability of OSS projects.

**Link**: [arxiv](http://arxiv.org/abs/2507.21678v2),  [pdf](http://arxiv.org/pdf/2507.21678v2)

**Tags**: cs.SE 



### INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization   Formats
**Authors**: Mengzhao Chen, Meng Wu, Hui Jin, Zhihang Yuan, Jing Liu, Chaoyi Zhang, Yunshui Li, Jie Huang, Jin Ma, Zeyue Xue, Zhiheng Liu, Xingyan Bin, Ping Luo

**Updated**: 2025-10-29T15:11:53Z

**Summary**: Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly embracing low-precision floating-point (FP) formats to handle the pervasive activation outliers in Large Language Models (LLMs). Despite this industry trend, a unified comparison of FP and integer (INT) quantization across varying granularities has been missing, leaving algorithm and hardware co-design without clear guidance. This paper fills that gap by systematically investigating the trade-offs between FP and INT formats. We reveal a critical performance crossover: while FP excels in coarse-grained quantization, the comparison at fine-grained (block-wise) levels is more nuanced. Our comprehensive comparison demonstrates that for popular 8-bit fine-grained formats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart in both algorithmic accuracy and hardware efficiency. However, for 4-bit formats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we show that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like Hadamard rotation are applied. We also introduce a symmetric clipping method that resolves gradient bias in fine-grained low-bit INT training, enabling nearly lossless performance for MXINT8 training. These findings challenge the current hardware trajectory, demonstrating that a one-size-fits-all FP approach is suboptimal and advocating that fine-grained INT formats, particularly MXINT8, offer a better balance of accuracy, power, and efficiency for future AI accelerators.

**Link**: [arxiv](http://arxiv.org/abs/2510.25602v1),  [pdf](http://arxiv.org/pdf/2510.25602v1)

**Tags**: cs.LG cs.AI 



### Communication and Verification in LLM Agents towards Collaboration under   Information Asymmetry
**Authors**: Run Peng, Ziqiao Ma, Amy Pang, Sikai Li, Zhang Xi-Jia, Yingzhuo Yu, Cristian-Paul Bara, Joyce Chai

**Updated**: 2025-10-29T15:03:53Z

**Summary**: While Large Language Model (LLM) agents are often approached from the angle of action planning/generation to accomplish a goal (e.g., given by language descriptions), their abilities to collaborate with each other to achieve a joint goal are not well explored. To address this limitation, this paper studies LLM agents in task collaboration, particularly under the condition of information asymmetry, where agents have disparities in their knowledge and skills and need to work together to complete a shared task. We extend Einstein Puzzles, a classical symbolic puzzle, to a table-top game. In this game, two LLM agents must reason, communicate, and act to satisfy spatial and relational constraints required to solve the puzzle. We apply a fine-tuning-plus-verifier framework in which LLM agents are equipped with various communication strategies and verification signals from the environment. Empirical results highlight the critical importance of aligned communication, especially when agents possess both information-seeking and -providing capabilities. Interestingly, agents without communication can still achieve high task performance; however, further analysis reveals a lack of true rule understanding and lower trust from human evaluators. Instead, by integrating an environment-based verifier, we enhance agents' ability to comprehend task rules and complete tasks, promoting both safer and more interpretable collaboration in AI systems. https://github.com/Roihn/EinsteinPuzzles

**Link**: [arxiv](http://arxiv.org/abs/2510.25595v1),  [pdf](http://arxiv.org/pdf/2510.25595v1)

**Tags**: cs.CL cs.AI 



### HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based   Sequential Recommendation
**Authors**: Jingyi Zhou, Cheng Chen, Kai Zuo, Manjie Xu, Zhendong Fu, Yibo Chen, Xu Tang, Yao Hu

**Updated**: 2025-10-29T15:00:42Z

**Summary**: Large language models (LLMs) have recently demonstrated strong potential for sequential recommendation. However, current LLM-based approaches face critical limitations in modeling users' long-term and diverse interests. First, due to inference latency and feature fetching bandwidth constraints, existing methods typically truncate user behavior sequences to include only the most recent interactions, resulting in the loss of valuable long-range preference signals. Second, most current methods rely on next-item prediction with a single predicted embedding, overlooking the multifaceted nature of user interests and limiting recommendation diversity. To address these challenges, we propose HyMiRec, a hybrid multi-interest sequential recommendation framework, which leverages a lightweight recommender to extracts coarse interest embeddings from long user sequences and an LLM-based recommender to captures refined interest embeddings. To alleviate the overhead of fetching features, we introduce a residual codebook based on cosine similarity, enabling efficient compression and reuse of user history embeddings. To model the diverse preferences of users, we design a disentangled multi-interest learning module, which leverages multiple interest queries to learn disentangles multiple interest signals adaptively, allowing the model to capture different facets of user intent. Extensive experiments are conducted on both benchmark datasets and a collected industrial dataset, demonstrating our effectiveness over existing state-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec brings consistent improvements in real-world recommendation systems. Code is available at https://github.com/FireRedTeam/FireRedSeqRec.

**Link**: [arxiv](http://arxiv.org/abs/2510.13738v2),  [pdf](http://arxiv.org/pdf/2510.13738v2)

**Tags**: cs.IR 



### Standardization of Psychiatric Diagnoses -- Role of Fine-tuned LLM   Consortium and OpenAI-gpt-oss Reasoning LLM Enabled Decision Support System
**Authors**: Eranga Bandara, Ross Gore, Atmaram Yarlagadda, Anita H. Clayton, Preston Samuel, Christopher K. Rhea, Sachin Shetty

**Updated**: 2025-10-29T14:54:22Z

**Summary**: The diagnosis of most mental disorders, including psychiatric evaluations, primarily depends on dialogues between psychiatrists and patients. This subjective process can lead to variability in diagnoses across clinicians and patients, resulting in inconsistencies and challenges in achieving reliable outcomes. To address these issues and standardize psychiatric diagnoses, we propose a Fine-Tuned Large Language Model (LLM) Consortium and OpenAI-gpt-oss Reasoning LLM-enabled Decision Support System for the clinical diagnosis of mental disorders. Our approach leverages fine-tuned LLMs trained on conversational datasets involving psychiatrist-patient interactions focused on mental health conditions (e.g., depression). The diagnostic predictions from individual models are aggregated through a consensus-based decision-making process, refined by the OpenAI-gpt-oss reasoning LLM. We propose a novel method for deploying LLM agents that orchestrate communication between the LLM consortium and the reasoning LLM, ensuring transparency, reliability, and responsible AI across the entire diagnostic workflow. Experimental results demonstrate the transformative potential of combining fine-tuned LLMs with a reasoning model to create a robust and highly accurate diagnostic system for mental health assessment. A prototype of the proposed platform, integrating three fine-tuned LLMs with the OpenAI-gpt-oss reasoning LLM, was developed in collaboration with the U.S. Army Medical Research Team in Norfolk, Virginia, USA. To the best of our knowledge, this work represents the first application of a fine-tuned LLM consortium integrated with a reasoning LLM for clinical mental health diagnosis paving the way for next-generation AI-powered eHealth systems aimed at standardizing psychiatric diagnoses.

**Link**: [arxiv](http://arxiv.org/abs/2510.25588v1),  [pdf](http://arxiv.org/pdf/2510.25588v1)

**Tags**: cs.AI 



### Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption   Masking And Normalization
**Authors**: Filip Sondej, Yushi Yang, MikoÅaj Kniejski, Marcel Windys

**Updated**: 2025-10-29T14:52:49Z

**Summary**: Language models can retain dangerous knowledge and skills even after extensive safety fine-tuning, posing both misuse and misalignment risks. Recent studies show that even specialized unlearning methods can be easily reversed. To address this, we systematically evaluate many existing and novel components of unlearning methods and identify ones crucial for irreversible unlearning.   We introduce Disruption Masking, a technique in which we only allow updating weights, where the signs of the unlearning gradient and the retaining gradient are the same. This ensures all updates are non-disruptive.   Additionally, we identify the need for normalizing the unlearning gradients, and also confirm the usefulness of meta-learning. We combine these insights into MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and validate its effectiveness at preventing the recovery of dangerous capabilities. MUDMAN outperforms the prior TAR method by 40%, setting a new state-of-the-art for robust unlearning.

**Link**: [arxiv](http://arxiv.org/abs/2506.12484v4),  [pdf](http://arxiv.org/pdf/2506.12484v4)

**Tags**: cs.LG cs.AI cs.CL 



### Quantum Transformer: Accelerating model inference via quantum linear   algebra
**Authors**: Naixu Guo, Zhan Yu, Matthew Choi, Yizhan Han, Aman Agrawal, Kouhei Nakaji, AlÃ¡n Aspuru-Guzik, Patrick Rebentrost

**Updated**: 2025-10-29T14:48:21Z

**Summary**: Powerful generative artificial intelligence from large language models (LLMs) harnesses extensive computational resources for inference. In this work, we investigate the transformer architecture, a key component of these models, under the lens of fault-tolerant quantum computing. We develop quantum subroutines to construct the building blocks in the transformer, including the self-attention, residual connection with layer normalization, and feed-forward network. As an important subroutine, we show how to efficiently implement the Hadamard product and element-wise functions of matrices on quantum computers. Our algorithm prepares an amplitude encoding of the transformer output, which can be measured for prediction or use in the next layer. We find that the matrix norm of the input sequence plays a dominant role in the quantum complexity. With numerical experiments on open-source LLMs, including for bio-informatics applications, we demonstrate the potential of a quantum speedup for transformer inference in practical regimes.

**Link**: [arxiv](http://arxiv.org/abs/2402.16714v3),  [pdf](http://arxiv.org/pdf/2402.16714v3)

**Tags**: quant-ph cs.AI cs.CL 



### Who You Are Matters: Bridging Topics and Social Roles via LLM-Enhanced   Logical Recommendation
**Authors**: Qing Yu, Xiaobei Wang, Shuchang Liu, Yandong Bai, Xiaoyu Yang, Xueliang Wang, Chang Meng, Shanshan Wu, Hailan Yang, Huihui Xiao, Xiang Li, Fan Yang, Xiaoqiang Feng, Lantao Hu, Han Li, Kun Gai, Lixin Zou

**Updated**: 2025-10-29T14:42:46Z

**Summary**: Recommender systems filter contents/items valuable to users by inferring preferences from user features and historical behaviors. Mainstream approaches follow the learning-to-rank paradigm, which focus on discovering and modeling item topics (e.g., categories), and capturing user preferences on these topics based on historical interactions. However, this paradigm often neglects the modeling of user characteristics and their social roles, which are logical confounders influencing the correlated interest and user preference transition. To bridge this gap, we introduce the user role identification task and the behavioral logic modeling task that aim to explicitly model user roles and learn the logical relations between item topics and user social roles. We show that it is possible to explicitly solve these tasks through an efficient integration framework of Large Language Model (LLM) and recommendation systems, for which we propose TagCF. On the one hand, TagCF exploits the (Multi-modal) LLM's world knowledge and logic inference ability to extract realistic tag-based virtual logic graphs that reveal dynamic and expressive knowledge of users, refining our understanding of user behaviors. On the other hand, TagCF presents empirically effective integration modules that take advantage of the extracted tag-logic information, augmenting the recommendation performance. We conduct both online experiments and offline experiments with industrial and public datasets as verification of TagCF's effectiveness, and we empirically show that the user role modeling strategy is potentially a better choice than the modeling of item topics. Additionally, we provide evidence that the extracted logic graphs are empirically a general and transferable knowledge that can benefit a wide range of recommendation tasks. Our code is available in https://github.com/Code2Q/TagCF.

**Link**: [arxiv](http://arxiv.org/abs/2505.10940v3),  [pdf](http://arxiv.org/pdf/2505.10940v3)

**Tags**: cs.IR cs.AI 



### The AI_INFN Platform: Artificial Intelligence Development in the Cloud
**Authors**: Lucio Anderlini, Giulio Bianchini, Diego Ciangottini, Stefano Dal Pra, Diego Michelotto, Rosa Petrini, Daniele Spiga

**Updated**: 2025-10-29T14:33:07Z

**Summary**: Machine Learning (ML) is profoundly reshaping the way researchers create, implement, and operate data-intensive software. Its adoption, however, introduces notable challenges for computing infrastructures, particularly when it comes to coordinating access to hardware accelerators across development, testing, and production environments. The INFN initiative AI_INFN (Artificial Intelligence at INFN) seeks to promote the use of ML methods across various INFN research scenarios by offering comprehensive technical support, including access to AI-focused computational resources. Leveraging the INFN Cloud ecosystem and cloud-native technologies, the project emphasizes efficient sharing of accelerator hardware while maintaining the breadth of the Institute's research activities. This contribution describes the deployment and commissioning of a Kubernetes-based platform designed to simplify GPU-powered data analysis workflows and enable their scalable execution on heterogeneous distributed resources. By integrating offloading mechanisms through Virtual Kubelet and the InterLink API, the platform allows workflows to span multiple resource providers, from Worldwide LHC Computing Grid sites to high-performance computing centers like CINECA Leonardo. We will present preliminary benchmarks, functional tests, and case studies, demonstrating both performance and integration outcomes.

**Link**: [arxiv](http://arxiv.org/abs/2509.22117v2),  [pdf](http://arxiv.org/pdf/2509.22117v2)

**Tags**: cs.DC cs.AI 



### Collab-REC: An LLM-based Agentic Framework for Balancing Recommendations   in Tourism
**Authors**: Ashmi Banerjee, Fitri Nur Aisyah, Adithi Satish, Wolfgang WÃ¶rndl, Yashar Deldjoo

**Updated**: 2025-10-29T14:31:38Z

**Summary**: We propose Collab-REC, a multi-agent framework designed to counteract popularity bias and enhance diversity in tourism recommendations. In our setting, three LLM-based agents -- Personalization, Popularity, and Sustainability generate city suggestions from complementary perspectives. A non-LLM moderator then merges and refines these proposals via multi-round negotiation, ensuring each agent's viewpoint is incorporated while penalizing spurious or repeated responses. Experiments on European city queries show that Collab-REC improves diversity and overall relevance compared to a single-agent baseline, surfacing lesser-visited locales that often remain overlooked. This balanced, context-aware approach addresses over-tourism and better aligns with constraints provided by the user, highlighting the promise of multi-stakeholder collaboration in LLM-driven recommender systems.

**Link**: [arxiv](http://arxiv.org/abs/2508.15030v2),  [pdf](http://arxiv.org/pdf/2508.15030v2)

**Tags**: cs.AI 



### GradeSQL: Test-Time Inference with Outcome Reward Models for Text-to-SQL   Generation from Large Language Models
**Authors**: Mattia Tritto, Giuseppe Farano, Dario Di Palma, Gaetano Rossiello, Fedelucio Narducci, Dharmashankar Subramanian, Tommaso Di Noia

**Updated**: 2025-10-29T14:09:33Z

**Summary**: Text-to-SQL, the task of translating natural language questions into SQL queries, has significantly advanced with the introduction of Large Language Models (LLMs), broadening database accessibility for a wide range of users. Despite substantial progress in generating valid SQL, current LLMs still struggle with complex queries. To address this limitation, test-time strategies such as Best-of-N (BoN) and Majority Voting (Maj) are often employed, based on the assumption that LLMs can produce correct answers after multiple attempts. However, these methods rely on surface-level heuristics, selecting the syntactically correct query through execution-based BoN (ex-BoN) or the most frequently generated one through Majority Voting. Recently, Outcome Reward Models (ORMs), which assign utility scores to generated outputs based on semantic correctness, have emerged as a promising reinforcement learning approach for improving model alignment. We argue that ORMs could serve as an effective new test-time heuristic, although their application in this context remains largely underexplored.   In this work, we propose a unified framework for training ORMs tailored to the Text-to-SQL task and assess their effectiveness as a test-time heuristic within the BoN strategy. We benchmark ORMs against ex-BoN and Maj across the BIRD and Spider datasets, fine-tuning diverse open-source LLMs from the Qwen2, Granite3, and Llama3 families. Results show that ORMs outperform ex-BoN and Maj, achieving execution accuracy gains of +4.33% (BIRD) and +2.10% (Spider) over ex-BoN, and +2.91% (BIRD) and +0.93% (Spider) over Maj. We further demonstrate that finetuning models already aligned with SQL generation, such as OmniSQL, yields superior ORM performance. Additionally, we observe that ORMs achieve competitive results on simple queries and benefit more from an increased number of candidates compared to ex-BoN and Maj.

**Link**: [arxiv](http://arxiv.org/abs/2509.01308v2),  [pdf](http://arxiv.org/pdf/2509.01308v2)

**Tags**: cs.AI cs.CL cs.DB 



### TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM   Persona Simulation
**Authors**: Bangde Du, Minghao Guo, Songming He, Ziyi Ye, Xi Zhu, Weihang Su, Shuqi Zhu, Yujia Zhou, Yongfeng Zhang, Qingyao Ai, Yiqun Liu

**Updated**: 2025-10-30T11:19:24Z

**Summary**: Large Language Models (LLMs) are exhibiting emergent human-like abilities and are increasingly envisioned as the foundation for simulating an individual's communication style, behavioral tendencies, and personality traits. However, current evaluations of LLM-based persona simulation remain limited: most rely on synthetic dialogues, lack systematic frameworks, and lack analysis of the capability requirement. To address these limitations, we introduce TwinVoice, a comprehensive benchmark for assessing persona simulation across diverse real-world contexts. TwinVoice encompasses three dimensions: Social Persona (public social interactions), Interpersonal Persona (private dialogues), and Narrative Persona (role-based expression). It further decomposes the evaluation of LLM performance into six fundamental capabilities, including opinion consistency, memory recall, logical reasoning, lexical fidelity, persona tone, and syntactic style. Experimental results reveal that while advanced models achieve moderate accuracy in persona simulation, they still fall short of capabilities such as syntactic style and memory recall. Consequently, the average performance achieved by LLMs remains considerably below the human baseline.

**Link**: [arxiv](http://arxiv.org/abs/2510.25536v2),  [pdf](http://arxiv.org/pdf/2510.25536v2)

**Tags**: cs.CL I.2.7; I.2.6; I.2.0 



### Zero Reinforcement Learning Towards General Domains
**Authors**: Yuyuan Zeng, Yufei Huang, Can Xu, Qingfeng Sun, Jianfeng Yan, Guanghui Xu, Tao Yang, Fengzong Lian

**Updated**: 2025-10-29T13:52:44Z

**Summary**: Zero Reinforcement Learning (Zero-RL) has proven to be an effective approach for enhancing the reasoning capabilities of large language models (LLMs) by directly applying reinforcement learning with verifiable rewards on pretrained models, without the need for a supervised fine-tuning phase. However, current research on zero-RL primarily focuses on domains with easily verifiable reward signals, such as mathematics, programming, and other reasoning tasks. The challenge of eliciting reasoning abilities in more diverse scenarios, where verification is not straightforward, remains underexplored. To address this gap, we propose a novel zero-RL paradigm designed to improve a model's reasoning ability across both verifiable and non-verifiable domains. By combining verifiable rewards with a generative reward model, we conduct multi-task zero-RL training across both domains, facilitating the transfer of reasoning capabilities between them. Furthermore, to mitigate reward hacking in the generative reward model, we design a smooth length penalty that encourages the generation of more comprehensive thinking tokens in general domains. Experimental results on Qwen3-8B-Base and Qwen3-14B-Base demonstrate that our approach achieves superior reasoning performance, not only on tasks requiring extensive reasoning but also on more general tasks.

**Link**: [arxiv](http://arxiv.org/abs/2510.25528v1),  [pdf](http://arxiv.org/pdf/2510.25528v1)

**Tags**: cs.AI 



### Meta-Learning Neural Process for Implied Volatility Surfaces with   SABR-induced Priors
**Authors**: Jirong Zhuang, Xuan Wu

**Updated**: 2025-10-29T13:44:51Z

**Summary**: We treat implied volatility surface (IVS) reconstruction as a learning problem guided by two principles. First, we adopt a meta-learning view that trains across trading days to learn a procedure that maps sparse option quotes to a full IVS via conditional prediction, avoiding per-day calibration at test time. Second, we impose a structural prior via transfer learning: pre-train on SABR-generated dataset to encode geometric prior, then fine-tune on historical market dataset to align with empirical patterns. We implement both principles in a single attention-based Neural Process (Volatility Neural Process, VolNP) that produces a complete IVS from a sparse context set in one forward pass. On SPX options, the VolNP outperforms SABR, SSVI, and Gaussian process. Relative to an ablation trained only on market data, the SABR-induced prior reduces RMSE by about 40% and suppresses large errors, with pronounced gains at long maturities where quotes are sparse. The resulting model is fast (single pass), stable (no daily recalibration), and practical for deployment at scale.

**Link**: [arxiv](http://arxiv.org/abs/2509.11928v3),  [pdf](http://arxiv.org/pdf/2509.11928v3)

**Tags**: q-fin.CP 



### Predicate Renaming via Large Language Models
**Authors**: Elisabetta Gentili, Tony Ribeiro, Fabrizio Riguzzi, Katsumi Inoue

**Updated**: 2025-10-29T13:39:41Z

**Summary**: In this paper, we address the problem of giving names to predicates in logic rules using Large Language Models (LLMs). In the context of Inductive Logic Programming, various rule generation methods produce rules containing unnamed predicates, with Predicate Invention being a key example. This hinders the readability, interpretability, and reusability of the logic theory. Leveraging recent advancements in LLMs development, we explore their ability to process natural language and code to provide semantically meaningful suggestions for giving a name to unnamed predicates. The evaluation of our approach on some hand-crafted logic rules indicates that LLMs hold potential for this task.

**Link**: [arxiv](http://arxiv.org/abs/2510.25517v1),  [pdf](http://arxiv.org/pdf/2510.25517v1)

**Tags**: cs.AI 



### Many LLMs Are More Utilitarian Than One
**Authors**: Anita Keshmirian, Razan Baltaji, Babak Hemmatian, Hadi Asghari, Lav R. Varshney

**Updated**: 2025-10-29T13:37:41Z

**Summary**: Moral judgment is integral to large language models' (LLMs) social reasoning. As multi-agent systems gain prominence, it becomes crucial to understand how LLMs function when collaborating compared to operating as individual agents. In human moral judgment, group deliberation leads to a Utilitarian Boost: a tendency to endorse norm violations that inflict harm but maximize benefits for the greatest number of people. We study whether a similar dynamic emerges in multi-agent LLM systems. We test six models on well-established sets of moral dilemmas across two conditions: (1) Solo, where models reason independently, and (2) Group, where they engage in multi-turn discussions in pairs or triads. In personal dilemmas, where agents decide whether to directly harm an individual for the benefit of others, all models rated moral violations as more acceptable when part of a group, demonstrating a Utilitarian Boost similar to that observed in humans. However, the mechanism for the Boost in LLMs differed: While humans in groups become more utilitarian due to heightened sensitivity to decision outcomes, LLM groups showed either reduced sensitivity to norms or enhanced impartiality. We report model differences in when and how strongly the Boost manifests. We also discuss prompt and agent compositions that enhance or mitigate the effect. We end with a discussion of the implications for AI alignment, multi-agent design, and artificial moral reasoning. Code available at: https://github.com/baltaci-r/MoralAgents

**Link**: [arxiv](http://arxiv.org/abs/2507.00814v2),  [pdf](http://arxiv.org/pdf/2507.00814v2)

**Tags**: cs.CL cs.AI cs.CY I.2.7; I.2.11 



### RoboOmni: Proactive Robot Manipulation in Omni-modal Context
**Authors**: Siyin Wang, Jinlan Fu, Feihong Liu, Xinzhe He, Huangxuan Wu, Junhao Shi, Kexin Huang, Zhaoye Fei, Jingjing Gong, Zuxuan Wu, Yugang Jiang, See-Kiong Ng, Tat-Seng Chua, Xipeng Qiu

**Updated**: 2025-10-29T13:37:19Z

**Summary**: Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid progress in Vision-Language-Action (VLA) models for robotic manipulation. Although effective in many scenarios, current approaches largely rely on explicit instructions, whereas in real-world interactions, humans rarely issue instructions directly. Effective collaboration requires robots to infer user intentions proactively. In this work, we introduce cross-modal contextual instructions, a new setting where intent is derived from spoken dialogue, environmental sounds, and visual cues rather than explicit commands. To address this new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor framework based on end-to-end omni-modal LLMs that unifies intention recognition, interaction confirmation, and action execution. RoboOmni fuses auditory and visual signals spatiotemporally for robust intention recognition, while supporting direct speech interaction. To address the absence of training data for proactive intention recognition in robotic manipulation, we build OmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640 backgrounds, and six contextual instruction types. Experiments in simulation and real-world settings show that RoboOmni surpasses text- and ASR-based baselines in success rate, inference speed, intention recognition, and proactive assistance.

**Link**: [arxiv](http://arxiv.org/abs/2510.23763v2),  [pdf](http://arxiv.org/pdf/2510.23763v2)

**Tags**: cs.RO cs.CL cs.CV 



### MTIR-SQL: Multi-turn Tool-Integrated Reasoning Reinforcement Learning   for Text-to-SQL
**Authors**: Zekun Xu, Siyu Xia, Chuhuai Yue, Jiajun Chai, Mingxue Tian, Xiaohan Wang, Wei Lin, Haoxuan Li, Guojun Yin

**Updated**: 2025-10-29T13:34:27Z

**Summary**: As large language models (LLMs) are increasingly used in Text-to-SQL tasks, Reinforcement Learning (RL) has become a common method for improving performance. Existing methods primarily rely on static execution feedback, which restricts real-time error correction. However, integrating multi-turn tool invocation along with dynamic feedback could significantly improve adaptability and robustness, ultimately enhancing model performance. To address these issues, we propose MTIR-SQL, an innovative Multi-turn Tool-Integrated Reasoning reinforcement learning framework for Text-to-SQL. Our approach introduces an execution-aware multi-turn reasoning paradigm that seamlessly incorporates database execution feedback at each reasoning step, enabling context-sensitive query generation and progressive refinement throughout the reasoning process. The framework extends the GRPO algorithm to accommodate complex multi-turn interaction scenarios. Considering the training instability characteristics of MTIR and the potential for significant Deviation of model distribution from the initial model, we enhance the GRPO algorithm by adding a trajectory filtering mechanism and removing KL loss constraints. Experimental results demonstrate that MTIR-SQL, with 4B parameters, achieves \textbf{64.4}\% accuracy in the BIRD Dev and 84.6% execution accuracy in the SPIDER Dev, significantly outperforming existing approaches.

**Link**: [arxiv](http://arxiv.org/abs/2510.25510v1),  [pdf](http://arxiv.org/pdf/2510.25510v1)

**Tags**: cs.AI 



### Reflections on the Reproducibility of Commercial LLM Performance in   Empirical Software Engineering Studies
**Authors**: Florian Angermeir, Maximilian Amougou, Mark Kreitz, Andreas Bauer, Matthias Linhuber, Davide Fucci, Fabiola MoyÃ³n C., Daniel Mendez, Tony Gorschek

**Updated**: 2025-10-29T13:31:32Z

**Summary**: Large Language Models have gained remarkable interest in industry and academia. The increasing interest in LLMs in academia is also reflected in the number of publications on this topic over the last years. For instance, alone 78 of the around 425 publications at ICSE 2024 performed experiments with LLMs. Conducting empirical studies with LLMs remains challenging and raises questions on how to achieve reproducible results, for both other researchers and practitioners. One important step towards excelling in empirical research on LLMs and their application is to first understand to what extent current research results are eventually reproducible and what factors may impede reproducibility. This investigation is within the scope of our work. We contribute an analysis of the reproducibility of LLM-centric studies, provide insights into the factors impeding reproducibility, and discuss suggestions on how to improve the current state. In particular, we studied the 86 articles describing LLM-centric studies, published at ICSE 2024 and ASE 2024. Of the 86 articles, 18 provided research artefacts and used OpenAI models. We attempted to replicate those 18 studies. Of the 18 studies, only five were fit for reproduction. For none of the five studies, we were able to fully reproduce the results. Two studies seemed to be partially reproducible, and three studies did not seem to be reproducible. Our results highlight not only the need for stricter research artefact evaluations but also for more robust study designs to ensure the reproducible value of future publications.

**Link**: [arxiv](http://arxiv.org/abs/2510.25506v1),  [pdf](http://arxiv.org/pdf/2510.25506v1)

**Tags**: cs.SE cs.AI 



### A Multilingual, Large-Scale Study of the Interplay between LLM   Safeguards, Personalisation, and Disinformation
**Authors**: JoÃ£o A. Leite, Arnav Arora, Silvia Gargova, JoÃ£o Luz, Gustavo Sampaio, Ian Roberts, Carolina Scarton, Kalina Bontcheva

**Updated**: 2025-10-29T13:26:49Z

**Summary**: Large Language Models (LLMs) can generate human-like disinformation, yet their ability to personalise such content across languages and demographics remains underexplored. This study presents the first large-scale, multilingual analysis of persona-targeted disinformation generation by LLMs. Employing a red teaming methodology, we prompt eight state-of-the-art LLMs with 324 false narratives and 150 demographic personas (combinations of country, generation, and political orientation) across four languages--English, Russian, Portuguese, and Hindi--resulting in AI-TRAITS, a comprehensive dataset of 1.6 million personalised disinformation texts. Results show that the use of even simple personalisation prompts significantly increases the likelihood of jailbreaks across all studied LLMs, up to 10 percentage points, and alters linguistic and rhetorical patterns that enhance narrative persuasiveness. Models such as Grok and GPT exhibited jailbreak rates and personalisation scores both exceeding 85%. These insights expose critical vulnerabilities in current state-of-the-art LLMs and offer a foundation for improving safety alignment and detection strategies in multilingual and cross-demographic contexts.

**Link**: [arxiv](http://arxiv.org/abs/2510.12993v2),  [pdf](http://arxiv.org/pdf/2510.12993v2)

**Tags**: cs.CL 



### Securing AI Agent Execution
**Authors**: Christoph BÃ¼hler, Matteo Biagiola, Luca Di Grazia, Guido Salvaneschi

**Updated**: 2025-10-29T13:11:21Z

**Summary**: Large Language Models (LLMs) have evolved into AI agents that interact with external tools and environments to perform complex tasks. The Model Context Protocol (MCP) has become the de facto standard for connecting agents with such resources, but security has lagged behind: thousands of MCP servers execute with unrestricted access to host systems, creating a broad attack surface. In this paper, we introduce AgentBound, the first access control framework for MCP servers. AgentBound combines a declarative policy mechanism, inspired by the Android permission model, with a policy enforcement engine that contains malicious behavior without requiring MCP server modifications. We build a dataset containing the 296 most popular MCP servers, and show that access control policies can be generated automatically from source code with 80.9% accuracy. We also show that AgentBound blocks the majority of security threats in several malicious MCP servers, and that policy enforcement engine introduces negligible overhead. Our contributions provide developers and project managers with a practical foundation for securing MCP servers while maintaining productivity, enabling researchers and tool builders to explore new directions for declarative access control and MCP security.

**Link**: [arxiv](http://arxiv.org/abs/2510.21236v2),  [pdf](http://arxiv.org/pdf/2510.21236v2)

**Tags**: cs.CR cs.AI cs.SE D.2.0 



### Generalized Pseudo-Relevance Feedback
**Authors**: Yiteng Tu, Weihang Su, Yujia Zhou, Yiqun Liu, Fen Lin, Qin Liu, Qingyao Ai

**Updated**: 2025-10-29T13:08:35Z

**Summary**: Query rewriting is a fundamental technique in information retrieval (IR). It typically employs the retrieval result as relevance feedback to refine the query and thereby addresses the vocabulary mismatch between user queries and relevant documents. Traditional pseudo-relevance feedback (PRF) and its vector-based extension (VPRF) improve retrieval performance by leveraging top-retrieved documents as relevance feedback. However, they are constructed based on two major hypotheses: the relevance assumption (top documents are relevant) and the model assumption (rewriting methods need to be designed specifically for particular model architectures). While recent large language models (LLMs)-based generative relevance feedback (GRF) enables model-free query reformulation, it either suffers from severe LLM hallucination or, again, relies on the relevance assumption to guarantee the effectiveness of rewriting quality. To overcome these limitations, we introduce an assumption-relaxed framework: \textit{Generalized Pseudo Relevance Feedback} (GPRF), which performs model-free, natural language rewriting based on retrieved documents, not only eliminating the model assumption but also reducing dependence on the relevance assumption. Specifically, we design a utility-oriented training pipeline with reinforcement learning to ensure robustness against noisy feedback. Extensive experiments across multiple benchmarks and retrievers demonstrate that GPRF consistently outperforms strong baselines, establishing it as an effective and generalizable framework for query rewriting.

**Link**: [arxiv](http://arxiv.org/abs/2510.25488v1),  [pdf](http://arxiv.org/pdf/2510.25488v1)

**Tags**: cs.IR 



### NetEcho: From Real-World Streaming Side-Channels to Full LLM   Conversation Recovery
**Authors**: Zheng Zhang, Guanlong Wu, Sen Deng, Shuai Wang, Yinqian Zhang

**Updated**: 2025-10-29T12:47:36Z

**Summary**: In the rapidly expanding landscape of Large Language Model (LLM) applications, real-time output streaming has become the dominant interaction paradigm. While this enhances user experience, recent research reveals that it exposes a non-trivial attack surface through network side-channels. Adversaries can exploit patterns in encrypted traffic to infer sensitive information and reconstruct private conversations. In response, LLM providers and third-party services are deploying defenses such as traffic padding and obfuscation to mitigate these vulnerabilities.   This paper starts by presenting a systematic analysis of contemporary side-channel defenses in mainstream LLM applications, with a focus on services from vendors like OpenAI and DeepSeek. We identify and examine seven representative deployment scenarios, each incorporating active/passive mitigation techniques. Despite these enhanced security measures, our investigation uncovers significant residual information that remains vulnerable to leakage within the network traffic.   Building on this discovery, we introduce NetEcho, a novel, LLM-based framework that comprehensively unleashes the network side-channel risks of today's LLM applications. NetEcho is designed to recover entire conversations -- including both user prompts and LLM responses -- directly from encrypted network traffic. It features a deliberate design that ensures high-fidelity text recovery, transferability across different deployment scenarios, and moderate operational cost. In our evaluations on medical and legal applications built upon leading models like DeepSeek-v3 and GPT-4o, NetEcho can recover avg $\sim$70\% information of each conversation, demonstrating a critical limitation in current defense mechanisms. We conclude by discussing the implications of our findings and proposing future directions for augmenting network traffic security.

**Link**: [arxiv](http://arxiv.org/abs/2510.25472v1),  [pdf](http://arxiv.org/pdf/2510.25472v1)

**Tags**: cs.CR 



### Fine-Tuned Language Models for Domain-Specific Summarization and Tagging
**Authors**: Jun Wang, Fuming Lin, Yuyu Chen

**Updated**: 2025-10-29T12:33:48Z

**Summary**: This paper presents a pipeline integrating fine-tuned large language models (LLMs) with named entity recognition (NER) for efficient domain-specific text summarization and tagging. The authors address the challenge posed by rapidly evolving sub-cultural languages and slang, which complicate automated information extraction and law enforcement monitoring. By leveraging the LLaMA Factory framework, the study fine-tunes LLMs on both generalpurpose and custom domain-specific datasets, particularly in the political and security domains. The models are evaluated using BLEU and ROUGE metrics, demonstrating that instruction fine-tuning significantly enhances summarization and tagging accuracy, especially for specialized corpora. Notably, the LLaMA3-8B-Instruct model, despite its initial limitations in Chinese comprehension, outperforms its Chinese-trained counterpart after domainspecific fine-tuning, suggesting that underlying reasoning capabilities can transfer across languages. The pipeline enables concise summaries and structured entity tagging, facilitating rapid document categorization and distribution. This approach proves scalable and adaptable for real-time applications, supporting efficient information management and the ongoing need to capture emerging language trends. The integration of LLMs and NER offers a robust solution for transforming unstructured text into actionable insights, crucial for modern knowledge management and security operations.

**Link**: [arxiv](http://arxiv.org/abs/2510.25460v1),  [pdf](http://arxiv.org/pdf/2510.25460v1)

**Tags**: cs.CL cs.AI 



### Grounded in Reality: Learning and Deploying Proactive LLM from Offline   Logs
**Authors**: Fei Wei, Daoyuan Chen, Ce Wang, Yilun Huang, Yushuo Chen, Xuchen Pan, Yaliang Li, Bolin Ding

**Updated**: 2025-10-29T12:08:07Z

**Summary**: Large Language Models (LLMs) excel as passive responders, but teaching them to be proactive, goal-oriented partners, a critical capability in high-stakes domains, remains a major challenge. Current paradigms either myopically optimize single-turn attributes or rely on brittle, high-cost user simulators, creating a persistent ``reality gap''. To bridge this gap, we introduce \texttt{Learn-to-Ask}, a general, simulator-free framework for learning and deploying proactive dialogue agents \textit{directly from offline expert data}, bypassing the need to model complex user dynamics. Our key insight is to reframe the offline policy learning problem by leveraging the \textbf{observed future} of each expert trajectory. This allows us to infer a dense, turn-by-turn reward signal grounded in the expert's revealed strategy, decomposing the intractable long-horizon problem into a series of supervised learning tasks, and training a policy to output a structured \texttt{(action, state_assessment)} tuple, governing both \textbf{what to ask} and, crucially, \textbf{when to stop}. To ensure reward fidelity, our Automated Grader Calibration pipeline systematically purges noise from the LLM-based reward model with minimal human supervision. Empirically, we demonstrate the efficacy of \texttt{Learn-to-Ask} in a real-world medical dataset, using LLMs of varying sizes up to 32B. Our approach culminates in the successful deployment of LLMs into a live, large-scale online AI service. In rigorous in-house evaluations, our model was launched and achieved performance even superior to human experts, proving our framework's ability to translate offline data into tangible, real-world impact. We hope this work provides a practical and economically viable blueprint for transforming passive LLMs into proactive, goal-oriented LLM applications.

**Link**: [arxiv](http://arxiv.org/abs/2510.25441v1),  [pdf](http://arxiv.org/pdf/2510.25441v1)

**Tags**: cs.CL cs.AI 



### SATURN: SAT-based Reinforcement Learning to Unleash Language Model   Reasoning
**Authors**: Huanyu Liu, Jia Li, Hao Zhu, Kechi Zhang, Yihong Dong, Ge Li

**Updated**: 2025-10-29T12:06:15Z

**Summary**: How to design reinforcement learning (RL) tasks that effectively unleash the reasoning capability of large language models (LLMs) remains an open question. Existing RL tasks (e.g., math, programming, and constructing reasoning tasks) suffer from three key limitations: (1) Scalability. They rely heavily on human annotation or expensive LLM synthesis to generate sufficient training data. (2) Verifiability. LLMs' outputs are hard to verify automatically and reliably. (3) Controllable Difficulty. Most tasks lack fine-grained difficulty control, making it hard to train LLMs to develop reasoning ability from easy to hard.   To address these limitations, we propose Saturn, a SAT-based RL framework that uses Boolean Satisfiability (SAT) problems to train and evaluate LLMs reasoning. Saturn enables scalable task construction, rule-based verification, and precise difficulty control. Saturn designs a curriculum learning pipeline that continuously improves LLMs' reasoning capability by constructing SAT tasks of increasing difficulty and training LLMs from easy to hard. To ensure stable training, we design a principled mechanism to control difficulty transitions.   We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying difficulty. It supports the evaluation of how LLM reasoning changes with problem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain Saturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT problems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of +14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B and Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g., AIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in constructing RL tasks, Saturn achieves further improvements of +8.8%. We release the source code, data, and models to support future research.

**Link**: [arxiv](http://arxiv.org/abs/2505.16368v2),  [pdf](http://arxiv.org/pdf/2505.16368v2)

**Tags**: cs.LG cs.AI 



### A Critical Study of Automatic Evaluation in Sign Language Translation
**Authors**: Shakib Yazdani, Yasser Hamidullah, Cristina EspaÃ±a-Bonet, Eleftherios Avramidis, Josef van Genabith

**Updated**: 2025-10-29T11:57:03Z

**Summary**: Automatic evaluation metrics are crucial for advancing sign language translation (SLT). Current SLT evaluation metrics, such as BLEU and ROUGE, are only text-based, and it remains unclear to what extent text-based metrics can reliably capture the quality of SLT outputs. To address this gap, we investigate the limitations of text-based SLT evaluation metrics by analyzing six metrics, including BLEU, chrF, and ROUGE, as well as BLEURT on the one hand, and large language model (LLM)-based evaluators such as G-Eval and GEMBA zero-shot direct assessment on the other hand. Specifically, we assess the consistency and robustness of these metrics under three controlled conditions: paraphrasing, hallucinations in model outputs, and variations in sentence length. Our analysis highlights the limitations of lexical overlap metrics and demonstrates that while LLM-based evaluators better capture semantic equivalence often missed by conventional metrics, they can also exhibit bias toward LLM-paraphrased translations. Moreover, although all metrics are able to detect hallucinations, BLEU tends to be overly sensitive, whereas BLEURT and LLM-based evaluators are comparatively lenient toward subtle cases. This motivates the need for multimodal evaluation frameworks that extend beyond text-based metrics to enable a more holistic assessment of SLT outputs.

**Link**: [arxiv](http://arxiv.org/abs/2510.25434v1),  [pdf](http://arxiv.org/pdf/2510.25434v1)

**Tags**: cs.CL 



### Depth and Autonomy: A Framework for Evaluating LLM Applications in   Social Science Research
**Authors**: Ali Sanaei, Ali Rajabzadeh

**Updated**: 2025-10-29T11:55:21Z

**Summary**: Large language models (LLMs) are increasingly utilized by researchers across a wide range of domains, and qualitative social science is no exception; however, this adoption faces persistent challenges, including interpretive bias, low reliability, and weak auditability. We introduce a framework that situates LLM usage along two dimensions, interpretive depth and autonomy, thereby offering a straightforward way to classify LLM applications in qualitative research and to derive practical design recommendations. We present the state of the literature with respect to these two dimensions, based on all published social science papers available on Web of Science that use LLMs as a tool and not strictly as the subject of study. Rather than granting models expansive freedom, our approach encourages researchers to decompose tasks into manageable segments, much as they would when delegating work to capable undergraduate research assistants. By maintaining low levels of autonomy and selectively increasing interpretive depth only where warranted and under supervision, one can plausibly reap the benefits of LLMs while preserving transparency and reliability.

**Link**: [arxiv](http://arxiv.org/abs/2510.25432v1),  [pdf](http://arxiv.org/pdf/2510.25432v1)

**Tags**: cs.CL 



### Alibaba International E-commerce Product Search Competition DcuRAGONs   Team Technical Report
**Authors**: Thang-Long Nguyen-Ho, Minh-Khoi Pham, Hoang-Bao Le

**Updated**: 2025-10-29T11:50:52Z

**Summary**: This report details our methodology and results developed for the Multilingual E-commerce Search Competition. The problem aims to recognize relevance between user queries versus product items in a multilingual context and improve recommendation performance on e-commerce platforms. Utilizing Large Language Models (LLMs) and their capabilities in other tasks, our data-centric method achieved the highest score compared to other solutions during the competition. Final leaderboard is publised at https://alibaba-international-cikm2025.github.io. The source code for our project is published at https://github.com/nhtlongcs/e-commerce-product-search.

**Link**: [arxiv](http://arxiv.org/abs/2510.25428v1),  [pdf](http://arxiv.org/pdf/2510.25428v1)

**Tags**: cs.IR cs.AI 



### RLMEval: Evaluating Research-Level Neural Theorem Proving
**Authors**: Auguste Poiroux, Antoine Bosselut, Viktor KunÄak

**Updated**: 2025-10-29T11:49:49Z

**Summary**: Despite impressive results on curated benchmarks, the practical impact of large language models (LLMs) on research-level neural theorem proving and proof autoformalization is still limited. We introduce RLMEval, an evaluation suite for these tasks, focusing on research-level mathematics from real-world Lean formalization projects. RLMEval targets the evaluation of neural theorem proving and proof autoformalization on challenging research-level theorems by leveraging real Lean Blueprint formalization projects. Our evaluation of state-of-the-art models on RLMEval, comprising 613 theorems from 6 Lean projects, reveals a significant gap: progress on existing benchmarks does not readily translate to these more realistic settings, with the best model achieving only a 10.3 % pass rate. RLMEval provides a new, challenging benchmark designed to guide and accelerate progress in automated reasoning for formal mathematics.

**Link**: [arxiv](http://arxiv.org/abs/2510.25427v1),  [pdf](http://arxiv.org/pdf/2510.25427v1)

**Tags**: cs.CL cs.AI 



### Implicature in Interaction: Understanding Implicature Improves Alignment   in Human-LLM Interaction
**Authors**: Asutosh Hota, Jussi P. P. Jokinen

**Updated**: 2025-10-29T11:49:42Z

**Summary**: The rapid advancement of Large Language Models (LLMs) is positioning language at the core of human-computer interaction (HCI). We argue that advancing HCI requires attention to the linguistic foundations of interaction, particularly implicature (meaning conveyed beyond explicit statements through shared context) which is essential for human-AI (HAI) alignment. This study examines LLMs' ability to infer user intent embedded in context-driven prompts and whether understanding implicature improves response generation. Results show that larger models approximate human interpretations more closely, while smaller models struggle with implicature inference. Furthermore, implicature-based prompts significantly enhance the perceived relevance and quality of responses across models, with notable gains in smaller models. Overall, 67.6% of participants preferred responses with implicature-embedded prompts to literal ones, highlighting a clear preference for contextually nuanced communication. Our work contributes to understanding how linguistic theory can be used to address the alignment problem by making HAI interaction more natural and contextually grounded.

**Link**: [arxiv](http://arxiv.org/abs/2510.25426v1),  [pdf](http://arxiv.org/pdf/2510.25426v1)

**Tags**: cs.CL cs.AI 



### Small Talk, Big Impact? LLM-based Conversational Agents to Mitigate   Passive Fatigue in Conditional Automated Driving
**Authors**: Lewis Cockram, Yueteng Yu, Jorge Pardo, Xiaomeng Li, Andry Rakotonirainy, Jonny Kuo, Sebastien Demmel, Mike LennÃ©, Ronald Schroeter

**Updated**: 2025-10-29T11:40:38Z

**Summary**: Passive fatigue during conditional automated driving can compromise driver readiness and safety. This paper presents findings from a test-track study with 40 participants in a real-world rural automated driving scenario. In this scenario, a Large Language Model (LLM) based conversational agent (CA) was designed to check in with drivers and re-engage them with their surroundings. Drawing on in-car video recordings, sleepiness ratings and interviews, we analysed how drivers interacted with the agent and how these interactions shaped alertness. Users found the CA helpful for supporting vigilance during passive fatigue. Thematic analysis of acceptability further revealed three user preference profiles that implicate future intention to use CAs. Positioning empirically observed profiles within existing CA archetype frameworks highlights the need for adaptive design sensitive to diverse user groups. This work underscores the potential of CAs as proactive Human-Machine Interface (HMI) interventions, demonstrating how natural language can support context-aware interaction during automated driving.

**Link**: [arxiv](http://arxiv.org/abs/2510.25421v1),  [pdf](http://arxiv.org/pdf/2510.25421v1)

**Tags**: cs.HC 



### Serve Programs, Not Prompts
**Authors**: In Gim, Lin Zhong

**Updated**: 2025-10-29T11:29:03Z

**Summary**: Current large language model (LLM) serving systems, primarily designed for text completion, are neither efficient nor adaptable for increasingly complex LLM applications due to their inflexible design. We propose a new LLM serving system architecture that serves programs instead of prompts to address this problem. These programs, called LLM Inference Programs (LIPs), allow users to customize token prediction and KV cache management at runtime and to offload parts of their application logic, such as tool execution, to the server. We describe an example of this architecture through a system named Symphony, which functions as an operating system for LIPs. Symphony exposes LLM model computations via system calls and virtualizes KV cache with a dedicated file system, while ensuring GPU efficiency with a two-level process scheduling scheme. Symphony has the potential to open the door to a more efficient and extensible ecosystem for LLM applications.

**Link**: [arxiv](http://arxiv.org/abs/2510.25412v1),  [pdf](http://arxiv.org/pdf/2510.25412v1)

**Tags**: cs.CL 



### Quantum-Resilient Threat Modelling for Secure RIS-Assisted ISAC in 6G   UAV Corridors
**Authors**: Sana Hafeez, Ghulam E Mustafa Abro, Hifza Mustafa

**Updated**: 2025-10-29T11:28:42Z

**Summary**: The rapid deployment of unmanned aerial vehicle (UAV) corridors in sixth-generation (6G) networks requires safe, intelligence-driven integrated sensing and communications (ISAC). Reconfigurable intelligent surfaces (RIS) enhance spectrum efficiency, localisation accuracy, and situational awareness, while introducing new vulnerabilities. The rise of quantum computing increases the risks associated with harvest-now-decrypt-later strategies and quantum-enhanced spoofing. We propose a Quantum-Resilient Threat Modelling (QRTM) framework for RIS-assisted ISAC in UAV corridors to address these challenges. QRTM integrates classical, quantum-ready, and quantum-aided adversaries, countered using post-quantum cryptographic (PQC) primitives: ML-KEM for key establishment and Falcon for authentication, both embedded within RIS control signalling and UAV coordination. To strengthen security sensing, the framework introduces RIS-coded scene watermarking validated through a generalised likelihood ratio test (GLRT), with its detection probability characterised by the Marcum Q function. Furthermore, a Secure ISAC Utility (SIU) jointly optimises secrecy rate, spoofing detection, and throughput under RIS constraints, enabled by a scheduler with computational complexity of O(n^2). Monte Carlo evaluations using 3GPP Release 19 mid-band urban-canyon models (7-15 GHz) demonstrate a spoof-detection probability approaching 0.99 at a false-alarm rate of 1e-3, secrecy-rate retention exceeding 90 percent against quantum-capable adversaries, and signal-interference utilisation improvements of about 25 percent compared with baselines. These results show a standards-compliant path towards reliable, quantum-resilient ISAC for UAV corridors in smart cities and non-terrestrial networks.

**Link**: [arxiv](http://arxiv.org/abs/2510.25411v1),  [pdf](http://arxiv.org/pdf/2510.25411v1)

**Tags**: eess.SY cs.SY 



### BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic   Domains
**Authors**: Vijay Devane, Mohd Nauman, Bhargav Patel, Aniket Mahendra Wakchoure, Yogeshkumar Sant, Shyam Pawar, Viraj Thakur, Ananya Godse, Sunil Patra, Neha Maurya, Suraj Racha, Nitish Kamal Singh, Ajay Nagpal, Piyush Sawarkar, Kundeshwar Vijayrao Pundalik, Rohit Saluja, Ganesh Ramakrishnan

**Updated**: 2025-10-30T10:48:05Z

**Summary**: The rapid advancement of large language models(LLMs) has intensified the need for domain and culture specific evaluation. Existing benchmarks are largely Anglocentric and domain-agnostic, limiting their applicability to India-centric contexts. To address this gap, we introduce BhashaBench V1, the first domain-specific, multi-task, bilingual benchmark focusing on critical Indic knowledge systems. BhashaBench V1 contains 74,166 meticulously curated question-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from authentic government and domain-specific exams. It spans four major domains: Agriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and covering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs reveals significant domain and language specific performance gaps, with especially large disparities in low-resource domains. For instance, GPT-4o achieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models consistently perform better on English content compared to Hindi across all domains. Subdomain-level analysis shows that areas such as Cyber Law, International Finance perform relatively well, while Panchakarma, Seed Science, and Human Rights remain notably weak. BhashaBench V1 provides a comprehensive dataset for evaluating large language models across India's diverse knowledge domains. It enables assessment of models' ability to integrate domain-specific knowledge with bilingual understanding. All code, benchmarks, and resources are publicly available to support open research.

**Link**: [arxiv](http://arxiv.org/abs/2510.25409v2),  [pdf](http://arxiv.org/pdf/2510.25409v2)

**Tags**: cs.CL cs.AI 



### GPTOpt: Towards Efficient LLM-Based Black-Box Optimization
**Authors**: Jamison Meindl, Yunsheng Tian, Tony Cui, Veronika Thost, Zhang-Wei Hong, Jie Chen, Wojciech Matusik, Mina KonakoviÄ LukoviÄ

**Updated**: 2025-10-29T11:21:55Z

**Summary**: Global optimization of expensive, derivative-free black-box functions demands extreme sample efficiency. Classical methods such as Bayesian Optimization (BO) can be effective, but they often require careful parameter tuning to each application domain. At the same time, Large Language Models (LLMs) have shown broad capabilities, yet state-of-the-art models remain limited in solving continuous black-box optimization tasks. We introduce GPTOpt, an LLM-based optimization method that equips LLMs with continuous black-box optimization capabilities. By fine-tuning large language models on extensive synthetic datasets derived from diverse BO parameterizations, GPTOpt leverages LLM pre-training to generalize across optimization tasks. On a variety of black-box optimization benchmarks, GPTOpt surpasses traditional optimizers, highlighting the capacity of LLMs for advanced numerical reasoning and introducing a flexible framework for global optimization without parameter tuning.

**Link**: [arxiv](http://arxiv.org/abs/2510.25404v1),  [pdf](http://arxiv.org/pdf/2510.25404v1)

**Tags**: cs.LG cs.AI 



### Towards Automated Quality Assurance of Patent Specifications: A   Multi-Dimensional LLM Framework
**Authors**: Yuqian Chai, Chaochao Wang, Weilei Wang

**Updated**: 2025-10-30T02:45:14Z

**Summary**: Although AI drafting tools have gained prominence in patent writing, the systematic evaluation of AI-generated patent content quality represents a significant research gap. To address this gap, We propose to evaluate patents using regulatory compliance, technical coherence, and figure-reference consistency detection modules, and then generate improvement suggestions via an integration module. The framework is validated on a comprehensive dataset comprising 80 human-authored and 80 AI-generated patents from two patent drafting tools. Evaluation is performed on 10,841 total sentences, 8,924 non-template sentences, and 554 patent figures for the three detection modules respectively, achieving balanced accuracies of 99.74%, 82.12%, and 91.2% against expert annotations. Additional analysis was conducted to examine defect distributions across patent sections, technical domains, and authoring sources. Section-based analysis indicates that figure-text consistency and technical detail precision require particular attention. Mechanical Engineering and Construction show more claim-specification inconsistencies due to complex technical documentation requirements. AI-generated patents show a significant gap compared to human-authored ones. While human-authored patents primarily contain surface-level errors like typos, AI-generated patents exhibit more structural defects in figure-text alignment and cross-references.

**Link**: [arxiv](http://arxiv.org/abs/2510.25402v2),  [pdf](http://arxiv.org/pdf/2510.25402v2)

**Tags**: cs.IR cs.CE 



### Robust Preference Optimization via Dynamic Target Margins
**Authors**: Jie Sun, Junkang Wu, Jiancan Wu, Zhibo Zhu, Xingyu Lu, Jun Zhou, Lintao Ma, Xiang Wang

**Updated**: 2025-10-29T11:04:12Z

**Summary**: The alignment of Large Language Models (LLMs) is crucial for ensuring their safety and reliability in practical applications. Direct Preference Optimization (DPO) has emerged as an efficient method that directly optimizes models using preference pairs, significantly reducing resource demands. However, the effectiveness of DPO heavily depends on the data quality, which is frequently compromised by noise. In this work, we propose $\gamma$-PO, a dynamic target margin preference optimization algorithm that adjust reward margins at the pairwise level. By introducing instance-specific margin calibration, $\gamma$-PO strategically prioritizes high-confidence pairs (those demonstrating higher reward margins) while suppressing potential noise from ambiguous pairs. Moreover, $\gamma$-PO is a plug-and-play method, compatible with variants of DPO that rely on reward margin between preference pairs. Across benchmarks such as AlpacaEval2 and Arena-Hard, $\gamma$-PO achieves an average 4.4\% improvement over other baselines, setting new benchmarks for state-of-the-art performance. Additionally, $\gamma$-PO requires minimal code changes and has a negligible impact on training efficiency, making it a robust solution for enhancing LLMs alignment. Our codes are available at \href{https://github.com/sunjie279/gammaPO}{https://github.com/sunjie279/gammaPO}.

**Link**: [arxiv](http://arxiv.org/abs/2506.03690v2),  [pdf](http://arxiv.org/pdf/2506.03690v2)

**Tags**: cs.CL 



### Taxonomy and Trends in Reinforcement Learning for Robotics and Control   Systems: A Structured Review
**Authors**: Kumater Ter, Ore-Ofe Ajayi, Daniel Udekwe

**Updated**: 2025-10-29T11:02:07Z

**Summary**: Reinforcement learning (RL) has become a foundational approach for enabling intelligent robotic behavior in dynamic and uncertain environments. This work presents an in-depth review of RL principles, advanced deep reinforcement learning (DRL) algorithms, and their integration into robotic and control systems. Beginning with the formalism of Markov Decision Processes (MDPs), the study outlines essential elements of the agent-environment interaction and explores core algorithmic strategies including actor-critic methods, value-based learning, and policy gradients. Emphasis is placed on modern DRL techniques such as DDPG, TD3, PPO, and SAC, which have shown promise in solving high-dimensional, continuous control tasks. A structured taxonomy is introduced to categorize RL applications across domains such as locomotion, manipulation, multi-agent coordination, and human-robot interaction, along with training methodologies and deployment readiness levels. The review synthesizes recent research efforts, highlighting technical trends, design patterns, and the growing maturity of RL in real-world robotics. Overall, this work aims to bridge theoretical advances with practical implementations, providing a consolidated perspective on the evolving role of RL in autonomous robotic systems.

**Link**: [arxiv](http://arxiv.org/abs/2510.21758v3),  [pdf](http://arxiv.org/pdf/2510.21758v3)

**Tags**: cs.RO cs.LG 



### U-DECN: End-to-End Underwater Object Detection ConvNet with Improved   DeNoising Training
**Authors**: Zhuoyan Liu, Bo Wang, Bing Wang, Ye Li

**Updated**: 2025-10-29T10:59:02Z

**Summary**: Underwater object detection has higher requirements of running speed and deployment efficiency for the detector due to its specific environmental challenges. NMS of two- or one-stage object detectors and transformer architecture of query-based end-to-end object detectors are not conducive to deployment on underwater embedded devices with limited processing power. As for the detrimental effect of underwater color cast noise, recent underwater object detectors make network architecture or training complex, which also hinders their application and deployment on unmanned underwater vehicles. In this paper, we propose the Underwater DECO with improved deNoising training (U-DECN), the query-based end-to-end object detector (with ConvNet encoder-decoder architecture) for underwater color cast noise that addresses the above problems. We integrate advanced technologies from DETR variants into DECO and design optimization methods specifically for the ConvNet architecture, including Deformable Convolution in SIM and Separate Contrastive DeNoising Forward methods. To address the underwater color cast noise issue, we propose an Underwater Color DeNoising Query method to improve the generalization of the model for the biased object feature information by different color cast noise. Our U-DECN, with ResNet-50 backbone, achieves the best 64.0 AP on DUO and the best 58.1 AP on RUOD, and 21 FPS (5 times faster than Deformable DETR and DINO 4 FPS) on NVIDIA AGX Orin by TensorRT FP16, outperforming the other state-of-the-art query-based end-to-end object detectors. The code is available at https://github.com/LEFTeyex/U-DECN.

**Link**: [arxiv](http://arxiv.org/abs/2408.05780v2),  [pdf](http://arxiv.org/pdf/2408.05780v2)

**Tags**: cs.CV I.4 



### GnnXemplar: Exemplars to Explanations -- Natural Language Rules for   Global GNN Interpretability
**Authors**: Burouj Armgaan, Eshan Jain, Harsh Pandey, Mahesh Chandran, Sayan Ranu

**Updated**: 2025-10-29T10:57:22Z

**Summary**: Graph Neural Networks (GNNs) are widely used for node classification, yet their opaque decision-making limits trust and adoption. While local explanations offer insights into individual predictions, global explanation methods, those that characterize an entire class, remain underdeveloped. Existing global explainers rely on motif discovery in small graphs, an approach that breaks down in large, real-world settings where subgraph repetition is rare, node attributes are high-dimensional, and predictions arise from complex structure-attribute interactions. We propose GnnXemplar, a novel global explainer inspired from Exemplar Theory from cognitive science. GnnXemplar identifies representative nodes in the GNN embedding space, exemplars, and explains predictions using natural language rules derived from their neighborhoods. Exemplar selection is framed as a coverage maximization problem over reverse k-nearest neighbors, for which we provide an efficient greedy approximation. To derive interpretable rules, we employ a self-refining prompt strategy using large language models (LLMs). Experiments across diverse benchmarks show that GnnXemplar significantly outperforms existing methods in fidelity, scalability, and human interpretability, as validated by a user study with 60 participants.

**Link**: [arxiv](http://arxiv.org/abs/2509.18376v2),  [pdf](http://arxiv.org/pdf/2509.18376v2)

**Tags**: cs.LG cs.SI 



### Roleplaying with Structure: Synthetic Therapist-Client Conversation   Generation from Questionnaires
**Authors**: Doan Nam Long Vu, Rui Tan, Lena Moench, Svenja Jule Francke, Daniel Woiwod, Florian Thomas-Odenthal, Sanna Stroth, Tilo Kircher, Christiane Hermann, Udo Dannlowski, Hamidreza Jamalabadi, Shaoxiong Ji

**Updated**: 2025-10-29T10:55:52Z

**Summary**: The development of AI for mental health is hindered by a lack of authentic therapy dialogues, due to strict privacy regulations and the fact that clinical sessions were historically rarely recorded. We present an LLM-driven pipeline that generates synthetic counseling dialogues based on structured client profiles and psychological questionnaires. Grounded on the principles of Cognitive Behavioral Therapy (CBT), our method creates synthetic therapeutic conversations for clinical disorders such as anxiety and depression. Our framework, SQPsych (Structured Questionnaire-based Psychotherapy), converts structured psychological input into natural language dialogues through therapist-client simulations. Due to data governance policies and privacy restrictions prohibiting the transmission of clinical questionnaire data to third-party services, previous methodologies relying on proprietary models are infeasible in our setting. We address this limitation by generating a high-quality corpus using open-weight LLMs, validated through human expert evaluation and LLM-based assessments. Our SQPsychLLM models fine-tuned on SQPsychConv achieve strong performance on counseling benchmarks, surpassing baselines in key therapeutic skills. Our findings highlight the potential of synthetic data to enable scalable, data-secure, and clinically informed AI for mental health support. We will release our code, models, and corpus at https://ai-mh.github.io/SQPsych

**Link**: [arxiv](http://arxiv.org/abs/2510.25384v1),  [pdf](http://arxiv.org/pdf/2510.25384v1)

**Tags**: cs.CL 



### CGM-Led Multimodal Tracking with Chatbot Support: An Autoethnography in   Sub-Health
**Authors**: Dongyijie Primo Pan, Lan Luo, Yike Wang, Pan Hui

**Updated**: 2025-10-29T10:53:32Z

**Summary**: Metabolic disorders present a pressing global health challenge, with China carrying the world's largest burden. While continuous glucose monitoring (CGM) has transformed diabetes care, its potential for supporting sub-health populations -- such as individuals who are overweight, prediabetic, or anxious -- remains underexplored. At the same time, large language models (LLMs) are increasingly used in health coaching, yet CGM is rarely incorporated as a first-class signal. To address this gap, we conducted a six-week autoethnography, combining CGM with multimodal indicators captured via common digital devices and a chatbot that offered personalized reflections and explanations of glucose fluctuations. Our findings show how CGM-led, data-first multimodal tracking, coupled with conversational support, shaped everyday practices of diet, activity, stress, and wellbeing. This work contributes to HCI by extending CGM research beyond clinical diabetes and demonstrating how LLM-driven agents can support preventive health and reflection in at-risk populations.

**Link**: [arxiv](http://arxiv.org/abs/2510.25381v1),  [pdf](http://arxiv.org/pdf/2510.25381v1)

**Tags**: cs.HC 



### Hallucinations in Bibliographic Recommendation: Citation Frequency as a   Proxy for Training Data Redundancy
**Authors**: Junichiro Niimi

**Updated**: 2025-10-29T10:51:35Z

**Summary**: Large language models (LLMs) have been increasingly applied to a wide range of tasks, from natural language understanding to code generation. While they have also been used to assist in bibliographic recommendation, the hallucination of non-existent papers remains a major issue. Building on prior studies, this study hypothesizes that an LLM's ability to correctly produce bibliographic information depends on whether the underlying knowledge is generated or memorized, with highly cited papers (i.e., more frequently appear in the training corpus) showing lower hallucination rates. We therefore assume citation count as a proxy for training data redundancy (i.e., the frequency with which a given bibliographic record is repeatedly represented in the pretraining corpus) and investigate how citation frequency affects hallucinated references in LLM outputs. Using GPT-4.1, we generated and manually verified 100 bibliographic records across twenty computer-science domains, and measured factual consistency via cosine similarity between generated and authentic metadata. The results revealed that (i) hallucination rates vary across research domains, (ii) citation count is strongly correlated with factual accuracy, and (iii) bibliographic information becomes almost verbatimly memorized beyond approximately 1,000 citations. These findings suggest that highly cited papers are nearly verbatimly retained in the model, indicating a threshold where generalization shifts into memorization.

**Link**: [arxiv](http://arxiv.org/abs/2510.25378v1),  [pdf](http://arxiv.org/pdf/2510.25378v1)

**Tags**: cs.CL cs.AI 



### LaMP-Val: Large Language Models Empower Personalized Valuation in   Auction
**Authors**: Jie Sun, Tianyu Zhang, Houcheng Jiang, Kexin Huang, Xiang Shu, Zhibo Zhu, Lintao Ma, Xingyu Lu, Jun Zhou, Junkang Wu, Chi Luo, An Zhang, Junkang Wu, Jiancan Wu, Xiang Wang

**Updated**: 2025-10-29T10:48:48Z

**Summary**: Auctions are a vital economic mechanism used to determine the market value of goods or services through competitive bidding within a specific framework. However, much of the current research primarily focuses on the bidding algorithms used within auction mechanisms. This often neglects the potential benefits of incorporating individual users' unique preferences into the valuation process. Our theoretical and empirical analysis demonstrates that valuation errors can significantly impact the overall utility. To bridge this gap, we propose a personalized valuation framework, namely Large \underline{La}nguage \underline{M}odels-powered \underline{P}ersonalized \underline{Val}uation (LaMP-Val), which integrates Large Language Models to incorporate personalized semantic preference into users valuation process. LaMP-Val integrating three components: data, learning, and evaluation. The data component tackles the challenge of building a novel dataset specifically for LLMs fine-tuning in personalized valuation modeling. The learning component introduces a diversity template to enhance LLMs' capacity for modeling fine-grained personal valuation patterns. The evaluation component establishes a closed-loop system where LLM-generated valuations interact with bidding strategies and auction. It proposes two novel metrics to quantify valuation precision and bidding intention accuracy in personalized scenarios. Extensive experiments show that LaMP-Val more accurately captures personalized values and achieves greater profits than baseline approaches.

**Link**: [arxiv](http://arxiv.org/abs/2410.15817v2),  [pdf](http://arxiv.org/pdf/2410.15817v2)

**Tags**: cs.CE 



### Monitoring Transformative Technological Convergence Through   LLM-Extracted Semantic Entity Triple Graphs
**Authors**: Alexander Sternfeld, Andrei Kucharavy, Dimitri Percia David, Alain Mermoud, Julian Jang-Jaccard, Nathan Monnet

**Updated**: 2025-10-29T10:41:03Z

**Summary**: Forecasting transformative technologies remains a critical but challenging task, particularly in fast-evolving domains such as Information and Communication Technologies (ICTs). Traditional expert-based methods struggle to keep pace with short innovation cycles and ambiguous early-stage terminology. In this work, we propose a novel, data-driven pipeline to monitor the emergence of transformative technologies by identifying patterns of technological convergence.   Our approach leverages advances in Large Language Models (LLMs) to extract semantic triples from unstructured text and construct a large-scale graph of technology-related entities and relations. We introduce a new method for grouping semantically similar technology terms (noun stapling) and develop graph-based metrics to detect convergence signals. The pipeline includes multi-stage filtering, domain-specific keyword clustering, and a temporal trend analysis of topic co-occurence.   We validate our methodology on two complementary datasets: 278,625 arXiv preprints (2017--2024) to capture early scientific signals, and 9,793 USPTO patent applications (2018-2024) to track downstream commercial developments. Our results demonstrate that the proposed pipeline can identify both established and emerging convergence patterns, offering a scalable and generalizable framework for technology forecasting grounded in full-text analysis.

**Link**: [arxiv](http://arxiv.org/abs/2510.25370v1),  [pdf](http://arxiv.org/pdf/2510.25370v1)

**Tags**: cs.CL 



### OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents
**Authors**: Thomas Kuntz, Agatha Duzan, Hao Zhao, Francesco Croce, Zico Kolter, Nicolas Flammarion, Maksym Andriushchenko

**Updated**: 2025-10-29T10:34:04Z

**Summary**: Computer use agents are LLM-based agents that can directly interact with a graphical user interface, by processing screenshots or accessibility trees. While these systems are gaining popularity, their safety has been largely overlooked, despite the fact that evaluating and understanding their potential for harmful behavior is essential for widespread adoption. To address this gap, we introduce OS-Harm, a new benchmark for measuring safety of computer use agents. OS-Harm is built on top of the OSWorld environment and aims to test models across three categories of harm: deliberate user misuse, prompt injection attacks, and model misbehavior. To cover these cases, we create 150 tasks that span several types of safety violations (harassment, copyright infringement, disinformation, data exfiltration, etc.) and require the agent to interact with a variety of OS applications (email client, code editor, browser, etc.). Moreover, we propose an automated judge to evaluate both accuracy and safety of agents that achieves high agreement with human annotations (0.76 and 0.79 F1 score). We evaluate computer use agents based on a range of frontier models - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide insights into their safety. In particular, all models tend to directly comply with many deliberate misuse queries, are relatively vulnerable to static prompt injections, and occasionally perform unsafe actions. The OS-Harm benchmark is available at https://github.com/tml-epfl/os-harm.

**Link**: [arxiv](http://arxiv.org/abs/2506.14866v2),  [pdf](http://arxiv.org/pdf/2506.14866v2)

**Tags**: cs.SE cs.LG 



### To what extent can current French mobile network support agricultural   robots?
**Authors**: Pierre La Rocca, GaÃ«l Guennebaud, AurÃ©lie Bugeau

**Updated**: 2025-10-29T10:31:39Z

**Summary**: The large-scale integration of robots in agriculture offers many promises for enhancing sustainability and increasing food production. The numerous applications of agricultural robots rely on the transmission of data via mobile network, with the amount of data depending on the services offered by the robots and the level of on-board technology. Nevertheless, infrastructure required to deploy these robots, as well as the related energy and environmental consequences, appear overlooked in the digital agriculture literature. In this study, we propose a method for assessing the additional energy consumption and carbon footprint induced by a large-scale deployment of agricultural robots. Our method also estimates the share of agricultural area that can be managed by the deployed robots with respect to network infrastructure constraints. We have applied this method to metropolitan France mobile network and agricultural parcels for five different robotic scenarios. Our results show that increasing the robot's bitrate needs leads to significant additional impacts, which increase at a pace that is poorly captured by classical linear extrapolation methods. When constraining the network to the existing sites, increased bitrate needs also comes with a rapidly decreasing manageable agricultural area.

**Link**: [arxiv](http://arxiv.org/abs/2505.10044v4),  [pdf](http://arxiv.org/pdf/2505.10044v4)

**Tags**: cs.CY 



### Not ready for the bench: LLM legal interpretation is unstable and out of   step with human judgments
**Authors**: Abhishek Purushothama, Junghyun Min, Brandon Waldon, Nathan Schneider

**Updated**: 2025-10-29T10:21:25Z

**Summary**: Legal interpretation frequently involves assessing how a legal text, as understood by an 'ordinary' speaker of the language, applies to the set of facts characterizing a legal dispute in the U.S. judicial system. Recent scholarship has proposed that legal practitioners add large language models (LLMs) to their interpretive toolkit. This work offers an empirical argument against LLM interpretation as recently practiced by legal scholars and federal judges. Our investigation in English shows that models do not provide stable interpretive judgments: varying the question format can lead the model to wildly different conclusions. Moreover, the models show weak to moderate correlation with human judgment, with large variance across model and question variant, suggesting that it is dangerous to give much credence to the conclusions produced by generative AI.

**Link**: [arxiv](http://arxiv.org/abs/2510.25356v1),  [pdf](http://arxiv.org/pdf/2510.25356v1)

**Tags**: cs.CL 



### Differential Mamba
**Authors**: Nadav Schneider, Itamar Zimerman, Eliya Nachmani

**Updated**: 2025-10-29T10:17:57Z

**Summary**: Sequence models like Transformers and RNNs often overallocate attention to irrelevant context, leading to noisy intermediate representations. This degrades LLM capabilities by promoting hallucinations, weakening long-range and retrieval abilities, and reducing robustness. Recent work has shown that differential design can mitigate this issue in Transformers, improving their effectiveness across various applications. In this paper, we explore whether these techniques, originally developed for Transformers, can be applied to Mamba, a recent architecture based on selective state-space layers that achieves Transformer-level performance with greater efficiency. We show that a naive adaptation of differential design to Mamba is insufficient and requires careful architectural modifications. To address this, we introduce a novel differential mechanism for Mamba, empirically validated on language modeling benchmarks, demonstrating improved retrieval capabilities and superior performance over vanilla Mamba. Finally, we conduct extensive ablation studies and empirical analyses to justify our design choices and provide evidence that our approach effectively mitigates the overallocation problem in Mamba-based models. Our code is publicly available: https://github.com/NadavSc/Diff-Mamba

**Link**: [arxiv](http://arxiv.org/abs/2507.06204v2),  [pdf](http://arxiv.org/pdf/2507.06204v2)

**Tags**: cs.LG cs.AI cs.CL 



### Joint Beamforming Design and Resource Allocation for IRS-Assisted   Full-Duplex Terahertz Systems
**Authors**: Chi Qiu, Wen Chen, Qingqing Wu, Fen Hou, Wanming Hao, Ruiqi Liu, Derrick Wing Kwan Ng

**Updated**: 2025-10-29T10:03:59Z

**Summary**: Intelligent reflecting surface (IRS)-assisted full-duplex (FD) terahertz (THz) communication systems have emerged as a promising paradigm to satisfy the escalating demand for ultra-high data rates and spectral efficiency in future wireless networks. However, the practical deployment of such systems presents unique technical challenges, stemming from severe propagation loss, frequency-dependent molecular absorption in the THz band, and the presence of strong residual self-interference (SI) inherent to FD communications. To tackle these issues, this paper proposes a joint resource allocation framework that aims to maximize the weighted minimum rate among all users, thereby ensuring fairness in quality of service. Specifically, the proposed design jointly optimizes IRS reflecting phase shifts, uplink/downlink transmit power control, sub-band bandwidth allocation, and sub-band assignment, explicitly capturing the unique propagation characteristics of THz channels and the impact of residual SI. To strike an balance between system performance and computational complexity, two computationally efficient algorithms are developed under distinct spectrum partitioning schemes: one assumes equal sub-band bandwidth allocation to facilliate tractable optimization, while the other introduces adaptive bandwidth allocation to further enhance spectral utilization and system flexibility. Simulation results validate the effectiveness of the proposed designs and demonstrate that the adopted scheme achieves significant spectral efficiency improvements over benchmark schemes.

**Link**: [arxiv](http://arxiv.org/abs/2510.25346v1),  [pdf](http://arxiv.org/pdf/2510.25346v1)

**Tags**: cs.IT math.IT 



### Informative Sample Selection Model for Skeleton-based Action Recognition   with Limited Training Samples
**Authors**: Zhigang Tu, Zhengbo Zhang, Jia Gong, Junsong Yuan, Bo Du

**Updated**: 2025-10-29T10:03:33Z

**Summary**: Skeleton-based human action recognition aims to classify human skeletal sequences, which are spatiotemporal representations of actions, into predefined categories. To reduce the reliance on costly annotations of skeletal sequences while maintaining competitive recognition accuracy, the task of 3D Action Recognition with Limited Training Samples, also known as semi-supervised 3D Action Recognition, has been proposed. In addition, active learning, which aims to proactively select the most informative unlabeled samples for annotation, has been explored in semi-supervised 3D Action Recognition for training sample selection. Specifically, researchers adopt an encoder-decoder framework to embed skeleton sequences into a latent space, where clustering information, combined with a margin-based selection strategy using a multi-head mechanism, is utilized to identify the most informative sequences in the unlabeled set for annotation. However, the most representative skeleton sequences may not necessarily be the most informative for the action recognizer, as the model may have already acquired similar knowledge from previously seen skeleton samples. To solve it, we reformulate Semi-supervised 3D action recognition via active learning from a novel perspective by casting it as a Markov Decision Process (MDP). Built upon the MDP framework and its training paradigm, we train an informative sample selection model to intelligently guide the selection of skeleton sequences for annotation. To enhance the representational capacity of the factors in the state-action pairs within our method, we project them from Euclidean space to hyperbolic space. Furthermore, we introduce a meta tuning strategy to accelerate the deployment of our method in real-world scenarios. Extensive experiments on three 3D action recognition benchmarks demonstrate the effectiveness of our method.

**Link**: [arxiv](http://arxiv.org/abs/2510.25345v1),  [pdf](http://arxiv.org/pdf/2510.25345v1)

**Tags**: cs.CV 



### Open3D-VQA: A Benchmark for Comprehensive Spatial Reasoning with   Multimodal Large Language Model in Open Space
**Authors**: Weichen Zhang, Zile Zhou, Xin Zeng, Xuchen Liu, Jianjie Fang, Chen Gao, Yong Li, Jinqiang Cui, Xinlei Chen, Xiao-Ping Zhang

**Updated**: 2025-10-30T08:44:27Z

**Summary**: Spatial reasoning is a fundamental capability of multimodal large language models (MLLMs), yet their performance in open aerial environments remains underexplored. In this work, we present Open3D-VQA, a novel benchmark for evaluating MLLMs' ability to reason about complex spatial relationships from an aerial perspective. The benchmark comprises 73k QA pairs spanning 7 general spatial reasoning tasks, including multiple-choice, true/false, and short-answer formats, and supports both visual and point cloud modalities. The questions are automatically generated from spatial relations extracted from both real-world and simulated aerial scenes. Evaluation on 13 popular MLLMs reveals that: 1) Models are generally better at answering questions about relative spatial relations than absolute distances, 2) 3D LLMs fail to demonstrate significant advantages over 2D LLMs, and 3) Fine-tuning solely on the simulated dataset can significantly improve the model's spatial reasoning performance in real-world scenarios. We release our benchmark, data generation pipeline, and evaluation toolkit to support further research: https://github.com/EmbodiedCity/Open3D-VQA.code.

**Link**: [arxiv](http://arxiv.org/abs/2503.11094v4),  [pdf](http://arxiv.org/pdf/2503.11094v4)

**Tags**: cs.CV 



### CRMWeaver: Building Powerful Business Agent via Agentic RL and Shared   Memories
**Authors**: Yilong Lai, Yipin Yang, Jialong Wu, Fengran Mo, Zhenglin Wang, Ting Liang, Jianguo Lin, Keping Yang

**Updated**: 2025-10-29T09:47:40Z

**Summary**: Recent years have witnessed the rapid development of LLM-based agents, which shed light on using language agents to solve complex real-world problems. A prominent application lies in business agents, which interact with databases and internal knowledge bases via tool calls to fulfill diverse user requirements. However, this domain is characterized by intricate data relationships and a wide range of heterogeneous tasks, from statistical data queries to knowledge-based question-answering. To address these challenges, we propose CRMWeaver, a novel approach that enhances business agents in such complex settings. To acclimate the agentic model to intricate business environments, we employ a synthesis data generation and RL-based paradigm during training, which significantly improves the model's ability to handle complex data and varied tasks. During inference, a shared memories mechanism is introduced, prompting the agent to learn from task guidelines in similar problems, thereby further boosting its effectiveness and generalization, especially in unseen scenarios. We validate the efficacy of our approach on the CRMArena-Pro dataset, where our lightweight model achieves competitive results in both B2B and B2C business scenarios, underscoring its practical value for real-world applications.

**Link**: [arxiv](http://arxiv.org/abs/2510.25333v1),  [pdf](http://arxiv.org/pdf/2510.25333v1)

**Tags**: cs.CL 



### GAP: Graph-Based Agent Planning with Parallel Tool Use and Reinforcement   Learning
**Authors**: Jiaqi Wu, Qinlao Zhao, Zefeng Chen, Kai Qin, Yifei Zhao, Xueqian Wang, Yuhang Yao

**Updated**: 2025-10-29T09:35:55Z

**Summary**: Autonomous agents powered by large language models (LLMs) have shown impressive capabilities in tool manipulation for complex task-solving. However, existing paradigms such as ReAct rely on sequential reasoning and execution, failing to exploit the inherent parallelism among independent sub-tasks. This sequential bottleneck leads to inefficient tool utilization and suboptimal performance in multi-step reasoning scenarios. We introduce Graph-based Agent Planning (GAP), a novel framework that explicitly models inter-task dependencies through graph-based planning to enable adaptive parallel and serial tool execution. Our approach trains agent foundation models to decompose complex tasks into dependency-aware sub-task graphs, autonomously determining which tools can be executed in parallel and which must follow sequential dependencies. This dependency-aware orchestration achieves substantial improvements in both execution efficiency and task accuracy. To train GAP, we construct a high-quality dataset of graph-based planning traces derived from the Multi-Hop Question Answering (MHQA) benchmark. We employ a two-stage training strategy: supervised fine-tuning (SFT) on the curated dataset, followed by reinforcement learning (RL) with a correctness-based reward function on strategically sampled queries where tool-based reasoning provides maximum value. Experimental results on MHQA datasets demonstrate that GAP significantly outperforms traditional ReAct baselines, particularly on multi-step retrieval tasks, while achieving dramatic improvements in tool invocation efficiency through intelligent parallelization. The project page is available at: https://github.com/WJQ7777/Graph-Agent-Planning.

**Link**: [arxiv](http://arxiv.org/abs/2510.25320v1),  [pdf](http://arxiv.org/pdf/2510.25320v1)

**Tags**: cs.AI cs.CL 



### Parrot: A Training Pipeline Enhances Both Program CoT and Natural   Language CoT for Reasoning
**Authors**: Senjie Jin, Lu Chen, Zhiheng Xi, Yuhui Wang, Sirui Song, Yuhao Zhou, Xinbo Zhang, Peng Sun, Hong Lu, Tao Gui, Qi Zhang, Xuanjing Huang

**Updated**: 2025-10-29T09:23:17Z

**Summary**: Natural language chain-of-thought (N-CoT) and Program chain-of-thought (P-CoT) have emerged as two primary paradigms for large language models (LLMs) to solve mathematical reasoning problems. Current research typically endeavors to achieve unidirectional enhancement: P-CoT enhanced N-CoT or N-CoT enhanced P-CoT. In this paper, we seek to fully unleash the two paradigms' strengths for mutual enhancement and ultimately achieve simultaneous improvements. We conduct a detailed analysis of the error types across two paradigms, based on which we propose Parrot, a novel training pipeline for mathematical problems: 1) Three target-designed subtasks integrate sequential P-CoT and N-CoT generation. 2) A subtask hybrid training strategy to facilitate natural language semantic transferability. 3) The converted N-CoT auxiliary reward is designed to alleviate the sparse rewards in P-CoT optimization. Extensive experiments demonstrate that Parrot significantly enhances both the performance of N-CoT and P-CoT, especially on N-CoT. Using Parrot SFT, the N-CoT performance of LLaMA2 and CodeLLaMA achieve gains of +21.87 and +21.48 on MathQA over the RL baseline, which is resource-intensive.

**Link**: [arxiv](http://arxiv.org/abs/2510.25310v1),  [pdf](http://arxiv.org/pdf/2510.25310v1)

**Tags**: cs.CL 



### GaTector+: A Unified Head-free Framework for Gaze Object and Gaze   Following Prediction
**Authors**: Yang Jin, Guangyu Guo, Binglu Wang

**Updated**: 2025-10-29T09:14:07Z

**Summary**: Gaze object detection and gaze following are fundamental tasks for interpreting human gaze behavior or intent. However, most previous methods usually solve these two tasks separately, and their prediction of gaze objects and gaze following typically depend on head-related prior knowledge during both the training phase and real-world deployment. This dependency necessitates an auxiliary network to extract head location, thus precluding joint optimization across the entire system and constraining the practical applicability. To this end, we propose GaTector+, a unified framework for gaze object detection and gaze following, which eliminates the dependence on the head-related priors during inference. Specifically, GaTector+ uses an expanded specific-general-specific feature extractor that leverages a shared backbone, which extracts general features for gaze following and object detection using the shared backbone while using specific blocks before and after the shared backbone to better consider the specificity of each sub-task. To obtain head-related knowledge without prior information, we first embed a head detection branch to predict the head of each person. Then, before regressing the gaze point, a head-based attention mechanism is proposed to fuse the sense feature and gaze feature with the help of head location. Since the suboptimization of the gaze point heatmap leads to the performance bottleneck, we propose an attention supervision mechanism to accelerate the learning of the gaze heatmap. Finally, we propose a novel evaluation metric, mean Similarity over Candidates (mSoC), for gaze object detection, which is more sensitive to variations between bounding boxes. The experimental results on multiple benchmark datasets demonstrate the effectiveness of our model in both gaze object detection and gaze following tasks.

**Link**: [arxiv](http://arxiv.org/abs/2510.25301v1),  [pdf](http://arxiv.org/pdf/2510.25301v1)

**Tags**: cs.CV 



### Understanding the Characteristics of LLM-Generated Property-Based Tests   in Exploring Edge Cases
**Authors**: Hidetake Tanaka, Haruto Tanaka, Kazumasa Shimari, Kenichi Matsumoto

**Updated**: 2025-10-29T09:07:03Z

**Summary**: As Large Language Models (LLMs) increasingly generate code in software development, ensuring the quality of LLM-generated code has become important. Traditional testing approaches using Example-based Testing (EBT) often miss edge cases -- defects that occur at boundary values, special input patterns, or extreme conditions. This research investigates the characteristics of LLM-generated Property-based Testing (PBT) compared to EBT for exploring edge cases. We analyze 16 HumanEval problems where standard solutions failed on extended test cases, generating both PBT and EBT test codes using Claude-4-sonnet. Our experimental results reveal that while each method individually achieved a 68.75\% bug detection rate, combining both approaches improved detection to 81.25\%. The analysis demonstrates complementary characteristics: PBT effectively detects performance issues and edge cases through extensive input space exploration, while EBT effectively detects specific boundary conditions and special patterns. These findings suggest that a hybrid approach leveraging both testing methods can improve the reliability of LLM-generated code, providing guidance for test generation strategies in LLM-based code generation.

**Link**: [arxiv](http://arxiv.org/abs/2510.25297v1),  [pdf](http://arxiv.org/pdf/2510.25297v1)

**Tags**: cs.SE 



### Fair Rate Maximization for Multi-user Multi-cell MISO Communication   Systems via Novel Transmissive RIS Transceiver
**Authors**: Yuan Guo, Wen Chen, Qingqing Wu, Zhendong Li, Kunlun Wang, Hongying Tang, Jun Li

**Updated**: 2025-10-29T08:47:54Z

**Summary**: This paper explores a multi-cell multiple-input single-output (MISO) downlink communication system enabled by a unique transmissive reconfigurable intelligent surface (RIS) transceiver (TRTC) configuration. Within this system framework, we formulate an optimization problem for the purpose of maximizing the minimum rate of users for each cell via designing the transmit beamforming of the TRTC, subject to the power constraints of each TRTC unit. Since the objective function is non-differentiable, the max-min rate problem is difficult to solve. In order to tackle this challenging optimization problem, an efficient low-complexity optimization algorithm is developed. Specifically, the log-form rate function is transformed into a tractable form by employing the fractional programming (FP) methodology. Next, the max-min objective function can be approximated using a differentiable function derived from smooth approximation theory. Moreover, by applying the majorization-minimization (MM) technique and examining the optimality conditions, a solution is proposed that updates all variables analytically without relying on any numerical solvers. Numerical results are presented to demonstrate the convergence and effectiveness of the proposed low-complexity algorithm. Additionally, the algorithm can significantly reduce the computational complexity without performance loss. Furthermore, the simulation results illustrate the clear superiority of the deployment of the TRTC over the benchmark schemes.

**Link**: [arxiv](http://arxiv.org/abs/2510.25290v1),  [pdf](http://arxiv.org/pdf/2510.25290v1)

**Tags**: eess.SP 



### Expand and Compress: Exploring Tuning Principles for Continual   Spatio-Temporal Graph Forecasting
**Authors**: Wei Chen, Yuxuan Liang

**Updated**: 2025-10-29T08:45:38Z

**Summary**: The widespread deployment of sensing devices leads to a surge in data for spatio-temporal forecasting applications such as traffic flow, air quality, and wind energy. Although spatio-temporal graph neural networks have achieved success in modeling various static spatio-temporal forecasting scenarios, real-world spatio-temporal data are typically received in a streaming manner, and the network continuously expands with the installation of new sensors. Thus, spatio-temporal forecasting in streaming scenarios faces dual challenges: the inefficiency of retraining models over newly arrived data and the detrimental effects of catastrophic forgetting over long-term history. To address these challenges, we propose a novel prompt tuning-based continuous forecasting method, following two fundamental tuning principles guided by empirical and theoretical analysis: expand and compress, which effectively resolve the aforementioned problems with lightweight tuning parameters. Specifically, we integrate the base spatio-temporal graph neural network with a continuous prompt pool, utilizing stored prompts (i.e., few learnable parameters) in memory, and jointly optimize them with the base spatio-temporal graph neural network. This method ensures that the model sequentially learns from the spatio-temporal data stream to accomplish tasks for corresponding periods. Extensive experimental results on multiple real-world datasets demonstrate the multi-faceted superiority of our method over the state-of-the-art baselines, including effectiveness, efficiency, universality, etc.

**Link**: [arxiv](http://arxiv.org/abs/2410.12593v3),  [pdf](http://arxiv.org/pdf/2410.12593v3)

**Tags**: cs.LG cs.AI 



### DIRC-RAG: Accelerating Edge RAG with Robust High-Density and   High-Loading-Bandwidth Digital In-ReRAM Computation
**Authors**: Kunming Shao, Zhipeng Liao, Jiangnan Yu, Liang Zhao, Qiwei Li, Xijie Huang, Jingyu He, Fengshi Tian, Yi Zou, Xiaomeng Wang, Tim Kwang-Ting Cheng, Chi-Ying Tsui

**Updated**: 2025-10-29T08:38:02Z

**Summary**: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieval but faces challenges on edge devices due to high storage, energy, and latency demands. Computing-in-Memory (CIM) offers a promising solution by storing document embeddings in CIM macros and enabling in-situ parallel retrievals but is constrained by either low memory density or limited computational accuracy. To address these challenges, we present DIRCRAG, a novel edge RAG acceleration architecture leveraging Digital In-ReRAM Computation (DIRC). DIRC integrates a high-density multi-level ReRAM subarray with an SRAM cell, utilizing SRAM and differential sensing for robust ReRAM readout and digital multiply-accumulate (MAC) operations. By storing all document embeddings within the CIM macro, DIRC achieves ultra-low-power, single-cycle data loading, substantially reducing both energy consumption and latency compared to offchip DRAM. A query-stationary (QS) dataflow is supported for RAG tasks, minimizing on-chip data movement and reducing SRAM buffer requirements. We introduce error optimization for the DIRC ReRAM-SRAM cell by extracting the bit-wise spatial error distribution of the ReRAM subarray and applying targeted bit-wise data remapping. An error detection circuit is also implemented to enhance readout resilience against deviceand circuit-level variations. Simulation results demonstrate that DIRC-RAG under TSMC40nm process achieves an on-chip non-volatile memory density of 5.18Mb/mm2 and a throughput of 131 TOPS. It delivers a 4MB retrieval latency of 5.6{\mu}s/query and an energy consumption of 0.956{\mu}J/query, while maintaining the retrieval precision.

**Link**: [arxiv](http://arxiv.org/abs/2510.25278v1),  [pdf](http://arxiv.org/pdf/2510.25278v1)

**Tags**: cs.AR 



### Adapting Small Language Models to Low-Resource Domains: A Case Study in   Hindi Tourism QA
**Authors**: Sandipan Majhi, Paheli Bhattacharya

**Updated**: 2025-10-29T08:32:22Z

**Summary**: Domain-specific question answering in low-resource languages faces two key challenges: scarcity of annotated datasets and limited domain knowledge in general-purpose language models. In this work, we present a multi-stage finetuning strategy to adapt lightweight language models to the Hindi tourism domain by leveraging both original and synthetic training data. Synthetic question-answer pairs are generated using large LLMs (LLaMA-70B, Phi-14B) and used to augment the limited original dataset. We explore several training methodologies and analyse their impact on domain generalisation. Our results demonstrate that large models can efficiently generate synthetic data, while small models can effectively adapt to it, offering a scalable pathway for low-resource, domain-specific QA.

**Link**: [arxiv](http://arxiv.org/abs/2510.25273v1),  [pdf](http://arxiv.org/pdf/2510.25273v1)

**Tags**: cs.CL 



### ReSeek: A Self-Correcting Framework for Search Agents with Instructive   Rewards
**Authors**: Shiyu Li, Yang Tang, Yifan Wang, Peiming Li, Xi Chen

**Updated**: 2025-10-29T08:22:54Z

**Summary**: Search agents powered by Large Language Models (LLMs) have demonstrated significant potential in tackling knowledge-intensive tasks. Reinforcement learning (RL) has emerged as a powerful paradigm for training these agents to perform complex, multi-step reasoning. However, prior RL-based methods often rely on sparse or rule-based rewards, which can lead agents to commit to suboptimal or erroneous reasoning paths without the ability to recover. To address these limitations, we propose ReSeek, a novel self-correcting framework for training search agents. Our framework introduces a self-correction mechanism that empowers the agent to dynamically identify and recover from erroneous search paths during an episode. By invoking a special JUDGE action, the agent can judge the information and re-plan its search strategy. To guide this process, we design a dense, instructive process reward function, which decomposes into a correctness reward for retrieving factual information and a utility reward for finding information genuinely useful for the query. Furthermore, to mitigate the risk of data contamination in existing datasets, we introduce FictionalHot, a new and challenging benchmark with recently curated questions requiring complex reasoning. Being intuitively reasonable and practically simple, extensive experiments show that agents trained with ReSeek significantly outperform SOTA baselines in task success rate and path faithfulness.

**Link**: [arxiv](http://arxiv.org/abs/2510.00568v2),  [pdf](http://arxiv.org/pdf/2510.00568v2)

**Tags**: cs.CL 



### Can LLMs Outshine Conventional Recommenders? A Comparative Evaluation
**Authors**: Qijiong Liu, Jieming Zhu, Lu Fan, Kun Wang, Hengchang Hu, Wei Guo, Yong Liu, Xiao-Ming Wu

**Updated**: 2025-10-29T08:19:03Z

**Summary**: In recent years, integrating large language models (LLMs) into recommender systems has created new opportunities for improving recommendation quality. However, a comprehensive benchmark is needed to thoroughly evaluate and compare the recommendation capabilities of LLMs with traditional recommender systems. In this paper, we introduce RecBench, which systematically investigates various item representation forms (including unique identifier, text, semantic embedding, and semantic identifier) and evaluates two primary recommendation tasks, i.e., click-through rate prediction (CTR) and sequential recommendation (SeqRec). Our extensive experiments cover up to 17 large models and are conducted across five diverse datasets from fashion, news, video, books, and music domains. Our findings indicate that LLM-based recommenders outperform conventional recommenders, achieving up to a 5% AUC improvement in the CTR scenario and up to a 170% NDCG@10 improvement in the SeqRec scenario. However, these substantial performance gains come at the expense of significantly reduced inference efficiency, rendering the LLM-as-RS paradigm impractical for real-time recommendation environments. We aim for our findings to inspire future research, including recommendation-specific model acceleration methods. We will release our code, data, configurations, and platform to enable other researchers to reproduce and build upon our experimental results.

**Link**: [arxiv](http://arxiv.org/abs/2503.05493v2),  [pdf](http://arxiv.org/pdf/2503.05493v2)

**Tags**: cs.IR cs.CL 



### MoEntwine: Unleashing the Potential of Wafer-scale Chips for Large-scale   Expert Parallel Inference
**Authors**: Xinru Tang, Jingxiang Hou, Dingcheng Jiang, Taiquan Wei, Jiaxin Liu, Jinyi Deng, Huizheng Wang, Qize Yang, Haoran Shang, Chao Li, Yang Hu, Shouyi Yin

**Updated**: 2025-10-29T08:13:47Z

**Summary**: As large language models (LLMs) continue to scale up, mixture-of-experts (MoE) has become a common technology in SOTA models. MoE models rely on expert parallelism (EP) to alleviate memory bottleneck, which introduces all-to-all communication to dispatch and combine tokens across devices. However, in widely-adopted GPU clusters, high-overhead cross-node communication makes all-to-all expensive, hindering the adoption of EP. Recently, wafer-scale chips (WSCs) have emerged as a platform integrating numerous devices on a wafer-sized interposer. WSCs provide a unified high-performance network connecting all devices, presenting a promising potential for hosting MoE models. Yet, their network is restricted to a mesh topology, causing imbalanced communication pressure and performance loss. Moreover, the lack of on-wafer disk leads to high-overhead expert migration on the critical path.   To fully unleash this potential, we first propose Entwined Ring Mapping (ER-Mapping), which co-designs the mapping of attention and MoE layers to balance communication pressure and achieve better performance. We find that under ER-Mapping, the distribution of cold and hot links in the attention and MoE layers is complementary. Therefore, to hide the migration overhead, we propose the Non-invasive Balancer (NI-Balancer), which splits a complete expert migration into multiple steps and alternately utilizes the cold links of both layers. Evaluation shows ER-Mapping achieves communication reduction up to 62%. NI-Balancer further delivers 54% and 22% improvements in MoE computation and communication, respectively. Compared with the SOTA NVL72 supernode, the WSC platform delivers an average 39% higher per-device MoE performance owing to its scalability to larger EP.

**Link**: [arxiv](http://arxiv.org/abs/2510.25258v1),  [pdf](http://arxiv.org/pdf/2510.25258v1)

**Tags**: cs.DC 



### RT-DETRv4: Painlessly Furthering Real-Time Object Detection with Vision   Foundation Models
**Authors**: Zijun Liao, Yian Zhao, Xin Shan, Yu Yan, Chang Liu, Lei Lu, Xiangyang Ji, Jie Chen

**Updated**: 2025-10-29T08:13:17Z

**Summary**: Real-time object detection has achieved substantial progress through meticulously designed architectures and optimization strategies. However, the pursuit of high-speed inference via lightweight network designs often leads to degraded feature representation, which hinders further performance improvements and practical on-device deployment. In this paper, we propose a cost-effective and highly adaptable distillation framework that harnesses the rapidly evolving capabilities of Vision Foundation Models (VFMs) to enhance lightweight object detectors. Given the significant architectural and learning objective disparities between VFMs and resource-constrained detectors, achieving stable and task-aligned semantic transfer is challenging. To address this, on one hand, we introduce a Deep Semantic Injector (DSI) module that facilitates the integration of high-level representations from VFMs into the deep layers of the detector. On the other hand, we devise a Gradient-guided Adaptive Modulation (GAM) strategy, which dynamically adjusts the intensity of semantic transfer based on gradient norm ratios. Without increasing deployment and inference overhead, our approach painlessly delivers striking and consistent performance gains across diverse DETR-based models, underscoring its practical utility for real-time detection. Our new model family, RT-DETRv4, achieves state-of-the-art results on COCO, attaining AP scores of 49.7/53.5/55.4/57.0 at corresponding speeds of 273/169/124/78 FPS.

**Link**: [arxiv](http://arxiv.org/abs/2510.25257v1),  [pdf](http://arxiv.org/pdf/2510.25257v1)

**Tags**: cs.CV 



### Revisiting Service Level Objectives and System Level Metrics in Large   Language Model Serving
**Authors**: Zhibin Wang, Shipeng Li, Yuhang Zhou, Xue Li, Zhonghui Zhang, Nguyen Cam-Tu, Rong Gu, Chen Tian, Guihai Chen, Sheng Zhong

**Updated**: 2025-10-29T07:56:51Z

**Summary**: User experience is a critical factor Large Language Model (LLM) serving systems must consider, where service level objectives (SLOs) considering the experience of individual requests and system level metrics (SLMs) considering the overall system performance are two key performance measures. However, we observe two notable issues in existing metrics: 1) manually delaying the delivery of some tokens can improve SLOs, and 2) actively abandoning requests that do not meet SLOs can improve SLMs, both of which are counterintuitive.   In this paper, we revisit SLOs and SLMs in LLM serving, and propose a new SLO that aligns with user experience. Based on the SLO, we propose a comprehensive metric framework called smooth goodput, which integrates SLOs and SLMs to reflect the nature of user experience in LLM serving. Through this unified framework, we reassess the performance of different LLM serving systems under multiple workloads. Evaluation results show that our metric framework provides a more comprehensive view of token delivery and request processing, and effectively captures the optimal point of user experience and system performance with different serving strategies.

**Link**: [arxiv](http://arxiv.org/abs/2410.14257v2),  [pdf](http://arxiv.org/pdf/2410.14257v2)

**Tags**: cs.LG cs.AI 



### XY-Cut++: Advanced Layout Ordering via Hierarchical Mask Mechanism on a   Novel Benchmark
**Authors**: Shuai Liu, Youmeng Li, Jizeng Wei

**Updated**: 2025-10-29T07:51:00Z

**Summary**: Document Reading Order Recovery is a fundamental task in document image understanding, playing a pivotal role in enhancing Retrieval-Augmented Generation (RAG) and serving as a critical preprocessing step for large language models (LLMs). Existing methods often struggle with complex layouts(e.g., multi-column newspapers), high-overhead interactions between cross-modal elements (visual regions and textual semantics), and a lack of robust evaluation benchmarks. We introduce XY-Cut++, an advanced layout ordering method that integrates pre-mask processing, multi-granularity segmentation, and cross-modal matching to address these challenges. Our method significantly enhances layout ordering accuracy compared to traditional XY-Cut techniques. Specifically, XY-Cut++ achieves state-of-the-art performance (98.8 BLEU overall) while maintaining simplicity and efficiency. It outperforms existing baselines by up to 24\% and demonstrates consistent accuracy across simple and complex layouts on the newly introduced DocBench-100 dataset. This advancement establishes a reliable foundation for document structure recovery, setting a new standard for layout ordering tasks and facilitating more effective RAG and LLM preprocessing.

**Link**: [arxiv](http://arxiv.org/abs/2504.10258v2),  [pdf](http://arxiv.org/pdf/2504.10258v2)

**Tags**: cs.CV cs.MM 



### MAD-Fact: A Multi-Agent Debate Framework for Long-Form Factuality   Evaluation in LLMs
**Authors**: Yucheng Ning, Xixun Lin, Fang Fang, Yanan Cao

**Updated**: 2025-10-29T07:50:03Z

**Summary**: The widespread adoption of Large Language Models (LLMs) raises critical concerns about the factual accuracy of their outputs, especially in high-risk domains such as biomedicine, law, and education. Existing evaluation methods for short texts often fail on long-form content due to complex reasoning chains, intertwined perspectives, and cumulative information. To address this, we propose a systematic approach integrating large-scale long-form datasets, multi-agent verification mechanisms, and weighted evaluation metrics. We construct LongHalluQA, a Chinese long-form factuality dataset; and develop MAD-Fact, a debate-based multi-agent verification system. We introduce a fact importance hierarchy to capture the varying significance of claims in long-form texts. Experiments on two benchmarks show that larger LLMs generally maintain higher factual consistency, while domestic models excel on Chinese content. Our work provides a structured framework for evaluating and enhancing factual reliability in long-form LLM outputs, guiding their safe deployment in sensitive domains.

**Link**: [arxiv](http://arxiv.org/abs/2510.22967v2),  [pdf](http://arxiv.org/pdf/2510.22967v2)

**Tags**: cs.CL cs.AI 



### NL-Debugging: Exploiting Natural Language as an Intermediate   Representation for Code Debugging
**Authors**: Weiming Zhang, Qingyao Li, Xinyi Dai, Jizheng Chen, Kounianhua Du, Weiwen Liu, Yasheng Wang, Ruiming Tang, Yong Yu, Weinan Zhang

**Updated**: 2025-10-29T07:34:05Z

**Summary**: Debugging is a critical aspect of LLM's coding ability. Early debugging efforts primarily focused on code-level analysis, which often falls short when addressing complex programming errors that require a deeper understanding of algorithmic logic. Recent advancements in large language models (LLMs) have shifted attention toward leveraging natural language reasoning to enhance code-related tasks. However, two fundamental questions remain unanswered: What type of natural language format is most effective for debugging tasks? And what specific benefits does natural language reasoning bring to the debugging process? In this paper, we introduce NL-DEBUGGING, a novel framework that employs natural language as an intermediate representation to improve code debugging. By debugging at a natural language level, we demonstrate that NL-DEBUGGING outperforms traditional debugging methods and enables a broader modification space through direct refinement guided by execution feedback. Our findings highlight the potential of natural language reasoning to advance automated code debugging and address complex programming challenges.

**Link**: [arxiv](http://arxiv.org/abs/2505.15356v2),  [pdf](http://arxiv.org/pdf/2505.15356v2)

**Tags**: cs.CL 



### WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and   Interaction
**Authors**: Binbin Zhang, Chengdong Liang, Shuai Wang, Xuelong Geng, Zhao Guo, Haoyu Li, Hao Yin, Xipeng Yang, Pengshen Zhang, Changwei Ma, Lei Xie

**Updated**: 2025-10-29T07:17:51Z

**Summary**: In this paper, we present WEST(WE Speech Toolkit), a speech toolkit based on a large language model (LLM) for speech understanding, generation, and interaction. There are three key features of WEST: 1) Fully LLM-based: Standing on the shoulders of giants by reusing mature architectures, ecosystems (e.g., Hugging Face), and methods (e.g., sequence packing) from large models. 2) Full-stack: Supports tasks such as recognition, synthesis, understanding, dialogue, and multimodal capabilities, with extensibility to incorporate open-source models. 3) Simple and Stupid: A simple and stupid speech toolkit that everyone can Touch. In addition, WEST provides two types of recipes, models, and experimental results. The first is entirely based on open-source models and open-source data, allowing users to fully reproduce the experiments in this paper and serving as a verification system or minimal system baseline. The second is trained on massive data, offering superior performance so the user can directly apply it out of the box. WEST is publicly avilable at https://github.com/wenet-e2e/west/

**Link**: [arxiv](http://arxiv.org/abs/2509.19902v2),  [pdf](http://arxiv.org/pdf/2509.19902v2)

**Tags**: cs.CL 



